<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260114.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Comprehensive language-image pre-training for 3D medical image understanding", "author": "Tassilo Wald and Ibrahim Ethem Hamamci and Yuan Gao and Sam Bond-Taylor and Harshita Sharma and Maximilian Ilse and Cynthia Lo and Olesya Melnichenko and Anton Schwaighofer and Noel C. F. Codella and Maria Teodora Wetscherek and Klaus H. Maier-Hein and Panagiotis Korfiatis and Valentina Salvatelli and Javier Alvarez-Valle and Fernando P\u00e9rez-Garc\u00eda", "abstract": "Vision-language pre-training, i.e., aligning images with paired text, is a powerful paradigm to create encoders that can be directly used for tasks such as classification, retrieval, and segmentation. In the 3D medical image domain, these capabilities allow vision-language encoders (VLEs) to support radiologists by retrieving patients with similar abnormalities, predicting likelihoods of abnormality, or, with downstream adaptation, generating radiological reports. While the methodology holds promise, data availability and domain-specific hurdles limit the capabilities of current 3D VLEs.\n  In this paper, we overcome these challenges by injecting additional supervision via a report generation objective and combining vision-language with vision-only pre-training. This allows us to leverage both image-only and paired image-text 3D datasets, increasing the total amount of data to which our model is exposed. Through these additional objectives, paired with best practices of the 3D medical imaging domain, we develop the Comprehensive Language-Image Pre-training (COLIPRI) encoder family. Our COLIPRI encoders achieve state-of-the-art performance in report generation, semantic segmentation, classification probing, and zero-shot classification. The model is available at https://huggingface.co/microsoft/colipri.", "link": "http://arxiv.org/abs/2510.15042v2", "date": "2026-01-14", "relevancy": 3.2205, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6616}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehensive%20language-image%20pre-training%20for%203D%20medical%20image%20understanding&body=Title%3A%20Comprehensive%20language-image%20pre-training%20for%203D%20medical%20image%20understanding%0AAuthor%3A%20Tassilo%20Wald%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Yuan%20Gao%20and%20Sam%20Bond-Taylor%20and%20Harshita%20Sharma%20and%20Maximilian%20Ilse%20and%20Cynthia%20Lo%20and%20Olesya%20Melnichenko%20and%20Anton%20Schwaighofer%20and%20Noel%20C.%20F.%20Codella%20and%20Maria%20Teodora%20Wetscherek%20and%20Klaus%20H.%20Maier-Hein%20and%20Panagiotis%20Korfiatis%20and%20Valentina%20Salvatelli%20and%20Javier%20Alvarez-Valle%20and%20Fernando%20P%C3%A9rez-Garc%C3%ADa%0AAbstract%3A%20Vision-language%20pre-training%2C%20i.e.%2C%20aligning%20images%20with%20paired%20text%2C%20is%20a%20powerful%20paradigm%20to%20create%20encoders%20that%20can%20be%20directly%20used%20for%20tasks%20such%20as%20classification%2C%20retrieval%2C%20and%20segmentation.%20In%20the%203D%20medical%20image%20domain%2C%20these%20capabilities%20allow%20vision-language%20encoders%20%28VLEs%29%20to%20support%20radiologists%20by%20retrieving%20patients%20with%20similar%20abnormalities%2C%20predicting%20likelihoods%20of%20abnormality%2C%20or%2C%20with%20downstream%20adaptation%2C%20generating%20radiological%20reports.%20While%20the%20methodology%20holds%20promise%2C%20data%20availability%20and%20domain-specific%20hurdles%20limit%20the%20capabilities%20of%20current%203D%20VLEs.%0A%20%20In%20this%20paper%2C%20we%20overcome%20these%20challenges%20by%20injecting%20additional%20supervision%20via%20a%20report%20generation%20objective%20and%20combining%20vision-language%20with%20vision-only%20pre-training.%20This%20allows%20us%20to%20leverage%20both%20image-only%20and%20paired%20image-text%203D%20datasets%2C%20increasing%20the%20total%20amount%20of%20data%20to%20which%20our%20model%20is%20exposed.%20Through%20these%20additional%20objectives%2C%20paired%20with%20best%20practices%20of%20the%203D%20medical%20imaging%20domain%2C%20we%20develop%20the%20Comprehensive%20Language-Image%20Pre-training%20%28COLIPRI%29%20encoder%20family.%20Our%20COLIPRI%20encoders%20achieve%20state-of-the-art%20performance%20in%20report%20generation%2C%20semantic%20segmentation%2C%20classification%20probing%2C%20and%20zero-shot%20classification.%20The%20model%20is%20available%20at%20https%3A//huggingface.co/microsoft/colipri.%0ALink%3A%20http%3A//arxiv.org/abs/2510.15042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehensive%2520language-image%2520pre-training%2520for%25203D%2520medical%2520image%2520understanding%26entry.906535625%3DTassilo%2520Wald%2520and%2520Ibrahim%2520Ethem%2520Hamamci%2520and%2520Yuan%2520Gao%2520and%2520Sam%2520Bond-Taylor%2520and%2520Harshita%2520Sharma%2520and%2520Maximilian%2520Ilse%2520and%2520Cynthia%2520Lo%2520and%2520Olesya%2520Melnichenko%2520and%2520Anton%2520Schwaighofer%2520and%2520Noel%2520C.%2520F.%2520Codella%2520and%2520Maria%2520Teodora%2520Wetscherek%2520and%2520Klaus%2520H.%2520Maier-Hein%2520and%2520Panagiotis%2520Korfiatis%2520and%2520Valentina%2520Salvatelli%2520and%2520Javier%2520Alvarez-Valle%2520and%2520Fernando%2520P%25C3%25A9rez-Garc%25C3%25ADa%26entry.1292438233%3DVision-language%2520pre-training%252C%2520i.e.%252C%2520aligning%2520images%2520with%2520paired%2520text%252C%2520is%2520a%2520powerful%2520paradigm%2520to%2520create%2520encoders%2520that%2520can%2520be%2520directly%2520used%2520for%2520tasks%2520such%2520as%2520classification%252C%2520retrieval%252C%2520and%2520segmentation.%2520In%2520the%25203D%2520medical%2520image%2520domain%252C%2520these%2520capabilities%2520allow%2520vision-language%2520encoders%2520%2528VLEs%2529%2520to%2520support%2520radiologists%2520by%2520retrieving%2520patients%2520with%2520similar%2520abnormalities%252C%2520predicting%2520likelihoods%2520of%2520abnormality%252C%2520or%252C%2520with%2520downstream%2520adaptation%252C%2520generating%2520radiological%2520reports.%2520While%2520the%2520methodology%2520holds%2520promise%252C%2520data%2520availability%2520and%2520domain-specific%2520hurdles%2520limit%2520the%2520capabilities%2520of%2520current%25203D%2520VLEs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520overcome%2520these%2520challenges%2520by%2520injecting%2520additional%2520supervision%2520via%2520a%2520report%2520generation%2520objective%2520and%2520combining%2520vision-language%2520with%2520vision-only%2520pre-training.%2520This%2520allows%2520us%2520to%2520leverage%2520both%2520image-only%2520and%2520paired%2520image-text%25203D%2520datasets%252C%2520increasing%2520the%2520total%2520amount%2520of%2520data%2520to%2520which%2520our%2520model%2520is%2520exposed.%2520Through%2520these%2520additional%2520objectives%252C%2520paired%2520with%2520best%2520practices%2520of%2520the%25203D%2520medical%2520imaging%2520domain%252C%2520we%2520develop%2520the%2520Comprehensive%2520Language-Image%2520Pre-training%2520%2528COLIPRI%2529%2520encoder%2520family.%2520Our%2520COLIPRI%2520encoders%2520achieve%2520state-of-the-art%2520performance%2520in%2520report%2520generation%252C%2520semantic%2520segmentation%252C%2520classification%2520probing%252C%2520and%2520zero-shot%2520classification.%2520The%2520model%2520is%2520available%2520at%2520https%253A//huggingface.co/microsoft/colipri.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehensive%20language-image%20pre-training%20for%203D%20medical%20image%20understanding&entry.906535625=Tassilo%20Wald%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Yuan%20Gao%20and%20Sam%20Bond-Taylor%20and%20Harshita%20Sharma%20and%20Maximilian%20Ilse%20and%20Cynthia%20Lo%20and%20Olesya%20Melnichenko%20and%20Anton%20Schwaighofer%20and%20Noel%20C.%20F.%20Codella%20and%20Maria%20Teodora%20Wetscherek%20and%20Klaus%20H.%20Maier-Hein%20and%20Panagiotis%20Korfiatis%20and%20Valentina%20Salvatelli%20and%20Javier%20Alvarez-Valle%20and%20Fernando%20P%C3%A9rez-Garc%C3%ADa&entry.1292438233=Vision-language%20pre-training%2C%20i.e.%2C%20aligning%20images%20with%20paired%20text%2C%20is%20a%20powerful%20paradigm%20to%20create%20encoders%20that%20can%20be%20directly%20used%20for%20tasks%20such%20as%20classification%2C%20retrieval%2C%20and%20segmentation.%20In%20the%203D%20medical%20image%20domain%2C%20these%20capabilities%20allow%20vision-language%20encoders%20%28VLEs%29%20to%20support%20radiologists%20by%20retrieving%20patients%20with%20similar%20abnormalities%2C%20predicting%20likelihoods%20of%20abnormality%2C%20or%2C%20with%20downstream%20adaptation%2C%20generating%20radiological%20reports.%20While%20the%20methodology%20holds%20promise%2C%20data%20availability%20and%20domain-specific%20hurdles%20limit%20the%20capabilities%20of%20current%203D%20VLEs.%0A%20%20In%20this%20paper%2C%20we%20overcome%20these%20challenges%20by%20injecting%20additional%20supervision%20via%20a%20report%20generation%20objective%20and%20combining%20vision-language%20with%20vision-only%20pre-training.%20This%20allows%20us%20to%20leverage%20both%20image-only%20and%20paired%20image-text%203D%20datasets%2C%20increasing%20the%20total%20amount%20of%20data%20to%20which%20our%20model%20is%20exposed.%20Through%20these%20additional%20objectives%2C%20paired%20with%20best%20practices%20of%20the%203D%20medical%20imaging%20domain%2C%20we%20develop%20the%20Comprehensive%20Language-Image%20Pre-training%20%28COLIPRI%29%20encoder%20family.%20Our%20COLIPRI%20encoders%20achieve%20state-of-the-art%20performance%20in%20report%20generation%2C%20semantic%20segmentation%2C%20classification%20probing%2C%20and%20zero-shot%20classification.%20The%20model%20is%20available%20at%20https%3A//huggingface.co/microsoft/colipri.&entry.1838667208=http%3A//arxiv.org/abs/2510.15042v2&entry.124074799=Read"},
{"title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding", "author": "Sheng-Yu Huang and Jaesung Choe and Yu-Chiang Frank Wang and Cheng Sun", "abstract": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.", "link": "http://arxiv.org/abs/2601.09575v1", "date": "2026-01-14", "relevancy": 3.1591, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenVoxel%3A%20Training-Free%20Grouping%20and%20Captioning%20Voxels%20for%20Open-Vocabulary%203D%20Scene%20Understanding&body=Title%3A%20OpenVoxel%3A%20Training-Free%20Grouping%20and%20Captioning%20Voxels%20for%20Open-Vocabulary%203D%20Scene%20Understanding%0AAuthor%3A%20Sheng-Yu%20Huang%20and%20Jaesung%20Choe%20and%20Yu-Chiang%20Frank%20Wang%20and%20Cheng%20Sun%0AAbstract%3A%20We%20propose%20OpenVoxel%2C%20a%20training-free%20algorithm%20for%20grouping%20and%20captioning%20sparse%20voxels%20for%20the%20open-vocabulary%203D%20scene%20understanding%20tasks.%20Given%20the%20sparse%20voxel%20rasterization%20%28SVR%29%20model%20obtained%20from%20multi-view%20images%20of%20a%203D%20scene%2C%20our%20OpenVoxel%20is%20able%20to%20produce%20meaningful%20groups%20that%20describe%20different%20objects%20in%20the%20scene.%20Also%2C%20by%20leveraging%20powerful%20Vision%20Language%20Models%20%28VLMs%29%20and%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20our%20OpenVoxel%20successfully%20build%20an%20informative%20scene%20map%20by%20captioning%20each%20group%2C%20enabling%20further%203D%20scene%20understanding%20tasks%20such%20as%20open-vocabulary%20segmentation%20%28OVS%29%20or%20referring%20expression%20segmentation%20%28RES%29.%20Unlike%20previous%20methods%2C%20our%20method%20is%20training-free%20and%20does%20not%20introduce%20embeddings%20from%20a%20CLIP/BERT%20text%20encoder.%20Instead%2C%20we%20directly%20proceed%20with%20text-to-text%20search%20using%20MLLMs.%20Through%20extensive%20experiments%2C%20our%20method%20demonstrates%20superior%20performance%20compared%20to%20recent%20studies%2C%20particularly%20in%20complex%20referring%20expression%20segmentation%20%28RES%29%20tasks.%20The%20code%20will%20be%20open.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenVoxel%253A%2520Training-Free%2520Grouping%2520and%2520Captioning%2520Voxels%2520for%2520Open-Vocabulary%25203D%2520Scene%2520Understanding%26entry.906535625%3DSheng-Yu%2520Huang%2520and%2520Jaesung%2520Choe%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Cheng%2520Sun%26entry.1292438233%3DWe%2520propose%2520OpenVoxel%252C%2520a%2520training-free%2520algorithm%2520for%2520grouping%2520and%2520captioning%2520sparse%2520voxels%2520for%2520the%2520open-vocabulary%25203D%2520scene%2520understanding%2520tasks.%2520Given%2520the%2520sparse%2520voxel%2520rasterization%2520%2528SVR%2529%2520model%2520obtained%2520from%2520multi-view%2520images%2520of%2520a%25203D%2520scene%252C%2520our%2520OpenVoxel%2520is%2520able%2520to%2520produce%2520meaningful%2520groups%2520that%2520describe%2520different%2520objects%2520in%2520the%2520scene.%2520Also%252C%2520by%2520leveraging%2520powerful%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520and%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520our%2520OpenVoxel%2520successfully%2520build%2520an%2520informative%2520scene%2520map%2520by%2520captioning%2520each%2520group%252C%2520enabling%2520further%25203D%2520scene%2520understanding%2520tasks%2520such%2520as%2520open-vocabulary%2520segmentation%2520%2528OVS%2529%2520or%2520referring%2520expression%2520segmentation%2520%2528RES%2529.%2520Unlike%2520previous%2520methods%252C%2520our%2520method%2520is%2520training-free%2520and%2520does%2520not%2520introduce%2520embeddings%2520from%2520a%2520CLIP/BERT%2520text%2520encoder.%2520Instead%252C%2520we%2520directly%2520proceed%2520with%2520text-to-text%2520search%2520using%2520MLLMs.%2520Through%2520extensive%2520experiments%252C%2520our%2520method%2520demonstrates%2520superior%2520performance%2520compared%2520to%2520recent%2520studies%252C%2520particularly%2520in%2520complex%2520referring%2520expression%2520segmentation%2520%2528RES%2529%2520tasks.%2520The%2520code%2520will%2520be%2520open.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenVoxel%3A%20Training-Free%20Grouping%20and%20Captioning%20Voxels%20for%20Open-Vocabulary%203D%20Scene%20Understanding&entry.906535625=Sheng-Yu%20Huang%20and%20Jaesung%20Choe%20and%20Yu-Chiang%20Frank%20Wang%20and%20Cheng%20Sun&entry.1292438233=We%20propose%20OpenVoxel%2C%20a%20training-free%20algorithm%20for%20grouping%20and%20captioning%20sparse%20voxels%20for%20the%20open-vocabulary%203D%20scene%20understanding%20tasks.%20Given%20the%20sparse%20voxel%20rasterization%20%28SVR%29%20model%20obtained%20from%20multi-view%20images%20of%20a%203D%20scene%2C%20our%20OpenVoxel%20is%20able%20to%20produce%20meaningful%20groups%20that%20describe%20different%20objects%20in%20the%20scene.%20Also%2C%20by%20leveraging%20powerful%20Vision%20Language%20Models%20%28VLMs%29%20and%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20our%20OpenVoxel%20successfully%20build%20an%20informative%20scene%20map%20by%20captioning%20each%20group%2C%20enabling%20further%203D%20scene%20understanding%20tasks%20such%20as%20open-vocabulary%20segmentation%20%28OVS%29%20or%20referring%20expression%20segmentation%20%28RES%29.%20Unlike%20previous%20methods%2C%20our%20method%20is%20training-free%20and%20does%20not%20introduce%20embeddings%20from%20a%20CLIP/BERT%20text%20encoder.%20Instead%2C%20we%20directly%20proceed%20with%20text-to-text%20search%20using%20MLLMs.%20Through%20extensive%20experiments%2C%20our%20method%20demonstrates%20superior%20performance%20compared%20to%20recent%20studies%2C%20particularly%20in%20complex%20referring%20expression%20segmentation%20%28RES%29%20tasks.%20The%20code%20will%20be%20open.&entry.1838667208=http%3A//arxiv.org/abs/2601.09575v1&entry.124074799=Read"},
{"title": "Beyond the final layer: Attentive multilayer fusion for vision transformers", "author": "Laure Ciernik and Marco Morik and Lukas Thede and Luca Eyring and Shinichi Nakajima and Zeynep Akata and Lukas Muttenthaler", "abstract": "With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.", "link": "http://arxiv.org/abs/2601.09322v1", "date": "2026-01-14", "relevancy": 2.998, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.614}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20final%20layer%3A%20Attentive%20multilayer%20fusion%20for%20vision%20transformers&body=Title%3A%20Beyond%20the%20final%20layer%3A%20Attentive%20multilayer%20fusion%20for%20vision%20transformers%0AAuthor%3A%20Laure%20Ciernik%20and%20Marco%20Morik%20and%20Lukas%20Thede%20and%20Luca%20Eyring%20and%20Shinichi%20Nakajima%20and%20Zeynep%20Akata%20and%20Lukas%20Muttenthaler%0AAbstract%3A%20With%20the%20rise%20of%20large-scale%20foundation%20models%2C%20efficiently%20adapting%20them%20to%20downstream%20tasks%20remains%20a%20central%20challenge.%20Linear%20probing%2C%20which%20freezes%20the%20backbone%20and%20trains%20a%20lightweight%20head%2C%20is%20computationally%20efficient%20but%20often%20restricted%20to%20last-layer%20representations.%20We%20show%20that%20task-relevant%20information%20is%20distributed%20across%20the%20network%20hierarchy%20rather%20than%20solely%20encoded%20in%20any%20of%20the%20last%20layers.%20To%20leverage%20this%20distribution%20of%20information%2C%20we%20apply%20an%20attentive%20probing%20mechanism%20that%20dynamically%20fuses%20representations%20from%20all%20layers%20of%20a%20Vision%20Transformer.%20This%20mechanism%20learns%20to%20identify%20the%20most%20relevant%20layers%20for%20a%20target%20task%20and%20combines%20low-level%20structural%20cues%20with%20high-level%20semantic%20abstractions.%20Across%2020%20diverse%20datasets%20and%20multiple%20pretrained%20foundation%20models%2C%20our%20method%20achieves%20consistent%2C%20substantial%20gains%20over%20standard%20linear%20probes.%20Attention%20heatmaps%20further%20reveal%20that%20tasks%20different%20from%20the%20pre-training%20domain%20benefit%20most%20from%20intermediate%20representations.%20Overall%2C%20our%20findings%20underscore%20the%20value%20of%20intermediate%20layer%20information%20and%20demonstrate%20a%20principled%2C%20task%20aware%20approach%20for%20unlocking%20their%20potential%20in%20probing-based%20adaptation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520final%2520layer%253A%2520Attentive%2520multilayer%2520fusion%2520for%2520vision%2520transformers%26entry.906535625%3DLaure%2520Ciernik%2520and%2520Marco%2520Morik%2520and%2520Lukas%2520Thede%2520and%2520Luca%2520Eyring%2520and%2520Shinichi%2520Nakajima%2520and%2520Zeynep%2520Akata%2520and%2520Lukas%2520Muttenthaler%26entry.1292438233%3DWith%2520the%2520rise%2520of%2520large-scale%2520foundation%2520models%252C%2520efficiently%2520adapting%2520them%2520to%2520downstream%2520tasks%2520remains%2520a%2520central%2520challenge.%2520Linear%2520probing%252C%2520which%2520freezes%2520the%2520backbone%2520and%2520trains%2520a%2520lightweight%2520head%252C%2520is%2520computationally%2520efficient%2520but%2520often%2520restricted%2520to%2520last-layer%2520representations.%2520We%2520show%2520that%2520task-relevant%2520information%2520is%2520distributed%2520across%2520the%2520network%2520hierarchy%2520rather%2520than%2520solely%2520encoded%2520in%2520any%2520of%2520the%2520last%2520layers.%2520To%2520leverage%2520this%2520distribution%2520of%2520information%252C%2520we%2520apply%2520an%2520attentive%2520probing%2520mechanism%2520that%2520dynamically%2520fuses%2520representations%2520from%2520all%2520layers%2520of%2520a%2520Vision%2520Transformer.%2520This%2520mechanism%2520learns%2520to%2520identify%2520the%2520most%2520relevant%2520layers%2520for%2520a%2520target%2520task%2520and%2520combines%2520low-level%2520structural%2520cues%2520with%2520high-level%2520semantic%2520abstractions.%2520Across%252020%2520diverse%2520datasets%2520and%2520multiple%2520pretrained%2520foundation%2520models%252C%2520our%2520method%2520achieves%2520consistent%252C%2520substantial%2520gains%2520over%2520standard%2520linear%2520probes.%2520Attention%2520heatmaps%2520further%2520reveal%2520that%2520tasks%2520different%2520from%2520the%2520pre-training%2520domain%2520benefit%2520most%2520from%2520intermediate%2520representations.%2520Overall%252C%2520our%2520findings%2520underscore%2520the%2520value%2520of%2520intermediate%2520layer%2520information%2520and%2520demonstrate%2520a%2520principled%252C%2520task%2520aware%2520approach%2520for%2520unlocking%2520their%2520potential%2520in%2520probing-based%2520adaptation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20final%20layer%3A%20Attentive%20multilayer%20fusion%20for%20vision%20transformers&entry.906535625=Laure%20Ciernik%20and%20Marco%20Morik%20and%20Lukas%20Thede%20and%20Luca%20Eyring%20and%20Shinichi%20Nakajima%20and%20Zeynep%20Akata%20and%20Lukas%20Muttenthaler&entry.1292438233=With%20the%20rise%20of%20large-scale%20foundation%20models%2C%20efficiently%20adapting%20them%20to%20downstream%20tasks%20remains%20a%20central%20challenge.%20Linear%20probing%2C%20which%20freezes%20the%20backbone%20and%20trains%20a%20lightweight%20head%2C%20is%20computationally%20efficient%20but%20often%20restricted%20to%20last-layer%20representations.%20We%20show%20that%20task-relevant%20information%20is%20distributed%20across%20the%20network%20hierarchy%20rather%20than%20solely%20encoded%20in%20any%20of%20the%20last%20layers.%20To%20leverage%20this%20distribution%20of%20information%2C%20we%20apply%20an%20attentive%20probing%20mechanism%20that%20dynamically%20fuses%20representations%20from%20all%20layers%20of%20a%20Vision%20Transformer.%20This%20mechanism%20learns%20to%20identify%20the%20most%20relevant%20layers%20for%20a%20target%20task%20and%20combines%20low-level%20structural%20cues%20with%20high-level%20semantic%20abstractions.%20Across%2020%20diverse%20datasets%20and%20multiple%20pretrained%20foundation%20models%2C%20our%20method%20achieves%20consistent%2C%20substantial%20gains%20over%20standard%20linear%20probes.%20Attention%20heatmaps%20further%20reveal%20that%20tasks%20different%20from%20the%20pre-training%20domain%20benefit%20most%20from%20intermediate%20representations.%20Overall%2C%20our%20findings%20underscore%20the%20value%20of%20intermediate%20layer%20information%20and%20demonstrate%20a%20principled%2C%20task%20aware%20approach%20for%20unlocking%20their%20potential%20in%20probing-based%20adaptation.&entry.1838667208=http%3A//arxiv.org/abs/2601.09322v1&entry.124074799=Read"},
{"title": "Self-Supervised Animal Identification for Long Videos", "author": "Xuyang Fang and Sion Hannuna and Edwin Simpson and Neill Campbell", "abstract": "Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \\href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.", "link": "http://arxiv.org/abs/2601.09663v1", "date": "2026-01-14", "relevancy": 2.9749, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6219}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5843}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Animal%20Identification%20for%20Long%20Videos&body=Title%3A%20Self-Supervised%20Animal%20Identification%20for%20Long%20Videos%0AAuthor%3A%20Xuyang%20Fang%20and%20Sion%20Hannuna%20and%20Edwin%20Simpson%20and%20Neill%20Campbell%0AAbstract%3A%20Identifying%20individual%20animals%20in%20long-duration%20videos%20is%20essential%20for%20behavioral%20ecology%2C%20wildlife%20monitoring%2C%20and%20livestock%20management.%20Traditional%20methods%20require%20extensive%20manual%20annotation%2C%20while%20existing%20self-supervised%20approaches%20are%20computationally%20demanding%20and%20ill-suited%20for%20long%20sequences%20due%20to%20memory%20constraints%20and%20temporal%20error%20propagation.%20We%20introduce%20a%20highly%20efficient%2C%20self-supervised%20method%20that%20reframes%20animal%20identification%20as%20a%20global%20clustering%20task%20rather%20than%20a%20sequential%20tracking%20problem.%20Our%20approach%20assumes%20a%20known%2C%20fixed%20number%20of%20individuals%20within%20a%20single%20video%20--%20a%20common%20scenario%20in%20practice%20--%20and%20requires%20only%20bounding%20box%20detections%20and%20the%20total%20count.%20By%20sampling%20pairs%20of%20frames%2C%20using%20a%20frozen%20pre-trained%20backbone%2C%20and%20employing%20a%20self-bootstrapping%20mechanism%20with%20the%20Hungarian%20algorithm%20for%20in-batch%20pseudo-label%20assignment%2C%20our%20method%20learns%20discriminative%20features%20without%20identity%20labels.%20We%20adapt%20a%20Binary%20Cross%20Entropy%20loss%20from%20vision-language%20models%2C%20enabling%20state-of-the-art%20accuracy%20%28%24%3E%2497%5C%25%29%20while%20consuming%20less%20than%201%20GB%20of%20GPU%20memory%20per%20batch%20--%20an%20order%20of%20magnitude%20less%20than%20standard%20contrastive%20methods.%20Evaluated%20on%20challenging%20real-world%20datasets%20%283D-POP%20pigeons%20and%208-calves%20feeding%20videos%29%2C%20our%20framework%20matches%20or%20surpasses%20supervised%20baselines%20trained%20on%20over%201%2C000%20labeled%20frames%2C%20effectively%20removing%20the%20manual%20annotation%20bottleneck.%20This%20work%20enables%20practical%2C%20high-accuracy%20animal%20identification%20on%20consumer-grade%20hardware%2C%20with%20broad%20applicability%20in%20resource-constrained%20research%20settings.%20All%20code%20written%20for%20this%20paper%20are%20%5Chref%7Bhttps%3A//huggingface.co/datasets/tonyFang04/8-calves%7D%7Bhere%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Animal%2520Identification%2520for%2520Long%2520Videos%26entry.906535625%3DXuyang%2520Fang%2520and%2520Sion%2520Hannuna%2520and%2520Edwin%2520Simpson%2520and%2520Neill%2520Campbell%26entry.1292438233%3DIdentifying%2520individual%2520animals%2520in%2520long-duration%2520videos%2520is%2520essential%2520for%2520behavioral%2520ecology%252C%2520wildlife%2520monitoring%252C%2520and%2520livestock%2520management.%2520Traditional%2520methods%2520require%2520extensive%2520manual%2520annotation%252C%2520while%2520existing%2520self-supervised%2520approaches%2520are%2520computationally%2520demanding%2520and%2520ill-suited%2520for%2520long%2520sequences%2520due%2520to%2520memory%2520constraints%2520and%2520temporal%2520error%2520propagation.%2520We%2520introduce%2520a%2520highly%2520efficient%252C%2520self-supervised%2520method%2520that%2520reframes%2520animal%2520identification%2520as%2520a%2520global%2520clustering%2520task%2520rather%2520than%2520a%2520sequential%2520tracking%2520problem.%2520Our%2520approach%2520assumes%2520a%2520known%252C%2520fixed%2520number%2520of%2520individuals%2520within%2520a%2520single%2520video%2520--%2520a%2520common%2520scenario%2520in%2520practice%2520--%2520and%2520requires%2520only%2520bounding%2520box%2520detections%2520and%2520the%2520total%2520count.%2520By%2520sampling%2520pairs%2520of%2520frames%252C%2520using%2520a%2520frozen%2520pre-trained%2520backbone%252C%2520and%2520employing%2520a%2520self-bootstrapping%2520mechanism%2520with%2520the%2520Hungarian%2520algorithm%2520for%2520in-batch%2520pseudo-label%2520assignment%252C%2520our%2520method%2520learns%2520discriminative%2520features%2520without%2520identity%2520labels.%2520We%2520adapt%2520a%2520Binary%2520Cross%2520Entropy%2520loss%2520from%2520vision-language%2520models%252C%2520enabling%2520state-of-the-art%2520accuracy%2520%2528%2524%253E%252497%255C%2525%2529%2520while%2520consuming%2520less%2520than%25201%2520GB%2520of%2520GPU%2520memory%2520per%2520batch%2520--%2520an%2520order%2520of%2520magnitude%2520less%2520than%2520standard%2520contrastive%2520methods.%2520Evaluated%2520on%2520challenging%2520real-world%2520datasets%2520%25283D-POP%2520pigeons%2520and%25208-calves%2520feeding%2520videos%2529%252C%2520our%2520framework%2520matches%2520or%2520surpasses%2520supervised%2520baselines%2520trained%2520on%2520over%25201%252C000%2520labeled%2520frames%252C%2520effectively%2520removing%2520the%2520manual%2520annotation%2520bottleneck.%2520This%2520work%2520enables%2520practical%252C%2520high-accuracy%2520animal%2520identification%2520on%2520consumer-grade%2520hardware%252C%2520with%2520broad%2520applicability%2520in%2520resource-constrained%2520research%2520settings.%2520All%2520code%2520written%2520for%2520this%2520paper%2520are%2520%255Chref%257Bhttps%253A//huggingface.co/datasets/tonyFang04/8-calves%257D%257Bhere%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Animal%20Identification%20for%20Long%20Videos&entry.906535625=Xuyang%20Fang%20and%20Sion%20Hannuna%20and%20Edwin%20Simpson%20and%20Neill%20Campbell&entry.1292438233=Identifying%20individual%20animals%20in%20long-duration%20videos%20is%20essential%20for%20behavioral%20ecology%2C%20wildlife%20monitoring%2C%20and%20livestock%20management.%20Traditional%20methods%20require%20extensive%20manual%20annotation%2C%20while%20existing%20self-supervised%20approaches%20are%20computationally%20demanding%20and%20ill-suited%20for%20long%20sequences%20due%20to%20memory%20constraints%20and%20temporal%20error%20propagation.%20We%20introduce%20a%20highly%20efficient%2C%20self-supervised%20method%20that%20reframes%20animal%20identification%20as%20a%20global%20clustering%20task%20rather%20than%20a%20sequential%20tracking%20problem.%20Our%20approach%20assumes%20a%20known%2C%20fixed%20number%20of%20individuals%20within%20a%20single%20video%20--%20a%20common%20scenario%20in%20practice%20--%20and%20requires%20only%20bounding%20box%20detections%20and%20the%20total%20count.%20By%20sampling%20pairs%20of%20frames%2C%20using%20a%20frozen%20pre-trained%20backbone%2C%20and%20employing%20a%20self-bootstrapping%20mechanism%20with%20the%20Hungarian%20algorithm%20for%20in-batch%20pseudo-label%20assignment%2C%20our%20method%20learns%20discriminative%20features%20without%20identity%20labels.%20We%20adapt%20a%20Binary%20Cross%20Entropy%20loss%20from%20vision-language%20models%2C%20enabling%20state-of-the-art%20accuracy%20%28%24%3E%2497%5C%25%29%20while%20consuming%20less%20than%201%20GB%20of%20GPU%20memory%20per%20batch%20--%20an%20order%20of%20magnitude%20less%20than%20standard%20contrastive%20methods.%20Evaluated%20on%20challenging%20real-world%20datasets%20%283D-POP%20pigeons%20and%208-calves%20feeding%20videos%29%2C%20our%20framework%20matches%20or%20surpasses%20supervised%20baselines%20trained%20on%20over%201%2C000%20labeled%20frames%2C%20effectively%20removing%20the%20manual%20annotation%20bottleneck.%20This%20work%20enables%20practical%2C%20high-accuracy%20animal%20identification%20on%20consumer-grade%20hardware%2C%20with%20broad%20applicability%20in%20resource-constrained%20research%20settings.%20All%20code%20written%20for%20this%20paper%20are%20%5Chref%7Bhttps%3A//huggingface.co/datasets/tonyFang04/8-calves%7D%7Bhere%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.09663v1&entry.124074799=Read"},
{"title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps", "author": "Edgar Sucar and Eldar Insafutdinov and Zihang Lai and Andrea Vedaldi", "abstract": "Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.", "link": "http://arxiv.org/abs/2601.09499v1", "date": "2026-01-14", "relevancy": 2.9564, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.62}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5769}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-DPM%3A%204D%20Video%20Reconstruction%20with%20Dynamic%20Point%20Maps&body=Title%3A%20V-DPM%3A%204D%20Video%20Reconstruction%20with%20Dynamic%20Point%20Maps%0AAuthor%3A%20Edgar%20Sucar%20and%20Eldar%20Insafutdinov%20and%20Zihang%20Lai%20and%20Andrea%20Vedaldi%0AAbstract%3A%20Powerful%203D%20representations%20such%20as%20DUSt3R%20invariant%20point%20maps%2C%20which%20encode%203D%20shape%20and%20camera%20parameters%2C%20have%20significantly%20advanced%20feed%20forward%203D%20reconstruction.%20While%20point%20maps%20assume%20static%20scenes%2C%20Dynamic%20Point%20Maps%20%28DPMs%29%20extend%20this%20concept%20to%20dynamic%203D%20content%20by%20additionally%20representing%20scene%20motion.%20However%2C%20existing%20DPMs%20are%20limited%20to%20image%20pairs%20and%2C%20like%20DUSt3R%2C%20require%20post%20processing%20via%20optimization%20when%20more%20than%20two%20views%20are%20involved.%20We%20argue%20that%20DPMs%20are%20more%20useful%20when%20applied%20to%20videos%20and%20introduce%20V-DPM%20to%20demonstrate%20this.%20First%2C%20we%20show%20how%20to%20formulate%20DPMs%20for%20video%20input%20in%20a%20way%20that%20maximizes%20representational%20power%2C%20facilitates%20neural%20prediction%2C%20and%20enables%20reuse%20of%20pretrained%20models.%20Second%2C%20we%20implement%20these%20ideas%20on%20top%20of%20VGGT%2C%20a%20recent%20and%20powerful%203D%20reconstructor.%20Although%20VGGT%20was%20trained%20on%20static%20scenes%2C%20we%20show%20that%20a%20modest%20amount%20of%20synthetic%20data%20is%20sufficient%20to%20adapt%20it%20into%20an%20effective%20V-DPM%20predictor.%20Our%20approach%20achieves%20state%20of%20the%20art%20performance%20in%203D%20and%204D%20reconstruction%20for%20dynamic%20scenes.%20In%20particular%2C%20unlike%20recent%20dynamic%20extensions%20of%20VGGT%20such%20as%20P3%2C%20DPMs%20recover%20not%20only%20dynamic%20depth%20but%20also%20the%20full%203D%20motion%20of%20every%20point%20in%20the%20scene.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-DPM%253A%25204D%2520Video%2520Reconstruction%2520with%2520Dynamic%2520Point%2520Maps%26entry.906535625%3DEdgar%2520Sucar%2520and%2520Eldar%2520Insafutdinov%2520and%2520Zihang%2520Lai%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3DPowerful%25203D%2520representations%2520such%2520as%2520DUSt3R%2520invariant%2520point%2520maps%252C%2520which%2520encode%25203D%2520shape%2520and%2520camera%2520parameters%252C%2520have%2520significantly%2520advanced%2520feed%2520forward%25203D%2520reconstruction.%2520While%2520point%2520maps%2520assume%2520static%2520scenes%252C%2520Dynamic%2520Point%2520Maps%2520%2528DPMs%2529%2520extend%2520this%2520concept%2520to%2520dynamic%25203D%2520content%2520by%2520additionally%2520representing%2520scene%2520motion.%2520However%252C%2520existing%2520DPMs%2520are%2520limited%2520to%2520image%2520pairs%2520and%252C%2520like%2520DUSt3R%252C%2520require%2520post%2520processing%2520via%2520optimization%2520when%2520more%2520than%2520two%2520views%2520are%2520involved.%2520We%2520argue%2520that%2520DPMs%2520are%2520more%2520useful%2520when%2520applied%2520to%2520videos%2520and%2520introduce%2520V-DPM%2520to%2520demonstrate%2520this.%2520First%252C%2520we%2520show%2520how%2520to%2520formulate%2520DPMs%2520for%2520video%2520input%2520in%2520a%2520way%2520that%2520maximizes%2520representational%2520power%252C%2520facilitates%2520neural%2520prediction%252C%2520and%2520enables%2520reuse%2520of%2520pretrained%2520models.%2520Second%252C%2520we%2520implement%2520these%2520ideas%2520on%2520top%2520of%2520VGGT%252C%2520a%2520recent%2520and%2520powerful%25203D%2520reconstructor.%2520Although%2520VGGT%2520was%2520trained%2520on%2520static%2520scenes%252C%2520we%2520show%2520that%2520a%2520modest%2520amount%2520of%2520synthetic%2520data%2520is%2520sufficient%2520to%2520adapt%2520it%2520into%2520an%2520effective%2520V-DPM%2520predictor.%2520Our%2520approach%2520achieves%2520state%2520of%2520the%2520art%2520performance%2520in%25203D%2520and%25204D%2520reconstruction%2520for%2520dynamic%2520scenes.%2520In%2520particular%252C%2520unlike%2520recent%2520dynamic%2520extensions%2520of%2520VGGT%2520such%2520as%2520P3%252C%2520DPMs%2520recover%2520not%2520only%2520dynamic%2520depth%2520but%2520also%2520the%2520full%25203D%2520motion%2520of%2520every%2520point%2520in%2520the%2520scene.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-DPM%3A%204D%20Video%20Reconstruction%20with%20Dynamic%20Point%20Maps&entry.906535625=Edgar%20Sucar%20and%20Eldar%20Insafutdinov%20and%20Zihang%20Lai%20and%20Andrea%20Vedaldi&entry.1292438233=Powerful%203D%20representations%20such%20as%20DUSt3R%20invariant%20point%20maps%2C%20which%20encode%203D%20shape%20and%20camera%20parameters%2C%20have%20significantly%20advanced%20feed%20forward%203D%20reconstruction.%20While%20point%20maps%20assume%20static%20scenes%2C%20Dynamic%20Point%20Maps%20%28DPMs%29%20extend%20this%20concept%20to%20dynamic%203D%20content%20by%20additionally%20representing%20scene%20motion.%20However%2C%20existing%20DPMs%20are%20limited%20to%20image%20pairs%20and%2C%20like%20DUSt3R%2C%20require%20post%20processing%20via%20optimization%20when%20more%20than%20two%20views%20are%20involved.%20We%20argue%20that%20DPMs%20are%20more%20useful%20when%20applied%20to%20videos%20and%20introduce%20V-DPM%20to%20demonstrate%20this.%20First%2C%20we%20show%20how%20to%20formulate%20DPMs%20for%20video%20input%20in%20a%20way%20that%20maximizes%20representational%20power%2C%20facilitates%20neural%20prediction%2C%20and%20enables%20reuse%20of%20pretrained%20models.%20Second%2C%20we%20implement%20these%20ideas%20on%20top%20of%20VGGT%2C%20a%20recent%20and%20powerful%203D%20reconstructor.%20Although%20VGGT%20was%20trained%20on%20static%20scenes%2C%20we%20show%20that%20a%20modest%20amount%20of%20synthetic%20data%20is%20sufficient%20to%20adapt%20it%20into%20an%20effective%20V-DPM%20predictor.%20Our%20approach%20achieves%20state%20of%20the%20art%20performance%20in%203D%20and%204D%20reconstruction%20for%20dynamic%20scenes.%20In%20particular%2C%20unlike%20recent%20dynamic%20extensions%20of%20VGGT%20such%20as%20P3%2C%20DPMs%20recover%20not%20only%20dynamic%20depth%20but%20also%20the%20full%203D%20motion%20of%20every%20point%20in%20the%20scene.&entry.1838667208=http%3A//arxiv.org/abs/2601.09499v1&entry.124074799=Read"},
{"title": "ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation", "author": "Edoardo Bianchi and Jacopo Staiano and Antonio Liotta", "abstract": "Existing approaches treat action quality assessment and skill proficiency estimation as classification problems, outputting discrete labels without interpretable reasoning. We reformulate this task as generative vision language modeling, introducing ProfVLM, a compact model that jointly predicts proficiency levels and generates expert-like natural language feedback from multi-view videos. ProfVLM leverages conditional language generation to provide actionable insights along with quantitative evaluation scores. Central to our method is an AttentiveGatedProjector that dynamically fuses and projects multi-view egocentric and exocentric features from a frozen TimeSformer backbone into a language model fine-tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60% compared to existing classification-based methods. By providing natural language critiques aligned with performance levels, this work shows that generative vision-language modeling offers a powerful and efficient paradigm shift for interpretable action quality assessment.", "link": "http://arxiv.org/abs/2509.26278v2", "date": "2026-01-14", "relevancy": 2.9379, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6016}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6016}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProfVLM%3A%20A%20Lightweight%20Video-Language%20Model%20for%20Multi-View%20Proficiency%20Estimation&body=Title%3A%20ProfVLM%3A%20A%20Lightweight%20Video-Language%20Model%20for%20Multi-View%20Proficiency%20Estimation%0AAuthor%3A%20Edoardo%20Bianchi%20and%20Jacopo%20Staiano%20and%20Antonio%20Liotta%0AAbstract%3A%20Existing%20approaches%20treat%20action%20quality%20assessment%20and%20skill%20proficiency%20estimation%20as%20classification%20problems%2C%20outputting%20discrete%20labels%20without%20interpretable%20reasoning.%20We%20reformulate%20this%20task%20as%20generative%20vision%20language%20modeling%2C%20introducing%20ProfVLM%2C%20a%20compact%20model%20that%20jointly%20predicts%20proficiency%20levels%20and%20generates%20expert-like%20natural%20language%20feedback%20from%20multi-view%20videos.%20ProfVLM%20leverages%20conditional%20language%20generation%20to%20provide%20actionable%20insights%20along%20with%20quantitative%20evaluation%20scores.%20Central%20to%20our%20method%20is%20an%20AttentiveGatedProjector%20that%20dynamically%20fuses%20and%20projects%20multi-view%20egocentric%20and%20exocentric%20features%20from%20a%20frozen%20TimeSformer%20backbone%20into%20a%20language%20model%20fine-tuned%20for%20feedback%20generation.%20Trained%20on%20EgoExo4D%20with%20expert%20commentaries%2C%20ProfVLM%20surpasses%20state-of-the-art%20methods%20while%20using%20up%20to%2020x%20fewer%20parameters%20and%20reducing%20training%20time%20by%20up%20to%2060%25%20compared%20to%20existing%20classification-based%20methods.%20By%20providing%20natural%20language%20critiques%20aligned%20with%20performance%20levels%2C%20this%20work%20shows%20that%20generative%20vision-language%20modeling%20offers%20a%20powerful%20and%20efficient%20paradigm%20shift%20for%20interpretable%20action%20quality%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2509.26278v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProfVLM%253A%2520A%2520Lightweight%2520Video-Language%2520Model%2520for%2520Multi-View%2520Proficiency%2520Estimation%26entry.906535625%3DEdoardo%2520Bianchi%2520and%2520Jacopo%2520Staiano%2520and%2520Antonio%2520Liotta%26entry.1292438233%3DExisting%2520approaches%2520treat%2520action%2520quality%2520assessment%2520and%2520skill%2520proficiency%2520estimation%2520as%2520classification%2520problems%252C%2520outputting%2520discrete%2520labels%2520without%2520interpretable%2520reasoning.%2520We%2520reformulate%2520this%2520task%2520as%2520generative%2520vision%2520language%2520modeling%252C%2520introducing%2520ProfVLM%252C%2520a%2520compact%2520model%2520that%2520jointly%2520predicts%2520proficiency%2520levels%2520and%2520generates%2520expert-like%2520natural%2520language%2520feedback%2520from%2520multi-view%2520videos.%2520ProfVLM%2520leverages%2520conditional%2520language%2520generation%2520to%2520provide%2520actionable%2520insights%2520along%2520with%2520quantitative%2520evaluation%2520scores.%2520Central%2520to%2520our%2520method%2520is%2520an%2520AttentiveGatedProjector%2520that%2520dynamically%2520fuses%2520and%2520projects%2520multi-view%2520egocentric%2520and%2520exocentric%2520features%2520from%2520a%2520frozen%2520TimeSformer%2520backbone%2520into%2520a%2520language%2520model%2520fine-tuned%2520for%2520feedback%2520generation.%2520Trained%2520on%2520EgoExo4D%2520with%2520expert%2520commentaries%252C%2520ProfVLM%2520surpasses%2520state-of-the-art%2520methods%2520while%2520using%2520up%2520to%252020x%2520fewer%2520parameters%2520and%2520reducing%2520training%2520time%2520by%2520up%2520to%252060%2525%2520compared%2520to%2520existing%2520classification-based%2520methods.%2520By%2520providing%2520natural%2520language%2520critiques%2520aligned%2520with%2520performance%2520levels%252C%2520this%2520work%2520shows%2520that%2520generative%2520vision-language%2520modeling%2520offers%2520a%2520powerful%2520and%2520efficient%2520paradigm%2520shift%2520for%2520interpretable%2520action%2520quality%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26278v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProfVLM%3A%20A%20Lightweight%20Video-Language%20Model%20for%20Multi-View%20Proficiency%20Estimation&entry.906535625=Edoardo%20Bianchi%20and%20Jacopo%20Staiano%20and%20Antonio%20Liotta&entry.1292438233=Existing%20approaches%20treat%20action%20quality%20assessment%20and%20skill%20proficiency%20estimation%20as%20classification%20problems%2C%20outputting%20discrete%20labels%20without%20interpretable%20reasoning.%20We%20reformulate%20this%20task%20as%20generative%20vision%20language%20modeling%2C%20introducing%20ProfVLM%2C%20a%20compact%20model%20that%20jointly%20predicts%20proficiency%20levels%20and%20generates%20expert-like%20natural%20language%20feedback%20from%20multi-view%20videos.%20ProfVLM%20leverages%20conditional%20language%20generation%20to%20provide%20actionable%20insights%20along%20with%20quantitative%20evaluation%20scores.%20Central%20to%20our%20method%20is%20an%20AttentiveGatedProjector%20that%20dynamically%20fuses%20and%20projects%20multi-view%20egocentric%20and%20exocentric%20features%20from%20a%20frozen%20TimeSformer%20backbone%20into%20a%20language%20model%20fine-tuned%20for%20feedback%20generation.%20Trained%20on%20EgoExo4D%20with%20expert%20commentaries%2C%20ProfVLM%20surpasses%20state-of-the-art%20methods%20while%20using%20up%20to%2020x%20fewer%20parameters%20and%20reducing%20training%20time%20by%20up%20to%2060%25%20compared%20to%20existing%20classification-based%20methods.%20By%20providing%20natural%20language%20critiques%20aligned%20with%20performance%20levels%2C%20this%20work%20shows%20that%20generative%20vision-language%20modeling%20offers%20a%20powerful%20and%20efficient%20paradigm%20shift%20for%20interpretable%20action%20quality%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2509.26278v2&entry.124074799=Read"},
{"title": "GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection", "author": "Alfio Spoto and Rosario Leonardi and Francesco Ragusa and Giovanni Maria Farinella", "abstract": "Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.", "link": "http://arxiv.org/abs/2601.09528v1", "date": "2026-01-14", "relevancy": 2.9354, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6277}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5947}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlovEgo-HOI%3A%20Bridging%20the%20Synthetic-to-Real%20Gap%20for%20Industrial%20Egocentric%20Human-Object%20Interaction%20Detection&body=Title%3A%20GlovEgo-HOI%3A%20Bridging%20the%20Synthetic-to-Real%20Gap%20for%20Industrial%20Egocentric%20Human-Object%20Interaction%20Detection%0AAuthor%3A%20Alfio%20Spoto%20and%20Rosario%20Leonardi%20and%20Francesco%20Ragusa%20and%20Giovanni%20Maria%20Farinella%0AAbstract%3A%20Egocentric%20Human-Object%20Interaction%20%28EHOI%29%20analysis%20is%20crucial%20for%20industrial%20safety%2C%20yet%20the%20development%20of%20robust%20models%20is%20hindered%20by%20the%20scarcity%20of%20annotated%20domain-specific%20data.%20We%20address%20this%20challenge%20by%20introducing%20a%20data%20generation%20framework%20that%20combines%20synthetic%20data%20with%20a%20diffusion-based%20process%20to%20augment%20real-world%20images%20with%20realistic%20Personal%20Protective%20Equipment%20%28PPE%29.%20We%20present%20GlovEgo-HOI%2C%20a%20new%20benchmark%20dataset%20for%20industrial%20EHOI%2C%20and%20GlovEgo-Net%2C%20a%20model%20integrating%20Glove-Head%20and%20Keypoint-%20Head%20modules%20to%20leverage%20hand%20pose%20information%20for%20enhanced%20interaction%20detection.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20data%20generation%20framework%20and%20GlovEgo-Net.%20To%20foster%20further%20research%2C%20we%20release%20the%20GlovEgo-HOI%20dataset%2C%20augmentation%20pipeline%2C%20and%20pre-trained%20models%20at%3A%20GitHub%20project.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlovEgo-HOI%253A%2520Bridging%2520the%2520Synthetic-to-Real%2520Gap%2520for%2520Industrial%2520Egocentric%2520Human-Object%2520Interaction%2520Detection%26entry.906535625%3DAlfio%2520Spoto%2520and%2520Rosario%2520Leonardi%2520and%2520Francesco%2520Ragusa%2520and%2520Giovanni%2520Maria%2520Farinella%26entry.1292438233%3DEgocentric%2520Human-Object%2520Interaction%2520%2528EHOI%2529%2520analysis%2520is%2520crucial%2520for%2520industrial%2520safety%252C%2520yet%2520the%2520development%2520of%2520robust%2520models%2520is%2520hindered%2520by%2520the%2520scarcity%2520of%2520annotated%2520domain-specific%2520data.%2520We%2520address%2520this%2520challenge%2520by%2520introducing%2520a%2520data%2520generation%2520framework%2520that%2520combines%2520synthetic%2520data%2520with%2520a%2520diffusion-based%2520process%2520to%2520augment%2520real-world%2520images%2520with%2520realistic%2520Personal%2520Protective%2520Equipment%2520%2528PPE%2529.%2520We%2520present%2520GlovEgo-HOI%252C%2520a%2520new%2520benchmark%2520dataset%2520for%2520industrial%2520EHOI%252C%2520and%2520GlovEgo-Net%252C%2520a%2520model%2520integrating%2520Glove-Head%2520and%2520Keypoint-%2520Head%2520modules%2520to%2520leverage%2520hand%2520pose%2520information%2520for%2520enhanced%2520interaction%2520detection.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520data%2520generation%2520framework%2520and%2520GlovEgo-Net.%2520To%2520foster%2520further%2520research%252C%2520we%2520release%2520the%2520GlovEgo-HOI%2520dataset%252C%2520augmentation%2520pipeline%252C%2520and%2520pre-trained%2520models%2520at%253A%2520GitHub%2520project.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlovEgo-HOI%3A%20Bridging%20the%20Synthetic-to-Real%20Gap%20for%20Industrial%20Egocentric%20Human-Object%20Interaction%20Detection&entry.906535625=Alfio%20Spoto%20and%20Rosario%20Leonardi%20and%20Francesco%20Ragusa%20and%20Giovanni%20Maria%20Farinella&entry.1292438233=Egocentric%20Human-Object%20Interaction%20%28EHOI%29%20analysis%20is%20crucial%20for%20industrial%20safety%2C%20yet%20the%20development%20of%20robust%20models%20is%20hindered%20by%20the%20scarcity%20of%20annotated%20domain-specific%20data.%20We%20address%20this%20challenge%20by%20introducing%20a%20data%20generation%20framework%20that%20combines%20synthetic%20data%20with%20a%20diffusion-based%20process%20to%20augment%20real-world%20images%20with%20realistic%20Personal%20Protective%20Equipment%20%28PPE%29.%20We%20present%20GlovEgo-HOI%2C%20a%20new%20benchmark%20dataset%20for%20industrial%20EHOI%2C%20and%20GlovEgo-Net%2C%20a%20model%20integrating%20Glove-Head%20and%20Keypoint-%20Head%20modules%20to%20leverage%20hand%20pose%20information%20for%20enhanced%20interaction%20detection.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20data%20generation%20framework%20and%20GlovEgo-Net.%20To%20foster%20further%20research%2C%20we%20release%20the%20GlovEgo-HOI%20dataset%2C%20augmentation%20pipeline%2C%20and%20pre-trained%20models%20at%3A%20GitHub%20project.&entry.1838667208=http%3A//arxiv.org/abs/2601.09528v1&entry.124074799=Read"},
{"title": "Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models", "author": "Junjie Li and Ziao Wang and Jianghong Ma and Xiaofeng Zhang", "abstract": "Large vision-language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.", "link": "http://arxiv.org/abs/2510.00040v2", "date": "2026-01-14", "relevancy": 2.8184, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Intrinsic%20Capabilities%3A%20A%20Paradigm%20for%20Data%20Curation%20in%20Vision-Language%20Models&body=Title%3A%20Uncovering%20Intrinsic%20Capabilities%3A%20A%20Paradigm%20for%20Data%20Curation%20in%20Vision-Language%20Models%0AAuthor%3A%20Junjie%20Li%20and%20Ziao%20Wang%20and%20Jianghong%20Ma%20and%20Xiaofeng%20Zhang%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20achieve%20strong%20benchmark%20performance%2C%20but%20controlling%20their%20behavior%20through%20instruction%20tuning%20remains%20difficult.%20Reducing%20the%20budget%20of%20instruction%20tuning%20dataset%20often%20causes%20regressions%2C%20as%20heuristic%20strategies%20treat%20models%20as%20black%20boxes%20and%20overlook%20the%20latent%20capabilities%20that%20govern%20learning.%20We%20introduce%20Capability-Attributed%20Data%20Curation%20%28CADC%29%2C%20a%20framework%20that%20shifts%20curation%20from%20task-specific%20heuristics%20to%20intrinsic%20capability%20analysis.%20CADC%20discovers%20intrinsic%20capabilities%20in%20an%20unsupervised%20manner%20from%20gradient-based%20learning%20trajectories%2C%20attributes%20training%20data%20to%20these%20capabilities%20via%20influence%20estimation%2C%20and%20curates%20capability-aware%20curricula%20through%20balanced%20selection%20and%20staged%20sequencing.%20This%20transforms%20black-box%20instruction%20tuning%20into%20a%20controllable%2C%20capability-driven%20process.%20With%20as%20little%20as%205%25%20of%20the%20original%20data%2C%20CADC%20surpasses%20full-data%20training%20on%20multimodal%20benchmarks.%20These%20results%20validate%20intrinsic%20capabilities%20as%20the%20fundamental%20building%20blocks%20of%20model%20learning%20and%20establish%20CADC%20as%20a%20principle%20paradigm%20for%20instruction%20data%20curation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.00040v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Intrinsic%2520Capabilities%253A%2520A%2520Paradigm%2520for%2520Data%2520Curation%2520in%2520Vision-Language%2520Models%26entry.906535625%3DJunjie%2520Li%2520and%2520Ziao%2520Wang%2520and%2520Jianghong%2520Ma%2520and%2520Xiaofeng%2520Zhang%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520achieve%2520strong%2520benchmark%2520performance%252C%2520but%2520controlling%2520their%2520behavior%2520through%2520instruction%2520tuning%2520remains%2520difficult.%2520Reducing%2520the%2520budget%2520of%2520instruction%2520tuning%2520dataset%2520often%2520causes%2520regressions%252C%2520as%2520heuristic%2520strategies%2520treat%2520models%2520as%2520black%2520boxes%2520and%2520overlook%2520the%2520latent%2520capabilities%2520that%2520govern%2520learning.%2520We%2520introduce%2520Capability-Attributed%2520Data%2520Curation%2520%2528CADC%2529%252C%2520a%2520framework%2520that%2520shifts%2520curation%2520from%2520task-specific%2520heuristics%2520to%2520intrinsic%2520capability%2520analysis.%2520CADC%2520discovers%2520intrinsic%2520capabilities%2520in%2520an%2520unsupervised%2520manner%2520from%2520gradient-based%2520learning%2520trajectories%252C%2520attributes%2520training%2520data%2520to%2520these%2520capabilities%2520via%2520influence%2520estimation%252C%2520and%2520curates%2520capability-aware%2520curricula%2520through%2520balanced%2520selection%2520and%2520staged%2520sequencing.%2520This%2520transforms%2520black-box%2520instruction%2520tuning%2520into%2520a%2520controllable%252C%2520capability-driven%2520process.%2520With%2520as%2520little%2520as%25205%2525%2520of%2520the%2520original%2520data%252C%2520CADC%2520surpasses%2520full-data%2520training%2520on%2520multimodal%2520benchmarks.%2520These%2520results%2520validate%2520intrinsic%2520capabilities%2520as%2520the%2520fundamental%2520building%2520blocks%2520of%2520model%2520learning%2520and%2520establish%2520CADC%2520as%2520a%2520principle%2520paradigm%2520for%2520instruction%2520data%2520curation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00040v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Intrinsic%20Capabilities%3A%20A%20Paradigm%20for%20Data%20Curation%20in%20Vision-Language%20Models&entry.906535625=Junjie%20Li%20and%20Ziao%20Wang%20and%20Jianghong%20Ma%20and%20Xiaofeng%20Zhang&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20achieve%20strong%20benchmark%20performance%2C%20but%20controlling%20their%20behavior%20through%20instruction%20tuning%20remains%20difficult.%20Reducing%20the%20budget%20of%20instruction%20tuning%20dataset%20often%20causes%20regressions%2C%20as%20heuristic%20strategies%20treat%20models%20as%20black%20boxes%20and%20overlook%20the%20latent%20capabilities%20that%20govern%20learning.%20We%20introduce%20Capability-Attributed%20Data%20Curation%20%28CADC%29%2C%20a%20framework%20that%20shifts%20curation%20from%20task-specific%20heuristics%20to%20intrinsic%20capability%20analysis.%20CADC%20discovers%20intrinsic%20capabilities%20in%20an%20unsupervised%20manner%20from%20gradient-based%20learning%20trajectories%2C%20attributes%20training%20data%20to%20these%20capabilities%20via%20influence%20estimation%2C%20and%20curates%20capability-aware%20curricula%20through%20balanced%20selection%20and%20staged%20sequencing.%20This%20transforms%20black-box%20instruction%20tuning%20into%20a%20controllable%2C%20capability-driven%20process.%20With%20as%20little%20as%205%25%20of%20the%20original%20data%2C%20CADC%20surpasses%20full-data%20training%20on%20multimodal%20benchmarks.%20These%20results%20validate%20intrinsic%20capabilities%20as%20the%20fundamental%20building%20blocks%20of%20model%20learning%20and%20establish%20CADC%20as%20a%20principle%20paradigm%20for%20instruction%20data%20curation.&entry.1838667208=http%3A//arxiv.org/abs/2510.00040v2&entry.124074799=Read"},
{"title": "Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation", "author": "Petros Koutsouvelis and Matej Gazda and Leroy Volmer and Sina Amirrajab and Kamil Barbierik and Branislav Setlak and Jakub Gazda and Peter Drotar", "abstract": "The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.", "link": "http://arxiv.org/abs/2511.11311v2", "date": "2026-01-14", "relevancy": 2.8134, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20modality-invariant%20foundation%20models%20for%20brain%20MRI%20analysis%3A%20Application%20to%20lesion%20segmentation&body=Title%3A%20Large-scale%20modality-invariant%20foundation%20models%20for%20brain%20MRI%20analysis%3A%20Application%20to%20lesion%20segmentation%0AAuthor%3A%20Petros%20Koutsouvelis%20and%20Matej%20Gazda%20and%20Leroy%20Volmer%20and%20Sina%20Amirrajab%20and%20Kamil%20Barbierik%20and%20Branislav%20Setlak%20and%20Jakub%20Gazda%20and%20Peter%20Drotar%0AAbstract%3A%20The%20field%20of%20computer%20vision%20is%20undergoing%20a%20paradigm%20shift%20toward%20large-scale%20foundation%20model%20pre-training%20via%20self-supervised%20learning%20%28SSL%29.%20Leveraging%20large%20volumes%20of%20unlabeled%20brain%20MRI%20data%2C%20such%20models%20can%20learn%20anatomical%20priors%20that%20improve%20few-shot%20performance%20in%20diverse%20neuroimaging%20tasks.%20However%2C%20most%20SSL%20frameworks%20are%20tailored%20to%20natural%20images%2C%20and%20their%20adaptation%20to%20capture%20multi-modal%20MRI%20information%20remains%20underexplored.%20This%20work%20proposes%20a%20modality-invariant%20representation%20learning%20setup%20and%20evaluates%20its%20effectiveness%20in%20stroke%20and%20epilepsy%20lesion%20segmentation%2C%20following%20large-scale%20pre-training.%20Experimental%20results%20suggest%20that%20despite%20successful%20cross-modality%20alignment%2C%20lesion%20segmentation%20primarily%20benefits%20from%20preserving%20fine-grained%20modality-specific%20features.%20Model%20checkpoints%20and%20code%20are%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11311v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520modality-invariant%2520foundation%2520models%2520for%2520brain%2520MRI%2520analysis%253A%2520Application%2520to%2520lesion%2520segmentation%26entry.906535625%3DPetros%2520Koutsouvelis%2520and%2520Matej%2520Gazda%2520and%2520Leroy%2520Volmer%2520and%2520Sina%2520Amirrajab%2520and%2520Kamil%2520Barbierik%2520and%2520Branislav%2520Setlak%2520and%2520Jakub%2520Gazda%2520and%2520Peter%2520Drotar%26entry.1292438233%3DThe%2520field%2520of%2520computer%2520vision%2520is%2520undergoing%2520a%2520paradigm%2520shift%2520toward%2520large-scale%2520foundation%2520model%2520pre-training%2520via%2520self-supervised%2520learning%2520%2528SSL%2529.%2520Leveraging%2520large%2520volumes%2520of%2520unlabeled%2520brain%2520MRI%2520data%252C%2520such%2520models%2520can%2520learn%2520anatomical%2520priors%2520that%2520improve%2520few-shot%2520performance%2520in%2520diverse%2520neuroimaging%2520tasks.%2520However%252C%2520most%2520SSL%2520frameworks%2520are%2520tailored%2520to%2520natural%2520images%252C%2520and%2520their%2520adaptation%2520to%2520capture%2520multi-modal%2520MRI%2520information%2520remains%2520underexplored.%2520This%2520work%2520proposes%2520a%2520modality-invariant%2520representation%2520learning%2520setup%2520and%2520evaluates%2520its%2520effectiveness%2520in%2520stroke%2520and%2520epilepsy%2520lesion%2520segmentation%252C%2520following%2520large-scale%2520pre-training.%2520Experimental%2520results%2520suggest%2520that%2520despite%2520successful%2520cross-modality%2520alignment%252C%2520lesion%2520segmentation%2520primarily%2520benefits%2520from%2520preserving%2520fine-grained%2520modality-specific%2520features.%2520Model%2520checkpoints%2520and%2520code%2520are%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11311v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20modality-invariant%20foundation%20models%20for%20brain%20MRI%20analysis%3A%20Application%20to%20lesion%20segmentation&entry.906535625=Petros%20Koutsouvelis%20and%20Matej%20Gazda%20and%20Leroy%20Volmer%20and%20Sina%20Amirrajab%20and%20Kamil%20Barbierik%20and%20Branislav%20Setlak%20and%20Jakub%20Gazda%20and%20Peter%20Drotar&entry.1292438233=The%20field%20of%20computer%20vision%20is%20undergoing%20a%20paradigm%20shift%20toward%20large-scale%20foundation%20model%20pre-training%20via%20self-supervised%20learning%20%28SSL%29.%20Leveraging%20large%20volumes%20of%20unlabeled%20brain%20MRI%20data%2C%20such%20models%20can%20learn%20anatomical%20priors%20that%20improve%20few-shot%20performance%20in%20diverse%20neuroimaging%20tasks.%20However%2C%20most%20SSL%20frameworks%20are%20tailored%20to%20natural%20images%2C%20and%20their%20adaptation%20to%20capture%20multi-modal%20MRI%20information%20remains%20underexplored.%20This%20work%20proposes%20a%20modality-invariant%20representation%20learning%20setup%20and%20evaluates%20its%20effectiveness%20in%20stroke%20and%20epilepsy%20lesion%20segmentation%2C%20following%20large-scale%20pre-training.%20Experimental%20results%20suggest%20that%20despite%20successful%20cross-modality%20alignment%2C%20lesion%20segmentation%20primarily%20benefits%20from%20preserving%20fine-grained%20modality-specific%20features.%20Model%20checkpoints%20and%20code%20are%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2511.11311v2&entry.124074799=Read"},
{"title": "Know Yourself Better: Diverse Object-Related Features Improve Open Set Recognition", "author": "Jiawen Xu and Margret Keuper", "abstract": "Open set recognition (OSR) is a critical aspect of machine learning, addressing the challenge of detecting novel classes during inference. Within the realm of deep learning, neural classifiers trained on a closed set of data typically struggle to identify novel classes, leading to erroneous predictions. To address this issue, various heuristic methods have been proposed, allowing models to express uncertainty by stating \"I don't know.\" However, a gap in the literature remains, as there has been limited exploration of the underlying mechanisms of these methods. In this paper, we conduct an analysis of open set recognition methods, focusing on the aspect of feature diversity. Our research reveals a significant correlation between learning diverse discriminative features and enhancing OSR performance. Building on this insight, we propose a novel OSR approach that leverages the advantages of feature diversity. The efficacy of our method is substantiated through rigorous evaluation on a standard OSR testbench, demonstrating a substantial improvement over state-of-the-art methods.", "link": "http://arxiv.org/abs/2404.10370v2", "date": "2026-01-14", "relevancy": 2.7803, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Know%20Yourself%20Better%3A%20Diverse%20Object-Related%20Features%20Improve%20Open%20Set%20Recognition&body=Title%3A%20Know%20Yourself%20Better%3A%20Diverse%20Object-Related%20Features%20Improve%20Open%20Set%20Recognition%0AAuthor%3A%20Jiawen%20Xu%20and%20Margret%20Keuper%0AAbstract%3A%20Open%20set%20recognition%20%28OSR%29%20is%20a%20critical%20aspect%20of%20machine%20learning%2C%20addressing%20the%20challenge%20of%20detecting%20novel%20classes%20during%20inference.%20Within%20the%20realm%20of%20deep%20learning%2C%20neural%20classifiers%20trained%20on%20a%20closed%20set%20of%20data%20typically%20struggle%20to%20identify%20novel%20classes%2C%20leading%20to%20erroneous%20predictions.%20To%20address%20this%20issue%2C%20various%20heuristic%20methods%20have%20been%20proposed%2C%20allowing%20models%20to%20express%20uncertainty%20by%20stating%20%22I%20don%27t%20know.%22%20However%2C%20a%20gap%20in%20the%20literature%20remains%2C%20as%20there%20has%20been%20limited%20exploration%20of%20the%20underlying%20mechanisms%20of%20these%20methods.%20In%20this%20paper%2C%20we%20conduct%20an%20analysis%20of%20open%20set%20recognition%20methods%2C%20focusing%20on%20the%20aspect%20of%20feature%20diversity.%20Our%20research%20reveals%20a%20significant%20correlation%20between%20learning%20diverse%20discriminative%20features%20and%20enhancing%20OSR%20performance.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20novel%20OSR%20approach%20that%20leverages%20the%20advantages%20of%20feature%20diversity.%20The%20efficacy%20of%20our%20method%20is%20substantiated%20through%20rigorous%20evaluation%20on%20a%20standard%20OSR%20testbench%2C%20demonstrating%20a%20substantial%20improvement%20over%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2404.10370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnow%2520Yourself%2520Better%253A%2520Diverse%2520Object-Related%2520Features%2520Improve%2520Open%2520Set%2520Recognition%26entry.906535625%3DJiawen%2520Xu%2520and%2520Margret%2520Keuper%26entry.1292438233%3DOpen%2520set%2520recognition%2520%2528OSR%2529%2520is%2520a%2520critical%2520aspect%2520of%2520machine%2520learning%252C%2520addressing%2520the%2520challenge%2520of%2520detecting%2520novel%2520classes%2520during%2520inference.%2520Within%2520the%2520realm%2520of%2520deep%2520learning%252C%2520neural%2520classifiers%2520trained%2520on%2520a%2520closed%2520set%2520of%2520data%2520typically%2520struggle%2520to%2520identify%2520novel%2520classes%252C%2520leading%2520to%2520erroneous%2520predictions.%2520To%2520address%2520this%2520issue%252C%2520various%2520heuristic%2520methods%2520have%2520been%2520proposed%252C%2520allowing%2520models%2520to%2520express%2520uncertainty%2520by%2520stating%2520%2522I%2520don%2527t%2520know.%2522%2520However%252C%2520a%2520gap%2520in%2520the%2520literature%2520remains%252C%2520as%2520there%2520has%2520been%2520limited%2520exploration%2520of%2520the%2520underlying%2520mechanisms%2520of%2520these%2520methods.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520an%2520analysis%2520of%2520open%2520set%2520recognition%2520methods%252C%2520focusing%2520on%2520the%2520aspect%2520of%2520feature%2520diversity.%2520Our%2520research%2520reveals%2520a%2520significant%2520correlation%2520between%2520learning%2520diverse%2520discriminative%2520features%2520and%2520enhancing%2520OSR%2520performance.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520novel%2520OSR%2520approach%2520that%2520leverages%2520the%2520advantages%2520of%2520feature%2520diversity.%2520The%2520efficacy%2520of%2520our%2520method%2520is%2520substantiated%2520through%2520rigorous%2520evaluation%2520on%2520a%2520standard%2520OSR%2520testbench%252C%2520demonstrating%2520a%2520substantial%2520improvement%2520over%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Know%20Yourself%20Better%3A%20Diverse%20Object-Related%20Features%20Improve%20Open%20Set%20Recognition&entry.906535625=Jiawen%20Xu%20and%20Margret%20Keuper&entry.1292438233=Open%20set%20recognition%20%28OSR%29%20is%20a%20critical%20aspect%20of%20machine%20learning%2C%20addressing%20the%20challenge%20of%20detecting%20novel%20classes%20during%20inference.%20Within%20the%20realm%20of%20deep%20learning%2C%20neural%20classifiers%20trained%20on%20a%20closed%20set%20of%20data%20typically%20struggle%20to%20identify%20novel%20classes%2C%20leading%20to%20erroneous%20predictions.%20To%20address%20this%20issue%2C%20various%20heuristic%20methods%20have%20been%20proposed%2C%20allowing%20models%20to%20express%20uncertainty%20by%20stating%20%22I%20don%27t%20know.%22%20However%2C%20a%20gap%20in%20the%20literature%20remains%2C%20as%20there%20has%20been%20limited%20exploration%20of%20the%20underlying%20mechanisms%20of%20these%20methods.%20In%20this%20paper%2C%20we%20conduct%20an%20analysis%20of%20open%20set%20recognition%20methods%2C%20focusing%20on%20the%20aspect%20of%20feature%20diversity.%20Our%20research%20reveals%20a%20significant%20correlation%20between%20learning%20diverse%20discriminative%20features%20and%20enhancing%20OSR%20performance.%20Building%20on%20this%20insight%2C%20we%20propose%20a%20novel%20OSR%20approach%20that%20leverages%20the%20advantages%20of%20feature%20diversity.%20The%20efficacy%20of%20our%20method%20is%20substantiated%20through%20rigorous%20evaluation%20on%20a%20standard%20OSR%20testbench%2C%20demonstrating%20a%20substantial%20improvement%20over%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2404.10370v2&entry.124074799=Read"},
{"title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning", "author": "Dongjie Cheng and Yongqi Li and Zhixin Ma and Hongru Cai and Yupeng Hu and Wenjie Wang and Liqiang Nie and Wenjie Li", "abstract": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.", "link": "http://arxiv.org/abs/2601.09536v1", "date": "2026-01-14", "relevancy": 2.7707, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-R1%3A%20Towards%20the%20Unified%20Generative%20Paradigm%20for%20Multimodal%20Reasoning&body=Title%3A%20Omni-R1%3A%20Towards%20the%20Unified%20Generative%20Paradigm%20for%20Multimodal%20Reasoning%0AAuthor%3A%20Dongjie%20Cheng%20and%20Yongqi%20Li%20and%20Zhixin%20Ma%20and%20Hongru%20Cai%20and%20Yupeng%20Hu%20and%20Wenjie%20Wang%20and%20Liqiang%20Nie%20and%20Wenjie%20Li%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20making%20significant%20progress%20in%20multimodal%20reasoning.%20Early%20approaches%20focus%20on%20pure%20text-based%20reasoning.%20More%20recent%20studies%20have%20incorporated%20multimodal%20information%20into%20the%20reasoning%20steps%3B%20however%2C%20they%20often%20follow%20a%20single%20task-specific%20reasoning%20pattern%2C%20which%20limits%20their%20generalizability%20across%20various%20multimodal%20tasks.%20In%20fact%2C%20there%20are%20numerous%20multimodal%20tasks%20requiring%20diverse%20reasoning%20skills%2C%20such%20as%20zooming%20in%20on%20a%20specific%20region%20or%20marking%20an%20object%20within%20an%20image.%20To%20address%20this%2C%20we%20propose%20unified%20generative%20multimodal%20reasoning%2C%20which%20unifies%20diverse%20multimodal%20reasoning%20skills%20by%20generating%20intermediate%20images%20during%20the%20reasoning%20process.%20We%20instantiate%20this%20paradigm%20with%20Omni-R1%2C%20a%20two-stage%20SFT%2BRL%20framework%20featuring%20perception%20alignment%20loss%20and%20perception%20reward%2C%20thereby%20enabling%20functional%20image%20generation.%20Additionally%2C%20we%20introduce%20Omni-R1-Zero%2C%20which%20eliminates%20the%20need%20for%20multimodal%20annotations%20by%20bootstrapping%20step-wise%20visualizations%20from%20text-only%20reasoning%20data.%20Empirical%20results%20show%20that%20Omni-R1%20achieves%20unified%20generative%20reasoning%20across%20a%20wide%20range%20of%20multimodal%20tasks%2C%20and%20Omni-R1-Zero%20can%20match%20or%20even%20surpass%20Omni-R1%20on%20average%2C%20suggesting%20a%20promising%20direction%20for%20generative%20multimodal%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-R1%253A%2520Towards%2520the%2520Unified%2520Generative%2520Paradigm%2520for%2520Multimodal%2520Reasoning%26entry.906535625%3DDongjie%2520Cheng%2520and%2520Yongqi%2520Li%2520and%2520Zhixin%2520Ma%2520and%2520Hongru%2520Cai%2520and%2520Yupeng%2520Hu%2520and%2520Wenjie%2520Wang%2520and%2520Liqiang%2520Nie%2520and%2520Wenjie%2520Li%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520are%2520making%2520significant%2520progress%2520in%2520multimodal%2520reasoning.%2520Early%2520approaches%2520focus%2520on%2520pure%2520text-based%2520reasoning.%2520More%2520recent%2520studies%2520have%2520incorporated%2520multimodal%2520information%2520into%2520the%2520reasoning%2520steps%253B%2520however%252C%2520they%2520often%2520follow%2520a%2520single%2520task-specific%2520reasoning%2520pattern%252C%2520which%2520limits%2520their%2520generalizability%2520across%2520various%2520multimodal%2520tasks.%2520In%2520fact%252C%2520there%2520are%2520numerous%2520multimodal%2520tasks%2520requiring%2520diverse%2520reasoning%2520skills%252C%2520such%2520as%2520zooming%2520in%2520on%2520a%2520specific%2520region%2520or%2520marking%2520an%2520object%2520within%2520an%2520image.%2520To%2520address%2520this%252C%2520we%2520propose%2520unified%2520generative%2520multimodal%2520reasoning%252C%2520which%2520unifies%2520diverse%2520multimodal%2520reasoning%2520skills%2520by%2520generating%2520intermediate%2520images%2520during%2520the%2520reasoning%2520process.%2520We%2520instantiate%2520this%2520paradigm%2520with%2520Omni-R1%252C%2520a%2520two-stage%2520SFT%252BRL%2520framework%2520featuring%2520perception%2520alignment%2520loss%2520and%2520perception%2520reward%252C%2520thereby%2520enabling%2520functional%2520image%2520generation.%2520Additionally%252C%2520we%2520introduce%2520Omni-R1-Zero%252C%2520which%2520eliminates%2520the%2520need%2520for%2520multimodal%2520annotations%2520by%2520bootstrapping%2520step-wise%2520visualizations%2520from%2520text-only%2520reasoning%2520data.%2520Empirical%2520results%2520show%2520that%2520Omni-R1%2520achieves%2520unified%2520generative%2520reasoning%2520across%2520a%2520wide%2520range%2520of%2520multimodal%2520tasks%252C%2520and%2520Omni-R1-Zero%2520can%2520match%2520or%2520even%2520surpass%2520Omni-R1%2520on%2520average%252C%2520suggesting%2520a%2520promising%2520direction%2520for%2520generative%2520multimodal%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-R1%3A%20Towards%20the%20Unified%20Generative%20Paradigm%20for%20Multimodal%20Reasoning&entry.906535625=Dongjie%20Cheng%20and%20Yongqi%20Li%20and%20Zhixin%20Ma%20and%20Hongru%20Cai%20and%20Yupeng%20Hu%20and%20Wenjie%20Wang%20and%20Liqiang%20Nie%20and%20Wenjie%20Li&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20are%20making%20significant%20progress%20in%20multimodal%20reasoning.%20Early%20approaches%20focus%20on%20pure%20text-based%20reasoning.%20More%20recent%20studies%20have%20incorporated%20multimodal%20information%20into%20the%20reasoning%20steps%3B%20however%2C%20they%20often%20follow%20a%20single%20task-specific%20reasoning%20pattern%2C%20which%20limits%20their%20generalizability%20across%20various%20multimodal%20tasks.%20In%20fact%2C%20there%20are%20numerous%20multimodal%20tasks%20requiring%20diverse%20reasoning%20skills%2C%20such%20as%20zooming%20in%20on%20a%20specific%20region%20or%20marking%20an%20object%20within%20an%20image.%20To%20address%20this%2C%20we%20propose%20unified%20generative%20multimodal%20reasoning%2C%20which%20unifies%20diverse%20multimodal%20reasoning%20skills%20by%20generating%20intermediate%20images%20during%20the%20reasoning%20process.%20We%20instantiate%20this%20paradigm%20with%20Omni-R1%2C%20a%20two-stage%20SFT%2BRL%20framework%20featuring%20perception%20alignment%20loss%20and%20perception%20reward%2C%20thereby%20enabling%20functional%20image%20generation.%20Additionally%2C%20we%20introduce%20Omni-R1-Zero%2C%20which%20eliminates%20the%20need%20for%20multimodal%20annotations%20by%20bootstrapping%20step-wise%20visualizations%20from%20text-only%20reasoning%20data.%20Empirical%20results%20show%20that%20Omni-R1%20achieves%20unified%20generative%20reasoning%20across%20a%20wide%20range%20of%20multimodal%20tasks%2C%20and%20Omni-R1-Zero%20can%20match%20or%20even%20surpass%20Omni-R1%20on%20average%2C%20suggesting%20a%20promising%20direction%20for%20generative%20multimodal%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2601.09536v1&entry.124074799=Read"},
{"title": "SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3", "author": "Ruiqi Shen and Chang Liu and Henghui Ding", "abstract": "Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.", "link": "http://arxiv.org/abs/2601.09699v1", "date": "2026-01-14", "relevancy": 2.7041, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM3-DMS%3A%20Decoupled%20Memory%20Selection%20for%20Multi-target%20Video%20Segmentation%20of%20SAM3&body=Title%3A%20SAM3-DMS%3A%20Decoupled%20Memory%20Selection%20for%20Multi-target%20Video%20Segmentation%20of%20SAM3%0AAuthor%3A%20Ruiqi%20Shen%20and%20Chang%20Liu%20and%20Henghui%20Ding%0AAbstract%3A%20Segment%20Anything%203%20%28SAM3%29%20has%20established%20a%20powerful%20foundation%20that%20robustly%20detects%2C%20segments%2C%20and%20tracks%20specified%20targets%20in%20videos.%20However%2C%20in%20its%20original%20implementation%2C%20its%20group-level%20collective%20memory%20selection%20is%20suboptimal%20for%20complex%20multi-object%20scenarios%2C%20as%20it%20employs%20a%20synchronized%20decision%20across%20all%20concurrent%20targets%20conditioned%20on%20their%20average%20performance%2C%20often%20overlooking%20individual%20reliability.%20To%20this%20end%2C%20we%20propose%20SAM3-DMS%2C%20a%20training-free%20decoupled%20strategy%20that%20utilizes%20fine-grained%20memory%20selection%20on%20individual%20objects.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%20robust%20identity%20preservation%20and%20tracking%20stability.%20Notably%2C%20our%20advantage%20becomes%20more%20pronounced%20with%20increased%20target%20density%2C%20establishing%20a%20solid%20foundation%20for%20simultaneous%20multi-target%20video%20segmentation%20in%20the%20wild.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM3-DMS%253A%2520Decoupled%2520Memory%2520Selection%2520for%2520Multi-target%2520Video%2520Segmentation%2520of%2520SAM3%26entry.906535625%3DRuiqi%2520Shen%2520and%2520Chang%2520Liu%2520and%2520Henghui%2520Ding%26entry.1292438233%3DSegment%2520Anything%25203%2520%2528SAM3%2529%2520has%2520established%2520a%2520powerful%2520foundation%2520that%2520robustly%2520detects%252C%2520segments%252C%2520and%2520tracks%2520specified%2520targets%2520in%2520videos.%2520However%252C%2520in%2520its%2520original%2520implementation%252C%2520its%2520group-level%2520collective%2520memory%2520selection%2520is%2520suboptimal%2520for%2520complex%2520multi-object%2520scenarios%252C%2520as%2520it%2520employs%2520a%2520synchronized%2520decision%2520across%2520all%2520concurrent%2520targets%2520conditioned%2520on%2520their%2520average%2520performance%252C%2520often%2520overlooking%2520individual%2520reliability.%2520To%2520this%2520end%252C%2520we%2520propose%2520SAM3-DMS%252C%2520a%2520training-free%2520decoupled%2520strategy%2520that%2520utilizes%2520fine-grained%2520memory%2520selection%2520on%2520individual%2520objects.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520robust%2520identity%2520preservation%2520and%2520tracking%2520stability.%2520Notably%252C%2520our%2520advantage%2520becomes%2520more%2520pronounced%2520with%2520increased%2520target%2520density%252C%2520establishing%2520a%2520solid%2520foundation%2520for%2520simultaneous%2520multi-target%2520video%2520segmentation%2520in%2520the%2520wild.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM3-DMS%3A%20Decoupled%20Memory%20Selection%20for%20Multi-target%20Video%20Segmentation%20of%20SAM3&entry.906535625=Ruiqi%20Shen%20and%20Chang%20Liu%20and%20Henghui%20Ding&entry.1292438233=Segment%20Anything%203%20%28SAM3%29%20has%20established%20a%20powerful%20foundation%20that%20robustly%20detects%2C%20segments%2C%20and%20tracks%20specified%20targets%20in%20videos.%20However%2C%20in%20its%20original%20implementation%2C%20its%20group-level%20collective%20memory%20selection%20is%20suboptimal%20for%20complex%20multi-object%20scenarios%2C%20as%20it%20employs%20a%20synchronized%20decision%20across%20all%20concurrent%20targets%20conditioned%20on%20their%20average%20performance%2C%20often%20overlooking%20individual%20reliability.%20To%20this%20end%2C%20we%20propose%20SAM3-DMS%2C%20a%20training-free%20decoupled%20strategy%20that%20utilizes%20fine-grained%20memory%20selection%20on%20individual%20objects.%20Experiments%20demonstrate%20that%20our%20approach%20achieves%20robust%20identity%20preservation%20and%20tracking%20stability.%20Notably%2C%20our%20advantage%20becomes%20more%20pronounced%20with%20increased%20target%20density%2C%20establishing%20a%20solid%20foundation%20for%20simultaneous%20multi-target%20video%20segmentation%20in%20the%20wild.&entry.1838667208=http%3A//arxiv.org/abs/2601.09699v1&entry.124074799=Read"},
{"title": "LiteEmbed: Adapting CLIP to Rare Classes", "author": "Aishwarya Agarwal and Srikrishna Karanam and Vineet Gandhi", "abstract": "Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.", "link": "http://arxiv.org/abs/2601.09661v1", "date": "2026-01-14", "relevancy": 2.702, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiteEmbed%3A%20Adapting%20CLIP%20to%20Rare%20Classes&body=Title%3A%20LiteEmbed%3A%20Adapting%20CLIP%20to%20Rare%20Classes%0AAuthor%3A%20Aishwarya%20Agarwal%20and%20Srikrishna%20Karanam%20and%20Vineet%20Gandhi%0AAbstract%3A%20Large-scale%20vision-language%20models%20such%20as%20CLIP%20achieve%20strong%20zero-shot%20recognition%20but%20struggle%20with%20classes%20that%20are%20rarely%20seen%20during%20pretraining%2C%20including%20newly%20emerging%20entities%20and%20culturally%20specific%20categories.%20We%20introduce%20LiteEmbed%2C%20a%20lightweight%20framework%20for%20few-shot%20personalization%20of%20CLIP%20that%20enables%20new%20classes%20to%20be%20added%20without%20retraining%20its%20encoders.%20LiteEmbed%20performs%20subspace-guided%20optimization%20of%20text%20embeddings%20within%20CLIP%27s%20vocabulary%2C%20leveraging%20a%20PCA-based%20decomposition%20that%20disentangles%20coarse%20semantic%20directions%20from%20fine-grained%20variations.%20Two%20complementary%20objectives%2C%20coarse%20alignment%20and%20fine%20separation%2C%20jointly%20preserve%20global%20semantic%20consistency%20while%20enhancing%20discriminability%20among%20visually%20similar%20classes.%20Once%20optimized%2C%20the%20embeddings%20are%20plug-and-play%2C%20seamlessly%20substituting%20CLIP%27s%20original%20text%20features%20across%20classification%2C%20retrieval%2C%20segmentation%2C%20and%20detection%20tasks.%20Extensive%20experiments%20demonstrate%20substantial%20gains%20over%20prior%20methods%2C%20establishing%20LiteEmbed%20as%20an%20effective%20approach%20for%20adapting%20CLIP%20to%20underrepresented%2C%20rare%2C%20or%20unseen%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiteEmbed%253A%2520Adapting%2520CLIP%2520to%2520Rare%2520Classes%26entry.906535625%3DAishwarya%2520Agarwal%2520and%2520Srikrishna%2520Karanam%2520and%2520Vineet%2520Gandhi%26entry.1292438233%3DLarge-scale%2520vision-language%2520models%2520such%2520as%2520CLIP%2520achieve%2520strong%2520zero-shot%2520recognition%2520but%2520struggle%2520with%2520classes%2520that%2520are%2520rarely%2520seen%2520during%2520pretraining%252C%2520including%2520newly%2520emerging%2520entities%2520and%2520culturally%2520specific%2520categories.%2520We%2520introduce%2520LiteEmbed%252C%2520a%2520lightweight%2520framework%2520for%2520few-shot%2520personalization%2520of%2520CLIP%2520that%2520enables%2520new%2520classes%2520to%2520be%2520added%2520without%2520retraining%2520its%2520encoders.%2520LiteEmbed%2520performs%2520subspace-guided%2520optimization%2520of%2520text%2520embeddings%2520within%2520CLIP%2527s%2520vocabulary%252C%2520leveraging%2520a%2520PCA-based%2520decomposition%2520that%2520disentangles%2520coarse%2520semantic%2520directions%2520from%2520fine-grained%2520variations.%2520Two%2520complementary%2520objectives%252C%2520coarse%2520alignment%2520and%2520fine%2520separation%252C%2520jointly%2520preserve%2520global%2520semantic%2520consistency%2520while%2520enhancing%2520discriminability%2520among%2520visually%2520similar%2520classes.%2520Once%2520optimized%252C%2520the%2520embeddings%2520are%2520plug-and-play%252C%2520seamlessly%2520substituting%2520CLIP%2527s%2520original%2520text%2520features%2520across%2520classification%252C%2520retrieval%252C%2520segmentation%252C%2520and%2520detection%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520substantial%2520gains%2520over%2520prior%2520methods%252C%2520establishing%2520LiteEmbed%2520as%2520an%2520effective%2520approach%2520for%2520adapting%2520CLIP%2520to%2520underrepresented%252C%2520rare%252C%2520or%2520unseen%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiteEmbed%3A%20Adapting%20CLIP%20to%20Rare%20Classes&entry.906535625=Aishwarya%20Agarwal%20and%20Srikrishna%20Karanam%20and%20Vineet%20Gandhi&entry.1292438233=Large-scale%20vision-language%20models%20such%20as%20CLIP%20achieve%20strong%20zero-shot%20recognition%20but%20struggle%20with%20classes%20that%20are%20rarely%20seen%20during%20pretraining%2C%20including%20newly%20emerging%20entities%20and%20culturally%20specific%20categories.%20We%20introduce%20LiteEmbed%2C%20a%20lightweight%20framework%20for%20few-shot%20personalization%20of%20CLIP%20that%20enables%20new%20classes%20to%20be%20added%20without%20retraining%20its%20encoders.%20LiteEmbed%20performs%20subspace-guided%20optimization%20of%20text%20embeddings%20within%20CLIP%27s%20vocabulary%2C%20leveraging%20a%20PCA-based%20decomposition%20that%20disentangles%20coarse%20semantic%20directions%20from%20fine-grained%20variations.%20Two%20complementary%20objectives%2C%20coarse%20alignment%20and%20fine%20separation%2C%20jointly%20preserve%20global%20semantic%20consistency%20while%20enhancing%20discriminability%20among%20visually%20similar%20classes.%20Once%20optimized%2C%20the%20embeddings%20are%20plug-and-play%2C%20seamlessly%20substituting%20CLIP%27s%20original%20text%20features%20across%20classification%2C%20retrieval%2C%20segmentation%2C%20and%20detection%20tasks.%20Extensive%20experiments%20demonstrate%20substantial%20gains%20over%20prior%20methods%2C%20establishing%20LiteEmbed%20as%20an%20effective%20approach%20for%20adapting%20CLIP%20to%20underrepresented%2C%20rare%2C%20or%20unseen%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2601.09661v1&entry.124074799=Read"},
{"title": "Decoupling Continual Semantic Segmentation", "author": "Yifu Guo and Yuquan Lu and Wentao Zhang and Zishan Xu and Dexia Chen and Siyu Zhang and Yizhe Zhang and Ruixuan Wang", "abstract": "Continual Semantic Segmentation (CSS) requires learning new classes without forgetting previously acquired knowledge, addressing the fundamental challenge of catastrophic forgetting in dense prediction tasks. However, existing CSS methods typically employ single-stage encoder-decoder architectures where segmentation masks and class labels are tightly coupled, leading to interference between old and new class learning and suboptimal retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage framework for CSS. By decoupling class-aware detection from class-agnostic segmentation, DecoupleCSS enables more effective continual learning, preserving past knowledge while learning new classes. The first stage leverages pre-trained text and image encoders, adapted using LoRA, to encode class-specific information and generate location-aware prompts. In the second stage, the Segment Anything Model (SAM) is employed to produce precise segmentation masks, ensuring that segmentation knowledge is shared across both new and previous classes. This approach improves the balance between retention and adaptability in CSS, achieving state-of-the-art performance across a variety of challenging tasks. Our code is publicly available at: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.", "link": "http://arxiv.org/abs/2508.05065v2", "date": "2026-01-14", "relevancy": 2.6765, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5363}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Continual%20Semantic%20Segmentation&body=Title%3A%20Decoupling%20Continual%20Semantic%20Segmentation%0AAuthor%3A%20Yifu%20Guo%20and%20Yuquan%20Lu%20and%20Wentao%20Zhang%20and%20Zishan%20Xu%20and%20Dexia%20Chen%20and%20Siyu%20Zhang%20and%20Yizhe%20Zhang%20and%20Ruixuan%20Wang%0AAbstract%3A%20Continual%20Semantic%20Segmentation%20%28CSS%29%20requires%20learning%20new%20classes%20without%20forgetting%20previously%20acquired%20knowledge%2C%20addressing%20the%20fundamental%20challenge%20of%20catastrophic%20forgetting%20in%20dense%20prediction%20tasks.%20However%2C%20existing%20CSS%20methods%20typically%20employ%20single-stage%20encoder-decoder%20architectures%20where%20segmentation%20masks%20and%20class%20labels%20are%20tightly%20coupled%2C%20leading%20to%20interference%20between%20old%20and%20new%20class%20learning%20and%20suboptimal%20retention-plasticity%20balance.%20We%20introduce%20DecoupleCSS%2C%20a%20novel%20two-stage%20framework%20for%20CSS.%20By%20decoupling%20class-aware%20detection%20from%20class-agnostic%20segmentation%2C%20DecoupleCSS%20enables%20more%20effective%20continual%20learning%2C%20preserving%20past%20knowledge%20while%20learning%20new%20classes.%20The%20first%20stage%20leverages%20pre-trained%20text%20and%20image%20encoders%2C%20adapted%20using%20LoRA%2C%20to%20encode%20class-specific%20information%20and%20generate%20location-aware%20prompts.%20In%20the%20second%20stage%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20is%20employed%20to%20produce%20precise%20segmentation%20masks%2C%20ensuring%20that%20segmentation%20knowledge%20is%20shared%20across%20both%20new%20and%20previous%20classes.%20This%20approach%20improves%20the%20balance%20between%20retention%20and%20adaptability%20in%20CSS%2C%20achieving%20state-of-the-art%20performance%20across%20a%20variety%20of%20challenging%20tasks.%20Our%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2508.05065v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Continual%2520Semantic%2520Segmentation%26entry.906535625%3DYifu%2520Guo%2520and%2520Yuquan%2520Lu%2520and%2520Wentao%2520Zhang%2520and%2520Zishan%2520Xu%2520and%2520Dexia%2520Chen%2520and%2520Siyu%2520Zhang%2520and%2520Yizhe%2520Zhang%2520and%2520Ruixuan%2520Wang%26entry.1292438233%3DContinual%2520Semantic%2520Segmentation%2520%2528CSS%2529%2520requires%2520learning%2520new%2520classes%2520without%2520forgetting%2520previously%2520acquired%2520knowledge%252C%2520addressing%2520the%2520fundamental%2520challenge%2520of%2520catastrophic%2520forgetting%2520in%2520dense%2520prediction%2520tasks.%2520However%252C%2520existing%2520CSS%2520methods%2520typically%2520employ%2520single-stage%2520encoder-decoder%2520architectures%2520where%2520segmentation%2520masks%2520and%2520class%2520labels%2520are%2520tightly%2520coupled%252C%2520leading%2520to%2520interference%2520between%2520old%2520and%2520new%2520class%2520learning%2520and%2520suboptimal%2520retention-plasticity%2520balance.%2520We%2520introduce%2520DecoupleCSS%252C%2520a%2520novel%2520two-stage%2520framework%2520for%2520CSS.%2520By%2520decoupling%2520class-aware%2520detection%2520from%2520class-agnostic%2520segmentation%252C%2520DecoupleCSS%2520enables%2520more%2520effective%2520continual%2520learning%252C%2520preserving%2520past%2520knowledge%2520while%2520learning%2520new%2520classes.%2520The%2520first%2520stage%2520leverages%2520pre-trained%2520text%2520and%2520image%2520encoders%252C%2520adapted%2520using%2520LoRA%252C%2520to%2520encode%2520class-specific%2520information%2520and%2520generate%2520location-aware%2520prompts.%2520In%2520the%2520second%2520stage%252C%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520is%2520employed%2520to%2520produce%2520precise%2520segmentation%2520masks%252C%2520ensuring%2520that%2520segmentation%2520knowledge%2520is%2520shared%2520across%2520both%2520new%2520and%2520previous%2520classes.%2520This%2520approach%2520improves%2520the%2520balance%2520between%2520retention%2520and%2520adaptability%2520in%2520CSS%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520a%2520variety%2520of%2520challenging%2520tasks.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.05065v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Continual%20Semantic%20Segmentation&entry.906535625=Yifu%20Guo%20and%20Yuquan%20Lu%20and%20Wentao%20Zhang%20and%20Zishan%20Xu%20and%20Dexia%20Chen%20and%20Siyu%20Zhang%20and%20Yizhe%20Zhang%20and%20Ruixuan%20Wang&entry.1292438233=Continual%20Semantic%20Segmentation%20%28CSS%29%20requires%20learning%20new%20classes%20without%20forgetting%20previously%20acquired%20knowledge%2C%20addressing%20the%20fundamental%20challenge%20of%20catastrophic%20forgetting%20in%20dense%20prediction%20tasks.%20However%2C%20existing%20CSS%20methods%20typically%20employ%20single-stage%20encoder-decoder%20architectures%20where%20segmentation%20masks%20and%20class%20labels%20are%20tightly%20coupled%2C%20leading%20to%20interference%20between%20old%20and%20new%20class%20learning%20and%20suboptimal%20retention-plasticity%20balance.%20We%20introduce%20DecoupleCSS%2C%20a%20novel%20two-stage%20framework%20for%20CSS.%20By%20decoupling%20class-aware%20detection%20from%20class-agnostic%20segmentation%2C%20DecoupleCSS%20enables%20more%20effective%20continual%20learning%2C%20preserving%20past%20knowledge%20while%20learning%20new%20classes.%20The%20first%20stage%20leverages%20pre-trained%20text%20and%20image%20encoders%2C%20adapted%20using%20LoRA%2C%20to%20encode%20class-specific%20information%20and%20generate%20location-aware%20prompts.%20In%20the%20second%20stage%2C%20the%20Segment%20Anything%20Model%20%28SAM%29%20is%20employed%20to%20produce%20precise%20segmentation%20masks%2C%20ensuring%20that%20segmentation%20knowledge%20is%20shared%20across%20both%20new%20and%20previous%20classes.%20This%20approach%20improves%20the%20balance%20between%20retention%20and%20adaptability%20in%20CSS%2C%20achieving%20state-of-the-art%20performance%20across%20a%20variety%20of%20challenging%20tasks.%20Our%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2508.05065v2&entry.124074799=Read"},
{"title": "Image2Garment: Simulation-ready Garment Generation from a Single Image", "author": "Selim Emir Can and Jan Ackermann and Kiyohiro Nakayama and Ruofan Liu and Tong Wu and Yang Zheng and Hugo Bertiche and Menglei Chai and Thabo Beeler and Gordon Wetzstein", "abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.", "link": "http://arxiv.org/abs/2601.09658v1", "date": "2026-01-14", "relevancy": 2.6745, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7053}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6548}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image2Garment%3A%20Simulation-ready%20Garment%20Generation%20from%20a%20Single%20Image&body=Title%3A%20Image2Garment%3A%20Simulation-ready%20Garment%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Selim%20Emir%20Can%20and%20Jan%20Ackermann%20and%20Kiyohiro%20Nakayama%20and%20Ruofan%20Liu%20and%20Tong%20Wu%20and%20Yang%20Zheng%20and%20Hugo%20Bertiche%20and%20Menglei%20Chai%20and%20Thabo%20Beeler%20and%20Gordon%20Wetzstein%0AAbstract%3A%20Estimating%20physically%20accurate%2C%20simulation-ready%20garments%20from%20a%20single%20image%20is%20challenging%20due%20to%20the%20absence%20of%20image-to-physics%20datasets%20and%20the%20ill-posed%20nature%20of%20this%20problem.%20Prior%20methods%20either%20require%20multi-view%20capture%20and%20expensive%20differentiable%20simulation%20or%20predict%20only%20garment%20geometry%20without%20the%20material%20properties%20required%20for%20realistic%20simulation.%20We%20propose%20a%20feed-forward%20framework%20that%20sidesteps%20these%20limitations%20by%20first%20fine-tuning%20a%20vision-language%20model%20to%20infer%20material%20composition%20and%20fabric%20attributes%20from%20real%20images%2C%20and%20then%20training%20a%20lightweight%20predictor%20that%20maps%20these%20attributes%20to%20the%20corresponding%20physical%20fabric%20parameters%20using%20a%20small%20dataset%20of%20material-physics%20measurements.%20Our%20approach%20introduces%20two%20new%20datasets%20%28FTAG%20and%20T2P%29%20and%20delivers%20simulation-ready%20garments%20from%20a%20single%20image%20without%20iterative%20optimization.%20Experiments%20show%20that%20our%20estimator%20achieves%20superior%20accuracy%20in%20material%20composition%20estimation%20and%20fabric%20attribute%20prediction%2C%20and%20by%20passing%20them%20through%20our%20physics%20parameter%20estimator%2C%20we%20further%20achieve%20higher-fidelity%20simulations%20compared%20to%20state-of-the-art%20image-to-garment%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09658v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage2Garment%253A%2520Simulation-ready%2520Garment%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DSelim%2520Emir%2520Can%2520and%2520Jan%2520Ackermann%2520and%2520Kiyohiro%2520Nakayama%2520and%2520Ruofan%2520Liu%2520and%2520Tong%2520Wu%2520and%2520Yang%2520Zheng%2520and%2520Hugo%2520Bertiche%2520and%2520Menglei%2520Chai%2520and%2520Thabo%2520Beeler%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3DEstimating%2520physically%2520accurate%252C%2520simulation-ready%2520garments%2520from%2520a%2520single%2520image%2520is%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520image-to-physics%2520datasets%2520and%2520the%2520ill-posed%2520nature%2520of%2520this%2520problem.%2520Prior%2520methods%2520either%2520require%2520multi-view%2520capture%2520and%2520expensive%2520differentiable%2520simulation%2520or%2520predict%2520only%2520garment%2520geometry%2520without%2520the%2520material%2520properties%2520required%2520for%2520realistic%2520simulation.%2520We%2520propose%2520a%2520feed-forward%2520framework%2520that%2520sidesteps%2520these%2520limitations%2520by%2520first%2520fine-tuning%2520a%2520vision-language%2520model%2520to%2520infer%2520material%2520composition%2520and%2520fabric%2520attributes%2520from%2520real%2520images%252C%2520and%2520then%2520training%2520a%2520lightweight%2520predictor%2520that%2520maps%2520these%2520attributes%2520to%2520the%2520corresponding%2520physical%2520fabric%2520parameters%2520using%2520a%2520small%2520dataset%2520of%2520material-physics%2520measurements.%2520Our%2520approach%2520introduces%2520two%2520new%2520datasets%2520%2528FTAG%2520and%2520T2P%2529%2520and%2520delivers%2520simulation-ready%2520garments%2520from%2520a%2520single%2520image%2520without%2520iterative%2520optimization.%2520Experiments%2520show%2520that%2520our%2520estimator%2520achieves%2520superior%2520accuracy%2520in%2520material%2520composition%2520estimation%2520and%2520fabric%2520attribute%2520prediction%252C%2520and%2520by%2520passing%2520them%2520through%2520our%2520physics%2520parameter%2520estimator%252C%2520we%2520further%2520achieve%2520higher-fidelity%2520simulations%2520compared%2520to%2520state-of-the-art%2520image-to-garment%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09658v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image2Garment%3A%20Simulation-ready%20Garment%20Generation%20from%20a%20Single%20Image&entry.906535625=Selim%20Emir%20Can%20and%20Jan%20Ackermann%20and%20Kiyohiro%20Nakayama%20and%20Ruofan%20Liu%20and%20Tong%20Wu%20and%20Yang%20Zheng%20and%20Hugo%20Bertiche%20and%20Menglei%20Chai%20and%20Thabo%20Beeler%20and%20Gordon%20Wetzstein&entry.1292438233=Estimating%20physically%20accurate%2C%20simulation-ready%20garments%20from%20a%20single%20image%20is%20challenging%20due%20to%20the%20absence%20of%20image-to-physics%20datasets%20and%20the%20ill-posed%20nature%20of%20this%20problem.%20Prior%20methods%20either%20require%20multi-view%20capture%20and%20expensive%20differentiable%20simulation%20or%20predict%20only%20garment%20geometry%20without%20the%20material%20properties%20required%20for%20realistic%20simulation.%20We%20propose%20a%20feed-forward%20framework%20that%20sidesteps%20these%20limitations%20by%20first%20fine-tuning%20a%20vision-language%20model%20to%20infer%20material%20composition%20and%20fabric%20attributes%20from%20real%20images%2C%20and%20then%20training%20a%20lightweight%20predictor%20that%20maps%20these%20attributes%20to%20the%20corresponding%20physical%20fabric%20parameters%20using%20a%20small%20dataset%20of%20material-physics%20measurements.%20Our%20approach%20introduces%20two%20new%20datasets%20%28FTAG%20and%20T2P%29%20and%20delivers%20simulation-ready%20garments%20from%20a%20single%20image%20without%20iterative%20optimization.%20Experiments%20show%20that%20our%20estimator%20achieves%20superior%20accuracy%20in%20material%20composition%20estimation%20and%20fabric%20attribute%20prediction%2C%20and%20by%20passing%20them%20through%20our%20physics%20parameter%20estimator%2C%20we%20further%20achieve%20higher-fidelity%20simulations%20compared%20to%20state-of-the-art%20image-to-garment%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.09658v1&entry.124074799=Read"},
{"title": "PrivLEX: Detecting legal concepts in images through Vision-Language Models", "author": "Darya Baranouskaya and Andrea Cavallaro", "abstract": "We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.", "link": "http://arxiv.org/abs/2601.09449v1", "date": "2026-01-14", "relevancy": 2.6688, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrivLEX%3A%20Detecting%20legal%20concepts%20in%20images%20through%20Vision-Language%20Models&body=Title%3A%20PrivLEX%3A%20Detecting%20legal%20concepts%20in%20images%20through%20Vision-Language%20Models%0AAuthor%3A%20Darya%20Baranouskaya%20and%20Andrea%20Cavallaro%0AAbstract%3A%20We%20present%20PrivLEX%2C%20a%20novel%20image%20privacy%20classifier%20that%20grounds%20its%20decisions%20in%20legally%20defined%20personal%20data%20concepts.%20PrivLEX%20is%20the%20first%20interpretable%20privacy%20classifier%20aligned%20with%20legal%20concepts%20that%20leverages%20the%20recognition%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29.%20PrivLEX%20relies%20on%20zero-shot%20VLM%20concept%20detection%20to%20provide%20interpretable%20classification%20through%20a%20label-free%20Concept%20Bottleneck%20Model%2C%20without%20requiring%20explicit%20concept%20labels%20during%20training.%20We%20demonstrate%20PrivLEX%27s%20ability%20to%20identify%20personal%20data%20concepts%20that%20are%20present%20in%20images.%20We%20further%20analyse%20the%20sensitivity%20of%20such%20concepts%20as%20perceived%20by%20human%20annotators%20of%20image%20privacy%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrivLEX%253A%2520Detecting%2520legal%2520concepts%2520in%2520images%2520through%2520Vision-Language%2520Models%26entry.906535625%3DDarya%2520Baranouskaya%2520and%2520Andrea%2520Cavallaro%26entry.1292438233%3DWe%2520present%2520PrivLEX%252C%2520a%2520novel%2520image%2520privacy%2520classifier%2520that%2520grounds%2520its%2520decisions%2520in%2520legally%2520defined%2520personal%2520data%2520concepts.%2520PrivLEX%2520is%2520the%2520first%2520interpretable%2520privacy%2520classifier%2520aligned%2520with%2520legal%2520concepts%2520that%2520leverages%2520the%2520recognition%2520capabilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520PrivLEX%2520relies%2520on%2520zero-shot%2520VLM%2520concept%2520detection%2520to%2520provide%2520interpretable%2520classification%2520through%2520a%2520label-free%2520Concept%2520Bottleneck%2520Model%252C%2520without%2520requiring%2520explicit%2520concept%2520labels%2520during%2520training.%2520We%2520demonstrate%2520PrivLEX%2527s%2520ability%2520to%2520identify%2520personal%2520data%2520concepts%2520that%2520are%2520present%2520in%2520images.%2520We%2520further%2520analyse%2520the%2520sensitivity%2520of%2520such%2520concepts%2520as%2520perceived%2520by%2520human%2520annotators%2520of%2520image%2520privacy%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrivLEX%3A%20Detecting%20legal%20concepts%20in%20images%20through%20Vision-Language%20Models&entry.906535625=Darya%20Baranouskaya%20and%20Andrea%20Cavallaro&entry.1292438233=We%20present%20PrivLEX%2C%20a%20novel%20image%20privacy%20classifier%20that%20grounds%20its%20decisions%20in%20legally%20defined%20personal%20data%20concepts.%20PrivLEX%20is%20the%20first%20interpretable%20privacy%20classifier%20aligned%20with%20legal%20concepts%20that%20leverages%20the%20recognition%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29.%20PrivLEX%20relies%20on%20zero-shot%20VLM%20concept%20detection%20to%20provide%20interpretable%20classification%20through%20a%20label-free%20Concept%20Bottleneck%20Model%2C%20without%20requiring%20explicit%20concept%20labels%20during%20training.%20We%20demonstrate%20PrivLEX%27s%20ability%20to%20identify%20personal%20data%20concepts%20that%20are%20present%20in%20images.%20We%20further%20analyse%20the%20sensitivity%20of%20such%20concepts%20as%20perceived%20by%20human%20annotators%20of%20image%20privacy%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.09449v1&entry.124074799=Read"},
{"title": "Class Adaptive Conformal Training", "author": "Badr-Eddine Marani and Julio Silva-Rodriguez and Ismail Ben Ayed and Maria Vakalopoulou and Stergios Christodoulidis and Jose Dolz", "abstract": "Deep neural networks have achieved remarkable success across a variety of tasks, yet they often suffer from unreliable probability estimates. As a result, they can be overconfident in their predictions. Conformal Prediction (CP) offers a principled framework for uncertainty quantification, yielding prediction sets with rigorous coverage guarantees. Existing conformal training methods optimize for overall set size, but shaping the prediction sets in a class-conditional manner is not straightforward and typically requires prior knowledge of the data distribution. In this work, we introduce Class Adaptive Conformal Training (CaCT), which formulates conformal training as an augmented Lagrangian optimization problem that adaptively learns to shape prediction sets class-conditionally without making any distributional assumptions. Experiments on multiple benchmark datasets, including standard and long-tailed image recognition as well as text classification, demonstrate that CaCT consistently outperforms prior conformal training methods, producing significantly smaller and more informative prediction sets while maintaining the desired coverage guarantees.", "link": "http://arxiv.org/abs/2601.09522v1", "date": "2026-01-14", "relevancy": 2.6499, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5425}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5284}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class%20Adaptive%20Conformal%20Training&body=Title%3A%20Class%20Adaptive%20Conformal%20Training%0AAuthor%3A%20Badr-Eddine%20Marani%20and%20Julio%20Silva-Rodriguez%20and%20Ismail%20Ben%20Ayed%20and%20Maria%20Vakalopoulou%20and%20Stergios%20Christodoulidis%20and%20Jose%20Dolz%0AAbstract%3A%20Deep%20neural%20networks%20have%20achieved%20remarkable%20success%20across%20a%20variety%20of%20tasks%2C%20yet%20they%20often%20suffer%20from%20unreliable%20probability%20estimates.%20As%20a%20result%2C%20they%20can%20be%20overconfident%20in%20their%20predictions.%20Conformal%20Prediction%20%28CP%29%20offers%20a%20principled%20framework%20for%20uncertainty%20quantification%2C%20yielding%20prediction%20sets%20with%20rigorous%20coverage%20guarantees.%20Existing%20conformal%20training%20methods%20optimize%20for%20overall%20set%20size%2C%20but%20shaping%20the%20prediction%20sets%20in%20a%20class-conditional%20manner%20is%20not%20straightforward%20and%20typically%20requires%20prior%20knowledge%20of%20the%20data%20distribution.%20In%20this%20work%2C%20we%20introduce%20Class%20Adaptive%20Conformal%20Training%20%28CaCT%29%2C%20which%20formulates%20conformal%20training%20as%20an%20augmented%20Lagrangian%20optimization%20problem%20that%20adaptively%20learns%20to%20shape%20prediction%20sets%20class-conditionally%20without%20making%20any%20distributional%20assumptions.%20Experiments%20on%20multiple%20benchmark%20datasets%2C%20including%20standard%20and%20long-tailed%20image%20recognition%20as%20well%20as%20text%20classification%2C%20demonstrate%20that%20CaCT%20consistently%20outperforms%20prior%20conformal%20training%20methods%2C%20producing%20significantly%20smaller%20and%20more%20informative%20prediction%20sets%20while%20maintaining%20the%20desired%20coverage%20guarantees.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass%2520Adaptive%2520Conformal%2520Training%26entry.906535625%3DBadr-Eddine%2520Marani%2520and%2520Julio%2520Silva-Rodriguez%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Maria%2520Vakalopoulou%2520and%2520Stergios%2520Christodoulidis%2520and%2520Jose%2520Dolz%26entry.1292438233%3DDeep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520success%2520across%2520a%2520variety%2520of%2520tasks%252C%2520yet%2520they%2520often%2520suffer%2520from%2520unreliable%2520probability%2520estimates.%2520As%2520a%2520result%252C%2520they%2520can%2520be%2520overconfident%2520in%2520their%2520predictions.%2520Conformal%2520Prediction%2520%2528CP%2529%2520offers%2520a%2520principled%2520framework%2520for%2520uncertainty%2520quantification%252C%2520yielding%2520prediction%2520sets%2520with%2520rigorous%2520coverage%2520guarantees.%2520Existing%2520conformal%2520training%2520methods%2520optimize%2520for%2520overall%2520set%2520size%252C%2520but%2520shaping%2520the%2520prediction%2520sets%2520in%2520a%2520class-conditional%2520manner%2520is%2520not%2520straightforward%2520and%2520typically%2520requires%2520prior%2520knowledge%2520of%2520the%2520data%2520distribution.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Class%2520Adaptive%2520Conformal%2520Training%2520%2528CaCT%2529%252C%2520which%2520formulates%2520conformal%2520training%2520as%2520an%2520augmented%2520Lagrangian%2520optimization%2520problem%2520that%2520adaptively%2520learns%2520to%2520shape%2520prediction%2520sets%2520class-conditionally%2520without%2520making%2520any%2520distributional%2520assumptions.%2520Experiments%2520on%2520multiple%2520benchmark%2520datasets%252C%2520including%2520standard%2520and%2520long-tailed%2520image%2520recognition%2520as%2520well%2520as%2520text%2520classification%252C%2520demonstrate%2520that%2520CaCT%2520consistently%2520outperforms%2520prior%2520conformal%2520training%2520methods%252C%2520producing%2520significantly%2520smaller%2520and%2520more%2520informative%2520prediction%2520sets%2520while%2520maintaining%2520the%2520desired%2520coverage%2520guarantees.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class%20Adaptive%20Conformal%20Training&entry.906535625=Badr-Eddine%20Marani%20and%20Julio%20Silva-Rodriguez%20and%20Ismail%20Ben%20Ayed%20and%20Maria%20Vakalopoulou%20and%20Stergios%20Christodoulidis%20and%20Jose%20Dolz&entry.1292438233=Deep%20neural%20networks%20have%20achieved%20remarkable%20success%20across%20a%20variety%20of%20tasks%2C%20yet%20they%20often%20suffer%20from%20unreliable%20probability%20estimates.%20As%20a%20result%2C%20they%20can%20be%20overconfident%20in%20their%20predictions.%20Conformal%20Prediction%20%28CP%29%20offers%20a%20principled%20framework%20for%20uncertainty%20quantification%2C%20yielding%20prediction%20sets%20with%20rigorous%20coverage%20guarantees.%20Existing%20conformal%20training%20methods%20optimize%20for%20overall%20set%20size%2C%20but%20shaping%20the%20prediction%20sets%20in%20a%20class-conditional%20manner%20is%20not%20straightforward%20and%20typically%20requires%20prior%20knowledge%20of%20the%20data%20distribution.%20In%20this%20work%2C%20we%20introduce%20Class%20Adaptive%20Conformal%20Training%20%28CaCT%29%2C%20which%20formulates%20conformal%20training%20as%20an%20augmented%20Lagrangian%20optimization%20problem%20that%20adaptively%20learns%20to%20shape%20prediction%20sets%20class-conditionally%20without%20making%20any%20distributional%20assumptions.%20Experiments%20on%20multiple%20benchmark%20datasets%2C%20including%20standard%20and%20long-tailed%20image%20recognition%20as%20well%20as%20text%20classification%2C%20demonstrate%20that%20CaCT%20consistently%20outperforms%20prior%20conformal%20training%20methods%2C%20producing%20significantly%20smaller%20and%20more%20informative%20prediction%20sets%20while%20maintaining%20the%20desired%20coverage%20guarantees.&entry.1838667208=http%3A//arxiv.org/abs/2601.09522v1&entry.124074799=Read"},
{"title": "Exploiting Task Relationships in Continual Learning via Transferability-Aware Task Embeddings", "author": "Yanru Wu and Jianning Wang and Xiangyu Chen and Enming Zhang and Yang Tan and Hanbing Liu and Yang Li", "abstract": "Continual learning (CL) has been a critical topic in contemporary deep neural network applications, where higher levels of both forward and backward transfer are desirable for an effective CL performance. Existing CL strategies primarily focus on task models, either by regularizing model updates or by separating task-specific and shared components, while often overlooking the potential of leveraging inter-task relationships to enhance transfer. To address this gap, we propose a transferability-aware task embedding, termed H-embedding, and construct a hypernet framework under its guidance to learn task-conditioned model weights for CL tasks. Specifically, H-embedding is derived from an information theoretic measure of transferability and is designed to be online and easy to compute. Our method is also characterized by notable practicality, requiring only the storage of a low-dimensional task embedding per task and supporting efficient end-to-end training. Extensive evaluations on benchmarks including CIFAR-100, ImageNet-R, and DomainNet show that our framework performs prominently compared to various baseline and SOTA approaches, demonstrating strong potential in capturing and utilizing intrinsic task relationships. Our code is publicly available at https://github.com/viki760/Hembedding_Guided_Hypernet.", "link": "http://arxiv.org/abs/2502.11609v4", "date": "2026-01-14", "relevancy": 2.649, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5216}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Task%20Relationships%20in%20Continual%20Learning%20via%20Transferability-Aware%20Task%20Embeddings&body=Title%3A%20Exploiting%20Task%20Relationships%20in%20Continual%20Learning%20via%20Transferability-Aware%20Task%20Embeddings%0AAuthor%3A%20Yanru%20Wu%20and%20Jianning%20Wang%20and%20Xiangyu%20Chen%20and%20Enming%20Zhang%20and%20Yang%20Tan%20and%20Hanbing%20Liu%20and%20Yang%20Li%0AAbstract%3A%20Continual%20learning%20%28CL%29%20has%20been%20a%20critical%20topic%20in%20contemporary%20deep%20neural%20network%20applications%2C%20where%20higher%20levels%20of%20both%20forward%20and%20backward%20transfer%20are%20desirable%20for%20an%20effective%20CL%20performance.%20Existing%20CL%20strategies%20primarily%20focus%20on%20task%20models%2C%20either%20by%20regularizing%20model%20updates%20or%20by%20separating%20task-specific%20and%20shared%20components%2C%20while%20often%20overlooking%20the%20potential%20of%20leveraging%20inter-task%20relationships%20to%20enhance%20transfer.%20To%20address%20this%20gap%2C%20we%20propose%20a%20transferability-aware%20task%20embedding%2C%20termed%20H-embedding%2C%20and%20construct%20a%20hypernet%20framework%20under%20its%20guidance%20to%20learn%20task-conditioned%20model%20weights%20for%20CL%20tasks.%20Specifically%2C%20H-embedding%20is%20derived%20from%20an%20information%20theoretic%20measure%20of%20transferability%20and%20is%20designed%20to%20be%20online%20and%20easy%20to%20compute.%20Our%20method%20is%20also%20characterized%20by%20notable%20practicality%2C%20requiring%20only%20the%20storage%20of%20a%20low-dimensional%20task%20embedding%20per%20task%20and%20supporting%20efficient%20end-to-end%20training.%20Extensive%20evaluations%20on%20benchmarks%20including%20CIFAR-100%2C%20ImageNet-R%2C%20and%20DomainNet%20show%20that%20our%20framework%20performs%20prominently%20compared%20to%20various%20baseline%20and%20SOTA%20approaches%2C%20demonstrating%20strong%20potential%20in%20capturing%20and%20utilizing%20intrinsic%20task%20relationships.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/viki760/Hembedding_Guided_Hypernet.%0ALink%3A%20http%3A//arxiv.org/abs/2502.11609v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Task%2520Relationships%2520in%2520Continual%2520Learning%2520via%2520Transferability-Aware%2520Task%2520Embeddings%26entry.906535625%3DYanru%2520Wu%2520and%2520Jianning%2520Wang%2520and%2520Xiangyu%2520Chen%2520and%2520Enming%2520Zhang%2520and%2520Yang%2520Tan%2520and%2520Hanbing%2520Liu%2520and%2520Yang%2520Li%26entry.1292438233%3DContinual%2520learning%2520%2528CL%2529%2520has%2520been%2520a%2520critical%2520topic%2520in%2520contemporary%2520deep%2520neural%2520network%2520applications%252C%2520where%2520higher%2520levels%2520of%2520both%2520forward%2520and%2520backward%2520transfer%2520are%2520desirable%2520for%2520an%2520effective%2520CL%2520performance.%2520Existing%2520CL%2520strategies%2520primarily%2520focus%2520on%2520task%2520models%252C%2520either%2520by%2520regularizing%2520model%2520updates%2520or%2520by%2520separating%2520task-specific%2520and%2520shared%2520components%252C%2520while%2520often%2520overlooking%2520the%2520potential%2520of%2520leveraging%2520inter-task%2520relationships%2520to%2520enhance%2520transfer.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520a%2520transferability-aware%2520task%2520embedding%252C%2520termed%2520H-embedding%252C%2520and%2520construct%2520a%2520hypernet%2520framework%2520under%2520its%2520guidance%2520to%2520learn%2520task-conditioned%2520model%2520weights%2520for%2520CL%2520tasks.%2520Specifically%252C%2520H-embedding%2520is%2520derived%2520from%2520an%2520information%2520theoretic%2520measure%2520of%2520transferability%2520and%2520is%2520designed%2520to%2520be%2520online%2520and%2520easy%2520to%2520compute.%2520Our%2520method%2520is%2520also%2520characterized%2520by%2520notable%2520practicality%252C%2520requiring%2520only%2520the%2520storage%2520of%2520a%2520low-dimensional%2520task%2520embedding%2520per%2520task%2520and%2520supporting%2520efficient%2520end-to-end%2520training.%2520Extensive%2520evaluations%2520on%2520benchmarks%2520including%2520CIFAR-100%252C%2520ImageNet-R%252C%2520and%2520DomainNet%2520show%2520that%2520our%2520framework%2520performs%2520prominently%2520compared%2520to%2520various%2520baseline%2520and%2520SOTA%2520approaches%252C%2520demonstrating%2520strong%2520potential%2520in%2520capturing%2520and%2520utilizing%2520intrinsic%2520task%2520relationships.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/viki760/Hembedding_Guided_Hypernet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11609v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Task%20Relationships%20in%20Continual%20Learning%20via%20Transferability-Aware%20Task%20Embeddings&entry.906535625=Yanru%20Wu%20and%20Jianning%20Wang%20and%20Xiangyu%20Chen%20and%20Enming%20Zhang%20and%20Yang%20Tan%20and%20Hanbing%20Liu%20and%20Yang%20Li&entry.1292438233=Continual%20learning%20%28CL%29%20has%20been%20a%20critical%20topic%20in%20contemporary%20deep%20neural%20network%20applications%2C%20where%20higher%20levels%20of%20both%20forward%20and%20backward%20transfer%20are%20desirable%20for%20an%20effective%20CL%20performance.%20Existing%20CL%20strategies%20primarily%20focus%20on%20task%20models%2C%20either%20by%20regularizing%20model%20updates%20or%20by%20separating%20task-specific%20and%20shared%20components%2C%20while%20often%20overlooking%20the%20potential%20of%20leveraging%20inter-task%20relationships%20to%20enhance%20transfer.%20To%20address%20this%20gap%2C%20we%20propose%20a%20transferability-aware%20task%20embedding%2C%20termed%20H-embedding%2C%20and%20construct%20a%20hypernet%20framework%20under%20its%20guidance%20to%20learn%20task-conditioned%20model%20weights%20for%20CL%20tasks.%20Specifically%2C%20H-embedding%20is%20derived%20from%20an%20information%20theoretic%20measure%20of%20transferability%20and%20is%20designed%20to%20be%20online%20and%20easy%20to%20compute.%20Our%20method%20is%20also%20characterized%20by%20notable%20practicality%2C%20requiring%20only%20the%20storage%20of%20a%20low-dimensional%20task%20embedding%20per%20task%20and%20supporting%20efficient%20end-to-end%20training.%20Extensive%20evaluations%20on%20benchmarks%20including%20CIFAR-100%2C%20ImageNet-R%2C%20and%20DomainNet%20show%20that%20our%20framework%20performs%20prominently%20compared%20to%20various%20baseline%20and%20SOTA%20approaches%2C%20demonstrating%20strong%20potential%20in%20capturing%20and%20utilizing%20intrinsic%20task%20relationships.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/viki760/Hembedding_Guided_Hypernet.&entry.1838667208=http%3A//arxiv.org/abs/2502.11609v4&entry.124074799=Read"},
{"title": "Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server", "author": "Yue Yao and Ruining Yang and Tom Gedeon", "abstract": "We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.", "link": "http://arxiv.org/abs/2601.09531v1", "date": "2026-01-14", "relevancy": 2.6376, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5198}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bipartite%20Mode%20Matching%20for%20Vision%20Training%20Set%20Search%20from%20a%20Hierarchical%20Data%20Server&body=Title%3A%20Bipartite%20Mode%20Matching%20for%20Vision%20Training%20Set%20Search%20from%20a%20Hierarchical%20Data%20Server%0AAuthor%3A%20Yue%20Yao%20and%20Ruining%20Yang%20and%20Tom%20Gedeon%0AAbstract%3A%20We%20explore%20a%20situation%20in%20which%20the%20target%20domain%20is%20accessible%2C%20but%20real-time%20data%20annotation%20is%20not%20feasible.%20Instead%2C%20we%20would%20like%20to%20construct%20an%20alternative%20training%20set%20from%20a%20large-scale%20data%20server%20so%20that%20a%20competitive%20model%20can%20be%20obtained.%20For%20this%20problem%2C%20because%20the%20target%20domain%20usually%20exhibits%20distinct%20modes%20%28i.e.%2C%20semantic%20clusters%20representing%20data%20distribution%29%2C%20if%20the%20training%20set%20does%20not%20contain%20these%20target%20modes%2C%20the%20model%20performance%20would%20be%20compromised.%20While%20prior%20existing%20works%20improve%20algorithms%20iteratively%2C%20our%20research%20explores%20the%20often-overlooked%20potential%20of%20optimizing%20the%20structure%20of%20the%20data%20server.%20Inspired%20by%20the%20hierarchical%20nature%20of%20web%20search%20engines%2C%20we%20introduce%20a%20hierarchical%20data%20server%2C%20together%20with%20a%20bipartite%20mode%20matching%20algorithm%20%28BMM%29%20to%20align%20source%20and%20target%20modes.%20For%20each%20target%20mode%2C%20we%20look%20in%20the%20server%20data%20tree%20for%20the%20best%20mode%20match%2C%20which%20might%20be%20large%20or%20small%20in%20size.%20Through%20bipartite%20matching%2C%20we%20aim%20for%20all%20target%20modes%20to%20be%20optimally%20matched%20with%20source%20modes%20in%20a%20one-on-one%20fashion.%20Compared%20with%20existing%20training%20set%20search%20algorithms%2C%20we%20show%20that%20the%20matched%20server%20modes%20constitute%20training%20sets%20that%20have%20consistently%20smaller%20domain%20gaps%20with%20the%20target%20domain%20across%20object%20re-identification%20%28re-ID%29%20and%20detection%20tasks.%20Consequently%2C%20models%20trained%20on%20our%20searched%20training%20sets%20have%20higher%20accuracy%20than%20those%20trained%20otherwise.%20BMM%20allows%20data-centric%20unsupervised%20domain%20adaptation%20%28UDA%29%20orthogonal%20to%20existing%20model-centric%20UDA%20methods.%20By%20combining%20the%20BMM%20with%20existing%20UDA%20methods%20like%20pseudo-labeling%2C%20further%20improvement%20is%20observed.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBipartite%2520Mode%2520Matching%2520for%2520Vision%2520Training%2520Set%2520Search%2520from%2520a%2520Hierarchical%2520Data%2520Server%26entry.906535625%3DYue%2520Yao%2520and%2520Ruining%2520Yang%2520and%2520Tom%2520Gedeon%26entry.1292438233%3DWe%2520explore%2520a%2520situation%2520in%2520which%2520the%2520target%2520domain%2520is%2520accessible%252C%2520but%2520real-time%2520data%2520annotation%2520is%2520not%2520feasible.%2520Instead%252C%2520we%2520would%2520like%2520to%2520construct%2520an%2520alternative%2520training%2520set%2520from%2520a%2520large-scale%2520data%2520server%2520so%2520that%2520a%2520competitive%2520model%2520can%2520be%2520obtained.%2520For%2520this%2520problem%252C%2520because%2520the%2520target%2520domain%2520usually%2520exhibits%2520distinct%2520modes%2520%2528i.e.%252C%2520semantic%2520clusters%2520representing%2520data%2520distribution%2529%252C%2520if%2520the%2520training%2520set%2520does%2520not%2520contain%2520these%2520target%2520modes%252C%2520the%2520model%2520performance%2520would%2520be%2520compromised.%2520While%2520prior%2520existing%2520works%2520improve%2520algorithms%2520iteratively%252C%2520our%2520research%2520explores%2520the%2520often-overlooked%2520potential%2520of%2520optimizing%2520the%2520structure%2520of%2520the%2520data%2520server.%2520Inspired%2520by%2520the%2520hierarchical%2520nature%2520of%2520web%2520search%2520engines%252C%2520we%2520introduce%2520a%2520hierarchical%2520data%2520server%252C%2520together%2520with%2520a%2520bipartite%2520mode%2520matching%2520algorithm%2520%2528BMM%2529%2520to%2520align%2520source%2520and%2520target%2520modes.%2520For%2520each%2520target%2520mode%252C%2520we%2520look%2520in%2520the%2520server%2520data%2520tree%2520for%2520the%2520best%2520mode%2520match%252C%2520which%2520might%2520be%2520large%2520or%2520small%2520in%2520size.%2520Through%2520bipartite%2520matching%252C%2520we%2520aim%2520for%2520all%2520target%2520modes%2520to%2520be%2520optimally%2520matched%2520with%2520source%2520modes%2520in%2520a%2520one-on-one%2520fashion.%2520Compared%2520with%2520existing%2520training%2520set%2520search%2520algorithms%252C%2520we%2520show%2520that%2520the%2520matched%2520server%2520modes%2520constitute%2520training%2520sets%2520that%2520have%2520consistently%2520smaller%2520domain%2520gaps%2520with%2520the%2520target%2520domain%2520across%2520object%2520re-identification%2520%2528re-ID%2529%2520and%2520detection%2520tasks.%2520Consequently%252C%2520models%2520trained%2520on%2520our%2520searched%2520training%2520sets%2520have%2520higher%2520accuracy%2520than%2520those%2520trained%2520otherwise.%2520BMM%2520allows%2520data-centric%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%2520orthogonal%2520to%2520existing%2520model-centric%2520UDA%2520methods.%2520By%2520combining%2520the%2520BMM%2520with%2520existing%2520UDA%2520methods%2520like%2520pseudo-labeling%252C%2520further%2520improvement%2520is%2520observed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bipartite%20Mode%20Matching%20for%20Vision%20Training%20Set%20Search%20from%20a%20Hierarchical%20Data%20Server&entry.906535625=Yue%20Yao%20and%20Ruining%20Yang%20and%20Tom%20Gedeon&entry.1292438233=We%20explore%20a%20situation%20in%20which%20the%20target%20domain%20is%20accessible%2C%20but%20real-time%20data%20annotation%20is%20not%20feasible.%20Instead%2C%20we%20would%20like%20to%20construct%20an%20alternative%20training%20set%20from%20a%20large-scale%20data%20server%20so%20that%20a%20competitive%20model%20can%20be%20obtained.%20For%20this%20problem%2C%20because%20the%20target%20domain%20usually%20exhibits%20distinct%20modes%20%28i.e.%2C%20semantic%20clusters%20representing%20data%20distribution%29%2C%20if%20the%20training%20set%20does%20not%20contain%20these%20target%20modes%2C%20the%20model%20performance%20would%20be%20compromised.%20While%20prior%20existing%20works%20improve%20algorithms%20iteratively%2C%20our%20research%20explores%20the%20often-overlooked%20potential%20of%20optimizing%20the%20structure%20of%20the%20data%20server.%20Inspired%20by%20the%20hierarchical%20nature%20of%20web%20search%20engines%2C%20we%20introduce%20a%20hierarchical%20data%20server%2C%20together%20with%20a%20bipartite%20mode%20matching%20algorithm%20%28BMM%29%20to%20align%20source%20and%20target%20modes.%20For%20each%20target%20mode%2C%20we%20look%20in%20the%20server%20data%20tree%20for%20the%20best%20mode%20match%2C%20which%20might%20be%20large%20or%20small%20in%20size.%20Through%20bipartite%20matching%2C%20we%20aim%20for%20all%20target%20modes%20to%20be%20optimally%20matched%20with%20source%20modes%20in%20a%20one-on-one%20fashion.%20Compared%20with%20existing%20training%20set%20search%20algorithms%2C%20we%20show%20that%20the%20matched%20server%20modes%20constitute%20training%20sets%20that%20have%20consistently%20smaller%20domain%20gaps%20with%20the%20target%20domain%20across%20object%20re-identification%20%28re-ID%29%20and%20detection%20tasks.%20Consequently%2C%20models%20trained%20on%20our%20searched%20training%20sets%20have%20higher%20accuracy%20than%20those%20trained%20otherwise.%20BMM%20allows%20data-centric%20unsupervised%20domain%20adaptation%20%28UDA%29%20orthogonal%20to%20existing%20model-centric%20UDA%20methods.%20By%20combining%20the%20BMM%20with%20existing%20UDA%20methods%20like%20pseudo-labeling%2C%20further%20improvement%20is%20observed.&entry.1838667208=http%3A//arxiv.org/abs/2601.09531v1&entry.124074799=Read"},
{"title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering", "author": "Jieying Chen and Jeffrey Hu and Joan Lasenby and Ayush Tewari", "abstract": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.", "link": "http://arxiv.org/abs/2601.09697v1", "date": "2026-01-14", "relevancy": 2.6329, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6657}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6634}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Camera-Controlled%20Video%20Generation%20of%20Static%20Scenes%20via%20Sparse%20Diffusion%20and%203D%20Rendering&body=Title%3A%20Efficient%20Camera-Controlled%20Video%20Generation%20of%20Static%20Scenes%20via%20Sparse%20Diffusion%20and%203D%20Rendering%0AAuthor%3A%20Jieying%20Chen%20and%20Jeffrey%20Hu%20and%20Joan%20Lasenby%20and%20Ayush%20Tewari%0AAbstract%3A%20Modern%20video%20generative%20models%20based%20on%20diffusion%20models%20can%20produce%20very%20realistic%20clips%2C%20but%20they%20are%20computationally%20inefficient%2C%20often%20requiring%20minutes%20of%20GPU%20time%20for%20just%20a%20few%20seconds%20of%20video.%20This%20inefficiency%20poses%20a%20critical%20barrier%20to%20deploying%20generative%20video%20in%20applications%20that%20require%20real-time%20interactions%2C%20such%20as%20embodied%20AI%20and%20VR/AR.%20This%20paper%20explores%20a%20new%20strategy%20for%20camera-conditioned%20video%20generation%20of%20static%20scenes%3A%20using%20diffusion-based%20generative%20models%20to%20generate%20a%20sparse%20set%20of%20keyframes%2C%20and%20then%20synthesizing%20the%20full%20video%20through%203D%20reconstruction%20and%20rendering.%20By%20lifting%20keyframes%20into%20a%203D%20representation%20and%20rendering%20intermediate%20views%2C%20our%20approach%20amortizes%20the%20generation%20cost%20across%20hundreds%20of%20frames%20while%20enforcing%20geometric%20consistency.%20We%20further%20introduce%20a%20model%20that%20predicts%20the%20optimal%20number%20of%20keyframes%20for%20a%20given%20camera%20trajectory%2C%20allowing%20the%20system%20to%20adaptively%20allocate%20computation.%20Our%20final%20method%2C%20SRENDER%2C%20uses%20very%20sparse%20keyframes%20for%20simple%20trajectories%20and%20denser%20ones%20for%20complex%20camera%20motion.%20This%20results%20in%20video%20generation%20that%20is%20more%20than%2040%20times%20faster%20than%20the%20diffusion-based%20baseline%20in%20generating%2020%20seconds%20of%20video%2C%20while%20maintaining%20high%20visual%20fidelity%20and%20temporal%20stability%2C%20offering%20a%20practical%20path%20toward%20efficient%20and%20controllable%20video%20synthesis.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Camera-Controlled%2520Video%2520Generation%2520of%2520Static%2520Scenes%2520via%2520Sparse%2520Diffusion%2520and%25203D%2520Rendering%26entry.906535625%3DJieying%2520Chen%2520and%2520Jeffrey%2520Hu%2520and%2520Joan%2520Lasenby%2520and%2520Ayush%2520Tewari%26entry.1292438233%3DModern%2520video%2520generative%2520models%2520based%2520on%2520diffusion%2520models%2520can%2520produce%2520very%2520realistic%2520clips%252C%2520but%2520they%2520are%2520computationally%2520inefficient%252C%2520often%2520requiring%2520minutes%2520of%2520GPU%2520time%2520for%2520just%2520a%2520few%2520seconds%2520of%2520video.%2520This%2520inefficiency%2520poses%2520a%2520critical%2520barrier%2520to%2520deploying%2520generative%2520video%2520in%2520applications%2520that%2520require%2520real-time%2520interactions%252C%2520such%2520as%2520embodied%2520AI%2520and%2520VR/AR.%2520This%2520paper%2520explores%2520a%2520new%2520strategy%2520for%2520camera-conditioned%2520video%2520generation%2520of%2520static%2520scenes%253A%2520using%2520diffusion-based%2520generative%2520models%2520to%2520generate%2520a%2520sparse%2520set%2520of%2520keyframes%252C%2520and%2520then%2520synthesizing%2520the%2520full%2520video%2520through%25203D%2520reconstruction%2520and%2520rendering.%2520By%2520lifting%2520keyframes%2520into%2520a%25203D%2520representation%2520and%2520rendering%2520intermediate%2520views%252C%2520our%2520approach%2520amortizes%2520the%2520generation%2520cost%2520across%2520hundreds%2520of%2520frames%2520while%2520enforcing%2520geometric%2520consistency.%2520We%2520further%2520introduce%2520a%2520model%2520that%2520predicts%2520the%2520optimal%2520number%2520of%2520keyframes%2520for%2520a%2520given%2520camera%2520trajectory%252C%2520allowing%2520the%2520system%2520to%2520adaptively%2520allocate%2520computation.%2520Our%2520final%2520method%252C%2520SRENDER%252C%2520uses%2520very%2520sparse%2520keyframes%2520for%2520simple%2520trajectories%2520and%2520denser%2520ones%2520for%2520complex%2520camera%2520motion.%2520This%2520results%2520in%2520video%2520generation%2520that%2520is%2520more%2520than%252040%2520times%2520faster%2520than%2520the%2520diffusion-based%2520baseline%2520in%2520generating%252020%2520seconds%2520of%2520video%252C%2520while%2520maintaining%2520high%2520visual%2520fidelity%2520and%2520temporal%2520stability%252C%2520offering%2520a%2520practical%2520path%2520toward%2520efficient%2520and%2520controllable%2520video%2520synthesis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Camera-Controlled%20Video%20Generation%20of%20Static%20Scenes%20via%20Sparse%20Diffusion%20and%203D%20Rendering&entry.906535625=Jieying%20Chen%20and%20Jeffrey%20Hu%20and%20Joan%20Lasenby%20and%20Ayush%20Tewari&entry.1292438233=Modern%20video%20generative%20models%20based%20on%20diffusion%20models%20can%20produce%20very%20realistic%20clips%2C%20but%20they%20are%20computationally%20inefficient%2C%20often%20requiring%20minutes%20of%20GPU%20time%20for%20just%20a%20few%20seconds%20of%20video.%20This%20inefficiency%20poses%20a%20critical%20barrier%20to%20deploying%20generative%20video%20in%20applications%20that%20require%20real-time%20interactions%2C%20such%20as%20embodied%20AI%20and%20VR/AR.%20This%20paper%20explores%20a%20new%20strategy%20for%20camera-conditioned%20video%20generation%20of%20static%20scenes%3A%20using%20diffusion-based%20generative%20models%20to%20generate%20a%20sparse%20set%20of%20keyframes%2C%20and%20then%20synthesizing%20the%20full%20video%20through%203D%20reconstruction%20and%20rendering.%20By%20lifting%20keyframes%20into%20a%203D%20representation%20and%20rendering%20intermediate%20views%2C%20our%20approach%20amortizes%20the%20generation%20cost%20across%20hundreds%20of%20frames%20while%20enforcing%20geometric%20consistency.%20We%20further%20introduce%20a%20model%20that%20predicts%20the%20optimal%20number%20of%20keyframes%20for%20a%20given%20camera%20trajectory%2C%20allowing%20the%20system%20to%20adaptively%20allocate%20computation.%20Our%20final%20method%2C%20SRENDER%2C%20uses%20very%20sparse%20keyframes%20for%20simple%20trajectories%20and%20denser%20ones%20for%20complex%20camera%20motion.%20This%20results%20in%20video%20generation%20that%20is%20more%20than%2040%20times%20faster%20than%20the%20diffusion-based%20baseline%20in%20generating%2020%20seconds%20of%20video%2C%20while%20maintaining%20high%20visual%20fidelity%20and%20temporal%20stability%2C%20offering%20a%20practical%20path%20toward%20efficient%20and%20controllable%20video%20synthesis.&entry.1838667208=http%3A//arxiv.org/abs/2601.09697v1&entry.124074799=Read"},
{"title": "GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR", "author": "Jiaying Zhang and Lei Shi and Jiguo Li and Jun Xu and Jiuchong Gao and Jinghua Hao and Renqing He", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.", "link": "http://arxiv.org/abs/2601.09361v1", "date": "2026-01-14", "relevancy": 2.6193, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.573}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5017}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoRA%3A%20Geometry-Aware%20Low-Rank%20Adaptation%20for%20RLVR&body=Title%3A%20GeoRA%3A%20Geometry-Aware%20Low-Rank%20Adaptation%20for%20RLVR%0AAuthor%3A%20Jiaying%20Zhang%20and%20Lei%20Shi%20and%20Jiguo%20Li%20and%20Jun%20Xu%20and%20Jiuchong%20Gao%20and%20Jinghua%20Hao%20and%20Renqing%20He%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20is%20crucial%20for%20advancing%20large-scale%20reasoning%20models.%20However%2C%20existing%20parameter-efficient%20methods%2C%20such%20as%20PiSSA%20and%20MiLoRA%2C%20are%20designed%20for%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20do%20not%20account%20for%20the%20distinct%20optimization%20dynamics%20and%20geometric%20structures%20of%20RLVR.%20Applying%20these%20methods%20directly%20leads%20to%20spectral%20collapse%20and%20optimization%20instability%2C%20which%20severely%20limit%20model%20performance.%20Meanwhile%2C%20alternative%20approaches%20that%20leverage%20update%20sparsity%20encounter%20significant%20efficiency%20bottlenecks%20on%20modern%20hardware%20due%20to%20unstructured%20computations.%20To%20address%20these%20challenges%2C%20we%20propose%20GeoRA%20%28Geometry-Aware%20Low-Rank%20Adaptation%29%2C%20which%20exploits%20the%20anisotropic%20and%20compressible%20nature%20of%20RL%20update%20subspaces.%20GeoRA%20initializes%20adapters%20by%20extracting%20principal%20directions%20via%20Singular%20Value%20Decomposition%20%28SVD%29%20within%20a%20geometrically%20constrained%20subspace%20while%20freezing%20the%20residual%20components.%20This%20method%20preserves%20the%20pre-trained%20geometric%20structure%20and%20enables%20efficient%20GPU%20computation%20through%20dense%20operators.%20Experiments%20on%20Qwen%20and%20Llama%20demonstrate%20that%20GeoRA%20mitigates%20optimization%20bottlenecks%20caused%20by%20geometric%20misalignment.%20It%20consistently%20outperforms%20established%20low-rank%20baselines%20on%20key%20mathematical%20benchmarks%2C%20achieving%20state-of-the-art%20%28SOTA%29%20results.%20Moreover%2C%20GeoRA%20shows%20superior%20generalization%20and%20resilience%20to%20catastrophic%20forgetting%20in%20out-of-domain%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoRA%253A%2520Geometry-Aware%2520Low-Rank%2520Adaptation%2520for%2520RLVR%26entry.906535625%3DJiaying%2520Zhang%2520and%2520Lei%2520Shi%2520and%2520Jiguo%2520Li%2520and%2520Jun%2520Xu%2520and%2520Jiuchong%2520Gao%2520and%2520Jinghua%2520Hao%2520and%2520Renqing%2520He%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520is%2520crucial%2520for%2520advancing%2520large-scale%2520reasoning%2520models.%2520However%252C%2520existing%2520parameter-efficient%2520methods%252C%2520such%2520as%2520PiSSA%2520and%2520MiLoRA%252C%2520are%2520designed%2520for%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520do%2520not%2520account%2520for%2520the%2520distinct%2520optimization%2520dynamics%2520and%2520geometric%2520structures%2520of%2520RLVR.%2520Applying%2520these%2520methods%2520directly%2520leads%2520to%2520spectral%2520collapse%2520and%2520optimization%2520instability%252C%2520which%2520severely%2520limit%2520model%2520performance.%2520Meanwhile%252C%2520alternative%2520approaches%2520that%2520leverage%2520update%2520sparsity%2520encounter%2520significant%2520efficiency%2520bottlenecks%2520on%2520modern%2520hardware%2520due%2520to%2520unstructured%2520computations.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GeoRA%2520%2528Geometry-Aware%2520Low-Rank%2520Adaptation%2529%252C%2520which%2520exploits%2520the%2520anisotropic%2520and%2520compressible%2520nature%2520of%2520RL%2520update%2520subspaces.%2520GeoRA%2520initializes%2520adapters%2520by%2520extracting%2520principal%2520directions%2520via%2520Singular%2520Value%2520Decomposition%2520%2528SVD%2529%2520within%2520a%2520geometrically%2520constrained%2520subspace%2520while%2520freezing%2520the%2520residual%2520components.%2520This%2520method%2520preserves%2520the%2520pre-trained%2520geometric%2520structure%2520and%2520enables%2520efficient%2520GPU%2520computation%2520through%2520dense%2520operators.%2520Experiments%2520on%2520Qwen%2520and%2520Llama%2520demonstrate%2520that%2520GeoRA%2520mitigates%2520optimization%2520bottlenecks%2520caused%2520by%2520geometric%2520misalignment.%2520It%2520consistently%2520outperforms%2520established%2520low-rank%2520baselines%2520on%2520key%2520mathematical%2520benchmarks%252C%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%2520results.%2520Moreover%252C%2520GeoRA%2520shows%2520superior%2520generalization%2520and%2520resilience%2520to%2520catastrophic%2520forgetting%2520in%2520out-of-domain%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoRA%3A%20Geometry-Aware%20Low-Rank%20Adaptation%20for%20RLVR&entry.906535625=Jiaying%20Zhang%20and%20Lei%20Shi%20and%20Jiguo%20Li%20and%20Jun%20Xu%20and%20Jiuchong%20Gao%20and%20Jinghua%20Hao%20and%20Renqing%20He&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20is%20crucial%20for%20advancing%20large-scale%20reasoning%20models.%20However%2C%20existing%20parameter-efficient%20methods%2C%20such%20as%20PiSSA%20and%20MiLoRA%2C%20are%20designed%20for%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20do%20not%20account%20for%20the%20distinct%20optimization%20dynamics%20and%20geometric%20structures%20of%20RLVR.%20Applying%20these%20methods%20directly%20leads%20to%20spectral%20collapse%20and%20optimization%20instability%2C%20which%20severely%20limit%20model%20performance.%20Meanwhile%2C%20alternative%20approaches%20that%20leverage%20update%20sparsity%20encounter%20significant%20efficiency%20bottlenecks%20on%20modern%20hardware%20due%20to%20unstructured%20computations.%20To%20address%20these%20challenges%2C%20we%20propose%20GeoRA%20%28Geometry-Aware%20Low-Rank%20Adaptation%29%2C%20which%20exploits%20the%20anisotropic%20and%20compressible%20nature%20of%20RL%20update%20subspaces.%20GeoRA%20initializes%20adapters%20by%20extracting%20principal%20directions%20via%20Singular%20Value%20Decomposition%20%28SVD%29%20within%20a%20geometrically%20constrained%20subspace%20while%20freezing%20the%20residual%20components.%20This%20method%20preserves%20the%20pre-trained%20geometric%20structure%20and%20enables%20efficient%20GPU%20computation%20through%20dense%20operators.%20Experiments%20on%20Qwen%20and%20Llama%20demonstrate%20that%20GeoRA%20mitigates%20optimization%20bottlenecks%20caused%20by%20geometric%20misalignment.%20It%20consistently%20outperforms%20established%20low-rank%20baselines%20on%20key%20mathematical%20benchmarks%2C%20achieving%20state-of-the-art%20%28SOTA%29%20results.%20Moreover%2C%20GeoRA%20shows%20superior%20generalization%20and%20resilience%20to%20catastrophic%20forgetting%20in%20out-of-domain%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.09361v1&entry.124074799=Read"},
{"title": "Video Joint-Embedding Predictive Architectures for Facial Expression Recognition", "author": "Lennart Eing and Cristina Luna-Jim\u00e9nez and Silvan Mertes and Elisabeth Andr\u00e9", "abstract": "This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.", "link": "http://arxiv.org/abs/2601.09524v1", "date": "2026-01-14", "relevancy": 2.6062, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5305}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5252}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Joint-Embedding%20Predictive%20Architectures%20for%20Facial%20Expression%20Recognition&body=Title%3A%20Video%20Joint-Embedding%20Predictive%20Architectures%20for%20Facial%20Expression%20Recognition%0AAuthor%3A%20Lennart%20Eing%20and%20Cristina%20Luna-Jim%C3%A9nez%20and%20Silvan%20Mertes%20and%20Elisabeth%20Andr%C3%A9%0AAbstract%3A%20This%20paper%20introduces%20a%20novel%20application%20of%20Video%20Joint-Embedding%20Predictive%20Architectures%20%28V-JEPAs%29%20for%20Facial%20Expression%20Recognition%20%28FER%29.%20Departing%20from%20conventional%20pre-training%20methods%20for%20video%20understanding%20that%20rely%20on%20pixel-level%20reconstructions%2C%20V-JEPAs%20learn%20by%20predicting%20embeddings%20of%20masked%20regions%20from%20the%20embeddings%20of%20unmasked%20regions.%20This%20enables%20the%20trained%20encoder%20to%20not%20capture%20irrelevant%20information%20about%20a%20given%20video%20like%20the%20color%20of%20a%20region%20of%20pixels%20in%20the%20background.%20Using%20a%20pre-trained%20V-JEPA%20video%20encoder%2C%20we%20train%20shallow%20classifiers%20using%20the%20RAVDESS%20and%20CREMA-D%20datasets%2C%20achieving%20state-of-the-art%20performance%20on%20RAVDESS%20and%20outperforming%20all%20other%20vision-based%20methods%20on%20CREMA-D%20%28%2B1.48%20WAR%29.%20Furthermore%2C%20cross-dataset%20evaluations%20reveal%20strong%20generalization%20capabilities%2C%20demonstrating%20the%20potential%20of%20purely%20embedding-based%20pre-training%20approaches%20to%20advance%20FER.%20We%20release%20our%20code%20at%20https%3A//github.com/lennarteingunia/vjepa-for-fer.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Joint-Embedding%2520Predictive%2520Architectures%2520for%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DLennart%2520Eing%2520and%2520Cristina%2520Luna-Jim%25C3%25A9nez%2520and%2520Silvan%2520Mertes%2520and%2520Elisabeth%2520Andr%25C3%25A9%26entry.1292438233%3DThis%2520paper%2520introduces%2520a%2520novel%2520application%2520of%2520Video%2520Joint-Embedding%2520Predictive%2520Architectures%2520%2528V-JEPAs%2529%2520for%2520Facial%2520Expression%2520Recognition%2520%2528FER%2529.%2520Departing%2520from%2520conventional%2520pre-training%2520methods%2520for%2520video%2520understanding%2520that%2520rely%2520on%2520pixel-level%2520reconstructions%252C%2520V-JEPAs%2520learn%2520by%2520predicting%2520embeddings%2520of%2520masked%2520regions%2520from%2520the%2520embeddings%2520of%2520unmasked%2520regions.%2520This%2520enables%2520the%2520trained%2520encoder%2520to%2520not%2520capture%2520irrelevant%2520information%2520about%2520a%2520given%2520video%2520like%2520the%2520color%2520of%2520a%2520region%2520of%2520pixels%2520in%2520the%2520background.%2520Using%2520a%2520pre-trained%2520V-JEPA%2520video%2520encoder%252C%2520we%2520train%2520shallow%2520classifiers%2520using%2520the%2520RAVDESS%2520and%2520CREMA-D%2520datasets%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520RAVDESS%2520and%2520outperforming%2520all%2520other%2520vision-based%2520methods%2520on%2520CREMA-D%2520%2528%252B1.48%2520WAR%2529.%2520Furthermore%252C%2520cross-dataset%2520evaluations%2520reveal%2520strong%2520generalization%2520capabilities%252C%2520demonstrating%2520the%2520potential%2520of%2520purely%2520embedding-based%2520pre-training%2520approaches%2520to%2520advance%2520FER.%2520We%2520release%2520our%2520code%2520at%2520https%253A//github.com/lennarteingunia/vjepa-for-fer.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Joint-Embedding%20Predictive%20Architectures%20for%20Facial%20Expression%20Recognition&entry.906535625=Lennart%20Eing%20and%20Cristina%20Luna-Jim%C3%A9nez%20and%20Silvan%20Mertes%20and%20Elisabeth%20Andr%C3%A9&entry.1292438233=This%20paper%20introduces%20a%20novel%20application%20of%20Video%20Joint-Embedding%20Predictive%20Architectures%20%28V-JEPAs%29%20for%20Facial%20Expression%20Recognition%20%28FER%29.%20Departing%20from%20conventional%20pre-training%20methods%20for%20video%20understanding%20that%20rely%20on%20pixel-level%20reconstructions%2C%20V-JEPAs%20learn%20by%20predicting%20embeddings%20of%20masked%20regions%20from%20the%20embeddings%20of%20unmasked%20regions.%20This%20enables%20the%20trained%20encoder%20to%20not%20capture%20irrelevant%20information%20about%20a%20given%20video%20like%20the%20color%20of%20a%20region%20of%20pixels%20in%20the%20background.%20Using%20a%20pre-trained%20V-JEPA%20video%20encoder%2C%20we%20train%20shallow%20classifiers%20using%20the%20RAVDESS%20and%20CREMA-D%20datasets%2C%20achieving%20state-of-the-art%20performance%20on%20RAVDESS%20and%20outperforming%20all%20other%20vision-based%20methods%20on%20CREMA-D%20%28%2B1.48%20WAR%29.%20Furthermore%2C%20cross-dataset%20evaluations%20reveal%20strong%20generalization%20capabilities%2C%20demonstrating%20the%20potential%20of%20purely%20embedding-based%20pre-training%20approaches%20to%20advance%20FER.%20We%20release%20our%20code%20at%20https%3A//github.com/lennarteingunia/vjepa-for-fer.&entry.1838667208=http%3A//arxiv.org/abs/2601.09524v1&entry.124074799=Read"},
{"title": "Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design", "author": "Lisa Schneckenreiter and Sohvi Luukkonen and Lukas Friedrich and Daniel Kuhn and G\u00fcnter Klambauer", "abstract": "Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.", "link": "http://arxiv.org/abs/2601.09693v1", "date": "2026-01-14", "relevancy": 2.5863, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5436}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.509}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Geometric%20Learning%20Unlocks%20Unified%20Structure-%20and%20Ligand-Based%20Drug%20Design&body=Title%3A%20Contrastive%20Geometric%20Learning%20Unlocks%20Unified%20Structure-%20and%20Ligand-Based%20Drug%20Design%0AAuthor%3A%20Lisa%20Schneckenreiter%20and%20Sohvi%20Luukkonen%20and%20Lukas%20Friedrich%20and%20Daniel%20Kuhn%20and%20G%C3%BCnter%20Klambauer%0AAbstract%3A%20Structure-based%20and%20ligand-based%20computational%20drug%20design%20have%20traditionally%20relied%20on%20disjoint%20data%20sources%20and%20modeling%20assumptions%2C%20limiting%20their%20joint%20use%20at%20scale.%20In%20this%20work%2C%20we%20introduce%20Contrastive%20Geometric%20Learning%20for%20Unified%20Computational%20Drug%20Design%20%28ConGLUDe%29%2C%20a%20single%20contrastive%20geometric%20model%20that%20unifies%20structure-%20and%20ligand-based%20training.%20ConGLUDe%20couples%20a%20geometric%20protein%20encoder%20that%20produces%20whole-protein%20representations%20and%20implicit%20embeddings%20of%20predicted%20binding%20sites%20with%20a%20fast%20ligand%20encoder%2C%20removing%20the%20need%20for%20pre-defined%20pockets.%20By%20aligning%20ligands%20with%20both%20global%20protein%20representations%20and%20multiple%20candidate%20binding%20sites%20through%20contrastive%20learning%2C%20ConGLUDe%20supports%20ligand-conditioned%20pocket%20prediction%20in%20addition%20to%20virtual%20screening%20and%20target%20fishing%2C%20while%20being%20trained%20jointly%20on%20protein-ligand%20complexes%20and%20large-scale%20bioactivity%20data.%20Across%20diverse%20benchmarks%2C%20ConGLUDe%20achieves%20state-of-the-art%20zero-shot%20virtual%20screening%20performance%20in%20settings%20where%20no%20binding%20pocket%20information%20is%20provided%20as%20input%2C%20substantially%20outperforms%20existing%20methods%20on%20a%20challenging%20target%20fishing%20task%2C%20and%20demonstrates%20competitive%20ligand-conditioned%20pocket%20selection.%20These%20results%20highlight%20the%20advantages%20of%20unified%20structure-ligand%20training%20and%20position%20ConGLUDe%20as%20a%20step%20toward%20general-purpose%20foundation%20models%20for%20drug%20discovery.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Geometric%2520Learning%2520Unlocks%2520Unified%2520Structure-%2520and%2520Ligand-Based%2520Drug%2520Design%26entry.906535625%3DLisa%2520Schneckenreiter%2520and%2520Sohvi%2520Luukkonen%2520and%2520Lukas%2520Friedrich%2520and%2520Daniel%2520Kuhn%2520and%2520G%25C3%25BCnter%2520Klambauer%26entry.1292438233%3DStructure-based%2520and%2520ligand-based%2520computational%2520drug%2520design%2520have%2520traditionally%2520relied%2520on%2520disjoint%2520data%2520sources%2520and%2520modeling%2520assumptions%252C%2520limiting%2520their%2520joint%2520use%2520at%2520scale.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Contrastive%2520Geometric%2520Learning%2520for%2520Unified%2520Computational%2520Drug%2520Design%2520%2528ConGLUDe%2529%252C%2520a%2520single%2520contrastive%2520geometric%2520model%2520that%2520unifies%2520structure-%2520and%2520ligand-based%2520training.%2520ConGLUDe%2520couples%2520a%2520geometric%2520protein%2520encoder%2520that%2520produces%2520whole-protein%2520representations%2520and%2520implicit%2520embeddings%2520of%2520predicted%2520binding%2520sites%2520with%2520a%2520fast%2520ligand%2520encoder%252C%2520removing%2520the%2520need%2520for%2520pre-defined%2520pockets.%2520By%2520aligning%2520ligands%2520with%2520both%2520global%2520protein%2520representations%2520and%2520multiple%2520candidate%2520binding%2520sites%2520through%2520contrastive%2520learning%252C%2520ConGLUDe%2520supports%2520ligand-conditioned%2520pocket%2520prediction%2520in%2520addition%2520to%2520virtual%2520screening%2520and%2520target%2520fishing%252C%2520while%2520being%2520trained%2520jointly%2520on%2520protein-ligand%2520complexes%2520and%2520large-scale%2520bioactivity%2520data.%2520Across%2520diverse%2520benchmarks%252C%2520ConGLUDe%2520achieves%2520state-of-the-art%2520zero-shot%2520virtual%2520screening%2520performance%2520in%2520settings%2520where%2520no%2520binding%2520pocket%2520information%2520is%2520provided%2520as%2520input%252C%2520substantially%2520outperforms%2520existing%2520methods%2520on%2520a%2520challenging%2520target%2520fishing%2520task%252C%2520and%2520demonstrates%2520competitive%2520ligand-conditioned%2520pocket%2520selection.%2520These%2520results%2520highlight%2520the%2520advantages%2520of%2520unified%2520structure-ligand%2520training%2520and%2520position%2520ConGLUDe%2520as%2520a%2520step%2520toward%2520general-purpose%2520foundation%2520models%2520for%2520drug%2520discovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Geometric%20Learning%20Unlocks%20Unified%20Structure-%20and%20Ligand-Based%20Drug%20Design&entry.906535625=Lisa%20Schneckenreiter%20and%20Sohvi%20Luukkonen%20and%20Lukas%20Friedrich%20and%20Daniel%20Kuhn%20and%20G%C3%BCnter%20Klambauer&entry.1292438233=Structure-based%20and%20ligand-based%20computational%20drug%20design%20have%20traditionally%20relied%20on%20disjoint%20data%20sources%20and%20modeling%20assumptions%2C%20limiting%20their%20joint%20use%20at%20scale.%20In%20this%20work%2C%20we%20introduce%20Contrastive%20Geometric%20Learning%20for%20Unified%20Computational%20Drug%20Design%20%28ConGLUDe%29%2C%20a%20single%20contrastive%20geometric%20model%20that%20unifies%20structure-%20and%20ligand-based%20training.%20ConGLUDe%20couples%20a%20geometric%20protein%20encoder%20that%20produces%20whole-protein%20representations%20and%20implicit%20embeddings%20of%20predicted%20binding%20sites%20with%20a%20fast%20ligand%20encoder%2C%20removing%20the%20need%20for%20pre-defined%20pockets.%20By%20aligning%20ligands%20with%20both%20global%20protein%20representations%20and%20multiple%20candidate%20binding%20sites%20through%20contrastive%20learning%2C%20ConGLUDe%20supports%20ligand-conditioned%20pocket%20prediction%20in%20addition%20to%20virtual%20screening%20and%20target%20fishing%2C%20while%20being%20trained%20jointly%20on%20protein-ligand%20complexes%20and%20large-scale%20bioactivity%20data.%20Across%20diverse%20benchmarks%2C%20ConGLUDe%20achieves%20state-of-the-art%20zero-shot%20virtual%20screening%20performance%20in%20settings%20where%20no%20binding%20pocket%20information%20is%20provided%20as%20input%2C%20substantially%20outperforms%20existing%20methods%20on%20a%20challenging%20target%20fishing%20task%2C%20and%20demonstrates%20competitive%20ligand-conditioned%20pocket%20selection.%20These%20results%20highlight%20the%20advantages%20of%20unified%20structure-ligand%20training%20and%20position%20ConGLUDe%20as%20a%20step%20toward%20general-purpose%20foundation%20models%20for%20drug%20discovery.&entry.1838667208=http%3A//arxiv.org/abs/2601.09693v1&entry.124074799=Read"},
{"title": "SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings", "author": "Yuchen Wu and Jiahe Li and Xiaohan Yu and Lina Yu and Jin Zheng and Xiao Bai", "abstract": "Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.", "link": "http://arxiv.org/abs/2601.09665v1", "date": "2026-01-14", "relevancy": 2.5647, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6732}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCE-SLAM%3A%20Scale-Consistent%20Monocular%20SLAM%20via%20Scene%20Coordinate%20Embeddings&body=Title%3A%20SCE-SLAM%3A%20Scale-Consistent%20Monocular%20SLAM%20via%20Scene%20Coordinate%20Embeddings%0AAuthor%3A%20Yuchen%20Wu%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Lina%20Yu%20and%20Jin%20Zheng%20and%20Xiao%20Bai%0AAbstract%3A%20Monocular%20visual%20SLAM%20enables%203D%20reconstruction%20from%20internet%20video%20and%20autonomous%20navigation%20on%20resource-constrained%20platforms%2C%20yet%20suffers%20from%20scale%20drift%2C%20i.e.%2C%20the%20gradual%20divergence%20of%20estimated%20scale%20over%20long%20sequences.%20Existing%20frame-to-frame%20methods%20achieve%20real-time%20performance%20through%20local%20optimization%20but%20accumulate%20scale%20drift%20due%20to%20the%20lack%20of%20global%20constraints%20among%20independent%20windows.%20To%20address%20this%2C%20we%20propose%20SCE-SLAM%2C%20an%20end-to-end%20SLAM%20system%20that%20maintains%20scale%20consistency%20through%20scene%20coordinate%20embeddings%2C%20which%20are%20learned%20patch-level%20representations%20encoding%203D%20geometric%20relationships%20under%20a%20canonical%20scale%20reference.%20The%20framework%20consists%20of%20two%20key%20modules%3A%20geometry-guided%20aggregation%20that%20leverages%203D%20spatial%20proximity%20to%20propagate%20scale%20information%20from%20historical%20observations%20through%20geometry-modulated%20attention%2C%20and%20scene%20coordinate%20bundle%20adjustment%20that%20anchors%20current%20estimates%20to%20the%20reference%20scale%20through%20explicit%203D%20coordinate%20constraints%20decoded%20from%20the%20scene%20coordinate%20embeddings.%20Experiments%20on%20KITTI%2C%20Waymo%2C%20and%20vKITTI%20demonstrate%20substantial%20improvements%3A%20our%20method%20reduces%20absolute%20trajectory%20error%20by%208.36m%20on%20KITTI%20compared%20to%20the%20best%20prior%20approach%2C%20while%20maintaining%2036%20FPS%20and%20achieving%20scale%20consistency%20across%20large-scale%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCE-SLAM%253A%2520Scale-Consistent%2520Monocular%2520SLAM%2520via%2520Scene%2520Coordinate%2520Embeddings%26entry.906535625%3DYuchen%2520Wu%2520and%2520Jiahe%2520Li%2520and%2520Xiaohan%2520Yu%2520and%2520Lina%2520Yu%2520and%2520Jin%2520Zheng%2520and%2520Xiao%2520Bai%26entry.1292438233%3DMonocular%2520visual%2520SLAM%2520enables%25203D%2520reconstruction%2520from%2520internet%2520video%2520and%2520autonomous%2520navigation%2520on%2520resource-constrained%2520platforms%252C%2520yet%2520suffers%2520from%2520scale%2520drift%252C%2520i.e.%252C%2520the%2520gradual%2520divergence%2520of%2520estimated%2520scale%2520over%2520long%2520sequences.%2520Existing%2520frame-to-frame%2520methods%2520achieve%2520real-time%2520performance%2520through%2520local%2520optimization%2520but%2520accumulate%2520scale%2520drift%2520due%2520to%2520the%2520lack%2520of%2520global%2520constraints%2520among%2520independent%2520windows.%2520To%2520address%2520this%252C%2520we%2520propose%2520SCE-SLAM%252C%2520an%2520end-to-end%2520SLAM%2520system%2520that%2520maintains%2520scale%2520consistency%2520through%2520scene%2520coordinate%2520embeddings%252C%2520which%2520are%2520learned%2520patch-level%2520representations%2520encoding%25203D%2520geometric%2520relationships%2520under%2520a%2520canonical%2520scale%2520reference.%2520The%2520framework%2520consists%2520of%2520two%2520key%2520modules%253A%2520geometry-guided%2520aggregation%2520that%2520leverages%25203D%2520spatial%2520proximity%2520to%2520propagate%2520scale%2520information%2520from%2520historical%2520observations%2520through%2520geometry-modulated%2520attention%252C%2520and%2520scene%2520coordinate%2520bundle%2520adjustment%2520that%2520anchors%2520current%2520estimates%2520to%2520the%2520reference%2520scale%2520through%2520explicit%25203D%2520coordinate%2520constraints%2520decoded%2520from%2520the%2520scene%2520coordinate%2520embeddings.%2520Experiments%2520on%2520KITTI%252C%2520Waymo%252C%2520and%2520vKITTI%2520demonstrate%2520substantial%2520improvements%253A%2520our%2520method%2520reduces%2520absolute%2520trajectory%2520error%2520by%25208.36m%2520on%2520KITTI%2520compared%2520to%2520the%2520best%2520prior%2520approach%252C%2520while%2520maintaining%252036%2520FPS%2520and%2520achieving%2520scale%2520consistency%2520across%2520large-scale%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCE-SLAM%3A%20Scale-Consistent%20Monocular%20SLAM%20via%20Scene%20Coordinate%20Embeddings&entry.906535625=Yuchen%20Wu%20and%20Jiahe%20Li%20and%20Xiaohan%20Yu%20and%20Lina%20Yu%20and%20Jin%20Zheng%20and%20Xiao%20Bai&entry.1292438233=Monocular%20visual%20SLAM%20enables%203D%20reconstruction%20from%20internet%20video%20and%20autonomous%20navigation%20on%20resource-constrained%20platforms%2C%20yet%20suffers%20from%20scale%20drift%2C%20i.e.%2C%20the%20gradual%20divergence%20of%20estimated%20scale%20over%20long%20sequences.%20Existing%20frame-to-frame%20methods%20achieve%20real-time%20performance%20through%20local%20optimization%20but%20accumulate%20scale%20drift%20due%20to%20the%20lack%20of%20global%20constraints%20among%20independent%20windows.%20To%20address%20this%2C%20we%20propose%20SCE-SLAM%2C%20an%20end-to-end%20SLAM%20system%20that%20maintains%20scale%20consistency%20through%20scene%20coordinate%20embeddings%2C%20which%20are%20learned%20patch-level%20representations%20encoding%203D%20geometric%20relationships%20under%20a%20canonical%20scale%20reference.%20The%20framework%20consists%20of%20two%20key%20modules%3A%20geometry-guided%20aggregation%20that%20leverages%203D%20spatial%20proximity%20to%20propagate%20scale%20information%20from%20historical%20observations%20through%20geometry-modulated%20attention%2C%20and%20scene%20coordinate%20bundle%20adjustment%20that%20anchors%20current%20estimates%20to%20the%20reference%20scale%20through%20explicit%203D%20coordinate%20constraints%20decoded%20from%20the%20scene%20coordinate%20embeddings.%20Experiments%20on%20KITTI%2C%20Waymo%2C%20and%20vKITTI%20demonstrate%20substantial%20improvements%3A%20our%20method%20reduces%20absolute%20trajectory%20error%20by%208.36m%20on%20KITTI%20compared%20to%20the%20best%20prior%20approach%2C%20while%20maintaining%2036%20FPS%20and%20achieving%20scale%20consistency%20across%20large-scale%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2601.09665v1&entry.124074799=Read"},
{"title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer", "author": "Petros Vavaroutsos and Theodoros Palamas and Pantelis Vikatos", "abstract": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation's model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.", "link": "http://arxiv.org/abs/2601.09603v1", "date": "2026-01-14", "relevancy": 2.5553, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Complexity%20Self-Supervised%20Learning%20for%20Music%20Understanding%20with%20Random%20Quantizer&body=Title%3A%20Linear%20Complexity%20Self-Supervised%20Learning%20for%20Music%20Understanding%20with%20Random%20Quantizer%0AAuthor%3A%20Petros%20Vavaroutsos%20and%20Theodoros%20Palamas%20and%20Pantelis%20Vikatos%0AAbstract%3A%20In%20recent%20years%2C%20foundation%20models%20have%20become%20very%20popular%20due%20to%20their%20exceptional%20performance%2C%20mainly%20in%20natural%20language%20%28NLP%29%20tasks%20where%20they%20were%20first%20introduced.%20These%20models%20usually%20consist%20of%20hundreds%20of%20millions%2C%20or%20even%20billions%2C%20of%20parameters%2C%20making%20them%20resource-intensive%20during%20training%20and%20in%20production%20systems%2C%20leading%20to%20increased%20costs.%20This%20paper%20focuses%20on%20the%20reduction%20of%20a%20foundation%27s%20model%20size%20when%20applied%20to%20music%20information%20retrieval%20%28MIR%29%20tasks.%20Our%20research%20combines%20the%20Branchformer%20architecture%20with%20SummaryMixing%2C%20which%20were%20first%20applied%20in%20speech%20recognition%2C%20along%20with%20a%20random%20quantization%20process.%20To%20facilitate%20reproducibility%2C%20we%20conduct%20pre-training%20on%20publicly%20available%20datasets%2C%20complemented%20by%20a%20proprietary%20dataset%20comparable%20in%20scale%20to%20other%20private%20datasets%20reported%20in%20the%20literature.%20We%20ensure%20robust%20evaluation%20by%20using%20a%20framework%20consisting%20of%20a%20variety%20of%20downstream%20MIR%20tasks.%20Our%20results%20show%20that%20our%20architecture%20achieves%20competitive%20performance%20when%20compared%20with%20other%20state-of-the-art%20models%20that%20use%20multi-head%20self-attention%2C%20while%20reducing%20the%20model%20size%20from%208.5%25%20up%20to%2012.3%25.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Complexity%2520Self-Supervised%2520Learning%2520for%2520Music%2520Understanding%2520with%2520Random%2520Quantizer%26entry.906535625%3DPetros%2520Vavaroutsos%2520and%2520Theodoros%2520Palamas%2520and%2520Pantelis%2520Vikatos%26entry.1292438233%3DIn%2520recent%2520years%252C%2520foundation%2520models%2520have%2520become%2520very%2520popular%2520due%2520to%2520their%2520exceptional%2520performance%252C%2520mainly%2520in%2520natural%2520language%2520%2528NLP%2529%2520tasks%2520where%2520they%2520were%2520first%2520introduced.%2520These%2520models%2520usually%2520consist%2520of%2520hundreds%2520of%2520millions%252C%2520or%2520even%2520billions%252C%2520of%2520parameters%252C%2520making%2520them%2520resource-intensive%2520during%2520training%2520and%2520in%2520production%2520systems%252C%2520leading%2520to%2520increased%2520costs.%2520This%2520paper%2520focuses%2520on%2520the%2520reduction%2520of%2520a%2520foundation%2527s%2520model%2520size%2520when%2520applied%2520to%2520music%2520information%2520retrieval%2520%2528MIR%2529%2520tasks.%2520Our%2520research%2520combines%2520the%2520Branchformer%2520architecture%2520with%2520SummaryMixing%252C%2520which%2520were%2520first%2520applied%2520in%2520speech%2520recognition%252C%2520along%2520with%2520a%2520random%2520quantization%2520process.%2520To%2520facilitate%2520reproducibility%252C%2520we%2520conduct%2520pre-training%2520on%2520publicly%2520available%2520datasets%252C%2520complemented%2520by%2520a%2520proprietary%2520dataset%2520comparable%2520in%2520scale%2520to%2520other%2520private%2520datasets%2520reported%2520in%2520the%2520literature.%2520We%2520ensure%2520robust%2520evaluation%2520by%2520using%2520a%2520framework%2520consisting%2520of%2520a%2520variety%2520of%2520downstream%2520MIR%2520tasks.%2520Our%2520results%2520show%2520that%2520our%2520architecture%2520achieves%2520competitive%2520performance%2520when%2520compared%2520with%2520other%2520state-of-the-art%2520models%2520that%2520use%2520multi-head%2520self-attention%252C%2520while%2520reducing%2520the%2520model%2520size%2520from%25208.5%2525%2520up%2520to%252012.3%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Complexity%20Self-Supervised%20Learning%20for%20Music%20Understanding%20with%20Random%20Quantizer&entry.906535625=Petros%20Vavaroutsos%20and%20Theodoros%20Palamas%20and%20Pantelis%20Vikatos&entry.1292438233=In%20recent%20years%2C%20foundation%20models%20have%20become%20very%20popular%20due%20to%20their%20exceptional%20performance%2C%20mainly%20in%20natural%20language%20%28NLP%29%20tasks%20where%20they%20were%20first%20introduced.%20These%20models%20usually%20consist%20of%20hundreds%20of%20millions%2C%20or%20even%20billions%2C%20of%20parameters%2C%20making%20them%20resource-intensive%20during%20training%20and%20in%20production%20systems%2C%20leading%20to%20increased%20costs.%20This%20paper%20focuses%20on%20the%20reduction%20of%20a%20foundation%27s%20model%20size%20when%20applied%20to%20music%20information%20retrieval%20%28MIR%29%20tasks.%20Our%20research%20combines%20the%20Branchformer%20architecture%20with%20SummaryMixing%2C%20which%20were%20first%20applied%20in%20speech%20recognition%2C%20along%20with%20a%20random%20quantization%20process.%20To%20facilitate%20reproducibility%2C%20we%20conduct%20pre-training%20on%20publicly%20available%20datasets%2C%20complemented%20by%20a%20proprietary%20dataset%20comparable%20in%20scale%20to%20other%20private%20datasets%20reported%20in%20the%20literature.%20We%20ensure%20robust%20evaluation%20by%20using%20a%20framework%20consisting%20of%20a%20variety%20of%20downstream%20MIR%20tasks.%20Our%20results%20show%20that%20our%20architecture%20achieves%20competitive%20performance%20when%20compared%20with%20other%20state-of-the-art%20models%20that%20use%20multi-head%20self-attention%2C%20while%20reducing%20the%20model%20size%20from%208.5%25%20up%20to%2012.3%25.&entry.1838667208=http%3A//arxiv.org/abs/2601.09603v1&entry.124074799=Read"},
{"title": "SPGD: Steepest Perturbed Gradient Descent Optimization", "author": "Amir M. Vahedi and Horea T. Ilies", "abstract": "Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.", "link": "http://arxiv.org/abs/2411.04946v3", "date": "2026-01-14", "relevancy": 2.5542, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5181}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5128}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPGD%3A%20Steepest%20Perturbed%20Gradient%20Descent%20Optimization&body=Title%3A%20SPGD%3A%20Steepest%20Perturbed%20Gradient%20Descent%20Optimization%0AAuthor%3A%20Amir%20M.%20Vahedi%20and%20Horea%20T.%20Ilies%0AAbstract%3A%20Optimization%20algorithms%20are%20pivotal%20in%20advancing%20various%20scientific%20and%20industrial%20fields%20but%20often%20encounter%20obstacles%20such%20as%20trapping%20in%20local%20minima%2C%20saddle%20points%2C%20and%20plateaus%20%28flat%20regions%29%2C%20which%20makes%20the%20convergence%20to%20reasonable%20or%20near-optimal%20solutions%20particularly%20challenging.%20This%20paper%20presents%20the%20Steepest%20Perturbed%20Gradient%20Descent%20%28SPGD%29%2C%20a%20novel%20algorithm%20that%20innovatively%20combines%20the%20principles%20of%20the%20gradient%20descent%20method%20with%20periodic%20uniform%20perturbation%20sampling%20to%20effectively%20circumvent%20these%20impediments%20and%20lead%20to%20better%20solutions%20whenever%20possible.%20SPGD%20is%20distinctively%20designed%20to%20generate%20a%20set%20of%20candidate%20solutions%20and%20select%20the%20one%20exhibiting%20the%20steepest%20loss%20difference%20relative%20to%20the%20current%20solution.%20It%20enhances%20the%20traditional%20gradient%20descent%20approach%20by%20integrating%20a%20strategic%20exploration%20mechanism%20that%20significantly%20increases%20the%20likelihood%20of%20escaping%20sub-optimal%20local%20minima%20and%20navigating%20complex%20optimization%20landscapes%20effectively.%20Our%20approach%20not%20only%20retains%20the%20directed%20efficiency%20of%20gradient%20descent%20but%20also%20leverages%20the%20exploratory%20benefits%20of%20stochastic%20perturbations%2C%20thus%20enabling%20a%20more%20comprehensive%20search%20for%20global%20optima%20across%20diverse%20problem%20spaces.%20We%20demonstrate%20the%20efficacy%20of%20SPGD%20in%20solving%20the%203D%20component%20packing%20problem%2C%20an%20NP-hard%20challenge.%20Preliminary%20results%20show%20a%20substantial%20improvement%20over%20four%20established%20methods%2C%20particularly%20on%20response%20surfaces%20with%20complex%20topographies%20and%20in%20multidimensional%20non-convex%20continuous%20optimization%20problems.%20Comparative%20analyses%20with%20established%202D%20benchmark%20functions%20highlight%20SPGD%27s%20superior%20performance%2C%20showcasing%20its%20ability%20to%20navigate%20complex%20optimization%20landscapes.%20These%20results%20emphasize%20SPGD%27s%20potential%20as%20a%20versatile%20tool%20for%20a%20wide%20range%20of%20optimization%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2411.04946v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPGD%253A%2520Steepest%2520Perturbed%2520Gradient%2520Descent%2520Optimization%26entry.906535625%3DAmir%2520M.%2520Vahedi%2520and%2520Horea%2520T.%2520Ilies%26entry.1292438233%3DOptimization%2520algorithms%2520are%2520pivotal%2520in%2520advancing%2520various%2520scientific%2520and%2520industrial%2520fields%2520but%2520often%2520encounter%2520obstacles%2520such%2520as%2520trapping%2520in%2520local%2520minima%252C%2520saddle%2520points%252C%2520and%2520plateaus%2520%2528flat%2520regions%2529%252C%2520which%2520makes%2520the%2520convergence%2520to%2520reasonable%2520or%2520near-optimal%2520solutions%2520particularly%2520challenging.%2520This%2520paper%2520presents%2520the%2520Steepest%2520Perturbed%2520Gradient%2520Descent%2520%2528SPGD%2529%252C%2520a%2520novel%2520algorithm%2520that%2520innovatively%2520combines%2520the%2520principles%2520of%2520the%2520gradient%2520descent%2520method%2520with%2520periodic%2520uniform%2520perturbation%2520sampling%2520to%2520effectively%2520circumvent%2520these%2520impediments%2520and%2520lead%2520to%2520better%2520solutions%2520whenever%2520possible.%2520SPGD%2520is%2520distinctively%2520designed%2520to%2520generate%2520a%2520set%2520of%2520candidate%2520solutions%2520and%2520select%2520the%2520one%2520exhibiting%2520the%2520steepest%2520loss%2520difference%2520relative%2520to%2520the%2520current%2520solution.%2520It%2520enhances%2520the%2520traditional%2520gradient%2520descent%2520approach%2520by%2520integrating%2520a%2520strategic%2520exploration%2520mechanism%2520that%2520significantly%2520increases%2520the%2520likelihood%2520of%2520escaping%2520sub-optimal%2520local%2520minima%2520and%2520navigating%2520complex%2520optimization%2520landscapes%2520effectively.%2520Our%2520approach%2520not%2520only%2520retains%2520the%2520directed%2520efficiency%2520of%2520gradient%2520descent%2520but%2520also%2520leverages%2520the%2520exploratory%2520benefits%2520of%2520stochastic%2520perturbations%252C%2520thus%2520enabling%2520a%2520more%2520comprehensive%2520search%2520for%2520global%2520optima%2520across%2520diverse%2520problem%2520spaces.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520SPGD%2520in%2520solving%2520the%25203D%2520component%2520packing%2520problem%252C%2520an%2520NP-hard%2520challenge.%2520Preliminary%2520results%2520show%2520a%2520substantial%2520improvement%2520over%2520four%2520established%2520methods%252C%2520particularly%2520on%2520response%2520surfaces%2520with%2520complex%2520topographies%2520and%2520in%2520multidimensional%2520non-convex%2520continuous%2520optimization%2520problems.%2520Comparative%2520analyses%2520with%2520established%25202D%2520benchmark%2520functions%2520highlight%2520SPGD%2527s%2520superior%2520performance%252C%2520showcasing%2520its%2520ability%2520to%2520navigate%2520complex%2520optimization%2520landscapes.%2520These%2520results%2520emphasize%2520SPGD%2527s%2520potential%2520as%2520a%2520versatile%2520tool%2520for%2520a%2520wide%2520range%2520of%2520optimization%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04946v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPGD%3A%20Steepest%20Perturbed%20Gradient%20Descent%20Optimization&entry.906535625=Amir%20M.%20Vahedi%20and%20Horea%20T.%20Ilies&entry.1292438233=Optimization%20algorithms%20are%20pivotal%20in%20advancing%20various%20scientific%20and%20industrial%20fields%20but%20often%20encounter%20obstacles%20such%20as%20trapping%20in%20local%20minima%2C%20saddle%20points%2C%20and%20plateaus%20%28flat%20regions%29%2C%20which%20makes%20the%20convergence%20to%20reasonable%20or%20near-optimal%20solutions%20particularly%20challenging.%20This%20paper%20presents%20the%20Steepest%20Perturbed%20Gradient%20Descent%20%28SPGD%29%2C%20a%20novel%20algorithm%20that%20innovatively%20combines%20the%20principles%20of%20the%20gradient%20descent%20method%20with%20periodic%20uniform%20perturbation%20sampling%20to%20effectively%20circumvent%20these%20impediments%20and%20lead%20to%20better%20solutions%20whenever%20possible.%20SPGD%20is%20distinctively%20designed%20to%20generate%20a%20set%20of%20candidate%20solutions%20and%20select%20the%20one%20exhibiting%20the%20steepest%20loss%20difference%20relative%20to%20the%20current%20solution.%20It%20enhances%20the%20traditional%20gradient%20descent%20approach%20by%20integrating%20a%20strategic%20exploration%20mechanism%20that%20significantly%20increases%20the%20likelihood%20of%20escaping%20sub-optimal%20local%20minima%20and%20navigating%20complex%20optimization%20landscapes%20effectively.%20Our%20approach%20not%20only%20retains%20the%20directed%20efficiency%20of%20gradient%20descent%20but%20also%20leverages%20the%20exploratory%20benefits%20of%20stochastic%20perturbations%2C%20thus%20enabling%20a%20more%20comprehensive%20search%20for%20global%20optima%20across%20diverse%20problem%20spaces.%20We%20demonstrate%20the%20efficacy%20of%20SPGD%20in%20solving%20the%203D%20component%20packing%20problem%2C%20an%20NP-hard%20challenge.%20Preliminary%20results%20show%20a%20substantial%20improvement%20over%20four%20established%20methods%2C%20particularly%20on%20response%20surfaces%20with%20complex%20topographies%20and%20in%20multidimensional%20non-convex%20continuous%20optimization%20problems.%20Comparative%20analyses%20with%20established%202D%20benchmark%20functions%20highlight%20SPGD%27s%20superior%20performance%2C%20showcasing%20its%20ability%20to%20navigate%20complex%20optimization%20landscapes.%20These%20results%20emphasize%20SPGD%27s%20potential%20as%20a%20versatile%20tool%20for%20a%20wide%20range%20of%20optimization%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2411.04946v3&entry.124074799=Read"},
{"title": "Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models", "author": "Minh Vu Pham and Hsuvas Borkakoty and Yufang Hou", "abstract": "In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.", "link": "http://arxiv.org/abs/2601.09445v1", "date": "2026-01-14", "relevancy": 2.5535, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20Knowledge%20Collides%3A%20A%20Mechanistic%20Study%20of%20Intra-Memory%20Knowledge%20Conflict%20in%20Language%20Models&body=Title%3A%20Where%20Knowledge%20Collides%3A%20A%20Mechanistic%20Study%20of%20Intra-Memory%20Knowledge%20Conflict%20in%20Language%20Models%0AAuthor%3A%20Minh%20Vu%20Pham%20and%20Hsuvas%20Borkakoty%20and%20Yufang%20Hou%0AAbstract%3A%20In%20language%20models%20%28LMs%29%2C%20intra-memory%20knowledge%20conflict%20largely%20arises%20when%20inconsistent%20information%20about%20the%20same%20event%20is%20encoded%20within%20the%20model%27s%20parametric%20knowledge.%20While%20prior%20work%20has%20primarily%20focused%20on%20resolving%20conflicts%20between%20a%20model%27s%20internal%20knowledge%20and%20external%20resources%20through%20approaches%20such%20as%20fine-tuning%20or%20knowledge%20editing%2C%20the%20problem%20of%20localizing%20conflicts%20that%20originate%20during%20pre-training%20within%20the%20model%27s%20internal%20representations%20remain%20unexplored.%20In%20this%20work%2C%20we%20design%20a%20framework%20based%20on%20mechanistic%20interpretability%20methods%20to%20identify%20where%20and%20how%20conflicting%20knowledge%20from%20the%20pre-training%20data%20is%20encoded%20within%20LMs.%20Our%20findings%20contribute%20to%20a%20growing%20body%20of%20evidence%20that%20specific%20internal%20components%20of%20a%20language%20model%20are%20responsible%20for%20encoding%20conflicting%20knowledge%20from%20pre-training%2C%20and%20we%20demonstrate%20how%20mechanistic%20interpretability%20methods%20can%20be%20leveraged%20to%20causally%20intervene%20in%20and%20control%20conflicting%20knowledge%20at%20inference%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520Knowledge%2520Collides%253A%2520A%2520Mechanistic%2520Study%2520of%2520Intra-Memory%2520Knowledge%2520Conflict%2520in%2520Language%2520Models%26entry.906535625%3DMinh%2520Vu%2520Pham%2520and%2520Hsuvas%2520Borkakoty%2520and%2520Yufang%2520Hou%26entry.1292438233%3DIn%2520language%2520models%2520%2528LMs%2529%252C%2520intra-memory%2520knowledge%2520conflict%2520largely%2520arises%2520when%2520inconsistent%2520information%2520about%2520the%2520same%2520event%2520is%2520encoded%2520within%2520the%2520model%2527s%2520parametric%2520knowledge.%2520While%2520prior%2520work%2520has%2520primarily%2520focused%2520on%2520resolving%2520conflicts%2520between%2520a%2520model%2527s%2520internal%2520knowledge%2520and%2520external%2520resources%2520through%2520approaches%2520such%2520as%2520fine-tuning%2520or%2520knowledge%2520editing%252C%2520the%2520problem%2520of%2520localizing%2520conflicts%2520that%2520originate%2520during%2520pre-training%2520within%2520the%2520model%2527s%2520internal%2520representations%2520remain%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520design%2520a%2520framework%2520based%2520on%2520mechanistic%2520interpretability%2520methods%2520to%2520identify%2520where%2520and%2520how%2520conflicting%2520knowledge%2520from%2520the%2520pre-training%2520data%2520is%2520encoded%2520within%2520LMs.%2520Our%2520findings%2520contribute%2520to%2520a%2520growing%2520body%2520of%2520evidence%2520that%2520specific%2520internal%2520components%2520of%2520a%2520language%2520model%2520are%2520responsible%2520for%2520encoding%2520conflicting%2520knowledge%2520from%2520pre-training%252C%2520and%2520we%2520demonstrate%2520how%2520mechanistic%2520interpretability%2520methods%2520can%2520be%2520leveraged%2520to%2520causally%2520intervene%2520in%2520and%2520control%2520conflicting%2520knowledge%2520at%2520inference%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20Knowledge%20Collides%3A%20A%20Mechanistic%20Study%20of%20Intra-Memory%20Knowledge%20Conflict%20in%20Language%20Models&entry.906535625=Minh%20Vu%20Pham%20and%20Hsuvas%20Borkakoty%20and%20Yufang%20Hou&entry.1292438233=In%20language%20models%20%28LMs%29%2C%20intra-memory%20knowledge%20conflict%20largely%20arises%20when%20inconsistent%20information%20about%20the%20same%20event%20is%20encoded%20within%20the%20model%27s%20parametric%20knowledge.%20While%20prior%20work%20has%20primarily%20focused%20on%20resolving%20conflicts%20between%20a%20model%27s%20internal%20knowledge%20and%20external%20resources%20through%20approaches%20such%20as%20fine-tuning%20or%20knowledge%20editing%2C%20the%20problem%20of%20localizing%20conflicts%20that%20originate%20during%20pre-training%20within%20the%20model%27s%20internal%20representations%20remain%20unexplored.%20In%20this%20work%2C%20we%20design%20a%20framework%20based%20on%20mechanistic%20interpretability%20methods%20to%20identify%20where%20and%20how%20conflicting%20knowledge%20from%20the%20pre-training%20data%20is%20encoded%20within%20LMs.%20Our%20findings%20contribute%20to%20a%20growing%20body%20of%20evidence%20that%20specific%20internal%20components%20of%20a%20language%20model%20are%20responsible%20for%20encoding%20conflicting%20knowledge%20from%20pre-training%2C%20and%20we%20demonstrate%20how%20mechanistic%20interpretability%20methods%20can%20be%20leveraged%20to%20causally%20intervene%20in%20and%20control%20conflicting%20knowledge%20at%20inference%20time.&entry.1838667208=http%3A//arxiv.org/abs/2601.09445v1&entry.124074799=Read"},
{"title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?", "author": "George Ma and Zhongyuan Liang and Irene Y. Chen and Somayeh Sojoudi", "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). We first show through a simple theoretical analysis that $\\ell_1$-regularized SAEs are intrinsically biased toward low-dimensional patterns, providing a mechanistic explanation for why shallow linguistic cues may be preferentially captured over distributed reasoning behaviors. Motivated by this bias, we introduce a falsification-oriented evaluation framework that combines causal token injection and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that features identified by contrastive methods are highly sensitive to token-level interventions, with 45% to 90% activating when a small number of associated tokens are injected into non-reasoning text. For the remaining features, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields no improvements in benchmark performance. Overall, our results suggest that SAE features identified by current contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves. Code is available at https://github.com/GeorgeMLP/reasoning-probing.", "link": "http://arxiv.org/abs/2601.05679v2", "date": "2026-01-14", "relevancy": 2.5326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F&body=Title%3A%20Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F%0AAuthor%3A%20George%20Ma%20and%20Zhongyuan%20Liang%20and%20Irene%20Y.%20Chen%20and%20Somayeh%20Sojoudi%0AAbstract%3A%20We%20investigate%20whether%20sparse%20autoencoders%20%28SAEs%29%20identify%20genuine%20reasoning%20features%20in%20large%20language%20models%20%28LLMs%29.%20We%20first%20show%20through%20a%20simple%20theoretical%20analysis%20that%20%24%5Cell_1%24-regularized%20SAEs%20are%20intrinsically%20biased%20toward%20low-dimensional%20patterns%2C%20providing%20a%20mechanistic%20explanation%20for%20why%20shallow%20linguistic%20cues%20may%20be%20preferentially%20captured%20over%20distributed%20reasoning%20behaviors.%20Motivated%20by%20this%20bias%2C%20we%20introduce%20a%20falsification-oriented%20evaluation%20framework%20that%20combines%20causal%20token%20injection%20and%20LLM-guided%20falsification%20to%20test%20whether%20feature%20activation%20reflects%20reasoning%20processes%20or%20superficial%20linguistic%20correlates.%20Across%2020%20configurations%20spanning%20multiple%20model%20families%2C%20layers%2C%20and%20reasoning%20datasets%2C%20we%20find%20that%20features%20identified%20by%20contrastive%20methods%20are%20highly%20sensitive%20to%20token-level%20interventions%2C%20with%2045%25%20to%2090%25%20activating%20when%20a%20small%20number%20of%20associated%20tokens%20are%20injected%20into%20non-reasoning%20text.%20For%20the%20remaining%20features%2C%20LLM-guided%20falsification%20consistently%20produces%20non-reasoning%20inputs%20that%20activate%20the%20feature%20and%20reasoning%20inputs%20that%20do%20not%2C%20with%20no%20analyzed%20feature%20satisfying%20our%20criteria%20for%20genuine%20reasoning%20behavior.%20Steering%20these%20features%20yields%20no%20improvements%20in%20benchmark%20performance.%20Overall%2C%20our%20results%20suggest%20that%20SAE%20features%20identified%20by%20current%20contrastive%20approaches%20primarily%20capture%20linguistic%20correlates%20of%20reasoning%20rather%20than%20the%20underlying%20reasoning%20computations%20themselves.%20Code%20is%20available%20at%20https%3A//github.com/GeorgeMLP/reasoning-probing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Sparse%2520Autoencoders%2520Identify%2520Reasoning%2520Features%2520in%2520Language%2520Models%253F%26entry.906535625%3DGeorge%2520Ma%2520and%2520Zhongyuan%2520Liang%2520and%2520Irene%2520Y.%2520Chen%2520and%2520Somayeh%2520Sojoudi%26entry.1292438233%3DWe%2520investigate%2520whether%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520identify%2520genuine%2520reasoning%2520features%2520in%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520first%2520show%2520through%2520a%2520simple%2520theoretical%2520analysis%2520that%2520%2524%255Cell_1%2524-regularized%2520SAEs%2520are%2520intrinsically%2520biased%2520toward%2520low-dimensional%2520patterns%252C%2520providing%2520a%2520mechanistic%2520explanation%2520for%2520why%2520shallow%2520linguistic%2520cues%2520may%2520be%2520preferentially%2520captured%2520over%2520distributed%2520reasoning%2520behaviors.%2520Motivated%2520by%2520this%2520bias%252C%2520we%2520introduce%2520a%2520falsification-oriented%2520evaluation%2520framework%2520that%2520combines%2520causal%2520token%2520injection%2520and%2520LLM-guided%2520falsification%2520to%2520test%2520whether%2520feature%2520activation%2520reflects%2520reasoning%2520processes%2520or%2520superficial%2520linguistic%2520correlates.%2520Across%252020%2520configurations%2520spanning%2520multiple%2520model%2520families%252C%2520layers%252C%2520and%2520reasoning%2520datasets%252C%2520we%2520find%2520that%2520features%2520identified%2520by%2520contrastive%2520methods%2520are%2520highly%2520sensitive%2520to%2520token-level%2520interventions%252C%2520with%252045%2525%2520to%252090%2525%2520activating%2520when%2520a%2520small%2520number%2520of%2520associated%2520tokens%2520are%2520injected%2520into%2520non-reasoning%2520text.%2520For%2520the%2520remaining%2520features%252C%2520LLM-guided%2520falsification%2520consistently%2520produces%2520non-reasoning%2520inputs%2520that%2520activate%2520the%2520feature%2520and%2520reasoning%2520inputs%2520that%2520do%2520not%252C%2520with%2520no%2520analyzed%2520feature%2520satisfying%2520our%2520criteria%2520for%2520genuine%2520reasoning%2520behavior.%2520Steering%2520these%2520features%2520yields%2520no%2520improvements%2520in%2520benchmark%2520performance.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520SAE%2520features%2520identified%2520by%2520current%2520contrastive%2520approaches%2520primarily%2520capture%2520linguistic%2520correlates%2520of%2520reasoning%2520rather%2520than%2520the%2520underlying%2520reasoning%2520computations%2520themselves.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/GeorgeMLP/reasoning-probing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Sparse%20Autoencoders%20Identify%20Reasoning%20Features%20in%20Language%20Models%3F&entry.906535625=George%20Ma%20and%20Zhongyuan%20Liang%20and%20Irene%20Y.%20Chen%20and%20Somayeh%20Sojoudi&entry.1292438233=We%20investigate%20whether%20sparse%20autoencoders%20%28SAEs%29%20identify%20genuine%20reasoning%20features%20in%20large%20language%20models%20%28LLMs%29.%20We%20first%20show%20through%20a%20simple%20theoretical%20analysis%20that%20%24%5Cell_1%24-regularized%20SAEs%20are%20intrinsically%20biased%20toward%20low-dimensional%20patterns%2C%20providing%20a%20mechanistic%20explanation%20for%20why%20shallow%20linguistic%20cues%20may%20be%20preferentially%20captured%20over%20distributed%20reasoning%20behaviors.%20Motivated%20by%20this%20bias%2C%20we%20introduce%20a%20falsification-oriented%20evaluation%20framework%20that%20combines%20causal%20token%20injection%20and%20LLM-guided%20falsification%20to%20test%20whether%20feature%20activation%20reflects%20reasoning%20processes%20or%20superficial%20linguistic%20correlates.%20Across%2020%20configurations%20spanning%20multiple%20model%20families%2C%20layers%2C%20and%20reasoning%20datasets%2C%20we%20find%20that%20features%20identified%20by%20contrastive%20methods%20are%20highly%20sensitive%20to%20token-level%20interventions%2C%20with%2045%25%20to%2090%25%20activating%20when%20a%20small%20number%20of%20associated%20tokens%20are%20injected%20into%20non-reasoning%20text.%20For%20the%20remaining%20features%2C%20LLM-guided%20falsification%20consistently%20produces%20non-reasoning%20inputs%20that%20activate%20the%20feature%20and%20reasoning%20inputs%20that%20do%20not%2C%20with%20no%20analyzed%20feature%20satisfying%20our%20criteria%20for%20genuine%20reasoning%20behavior.%20Steering%20these%20features%20yields%20no%20improvements%20in%20benchmark%20performance.%20Overall%2C%20our%20results%20suggest%20that%20SAE%20features%20identified%20by%20current%20contrastive%20approaches%20primarily%20capture%20linguistic%20correlates%20of%20reasoning%20rather%20than%20the%20underlying%20reasoning%20computations%20themselves.%20Code%20is%20available%20at%20https%3A//github.com/GeorgeMLP/reasoning-probing.&entry.1838667208=http%3A//arxiv.org/abs/2601.05679v2&entry.124074799=Read"},
{"title": "STEP3-VL-10B Technical Report", "author": "Ailin Huang and Chengyuan Yao and Chunrui Han and Fanqi Wan and Hangyu Guo and Haoran Lv and Hongyu Zhou and Jia Wang and Jian Zhou and Jianjian Sun and Jingcheng Hu and Kangheng Lin and Liang Zhao and Mitt Huang and Song Yuan and Wenwen Qu and Xiangfeng Wang and Yanlin Lai and Yingxiu Zhao and Yinmin Zhang and Yukang Shi and Yuyang Chen and Zejia Weng and Ziyang Meng and Ang Li and Aobo Kong and Bo Dong and Changyi Wan and David Wang and Di Qi and Dingming Li and En Yu and Guopeng Li and Haiquan Yin and Han Zhou and Hanshan Zhang and Haolong Yan and Hebin Zhou and Hongbo Peng and Jiaran Zhang and Jiashu Lv and Jiayi Fu and Jie Cheng and Jie Zhou and Jisheng Yin and Jingjing Xie and Jingwei Wu and Jun Zhang and Junfeng Liu and Kaijun Tan and Kaiwen Yan and Liangyu Chen and Lina Chen and Mingliang Li and Qian Zhao and Quan Sun and Shaoliang Pang and Shengjie Fan and Shijie Shang and Siyuan Zhang and Tianhao You and Wei Ji and Wuxun Xie and Xiaobo Yang and Xiaojie Hou and Xiaoran Jiao and Xiaoxiao Ren and Xiangwen Kong and Xin Huang and Xin Wu and Xing Chen and Xinran Wang and Xuelin Zhang and Yana Wei and Yang Li and Yanming Xu and Yeqing Shen and Yuang Peng and Yue Peng and Yu Zhou and Yusheng Li and Yuxiang Yang and Yuyang Zhang and Zhe Xie and Zhewei Huang and Zhenyi Lu and Zhimin Fan and Zihui Cheng and Daxin Jiang and Qi Han and Xiangyu Zhang and Yibo Zhu and Zheng Ge", "abstract": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\\times$-20$\\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "link": "http://arxiv.org/abs/2601.09668v1", "date": "2026-01-14", "relevancy": 2.5279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6447}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STEP3-VL-10B%20Technical%20Report&body=Title%3A%20STEP3-VL-10B%20Technical%20Report%0AAuthor%3A%20Ailin%20Huang%20and%20Chengyuan%20Yao%20and%20Chunrui%20Han%20and%20Fanqi%20Wan%20and%20Hangyu%20Guo%20and%20Haoran%20Lv%20and%20Hongyu%20Zhou%20and%20Jia%20Wang%20and%20Jian%20Zhou%20and%20Jianjian%20Sun%20and%20Jingcheng%20Hu%20and%20Kangheng%20Lin%20and%20Liang%20Zhao%20and%20Mitt%20Huang%20and%20Song%20Yuan%20and%20Wenwen%20Qu%20and%20Xiangfeng%20Wang%20and%20Yanlin%20Lai%20and%20Yingxiu%20Zhao%20and%20Yinmin%20Zhang%20and%20Yukang%20Shi%20and%20Yuyang%20Chen%20and%20Zejia%20Weng%20and%20Ziyang%20Meng%20and%20Ang%20Li%20and%20Aobo%20Kong%20and%20Bo%20Dong%20and%20Changyi%20Wan%20and%20David%20Wang%20and%20Di%20Qi%20and%20Dingming%20Li%20and%20En%20Yu%20and%20Guopeng%20Li%20and%20Haiquan%20Yin%20and%20Han%20Zhou%20and%20Hanshan%20Zhang%20and%20Haolong%20Yan%20and%20Hebin%20Zhou%20and%20Hongbo%20Peng%20and%20Jiaran%20Zhang%20and%20Jiashu%20Lv%20and%20Jiayi%20Fu%20and%20Jie%20Cheng%20and%20Jie%20Zhou%20and%20Jisheng%20Yin%20and%20Jingjing%20Xie%20and%20Jingwei%20Wu%20and%20Jun%20Zhang%20and%20Junfeng%20Liu%20and%20Kaijun%20Tan%20and%20Kaiwen%20Yan%20and%20Liangyu%20Chen%20and%20Lina%20Chen%20and%20Mingliang%20Li%20and%20Qian%20Zhao%20and%20Quan%20Sun%20and%20Shaoliang%20Pang%20and%20Shengjie%20Fan%20and%20Shijie%20Shang%20and%20Siyuan%20Zhang%20and%20Tianhao%20You%20and%20Wei%20Ji%20and%20Wuxun%20Xie%20and%20Xiaobo%20Yang%20and%20Xiaojie%20Hou%20and%20Xiaoran%20Jiao%20and%20Xiaoxiao%20Ren%20and%20Xiangwen%20Kong%20and%20Xin%20Huang%20and%20Xin%20Wu%20and%20Xing%20Chen%20and%20Xinran%20Wang%20and%20Xuelin%20Zhang%20and%20Yana%20Wei%20and%20Yang%20Li%20and%20Yanming%20Xu%20and%20Yeqing%20Shen%20and%20Yuang%20Peng%20and%20Yue%20Peng%20and%20Yu%20Zhou%20and%20Yusheng%20Li%20and%20Yuxiang%20Yang%20and%20Yuyang%20Zhang%20and%20Zhe%20Xie%20and%20Zhewei%20Huang%20and%20Zhenyi%20Lu%20and%20Zhimin%20Fan%20and%20Zihui%20Cheng%20and%20Daxin%20Jiang%20and%20Qi%20Han%20and%20Xiangyu%20Zhang%20and%20Yibo%20Zhu%20and%20Zheng%20Ge%0AAbstract%3A%20We%20present%20STEP3-VL-10B%2C%20a%20lightweight%20open-source%20foundation%20model%20designed%20to%20redefine%20the%20trade-off%20between%20compact%20efficiency%20and%20frontier-level%20multimodal%20intelligence.%20STEP3-VL-10B%20is%20realized%20through%20two%20strategic%20shifts%3A%20first%2C%20a%20unified%2C%20fully%20unfrozen%20pre-training%20strategy%20on%201.2T%20multimodal%20tokens%20that%20integrates%20a%20language-aligned%20Perception%20Encoder%20with%20a%20Qwen3-8B%20decoder%20to%20establish%20intrinsic%20vision-language%20synergy%3B%20and%20second%2C%20a%20scaled%20post-training%20pipeline%20featuring%20over%201k%20iterations%20of%20reinforcement%20learning.%20Crucially%2C%20we%20implement%20Parallel%20Coordinated%20Reasoning%20%28PaCoRe%29%20to%20scale%20test-time%20compute%2C%20allocating%20resources%20to%20scalable%20perceptual%20reasoning%20that%20explores%20and%20synthesizes%20diverse%20visual%20hypotheses.%20Consequently%2C%20despite%20its%20compact%2010B%20footprint%2C%20STEP3-VL-10B%20rivals%20or%20surpasses%20models%2010%24%5Ctimes%24-20%24%5Ctimes%24%20larger%20%28e.g.%2C%20GLM-4.6V-106B%2C%20Qwen3-VL-235B%29%20and%20top-tier%20proprietary%20flagships%20like%20Gemini%202.5%20Pro%20and%20Seed-1.5-VL.%20Delivering%20best-in-class%20performance%2C%20it%20records%2092.2%25%20on%20MMBench%20and%2080.11%25%20on%20MMMU%2C%20while%20excelling%20in%20complex%20reasoning%20with%2094.43%25%20on%20AIME2025%20and%2075.95%25%20on%20MathVision.%20We%20release%20the%20full%20model%20suite%20to%20provide%20the%20community%20with%20a%20powerful%2C%20efficient%2C%20and%20reproducible%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTEP3-VL-10B%2520Technical%2520Report%26entry.906535625%3DAilin%2520Huang%2520and%2520Chengyuan%2520Yao%2520and%2520Chunrui%2520Han%2520and%2520Fanqi%2520Wan%2520and%2520Hangyu%2520Guo%2520and%2520Haoran%2520Lv%2520and%2520Hongyu%2520Zhou%2520and%2520Jia%2520Wang%2520and%2520Jian%2520Zhou%2520and%2520Jianjian%2520Sun%2520and%2520Jingcheng%2520Hu%2520and%2520Kangheng%2520Lin%2520and%2520Liang%2520Zhao%2520and%2520Mitt%2520Huang%2520and%2520Song%2520Yuan%2520and%2520Wenwen%2520Qu%2520and%2520Xiangfeng%2520Wang%2520and%2520Yanlin%2520Lai%2520and%2520Yingxiu%2520Zhao%2520and%2520Yinmin%2520Zhang%2520and%2520Yukang%2520Shi%2520and%2520Yuyang%2520Chen%2520and%2520Zejia%2520Weng%2520and%2520Ziyang%2520Meng%2520and%2520Ang%2520Li%2520and%2520Aobo%2520Kong%2520and%2520Bo%2520Dong%2520and%2520Changyi%2520Wan%2520and%2520David%2520Wang%2520and%2520Di%2520Qi%2520and%2520Dingming%2520Li%2520and%2520En%2520Yu%2520and%2520Guopeng%2520Li%2520and%2520Haiquan%2520Yin%2520and%2520Han%2520Zhou%2520and%2520Hanshan%2520Zhang%2520and%2520Haolong%2520Yan%2520and%2520Hebin%2520Zhou%2520and%2520Hongbo%2520Peng%2520and%2520Jiaran%2520Zhang%2520and%2520Jiashu%2520Lv%2520and%2520Jiayi%2520Fu%2520and%2520Jie%2520Cheng%2520and%2520Jie%2520Zhou%2520and%2520Jisheng%2520Yin%2520and%2520Jingjing%2520Xie%2520and%2520Jingwei%2520Wu%2520and%2520Jun%2520Zhang%2520and%2520Junfeng%2520Liu%2520and%2520Kaijun%2520Tan%2520and%2520Kaiwen%2520Yan%2520and%2520Liangyu%2520Chen%2520and%2520Lina%2520Chen%2520and%2520Mingliang%2520Li%2520and%2520Qian%2520Zhao%2520and%2520Quan%2520Sun%2520and%2520Shaoliang%2520Pang%2520and%2520Shengjie%2520Fan%2520and%2520Shijie%2520Shang%2520and%2520Siyuan%2520Zhang%2520and%2520Tianhao%2520You%2520and%2520Wei%2520Ji%2520and%2520Wuxun%2520Xie%2520and%2520Xiaobo%2520Yang%2520and%2520Xiaojie%2520Hou%2520and%2520Xiaoran%2520Jiao%2520and%2520Xiaoxiao%2520Ren%2520and%2520Xiangwen%2520Kong%2520and%2520Xin%2520Huang%2520and%2520Xin%2520Wu%2520and%2520Xing%2520Chen%2520and%2520Xinran%2520Wang%2520and%2520Xuelin%2520Zhang%2520and%2520Yana%2520Wei%2520and%2520Yang%2520Li%2520and%2520Yanming%2520Xu%2520and%2520Yeqing%2520Shen%2520and%2520Yuang%2520Peng%2520and%2520Yue%2520Peng%2520and%2520Yu%2520Zhou%2520and%2520Yusheng%2520Li%2520and%2520Yuxiang%2520Yang%2520and%2520Yuyang%2520Zhang%2520and%2520Zhe%2520Xie%2520and%2520Zhewei%2520Huang%2520and%2520Zhenyi%2520Lu%2520and%2520Zhimin%2520Fan%2520and%2520Zihui%2520Cheng%2520and%2520Daxin%2520Jiang%2520and%2520Qi%2520Han%2520and%2520Xiangyu%2520Zhang%2520and%2520Yibo%2520Zhu%2520and%2520Zheng%2520Ge%26entry.1292438233%3DWe%2520present%2520STEP3-VL-10B%252C%2520a%2520lightweight%2520open-source%2520foundation%2520model%2520designed%2520to%2520redefine%2520the%2520trade-off%2520between%2520compact%2520efficiency%2520and%2520frontier-level%2520multimodal%2520intelligence.%2520STEP3-VL-10B%2520is%2520realized%2520through%2520two%2520strategic%2520shifts%253A%2520first%252C%2520a%2520unified%252C%2520fully%2520unfrozen%2520pre-training%2520strategy%2520on%25201.2T%2520multimodal%2520tokens%2520that%2520integrates%2520a%2520language-aligned%2520Perception%2520Encoder%2520with%2520a%2520Qwen3-8B%2520decoder%2520to%2520establish%2520intrinsic%2520vision-language%2520synergy%253B%2520and%2520second%252C%2520a%2520scaled%2520post-training%2520pipeline%2520featuring%2520over%25201k%2520iterations%2520of%2520reinforcement%2520learning.%2520Crucially%252C%2520we%2520implement%2520Parallel%2520Coordinated%2520Reasoning%2520%2528PaCoRe%2529%2520to%2520scale%2520test-time%2520compute%252C%2520allocating%2520resources%2520to%2520scalable%2520perceptual%2520reasoning%2520that%2520explores%2520and%2520synthesizes%2520diverse%2520visual%2520hypotheses.%2520Consequently%252C%2520despite%2520its%2520compact%252010B%2520footprint%252C%2520STEP3-VL-10B%2520rivals%2520or%2520surpasses%2520models%252010%2524%255Ctimes%2524-20%2524%255Ctimes%2524%2520larger%2520%2528e.g.%252C%2520GLM-4.6V-106B%252C%2520Qwen3-VL-235B%2529%2520and%2520top-tier%2520proprietary%2520flagships%2520like%2520Gemini%25202.5%2520Pro%2520and%2520Seed-1.5-VL.%2520Delivering%2520best-in-class%2520performance%252C%2520it%2520records%252092.2%2525%2520on%2520MMBench%2520and%252080.11%2525%2520on%2520MMMU%252C%2520while%2520excelling%2520in%2520complex%2520reasoning%2520with%252094.43%2525%2520on%2520AIME2025%2520and%252075.95%2525%2520on%2520MathVision.%2520We%2520release%2520the%2520full%2520model%2520suite%2520to%2520provide%2520the%2520community%2520with%2520a%2520powerful%252C%2520efficient%252C%2520and%2520reproducible%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STEP3-VL-10B%20Technical%20Report&entry.906535625=Ailin%20Huang%20and%20Chengyuan%20Yao%20and%20Chunrui%20Han%20and%20Fanqi%20Wan%20and%20Hangyu%20Guo%20and%20Haoran%20Lv%20and%20Hongyu%20Zhou%20and%20Jia%20Wang%20and%20Jian%20Zhou%20and%20Jianjian%20Sun%20and%20Jingcheng%20Hu%20and%20Kangheng%20Lin%20and%20Liang%20Zhao%20and%20Mitt%20Huang%20and%20Song%20Yuan%20and%20Wenwen%20Qu%20and%20Xiangfeng%20Wang%20and%20Yanlin%20Lai%20and%20Yingxiu%20Zhao%20and%20Yinmin%20Zhang%20and%20Yukang%20Shi%20and%20Yuyang%20Chen%20and%20Zejia%20Weng%20and%20Ziyang%20Meng%20and%20Ang%20Li%20and%20Aobo%20Kong%20and%20Bo%20Dong%20and%20Changyi%20Wan%20and%20David%20Wang%20and%20Di%20Qi%20and%20Dingming%20Li%20and%20En%20Yu%20and%20Guopeng%20Li%20and%20Haiquan%20Yin%20and%20Han%20Zhou%20and%20Hanshan%20Zhang%20and%20Haolong%20Yan%20and%20Hebin%20Zhou%20and%20Hongbo%20Peng%20and%20Jiaran%20Zhang%20and%20Jiashu%20Lv%20and%20Jiayi%20Fu%20and%20Jie%20Cheng%20and%20Jie%20Zhou%20and%20Jisheng%20Yin%20and%20Jingjing%20Xie%20and%20Jingwei%20Wu%20and%20Jun%20Zhang%20and%20Junfeng%20Liu%20and%20Kaijun%20Tan%20and%20Kaiwen%20Yan%20and%20Liangyu%20Chen%20and%20Lina%20Chen%20and%20Mingliang%20Li%20and%20Qian%20Zhao%20and%20Quan%20Sun%20and%20Shaoliang%20Pang%20and%20Shengjie%20Fan%20and%20Shijie%20Shang%20and%20Siyuan%20Zhang%20and%20Tianhao%20You%20and%20Wei%20Ji%20and%20Wuxun%20Xie%20and%20Xiaobo%20Yang%20and%20Xiaojie%20Hou%20and%20Xiaoran%20Jiao%20and%20Xiaoxiao%20Ren%20and%20Xiangwen%20Kong%20and%20Xin%20Huang%20and%20Xin%20Wu%20and%20Xing%20Chen%20and%20Xinran%20Wang%20and%20Xuelin%20Zhang%20and%20Yana%20Wei%20and%20Yang%20Li%20and%20Yanming%20Xu%20and%20Yeqing%20Shen%20and%20Yuang%20Peng%20and%20Yue%20Peng%20and%20Yu%20Zhou%20and%20Yusheng%20Li%20and%20Yuxiang%20Yang%20and%20Yuyang%20Zhang%20and%20Zhe%20Xie%20and%20Zhewei%20Huang%20and%20Zhenyi%20Lu%20and%20Zhimin%20Fan%20and%20Zihui%20Cheng%20and%20Daxin%20Jiang%20and%20Qi%20Han%20and%20Xiangyu%20Zhang%20and%20Yibo%20Zhu%20and%20Zheng%20Ge&entry.1292438233=We%20present%20STEP3-VL-10B%2C%20a%20lightweight%20open-source%20foundation%20model%20designed%20to%20redefine%20the%20trade-off%20between%20compact%20efficiency%20and%20frontier-level%20multimodal%20intelligence.%20STEP3-VL-10B%20is%20realized%20through%20two%20strategic%20shifts%3A%20first%2C%20a%20unified%2C%20fully%20unfrozen%20pre-training%20strategy%20on%201.2T%20multimodal%20tokens%20that%20integrates%20a%20language-aligned%20Perception%20Encoder%20with%20a%20Qwen3-8B%20decoder%20to%20establish%20intrinsic%20vision-language%20synergy%3B%20and%20second%2C%20a%20scaled%20post-training%20pipeline%20featuring%20over%201k%20iterations%20of%20reinforcement%20learning.%20Crucially%2C%20we%20implement%20Parallel%20Coordinated%20Reasoning%20%28PaCoRe%29%20to%20scale%20test-time%20compute%2C%20allocating%20resources%20to%20scalable%20perceptual%20reasoning%20that%20explores%20and%20synthesizes%20diverse%20visual%20hypotheses.%20Consequently%2C%20despite%20its%20compact%2010B%20footprint%2C%20STEP3-VL-10B%20rivals%20or%20surpasses%20models%2010%24%5Ctimes%24-20%24%5Ctimes%24%20larger%20%28e.g.%2C%20GLM-4.6V-106B%2C%20Qwen3-VL-235B%29%20and%20top-tier%20proprietary%20flagships%20like%20Gemini%202.5%20Pro%20and%20Seed-1.5-VL.%20Delivering%20best-in-class%20performance%2C%20it%20records%2092.2%25%20on%20MMBench%20and%2080.11%25%20on%20MMMU%2C%20while%20excelling%20in%20complex%20reasoning%20with%2094.43%25%20on%20AIME2025%20and%2075.95%25%20on%20MathVision.%20We%20release%20the%20full%20model%20suite%20to%20provide%20the%20community%20with%20a%20powerful%2C%20efficient%2C%20and%20reproducible%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2601.09668v1&entry.124074799=Read"},
{"title": "Positional Embedding-Aware Activations", "author": "Kathan Shah and Chawin Sitawarin", "abstract": "We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional activation functions. Our proposed architecture, SPDER, is a simple MLP that uses an activation function composed of a sinusoidal multiplied by a sublinear function, called the damping function. The sinusoidal enables the network to automatically learn the positional embedding of an input coordinate while the damping passes on the actual coordinate value by preventing it from being projected down to within a finite range of values. Our results indicate that SPDERs speed up training by 10x and converge to losses 1,500-50,000x lower than that of the state-of-the-art for image representation. SPDER is also state-of-the-art in audio representation. The superior representation capability allows SPDER to also excel on multiple downstream tasks such as image super-resolution and video frame interpolation. We provide intuition as to why SPDER significantly improves fitting compared to that of other INR methods while requiring no hyperparameter tuning or preprocessing.", "link": "http://arxiv.org/abs/2306.15242v2", "date": "2026-01-14", "relevancy": 2.5216, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5181}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.499}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Positional%20Embedding-Aware%20Activations&body=Title%3A%20Positional%20Embedding-Aware%20Activations%0AAuthor%3A%20Kathan%20Shah%20and%20Chawin%20Sitawarin%0AAbstract%3A%20We%20present%20a%20neural%20network%20architecture%20designed%20to%20naturally%20learn%20a%20positional%20embedding%20and%20overcome%20the%20spectral%20bias%20towards%20lower%20frequencies%20faced%20by%20conventional%20activation%20functions.%20Our%20proposed%20architecture%2C%20SPDER%2C%20is%20a%20simple%20MLP%20that%20uses%20an%20activation%20function%20composed%20of%20a%20sinusoidal%20multiplied%20by%20a%20sublinear%20function%2C%20called%20the%20damping%20function.%20The%20sinusoidal%20enables%20the%20network%20to%20automatically%20learn%20the%20positional%20embedding%20of%20an%20input%20coordinate%20while%20the%20damping%20passes%20on%20the%20actual%20coordinate%20value%20by%20preventing%20it%20from%20being%20projected%20down%20to%20within%20a%20finite%20range%20of%20values.%20Our%20results%20indicate%20that%20SPDERs%20speed%20up%20training%20by%2010x%20and%20converge%20to%20losses%201%2C500-50%2C000x%20lower%20than%20that%20of%20the%20state-of-the-art%20for%20image%20representation.%20SPDER%20is%20also%20state-of-the-art%20in%20audio%20representation.%20The%20superior%20representation%20capability%20allows%20SPDER%20to%20also%20excel%20on%20multiple%20downstream%20tasks%20such%20as%20image%20super-resolution%20and%20video%20frame%20interpolation.%20We%20provide%20intuition%20as%20to%20why%20SPDER%20significantly%20improves%20fitting%20compared%20to%20that%20of%20other%20INR%20methods%20while%20requiring%20no%20hyperparameter%20tuning%20or%20preprocessing.%0ALink%3A%20http%3A//arxiv.org/abs/2306.15242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositional%2520Embedding-Aware%2520Activations%26entry.906535625%3DKathan%2520Shah%2520and%2520Chawin%2520Sitawarin%26entry.1292438233%3DWe%2520present%2520a%2520neural%2520network%2520architecture%2520designed%2520to%2520naturally%2520learn%2520a%2520positional%2520embedding%2520and%2520overcome%2520the%2520spectral%2520bias%2520towards%2520lower%2520frequencies%2520faced%2520by%2520conventional%2520activation%2520functions.%2520Our%2520proposed%2520architecture%252C%2520SPDER%252C%2520is%2520a%2520simple%2520MLP%2520that%2520uses%2520an%2520activation%2520function%2520composed%2520of%2520a%2520sinusoidal%2520multiplied%2520by%2520a%2520sublinear%2520function%252C%2520called%2520the%2520damping%2520function.%2520The%2520sinusoidal%2520enables%2520the%2520network%2520to%2520automatically%2520learn%2520the%2520positional%2520embedding%2520of%2520an%2520input%2520coordinate%2520while%2520the%2520damping%2520passes%2520on%2520the%2520actual%2520coordinate%2520value%2520by%2520preventing%2520it%2520from%2520being%2520projected%2520down%2520to%2520within%2520a%2520finite%2520range%2520of%2520values.%2520Our%2520results%2520indicate%2520that%2520SPDERs%2520speed%2520up%2520training%2520by%252010x%2520and%2520converge%2520to%2520losses%25201%252C500-50%252C000x%2520lower%2520than%2520that%2520of%2520the%2520state-of-the-art%2520for%2520image%2520representation.%2520SPDER%2520is%2520also%2520state-of-the-art%2520in%2520audio%2520representation.%2520The%2520superior%2520representation%2520capability%2520allows%2520SPDER%2520to%2520also%2520excel%2520on%2520multiple%2520downstream%2520tasks%2520such%2520as%2520image%2520super-resolution%2520and%2520video%2520frame%2520interpolation.%2520We%2520provide%2520intuition%2520as%2520to%2520why%2520SPDER%2520significantly%2520improves%2520fitting%2520compared%2520to%2520that%2520of%2520other%2520INR%2520methods%2520while%2520requiring%2520no%2520hyperparameter%2520tuning%2520or%2520preprocessing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.15242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Positional%20Embedding-Aware%20Activations&entry.906535625=Kathan%20Shah%20and%20Chawin%20Sitawarin&entry.1292438233=We%20present%20a%20neural%20network%20architecture%20designed%20to%20naturally%20learn%20a%20positional%20embedding%20and%20overcome%20the%20spectral%20bias%20towards%20lower%20frequencies%20faced%20by%20conventional%20activation%20functions.%20Our%20proposed%20architecture%2C%20SPDER%2C%20is%20a%20simple%20MLP%20that%20uses%20an%20activation%20function%20composed%20of%20a%20sinusoidal%20multiplied%20by%20a%20sublinear%20function%2C%20called%20the%20damping%20function.%20The%20sinusoidal%20enables%20the%20network%20to%20automatically%20learn%20the%20positional%20embedding%20of%20an%20input%20coordinate%20while%20the%20damping%20passes%20on%20the%20actual%20coordinate%20value%20by%20preventing%20it%20from%20being%20projected%20down%20to%20within%20a%20finite%20range%20of%20values.%20Our%20results%20indicate%20that%20SPDERs%20speed%20up%20training%20by%2010x%20and%20converge%20to%20losses%201%2C500-50%2C000x%20lower%20than%20that%20of%20the%20state-of-the-art%20for%20image%20representation.%20SPDER%20is%20also%20state-of-the-art%20in%20audio%20representation.%20The%20superior%20representation%20capability%20allows%20SPDER%20to%20also%20excel%20on%20multiple%20downstream%20tasks%20such%20as%20image%20super-resolution%20and%20video%20frame%20interpolation.%20We%20provide%20intuition%20as%20to%20why%20SPDER%20significantly%20improves%20fitting%20compared%20to%20that%20of%20other%20INR%20methods%20while%20requiring%20no%20hyperparameter%20tuning%20or%20preprocessing.&entry.1838667208=http%3A//arxiv.org/abs/2306.15242v2&entry.124074799=Read"},
{"title": "Towards Realistic Synthetic Data for Automatic Drum Transcription", "author": "Pierfrancesco Melucci and Paolo Merialdo and Taketo Akama", "abstract": "Deep learning models define the state-of-the-art in Automatic Drum Transcription (ADT), yet their performance is contingent upon large-scale, paired audio-MIDI datasets, which are scarce. Existing workarounds that use synthetic data often introduce a significant domain gap, as they typically rely on low-fidelity SoundFont libraries that lack acoustic diversity. While high-quality one-shot samples offer a better alternative, they are not available in a standardized, large-scale format suitable for training. This paper introduces a new paradigm for ADT that circumvents the need for paired audio-MIDI training data. Our primary contribution is a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources. We then use this corpus to synthesize a high-quality dataset from MIDI files alone, which we use to train a sequence-to-sequence transcription model. We evaluate our model on the ENST and MDB test sets, where it achieves new state-of-the-art results, significantly outperforming both fully supervised methods and previous synthetic-data approaches. The code for reproducing our experiments is publicly available at https://github.com/pier-maker92/ADT_STR", "link": "http://arxiv.org/abs/2601.09520v1", "date": "2026-01-14", "relevancy": 2.5191, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5099}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.501}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Realistic%20Synthetic%20Data%20for%20Automatic%20Drum%20Transcription&body=Title%3A%20Towards%20Realistic%20Synthetic%20Data%20for%20Automatic%20Drum%20Transcription%0AAuthor%3A%20Pierfrancesco%20Melucci%20and%20Paolo%20Merialdo%20and%20Taketo%20Akama%0AAbstract%3A%20Deep%20learning%20models%20define%20the%20state-of-the-art%20in%20Automatic%20Drum%20Transcription%20%28ADT%29%2C%20yet%20their%20performance%20is%20contingent%20upon%20large-scale%2C%20paired%20audio-MIDI%20datasets%2C%20which%20are%20scarce.%20Existing%20workarounds%20that%20use%20synthetic%20data%20often%20introduce%20a%20significant%20domain%20gap%2C%20as%20they%20typically%20rely%20on%20low-fidelity%20SoundFont%20libraries%20that%20lack%20acoustic%20diversity.%20While%20high-quality%20one-shot%20samples%20offer%20a%20better%20alternative%2C%20they%20are%20not%20available%20in%20a%20standardized%2C%20large-scale%20format%20suitable%20for%20training.%20This%20paper%20introduces%20a%20new%20paradigm%20for%20ADT%20that%20circumvents%20the%20need%20for%20paired%20audio-MIDI%20training%20data.%20Our%20primary%20contribution%20is%20a%20semi-supervised%20method%20to%20automatically%20curate%20a%20large%20and%20diverse%20corpus%20of%20one-shot%20drum%20samples%20from%20unlabeled%20audio%20sources.%20We%20then%20use%20this%20corpus%20to%20synthesize%20a%20high-quality%20dataset%20from%20MIDI%20files%20alone%2C%20which%20we%20use%20to%20train%20a%20sequence-to-sequence%20transcription%20model.%20We%20evaluate%20our%20model%20on%20the%20ENST%20and%20MDB%20test%20sets%2C%20where%20it%20achieves%20new%20state-of-the-art%20results%2C%20significantly%20outperforming%20both%20fully%20supervised%20methods%20and%20previous%20synthetic-data%20approaches.%20The%20code%20for%20reproducing%20our%20experiments%20is%20publicly%20available%20at%20https%3A//github.com/pier-maker92/ADT_STR%0ALink%3A%20http%3A//arxiv.org/abs/2601.09520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Realistic%2520Synthetic%2520Data%2520for%2520Automatic%2520Drum%2520Transcription%26entry.906535625%3DPierfrancesco%2520Melucci%2520and%2520Paolo%2520Merialdo%2520and%2520Taketo%2520Akama%26entry.1292438233%3DDeep%2520learning%2520models%2520define%2520the%2520state-of-the-art%2520in%2520Automatic%2520Drum%2520Transcription%2520%2528ADT%2529%252C%2520yet%2520their%2520performance%2520is%2520contingent%2520upon%2520large-scale%252C%2520paired%2520audio-MIDI%2520datasets%252C%2520which%2520are%2520scarce.%2520Existing%2520workarounds%2520that%2520use%2520synthetic%2520data%2520often%2520introduce%2520a%2520significant%2520domain%2520gap%252C%2520as%2520they%2520typically%2520rely%2520on%2520low-fidelity%2520SoundFont%2520libraries%2520that%2520lack%2520acoustic%2520diversity.%2520While%2520high-quality%2520one-shot%2520samples%2520offer%2520a%2520better%2520alternative%252C%2520they%2520are%2520not%2520available%2520in%2520a%2520standardized%252C%2520large-scale%2520format%2520suitable%2520for%2520training.%2520This%2520paper%2520introduces%2520a%2520new%2520paradigm%2520for%2520ADT%2520that%2520circumvents%2520the%2520need%2520for%2520paired%2520audio-MIDI%2520training%2520data.%2520Our%2520primary%2520contribution%2520is%2520a%2520semi-supervised%2520method%2520to%2520automatically%2520curate%2520a%2520large%2520and%2520diverse%2520corpus%2520of%2520one-shot%2520drum%2520samples%2520from%2520unlabeled%2520audio%2520sources.%2520We%2520then%2520use%2520this%2520corpus%2520to%2520synthesize%2520a%2520high-quality%2520dataset%2520from%2520MIDI%2520files%2520alone%252C%2520which%2520we%2520use%2520to%2520train%2520a%2520sequence-to-sequence%2520transcription%2520model.%2520We%2520evaluate%2520our%2520model%2520on%2520the%2520ENST%2520and%2520MDB%2520test%2520sets%252C%2520where%2520it%2520achieves%2520new%2520state-of-the-art%2520results%252C%2520significantly%2520outperforming%2520both%2520fully%2520supervised%2520methods%2520and%2520previous%2520synthetic-data%2520approaches.%2520The%2520code%2520for%2520reproducing%2520our%2520experiments%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/pier-maker92/ADT_STR%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Realistic%20Synthetic%20Data%20for%20Automatic%20Drum%20Transcription&entry.906535625=Pierfrancesco%20Melucci%20and%20Paolo%20Merialdo%20and%20Taketo%20Akama&entry.1292438233=Deep%20learning%20models%20define%20the%20state-of-the-art%20in%20Automatic%20Drum%20Transcription%20%28ADT%29%2C%20yet%20their%20performance%20is%20contingent%20upon%20large-scale%2C%20paired%20audio-MIDI%20datasets%2C%20which%20are%20scarce.%20Existing%20workarounds%20that%20use%20synthetic%20data%20often%20introduce%20a%20significant%20domain%20gap%2C%20as%20they%20typically%20rely%20on%20low-fidelity%20SoundFont%20libraries%20that%20lack%20acoustic%20diversity.%20While%20high-quality%20one-shot%20samples%20offer%20a%20better%20alternative%2C%20they%20are%20not%20available%20in%20a%20standardized%2C%20large-scale%20format%20suitable%20for%20training.%20This%20paper%20introduces%20a%20new%20paradigm%20for%20ADT%20that%20circumvents%20the%20need%20for%20paired%20audio-MIDI%20training%20data.%20Our%20primary%20contribution%20is%20a%20semi-supervised%20method%20to%20automatically%20curate%20a%20large%20and%20diverse%20corpus%20of%20one-shot%20drum%20samples%20from%20unlabeled%20audio%20sources.%20We%20then%20use%20this%20corpus%20to%20synthesize%20a%20high-quality%20dataset%20from%20MIDI%20files%20alone%2C%20which%20we%20use%20to%20train%20a%20sequence-to-sequence%20transcription%20model.%20We%20evaluate%20our%20model%20on%20the%20ENST%20and%20MDB%20test%20sets%2C%20where%20it%20achieves%20new%20state-of-the-art%20results%2C%20significantly%20outperforming%20both%20fully%20supervised%20methods%20and%20previous%20synthetic-data%20approaches.%20The%20code%20for%20reproducing%20our%20experiments%20is%20publicly%20available%20at%20https%3A//github.com/pier-maker92/ADT_STR&entry.1838667208=http%3A//arxiv.org/abs/2601.09520v1&entry.124074799=Read"},
{"title": "High-fidelity lunar topographic reconstruction across diverse terrain and illumination environments using deep learning", "author": "Hao Chen and Philipp Gl\u00e4ser and Konrad Willner and J\u00fcrgen Oberst", "abstract": "Topographic models are essential for characterizing planetary surfaces and for inferring underlying geological processes. Nevertheless, meter-scale topographic data remain limited, which constrains detailed planetary investigations, even for the Moon, where extensive high-resolution orbital images are available. Recent advances in deep learning (DL) exploit single-view imagery, constrained by low-resolution topography, for fast and flexible reconstruction of fine-scale topography. However, their robustness and general applicability across diverse lunar landforms and illumination conditions remain insufficiently explored. In this study, we build upon our previously proposed DL framework by incorporating a more robust scale recovery scheme and extending the model to polar regions under low solar illumination conditions. We demonstrate that, compared with single-view shape-from-shading methods, the proposed DL approach exhibits greater robustness to varying illumination and achieves more consistent and accurate topographic reconstructions. Furthermore, it reliably reconstructs topography across lunar features of diverse scales, morphologies, and geological ages. High-quality topographic models are also produced for the lunar south polar areas, including permanently shadowed regions, demonstrating the method's capability in reconstructing complex and low-illumination terrain. These findings suggest that DL-based approaches have the potential to leverage extensive lunar datasets to support advanced exploration missions and enable investigations of the Moon at unprecedented topographic resolution.", "link": "http://arxiv.org/abs/2601.09468v1", "date": "2026-01-14", "relevancy": 2.5076, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-fidelity%20lunar%20topographic%20reconstruction%20across%20diverse%20terrain%20and%20illumination%20environments%20using%20deep%20learning&body=Title%3A%20High-fidelity%20lunar%20topographic%20reconstruction%20across%20diverse%20terrain%20and%20illumination%20environments%20using%20deep%20learning%0AAuthor%3A%20Hao%20Chen%20and%20Philipp%20Gl%C3%A4ser%20and%20Konrad%20Willner%20and%20J%C3%BCrgen%20Oberst%0AAbstract%3A%20Topographic%20models%20are%20essential%20for%20characterizing%20planetary%20surfaces%20and%20for%20inferring%20underlying%20geological%20processes.%20Nevertheless%2C%20meter-scale%20topographic%20data%20remain%20limited%2C%20which%20constrains%20detailed%20planetary%20investigations%2C%20even%20for%20the%20Moon%2C%20where%20extensive%20high-resolution%20orbital%20images%20are%20available.%20Recent%20advances%20in%20deep%20learning%20%28DL%29%20exploit%20single-view%20imagery%2C%20constrained%20by%20low-resolution%20topography%2C%20for%20fast%20and%20flexible%20reconstruction%20of%20fine-scale%20topography.%20However%2C%20their%20robustness%20and%20general%20applicability%20across%20diverse%20lunar%20landforms%20and%20illumination%20conditions%20remain%20insufficiently%20explored.%20In%20this%20study%2C%20we%20build%20upon%20our%20previously%20proposed%20DL%20framework%20by%20incorporating%20a%20more%20robust%20scale%20recovery%20scheme%20and%20extending%20the%20model%20to%20polar%20regions%20under%20low%20solar%20illumination%20conditions.%20We%20demonstrate%20that%2C%20compared%20with%20single-view%20shape-from-shading%20methods%2C%20the%20proposed%20DL%20approach%20exhibits%20greater%20robustness%20to%20varying%20illumination%20and%20achieves%20more%20consistent%20and%20accurate%20topographic%20reconstructions.%20Furthermore%2C%20it%20reliably%20reconstructs%20topography%20across%20lunar%20features%20of%20diverse%20scales%2C%20morphologies%2C%20and%20geological%20ages.%20High-quality%20topographic%20models%20are%20also%20produced%20for%20the%20lunar%20south%20polar%20areas%2C%20including%20permanently%20shadowed%20regions%2C%20demonstrating%20the%20method%27s%20capability%20in%20reconstructing%20complex%20and%20low-illumination%20terrain.%20These%20findings%20suggest%20that%20DL-based%20approaches%20have%20the%20potential%20to%20leverage%20extensive%20lunar%20datasets%20to%20support%20advanced%20exploration%20missions%20and%20enable%20investigations%20of%20the%20Moon%20at%20unprecedented%20topographic%20resolution.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-fidelity%2520lunar%2520topographic%2520reconstruction%2520across%2520diverse%2520terrain%2520and%2520illumination%2520environments%2520using%2520deep%2520learning%26entry.906535625%3DHao%2520Chen%2520and%2520Philipp%2520Gl%25C3%25A4ser%2520and%2520Konrad%2520Willner%2520and%2520J%25C3%25BCrgen%2520Oberst%26entry.1292438233%3DTopographic%2520models%2520are%2520essential%2520for%2520characterizing%2520planetary%2520surfaces%2520and%2520for%2520inferring%2520underlying%2520geological%2520processes.%2520Nevertheless%252C%2520meter-scale%2520topographic%2520data%2520remain%2520limited%252C%2520which%2520constrains%2520detailed%2520planetary%2520investigations%252C%2520even%2520for%2520the%2520Moon%252C%2520where%2520extensive%2520high-resolution%2520orbital%2520images%2520are%2520available.%2520Recent%2520advances%2520in%2520deep%2520learning%2520%2528DL%2529%2520exploit%2520single-view%2520imagery%252C%2520constrained%2520by%2520low-resolution%2520topography%252C%2520for%2520fast%2520and%2520flexible%2520reconstruction%2520of%2520fine-scale%2520topography.%2520However%252C%2520their%2520robustness%2520and%2520general%2520applicability%2520across%2520diverse%2520lunar%2520landforms%2520and%2520illumination%2520conditions%2520remain%2520insufficiently%2520explored.%2520In%2520this%2520study%252C%2520we%2520build%2520upon%2520our%2520previously%2520proposed%2520DL%2520framework%2520by%2520incorporating%2520a%2520more%2520robust%2520scale%2520recovery%2520scheme%2520and%2520extending%2520the%2520model%2520to%2520polar%2520regions%2520under%2520low%2520solar%2520illumination%2520conditions.%2520We%2520demonstrate%2520that%252C%2520compared%2520with%2520single-view%2520shape-from-shading%2520methods%252C%2520the%2520proposed%2520DL%2520approach%2520exhibits%2520greater%2520robustness%2520to%2520varying%2520illumination%2520and%2520achieves%2520more%2520consistent%2520and%2520accurate%2520topographic%2520reconstructions.%2520Furthermore%252C%2520it%2520reliably%2520reconstructs%2520topography%2520across%2520lunar%2520features%2520of%2520diverse%2520scales%252C%2520morphologies%252C%2520and%2520geological%2520ages.%2520High-quality%2520topographic%2520models%2520are%2520also%2520produced%2520for%2520the%2520lunar%2520south%2520polar%2520areas%252C%2520including%2520permanently%2520shadowed%2520regions%252C%2520demonstrating%2520the%2520method%2527s%2520capability%2520in%2520reconstructing%2520complex%2520and%2520low-illumination%2520terrain.%2520These%2520findings%2520suggest%2520that%2520DL-based%2520approaches%2520have%2520the%2520potential%2520to%2520leverage%2520extensive%2520lunar%2520datasets%2520to%2520support%2520advanced%2520exploration%2520missions%2520and%2520enable%2520investigations%2520of%2520the%2520Moon%2520at%2520unprecedented%2520topographic%2520resolution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-fidelity%20lunar%20topographic%20reconstruction%20across%20diverse%20terrain%20and%20illumination%20environments%20using%20deep%20learning&entry.906535625=Hao%20Chen%20and%20Philipp%20Gl%C3%A4ser%20and%20Konrad%20Willner%20and%20J%C3%BCrgen%20Oberst&entry.1292438233=Topographic%20models%20are%20essential%20for%20characterizing%20planetary%20surfaces%20and%20for%20inferring%20underlying%20geological%20processes.%20Nevertheless%2C%20meter-scale%20topographic%20data%20remain%20limited%2C%20which%20constrains%20detailed%20planetary%20investigations%2C%20even%20for%20the%20Moon%2C%20where%20extensive%20high-resolution%20orbital%20images%20are%20available.%20Recent%20advances%20in%20deep%20learning%20%28DL%29%20exploit%20single-view%20imagery%2C%20constrained%20by%20low-resolution%20topography%2C%20for%20fast%20and%20flexible%20reconstruction%20of%20fine-scale%20topography.%20However%2C%20their%20robustness%20and%20general%20applicability%20across%20diverse%20lunar%20landforms%20and%20illumination%20conditions%20remain%20insufficiently%20explored.%20In%20this%20study%2C%20we%20build%20upon%20our%20previously%20proposed%20DL%20framework%20by%20incorporating%20a%20more%20robust%20scale%20recovery%20scheme%20and%20extending%20the%20model%20to%20polar%20regions%20under%20low%20solar%20illumination%20conditions.%20We%20demonstrate%20that%2C%20compared%20with%20single-view%20shape-from-shading%20methods%2C%20the%20proposed%20DL%20approach%20exhibits%20greater%20robustness%20to%20varying%20illumination%20and%20achieves%20more%20consistent%20and%20accurate%20topographic%20reconstructions.%20Furthermore%2C%20it%20reliably%20reconstructs%20topography%20across%20lunar%20features%20of%20diverse%20scales%2C%20morphologies%2C%20and%20geological%20ages.%20High-quality%20topographic%20models%20are%20also%20produced%20for%20the%20lunar%20south%20polar%20areas%2C%20including%20permanently%20shadowed%20regions%2C%20demonstrating%20the%20method%27s%20capability%20in%20reconstructing%20complex%20and%20low-illumination%20terrain.%20These%20findings%20suggest%20that%20DL-based%20approaches%20have%20the%20potential%20to%20leverage%20extensive%20lunar%20datasets%20to%20support%20advanced%20exploration%20missions%20and%20enable%20investigations%20of%20the%20Moon%20at%20unprecedented%20topographic%20resolution.&entry.1838667208=http%3A//arxiv.org/abs/2601.09468v1&entry.124074799=Read"},
{"title": "Autofocus Retrieval: An Effective Pipeline for Multi-Hop Question Answering With Semi-Structured Knowledge", "author": "Derian Boer and Stephen Roth and Stefan Kramer", "abstract": "In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. Yet, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data. In this work, we present Autofocus-Retriever (AF-Retriever), a modular framework for SKB-based, multi-hop question answering. It combines structural and textual retrieval through novel integration steps and optimizations, achieving the best zero- and one-shot results across all three STaRK QA benchmarks, which span diverse domains and evaluation metrics.\n  AF-Retriever's average first-hit rate surpasses the second-best method by 32.1%. Its performance is driven by (1) leveraging exchangeable large language models (LLMs) to extract entity attributes and relational constraints for both parsing and reranking the top-k answers, (2) vector similarity search for ranking both extracted entities and final answers, (3) a novel incremental scope expansion procedure that prepares for the reranking on a configurable amount of suitable candidates that fulfill the given constraints the most, and (4) a hybrid retrieval strategy that reduces error susceptibility.\n  In summary, while constantly adjusting the focus like an optical autofocus, AF-Retriever delivers a configurable amount of answer candidates in four constraint-driven retrieval steps, which are then supplemented and ranked through four additional processing steps. An ablation study and a detailed error analysis, including a comparison of three different LLM reranking strategies, provide component-level insights. The source code is available at https://github.com/kramerlab/AF-Retriever.", "link": "http://arxiv.org/abs/2505.09246v3", "date": "2026-01-14", "relevancy": 2.5062, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autofocus%20Retrieval%3A%20An%20Effective%20Pipeline%20for%20Multi-Hop%20Question%20Answering%20With%20Semi-Structured%20Knowledge&body=Title%3A%20Autofocus%20Retrieval%3A%20An%20Effective%20Pipeline%20for%20Multi-Hop%20Question%20Answering%20With%20Semi-Structured%20Knowledge%0AAuthor%3A%20Derian%20Boer%20and%20Stephen%20Roth%20and%20Stefan%20Kramer%0AAbstract%3A%20In%20many%20real-world%20settings%2C%20machine%20learning%20models%20and%20interactive%20systems%20have%20access%20to%20both%20structured%20knowledge%2C%20e.g.%2C%20knowledge%20graphs%20or%20tables%2C%20and%20unstructured%20content%2C%20e.g.%2C%20natural%20language%20documents.%20Yet%2C%20most%20rely%20on%20either.%20Semi-Structured%20Knowledge%20Bases%20%28SKBs%29%20bridge%20this%20gap%20by%20linking%20unstructured%20content%20to%20nodes%20within%20structured%20data.%20In%20this%20work%2C%20we%20present%20Autofocus-Retriever%20%28AF-Retriever%29%2C%20a%20modular%20framework%20for%20SKB-based%2C%20multi-hop%20question%20answering.%20It%20combines%20structural%20and%20textual%20retrieval%20through%20novel%20integration%20steps%20and%20optimizations%2C%20achieving%20the%20best%20zero-%20and%20one-shot%20results%20across%20all%20three%20STaRK%20QA%20benchmarks%2C%20which%20span%20diverse%20domains%20and%20evaluation%20metrics.%0A%20%20AF-Retriever%27s%20average%20first-hit%20rate%20surpasses%20the%20second-best%20method%20by%2032.1%25.%20Its%20performance%20is%20driven%20by%20%281%29%20leveraging%20exchangeable%20large%20language%20models%20%28LLMs%29%20to%20extract%20entity%20attributes%20and%20relational%20constraints%20for%20both%20parsing%20and%20reranking%20the%20top-k%20answers%2C%20%282%29%20vector%20similarity%20search%20for%20ranking%20both%20extracted%20entities%20and%20final%20answers%2C%20%283%29%20a%20novel%20incremental%20scope%20expansion%20procedure%20that%20prepares%20for%20the%20reranking%20on%20a%20configurable%20amount%20of%20suitable%20candidates%20that%20fulfill%20the%20given%20constraints%20the%20most%2C%20and%20%284%29%20a%20hybrid%20retrieval%20strategy%20that%20reduces%20error%20susceptibility.%0A%20%20In%20summary%2C%20while%20constantly%20adjusting%20the%20focus%20like%20an%20optical%20autofocus%2C%20AF-Retriever%20delivers%20a%20configurable%20amount%20of%20answer%20candidates%20in%20four%20constraint-driven%20retrieval%20steps%2C%20which%20are%20then%20supplemented%20and%20ranked%20through%20four%20additional%20processing%20steps.%20An%20ablation%20study%20and%20a%20detailed%20error%20analysis%2C%20including%20a%20comparison%20of%20three%20different%20LLM%20reranking%20strategies%2C%20provide%20component-level%20insights.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/kramerlab/AF-Retriever.%0ALink%3A%20http%3A//arxiv.org/abs/2505.09246v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutofocus%2520Retrieval%253A%2520An%2520Effective%2520Pipeline%2520for%2520Multi-Hop%2520Question%2520Answering%2520With%2520Semi-Structured%2520Knowledge%26entry.906535625%3DDerian%2520Boer%2520and%2520Stephen%2520Roth%2520and%2520Stefan%2520Kramer%26entry.1292438233%3DIn%2520many%2520real-world%2520settings%252C%2520machine%2520learning%2520models%2520and%2520interactive%2520systems%2520have%2520access%2520to%2520both%2520structured%2520knowledge%252C%2520e.g.%252C%2520knowledge%2520graphs%2520or%2520tables%252C%2520and%2520unstructured%2520content%252C%2520e.g.%252C%2520natural%2520language%2520documents.%2520Yet%252C%2520most%2520rely%2520on%2520either.%2520Semi-Structured%2520Knowledge%2520Bases%2520%2528SKBs%2529%2520bridge%2520this%2520gap%2520by%2520linking%2520unstructured%2520content%2520to%2520nodes%2520within%2520structured%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520Autofocus-Retriever%2520%2528AF-Retriever%2529%252C%2520a%2520modular%2520framework%2520for%2520SKB-based%252C%2520multi-hop%2520question%2520answering.%2520It%2520combines%2520structural%2520and%2520textual%2520retrieval%2520through%2520novel%2520integration%2520steps%2520and%2520optimizations%252C%2520achieving%2520the%2520best%2520zero-%2520and%2520one-shot%2520results%2520across%2520all%2520three%2520STaRK%2520QA%2520benchmarks%252C%2520which%2520span%2520diverse%2520domains%2520and%2520evaluation%2520metrics.%250A%2520%2520AF-Retriever%2527s%2520average%2520first-hit%2520rate%2520surpasses%2520the%2520second-best%2520method%2520by%252032.1%2525.%2520Its%2520performance%2520is%2520driven%2520by%2520%25281%2529%2520leveraging%2520exchangeable%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520extract%2520entity%2520attributes%2520and%2520relational%2520constraints%2520for%2520both%2520parsing%2520and%2520reranking%2520the%2520top-k%2520answers%252C%2520%25282%2529%2520vector%2520similarity%2520search%2520for%2520ranking%2520both%2520extracted%2520entities%2520and%2520final%2520answers%252C%2520%25283%2529%2520a%2520novel%2520incremental%2520scope%2520expansion%2520procedure%2520that%2520prepares%2520for%2520the%2520reranking%2520on%2520a%2520configurable%2520amount%2520of%2520suitable%2520candidates%2520that%2520fulfill%2520the%2520given%2520constraints%2520the%2520most%252C%2520and%2520%25284%2529%2520a%2520hybrid%2520retrieval%2520strategy%2520that%2520reduces%2520error%2520susceptibility.%250A%2520%2520In%2520summary%252C%2520while%2520constantly%2520adjusting%2520the%2520focus%2520like%2520an%2520optical%2520autofocus%252C%2520AF-Retriever%2520delivers%2520a%2520configurable%2520amount%2520of%2520answer%2520candidates%2520in%2520four%2520constraint-driven%2520retrieval%2520steps%252C%2520which%2520are%2520then%2520supplemented%2520and%2520ranked%2520through%2520four%2520additional%2520processing%2520steps.%2520An%2520ablation%2520study%2520and%2520a%2520detailed%2520error%2520analysis%252C%2520including%2520a%2520comparison%2520of%2520three%2520different%2520LLM%2520reranking%2520strategies%252C%2520provide%2520component-level%2520insights.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/kramerlab/AF-Retriever.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.09246v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autofocus%20Retrieval%3A%20An%20Effective%20Pipeline%20for%20Multi-Hop%20Question%20Answering%20With%20Semi-Structured%20Knowledge&entry.906535625=Derian%20Boer%20and%20Stephen%20Roth%20and%20Stefan%20Kramer&entry.1292438233=In%20many%20real-world%20settings%2C%20machine%20learning%20models%20and%20interactive%20systems%20have%20access%20to%20both%20structured%20knowledge%2C%20e.g.%2C%20knowledge%20graphs%20or%20tables%2C%20and%20unstructured%20content%2C%20e.g.%2C%20natural%20language%20documents.%20Yet%2C%20most%20rely%20on%20either.%20Semi-Structured%20Knowledge%20Bases%20%28SKBs%29%20bridge%20this%20gap%20by%20linking%20unstructured%20content%20to%20nodes%20within%20structured%20data.%20In%20this%20work%2C%20we%20present%20Autofocus-Retriever%20%28AF-Retriever%29%2C%20a%20modular%20framework%20for%20SKB-based%2C%20multi-hop%20question%20answering.%20It%20combines%20structural%20and%20textual%20retrieval%20through%20novel%20integration%20steps%20and%20optimizations%2C%20achieving%20the%20best%20zero-%20and%20one-shot%20results%20across%20all%20three%20STaRK%20QA%20benchmarks%2C%20which%20span%20diverse%20domains%20and%20evaluation%20metrics.%0A%20%20AF-Retriever%27s%20average%20first-hit%20rate%20surpasses%20the%20second-best%20method%20by%2032.1%25.%20Its%20performance%20is%20driven%20by%20%281%29%20leveraging%20exchangeable%20large%20language%20models%20%28LLMs%29%20to%20extract%20entity%20attributes%20and%20relational%20constraints%20for%20both%20parsing%20and%20reranking%20the%20top-k%20answers%2C%20%282%29%20vector%20similarity%20search%20for%20ranking%20both%20extracted%20entities%20and%20final%20answers%2C%20%283%29%20a%20novel%20incremental%20scope%20expansion%20procedure%20that%20prepares%20for%20the%20reranking%20on%20a%20configurable%20amount%20of%20suitable%20candidates%20that%20fulfill%20the%20given%20constraints%20the%20most%2C%20and%20%284%29%20a%20hybrid%20retrieval%20strategy%20that%20reduces%20error%20susceptibility.%0A%20%20In%20summary%2C%20while%20constantly%20adjusting%20the%20focus%20like%20an%20optical%20autofocus%2C%20AF-Retriever%20delivers%20a%20configurable%20amount%20of%20answer%20candidates%20in%20four%20constraint-driven%20retrieval%20steps%2C%20which%20are%20then%20supplemented%20and%20ranked%20through%20four%20additional%20processing%20steps.%20An%20ablation%20study%20and%20a%20detailed%20error%20analysis%2C%20including%20a%20comparison%20of%20three%20different%20LLM%20reranking%20strategies%2C%20provide%20component-level%20insights.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/kramerlab/AF-Retriever.&entry.1838667208=http%3A//arxiv.org/abs/2505.09246v3&entry.124074799=Read"},
{"title": "Improving Symbolic Translation of Language Models for Logical Reasoning", "author": "Ramya Keerthy Thatikonda and Jiuzhou Han and Wray Buntine and Ehsan Shareghi", "abstract": "The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.", "link": "http://arxiv.org/abs/2601.09446v1", "date": "2026-01-14", "relevancy": 2.4823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Symbolic%20Translation%20of%20Language%20Models%20for%20Logical%20Reasoning&body=Title%3A%20Improving%20Symbolic%20Translation%20of%20Language%20Models%20for%20Logical%20Reasoning%0AAuthor%3A%20Ramya%20Keerthy%20Thatikonda%20and%20Jiuzhou%20Han%20and%20Wray%20Buntine%20and%20Ehsan%20Shareghi%0AAbstract%3A%20The%20use%20of%20formal%20language%20for%20deductive%20logical%20reasoning%20aligns%20well%20with%20language%20models%20%28LMs%29%2C%20where%20translating%20natural%20language%20%28NL%29%20into%20first-order%20logic%20%28FOL%29%20and%20employing%20an%20external%20solver%20results%20in%20a%20verifiable%20and%20therefore%20reliable%20reasoning%20system.%20However%2C%20smaller%20LMs%20often%20struggle%20with%20this%20translation%20task%2C%20frequently%20producing%20incorrect%20symbolic%20outputs%20due%20to%20formatting%20and%20translation%20errors.%20Existing%20approaches%20typically%20rely%20on%20self-iteration%20to%20correct%20these%20errors%2C%20but%20such%20methods%20depend%20heavily%20on%20the%20capabilities%20of%20the%20underlying%20model.%20To%20address%20this%2C%20we%20first%20categorize%20common%20errors%20and%20fine-tune%20smaller%20LMs%20using%20data%20synthesized%20by%20large%20language%20models.%20The%20evaluation%20is%20performed%20using%20the%20defined%20error%20categories.%20We%20introduce%20incremental%20inference%2C%20which%20divides%20inference%20into%20two%20stages%2C%20predicate%20generation%20and%20FOL%20translation%2C%20providing%20greater%20control%20over%20model%20behavior%20and%20enhancing%20generation%20quality%20as%20measured%20by%20predicate%20metrics.%20This%20decomposition%20framework%20also%20enables%20the%20use%20of%20a%20verification%20module%20that%20targets%20predicate-arity%20errors%20to%20further%20improve%20performance.%20Our%20study%20evaluates%20three%20families%20of%20models%20across%20four%20logical-reasoning%20datasets.%20The%20comprehensive%20fine-tuning%2C%20incremental%20inference%2C%20and%20verification%20modules%20reduce%20error%20rates%2C%20increase%20predicate%20coverage%2C%20and%20improve%20reasoning%20performance%20for%20smaller%20LMs%2C%20moving%20us%20closer%20to%20developing%20reliable%20and%20accessible%20symbolic-reasoning%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Symbolic%2520Translation%2520of%2520Language%2520Models%2520for%2520Logical%2520Reasoning%26entry.906535625%3DRamya%2520Keerthy%2520Thatikonda%2520and%2520Jiuzhou%2520Han%2520and%2520Wray%2520Buntine%2520and%2520Ehsan%2520Shareghi%26entry.1292438233%3DThe%2520use%2520of%2520formal%2520language%2520for%2520deductive%2520logical%2520reasoning%2520aligns%2520well%2520with%2520language%2520models%2520%2528LMs%2529%252C%2520where%2520translating%2520natural%2520language%2520%2528NL%2529%2520into%2520first-order%2520logic%2520%2528FOL%2529%2520and%2520employing%2520an%2520external%2520solver%2520results%2520in%2520a%2520verifiable%2520and%2520therefore%2520reliable%2520reasoning%2520system.%2520However%252C%2520smaller%2520LMs%2520often%2520struggle%2520with%2520this%2520translation%2520task%252C%2520frequently%2520producing%2520incorrect%2520symbolic%2520outputs%2520due%2520to%2520formatting%2520and%2520translation%2520errors.%2520Existing%2520approaches%2520typically%2520rely%2520on%2520self-iteration%2520to%2520correct%2520these%2520errors%252C%2520but%2520such%2520methods%2520depend%2520heavily%2520on%2520the%2520capabilities%2520of%2520the%2520underlying%2520model.%2520To%2520address%2520this%252C%2520we%2520first%2520categorize%2520common%2520errors%2520and%2520fine-tune%2520smaller%2520LMs%2520using%2520data%2520synthesized%2520by%2520large%2520language%2520models.%2520The%2520evaluation%2520is%2520performed%2520using%2520the%2520defined%2520error%2520categories.%2520We%2520introduce%2520incremental%2520inference%252C%2520which%2520divides%2520inference%2520into%2520two%2520stages%252C%2520predicate%2520generation%2520and%2520FOL%2520translation%252C%2520providing%2520greater%2520control%2520over%2520model%2520behavior%2520and%2520enhancing%2520generation%2520quality%2520as%2520measured%2520by%2520predicate%2520metrics.%2520This%2520decomposition%2520framework%2520also%2520enables%2520the%2520use%2520of%2520a%2520verification%2520module%2520that%2520targets%2520predicate-arity%2520errors%2520to%2520further%2520improve%2520performance.%2520Our%2520study%2520evaluates%2520three%2520families%2520of%2520models%2520across%2520four%2520logical-reasoning%2520datasets.%2520The%2520comprehensive%2520fine-tuning%252C%2520incremental%2520inference%252C%2520and%2520verification%2520modules%2520reduce%2520error%2520rates%252C%2520increase%2520predicate%2520coverage%252C%2520and%2520improve%2520reasoning%2520performance%2520for%2520smaller%2520LMs%252C%2520moving%2520us%2520closer%2520to%2520developing%2520reliable%2520and%2520accessible%2520symbolic-reasoning%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Symbolic%20Translation%20of%20Language%20Models%20for%20Logical%20Reasoning&entry.906535625=Ramya%20Keerthy%20Thatikonda%20and%20Jiuzhou%20Han%20and%20Wray%20Buntine%20and%20Ehsan%20Shareghi&entry.1292438233=The%20use%20of%20formal%20language%20for%20deductive%20logical%20reasoning%20aligns%20well%20with%20language%20models%20%28LMs%29%2C%20where%20translating%20natural%20language%20%28NL%29%20into%20first-order%20logic%20%28FOL%29%20and%20employing%20an%20external%20solver%20results%20in%20a%20verifiable%20and%20therefore%20reliable%20reasoning%20system.%20However%2C%20smaller%20LMs%20often%20struggle%20with%20this%20translation%20task%2C%20frequently%20producing%20incorrect%20symbolic%20outputs%20due%20to%20formatting%20and%20translation%20errors.%20Existing%20approaches%20typically%20rely%20on%20self-iteration%20to%20correct%20these%20errors%2C%20but%20such%20methods%20depend%20heavily%20on%20the%20capabilities%20of%20the%20underlying%20model.%20To%20address%20this%2C%20we%20first%20categorize%20common%20errors%20and%20fine-tune%20smaller%20LMs%20using%20data%20synthesized%20by%20large%20language%20models.%20The%20evaluation%20is%20performed%20using%20the%20defined%20error%20categories.%20We%20introduce%20incremental%20inference%2C%20which%20divides%20inference%20into%20two%20stages%2C%20predicate%20generation%20and%20FOL%20translation%2C%20providing%20greater%20control%20over%20model%20behavior%20and%20enhancing%20generation%20quality%20as%20measured%20by%20predicate%20metrics.%20This%20decomposition%20framework%20also%20enables%20the%20use%20of%20a%20verification%20module%20that%20targets%20predicate-arity%20errors%20to%20further%20improve%20performance.%20Our%20study%20evaluates%20three%20families%20of%20models%20across%20four%20logical-reasoning%20datasets.%20The%20comprehensive%20fine-tuning%2C%20incremental%20inference%2C%20and%20verification%20modules%20reduce%20error%20rates%2C%20increase%20predicate%20coverage%2C%20and%20improve%20reasoning%20performance%20for%20smaller%20LMs%2C%20moving%20us%20closer%20to%20developing%20reliable%20and%20accessible%20symbolic-reasoning%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.09446v1&entry.124074799=Read"},
{"title": "Exploring Fine-Tuning for Tabular Foundation Models", "author": "Aditya Tanna and Pratinav Seth and Mohamed Bouadi and Vinay Kumar Sankarapu", "abstract": "Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.", "link": "http://arxiv.org/abs/2601.09654v1", "date": "2026-01-14", "relevancy": 2.4812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Fine-Tuning%20for%20Tabular%20Foundation%20Models&body=Title%3A%20Exploring%20Fine-Tuning%20for%20Tabular%20Foundation%20Models%0AAuthor%3A%20Aditya%20Tanna%20and%20Pratinav%20Seth%20and%20Mohamed%20Bouadi%20and%20Vinay%20Kumar%20Sankarapu%0AAbstract%3A%20Tabular%20Foundation%20Models%20%28TFMs%29%20have%20recently%20shown%20strong%20in-context%20learning%20capabilities%20on%20structured%20data%2C%20achieving%20zero-shot%20performance%20comparable%20to%20traditional%20machine%20learning%20methods.%20We%20find%20that%20zero-shot%20TFMs%20already%20achieve%20strong%20performance%2C%20while%20the%20benefits%20of%20fine-tuning%20are%20highly%20model%20and%20data-dependent.%20Meta-learning%20and%20PEFT%20provide%20moderate%20gains%20under%20specific%20conditions%2C%20whereas%20full%20supervised%20fine-tuning%20%28SFT%29%20often%20reduces%20accuracy%20or%20calibration%20quality.%20This%20work%20presents%20the%20first%20comprehensive%20study%20of%20fine-tuning%20in%20TFMs%20across%20benchmarks%20including%20TALENT%2C%20OpenML-CC18%2C%20and%20TabZilla.%20We%20compare%20Zero-Shot%2C%20Meta-Learning%2C%20Supervised%20%28SFT%29%2C%20and%20parameter-efficient%20%28PEFT%29%20approaches%2C%20analyzing%20how%20dataset%20factors%20such%20as%20imbalance%2C%20size%2C%20and%20dimensionality%20affect%20outcomes.%20Our%20findings%20cover%20performance%2C%20calibration%2C%20and%20fairness%2C%20offering%20practical%20guidelines%20on%20when%20fine-tuning%20is%20most%20beneficial%20and%20its%20limitations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Fine-Tuning%2520for%2520Tabular%2520Foundation%2520Models%26entry.906535625%3DAditya%2520Tanna%2520and%2520Pratinav%2520Seth%2520and%2520Mohamed%2520Bouadi%2520and%2520Vinay%2520Kumar%2520Sankarapu%26entry.1292438233%3DTabular%2520Foundation%2520Models%2520%2528TFMs%2529%2520have%2520recently%2520shown%2520strong%2520in-context%2520learning%2520capabilities%2520on%2520structured%2520data%252C%2520achieving%2520zero-shot%2520performance%2520comparable%2520to%2520traditional%2520machine%2520learning%2520methods.%2520We%2520find%2520that%2520zero-shot%2520TFMs%2520already%2520achieve%2520strong%2520performance%252C%2520while%2520the%2520benefits%2520of%2520fine-tuning%2520are%2520highly%2520model%2520and%2520data-dependent.%2520Meta-learning%2520and%2520PEFT%2520provide%2520moderate%2520gains%2520under%2520specific%2520conditions%252C%2520whereas%2520full%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520often%2520reduces%2520accuracy%2520or%2520calibration%2520quality.%2520This%2520work%2520presents%2520the%2520first%2520comprehensive%2520study%2520of%2520fine-tuning%2520in%2520TFMs%2520across%2520benchmarks%2520including%2520TALENT%252C%2520OpenML-CC18%252C%2520and%2520TabZilla.%2520We%2520compare%2520Zero-Shot%252C%2520Meta-Learning%252C%2520Supervised%2520%2528SFT%2529%252C%2520and%2520parameter-efficient%2520%2528PEFT%2529%2520approaches%252C%2520analyzing%2520how%2520dataset%2520factors%2520such%2520as%2520imbalance%252C%2520size%252C%2520and%2520dimensionality%2520affect%2520outcomes.%2520Our%2520findings%2520cover%2520performance%252C%2520calibration%252C%2520and%2520fairness%252C%2520offering%2520practical%2520guidelines%2520on%2520when%2520fine-tuning%2520is%2520most%2520beneficial%2520and%2520its%2520limitations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Fine-Tuning%20for%20Tabular%20Foundation%20Models&entry.906535625=Aditya%20Tanna%20and%20Pratinav%20Seth%20and%20Mohamed%20Bouadi%20and%20Vinay%20Kumar%20Sankarapu&entry.1292438233=Tabular%20Foundation%20Models%20%28TFMs%29%20have%20recently%20shown%20strong%20in-context%20learning%20capabilities%20on%20structured%20data%2C%20achieving%20zero-shot%20performance%20comparable%20to%20traditional%20machine%20learning%20methods.%20We%20find%20that%20zero-shot%20TFMs%20already%20achieve%20strong%20performance%2C%20while%20the%20benefits%20of%20fine-tuning%20are%20highly%20model%20and%20data-dependent.%20Meta-learning%20and%20PEFT%20provide%20moderate%20gains%20under%20specific%20conditions%2C%20whereas%20full%20supervised%20fine-tuning%20%28SFT%29%20often%20reduces%20accuracy%20or%20calibration%20quality.%20This%20work%20presents%20the%20first%20comprehensive%20study%20of%20fine-tuning%20in%20TFMs%20across%20benchmarks%20including%20TALENT%2C%20OpenML-CC18%2C%20and%20TabZilla.%20We%20compare%20Zero-Shot%2C%20Meta-Learning%2C%20Supervised%20%28SFT%29%2C%20and%20parameter-efficient%20%28PEFT%29%20approaches%2C%20analyzing%20how%20dataset%20factors%20such%20as%20imbalance%2C%20size%2C%20and%20dimensionality%20affect%20outcomes.%20Our%20findings%20cover%20performance%2C%20calibration%2C%20and%20fairness%2C%20offering%20practical%20guidelines%20on%20when%20fine-tuning%20is%20most%20beneficial%20and%20its%20limitations.&entry.1838667208=http%3A//arxiv.org/abs/2601.09654v1&entry.124074799=Read"},
{"title": "SGAC: A Graph Neural Network Framework for Imbalanced and Structure-Aware AMP Classification", "author": "Yingxu Wang and Victor Liang and Nan Yin and Siwei Liu and Eran Segal", "abstract": "Classifying Antimicrobial Peptides (AMPs) from the vast collection of peptides derived from metagenomic sequencing offers a promising avenue for combating antibiotic resistance. However, most existing AMP classification methods rely primarily on sequence-based representations and fail to capture the spatial structural information critical for accurate identification. Although recent graph-based approaches attempt to incorporate structural information, they typically construct residue- or atom-level graphs that introduce redundant atomic details and increase structural complexity. Furthermore, the class imbalance between the small number of known AMPs and the abundant non-AMPs significantly hinders predictive performance. To address these challenges, we employ lightweight OmegaFold to predict the three-dimensional structures of peptides and construct peptide graphs using C \u03b1 atoms to capture their backbone geometry and spatial topology. Building on this representation, we propose the Spatial GNN-based AMP Classifier (SGAC), a novel framework that leverages Graph Neural Networks (GNNs) to extract structural features and generate discriminative graph representations. To handle class imbalance, SGAC incorporates Weight-enhanced Contrastive Learning to cluster structurally similar peptides and separate dissimilar ones through adaptive weighting, and applies Weight-enhanced Pseudo-label Distillation to generate high-confidence pseudo labels for unlabeled samples, achieving balanced and consistent representation learning. Experiments on publicly available AMP and non-AMP datasets demonstrate that SGAC significantly achieves state-of-the-art performance compared to baselines.", "link": "http://arxiv.org/abs/2412.16276v2", "date": "2026-01-14", "relevancy": 2.4805, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SGAC%3A%20A%20Graph%20Neural%20Network%20Framework%20for%20Imbalanced%20and%20Structure-Aware%20AMP%20Classification&body=Title%3A%20SGAC%3A%20A%20Graph%20Neural%20Network%20Framework%20for%20Imbalanced%20and%20Structure-Aware%20AMP%20Classification%0AAuthor%3A%20Yingxu%20Wang%20and%20Victor%20Liang%20and%20Nan%20Yin%20and%20Siwei%20Liu%20and%20Eran%20Segal%0AAbstract%3A%20Classifying%20Antimicrobial%20Peptides%20%28AMPs%29%20from%20the%20vast%20collection%20of%20peptides%20derived%20from%20metagenomic%20sequencing%20offers%20a%20promising%20avenue%20for%20combating%20antibiotic%20resistance.%20However%2C%20most%20existing%20AMP%20classification%20methods%20rely%20primarily%20on%20sequence-based%20representations%20and%20fail%20to%20capture%20the%20spatial%20structural%20information%20critical%20for%20accurate%20identification.%20Although%20recent%20graph-based%20approaches%20attempt%20to%20incorporate%20structural%20information%2C%20they%20typically%20construct%20residue-%20or%20atom-level%20graphs%20that%20introduce%20redundant%20atomic%20details%20and%20increase%20structural%20complexity.%20Furthermore%2C%20the%20class%20imbalance%20between%20the%20small%20number%20of%20known%20AMPs%20and%20the%20abundant%20non-AMPs%20significantly%20hinders%20predictive%20performance.%20To%20address%20these%20challenges%2C%20we%20employ%20lightweight%20OmegaFold%20to%20predict%20the%20three-dimensional%20structures%20of%20peptides%20and%20construct%20peptide%20graphs%20using%20C%20%CE%B1%20atoms%20to%20capture%20their%20backbone%20geometry%20and%20spatial%20topology.%20Building%20on%20this%20representation%2C%20we%20propose%20the%20Spatial%20GNN-based%20AMP%20Classifier%20%28SGAC%29%2C%20a%20novel%20framework%20that%20leverages%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20extract%20structural%20features%20and%20generate%20discriminative%20graph%20representations.%20To%20handle%20class%20imbalance%2C%20SGAC%20incorporates%20Weight-enhanced%20Contrastive%20Learning%20to%20cluster%20structurally%20similar%20peptides%20and%20separate%20dissimilar%20ones%20through%20adaptive%20weighting%2C%20and%20applies%20Weight-enhanced%20Pseudo-label%20Distillation%20to%20generate%20high-confidence%20pseudo%20labels%20for%20unlabeled%20samples%2C%20achieving%20balanced%20and%20consistent%20representation%20learning.%20Experiments%20on%20publicly%20available%20AMP%20and%20non-AMP%20datasets%20demonstrate%20that%20SGAC%20significantly%20achieves%20state-of-the-art%20performance%20compared%20to%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2412.16276v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSGAC%253A%2520A%2520Graph%2520Neural%2520Network%2520Framework%2520for%2520Imbalanced%2520and%2520Structure-Aware%2520AMP%2520Classification%26entry.906535625%3DYingxu%2520Wang%2520and%2520Victor%2520Liang%2520and%2520Nan%2520Yin%2520and%2520Siwei%2520Liu%2520and%2520Eran%2520Segal%26entry.1292438233%3DClassifying%2520Antimicrobial%2520Peptides%2520%2528AMPs%2529%2520from%2520the%2520vast%2520collection%2520of%2520peptides%2520derived%2520from%2520metagenomic%2520sequencing%2520offers%2520a%2520promising%2520avenue%2520for%2520combating%2520antibiotic%2520resistance.%2520However%252C%2520most%2520existing%2520AMP%2520classification%2520methods%2520rely%2520primarily%2520on%2520sequence-based%2520representations%2520and%2520fail%2520to%2520capture%2520the%2520spatial%2520structural%2520information%2520critical%2520for%2520accurate%2520identification.%2520Although%2520recent%2520graph-based%2520approaches%2520attempt%2520to%2520incorporate%2520structural%2520information%252C%2520they%2520typically%2520construct%2520residue-%2520or%2520atom-level%2520graphs%2520that%2520introduce%2520redundant%2520atomic%2520details%2520and%2520increase%2520structural%2520complexity.%2520Furthermore%252C%2520the%2520class%2520imbalance%2520between%2520the%2520small%2520number%2520of%2520known%2520AMPs%2520and%2520the%2520abundant%2520non-AMPs%2520significantly%2520hinders%2520predictive%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520employ%2520lightweight%2520OmegaFold%2520to%2520predict%2520the%2520three-dimensional%2520structures%2520of%2520peptides%2520and%2520construct%2520peptide%2520graphs%2520using%2520C%2520%25CE%25B1%2520atoms%2520to%2520capture%2520their%2520backbone%2520geometry%2520and%2520spatial%2520topology.%2520Building%2520on%2520this%2520representation%252C%2520we%2520propose%2520the%2520Spatial%2520GNN-based%2520AMP%2520Classifier%2520%2528SGAC%2529%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520extract%2520structural%2520features%2520and%2520generate%2520discriminative%2520graph%2520representations.%2520To%2520handle%2520class%2520imbalance%252C%2520SGAC%2520incorporates%2520Weight-enhanced%2520Contrastive%2520Learning%2520to%2520cluster%2520structurally%2520similar%2520peptides%2520and%2520separate%2520dissimilar%2520ones%2520through%2520adaptive%2520weighting%252C%2520and%2520applies%2520Weight-enhanced%2520Pseudo-label%2520Distillation%2520to%2520generate%2520high-confidence%2520pseudo%2520labels%2520for%2520unlabeled%2520samples%252C%2520achieving%2520balanced%2520and%2520consistent%2520representation%2520learning.%2520Experiments%2520on%2520publicly%2520available%2520AMP%2520and%2520non-AMP%2520datasets%2520demonstrate%2520that%2520SGAC%2520significantly%2520achieves%2520state-of-the-art%2520performance%2520compared%2520to%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16276v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SGAC%3A%20A%20Graph%20Neural%20Network%20Framework%20for%20Imbalanced%20and%20Structure-Aware%20AMP%20Classification&entry.906535625=Yingxu%20Wang%20and%20Victor%20Liang%20and%20Nan%20Yin%20and%20Siwei%20Liu%20and%20Eran%20Segal&entry.1292438233=Classifying%20Antimicrobial%20Peptides%20%28AMPs%29%20from%20the%20vast%20collection%20of%20peptides%20derived%20from%20metagenomic%20sequencing%20offers%20a%20promising%20avenue%20for%20combating%20antibiotic%20resistance.%20However%2C%20most%20existing%20AMP%20classification%20methods%20rely%20primarily%20on%20sequence-based%20representations%20and%20fail%20to%20capture%20the%20spatial%20structural%20information%20critical%20for%20accurate%20identification.%20Although%20recent%20graph-based%20approaches%20attempt%20to%20incorporate%20structural%20information%2C%20they%20typically%20construct%20residue-%20or%20atom-level%20graphs%20that%20introduce%20redundant%20atomic%20details%20and%20increase%20structural%20complexity.%20Furthermore%2C%20the%20class%20imbalance%20between%20the%20small%20number%20of%20known%20AMPs%20and%20the%20abundant%20non-AMPs%20significantly%20hinders%20predictive%20performance.%20To%20address%20these%20challenges%2C%20we%20employ%20lightweight%20OmegaFold%20to%20predict%20the%20three-dimensional%20structures%20of%20peptides%20and%20construct%20peptide%20graphs%20using%20C%20%CE%B1%20atoms%20to%20capture%20their%20backbone%20geometry%20and%20spatial%20topology.%20Building%20on%20this%20representation%2C%20we%20propose%20the%20Spatial%20GNN-based%20AMP%20Classifier%20%28SGAC%29%2C%20a%20novel%20framework%20that%20leverages%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20extract%20structural%20features%20and%20generate%20discriminative%20graph%20representations.%20To%20handle%20class%20imbalance%2C%20SGAC%20incorporates%20Weight-enhanced%20Contrastive%20Learning%20to%20cluster%20structurally%20similar%20peptides%20and%20separate%20dissimilar%20ones%20through%20adaptive%20weighting%2C%20and%20applies%20Weight-enhanced%20Pseudo-label%20Distillation%20to%20generate%20high-confidence%20pseudo%20labels%20for%20unlabeled%20samples%2C%20achieving%20balanced%20and%20consistent%20representation%20learning.%20Experiments%20on%20publicly%20available%20AMP%20and%20non-AMP%20datasets%20demonstrate%20that%20SGAC%20significantly%20achieves%20state-of-the-art%20performance%20compared%20to%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2412.16276v2&entry.124074799=Read"},
{"title": "Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception", "author": "Zhen Wan and Chao-Han Huck Yang and Jinchuan Tian and Hanrong Ye and Ankita Pasad and Szu-wei Fu and Arushi Goel and Ryo Hachiuma and Shizhe Diao and Kunal Dhawan and Sreyan Ghosh and Yusuke Hirota and Zhehuai Chen and Rafael Valle and Ehsan Hosseini Asl and Chenhui Chu and Shinji Watanabe and Yu-Chiang Frank Wang and Boris Ginsburg", "abstract": "We introduce a voice-agentic framework that learns one critical omni-understanding skill: knowing when to trust itself versus when to consult external audio perception. Our work is motivated by a crucial yet counterintuitive finding: naively fine-tuning an omni-model on both speech recognition and external sound understanding tasks often degrades performance, as the model can be easily misled by noisy hypotheses. To address this, our framework, Speech-Hands, recasts the problem as an explicit self-reflection decision. This learnable reflection primitive proves effective in preventing the model from being derailed by flawed external candidates. We show that this agentic action mechanism generalizes naturally from speech recognition to complex, multiple-choice audio reasoning. Across the OpenASR leaderboard, Speech-Hands consistently outperforms strong baselines by 12.1% WER on seven benchmarks. The model also achieves 77.37% accuracy and high F1 on audio QA decisions, showing robust generalization and reliability across diverse audio question answering datasets. By unifying perception and decision-making, our work offers a practical path toward more reliable and resilient audio intelligence.", "link": "http://arxiv.org/abs/2601.09413v1", "date": "2026-01-14", "relevancy": 2.4744, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4959}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Speech-Hands%3A%20A%20Self-Reflection%20Voice%20Agentic%20Approach%20to%20Speech%20Recognition%20and%20Audio%20Reasoning%20with%20Omni%20Perception&body=Title%3A%20Speech-Hands%3A%20A%20Self-Reflection%20Voice%20Agentic%20Approach%20to%20Speech%20Recognition%20and%20Audio%20Reasoning%20with%20Omni%20Perception%0AAuthor%3A%20Zhen%20Wan%20and%20Chao-Han%20Huck%20Yang%20and%20Jinchuan%20Tian%20and%20Hanrong%20Ye%20and%20Ankita%20Pasad%20and%20Szu-wei%20Fu%20and%20Arushi%20Goel%20and%20Ryo%20Hachiuma%20and%20Shizhe%20Diao%20and%20Kunal%20Dhawan%20and%20Sreyan%20Ghosh%20and%20Yusuke%20Hirota%20and%20Zhehuai%20Chen%20and%20Rafael%20Valle%20and%20Ehsan%20Hosseini%20Asl%20and%20Chenhui%20Chu%20and%20Shinji%20Watanabe%20and%20Yu-Chiang%20Frank%20Wang%20and%20Boris%20Ginsburg%0AAbstract%3A%20We%20introduce%20a%20voice-agentic%20framework%20that%20learns%20one%20critical%20omni-understanding%20skill%3A%20knowing%20when%20to%20trust%20itself%20versus%20when%20to%20consult%20external%20audio%20perception.%20Our%20work%20is%20motivated%20by%20a%20crucial%20yet%20counterintuitive%20finding%3A%20naively%20fine-tuning%20an%20omni-model%20on%20both%20speech%20recognition%20and%20external%20sound%20understanding%20tasks%20often%20degrades%20performance%2C%20as%20the%20model%20can%20be%20easily%20misled%20by%20noisy%20hypotheses.%20To%20address%20this%2C%20our%20framework%2C%20Speech-Hands%2C%20recasts%20the%20problem%20as%20an%20explicit%20self-reflection%20decision.%20This%20learnable%20reflection%20primitive%20proves%20effective%20in%20preventing%20the%20model%20from%20being%20derailed%20by%20flawed%20external%20candidates.%20We%20show%20that%20this%20agentic%20action%20mechanism%20generalizes%20naturally%20from%20speech%20recognition%20to%20complex%2C%20multiple-choice%20audio%20reasoning.%20Across%20the%20OpenASR%20leaderboard%2C%20Speech-Hands%20consistently%20outperforms%20strong%20baselines%20by%2012.1%25%20WER%20on%20seven%20benchmarks.%20The%20model%20also%20achieves%2077.37%25%20accuracy%20and%20high%20F1%20on%20audio%20QA%20decisions%2C%20showing%20robust%20generalization%20and%20reliability%20across%20diverse%20audio%20question%20answering%20datasets.%20By%20unifying%20perception%20and%20decision-making%2C%20our%20work%20offers%20a%20practical%20path%20toward%20more%20reliable%20and%20resilient%20audio%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpeech-Hands%253A%2520A%2520Self-Reflection%2520Voice%2520Agentic%2520Approach%2520to%2520Speech%2520Recognition%2520and%2520Audio%2520Reasoning%2520with%2520Omni%2520Perception%26entry.906535625%3DZhen%2520Wan%2520and%2520Chao-Han%2520Huck%2520Yang%2520and%2520Jinchuan%2520Tian%2520and%2520Hanrong%2520Ye%2520and%2520Ankita%2520Pasad%2520and%2520Szu-wei%2520Fu%2520and%2520Arushi%2520Goel%2520and%2520Ryo%2520Hachiuma%2520and%2520Shizhe%2520Diao%2520and%2520Kunal%2520Dhawan%2520and%2520Sreyan%2520Ghosh%2520and%2520Yusuke%2520Hirota%2520and%2520Zhehuai%2520Chen%2520and%2520Rafael%2520Valle%2520and%2520Ehsan%2520Hosseini%2520Asl%2520and%2520Chenhui%2520Chu%2520and%2520Shinji%2520Watanabe%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Boris%2520Ginsburg%26entry.1292438233%3DWe%2520introduce%2520a%2520voice-agentic%2520framework%2520that%2520learns%2520one%2520critical%2520omni-understanding%2520skill%253A%2520knowing%2520when%2520to%2520trust%2520itself%2520versus%2520when%2520to%2520consult%2520external%2520audio%2520perception.%2520Our%2520work%2520is%2520motivated%2520by%2520a%2520crucial%2520yet%2520counterintuitive%2520finding%253A%2520naively%2520fine-tuning%2520an%2520omni-model%2520on%2520both%2520speech%2520recognition%2520and%2520external%2520sound%2520understanding%2520tasks%2520often%2520degrades%2520performance%252C%2520as%2520the%2520model%2520can%2520be%2520easily%2520misled%2520by%2520noisy%2520hypotheses.%2520To%2520address%2520this%252C%2520our%2520framework%252C%2520Speech-Hands%252C%2520recasts%2520the%2520problem%2520as%2520an%2520explicit%2520self-reflection%2520decision.%2520This%2520learnable%2520reflection%2520primitive%2520proves%2520effective%2520in%2520preventing%2520the%2520model%2520from%2520being%2520derailed%2520by%2520flawed%2520external%2520candidates.%2520We%2520show%2520that%2520this%2520agentic%2520action%2520mechanism%2520generalizes%2520naturally%2520from%2520speech%2520recognition%2520to%2520complex%252C%2520multiple-choice%2520audio%2520reasoning.%2520Across%2520the%2520OpenASR%2520leaderboard%252C%2520Speech-Hands%2520consistently%2520outperforms%2520strong%2520baselines%2520by%252012.1%2525%2520WER%2520on%2520seven%2520benchmarks.%2520The%2520model%2520also%2520achieves%252077.37%2525%2520accuracy%2520and%2520high%2520F1%2520on%2520audio%2520QA%2520decisions%252C%2520showing%2520robust%2520generalization%2520and%2520reliability%2520across%2520diverse%2520audio%2520question%2520answering%2520datasets.%2520By%2520unifying%2520perception%2520and%2520decision-making%252C%2520our%2520work%2520offers%2520a%2520practical%2520path%2520toward%2520more%2520reliable%2520and%2520resilient%2520audio%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Speech-Hands%3A%20A%20Self-Reflection%20Voice%20Agentic%20Approach%20to%20Speech%20Recognition%20and%20Audio%20Reasoning%20with%20Omni%20Perception&entry.906535625=Zhen%20Wan%20and%20Chao-Han%20Huck%20Yang%20and%20Jinchuan%20Tian%20and%20Hanrong%20Ye%20and%20Ankita%20Pasad%20and%20Szu-wei%20Fu%20and%20Arushi%20Goel%20and%20Ryo%20Hachiuma%20and%20Shizhe%20Diao%20and%20Kunal%20Dhawan%20and%20Sreyan%20Ghosh%20and%20Yusuke%20Hirota%20and%20Zhehuai%20Chen%20and%20Rafael%20Valle%20and%20Ehsan%20Hosseini%20Asl%20and%20Chenhui%20Chu%20and%20Shinji%20Watanabe%20and%20Yu-Chiang%20Frank%20Wang%20and%20Boris%20Ginsburg&entry.1292438233=We%20introduce%20a%20voice-agentic%20framework%20that%20learns%20one%20critical%20omni-understanding%20skill%3A%20knowing%20when%20to%20trust%20itself%20versus%20when%20to%20consult%20external%20audio%20perception.%20Our%20work%20is%20motivated%20by%20a%20crucial%20yet%20counterintuitive%20finding%3A%20naively%20fine-tuning%20an%20omni-model%20on%20both%20speech%20recognition%20and%20external%20sound%20understanding%20tasks%20often%20degrades%20performance%2C%20as%20the%20model%20can%20be%20easily%20misled%20by%20noisy%20hypotheses.%20To%20address%20this%2C%20our%20framework%2C%20Speech-Hands%2C%20recasts%20the%20problem%20as%20an%20explicit%20self-reflection%20decision.%20This%20learnable%20reflection%20primitive%20proves%20effective%20in%20preventing%20the%20model%20from%20being%20derailed%20by%20flawed%20external%20candidates.%20We%20show%20that%20this%20agentic%20action%20mechanism%20generalizes%20naturally%20from%20speech%20recognition%20to%20complex%2C%20multiple-choice%20audio%20reasoning.%20Across%20the%20OpenASR%20leaderboard%2C%20Speech-Hands%20consistently%20outperforms%20strong%20baselines%20by%2012.1%25%20WER%20on%20seven%20benchmarks.%20The%20model%20also%20achieves%2077.37%25%20accuracy%20and%20high%20F1%20on%20audio%20QA%20decisions%2C%20showing%20robust%20generalization%20and%20reliability%20across%20diverse%20audio%20question%20answering%20datasets.%20By%20unifying%20perception%20and%20decision-making%2C%20our%20work%20offers%20a%20practical%20path%20toward%20more%20reliable%20and%20resilient%20audio%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2601.09413v1&entry.124074799=Read"},
{"title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "author": "Xinzhi Zhang and Zeyi Chen and Humishka Zope and Hugo Barbalho and Konstantina Mellou and Marco Molinaro and Janardhan Kulkarni and Ishai Menache and Sirui Li", "abstract": "Mathematical programming -- the task of expressing operations and decision-making problems in precise mathematical language -- is fundamental across domains, yet remains a skill-intensive process requiring operations research expertise. Recent advances in large language models for complex reasoning have spurred interest in automating this task, translating natural language into executable optimization models. Current approaches, however, achieve limited accuracy, hindered by scarce and noisy training data without leveraging domain knowledge. In this work, we systematically integrate optimization expertise to improve formulation accuracy for mixed-integer linear programming, a key family of mathematical programs. Our OptiMind framework leverages semi-automated, class-based error analysis to guide both training and inference, explicitly preventing common mistakes within each optimization class. Our resulting fine-tuned LLM significantly improves formulation accuracy by 20.7% across multiple optimization benchmarks, with consistent gains under test-time scaling methods such as self-consistency and multi-turn feedback, enabling further progress toward robust LLM-assisted optimization formulation.", "link": "http://arxiv.org/abs/2509.22979v2", "date": "2026-01-14", "relevancy": 2.4386, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OptiMind%3A%20Teaching%20LLMs%20to%20Think%20Like%20Optimization%20Experts&body=Title%3A%20OptiMind%3A%20Teaching%20LLMs%20to%20Think%20Like%20Optimization%20Experts%0AAuthor%3A%20Xinzhi%20Zhang%20and%20Zeyi%20Chen%20and%20Humishka%20Zope%20and%20Hugo%20Barbalho%20and%20Konstantina%20Mellou%20and%20Marco%20Molinaro%20and%20Janardhan%20Kulkarni%20and%20Ishai%20Menache%20and%20Sirui%20Li%0AAbstract%3A%20Mathematical%20programming%20--%20the%20task%20of%20expressing%20operations%20and%20decision-making%20problems%20in%20precise%20mathematical%20language%20--%20is%20fundamental%20across%20domains%2C%20yet%20remains%20a%20skill-intensive%20process%20requiring%20operations%20research%20expertise.%20Recent%20advances%20in%20large%20language%20models%20for%20complex%20reasoning%20have%20spurred%20interest%20in%20automating%20this%20task%2C%20translating%20natural%20language%20into%20executable%20optimization%20models.%20Current%20approaches%2C%20however%2C%20achieve%20limited%20accuracy%2C%20hindered%20by%20scarce%20and%20noisy%20training%20data%20without%20leveraging%20domain%20knowledge.%20In%20this%20work%2C%20we%20systematically%20integrate%20optimization%20expertise%20to%20improve%20formulation%20accuracy%20for%20mixed-integer%20linear%20programming%2C%20a%20key%20family%20of%20mathematical%20programs.%20Our%20OptiMind%20framework%20leverages%20semi-automated%2C%20class-based%20error%20analysis%20to%20guide%20both%20training%20and%20inference%2C%20explicitly%20preventing%20common%20mistakes%20within%20each%20optimization%20class.%20Our%20resulting%20fine-tuned%20LLM%20significantly%20improves%20formulation%20accuracy%20by%2020.7%25%20across%20multiple%20optimization%20benchmarks%2C%20with%20consistent%20gains%20under%20test-time%20scaling%20methods%20such%20as%20self-consistency%20and%20multi-turn%20feedback%2C%20enabling%20further%20progress%20toward%20robust%20LLM-assisted%20optimization%20formulation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptiMind%253A%2520Teaching%2520LLMs%2520to%2520Think%2520Like%2520Optimization%2520Experts%26entry.906535625%3DXinzhi%2520Zhang%2520and%2520Zeyi%2520Chen%2520and%2520Humishka%2520Zope%2520and%2520Hugo%2520Barbalho%2520and%2520Konstantina%2520Mellou%2520and%2520Marco%2520Molinaro%2520and%2520Janardhan%2520Kulkarni%2520and%2520Ishai%2520Menache%2520and%2520Sirui%2520Li%26entry.1292438233%3DMathematical%2520programming%2520--%2520the%2520task%2520of%2520expressing%2520operations%2520and%2520decision-making%2520problems%2520in%2520precise%2520mathematical%2520language%2520--%2520is%2520fundamental%2520across%2520domains%252C%2520yet%2520remains%2520a%2520skill-intensive%2520process%2520requiring%2520operations%2520research%2520expertise.%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520for%2520complex%2520reasoning%2520have%2520spurred%2520interest%2520in%2520automating%2520this%2520task%252C%2520translating%2520natural%2520language%2520into%2520executable%2520optimization%2520models.%2520Current%2520approaches%252C%2520however%252C%2520achieve%2520limited%2520accuracy%252C%2520hindered%2520by%2520scarce%2520and%2520noisy%2520training%2520data%2520without%2520leveraging%2520domain%2520knowledge.%2520In%2520this%2520work%252C%2520we%2520systematically%2520integrate%2520optimization%2520expertise%2520to%2520improve%2520formulation%2520accuracy%2520for%2520mixed-integer%2520linear%2520programming%252C%2520a%2520key%2520family%2520of%2520mathematical%2520programs.%2520Our%2520OptiMind%2520framework%2520leverages%2520semi-automated%252C%2520class-based%2520error%2520analysis%2520to%2520guide%2520both%2520training%2520and%2520inference%252C%2520explicitly%2520preventing%2520common%2520mistakes%2520within%2520each%2520optimization%2520class.%2520Our%2520resulting%2520fine-tuned%2520LLM%2520significantly%2520improves%2520formulation%2520accuracy%2520by%252020.7%2525%2520across%2520multiple%2520optimization%2520benchmarks%252C%2520with%2520consistent%2520gains%2520under%2520test-time%2520scaling%2520methods%2520such%2520as%2520self-consistency%2520and%2520multi-turn%2520feedback%252C%2520enabling%2520further%2520progress%2520toward%2520robust%2520LLM-assisted%2520optimization%2520formulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OptiMind%3A%20Teaching%20LLMs%20to%20Think%20Like%20Optimization%20Experts&entry.906535625=Xinzhi%20Zhang%20and%20Zeyi%20Chen%20and%20Humishka%20Zope%20and%20Hugo%20Barbalho%20and%20Konstantina%20Mellou%20and%20Marco%20Molinaro%20and%20Janardhan%20Kulkarni%20and%20Ishai%20Menache%20and%20Sirui%20Li&entry.1292438233=Mathematical%20programming%20--%20the%20task%20of%20expressing%20operations%20and%20decision-making%20problems%20in%20precise%20mathematical%20language%20--%20is%20fundamental%20across%20domains%2C%20yet%20remains%20a%20skill-intensive%20process%20requiring%20operations%20research%20expertise.%20Recent%20advances%20in%20large%20language%20models%20for%20complex%20reasoning%20have%20spurred%20interest%20in%20automating%20this%20task%2C%20translating%20natural%20language%20into%20executable%20optimization%20models.%20Current%20approaches%2C%20however%2C%20achieve%20limited%20accuracy%2C%20hindered%20by%20scarce%20and%20noisy%20training%20data%20without%20leveraging%20domain%20knowledge.%20In%20this%20work%2C%20we%20systematically%20integrate%20optimization%20expertise%20to%20improve%20formulation%20accuracy%20for%20mixed-integer%20linear%20programming%2C%20a%20key%20family%20of%20mathematical%20programs.%20Our%20OptiMind%20framework%20leverages%20semi-automated%2C%20class-based%20error%20analysis%20to%20guide%20both%20training%20and%20inference%2C%20explicitly%20preventing%20common%20mistakes%20within%20each%20optimization%20class.%20Our%20resulting%20fine-tuned%20LLM%20significantly%20improves%20formulation%20accuracy%20by%2020.7%25%20across%20multiple%20optimization%20benchmarks%2C%20with%20consistent%20gains%20under%20test-time%20scaling%20methods%20such%20as%20self-consistency%20and%20multi-turn%20feedback%2C%20enabling%20further%20progress%20toward%20robust%20LLM-assisted%20optimization%20formulation.&entry.1838667208=http%3A//arxiv.org/abs/2509.22979v2&entry.124074799=Read"},
{"title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers", "author": "Lorenzo Basile and Valentino Maiorca and Diego Doimo and Francesco Locatello and Alberto Cazzaniga", "abstract": "Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.", "link": "http://arxiv.org/abs/2510.21518v2", "date": "2026-01-14", "relevancy": 2.4276, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.615}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Head%20Pursuit%3A%20Probing%20Attention%20Specialization%20in%20Multimodal%20Transformers&body=Title%3A%20Head%20Pursuit%3A%20Probing%20Attention%20Specialization%20in%20Multimodal%20Transformers%0AAuthor%3A%20Lorenzo%20Basile%20and%20Valentino%20Maiorca%20and%20Diego%20Doimo%20and%20Francesco%20Locatello%20and%20Alberto%20Cazzaniga%0AAbstract%3A%20Language%20and%20vision-language%20models%20have%20shown%20impressive%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20but%20their%20internal%20mechanisms%20remain%20only%20partly%20understood.%20In%20this%20work%2C%20we%20study%20how%20individual%20attention%20heads%20in%20text-generative%20models%20specialize%20in%20specific%20semantic%20or%20visual%20attributes.%20Building%20on%20an%20established%20interpretability%20method%2C%20we%20reinterpret%20the%20practice%20of%20probing%20intermediate%20activations%20with%20the%20final%20decoding%20layer%20through%20the%20lens%20of%20signal%20processing.%20This%20lets%20us%20analyze%20multiple%20samples%20in%20a%20principled%20way%20and%20rank%20attention%20heads%20based%20on%20their%20relevance%20to%20target%20concepts.%20Our%20results%20show%20consistent%20patterns%20of%20specialization%20at%20the%20head%20level%20across%20both%20unimodal%20and%20multimodal%20transformers.%20Remarkably%2C%20we%20find%20that%20editing%20as%20few%20as%201%25%20of%20the%20heads%2C%20selected%20using%20our%20method%2C%20can%20reliably%20suppress%20or%20enhance%20targeted%20concepts%20in%20the%20model%20output.%20We%20validate%20our%20approach%20on%20language%20tasks%20such%20as%20question%20answering%20and%20toxicity%20mitigation%2C%20as%20well%20as%20vision-language%20tasks%20including%20image%20classification%20and%20captioning.%20Our%20findings%20highlight%20an%20interpretable%20and%20controllable%20structure%20within%20attention%20layers%2C%20offering%20simple%20tools%20for%20understanding%20and%20editing%20large-scale%20generative%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2510.21518v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHead%2520Pursuit%253A%2520Probing%2520Attention%2520Specialization%2520in%2520Multimodal%2520Transformers%26entry.906535625%3DLorenzo%2520Basile%2520and%2520Valentino%2520Maiorca%2520and%2520Diego%2520Doimo%2520and%2520Francesco%2520Locatello%2520and%2520Alberto%2520Cazzaniga%26entry.1292438233%3DLanguage%2520and%2520vision-language%2520models%2520have%2520shown%2520impressive%2520performance%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%2520but%2520their%2520internal%2520mechanisms%2520remain%2520only%2520partly%2520understood.%2520In%2520this%2520work%252C%2520we%2520study%2520how%2520individual%2520attention%2520heads%2520in%2520text-generative%2520models%2520specialize%2520in%2520specific%2520semantic%2520or%2520visual%2520attributes.%2520Building%2520on%2520an%2520established%2520interpretability%2520method%252C%2520we%2520reinterpret%2520the%2520practice%2520of%2520probing%2520intermediate%2520activations%2520with%2520the%2520final%2520decoding%2520layer%2520through%2520the%2520lens%2520of%2520signal%2520processing.%2520This%2520lets%2520us%2520analyze%2520multiple%2520samples%2520in%2520a%2520principled%2520way%2520and%2520rank%2520attention%2520heads%2520based%2520on%2520their%2520relevance%2520to%2520target%2520concepts.%2520Our%2520results%2520show%2520consistent%2520patterns%2520of%2520specialization%2520at%2520the%2520head%2520level%2520across%2520both%2520unimodal%2520and%2520multimodal%2520transformers.%2520Remarkably%252C%2520we%2520find%2520that%2520editing%2520as%2520few%2520as%25201%2525%2520of%2520the%2520heads%252C%2520selected%2520using%2520our%2520method%252C%2520can%2520reliably%2520suppress%2520or%2520enhance%2520targeted%2520concepts%2520in%2520the%2520model%2520output.%2520We%2520validate%2520our%2520approach%2520on%2520language%2520tasks%2520such%2520as%2520question%2520answering%2520and%2520toxicity%2520mitigation%252C%2520as%2520well%2520as%2520vision-language%2520tasks%2520including%2520image%2520classification%2520and%2520captioning.%2520Our%2520findings%2520highlight%2520an%2520interpretable%2520and%2520controllable%2520structure%2520within%2520attention%2520layers%252C%2520offering%2520simple%2520tools%2520for%2520understanding%2520and%2520editing%2520large-scale%2520generative%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.21518v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Head%20Pursuit%3A%20Probing%20Attention%20Specialization%20in%20Multimodal%20Transformers&entry.906535625=Lorenzo%20Basile%20and%20Valentino%20Maiorca%20and%20Diego%20Doimo%20and%20Francesco%20Locatello%20and%20Alberto%20Cazzaniga&entry.1292438233=Language%20and%20vision-language%20models%20have%20shown%20impressive%20performance%20across%20a%20wide%20range%20of%20tasks%2C%20but%20their%20internal%20mechanisms%20remain%20only%20partly%20understood.%20In%20this%20work%2C%20we%20study%20how%20individual%20attention%20heads%20in%20text-generative%20models%20specialize%20in%20specific%20semantic%20or%20visual%20attributes.%20Building%20on%20an%20established%20interpretability%20method%2C%20we%20reinterpret%20the%20practice%20of%20probing%20intermediate%20activations%20with%20the%20final%20decoding%20layer%20through%20the%20lens%20of%20signal%20processing.%20This%20lets%20us%20analyze%20multiple%20samples%20in%20a%20principled%20way%20and%20rank%20attention%20heads%20based%20on%20their%20relevance%20to%20target%20concepts.%20Our%20results%20show%20consistent%20patterns%20of%20specialization%20at%20the%20head%20level%20across%20both%20unimodal%20and%20multimodal%20transformers.%20Remarkably%2C%20we%20find%20that%20editing%20as%20few%20as%201%25%20of%20the%20heads%2C%20selected%20using%20our%20method%2C%20can%20reliably%20suppress%20or%20enhance%20targeted%20concepts%20in%20the%20model%20output.%20We%20validate%20our%20approach%20on%20language%20tasks%20such%20as%20question%20answering%20and%20toxicity%20mitigation%2C%20as%20well%20as%20vision-language%20tasks%20including%20image%20classification%20and%20captioning.%20Our%20findings%20highlight%20an%20interpretable%20and%20controllable%20structure%20within%20attention%20layers%2C%20offering%20simple%20tools%20for%20understanding%20and%20editing%20large-scale%20generative%20models.&entry.1838667208=http%3A//arxiv.org/abs/2510.21518v2&entry.124074799=Read"},
{"title": "Provable Acceleration of Distributed Optimization with Local Updates", "author": "Zuang Wang and Yongqiang Wang", "abstract": "In conventional distributed optimization, each agent performs a single local update between two communication rounds with its neighbors to synchronize solutions. Inspired by the success of using multiple local updates in federated learning, incorporating local updates into distributed optimization has recently attracted increasing attention. However, unlike federated learning, where multiple local updates can accelerate learning by improving gradient estimation under mini-batch settings, it remains unclear whether similar benefits hold in distributed optimization when gradients are exact. Moreover, existing theoretical results typically require reducing the step size when multiple local updates are employed, which can entirely offset any potential benefit of these additional local updates and obscure their true impact on convergence. In this paper, we focus on the classic DIGing algorithm and leverage the tight performance bounds provided by Performance Estimation Problems (PEP) to show that incorporating local updates can indeed accelerate distributed optimization. To the best of our knowledge, this is the first rigorous demonstration of such acceleration for a broad class of objective functions. Our analysis further reveals that, under an appropriate step size, performing only two local updates is sufficient to achieve the maximal possible improvement, and that additional local updates provide no further gains. Because more updates increase computational cost, these findings offer practical guidance for efficient implementation. Extensive experiments on both synthetic and real-world datasets corroborate the theoretical findings.", "link": "http://arxiv.org/abs/2601.03442v2", "date": "2026-01-14", "relevancy": 2.4039, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5085}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4713}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Acceleration%20of%20Distributed%20Optimization%20with%20Local%20Updates&body=Title%3A%20Provable%20Acceleration%20of%20Distributed%20Optimization%20with%20Local%20Updates%0AAuthor%3A%20Zuang%20Wang%20and%20Yongqiang%20Wang%0AAbstract%3A%20In%20conventional%20distributed%20optimization%2C%20each%20agent%20performs%20a%20single%20local%20update%20between%20two%20communication%20rounds%20with%20its%20neighbors%20to%20synchronize%20solutions.%20Inspired%20by%20the%20success%20of%20using%20multiple%20local%20updates%20in%20federated%20learning%2C%20incorporating%20local%20updates%20into%20distributed%20optimization%20has%20recently%20attracted%20increasing%20attention.%20However%2C%20unlike%20federated%20learning%2C%20where%20multiple%20local%20updates%20can%20accelerate%20learning%20by%20improving%20gradient%20estimation%20under%20mini-batch%20settings%2C%20it%20remains%20unclear%20whether%20similar%20benefits%20hold%20in%20distributed%20optimization%20when%20gradients%20are%20exact.%20Moreover%2C%20existing%20theoretical%20results%20typically%20require%20reducing%20the%20step%20size%20when%20multiple%20local%20updates%20are%20employed%2C%20which%20can%20entirely%20offset%20any%20potential%20benefit%20of%20these%20additional%20local%20updates%20and%20obscure%20their%20true%20impact%20on%20convergence.%20In%20this%20paper%2C%20we%20focus%20on%20the%20classic%20DIGing%20algorithm%20and%20leverage%20the%20tight%20performance%20bounds%20provided%20by%20Performance%20Estimation%20Problems%20%28PEP%29%20to%20show%20that%20incorporating%20local%20updates%20can%20indeed%20accelerate%20distributed%20optimization.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20rigorous%20demonstration%20of%20such%20acceleration%20for%20a%20broad%20class%20of%20objective%20functions.%20Our%20analysis%20further%20reveals%20that%2C%20under%20an%20appropriate%20step%20size%2C%20performing%20only%20two%20local%20updates%20is%20sufficient%20to%20achieve%20the%20maximal%20possible%20improvement%2C%20and%20that%20additional%20local%20updates%20provide%20no%20further%20gains.%20Because%20more%20updates%20increase%20computational%20cost%2C%20these%20findings%20offer%20practical%20guidance%20for%20efficient%20implementation.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20corroborate%20the%20theoretical%20findings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Acceleration%2520of%2520Distributed%2520Optimization%2520with%2520Local%2520Updates%26entry.906535625%3DZuang%2520Wang%2520and%2520Yongqiang%2520Wang%26entry.1292438233%3DIn%2520conventional%2520distributed%2520optimization%252C%2520each%2520agent%2520performs%2520a%2520single%2520local%2520update%2520between%2520two%2520communication%2520rounds%2520with%2520its%2520neighbors%2520to%2520synchronize%2520solutions.%2520Inspired%2520by%2520the%2520success%2520of%2520using%2520multiple%2520local%2520updates%2520in%2520federated%2520learning%252C%2520incorporating%2520local%2520updates%2520into%2520distributed%2520optimization%2520has%2520recently%2520attracted%2520increasing%2520attention.%2520However%252C%2520unlike%2520federated%2520learning%252C%2520where%2520multiple%2520local%2520updates%2520can%2520accelerate%2520learning%2520by%2520improving%2520gradient%2520estimation%2520under%2520mini-batch%2520settings%252C%2520it%2520remains%2520unclear%2520whether%2520similar%2520benefits%2520hold%2520in%2520distributed%2520optimization%2520when%2520gradients%2520are%2520exact.%2520Moreover%252C%2520existing%2520theoretical%2520results%2520typically%2520require%2520reducing%2520the%2520step%2520size%2520when%2520multiple%2520local%2520updates%2520are%2520employed%252C%2520which%2520can%2520entirely%2520offset%2520any%2520potential%2520benefit%2520of%2520these%2520additional%2520local%2520updates%2520and%2520obscure%2520their%2520true%2520impact%2520on%2520convergence.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520classic%2520DIGing%2520algorithm%2520and%2520leverage%2520the%2520tight%2520performance%2520bounds%2520provided%2520by%2520Performance%2520Estimation%2520Problems%2520%2528PEP%2529%2520to%2520show%2520that%2520incorporating%2520local%2520updates%2520can%2520indeed%2520accelerate%2520distributed%2520optimization.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520rigorous%2520demonstration%2520of%2520such%2520acceleration%2520for%2520a%2520broad%2520class%2520of%2520objective%2520functions.%2520Our%2520analysis%2520further%2520reveals%2520that%252C%2520under%2520an%2520appropriate%2520step%2520size%252C%2520performing%2520only%2520two%2520local%2520updates%2520is%2520sufficient%2520to%2520achieve%2520the%2520maximal%2520possible%2520improvement%252C%2520and%2520that%2520additional%2520local%2520updates%2520provide%2520no%2520further%2520gains.%2520Because%2520more%2520updates%2520increase%2520computational%2520cost%252C%2520these%2520findings%2520offer%2520practical%2520guidance%2520for%2520efficient%2520implementation.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520corroborate%2520the%2520theoretical%2520findings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Acceleration%20of%20Distributed%20Optimization%20with%20Local%20Updates&entry.906535625=Zuang%20Wang%20and%20Yongqiang%20Wang&entry.1292438233=In%20conventional%20distributed%20optimization%2C%20each%20agent%20performs%20a%20single%20local%20update%20between%20two%20communication%20rounds%20with%20its%20neighbors%20to%20synchronize%20solutions.%20Inspired%20by%20the%20success%20of%20using%20multiple%20local%20updates%20in%20federated%20learning%2C%20incorporating%20local%20updates%20into%20distributed%20optimization%20has%20recently%20attracted%20increasing%20attention.%20However%2C%20unlike%20federated%20learning%2C%20where%20multiple%20local%20updates%20can%20accelerate%20learning%20by%20improving%20gradient%20estimation%20under%20mini-batch%20settings%2C%20it%20remains%20unclear%20whether%20similar%20benefits%20hold%20in%20distributed%20optimization%20when%20gradients%20are%20exact.%20Moreover%2C%20existing%20theoretical%20results%20typically%20require%20reducing%20the%20step%20size%20when%20multiple%20local%20updates%20are%20employed%2C%20which%20can%20entirely%20offset%20any%20potential%20benefit%20of%20these%20additional%20local%20updates%20and%20obscure%20their%20true%20impact%20on%20convergence.%20In%20this%20paper%2C%20we%20focus%20on%20the%20classic%20DIGing%20algorithm%20and%20leverage%20the%20tight%20performance%20bounds%20provided%20by%20Performance%20Estimation%20Problems%20%28PEP%29%20to%20show%20that%20incorporating%20local%20updates%20can%20indeed%20accelerate%20distributed%20optimization.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20rigorous%20demonstration%20of%20such%20acceleration%20for%20a%20broad%20class%20of%20objective%20functions.%20Our%20analysis%20further%20reveals%20that%2C%20under%20an%20appropriate%20step%20size%2C%20performing%20only%20two%20local%20updates%20is%20sufficient%20to%20achieve%20the%20maximal%20possible%20improvement%2C%20and%20that%20additional%20local%20updates%20provide%20no%20further%20gains.%20Because%20more%20updates%20increase%20computational%20cost%2C%20these%20findings%20offer%20practical%20guidance%20for%20efficient%20implementation.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20corroborate%20the%20theoretical%20findings.&entry.1838667208=http%3A//arxiv.org/abs/2601.03442v2&entry.124074799=Read"},
{"title": "SimMerge: Learning to Select Merge Operators from Similarity Signals", "author": "Oliver Bolton and  Aakanksha and Arash Ahmadian and Sara Hooker and Marzieh Fadaee and Beyza Ermis", "abstract": "Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \\simmerge{}, \\emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \\simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \\simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.", "link": "http://arxiv.org/abs/2601.09473v1", "date": "2026-01-14", "relevancy": 2.3898, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimMerge%3A%20Learning%20to%20Select%20Merge%20Operators%20from%20Similarity%20Signals&body=Title%3A%20SimMerge%3A%20Learning%20to%20Select%20Merge%20Operators%20from%20Similarity%20Signals%0AAuthor%3A%20Oliver%20Bolton%20and%20%20Aakanksha%20and%20Arash%20Ahmadian%20and%20Sara%20Hooker%20and%20Marzieh%20Fadaee%20and%20Beyza%20Ermis%0AAbstract%3A%20Model%20merging%20enables%20multiple%20large%20language%20models%20%28LLMs%29%20to%20be%20combined%20into%20a%20single%20model%20while%20preserving%20performance.%20This%20makes%20it%20a%20valuable%20tool%20in%20LLM%20development%2C%20offering%20a%20competitive%20alternative%20to%20multi-task%20training.%20However%2C%20merging%20can%20be%20difficult%20at%20scale%2C%20as%20successful%20merging%20requires%20choosing%20the%20right%20merge%20operator%2C%20selecting%20the%20right%20models%2C%20and%20merging%20them%20in%20the%20right%20order.%20This%20often%20leads%20researchers%20to%20run%20expensive%20merge-and-evaluate%20searches%20to%20select%20the%20best%20merge.%20In%20this%20work%2C%20we%20provide%20an%20alternative%20by%20introducing%20%5Csimmerge%7B%7D%2C%20%5Cemph%7Ba%20predictive%20merge-selection%20method%7D%20that%20selects%20the%20best%20merge%20using%20inexpensive%2C%20task-agnostic%20similarity%20signals%20between%20models.%20From%20a%20small%20set%20of%20unlabeled%20probes%2C%20we%20compute%20functional%20and%20structural%20features%20and%20use%20them%20to%20predict%20the%20performance%20of%20a%20given%202-way%20merge.%20Using%20these%20predictions%2C%20%5Csimmerge%7B%7D%20selects%20the%20best%20merge%20operator%2C%20the%20subset%20of%20models%20to%20merge%2C%20and%20the%20merge%20order%2C%20eliminating%20the%20expensive%20merge-and-evaluate%20loop.%20We%20demonstrate%20that%20we%20surpass%20standard%20merge-operator%20performance%20on%202-way%20merges%20of%207B-parameter%20LLMs%2C%20and%20that%20%5Csimmerge%7B%7D%20generalizes%20to%20multi-way%20merges%20and%20111B-parameter%20LLM%20merges%20without%20retraining.%20Additionally%2C%20we%20present%20a%20bandit%20variant%20that%20supports%20adding%20new%20tasks%2C%20models%2C%20and%20operators%20on%20the%20fly.%20Our%20results%20suggest%20that%20learning%20how%20to%20merge%20is%20a%20practical%20route%20to%20scalable%20model%20composition%20when%20checkpoint%20catalogs%20are%20large%20and%20evaluation%20budgets%20are%20tight.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimMerge%253A%2520Learning%2520to%2520Select%2520Merge%2520Operators%2520from%2520Similarity%2520Signals%26entry.906535625%3DOliver%2520Bolton%2520and%2520%2520Aakanksha%2520and%2520Arash%2520Ahmadian%2520and%2520Sara%2520Hooker%2520and%2520Marzieh%2520Fadaee%2520and%2520Beyza%2520Ermis%26entry.1292438233%3DModel%2520merging%2520enables%2520multiple%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520be%2520combined%2520into%2520a%2520single%2520model%2520while%2520preserving%2520performance.%2520This%2520makes%2520it%2520a%2520valuable%2520tool%2520in%2520LLM%2520development%252C%2520offering%2520a%2520competitive%2520alternative%2520to%2520multi-task%2520training.%2520However%252C%2520merging%2520can%2520be%2520difficult%2520at%2520scale%252C%2520as%2520successful%2520merging%2520requires%2520choosing%2520the%2520right%2520merge%2520operator%252C%2520selecting%2520the%2520right%2520models%252C%2520and%2520merging%2520them%2520in%2520the%2520right%2520order.%2520This%2520often%2520leads%2520researchers%2520to%2520run%2520expensive%2520merge-and-evaluate%2520searches%2520to%2520select%2520the%2520best%2520merge.%2520In%2520this%2520work%252C%2520we%2520provide%2520an%2520alternative%2520by%2520introducing%2520%255Csimmerge%257B%257D%252C%2520%255Cemph%257Ba%2520predictive%2520merge-selection%2520method%257D%2520that%2520selects%2520the%2520best%2520merge%2520using%2520inexpensive%252C%2520task-agnostic%2520similarity%2520signals%2520between%2520models.%2520From%2520a%2520small%2520set%2520of%2520unlabeled%2520probes%252C%2520we%2520compute%2520functional%2520and%2520structural%2520features%2520and%2520use%2520them%2520to%2520predict%2520the%2520performance%2520of%2520a%2520given%25202-way%2520merge.%2520Using%2520these%2520predictions%252C%2520%255Csimmerge%257B%257D%2520selects%2520the%2520best%2520merge%2520operator%252C%2520the%2520subset%2520of%2520models%2520to%2520merge%252C%2520and%2520the%2520merge%2520order%252C%2520eliminating%2520the%2520expensive%2520merge-and-evaluate%2520loop.%2520We%2520demonstrate%2520that%2520we%2520surpass%2520standard%2520merge-operator%2520performance%2520on%25202-way%2520merges%2520of%25207B-parameter%2520LLMs%252C%2520and%2520that%2520%255Csimmerge%257B%257D%2520generalizes%2520to%2520multi-way%2520merges%2520and%2520111B-parameter%2520LLM%2520merges%2520without%2520retraining.%2520Additionally%252C%2520we%2520present%2520a%2520bandit%2520variant%2520that%2520supports%2520adding%2520new%2520tasks%252C%2520models%252C%2520and%2520operators%2520on%2520the%2520fly.%2520Our%2520results%2520suggest%2520that%2520learning%2520how%2520to%2520merge%2520is%2520a%2520practical%2520route%2520to%2520scalable%2520model%2520composition%2520when%2520checkpoint%2520catalogs%2520are%2520large%2520and%2520evaluation%2520budgets%2520are%2520tight.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimMerge%3A%20Learning%20to%20Select%20Merge%20Operators%20from%20Similarity%20Signals&entry.906535625=Oliver%20Bolton%20and%20%20Aakanksha%20and%20Arash%20Ahmadian%20and%20Sara%20Hooker%20and%20Marzieh%20Fadaee%20and%20Beyza%20Ermis&entry.1292438233=Model%20merging%20enables%20multiple%20large%20language%20models%20%28LLMs%29%20to%20be%20combined%20into%20a%20single%20model%20while%20preserving%20performance.%20This%20makes%20it%20a%20valuable%20tool%20in%20LLM%20development%2C%20offering%20a%20competitive%20alternative%20to%20multi-task%20training.%20However%2C%20merging%20can%20be%20difficult%20at%20scale%2C%20as%20successful%20merging%20requires%20choosing%20the%20right%20merge%20operator%2C%20selecting%20the%20right%20models%2C%20and%20merging%20them%20in%20the%20right%20order.%20This%20often%20leads%20researchers%20to%20run%20expensive%20merge-and-evaluate%20searches%20to%20select%20the%20best%20merge.%20In%20this%20work%2C%20we%20provide%20an%20alternative%20by%20introducing%20%5Csimmerge%7B%7D%2C%20%5Cemph%7Ba%20predictive%20merge-selection%20method%7D%20that%20selects%20the%20best%20merge%20using%20inexpensive%2C%20task-agnostic%20similarity%20signals%20between%20models.%20From%20a%20small%20set%20of%20unlabeled%20probes%2C%20we%20compute%20functional%20and%20structural%20features%20and%20use%20them%20to%20predict%20the%20performance%20of%20a%20given%202-way%20merge.%20Using%20these%20predictions%2C%20%5Csimmerge%7B%7D%20selects%20the%20best%20merge%20operator%2C%20the%20subset%20of%20models%20to%20merge%2C%20and%20the%20merge%20order%2C%20eliminating%20the%20expensive%20merge-and-evaluate%20loop.%20We%20demonstrate%20that%20we%20surpass%20standard%20merge-operator%20performance%20on%202-way%20merges%20of%207B-parameter%20LLMs%2C%20and%20that%20%5Csimmerge%7B%7D%20generalizes%20to%20multi-way%20merges%20and%20111B-parameter%20LLM%20merges%20without%20retraining.%20Additionally%2C%20we%20present%20a%20bandit%20variant%20that%20supports%20adding%20new%20tasks%2C%20models%2C%20and%20operators%20on%20the%20fly.%20Our%20results%20suggest%20that%20learning%20how%20to%20merge%20is%20a%20practical%20route%20to%20scalable%20model%20composition%20when%20checkpoint%20catalogs%20are%20large%20and%20evaluation%20budgets%20are%20tight.&entry.1838667208=http%3A//arxiv.org/abs/2601.09473v1&entry.124074799=Read"},
{"title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction", "author": "Longbin Ji and Xiaoxiong Liu and Junyuan Shang and Shuohuan Wang and Yu Sun and Hua Wu and Haifeng Wang", "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.", "link": "http://arxiv.org/abs/2601.05966v2", "date": "2026-01-14", "relevancy": 2.3702, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6072}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5907}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoAR%3A%20Autoregressive%20Video%20Generation%20via%20Next-Frame%20%26%20Scale%20Prediction&body=Title%3A%20VideoAR%3A%20Autoregressive%20Video%20Generation%20via%20Next-Frame%20%26%20Scale%20Prediction%0AAuthor%3A%20Longbin%20Ji%20and%20Xiaoxiong%20Liu%20and%20Junyuan%20Shang%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%20and%20Haifeng%20Wang%0AAbstract%3A%20Recent%20advances%20in%20video%20generation%20have%20been%20dominated%20by%20diffusion%20and%20flow-matching%20models%2C%20which%20produce%20high-quality%20results%20but%20remain%20computationally%20intensive%20and%20difficult%20to%20scale.%20In%20this%20work%2C%20we%20introduce%20VideoAR%2C%20the%20first%20large-scale%20Visual%20Autoregressive%20%28VAR%29%20framework%20for%20video%20generation%20that%20combines%20multi-scale%20next-frame%20prediction%20with%20autoregressive%20modeling.%20VideoAR%20disentangles%20spatial%20and%20temporal%20dependencies%20by%20integrating%20intra-frame%20VAR%20modeling%20with%20causal%20next-frame%20prediction%2C%20supported%20by%20a%203D%20multi-scale%20tokenizer%20that%20efficiently%20encodes%20spatio-temporal%20dynamics.%20To%20improve%20long-term%20consistency%2C%20we%20propose%20Multi-scale%20Temporal%20RoPE%2C%20Cross-Frame%20Error%20Correction%2C%20and%20Random%20Frame%20Mask%2C%20which%20collectively%20mitigate%20error%20propagation%20and%20stabilize%20temporal%20coherence.%20Our%20multi-stage%20pretraining%20pipeline%20progressively%20aligns%20spatial%20and%20temporal%20learning%20across%20increasing%20resolutions%20and%20durations.%20Empirically%2C%20VideoAR%20achieves%20new%20state-of-the-art%20results%20among%20autoregressive%20models%2C%20improving%20FVD%20on%20UCF-101%20from%2099.5%20to%2088.6%20while%20reducing%20inference%20steps%20by%20over%2010x%2C%20and%20reaching%20a%20VBench%20score%20of%2081.74-competitive%20with%20diffusion-based%20models%20an%20order%20of%20magnitude%20larger.%20These%20results%20demonstrate%20that%20VideoAR%20narrows%20the%20performance%20gap%20between%20autoregressive%20and%20diffusion%20paradigms%2C%20offering%20a%20scalable%2C%20efficient%2C%20and%20temporally%20consistent%20foundation%20for%20future%20video%20generation%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05966v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoAR%253A%2520Autoregressive%2520Video%2520Generation%2520via%2520Next-Frame%2520%2526%2520Scale%2520Prediction%26entry.906535625%3DLongbin%2520Ji%2520and%2520Xiaoxiong%2520Liu%2520and%2520Junyuan%2520Shang%2520and%2520Shuohuan%2520Wang%2520and%2520Yu%2520Sun%2520and%2520Hua%2520Wu%2520and%2520Haifeng%2520Wang%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520generation%2520have%2520been%2520dominated%2520by%2520diffusion%2520and%2520flow-matching%2520models%252C%2520which%2520produce%2520high-quality%2520results%2520but%2520remain%2520computationally%2520intensive%2520and%2520difficult%2520to%2520scale.%2520In%2520this%2520work%252C%2520we%2520introduce%2520VideoAR%252C%2520the%2520first%2520large-scale%2520Visual%2520Autoregressive%2520%2528VAR%2529%2520framework%2520for%2520video%2520generation%2520that%2520combines%2520multi-scale%2520next-frame%2520prediction%2520with%2520autoregressive%2520modeling.%2520VideoAR%2520disentangles%2520spatial%2520and%2520temporal%2520dependencies%2520by%2520integrating%2520intra-frame%2520VAR%2520modeling%2520with%2520causal%2520next-frame%2520prediction%252C%2520supported%2520by%2520a%25203D%2520multi-scale%2520tokenizer%2520that%2520efficiently%2520encodes%2520spatio-temporal%2520dynamics.%2520To%2520improve%2520long-term%2520consistency%252C%2520we%2520propose%2520Multi-scale%2520Temporal%2520RoPE%252C%2520Cross-Frame%2520Error%2520Correction%252C%2520and%2520Random%2520Frame%2520Mask%252C%2520which%2520collectively%2520mitigate%2520error%2520propagation%2520and%2520stabilize%2520temporal%2520coherence.%2520Our%2520multi-stage%2520pretraining%2520pipeline%2520progressively%2520aligns%2520spatial%2520and%2520temporal%2520learning%2520across%2520increasing%2520resolutions%2520and%2520durations.%2520Empirically%252C%2520VideoAR%2520achieves%2520new%2520state-of-the-art%2520results%2520among%2520autoregressive%2520models%252C%2520improving%2520FVD%2520on%2520UCF-101%2520from%252099.5%2520to%252088.6%2520while%2520reducing%2520inference%2520steps%2520by%2520over%252010x%252C%2520and%2520reaching%2520a%2520VBench%2520score%2520of%252081.74-competitive%2520with%2520diffusion-based%2520models%2520an%2520order%2520of%2520magnitude%2520larger.%2520These%2520results%2520demonstrate%2520that%2520VideoAR%2520narrows%2520the%2520performance%2520gap%2520between%2520autoregressive%2520and%2520diffusion%2520paradigms%252C%2520offering%2520a%2520scalable%252C%2520efficient%252C%2520and%2520temporally%2520consistent%2520foundation%2520for%2520future%2520video%2520generation%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05966v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoAR%3A%20Autoregressive%20Video%20Generation%20via%20Next-Frame%20%26%20Scale%20Prediction&entry.906535625=Longbin%20Ji%20and%20Xiaoxiong%20Liu%20and%20Junyuan%20Shang%20and%20Shuohuan%20Wang%20and%20Yu%20Sun%20and%20Hua%20Wu%20and%20Haifeng%20Wang&entry.1292438233=Recent%20advances%20in%20video%20generation%20have%20been%20dominated%20by%20diffusion%20and%20flow-matching%20models%2C%20which%20produce%20high-quality%20results%20but%20remain%20computationally%20intensive%20and%20difficult%20to%20scale.%20In%20this%20work%2C%20we%20introduce%20VideoAR%2C%20the%20first%20large-scale%20Visual%20Autoregressive%20%28VAR%29%20framework%20for%20video%20generation%20that%20combines%20multi-scale%20next-frame%20prediction%20with%20autoregressive%20modeling.%20VideoAR%20disentangles%20spatial%20and%20temporal%20dependencies%20by%20integrating%20intra-frame%20VAR%20modeling%20with%20causal%20next-frame%20prediction%2C%20supported%20by%20a%203D%20multi-scale%20tokenizer%20that%20efficiently%20encodes%20spatio-temporal%20dynamics.%20To%20improve%20long-term%20consistency%2C%20we%20propose%20Multi-scale%20Temporal%20RoPE%2C%20Cross-Frame%20Error%20Correction%2C%20and%20Random%20Frame%20Mask%2C%20which%20collectively%20mitigate%20error%20propagation%20and%20stabilize%20temporal%20coherence.%20Our%20multi-stage%20pretraining%20pipeline%20progressively%20aligns%20spatial%20and%20temporal%20learning%20across%20increasing%20resolutions%20and%20durations.%20Empirically%2C%20VideoAR%20achieves%20new%20state-of-the-art%20results%20among%20autoregressive%20models%2C%20improving%20FVD%20on%20UCF-101%20from%2099.5%20to%2088.6%20while%20reducing%20inference%20steps%20by%20over%2010x%2C%20and%20reaching%20a%20VBench%20score%20of%2081.74-competitive%20with%20diffusion-based%20models%20an%20order%20of%20magnitude%20larger.%20These%20results%20demonstrate%20that%20VideoAR%20narrows%20the%20performance%20gap%20between%20autoregressive%20and%20diffusion%20paradigms%2C%20offering%20a%20scalable%2C%20efficient%2C%20and%20temporally%20consistent%20foundation%20for%20future%20video%20generation%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.05966v2&entry.124074799=Read"},
{"title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing", "author": "Qian Cao and Yahui Liu and Wei Bi and Yi Zhao and Ruihua Song and Xiting Wang and Ruiming Tang and Guorui Zhou and Han Li", "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.", "link": "http://arxiv.org/abs/2601.09609v1", "date": "2026-01-14", "relevancy": 2.3596, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPWriter%3A%20Reinforcement%20Learning%20with%20Diverse%20Planning%20Branching%20for%20Creative%20Writing&body=Title%3A%20DPWriter%3A%20Reinforcement%20Learning%20with%20Diverse%20Planning%20Branching%20for%20Creative%20Writing%0AAuthor%3A%20Qian%20Cao%20and%20Yahui%20Liu%20and%20Wei%20Bi%20and%20Yi%20Zhao%20and%20Ruihua%20Song%20and%20Xiting%20Wang%20and%20Ruiming%20Tang%20and%20Guorui%20Zhou%20and%20Han%20Li%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29-based%20enhancement%20of%20large%20language%20models%20%28LLMs%29%20often%20leads%20to%20reduced%20output%20diversity%2C%20undermining%20their%20utility%20in%20open-ended%20tasks%20like%20creative%20writing.%20Current%20methods%20lack%20explicit%20mechanisms%20for%20guiding%20diverse%20exploration%20and%20instead%20prioritize%20optimization%20efficiency%20and%20performance%20over%20diversity.%20This%20paper%20proposes%20an%20RL%20framework%20structured%20around%20a%20semi-structured%20long%20Chain-of-Thought%20%28CoT%29%2C%20in%20which%20the%20generation%20process%20is%20decomposed%20into%20explicitly%20planned%20intermediate%20steps.%20We%20introduce%20a%20Diverse%20Planning%20Branching%20method%20that%20strategically%20introduces%20divergence%20at%20the%20planning%20phase%20based%20on%20diversity%20variation%2C%20alongside%20a%20group-aware%20diversity%20reward%20to%20encourage%20distinct%20trajectories.%20Experimental%20results%20on%20creative%20writing%20benchmarks%20demonstrate%20that%20our%20approach%20significantly%20improves%20output%20diversity%20without%20compromising%20generation%20quality%2C%20consistently%20outperforming%20existing%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPWriter%253A%2520Reinforcement%2520Learning%2520with%2520Diverse%2520Planning%2520Branching%2520for%2520Creative%2520Writing%26entry.906535625%3DQian%2520Cao%2520and%2520Yahui%2520Liu%2520and%2520Wei%2520Bi%2520and%2520Yi%2520Zhao%2520and%2520Ruihua%2520Song%2520and%2520Xiting%2520Wang%2520and%2520Ruiming%2520Tang%2520and%2520Guorui%2520Zhou%2520and%2520Han%2520Li%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529-based%2520enhancement%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520leads%2520to%2520reduced%2520output%2520diversity%252C%2520undermining%2520their%2520utility%2520in%2520open-ended%2520tasks%2520like%2520creative%2520writing.%2520Current%2520methods%2520lack%2520explicit%2520mechanisms%2520for%2520guiding%2520diverse%2520exploration%2520and%2520instead%2520prioritize%2520optimization%2520efficiency%2520and%2520performance%2520over%2520diversity.%2520This%2520paper%2520proposes%2520an%2520RL%2520framework%2520structured%2520around%2520a%2520semi-structured%2520long%2520Chain-of-Thought%2520%2528CoT%2529%252C%2520in%2520which%2520the%2520generation%2520process%2520is%2520decomposed%2520into%2520explicitly%2520planned%2520intermediate%2520steps.%2520We%2520introduce%2520a%2520Diverse%2520Planning%2520Branching%2520method%2520that%2520strategically%2520introduces%2520divergence%2520at%2520the%2520planning%2520phase%2520based%2520on%2520diversity%2520variation%252C%2520alongside%2520a%2520group-aware%2520diversity%2520reward%2520to%2520encourage%2520distinct%2520trajectories.%2520Experimental%2520results%2520on%2520creative%2520writing%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%2520output%2520diversity%2520without%2520compromising%2520generation%2520quality%252C%2520consistently%2520outperforming%2520existing%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPWriter%3A%20Reinforcement%20Learning%20with%20Diverse%20Planning%20Branching%20for%20Creative%20Writing&entry.906535625=Qian%20Cao%20and%20Yahui%20Liu%20and%20Wei%20Bi%20and%20Yi%20Zhao%20and%20Ruihua%20Song%20and%20Xiting%20Wang%20and%20Ruiming%20Tang%20and%20Guorui%20Zhou%20and%20Han%20Li&entry.1292438233=Reinforcement%20learning%20%28RL%29-based%20enhancement%20of%20large%20language%20models%20%28LLMs%29%20often%20leads%20to%20reduced%20output%20diversity%2C%20undermining%20their%20utility%20in%20open-ended%20tasks%20like%20creative%20writing.%20Current%20methods%20lack%20explicit%20mechanisms%20for%20guiding%20diverse%20exploration%20and%20instead%20prioritize%20optimization%20efficiency%20and%20performance%20over%20diversity.%20This%20paper%20proposes%20an%20RL%20framework%20structured%20around%20a%20semi-structured%20long%20Chain-of-Thought%20%28CoT%29%2C%20in%20which%20the%20generation%20process%20is%20decomposed%20into%20explicitly%20planned%20intermediate%20steps.%20We%20introduce%20a%20Diverse%20Planning%20Branching%20method%20that%20strategically%20introduces%20divergence%20at%20the%20planning%20phase%20based%20on%20diversity%20variation%2C%20alongside%20a%20group-aware%20diversity%20reward%20to%20encourage%20distinct%20trajectories.%20Experimental%20results%20on%20creative%20writing%20benchmarks%20demonstrate%20that%20our%20approach%20significantly%20improves%20output%20diversity%20without%20compromising%20generation%20quality%2C%20consistently%20outperforming%20existing%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.09609v1&entry.124074799=Read"},
{"title": "LLMs can Compress LLMs: Adaptive Pruning by Agents", "author": "Sai Varun Kodathala and Rakesh Vunnam", "abstract": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.", "link": "http://arxiv.org/abs/2601.09694v1", "date": "2026-01-14", "relevancy": 2.3545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20can%20Compress%20LLMs%3A%20Adaptive%20Pruning%20by%20Agents&body=Title%3A%20LLMs%20can%20Compress%20LLMs%3A%20Adaptive%20Pruning%20by%20Agents%0AAuthor%3A%20Sai%20Varun%20Kodathala%20and%20Rakesh%20Vunnam%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20scale%2C%20post-training%20pruning%20has%20emerged%20as%20a%20promising%20approach%20to%20reduce%20computational%20costs%20while%20preserving%20performance.%20Existing%20methods%20such%20as%20SparseGPT%20and%20Wanda%20achieve%20high%20sparsity%20through%20layer-wise%20weight%20reconstruction%20or%20activation-aware%20magnitude%20pruning%2C%20but%20rely%20on%20uniform%20or%20hand-crafted%20heuristics%20to%20determine%20per-layer%20sparsity%20ratios.%20Moreover%2C%20recent%20work%20has%20shown%20that%20pruned%20LLMs%20suffer%20from%20severe%20factual%20knowledge%20degradation%2C%20with%20structured%20pruning%20methods%20experiencing%20near-total%20collapse%20in%20factual%20question-answering%20capabilities.%20We%20introduce%20agent-guided%20pruning%2C%20where%20a%20foundation%20model%20acts%20as%20an%20adaptive%20pruning%20agent%20to%20intelligently%20select%20which%20layers%20to%20prune%20at%20each%20iteration%20while%20preserving%20critical%20knowledge%20pathways.%20Our%20method%20constructs%20layer-wise%20sensitivity%20profiles%20by%20combining%20Wanda-inspired%20weight-activation%20metrics%20with%20gradient%20importance%20scores%2C%20normalized%20as%20z-scores%20for%20model-agnostic%20comparison.%20These%20statistics%20are%20processed%20by%20an%20LLM%20agent%20equipped%20with%20self-reflection%20capabilities%2C%20enabling%20it%20to%20learn%20from%20previous%20pruning%20outcomes%20and%20iteratively%20refine%20its%20strategy.%20A%20checkpoint%20rollback%20mechanism%20maintains%20model%20quality%20by%20reverting%20when%20perplexity%20degradation%20exceeds%20a%20threshold.%20We%20evaluate%20our%20approach%20on%20Qwen3%20models%20%284B%20and%208B%20parameters%29%20at%20approximately%2045%25%20sparsity%2C%20demonstrating%20substantial%20improvements%20over%20structured%20pruning%20baselines%3A%2056%25%20relative%20improvement%20in%20MMLU%20accuracy%2C%2019x%20better%20factual%20knowledge%20retention%20on%20FreebaseQA%2C%20and%2069%25%20lower%20perplexity%20degradation.%20Notably%2C%20our%20framework%20requires%20no%20retraining%2C%20operates%20in%20a%20model-agnostic%20manner%2C%20and%20exhibits%20effective%20self-correction%20with%20only%202-4%20rollbacks%20across%2021-40%20iterations%2C%20demonstrating%20that%20foundation%20models%20can%20effectively%20guide%20the%20compression%20of%20other%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520can%2520Compress%2520LLMs%253A%2520Adaptive%2520Pruning%2520by%2520Agents%26entry.906535625%3DSai%2520Varun%2520Kodathala%2520and%2520Rakesh%2520Vunnam%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520continue%2520to%2520scale%252C%2520post-training%2520pruning%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520reduce%2520computational%2520costs%2520while%2520preserving%2520performance.%2520Existing%2520methods%2520such%2520as%2520SparseGPT%2520and%2520Wanda%2520achieve%2520high%2520sparsity%2520through%2520layer-wise%2520weight%2520reconstruction%2520or%2520activation-aware%2520magnitude%2520pruning%252C%2520but%2520rely%2520on%2520uniform%2520or%2520hand-crafted%2520heuristics%2520to%2520determine%2520per-layer%2520sparsity%2520ratios.%2520Moreover%252C%2520recent%2520work%2520has%2520shown%2520that%2520pruned%2520LLMs%2520suffer%2520from%2520severe%2520factual%2520knowledge%2520degradation%252C%2520with%2520structured%2520pruning%2520methods%2520experiencing%2520near-total%2520collapse%2520in%2520factual%2520question-answering%2520capabilities.%2520We%2520introduce%2520agent-guided%2520pruning%252C%2520where%2520a%2520foundation%2520model%2520acts%2520as%2520an%2520adaptive%2520pruning%2520agent%2520to%2520intelligently%2520select%2520which%2520layers%2520to%2520prune%2520at%2520each%2520iteration%2520while%2520preserving%2520critical%2520knowledge%2520pathways.%2520Our%2520method%2520constructs%2520layer-wise%2520sensitivity%2520profiles%2520by%2520combining%2520Wanda-inspired%2520weight-activation%2520metrics%2520with%2520gradient%2520importance%2520scores%252C%2520normalized%2520as%2520z-scores%2520for%2520model-agnostic%2520comparison.%2520These%2520statistics%2520are%2520processed%2520by%2520an%2520LLM%2520agent%2520equipped%2520with%2520self-reflection%2520capabilities%252C%2520enabling%2520it%2520to%2520learn%2520from%2520previous%2520pruning%2520outcomes%2520and%2520iteratively%2520refine%2520its%2520strategy.%2520A%2520checkpoint%2520rollback%2520mechanism%2520maintains%2520model%2520quality%2520by%2520reverting%2520when%2520perplexity%2520degradation%2520exceeds%2520a%2520threshold.%2520We%2520evaluate%2520our%2520approach%2520on%2520Qwen3%2520models%2520%25284B%2520and%25208B%2520parameters%2529%2520at%2520approximately%252045%2525%2520sparsity%252C%2520demonstrating%2520substantial%2520improvements%2520over%2520structured%2520pruning%2520baselines%253A%252056%2525%2520relative%2520improvement%2520in%2520MMLU%2520accuracy%252C%252019x%2520better%2520factual%2520knowledge%2520retention%2520on%2520FreebaseQA%252C%2520and%252069%2525%2520lower%2520perplexity%2520degradation.%2520Notably%252C%2520our%2520framework%2520requires%2520no%2520retraining%252C%2520operates%2520in%2520a%2520model-agnostic%2520manner%252C%2520and%2520exhibits%2520effective%2520self-correction%2520with%2520only%25202-4%2520rollbacks%2520across%252021-40%2520iterations%252C%2520demonstrating%2520that%2520foundation%2520models%2520can%2520effectively%2520guide%2520the%2520compression%2520of%2520other%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20can%20Compress%20LLMs%3A%20Adaptive%20Pruning%20by%20Agents&entry.906535625=Sai%20Varun%20Kodathala%20and%20Rakesh%20Vunnam&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20scale%2C%20post-training%20pruning%20has%20emerged%20as%20a%20promising%20approach%20to%20reduce%20computational%20costs%20while%20preserving%20performance.%20Existing%20methods%20such%20as%20SparseGPT%20and%20Wanda%20achieve%20high%20sparsity%20through%20layer-wise%20weight%20reconstruction%20or%20activation-aware%20magnitude%20pruning%2C%20but%20rely%20on%20uniform%20or%20hand-crafted%20heuristics%20to%20determine%20per-layer%20sparsity%20ratios.%20Moreover%2C%20recent%20work%20has%20shown%20that%20pruned%20LLMs%20suffer%20from%20severe%20factual%20knowledge%20degradation%2C%20with%20structured%20pruning%20methods%20experiencing%20near-total%20collapse%20in%20factual%20question-answering%20capabilities.%20We%20introduce%20agent-guided%20pruning%2C%20where%20a%20foundation%20model%20acts%20as%20an%20adaptive%20pruning%20agent%20to%20intelligently%20select%20which%20layers%20to%20prune%20at%20each%20iteration%20while%20preserving%20critical%20knowledge%20pathways.%20Our%20method%20constructs%20layer-wise%20sensitivity%20profiles%20by%20combining%20Wanda-inspired%20weight-activation%20metrics%20with%20gradient%20importance%20scores%2C%20normalized%20as%20z-scores%20for%20model-agnostic%20comparison.%20These%20statistics%20are%20processed%20by%20an%20LLM%20agent%20equipped%20with%20self-reflection%20capabilities%2C%20enabling%20it%20to%20learn%20from%20previous%20pruning%20outcomes%20and%20iteratively%20refine%20its%20strategy.%20A%20checkpoint%20rollback%20mechanism%20maintains%20model%20quality%20by%20reverting%20when%20perplexity%20degradation%20exceeds%20a%20threshold.%20We%20evaluate%20our%20approach%20on%20Qwen3%20models%20%284B%20and%208B%20parameters%29%20at%20approximately%2045%25%20sparsity%2C%20demonstrating%20substantial%20improvements%20over%20structured%20pruning%20baselines%3A%2056%25%20relative%20improvement%20in%20MMLU%20accuracy%2C%2019x%20better%20factual%20knowledge%20retention%20on%20FreebaseQA%2C%20and%2069%25%20lower%20perplexity%20degradation.%20Notably%2C%20our%20framework%20requires%20no%20retraining%2C%20operates%20in%20a%20model-agnostic%20manner%2C%20and%20exhibits%20effective%20self-correction%20with%20only%202-4%20rollbacks%20across%2021-40%20iterations%2C%20demonstrating%20that%20foundation%20models%20can%20effectively%20guide%20the%20compression%20of%20other%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.09694v1&entry.124074799=Read"},
{"title": "Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs", "author": "Rui Zhu and Xin Shen and Shuchen Wu and Chenxi Miao and Xin Yu and Yang Li and Weikang Li and Deguo Xia and Jizhou Huang", "abstract": "Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.", "link": "http://arxiv.org/abs/2601.09430v1", "date": "2026-01-14", "relevancy": 2.328, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-MSR%3A%20Benchmarking%20Multi-hop%20Spatial%20Reasoning%20Capabilities%20of%20MLLMs&body=Title%3A%20Video-MSR%3A%20Benchmarking%20Multi-hop%20Spatial%20Reasoning%20Capabilities%20of%20MLLMs%0AAuthor%3A%20Rui%20Zhu%20and%20Xin%20Shen%20and%20Shuchen%20Wu%20and%20Chenxi%20Miao%20and%20Xin%20Yu%20and%20Yang%20Li%20and%20Weikang%20Li%20and%20Deguo%20Xia%20and%20Jizhou%20Huang%0AAbstract%3A%20Spatial%20reasoning%20has%20emerged%20as%20a%20critical%20capability%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20drawing%20increasing%20attention%20and%20rapid%20advancement.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20single-step%20perception-to-judgment%20tasks%2C%20leaving%20scenarios%20requiring%20complex%20visual-spatial%20logical%20chains%20significantly%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Video-MSR%2C%20the%20first%20benchmark%20specifically%20designed%20to%20evaluate%20Multi-hop%20Spatial%20Reasoning%20%28MSR%29%20in%20dynamic%20video%20scenarios.%20Video-MSR%20systematically%20probes%20MSR%20capabilities%20through%20four%20distinct%20tasks%3A%20Constrained%20Localization%2C%20Chain-based%20Reference%20Retrieval%2C%20Route%20Planning%2C%20and%20Counterfactual%20Physical%20Deduction.%20Our%20benchmark%20comprises%203%2C052%20high-quality%20video%20instances%20with%204%2C993%20question-answer%20pairs%2C%20constructed%20via%20a%20scalable%2C%20visually-grounded%20pipeline%20combining%20advanced%20model%20generation%20with%20rigorous%20human%20verification.%20Through%20a%20comprehensive%20evaluation%20of%2020%20state-of-the-art%20MLLMs%2C%20we%20uncover%20significant%20limitations%2C%20revealing%20that%20while%20models%20demonstrate%20proficiency%20in%20surface-level%20perception%2C%20they%20exhibit%20distinct%20performance%20drops%20in%20MSR%20tasks%2C%20frequently%20suffering%20from%20spatial%20disorientation%20and%20hallucination%20during%20multi-step%20deductions.%20To%20mitigate%20these%20shortcomings%20and%20empower%20models%20with%20stronger%20MSR%20capabilities%2C%20we%20further%20curate%20MSR-9K%2C%20a%20specialized%20instruction-tuning%20dataset%2C%20and%20fine-tune%20Qwen-VL%2C%20achieving%20a%20%2B7.82%25%20absolute%20improvement%20on%20Video-MSR.%20Our%20results%20underscore%20the%20efficacy%20of%20multi-hop%20spatial%20instruction%20data%20and%20establish%20Video-MSR%20as%20a%20vital%20foundation%20for%20future%20research.%20The%20code%20and%20data%20will%20be%20available%20at%20https%3A//github.com/ruiz-nju/Video-MSR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-MSR%253A%2520Benchmarking%2520Multi-hop%2520Spatial%2520Reasoning%2520Capabilities%2520of%2520MLLMs%26entry.906535625%3DRui%2520Zhu%2520and%2520Xin%2520Shen%2520and%2520Shuchen%2520Wu%2520and%2520Chenxi%2520Miao%2520and%2520Xin%2520Yu%2520and%2520Yang%2520Li%2520and%2520Weikang%2520Li%2520and%2520Deguo%2520Xia%2520and%2520Jizhou%2520Huang%26entry.1292438233%3DSpatial%2520reasoning%2520has%2520emerged%2520as%2520a%2520critical%2520capability%2520for%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520drawing%2520increasing%2520attention%2520and%2520rapid%2520advancement.%2520However%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520single-step%2520perception-to-judgment%2520tasks%252C%2520leaving%2520scenarios%2520requiring%2520complex%2520visual-spatial%2520logical%2520chains%2520significantly%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Video-MSR%252C%2520the%2520first%2520benchmark%2520specifically%2520designed%2520to%2520evaluate%2520Multi-hop%2520Spatial%2520Reasoning%2520%2528MSR%2529%2520in%2520dynamic%2520video%2520scenarios.%2520Video-MSR%2520systematically%2520probes%2520MSR%2520capabilities%2520through%2520four%2520distinct%2520tasks%253A%2520Constrained%2520Localization%252C%2520Chain-based%2520Reference%2520Retrieval%252C%2520Route%2520Planning%252C%2520and%2520Counterfactual%2520Physical%2520Deduction.%2520Our%2520benchmark%2520comprises%25203%252C052%2520high-quality%2520video%2520instances%2520with%25204%252C993%2520question-answer%2520pairs%252C%2520constructed%2520via%2520a%2520scalable%252C%2520visually-grounded%2520pipeline%2520combining%2520advanced%2520model%2520generation%2520with%2520rigorous%2520human%2520verification.%2520Through%2520a%2520comprehensive%2520evaluation%2520of%252020%2520state-of-the-art%2520MLLMs%252C%2520we%2520uncover%2520significant%2520limitations%252C%2520revealing%2520that%2520while%2520models%2520demonstrate%2520proficiency%2520in%2520surface-level%2520perception%252C%2520they%2520exhibit%2520distinct%2520performance%2520drops%2520in%2520MSR%2520tasks%252C%2520frequently%2520suffering%2520from%2520spatial%2520disorientation%2520and%2520hallucination%2520during%2520multi-step%2520deductions.%2520To%2520mitigate%2520these%2520shortcomings%2520and%2520empower%2520models%2520with%2520stronger%2520MSR%2520capabilities%252C%2520we%2520further%2520curate%2520MSR-9K%252C%2520a%2520specialized%2520instruction-tuning%2520dataset%252C%2520and%2520fine-tune%2520Qwen-VL%252C%2520achieving%2520a%2520%252B7.82%2525%2520absolute%2520improvement%2520on%2520Video-MSR.%2520Our%2520results%2520underscore%2520the%2520efficacy%2520of%2520multi-hop%2520spatial%2520instruction%2520data%2520and%2520establish%2520Video-MSR%2520as%2520a%2520vital%2520foundation%2520for%2520future%2520research.%2520The%2520code%2520and%2520data%2520will%2520be%2520available%2520at%2520https%253A//github.com/ruiz-nju/Video-MSR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-MSR%3A%20Benchmarking%20Multi-hop%20Spatial%20Reasoning%20Capabilities%20of%20MLLMs&entry.906535625=Rui%20Zhu%20and%20Xin%20Shen%20and%20Shuchen%20Wu%20and%20Chenxi%20Miao%20and%20Xin%20Yu%20and%20Yang%20Li%20and%20Weikang%20Li%20and%20Deguo%20Xia%20and%20Jizhou%20Huang&entry.1292438233=Spatial%20reasoning%20has%20emerged%20as%20a%20critical%20capability%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20drawing%20increasing%20attention%20and%20rapid%20advancement.%20However%2C%20existing%20benchmarks%20primarily%20focus%20on%20single-step%20perception-to-judgment%20tasks%2C%20leaving%20scenarios%20requiring%20complex%20visual-spatial%20logical%20chains%20significantly%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Video-MSR%2C%20the%20first%20benchmark%20specifically%20designed%20to%20evaluate%20Multi-hop%20Spatial%20Reasoning%20%28MSR%29%20in%20dynamic%20video%20scenarios.%20Video-MSR%20systematically%20probes%20MSR%20capabilities%20through%20four%20distinct%20tasks%3A%20Constrained%20Localization%2C%20Chain-based%20Reference%20Retrieval%2C%20Route%20Planning%2C%20and%20Counterfactual%20Physical%20Deduction.%20Our%20benchmark%20comprises%203%2C052%20high-quality%20video%20instances%20with%204%2C993%20question-answer%20pairs%2C%20constructed%20via%20a%20scalable%2C%20visually-grounded%20pipeline%20combining%20advanced%20model%20generation%20with%20rigorous%20human%20verification.%20Through%20a%20comprehensive%20evaluation%20of%2020%20state-of-the-art%20MLLMs%2C%20we%20uncover%20significant%20limitations%2C%20revealing%20that%20while%20models%20demonstrate%20proficiency%20in%20surface-level%20perception%2C%20they%20exhibit%20distinct%20performance%20drops%20in%20MSR%20tasks%2C%20frequently%20suffering%20from%20spatial%20disorientation%20and%20hallucination%20during%20multi-step%20deductions.%20To%20mitigate%20these%20shortcomings%20and%20empower%20models%20with%20stronger%20MSR%20capabilities%2C%20we%20further%20curate%20MSR-9K%2C%20a%20specialized%20instruction-tuning%20dataset%2C%20and%20fine-tune%20Qwen-VL%2C%20achieving%20a%20%2B7.82%25%20absolute%20improvement%20on%20Video-MSR.%20Our%20results%20underscore%20the%20efficacy%20of%20multi-hop%20spatial%20instruction%20data%20and%20establish%20Video-MSR%20as%20a%20vital%20foundation%20for%20future%20research.%20The%20code%20and%20data%20will%20be%20available%20at%20https%3A//github.com/ruiz-nju/Video-MSR.&entry.1838667208=http%3A//arxiv.org/abs/2601.09430v1&entry.124074799=Read"},
{"title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking", "author": "Yujin Roh and Inho Jake Park and Chigon Hwang", "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.", "link": "http://arxiv.org/abs/2512.20975v2", "date": "2026-01-14", "relevancy": 2.3127, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.594}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5692}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPOT%21%3A%20Map-Guided%20LLM%20Agent%20for%20Unsupervised%20Multi-CCTV%20Dynamic%20Object%20Tracking&body=Title%3A%20SPOT%21%3A%20Map-Guided%20LLM%20Agent%20for%20Unsupervised%20Multi-CCTV%20Dynamic%20Object%20Tracking%0AAuthor%3A%20Yujin%20Roh%20and%20Inho%20Jake%20Park%20and%20Chigon%20Hwang%0AAbstract%3A%20CCTV-based%20vehicle%20tracking%20systems%20face%20structural%20limitations%20in%20continuously%20connecting%20the%20trajectories%20of%20the%20same%20vehicle%20across%20multiple%20camera%20environments.%20In%20particular%2C%20blind%20spots%20occur%20due%20to%20the%20intervals%20between%20CCTVs%20and%20limited%20Fields%20of%20View%20%28FOV%29%2C%20which%20leads%20to%20object%20ID%20switching%20and%20trajectory%20loss%2C%20thereby%20reducing%20the%20reliability%20of%20real-time%20path%20prediction.%20This%20paper%20proposes%20SPOT%20%28Spatial%20Prediction%20Over%20Trajectories%29%2C%20a%20map-guided%20LLM%20agent%20capable%20of%20tracking%20vehicles%20even%20in%20blind%20spots%20of%20multi-CCTV%20environments%20without%20prior%20training.%20The%20proposed%20method%20represents%20road%20structures%20%28Waypoints%29%20and%20CCTV%20placement%20information%20as%20documents%20based%20on%202D%20spatial%20coordinates%20and%20organizes%20them%20through%20chunking%20techniques%20to%20enable%20real-time%20querying%20and%20inference.%20Furthermore%2C%20it%20transforms%20the%20vehicle%27s%20position%20into%20the%20actual%20world%20coordinate%20system%20using%20the%20relative%20position%20and%20FOV%20information%20of%20objects%20observed%20in%20CCTV%20images.%20By%20combining%20map%20spatial%20information%20with%20the%20vehicle%27s%20moving%20direction%2C%20speed%2C%20and%20driving%20patterns%2C%20a%20beam%20search%20is%20performed%20at%20the%20intersection%20level%20to%20derive%20candidate%20CCTV%20locations%20where%20the%20vehicle%20is%20most%20likely%20to%20enter%20after%20the%20blind%20spot.%20Experimental%20results%20based%20on%20the%20CARLA%20simulator%20in%20a%20virtual%20city%20environment%20confirmed%20that%20the%20proposed%20method%20accurately%20predicts%20the%20next%20appearing%20CCTV%20even%20in%20blind%20spot%20sections%2C%20maintaining%20continuous%20vehicle%20trajectories%20more%20effectively%20than%20existing%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2512.20975v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPOT%2521%253A%2520Map-Guided%2520LLM%2520Agent%2520for%2520Unsupervised%2520Multi-CCTV%2520Dynamic%2520Object%2520Tracking%26entry.906535625%3DYujin%2520Roh%2520and%2520Inho%2520Jake%2520Park%2520and%2520Chigon%2520Hwang%26entry.1292438233%3DCCTV-based%2520vehicle%2520tracking%2520systems%2520face%2520structural%2520limitations%2520in%2520continuously%2520connecting%2520the%2520trajectories%2520of%2520the%2520same%2520vehicle%2520across%2520multiple%2520camera%2520environments.%2520In%2520particular%252C%2520blind%2520spots%2520occur%2520due%2520to%2520the%2520intervals%2520between%2520CCTVs%2520and%2520limited%2520Fields%2520of%2520View%2520%2528FOV%2529%252C%2520which%2520leads%2520to%2520object%2520ID%2520switching%2520and%2520trajectory%2520loss%252C%2520thereby%2520reducing%2520the%2520reliability%2520of%2520real-time%2520path%2520prediction.%2520This%2520paper%2520proposes%2520SPOT%2520%2528Spatial%2520Prediction%2520Over%2520Trajectories%2529%252C%2520a%2520map-guided%2520LLM%2520agent%2520capable%2520of%2520tracking%2520vehicles%2520even%2520in%2520blind%2520spots%2520of%2520multi-CCTV%2520environments%2520without%2520prior%2520training.%2520The%2520proposed%2520method%2520represents%2520road%2520structures%2520%2528Waypoints%2529%2520and%2520CCTV%2520placement%2520information%2520as%2520documents%2520based%2520on%25202D%2520spatial%2520coordinates%2520and%2520organizes%2520them%2520through%2520chunking%2520techniques%2520to%2520enable%2520real-time%2520querying%2520and%2520inference.%2520Furthermore%252C%2520it%2520transforms%2520the%2520vehicle%2527s%2520position%2520into%2520the%2520actual%2520world%2520coordinate%2520system%2520using%2520the%2520relative%2520position%2520and%2520FOV%2520information%2520of%2520objects%2520observed%2520in%2520CCTV%2520images.%2520By%2520combining%2520map%2520spatial%2520information%2520with%2520the%2520vehicle%2527s%2520moving%2520direction%252C%2520speed%252C%2520and%2520driving%2520patterns%252C%2520a%2520beam%2520search%2520is%2520performed%2520at%2520the%2520intersection%2520level%2520to%2520derive%2520candidate%2520CCTV%2520locations%2520where%2520the%2520vehicle%2520is%2520most%2520likely%2520to%2520enter%2520after%2520the%2520blind%2520spot.%2520Experimental%2520results%2520based%2520on%2520the%2520CARLA%2520simulator%2520in%2520a%2520virtual%2520city%2520environment%2520confirmed%2520that%2520the%2520proposed%2520method%2520accurately%2520predicts%2520the%2520next%2520appearing%2520CCTV%2520even%2520in%2520blind%2520spot%2520sections%252C%2520maintaining%2520continuous%2520vehicle%2520trajectories%2520more%2520effectively%2520than%2520existing%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.20975v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOT%21%3A%20Map-Guided%20LLM%20Agent%20for%20Unsupervised%20Multi-CCTV%20Dynamic%20Object%20Tracking&entry.906535625=Yujin%20Roh%20and%20Inho%20Jake%20Park%20and%20Chigon%20Hwang&entry.1292438233=CCTV-based%20vehicle%20tracking%20systems%20face%20structural%20limitations%20in%20continuously%20connecting%20the%20trajectories%20of%20the%20same%20vehicle%20across%20multiple%20camera%20environments.%20In%20particular%2C%20blind%20spots%20occur%20due%20to%20the%20intervals%20between%20CCTVs%20and%20limited%20Fields%20of%20View%20%28FOV%29%2C%20which%20leads%20to%20object%20ID%20switching%20and%20trajectory%20loss%2C%20thereby%20reducing%20the%20reliability%20of%20real-time%20path%20prediction.%20This%20paper%20proposes%20SPOT%20%28Spatial%20Prediction%20Over%20Trajectories%29%2C%20a%20map-guided%20LLM%20agent%20capable%20of%20tracking%20vehicles%20even%20in%20blind%20spots%20of%20multi-CCTV%20environments%20without%20prior%20training.%20The%20proposed%20method%20represents%20road%20structures%20%28Waypoints%29%20and%20CCTV%20placement%20information%20as%20documents%20based%20on%202D%20spatial%20coordinates%20and%20organizes%20them%20through%20chunking%20techniques%20to%20enable%20real-time%20querying%20and%20inference.%20Furthermore%2C%20it%20transforms%20the%20vehicle%27s%20position%20into%20the%20actual%20world%20coordinate%20system%20using%20the%20relative%20position%20and%20FOV%20information%20of%20objects%20observed%20in%20CCTV%20images.%20By%20combining%20map%20spatial%20information%20with%20the%20vehicle%27s%20moving%20direction%2C%20speed%2C%20and%20driving%20patterns%2C%20a%20beam%20search%20is%20performed%20at%20the%20intersection%20level%20to%20derive%20candidate%20CCTV%20locations%20where%20the%20vehicle%20is%20most%20likely%20to%20enter%20after%20the%20blind%20spot.%20Experimental%20results%20based%20on%20the%20CARLA%20simulator%20in%20a%20virtual%20city%20environment%20confirmed%20that%20the%20proposed%20method%20accurately%20predicts%20the%20next%20appearing%20CCTV%20even%20in%20blind%20spot%20sections%2C%20maintaining%20continuous%20vehicle%20trajectories%20more%20effectively%20than%20existing%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2512.20975v2&entry.124074799=Read"},
{"title": "Enhancing Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation", "author": "Zenghao Guan and Guojun Zhu and Yucan Zhou and Wu Liu and Weiping Wang and Jiebo Luo and Xiaoyan Gu", "abstract": "Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency. The code is available at https://github.com/Yuqin-G/STSA.", "link": "http://arxiv.org/abs/2506.01327v2", "date": "2026-01-14", "relevancy": 2.3042, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4692}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4602}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Federated%20Class-Incremental%20Learning%20via%20Spatial-Temporal%20Statistics%20Aggregation&body=Title%3A%20Enhancing%20Federated%20Class-Incremental%20Learning%20via%20Spatial-Temporal%20Statistics%20Aggregation%0AAuthor%3A%20Zenghao%20Guan%20and%20Guojun%20Zhu%20and%20Yucan%20Zhou%20and%20Wu%20Liu%20and%20Weiping%20Wang%20and%20Jiebo%20Luo%20and%20Xiaoyan%20Gu%0AAbstract%3A%20Federated%20Class-Incremental%20Learning%20%28FCIL%29%20enables%20Class-Incremental%20Learning%20%28CIL%29%20from%20distributed%20data.%20Existing%20FCIL%20methods%20typically%20integrate%20old%20knowledge%20preservation%20into%20local%20client%20training.%20However%2C%20these%20methods%20cannot%20avoid%20spatial-temporal%20client%20drift%20caused%20by%20data%20heterogeneity%20and%20often%20incur%20significant%20computational%20and%20communication%20overhead%2C%20limiting%20practical%20deployment.%20To%20address%20these%20challenges%20simultaneously%2C%20we%20propose%20a%20novel%20approach%2C%20Spatial-Temporal%20Statistics%20Aggregation%20%28STSA%29%2C%20which%20provides%20a%20unified%20framework%20to%20aggregate%20feature%20statistics%20both%20spatially%20%28across%20clients%29%20and%20temporally%20%28across%20stages%29.%20The%20aggregated%20feature%20statistics%20are%20unaffected%20by%20data%20heterogeneity%20and%20can%20be%20used%20to%20update%20the%20classifier%20in%20closed%20form%20at%20each%20stage.%20Additionally%2C%20we%20introduce%20STSA-E%2C%20a%20communication-efficient%20variant%20with%20theoretical%20guarantees%2C%20achieving%20similar%20performance%20to%20STSA-E%20with%20much%20lower%20communication%20overhead.%20Extensive%20experiments%20on%20three%20widely%20used%20FCIL%20datasets%2C%20with%20varying%20degrees%20of%20data%20heterogeneity%2C%20show%20that%20our%20method%20outperforms%20state-of-the-art%20FCIL%20methods%20in%20terms%20of%20performance%2C%20flexibility%2C%20and%20both%20communication%20and%20computation%20efficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/Yuqin-G/STSA.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Federated%2520Class-Incremental%2520Learning%2520via%2520Spatial-Temporal%2520Statistics%2520Aggregation%26entry.906535625%3DZenghao%2520Guan%2520and%2520Guojun%2520Zhu%2520and%2520Yucan%2520Zhou%2520and%2520Wu%2520Liu%2520and%2520Weiping%2520Wang%2520and%2520Jiebo%2520Luo%2520and%2520Xiaoyan%2520Gu%26entry.1292438233%3DFederated%2520Class-Incremental%2520Learning%2520%2528FCIL%2529%2520enables%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520from%2520distributed%2520data.%2520Existing%2520FCIL%2520methods%2520typically%2520integrate%2520old%2520knowledge%2520preservation%2520into%2520local%2520client%2520training.%2520However%252C%2520these%2520methods%2520cannot%2520avoid%2520spatial-temporal%2520client%2520drift%2520caused%2520by%2520data%2520heterogeneity%2520and%2520often%2520incur%2520significant%2520computational%2520and%2520communication%2520overhead%252C%2520limiting%2520practical%2520deployment.%2520To%2520address%2520these%2520challenges%2520simultaneously%252C%2520we%2520propose%2520a%2520novel%2520approach%252C%2520Spatial-Temporal%2520Statistics%2520Aggregation%2520%2528STSA%2529%252C%2520which%2520provides%2520a%2520unified%2520framework%2520to%2520aggregate%2520feature%2520statistics%2520both%2520spatially%2520%2528across%2520clients%2529%2520and%2520temporally%2520%2528across%2520stages%2529.%2520The%2520aggregated%2520feature%2520statistics%2520are%2520unaffected%2520by%2520data%2520heterogeneity%2520and%2520can%2520be%2520used%2520to%2520update%2520the%2520classifier%2520in%2520closed%2520form%2520at%2520each%2520stage.%2520Additionally%252C%2520we%2520introduce%2520STSA-E%252C%2520a%2520communication-efficient%2520variant%2520with%2520theoretical%2520guarantees%252C%2520achieving%2520similar%2520performance%2520to%2520STSA-E%2520with%2520much%2520lower%2520communication%2520overhead.%2520Extensive%2520experiments%2520on%2520three%2520widely%2520used%2520FCIL%2520datasets%252C%2520with%2520varying%2520degrees%2520of%2520data%2520heterogeneity%252C%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520FCIL%2520methods%2520in%2520terms%2520of%2520performance%252C%2520flexibility%252C%2520and%2520both%2520communication%2520and%2520computation%2520efficiency.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Yuqin-G/STSA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Federated%20Class-Incremental%20Learning%20via%20Spatial-Temporal%20Statistics%20Aggregation&entry.906535625=Zenghao%20Guan%20and%20Guojun%20Zhu%20and%20Yucan%20Zhou%20and%20Wu%20Liu%20and%20Weiping%20Wang%20and%20Jiebo%20Luo%20and%20Xiaoyan%20Gu&entry.1292438233=Federated%20Class-Incremental%20Learning%20%28FCIL%29%20enables%20Class-Incremental%20Learning%20%28CIL%29%20from%20distributed%20data.%20Existing%20FCIL%20methods%20typically%20integrate%20old%20knowledge%20preservation%20into%20local%20client%20training.%20However%2C%20these%20methods%20cannot%20avoid%20spatial-temporal%20client%20drift%20caused%20by%20data%20heterogeneity%20and%20often%20incur%20significant%20computational%20and%20communication%20overhead%2C%20limiting%20practical%20deployment.%20To%20address%20these%20challenges%20simultaneously%2C%20we%20propose%20a%20novel%20approach%2C%20Spatial-Temporal%20Statistics%20Aggregation%20%28STSA%29%2C%20which%20provides%20a%20unified%20framework%20to%20aggregate%20feature%20statistics%20both%20spatially%20%28across%20clients%29%20and%20temporally%20%28across%20stages%29.%20The%20aggregated%20feature%20statistics%20are%20unaffected%20by%20data%20heterogeneity%20and%20can%20be%20used%20to%20update%20the%20classifier%20in%20closed%20form%20at%20each%20stage.%20Additionally%2C%20we%20introduce%20STSA-E%2C%20a%20communication-efficient%20variant%20with%20theoretical%20guarantees%2C%20achieving%20similar%20performance%20to%20STSA-E%20with%20much%20lower%20communication%20overhead.%20Extensive%20experiments%20on%20three%20widely%20used%20FCIL%20datasets%2C%20with%20varying%20degrees%20of%20data%20heterogeneity%2C%20show%20that%20our%20method%20outperforms%20state-of-the-art%20FCIL%20methods%20in%20terms%20of%20performance%2C%20flexibility%2C%20and%20both%20communication%20and%20computation%20efficiency.%20The%20code%20is%20available%20at%20https%3A//github.com/Yuqin-G/STSA.&entry.1838667208=http%3A//arxiv.org/abs/2506.01327v2&entry.124074799=Read"},
{"title": "Data Scaling for Navigation in Unknown Environments", "author": "Lauri Suomela and Naoki Takahata and Sasanka Kuruppu Arachchige and Harry Edelman and Joni-Kristian K\u00e4m\u00e4r\u00e4inen", "abstract": "Generalization of imitation-learned navigation policies to environments unseen in training remains a major challenge. We address this by conducting the first large-scale study of how data quantity and data diversity affect real-world generalization in end-to-end, map-free visual navigation. Using a curated 4,565-hour crowd-sourced dataset collected across 161 locations in 35 countries, we train policies for point goal navigation and evaluate their closed-loop control performance on sidewalk robots operating in four countries, covering 125 km of autonomous driving.\n  Our results show that large-scale training data enables zero-shot navigation in unknown environments, approaching the performance of policies trained with environment-specific demonstrations. Critically, we find that data diversity is far more important than data quantity. Doubling the number of geographical locations in a training set decreases navigation errors by ~15%, while performance benefit from adding data from existing locations saturates with very little data. We also observe that, with noisy crowd-sourced data, simple regression-based models outperform generative and sequence-based architectures. We release our policies, evaluation setup and example videos on the project page.", "link": "http://arxiv.org/abs/2601.09444v1", "date": "2026-01-14", "relevancy": 2.3033, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5917}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5712}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Scaling%20for%20Navigation%20in%20Unknown%20Environments&body=Title%3A%20Data%20Scaling%20for%20Navigation%20in%20Unknown%20Environments%0AAuthor%3A%20Lauri%20Suomela%20and%20Naoki%20Takahata%20and%20Sasanka%20Kuruppu%20Arachchige%20and%20Harry%20Edelman%20and%20Joni-Kristian%20K%C3%A4m%C3%A4r%C3%A4inen%0AAbstract%3A%20Generalization%20of%20imitation-learned%20navigation%20policies%20to%20environments%20unseen%20in%20training%20remains%20a%20major%20challenge.%20We%20address%20this%20by%20conducting%20the%20first%20large-scale%20study%20of%20how%20data%20quantity%20and%20data%20diversity%20affect%20real-world%20generalization%20in%20end-to-end%2C%20map-free%20visual%20navigation.%20Using%20a%20curated%204%2C565-hour%20crowd-sourced%20dataset%20collected%20across%20161%20locations%20in%2035%20countries%2C%20we%20train%20policies%20for%20point%20goal%20navigation%20and%20evaluate%20their%20closed-loop%20control%20performance%20on%20sidewalk%20robots%20operating%20in%20four%20countries%2C%20covering%20125%20km%20of%20autonomous%20driving.%0A%20%20Our%20results%20show%20that%20large-scale%20training%20data%20enables%20zero-shot%20navigation%20in%20unknown%20environments%2C%20approaching%20the%20performance%20of%20policies%20trained%20with%20environment-specific%20demonstrations.%20Critically%2C%20we%20find%20that%20data%20diversity%20is%20far%20more%20important%20than%20data%20quantity.%20Doubling%20the%20number%20of%20geographical%20locations%20in%20a%20training%20set%20decreases%20navigation%20errors%20by%20~15%25%2C%20while%20performance%20benefit%20from%20adding%20data%20from%20existing%20locations%20saturates%20with%20very%20little%20data.%20We%20also%20observe%20that%2C%20with%20noisy%20crowd-sourced%20data%2C%20simple%20regression-based%20models%20outperform%20generative%20and%20sequence-based%20architectures.%20We%20release%20our%20policies%2C%20evaluation%20setup%20and%20example%20videos%20on%20the%20project%20page.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Scaling%2520for%2520Navigation%2520in%2520Unknown%2520Environments%26entry.906535625%3DLauri%2520Suomela%2520and%2520Naoki%2520Takahata%2520and%2520Sasanka%2520Kuruppu%2520Arachchige%2520and%2520Harry%2520Edelman%2520and%2520Joni-Kristian%2520K%25C3%25A4m%25C3%25A4r%25C3%25A4inen%26entry.1292438233%3DGeneralization%2520of%2520imitation-learned%2520navigation%2520policies%2520to%2520environments%2520unseen%2520in%2520training%2520remains%2520a%2520major%2520challenge.%2520We%2520address%2520this%2520by%2520conducting%2520the%2520first%2520large-scale%2520study%2520of%2520how%2520data%2520quantity%2520and%2520data%2520diversity%2520affect%2520real-world%2520generalization%2520in%2520end-to-end%252C%2520map-free%2520visual%2520navigation.%2520Using%2520a%2520curated%25204%252C565-hour%2520crowd-sourced%2520dataset%2520collected%2520across%2520161%2520locations%2520in%252035%2520countries%252C%2520we%2520train%2520policies%2520for%2520point%2520goal%2520navigation%2520and%2520evaluate%2520their%2520closed-loop%2520control%2520performance%2520on%2520sidewalk%2520robots%2520operating%2520in%2520four%2520countries%252C%2520covering%2520125%2520km%2520of%2520autonomous%2520driving.%250A%2520%2520Our%2520results%2520show%2520that%2520large-scale%2520training%2520data%2520enables%2520zero-shot%2520navigation%2520in%2520unknown%2520environments%252C%2520approaching%2520the%2520performance%2520of%2520policies%2520trained%2520with%2520environment-specific%2520demonstrations.%2520Critically%252C%2520we%2520find%2520that%2520data%2520diversity%2520is%2520far%2520more%2520important%2520than%2520data%2520quantity.%2520Doubling%2520the%2520number%2520of%2520geographical%2520locations%2520in%2520a%2520training%2520set%2520decreases%2520navigation%2520errors%2520by%2520~15%2525%252C%2520while%2520performance%2520benefit%2520from%2520adding%2520data%2520from%2520existing%2520locations%2520saturates%2520with%2520very%2520little%2520data.%2520We%2520also%2520observe%2520that%252C%2520with%2520noisy%2520crowd-sourced%2520data%252C%2520simple%2520regression-based%2520models%2520outperform%2520generative%2520and%2520sequence-based%2520architectures.%2520We%2520release%2520our%2520policies%252C%2520evaluation%2520setup%2520and%2520example%2520videos%2520on%2520the%2520project%2520page.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Scaling%20for%20Navigation%20in%20Unknown%20Environments&entry.906535625=Lauri%20Suomela%20and%20Naoki%20Takahata%20and%20Sasanka%20Kuruppu%20Arachchige%20and%20Harry%20Edelman%20and%20Joni-Kristian%20K%C3%A4m%C3%A4r%C3%A4inen&entry.1292438233=Generalization%20of%20imitation-learned%20navigation%20policies%20to%20environments%20unseen%20in%20training%20remains%20a%20major%20challenge.%20We%20address%20this%20by%20conducting%20the%20first%20large-scale%20study%20of%20how%20data%20quantity%20and%20data%20diversity%20affect%20real-world%20generalization%20in%20end-to-end%2C%20map-free%20visual%20navigation.%20Using%20a%20curated%204%2C565-hour%20crowd-sourced%20dataset%20collected%20across%20161%20locations%20in%2035%20countries%2C%20we%20train%20policies%20for%20point%20goal%20navigation%20and%20evaluate%20their%20closed-loop%20control%20performance%20on%20sidewalk%20robots%20operating%20in%20four%20countries%2C%20covering%20125%20km%20of%20autonomous%20driving.%0A%20%20Our%20results%20show%20that%20large-scale%20training%20data%20enables%20zero-shot%20navigation%20in%20unknown%20environments%2C%20approaching%20the%20performance%20of%20policies%20trained%20with%20environment-specific%20demonstrations.%20Critically%2C%20we%20find%20that%20data%20diversity%20is%20far%20more%20important%20than%20data%20quantity.%20Doubling%20the%20number%20of%20geographical%20locations%20in%20a%20training%20set%20decreases%20navigation%20errors%20by%20~15%25%2C%20while%20performance%20benefit%20from%20adding%20data%20from%20existing%20locations%20saturates%20with%20very%20little%20data.%20We%20also%20observe%20that%2C%20with%20noisy%20crowd-sourced%20data%2C%20simple%20regression-based%20models%20outperform%20generative%20and%20sequence-based%20architectures.%20We%20release%20our%20policies%2C%20evaluation%20setup%20and%20example%20videos%20on%20the%20project%20page.&entry.1838667208=http%3A//arxiv.org/abs/2601.09444v1&entry.124074799=Read"},
{"title": "FairGU: Fairness-aware Graph Unlearning in Social Network", "author": "Renqiang Luo and Yongshuai Yang and Huafei Huang and Qing Qing and Mingliang Hou and Ziqi Xu and Yi Yu and Jingjing Zhou and Feng Xia", "abstract": "Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, we observe that existing graph unlearning techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this gap, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems. The codes are available at https://github.com/LuoRenqiang/FairGU.", "link": "http://arxiv.org/abs/2601.09469v1", "date": "2026-01-14", "relevancy": 2.2811, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4471}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairGU%3A%20Fairness-aware%20Graph%20Unlearning%20in%20Social%20Network&body=Title%3A%20FairGU%3A%20Fairness-aware%20Graph%20Unlearning%20in%20Social%20Network%0AAuthor%3A%20Renqiang%20Luo%20and%20Yongshuai%20Yang%20and%20Huafei%20Huang%20and%20Qing%20Qing%20and%20Mingliang%20Hou%20and%20Ziqi%20Xu%20and%20Yi%20Yu%20and%20Jingjing%20Zhou%20and%20Feng%20Xia%0AAbstract%3A%20Graph%20unlearning%20has%20emerged%20as%20a%20critical%20mechanism%20for%20supporting%20sustainable%20and%20privacy-preserving%20social%20networks%2C%20enabling%20models%20to%20remove%20the%20influence%20of%20deleted%20nodes%20and%20thereby%20better%20safeguard%20user%20information.%20However%2C%20we%20observe%20that%20existing%20graph%20unlearning%20techniques%20insufficiently%20protect%20sensitive%20attributes%2C%20often%20leading%20to%20degraded%20algorithmic%20fairness%20compared%20with%20traditional%20graph%20learning%20methods.%20To%20address%20this%20gap%2C%20we%20introduce%20FairGU%2C%20a%20fairness-aware%20graph%20unlearning%20framework%20designed%20to%20preserve%20both%20utility%20and%20fairness%20during%20the%20unlearning%20process.%20FairGU%20integrates%20a%20dedicated%20fairness-aware%20module%20with%20effective%20data%20protection%20strategies%2C%20ensuring%20that%20sensitive%20attributes%20are%20neither%20inadvertently%20amplified%20nor%20structurally%20exposed%20when%20nodes%20are%20removed.%20Through%20extensive%20experiments%20on%20multiple%20real-world%20datasets%2C%20we%20demonstrate%20that%20FairGU%20consistently%20outperforms%20state-of-the-art%20graph%20unlearning%20methods%20and%20fairness-enhanced%20graph%20learning%20baselines%20in%20terms%20of%20both%20accuracy%20and%20fairness%20metrics.%20Our%20findings%20highlight%20a%20previously%20overlooked%20risk%20in%20current%20unlearning%20practices%20and%20establish%20FairGU%20as%20a%20robust%20and%20equitable%20solution%20for%20the%20next%20generation%20of%20socially%20sustainable%20networked%20systems.%20The%20codes%20are%20available%20at%20https%3A//github.com/LuoRenqiang/FairGU.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairGU%253A%2520Fairness-aware%2520Graph%2520Unlearning%2520in%2520Social%2520Network%26entry.906535625%3DRenqiang%2520Luo%2520and%2520Yongshuai%2520Yang%2520and%2520Huafei%2520Huang%2520and%2520Qing%2520Qing%2520and%2520Mingliang%2520Hou%2520and%2520Ziqi%2520Xu%2520and%2520Yi%2520Yu%2520and%2520Jingjing%2520Zhou%2520and%2520Feng%2520Xia%26entry.1292438233%3DGraph%2520unlearning%2520has%2520emerged%2520as%2520a%2520critical%2520mechanism%2520for%2520supporting%2520sustainable%2520and%2520privacy-preserving%2520social%2520networks%252C%2520enabling%2520models%2520to%2520remove%2520the%2520influence%2520of%2520deleted%2520nodes%2520and%2520thereby%2520better%2520safeguard%2520user%2520information.%2520However%252C%2520we%2520observe%2520that%2520existing%2520graph%2520unlearning%2520techniques%2520insufficiently%2520protect%2520sensitive%2520attributes%252C%2520often%2520leading%2520to%2520degraded%2520algorithmic%2520fairness%2520compared%2520with%2520traditional%2520graph%2520learning%2520methods.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520FairGU%252C%2520a%2520fairness-aware%2520graph%2520unlearning%2520framework%2520designed%2520to%2520preserve%2520both%2520utility%2520and%2520fairness%2520during%2520the%2520unlearning%2520process.%2520FairGU%2520integrates%2520a%2520dedicated%2520fairness-aware%2520module%2520with%2520effective%2520data%2520protection%2520strategies%252C%2520ensuring%2520that%2520sensitive%2520attributes%2520are%2520neither%2520inadvertently%2520amplified%2520nor%2520structurally%2520exposed%2520when%2520nodes%2520are%2520removed.%2520Through%2520extensive%2520experiments%2520on%2520multiple%2520real-world%2520datasets%252C%2520we%2520demonstrate%2520that%2520FairGU%2520consistently%2520outperforms%2520state-of-the-art%2520graph%2520unlearning%2520methods%2520and%2520fairness-enhanced%2520graph%2520learning%2520baselines%2520in%2520terms%2520of%2520both%2520accuracy%2520and%2520fairness%2520metrics.%2520Our%2520findings%2520highlight%2520a%2520previously%2520overlooked%2520risk%2520in%2520current%2520unlearning%2520practices%2520and%2520establish%2520FairGU%2520as%2520a%2520robust%2520and%2520equitable%2520solution%2520for%2520the%2520next%2520generation%2520of%2520socially%2520sustainable%2520networked%2520systems.%2520The%2520codes%2520are%2520available%2520at%2520https%253A//github.com/LuoRenqiang/FairGU.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairGU%3A%20Fairness-aware%20Graph%20Unlearning%20in%20Social%20Network&entry.906535625=Renqiang%20Luo%20and%20Yongshuai%20Yang%20and%20Huafei%20Huang%20and%20Qing%20Qing%20and%20Mingliang%20Hou%20and%20Ziqi%20Xu%20and%20Yi%20Yu%20and%20Jingjing%20Zhou%20and%20Feng%20Xia&entry.1292438233=Graph%20unlearning%20has%20emerged%20as%20a%20critical%20mechanism%20for%20supporting%20sustainable%20and%20privacy-preserving%20social%20networks%2C%20enabling%20models%20to%20remove%20the%20influence%20of%20deleted%20nodes%20and%20thereby%20better%20safeguard%20user%20information.%20However%2C%20we%20observe%20that%20existing%20graph%20unlearning%20techniques%20insufficiently%20protect%20sensitive%20attributes%2C%20often%20leading%20to%20degraded%20algorithmic%20fairness%20compared%20with%20traditional%20graph%20learning%20methods.%20To%20address%20this%20gap%2C%20we%20introduce%20FairGU%2C%20a%20fairness-aware%20graph%20unlearning%20framework%20designed%20to%20preserve%20both%20utility%20and%20fairness%20during%20the%20unlearning%20process.%20FairGU%20integrates%20a%20dedicated%20fairness-aware%20module%20with%20effective%20data%20protection%20strategies%2C%20ensuring%20that%20sensitive%20attributes%20are%20neither%20inadvertently%20amplified%20nor%20structurally%20exposed%20when%20nodes%20are%20removed.%20Through%20extensive%20experiments%20on%20multiple%20real-world%20datasets%2C%20we%20demonstrate%20that%20FairGU%20consistently%20outperforms%20state-of-the-art%20graph%20unlearning%20methods%20and%20fairness-enhanced%20graph%20learning%20baselines%20in%20terms%20of%20both%20accuracy%20and%20fairness%20metrics.%20Our%20findings%20highlight%20a%20previously%20overlooked%20risk%20in%20current%20unlearning%20practices%20and%20establish%20FairGU%20as%20a%20robust%20and%20equitable%20solution%20for%20the%20next%20generation%20of%20socially%20sustainable%20networked%20systems.%20The%20codes%20are%20available%20at%20https%3A//github.com/LuoRenqiang/FairGU.&entry.1838667208=http%3A//arxiv.org/abs/2601.09469v1&entry.124074799=Read"},
{"title": "COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation", "author": "Tony Danjun Wang and Tolga Birdal and Nassir Navab and Lennart Bastian", "abstract": "3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.", "link": "http://arxiv.org/abs/2601.09698v1", "date": "2026-01-14", "relevancy": 2.2791, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5818}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.562}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMPOSE%3A%20Hypergraph%20Cover%20Optimization%20for%20Multi-view%203D%20Human%20Pose%20Estimation&body=Title%3A%20COMPOSE%3A%20Hypergraph%20Cover%20Optimization%20for%20Multi-view%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Tony%20Danjun%20Wang%20and%20Tolga%20Birdal%20and%20Nassir%20Navab%20and%20Lennart%20Bastian%0AAbstract%3A%203D%20pose%20estimation%20from%20sparse%20multi-views%20is%20a%20critical%20task%20for%20numerous%20applications%2C%20including%20action%20recognition%2C%20sports%20analysis%2C%20and%20human-robot%20interaction.%20Optimization-based%20methods%20typically%20follow%20a%20two-stage%20pipeline%2C%20first%20detecting%202D%20keypoints%20in%20each%20view%20and%20then%20associating%20these%20detections%20across%20views%20to%20triangulate%20the%203D%20pose.%20Existing%20methods%20rely%20on%20mere%20pairwise%20associations%20to%20model%20this%20correspondence%20problem%2C%20treating%20global%20consistency%20between%20views%20%28i.e.%2C%20cycle%20consistency%29%20as%20a%20soft%20constraint.%20Yet%2C%20reconciling%20these%20constraints%20for%20multiple%20views%20becomes%20brittle%20when%20spurious%20associations%20propagate%20errors.%20We%20thus%20propose%20COMPOSE%2C%20a%20novel%20framework%20that%20formulates%20multi-view%20pose%20correspondence%20matching%20as%20a%20hypergraph%20partitioning%20problem%20rather%20than%20through%20pairwise%20association.%20While%20the%20complexity%20of%20the%20resulting%20integer%20linear%20program%20grows%20exponentially%20in%20theory%2C%20we%20introduce%20an%20efficient%20geometric%20pruning%20strategy%20to%20substantially%20reduce%20the%20search%20space.%20COMPOSE%20achieves%20improvements%20of%20up%20to%2023%25%20in%20average%20precision%20over%20previous%20optimization-based%20methods%20and%20up%20to%2011%25%20over%20self-supervised%20end-to-end%20learned%20methods%2C%20offering%20a%20promising%20solution%20to%20a%20widely%20studied%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMPOSE%253A%2520Hypergraph%2520Cover%2520Optimization%2520for%2520Multi-view%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DTony%2520Danjun%2520Wang%2520and%2520Tolga%2520Birdal%2520and%2520Nassir%2520Navab%2520and%2520Lennart%2520Bastian%26entry.1292438233%3D3D%2520pose%2520estimation%2520from%2520sparse%2520multi-views%2520is%2520a%2520critical%2520task%2520for%2520numerous%2520applications%252C%2520including%2520action%2520recognition%252C%2520sports%2520analysis%252C%2520and%2520human-robot%2520interaction.%2520Optimization-based%2520methods%2520typically%2520follow%2520a%2520two-stage%2520pipeline%252C%2520first%2520detecting%25202D%2520keypoints%2520in%2520each%2520view%2520and%2520then%2520associating%2520these%2520detections%2520across%2520views%2520to%2520triangulate%2520the%25203D%2520pose.%2520Existing%2520methods%2520rely%2520on%2520mere%2520pairwise%2520associations%2520to%2520model%2520this%2520correspondence%2520problem%252C%2520treating%2520global%2520consistency%2520between%2520views%2520%2528i.e.%252C%2520cycle%2520consistency%2529%2520as%2520a%2520soft%2520constraint.%2520Yet%252C%2520reconciling%2520these%2520constraints%2520for%2520multiple%2520views%2520becomes%2520brittle%2520when%2520spurious%2520associations%2520propagate%2520errors.%2520We%2520thus%2520propose%2520COMPOSE%252C%2520a%2520novel%2520framework%2520that%2520formulates%2520multi-view%2520pose%2520correspondence%2520matching%2520as%2520a%2520hypergraph%2520partitioning%2520problem%2520rather%2520than%2520through%2520pairwise%2520association.%2520While%2520the%2520complexity%2520of%2520the%2520resulting%2520integer%2520linear%2520program%2520grows%2520exponentially%2520in%2520theory%252C%2520we%2520introduce%2520an%2520efficient%2520geometric%2520pruning%2520strategy%2520to%2520substantially%2520reduce%2520the%2520search%2520space.%2520COMPOSE%2520achieves%2520improvements%2520of%2520up%2520to%252023%2525%2520in%2520average%2520precision%2520over%2520previous%2520optimization-based%2520methods%2520and%2520up%2520to%252011%2525%2520over%2520self-supervised%2520end-to-end%2520learned%2520methods%252C%2520offering%2520a%2520promising%2520solution%2520to%2520a%2520widely%2520studied%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMPOSE%3A%20Hypergraph%20Cover%20Optimization%20for%20Multi-view%203D%20Human%20Pose%20Estimation&entry.906535625=Tony%20Danjun%20Wang%20and%20Tolga%20Birdal%20and%20Nassir%20Navab%20and%20Lennart%20Bastian&entry.1292438233=3D%20pose%20estimation%20from%20sparse%20multi-views%20is%20a%20critical%20task%20for%20numerous%20applications%2C%20including%20action%20recognition%2C%20sports%20analysis%2C%20and%20human-robot%20interaction.%20Optimization-based%20methods%20typically%20follow%20a%20two-stage%20pipeline%2C%20first%20detecting%202D%20keypoints%20in%20each%20view%20and%20then%20associating%20these%20detections%20across%20views%20to%20triangulate%20the%203D%20pose.%20Existing%20methods%20rely%20on%20mere%20pairwise%20associations%20to%20model%20this%20correspondence%20problem%2C%20treating%20global%20consistency%20between%20views%20%28i.e.%2C%20cycle%20consistency%29%20as%20a%20soft%20constraint.%20Yet%2C%20reconciling%20these%20constraints%20for%20multiple%20views%20becomes%20brittle%20when%20spurious%20associations%20propagate%20errors.%20We%20thus%20propose%20COMPOSE%2C%20a%20novel%20framework%20that%20formulates%20multi-view%20pose%20correspondence%20matching%20as%20a%20hypergraph%20partitioning%20problem%20rather%20than%20through%20pairwise%20association.%20While%20the%20complexity%20of%20the%20resulting%20integer%20linear%20program%20grows%20exponentially%20in%20theory%2C%20we%20introduce%20an%20efficient%20geometric%20pruning%20strategy%20to%20substantially%20reduce%20the%20search%20space.%20COMPOSE%20achieves%20improvements%20of%20up%20to%2023%25%20in%20average%20precision%20over%20previous%20optimization-based%20methods%20and%20up%20to%2011%25%20over%20self-supervised%20end-to-end%20learned%20methods%2C%20offering%20a%20promising%20solution%20to%20a%20widely%20studied%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2601.09698v1&entry.124074799=Read"},
{"title": "Spectral Complex Autoencoder Pruning: A Fidelity-Guided Criterion for Extreme Structured Channel Compression", "author": "Wei Liu and Xing Deng and Haijian Shao and Yingtao Jiang", "abstract": "We propose Spectral Complex Autoencoder Pruning (SCAP), a reconstruction-based criterion that measures functional redundancy at the level of individual output channels. For each convolutional layer, we construct a complex interaction field by pairing the full multi-channel input activation as the real part with a single output-channel activation (spatially aligned and broadcast across input channels) as the imaginary part. We transform this complex field to the frequency domain and train a low-capacity autoencoder to reconstruct normalized spectra. Channels whose spectra are reconstructed with high fidelity are interpreted as lying close to a low-dimensional manifold captured by the autoencoder and are therefore more compressible; conversely, channels with low fidelity are retained as they encode information that cannot be compactly represented by the learned manifold. This yields an importance score (optionally fused with the filter L1 norm) that supports simple threshold-based pruning and produces a structurally consistent pruned network. On VGG16 trained on CIFAR-10, at a fixed threshold of 0.6, we obtain 90.11% FLOP reduction and 96.30% parameter reduction with an absolute Top-1 accuracy drop of 1.67% from a 93.44% baseline after fine-tuning, demonstrating that spectral reconstruction fidelity of complex interaction fields is an effective proxy for channel-level redundancy under aggressive compression.", "link": "http://arxiv.org/abs/2601.09352v1", "date": "2026-01-14", "relevancy": 2.2757, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4773}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Complex%20Autoencoder%20Pruning%3A%20A%20Fidelity-Guided%20Criterion%20for%20Extreme%20Structured%20Channel%20Compression&body=Title%3A%20Spectral%20Complex%20Autoencoder%20Pruning%3A%20A%20Fidelity-Guided%20Criterion%20for%20Extreme%20Structured%20Channel%20Compression%0AAuthor%3A%20Wei%20Liu%20and%20Xing%20Deng%20and%20Haijian%20Shao%20and%20Yingtao%20Jiang%0AAbstract%3A%20We%20propose%20Spectral%20Complex%20Autoencoder%20Pruning%20%28SCAP%29%2C%20a%20reconstruction-based%20criterion%20that%20measures%20functional%20redundancy%20at%20the%20level%20of%20individual%20output%20channels.%20For%20each%20convolutional%20layer%2C%20we%20construct%20a%20complex%20interaction%20field%20by%20pairing%20the%20full%20multi-channel%20input%20activation%20as%20the%20real%20part%20with%20a%20single%20output-channel%20activation%20%28spatially%20aligned%20and%20broadcast%20across%20input%20channels%29%20as%20the%20imaginary%20part.%20We%20transform%20this%20complex%20field%20to%20the%20frequency%20domain%20and%20train%20a%20low-capacity%20autoencoder%20to%20reconstruct%20normalized%20spectra.%20Channels%20whose%20spectra%20are%20reconstructed%20with%20high%20fidelity%20are%20interpreted%20as%20lying%20close%20to%20a%20low-dimensional%20manifold%20captured%20by%20the%20autoencoder%20and%20are%20therefore%20more%20compressible%3B%20conversely%2C%20channels%20with%20low%20fidelity%20are%20retained%20as%20they%20encode%20information%20that%20cannot%20be%20compactly%20represented%20by%20the%20learned%20manifold.%20This%20yields%20an%20importance%20score%20%28optionally%20fused%20with%20the%20filter%20L1%20norm%29%20that%20supports%20simple%20threshold-based%20pruning%20and%20produces%20a%20structurally%20consistent%20pruned%20network.%20On%20VGG16%20trained%20on%20CIFAR-10%2C%20at%20a%20fixed%20threshold%20of%200.6%2C%20we%20obtain%2090.11%25%20FLOP%20reduction%20and%2096.30%25%20parameter%20reduction%20with%20an%20absolute%20Top-1%20accuracy%20drop%20of%201.67%25%20from%20a%2093.44%25%20baseline%20after%20fine-tuning%2C%20demonstrating%20that%20spectral%20reconstruction%20fidelity%20of%20complex%20interaction%20fields%20is%20an%20effective%20proxy%20for%20channel-level%20redundancy%20under%20aggressive%20compression.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Complex%2520Autoencoder%2520Pruning%253A%2520A%2520Fidelity-Guided%2520Criterion%2520for%2520Extreme%2520Structured%2520Channel%2520Compression%26entry.906535625%3DWei%2520Liu%2520and%2520Xing%2520Deng%2520and%2520Haijian%2520Shao%2520and%2520Yingtao%2520Jiang%26entry.1292438233%3DWe%2520propose%2520Spectral%2520Complex%2520Autoencoder%2520Pruning%2520%2528SCAP%2529%252C%2520a%2520reconstruction-based%2520criterion%2520that%2520measures%2520functional%2520redundancy%2520at%2520the%2520level%2520of%2520individual%2520output%2520channels.%2520For%2520each%2520convolutional%2520layer%252C%2520we%2520construct%2520a%2520complex%2520interaction%2520field%2520by%2520pairing%2520the%2520full%2520multi-channel%2520input%2520activation%2520as%2520the%2520real%2520part%2520with%2520a%2520single%2520output-channel%2520activation%2520%2528spatially%2520aligned%2520and%2520broadcast%2520across%2520input%2520channels%2529%2520as%2520the%2520imaginary%2520part.%2520We%2520transform%2520this%2520complex%2520field%2520to%2520the%2520frequency%2520domain%2520and%2520train%2520a%2520low-capacity%2520autoencoder%2520to%2520reconstruct%2520normalized%2520spectra.%2520Channels%2520whose%2520spectra%2520are%2520reconstructed%2520with%2520high%2520fidelity%2520are%2520interpreted%2520as%2520lying%2520close%2520to%2520a%2520low-dimensional%2520manifold%2520captured%2520by%2520the%2520autoencoder%2520and%2520are%2520therefore%2520more%2520compressible%253B%2520conversely%252C%2520channels%2520with%2520low%2520fidelity%2520are%2520retained%2520as%2520they%2520encode%2520information%2520that%2520cannot%2520be%2520compactly%2520represented%2520by%2520the%2520learned%2520manifold.%2520This%2520yields%2520an%2520importance%2520score%2520%2528optionally%2520fused%2520with%2520the%2520filter%2520L1%2520norm%2529%2520that%2520supports%2520simple%2520threshold-based%2520pruning%2520and%2520produces%2520a%2520structurally%2520consistent%2520pruned%2520network.%2520On%2520VGG16%2520trained%2520on%2520CIFAR-10%252C%2520at%2520a%2520fixed%2520threshold%2520of%25200.6%252C%2520we%2520obtain%252090.11%2525%2520FLOP%2520reduction%2520and%252096.30%2525%2520parameter%2520reduction%2520with%2520an%2520absolute%2520Top-1%2520accuracy%2520drop%2520of%25201.67%2525%2520from%2520a%252093.44%2525%2520baseline%2520after%2520fine-tuning%252C%2520demonstrating%2520that%2520spectral%2520reconstruction%2520fidelity%2520of%2520complex%2520interaction%2520fields%2520is%2520an%2520effective%2520proxy%2520for%2520channel-level%2520redundancy%2520under%2520aggressive%2520compression.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Complex%20Autoencoder%20Pruning%3A%20A%20Fidelity-Guided%20Criterion%20for%20Extreme%20Structured%20Channel%20Compression&entry.906535625=Wei%20Liu%20and%20Xing%20Deng%20and%20Haijian%20Shao%20and%20Yingtao%20Jiang&entry.1292438233=We%20propose%20Spectral%20Complex%20Autoencoder%20Pruning%20%28SCAP%29%2C%20a%20reconstruction-based%20criterion%20that%20measures%20functional%20redundancy%20at%20the%20level%20of%20individual%20output%20channels.%20For%20each%20convolutional%20layer%2C%20we%20construct%20a%20complex%20interaction%20field%20by%20pairing%20the%20full%20multi-channel%20input%20activation%20as%20the%20real%20part%20with%20a%20single%20output-channel%20activation%20%28spatially%20aligned%20and%20broadcast%20across%20input%20channels%29%20as%20the%20imaginary%20part.%20We%20transform%20this%20complex%20field%20to%20the%20frequency%20domain%20and%20train%20a%20low-capacity%20autoencoder%20to%20reconstruct%20normalized%20spectra.%20Channels%20whose%20spectra%20are%20reconstructed%20with%20high%20fidelity%20are%20interpreted%20as%20lying%20close%20to%20a%20low-dimensional%20manifold%20captured%20by%20the%20autoencoder%20and%20are%20therefore%20more%20compressible%3B%20conversely%2C%20channels%20with%20low%20fidelity%20are%20retained%20as%20they%20encode%20information%20that%20cannot%20be%20compactly%20represented%20by%20the%20learned%20manifold.%20This%20yields%20an%20importance%20score%20%28optionally%20fused%20with%20the%20filter%20L1%20norm%29%20that%20supports%20simple%20threshold-based%20pruning%20and%20produces%20a%20structurally%20consistent%20pruned%20network.%20On%20VGG16%20trained%20on%20CIFAR-10%2C%20at%20a%20fixed%20threshold%20of%200.6%2C%20we%20obtain%2090.11%25%20FLOP%20reduction%20and%2096.30%25%20parameter%20reduction%20with%20an%20absolute%20Top-1%20accuracy%20drop%20of%201.67%25%20from%20a%2093.44%25%20baseline%20after%20fine-tuning%2C%20demonstrating%20that%20spectral%20reconstruction%20fidelity%20of%20complex%20interaction%20fields%20is%20an%20effective%20proxy%20for%20channel-level%20redundancy%20under%20aggressive%20compression.&entry.1838667208=http%3A//arxiv.org/abs/2601.09352v1&entry.124074799=Read"},
{"title": "Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery", "author": "Jiayin Liu and Yulong Yang and Vineet Bansal and Christine Allen-Blanchette", "abstract": "From metronomes to celestial bodies, mechanics underpins how the world evolves in time and space. With consideration of this, a number of recent neural network models leverage inductive biases from classical mechanics to encourage model interpretability and ensure forecasted states are physical. However, in general, these models are designed to capture the dynamics of a single system with fixed physical parameters, from state-space measurements of a known configuration space. In this paper we introduce Symplectic Phase Space GAN (SPS-GAN) which can capture the dynamics of multiple systems, and generalize to unseen physical parameters from. Moreover, SPS-GAN does not require prior knowledge of the system configuration space. In fact, SPS-GAN can discover the configuration space structure of the system from arbitrary measurement types (e.g., state-space measurements, video frames). To achieve physically plausible generation, we introduce a novel architecture which embeds a Hamiltonian neural network recurrent module in a conditional GAN backbone. To discover the structure of the configuration space, we optimize the conditional time-series GAN objective with an additional physically motivated term to encourages a sparse representation of the configuration space. We demonstrate the utility of SPS-GAN for trajectory prediction, video generation and symmetry discovery. Our approach captures multiple systems and achieves performance on par with supervised models designed for single systems.", "link": "http://arxiv.org/abs/2509.23003v2", "date": "2026-01-14", "relevancy": 2.2737, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5916}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5712}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Plausible%20Multi-System%20Trajectory%20Generation%20and%20Symmetry%20Discovery&body=Title%3A%20Physically%20Plausible%20Multi-System%20Trajectory%20Generation%20and%20Symmetry%20Discovery%0AAuthor%3A%20Jiayin%20Liu%20and%20Yulong%20Yang%20and%20Vineet%20Bansal%20and%20Christine%20Allen-Blanchette%0AAbstract%3A%20From%20metronomes%20to%20celestial%20bodies%2C%20mechanics%20underpins%20how%20the%20world%20evolves%20in%20time%20and%20space.%20With%20consideration%20of%20this%2C%20a%20number%20of%20recent%20neural%20network%20models%20leverage%20inductive%20biases%20from%20classical%20mechanics%20to%20encourage%20model%20interpretability%20and%20ensure%20forecasted%20states%20are%20physical.%20However%2C%20in%20general%2C%20these%20models%20are%20designed%20to%20capture%20the%20dynamics%20of%20a%20single%20system%20with%20fixed%20physical%20parameters%2C%20from%20state-space%20measurements%20of%20a%20known%20configuration%20space.%20In%20this%20paper%20we%20introduce%20Symplectic%20Phase%20Space%20GAN%20%28SPS-GAN%29%20which%20can%20capture%20the%20dynamics%20of%20multiple%20systems%2C%20and%20generalize%20to%20unseen%20physical%20parameters%20from.%20Moreover%2C%20SPS-GAN%20does%20not%20require%20prior%20knowledge%20of%20the%20system%20configuration%20space.%20In%20fact%2C%20SPS-GAN%20can%20discover%20the%20configuration%20space%20structure%20of%20the%20system%20from%20arbitrary%20measurement%20types%20%28e.g.%2C%20state-space%20measurements%2C%20video%20frames%29.%20To%20achieve%20physically%20plausible%20generation%2C%20we%20introduce%20a%20novel%20architecture%20which%20embeds%20a%20Hamiltonian%20neural%20network%20recurrent%20module%20in%20a%20conditional%20GAN%20backbone.%20To%20discover%20the%20structure%20of%20the%20configuration%20space%2C%20we%20optimize%20the%20conditional%20time-series%20GAN%20objective%20with%20an%20additional%20physically%20motivated%20term%20to%20encourages%20a%20sparse%20representation%20of%20the%20configuration%20space.%20We%20demonstrate%20the%20utility%20of%20SPS-GAN%20for%20trajectory%20prediction%2C%20video%20generation%20and%20symmetry%20discovery.%20Our%20approach%20captures%20multiple%20systems%20and%20achieves%20performance%20on%20par%20with%20supervised%20models%20designed%20for%20single%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Plausible%2520Multi-System%2520Trajectory%2520Generation%2520and%2520Symmetry%2520Discovery%26entry.906535625%3DJiayin%2520Liu%2520and%2520Yulong%2520Yang%2520and%2520Vineet%2520Bansal%2520and%2520Christine%2520Allen-Blanchette%26entry.1292438233%3DFrom%2520metronomes%2520to%2520celestial%2520bodies%252C%2520mechanics%2520underpins%2520how%2520the%2520world%2520evolves%2520in%2520time%2520and%2520space.%2520With%2520consideration%2520of%2520this%252C%2520a%2520number%2520of%2520recent%2520neural%2520network%2520models%2520leverage%2520inductive%2520biases%2520from%2520classical%2520mechanics%2520to%2520encourage%2520model%2520interpretability%2520and%2520ensure%2520forecasted%2520states%2520are%2520physical.%2520However%252C%2520in%2520general%252C%2520these%2520models%2520are%2520designed%2520to%2520capture%2520the%2520dynamics%2520of%2520a%2520single%2520system%2520with%2520fixed%2520physical%2520parameters%252C%2520from%2520state-space%2520measurements%2520of%2520a%2520known%2520configuration%2520space.%2520In%2520this%2520paper%2520we%2520introduce%2520Symplectic%2520Phase%2520Space%2520GAN%2520%2528SPS-GAN%2529%2520which%2520can%2520capture%2520the%2520dynamics%2520of%2520multiple%2520systems%252C%2520and%2520generalize%2520to%2520unseen%2520physical%2520parameters%2520from.%2520Moreover%252C%2520SPS-GAN%2520does%2520not%2520require%2520prior%2520knowledge%2520of%2520the%2520system%2520configuration%2520space.%2520In%2520fact%252C%2520SPS-GAN%2520can%2520discover%2520the%2520configuration%2520space%2520structure%2520of%2520the%2520system%2520from%2520arbitrary%2520measurement%2520types%2520%2528e.g.%252C%2520state-space%2520measurements%252C%2520video%2520frames%2529.%2520To%2520achieve%2520physically%2520plausible%2520generation%252C%2520we%2520introduce%2520a%2520novel%2520architecture%2520which%2520embeds%2520a%2520Hamiltonian%2520neural%2520network%2520recurrent%2520module%2520in%2520a%2520conditional%2520GAN%2520backbone.%2520To%2520discover%2520the%2520structure%2520of%2520the%2520configuration%2520space%252C%2520we%2520optimize%2520the%2520conditional%2520time-series%2520GAN%2520objective%2520with%2520an%2520additional%2520physically%2520motivated%2520term%2520to%2520encourages%2520a%2520sparse%2520representation%2520of%2520the%2520configuration%2520space.%2520We%2520demonstrate%2520the%2520utility%2520of%2520SPS-GAN%2520for%2520trajectory%2520prediction%252C%2520video%2520generation%2520and%2520symmetry%2520discovery.%2520Our%2520approach%2520captures%2520multiple%2520systems%2520and%2520achieves%2520performance%2520on%2520par%2520with%2520supervised%2520models%2520designed%2520for%2520single%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Plausible%20Multi-System%20Trajectory%20Generation%20and%20Symmetry%20Discovery&entry.906535625=Jiayin%20Liu%20and%20Yulong%20Yang%20and%20Vineet%20Bansal%20and%20Christine%20Allen-Blanchette&entry.1292438233=From%20metronomes%20to%20celestial%20bodies%2C%20mechanics%20underpins%20how%20the%20world%20evolves%20in%20time%20and%20space.%20With%20consideration%20of%20this%2C%20a%20number%20of%20recent%20neural%20network%20models%20leverage%20inductive%20biases%20from%20classical%20mechanics%20to%20encourage%20model%20interpretability%20and%20ensure%20forecasted%20states%20are%20physical.%20However%2C%20in%20general%2C%20these%20models%20are%20designed%20to%20capture%20the%20dynamics%20of%20a%20single%20system%20with%20fixed%20physical%20parameters%2C%20from%20state-space%20measurements%20of%20a%20known%20configuration%20space.%20In%20this%20paper%20we%20introduce%20Symplectic%20Phase%20Space%20GAN%20%28SPS-GAN%29%20which%20can%20capture%20the%20dynamics%20of%20multiple%20systems%2C%20and%20generalize%20to%20unseen%20physical%20parameters%20from.%20Moreover%2C%20SPS-GAN%20does%20not%20require%20prior%20knowledge%20of%20the%20system%20configuration%20space.%20In%20fact%2C%20SPS-GAN%20can%20discover%20the%20configuration%20space%20structure%20of%20the%20system%20from%20arbitrary%20measurement%20types%20%28e.g.%2C%20state-space%20measurements%2C%20video%20frames%29.%20To%20achieve%20physically%20plausible%20generation%2C%20we%20introduce%20a%20novel%20architecture%20which%20embeds%20a%20Hamiltonian%20neural%20network%20recurrent%20module%20in%20a%20conditional%20GAN%20backbone.%20To%20discover%20the%20structure%20of%20the%20configuration%20space%2C%20we%20optimize%20the%20conditional%20time-series%20GAN%20objective%20with%20an%20additional%20physically%20motivated%20term%20to%20encourages%20a%20sparse%20representation%20of%20the%20configuration%20space.%20We%20demonstrate%20the%20utility%20of%20SPS-GAN%20for%20trajectory%20prediction%2C%20video%20generation%20and%20symmetry%20discovery.%20Our%20approach%20captures%20multiple%20systems%20and%20achieves%20performance%20on%20par%20with%20supervised%20models%20designed%20for%20single%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2509.23003v2&entry.124074799=Read"},
{"title": "Multimodal Signal Processing For Thermo-Visible-Lidar Fusion In Real-time 3D Semantic Mapping", "author": "Jiajun Sun and Yangyi Ou and Haoyuan Zheng and Chao yang and Yue Ma", "abstract": "In complex environments, autonomous robot navigation and environmental perception pose higher requirements for SLAM technology. This paper presents a novel method for semantically enhancing 3D point cloud maps with thermal information. By first performing pixel-level fusion of visible and infrared images, the system projects real-time LiDAR point clouds onto this fused image stream. It then segments heat source features in the thermal channel to instantly identify high temperature targets and applies this temperature information as a semantic layer on the final 3D map. This approach generates maps that not only have accurate geometry but also possess a critical semantic understanding of the environment, making it highly valuable for specific applications like rapid disaster assessment and industrial preventive maintenance.", "link": "http://arxiv.org/abs/2601.09578v1", "date": "2026-01-14", "relevancy": 2.2654, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6032}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5647}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Signal%20Processing%20For%20Thermo-Visible-Lidar%20Fusion%20In%20Real-time%203D%20Semantic%20Mapping&body=Title%3A%20Multimodal%20Signal%20Processing%20For%20Thermo-Visible-Lidar%20Fusion%20In%20Real-time%203D%20Semantic%20Mapping%0AAuthor%3A%20Jiajun%20Sun%20and%20Yangyi%20Ou%20and%20Haoyuan%20Zheng%20and%20Chao%20yang%20and%20Yue%20Ma%0AAbstract%3A%20In%20complex%20environments%2C%20autonomous%20robot%20navigation%20and%20environmental%20perception%20pose%20higher%20requirements%20for%20SLAM%20technology.%20This%20paper%20presents%20a%20novel%20method%20for%20semantically%20enhancing%203D%20point%20cloud%20maps%20with%20thermal%20information.%20By%20first%20performing%20pixel-level%20fusion%20of%20visible%20and%20infrared%20images%2C%20the%20system%20projects%20real-time%20LiDAR%20point%20clouds%20onto%20this%20fused%20image%20stream.%20It%20then%20segments%20heat%20source%20features%20in%20the%20thermal%20channel%20to%20instantly%20identify%20high%20temperature%20targets%20and%20applies%20this%20temperature%20information%20as%20a%20semantic%20layer%20on%20the%20final%203D%20map.%20This%20approach%20generates%20maps%20that%20not%20only%20have%20accurate%20geometry%20but%20also%20possess%20a%20critical%20semantic%20understanding%20of%20the%20environment%2C%20making%20it%20highly%20valuable%20for%20specific%20applications%20like%20rapid%20disaster%20assessment%20and%20industrial%20preventive%20maintenance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Signal%2520Processing%2520For%2520Thermo-Visible-Lidar%2520Fusion%2520In%2520Real-time%25203D%2520Semantic%2520Mapping%26entry.906535625%3DJiajun%2520Sun%2520and%2520Yangyi%2520Ou%2520and%2520Haoyuan%2520Zheng%2520and%2520Chao%2520yang%2520and%2520Yue%2520Ma%26entry.1292438233%3DIn%2520complex%2520environments%252C%2520autonomous%2520robot%2520navigation%2520and%2520environmental%2520perception%2520pose%2520higher%2520requirements%2520for%2520SLAM%2520technology.%2520This%2520paper%2520presents%2520a%2520novel%2520method%2520for%2520semantically%2520enhancing%25203D%2520point%2520cloud%2520maps%2520with%2520thermal%2520information.%2520By%2520first%2520performing%2520pixel-level%2520fusion%2520of%2520visible%2520and%2520infrared%2520images%252C%2520the%2520system%2520projects%2520real-time%2520LiDAR%2520point%2520clouds%2520onto%2520this%2520fused%2520image%2520stream.%2520It%2520then%2520segments%2520heat%2520source%2520features%2520in%2520the%2520thermal%2520channel%2520to%2520instantly%2520identify%2520high%2520temperature%2520targets%2520and%2520applies%2520this%2520temperature%2520information%2520as%2520a%2520semantic%2520layer%2520on%2520the%2520final%25203D%2520map.%2520This%2520approach%2520generates%2520maps%2520that%2520not%2520only%2520have%2520accurate%2520geometry%2520but%2520also%2520possess%2520a%2520critical%2520semantic%2520understanding%2520of%2520the%2520environment%252C%2520making%2520it%2520highly%2520valuable%2520for%2520specific%2520applications%2520like%2520rapid%2520disaster%2520assessment%2520and%2520industrial%2520preventive%2520maintenance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Signal%20Processing%20For%20Thermo-Visible-Lidar%20Fusion%20In%20Real-time%203D%20Semantic%20Mapping&entry.906535625=Jiajun%20Sun%20and%20Yangyi%20Ou%20and%20Haoyuan%20Zheng%20and%20Chao%20yang%20and%20Yue%20Ma&entry.1292438233=In%20complex%20environments%2C%20autonomous%20robot%20navigation%20and%20environmental%20perception%20pose%20higher%20requirements%20for%20SLAM%20technology.%20This%20paper%20presents%20a%20novel%20method%20for%20semantically%20enhancing%203D%20point%20cloud%20maps%20with%20thermal%20information.%20By%20first%20performing%20pixel-level%20fusion%20of%20visible%20and%20infrared%20images%2C%20the%20system%20projects%20real-time%20LiDAR%20point%20clouds%20onto%20this%20fused%20image%20stream.%20It%20then%20segments%20heat%20source%20features%20in%20the%20thermal%20channel%20to%20instantly%20identify%20high%20temperature%20targets%20and%20applies%20this%20temperature%20information%20as%20a%20semantic%20layer%20on%20the%20final%203D%20map.%20This%20approach%20generates%20maps%20that%20not%20only%20have%20accurate%20geometry%20but%20also%20possess%20a%20critical%20semantic%20understanding%20of%20the%20environment%2C%20making%20it%20highly%20valuable%20for%20specific%20applications%20like%20rapid%20disaster%20assessment%20and%20industrial%20preventive%20maintenance.&entry.1838667208=http%3A//arxiv.org/abs/2601.09578v1&entry.124074799=Read"},
{"title": "FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks", "author": "Renqiang Luo and Huafei Huang and Tao Tang and Jing Ren and Ziqi Xu and Mingliang Hou and Enyan Dai and Feng Xia", "abstract": "Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.", "link": "http://arxiv.org/abs/2601.09394v1", "date": "2026-01-14", "relevancy": 2.2592, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4677}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4442}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FairGE%3A%20Fairness-Aware%20Graph%20Encoding%20in%20Incomplete%20Social%20Networks&body=Title%3A%20FairGE%3A%20Fairness-Aware%20Graph%20Encoding%20in%20Incomplete%20Social%20Networks%0AAuthor%3A%20Renqiang%20Luo%20and%20Huafei%20Huang%20and%20Tao%20Tang%20and%20Jing%20Ren%20and%20Ziqi%20Xu%20and%20Mingliang%20Hou%20and%20Enyan%20Dai%20and%20Feng%20Xia%0AAbstract%3A%20Graph%20Transformers%20%28GTs%29%20are%20increasingly%20applied%20to%20social%20network%20analysis%2C%20yet%20their%20deployment%20is%20often%20constrained%20by%20fairness%20concerns.%20This%20issue%20is%20particularly%20critical%20in%20incomplete%20social%20networks%2C%20where%20sensitive%20attributes%20are%20frequently%20missing%20due%20to%20privacy%20and%20ethical%20restrictions.%20Existing%20solutions%20commonly%20generate%20these%20incomplete%20attributes%2C%20which%20may%20introduce%20additional%20biases%20and%20further%20compromise%20user%20privacy.%20To%20address%20this%20challenge%2C%20FairGE%20%28Fair%20Graph%20Encoding%29%20is%20introduced%20as%20a%20fairness-aware%20framework%20for%20GTs%20in%20incomplete%20social%20networks.%20Instead%20of%20generating%20sensitive%20attributes%2C%20FairGE%20encodes%20fairness%20directly%20through%20spectral%20graph%20theory.%20By%20leveraging%20the%20principal%20eigenvector%20to%20represent%20structural%20information%20and%20padding%20incomplete%20sensitive%20attributes%20with%20zeros%20to%20maintain%20independence%2C%20FairGE%20ensures%20fairness%20without%20data%20reconstruction.%20Theoretical%20analysis%20demonstrates%20that%20the%20method%20suppresses%20the%20influence%20of%20non-principal%20spectral%20components%2C%20thereby%20enhancing%20fairness.%20Extensive%20experiments%20on%20seven%20real-world%20social%20network%20datasets%20confirm%20that%20FairGE%20achieves%20at%20least%20a%2016%25%20improvement%20in%20both%20statistical%20parity%20and%20equality%20of%20opportunity%20compared%20with%20state-of-the-art%20baselines.%20The%20source%20code%20is%20shown%20in%20https%3A//github.com/LuoRenqiang/FairGE.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairGE%253A%2520Fairness-Aware%2520Graph%2520Encoding%2520in%2520Incomplete%2520Social%2520Networks%26entry.906535625%3DRenqiang%2520Luo%2520and%2520Huafei%2520Huang%2520and%2520Tao%2520Tang%2520and%2520Jing%2520Ren%2520and%2520Ziqi%2520Xu%2520and%2520Mingliang%2520Hou%2520and%2520Enyan%2520Dai%2520and%2520Feng%2520Xia%26entry.1292438233%3DGraph%2520Transformers%2520%2528GTs%2529%2520are%2520increasingly%2520applied%2520to%2520social%2520network%2520analysis%252C%2520yet%2520their%2520deployment%2520is%2520often%2520constrained%2520by%2520fairness%2520concerns.%2520This%2520issue%2520is%2520particularly%2520critical%2520in%2520incomplete%2520social%2520networks%252C%2520where%2520sensitive%2520attributes%2520are%2520frequently%2520missing%2520due%2520to%2520privacy%2520and%2520ethical%2520restrictions.%2520Existing%2520solutions%2520commonly%2520generate%2520these%2520incomplete%2520attributes%252C%2520which%2520may%2520introduce%2520additional%2520biases%2520and%2520further%2520compromise%2520user%2520privacy.%2520To%2520address%2520this%2520challenge%252C%2520FairGE%2520%2528Fair%2520Graph%2520Encoding%2529%2520is%2520introduced%2520as%2520a%2520fairness-aware%2520framework%2520for%2520GTs%2520in%2520incomplete%2520social%2520networks.%2520Instead%2520of%2520generating%2520sensitive%2520attributes%252C%2520FairGE%2520encodes%2520fairness%2520directly%2520through%2520spectral%2520graph%2520theory.%2520By%2520leveraging%2520the%2520principal%2520eigenvector%2520to%2520represent%2520structural%2520information%2520and%2520padding%2520incomplete%2520sensitive%2520attributes%2520with%2520zeros%2520to%2520maintain%2520independence%252C%2520FairGE%2520ensures%2520fairness%2520without%2520data%2520reconstruction.%2520Theoretical%2520analysis%2520demonstrates%2520that%2520the%2520method%2520suppresses%2520the%2520influence%2520of%2520non-principal%2520spectral%2520components%252C%2520thereby%2520enhancing%2520fairness.%2520Extensive%2520experiments%2520on%2520seven%2520real-world%2520social%2520network%2520datasets%2520confirm%2520that%2520FairGE%2520achieves%2520at%2520least%2520a%252016%2525%2520improvement%2520in%2520both%2520statistical%2520parity%2520and%2520equality%2520of%2520opportunity%2520compared%2520with%2520state-of-the-art%2520baselines.%2520The%2520source%2520code%2520is%2520shown%2520in%2520https%253A//github.com/LuoRenqiang/FairGE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FairGE%3A%20Fairness-Aware%20Graph%20Encoding%20in%20Incomplete%20Social%20Networks&entry.906535625=Renqiang%20Luo%20and%20Huafei%20Huang%20and%20Tao%20Tang%20and%20Jing%20Ren%20and%20Ziqi%20Xu%20and%20Mingliang%20Hou%20and%20Enyan%20Dai%20and%20Feng%20Xia&entry.1292438233=Graph%20Transformers%20%28GTs%29%20are%20increasingly%20applied%20to%20social%20network%20analysis%2C%20yet%20their%20deployment%20is%20often%20constrained%20by%20fairness%20concerns.%20This%20issue%20is%20particularly%20critical%20in%20incomplete%20social%20networks%2C%20where%20sensitive%20attributes%20are%20frequently%20missing%20due%20to%20privacy%20and%20ethical%20restrictions.%20Existing%20solutions%20commonly%20generate%20these%20incomplete%20attributes%2C%20which%20may%20introduce%20additional%20biases%20and%20further%20compromise%20user%20privacy.%20To%20address%20this%20challenge%2C%20FairGE%20%28Fair%20Graph%20Encoding%29%20is%20introduced%20as%20a%20fairness-aware%20framework%20for%20GTs%20in%20incomplete%20social%20networks.%20Instead%20of%20generating%20sensitive%20attributes%2C%20FairGE%20encodes%20fairness%20directly%20through%20spectral%20graph%20theory.%20By%20leveraging%20the%20principal%20eigenvector%20to%20represent%20structural%20information%20and%20padding%20incomplete%20sensitive%20attributes%20with%20zeros%20to%20maintain%20independence%2C%20FairGE%20ensures%20fairness%20without%20data%20reconstruction.%20Theoretical%20analysis%20demonstrates%20that%20the%20method%20suppresses%20the%20influence%20of%20non-principal%20spectral%20components%2C%20thereby%20enhancing%20fairness.%20Extensive%20experiments%20on%20seven%20real-world%20social%20network%20datasets%20confirm%20that%20FairGE%20achieves%20at%20least%20a%2016%25%20improvement%20in%20both%20statistical%20parity%20and%20equality%20of%20opportunity%20compared%20with%20state-of-the-art%20baselines.%20The%20source%20code%20is%20shown%20in%20https%3A//github.com/LuoRenqiang/FairGE.&entry.1838667208=http%3A//arxiv.org/abs/2601.09394v1&entry.124074799=Read"},
{"title": "Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents", "author": "Said Yasin and Torsten Zesch", "abstract": "Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achieve acceptable overall quality. We identify the major challenges and outline an agenda for future research.", "link": "http://arxiv.org/abs/2601.09586v1", "date": "2026-01-14", "relevancy": 2.2571, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4611}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4516}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Show%2C%20don%27t%20tell%20--%20Providing%20Visual%20Error%20Feedback%20for%20Handwritten%20Documents&body=Title%3A%20Show%2C%20don%27t%20tell%20--%20Providing%20Visual%20Error%20Feedback%20for%20Handwritten%20Documents%0AAuthor%3A%20Said%20Yasin%20and%20Torsten%20Zesch%0AAbstract%3A%20Handwriting%20remains%20an%20essential%20skill%2C%20particularly%20in%20education.%20Therefore%2C%20providing%20visual%20feedback%20on%20handwritten%20documents%20is%20an%20important%20but%20understudied%20area.%20We%20outline%20the%20many%20challenges%20when%20going%20from%20an%20image%20of%20handwritten%20input%20to%20correctly%20placed%20informative%20error%20feedback.%20We%20empirically%20compare%20modular%20and%20end-to-end%20systems%20and%20find%20that%20both%20approaches%20currently%20do%20not%20achieve%20acceptable%20overall%20quality.%20We%20identify%20the%20major%20challenges%20and%20outline%20an%20agenda%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShow%252C%2520don%2527t%2520tell%2520--%2520Providing%2520Visual%2520Error%2520Feedback%2520for%2520Handwritten%2520Documents%26entry.906535625%3DSaid%2520Yasin%2520and%2520Torsten%2520Zesch%26entry.1292438233%3DHandwriting%2520remains%2520an%2520essential%2520skill%252C%2520particularly%2520in%2520education.%2520Therefore%252C%2520providing%2520visual%2520feedback%2520on%2520handwritten%2520documents%2520is%2520an%2520important%2520but%2520understudied%2520area.%2520We%2520outline%2520the%2520many%2520challenges%2520when%2520going%2520from%2520an%2520image%2520of%2520handwritten%2520input%2520to%2520correctly%2520placed%2520informative%2520error%2520feedback.%2520We%2520empirically%2520compare%2520modular%2520and%2520end-to-end%2520systems%2520and%2520find%2520that%2520both%2520approaches%2520currently%2520do%2520not%2520achieve%2520acceptable%2520overall%2520quality.%2520We%2520identify%2520the%2520major%2520challenges%2520and%2520outline%2520an%2520agenda%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Show%2C%20don%27t%20tell%20--%20Providing%20Visual%20Error%20Feedback%20for%20Handwritten%20Documents&entry.906535625=Said%20Yasin%20and%20Torsten%20Zesch&entry.1292438233=Handwriting%20remains%20an%20essential%20skill%2C%20particularly%20in%20education.%20Therefore%2C%20providing%20visual%20feedback%20on%20handwritten%20documents%20is%20an%20important%20but%20understudied%20area.%20We%20outline%20the%20many%20challenges%20when%20going%20from%20an%20image%20of%20handwritten%20input%20to%20correctly%20placed%20informative%20error%20feedback.%20We%20empirically%20compare%20modular%20and%20end-to-end%20systems%20and%20find%20that%20both%20approaches%20currently%20do%20not%20achieve%20acceptable%20overall%20quality.%20We%20identify%20the%20major%20challenges%20and%20outline%20an%20agenda%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.09586v1&entry.124074799=Read"},
{"title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation", "author": "Giorgio Franceschelli and Mirco Musolesi", "abstract": "Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose DiffSampling, a new decoding method that leverages a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. In addition, we also propose two variations of the proposed method that aim to correct the subtle inconsistencies of common sampling strategies. Experiments involving four different text-generation tasks demonstrate that our approach consistently performs at least on par with the existing methods it builds upon in terms of quality, despite sampling from a larger set of tokens.", "link": "http://arxiv.org/abs/2502.14037v5", "date": "2026-01-14", "relevancy": 2.2515, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.597}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5679}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffSampling%3A%20Enhancing%20Diversity%20and%20Accuracy%20in%20Neural%20Text%20Generation&body=Title%3A%20DiffSampling%3A%20Enhancing%20Diversity%20and%20Accuracy%20in%20Neural%20Text%20Generation%0AAuthor%3A%20Giorgio%20Franceschelli%20and%20Mirco%20Musolesi%0AAbstract%3A%20Despite%20their%20growing%20capabilities%2C%20language%20models%20still%20frequently%20reproduce%20content%20from%20their%20training%20data%2C%20generate%20repetitive%20text%2C%20and%20favor%20common%20grammatical%20patterns%20and%20vocabulary.%20A%20possible%20cause%20is%20the%20decoding%20strategy%3A%20the%20most%20common%20strategies%20either%20consider%20only%20the%20most%20probable%20tokens%2C%20which%20reduces%20output%20diversity%2C%20or%20increase%20the%20likelihood%20of%20unlikely%20tokens%2C%20compromising%20output%20accuracy%20and%20correctness.%20In%20this%20paper%2C%20we%20propose%20DiffSampling%2C%20a%20new%20decoding%20method%20that%20leverages%20a%20mathematical%20analysis%20of%20the%20token%20probability%20distribution%20to%20ensure%20the%20generation%20of%20contextually%20appropriate%20text.%20In%20particular%2C%20the%20difference%20between%20consecutive%2C%20sorted%20probabilities%20can%20be%20used%20to%20truncate%20incorrect%20tokens.%20In%20addition%2C%20we%20also%20propose%20two%20variations%20of%20the%20proposed%20method%20that%20aim%20to%20correct%20the%20subtle%20inconsistencies%20of%20common%20sampling%20strategies.%20Experiments%20involving%20four%20different%20text-generation%20tasks%20demonstrate%20that%20our%20approach%20consistently%20performs%20at%20least%20on%20par%20with%20the%20existing%20methods%20it%20builds%20upon%20in%20terms%20of%20quality%2C%20despite%20sampling%20from%20a%20larger%20set%20of%20tokens.%0ALink%3A%20http%3A//arxiv.org/abs/2502.14037v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffSampling%253A%2520Enhancing%2520Diversity%2520and%2520Accuracy%2520in%2520Neural%2520Text%2520Generation%26entry.906535625%3DGiorgio%2520Franceschelli%2520and%2520Mirco%2520Musolesi%26entry.1292438233%3DDespite%2520their%2520growing%2520capabilities%252C%2520language%2520models%2520still%2520frequently%2520reproduce%2520content%2520from%2520their%2520training%2520data%252C%2520generate%2520repetitive%2520text%252C%2520and%2520favor%2520common%2520grammatical%2520patterns%2520and%2520vocabulary.%2520A%2520possible%2520cause%2520is%2520the%2520decoding%2520strategy%253A%2520the%2520most%2520common%2520strategies%2520either%2520consider%2520only%2520the%2520most%2520probable%2520tokens%252C%2520which%2520reduces%2520output%2520diversity%252C%2520or%2520increase%2520the%2520likelihood%2520of%2520unlikely%2520tokens%252C%2520compromising%2520output%2520accuracy%2520and%2520correctness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DiffSampling%252C%2520a%2520new%2520decoding%2520method%2520that%2520leverages%2520a%2520mathematical%2520analysis%2520of%2520the%2520token%2520probability%2520distribution%2520to%2520ensure%2520the%2520generation%2520of%2520contextually%2520appropriate%2520text.%2520In%2520particular%252C%2520the%2520difference%2520between%2520consecutive%252C%2520sorted%2520probabilities%2520can%2520be%2520used%2520to%2520truncate%2520incorrect%2520tokens.%2520In%2520addition%252C%2520we%2520also%2520propose%2520two%2520variations%2520of%2520the%2520proposed%2520method%2520that%2520aim%2520to%2520correct%2520the%2520subtle%2520inconsistencies%2520of%2520common%2520sampling%2520strategies.%2520Experiments%2520involving%2520four%2520different%2520text-generation%2520tasks%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520performs%2520at%2520least%2520on%2520par%2520with%2520the%2520existing%2520methods%2520it%2520builds%2520upon%2520in%2520terms%2520of%2520quality%252C%2520despite%2520sampling%2520from%2520a%2520larger%2520set%2520of%2520tokens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14037v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffSampling%3A%20Enhancing%20Diversity%20and%20Accuracy%20in%20Neural%20Text%20Generation&entry.906535625=Giorgio%20Franceschelli%20and%20Mirco%20Musolesi&entry.1292438233=Despite%20their%20growing%20capabilities%2C%20language%20models%20still%20frequently%20reproduce%20content%20from%20their%20training%20data%2C%20generate%20repetitive%20text%2C%20and%20favor%20common%20grammatical%20patterns%20and%20vocabulary.%20A%20possible%20cause%20is%20the%20decoding%20strategy%3A%20the%20most%20common%20strategies%20either%20consider%20only%20the%20most%20probable%20tokens%2C%20which%20reduces%20output%20diversity%2C%20or%20increase%20the%20likelihood%20of%20unlikely%20tokens%2C%20compromising%20output%20accuracy%20and%20correctness.%20In%20this%20paper%2C%20we%20propose%20DiffSampling%2C%20a%20new%20decoding%20method%20that%20leverages%20a%20mathematical%20analysis%20of%20the%20token%20probability%20distribution%20to%20ensure%20the%20generation%20of%20contextually%20appropriate%20text.%20In%20particular%2C%20the%20difference%20between%20consecutive%2C%20sorted%20probabilities%20can%20be%20used%20to%20truncate%20incorrect%20tokens.%20In%20addition%2C%20we%20also%20propose%20two%20variations%20of%20the%20proposed%20method%20that%20aim%20to%20correct%20the%20subtle%20inconsistencies%20of%20common%20sampling%20strategies.%20Experiments%20involving%20four%20different%20text-generation%20tasks%20demonstrate%20that%20our%20approach%20consistently%20performs%20at%20least%20on%20par%20with%20the%20existing%20methods%20it%20builds%20upon%20in%20terms%20of%20quality%2C%20despite%20sampling%20from%20a%20larger%20set%20of%20tokens.&entry.1838667208=http%3A//arxiv.org/abs/2502.14037v5&entry.124074799=Read"},
{"title": "BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation", "author": "Shujian Gao and Yuan Wang and Zekuan Yu", "abstract": "Semi-supervised medical image segmentation (SSMIS) seeks to match fully supervised performance while sharply reducing annotation cost. Mainstream SSMIS methods rely on \\emph{label-space consistency}, yet they overlook the equally critical \\emph{representation-space alignment}. Without harmonizing latent features, models struggle to learn representations that are both discriminative and spatially coherent. To this end, we introduce \\textbf{Bilateral Alignment in Representation and Label spaces (BARL)}, a unified framework that couples two collaborative branches and enforces alignment in both spaces. For label-space alignment, inspired by co-training and multi-scale decoding, we devise \\textbf{Dual-Path Regularization (DPR)} and \\textbf{Progressively Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch consistency while mitigating error accumulation from coarse to fine scales. For representation-space alignment, we conduct region-level and lesion-instance matching between branches, explicitly capturing the fragmented, complex pathological patterns common in medical imagery. Extensive experiments on four public benchmarks and a proprietary CBCT dataset demonstrate that BARL consistently surpasses state-of-the-art SSMIS methods. Ablative studies further validate the contribution of each component. Code will be released soon.", "link": "http://arxiv.org/abs/2510.16863v2", "date": "2026-01-14", "relevancy": 2.2436, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5936}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5518}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BARL%3A%20Bilateral%20Alignment%20in%20Representation%20and%20Label%20Spaces%20for%20Semi-Supervised%20Volumetric%20Medical%20Image%20Segmentation&body=Title%3A%20BARL%3A%20Bilateral%20Alignment%20in%20Representation%20and%20Label%20Spaces%20for%20Semi-Supervised%20Volumetric%20Medical%20Image%20Segmentation%0AAuthor%3A%20Shujian%20Gao%20and%20Yuan%20Wang%20and%20Zekuan%20Yu%0AAbstract%3A%20Semi-supervised%20medical%20image%20segmentation%20%28SSMIS%29%20seeks%20to%20match%20fully%20supervised%20performance%20while%20sharply%20reducing%20annotation%20cost.%20Mainstream%20SSMIS%20methods%20rely%20on%20%5Cemph%7Blabel-space%20consistency%7D%2C%20yet%20they%20overlook%20the%20equally%20critical%20%5Cemph%7Brepresentation-space%20alignment%7D.%20Without%20harmonizing%20latent%20features%2C%20models%20struggle%20to%20learn%20representations%20that%20are%20both%20discriminative%20and%20spatially%20coherent.%20To%20this%20end%2C%20we%20introduce%20%5Ctextbf%7BBilateral%20Alignment%20in%20Representation%20and%20Label%20spaces%20%28BARL%29%7D%2C%20a%20unified%20framework%20that%20couples%20two%20collaborative%20branches%20and%20enforces%20alignment%20in%20both%20spaces.%20For%20label-space%20alignment%2C%20inspired%20by%20co-training%20and%20multi-scale%20decoding%2C%20we%20devise%20%5Ctextbf%7BDual-Path%20Regularization%20%28DPR%29%7D%20and%20%5Ctextbf%7BProgressively%20Cognitive%20Bias%20Correction%20%28PCBC%29%7D%20to%20impose%20fine-grained%20cross-branch%20consistency%20while%20mitigating%20error%20accumulation%20from%20coarse%20to%20fine%20scales.%20For%20representation-space%20alignment%2C%20we%20conduct%20region-level%20and%20lesion-instance%20matching%20between%20branches%2C%20explicitly%20capturing%20the%20fragmented%2C%20complex%20pathological%20patterns%20common%20in%20medical%20imagery.%20Extensive%20experiments%20on%20four%20public%20benchmarks%20and%20a%20proprietary%20CBCT%20dataset%20demonstrate%20that%20BARL%20consistently%20surpasses%20state-of-the-art%20SSMIS%20methods.%20Ablative%20studies%20further%20validate%20the%20contribution%20of%20each%20component.%20Code%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBARL%253A%2520Bilateral%2520Alignment%2520in%2520Representation%2520and%2520Label%2520Spaces%2520for%2520Semi-Supervised%2520Volumetric%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DShujian%2520Gao%2520and%2520Yuan%2520Wang%2520and%2520Zekuan%2520Yu%26entry.1292438233%3DSemi-supervised%2520medical%2520image%2520segmentation%2520%2528SSMIS%2529%2520seeks%2520to%2520match%2520fully%2520supervised%2520performance%2520while%2520sharply%2520reducing%2520annotation%2520cost.%2520Mainstream%2520SSMIS%2520methods%2520rely%2520on%2520%255Cemph%257Blabel-space%2520consistency%257D%252C%2520yet%2520they%2520overlook%2520the%2520equally%2520critical%2520%255Cemph%257Brepresentation-space%2520alignment%257D.%2520Without%2520harmonizing%2520latent%2520features%252C%2520models%2520struggle%2520to%2520learn%2520representations%2520that%2520are%2520both%2520discriminative%2520and%2520spatially%2520coherent.%2520To%2520this%2520end%252C%2520we%2520introduce%2520%255Ctextbf%257BBilateral%2520Alignment%2520in%2520Representation%2520and%2520Label%2520spaces%2520%2528BARL%2529%257D%252C%2520a%2520unified%2520framework%2520that%2520couples%2520two%2520collaborative%2520branches%2520and%2520enforces%2520alignment%2520in%2520both%2520spaces.%2520For%2520label-space%2520alignment%252C%2520inspired%2520by%2520co-training%2520and%2520multi-scale%2520decoding%252C%2520we%2520devise%2520%255Ctextbf%257BDual-Path%2520Regularization%2520%2528DPR%2529%257D%2520and%2520%255Ctextbf%257BProgressively%2520Cognitive%2520Bias%2520Correction%2520%2528PCBC%2529%257D%2520to%2520impose%2520fine-grained%2520cross-branch%2520consistency%2520while%2520mitigating%2520error%2520accumulation%2520from%2520coarse%2520to%2520fine%2520scales.%2520For%2520representation-space%2520alignment%252C%2520we%2520conduct%2520region-level%2520and%2520lesion-instance%2520matching%2520between%2520branches%252C%2520explicitly%2520capturing%2520the%2520fragmented%252C%2520complex%2520pathological%2520patterns%2520common%2520in%2520medical%2520imagery.%2520Extensive%2520experiments%2520on%2520four%2520public%2520benchmarks%2520and%2520a%2520proprietary%2520CBCT%2520dataset%2520demonstrate%2520that%2520BARL%2520consistently%2520surpasses%2520state-of-the-art%2520SSMIS%2520methods.%2520Ablative%2520studies%2520further%2520validate%2520the%2520contribution%2520of%2520each%2520component.%2520Code%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BARL%3A%20Bilateral%20Alignment%20in%20Representation%20and%20Label%20Spaces%20for%20Semi-Supervised%20Volumetric%20Medical%20Image%20Segmentation&entry.906535625=Shujian%20Gao%20and%20Yuan%20Wang%20and%20Zekuan%20Yu&entry.1292438233=Semi-supervised%20medical%20image%20segmentation%20%28SSMIS%29%20seeks%20to%20match%20fully%20supervised%20performance%20while%20sharply%20reducing%20annotation%20cost.%20Mainstream%20SSMIS%20methods%20rely%20on%20%5Cemph%7Blabel-space%20consistency%7D%2C%20yet%20they%20overlook%20the%20equally%20critical%20%5Cemph%7Brepresentation-space%20alignment%7D.%20Without%20harmonizing%20latent%20features%2C%20models%20struggle%20to%20learn%20representations%20that%20are%20both%20discriminative%20and%20spatially%20coherent.%20To%20this%20end%2C%20we%20introduce%20%5Ctextbf%7BBilateral%20Alignment%20in%20Representation%20and%20Label%20spaces%20%28BARL%29%7D%2C%20a%20unified%20framework%20that%20couples%20two%20collaborative%20branches%20and%20enforces%20alignment%20in%20both%20spaces.%20For%20label-space%20alignment%2C%20inspired%20by%20co-training%20and%20multi-scale%20decoding%2C%20we%20devise%20%5Ctextbf%7BDual-Path%20Regularization%20%28DPR%29%7D%20and%20%5Ctextbf%7BProgressively%20Cognitive%20Bias%20Correction%20%28PCBC%29%7D%20to%20impose%20fine-grained%20cross-branch%20consistency%20while%20mitigating%20error%20accumulation%20from%20coarse%20to%20fine%20scales.%20For%20representation-space%20alignment%2C%20we%20conduct%20region-level%20and%20lesion-instance%20matching%20between%20branches%2C%20explicitly%20capturing%20the%20fragmented%2C%20complex%20pathological%20patterns%20common%20in%20medical%20imagery.%20Extensive%20experiments%20on%20four%20public%20benchmarks%20and%20a%20proprietary%20CBCT%20dataset%20demonstrate%20that%20BARL%20consistently%20surpasses%20state-of-the-art%20SSMIS%20methods.%20Ablative%20studies%20further%20validate%20the%20contribution%20of%20each%20component.%20Code%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2510.16863v2&entry.124074799=Read"},
{"title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization", "author": "Frank R\u00f6der and Jan Benad and Manfred Eppe and Pradeep Kr. Banerjee", "abstract": "Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.", "link": "http://arxiv.org/abs/2508.20294v2", "date": "2026-01-14", "relevancy": 2.2296, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5798}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamics-Aligned%20Latent%20Imagination%20in%20Contextual%20World%20Models%20for%20Zero-Shot%20Generalization&body=Title%3A%20Dynamics-Aligned%20Latent%20Imagination%20in%20Contextual%20World%20Models%20for%20Zero-Shot%20Generalization%0AAuthor%3A%20Frank%20R%C3%B6der%20and%20Jan%20Benad%20and%20Manfred%20Eppe%20and%20Pradeep%20Kr.%20Banerjee%0AAbstract%3A%20Real-world%20reinforcement%20learning%20demands%20adaptation%20to%20unseen%20environmental%20conditions%20without%20costly%20retraining.%20Contextual%20Markov%20Decision%20Processes%20%28cMDP%29%20model%20this%20challenge%2C%20but%20existing%20methods%20often%20require%20explicit%20context%20variables%20%28e.g.%2C%20friction%2C%20gravity%29%2C%20limiting%20their%20use%20when%20contexts%20are%20latent%20or%20hard%20to%20measure.%20We%20introduce%20Dynamics-Aligned%20Latent%20Imagination%20%28DALI%29%2C%20a%20framework%20integrated%20within%20the%20Dreamer%20architecture%20that%20infers%20latent%20context%20representations%20from%20agent-environment%20interactions.%20By%20training%20a%20self-supervised%20encoder%20to%20predict%20forward%20dynamics%2C%20DALI%20generates%20actionable%20representations%20conditioning%20the%20world%20model%20and%20policy%2C%20bridging%20perception%20and%20control.%20We%20theoretically%20prove%20this%20encoder%20is%20essential%20for%20efficient%20context%20inference%20and%20robust%20generalization.%20DALI%27s%20latent%20space%20enables%20counterfactual%20consistency%3A%20Perturbing%20a%20gravity-encoding%20dimension%20alters%20imagined%20rollouts%20in%20physically%20plausible%20ways.%20On%20challenging%20cMDP%20benchmarks%2C%20DALI%20achieves%20significant%20gains%20over%20context-unaware%20baselines%2C%20often%20surpassing%20context-aware%20baselines%20in%20extrapolation%20tasks%2C%20enabling%20zero-shot%20generalization%20to%20unseen%20contextual%20variations.%0ALink%3A%20http%3A//arxiv.org/abs/2508.20294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamics-Aligned%2520Latent%2520Imagination%2520in%2520Contextual%2520World%2520Models%2520for%2520Zero-Shot%2520Generalization%26entry.906535625%3DFrank%2520R%25C3%25B6der%2520and%2520Jan%2520Benad%2520and%2520Manfred%2520Eppe%2520and%2520Pradeep%2520Kr.%2520Banerjee%26entry.1292438233%3DReal-world%2520reinforcement%2520learning%2520demands%2520adaptation%2520to%2520unseen%2520environmental%2520conditions%2520without%2520costly%2520retraining.%2520Contextual%2520Markov%2520Decision%2520Processes%2520%2528cMDP%2529%2520model%2520this%2520challenge%252C%2520but%2520existing%2520methods%2520often%2520require%2520explicit%2520context%2520variables%2520%2528e.g.%252C%2520friction%252C%2520gravity%2529%252C%2520limiting%2520their%2520use%2520when%2520contexts%2520are%2520latent%2520or%2520hard%2520to%2520measure.%2520We%2520introduce%2520Dynamics-Aligned%2520Latent%2520Imagination%2520%2528DALI%2529%252C%2520a%2520framework%2520integrated%2520within%2520the%2520Dreamer%2520architecture%2520that%2520infers%2520latent%2520context%2520representations%2520from%2520agent-environment%2520interactions.%2520By%2520training%2520a%2520self-supervised%2520encoder%2520to%2520predict%2520forward%2520dynamics%252C%2520DALI%2520generates%2520actionable%2520representations%2520conditioning%2520the%2520world%2520model%2520and%2520policy%252C%2520bridging%2520perception%2520and%2520control.%2520We%2520theoretically%2520prove%2520this%2520encoder%2520is%2520essential%2520for%2520efficient%2520context%2520inference%2520and%2520robust%2520generalization.%2520DALI%2527s%2520latent%2520space%2520enables%2520counterfactual%2520consistency%253A%2520Perturbing%2520a%2520gravity-encoding%2520dimension%2520alters%2520imagined%2520rollouts%2520in%2520physically%2520plausible%2520ways.%2520On%2520challenging%2520cMDP%2520benchmarks%252C%2520DALI%2520achieves%2520significant%2520gains%2520over%2520context-unaware%2520baselines%252C%2520often%2520surpassing%2520context-aware%2520baselines%2520in%2520extrapolation%2520tasks%252C%2520enabling%2520zero-shot%2520generalization%2520to%2520unseen%2520contextual%2520variations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics-Aligned%20Latent%20Imagination%20in%20Contextual%20World%20Models%20for%20Zero-Shot%20Generalization&entry.906535625=Frank%20R%C3%B6der%20and%20Jan%20Benad%20and%20Manfred%20Eppe%20and%20Pradeep%20Kr.%20Banerjee&entry.1292438233=Real-world%20reinforcement%20learning%20demands%20adaptation%20to%20unseen%20environmental%20conditions%20without%20costly%20retraining.%20Contextual%20Markov%20Decision%20Processes%20%28cMDP%29%20model%20this%20challenge%2C%20but%20existing%20methods%20often%20require%20explicit%20context%20variables%20%28e.g.%2C%20friction%2C%20gravity%29%2C%20limiting%20their%20use%20when%20contexts%20are%20latent%20or%20hard%20to%20measure.%20We%20introduce%20Dynamics-Aligned%20Latent%20Imagination%20%28DALI%29%2C%20a%20framework%20integrated%20within%20the%20Dreamer%20architecture%20that%20infers%20latent%20context%20representations%20from%20agent-environment%20interactions.%20By%20training%20a%20self-supervised%20encoder%20to%20predict%20forward%20dynamics%2C%20DALI%20generates%20actionable%20representations%20conditioning%20the%20world%20model%20and%20policy%2C%20bridging%20perception%20and%20control.%20We%20theoretically%20prove%20this%20encoder%20is%20essential%20for%20efficient%20context%20inference%20and%20robust%20generalization.%20DALI%27s%20latent%20space%20enables%20counterfactual%20consistency%3A%20Perturbing%20a%20gravity-encoding%20dimension%20alters%20imagined%20rollouts%20in%20physically%20plausible%20ways.%20On%20challenging%20cMDP%20benchmarks%2C%20DALI%20achieves%20significant%20gains%20over%20context-unaware%20baselines%2C%20often%20surpassing%20context-aware%20baselines%20in%20extrapolation%20tasks%2C%20enabling%20zero-shot%20generalization%20to%20unseen%20contextual%20variations.&entry.1838667208=http%3A//arxiv.org/abs/2508.20294v2&entry.124074799=Read"},
{"title": "See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval", "author": "Mingyu Jeon and Sungjin Han and Jinkwon Hwang and Minchol Kwon and Jonghee Kim and Junyeong Kim", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.", "link": "http://arxiv.org/abs/2601.09350v1", "date": "2026-01-14", "relevancy": 2.2271, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20More%2C%20Store%20Less%3A%20Memory-Efficient%20Resolution%20for%20Video%20Moment%20Retrieval&body=Title%3A%20See%20More%2C%20Store%20Less%3A%20Memory-Efficient%20Resolution%20for%20Video%20Moment%20Retrieval%0AAuthor%3A%20Mingyu%20Jeon%20and%20Sungjin%20Han%20and%20Jinkwon%20Hwang%20and%20Minchol%20Kwon%20and%20Jonghee%20Kim%20and%20Junyeong%20Kim%0AAbstract%3A%20Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20improved%20image%20recognition%20and%20reasoning%2C%20but%20video-related%20tasks%20remain%20challenging%20due%20to%20memory%20constraints%20from%20dense%20frame%20processing.%20Existing%20Video%20Moment%20Retrieval%20%28VMR%29%20methodologies%20rely%20on%20sparse%20frame%20sampling%2C%20risking%20potential%20information%20loss%2C%20especially%20in%20lengthy%20videos.%20We%20propose%20SMORE%20%28See%20MORE%2C%20store%20less%29%2C%20a%20framework%20that%20enhances%20memory%20efficiency%20while%20maintaining%20high%20information%20resolution.%20SMORE%20%281%29%20uses%20query-guided%20captions%20to%20encode%20semantics%20aligned%20with%20user%20intent%2C%20%282%29%20applies%20query-aware%20importance%20modulation%20to%20highlight%20relevant%20segments%2C%20and%20%283%29%20adaptively%20compresses%20frames%20to%20preserve%20key%20content%20while%20reducing%20redundancy.%20This%20enables%20efficient%20video%20understanding%20without%20exceeding%20memory%20budgets.%20Experimental%20validation%20reveals%20that%20SMORE%20achieves%20state-of-the-art%20performance%20on%20QVHighlights%2C%20Charades-STA%2C%20and%20ActivityNet-Captions%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520More%252C%2520Store%2520Less%253A%2520Memory-Efficient%2520Resolution%2520for%2520Video%2520Moment%2520Retrieval%26entry.906535625%3DMingyu%2520Jeon%2520and%2520Sungjin%2520Han%2520and%2520Jinkwon%2520Hwang%2520and%2520Minchol%2520Kwon%2520and%2520Jonghee%2520Kim%2520and%2520Junyeong%2520Kim%26entry.1292438233%3DRecent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520improved%2520image%2520recognition%2520and%2520reasoning%252C%2520but%2520video-related%2520tasks%2520remain%2520challenging%2520due%2520to%2520memory%2520constraints%2520from%2520dense%2520frame%2520processing.%2520Existing%2520Video%2520Moment%2520Retrieval%2520%2528VMR%2529%2520methodologies%2520rely%2520on%2520sparse%2520frame%2520sampling%252C%2520risking%2520potential%2520information%2520loss%252C%2520especially%2520in%2520lengthy%2520videos.%2520We%2520propose%2520SMORE%2520%2528See%2520MORE%252C%2520store%2520less%2529%252C%2520a%2520framework%2520that%2520enhances%2520memory%2520efficiency%2520while%2520maintaining%2520high%2520information%2520resolution.%2520SMORE%2520%25281%2529%2520uses%2520query-guided%2520captions%2520to%2520encode%2520semantics%2520aligned%2520with%2520user%2520intent%252C%2520%25282%2529%2520applies%2520query-aware%2520importance%2520modulation%2520to%2520highlight%2520relevant%2520segments%252C%2520and%2520%25283%2529%2520adaptively%2520compresses%2520frames%2520to%2520preserve%2520key%2520content%2520while%2520reducing%2520redundancy.%2520This%2520enables%2520efficient%2520video%2520understanding%2520without%2520exceeding%2520memory%2520budgets.%2520Experimental%2520validation%2520reveals%2520that%2520SMORE%2520achieves%2520state-of-the-art%2520performance%2520on%2520QVHighlights%252C%2520Charades-STA%252C%2520and%2520ActivityNet-Captions%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20More%2C%20Store%20Less%3A%20Memory-Efficient%20Resolution%20for%20Video%20Moment%20Retrieval&entry.906535625=Mingyu%20Jeon%20and%20Sungjin%20Han%20and%20Jinkwon%20Hwang%20and%20Minchol%20Kwon%20and%20Jonghee%20Kim%20and%20Junyeong%20Kim&entry.1292438233=Recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20improved%20image%20recognition%20and%20reasoning%2C%20but%20video-related%20tasks%20remain%20challenging%20due%20to%20memory%20constraints%20from%20dense%20frame%20processing.%20Existing%20Video%20Moment%20Retrieval%20%28VMR%29%20methodologies%20rely%20on%20sparse%20frame%20sampling%2C%20risking%20potential%20information%20loss%2C%20especially%20in%20lengthy%20videos.%20We%20propose%20SMORE%20%28See%20MORE%2C%20store%20less%29%2C%20a%20framework%20that%20enhances%20memory%20efficiency%20while%20maintaining%20high%20information%20resolution.%20SMORE%20%281%29%20uses%20query-guided%20captions%20to%20encode%20semantics%20aligned%20with%20user%20intent%2C%20%282%29%20applies%20query-aware%20importance%20modulation%20to%20highlight%20relevant%20segments%2C%20and%20%283%29%20adaptively%20compresses%20frames%20to%20preserve%20key%20content%20while%20reducing%20redundancy.%20This%20enables%20efficient%20video%20understanding%20without%20exceeding%20memory%20budgets.%20Experimental%20validation%20reveals%20that%20SMORE%20achieves%20state-of-the-art%20performance%20on%20QVHighlights%2C%20Charades-STA%2C%20and%20ActivityNet-Captions%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.09350v1&entry.124074799=Read"},
{"title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems", "author": "Yonglin Tian and Qiyao Zhang and Wei Xu and Yutong Wang and Yihao Wu and Xinyi Li and Xingyuan Dai and Hui Zhang and Zhiyong Cui and Baoqing Guo and Zujun Yu and Yisheng Lv", "abstract": "Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.", "link": "http://arxiv.org/abs/2601.09613v1", "date": "2026-01-14", "relevancy": 2.2247, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogRail%3A%20Benchmarking%20VLMs%20in%20Cognitive%20Intrusion%20Perception%20for%20Intelligent%20Railway%20Transportation%20Systems&body=Title%3A%20CogRail%3A%20Benchmarking%20VLMs%20in%20Cognitive%20Intrusion%20Perception%20for%20Intelligent%20Railway%20Transportation%20Systems%0AAuthor%3A%20Yonglin%20Tian%20and%20Qiyao%20Zhang%20and%20Wei%20Xu%20and%20Yutong%20Wang%20and%20Yihao%20Wu%20and%20Xinyi%20Li%20and%20Xingyuan%20Dai%20and%20Hui%20Zhang%20and%20Zhiyong%20Cui%20and%20Baoqing%20Guo%20and%20Zujun%20Yu%20and%20Yisheng%20Lv%0AAbstract%3A%20Accurate%20and%20early%20perception%20of%20potential%20intrusion%20targets%20is%20essential%20for%20ensuring%20the%20safety%20of%20railway%20transportation%20systems.%20However%2C%20most%20existing%20systems%20focus%20narrowly%20on%20object%20classification%20within%20fixed%20visual%20scopes%20and%20apply%20rule-based%20heuristics%20to%20determine%20intrusion%20status%2C%20often%20overlooking%20targets%20that%20pose%20latent%20intrusion%20risks.%20Anticipating%20such%20risks%20requires%20the%20cognition%20of%20spatial%20context%20and%20temporal%20dynamics%20for%20the%20object%20of%20interest%20%28OOI%29%2C%20which%20presents%20challenges%20for%20conventional%20visual%20models.%20To%20facilitate%20deep%20intrusion%20perception%2C%20we%20introduce%20a%20novel%20benchmark%2C%20CogRail%2C%20which%20integrates%20curated%20open-source%20datasets%20with%20cognitively%20driven%20question-answer%20annotations%20to%20support%20spatio-temporal%20reasoning%20and%20prediction.%20Building%20upon%20this%20benchmark%2C%20we%20conduct%20a%20systematic%20evaluation%20of%20state-of-the-art%20visual-language%20models%20%28VLMs%29%20using%20multimodal%20prompts%20to%20identify%20their%20strengths%20and%20limitations%20in%20this%20domain.%20Furthermore%2C%20we%20fine-tune%20VLMs%20for%20better%20performance%20and%20propose%20a%20joint%20fine-tuning%20framework%20that%20integrates%20three%20core%20tasks%2C%20position%20perception%2C%20movement%20prediction%2C%20and%20threat%20analysis%2C%20facilitating%20effective%20adaptation%20of%20general-purpose%20foundation%20models%20into%20specialized%20models%20tailored%20for%20cognitive%20intrusion%20perception.%20Extensive%20experiments%20reveal%20that%20current%20large-scale%20multimodal%20models%20struggle%20with%20the%20complex%20spatial-temporal%20reasoning%20required%20by%20the%20cognitive%20intrusion%20perception%20task%2C%20underscoring%20the%20limitations%20of%20existing%20foundation%20models%20in%20this%20safety-critical%20domain.%20In%20contrast%2C%20our%20proposed%20joint%20fine-tuning%20framework%20significantly%20enhances%20model%20performance%20by%20enabling%20targeted%20adaptation%20to%20domain-specific%20reasoning%20demands%2C%20highlighting%20the%20advantages%20of%20structured%20multi-task%20learning%20in%20improving%20both%20accuracy%20and%20interpretability.%20Code%20will%20be%20available%20at%20https%3A//github.com/Hub-Tian/CogRail.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogRail%253A%2520Benchmarking%2520VLMs%2520in%2520Cognitive%2520Intrusion%2520Perception%2520for%2520Intelligent%2520Railway%2520Transportation%2520Systems%26entry.906535625%3DYonglin%2520Tian%2520and%2520Qiyao%2520Zhang%2520and%2520Wei%2520Xu%2520and%2520Yutong%2520Wang%2520and%2520Yihao%2520Wu%2520and%2520Xinyi%2520Li%2520and%2520Xingyuan%2520Dai%2520and%2520Hui%2520Zhang%2520and%2520Zhiyong%2520Cui%2520and%2520Baoqing%2520Guo%2520and%2520Zujun%2520Yu%2520and%2520Yisheng%2520Lv%26entry.1292438233%3DAccurate%2520and%2520early%2520perception%2520of%2520potential%2520intrusion%2520targets%2520is%2520essential%2520for%2520ensuring%2520the%2520safety%2520of%2520railway%2520transportation%2520systems.%2520However%252C%2520most%2520existing%2520systems%2520focus%2520narrowly%2520on%2520object%2520classification%2520within%2520fixed%2520visual%2520scopes%2520and%2520apply%2520rule-based%2520heuristics%2520to%2520determine%2520intrusion%2520status%252C%2520often%2520overlooking%2520targets%2520that%2520pose%2520latent%2520intrusion%2520risks.%2520Anticipating%2520such%2520risks%2520requires%2520the%2520cognition%2520of%2520spatial%2520context%2520and%2520temporal%2520dynamics%2520for%2520the%2520object%2520of%2520interest%2520%2528OOI%2529%252C%2520which%2520presents%2520challenges%2520for%2520conventional%2520visual%2520models.%2520To%2520facilitate%2520deep%2520intrusion%2520perception%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%252C%2520CogRail%252C%2520which%2520integrates%2520curated%2520open-source%2520datasets%2520with%2520cognitively%2520driven%2520question-answer%2520annotations%2520to%2520support%2520spatio-temporal%2520reasoning%2520and%2520prediction.%2520Building%2520upon%2520this%2520benchmark%252C%2520we%2520conduct%2520a%2520systematic%2520evaluation%2520of%2520state-of-the-art%2520visual-language%2520models%2520%2528VLMs%2529%2520using%2520multimodal%2520prompts%2520to%2520identify%2520their%2520strengths%2520and%2520limitations%2520in%2520this%2520domain.%2520Furthermore%252C%2520we%2520fine-tune%2520VLMs%2520for%2520better%2520performance%2520and%2520propose%2520a%2520joint%2520fine-tuning%2520framework%2520that%2520integrates%2520three%2520core%2520tasks%252C%2520position%2520perception%252C%2520movement%2520prediction%252C%2520and%2520threat%2520analysis%252C%2520facilitating%2520effective%2520adaptation%2520of%2520general-purpose%2520foundation%2520models%2520into%2520specialized%2520models%2520tailored%2520for%2520cognitive%2520intrusion%2520perception.%2520Extensive%2520experiments%2520reveal%2520that%2520current%2520large-scale%2520multimodal%2520models%2520struggle%2520with%2520the%2520complex%2520spatial-temporal%2520reasoning%2520required%2520by%2520the%2520cognitive%2520intrusion%2520perception%2520task%252C%2520underscoring%2520the%2520limitations%2520of%2520existing%2520foundation%2520models%2520in%2520this%2520safety-critical%2520domain.%2520In%2520contrast%252C%2520our%2520proposed%2520joint%2520fine-tuning%2520framework%2520significantly%2520enhances%2520model%2520performance%2520by%2520enabling%2520targeted%2520adaptation%2520to%2520domain-specific%2520reasoning%2520demands%252C%2520highlighting%2520the%2520advantages%2520of%2520structured%2520multi-task%2520learning%2520in%2520improving%2520both%2520accuracy%2520and%2520interpretability.%2520Code%2520will%2520be%2520available%2520at%2520https%253A//github.com/Hub-Tian/CogRail.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogRail%3A%20Benchmarking%20VLMs%20in%20Cognitive%20Intrusion%20Perception%20for%20Intelligent%20Railway%20Transportation%20Systems&entry.906535625=Yonglin%20Tian%20and%20Qiyao%20Zhang%20and%20Wei%20Xu%20and%20Yutong%20Wang%20and%20Yihao%20Wu%20and%20Xinyi%20Li%20and%20Xingyuan%20Dai%20and%20Hui%20Zhang%20and%20Zhiyong%20Cui%20and%20Baoqing%20Guo%20and%20Zujun%20Yu%20and%20Yisheng%20Lv&entry.1292438233=Accurate%20and%20early%20perception%20of%20potential%20intrusion%20targets%20is%20essential%20for%20ensuring%20the%20safety%20of%20railway%20transportation%20systems.%20However%2C%20most%20existing%20systems%20focus%20narrowly%20on%20object%20classification%20within%20fixed%20visual%20scopes%20and%20apply%20rule-based%20heuristics%20to%20determine%20intrusion%20status%2C%20often%20overlooking%20targets%20that%20pose%20latent%20intrusion%20risks.%20Anticipating%20such%20risks%20requires%20the%20cognition%20of%20spatial%20context%20and%20temporal%20dynamics%20for%20the%20object%20of%20interest%20%28OOI%29%2C%20which%20presents%20challenges%20for%20conventional%20visual%20models.%20To%20facilitate%20deep%20intrusion%20perception%2C%20we%20introduce%20a%20novel%20benchmark%2C%20CogRail%2C%20which%20integrates%20curated%20open-source%20datasets%20with%20cognitively%20driven%20question-answer%20annotations%20to%20support%20spatio-temporal%20reasoning%20and%20prediction.%20Building%20upon%20this%20benchmark%2C%20we%20conduct%20a%20systematic%20evaluation%20of%20state-of-the-art%20visual-language%20models%20%28VLMs%29%20using%20multimodal%20prompts%20to%20identify%20their%20strengths%20and%20limitations%20in%20this%20domain.%20Furthermore%2C%20we%20fine-tune%20VLMs%20for%20better%20performance%20and%20propose%20a%20joint%20fine-tuning%20framework%20that%20integrates%20three%20core%20tasks%2C%20position%20perception%2C%20movement%20prediction%2C%20and%20threat%20analysis%2C%20facilitating%20effective%20adaptation%20of%20general-purpose%20foundation%20models%20into%20specialized%20models%20tailored%20for%20cognitive%20intrusion%20perception.%20Extensive%20experiments%20reveal%20that%20current%20large-scale%20multimodal%20models%20struggle%20with%20the%20complex%20spatial-temporal%20reasoning%20required%20by%20the%20cognitive%20intrusion%20perception%20task%2C%20underscoring%20the%20limitations%20of%20existing%20foundation%20models%20in%20this%20safety-critical%20domain.%20In%20contrast%2C%20our%20proposed%20joint%20fine-tuning%20framework%20significantly%20enhances%20model%20performance%20by%20enabling%20targeted%20adaptation%20to%20domain-specific%20reasoning%20demands%2C%20highlighting%20the%20advantages%20of%20structured%20multi-task%20learning%20in%20improving%20both%20accuracy%20and%20interpretability.%20Code%20will%20be%20available%20at%20https%3A//github.com/Hub-Tian/CogRail.&entry.1838667208=http%3A//arxiv.org/abs/2601.09613v1&entry.124074799=Read"},
{"title": "Nonlinear reconciliation: Error reduction theorems", "author": "Lorenzo Nespoli and Anubhab Biswas and Roberto Rocchetta and Vasco Medici", "abstract": "Forecast reconciliation, an ex-post technique applied to forecasts that must satisfy constraints, has been a prominent topic in the forecasting literature over the past two decades. Recently, several efforts have sought to extend reconciliation methods to the probabilistic settings. Nevertheless, formal theorems demonstrating error reduction in nonlinear constraints, analogous to those presented in Panagiotelis et al.(2021), are still lacking. This paper addresses that gap by establishing such theorems for various classes of nonlinear hypersurfaces and vector-valued functions. Specifically, we derive an exact analog of Theorem 3.1 from Panagiotelis et al.(2021) for hypersurfaces with constant-sign curvature. Additionally, we provide an error reduction theorem for the broader case of hypersurfaces with non-constant-sign curvature and for general manifolds with codimension > 1. To support reproducibility and practical adoption, we release a JAX-based Python package, JNLR, implementing the presented theorems and reconciliation procedures.", "link": "http://arxiv.org/abs/2507.22500v2", "date": "2026-01-14", "relevancy": 2.2208, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4451}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4442}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlinear%20reconciliation%3A%20Error%20reduction%20theorems&body=Title%3A%20Nonlinear%20reconciliation%3A%20Error%20reduction%20theorems%0AAuthor%3A%20Lorenzo%20Nespoli%20and%20Anubhab%20Biswas%20and%20Roberto%20Rocchetta%20and%20Vasco%20Medici%0AAbstract%3A%20Forecast%20reconciliation%2C%20an%20ex-post%20technique%20applied%20to%20forecasts%20that%20must%20satisfy%20constraints%2C%20has%20been%20a%20prominent%20topic%20in%20the%20forecasting%20literature%20over%20the%20past%20two%20decades.%20Recently%2C%20several%20efforts%20have%20sought%20to%20extend%20reconciliation%20methods%20to%20the%20probabilistic%20settings.%20Nevertheless%2C%20formal%20theorems%20demonstrating%20error%20reduction%20in%20nonlinear%20constraints%2C%20analogous%20to%20those%20presented%20in%20Panagiotelis%20et%20al.%282021%29%2C%20are%20still%20lacking.%20This%20paper%20addresses%20that%20gap%20by%20establishing%20such%20theorems%20for%20various%20classes%20of%20nonlinear%20hypersurfaces%20and%20vector-valued%20functions.%20Specifically%2C%20we%20derive%20an%20exact%20analog%20of%20Theorem%203.1%20from%20Panagiotelis%20et%20al.%282021%29%20for%20hypersurfaces%20with%20constant-sign%20curvature.%20Additionally%2C%20we%20provide%20an%20error%20reduction%20theorem%20for%20the%20broader%20case%20of%20hypersurfaces%20with%20non-constant-sign%20curvature%20and%20for%20general%20manifolds%20with%20codimension%20%3E%201.%20To%20support%20reproducibility%20and%20practical%20adoption%2C%20we%20release%20a%20JAX-based%20Python%20package%2C%20JNLR%2C%20implementing%20the%20presented%20theorems%20and%20reconciliation%20procedures.%0ALink%3A%20http%3A//arxiv.org/abs/2507.22500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlinear%2520reconciliation%253A%2520Error%2520reduction%2520theorems%26entry.906535625%3DLorenzo%2520Nespoli%2520and%2520Anubhab%2520Biswas%2520and%2520Roberto%2520Rocchetta%2520and%2520Vasco%2520Medici%26entry.1292438233%3DForecast%2520reconciliation%252C%2520an%2520ex-post%2520technique%2520applied%2520to%2520forecasts%2520that%2520must%2520satisfy%2520constraints%252C%2520has%2520been%2520a%2520prominent%2520topic%2520in%2520the%2520forecasting%2520literature%2520over%2520the%2520past%2520two%2520decades.%2520Recently%252C%2520several%2520efforts%2520have%2520sought%2520to%2520extend%2520reconciliation%2520methods%2520to%2520the%2520probabilistic%2520settings.%2520Nevertheless%252C%2520formal%2520theorems%2520demonstrating%2520error%2520reduction%2520in%2520nonlinear%2520constraints%252C%2520analogous%2520to%2520those%2520presented%2520in%2520Panagiotelis%2520et%2520al.%25282021%2529%252C%2520are%2520still%2520lacking.%2520This%2520paper%2520addresses%2520that%2520gap%2520by%2520establishing%2520such%2520theorems%2520for%2520various%2520classes%2520of%2520nonlinear%2520hypersurfaces%2520and%2520vector-valued%2520functions.%2520Specifically%252C%2520we%2520derive%2520an%2520exact%2520analog%2520of%2520Theorem%25203.1%2520from%2520Panagiotelis%2520et%2520al.%25282021%2529%2520for%2520hypersurfaces%2520with%2520constant-sign%2520curvature.%2520Additionally%252C%2520we%2520provide%2520an%2520error%2520reduction%2520theorem%2520for%2520the%2520broader%2520case%2520of%2520hypersurfaces%2520with%2520non-constant-sign%2520curvature%2520and%2520for%2520general%2520manifolds%2520with%2520codimension%2520%253E%25201.%2520To%2520support%2520reproducibility%2520and%2520practical%2520adoption%252C%2520we%2520release%2520a%2520JAX-based%2520Python%2520package%252C%2520JNLR%252C%2520implementing%2520the%2520presented%2520theorems%2520and%2520reconciliation%2520procedures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlinear%20reconciliation%3A%20Error%20reduction%20theorems&entry.906535625=Lorenzo%20Nespoli%20and%20Anubhab%20Biswas%20and%20Roberto%20Rocchetta%20and%20Vasco%20Medici&entry.1292438233=Forecast%20reconciliation%2C%20an%20ex-post%20technique%20applied%20to%20forecasts%20that%20must%20satisfy%20constraints%2C%20has%20been%20a%20prominent%20topic%20in%20the%20forecasting%20literature%20over%20the%20past%20two%20decades.%20Recently%2C%20several%20efforts%20have%20sought%20to%20extend%20reconciliation%20methods%20to%20the%20probabilistic%20settings.%20Nevertheless%2C%20formal%20theorems%20demonstrating%20error%20reduction%20in%20nonlinear%20constraints%2C%20analogous%20to%20those%20presented%20in%20Panagiotelis%20et%20al.%282021%29%2C%20are%20still%20lacking.%20This%20paper%20addresses%20that%20gap%20by%20establishing%20such%20theorems%20for%20various%20classes%20of%20nonlinear%20hypersurfaces%20and%20vector-valued%20functions.%20Specifically%2C%20we%20derive%20an%20exact%20analog%20of%20Theorem%203.1%20from%20Panagiotelis%20et%20al.%282021%29%20for%20hypersurfaces%20with%20constant-sign%20curvature.%20Additionally%2C%20we%20provide%20an%20error%20reduction%20theorem%20for%20the%20broader%20case%20of%20hypersurfaces%20with%20non-constant-sign%20curvature%20and%20for%20general%20manifolds%20with%20codimension%20%3E%201.%20To%20support%20reproducibility%20and%20practical%20adoption%2C%20we%20release%20a%20JAX-based%20Python%20package%2C%20JNLR%2C%20implementing%20the%20presented%20theorems%20and%20reconciliation%20procedures.&entry.1838667208=http%3A//arxiv.org/abs/2507.22500v2&entry.124074799=Read"},
{"title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols", "author": "Ziming Liu and Bryan Liu and Alvaro Valcarce and Xiaoli Chu", "abstract": "Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler of the AI-Native Air Interface (AI-AI), where protocol intelligence must scale beyond handcrafted logic. This paper presents, to our knowledge, the first standards-compliant emulation of the Radio Resource Control (RRC) layer using a decoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a multi-vendor corpus of real-world traces spanning both 5G and 4G systems. We treat RRC as a domain-specific language and construct a segmentation-safe, question-answer (Question-and-Answer (QA)) dataset that preserves Abstract Syntax Notation (ASN.1) structure through linearization prior to Byte Pair Encoding (BPE) tokenization. The proposed approach combines parameter-efficient adaptation with schema-bounded prompting to ensure syntactic and procedural fidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance, field-level coverage analysis, and uplink-to-downlink state-machine checks -- alongside semantic similarity and latency profiling across 120 configurations. On 30k 5G request-response pairs plus an additional 4.8k QA turns from 4G sessions, our 8B model achieves a median cosine similarity of 0.97, a 61% relative gain over a zero-shot baseline, while sustaining high conformance rates. These results demonstrate that LAMs, when augmented with protocol-aware reasoning, can directly orchestrate control-plane procedures, laying the foundation for the future Artificial Intelligence (AI)-native Radio Access Network (RAN).", "link": "http://arxiv.org/abs/2505.16821v4", "date": "2026-01-14", "relevancy": 2.1948, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Based%20Emulation%20of%20the%20Radio%20Resource%20Control%20Layer%3A%20Towards%20AI-Native%20RAN%20Protocols&body=Title%3A%20LLM-Based%20Emulation%20of%20the%20Radio%20Resource%20Control%20Layer%3A%20Towards%20AI-Native%20RAN%20Protocols%0AAuthor%3A%20Ziming%20Liu%20and%20Bryan%20Liu%20and%20Alvaro%20Valcarce%20and%20Xiaoli%20Chu%0AAbstract%3A%20Integrating%20Large%20AI%20Models%20%28LAMs%29%20into%206G%20mobile%20networks%20is%20a%20key%20enabler%20of%20the%20AI-Native%20Air%20Interface%20%28AI-AI%29%2C%20where%20protocol%20intelligence%20must%20scale%20beyond%20handcrafted%20logic.%20This%20paper%20presents%2C%20to%20our%20knowledge%2C%20the%20first%20standards-compliant%20emulation%20of%20the%20Radio%20Resource%20Control%20%28RRC%29%20layer%20using%20a%20decoder-only%20LAM%20%28LLAMA-class%29%20fine-tuned%20with%20Low-Rank%20Adaptation%20%28LoRA%29%20on%20a%20multi-vendor%20corpus%20of%20real-world%20traces%20spanning%20both%205G%20and%204G%20systems.%20We%20treat%20RRC%20as%20a%20domain-specific%20language%20and%20construct%20a%20segmentation-safe%2C%20question-answer%20%28Question-and-Answer%20%28QA%29%29%20dataset%20that%20preserves%20Abstract%20Syntax%20Notation%20%28ASN.1%29%20structure%20through%20linearization%20prior%20to%20Byte%20Pair%20Encoding%20%28BPE%29%20tokenization.%20The%20proposed%20approach%20combines%20parameter-efficient%20adaptation%20with%20schema-bounded%20prompting%20to%20ensure%20syntactic%20and%20procedural%20fidelity.%20Evaluation%20introduces%20a%20standards-aware%20triad%20--%20ASN.1%20conformance%2C%20field-level%20coverage%20analysis%2C%20and%20uplink-to-downlink%20state-machine%20checks%20--%20alongside%20semantic%20similarity%20and%20latency%20profiling%20across%20120%20configurations.%20On%2030k%205G%20request-response%20pairs%20plus%20an%20additional%204.8k%20QA%20turns%20from%204G%20sessions%2C%20our%208B%20model%20achieves%20a%20median%20cosine%20similarity%20of%200.97%2C%20a%2061%25%20relative%20gain%20over%20a%20zero-shot%20baseline%2C%20while%20sustaining%20high%20conformance%20rates.%20These%20results%20demonstrate%20that%20LAMs%2C%20when%20augmented%20with%20protocol-aware%20reasoning%2C%20can%20directly%20orchestrate%20control-plane%20procedures%2C%20laying%20the%20foundation%20for%20the%20future%20Artificial%20Intelligence%20%28AI%29-native%20Radio%20Access%20Network%20%28RAN%29.%0ALink%3A%20http%3A//arxiv.org/abs/2505.16821v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Based%2520Emulation%2520of%2520the%2520Radio%2520Resource%2520Control%2520Layer%253A%2520Towards%2520AI-Native%2520RAN%2520Protocols%26entry.906535625%3DZiming%2520Liu%2520and%2520Bryan%2520Liu%2520and%2520Alvaro%2520Valcarce%2520and%2520Xiaoli%2520Chu%26entry.1292438233%3DIntegrating%2520Large%2520AI%2520Models%2520%2528LAMs%2529%2520into%25206G%2520mobile%2520networks%2520is%2520a%2520key%2520enabler%2520of%2520the%2520AI-Native%2520Air%2520Interface%2520%2528AI-AI%2529%252C%2520where%2520protocol%2520intelligence%2520must%2520scale%2520beyond%2520handcrafted%2520logic.%2520This%2520paper%2520presents%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520standards-compliant%2520emulation%2520of%2520the%2520Radio%2520Resource%2520Control%2520%2528RRC%2529%2520layer%2520using%2520a%2520decoder-only%2520LAM%2520%2528LLAMA-class%2529%2520fine-tuned%2520with%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520on%2520a%2520multi-vendor%2520corpus%2520of%2520real-world%2520traces%2520spanning%2520both%25205G%2520and%25204G%2520systems.%2520We%2520treat%2520RRC%2520as%2520a%2520domain-specific%2520language%2520and%2520construct%2520a%2520segmentation-safe%252C%2520question-answer%2520%2528Question-and-Answer%2520%2528QA%2529%2529%2520dataset%2520that%2520preserves%2520Abstract%2520Syntax%2520Notation%2520%2528ASN.1%2529%2520structure%2520through%2520linearization%2520prior%2520to%2520Byte%2520Pair%2520Encoding%2520%2528BPE%2529%2520tokenization.%2520The%2520proposed%2520approach%2520combines%2520parameter-efficient%2520adaptation%2520with%2520schema-bounded%2520prompting%2520to%2520ensure%2520syntactic%2520and%2520procedural%2520fidelity.%2520Evaluation%2520introduces%2520a%2520standards-aware%2520triad%2520--%2520ASN.1%2520conformance%252C%2520field-level%2520coverage%2520analysis%252C%2520and%2520uplink-to-downlink%2520state-machine%2520checks%2520--%2520alongside%2520semantic%2520similarity%2520and%2520latency%2520profiling%2520across%2520120%2520configurations.%2520On%252030k%25205G%2520request-response%2520pairs%2520plus%2520an%2520additional%25204.8k%2520QA%2520turns%2520from%25204G%2520sessions%252C%2520our%25208B%2520model%2520achieves%2520a%2520median%2520cosine%2520similarity%2520of%25200.97%252C%2520a%252061%2525%2520relative%2520gain%2520over%2520a%2520zero-shot%2520baseline%252C%2520while%2520sustaining%2520high%2520conformance%2520rates.%2520These%2520results%2520demonstrate%2520that%2520LAMs%252C%2520when%2520augmented%2520with%2520protocol-aware%2520reasoning%252C%2520can%2520directly%2520orchestrate%2520control-plane%2520procedures%252C%2520laying%2520the%2520foundation%2520for%2520the%2520future%2520Artificial%2520Intelligence%2520%2528AI%2529-native%2520Radio%2520Access%2520Network%2520%2528RAN%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16821v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Based%20Emulation%20of%20the%20Radio%20Resource%20Control%20Layer%3A%20Towards%20AI-Native%20RAN%20Protocols&entry.906535625=Ziming%20Liu%20and%20Bryan%20Liu%20and%20Alvaro%20Valcarce%20and%20Xiaoli%20Chu&entry.1292438233=Integrating%20Large%20AI%20Models%20%28LAMs%29%20into%206G%20mobile%20networks%20is%20a%20key%20enabler%20of%20the%20AI-Native%20Air%20Interface%20%28AI-AI%29%2C%20where%20protocol%20intelligence%20must%20scale%20beyond%20handcrafted%20logic.%20This%20paper%20presents%2C%20to%20our%20knowledge%2C%20the%20first%20standards-compliant%20emulation%20of%20the%20Radio%20Resource%20Control%20%28RRC%29%20layer%20using%20a%20decoder-only%20LAM%20%28LLAMA-class%29%20fine-tuned%20with%20Low-Rank%20Adaptation%20%28LoRA%29%20on%20a%20multi-vendor%20corpus%20of%20real-world%20traces%20spanning%20both%205G%20and%204G%20systems.%20We%20treat%20RRC%20as%20a%20domain-specific%20language%20and%20construct%20a%20segmentation-safe%2C%20question-answer%20%28Question-and-Answer%20%28QA%29%29%20dataset%20that%20preserves%20Abstract%20Syntax%20Notation%20%28ASN.1%29%20structure%20through%20linearization%20prior%20to%20Byte%20Pair%20Encoding%20%28BPE%29%20tokenization.%20The%20proposed%20approach%20combines%20parameter-efficient%20adaptation%20with%20schema-bounded%20prompting%20to%20ensure%20syntactic%20and%20procedural%20fidelity.%20Evaluation%20introduces%20a%20standards-aware%20triad%20--%20ASN.1%20conformance%2C%20field-level%20coverage%20analysis%2C%20and%20uplink-to-downlink%20state-machine%20checks%20--%20alongside%20semantic%20similarity%20and%20latency%20profiling%20across%20120%20configurations.%20On%2030k%205G%20request-response%20pairs%20plus%20an%20additional%204.8k%20QA%20turns%20from%204G%20sessions%2C%20our%208B%20model%20achieves%20a%20median%20cosine%20similarity%20of%200.97%2C%20a%2061%25%20relative%20gain%20over%20a%20zero-shot%20baseline%2C%20while%20sustaining%20high%20conformance%20rates.%20These%20results%20demonstrate%20that%20LAMs%2C%20when%20augmented%20with%20protocol-aware%20reasoning%2C%20can%20directly%20orchestrate%20control-plane%20procedures%2C%20laying%20the%20foundation%20for%20the%20future%20Artificial%20Intelligence%20%28AI%29-native%20Radio%20Access%20Network%20%28RAN%29.&entry.1838667208=http%3A//arxiv.org/abs/2505.16821v4&entry.124074799=Read"},
{"title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding", "author": "Siyuan Liu and Hongbang Yuan and Xinze Li and Ziyue Zhu and Yixin Cao and Yu-Gang Jiang", "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.", "link": "http://arxiv.org/abs/2601.09503v1", "date": "2026-01-14", "relevancy": 2.1915, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Do%20LLM%20Agents%20Know%20About%20Their%20World%3F%20Task2Quiz%3A%20A%20Paradigm%20for%20Studying%20Environment%20Understanding&body=Title%3A%20What%20Do%20LLM%20Agents%20Know%20About%20Their%20World%3F%20Task2Quiz%3A%20A%20Paradigm%20for%20Studying%20Environment%20Understanding%0AAuthor%3A%20Siyuan%20Liu%20and%20Hongbang%20Yuan%20and%20Xinze%20Li%20and%20Ziyue%20Zhu%20and%20Yixin%20Cao%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20Large%20language%20model%20%28LLM%29%20agents%20have%20demonstrated%20remarkable%20capabilities%20in%20complex%20decision-making%20and%20tool-use%20tasks%2C%20yet%20their%20ability%20to%20generalize%20across%20varying%20environments%20remains%20a%20under-examined%20concern.%20Current%20evaluation%20paradigms%20predominantly%20rely%20on%20trajectory-based%20metrics%20that%20measure%20task%20success%2C%20while%20failing%20to%20assess%20whether%20agents%20possess%20a%20grounded%2C%20transferable%20model%20of%20the%20environment.%20To%20address%20this%20gap%2C%20we%20propose%20Task-to-Quiz%20%28T2Q%29%2C%20a%20deterministic%20and%20automated%20evaluation%20paradigm%20designed%20to%20decouple%20task%20execution%20from%20world-state%20understanding.%20We%20instantiate%20this%20paradigm%20in%20T2QBench%2C%20a%20suite%20comprising%2030%20environments%20and%201%2C967%20grounded%20QA%20pairs%20across%20multiple%20difficulty%20levels.%20Our%20extensive%20experiments%20reveal%20that%20task%20success%20is%20often%20a%20poor%20proxy%20for%20environment%20understanding%2C%20and%20that%20current%20memory%20machanism%20can%20not%20effectively%20help%20agents%20acquire%20a%20grounded%20model%20of%20the%20environment.%20These%20findings%20identify%20proactive%20exploration%20and%20fine-grained%20state%20representation%20as%20primary%20bottlenecks%2C%20offering%20a%20robust%20foundation%20for%20developing%20more%20generalizable%20autonomous%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Do%2520LLM%2520Agents%2520Know%2520About%2520Their%2520World%253F%2520Task2Quiz%253A%2520A%2520Paradigm%2520for%2520Studying%2520Environment%2520Understanding%26entry.906535625%3DSiyuan%2520Liu%2520and%2520Hongbang%2520Yuan%2520and%2520Xinze%2520Li%2520and%2520Ziyue%2520Zhu%2520and%2520Yixin%2520Cao%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3DLarge%2520language%2520model%2520%2528LLM%2529%2520agents%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520complex%2520decision-making%2520and%2520tool-use%2520tasks%252C%2520yet%2520their%2520ability%2520to%2520generalize%2520across%2520varying%2520environments%2520remains%2520a%2520under-examined%2520concern.%2520Current%2520evaluation%2520paradigms%2520predominantly%2520rely%2520on%2520trajectory-based%2520metrics%2520that%2520measure%2520task%2520success%252C%2520while%2520failing%2520to%2520assess%2520whether%2520agents%2520possess%2520a%2520grounded%252C%2520transferable%2520model%2520of%2520the%2520environment.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Task-to-Quiz%2520%2528T2Q%2529%252C%2520a%2520deterministic%2520and%2520automated%2520evaluation%2520paradigm%2520designed%2520to%2520decouple%2520task%2520execution%2520from%2520world-state%2520understanding.%2520We%2520instantiate%2520this%2520paradigm%2520in%2520T2QBench%252C%2520a%2520suite%2520comprising%252030%2520environments%2520and%25201%252C967%2520grounded%2520QA%2520pairs%2520across%2520multiple%2520difficulty%2520levels.%2520Our%2520extensive%2520experiments%2520reveal%2520that%2520task%2520success%2520is%2520often%2520a%2520poor%2520proxy%2520for%2520environment%2520understanding%252C%2520and%2520that%2520current%2520memory%2520machanism%2520can%2520not%2520effectively%2520help%2520agents%2520acquire%2520a%2520grounded%2520model%2520of%2520the%2520environment.%2520These%2520findings%2520identify%2520proactive%2520exploration%2520and%2520fine-grained%2520state%2520representation%2520as%2520primary%2520bottlenecks%252C%2520offering%2520a%2520robust%2520foundation%2520for%2520developing%2520more%2520generalizable%2520autonomous%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Do%20LLM%20Agents%20Know%20About%20Their%20World%3F%20Task2Quiz%3A%20A%20Paradigm%20for%20Studying%20Environment%20Understanding&entry.906535625=Siyuan%20Liu%20and%20Hongbang%20Yuan%20and%20Xinze%20Li%20and%20Ziyue%20Zhu%20and%20Yixin%20Cao%20and%20Yu-Gang%20Jiang&entry.1292438233=Large%20language%20model%20%28LLM%29%20agents%20have%20demonstrated%20remarkable%20capabilities%20in%20complex%20decision-making%20and%20tool-use%20tasks%2C%20yet%20their%20ability%20to%20generalize%20across%20varying%20environments%20remains%20a%20under-examined%20concern.%20Current%20evaluation%20paradigms%20predominantly%20rely%20on%20trajectory-based%20metrics%20that%20measure%20task%20success%2C%20while%20failing%20to%20assess%20whether%20agents%20possess%20a%20grounded%2C%20transferable%20model%20of%20the%20environment.%20To%20address%20this%20gap%2C%20we%20propose%20Task-to-Quiz%20%28T2Q%29%2C%20a%20deterministic%20and%20automated%20evaluation%20paradigm%20designed%20to%20decouple%20task%20execution%20from%20world-state%20understanding.%20We%20instantiate%20this%20paradigm%20in%20T2QBench%2C%20a%20suite%20comprising%2030%20environments%20and%201%2C967%20grounded%20QA%20pairs%20across%20multiple%20difficulty%20levels.%20Our%20extensive%20experiments%20reveal%20that%20task%20success%20is%20often%20a%20poor%20proxy%20for%20environment%20understanding%2C%20and%20that%20current%20memory%20machanism%20can%20not%20effectively%20help%20agents%20acquire%20a%20grounded%20model%20of%20the%20environment.%20These%20findings%20identify%20proactive%20exploration%20and%20fine-grained%20state%20representation%20as%20primary%20bottlenecks%2C%20offering%20a%20robust%20foundation%20for%20developing%20more%20generalizable%20autonomous%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.09503v1&entry.124074799=Read"},
{"title": "What Can RL Bring to VLA Generalization? An Empirical Study", "author": "Jijia Liu and Feng Gao and Bingwen Wei and Xinlei Chen and Qingmin Liao and Yi Wu and Chao Yu and Yu Wang", "abstract": "Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io", "link": "http://arxiv.org/abs/2505.19789v4", "date": "2026-01-14", "relevancy": 2.1683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20RL%20Bring%20to%20VLA%20Generalization%3F%20An%20Empirical%20Study&body=Title%3A%20What%20Can%20RL%20Bring%20to%20VLA%20Generalization%3F%20An%20Empirical%20Study%0AAuthor%3A%20Jijia%20Liu%20and%20Feng%20Gao%20and%20Bingwen%20Wei%20and%20Xinlei%20Chen%20and%20Qingmin%20Liao%20and%20Yi%20Wu%20and%20Chao%20Yu%20and%20Yu%20Wang%0AAbstract%3A%20Large%20Vision-Language%20Action%20%28VLA%29%20models%20have%20shown%20significant%20potential%20for%20embodied%20AI.%20However%2C%20their%20predominant%20training%20via%20supervised%20fine-tuning%20%28SFT%29%20limits%20generalization%20due%20to%20susceptibility%20to%20compounding%20errors%20under%20distribution%20shifts.%20Reinforcement%20learning%20%28RL%29%20offers%20a%20path%20to%20overcome%20these%20limitations%20by%20optimizing%20for%20task%20objectives%20via%20trial-and-error%2C%20yet%20a%20systematic%20understanding%20of%20its%20specific%20generalization%20benefits%20for%20VLAs%20compared%20to%20SFT%20is%20lacking.%20To%20address%20this%2C%20our%20study%20introduces%20a%20comprehensive%20benchmark%20for%20evaluating%20VLA%20generalization%20and%20systematically%20investigates%20the%20impact%20of%20RL%20fine-tuning%20across%20diverse%20visual%2C%20semantic%2C%20and%20execution%20dimensions.%20Our%20extensive%20experiments%20reveal%20that%20RL%20fine-tuning%2C%20particularly%20with%20PPO%2C%20significantly%20enhances%20generalization%20in%20semantic%20understanding%20and%20execution%20robustness%20over%20SFT%2C%20while%20maintaining%20comparable%20visual%20robustness.%20We%20identify%20PPO%20as%20a%20more%20effective%20RL%20algorithm%20for%20VLAs%20than%20LLM-derived%20methods%20like%20DPO%20and%20GRPO.%20We%20also%20develop%20a%20simple%20recipe%20for%20efficient%20PPO%20training%20on%20VLAs%2C%20and%20demonstrate%20its%20practical%20utility%20for%20improving%20VLA%20generalization.%20The%20project%20page%20is%20at%20https%3A//rlvla.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2505.19789v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520RL%2520Bring%2520to%2520VLA%2520Generalization%253F%2520An%2520Empirical%2520Study%26entry.906535625%3DJijia%2520Liu%2520and%2520Feng%2520Gao%2520and%2520Bingwen%2520Wei%2520and%2520Xinlei%2520Chen%2520and%2520Qingmin%2520Liao%2520and%2520Yi%2520Wu%2520and%2520Chao%2520Yu%2520and%2520Yu%2520Wang%26entry.1292438233%3DLarge%2520Vision-Language%2520Action%2520%2528VLA%2529%2520models%2520have%2520shown%2520significant%2520potential%2520for%2520embodied%2520AI.%2520However%252C%2520their%2520predominant%2520training%2520via%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520limits%2520generalization%2520due%2520to%2520susceptibility%2520to%2520compounding%2520errors%2520under%2520distribution%2520shifts.%2520Reinforcement%2520learning%2520%2528RL%2529%2520offers%2520a%2520path%2520to%2520overcome%2520these%2520limitations%2520by%2520optimizing%2520for%2520task%2520objectives%2520via%2520trial-and-error%252C%2520yet%2520a%2520systematic%2520understanding%2520of%2520its%2520specific%2520generalization%2520benefits%2520for%2520VLAs%2520compared%2520to%2520SFT%2520is%2520lacking.%2520To%2520address%2520this%252C%2520our%2520study%2520introduces%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520VLA%2520generalization%2520and%2520systematically%2520investigates%2520the%2520impact%2520of%2520RL%2520fine-tuning%2520across%2520diverse%2520visual%252C%2520semantic%252C%2520and%2520execution%2520dimensions.%2520Our%2520extensive%2520experiments%2520reveal%2520that%2520RL%2520fine-tuning%252C%2520particularly%2520with%2520PPO%252C%2520significantly%2520enhances%2520generalization%2520in%2520semantic%2520understanding%2520and%2520execution%2520robustness%2520over%2520SFT%252C%2520while%2520maintaining%2520comparable%2520visual%2520robustness.%2520We%2520identify%2520PPO%2520as%2520a%2520more%2520effective%2520RL%2520algorithm%2520for%2520VLAs%2520than%2520LLM-derived%2520methods%2520like%2520DPO%2520and%2520GRPO.%2520We%2520also%2520develop%2520a%2520simple%2520recipe%2520for%2520efficient%2520PPO%2520training%2520on%2520VLAs%252C%2520and%2520demonstrate%2520its%2520practical%2520utility%2520for%2520improving%2520VLA%2520generalization.%2520The%2520project%2520page%2520is%2520at%2520https%253A//rlvla.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19789v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20RL%20Bring%20to%20VLA%20Generalization%3F%20An%20Empirical%20Study&entry.906535625=Jijia%20Liu%20and%20Feng%20Gao%20and%20Bingwen%20Wei%20and%20Xinlei%20Chen%20and%20Qingmin%20Liao%20and%20Yi%20Wu%20and%20Chao%20Yu%20and%20Yu%20Wang&entry.1292438233=Large%20Vision-Language%20Action%20%28VLA%29%20models%20have%20shown%20significant%20potential%20for%20embodied%20AI.%20However%2C%20their%20predominant%20training%20via%20supervised%20fine-tuning%20%28SFT%29%20limits%20generalization%20due%20to%20susceptibility%20to%20compounding%20errors%20under%20distribution%20shifts.%20Reinforcement%20learning%20%28RL%29%20offers%20a%20path%20to%20overcome%20these%20limitations%20by%20optimizing%20for%20task%20objectives%20via%20trial-and-error%2C%20yet%20a%20systematic%20understanding%20of%20its%20specific%20generalization%20benefits%20for%20VLAs%20compared%20to%20SFT%20is%20lacking.%20To%20address%20this%2C%20our%20study%20introduces%20a%20comprehensive%20benchmark%20for%20evaluating%20VLA%20generalization%20and%20systematically%20investigates%20the%20impact%20of%20RL%20fine-tuning%20across%20diverse%20visual%2C%20semantic%2C%20and%20execution%20dimensions.%20Our%20extensive%20experiments%20reveal%20that%20RL%20fine-tuning%2C%20particularly%20with%20PPO%2C%20significantly%20enhances%20generalization%20in%20semantic%20understanding%20and%20execution%20robustness%20over%20SFT%2C%20while%20maintaining%20comparable%20visual%20robustness.%20We%20identify%20PPO%20as%20a%20more%20effective%20RL%20algorithm%20for%20VLAs%20than%20LLM-derived%20methods%20like%20DPO%20and%20GRPO.%20We%20also%20develop%20a%20simple%20recipe%20for%20efficient%20PPO%20training%20on%20VLAs%2C%20and%20demonstrate%20its%20practical%20utility%20for%20improving%20VLA%20generalization.%20The%20project%20page%20is%20at%20https%3A//rlvla.github.io&entry.1838667208=http%3A//arxiv.org/abs/2505.19789v4&entry.124074799=Read"},
{"title": "Template-Based Probes Are Imperfect Lenses for Counterfactual Bias Evaluation in LLMs", "author": "Farnaz Kohankhaki and D. B. Emerson and Jacob-Junqi Tian and Laleh Seyyed-Kalantari and Faiza Khan Khattak", "abstract": "Bias in large language models (LLMs) has many forms, from overt discrimination to implicit stereotypes. Counterfactual bias evaluation is a widely used approach to quantifying bias and often relies on template-based probes that explicitly state group membership. It aims to measure whether the outcome of a task performed by an LLM is invariant to a change in group membership. In this work, we find that template-based probes can introduce systematic distortions in bias measurements. Specifically, we consistently find that such probes suggest that LLMs classify text associated with White race as negative at disproportionately elevated rates. This is observed consistently across a large collection of LLMs, over several diverse template-based probes, and with different classification approaches. We hypothesize that this arises artificially due to linguistic asymmetries present in LLM pretraining data, in the form of markedness, (e.g., Black president vs. president) and templates used for bias measurement (e.g., Black president vs. White president). These findings highlight the need for more rigorous methodologies in counterfactual bias evaluation, ensuring that observed disparities reflect genuine biases rather than artifacts of linguistic conventions.", "link": "http://arxiv.org/abs/2404.03471v5", "date": "2026-01-14", "relevancy": 2.1648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.439}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.439}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Template-Based%20Probes%20Are%20Imperfect%20Lenses%20for%20Counterfactual%20Bias%20Evaluation%20in%20LLMs&body=Title%3A%20Template-Based%20Probes%20Are%20Imperfect%20Lenses%20for%20Counterfactual%20Bias%20Evaluation%20in%20LLMs%0AAuthor%3A%20Farnaz%20Kohankhaki%20and%20D.%20B.%20Emerson%20and%20Jacob-Junqi%20Tian%20and%20Laleh%20Seyyed-Kalantari%20and%20Faiza%20Khan%20Khattak%0AAbstract%3A%20Bias%20in%20large%20language%20models%20%28LLMs%29%20has%20many%20forms%2C%20from%20overt%20discrimination%20to%20implicit%20stereotypes.%20Counterfactual%20bias%20evaluation%20is%20a%20widely%20used%20approach%20to%20quantifying%20bias%20and%20often%20relies%20on%20template-based%20probes%20that%20explicitly%20state%20group%20membership.%20It%20aims%20to%20measure%20whether%20the%20outcome%20of%20a%20task%20performed%20by%20an%20LLM%20is%20invariant%20to%20a%20change%20in%20group%20membership.%20In%20this%20work%2C%20we%20find%20that%20template-based%20probes%20can%20introduce%20systematic%20distortions%20in%20bias%20measurements.%20Specifically%2C%20we%20consistently%20find%20that%20such%20probes%20suggest%20that%20LLMs%20classify%20text%20associated%20with%20White%20race%20as%20negative%20at%20disproportionately%20elevated%20rates.%20This%20is%20observed%20consistently%20across%20a%20large%20collection%20of%20LLMs%2C%20over%20several%20diverse%20template-based%20probes%2C%20and%20with%20different%20classification%20approaches.%20We%20hypothesize%20that%20this%20arises%20artificially%20due%20to%20linguistic%20asymmetries%20present%20in%20LLM%20pretraining%20data%2C%20in%20the%20form%20of%20markedness%2C%20%28e.g.%2C%20Black%20president%20vs.%20president%29%20and%20templates%20used%20for%20bias%20measurement%20%28e.g.%2C%20Black%20president%20vs.%20White%20president%29.%20These%20findings%20highlight%20the%20need%20for%20more%20rigorous%20methodologies%20in%20counterfactual%20bias%20evaluation%2C%20ensuring%20that%20observed%20disparities%20reflect%20genuine%20biases%20rather%20than%20artifacts%20of%20linguistic%20conventions.%0ALink%3A%20http%3A//arxiv.org/abs/2404.03471v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemplate-Based%2520Probes%2520Are%2520Imperfect%2520Lenses%2520for%2520Counterfactual%2520Bias%2520Evaluation%2520in%2520LLMs%26entry.906535625%3DFarnaz%2520Kohankhaki%2520and%2520D.%2520B.%2520Emerson%2520and%2520Jacob-Junqi%2520Tian%2520and%2520Laleh%2520Seyyed-Kalantari%2520and%2520Faiza%2520Khan%2520Khattak%26entry.1292438233%3DBias%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520many%2520forms%252C%2520from%2520overt%2520discrimination%2520to%2520implicit%2520stereotypes.%2520Counterfactual%2520bias%2520evaluation%2520is%2520a%2520widely%2520used%2520approach%2520to%2520quantifying%2520bias%2520and%2520often%2520relies%2520on%2520template-based%2520probes%2520that%2520explicitly%2520state%2520group%2520membership.%2520It%2520aims%2520to%2520measure%2520whether%2520the%2520outcome%2520of%2520a%2520task%2520performed%2520by%2520an%2520LLM%2520is%2520invariant%2520to%2520a%2520change%2520in%2520group%2520membership.%2520In%2520this%2520work%252C%2520we%2520find%2520that%2520template-based%2520probes%2520can%2520introduce%2520systematic%2520distortions%2520in%2520bias%2520measurements.%2520Specifically%252C%2520we%2520consistently%2520find%2520that%2520such%2520probes%2520suggest%2520that%2520LLMs%2520classify%2520text%2520associated%2520with%2520White%2520race%2520as%2520negative%2520at%2520disproportionately%2520elevated%2520rates.%2520This%2520is%2520observed%2520consistently%2520across%2520a%2520large%2520collection%2520of%2520LLMs%252C%2520over%2520several%2520diverse%2520template-based%2520probes%252C%2520and%2520with%2520different%2520classification%2520approaches.%2520We%2520hypothesize%2520that%2520this%2520arises%2520artificially%2520due%2520to%2520linguistic%2520asymmetries%2520present%2520in%2520LLM%2520pretraining%2520data%252C%2520in%2520the%2520form%2520of%2520markedness%252C%2520%2528e.g.%252C%2520Black%2520president%2520vs.%2520president%2529%2520and%2520templates%2520used%2520for%2520bias%2520measurement%2520%2528e.g.%252C%2520Black%2520president%2520vs.%2520White%2520president%2529.%2520These%2520findings%2520highlight%2520the%2520need%2520for%2520more%2520rigorous%2520methodologies%2520in%2520counterfactual%2520bias%2520evaluation%252C%2520ensuring%2520that%2520observed%2520disparities%2520reflect%2520genuine%2520biases%2520rather%2520than%2520artifacts%2520of%2520linguistic%2520conventions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03471v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Template-Based%20Probes%20Are%20Imperfect%20Lenses%20for%20Counterfactual%20Bias%20Evaluation%20in%20LLMs&entry.906535625=Farnaz%20Kohankhaki%20and%20D.%20B.%20Emerson%20and%20Jacob-Junqi%20Tian%20and%20Laleh%20Seyyed-Kalantari%20and%20Faiza%20Khan%20Khattak&entry.1292438233=Bias%20in%20large%20language%20models%20%28LLMs%29%20has%20many%20forms%2C%20from%20overt%20discrimination%20to%20implicit%20stereotypes.%20Counterfactual%20bias%20evaluation%20is%20a%20widely%20used%20approach%20to%20quantifying%20bias%20and%20often%20relies%20on%20template-based%20probes%20that%20explicitly%20state%20group%20membership.%20It%20aims%20to%20measure%20whether%20the%20outcome%20of%20a%20task%20performed%20by%20an%20LLM%20is%20invariant%20to%20a%20change%20in%20group%20membership.%20In%20this%20work%2C%20we%20find%20that%20template-based%20probes%20can%20introduce%20systematic%20distortions%20in%20bias%20measurements.%20Specifically%2C%20we%20consistently%20find%20that%20such%20probes%20suggest%20that%20LLMs%20classify%20text%20associated%20with%20White%20race%20as%20negative%20at%20disproportionately%20elevated%20rates.%20This%20is%20observed%20consistently%20across%20a%20large%20collection%20of%20LLMs%2C%20over%20several%20diverse%20template-based%20probes%2C%20and%20with%20different%20classification%20approaches.%20We%20hypothesize%20that%20this%20arises%20artificially%20due%20to%20linguistic%20asymmetries%20present%20in%20LLM%20pretraining%20data%2C%20in%20the%20form%20of%20markedness%2C%20%28e.g.%2C%20Black%20president%20vs.%20president%29%20and%20templates%20used%20for%20bias%20measurement%20%28e.g.%2C%20Black%20president%20vs.%20White%20president%29.%20These%20findings%20highlight%20the%20need%20for%20more%20rigorous%20methodologies%20in%20counterfactual%20bias%20evaluation%2C%20ensuring%20that%20observed%20disparities%20reflect%20genuine%20biases%20rather%20than%20artifacts%20of%20linguistic%20conventions.&entry.1838667208=http%3A//arxiv.org/abs/2404.03471v5&entry.124074799=Read"},
{"title": "Iterative Differential Entropy Minimization (IDEM) method for fine rigid pairwise 3D Point Cloud Registration: A Focus on the Metric", "author": "Emmanuele Barberi and Felice Sfravara and Filippo Cucinotta", "abstract": "Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.", "link": "http://arxiv.org/abs/2601.09601v1", "date": "2026-01-14", "relevancy": 2.1574, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5602}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5448}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Differential%20Entropy%20Minimization%20%28IDEM%29%20method%20for%20fine%20rigid%20pairwise%203D%20Point%20Cloud%20Registration%3A%20A%20Focus%20on%20the%20Metric&body=Title%3A%20Iterative%20Differential%20Entropy%20Minimization%20%28IDEM%29%20method%20for%20fine%20rigid%20pairwise%203D%20Point%20Cloud%20Registration%3A%20A%20Focus%20on%20the%20Metric%0AAuthor%3A%20Emmanuele%20Barberi%20and%20Felice%20Sfravara%20and%20Filippo%20Cucinotta%0AAbstract%3A%20Point%20cloud%20registration%20is%20a%20central%20theme%20in%20computer%20vision%2C%20with%20alignment%20algorithms%20continuously%20improving%20for%20greater%20robustness.%20Commonly%20used%20methods%20evaluate%20Euclidean%20distances%20between%20point%20clouds%20and%20minimize%20an%20objective%20function%2C%20such%20as%20Root%20Mean%20Square%20Error%20%28RMSE%29.%20However%2C%20these%20approaches%20are%20most%20effective%20when%20the%20point%20clouds%20are%20well-prealigned%20and%20issues%20such%20as%20differences%20in%20density%2C%20noise%2C%20holes%2C%20and%20limited%20overlap%20can%20compromise%20the%20results.%20Traditional%20methods%2C%20such%20as%20Iterative%20Closest%20Point%20%28ICP%29%2C%20require%20choosing%20one%20point%20cloud%20as%20fixed%2C%20since%20Euclidean%20distances%20lack%20commutativity.%20When%20only%20one%20point%20cloud%20has%20issues%2C%20adjustments%20can%20be%20made%2C%20but%20in%20real%20scenarios%2C%20both%20point%20clouds%20may%20be%20affected%2C%20often%20necessitating%20preprocessing.%20The%20authors%20introduce%20a%20novel%20differential%20entropy-based%20metric%2C%20designed%20to%20serve%20as%20the%20objective%20function%20within%20an%20optimization%20framework%20for%20fine%20rigid%20pairwise%203D%20point%20cloud%20registration%2C%20denoted%20as%20Iterative%20Differential%20Entropy%20Minimization%20%28IDEM%29.%20This%20metric%20does%20not%20depend%20on%20the%20choice%20of%20a%20fixed%20point%20cloud%20and%2C%20during%20transformations%2C%20reveals%20a%20clear%20minimum%20corresponding%20to%20the%20best%20alignment.%20Multiple%20case%20studies%20are%20conducted%2C%20and%20the%20results%20are%20compared%20with%20those%20obtained%20using%20RMSE%2C%20Chamfer%20distance%2C%20and%20Hausdorff%20distance.%20The%20proposed%20metric%20proves%20effective%20even%20with%20density%20differences%2C%20noise%2C%20holes%2C%20and%20partial%20overlap%2C%20where%20RMSE%20does%20not%20always%20yield%20optimal%20alignment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Differential%2520Entropy%2520Minimization%2520%2528IDEM%2529%2520method%2520for%2520fine%2520rigid%2520pairwise%25203D%2520Point%2520Cloud%2520Registration%253A%2520A%2520Focus%2520on%2520the%2520Metric%26entry.906535625%3DEmmanuele%2520Barberi%2520and%2520Felice%2520Sfravara%2520and%2520Filippo%2520Cucinotta%26entry.1292438233%3DPoint%2520cloud%2520registration%2520is%2520a%2520central%2520theme%2520in%2520computer%2520vision%252C%2520with%2520alignment%2520algorithms%2520continuously%2520improving%2520for%2520greater%2520robustness.%2520Commonly%2520used%2520methods%2520evaluate%2520Euclidean%2520distances%2520between%2520point%2520clouds%2520and%2520minimize%2520an%2520objective%2520function%252C%2520such%2520as%2520Root%2520Mean%2520Square%2520Error%2520%2528RMSE%2529.%2520However%252C%2520these%2520approaches%2520are%2520most%2520effective%2520when%2520the%2520point%2520clouds%2520are%2520well-prealigned%2520and%2520issues%2520such%2520as%2520differences%2520in%2520density%252C%2520noise%252C%2520holes%252C%2520and%2520limited%2520overlap%2520can%2520compromise%2520the%2520results.%2520Traditional%2520methods%252C%2520such%2520as%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%252C%2520require%2520choosing%2520one%2520point%2520cloud%2520as%2520fixed%252C%2520since%2520Euclidean%2520distances%2520lack%2520commutativity.%2520When%2520only%2520one%2520point%2520cloud%2520has%2520issues%252C%2520adjustments%2520can%2520be%2520made%252C%2520but%2520in%2520real%2520scenarios%252C%2520both%2520point%2520clouds%2520may%2520be%2520affected%252C%2520often%2520necessitating%2520preprocessing.%2520The%2520authors%2520introduce%2520a%2520novel%2520differential%2520entropy-based%2520metric%252C%2520designed%2520to%2520serve%2520as%2520the%2520objective%2520function%2520within%2520an%2520optimization%2520framework%2520for%2520fine%2520rigid%2520pairwise%25203D%2520point%2520cloud%2520registration%252C%2520denoted%2520as%2520Iterative%2520Differential%2520Entropy%2520Minimization%2520%2528IDEM%2529.%2520This%2520metric%2520does%2520not%2520depend%2520on%2520the%2520choice%2520of%2520a%2520fixed%2520point%2520cloud%2520and%252C%2520during%2520transformations%252C%2520reveals%2520a%2520clear%2520minimum%2520corresponding%2520to%2520the%2520best%2520alignment.%2520Multiple%2520case%2520studies%2520are%2520conducted%252C%2520and%2520the%2520results%2520are%2520compared%2520with%2520those%2520obtained%2520using%2520RMSE%252C%2520Chamfer%2520distance%252C%2520and%2520Hausdorff%2520distance.%2520The%2520proposed%2520metric%2520proves%2520effective%2520even%2520with%2520density%2520differences%252C%2520noise%252C%2520holes%252C%2520and%2520partial%2520overlap%252C%2520where%2520RMSE%2520does%2520not%2520always%2520yield%2520optimal%2520alignment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Differential%20Entropy%20Minimization%20%28IDEM%29%20method%20for%20fine%20rigid%20pairwise%203D%20Point%20Cloud%20Registration%3A%20A%20Focus%20on%20the%20Metric&entry.906535625=Emmanuele%20Barberi%20and%20Felice%20Sfravara%20and%20Filippo%20Cucinotta&entry.1292438233=Point%20cloud%20registration%20is%20a%20central%20theme%20in%20computer%20vision%2C%20with%20alignment%20algorithms%20continuously%20improving%20for%20greater%20robustness.%20Commonly%20used%20methods%20evaluate%20Euclidean%20distances%20between%20point%20clouds%20and%20minimize%20an%20objective%20function%2C%20such%20as%20Root%20Mean%20Square%20Error%20%28RMSE%29.%20However%2C%20these%20approaches%20are%20most%20effective%20when%20the%20point%20clouds%20are%20well-prealigned%20and%20issues%20such%20as%20differences%20in%20density%2C%20noise%2C%20holes%2C%20and%20limited%20overlap%20can%20compromise%20the%20results.%20Traditional%20methods%2C%20such%20as%20Iterative%20Closest%20Point%20%28ICP%29%2C%20require%20choosing%20one%20point%20cloud%20as%20fixed%2C%20since%20Euclidean%20distances%20lack%20commutativity.%20When%20only%20one%20point%20cloud%20has%20issues%2C%20adjustments%20can%20be%20made%2C%20but%20in%20real%20scenarios%2C%20both%20point%20clouds%20may%20be%20affected%2C%20often%20necessitating%20preprocessing.%20The%20authors%20introduce%20a%20novel%20differential%20entropy-based%20metric%2C%20designed%20to%20serve%20as%20the%20objective%20function%20within%20an%20optimization%20framework%20for%20fine%20rigid%20pairwise%203D%20point%20cloud%20registration%2C%20denoted%20as%20Iterative%20Differential%20Entropy%20Minimization%20%28IDEM%29.%20This%20metric%20does%20not%20depend%20on%20the%20choice%20of%20a%20fixed%20point%20cloud%20and%2C%20during%20transformations%2C%20reveals%20a%20clear%20minimum%20corresponding%20to%20the%20best%20alignment.%20Multiple%20case%20studies%20are%20conducted%2C%20and%20the%20results%20are%20compared%20with%20those%20obtained%20using%20RMSE%2C%20Chamfer%20distance%2C%20and%20Hausdorff%20distance.%20The%20proposed%20metric%20proves%20effective%20even%20with%20density%20differences%2C%20noise%2C%20holes%2C%20and%20partial%20overlap%2C%20where%20RMSE%20does%20not%20always%20yield%20optimal%20alignment.&entry.1838667208=http%3A//arxiv.org/abs/2601.09601v1&entry.124074799=Read"},
{"title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments", "author": "Qinglong Shi and Donghai Wang and Hantao Zhou and Jiguo Li and Jun Xu and Jiuchong Gao and Jinghua Hao and Renqing He", "abstract": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.", "link": "http://arxiv.org/abs/2601.09382v1", "date": "2026-01-14", "relevancy": 2.1525, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5636}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5533}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-term%20Task-oriented%20Agent%3A%20Proactive%20Long-term%20Intent%20Maintenance%20in%20Dynamic%20Environments&body=Title%3A%20Long-term%20Task-oriented%20Agent%3A%20Proactive%20Long-term%20Intent%20Maintenance%20in%20Dynamic%20Environments%0AAuthor%3A%20Qinglong%20Shi%20and%20Donghai%20Wang%20and%20Hantao%20Zhou%20and%20Jiguo%20Li%20and%20Jun%20Xu%20and%20Jiuchong%20Gao%20and%20Jinghua%20Hao%20and%20Renqing%20He%0AAbstract%3A%20Current%20large%20language%20model%20agents%20predominantly%20operate%20under%20a%20reactive%20paradigm%2C%20responding%20only%20to%20immediate%20user%20queries%20within%20short-term%20sessions.%20This%20limitation%20hinders%20their%20ability%20to%20maintain%20long-term%20user%27s%20intents%20and%20dynamically%20adapt%20to%20evolving%20external%20environments.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20interaction%20paradigm%20for%20proactive%20Task-oriented%20Agents%20capable%20of%20bridging%20the%20gap%20between%20relatively%20static%20user%27s%20needs%20and%20a%20dynamic%20environment.%20We%20formalize%20proactivity%20through%20two%20key%20capabilities%2C%20%28i%29%20Intent-Conditioned%20Monitoring%3A%20The%20agent%20autonomously%20formulates%20trigger%20conditions%20based%20on%20dialog%20history%3B%20%28ii%29%20Event-Triggered%20Follow-up%3A%20The%20agent%20actively%20engages%20the%20user%20upon%20detecting%20useful%20environmental%20updates.%20We%20introduce%20a%20high-quality%20data%20synthesis%20pipeline%20to%20construct%20complex%2C%20multi-turn%20dialog%20data%20in%20a%20dynamic%20environment.%20Furthermore%2C%20we%20attempt%20to%20address%20the%20lack%20of%20evaluation%20criteria%20of%20task-oriented%20interaction%20in%20a%20dynamic%20environment%20by%20proposing%20a%20new%20benchmark%2C%20namely%20ChronosBench.%20We%20evaluated%20some%20leading%20close-source%20and%20open-source%20models%20at%20present%20and%20revealed%20their%20flaws%20in%20long-term%20task-oriented%20interaction.%20Furthermore%2C%20our%20fine-tuned%20model%20trained%20using%20synthetic%20data%20for%20supervised%20learning%20achieves%20a%20task%20completion%20rate%20of%2085.19%25%20for%20complex%20tasks%20including%20shifts%20in%20user%20intent%2C%20outperforming%20other%20models%20under%20test.%20And%20the%20result%20validated%20the%20effectiveness%20of%20our%20data-driven%20strategy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-term%2520Task-oriented%2520Agent%253A%2520Proactive%2520Long-term%2520Intent%2520Maintenance%2520in%2520Dynamic%2520Environments%26entry.906535625%3DQinglong%2520Shi%2520and%2520Donghai%2520Wang%2520and%2520Hantao%2520Zhou%2520and%2520Jiguo%2520Li%2520and%2520Jun%2520Xu%2520and%2520Jiuchong%2520Gao%2520and%2520Jinghua%2520Hao%2520and%2520Renqing%2520He%26entry.1292438233%3DCurrent%2520large%2520language%2520model%2520agents%2520predominantly%2520operate%2520under%2520a%2520reactive%2520paradigm%252C%2520responding%2520only%2520to%2520immediate%2520user%2520queries%2520within%2520short-term%2520sessions.%2520This%2520limitation%2520hinders%2520their%2520ability%2520to%2520maintain%2520long-term%2520user%2527s%2520intents%2520and%2520dynamically%2520adapt%2520to%2520evolving%2520external%2520environments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520interaction%2520paradigm%2520for%2520proactive%2520Task-oriented%2520Agents%2520capable%2520of%2520bridging%2520the%2520gap%2520between%2520relatively%2520static%2520user%2527s%2520needs%2520and%2520a%2520dynamic%2520environment.%2520We%2520formalize%2520proactivity%2520through%2520two%2520key%2520capabilities%252C%2520%2528i%2529%2520Intent-Conditioned%2520Monitoring%253A%2520The%2520agent%2520autonomously%2520formulates%2520trigger%2520conditions%2520based%2520on%2520dialog%2520history%253B%2520%2528ii%2529%2520Event-Triggered%2520Follow-up%253A%2520The%2520agent%2520actively%2520engages%2520the%2520user%2520upon%2520detecting%2520useful%2520environmental%2520updates.%2520We%2520introduce%2520a%2520high-quality%2520data%2520synthesis%2520pipeline%2520to%2520construct%2520complex%252C%2520multi-turn%2520dialog%2520data%2520in%2520a%2520dynamic%2520environment.%2520Furthermore%252C%2520we%2520attempt%2520to%2520address%2520the%2520lack%2520of%2520evaluation%2520criteria%2520of%2520task-oriented%2520interaction%2520in%2520a%2520dynamic%2520environment%2520by%2520proposing%2520a%2520new%2520benchmark%252C%2520namely%2520ChronosBench.%2520We%2520evaluated%2520some%2520leading%2520close-source%2520and%2520open-source%2520models%2520at%2520present%2520and%2520revealed%2520their%2520flaws%2520in%2520long-term%2520task-oriented%2520interaction.%2520Furthermore%252C%2520our%2520fine-tuned%2520model%2520trained%2520using%2520synthetic%2520data%2520for%2520supervised%2520learning%2520achieves%2520a%2520task%2520completion%2520rate%2520of%252085.19%2525%2520for%2520complex%2520tasks%2520including%2520shifts%2520in%2520user%2520intent%252C%2520outperforming%2520other%2520models%2520under%2520test.%2520And%2520the%2520result%2520validated%2520the%2520effectiveness%2520of%2520our%2520data-driven%2520strategy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-term%20Task-oriented%20Agent%3A%20Proactive%20Long-term%20Intent%20Maintenance%20in%20Dynamic%20Environments&entry.906535625=Qinglong%20Shi%20and%20Donghai%20Wang%20and%20Hantao%20Zhou%20and%20Jiguo%20Li%20and%20Jun%20Xu%20and%20Jiuchong%20Gao%20and%20Jinghua%20Hao%20and%20Renqing%20He&entry.1292438233=Current%20large%20language%20model%20agents%20predominantly%20operate%20under%20a%20reactive%20paradigm%2C%20responding%20only%20to%20immediate%20user%20queries%20within%20short-term%20sessions.%20This%20limitation%20hinders%20their%20ability%20to%20maintain%20long-term%20user%27s%20intents%20and%20dynamically%20adapt%20to%20evolving%20external%20environments.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20interaction%20paradigm%20for%20proactive%20Task-oriented%20Agents%20capable%20of%20bridging%20the%20gap%20between%20relatively%20static%20user%27s%20needs%20and%20a%20dynamic%20environment.%20We%20formalize%20proactivity%20through%20two%20key%20capabilities%2C%20%28i%29%20Intent-Conditioned%20Monitoring%3A%20The%20agent%20autonomously%20formulates%20trigger%20conditions%20based%20on%20dialog%20history%3B%20%28ii%29%20Event-Triggered%20Follow-up%3A%20The%20agent%20actively%20engages%20the%20user%20upon%20detecting%20useful%20environmental%20updates.%20We%20introduce%20a%20high-quality%20data%20synthesis%20pipeline%20to%20construct%20complex%2C%20multi-turn%20dialog%20data%20in%20a%20dynamic%20environment.%20Furthermore%2C%20we%20attempt%20to%20address%20the%20lack%20of%20evaluation%20criteria%20of%20task-oriented%20interaction%20in%20a%20dynamic%20environment%20by%20proposing%20a%20new%20benchmark%2C%20namely%20ChronosBench.%20We%20evaluated%20some%20leading%20close-source%20and%20open-source%20models%20at%20present%20and%20revealed%20their%20flaws%20in%20long-term%20task-oriented%20interaction.%20Furthermore%2C%20our%20fine-tuned%20model%20trained%20using%20synthetic%20data%20for%20supervised%20learning%20achieves%20a%20task%20completion%20rate%20of%2085.19%25%20for%20complex%20tasks%20including%20shifts%20in%20user%20intent%2C%20outperforming%20other%20models%20under%20test.%20And%20the%20result%20validated%20the%20effectiveness%20of%20our%20data-driven%20strategy.&entry.1838667208=http%3A//arxiv.org/abs/2601.09382v1&entry.124074799=Read"},
{"title": "Training Large Neural Networks With Low-Dimensional Error Feedback", "author": "Maher Hanut and Jonathan Kadmon", "abstract": "Training deep neural networks typically relies on backpropagating high dimensional error signals a computationally intensive process with little evidence supporting its implementation in the brain. However, since most tasks involve low-dimensional outputs, we propose that low-dimensional error signals may suffice for effective learning. To test this hypothesis, we introduce a novel local learning rule based on Feedback Alignment that leverages indirect, low-dimensional error feedback to train large networks. Our method decouples the backward pass from the forward pass, enabling precise control over error signal dimensionality while maintaining high-dimensional representations. We begin with a detailed theoretical derivation for linear networks, which forms the foundation of our learning framework, and extend our approach to nonlinear, convolutional, and transformer architectures. Remarkably, we demonstrate that even minimal error dimensionality on the order of the task dimensionality can achieve performance matching that of traditional backpropagation. Furthermore, our rule enables efficient training of convolutional networks, which have previously been resistant to Feedback Alignment methods, with minimal error. This breakthrough not only paves the way toward more biologically accurate models of learning but also challenges the conventional reliance on high-dimensional gradient signals in neural network training. Our findings suggest that low-dimensional error signals can be as effective as high-dimensional ones, prompting a reevaluation of gradient-based learning in high-dimensional systems. Ultimately, our work offers a fresh perspective on neural network optimization and contributes to understanding learning mechanisms in both artificial and biological systems.", "link": "http://arxiv.org/abs/2502.20580v4", "date": "2026-01-14", "relevancy": 2.1467, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5524}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5298}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Large%20Neural%20Networks%20With%20Low-Dimensional%20Error%20Feedback&body=Title%3A%20Training%20Large%20Neural%20Networks%20With%20Low-Dimensional%20Error%20Feedback%0AAuthor%3A%20Maher%20Hanut%20and%20Jonathan%20Kadmon%0AAbstract%3A%20Training%20deep%20neural%20networks%20typically%20relies%20on%20backpropagating%20high%20dimensional%20error%20signals%20a%20computationally%20intensive%20process%20with%20little%20evidence%20supporting%20its%20implementation%20in%20the%20brain.%20However%2C%20since%20most%20tasks%20involve%20low-dimensional%20outputs%2C%20we%20propose%20that%20low-dimensional%20error%20signals%20may%20suffice%20for%20effective%20learning.%20To%20test%20this%20hypothesis%2C%20we%20introduce%20a%20novel%20local%20learning%20rule%20based%20on%20Feedback%20Alignment%20that%20leverages%20indirect%2C%20low-dimensional%20error%20feedback%20to%20train%20large%20networks.%20Our%20method%20decouples%20the%20backward%20pass%20from%20the%20forward%20pass%2C%20enabling%20precise%20control%20over%20error%20signal%20dimensionality%20while%20maintaining%20high-dimensional%20representations.%20We%20begin%20with%20a%20detailed%20theoretical%20derivation%20for%20linear%20networks%2C%20which%20forms%20the%20foundation%20of%20our%20learning%20framework%2C%20and%20extend%20our%20approach%20to%20nonlinear%2C%20convolutional%2C%20and%20transformer%20architectures.%20Remarkably%2C%20we%20demonstrate%20that%20even%20minimal%20error%20dimensionality%20on%20the%20order%20of%20the%20task%20dimensionality%20can%20achieve%20performance%20matching%20that%20of%20traditional%20backpropagation.%20Furthermore%2C%20our%20rule%20enables%20efficient%20training%20of%20convolutional%20networks%2C%20which%20have%20previously%20been%20resistant%20to%20Feedback%20Alignment%20methods%2C%20with%20minimal%20error.%20This%20breakthrough%20not%20only%20paves%20the%20way%20toward%20more%20biologically%20accurate%20models%20of%20learning%20but%20also%20challenges%20the%20conventional%20reliance%20on%20high-dimensional%20gradient%20signals%20in%20neural%20network%20training.%20Our%20findings%20suggest%20that%20low-dimensional%20error%20signals%20can%20be%20as%20effective%20as%20high-dimensional%20ones%2C%20prompting%20a%20reevaluation%20of%20gradient-based%20learning%20in%20high-dimensional%20systems.%20Ultimately%2C%20our%20work%20offers%20a%20fresh%20perspective%20on%20neural%20network%20optimization%20and%20contributes%20to%20understanding%20learning%20mechanisms%20in%20both%20artificial%20and%20biological%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2502.20580v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Large%2520Neural%2520Networks%2520With%2520Low-Dimensional%2520Error%2520Feedback%26entry.906535625%3DMaher%2520Hanut%2520and%2520Jonathan%2520Kadmon%26entry.1292438233%3DTraining%2520deep%2520neural%2520networks%2520typically%2520relies%2520on%2520backpropagating%2520high%2520dimensional%2520error%2520signals%2520a%2520computationally%2520intensive%2520process%2520with%2520little%2520evidence%2520supporting%2520its%2520implementation%2520in%2520the%2520brain.%2520However%252C%2520since%2520most%2520tasks%2520involve%2520low-dimensional%2520outputs%252C%2520we%2520propose%2520that%2520low-dimensional%2520error%2520signals%2520may%2520suffice%2520for%2520effective%2520learning.%2520To%2520test%2520this%2520hypothesis%252C%2520we%2520introduce%2520a%2520novel%2520local%2520learning%2520rule%2520based%2520on%2520Feedback%2520Alignment%2520that%2520leverages%2520indirect%252C%2520low-dimensional%2520error%2520feedback%2520to%2520train%2520large%2520networks.%2520Our%2520method%2520decouples%2520the%2520backward%2520pass%2520from%2520the%2520forward%2520pass%252C%2520enabling%2520precise%2520control%2520over%2520error%2520signal%2520dimensionality%2520while%2520maintaining%2520high-dimensional%2520representations.%2520We%2520begin%2520with%2520a%2520detailed%2520theoretical%2520derivation%2520for%2520linear%2520networks%252C%2520which%2520forms%2520the%2520foundation%2520of%2520our%2520learning%2520framework%252C%2520and%2520extend%2520our%2520approach%2520to%2520nonlinear%252C%2520convolutional%252C%2520and%2520transformer%2520architectures.%2520Remarkably%252C%2520we%2520demonstrate%2520that%2520even%2520minimal%2520error%2520dimensionality%2520on%2520the%2520order%2520of%2520the%2520task%2520dimensionality%2520can%2520achieve%2520performance%2520matching%2520that%2520of%2520traditional%2520backpropagation.%2520Furthermore%252C%2520our%2520rule%2520enables%2520efficient%2520training%2520of%2520convolutional%2520networks%252C%2520which%2520have%2520previously%2520been%2520resistant%2520to%2520Feedback%2520Alignment%2520methods%252C%2520with%2520minimal%2520error.%2520This%2520breakthrough%2520not%2520only%2520paves%2520the%2520way%2520toward%2520more%2520biologically%2520accurate%2520models%2520of%2520learning%2520but%2520also%2520challenges%2520the%2520conventional%2520reliance%2520on%2520high-dimensional%2520gradient%2520signals%2520in%2520neural%2520network%2520training.%2520Our%2520findings%2520suggest%2520that%2520low-dimensional%2520error%2520signals%2520can%2520be%2520as%2520effective%2520as%2520high-dimensional%2520ones%252C%2520prompting%2520a%2520reevaluation%2520of%2520gradient-based%2520learning%2520in%2520high-dimensional%2520systems.%2520Ultimately%252C%2520our%2520work%2520offers%2520a%2520fresh%2520perspective%2520on%2520neural%2520network%2520optimization%2520and%2520contributes%2520to%2520understanding%2520learning%2520mechanisms%2520in%2520both%2520artificial%2520and%2520biological%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20580v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Large%20Neural%20Networks%20With%20Low-Dimensional%20Error%20Feedback&entry.906535625=Maher%20Hanut%20and%20Jonathan%20Kadmon&entry.1292438233=Training%20deep%20neural%20networks%20typically%20relies%20on%20backpropagating%20high%20dimensional%20error%20signals%20a%20computationally%20intensive%20process%20with%20little%20evidence%20supporting%20its%20implementation%20in%20the%20brain.%20However%2C%20since%20most%20tasks%20involve%20low-dimensional%20outputs%2C%20we%20propose%20that%20low-dimensional%20error%20signals%20may%20suffice%20for%20effective%20learning.%20To%20test%20this%20hypothesis%2C%20we%20introduce%20a%20novel%20local%20learning%20rule%20based%20on%20Feedback%20Alignment%20that%20leverages%20indirect%2C%20low-dimensional%20error%20feedback%20to%20train%20large%20networks.%20Our%20method%20decouples%20the%20backward%20pass%20from%20the%20forward%20pass%2C%20enabling%20precise%20control%20over%20error%20signal%20dimensionality%20while%20maintaining%20high-dimensional%20representations.%20We%20begin%20with%20a%20detailed%20theoretical%20derivation%20for%20linear%20networks%2C%20which%20forms%20the%20foundation%20of%20our%20learning%20framework%2C%20and%20extend%20our%20approach%20to%20nonlinear%2C%20convolutional%2C%20and%20transformer%20architectures.%20Remarkably%2C%20we%20demonstrate%20that%20even%20minimal%20error%20dimensionality%20on%20the%20order%20of%20the%20task%20dimensionality%20can%20achieve%20performance%20matching%20that%20of%20traditional%20backpropagation.%20Furthermore%2C%20our%20rule%20enables%20efficient%20training%20of%20convolutional%20networks%2C%20which%20have%20previously%20been%20resistant%20to%20Feedback%20Alignment%20methods%2C%20with%20minimal%20error.%20This%20breakthrough%20not%20only%20paves%20the%20way%20toward%20more%20biologically%20accurate%20models%20of%20learning%20but%20also%20challenges%20the%20conventional%20reliance%20on%20high-dimensional%20gradient%20signals%20in%20neural%20network%20training.%20Our%20findings%20suggest%20that%20low-dimensional%20error%20signals%20can%20be%20as%20effective%20as%20high-dimensional%20ones%2C%20prompting%20a%20reevaluation%20of%20gradient-based%20learning%20in%20high-dimensional%20systems.%20Ultimately%2C%20our%20work%20offers%20a%20fresh%20perspective%20on%20neural%20network%20optimization%20and%20contributes%20to%20understanding%20learning%20mechanisms%20in%20both%20artificial%20and%20biological%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2502.20580v4&entry.124074799=Read"},
{"title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration", "author": "Aditri Paul and Archan Paul", "abstract": "Successful autonomous planetary exploration hinges on real-time, high-fidelity environmental perception. However, standard deep learning models usually demand far more memory and computation power than space-qualified, radiation-hardened onboard hardware can provide. This creates a fundamental design challenge of deploying sophisticated detection architectures without saturating the rigid power and memory envelopes of the computation hardware of planetary exploration platforms. We propose the Adaptive Quantized Planetary Crater Detection System to resolve this bottleneck. Our framework integrates a Quantized Neural Network, refined through Quantization Aware Training, with an Adaptive Multi-Sensor Fusion module. By forcing weights into low-precision integer arithmetic, we effectively strip away the floating-point overhead that typically bottlenecks onboard processors and system memory. This yields a leaner model footprint and significantly faster processing while the detection fidelity remains high. Such efficiency enables AMF module to merge high-bandwidth Optical Imagery streams with Digital Elevation Models using an Adaptive Weighting Mechanism to re-balance sensor priority under variable conditions like deep shadows or high albedo. Integrated Multi-Scale Detection Heads then resolve craters across a wide range of diameters, providing a computationally efficient and precise solution for real-time detection, localization of craters and hazard avoidance. This paper establishes the architectural design and theoretical justification of the system. While our methodology is grounded in principles of hybrid computer vision and planetary science, we present this as a blueprint for future empirical validation and hardware benchmarking on integer-arithmetic units. This system provides a capability vital for the next generation of autonomous landing, navigation, and deep space explorations.", "link": "http://arxiv.org/abs/2508.18025v2", "date": "2026-01-14", "relevancy": 2.1206, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5425}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5392}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AQ-PCDSys%3A%20An%20Adaptive%20Quantized%20Planetary%20Crater%20Detection%20System%20for%20Autonomous%20Space%20Exploration&body=Title%3A%20AQ-PCDSys%3A%20An%20Adaptive%20Quantized%20Planetary%20Crater%20Detection%20System%20for%20Autonomous%20Space%20Exploration%0AAuthor%3A%20Aditri%20Paul%20and%20Archan%20Paul%0AAbstract%3A%20Successful%20autonomous%20planetary%20exploration%20hinges%20on%20real-time%2C%20high-fidelity%20environmental%20perception.%20However%2C%20standard%20deep%20learning%20models%20usually%20demand%20far%20more%20memory%20and%20computation%20power%20than%20space-qualified%2C%20radiation-hardened%20onboard%20hardware%20can%20provide.%20This%20creates%20a%20fundamental%20design%20challenge%20of%20deploying%20sophisticated%20detection%20architectures%20without%20saturating%20the%20rigid%20power%20and%20memory%20envelopes%20of%20the%20computation%20hardware%20of%20planetary%20exploration%20platforms.%20We%20propose%20the%20Adaptive%20Quantized%20Planetary%20Crater%20Detection%20System%20to%20resolve%20this%20bottleneck.%20Our%20framework%20integrates%20a%20Quantized%20Neural%20Network%2C%20refined%20through%20Quantization%20Aware%20Training%2C%20with%20an%20Adaptive%20Multi-Sensor%20Fusion%20module.%20By%20forcing%20weights%20into%20low-precision%20integer%20arithmetic%2C%20we%20effectively%20strip%20away%20the%20floating-point%20overhead%20that%20typically%20bottlenecks%20onboard%20processors%20and%20system%20memory.%20This%20yields%20a%20leaner%20model%20footprint%20and%20significantly%20faster%20processing%20while%20the%20detection%20fidelity%20remains%20high.%20Such%20efficiency%20enables%20AMF%20module%20to%20merge%20high-bandwidth%20Optical%20Imagery%20streams%20with%20Digital%20Elevation%20Models%20using%20an%20Adaptive%20Weighting%20Mechanism%20to%20re-balance%20sensor%20priority%20under%20variable%20conditions%20like%20deep%20shadows%20or%20high%20albedo.%20Integrated%20Multi-Scale%20Detection%20Heads%20then%20resolve%20craters%20across%20a%20wide%20range%20of%20diameters%2C%20providing%20a%20computationally%20efficient%20and%20precise%20solution%20for%20real-time%20detection%2C%20localization%20of%20craters%20and%20hazard%20avoidance.%20This%20paper%20establishes%20the%20architectural%20design%20and%20theoretical%20justification%20of%20the%20system.%20While%20our%20methodology%20is%20grounded%20in%20principles%20of%20hybrid%20computer%20vision%20and%20planetary%20science%2C%20we%20present%20this%20as%20a%20blueprint%20for%20future%20empirical%20validation%20and%20hardware%20benchmarking%20on%20integer-arithmetic%20units.%20This%20system%20provides%20a%20capability%20vital%20for%20the%20next%20generation%20of%20autonomous%20landing%2C%20navigation%2C%20and%20deep%20space%20explorations.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAQ-PCDSys%253A%2520An%2520Adaptive%2520Quantized%2520Planetary%2520Crater%2520Detection%2520System%2520for%2520Autonomous%2520Space%2520Exploration%26entry.906535625%3DAditri%2520Paul%2520and%2520Archan%2520Paul%26entry.1292438233%3DSuccessful%2520autonomous%2520planetary%2520exploration%2520hinges%2520on%2520real-time%252C%2520high-fidelity%2520environmental%2520perception.%2520However%252C%2520standard%2520deep%2520learning%2520models%2520usually%2520demand%2520far%2520more%2520memory%2520and%2520computation%2520power%2520than%2520space-qualified%252C%2520radiation-hardened%2520onboard%2520hardware%2520can%2520provide.%2520This%2520creates%2520a%2520fundamental%2520design%2520challenge%2520of%2520deploying%2520sophisticated%2520detection%2520architectures%2520without%2520saturating%2520the%2520rigid%2520power%2520and%2520memory%2520envelopes%2520of%2520the%2520computation%2520hardware%2520of%2520planetary%2520exploration%2520platforms.%2520We%2520propose%2520the%2520Adaptive%2520Quantized%2520Planetary%2520Crater%2520Detection%2520System%2520to%2520resolve%2520this%2520bottleneck.%2520Our%2520framework%2520integrates%2520a%2520Quantized%2520Neural%2520Network%252C%2520refined%2520through%2520Quantization%2520Aware%2520Training%252C%2520with%2520an%2520Adaptive%2520Multi-Sensor%2520Fusion%2520module.%2520By%2520forcing%2520weights%2520into%2520low-precision%2520integer%2520arithmetic%252C%2520we%2520effectively%2520strip%2520away%2520the%2520floating-point%2520overhead%2520that%2520typically%2520bottlenecks%2520onboard%2520processors%2520and%2520system%2520memory.%2520This%2520yields%2520a%2520leaner%2520model%2520footprint%2520and%2520significantly%2520faster%2520processing%2520while%2520the%2520detection%2520fidelity%2520remains%2520high.%2520Such%2520efficiency%2520enables%2520AMF%2520module%2520to%2520merge%2520high-bandwidth%2520Optical%2520Imagery%2520streams%2520with%2520Digital%2520Elevation%2520Models%2520using%2520an%2520Adaptive%2520Weighting%2520Mechanism%2520to%2520re-balance%2520sensor%2520priority%2520under%2520variable%2520conditions%2520like%2520deep%2520shadows%2520or%2520high%2520albedo.%2520Integrated%2520Multi-Scale%2520Detection%2520Heads%2520then%2520resolve%2520craters%2520across%2520a%2520wide%2520range%2520of%2520diameters%252C%2520providing%2520a%2520computationally%2520efficient%2520and%2520precise%2520solution%2520for%2520real-time%2520detection%252C%2520localization%2520of%2520craters%2520and%2520hazard%2520avoidance.%2520This%2520paper%2520establishes%2520the%2520architectural%2520design%2520and%2520theoretical%2520justification%2520of%2520the%2520system.%2520While%2520our%2520methodology%2520is%2520grounded%2520in%2520principles%2520of%2520hybrid%2520computer%2520vision%2520and%2520planetary%2520science%252C%2520we%2520present%2520this%2520as%2520a%2520blueprint%2520for%2520future%2520empirical%2520validation%2520and%2520hardware%2520benchmarking%2520on%2520integer-arithmetic%2520units.%2520This%2520system%2520provides%2520a%2520capability%2520vital%2520for%2520the%2520next%2520generation%2520of%2520autonomous%2520landing%252C%2520navigation%252C%2520and%2520deep%2520space%2520explorations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AQ-PCDSys%3A%20An%20Adaptive%20Quantized%20Planetary%20Crater%20Detection%20System%20for%20Autonomous%20Space%20Exploration&entry.906535625=Aditri%20Paul%20and%20Archan%20Paul&entry.1292438233=Successful%20autonomous%20planetary%20exploration%20hinges%20on%20real-time%2C%20high-fidelity%20environmental%20perception.%20However%2C%20standard%20deep%20learning%20models%20usually%20demand%20far%20more%20memory%20and%20computation%20power%20than%20space-qualified%2C%20radiation-hardened%20onboard%20hardware%20can%20provide.%20This%20creates%20a%20fundamental%20design%20challenge%20of%20deploying%20sophisticated%20detection%20architectures%20without%20saturating%20the%20rigid%20power%20and%20memory%20envelopes%20of%20the%20computation%20hardware%20of%20planetary%20exploration%20platforms.%20We%20propose%20the%20Adaptive%20Quantized%20Planetary%20Crater%20Detection%20System%20to%20resolve%20this%20bottleneck.%20Our%20framework%20integrates%20a%20Quantized%20Neural%20Network%2C%20refined%20through%20Quantization%20Aware%20Training%2C%20with%20an%20Adaptive%20Multi-Sensor%20Fusion%20module.%20By%20forcing%20weights%20into%20low-precision%20integer%20arithmetic%2C%20we%20effectively%20strip%20away%20the%20floating-point%20overhead%20that%20typically%20bottlenecks%20onboard%20processors%20and%20system%20memory.%20This%20yields%20a%20leaner%20model%20footprint%20and%20significantly%20faster%20processing%20while%20the%20detection%20fidelity%20remains%20high.%20Such%20efficiency%20enables%20AMF%20module%20to%20merge%20high-bandwidth%20Optical%20Imagery%20streams%20with%20Digital%20Elevation%20Models%20using%20an%20Adaptive%20Weighting%20Mechanism%20to%20re-balance%20sensor%20priority%20under%20variable%20conditions%20like%20deep%20shadows%20or%20high%20albedo.%20Integrated%20Multi-Scale%20Detection%20Heads%20then%20resolve%20craters%20across%20a%20wide%20range%20of%20diameters%2C%20providing%20a%20computationally%20efficient%20and%20precise%20solution%20for%20real-time%20detection%2C%20localization%20of%20craters%20and%20hazard%20avoidance.%20This%20paper%20establishes%20the%20architectural%20design%20and%20theoretical%20justification%20of%20the%20system.%20While%20our%20methodology%20is%20grounded%20in%20principles%20of%20hybrid%20computer%20vision%20and%20planetary%20science%2C%20we%20present%20this%20as%20a%20blueprint%20for%20future%20empirical%20validation%20and%20hardware%20benchmarking%20on%20integer-arithmetic%20units.%20This%20system%20provides%20a%20capability%20vital%20for%20the%20next%20generation%20of%20autonomous%20landing%2C%20navigation%2C%20and%20deep%20space%20explorations.&entry.1838667208=http%3A//arxiv.org/abs/2508.18025v2&entry.124074799=Read"},
{"title": "Fusion or Confusion? Multimodal Complexity Is Not All You Need", "author": "Tillmann Rheude and Roland Eils and Benjamin Wild", "abstract": "Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions. We evaluate them across nine diverse datasets with up to 23 modalities, and test their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analyses show that complex methods perform on par with SimBaMM and often fail to consistently outperform well-tuned unimodal baselines, especially in small-data settings. To support our findings, we include a case study highlighting common methodological shortcomings in the literature followed by a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.", "link": "http://arxiv.org/abs/2512.22991v2", "date": "2026-01-14", "relevancy": 2.1099, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5466}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20or%20Confusion%3F%20Multimodal%20Complexity%20Is%20Not%20All%20You%20Need&body=Title%3A%20Fusion%20or%20Confusion%3F%20Multimodal%20Complexity%20Is%20Not%20All%20You%20Need%0AAuthor%3A%20Tillmann%20Rheude%20and%20Roland%20Eils%20and%20Benjamin%20Wild%0AAbstract%3A%20Deep%20learning%20architectures%20for%20multimodal%20learning%20have%20increased%20in%20complexity%2C%20driven%20by%20the%20assumption%20that%20multimodal-specific%20methods%20improve%20performance.%20We%20challenge%20this%20assumption%20through%20a%20large-scale%20empirical%20study%20reimplementing%2019%20high-impact%20methods%20under%20standardized%20conditions.%20We%20evaluate%20them%20across%20nine%20diverse%20datasets%20with%20up%20to%2023%20modalities%2C%20and%20test%20their%20generalizability%20to%20new%20tasks%20beyond%20their%20original%20scope%2C%20including%20settings%20with%20missing%20modalities.%20We%20propose%20a%20Simple%20Baseline%20for%20Multimodal%20Learning%20%28SimBaMM%29%2C%20a%20late-fusion%20Transformer%20architecture%2C%20and%20demonstrate%20that%20under%20standardized%20experimental%20conditions%20with%20rigorous%20hyperparameter%20tuning%20of%20all%20methods%2C%20more%20complex%20architectures%20do%20not%20reliably%20outperform%20SimBaMM.%20Statistical%20analyses%20show%20that%20complex%20methods%20perform%20on%20par%20with%20SimBaMM%20and%20often%20fail%20to%20consistently%20outperform%20well-tuned%20unimodal%20baselines%2C%20especially%20in%20small-data%20settings.%20To%20support%20our%20findings%2C%20we%20include%20a%20case%20study%20highlighting%20common%20methodological%20shortcomings%20in%20the%20literature%20followed%20by%20a%20pragmatic%20reliability%20checklist%20to%20promote%20comparable%2C%20robust%2C%20and%20trustworthy%20future%20evaluations.%20In%20summary%2C%20we%20argue%20for%20a%20shift%20in%20focus%3A%20away%20from%20the%20pursuit%20of%20architectural%20novelty%20and%20toward%20methodological%20rigor.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520or%2520Confusion%253F%2520Multimodal%2520Complexity%2520Is%2520Not%2520All%2520You%2520Need%26entry.906535625%3DTillmann%2520Rheude%2520and%2520Roland%2520Eils%2520and%2520Benjamin%2520Wild%26entry.1292438233%3DDeep%2520learning%2520architectures%2520for%2520multimodal%2520learning%2520have%2520increased%2520in%2520complexity%252C%2520driven%2520by%2520the%2520assumption%2520that%2520multimodal-specific%2520methods%2520improve%2520performance.%2520We%2520challenge%2520this%2520assumption%2520through%2520a%2520large-scale%2520empirical%2520study%2520reimplementing%252019%2520high-impact%2520methods%2520under%2520standardized%2520conditions.%2520We%2520evaluate%2520them%2520across%2520nine%2520diverse%2520datasets%2520with%2520up%2520to%252023%2520modalities%252C%2520and%2520test%2520their%2520generalizability%2520to%2520new%2520tasks%2520beyond%2520their%2520original%2520scope%252C%2520including%2520settings%2520with%2520missing%2520modalities.%2520We%2520propose%2520a%2520Simple%2520Baseline%2520for%2520Multimodal%2520Learning%2520%2528SimBaMM%2529%252C%2520a%2520late-fusion%2520Transformer%2520architecture%252C%2520and%2520demonstrate%2520that%2520under%2520standardized%2520experimental%2520conditions%2520with%2520rigorous%2520hyperparameter%2520tuning%2520of%2520all%2520methods%252C%2520more%2520complex%2520architectures%2520do%2520not%2520reliably%2520outperform%2520SimBaMM.%2520Statistical%2520analyses%2520show%2520that%2520complex%2520methods%2520perform%2520on%2520par%2520with%2520SimBaMM%2520and%2520often%2520fail%2520to%2520consistently%2520outperform%2520well-tuned%2520unimodal%2520baselines%252C%2520especially%2520in%2520small-data%2520settings.%2520To%2520support%2520our%2520findings%252C%2520we%2520include%2520a%2520case%2520study%2520highlighting%2520common%2520methodological%2520shortcomings%2520in%2520the%2520literature%2520followed%2520by%2520a%2520pragmatic%2520reliability%2520checklist%2520to%2520promote%2520comparable%252C%2520robust%252C%2520and%2520trustworthy%2520future%2520evaluations.%2520In%2520summary%252C%2520we%2520argue%2520for%2520a%2520shift%2520in%2520focus%253A%2520away%2520from%2520the%2520pursuit%2520of%2520architectural%2520novelty%2520and%2520toward%2520methodological%2520rigor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20or%20Confusion%3F%20Multimodal%20Complexity%20Is%20Not%20All%20You%20Need&entry.906535625=Tillmann%20Rheude%20and%20Roland%20Eils%20and%20Benjamin%20Wild&entry.1292438233=Deep%20learning%20architectures%20for%20multimodal%20learning%20have%20increased%20in%20complexity%2C%20driven%20by%20the%20assumption%20that%20multimodal-specific%20methods%20improve%20performance.%20We%20challenge%20this%20assumption%20through%20a%20large-scale%20empirical%20study%20reimplementing%2019%20high-impact%20methods%20under%20standardized%20conditions.%20We%20evaluate%20them%20across%20nine%20diverse%20datasets%20with%20up%20to%2023%20modalities%2C%20and%20test%20their%20generalizability%20to%20new%20tasks%20beyond%20their%20original%20scope%2C%20including%20settings%20with%20missing%20modalities.%20We%20propose%20a%20Simple%20Baseline%20for%20Multimodal%20Learning%20%28SimBaMM%29%2C%20a%20late-fusion%20Transformer%20architecture%2C%20and%20demonstrate%20that%20under%20standardized%20experimental%20conditions%20with%20rigorous%20hyperparameter%20tuning%20of%20all%20methods%2C%20more%20complex%20architectures%20do%20not%20reliably%20outperform%20SimBaMM.%20Statistical%20analyses%20show%20that%20complex%20methods%20perform%20on%20par%20with%20SimBaMM%20and%20often%20fail%20to%20consistently%20outperform%20well-tuned%20unimodal%20baselines%2C%20especially%20in%20small-data%20settings.%20To%20support%20our%20findings%2C%20we%20include%20a%20case%20study%20highlighting%20common%20methodological%20shortcomings%20in%20the%20literature%20followed%20by%20a%20pragmatic%20reliability%20checklist%20to%20promote%20comparable%2C%20robust%2C%20and%20trustworthy%20future%20evaluations.%20In%20summary%2C%20we%20argue%20for%20a%20shift%20in%20focus%3A%20away%20from%20the%20pursuit%20of%20architectural%20novelty%20and%20toward%20methodological%20rigor.&entry.1838667208=http%3A//arxiv.org/abs/2512.22991v2&entry.124074799=Read"},
{"title": "Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process", "author": "Sangjun Han and Youngmi Hur", "abstract": "With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.", "link": "http://arxiv.org/abs/2601.09410v1", "date": "2026-01-14", "relevancy": 2.101, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5452}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5142}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detail%20Loss%20in%20Super-Resolution%20Models%20Based%20on%20the%20Laplacian%20Pyramid%20and%20Repeated%20Upscaling%20and%20Downscaling%20Process&body=Title%3A%20Detail%20Loss%20in%20Super-Resolution%20Models%20Based%20on%20the%20Laplacian%20Pyramid%20and%20Repeated%20Upscaling%20and%20Downscaling%20Process%0AAuthor%3A%20Sangjun%20Han%20and%20Youngmi%20Hur%0AAbstract%3A%20With%20advances%20in%20artificial%20intelligence%2C%20image%20processing%20has%20gained%20significant%20interest.%20Image%20super-resolution%20is%20a%20vital%20technology%20closely%20related%20to%20real-world%20applications%2C%20as%20it%20enhances%20the%20quality%20of%20existing%20images.%20Since%20enhancing%20fine%20details%20is%20crucial%20for%20the%20super-resolution%20task%2C%20pixels%20that%20contribute%20to%20high-frequency%20information%20should%20be%20emphasized.%20This%20paper%20proposes%20two%20methods%20to%20enhance%20high-frequency%20details%20in%20super-resolution%20images%3A%20a%20Laplacian%20pyramid-based%20detail%20loss%20and%20a%20repeated%20upscaling%20and%20downscaling%20process.%20Total%20loss%20with%20our%20detail%20loss%20guides%20a%20model%20by%20separately%20generating%20and%20controlling%20super-resolution%20and%20detail%20images.%20This%20approach%20allows%20the%20model%20to%20focus%20more%20effectively%20on%20high-frequency%20components%2C%20resulting%20in%20improved%20super-resolution%20images.%20Additionally%2C%20repeated%20upscaling%20and%20downscaling%20amplify%20the%20effectiveness%20of%20the%20detail%20loss%20by%20extracting%20diverse%20information%20from%20multiple%20low-resolution%20features.%20We%20conduct%20two%20types%20of%20experiments.%20First%2C%20we%20design%20a%20CNN-based%20model%20incorporating%20our%20methods.%20This%20model%20achieves%20state-of-the-art%20results%2C%20surpassing%20all%20currently%20available%20CNN-based%20and%20even%20some%20attention-based%20models.%20Second%2C%20we%20apply%20our%20methods%20to%20existing%20attention-based%20models%20on%20a%20small%20scale.%20In%20all%20our%20experiments%2C%20attention-based%20models%20adding%20our%20detail%20loss%20show%20improvements%20compared%20to%20the%20originals.%20These%20results%20demonstrate%20our%20approaches%20effectively%20enhance%20super-resolution%20images%20across%20different%20model%20structures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetail%2520Loss%2520in%2520Super-Resolution%2520Models%2520Based%2520on%2520the%2520Laplacian%2520Pyramid%2520and%2520Repeated%2520Upscaling%2520and%2520Downscaling%2520Process%26entry.906535625%3DSangjun%2520Han%2520and%2520Youngmi%2520Hur%26entry.1292438233%3DWith%2520advances%2520in%2520artificial%2520intelligence%252C%2520image%2520processing%2520has%2520gained%2520significant%2520interest.%2520Image%2520super-resolution%2520is%2520a%2520vital%2520technology%2520closely%2520related%2520to%2520real-world%2520applications%252C%2520as%2520it%2520enhances%2520the%2520quality%2520of%2520existing%2520images.%2520Since%2520enhancing%2520fine%2520details%2520is%2520crucial%2520for%2520the%2520super-resolution%2520task%252C%2520pixels%2520that%2520contribute%2520to%2520high-frequency%2520information%2520should%2520be%2520emphasized.%2520This%2520paper%2520proposes%2520two%2520methods%2520to%2520enhance%2520high-frequency%2520details%2520in%2520super-resolution%2520images%253A%2520a%2520Laplacian%2520pyramid-based%2520detail%2520loss%2520and%2520a%2520repeated%2520upscaling%2520and%2520downscaling%2520process.%2520Total%2520loss%2520with%2520our%2520detail%2520loss%2520guides%2520a%2520model%2520by%2520separately%2520generating%2520and%2520controlling%2520super-resolution%2520and%2520detail%2520images.%2520This%2520approach%2520allows%2520the%2520model%2520to%2520focus%2520more%2520effectively%2520on%2520high-frequency%2520components%252C%2520resulting%2520in%2520improved%2520super-resolution%2520images.%2520Additionally%252C%2520repeated%2520upscaling%2520and%2520downscaling%2520amplify%2520the%2520effectiveness%2520of%2520the%2520detail%2520loss%2520by%2520extracting%2520diverse%2520information%2520from%2520multiple%2520low-resolution%2520features.%2520We%2520conduct%2520two%2520types%2520of%2520experiments.%2520First%252C%2520we%2520design%2520a%2520CNN-based%2520model%2520incorporating%2520our%2520methods.%2520This%2520model%2520achieves%2520state-of-the-art%2520results%252C%2520surpassing%2520all%2520currently%2520available%2520CNN-based%2520and%2520even%2520some%2520attention-based%2520models.%2520Second%252C%2520we%2520apply%2520our%2520methods%2520to%2520existing%2520attention-based%2520models%2520on%2520a%2520small%2520scale.%2520In%2520all%2520our%2520experiments%252C%2520attention-based%2520models%2520adding%2520our%2520detail%2520loss%2520show%2520improvements%2520compared%2520to%2520the%2520originals.%2520These%2520results%2520demonstrate%2520our%2520approaches%2520effectively%2520enhance%2520super-resolution%2520images%2520across%2520different%2520model%2520structures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detail%20Loss%20in%20Super-Resolution%20Models%20Based%20on%20the%20Laplacian%20Pyramid%20and%20Repeated%20Upscaling%20and%20Downscaling%20Process&entry.906535625=Sangjun%20Han%20and%20Youngmi%20Hur&entry.1292438233=With%20advances%20in%20artificial%20intelligence%2C%20image%20processing%20has%20gained%20significant%20interest.%20Image%20super-resolution%20is%20a%20vital%20technology%20closely%20related%20to%20real-world%20applications%2C%20as%20it%20enhances%20the%20quality%20of%20existing%20images.%20Since%20enhancing%20fine%20details%20is%20crucial%20for%20the%20super-resolution%20task%2C%20pixels%20that%20contribute%20to%20high-frequency%20information%20should%20be%20emphasized.%20This%20paper%20proposes%20two%20methods%20to%20enhance%20high-frequency%20details%20in%20super-resolution%20images%3A%20a%20Laplacian%20pyramid-based%20detail%20loss%20and%20a%20repeated%20upscaling%20and%20downscaling%20process.%20Total%20loss%20with%20our%20detail%20loss%20guides%20a%20model%20by%20separately%20generating%20and%20controlling%20super-resolution%20and%20detail%20images.%20This%20approach%20allows%20the%20model%20to%20focus%20more%20effectively%20on%20high-frequency%20components%2C%20resulting%20in%20improved%20super-resolution%20images.%20Additionally%2C%20repeated%20upscaling%20and%20downscaling%20amplify%20the%20effectiveness%20of%20the%20detail%20loss%20by%20extracting%20diverse%20information%20from%20multiple%20low-resolution%20features.%20We%20conduct%20two%20types%20of%20experiments.%20First%2C%20we%20design%20a%20CNN-based%20model%20incorporating%20our%20methods.%20This%20model%20achieves%20state-of-the-art%20results%2C%20surpassing%20all%20currently%20available%20CNN-based%20and%20even%20some%20attention-based%20models.%20Second%2C%20we%20apply%20our%20methods%20to%20existing%20attention-based%20models%20on%20a%20small%20scale.%20In%20all%20our%20experiments%2C%20attention-based%20models%20adding%20our%20detail%20loss%20show%20improvements%20compared%20to%20the%20originals.%20These%20results%20demonstrate%20our%20approaches%20effectively%20enhance%20super-resolution%20images%20across%20different%20model%20structures.&entry.1838667208=http%3A//arxiv.org/abs/2601.09410v1&entry.124074799=Read"},
{"title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "author": "Zhiyuan Hu and Yunhai Hu and Juncheng Liu and Shuyue Stella Li and Yucheng Wang and Zhen Xu and See-Kiong Ng and Anh Tuan Luu and Xinxing Xu and Bryan Hooi and Cynthia Breazeal and Hae Won Park", "abstract": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "link": "http://arxiv.org/abs/2601.09667v1", "date": "2026-01-14", "relevancy": 2.0992, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5307}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Multi-Agent%20Test-Time%20Reinforcement%20Learning%20for%20Reasoning&body=Title%3A%20Collaborative%20Multi-Agent%20Test-Time%20Reinforcement%20Learning%20for%20Reasoning%0AAuthor%3A%20Zhiyuan%20Hu%20and%20Yunhai%20Hu%20and%20Juncheng%20Liu%20and%20Shuyue%20Stella%20Li%20and%20Yucheng%20Wang%20and%20Zhen%20Xu%20and%20See-Kiong%20Ng%20and%20Anh%20Tuan%20Luu%20and%20Xinxing%20Xu%20and%20Bryan%20Hooi%20and%20Cynthia%20Breazeal%20and%20Hae%20Won%20Park%0AAbstract%3A%20Multi-agent%20systems%20have%20evolved%20into%20practical%20LLM-driven%20collaborators%20for%20many%20applications%2C%20gaining%20robustness%20from%20diversity%20and%20cross-checking.%20However%2C%20multi-agent%20RL%20%28MARL%29%20training%20is%20resource-intensive%20and%20unstable%3A%20co-adapting%20teammates%20induce%20non-stationarity%2C%20and%20rewards%20are%20often%20sparse%20and%20high-variance.%20Therefore%2C%20we%20introduce%20%5Ctextbf%7BMulti-Agent%20Test-Time%20Reinforcement%20Learning%20%28MATTRL%29%7D%2C%20a%20framework%20that%20injects%20structured%20textual%20experience%20into%20multi-agent%20deliberation%20at%20inference%20time.%20MATTRL%20forms%20a%20multi-expert%20team%20of%20specialists%20for%20multi-turn%20discussions%2C%20retrieves%20and%20integrates%20test-time%20experiences%2C%20and%20reaches%20consensus%20for%20final%20decision-making.%20We%20also%20study%20credit%20assignment%20for%20constructing%20a%20turn-level%20experience%20pool%2C%20then%20reinjecting%20it%20into%20the%20dialogue.%20Across%20challenging%20benchmarks%20in%20medicine%2C%20math%2C%20and%20education%2C%20MATTRL%20improves%20accuracy%20by%20an%20average%20of%203.67%5C%25%20over%20a%20multi-agent%20baseline%2C%20and%20by%208.67%5C%25%20over%20comparable%20single-agent%20baselines.%20Ablation%20studies%20examine%20different%20credit-assignment%20schemes%20and%20provide%20a%20detailed%20comparison%20of%20how%20they%20affect%20training%20outcomes.%20MATTRL%20offers%20a%20stable%2C%20effective%20and%20efficient%20path%20to%20distribution-shift-robust%20multi-agent%20reasoning%20without%20tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Multi-Agent%2520Test-Time%2520Reinforcement%2520Learning%2520for%2520Reasoning%26entry.906535625%3DZhiyuan%2520Hu%2520and%2520Yunhai%2520Hu%2520and%2520Juncheng%2520Liu%2520and%2520Shuyue%2520Stella%2520Li%2520and%2520Yucheng%2520Wang%2520and%2520Zhen%2520Xu%2520and%2520See-Kiong%2520Ng%2520and%2520Anh%2520Tuan%2520Luu%2520and%2520Xinxing%2520Xu%2520and%2520Bryan%2520Hooi%2520and%2520Cynthia%2520Breazeal%2520and%2520Hae%2520Won%2520Park%26entry.1292438233%3DMulti-agent%2520systems%2520have%2520evolved%2520into%2520practical%2520LLM-driven%2520collaborators%2520for%2520many%2520applications%252C%2520gaining%2520robustness%2520from%2520diversity%2520and%2520cross-checking.%2520However%252C%2520multi-agent%2520RL%2520%2528MARL%2529%2520training%2520is%2520resource-intensive%2520and%2520unstable%253A%2520co-adapting%2520teammates%2520induce%2520non-stationarity%252C%2520and%2520rewards%2520are%2520often%2520sparse%2520and%2520high-variance.%2520Therefore%252C%2520we%2520introduce%2520%255Ctextbf%257BMulti-Agent%2520Test-Time%2520Reinforcement%2520Learning%2520%2528MATTRL%2529%257D%252C%2520a%2520framework%2520that%2520injects%2520structured%2520textual%2520experience%2520into%2520multi-agent%2520deliberation%2520at%2520inference%2520time.%2520MATTRL%2520forms%2520a%2520multi-expert%2520team%2520of%2520specialists%2520for%2520multi-turn%2520discussions%252C%2520retrieves%2520and%2520integrates%2520test-time%2520experiences%252C%2520and%2520reaches%2520consensus%2520for%2520final%2520decision-making.%2520We%2520also%2520study%2520credit%2520assignment%2520for%2520constructing%2520a%2520turn-level%2520experience%2520pool%252C%2520then%2520reinjecting%2520it%2520into%2520the%2520dialogue.%2520Across%2520challenging%2520benchmarks%2520in%2520medicine%252C%2520math%252C%2520and%2520education%252C%2520MATTRL%2520improves%2520accuracy%2520by%2520an%2520average%2520of%25203.67%255C%2525%2520over%2520a%2520multi-agent%2520baseline%252C%2520and%2520by%25208.67%255C%2525%2520over%2520comparable%2520single-agent%2520baselines.%2520Ablation%2520studies%2520examine%2520different%2520credit-assignment%2520schemes%2520and%2520provide%2520a%2520detailed%2520comparison%2520of%2520how%2520they%2520affect%2520training%2520outcomes.%2520MATTRL%2520offers%2520a%2520stable%252C%2520effective%2520and%2520efficient%2520path%2520to%2520distribution-shift-robust%2520multi-agent%2520reasoning%2520without%2520tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Multi-Agent%20Test-Time%20Reinforcement%20Learning%20for%20Reasoning&entry.906535625=Zhiyuan%20Hu%20and%20Yunhai%20Hu%20and%20Juncheng%20Liu%20and%20Shuyue%20Stella%20Li%20and%20Yucheng%20Wang%20and%20Zhen%20Xu%20and%20See-Kiong%20Ng%20and%20Anh%20Tuan%20Luu%20and%20Xinxing%20Xu%20and%20Bryan%20Hooi%20and%20Cynthia%20Breazeal%20and%20Hae%20Won%20Park&entry.1292438233=Multi-agent%20systems%20have%20evolved%20into%20practical%20LLM-driven%20collaborators%20for%20many%20applications%2C%20gaining%20robustness%20from%20diversity%20and%20cross-checking.%20However%2C%20multi-agent%20RL%20%28MARL%29%20training%20is%20resource-intensive%20and%20unstable%3A%20co-adapting%20teammates%20induce%20non-stationarity%2C%20and%20rewards%20are%20often%20sparse%20and%20high-variance.%20Therefore%2C%20we%20introduce%20%5Ctextbf%7BMulti-Agent%20Test-Time%20Reinforcement%20Learning%20%28MATTRL%29%7D%2C%20a%20framework%20that%20injects%20structured%20textual%20experience%20into%20multi-agent%20deliberation%20at%20inference%20time.%20MATTRL%20forms%20a%20multi-expert%20team%20of%20specialists%20for%20multi-turn%20discussions%2C%20retrieves%20and%20integrates%20test-time%20experiences%2C%20and%20reaches%20consensus%20for%20final%20decision-making.%20We%20also%20study%20credit%20assignment%20for%20constructing%20a%20turn-level%20experience%20pool%2C%20then%20reinjecting%20it%20into%20the%20dialogue.%20Across%20challenging%20benchmarks%20in%20medicine%2C%20math%2C%20and%20education%2C%20MATTRL%20improves%20accuracy%20by%20an%20average%20of%203.67%5C%25%20over%20a%20multi-agent%20baseline%2C%20and%20by%208.67%5C%25%20over%20comparable%20single-agent%20baselines.%20Ablation%20studies%20examine%20different%20credit-assignment%20schemes%20and%20provide%20a%20detailed%20comparison%20of%20how%20they%20affect%20training%20outcomes.%20MATTRL%20offers%20a%20stable%2C%20effective%20and%20efficient%20path%20to%20distribution-shift-robust%20multi-agent%20reasoning%20without%20tuning.&entry.1838667208=http%3A//arxiv.org/abs/2601.09667v1&entry.124074799=Read"},
{"title": "Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling", "author": "Shuyang Xiang and Hao Guan", "abstract": "Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \\times 8$ pixels. Remarkably, these inputs achieve 39.2\\% accuracy, comparable to the index-based baseline of 39.1\\%. Such low-resource settings also exhibit a pronounced \\emph{hot-start} effect: by 0.4\\% of total training, accuracy reaches above 12\\%, while index-based models lag at below 6\\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.", "link": "http://arxiv.org/abs/2601.09566v1", "date": "2026-01-14", "relevancy": 2.0988, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5343}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hot-Start%20from%20Pixels%3A%20Low-Resolution%20Visual%20Tokens%20for%20Chinese%20Language%20Modeling&body=Title%3A%20Hot-Start%20from%20Pixels%3A%20Low-Resolution%20Visual%20Tokens%20for%20Chinese%20Language%20Modeling%0AAuthor%3A%20Shuyang%20Xiang%20and%20Hao%20Guan%0AAbstract%3A%20Large%20language%20models%20typically%20represent%20Chinese%20characters%20as%20discrete%20index-based%20tokens%2C%20largely%20ignoring%20their%20visual%20form.%20For%20logographic%20scripts%2C%20visual%20structure%20carries%20semantic%20and%20phonetic%20information%2C%20which%20may%20aid%20prediction.%20We%20investigate%20whether%20low-resolution%20visual%20inputs%20can%20serve%20as%20an%20alternative%20for%20character-level%20modeling.%20Instead%20of%20token%20IDs%2C%20our%20decoder%20receives%20grayscale%20images%20of%20individual%20characters%2C%20with%20resolutions%20as%20low%20as%20%248%20%5Ctimes%208%24%20pixels.%20Remarkably%2C%20these%20inputs%20achieve%2039.2%5C%25%20accuracy%2C%20comparable%20to%20the%20index-based%20baseline%20of%2039.1%5C%25.%20Such%20low-resource%20settings%20also%20exhibit%20a%20pronounced%20%5Cemph%7Bhot-start%7D%20effect%3A%20by%200.4%5C%25%20of%20total%20training%2C%20accuracy%20reaches%20above%2012%5C%25%2C%20while%20index-based%20models%20lag%20at%20below%206%5C%25.%20Overall%2C%20our%20results%20demonstrate%20that%20minimal%20visual%20structure%20can%20provide%20a%20robust%20and%20efficient%20signal%20for%20Chinese%20language%20modeling%2C%20offering%20an%20alternative%20perspective%20on%20character%20representation%20that%20complements%20traditional%20index-based%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHot-Start%2520from%2520Pixels%253A%2520Low-Resolution%2520Visual%2520Tokens%2520for%2520Chinese%2520Language%2520Modeling%26entry.906535625%3DShuyang%2520Xiang%2520and%2520Hao%2520Guan%26entry.1292438233%3DLarge%2520language%2520models%2520typically%2520represent%2520Chinese%2520characters%2520as%2520discrete%2520index-based%2520tokens%252C%2520largely%2520ignoring%2520their%2520visual%2520form.%2520For%2520logographic%2520scripts%252C%2520visual%2520structure%2520carries%2520semantic%2520and%2520phonetic%2520information%252C%2520which%2520may%2520aid%2520prediction.%2520We%2520investigate%2520whether%2520low-resolution%2520visual%2520inputs%2520can%2520serve%2520as%2520an%2520alternative%2520for%2520character-level%2520modeling.%2520Instead%2520of%2520token%2520IDs%252C%2520our%2520decoder%2520receives%2520grayscale%2520images%2520of%2520individual%2520characters%252C%2520with%2520resolutions%2520as%2520low%2520as%2520%25248%2520%255Ctimes%25208%2524%2520pixels.%2520Remarkably%252C%2520these%2520inputs%2520achieve%252039.2%255C%2525%2520accuracy%252C%2520comparable%2520to%2520the%2520index-based%2520baseline%2520of%252039.1%255C%2525.%2520Such%2520low-resource%2520settings%2520also%2520exhibit%2520a%2520pronounced%2520%255Cemph%257Bhot-start%257D%2520effect%253A%2520by%25200.4%255C%2525%2520of%2520total%2520training%252C%2520accuracy%2520reaches%2520above%252012%255C%2525%252C%2520while%2520index-based%2520models%2520lag%2520at%2520below%25206%255C%2525.%2520Overall%252C%2520our%2520results%2520demonstrate%2520that%2520minimal%2520visual%2520structure%2520can%2520provide%2520a%2520robust%2520and%2520efficient%2520signal%2520for%2520Chinese%2520language%2520modeling%252C%2520offering%2520an%2520alternative%2520perspective%2520on%2520character%2520representation%2520that%2520complements%2520traditional%2520index-based%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hot-Start%20from%20Pixels%3A%20Low-Resolution%20Visual%20Tokens%20for%20Chinese%20Language%20Modeling&entry.906535625=Shuyang%20Xiang%20and%20Hao%20Guan&entry.1292438233=Large%20language%20models%20typically%20represent%20Chinese%20characters%20as%20discrete%20index-based%20tokens%2C%20largely%20ignoring%20their%20visual%20form.%20For%20logographic%20scripts%2C%20visual%20structure%20carries%20semantic%20and%20phonetic%20information%2C%20which%20may%20aid%20prediction.%20We%20investigate%20whether%20low-resolution%20visual%20inputs%20can%20serve%20as%20an%20alternative%20for%20character-level%20modeling.%20Instead%20of%20token%20IDs%2C%20our%20decoder%20receives%20grayscale%20images%20of%20individual%20characters%2C%20with%20resolutions%20as%20low%20as%20%248%20%5Ctimes%208%24%20pixels.%20Remarkably%2C%20these%20inputs%20achieve%2039.2%5C%25%20accuracy%2C%20comparable%20to%20the%20index-based%20baseline%20of%2039.1%5C%25.%20Such%20low-resource%20settings%20also%20exhibit%20a%20pronounced%20%5Cemph%7Bhot-start%7D%20effect%3A%20by%200.4%5C%25%20of%20total%20training%2C%20accuracy%20reaches%20above%2012%5C%25%2C%20while%20index-based%20models%20lag%20at%20below%206%5C%25.%20Overall%2C%20our%20results%20demonstrate%20that%20minimal%20visual%20structure%20can%20provide%20a%20robust%20and%20efficient%20signal%20for%20Chinese%20language%20modeling%2C%20offering%20an%20alternative%20perspective%20on%20character%20representation%20that%20complements%20traditional%20index-based%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.09566v1&entry.124074799=Read"},
{"title": "FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures", "author": "Jifeng Song and Arun Das and Pan Wang and Hui Ji and Kun Zhao and Yufei Huang", "abstract": "Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.", "link": "http://arxiv.org/abs/2601.08026v2", "date": "2026-01-14", "relevancy": 2.0963, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5275}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FigEx2%3A%20Visual-Conditioned%20Panel%20Detection%20and%20Captioning%20for%20Scientific%20Compound%20Figures&body=Title%3A%20FigEx2%3A%20Visual-Conditioned%20Panel%20Detection%20and%20Captioning%20for%20Scientific%20Compound%20Figures%0AAuthor%3A%20Jifeng%20Song%20and%20Arun%20Das%20and%20Pan%20Wang%20and%20Hui%20Ji%20and%20Kun%20Zhao%20and%20Yufei%20Huang%0AAbstract%3A%20Scientific%20compound%20figures%20combine%20multiple%20labeled%20panels%20into%20a%20single%20image%2C%20but%20captions%20in%20real%20pipelines%20are%20often%20missing%20or%20only%20provide%20figure-level%20summaries%2C%20making%20panel-level%20understanding%20difficult.%20In%20this%20paper%2C%20we%20propose%20FigEx2%2C%20visual-conditioned%20framework%20that%20localizes%20panels%20and%20generates%20panel-wise%20captions%20directly%20from%20the%20compound%20figure.%20To%20mitigate%20the%20impact%20of%20diverse%20phrasing%20in%20open-ended%20captioning%2C%20we%20introduce%20a%20noise-aware%20gated%20fusion%20module%20that%20adaptively%20filters%20token-level%20features%20to%20stabilize%20the%20detection%20query%20space.%20Furthermore%2C%20we%20employ%20a%20staged%20optimization%20strategy%20combining%20supervised%20learning%20with%20reinforcement%20learning%20%28RL%29%2C%20utilizing%20CLIP-based%20alignment%20and%20BERTScore-based%20semantic%20rewards%20to%20enforce%20strict%20multimodal%20consistency.%20To%20support%20high-quality%20supervision%2C%20we%20curate%20BioSci-Fig-Cap%2C%20a%20refined%20benchmark%20for%20panel-level%20grounding%2C%20alongside%20cross-disciplinary%20test%20suites%20in%20physics%20and%20chemistry.%20Experimental%20results%20demonstrate%20that%20FigEx2%20achieves%20a%20superior%200.726%20mAP%400.5%3A0.95%20for%20detection%20and%20significantly%20outperforms%20Qwen3-VL-8B%20by%200.51%20in%20METEOR%20and%200.24%20in%20BERTScore.%20Notably%2C%20FigEx2%20exhibits%20remarkable%20zero-shot%20transferability%20to%20out-of-distribution%20scientific%20domains%20without%20any%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFigEx2%253A%2520Visual-Conditioned%2520Panel%2520Detection%2520and%2520Captioning%2520for%2520Scientific%2520Compound%2520Figures%26entry.906535625%3DJifeng%2520Song%2520and%2520Arun%2520Das%2520and%2520Pan%2520Wang%2520and%2520Hui%2520Ji%2520and%2520Kun%2520Zhao%2520and%2520Yufei%2520Huang%26entry.1292438233%3DScientific%2520compound%2520figures%2520combine%2520multiple%2520labeled%2520panels%2520into%2520a%2520single%2520image%252C%2520but%2520captions%2520in%2520real%2520pipelines%2520are%2520often%2520missing%2520or%2520only%2520provide%2520figure-level%2520summaries%252C%2520making%2520panel-level%2520understanding%2520difficult.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FigEx2%252C%2520visual-conditioned%2520framework%2520that%2520localizes%2520panels%2520and%2520generates%2520panel-wise%2520captions%2520directly%2520from%2520the%2520compound%2520figure.%2520To%2520mitigate%2520the%2520impact%2520of%2520diverse%2520phrasing%2520in%2520open-ended%2520captioning%252C%2520we%2520introduce%2520a%2520noise-aware%2520gated%2520fusion%2520module%2520that%2520adaptively%2520filters%2520token-level%2520features%2520to%2520stabilize%2520the%2520detection%2520query%2520space.%2520Furthermore%252C%2520we%2520employ%2520a%2520staged%2520optimization%2520strategy%2520combining%2520supervised%2520learning%2520with%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520utilizing%2520CLIP-based%2520alignment%2520and%2520BERTScore-based%2520semantic%2520rewards%2520to%2520enforce%2520strict%2520multimodal%2520consistency.%2520To%2520support%2520high-quality%2520supervision%252C%2520we%2520curate%2520BioSci-Fig-Cap%252C%2520a%2520refined%2520benchmark%2520for%2520panel-level%2520grounding%252C%2520alongside%2520cross-disciplinary%2520test%2520suites%2520in%2520physics%2520and%2520chemistry.%2520Experimental%2520results%2520demonstrate%2520that%2520FigEx2%2520achieves%2520a%2520superior%25200.726%2520mAP%25400.5%253A0.95%2520for%2520detection%2520and%2520significantly%2520outperforms%2520Qwen3-VL-8B%2520by%25200.51%2520in%2520METEOR%2520and%25200.24%2520in%2520BERTScore.%2520Notably%252C%2520FigEx2%2520exhibits%2520remarkable%2520zero-shot%2520transferability%2520to%2520out-of-distribution%2520scientific%2520domains%2520without%2520any%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FigEx2%3A%20Visual-Conditioned%20Panel%20Detection%20and%20Captioning%20for%20Scientific%20Compound%20Figures&entry.906535625=Jifeng%20Song%20and%20Arun%20Das%20and%20Pan%20Wang%20and%20Hui%20Ji%20and%20Kun%20Zhao%20and%20Yufei%20Huang&entry.1292438233=Scientific%20compound%20figures%20combine%20multiple%20labeled%20panels%20into%20a%20single%20image%2C%20but%20captions%20in%20real%20pipelines%20are%20often%20missing%20or%20only%20provide%20figure-level%20summaries%2C%20making%20panel-level%20understanding%20difficult.%20In%20this%20paper%2C%20we%20propose%20FigEx2%2C%20visual-conditioned%20framework%20that%20localizes%20panels%20and%20generates%20panel-wise%20captions%20directly%20from%20the%20compound%20figure.%20To%20mitigate%20the%20impact%20of%20diverse%20phrasing%20in%20open-ended%20captioning%2C%20we%20introduce%20a%20noise-aware%20gated%20fusion%20module%20that%20adaptively%20filters%20token-level%20features%20to%20stabilize%20the%20detection%20query%20space.%20Furthermore%2C%20we%20employ%20a%20staged%20optimization%20strategy%20combining%20supervised%20learning%20with%20reinforcement%20learning%20%28RL%29%2C%20utilizing%20CLIP-based%20alignment%20and%20BERTScore-based%20semantic%20rewards%20to%20enforce%20strict%20multimodal%20consistency.%20To%20support%20high-quality%20supervision%2C%20we%20curate%20BioSci-Fig-Cap%2C%20a%20refined%20benchmark%20for%20panel-level%20grounding%2C%20alongside%20cross-disciplinary%20test%20suites%20in%20physics%20and%20chemistry.%20Experimental%20results%20demonstrate%20that%20FigEx2%20achieves%20a%20superior%200.726%20mAP%400.5%3A0.95%20for%20detection%20and%20significantly%20outperforms%20Qwen3-VL-8B%20by%200.51%20in%20METEOR%20and%200.24%20in%20BERTScore.%20Notably%2C%20FigEx2%20exhibits%20remarkable%20zero-shot%20transferability%20to%20out-of-distribution%20scientific%20domains%20without%20any%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2601.08026v2&entry.124074799=Read"},
{"title": "Lens: A Knowledge-Guided Foundation Model for Network Traffic", "author": "Xiaochang Li and Chen Qian and Qineng Wang and Jiangtao Kong and Yuchen Wang and Ziyu Yao and Bo Ji and Long Cheng and Gang Zhou and Huajie Shao", "abstract": "Network traffic refers to the amount of data being sent and received over the Internet or any system that connects computers. Analyzing network traffic is vital for security and management, yet remains challenging due to the heterogeneity of plain-text packet headers and encrypted payloads. To capture the latent semantics of traffic, recent studies have adopted Transformer-based pretraining techniques to learn network representations from massive traffic data. However, these methods pre-train on data-driven tasks but overlook network knowledge, such as masking partial digits of the indivisible network port numbers for prediction, thereby limiting semantic understanding. In addition, they struggle to extend classification to new classes during fine-tuning due to the distribution shift. Motivated by these limitations, we propose \\Lens, a unified knowledge-guided foundation model for both network traffic classification and generation. In pretraining, we propose a Knowledge-Guided Mask Span Prediction method with textual context for learning knowledge-enriched representations. For extending to new classes in finetuning, we reframe the traffic classification as a closed-ended generation task and introduce context-aware finetuning to adapt to the distribution shift. Evaluation results across various benchmark datasets demonstrate that the proposed Lens~achieves superior performance on both classification and generation tasks. For traffic classification, Lens~outperforms competitive baselines substantially on 8 out of 12 tasks with an average accuracy of \\textbf{96.33\\%} and extends to novel classes with significantly better performance. For traffic generation, Lens~generates better high-fidelity network traffic for network simulation, gaining up to \\textbf{30.46\\%} and \\textbf{33.3\\%} better accuracy and F1 in fuzzing tests. We will open-source the code upon publication.", "link": "http://arxiv.org/abs/2402.03646v5", "date": "2026-01-14", "relevancy": 2.0893, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lens%3A%20A%20Knowledge-Guided%20Foundation%20Model%20for%20Network%20Traffic&body=Title%3A%20Lens%3A%20A%20Knowledge-Guided%20Foundation%20Model%20for%20Network%20Traffic%0AAuthor%3A%20Xiaochang%20Li%20and%20Chen%20Qian%20and%20Qineng%20Wang%20and%20Jiangtao%20Kong%20and%20Yuchen%20Wang%20and%20Ziyu%20Yao%20and%20Bo%20Ji%20and%20Long%20Cheng%20and%20Gang%20Zhou%20and%20Huajie%20Shao%0AAbstract%3A%20Network%20traffic%20refers%20to%20the%20amount%20of%20data%20being%20sent%20and%20received%20over%20the%20Internet%20or%20any%20system%20that%20connects%20computers.%20Analyzing%20network%20traffic%20is%20vital%20for%20security%20and%20management%2C%20yet%20remains%20challenging%20due%20to%20the%20heterogeneity%20of%20plain-text%20packet%20headers%20and%20encrypted%20payloads.%20To%20capture%20the%20latent%20semantics%20of%20traffic%2C%20recent%20studies%20have%20adopted%20Transformer-based%20pretraining%20techniques%20to%20learn%20network%20representations%20from%20massive%20traffic%20data.%20However%2C%20these%20methods%20pre-train%20on%20data-driven%20tasks%20but%20overlook%20network%20knowledge%2C%20such%20as%20masking%20partial%20digits%20of%20the%20indivisible%20network%20port%20numbers%20for%20prediction%2C%20thereby%20limiting%20semantic%20understanding.%20In%20addition%2C%20they%20struggle%20to%20extend%20classification%20to%20new%20classes%20during%20fine-tuning%20due%20to%20the%20distribution%20shift.%20Motivated%20by%20these%20limitations%2C%20we%20propose%20%5CLens%2C%20a%20unified%20knowledge-guided%20foundation%20model%20for%20both%20network%20traffic%20classification%20and%20generation.%20In%20pretraining%2C%20we%20propose%20a%20Knowledge-Guided%20Mask%20Span%20Prediction%20method%20with%20textual%20context%20for%20learning%20knowledge-enriched%20representations.%20For%20extending%20to%20new%20classes%20in%20finetuning%2C%20we%20reframe%20the%20traffic%20classification%20as%20a%20closed-ended%20generation%20task%20and%20introduce%20context-aware%20finetuning%20to%20adapt%20to%20the%20distribution%20shift.%20Evaluation%20results%20across%20various%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%20Lens~achieves%20superior%20performance%20on%20both%20classification%20and%20generation%20tasks.%20For%20traffic%20classification%2C%20Lens~outperforms%20competitive%20baselines%20substantially%20on%208%20out%20of%2012%20tasks%20with%20an%20average%20accuracy%20of%20%5Ctextbf%7B96.33%5C%25%7D%20and%20extends%20to%20novel%20classes%20with%20significantly%20better%20performance.%20For%20traffic%20generation%2C%20Lens~generates%20better%20high-fidelity%20network%20traffic%20for%20network%20simulation%2C%20gaining%20up%20to%20%5Ctextbf%7B30.46%5C%25%7D%20and%20%5Ctextbf%7B33.3%5C%25%7D%20better%20accuracy%20and%20F1%20in%20fuzzing%20tests.%20We%20will%20open-source%20the%20code%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2402.03646v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLens%253A%2520A%2520Knowledge-Guided%2520Foundation%2520Model%2520for%2520Network%2520Traffic%26entry.906535625%3DXiaochang%2520Li%2520and%2520Chen%2520Qian%2520and%2520Qineng%2520Wang%2520and%2520Jiangtao%2520Kong%2520and%2520Yuchen%2520Wang%2520and%2520Ziyu%2520Yao%2520and%2520Bo%2520Ji%2520and%2520Long%2520Cheng%2520and%2520Gang%2520Zhou%2520and%2520Huajie%2520Shao%26entry.1292438233%3DNetwork%2520traffic%2520refers%2520to%2520the%2520amount%2520of%2520data%2520being%2520sent%2520and%2520received%2520over%2520the%2520Internet%2520or%2520any%2520system%2520that%2520connects%2520computers.%2520Analyzing%2520network%2520traffic%2520is%2520vital%2520for%2520security%2520and%2520management%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520the%2520heterogeneity%2520of%2520plain-text%2520packet%2520headers%2520and%2520encrypted%2520payloads.%2520To%2520capture%2520the%2520latent%2520semantics%2520of%2520traffic%252C%2520recent%2520studies%2520have%2520adopted%2520Transformer-based%2520pretraining%2520techniques%2520to%2520learn%2520network%2520representations%2520from%2520massive%2520traffic%2520data.%2520However%252C%2520these%2520methods%2520pre-train%2520on%2520data-driven%2520tasks%2520but%2520overlook%2520network%2520knowledge%252C%2520such%2520as%2520masking%2520partial%2520digits%2520of%2520the%2520indivisible%2520network%2520port%2520numbers%2520for%2520prediction%252C%2520thereby%2520limiting%2520semantic%2520understanding.%2520In%2520addition%252C%2520they%2520struggle%2520to%2520extend%2520classification%2520to%2520new%2520classes%2520during%2520fine-tuning%2520due%2520to%2520the%2520distribution%2520shift.%2520Motivated%2520by%2520these%2520limitations%252C%2520we%2520propose%2520%255CLens%252C%2520a%2520unified%2520knowledge-guided%2520foundation%2520model%2520for%2520both%2520network%2520traffic%2520classification%2520and%2520generation.%2520In%2520pretraining%252C%2520we%2520propose%2520a%2520Knowledge-Guided%2520Mask%2520Span%2520Prediction%2520method%2520with%2520textual%2520context%2520for%2520learning%2520knowledge-enriched%2520representations.%2520For%2520extending%2520to%2520new%2520classes%2520in%2520finetuning%252C%2520we%2520reframe%2520the%2520traffic%2520classification%2520as%2520a%2520closed-ended%2520generation%2520task%2520and%2520introduce%2520context-aware%2520finetuning%2520to%2520adapt%2520to%2520the%2520distribution%2520shift.%2520Evaluation%2520results%2520across%2520various%2520benchmark%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520Lens~achieves%2520superior%2520performance%2520on%2520both%2520classification%2520and%2520generation%2520tasks.%2520For%2520traffic%2520classification%252C%2520Lens~outperforms%2520competitive%2520baselines%2520substantially%2520on%25208%2520out%2520of%252012%2520tasks%2520with%2520an%2520average%2520accuracy%2520of%2520%255Ctextbf%257B96.33%255C%2525%257D%2520and%2520extends%2520to%2520novel%2520classes%2520with%2520significantly%2520better%2520performance.%2520For%2520traffic%2520generation%252C%2520Lens~generates%2520better%2520high-fidelity%2520network%2520traffic%2520for%2520network%2520simulation%252C%2520gaining%2520up%2520to%2520%255Ctextbf%257B30.46%255C%2525%257D%2520and%2520%255Ctextbf%257B33.3%255C%2525%257D%2520better%2520accuracy%2520and%2520F1%2520in%2520fuzzing%2520tests.%2520We%2520will%2520open-source%2520the%2520code%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03646v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lens%3A%20A%20Knowledge-Guided%20Foundation%20Model%20for%20Network%20Traffic&entry.906535625=Xiaochang%20Li%20and%20Chen%20Qian%20and%20Qineng%20Wang%20and%20Jiangtao%20Kong%20and%20Yuchen%20Wang%20and%20Ziyu%20Yao%20and%20Bo%20Ji%20and%20Long%20Cheng%20and%20Gang%20Zhou%20and%20Huajie%20Shao&entry.1292438233=Network%20traffic%20refers%20to%20the%20amount%20of%20data%20being%20sent%20and%20received%20over%20the%20Internet%20or%20any%20system%20that%20connects%20computers.%20Analyzing%20network%20traffic%20is%20vital%20for%20security%20and%20management%2C%20yet%20remains%20challenging%20due%20to%20the%20heterogeneity%20of%20plain-text%20packet%20headers%20and%20encrypted%20payloads.%20To%20capture%20the%20latent%20semantics%20of%20traffic%2C%20recent%20studies%20have%20adopted%20Transformer-based%20pretraining%20techniques%20to%20learn%20network%20representations%20from%20massive%20traffic%20data.%20However%2C%20these%20methods%20pre-train%20on%20data-driven%20tasks%20but%20overlook%20network%20knowledge%2C%20such%20as%20masking%20partial%20digits%20of%20the%20indivisible%20network%20port%20numbers%20for%20prediction%2C%20thereby%20limiting%20semantic%20understanding.%20In%20addition%2C%20they%20struggle%20to%20extend%20classification%20to%20new%20classes%20during%20fine-tuning%20due%20to%20the%20distribution%20shift.%20Motivated%20by%20these%20limitations%2C%20we%20propose%20%5CLens%2C%20a%20unified%20knowledge-guided%20foundation%20model%20for%20both%20network%20traffic%20classification%20and%20generation.%20In%20pretraining%2C%20we%20propose%20a%20Knowledge-Guided%20Mask%20Span%20Prediction%20method%20with%20textual%20context%20for%20learning%20knowledge-enriched%20representations.%20For%20extending%20to%20new%20classes%20in%20finetuning%2C%20we%20reframe%20the%20traffic%20classification%20as%20a%20closed-ended%20generation%20task%20and%20introduce%20context-aware%20finetuning%20to%20adapt%20to%20the%20distribution%20shift.%20Evaluation%20results%20across%20various%20benchmark%20datasets%20demonstrate%20that%20the%20proposed%20Lens~achieves%20superior%20performance%20on%20both%20classification%20and%20generation%20tasks.%20For%20traffic%20classification%2C%20Lens~outperforms%20competitive%20baselines%20substantially%20on%208%20out%20of%2012%20tasks%20with%20an%20average%20accuracy%20of%20%5Ctextbf%7B96.33%5C%25%7D%20and%20extends%20to%20novel%20classes%20with%20significantly%20better%20performance.%20For%20traffic%20generation%2C%20Lens~generates%20better%20high-fidelity%20network%20traffic%20for%20network%20simulation%2C%20gaining%20up%20to%20%5Ctextbf%7B30.46%5C%25%7D%20and%20%5Ctextbf%7B33.3%5C%25%7D%20better%20accuracy%20and%20F1%20in%20fuzzing%20tests.%20We%20will%20open-source%20the%20code%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2402.03646v5&entry.124074799=Read"},
{"title": "LVLM-Aware Multimodal Retrieval for RAG-Based Medical Diagnosis with General-Purpose Models", "author": "Nir Mazor and Tom Hope", "abstract": "Retrieving visual and textual information from medical literature and hospital records can enhance diagnostic accuracy for clinical image interpretation. However, multimodal retrieval-augmented diagnosis is highly challenging. We explore a lightweight mechanism for enhancing diagnostic performance of retrieval-augmented LVLMs. We train a lightweight LVLM-aware multimodal retriever, such that the retriever learns to return images and texts that guide the LVLM toward correct predictions. In our low-resource setting, we perform only lightweight fine-tuning with small amounts of data, and use only general-purpose backbone models, achieving competitive results in clinical classification and VQA tasks compared to medically pre-trained models with extensive training. In a novel analysis, we highlight a previously unexplored class of errors that we term inconsistent retrieval predictions: cases where different top-retrieved images yield different predictions for the same target. We find that these cases are challenging for all models, even for non-retrieval models, and that our retrieval optimization mechanism significantly improves these cases over standard RAG. However, our analysis also sheds light on gaps in the ability of LVLMs to utilize retrieved information for clinical predictions. Code and models available at: https://github.com/Nirmaz/CLARE.", "link": "http://arxiv.org/abs/2508.17394v5", "date": "2026-01-14", "relevancy": 2.0817, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.538}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVLM-Aware%20Multimodal%20Retrieval%20for%20RAG-Based%20Medical%20Diagnosis%20with%20General-Purpose%20Models&body=Title%3A%20LVLM-Aware%20Multimodal%20Retrieval%20for%20RAG-Based%20Medical%20Diagnosis%20with%20General-Purpose%20Models%0AAuthor%3A%20Nir%20Mazor%20and%20Tom%20Hope%0AAbstract%3A%20Retrieving%20visual%20and%20textual%20information%20from%20medical%20literature%20and%20hospital%20records%20can%20enhance%20diagnostic%20accuracy%20for%20clinical%20image%20interpretation.%20However%2C%20multimodal%20retrieval-augmented%20diagnosis%20is%20highly%20challenging.%20We%20explore%20a%20lightweight%20mechanism%20for%20enhancing%20diagnostic%20performance%20of%20retrieval-augmented%20LVLMs.%20We%20train%20a%20lightweight%20LVLM-aware%20multimodal%20retriever%2C%20such%20that%20the%20retriever%20learns%20to%20return%20images%20and%20texts%20that%20guide%20the%20LVLM%20toward%20correct%20predictions.%20In%20our%20low-resource%20setting%2C%20we%20perform%20only%20lightweight%20fine-tuning%20with%20small%20amounts%20of%20data%2C%20and%20use%20only%20general-purpose%20backbone%20models%2C%20achieving%20competitive%20results%20in%20clinical%20classification%20and%20VQA%20tasks%20compared%20to%20medically%20pre-trained%20models%20with%20extensive%20training.%20In%20a%20novel%20analysis%2C%20we%20highlight%20a%20previously%20unexplored%20class%20of%20errors%20that%20we%20term%20inconsistent%20retrieval%20predictions%3A%20cases%20where%20different%20top-retrieved%20images%20yield%20different%20predictions%20for%20the%20same%20target.%20We%20find%20that%20these%20cases%20are%20challenging%20for%20all%20models%2C%20even%20for%20non-retrieval%20models%2C%20and%20that%20our%20retrieval%20optimization%20mechanism%20significantly%20improves%20these%20cases%20over%20standard%20RAG.%20However%2C%20our%20analysis%20also%20sheds%20light%20on%20gaps%20in%20the%20ability%20of%20LVLMs%20to%20utilize%20retrieved%20information%20for%20clinical%20predictions.%20Code%20and%20models%20available%20at%3A%20https%3A//github.com/Nirmaz/CLARE.%0ALink%3A%20http%3A//arxiv.org/abs/2508.17394v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVLM-Aware%2520Multimodal%2520Retrieval%2520for%2520RAG-Based%2520Medical%2520Diagnosis%2520with%2520General-Purpose%2520Models%26entry.906535625%3DNir%2520Mazor%2520and%2520Tom%2520Hope%26entry.1292438233%3DRetrieving%2520visual%2520and%2520textual%2520information%2520from%2520medical%2520literature%2520and%2520hospital%2520records%2520can%2520enhance%2520diagnostic%2520accuracy%2520for%2520clinical%2520image%2520interpretation.%2520However%252C%2520multimodal%2520retrieval-augmented%2520diagnosis%2520is%2520highly%2520challenging.%2520We%2520explore%2520a%2520lightweight%2520mechanism%2520for%2520enhancing%2520diagnostic%2520performance%2520of%2520retrieval-augmented%2520LVLMs.%2520We%2520train%2520a%2520lightweight%2520LVLM-aware%2520multimodal%2520retriever%252C%2520such%2520that%2520the%2520retriever%2520learns%2520to%2520return%2520images%2520and%2520texts%2520that%2520guide%2520the%2520LVLM%2520toward%2520correct%2520predictions.%2520In%2520our%2520low-resource%2520setting%252C%2520we%2520perform%2520only%2520lightweight%2520fine-tuning%2520with%2520small%2520amounts%2520of%2520data%252C%2520and%2520use%2520only%2520general-purpose%2520backbone%2520models%252C%2520achieving%2520competitive%2520results%2520in%2520clinical%2520classification%2520and%2520VQA%2520tasks%2520compared%2520to%2520medically%2520pre-trained%2520models%2520with%2520extensive%2520training.%2520In%2520a%2520novel%2520analysis%252C%2520we%2520highlight%2520a%2520previously%2520unexplored%2520class%2520of%2520errors%2520that%2520we%2520term%2520inconsistent%2520retrieval%2520predictions%253A%2520cases%2520where%2520different%2520top-retrieved%2520images%2520yield%2520different%2520predictions%2520for%2520the%2520same%2520target.%2520We%2520find%2520that%2520these%2520cases%2520are%2520challenging%2520for%2520all%2520models%252C%2520even%2520for%2520non-retrieval%2520models%252C%2520and%2520that%2520our%2520retrieval%2520optimization%2520mechanism%2520significantly%2520improves%2520these%2520cases%2520over%2520standard%2520RAG.%2520However%252C%2520our%2520analysis%2520also%2520sheds%2520light%2520on%2520gaps%2520in%2520the%2520ability%2520of%2520LVLMs%2520to%2520utilize%2520retrieved%2520information%2520for%2520clinical%2520predictions.%2520Code%2520and%2520models%2520available%2520at%253A%2520https%253A//github.com/Nirmaz/CLARE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17394v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVLM-Aware%20Multimodal%20Retrieval%20for%20RAG-Based%20Medical%20Diagnosis%20with%20General-Purpose%20Models&entry.906535625=Nir%20Mazor%20and%20Tom%20Hope&entry.1292438233=Retrieving%20visual%20and%20textual%20information%20from%20medical%20literature%20and%20hospital%20records%20can%20enhance%20diagnostic%20accuracy%20for%20clinical%20image%20interpretation.%20However%2C%20multimodal%20retrieval-augmented%20diagnosis%20is%20highly%20challenging.%20We%20explore%20a%20lightweight%20mechanism%20for%20enhancing%20diagnostic%20performance%20of%20retrieval-augmented%20LVLMs.%20We%20train%20a%20lightweight%20LVLM-aware%20multimodal%20retriever%2C%20such%20that%20the%20retriever%20learns%20to%20return%20images%20and%20texts%20that%20guide%20the%20LVLM%20toward%20correct%20predictions.%20In%20our%20low-resource%20setting%2C%20we%20perform%20only%20lightweight%20fine-tuning%20with%20small%20amounts%20of%20data%2C%20and%20use%20only%20general-purpose%20backbone%20models%2C%20achieving%20competitive%20results%20in%20clinical%20classification%20and%20VQA%20tasks%20compared%20to%20medically%20pre-trained%20models%20with%20extensive%20training.%20In%20a%20novel%20analysis%2C%20we%20highlight%20a%20previously%20unexplored%20class%20of%20errors%20that%20we%20term%20inconsistent%20retrieval%20predictions%3A%20cases%20where%20different%20top-retrieved%20images%20yield%20different%20predictions%20for%20the%20same%20target.%20We%20find%20that%20these%20cases%20are%20challenging%20for%20all%20models%2C%20even%20for%20non-retrieval%20models%2C%20and%20that%20our%20retrieval%20optimization%20mechanism%20significantly%20improves%20these%20cases%20over%20standard%20RAG.%20However%2C%20our%20analysis%20also%20sheds%20light%20on%20gaps%20in%20the%20ability%20of%20LVLMs%20to%20utilize%20retrieved%20information%20for%20clinical%20predictions.%20Code%20and%20models%20available%20at%3A%20https%3A//github.com/Nirmaz/CLARE.&entry.1838667208=http%3A//arxiv.org/abs/2508.17394v5&entry.124074799=Read"},
{"title": "DNN Modularization via Activation-Driven Training", "author": "Tuan Ngo and Abid Hassan and Saad Shafiq and Nenad Medvidovic", "abstract": "Deep Neural Networks (DNNs) tend to accrue technical debt and suffer from significant retraining costs when adapting to evolving requirements. Modularizing DNNs offers the promise of improving their reusability. Previous work has proposed techniques to decompose DNN models into modules both during and after training. However, these strategies yield several shortcomings, including significant weight overlaps and accuracy losses across modules, restricted focus on convolutional layers only, and added complexity and training time by introducing auxiliary masks to control modularity. In this work, we propose MODA, an activation-driven modular training approach. MODA promotes inherent modularity within a DNN model by directly regulating the activation outputs of its layers based on three modular objectives: intra-class affinity, inter-class dispersion, and compactness. MODA is evaluated using three well-known DNN models and five datasets with varying sizes. This evaluation indicates that, compared to the existing state-of-the-art, using MODA yields several advantages: (1) MODA accomplishes modularization with 22% less training time; (2) the resultant modules generated by MODA comprise up to 24x fewer weights and 37x less weight overlap while (3) preserving the original model's accuracy without additional fine-tuning; in module replacement scenarios, (4) MODA improves the accuracy of a target class by 12% on average while ensuring minimal impact on the accuracy of other classes.", "link": "http://arxiv.org/abs/2411.01074v4", "date": "2026-01-14", "relevancy": 2.0733, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5215}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DNN%20Modularization%20via%20Activation-Driven%20Training&body=Title%3A%20DNN%20Modularization%20via%20Activation-Driven%20Training%0AAuthor%3A%20Tuan%20Ngo%20and%20Abid%20Hassan%20and%20Saad%20Shafiq%20and%20Nenad%20Medvidovic%0AAbstract%3A%20Deep%20Neural%20Networks%20%28DNNs%29%20tend%20to%20accrue%20technical%20debt%20and%20suffer%20from%20significant%20retraining%20costs%20when%20adapting%20to%20evolving%20requirements.%20Modularizing%20DNNs%20offers%20the%20promise%20of%20improving%20their%20reusability.%20Previous%20work%20has%20proposed%20techniques%20to%20decompose%20DNN%20models%20into%20modules%20both%20during%20and%20after%20training.%20However%2C%20these%20strategies%20yield%20several%20shortcomings%2C%20including%20significant%20weight%20overlaps%20and%20accuracy%20losses%20across%20modules%2C%20restricted%20focus%20on%20convolutional%20layers%20only%2C%20and%20added%20complexity%20and%20training%20time%20by%20introducing%20auxiliary%20masks%20to%20control%20modularity.%20In%20this%20work%2C%20we%20propose%20MODA%2C%20an%20activation-driven%20modular%20training%20approach.%20MODA%20promotes%20inherent%20modularity%20within%20a%20DNN%20model%20by%20directly%20regulating%20the%20activation%20outputs%20of%20its%20layers%20based%20on%20three%20modular%20objectives%3A%20intra-class%20affinity%2C%20inter-class%20dispersion%2C%20and%20compactness.%20MODA%20is%20evaluated%20using%20three%20well-known%20DNN%20models%20and%20five%20datasets%20with%20varying%20sizes.%20This%20evaluation%20indicates%20that%2C%20compared%20to%20the%20existing%20state-of-the-art%2C%20using%20MODA%20yields%20several%20advantages%3A%20%281%29%20MODA%20accomplishes%20modularization%20with%2022%25%20less%20training%20time%3B%20%282%29%20the%20resultant%20modules%20generated%20by%20MODA%20comprise%20up%20to%2024x%20fewer%20weights%20and%2037x%20less%20weight%20overlap%20while%20%283%29%20preserving%20the%20original%20model%27s%20accuracy%20without%20additional%20fine-tuning%3B%20in%20module%20replacement%20scenarios%2C%20%284%29%20MODA%20improves%20the%20accuracy%20of%20a%20target%20class%20by%2012%25%20on%20average%20while%20ensuring%20minimal%20impact%20on%20the%20accuracy%20of%20other%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2411.01074v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDNN%2520Modularization%2520via%2520Activation-Driven%2520Training%26entry.906535625%3DTuan%2520Ngo%2520and%2520Abid%2520Hassan%2520and%2520Saad%2520Shafiq%2520and%2520Nenad%2520Medvidovic%26entry.1292438233%3DDeep%2520Neural%2520Networks%2520%2528DNNs%2529%2520tend%2520to%2520accrue%2520technical%2520debt%2520and%2520suffer%2520from%2520significant%2520retraining%2520costs%2520when%2520adapting%2520to%2520evolving%2520requirements.%2520Modularizing%2520DNNs%2520offers%2520the%2520promise%2520of%2520improving%2520their%2520reusability.%2520Previous%2520work%2520has%2520proposed%2520techniques%2520to%2520decompose%2520DNN%2520models%2520into%2520modules%2520both%2520during%2520and%2520after%2520training.%2520However%252C%2520these%2520strategies%2520yield%2520several%2520shortcomings%252C%2520including%2520significant%2520weight%2520overlaps%2520and%2520accuracy%2520losses%2520across%2520modules%252C%2520restricted%2520focus%2520on%2520convolutional%2520layers%2520only%252C%2520and%2520added%2520complexity%2520and%2520training%2520time%2520by%2520introducing%2520auxiliary%2520masks%2520to%2520control%2520modularity.%2520In%2520this%2520work%252C%2520we%2520propose%2520MODA%252C%2520an%2520activation-driven%2520modular%2520training%2520approach.%2520MODA%2520promotes%2520inherent%2520modularity%2520within%2520a%2520DNN%2520model%2520by%2520directly%2520regulating%2520the%2520activation%2520outputs%2520of%2520its%2520layers%2520based%2520on%2520three%2520modular%2520objectives%253A%2520intra-class%2520affinity%252C%2520inter-class%2520dispersion%252C%2520and%2520compactness.%2520MODA%2520is%2520evaluated%2520using%2520three%2520well-known%2520DNN%2520models%2520and%2520five%2520datasets%2520with%2520varying%2520sizes.%2520This%2520evaluation%2520indicates%2520that%252C%2520compared%2520to%2520the%2520existing%2520state-of-the-art%252C%2520using%2520MODA%2520yields%2520several%2520advantages%253A%2520%25281%2529%2520MODA%2520accomplishes%2520modularization%2520with%252022%2525%2520less%2520training%2520time%253B%2520%25282%2529%2520the%2520resultant%2520modules%2520generated%2520by%2520MODA%2520comprise%2520up%2520to%252024x%2520fewer%2520weights%2520and%252037x%2520less%2520weight%2520overlap%2520while%2520%25283%2529%2520preserving%2520the%2520original%2520model%2527s%2520accuracy%2520without%2520additional%2520fine-tuning%253B%2520in%2520module%2520replacement%2520scenarios%252C%2520%25284%2529%2520MODA%2520improves%2520the%2520accuracy%2520of%2520a%2520target%2520class%2520by%252012%2525%2520on%2520average%2520while%2520ensuring%2520minimal%2520impact%2520on%2520the%2520accuracy%2520of%2520other%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01074v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DNN%20Modularization%20via%20Activation-Driven%20Training&entry.906535625=Tuan%20Ngo%20and%20Abid%20Hassan%20and%20Saad%20Shafiq%20and%20Nenad%20Medvidovic&entry.1292438233=Deep%20Neural%20Networks%20%28DNNs%29%20tend%20to%20accrue%20technical%20debt%20and%20suffer%20from%20significant%20retraining%20costs%20when%20adapting%20to%20evolving%20requirements.%20Modularizing%20DNNs%20offers%20the%20promise%20of%20improving%20their%20reusability.%20Previous%20work%20has%20proposed%20techniques%20to%20decompose%20DNN%20models%20into%20modules%20both%20during%20and%20after%20training.%20However%2C%20these%20strategies%20yield%20several%20shortcomings%2C%20including%20significant%20weight%20overlaps%20and%20accuracy%20losses%20across%20modules%2C%20restricted%20focus%20on%20convolutional%20layers%20only%2C%20and%20added%20complexity%20and%20training%20time%20by%20introducing%20auxiliary%20masks%20to%20control%20modularity.%20In%20this%20work%2C%20we%20propose%20MODA%2C%20an%20activation-driven%20modular%20training%20approach.%20MODA%20promotes%20inherent%20modularity%20within%20a%20DNN%20model%20by%20directly%20regulating%20the%20activation%20outputs%20of%20its%20layers%20based%20on%20three%20modular%20objectives%3A%20intra-class%20affinity%2C%20inter-class%20dispersion%2C%20and%20compactness.%20MODA%20is%20evaluated%20using%20three%20well-known%20DNN%20models%20and%20five%20datasets%20with%20varying%20sizes.%20This%20evaluation%20indicates%20that%2C%20compared%20to%20the%20existing%20state-of-the-art%2C%20using%20MODA%20yields%20several%20advantages%3A%20%281%29%20MODA%20accomplishes%20modularization%20with%2022%25%20less%20training%20time%3B%20%282%29%20the%20resultant%20modules%20generated%20by%20MODA%20comprise%20up%20to%2024x%20fewer%20weights%20and%2037x%20less%20weight%20overlap%20while%20%283%29%20preserving%20the%20original%20model%27s%20accuracy%20without%20additional%20fine-tuning%3B%20in%20module%20replacement%20scenarios%2C%20%284%29%20MODA%20improves%20the%20accuracy%20of%20a%20target%20class%20by%2012%25%20on%20average%20while%20ensuring%20minimal%20impact%20on%20the%20accuracy%20of%20other%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2411.01074v4&entry.124074799=Read"},
{"title": "A Taxonomy and Review of Algorithms for Modeling and Predicting Human Driver Behavior", "author": "Raunak P. Bhattacharyya and Kyle Brown and Juanran Wang and Katherine Driggs-Campbell and Mykel J. Kochenderfer", "abstract": "An open problem in autonomous driving research is modeling human driving behavior, which is needed for the planning component of the autonomy stack, safety validation through traffic simulation, and causal inference for generating explanations for autonomous driving. Modeling human driving behavior is challenging because it is stochastic, high-dimensional, and involves interaction between multiple agents. This problem has been studied in various communities with a vast body of literature. Existing reviews have generally focused on one aspect: motion prediction. In this article, we present a unification of the literature that covers intent estimation, trait estimation, and motion prediction. This unification is enabled by modeling multi-agent driving as a partially observable stochastic game, which allows us to cast driver modeling tasks as inference problems. We classify driver models into a taxonomy based on the specific tasks they address and the key attributes of their approach. Finally, we identify open research opportunities in the field of driver modeling.", "link": "http://arxiv.org/abs/2006.08832v4", "date": "2026-01-14", "relevancy": 2.0661, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5553}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5275}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Taxonomy%20and%20Review%20of%20Algorithms%20for%20Modeling%20and%20Predicting%20Human%20Driver%20Behavior&body=Title%3A%20A%20Taxonomy%20and%20Review%20of%20Algorithms%20for%20Modeling%20and%20Predicting%20Human%20Driver%20Behavior%0AAuthor%3A%20Raunak%20P.%20Bhattacharyya%20and%20Kyle%20Brown%20and%20Juanran%20Wang%20and%20Katherine%20Driggs-Campbell%20and%20Mykel%20J.%20Kochenderfer%0AAbstract%3A%20An%20open%20problem%20in%20autonomous%20driving%20research%20is%20modeling%20human%20driving%20behavior%2C%20which%20is%20needed%20for%20the%20planning%20component%20of%20the%20autonomy%20stack%2C%20safety%20validation%20through%20traffic%20simulation%2C%20and%20causal%20inference%20for%20generating%20explanations%20for%20autonomous%20driving.%20Modeling%20human%20driving%20behavior%20is%20challenging%20because%20it%20is%20stochastic%2C%20high-dimensional%2C%20and%20involves%20interaction%20between%20multiple%20agents.%20This%20problem%20has%20been%20studied%20in%20various%20communities%20with%20a%20vast%20body%20of%20literature.%20Existing%20reviews%20have%20generally%20focused%20on%20one%20aspect%3A%20motion%20prediction.%20In%20this%20article%2C%20we%20present%20a%20unification%20of%20the%20literature%20that%20covers%20intent%20estimation%2C%20trait%20estimation%2C%20and%20motion%20prediction.%20This%20unification%20is%20enabled%20by%20modeling%20multi-agent%20driving%20as%20a%20partially%20observable%20stochastic%20game%2C%20which%20allows%20us%20to%20cast%20driver%20modeling%20tasks%20as%20inference%20problems.%20We%20classify%20driver%20models%20into%20a%20taxonomy%20based%20on%20the%20specific%20tasks%20they%20address%20and%20the%20key%20attributes%20of%20their%20approach.%20Finally%2C%20we%20identify%20open%20research%20opportunities%20in%20the%20field%20of%20driver%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2006.08832v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Taxonomy%2520and%2520Review%2520of%2520Algorithms%2520for%2520Modeling%2520and%2520Predicting%2520Human%2520Driver%2520Behavior%26entry.906535625%3DRaunak%2520P.%2520Bhattacharyya%2520and%2520Kyle%2520Brown%2520and%2520Juanran%2520Wang%2520and%2520Katherine%2520Driggs-Campbell%2520and%2520Mykel%2520J.%2520Kochenderfer%26entry.1292438233%3DAn%2520open%2520problem%2520in%2520autonomous%2520driving%2520research%2520is%2520modeling%2520human%2520driving%2520behavior%252C%2520which%2520is%2520needed%2520for%2520the%2520planning%2520component%2520of%2520the%2520autonomy%2520stack%252C%2520safety%2520validation%2520through%2520traffic%2520simulation%252C%2520and%2520causal%2520inference%2520for%2520generating%2520explanations%2520for%2520autonomous%2520driving.%2520Modeling%2520human%2520driving%2520behavior%2520is%2520challenging%2520because%2520it%2520is%2520stochastic%252C%2520high-dimensional%252C%2520and%2520involves%2520interaction%2520between%2520multiple%2520agents.%2520This%2520problem%2520has%2520been%2520studied%2520in%2520various%2520communities%2520with%2520a%2520vast%2520body%2520of%2520literature.%2520Existing%2520reviews%2520have%2520generally%2520focused%2520on%2520one%2520aspect%253A%2520motion%2520prediction.%2520In%2520this%2520article%252C%2520we%2520present%2520a%2520unification%2520of%2520the%2520literature%2520that%2520covers%2520intent%2520estimation%252C%2520trait%2520estimation%252C%2520and%2520motion%2520prediction.%2520This%2520unification%2520is%2520enabled%2520by%2520modeling%2520multi-agent%2520driving%2520as%2520a%2520partially%2520observable%2520stochastic%2520game%252C%2520which%2520allows%2520us%2520to%2520cast%2520driver%2520modeling%2520tasks%2520as%2520inference%2520problems.%2520We%2520classify%2520driver%2520models%2520into%2520a%2520taxonomy%2520based%2520on%2520the%2520specific%2520tasks%2520they%2520address%2520and%2520the%2520key%2520attributes%2520of%2520their%2520approach.%2520Finally%252C%2520we%2520identify%2520open%2520research%2520opportunities%2520in%2520the%2520field%2520of%2520driver%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2006.08832v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Taxonomy%20and%20Review%20of%20Algorithms%20for%20Modeling%20and%20Predicting%20Human%20Driver%20Behavior&entry.906535625=Raunak%20P.%20Bhattacharyya%20and%20Kyle%20Brown%20and%20Juanran%20Wang%20and%20Katherine%20Driggs-Campbell%20and%20Mykel%20J.%20Kochenderfer&entry.1292438233=An%20open%20problem%20in%20autonomous%20driving%20research%20is%20modeling%20human%20driving%20behavior%2C%20which%20is%20needed%20for%20the%20planning%20component%20of%20the%20autonomy%20stack%2C%20safety%20validation%20through%20traffic%20simulation%2C%20and%20causal%20inference%20for%20generating%20explanations%20for%20autonomous%20driving.%20Modeling%20human%20driving%20behavior%20is%20challenging%20because%20it%20is%20stochastic%2C%20high-dimensional%2C%20and%20involves%20interaction%20between%20multiple%20agents.%20This%20problem%20has%20been%20studied%20in%20various%20communities%20with%20a%20vast%20body%20of%20literature.%20Existing%20reviews%20have%20generally%20focused%20on%20one%20aspect%3A%20motion%20prediction.%20In%20this%20article%2C%20we%20present%20a%20unification%20of%20the%20literature%20that%20covers%20intent%20estimation%2C%20trait%20estimation%2C%20and%20motion%20prediction.%20This%20unification%20is%20enabled%20by%20modeling%20multi-agent%20driving%20as%20a%20partially%20observable%20stochastic%20game%2C%20which%20allows%20us%20to%20cast%20driver%20modeling%20tasks%20as%20inference%20problems.%20We%20classify%20driver%20models%20into%20a%20taxonomy%20based%20on%20the%20specific%20tasks%20they%20address%20and%20the%20key%20attributes%20of%20their%20approach.%20Finally%2C%20we%20identify%20open%20research%20opportunities%20in%20the%20field%20of%20driver%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2006.08832v4&entry.124074799=Read"},
{"title": "Enabling Global, Human-Centered Explanations for LLMs:From Tokens to Interpretable Code and Test Generation", "author": "Dipin Khati and Daniel Rodriguez-Cardenas and David N. Palacio and Alejandro Velasco and Denys Poshyvanyk", "abstract": "As Large Language Models for Code (LM4Code) become integral to software engineering, establishing trust in their output becomes critical. However, standard accuracy metrics obscure the underlying reasoning of generative models, offering little insight into how decisions are made. Although post-hoc interpretability methods attempt to fill this gap, they often restrict explanations to local, token-level insights, which fail to provide a developer-understandable global analysis. Our work highlights the urgent need for \\textbf{global, code-based} explanations that reveal how models reason across code. To support this vision, we introduce \\textit{code rationales} (CodeQ), a framework that enables global interpretability by mapping token-level rationales to high-level programming categories. Aggregating thousands of these token-level explanations allows us to perform statistical analyses that expose systemic reasoning behaviors. We validate this aggregation by showing it distills a clear signal from noisy token data, reducing explanation uncertainty (Shannon entropy) by over 50%. Additionally, we find that a code generation model (\\textit{codeparrot-small}) consistently favors shallow syntactic cues (e.g., \\textbf{indentation}) over deeper semantic logic. Furthermore, in a user study with 37 participants, we find its reasoning is significantly misaligned with that of human developers. These findings, hidden from traditional metrics, demonstrate the importance of global interpretability techniques to foster trust in LM4Code.", "link": "http://arxiv.org/abs/2503.16771v2", "date": "2026-01-14", "relevancy": 2.0652, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20Global%2C%20Human-Centered%20Explanations%20for%20LLMs%3AFrom%20Tokens%20to%20Interpretable%20Code%20and%20Test%20Generation&body=Title%3A%20Enabling%20Global%2C%20Human-Centered%20Explanations%20for%20LLMs%3AFrom%20Tokens%20to%20Interpretable%20Code%20and%20Test%20Generation%0AAuthor%3A%20Dipin%20Khati%20and%20Daniel%20Rodriguez-Cardenas%20and%20David%20N.%20Palacio%20and%20Alejandro%20Velasco%20and%20Denys%20Poshyvanyk%0AAbstract%3A%20As%20Large%20Language%20Models%20for%20Code%20%28LM4Code%29%20become%20integral%20to%20software%20engineering%2C%20establishing%20trust%20in%20their%20output%20becomes%20critical.%20However%2C%20standard%20accuracy%20metrics%20obscure%20the%20underlying%20reasoning%20of%20generative%20models%2C%20offering%20little%20insight%20into%20how%20decisions%20are%20made.%20Although%20post-hoc%20interpretability%20methods%20attempt%20to%20fill%20this%20gap%2C%20they%20often%20restrict%20explanations%20to%20local%2C%20token-level%20insights%2C%20which%20fail%20to%20provide%20a%20developer-understandable%20global%20analysis.%20Our%20work%20highlights%20the%20urgent%20need%20for%20%5Ctextbf%7Bglobal%2C%20code-based%7D%20explanations%20that%20reveal%20how%20models%20reason%20across%20code.%20To%20support%20this%20vision%2C%20we%20introduce%20%5Ctextit%7Bcode%20rationales%7D%20%28CodeQ%29%2C%20a%20framework%20that%20enables%20global%20interpretability%20by%20mapping%20token-level%20rationales%20to%20high-level%20programming%20categories.%20Aggregating%20thousands%20of%20these%20token-level%20explanations%20allows%20us%20to%20perform%20statistical%20analyses%20that%20expose%20systemic%20reasoning%20behaviors.%20We%20validate%20this%20aggregation%20by%20showing%20it%20distills%20a%20clear%20signal%20from%20noisy%20token%20data%2C%20reducing%20explanation%20uncertainty%20%28Shannon%20entropy%29%20by%20over%2050%25.%20Additionally%2C%20we%20find%20that%20a%20code%20generation%20model%20%28%5Ctextit%7Bcodeparrot-small%7D%29%20consistently%20favors%20shallow%20syntactic%20cues%20%28e.g.%2C%20%5Ctextbf%7Bindentation%7D%29%20over%20deeper%20semantic%20logic.%20Furthermore%2C%20in%20a%20user%20study%20with%2037%20participants%2C%20we%20find%20its%20reasoning%20is%20significantly%20misaligned%20with%20that%20of%20human%20developers.%20These%20findings%2C%20hidden%20from%20traditional%20metrics%2C%20demonstrate%20the%20importance%20of%20global%20interpretability%20techniques%20to%20foster%20trust%20in%20LM4Code.%0ALink%3A%20http%3A//arxiv.org/abs/2503.16771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520Global%252C%2520Human-Centered%2520Explanations%2520for%2520LLMs%253AFrom%2520Tokens%2520to%2520Interpretable%2520Code%2520and%2520Test%2520Generation%26entry.906535625%3DDipin%2520Khati%2520and%2520Daniel%2520Rodriguez-Cardenas%2520and%2520David%2520N.%2520Palacio%2520and%2520Alejandro%2520Velasco%2520and%2520Denys%2520Poshyvanyk%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520for%2520Code%2520%2528LM4Code%2529%2520become%2520integral%2520to%2520software%2520engineering%252C%2520establishing%2520trust%2520in%2520their%2520output%2520becomes%2520critical.%2520However%252C%2520standard%2520accuracy%2520metrics%2520obscure%2520the%2520underlying%2520reasoning%2520of%2520generative%2520models%252C%2520offering%2520little%2520insight%2520into%2520how%2520decisions%2520are%2520made.%2520Although%2520post-hoc%2520interpretability%2520methods%2520attempt%2520to%2520fill%2520this%2520gap%252C%2520they%2520often%2520restrict%2520explanations%2520to%2520local%252C%2520token-level%2520insights%252C%2520which%2520fail%2520to%2520provide%2520a%2520developer-understandable%2520global%2520analysis.%2520Our%2520work%2520highlights%2520the%2520urgent%2520need%2520for%2520%255Ctextbf%257Bglobal%252C%2520code-based%257D%2520explanations%2520that%2520reveal%2520how%2520models%2520reason%2520across%2520code.%2520To%2520support%2520this%2520vision%252C%2520we%2520introduce%2520%255Ctextit%257Bcode%2520rationales%257D%2520%2528CodeQ%2529%252C%2520a%2520framework%2520that%2520enables%2520global%2520interpretability%2520by%2520mapping%2520token-level%2520rationales%2520to%2520high-level%2520programming%2520categories.%2520Aggregating%2520thousands%2520of%2520these%2520token-level%2520explanations%2520allows%2520us%2520to%2520perform%2520statistical%2520analyses%2520that%2520expose%2520systemic%2520reasoning%2520behaviors.%2520We%2520validate%2520this%2520aggregation%2520by%2520showing%2520it%2520distills%2520a%2520clear%2520signal%2520from%2520noisy%2520token%2520data%252C%2520reducing%2520explanation%2520uncertainty%2520%2528Shannon%2520entropy%2529%2520by%2520over%252050%2525.%2520Additionally%252C%2520we%2520find%2520that%2520a%2520code%2520generation%2520model%2520%2528%255Ctextit%257Bcodeparrot-small%257D%2529%2520consistently%2520favors%2520shallow%2520syntactic%2520cues%2520%2528e.g.%252C%2520%255Ctextbf%257Bindentation%257D%2529%2520over%2520deeper%2520semantic%2520logic.%2520Furthermore%252C%2520in%2520a%2520user%2520study%2520with%252037%2520participants%252C%2520we%2520find%2520its%2520reasoning%2520is%2520significantly%2520misaligned%2520with%2520that%2520of%2520human%2520developers.%2520These%2520findings%252C%2520hidden%2520from%2520traditional%2520metrics%252C%2520demonstrate%2520the%2520importance%2520of%2520global%2520interpretability%2520techniques%2520to%2520foster%2520trust%2520in%2520LM4Code.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20Global%2C%20Human-Centered%20Explanations%20for%20LLMs%3AFrom%20Tokens%20to%20Interpretable%20Code%20and%20Test%20Generation&entry.906535625=Dipin%20Khati%20and%20Daniel%20Rodriguez-Cardenas%20and%20David%20N.%20Palacio%20and%20Alejandro%20Velasco%20and%20Denys%20Poshyvanyk&entry.1292438233=As%20Large%20Language%20Models%20for%20Code%20%28LM4Code%29%20become%20integral%20to%20software%20engineering%2C%20establishing%20trust%20in%20their%20output%20becomes%20critical.%20However%2C%20standard%20accuracy%20metrics%20obscure%20the%20underlying%20reasoning%20of%20generative%20models%2C%20offering%20little%20insight%20into%20how%20decisions%20are%20made.%20Although%20post-hoc%20interpretability%20methods%20attempt%20to%20fill%20this%20gap%2C%20they%20often%20restrict%20explanations%20to%20local%2C%20token-level%20insights%2C%20which%20fail%20to%20provide%20a%20developer-understandable%20global%20analysis.%20Our%20work%20highlights%20the%20urgent%20need%20for%20%5Ctextbf%7Bglobal%2C%20code-based%7D%20explanations%20that%20reveal%20how%20models%20reason%20across%20code.%20To%20support%20this%20vision%2C%20we%20introduce%20%5Ctextit%7Bcode%20rationales%7D%20%28CodeQ%29%2C%20a%20framework%20that%20enables%20global%20interpretability%20by%20mapping%20token-level%20rationales%20to%20high-level%20programming%20categories.%20Aggregating%20thousands%20of%20these%20token-level%20explanations%20allows%20us%20to%20perform%20statistical%20analyses%20that%20expose%20systemic%20reasoning%20behaviors.%20We%20validate%20this%20aggregation%20by%20showing%20it%20distills%20a%20clear%20signal%20from%20noisy%20token%20data%2C%20reducing%20explanation%20uncertainty%20%28Shannon%20entropy%29%20by%20over%2050%25.%20Additionally%2C%20we%20find%20that%20a%20code%20generation%20model%20%28%5Ctextit%7Bcodeparrot-small%7D%29%20consistently%20favors%20shallow%20syntactic%20cues%20%28e.g.%2C%20%5Ctextbf%7Bindentation%7D%29%20over%20deeper%20semantic%20logic.%20Furthermore%2C%20in%20a%20user%20study%20with%2037%20participants%2C%20we%20find%20its%20reasoning%20is%20significantly%20misaligned%20with%20that%20of%20human%20developers.%20These%20findings%2C%20hidden%20from%20traditional%20metrics%2C%20demonstrate%20the%20importance%20of%20global%20interpretability%20techniques%20to%20foster%20trust%20in%20LM4Code.&entry.1838667208=http%3A//arxiv.org/abs/2503.16771v2&entry.124074799=Read"},
{"title": "Efficient Test-Time Scaling of Multi-Step Reasoning by Probing Internal States of Large Language Models", "author": "Jingwei Ni and Ekaterina Fadeeva and Tianyi Wu and Mubashara Akhtar and Jiaheng Zhang and Elliott Ash and Markus Leippold and Timothy Baldwin and See-Kiong Ng and Artem Shelmanov and Mrinmaya Sachan", "abstract": "LLMs can solve complex tasks by generating long, multi-step reasoning chains. Test-time scaling (TTS) can further improve LLM performance by sampling multiple variants of intermediate reasoning steps, verifying their correctness, and strategically choosing the best steps for continuation. However, existing verification approaches, such as Process Reward Models (PRMs), are computationally expensive, limited to specific domains, and require large-scale human or model-generated annotations. We propose a lightweight alternative for step-level reasoning verification based on probing the internal states of LLMs. We train a transformer-based probe that uses the internal states of the frozen LLM to estimate the credibility of its reasoning steps during generation. Annotation can be generated either by another larger LLM (e.g., DeepSeek-R1) or in a self-supervised manner by the original model itself. The probes are both effective and lightweight, containing fewer than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, our probes match or even exceed the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their confidence in reasoning processes and can serve as reliable signals for reasoning step verification, offering a promising direction towards scalable and generalizable TTS and introspective LLMs.", "link": "http://arxiv.org/abs/2511.06209v3", "date": "2026-01-14", "relevancy": 2.0452, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Test-Time%20Scaling%20of%20Multi-Step%20Reasoning%20by%20Probing%20Internal%20States%20of%20Large%20Language%20Models&body=Title%3A%20Efficient%20Test-Time%20Scaling%20of%20Multi-Step%20Reasoning%20by%20Probing%20Internal%20States%20of%20Large%20Language%20Models%0AAuthor%3A%20Jingwei%20Ni%20and%20Ekaterina%20Fadeeva%20and%20Tianyi%20Wu%20and%20Mubashara%20Akhtar%20and%20Jiaheng%20Zhang%20and%20Elliott%20Ash%20and%20Markus%20Leippold%20and%20Timothy%20Baldwin%20and%20See-Kiong%20Ng%20and%20Artem%20Shelmanov%20and%20Mrinmaya%20Sachan%0AAbstract%3A%20LLMs%20can%20solve%20complex%20tasks%20by%20generating%20long%2C%20multi-step%20reasoning%20chains.%20Test-time%20scaling%20%28TTS%29%20can%20further%20improve%20LLM%20performance%20by%20sampling%20multiple%20variants%20of%20intermediate%20reasoning%20steps%2C%20verifying%20their%20correctness%2C%20and%20strategically%20choosing%20the%20best%20steps%20for%20continuation.%20However%2C%20existing%20verification%20approaches%2C%20such%20as%20Process%20Reward%20Models%20%28PRMs%29%2C%20are%20computationally%20expensive%2C%20limited%20to%20specific%20domains%2C%20and%20require%20large-scale%20human%20or%20model-generated%20annotations.%20We%20propose%20a%20lightweight%20alternative%20for%20step-level%20reasoning%20verification%20based%20on%20probing%20the%20internal%20states%20of%20LLMs.%20We%20train%20a%20transformer-based%20probe%20that%20uses%20the%20internal%20states%20of%20the%20frozen%20LLM%20to%20estimate%20the%20credibility%20of%20its%20reasoning%20steps%20during%20generation.%20Annotation%20can%20be%20generated%20either%20by%20another%20larger%20LLM%20%28e.g.%2C%20DeepSeek-R1%29%20or%20in%20a%20self-supervised%20manner%20by%20the%20original%20model%20itself.%20The%20probes%20are%20both%20effective%20and%20lightweight%2C%20containing%20fewer%20than%2010M%20parameters.%20Across%20multiple%20domains%2C%20including%20mathematics%2C%20planning%2C%20and%20general%20knowledge%20question%20answering%2C%20our%20probes%20match%20or%20even%20exceed%20the%20performance%20of%20PRMs%20that%20are%20up%20to%20810x%20larger.%20Our%20findings%20suggest%20that%20the%20internal%20states%20of%20LLMs%20encode%20their%20confidence%20in%20reasoning%20processes%20and%20can%20serve%20as%20reliable%20signals%20for%20reasoning%20step%20verification%2C%20offering%20a%20promising%20direction%20towards%20scalable%20and%20generalizable%20TTS%20and%20introspective%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2511.06209v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Test-Time%2520Scaling%2520of%2520Multi-Step%2520Reasoning%2520by%2520Probing%2520Internal%2520States%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DJingwei%2520Ni%2520and%2520Ekaterina%2520Fadeeva%2520and%2520Tianyi%2520Wu%2520and%2520Mubashara%2520Akhtar%2520and%2520Jiaheng%2520Zhang%2520and%2520Elliott%2520Ash%2520and%2520Markus%2520Leippold%2520and%2520Timothy%2520Baldwin%2520and%2520See-Kiong%2520Ng%2520and%2520Artem%2520Shelmanov%2520and%2520Mrinmaya%2520Sachan%26entry.1292438233%3DLLMs%2520can%2520solve%2520complex%2520tasks%2520by%2520generating%2520long%252C%2520multi-step%2520reasoning%2520chains.%2520Test-time%2520scaling%2520%2528TTS%2529%2520can%2520further%2520improve%2520LLM%2520performance%2520by%2520sampling%2520multiple%2520variants%2520of%2520intermediate%2520reasoning%2520steps%252C%2520verifying%2520their%2520correctness%252C%2520and%2520strategically%2520choosing%2520the%2520best%2520steps%2520for%2520continuation.%2520However%252C%2520existing%2520verification%2520approaches%252C%2520such%2520as%2520Process%2520Reward%2520Models%2520%2528PRMs%2529%252C%2520are%2520computationally%2520expensive%252C%2520limited%2520to%2520specific%2520domains%252C%2520and%2520require%2520large-scale%2520human%2520or%2520model-generated%2520annotations.%2520We%2520propose%2520a%2520lightweight%2520alternative%2520for%2520step-level%2520reasoning%2520verification%2520based%2520on%2520probing%2520the%2520internal%2520states%2520of%2520LLMs.%2520We%2520train%2520a%2520transformer-based%2520probe%2520that%2520uses%2520the%2520internal%2520states%2520of%2520the%2520frozen%2520LLM%2520to%2520estimate%2520the%2520credibility%2520of%2520its%2520reasoning%2520steps%2520during%2520generation.%2520Annotation%2520can%2520be%2520generated%2520either%2520by%2520another%2520larger%2520LLM%2520%2528e.g.%252C%2520DeepSeek-R1%2529%2520or%2520in%2520a%2520self-supervised%2520manner%2520by%2520the%2520original%2520model%2520itself.%2520The%2520probes%2520are%2520both%2520effective%2520and%2520lightweight%252C%2520containing%2520fewer%2520than%252010M%2520parameters.%2520Across%2520multiple%2520domains%252C%2520including%2520mathematics%252C%2520planning%252C%2520and%2520general%2520knowledge%2520question%2520answering%252C%2520our%2520probes%2520match%2520or%2520even%2520exceed%2520the%2520performance%2520of%2520PRMs%2520that%2520are%2520up%2520to%2520810x%2520larger.%2520Our%2520findings%2520suggest%2520that%2520the%2520internal%2520states%2520of%2520LLMs%2520encode%2520their%2520confidence%2520in%2520reasoning%2520processes%2520and%2520can%2520serve%2520as%2520reliable%2520signals%2520for%2520reasoning%2520step%2520verification%252C%2520offering%2520a%2520promising%2520direction%2520towards%2520scalable%2520and%2520generalizable%2520TTS%2520and%2520introspective%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.06209v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Test-Time%20Scaling%20of%20Multi-Step%20Reasoning%20by%20Probing%20Internal%20States%20of%20Large%20Language%20Models&entry.906535625=Jingwei%20Ni%20and%20Ekaterina%20Fadeeva%20and%20Tianyi%20Wu%20and%20Mubashara%20Akhtar%20and%20Jiaheng%20Zhang%20and%20Elliott%20Ash%20and%20Markus%20Leippold%20and%20Timothy%20Baldwin%20and%20See-Kiong%20Ng%20and%20Artem%20Shelmanov%20and%20Mrinmaya%20Sachan&entry.1292438233=LLMs%20can%20solve%20complex%20tasks%20by%20generating%20long%2C%20multi-step%20reasoning%20chains.%20Test-time%20scaling%20%28TTS%29%20can%20further%20improve%20LLM%20performance%20by%20sampling%20multiple%20variants%20of%20intermediate%20reasoning%20steps%2C%20verifying%20their%20correctness%2C%20and%20strategically%20choosing%20the%20best%20steps%20for%20continuation.%20However%2C%20existing%20verification%20approaches%2C%20such%20as%20Process%20Reward%20Models%20%28PRMs%29%2C%20are%20computationally%20expensive%2C%20limited%20to%20specific%20domains%2C%20and%20require%20large-scale%20human%20or%20model-generated%20annotations.%20We%20propose%20a%20lightweight%20alternative%20for%20step-level%20reasoning%20verification%20based%20on%20probing%20the%20internal%20states%20of%20LLMs.%20We%20train%20a%20transformer-based%20probe%20that%20uses%20the%20internal%20states%20of%20the%20frozen%20LLM%20to%20estimate%20the%20credibility%20of%20its%20reasoning%20steps%20during%20generation.%20Annotation%20can%20be%20generated%20either%20by%20another%20larger%20LLM%20%28e.g.%2C%20DeepSeek-R1%29%20or%20in%20a%20self-supervised%20manner%20by%20the%20original%20model%20itself.%20The%20probes%20are%20both%20effective%20and%20lightweight%2C%20containing%20fewer%20than%2010M%20parameters.%20Across%20multiple%20domains%2C%20including%20mathematics%2C%20planning%2C%20and%20general%20knowledge%20question%20answering%2C%20our%20probes%20match%20or%20even%20exceed%20the%20performance%20of%20PRMs%20that%20are%20up%20to%20810x%20larger.%20Our%20findings%20suggest%20that%20the%20internal%20states%20of%20LLMs%20encode%20their%20confidence%20in%20reasoning%20processes%20and%20can%20serve%20as%20reliable%20signals%20for%20reasoning%20step%20verification%2C%20offering%20a%20promising%20direction%20towards%20scalable%20and%20generalizable%20TTS%20and%20introspective%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2511.06209v3&entry.124074799=Read"},
{"title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders", "author": "James Oldfield and Shawn Im and Sharon Li and Mihalis A. Nicolaou and Ioannis Patras and Grigorios G Chrysos", "abstract": "Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.", "link": "http://arxiv.org/abs/2505.21364v3", "date": "2026-01-14", "relevancy": 2.0425, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5118}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretability%20Without%20Sacrifice%3A%20Faithful%20Dense%20Layer%20Decomposition%20with%20Mixture%20of%20Decoders&body=Title%3A%20Towards%20Interpretability%20Without%20Sacrifice%3A%20Faithful%20Dense%20Layer%20Decomposition%20with%20Mixture%20of%20Decoders%0AAuthor%3A%20James%20Oldfield%20and%20Shawn%20Im%20and%20Sharon%20Li%20and%20Mihalis%20A.%20Nicolaou%20and%20Ioannis%20Patras%20and%20Grigorios%20G%20Chrysos%0AAbstract%3A%20Multilayer%20perceptrons%20%28MLPs%29%20are%20an%20integral%20part%20of%20large%20language%20models%2C%20yet%20their%20dense%20representations%20render%20them%20difficult%20to%20understand%2C%20edit%2C%20and%20steer.%20Recent%20methods%20learn%20interpretable%20approximations%20via%20neuron-level%20sparsity%2C%20yet%20fail%20to%20faithfully%20reconstruct%20the%20original%20mapping--significantly%20increasing%20model%27s%20next-token%20cross-entropy%20loss.%20In%20this%20paper%2C%20we%20advocate%20for%20moving%20to%20layer-level%20sparsity%20to%20overcome%20the%20accuracy%20trade-off%20in%20sparse%20layer%20approximation.%20Under%20this%20paradigm%2C%20we%20introduce%20Mixture%20of%20Decoders%20%28MxDs%29.%20MxDs%20generalize%20MLPs%20and%20Gated%20Linear%20Units%2C%20expanding%20pre-trained%20dense%20layers%20into%20tens%20of%20thousands%20of%20specialized%20sublayers.%20Through%20a%20flexible%20form%20of%20tensor%20factorization%2C%20each%20sparsely%20activating%20MxD%20sublayer%20implements%20a%20linear%20transformation%20with%20full-rank%20weights--preserving%20the%20original%20decoders%27%20expressive%20capacity%20even%20under%20heavy%20sparsity.%20Experimentally%2C%20we%20show%20that%20MxDs%20significantly%20outperform%20state-of-the-art%20methods%20%28e.g.%2C%20Transcoders%29%20on%20the%20sparsity-accuracy%20frontier%20in%20language%20models%20with%20up%20to%203B%20parameters.%20Further%20evaluations%20on%20sparse%20probing%20and%20feature%20steering%20demonstrate%20that%20MxDs%20learn%20similarly%20specialized%20features%20of%20natural%20language--opening%20up%20a%20promising%20new%20avenue%20for%20designing%20interpretable%20yet%20faithful%20decompositions.%20Our%20code%20is%20included%20at%3A%20https%3A//github.com/james-oldfield/MxD/.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21364v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretability%2520Without%2520Sacrifice%253A%2520Faithful%2520Dense%2520Layer%2520Decomposition%2520with%2520Mixture%2520of%2520Decoders%26entry.906535625%3DJames%2520Oldfield%2520and%2520Shawn%2520Im%2520and%2520Sharon%2520Li%2520and%2520Mihalis%2520A.%2520Nicolaou%2520and%2520Ioannis%2520Patras%2520and%2520Grigorios%2520G%2520Chrysos%26entry.1292438233%3DMultilayer%2520perceptrons%2520%2528MLPs%2529%2520are%2520an%2520integral%2520part%2520of%2520large%2520language%2520models%252C%2520yet%2520their%2520dense%2520representations%2520render%2520them%2520difficult%2520to%2520understand%252C%2520edit%252C%2520and%2520steer.%2520Recent%2520methods%2520learn%2520interpretable%2520approximations%2520via%2520neuron-level%2520sparsity%252C%2520yet%2520fail%2520to%2520faithfully%2520reconstruct%2520the%2520original%2520mapping--significantly%2520increasing%2520model%2527s%2520next-token%2520cross-entropy%2520loss.%2520In%2520this%2520paper%252C%2520we%2520advocate%2520for%2520moving%2520to%2520layer-level%2520sparsity%2520to%2520overcome%2520the%2520accuracy%2520trade-off%2520in%2520sparse%2520layer%2520approximation.%2520Under%2520this%2520paradigm%252C%2520we%2520introduce%2520Mixture%2520of%2520Decoders%2520%2528MxDs%2529.%2520MxDs%2520generalize%2520MLPs%2520and%2520Gated%2520Linear%2520Units%252C%2520expanding%2520pre-trained%2520dense%2520layers%2520into%2520tens%2520of%2520thousands%2520of%2520specialized%2520sublayers.%2520Through%2520a%2520flexible%2520form%2520of%2520tensor%2520factorization%252C%2520each%2520sparsely%2520activating%2520MxD%2520sublayer%2520implements%2520a%2520linear%2520transformation%2520with%2520full-rank%2520weights--preserving%2520the%2520original%2520decoders%2527%2520expressive%2520capacity%2520even%2520under%2520heavy%2520sparsity.%2520Experimentally%252C%2520we%2520show%2520that%2520MxDs%2520significantly%2520outperform%2520state-of-the-art%2520methods%2520%2528e.g.%252C%2520Transcoders%2529%2520on%2520the%2520sparsity-accuracy%2520frontier%2520in%2520language%2520models%2520with%2520up%2520to%25203B%2520parameters.%2520Further%2520evaluations%2520on%2520sparse%2520probing%2520and%2520feature%2520steering%2520demonstrate%2520that%2520MxDs%2520learn%2520similarly%2520specialized%2520features%2520of%2520natural%2520language--opening%2520up%2520a%2520promising%2520new%2520avenue%2520for%2520designing%2520interpretable%2520yet%2520faithful%2520decompositions.%2520Our%2520code%2520is%2520included%2520at%253A%2520https%253A//github.com/james-oldfield/MxD/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21364v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretability%20Without%20Sacrifice%3A%20Faithful%20Dense%20Layer%20Decomposition%20with%20Mixture%20of%20Decoders&entry.906535625=James%20Oldfield%20and%20Shawn%20Im%20and%20Sharon%20Li%20and%20Mihalis%20A.%20Nicolaou%20and%20Ioannis%20Patras%20and%20Grigorios%20G%20Chrysos&entry.1292438233=Multilayer%20perceptrons%20%28MLPs%29%20are%20an%20integral%20part%20of%20large%20language%20models%2C%20yet%20their%20dense%20representations%20render%20them%20difficult%20to%20understand%2C%20edit%2C%20and%20steer.%20Recent%20methods%20learn%20interpretable%20approximations%20via%20neuron-level%20sparsity%2C%20yet%20fail%20to%20faithfully%20reconstruct%20the%20original%20mapping--significantly%20increasing%20model%27s%20next-token%20cross-entropy%20loss.%20In%20this%20paper%2C%20we%20advocate%20for%20moving%20to%20layer-level%20sparsity%20to%20overcome%20the%20accuracy%20trade-off%20in%20sparse%20layer%20approximation.%20Under%20this%20paradigm%2C%20we%20introduce%20Mixture%20of%20Decoders%20%28MxDs%29.%20MxDs%20generalize%20MLPs%20and%20Gated%20Linear%20Units%2C%20expanding%20pre-trained%20dense%20layers%20into%20tens%20of%20thousands%20of%20specialized%20sublayers.%20Through%20a%20flexible%20form%20of%20tensor%20factorization%2C%20each%20sparsely%20activating%20MxD%20sublayer%20implements%20a%20linear%20transformation%20with%20full-rank%20weights--preserving%20the%20original%20decoders%27%20expressive%20capacity%20even%20under%20heavy%20sparsity.%20Experimentally%2C%20we%20show%20that%20MxDs%20significantly%20outperform%20state-of-the-art%20methods%20%28e.g.%2C%20Transcoders%29%20on%20the%20sparsity-accuracy%20frontier%20in%20language%20models%20with%20up%20to%203B%20parameters.%20Further%20evaluations%20on%20sparse%20probing%20and%20feature%20steering%20demonstrate%20that%20MxDs%20learn%20similarly%20specialized%20features%20of%20natural%20language--opening%20up%20a%20promising%20new%20avenue%20for%20designing%20interpretable%20yet%20faithful%20decompositions.%20Our%20code%20is%20included%20at%3A%20https%3A//github.com/james-oldfield/MxD/.&entry.1838667208=http%3A//arxiv.org/abs/2505.21364v3&entry.124074799=Read"},
{"title": "Query Languages for Machine-Learning Models", "author": "Martin Grohe", "abstract": "In this paper, I discuss two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics originate from foundational work by Gr\u00e4del, Gurevich, and Meer in the 1990s. In recent joint work with Standke, Steegmans, and Van den Bussche, we have investigated these logics as query languages for machine learning models, specifically neural networks, which are naturally represented as weighted graphs. I present illustrative examples of queries to neural networks that can be expressed in these logics and discuss fundamental results on their expressiveness and computational complexity.", "link": "http://arxiv.org/abs/2601.09381v1", "date": "2026-01-14", "relevancy": 2.0403, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Query%20Languages%20for%20Machine-Learning%20Models&body=Title%3A%20Query%20Languages%20for%20Machine-Learning%20Models%0AAuthor%3A%20Martin%20Grohe%0AAbstract%3A%20In%20this%20paper%2C%20I%20discuss%20two%20logics%20for%20weighted%20finite%20structures%3A%20first-order%20logic%20with%20summation%20%28FO%28SUM%29%29%20and%20its%20recursive%20extension%20IFP%28SUM%29.%20These%20logics%20originate%20from%20foundational%20work%20by%20Gr%C3%A4del%2C%20Gurevich%2C%20and%20Meer%20in%20the%201990s.%20In%20recent%20joint%20work%20with%20Standke%2C%20Steegmans%2C%20and%20Van%20den%20Bussche%2C%20we%20have%20investigated%20these%20logics%20as%20query%20languages%20for%20machine%20learning%20models%2C%20specifically%20neural%20networks%2C%20which%20are%20naturally%20represented%20as%20weighted%20graphs.%20I%20present%20illustrative%20examples%20of%20queries%20to%20neural%20networks%20that%20can%20be%20expressed%20in%20these%20logics%20and%20discuss%20fundamental%20results%20on%20their%20expressiveness%20and%20computational%20complexity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuery%2520Languages%2520for%2520Machine-Learning%2520Models%26entry.906535625%3DMartin%2520Grohe%26entry.1292438233%3DIn%2520this%2520paper%252C%2520I%2520discuss%2520two%2520logics%2520for%2520weighted%2520finite%2520structures%253A%2520first-order%2520logic%2520with%2520summation%2520%2528FO%2528SUM%2529%2529%2520and%2520its%2520recursive%2520extension%2520IFP%2528SUM%2529.%2520These%2520logics%2520originate%2520from%2520foundational%2520work%2520by%2520Gr%25C3%25A4del%252C%2520Gurevich%252C%2520and%2520Meer%2520in%2520the%25201990s.%2520In%2520recent%2520joint%2520work%2520with%2520Standke%252C%2520Steegmans%252C%2520and%2520Van%2520den%2520Bussche%252C%2520we%2520have%2520investigated%2520these%2520logics%2520as%2520query%2520languages%2520for%2520machine%2520learning%2520models%252C%2520specifically%2520neural%2520networks%252C%2520which%2520are%2520naturally%2520represented%2520as%2520weighted%2520graphs.%2520I%2520present%2520illustrative%2520examples%2520of%2520queries%2520to%2520neural%2520networks%2520that%2520can%2520be%2520expressed%2520in%2520these%2520logics%2520and%2520discuss%2520fundamental%2520results%2520on%2520their%2520expressiveness%2520and%2520computational%2520complexity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Query%20Languages%20for%20Machine-Learning%20Models&entry.906535625=Martin%20Grohe&entry.1292438233=In%20this%20paper%2C%20I%20discuss%20two%20logics%20for%20weighted%20finite%20structures%3A%20first-order%20logic%20with%20summation%20%28FO%28SUM%29%29%20and%20its%20recursive%20extension%20IFP%28SUM%29.%20These%20logics%20originate%20from%20foundational%20work%20by%20Gr%C3%A4del%2C%20Gurevich%2C%20and%20Meer%20in%20the%201990s.%20In%20recent%20joint%20work%20with%20Standke%2C%20Steegmans%2C%20and%20Van%20den%20Bussche%2C%20we%20have%20investigated%20these%20logics%20as%20query%20languages%20for%20machine%20learning%20models%2C%20specifically%20neural%20networks%2C%20which%20are%20naturally%20represented%20as%20weighted%20graphs.%20I%20present%20illustrative%20examples%20of%20queries%20to%20neural%20networks%20that%20can%20be%20expressed%20in%20these%20logics%20and%20discuss%20fundamental%20results%20on%20their%20expressiveness%20and%20computational%20complexity.&entry.1838667208=http%3A//arxiv.org/abs/2601.09381v1&entry.124074799=Read"},
{"title": "DEAR: Dataset for Evaluating the Aesthetics of Rendering", "author": "Vsevolod Plohotnuk and Artyom Panshin and Nikola Bani\u0107 and Simone Bianco and Michael Freeman and Egor Ershov", "abstract": "Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).", "link": "http://arxiv.org/abs/2512.05209v4", "date": "2026-01-14", "relevancy": 2.0315, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5248}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5194}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEAR%3A%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering&body=Title%3A%20DEAR%3A%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering%0AAuthor%3A%20Vsevolod%20Plohotnuk%20and%20Artyom%20Panshin%20and%20Nikola%20Bani%C4%87%20and%20Simone%20Bianco%20and%20Michael%20Freeman%20and%20Egor%20Ershov%0AAbstract%3A%20Traditional%20Image%20Quality%20Assessment~%28IQA%29%20focuses%20on%20quantifying%20technical%20degradations%20such%20as%20noise%2C%20blur%2C%20or%20compression%20artifacts%2C%20using%20both%20full-reference%20and%20no-reference%20objective%20metrics.%20However%2C%20evaluation%20of%20rendering%20aesthetics%2C%20a%20growing%20domain%20relevant%20to%20photographic%20editing%2C%20content%20creation%2C%20and%20AI-generated%20imagery%2C%20remains%20underexplored%20due%20to%20the%20lack%20of%20datasets%20that%20reflect%20the%20inherently%20subjective%20nature%20of%20style%20preference.%20In%20this%20work%2C%20a%20novel%20benchmark%20dataset%20designed%20to%20model%20human%20aesthetic%20judgments%20of%20image%20rendering%20styles%20is%20introduced%3A%20the%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering%20%28DEAR%29.%20Built%20upon%20the%20MIT-Adobe%20FiveK%20dataset%2C%20DEAR%20incorporates%20pairwise%20human%20preference%20scores%20collected%20via%20large-scale%20crowdsourcing%2C%20with%20each%20image%20pair%20evaluated%20by%2025%20distinct%20human%20evaluators%20with%20a%20total%20of%2013%2C648%20of%20them%20participating%20overall.%20These%20annotations%20capture%20nuanced%2C%20context-sensitive%20aesthetic%20preferences%2C%20enabling%20the%20development%20and%20evaluation%20of%20models%20that%20go%20beyond%20traditional%20distortion-based%20IQA%2C%20focusing%20on%20a%20new%20task%3A%20Evaluation%20of%20Aesthetics%20of%20Rendering%20%28EAR%29.%20The%20data%20collection%20pipeline%20is%20described%2C%20human%20voting%20patterns%20are%20analyzed%2C%20and%20multiple%20use%20cases%20are%20outlined%2C%20including%20style%20preference%20prediction%2C%20aesthetic%20benchmarking%2C%20and%20personalized%20aesthetic%20modeling.%20To%20the%20best%20of%20the%20authors%27%20knowledge%2C%20DEAR%20is%20the%20first%20dataset%20to%20systematically%20address%20image%20aesthetics%20of%20rendering%20assessment%20grounded%20in%20subjective%20human%20preferences.%20A%20subset%20of%20100%20images%20with%20markup%20for%20them%20is%20published%20on%20HuggingFace%20%28huggingface.co/datasets/vsevolodpl/DEAR%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05209v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEAR%253A%2520Dataset%2520for%2520Evaluating%2520the%2520Aesthetics%2520of%2520Rendering%26entry.906535625%3DVsevolod%2520Plohotnuk%2520and%2520Artyom%2520Panshin%2520and%2520Nikola%2520Bani%25C4%2587%2520and%2520Simone%2520Bianco%2520and%2520Michael%2520Freeman%2520and%2520Egor%2520Ershov%26entry.1292438233%3DTraditional%2520Image%2520Quality%2520Assessment~%2528IQA%2529%2520focuses%2520on%2520quantifying%2520technical%2520degradations%2520such%2520as%2520noise%252C%2520blur%252C%2520or%2520compression%2520artifacts%252C%2520using%2520both%2520full-reference%2520and%2520no-reference%2520objective%2520metrics.%2520However%252C%2520evaluation%2520of%2520rendering%2520aesthetics%252C%2520a%2520growing%2520domain%2520relevant%2520to%2520photographic%2520editing%252C%2520content%2520creation%252C%2520and%2520AI-generated%2520imagery%252C%2520remains%2520underexplored%2520due%2520to%2520the%2520lack%2520of%2520datasets%2520that%2520reflect%2520the%2520inherently%2520subjective%2520nature%2520of%2520style%2520preference.%2520In%2520this%2520work%252C%2520a%2520novel%2520benchmark%2520dataset%2520designed%2520to%2520model%2520human%2520aesthetic%2520judgments%2520of%2520image%2520rendering%2520styles%2520is%2520introduced%253A%2520the%2520Dataset%2520for%2520Evaluating%2520the%2520Aesthetics%2520of%2520Rendering%2520%2528DEAR%2529.%2520Built%2520upon%2520the%2520MIT-Adobe%2520FiveK%2520dataset%252C%2520DEAR%2520incorporates%2520pairwise%2520human%2520preference%2520scores%2520collected%2520via%2520large-scale%2520crowdsourcing%252C%2520with%2520each%2520image%2520pair%2520evaluated%2520by%252025%2520distinct%2520human%2520evaluators%2520with%2520a%2520total%2520of%252013%252C648%2520of%2520them%2520participating%2520overall.%2520These%2520annotations%2520capture%2520nuanced%252C%2520context-sensitive%2520aesthetic%2520preferences%252C%2520enabling%2520the%2520development%2520and%2520evaluation%2520of%2520models%2520that%2520go%2520beyond%2520traditional%2520distortion-based%2520IQA%252C%2520focusing%2520on%2520a%2520new%2520task%253A%2520Evaluation%2520of%2520Aesthetics%2520of%2520Rendering%2520%2528EAR%2529.%2520The%2520data%2520collection%2520pipeline%2520is%2520described%252C%2520human%2520voting%2520patterns%2520are%2520analyzed%252C%2520and%2520multiple%2520use%2520cases%2520are%2520outlined%252C%2520including%2520style%2520preference%2520prediction%252C%2520aesthetic%2520benchmarking%252C%2520and%2520personalized%2520aesthetic%2520modeling.%2520To%2520the%2520best%2520of%2520the%2520authors%2527%2520knowledge%252C%2520DEAR%2520is%2520the%2520first%2520dataset%2520to%2520systematically%2520address%2520image%2520aesthetics%2520of%2520rendering%2520assessment%2520grounded%2520in%2520subjective%2520human%2520preferences.%2520A%2520subset%2520of%2520100%2520images%2520with%2520markup%2520for%2520them%2520is%2520published%2520on%2520HuggingFace%2520%2528huggingface.co/datasets/vsevolodpl/DEAR%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05209v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEAR%3A%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering&entry.906535625=Vsevolod%20Plohotnuk%20and%20Artyom%20Panshin%20and%20Nikola%20Bani%C4%87%20and%20Simone%20Bianco%20and%20Michael%20Freeman%20and%20Egor%20Ershov&entry.1292438233=Traditional%20Image%20Quality%20Assessment~%28IQA%29%20focuses%20on%20quantifying%20technical%20degradations%20such%20as%20noise%2C%20blur%2C%20or%20compression%20artifacts%2C%20using%20both%20full-reference%20and%20no-reference%20objective%20metrics.%20However%2C%20evaluation%20of%20rendering%20aesthetics%2C%20a%20growing%20domain%20relevant%20to%20photographic%20editing%2C%20content%20creation%2C%20and%20AI-generated%20imagery%2C%20remains%20underexplored%20due%20to%20the%20lack%20of%20datasets%20that%20reflect%20the%20inherently%20subjective%20nature%20of%20style%20preference.%20In%20this%20work%2C%20a%20novel%20benchmark%20dataset%20designed%20to%20model%20human%20aesthetic%20judgments%20of%20image%20rendering%20styles%20is%20introduced%3A%20the%20Dataset%20for%20Evaluating%20the%20Aesthetics%20of%20Rendering%20%28DEAR%29.%20Built%20upon%20the%20MIT-Adobe%20FiveK%20dataset%2C%20DEAR%20incorporates%20pairwise%20human%20preference%20scores%20collected%20via%20large-scale%20crowdsourcing%2C%20with%20each%20image%20pair%20evaluated%20by%2025%20distinct%20human%20evaluators%20with%20a%20total%20of%2013%2C648%20of%20them%20participating%20overall.%20These%20annotations%20capture%20nuanced%2C%20context-sensitive%20aesthetic%20preferences%2C%20enabling%20the%20development%20and%20evaluation%20of%20models%20that%20go%20beyond%20traditional%20distortion-based%20IQA%2C%20focusing%20on%20a%20new%20task%3A%20Evaluation%20of%20Aesthetics%20of%20Rendering%20%28EAR%29.%20The%20data%20collection%20pipeline%20is%20described%2C%20human%20voting%20patterns%20are%20analyzed%2C%20and%20multiple%20use%20cases%20are%20outlined%2C%20including%20style%20preference%20prediction%2C%20aesthetic%20benchmarking%2C%20and%20personalized%20aesthetic%20modeling.%20To%20the%20best%20of%20the%20authors%27%20knowledge%2C%20DEAR%20is%20the%20first%20dataset%20to%20systematically%20address%20image%20aesthetics%20of%20rendering%20assessment%20grounded%20in%20subjective%20human%20preferences.%20A%20subset%20of%20100%20images%20with%20markup%20for%20them%20is%20published%20on%20HuggingFace%20%28huggingface.co/datasets/vsevolodpl/DEAR%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.05209v4&entry.124074799=Read"},
{"title": "GroupNL: Low-Resource and Robust CNN Design over Cloud and Device", "author": "Chuntao Ding and Jianhang Xie and Junna Zhang and Salman Raza and Shangguang Wang and Jiannong Cao", "abstract": "Deploying Convolutional Neural Network (CNN) models on ubiquitous Internet of Things (IoT) devices in a cloud-assisted manner to provide users with a variety of high-quality services has become mainstream. Most existing studies speed up model cloud training/on-device inference by reducing the number of convolution (Conv) parameters and floating-point operations (FLOPs). However, they usually employ two or more lightweight operations (e.g., depthwise Conv, $1\\times1$ cheap Conv) to replace a Conv, which can still affect the model's speedup even with fewer parameters and FLOPs. To this end, we propose the Grouped NonLinear transformation generation method (GroupNL), leveraging data-agnostic, hyperparameters-fixed, and lightweight Nonlinear Transformation Functions (NLFs) to generate diversified feature maps on demand via grouping, thereby reducing resource consumption while improving the robustness of CNNs. First, in a GroupNL Conv layer, a small set of feature maps, i.e., seed feature maps, are generated based on the seed Conv operation. Then, we split seed feature maps into several groups, each with a set of different NLFs, to generate the required number of diversified feature maps with tensor manipulation operators and nonlinear processing in a lightweight manner without additional Conv operations. We further introduce a sparse GroupNL Conv to speed up by reasonably designing the seed Conv groups between the number of input channels and seed feature maps. Experiments conducted on benchmarks and on-device resource measurements demonstrate that the GroupNL Conv is an impressive alternative to Conv layers in baseline models. Specifically, on Icons-50 dataset, the accuracy of GroupNL-ResNet-18 is 2.86% higher than ResNet-18; on ImageNet-C dataset, the accuracy of GroupNL-EfficientNet-ES achieves about 1.1% higher than EfficientNet-ES.", "link": "http://arxiv.org/abs/2506.12335v2", "date": "2026-01-14", "relevancy": 2.0314, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.524}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5215}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroupNL%3A%20Low-Resource%20and%20Robust%20CNN%20Design%20over%20Cloud%20and%20Device&body=Title%3A%20GroupNL%3A%20Low-Resource%20and%20Robust%20CNN%20Design%20over%20Cloud%20and%20Device%0AAuthor%3A%20Chuntao%20Ding%20and%20Jianhang%20Xie%20and%20Junna%20Zhang%20and%20Salman%20Raza%20and%20Shangguang%20Wang%20and%20Jiannong%20Cao%0AAbstract%3A%20Deploying%20Convolutional%20Neural%20Network%20%28CNN%29%20models%20on%20ubiquitous%20Internet%20of%20Things%20%28IoT%29%20devices%20in%20a%20cloud-assisted%20manner%20to%20provide%20users%20with%20a%20variety%20of%20high-quality%20services%20has%20become%20mainstream.%20Most%20existing%20studies%20speed%20up%20model%20cloud%20training/on-device%20inference%20by%20reducing%20the%20number%20of%20convolution%20%28Conv%29%20parameters%20and%20floating-point%20operations%20%28FLOPs%29.%20However%2C%20they%20usually%20employ%20two%20or%20more%20lightweight%20operations%20%28e.g.%2C%20depthwise%20Conv%2C%20%241%5Ctimes1%24%20cheap%20Conv%29%20to%20replace%20a%20Conv%2C%20which%20can%20still%20affect%20the%20model%27s%20speedup%20even%20with%20fewer%20parameters%20and%20FLOPs.%20To%20this%20end%2C%20we%20propose%20the%20Grouped%20NonLinear%20transformation%20generation%20method%20%28GroupNL%29%2C%20leveraging%20data-agnostic%2C%20hyperparameters-fixed%2C%20and%20lightweight%20Nonlinear%20Transformation%20Functions%20%28NLFs%29%20to%20generate%20diversified%20feature%20maps%20on%20demand%20via%20grouping%2C%20thereby%20reducing%20resource%20consumption%20while%20improving%20the%20robustness%20of%20CNNs.%20First%2C%20in%20a%20GroupNL%20Conv%20layer%2C%20a%20small%20set%20of%20feature%20maps%2C%20i.e.%2C%20seed%20feature%20maps%2C%20are%20generated%20based%20on%20the%20seed%20Conv%20operation.%20Then%2C%20we%20split%20seed%20feature%20maps%20into%20several%20groups%2C%20each%20with%20a%20set%20of%20different%20NLFs%2C%20to%20generate%20the%20required%20number%20of%20diversified%20feature%20maps%20with%20tensor%20manipulation%20operators%20and%20nonlinear%20processing%20in%20a%20lightweight%20manner%20without%20additional%20Conv%20operations.%20We%20further%20introduce%20a%20sparse%20GroupNL%20Conv%20to%20speed%20up%20by%20reasonably%20designing%20the%20seed%20Conv%20groups%20between%20the%20number%20of%20input%20channels%20and%20seed%20feature%20maps.%20Experiments%20conducted%20on%20benchmarks%20and%20on-device%20resource%20measurements%20demonstrate%20that%20the%20GroupNL%20Conv%20is%20an%20impressive%20alternative%20to%20Conv%20layers%20in%20baseline%20models.%20Specifically%2C%20on%20Icons-50%20dataset%2C%20the%20accuracy%20of%20GroupNL-ResNet-18%20is%202.86%25%20higher%20than%20ResNet-18%3B%20on%20ImageNet-C%20dataset%2C%20the%20accuracy%20of%20GroupNL-EfficientNet-ES%20achieves%20about%201.1%25%20higher%20than%20EfficientNet-ES.%0ALink%3A%20http%3A//arxiv.org/abs/2506.12335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroupNL%253A%2520Low-Resource%2520and%2520Robust%2520CNN%2520Design%2520over%2520Cloud%2520and%2520Device%26entry.906535625%3DChuntao%2520Ding%2520and%2520Jianhang%2520Xie%2520and%2520Junna%2520Zhang%2520and%2520Salman%2520Raza%2520and%2520Shangguang%2520Wang%2520and%2520Jiannong%2520Cao%26entry.1292438233%3DDeploying%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520models%2520on%2520ubiquitous%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520devices%2520in%2520a%2520cloud-assisted%2520manner%2520to%2520provide%2520users%2520with%2520a%2520variety%2520of%2520high-quality%2520services%2520has%2520become%2520mainstream.%2520Most%2520existing%2520studies%2520speed%2520up%2520model%2520cloud%2520training/on-device%2520inference%2520by%2520reducing%2520the%2520number%2520of%2520convolution%2520%2528Conv%2529%2520parameters%2520and%2520floating-point%2520operations%2520%2528FLOPs%2529.%2520However%252C%2520they%2520usually%2520employ%2520two%2520or%2520more%2520lightweight%2520operations%2520%2528e.g.%252C%2520depthwise%2520Conv%252C%2520%25241%255Ctimes1%2524%2520cheap%2520Conv%2529%2520to%2520replace%2520a%2520Conv%252C%2520which%2520can%2520still%2520affect%2520the%2520model%2527s%2520speedup%2520even%2520with%2520fewer%2520parameters%2520and%2520FLOPs.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520Grouped%2520NonLinear%2520transformation%2520generation%2520method%2520%2528GroupNL%2529%252C%2520leveraging%2520data-agnostic%252C%2520hyperparameters-fixed%252C%2520and%2520lightweight%2520Nonlinear%2520Transformation%2520Functions%2520%2528NLFs%2529%2520to%2520generate%2520diversified%2520feature%2520maps%2520on%2520demand%2520via%2520grouping%252C%2520thereby%2520reducing%2520resource%2520consumption%2520while%2520improving%2520the%2520robustness%2520of%2520CNNs.%2520First%252C%2520in%2520a%2520GroupNL%2520Conv%2520layer%252C%2520a%2520small%2520set%2520of%2520feature%2520maps%252C%2520i.e.%252C%2520seed%2520feature%2520maps%252C%2520are%2520generated%2520based%2520on%2520the%2520seed%2520Conv%2520operation.%2520Then%252C%2520we%2520split%2520seed%2520feature%2520maps%2520into%2520several%2520groups%252C%2520each%2520with%2520a%2520set%2520of%2520different%2520NLFs%252C%2520to%2520generate%2520the%2520required%2520number%2520of%2520diversified%2520feature%2520maps%2520with%2520tensor%2520manipulation%2520operators%2520and%2520nonlinear%2520processing%2520in%2520a%2520lightweight%2520manner%2520without%2520additional%2520Conv%2520operations.%2520We%2520further%2520introduce%2520a%2520sparse%2520GroupNL%2520Conv%2520to%2520speed%2520up%2520by%2520reasonably%2520designing%2520the%2520seed%2520Conv%2520groups%2520between%2520the%2520number%2520of%2520input%2520channels%2520and%2520seed%2520feature%2520maps.%2520Experiments%2520conducted%2520on%2520benchmarks%2520and%2520on-device%2520resource%2520measurements%2520demonstrate%2520that%2520the%2520GroupNL%2520Conv%2520is%2520an%2520impressive%2520alternative%2520to%2520Conv%2520layers%2520in%2520baseline%2520models.%2520Specifically%252C%2520on%2520Icons-50%2520dataset%252C%2520the%2520accuracy%2520of%2520GroupNL-ResNet-18%2520is%25202.86%2525%2520higher%2520than%2520ResNet-18%253B%2520on%2520ImageNet-C%2520dataset%252C%2520the%2520accuracy%2520of%2520GroupNL-EfficientNet-ES%2520achieves%2520about%25201.1%2525%2520higher%2520than%2520EfficientNet-ES.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.12335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroupNL%3A%20Low-Resource%20and%20Robust%20CNN%20Design%20over%20Cloud%20and%20Device&entry.906535625=Chuntao%20Ding%20and%20Jianhang%20Xie%20and%20Junna%20Zhang%20and%20Salman%20Raza%20and%20Shangguang%20Wang%20and%20Jiannong%20Cao&entry.1292438233=Deploying%20Convolutional%20Neural%20Network%20%28CNN%29%20models%20on%20ubiquitous%20Internet%20of%20Things%20%28IoT%29%20devices%20in%20a%20cloud-assisted%20manner%20to%20provide%20users%20with%20a%20variety%20of%20high-quality%20services%20has%20become%20mainstream.%20Most%20existing%20studies%20speed%20up%20model%20cloud%20training/on-device%20inference%20by%20reducing%20the%20number%20of%20convolution%20%28Conv%29%20parameters%20and%20floating-point%20operations%20%28FLOPs%29.%20However%2C%20they%20usually%20employ%20two%20or%20more%20lightweight%20operations%20%28e.g.%2C%20depthwise%20Conv%2C%20%241%5Ctimes1%24%20cheap%20Conv%29%20to%20replace%20a%20Conv%2C%20which%20can%20still%20affect%20the%20model%27s%20speedup%20even%20with%20fewer%20parameters%20and%20FLOPs.%20To%20this%20end%2C%20we%20propose%20the%20Grouped%20NonLinear%20transformation%20generation%20method%20%28GroupNL%29%2C%20leveraging%20data-agnostic%2C%20hyperparameters-fixed%2C%20and%20lightweight%20Nonlinear%20Transformation%20Functions%20%28NLFs%29%20to%20generate%20diversified%20feature%20maps%20on%20demand%20via%20grouping%2C%20thereby%20reducing%20resource%20consumption%20while%20improving%20the%20robustness%20of%20CNNs.%20First%2C%20in%20a%20GroupNL%20Conv%20layer%2C%20a%20small%20set%20of%20feature%20maps%2C%20i.e.%2C%20seed%20feature%20maps%2C%20are%20generated%20based%20on%20the%20seed%20Conv%20operation.%20Then%2C%20we%20split%20seed%20feature%20maps%20into%20several%20groups%2C%20each%20with%20a%20set%20of%20different%20NLFs%2C%20to%20generate%20the%20required%20number%20of%20diversified%20feature%20maps%20with%20tensor%20manipulation%20operators%20and%20nonlinear%20processing%20in%20a%20lightweight%20manner%20without%20additional%20Conv%20operations.%20We%20further%20introduce%20a%20sparse%20GroupNL%20Conv%20to%20speed%20up%20by%20reasonably%20designing%20the%20seed%20Conv%20groups%20between%20the%20number%20of%20input%20channels%20and%20seed%20feature%20maps.%20Experiments%20conducted%20on%20benchmarks%20and%20on-device%20resource%20measurements%20demonstrate%20that%20the%20GroupNL%20Conv%20is%20an%20impressive%20alternative%20to%20Conv%20layers%20in%20baseline%20models.%20Specifically%2C%20on%20Icons-50%20dataset%2C%20the%20accuracy%20of%20GroupNL-ResNet-18%20is%202.86%25%20higher%20than%20ResNet-18%3B%20on%20ImageNet-C%20dataset%2C%20the%20accuracy%20of%20GroupNL-EfficientNet-ES%20achieves%20about%201.1%25%20higher%20than%20EfficientNet-ES.&entry.1838667208=http%3A//arxiv.org/abs/2506.12335v2&entry.124074799=Read"},
{"title": "AGE-US: automated gestational age estimation based on fetal ultrasound images", "author": "C\u00e9sar D\u00edaz-Parga and Marta Nu\u00f1ez-Garcia and Maria J. Carreira and Gabriel Bernardino and Nicol\u00e1s Vila-Blanco", "abstract": "Being born small carries significant health risks, including increased neonatal mortality and a higher likelihood of future cardiac diseases. Accurate estimation of gestational age is critical for monitoring fetal growth, but traditional methods, such as estimation based on the last menstrual period, are in some situations difficult to obtain. While ultrasound-based approaches offer greater reliability, they rely on manual measurements that introduce variability. This study presents an interpretable deep learning-based method for automated gestational age calculation, leveraging a novel segmentation architecture and distance maps to overcome dataset limitations and the scarcity of segmentation masks. Our approach achieves performance comparable to state-of-the-art models while reducing complexity, making it particularly suitable for resource-constrained settings and with limited annotated data. Furthermore, our results demonstrate that the use of distance maps is particularly suitable for estimating femur endpoints.", "link": "http://arxiv.org/abs/2506.16256v2", "date": "2026-01-14", "relevancy": 2.0313, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5129}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5067}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGE-US%3A%20automated%20gestational%20age%20estimation%20based%20on%20fetal%20ultrasound%20images&body=Title%3A%20AGE-US%3A%20automated%20gestational%20age%20estimation%20based%20on%20fetal%20ultrasound%20images%0AAuthor%3A%20C%C3%A9sar%20D%C3%ADaz-Parga%20and%20Marta%20Nu%C3%B1ez-Garcia%20and%20Maria%20J.%20Carreira%20and%20Gabriel%20Bernardino%20and%20Nicol%C3%A1s%20Vila-Blanco%0AAbstract%3A%20Being%20born%20small%20carries%20significant%20health%20risks%2C%20including%20increased%20neonatal%20mortality%20and%20a%20higher%20likelihood%20of%20future%20cardiac%20diseases.%20Accurate%20estimation%20of%20gestational%20age%20is%20critical%20for%20monitoring%20fetal%20growth%2C%20but%20traditional%20methods%2C%20such%20as%20estimation%20based%20on%20the%20last%20menstrual%20period%2C%20are%20in%20some%20situations%20difficult%20to%20obtain.%20While%20ultrasound-based%20approaches%20offer%20greater%20reliability%2C%20they%20rely%20on%20manual%20measurements%20that%20introduce%20variability.%20This%20study%20presents%20an%20interpretable%20deep%20learning-based%20method%20for%20automated%20gestational%20age%20calculation%2C%20leveraging%20a%20novel%20segmentation%20architecture%20and%20distance%20maps%20to%20overcome%20dataset%20limitations%20and%20the%20scarcity%20of%20segmentation%20masks.%20Our%20approach%20achieves%20performance%20comparable%20to%20state-of-the-art%20models%20while%20reducing%20complexity%2C%20making%20it%20particularly%20suitable%20for%20resource-constrained%20settings%20and%20with%20limited%20annotated%20data.%20Furthermore%2C%20our%20results%20demonstrate%20that%20the%20use%20of%20distance%20maps%20is%20particularly%20suitable%20for%20estimating%20femur%20endpoints.%0ALink%3A%20http%3A//arxiv.org/abs/2506.16256v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGE-US%253A%2520automated%2520gestational%2520age%2520estimation%2520based%2520on%2520fetal%2520ultrasound%2520images%26entry.906535625%3DC%25C3%25A9sar%2520D%25C3%25ADaz-Parga%2520and%2520Marta%2520Nu%25C3%25B1ez-Garcia%2520and%2520Maria%2520J.%2520Carreira%2520and%2520Gabriel%2520Bernardino%2520and%2520Nicol%25C3%25A1s%2520Vila-Blanco%26entry.1292438233%3DBeing%2520born%2520small%2520carries%2520significant%2520health%2520risks%252C%2520including%2520increased%2520neonatal%2520mortality%2520and%2520a%2520higher%2520likelihood%2520of%2520future%2520cardiac%2520diseases.%2520Accurate%2520estimation%2520of%2520gestational%2520age%2520is%2520critical%2520for%2520monitoring%2520fetal%2520growth%252C%2520but%2520traditional%2520methods%252C%2520such%2520as%2520estimation%2520based%2520on%2520the%2520last%2520menstrual%2520period%252C%2520are%2520in%2520some%2520situations%2520difficult%2520to%2520obtain.%2520While%2520ultrasound-based%2520approaches%2520offer%2520greater%2520reliability%252C%2520they%2520rely%2520on%2520manual%2520measurements%2520that%2520introduce%2520variability.%2520This%2520study%2520presents%2520an%2520interpretable%2520deep%2520learning-based%2520method%2520for%2520automated%2520gestational%2520age%2520calculation%252C%2520leveraging%2520a%2520novel%2520segmentation%2520architecture%2520and%2520distance%2520maps%2520to%2520overcome%2520dataset%2520limitations%2520and%2520the%2520scarcity%2520of%2520segmentation%2520masks.%2520Our%2520approach%2520achieves%2520performance%2520comparable%2520to%2520state-of-the-art%2520models%2520while%2520reducing%2520complexity%252C%2520making%2520it%2520particularly%2520suitable%2520for%2520resource-constrained%2520settings%2520and%2520with%2520limited%2520annotated%2520data.%2520Furthermore%252C%2520our%2520results%2520demonstrate%2520that%2520the%2520use%2520of%2520distance%2520maps%2520is%2520particularly%2520suitable%2520for%2520estimating%2520femur%2520endpoints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.16256v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGE-US%3A%20automated%20gestational%20age%20estimation%20based%20on%20fetal%20ultrasound%20images&entry.906535625=C%C3%A9sar%20D%C3%ADaz-Parga%20and%20Marta%20Nu%C3%B1ez-Garcia%20and%20Maria%20J.%20Carreira%20and%20Gabriel%20Bernardino%20and%20Nicol%C3%A1s%20Vila-Blanco&entry.1292438233=Being%20born%20small%20carries%20significant%20health%20risks%2C%20including%20increased%20neonatal%20mortality%20and%20a%20higher%20likelihood%20of%20future%20cardiac%20diseases.%20Accurate%20estimation%20of%20gestational%20age%20is%20critical%20for%20monitoring%20fetal%20growth%2C%20but%20traditional%20methods%2C%20such%20as%20estimation%20based%20on%20the%20last%20menstrual%20period%2C%20are%20in%20some%20situations%20difficult%20to%20obtain.%20While%20ultrasound-based%20approaches%20offer%20greater%20reliability%2C%20they%20rely%20on%20manual%20measurements%20that%20introduce%20variability.%20This%20study%20presents%20an%20interpretable%20deep%20learning-based%20method%20for%20automated%20gestational%20age%20calculation%2C%20leveraging%20a%20novel%20segmentation%20architecture%20and%20distance%20maps%20to%20overcome%20dataset%20limitations%20and%20the%20scarcity%20of%20segmentation%20masks.%20Our%20approach%20achieves%20performance%20comparable%20to%20state-of-the-art%20models%20while%20reducing%20complexity%2C%20making%20it%20particularly%20suitable%20for%20resource-constrained%20settings%20and%20with%20limited%20annotated%20data.%20Furthermore%2C%20our%20results%20demonstrate%20that%20the%20use%20of%20distance%20maps%20is%20particularly%20suitable%20for%20estimating%20femur%20endpoints.&entry.1838667208=http%3A//arxiv.org/abs/2506.16256v2&entry.124074799=Read"},
{"title": "Global Benchmark Database", "author": "Ashlin Iser and Christoph Jabs", "abstract": "This paper presents Global Benchmark Database (GBD), a comprehensive suite of tools for provisioning and sustainably maintaining benchmark instances and their metadata. The availability of benchmark metadata is essential for many tasks in empirical research, e.g., for the data-driven compilation of benchmarks, the domain-specific analysis of runtime experiments, or the instance-specific selection of solvers. In this paper, we introduce the data model of GBD as well as its interfaces and provide examples of how to interact with them. We also demonstrate the integration of custom data sources and explain how to extend GBD with additional problem domains, instance formats and feature extractors.", "link": "http://arxiv.org/abs/2405.10045v3", "date": "2026-01-14", "relevancy": 2.0287, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4091}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4086}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Benchmark%20Database&body=Title%3A%20Global%20Benchmark%20Database%0AAuthor%3A%20Ashlin%20Iser%20and%20Christoph%20Jabs%0AAbstract%3A%20This%20paper%20presents%20Global%20Benchmark%20Database%20%28GBD%29%2C%20a%20comprehensive%20suite%20of%20tools%20for%20provisioning%20and%20sustainably%20maintaining%20benchmark%20instances%20and%20their%20metadata.%20The%20availability%20of%20benchmark%20metadata%20is%20essential%20for%20many%20tasks%20in%20empirical%20research%2C%20e.g.%2C%20for%20the%20data-driven%20compilation%20of%20benchmarks%2C%20the%20domain-specific%20analysis%20of%20runtime%20experiments%2C%20or%20the%20instance-specific%20selection%20of%20solvers.%20In%20this%20paper%2C%20we%20introduce%20the%20data%20model%20of%20GBD%20as%20well%20as%20its%20interfaces%20and%20provide%20examples%20of%20how%20to%20interact%20with%20them.%20We%20also%20demonstrate%20the%20integration%20of%20custom%20data%20sources%20and%20explain%20how%20to%20extend%20GBD%20with%20additional%20problem%20domains%2C%20instance%20formats%20and%20feature%20extractors.%0ALink%3A%20http%3A//arxiv.org/abs/2405.10045v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Benchmark%2520Database%26entry.906535625%3DAshlin%2520Iser%2520and%2520Christoph%2520Jabs%26entry.1292438233%3DThis%2520paper%2520presents%2520Global%2520Benchmark%2520Database%2520%2528GBD%2529%252C%2520a%2520comprehensive%2520suite%2520of%2520tools%2520for%2520provisioning%2520and%2520sustainably%2520maintaining%2520benchmark%2520instances%2520and%2520their%2520metadata.%2520The%2520availability%2520of%2520benchmark%2520metadata%2520is%2520essential%2520for%2520many%2520tasks%2520in%2520empirical%2520research%252C%2520e.g.%252C%2520for%2520the%2520data-driven%2520compilation%2520of%2520benchmarks%252C%2520the%2520domain-specific%2520analysis%2520of%2520runtime%2520experiments%252C%2520or%2520the%2520instance-specific%2520selection%2520of%2520solvers.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520data%2520model%2520of%2520GBD%2520as%2520well%2520as%2520its%2520interfaces%2520and%2520provide%2520examples%2520of%2520how%2520to%2520interact%2520with%2520them.%2520We%2520also%2520demonstrate%2520the%2520integration%2520of%2520custom%2520data%2520sources%2520and%2520explain%2520how%2520to%2520extend%2520GBD%2520with%2520additional%2520problem%2520domains%252C%2520instance%2520formats%2520and%2520feature%2520extractors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10045v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Benchmark%20Database&entry.906535625=Ashlin%20Iser%20and%20Christoph%20Jabs&entry.1292438233=This%20paper%20presents%20Global%20Benchmark%20Database%20%28GBD%29%2C%20a%20comprehensive%20suite%20of%20tools%20for%20provisioning%20and%20sustainably%20maintaining%20benchmark%20instances%20and%20their%20metadata.%20The%20availability%20of%20benchmark%20metadata%20is%20essential%20for%20many%20tasks%20in%20empirical%20research%2C%20e.g.%2C%20for%20the%20data-driven%20compilation%20of%20benchmarks%2C%20the%20domain-specific%20analysis%20of%20runtime%20experiments%2C%20or%20the%20instance-specific%20selection%20of%20solvers.%20In%20this%20paper%2C%20we%20introduce%20the%20data%20model%20of%20GBD%20as%20well%20as%20its%20interfaces%20and%20provide%20examples%20of%20how%20to%20interact%20with%20them.%20We%20also%20demonstrate%20the%20integration%20of%20custom%20data%20sources%20and%20explain%20how%20to%20extend%20GBD%20with%20additional%20problem%20domains%2C%20instance%20formats%20and%20feature%20extractors.&entry.1838667208=http%3A//arxiv.org/abs/2405.10045v3&entry.124074799=Read"},
{"title": "Red Teaming Large Reasoning Models", "author": "Jiawei Chen and Yang Yang and Chao Yu and Yu Tian and Zhi Cao and Xue Yang and Linghao Li and Hang Su and Zhaoxia Yin", "abstract": "Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.", "link": "http://arxiv.org/abs/2512.00412v3", "date": "2026-01-14", "relevancy": 2.0279, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Red%20Teaming%20Large%20Reasoning%20Models&body=Title%3A%20Red%20Teaming%20Large%20Reasoning%20Models%0AAuthor%3A%20Jiawei%20Chen%20and%20Yang%20Yang%20and%20Chao%20Yu%20and%20Yu%20Tian%20and%20Zhi%20Cao%20and%20Xue%20Yang%20and%20Linghao%20Li%20and%20Hang%20Su%20and%20Zhaoxia%20Yin%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20have%20emerged%20as%20a%20powerful%20advancement%20in%20multi-step%20reasoning%20tasks%2C%20offering%20enhanced%20transparency%20and%20logical%20consistency%20through%20explicit%20chains%20of%20thought%20%28CoT%29.%20However%2C%20these%20models%20introduce%20novel%20safety%20and%20reliability%20risks%2C%20such%20as%20CoT-hijacking%20and%20prompt-induced%20inefficiencies%2C%20which%20are%20not%20fully%20captured%20by%20existing%20evaluation%20methods.%20To%20address%20this%20gap%2C%20we%20propose%20RT-LRM%2C%20a%20unified%20benchmark%20designed%20to%20assess%20the%20trustworthiness%20of%20LRMs.%20RT-LRM%20evaluates%20three%20core%20dimensions%3A%20truthfulness%2C%20safety%20and%20efficiency.%20Beyond%20metric-based%20evaluation%2C%20we%20further%20introduce%20the%20training%20paradigm%20as%20a%20key%20analytical%20perspective%20to%20investigate%20the%20systematic%20impact%20of%20different%20training%20strategies%20on%20model%20trustworthiness.%20We%20achieve%20this%20by%20designing%20a%20curated%20suite%20of%2030%20reasoning%20tasks%20from%20an%20observational%20standpoint.%20We%20conduct%20extensive%20experiments%20on%2026%20models%20and%20identify%20several%20valuable%20insights%20into%20the%20trustworthiness%20of%20LRMs.%20For%20example%2C%20LRMs%20generally%20face%20trustworthiness%20challenges%20and%20tend%20to%20be%20more%20fragile%20than%20Large%20Language%20Models%20%28LLMs%29%20when%20encountering%20reasoning-induced%20risks.%20These%20findings%20uncover%20previously%20underexplored%20vulnerabilities%20and%20highlight%20the%20need%20for%20more%20targeted%20evaluations.%20In%20addition%2C%20we%20release%20a%20scalable%20toolbox%20for%20standardized%20trustworthiness%20research%20to%20support%20future%20advancements%20in%20this%20important%20field.%20Our%20code%20and%20datasets%20will%20be%20open-sourced.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00412v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRed%2520Teaming%2520Large%2520Reasoning%2520Models%26entry.906535625%3DJiawei%2520Chen%2520and%2520Yang%2520Yang%2520and%2520Chao%2520Yu%2520and%2520Yu%2520Tian%2520and%2520Zhi%2520Cao%2520and%2520Xue%2520Yang%2520and%2520Linghao%2520Li%2520and%2520Hang%2520Su%2520and%2520Zhaoxia%2520Yin%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520advancement%2520in%2520multi-step%2520reasoning%2520tasks%252C%2520offering%2520enhanced%2520transparency%2520and%2520logical%2520consistency%2520through%2520explicit%2520chains%2520of%2520thought%2520%2528CoT%2529.%2520However%252C%2520these%2520models%2520introduce%2520novel%2520safety%2520and%2520reliability%2520risks%252C%2520such%2520as%2520CoT-hijacking%2520and%2520prompt-induced%2520inefficiencies%252C%2520which%2520are%2520not%2520fully%2520captured%2520by%2520existing%2520evaluation%2520methods.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520RT-LRM%252C%2520a%2520unified%2520benchmark%2520designed%2520to%2520assess%2520the%2520trustworthiness%2520of%2520LRMs.%2520RT-LRM%2520evaluates%2520three%2520core%2520dimensions%253A%2520truthfulness%252C%2520safety%2520and%2520efficiency.%2520Beyond%2520metric-based%2520evaluation%252C%2520we%2520further%2520introduce%2520the%2520training%2520paradigm%2520as%2520a%2520key%2520analytical%2520perspective%2520to%2520investigate%2520the%2520systematic%2520impact%2520of%2520different%2520training%2520strategies%2520on%2520model%2520trustworthiness.%2520We%2520achieve%2520this%2520by%2520designing%2520a%2520curated%2520suite%2520of%252030%2520reasoning%2520tasks%2520from%2520an%2520observational%2520standpoint.%2520We%2520conduct%2520extensive%2520experiments%2520on%252026%2520models%2520and%2520identify%2520several%2520valuable%2520insights%2520into%2520the%2520trustworthiness%2520of%2520LRMs.%2520For%2520example%252C%2520LRMs%2520generally%2520face%2520trustworthiness%2520challenges%2520and%2520tend%2520to%2520be%2520more%2520fragile%2520than%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520when%2520encountering%2520reasoning-induced%2520risks.%2520These%2520findings%2520uncover%2520previously%2520underexplored%2520vulnerabilities%2520and%2520highlight%2520the%2520need%2520for%2520more%2520targeted%2520evaluations.%2520In%2520addition%252C%2520we%2520release%2520a%2520scalable%2520toolbox%2520for%2520standardized%2520trustworthiness%2520research%2520to%2520support%2520future%2520advancements%2520in%2520this%2520important%2520field.%2520Our%2520code%2520and%2520datasets%2520will%2520be%2520open-sourced.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00412v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Red%20Teaming%20Large%20Reasoning%20Models&entry.906535625=Jiawei%20Chen%20and%20Yang%20Yang%20and%20Chao%20Yu%20and%20Yu%20Tian%20and%20Zhi%20Cao%20and%20Xue%20Yang%20and%20Linghao%20Li%20and%20Hang%20Su%20and%20Zhaoxia%20Yin&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20have%20emerged%20as%20a%20powerful%20advancement%20in%20multi-step%20reasoning%20tasks%2C%20offering%20enhanced%20transparency%20and%20logical%20consistency%20through%20explicit%20chains%20of%20thought%20%28CoT%29.%20However%2C%20these%20models%20introduce%20novel%20safety%20and%20reliability%20risks%2C%20such%20as%20CoT-hijacking%20and%20prompt-induced%20inefficiencies%2C%20which%20are%20not%20fully%20captured%20by%20existing%20evaluation%20methods.%20To%20address%20this%20gap%2C%20we%20propose%20RT-LRM%2C%20a%20unified%20benchmark%20designed%20to%20assess%20the%20trustworthiness%20of%20LRMs.%20RT-LRM%20evaluates%20three%20core%20dimensions%3A%20truthfulness%2C%20safety%20and%20efficiency.%20Beyond%20metric-based%20evaluation%2C%20we%20further%20introduce%20the%20training%20paradigm%20as%20a%20key%20analytical%20perspective%20to%20investigate%20the%20systematic%20impact%20of%20different%20training%20strategies%20on%20model%20trustworthiness.%20We%20achieve%20this%20by%20designing%20a%20curated%20suite%20of%2030%20reasoning%20tasks%20from%20an%20observational%20standpoint.%20We%20conduct%20extensive%20experiments%20on%2026%20models%20and%20identify%20several%20valuable%20insights%20into%20the%20trustworthiness%20of%20LRMs.%20For%20example%2C%20LRMs%20generally%20face%20trustworthiness%20challenges%20and%20tend%20to%20be%20more%20fragile%20than%20Large%20Language%20Models%20%28LLMs%29%20when%20encountering%20reasoning-induced%20risks.%20These%20findings%20uncover%20previously%20underexplored%20vulnerabilities%20and%20highlight%20the%20need%20for%20more%20targeted%20evaluations.%20In%20addition%2C%20we%20release%20a%20scalable%20toolbox%20for%20standardized%20trustworthiness%20research%20to%20support%20future%20advancements%20in%20this%20important%20field.%20Our%20code%20and%20datasets%20will%20be%20open-sourced.&entry.1838667208=http%3A//arxiv.org/abs/2512.00412v3&entry.124074799=Read"},
{"title": "LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models", "author": "Siddharth Betala and Samuel P. Gleason and Ali Ramlaoui and Andy Xu and Georgia Channing and Daniel Levy and Cl\u00e9mentine Fourrier and Nikita Kazeev and Chaitanya K. Joshi and S\u00e9kou-Oumar Kaba and F\u00e9lix Therrien and Alex Hernandez-Garcia and Roc\u00edo Mercado and N. M. Anoop Krishnan and Alexandre Duval", "abstract": "Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.", "link": "http://arxiv.org/abs/2512.04562v2", "date": "2026-01-14", "relevancy": 2.0256, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5093}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5051}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeMat-GenBench%3A%20A%20Unified%20Evaluation%20Framework%20for%20Crystal%20Generative%20Models&body=Title%3A%20LeMat-GenBench%3A%20A%20Unified%20Evaluation%20Framework%20for%20Crystal%20Generative%20Models%0AAuthor%3A%20Siddharth%20Betala%20and%20Samuel%20P.%20Gleason%20and%20Ali%20Ramlaoui%20and%20Andy%20Xu%20and%20Georgia%20Channing%20and%20Daniel%20Levy%20and%20Cl%C3%A9mentine%20Fourrier%20and%20Nikita%20Kazeev%20and%20Chaitanya%20K.%20Joshi%20and%20S%C3%A9kou-Oumar%20Kaba%20and%20F%C3%A9lix%20Therrien%20and%20Alex%20Hernandez-Garcia%20and%20Roc%C3%ADo%20Mercado%20and%20N.%20M.%20Anoop%20Krishnan%20and%20Alexandre%20Duval%0AAbstract%3A%20Generative%20machine%20learning%20%28ML%29%20models%20hold%20great%20promise%20for%20accelerating%20materials%20discovery%20through%20the%20inverse%20design%20of%20inorganic%20crystals%2C%20enabling%20an%20unprecedented%20exploration%20of%20chemical%20space.%20Yet%2C%20the%20lack%20of%20standardized%20evaluation%20frameworks%20makes%20it%20challenging%20to%20evaluate%2C%20compare%2C%20and%20further%20develop%20these%20ML%20models%20meaningfully.%20In%20this%20work%2C%20we%20introduce%20LeMat-GenBench%2C%20a%20unified%20benchmark%20for%20generative%20models%20of%20crystalline%20materials%2C%20supported%20by%20a%20set%20of%20evaluation%20metrics%20designed%20to%20better%20inform%20model%20development%20and%20downstream%20applications.%20We%20release%20both%20an%20open-source%20evaluation%20suite%20and%20a%20public%20leaderboard%20on%20Hugging%20Face%2C%20and%20benchmark%2012%20recent%20generative%20models.%20Results%20reveal%20that%20an%20increase%20in%20stability%20leads%20to%20a%20decrease%20in%20novelty%20and%20diversity%20on%20average%2C%20with%20no%20model%20excelling%20across%20all%20dimensions.%20Altogether%2C%20LeMat-GenBench%20establishes%20a%20reproducible%20and%20extensible%20foundation%20for%20fair%20model%20comparison%20and%20aims%20to%20guide%20the%20development%20of%20more%20reliable%2C%20discovery-oriented%20generative%20models%20for%20crystalline%20materials.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeMat-GenBench%253A%2520A%2520Unified%2520Evaluation%2520Framework%2520for%2520Crystal%2520Generative%2520Models%26entry.906535625%3DSiddharth%2520Betala%2520and%2520Samuel%2520P.%2520Gleason%2520and%2520Ali%2520Ramlaoui%2520and%2520Andy%2520Xu%2520and%2520Georgia%2520Channing%2520and%2520Daniel%2520Levy%2520and%2520Cl%25C3%25A9mentine%2520Fourrier%2520and%2520Nikita%2520Kazeev%2520and%2520Chaitanya%2520K.%2520Joshi%2520and%2520S%25C3%25A9kou-Oumar%2520Kaba%2520and%2520F%25C3%25A9lix%2520Therrien%2520and%2520Alex%2520Hernandez-Garcia%2520and%2520Roc%25C3%25ADo%2520Mercado%2520and%2520N.%2520M.%2520Anoop%2520Krishnan%2520and%2520Alexandre%2520Duval%26entry.1292438233%3DGenerative%2520machine%2520learning%2520%2528ML%2529%2520models%2520hold%2520great%2520promise%2520for%2520accelerating%2520materials%2520discovery%2520through%2520the%2520inverse%2520design%2520of%2520inorganic%2520crystals%252C%2520enabling%2520an%2520unprecedented%2520exploration%2520of%2520chemical%2520space.%2520Yet%252C%2520the%2520lack%2520of%2520standardized%2520evaluation%2520frameworks%2520makes%2520it%2520challenging%2520to%2520evaluate%252C%2520compare%252C%2520and%2520further%2520develop%2520these%2520ML%2520models%2520meaningfully.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LeMat-GenBench%252C%2520a%2520unified%2520benchmark%2520for%2520generative%2520models%2520of%2520crystalline%2520materials%252C%2520supported%2520by%2520a%2520set%2520of%2520evaluation%2520metrics%2520designed%2520to%2520better%2520inform%2520model%2520development%2520and%2520downstream%2520applications.%2520We%2520release%2520both%2520an%2520open-source%2520evaluation%2520suite%2520and%2520a%2520public%2520leaderboard%2520on%2520Hugging%2520Face%252C%2520and%2520benchmark%252012%2520recent%2520generative%2520models.%2520Results%2520reveal%2520that%2520an%2520increase%2520in%2520stability%2520leads%2520to%2520a%2520decrease%2520in%2520novelty%2520and%2520diversity%2520on%2520average%252C%2520with%2520no%2520model%2520excelling%2520across%2520all%2520dimensions.%2520Altogether%252C%2520LeMat-GenBench%2520establishes%2520a%2520reproducible%2520and%2520extensible%2520foundation%2520for%2520fair%2520model%2520comparison%2520and%2520aims%2520to%2520guide%2520the%2520development%2520of%2520more%2520reliable%252C%2520discovery-oriented%2520generative%2520models%2520for%2520crystalline%2520materials.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeMat-GenBench%3A%20A%20Unified%20Evaluation%20Framework%20for%20Crystal%20Generative%20Models&entry.906535625=Siddharth%20Betala%20and%20Samuel%20P.%20Gleason%20and%20Ali%20Ramlaoui%20and%20Andy%20Xu%20and%20Georgia%20Channing%20and%20Daniel%20Levy%20and%20Cl%C3%A9mentine%20Fourrier%20and%20Nikita%20Kazeev%20and%20Chaitanya%20K.%20Joshi%20and%20S%C3%A9kou-Oumar%20Kaba%20and%20F%C3%A9lix%20Therrien%20and%20Alex%20Hernandez-Garcia%20and%20Roc%C3%ADo%20Mercado%20and%20N.%20M.%20Anoop%20Krishnan%20and%20Alexandre%20Duval&entry.1292438233=Generative%20machine%20learning%20%28ML%29%20models%20hold%20great%20promise%20for%20accelerating%20materials%20discovery%20through%20the%20inverse%20design%20of%20inorganic%20crystals%2C%20enabling%20an%20unprecedented%20exploration%20of%20chemical%20space.%20Yet%2C%20the%20lack%20of%20standardized%20evaluation%20frameworks%20makes%20it%20challenging%20to%20evaluate%2C%20compare%2C%20and%20further%20develop%20these%20ML%20models%20meaningfully.%20In%20this%20work%2C%20we%20introduce%20LeMat-GenBench%2C%20a%20unified%20benchmark%20for%20generative%20models%20of%20crystalline%20materials%2C%20supported%20by%20a%20set%20of%20evaluation%20metrics%20designed%20to%20better%20inform%20model%20development%20and%20downstream%20applications.%20We%20release%20both%20an%20open-source%20evaluation%20suite%20and%20a%20public%20leaderboard%20on%20Hugging%20Face%2C%20and%20benchmark%2012%20recent%20generative%20models.%20Results%20reveal%20that%20an%20increase%20in%20stability%20leads%20to%20a%20decrease%20in%20novelty%20and%20diversity%20on%20average%2C%20with%20no%20model%20excelling%20across%20all%20dimensions.%20Altogether%2C%20LeMat-GenBench%20establishes%20a%20reproducible%20and%20extensible%20foundation%20for%20fair%20model%20comparison%20and%20aims%20to%20guide%20the%20development%20of%20more%20reliable%2C%20discovery-oriented%20generative%20models%20for%20crystalline%20materials.&entry.1838667208=http%3A//arxiv.org/abs/2512.04562v2&entry.124074799=Read"},
{"title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning", "author": "Caijun Xu and Changyi Xiao and Zhongyuan Peng and Xinrun Wang and Yixin Cao", "abstract": "Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.", "link": "http://arxiv.org/abs/2601.04809v2", "date": "2026-01-14", "relevancy": 2.0235, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5322}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCALER%3ASynthetic%20Scalable%20Adaptive%20Learning%20Environment%20for%20Reasoning&body=Title%3A%20SCALER%3ASynthetic%20Scalable%20Adaptive%20Learning%20Environment%20for%20Reasoning%0AAuthor%3A%20Caijun%20Xu%20and%20Changyi%20Xiao%20and%20Zhongyuan%20Peng%20and%20Xinrun%20Wang%20and%20Yixin%20Cao%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20offers%20a%20principled%20way%20to%20enhance%20the%20reasoning%20capabilities%20of%20large%20language%20models%2C%20yet%20its%20effectiveness%20hinges%20on%20training%20signals%20that%20remain%20informative%20as%20models%20evolve.%20In%20practice%2C%20RL%20progress%20often%20slows%20when%20task%20difficulty%20becomes%20poorly%20aligned%20with%20model%20capability%2C%20or%20when%20training%20is%20dominated%20by%20a%20narrow%20set%20of%20recurring%20problem%20patterns.%20To%20jointly%20address%20these%20issues%2C%20we%20propose%20SCALER%20%28Synthetic%20sCalable%20Adaptive%20Learning%20Environment%20for%20Reasoning%29%2C%20a%20framework%20that%20sustains%20effective%20learning%20signals%20through%20adaptive%20environment%20design.%20SCALER%20introduces%20a%20scalable%20synthesis%20pipeline%20that%20converts%20real-world%20programming%20problems%20into%20verifiable%20reasoning%20environments%20with%20controllable%20difficulty%20and%20unbounded%20instance%20generation%2C%20enabling%20RL%20training%20beyond%20finite%20datasets%20while%20preserving%20strong%20correctness%20guarantees.%20Building%20on%20this%2C%20SCALER%20further%20employs%20an%20adaptive%20multi-environment%20RL%20strategy%20that%20dynamically%20adjusts%20instance%20difficulty%20and%20curates%20the%20active%20set%20of%20environments%20to%20track%20the%20model%27s%20capability%20frontier%20and%20maintain%20distributional%20diversity.%20This%20co-adaptation%20prevents%20reward%20sparsity%2C%20mitigates%20overfitting%20to%20narrow%20task%20patterns%2C%20and%20supports%20sustained%20improvement%20throughout%20training.%20Extensive%20experiments%20show%20that%20SCALER%20consistently%20outperforms%20dataset-based%20RL%20baselines%20across%20diverse%20reasoning%20benchmarks%20and%20exhibits%20more%20stable%2C%20long-horizon%20training%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.04809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCALER%253ASynthetic%2520Scalable%2520Adaptive%2520Learning%2520Environment%2520for%2520Reasoning%26entry.906535625%3DCaijun%2520Xu%2520and%2520Changyi%2520Xiao%2520and%2520Zhongyuan%2520Peng%2520and%2520Xinrun%2520Wang%2520and%2520Yixin%2520Cao%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520offers%2520a%2520principled%2520way%2520to%2520enhance%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%252C%2520yet%2520its%2520effectiveness%2520hinges%2520on%2520training%2520signals%2520that%2520remain%2520informative%2520as%2520models%2520evolve.%2520In%2520practice%252C%2520RL%2520progress%2520often%2520slows%2520when%2520task%2520difficulty%2520becomes%2520poorly%2520aligned%2520with%2520model%2520capability%252C%2520or%2520when%2520training%2520is%2520dominated%2520by%2520a%2520narrow%2520set%2520of%2520recurring%2520problem%2520patterns.%2520To%2520jointly%2520address%2520these%2520issues%252C%2520we%2520propose%2520SCALER%2520%2528Synthetic%2520sCalable%2520Adaptive%2520Learning%2520Environment%2520for%2520Reasoning%2529%252C%2520a%2520framework%2520that%2520sustains%2520effective%2520learning%2520signals%2520through%2520adaptive%2520environment%2520design.%2520SCALER%2520introduces%2520a%2520scalable%2520synthesis%2520pipeline%2520that%2520converts%2520real-world%2520programming%2520problems%2520into%2520verifiable%2520reasoning%2520environments%2520with%2520controllable%2520difficulty%2520and%2520unbounded%2520instance%2520generation%252C%2520enabling%2520RL%2520training%2520beyond%2520finite%2520datasets%2520while%2520preserving%2520strong%2520correctness%2520guarantees.%2520Building%2520on%2520this%252C%2520SCALER%2520further%2520employs%2520an%2520adaptive%2520multi-environment%2520RL%2520strategy%2520that%2520dynamically%2520adjusts%2520instance%2520difficulty%2520and%2520curates%2520the%2520active%2520set%2520of%2520environments%2520to%2520track%2520the%2520model%2527s%2520capability%2520frontier%2520and%2520maintain%2520distributional%2520diversity.%2520This%2520co-adaptation%2520prevents%2520reward%2520sparsity%252C%2520mitigates%2520overfitting%2520to%2520narrow%2520task%2520patterns%252C%2520and%2520supports%2520sustained%2520improvement%2520throughout%2520training.%2520Extensive%2520experiments%2520show%2520that%2520SCALER%2520consistently%2520outperforms%2520dataset-based%2520RL%2520baselines%2520across%2520diverse%2520reasoning%2520benchmarks%2520and%2520exhibits%2520more%2520stable%252C%2520long-horizon%2520training%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.04809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCALER%3ASynthetic%20Scalable%20Adaptive%20Learning%20Environment%20for%20Reasoning&entry.906535625=Caijun%20Xu%20and%20Changyi%20Xiao%20and%20Zhongyuan%20Peng%20and%20Xinrun%20Wang%20and%20Yixin%20Cao&entry.1292438233=Reinforcement%20learning%20%28RL%29%20offers%20a%20principled%20way%20to%20enhance%20the%20reasoning%20capabilities%20of%20large%20language%20models%2C%20yet%20its%20effectiveness%20hinges%20on%20training%20signals%20that%20remain%20informative%20as%20models%20evolve.%20In%20practice%2C%20RL%20progress%20often%20slows%20when%20task%20difficulty%20becomes%20poorly%20aligned%20with%20model%20capability%2C%20or%20when%20training%20is%20dominated%20by%20a%20narrow%20set%20of%20recurring%20problem%20patterns.%20To%20jointly%20address%20these%20issues%2C%20we%20propose%20SCALER%20%28Synthetic%20sCalable%20Adaptive%20Learning%20Environment%20for%20Reasoning%29%2C%20a%20framework%20that%20sustains%20effective%20learning%20signals%20through%20adaptive%20environment%20design.%20SCALER%20introduces%20a%20scalable%20synthesis%20pipeline%20that%20converts%20real-world%20programming%20problems%20into%20verifiable%20reasoning%20environments%20with%20controllable%20difficulty%20and%20unbounded%20instance%20generation%2C%20enabling%20RL%20training%20beyond%20finite%20datasets%20while%20preserving%20strong%20correctness%20guarantees.%20Building%20on%20this%2C%20SCALER%20further%20employs%20an%20adaptive%20multi-environment%20RL%20strategy%20that%20dynamically%20adjusts%20instance%20difficulty%20and%20curates%20the%20active%20set%20of%20environments%20to%20track%20the%20model%27s%20capability%20frontier%20and%20maintain%20distributional%20diversity.%20This%20co-adaptation%20prevents%20reward%20sparsity%2C%20mitigates%20overfitting%20to%20narrow%20task%20patterns%2C%20and%20supports%20sustained%20improvement%20throughout%20training.%20Extensive%20experiments%20show%20that%20SCALER%20consistently%20outperforms%20dataset-based%20RL%20baselines%20across%20diverse%20reasoning%20benchmarks%20and%20exhibits%20more%20stable%2C%20long-horizon%20training%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2601.04809v2&entry.124074799=Read"},
{"title": "Ability Transfer and Recovery via Modularized Parameters Localization", "author": "Songyao Jin and Kun Zhou and Wenqi Li and Peng Wang and Biwei Huang", "abstract": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.", "link": "http://arxiv.org/abs/2601.09398v1", "date": "2026-01-14", "relevancy": 2.0119, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ability%20Transfer%20and%20Recovery%20via%20Modularized%20Parameters%20Localization&body=Title%3A%20Ability%20Transfer%20and%20Recovery%20via%20Modularized%20Parameters%20Localization%0AAuthor%3A%20Songyao%20Jin%20and%20Kun%20Zhou%20and%20Wenqi%20Li%20and%20Peng%20Wang%20and%20Biwei%20Huang%0AAbstract%3A%20Large%20language%20models%20can%20be%20continually%20pre-trained%20or%20fine-tuned%20to%20improve%20performance%20in%20specific%20domains%2C%20languages%2C%20or%20skills%2C%20but%20this%20specialization%20often%20degrades%20other%20capabilities%20and%20may%20cause%20catastrophic%20forgetting.%20We%20investigate%20how%20abilities%20are%20distributed%20within%20LLM%20parameters%20by%20analyzing%20module%20activations%20under%20domain-%20and%20language-specific%20inputs%20for%20closely%20related%20models.%20Across%20layers%20and%20modules%2C%20we%20find%20that%20ability-related%20activations%20are%20highly%20concentrated%20in%20a%20small%20set%20of%20channels%20%28typically%20%3C5%5C%25%29%2C%20and%20these%20channels%20are%20largely%20disentangled%20with%20good%20sufficiency%20and%20stability.%20Building%20on%20these%20observations%2C%20we%20propose%20ACT%20%28Activation-Guided%20Channel-wise%20Ability%20Transfer%29%2C%20which%20localizes%20ability-relevant%20channels%20via%20activation%20differences%20and%20selectively%20transfers%20only%20the%20corresponding%20parameters%2C%20followed%20by%20lightweight%20fine-tuning%20for%20compatibility.%20Experiments%20on%20multilingual%20mathematical%20and%20scientific%20reasoning%20show%20that%20ACT%20can%20recover%20forgotten%20abilities%20while%20preserving%20retained%20skills.%20It%20can%20also%20merge%20multiple%20specialized%20models%20to%20integrate%20several%20abilities%20into%20a%20single%20model%20with%20minimal%20interference.%20Our%20code%20and%20data%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbility%2520Transfer%2520and%2520Recovery%2520via%2520Modularized%2520Parameters%2520Localization%26entry.906535625%3DSongyao%2520Jin%2520and%2520Kun%2520Zhou%2520and%2520Wenqi%2520Li%2520and%2520Peng%2520Wang%2520and%2520Biwei%2520Huang%26entry.1292438233%3DLarge%2520language%2520models%2520can%2520be%2520continually%2520pre-trained%2520or%2520fine-tuned%2520to%2520improve%2520performance%2520in%2520specific%2520domains%252C%2520languages%252C%2520or%2520skills%252C%2520but%2520this%2520specialization%2520often%2520degrades%2520other%2520capabilities%2520and%2520may%2520cause%2520catastrophic%2520forgetting.%2520We%2520investigate%2520how%2520abilities%2520are%2520distributed%2520within%2520LLM%2520parameters%2520by%2520analyzing%2520module%2520activations%2520under%2520domain-%2520and%2520language-specific%2520inputs%2520for%2520closely%2520related%2520models.%2520Across%2520layers%2520and%2520modules%252C%2520we%2520find%2520that%2520ability-related%2520activations%2520are%2520highly%2520concentrated%2520in%2520a%2520small%2520set%2520of%2520channels%2520%2528typically%2520%253C5%255C%2525%2529%252C%2520and%2520these%2520channels%2520are%2520largely%2520disentangled%2520with%2520good%2520sufficiency%2520and%2520stability.%2520Building%2520on%2520these%2520observations%252C%2520we%2520propose%2520ACT%2520%2528Activation-Guided%2520Channel-wise%2520Ability%2520Transfer%2529%252C%2520which%2520localizes%2520ability-relevant%2520channels%2520via%2520activation%2520differences%2520and%2520selectively%2520transfers%2520only%2520the%2520corresponding%2520parameters%252C%2520followed%2520by%2520lightweight%2520fine-tuning%2520for%2520compatibility.%2520Experiments%2520on%2520multilingual%2520mathematical%2520and%2520scientific%2520reasoning%2520show%2520that%2520ACT%2520can%2520recover%2520forgotten%2520abilities%2520while%2520preserving%2520retained%2520skills.%2520It%2520can%2520also%2520merge%2520multiple%2520specialized%2520models%2520to%2520integrate%2520several%2520abilities%2520into%2520a%2520single%2520model%2520with%2520minimal%2520interference.%2520Our%2520code%2520and%2520data%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ability%20Transfer%20and%20Recovery%20via%20Modularized%20Parameters%20Localization&entry.906535625=Songyao%20Jin%20and%20Kun%20Zhou%20and%20Wenqi%20Li%20and%20Peng%20Wang%20and%20Biwei%20Huang&entry.1292438233=Large%20language%20models%20can%20be%20continually%20pre-trained%20or%20fine-tuned%20to%20improve%20performance%20in%20specific%20domains%2C%20languages%2C%20or%20skills%2C%20but%20this%20specialization%20often%20degrades%20other%20capabilities%20and%20may%20cause%20catastrophic%20forgetting.%20We%20investigate%20how%20abilities%20are%20distributed%20within%20LLM%20parameters%20by%20analyzing%20module%20activations%20under%20domain-%20and%20language-specific%20inputs%20for%20closely%20related%20models.%20Across%20layers%20and%20modules%2C%20we%20find%20that%20ability-related%20activations%20are%20highly%20concentrated%20in%20a%20small%20set%20of%20channels%20%28typically%20%3C5%5C%25%29%2C%20and%20these%20channels%20are%20largely%20disentangled%20with%20good%20sufficiency%20and%20stability.%20Building%20on%20these%20observations%2C%20we%20propose%20ACT%20%28Activation-Guided%20Channel-wise%20Ability%20Transfer%29%2C%20which%20localizes%20ability-relevant%20channels%20via%20activation%20differences%20and%20selectively%20transfers%20only%20the%20corresponding%20parameters%2C%20followed%20by%20lightweight%20fine-tuning%20for%20compatibility.%20Experiments%20on%20multilingual%20mathematical%20and%20scientific%20reasoning%20show%20that%20ACT%20can%20recover%20forgotten%20abilities%20while%20preserving%20retained%20skills.%20It%20can%20also%20merge%20multiple%20specialized%20models%20to%20integrate%20several%20abilities%20into%20a%20single%20model%20with%20minimal%20interference.%20Our%20code%20and%20data%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.09398v1&entry.124074799=Read"},
{"title": "Population-Aligned Audio Reproduction With LLM-Based Equalizers", "author": "Ioannis Stylianou and Jon Francombe and Pablo Martinez-Nuevo and Sven Ewan Shepstone and Zheng-Hua Tan", "abstract": "Conventional audio equalization is a static process that requires manual and cumbersome adjustments to adapt to changing listening contexts (e.g., mood, location, or social setting). In this paper, we introduce a Large Language Model (LLM)-based alternative that maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control. By utilizing data collected from a controlled listening experiment, our models exploit in-context learning and parameter-efficient fine-tuning techniques to reliably align with population-preferred equalization settings. Our evaluation methods, which leverage distributional metrics that capture users' varied preferences, show statistically significant improvements in distributional alignment over random sampling and static preset baselines. These results indicate that LLMs could function as \"artificial equalizers,\" contributing to the development of more accessible, context-aware, and expert-level audio tuning methods.", "link": "http://arxiv.org/abs/2601.09448v1", "date": "2026-01-14", "relevancy": 1.6745, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4257}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Population-Aligned%20Audio%20Reproduction%20With%20LLM-Based%20Equalizers&body=Title%3A%20Population-Aligned%20Audio%20Reproduction%20With%20LLM-Based%20Equalizers%0AAuthor%3A%20Ioannis%20Stylianou%20and%20Jon%20Francombe%20and%20Pablo%20Martinez-Nuevo%20and%20Sven%20Ewan%20Shepstone%20and%20Zheng-Hua%20Tan%0AAbstract%3A%20Conventional%20audio%20equalization%20is%20a%20static%20process%20that%20requires%20manual%20and%20cumbersome%20adjustments%20to%20adapt%20to%20changing%20listening%20contexts%20%28e.g.%2C%20mood%2C%20location%2C%20or%20social%20setting%29.%20In%20this%20paper%2C%20we%20introduce%20a%20Large%20Language%20Model%20%28LLM%29-based%20alternative%20that%20maps%20natural%20language%20text%20prompts%20to%20equalization%20settings.%20This%20enables%20a%20conversational%20approach%20to%20sound%20system%20control.%20By%20utilizing%20data%20collected%20from%20a%20controlled%20listening%20experiment%2C%20our%20models%20exploit%20in-context%20learning%20and%20parameter-efficient%20fine-tuning%20techniques%20to%20reliably%20align%20with%20population-preferred%20equalization%20settings.%20Our%20evaluation%20methods%2C%20which%20leverage%20distributional%20metrics%20that%20capture%20users%27%20varied%20preferences%2C%20show%20statistically%20significant%20improvements%20in%20distributional%20alignment%20over%20random%20sampling%20and%20static%20preset%20baselines.%20These%20results%20indicate%20that%20LLMs%20could%20function%20as%20%22artificial%20equalizers%2C%22%20contributing%20to%20the%20development%20of%20more%20accessible%2C%20context-aware%2C%20and%20expert-level%20audio%20tuning%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPopulation-Aligned%2520Audio%2520Reproduction%2520With%2520LLM-Based%2520Equalizers%26entry.906535625%3DIoannis%2520Stylianou%2520and%2520Jon%2520Francombe%2520and%2520Pablo%2520Martinez-Nuevo%2520and%2520Sven%2520Ewan%2520Shepstone%2520and%2520Zheng-Hua%2520Tan%26entry.1292438233%3DConventional%2520audio%2520equalization%2520is%2520a%2520static%2520process%2520that%2520requires%2520manual%2520and%2520cumbersome%2520adjustments%2520to%2520adapt%2520to%2520changing%2520listening%2520contexts%2520%2528e.g.%252C%2520mood%252C%2520location%252C%2520or%2520social%2520setting%2529.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520alternative%2520that%2520maps%2520natural%2520language%2520text%2520prompts%2520to%2520equalization%2520settings.%2520This%2520enables%2520a%2520conversational%2520approach%2520to%2520sound%2520system%2520control.%2520By%2520utilizing%2520data%2520collected%2520from%2520a%2520controlled%2520listening%2520experiment%252C%2520our%2520models%2520exploit%2520in-context%2520learning%2520and%2520parameter-efficient%2520fine-tuning%2520techniques%2520to%2520reliably%2520align%2520with%2520population-preferred%2520equalization%2520settings.%2520Our%2520evaluation%2520methods%252C%2520which%2520leverage%2520distributional%2520metrics%2520that%2520capture%2520users%2527%2520varied%2520preferences%252C%2520show%2520statistically%2520significant%2520improvements%2520in%2520distributional%2520alignment%2520over%2520random%2520sampling%2520and%2520static%2520preset%2520baselines.%2520These%2520results%2520indicate%2520that%2520LLMs%2520could%2520function%2520as%2520%2522artificial%2520equalizers%252C%2522%2520contributing%2520to%2520the%2520development%2520of%2520more%2520accessible%252C%2520context-aware%252C%2520and%2520expert-level%2520audio%2520tuning%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Population-Aligned%20Audio%20Reproduction%20With%20LLM-Based%20Equalizers&entry.906535625=Ioannis%20Stylianou%20and%20Jon%20Francombe%20and%20Pablo%20Martinez-Nuevo%20and%20Sven%20Ewan%20Shepstone%20and%20Zheng-Hua%20Tan&entry.1292438233=Conventional%20audio%20equalization%20is%20a%20static%20process%20that%20requires%20manual%20and%20cumbersome%20adjustments%20to%20adapt%20to%20changing%20listening%20contexts%20%28e.g.%2C%20mood%2C%20location%2C%20or%20social%20setting%29.%20In%20this%20paper%2C%20we%20introduce%20a%20Large%20Language%20Model%20%28LLM%29-based%20alternative%20that%20maps%20natural%20language%20text%20prompts%20to%20equalization%20settings.%20This%20enables%20a%20conversational%20approach%20to%20sound%20system%20control.%20By%20utilizing%20data%20collected%20from%20a%20controlled%20listening%20experiment%2C%20our%20models%20exploit%20in-context%20learning%20and%20parameter-efficient%20fine-tuning%20techniques%20to%20reliably%20align%20with%20population-preferred%20equalization%20settings.%20Our%20evaluation%20methods%2C%20which%20leverage%20distributional%20metrics%20that%20capture%20users%27%20varied%20preferences%2C%20show%20statistically%20significant%20improvements%20in%20distributional%20alignment%20over%20random%20sampling%20and%20static%20preset%20baselines.%20These%20results%20indicate%20that%20LLMs%20could%20function%20as%20%22artificial%20equalizers%2C%22%20contributing%20to%20the%20development%20of%20more%20accessible%2C%20context-aware%2C%20and%20expert-level%20audio%20tuning%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.09448v1&entry.124074799=Read"},
{"title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "author": "Chenliang Zhang and Lin Wang and Yuanyuan Lu and Yusheng Qi and Kexin Wang and Peixu Hou and Wenshi Chen", "abstract": "This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline.", "link": "http://arxiv.org/abs/2508.10337v2", "date": "2026-01-14", "relevancy": 1.5873, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Curriculum%20Learning%20Approach%20to%20Reinforcement%20Learning%3A%20Leveraging%20RAG%20for%20Multimodal%20Question%20Answering&body=Title%3A%20A%20Curriculum%20Learning%20Approach%20to%20Reinforcement%20Learning%3A%20Leveraging%20RAG%20for%20Multimodal%20Question%20Answering%0AAuthor%3A%20Chenliang%20Zhang%20and%20Lin%20Wang%20and%20Yuanyuan%20Lu%20and%20Yusheng%20Qi%20and%20Kexin%20Wang%20and%20Peixu%20Hou%20and%20Wenshi%20Chen%0AAbstract%3A%20This%20paper%20describes%20the%20solutions%20of%20the%20Dianping-Trust-Safety%20team%20for%20the%20META%20CRAG-MM%20challenge.%20The%20challenge%20requires%20building%20a%20comprehensive%20retrieval-augmented%20generation%20system%20capable%20for%20multi-modal%20multi-turn%20question%20answering.%20The%20competition%20consists%20of%20three%20tasks%3A%20%281%29%20answering%20questions%20using%20structured%20data%20retrieved%20from%20an%20image-based%20mock%20knowledge%20graph%2C%20%282%29%20synthesizing%20information%20from%20both%20knowledge%20graphs%20and%20web%20search%20results%2C%20and%20%283%29%20handling%20multi-turn%20conversations%20that%20require%20context%20understanding%20and%20information%20aggregation%20from%20multiple%20sources.%20For%20Task%201%2C%20our%20solution%20is%20based%20on%20the%20vision%20large%20language%20model%2C%20enhanced%20by%20supervised%20fine-tuning%20with%20knowledge%20distilled%20from%20GPT-4.1.%20We%20further%20applied%20curriculum%20learning%20strategies%20to%20guide%20reinforcement%20learning%2C%20resulting%20in%20improved%20answer%20accuracy%20and%20reduced%20hallucination.%20For%20Task%202%20and%20Task%203%2C%20we%20additionally%20leveraged%20web%20search%20APIs%20to%20incorporate%20external%20knowledge%2C%20enabling%20the%20system%20to%20better%20handle%20complex%20queries%20and%20multi-turn%20conversations.%20Our%20approach%20achieved%201st%20place%20in%20Task%201%20with%20a%20significant%20lead%20of%2052.38%25%2C%20and%203rd%20place%20in%20Task%203%2C%20demonstrating%20the%20effectiveness%20of%20the%20integration%20of%20curriculum%20learning%20with%20reinforcement%20learning%20in%20our%20training%20pipeline.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10337v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Curriculum%2520Learning%2520Approach%2520to%2520Reinforcement%2520Learning%253A%2520Leveraging%2520RAG%2520for%2520Multimodal%2520Question%2520Answering%26entry.906535625%3DChenliang%2520Zhang%2520and%2520Lin%2520Wang%2520and%2520Yuanyuan%2520Lu%2520and%2520Yusheng%2520Qi%2520and%2520Kexin%2520Wang%2520and%2520Peixu%2520Hou%2520and%2520Wenshi%2520Chen%26entry.1292438233%3DThis%2520paper%2520describes%2520the%2520solutions%2520of%2520the%2520Dianping-Trust-Safety%2520team%2520for%2520the%2520META%2520CRAG-MM%2520challenge.%2520The%2520challenge%2520requires%2520building%2520a%2520comprehensive%2520retrieval-augmented%2520generation%2520system%2520capable%2520for%2520multi-modal%2520multi-turn%2520question%2520answering.%2520The%2520competition%2520consists%2520of%2520three%2520tasks%253A%2520%25281%2529%2520answering%2520questions%2520using%2520structured%2520data%2520retrieved%2520from%2520an%2520image-based%2520mock%2520knowledge%2520graph%252C%2520%25282%2529%2520synthesizing%2520information%2520from%2520both%2520knowledge%2520graphs%2520and%2520web%2520search%2520results%252C%2520and%2520%25283%2529%2520handling%2520multi-turn%2520conversations%2520that%2520require%2520context%2520understanding%2520and%2520information%2520aggregation%2520from%2520multiple%2520sources.%2520For%2520Task%25201%252C%2520our%2520solution%2520is%2520based%2520on%2520the%2520vision%2520large%2520language%2520model%252C%2520enhanced%2520by%2520supervised%2520fine-tuning%2520with%2520knowledge%2520distilled%2520from%2520GPT-4.1.%2520We%2520further%2520applied%2520curriculum%2520learning%2520strategies%2520to%2520guide%2520reinforcement%2520learning%252C%2520resulting%2520in%2520improved%2520answer%2520accuracy%2520and%2520reduced%2520hallucination.%2520For%2520Task%25202%2520and%2520Task%25203%252C%2520we%2520additionally%2520leveraged%2520web%2520search%2520APIs%2520to%2520incorporate%2520external%2520knowledge%252C%2520enabling%2520the%2520system%2520to%2520better%2520handle%2520complex%2520queries%2520and%2520multi-turn%2520conversations.%2520Our%2520approach%2520achieved%25201st%2520place%2520in%2520Task%25201%2520with%2520a%2520significant%2520lead%2520of%252052.38%2525%252C%2520and%25203rd%2520place%2520in%2520Task%25203%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%2520integration%2520of%2520curriculum%2520learning%2520with%2520reinforcement%2520learning%2520in%2520our%2520training%2520pipeline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10337v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Curriculum%20Learning%20Approach%20to%20Reinforcement%20Learning%3A%20Leveraging%20RAG%20for%20Multimodal%20Question%20Answering&entry.906535625=Chenliang%20Zhang%20and%20Lin%20Wang%20and%20Yuanyuan%20Lu%20and%20Yusheng%20Qi%20and%20Kexin%20Wang%20and%20Peixu%20Hou%20and%20Wenshi%20Chen&entry.1292438233=This%20paper%20describes%20the%20solutions%20of%20the%20Dianping-Trust-Safety%20team%20for%20the%20META%20CRAG-MM%20challenge.%20The%20challenge%20requires%20building%20a%20comprehensive%20retrieval-augmented%20generation%20system%20capable%20for%20multi-modal%20multi-turn%20question%20answering.%20The%20competition%20consists%20of%20three%20tasks%3A%20%281%29%20answering%20questions%20using%20structured%20data%20retrieved%20from%20an%20image-based%20mock%20knowledge%20graph%2C%20%282%29%20synthesizing%20information%20from%20both%20knowledge%20graphs%20and%20web%20search%20results%2C%20and%20%283%29%20handling%20multi-turn%20conversations%20that%20require%20context%20understanding%20and%20information%20aggregation%20from%20multiple%20sources.%20For%20Task%201%2C%20our%20solution%20is%20based%20on%20the%20vision%20large%20language%20model%2C%20enhanced%20by%20supervised%20fine-tuning%20with%20knowledge%20distilled%20from%20GPT-4.1.%20We%20further%20applied%20curriculum%20learning%20strategies%20to%20guide%20reinforcement%20learning%2C%20resulting%20in%20improved%20answer%20accuracy%20and%20reduced%20hallucination.%20For%20Task%202%20and%20Task%203%2C%20we%20additionally%20leveraged%20web%20search%20APIs%20to%20incorporate%20external%20knowledge%2C%20enabling%20the%20system%20to%20better%20handle%20complex%20queries%20and%20multi-turn%20conversations.%20Our%20approach%20achieved%201st%20place%20in%20Task%201%20with%20a%20significant%20lead%20of%2052.38%25%2C%20and%203rd%20place%20in%20Task%203%2C%20demonstrating%20the%20effectiveness%20of%20the%20integration%20of%20curriculum%20learning%20with%20reinforcement%20learning%20in%20our%20training%20pipeline.&entry.1838667208=http%3A//arxiv.org/abs/2508.10337v2&entry.124074799=Read"},
{"title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "author": "Yuhan Cao and Zian Chen and Kun Quan and Ziliang Zhang and Yu Wang and Xiaoning Dong and Yeqi Feng and Guanzhong He and Jingcheng Huang and Jianhao Li and Yixuan Tan and Jiafu Tang and Yilin Tang and Junlei Wu and Qianyu Xiao and Can Zheng and Shouchen Zhou and Yuxiang Zhu and Yiming Huang and Tianxing He", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.", "link": "http://arxiv.org/abs/2506.06821v4", "date": "2026-01-14", "relevancy": 1.7671, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4469}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Generate%20Reliable%20Test%20Case%20Generators%3F%20A%20Study%20on%20Competition-Level%20Programming%20Problems&body=Title%3A%20Can%20LLMs%20Generate%20Reliable%20Test%20Case%20Generators%3F%20A%20Study%20on%20Competition-Level%20Programming%20Problems%0AAuthor%3A%20Yuhan%20Cao%20and%20Zian%20Chen%20and%20Kun%20Quan%20and%20Ziliang%20Zhang%20and%20Yu%20Wang%20and%20Xiaoning%20Dong%20and%20Yeqi%20Feng%20and%20Guanzhong%20He%20and%20Jingcheng%20Huang%20and%20Jianhao%20Li%20and%20Yixuan%20Tan%20and%20Jiafu%20Tang%20and%20Yilin%20Tang%20and%20Junlei%20Wu%20and%20Qianyu%20Xiao%20and%20Can%20Zheng%20and%20Shouchen%20Zhou%20and%20Yuxiang%20Zhu%20and%20Yiming%20Huang%20and%20Tianxing%20He%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20code%20generation%2C%20capable%20of%20tackling%20complex%20tasks%20during%20inference.%20However%2C%20the%20extent%20to%20which%20LLMs%20can%20be%20utilized%20for%20code%20checking%20or%20debugging%20through%20test%20case%20generation%20remains%20largely%20unexplored.%20We%20investigate%20this%20problem%20from%20the%20perspective%20of%20competition-level%20programming%20%28CP%29%20programs%20and%20propose%20TCGBench%2C%20a%20Benchmark%20for%20%28LLM%20generation%20of%29%20Test%20Case%20Generators.%20This%20benchmark%20comprises%20two%20tasks%2C%20aimed%20at%20studying%20the%20capabilities%20of%20LLMs%20in%20%281%29%20generating%20valid%20test%20case%20generators%20for%20a%20given%20CP%20problem%2C%20and%20further%20%282%29%20generating%20targeted%20test%20case%20generators%20that%20expose%20bugs%20in%20human-written%20code.%20Experimental%20results%20indicate%20that%20while%20state-of-the-art%20LLMs%20can%20generate%20valid%20test%20case%20generators%20in%20most%20cases%2C%20most%20LLMs%20struggle%20to%20generate%20targeted%20test%20cases%20that%20reveal%20flaws%20in%20human%20code%20effectively.%20Especially%2C%20even%20advanced%20reasoning%20models%20%28e.g.%2C%20o3-mini%29%20fall%20significantly%20short%20of%20human%20performance%20in%20the%20task%20of%20generating%20targeted%20generators.%20Furthermore%2C%20we%20construct%20a%20high-quality%2C%20manually%20curated%20dataset%20of%20instructions%20for%20generating%20targeted%20generators.%20Analysis%20demonstrates%20that%20the%20performance%20of%20LLMs%20can%20be%20enhanced%20with%20the%20aid%20of%20this%20dataset%2C%20by%20both%20prompting%20and%20fine-tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06821v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Generate%2520Reliable%2520Test%2520Case%2520Generators%253F%2520A%2520Study%2520on%2520Competition-Level%2520Programming%2520Problems%26entry.906535625%3DYuhan%2520Cao%2520and%2520Zian%2520Chen%2520and%2520Kun%2520Quan%2520and%2520Ziliang%2520Zhang%2520and%2520Yu%2520Wang%2520and%2520Xiaoning%2520Dong%2520and%2520Yeqi%2520Feng%2520and%2520Guanzhong%2520He%2520and%2520Jingcheng%2520Huang%2520and%2520Jianhao%2520Li%2520and%2520Yixuan%2520Tan%2520and%2520Jiafu%2520Tang%2520and%2520Yilin%2520Tang%2520and%2520Junlei%2520Wu%2520and%2520Qianyu%2520Xiao%2520and%2520Can%2520Zheng%2520and%2520Shouchen%2520Zhou%2520and%2520Yuxiang%2520Zhu%2520and%2520Yiming%2520Huang%2520and%2520Tianxing%2520He%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520code%2520generation%252C%2520capable%2520of%2520tackling%2520complex%2520tasks%2520during%2520inference.%2520However%252C%2520the%2520extent%2520to%2520which%2520LLMs%2520can%2520be%2520utilized%2520for%2520code%2520checking%2520or%2520debugging%2520through%2520test%2520case%2520generation%2520remains%2520largely%2520unexplored.%2520We%2520investigate%2520this%2520problem%2520from%2520the%2520perspective%2520of%2520competition-level%2520programming%2520%2528CP%2529%2520programs%2520and%2520propose%2520TCGBench%252C%2520a%2520Benchmark%2520for%2520%2528LLM%2520generation%2520of%2529%2520Test%2520Case%2520Generators.%2520This%2520benchmark%2520comprises%2520two%2520tasks%252C%2520aimed%2520at%2520studying%2520the%2520capabilities%2520of%2520LLMs%2520in%2520%25281%2529%2520generating%2520valid%2520test%2520case%2520generators%2520for%2520a%2520given%2520CP%2520problem%252C%2520and%2520further%2520%25282%2529%2520generating%2520targeted%2520test%2520case%2520generators%2520that%2520expose%2520bugs%2520in%2520human-written%2520code.%2520Experimental%2520results%2520indicate%2520that%2520while%2520state-of-the-art%2520LLMs%2520can%2520generate%2520valid%2520test%2520case%2520generators%2520in%2520most%2520cases%252C%2520most%2520LLMs%2520struggle%2520to%2520generate%2520targeted%2520test%2520cases%2520that%2520reveal%2520flaws%2520in%2520human%2520code%2520effectively.%2520Especially%252C%2520even%2520advanced%2520reasoning%2520models%2520%2528e.g.%252C%2520o3-mini%2529%2520fall%2520significantly%2520short%2520of%2520human%2520performance%2520in%2520the%2520task%2520of%2520generating%2520targeted%2520generators.%2520Furthermore%252C%2520we%2520construct%2520a%2520high-quality%252C%2520manually%2520curated%2520dataset%2520of%2520instructions%2520for%2520generating%2520targeted%2520generators.%2520Analysis%2520demonstrates%2520that%2520the%2520performance%2520of%2520LLMs%2520can%2520be%2520enhanced%2520with%2520the%2520aid%2520of%2520this%2520dataset%252C%2520by%2520both%2520prompting%2520and%2520fine-tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06821v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Generate%20Reliable%20Test%20Case%20Generators%3F%20A%20Study%20on%20Competition-Level%20Programming%20Problems&entry.906535625=Yuhan%20Cao%20and%20Zian%20Chen%20and%20Kun%20Quan%20and%20Ziliang%20Zhang%20and%20Yu%20Wang%20and%20Xiaoning%20Dong%20and%20Yeqi%20Feng%20and%20Guanzhong%20He%20and%20Jingcheng%20Huang%20and%20Jianhao%20Li%20and%20Yixuan%20Tan%20and%20Jiafu%20Tang%20and%20Yilin%20Tang%20and%20Junlei%20Wu%20and%20Qianyu%20Xiao%20and%20Can%20Zheng%20and%20Shouchen%20Zhou%20and%20Yuxiang%20Zhu%20and%20Yiming%20Huang%20and%20Tianxing%20He&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%20code%20generation%2C%20capable%20of%20tackling%20complex%20tasks%20during%20inference.%20However%2C%20the%20extent%20to%20which%20LLMs%20can%20be%20utilized%20for%20code%20checking%20or%20debugging%20through%20test%20case%20generation%20remains%20largely%20unexplored.%20We%20investigate%20this%20problem%20from%20the%20perspective%20of%20competition-level%20programming%20%28CP%29%20programs%20and%20propose%20TCGBench%2C%20a%20Benchmark%20for%20%28LLM%20generation%20of%29%20Test%20Case%20Generators.%20This%20benchmark%20comprises%20two%20tasks%2C%20aimed%20at%20studying%20the%20capabilities%20of%20LLMs%20in%20%281%29%20generating%20valid%20test%20case%20generators%20for%20a%20given%20CP%20problem%2C%20and%20further%20%282%29%20generating%20targeted%20test%20case%20generators%20that%20expose%20bugs%20in%20human-written%20code.%20Experimental%20results%20indicate%20that%20while%20state-of-the-art%20LLMs%20can%20generate%20valid%20test%20case%20generators%20in%20most%20cases%2C%20most%20LLMs%20struggle%20to%20generate%20targeted%20test%20cases%20that%20reveal%20flaws%20in%20human%20code%20effectively.%20Especially%2C%20even%20advanced%20reasoning%20models%20%28e.g.%2C%20o3-mini%29%20fall%20significantly%20short%20of%20human%20performance%20in%20the%20task%20of%20generating%20targeted%20generators.%20Furthermore%2C%20we%20construct%20a%20high-quality%2C%20manually%20curated%20dataset%20of%20instructions%20for%20generating%20targeted%20generators.%20Analysis%20demonstrates%20that%20the%20performance%20of%20LLMs%20can%20be%20enhanced%20with%20the%20aid%20of%20this%20dataset%2C%20by%20both%20prompting%20and%20fine-tuning.&entry.1838667208=http%3A//arxiv.org/abs/2506.06821v4&entry.124074799=Read"},
{"title": "Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments", "author": "Benjamin Bogenberger and Oliver Harrison and Orrin Dahanaggamaarachchi and Lukas Brunke and Jingxing Qian and Siqi Zhou and Angela P. Schoellig", "abstract": "Robots deployed in real-world environments, such as homes, must not only navigate safely but also understand their surroundings and adapt to changes in the environment. To perform tasks efficiently, they must build and maintain a semantic map that accurately reflects the current state of the environment. Existing research on semantic exploration largely focuses on static scenes without persistent object-level instance tracking. In this work, we propose an open-vocabulary, semantic exploration system for semi-static environments. Our system maintains a consistent map by building a probabilistic model of object instance stationarity, systematically tracking semi-static changes, and actively exploring areas that have not been visited for an extended period. In addition to active map maintenance, our approach leverages the map's semantic richness with large language model (LLM)-based reasoning for open-vocabulary object-goal navigation. This enables the robot to search more efficiently by prioritizing contextually relevant areas. We compare our approach against state-of-the-art baselines using publicly available object navigation and mapping datasets, and we further demonstrate real-world transferability in three real-world environments. Our approach outperforms the compared baselines in both success rate and search efficiency for object-navigation tasks and can more reliably handle changes in mapping semi-static environments. In real-world experiments, our system detects 95% of map changes on average, improving efficiency by more than 29% as compared to random and patrol strategies.", "link": "http://arxiv.org/abs/2509.19851v2", "date": "2026-01-14", "relevancy": 1.9202, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.7006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20Did%20I%20Leave%20My%20Glasses%3F%20Open-Vocabulary%20Semantic%20Exploration%20in%20Real-World%20Semi-Static%20Environments&body=Title%3A%20Where%20Did%20I%20Leave%20My%20Glasses%3F%20Open-Vocabulary%20Semantic%20Exploration%20in%20Real-World%20Semi-Static%20Environments%0AAuthor%3A%20Benjamin%20Bogenberger%20and%20Oliver%20Harrison%20and%20Orrin%20Dahanaggamaarachchi%20and%20Lukas%20Brunke%20and%20Jingxing%20Qian%20and%20Siqi%20Zhou%20and%20Angela%20P.%20Schoellig%0AAbstract%3A%20Robots%20deployed%20in%20real-world%20environments%2C%20such%20as%20homes%2C%20must%20not%20only%20navigate%20safely%20but%20also%20understand%20their%20surroundings%20and%20adapt%20to%20changes%20in%20the%20environment.%20To%20perform%20tasks%20efficiently%2C%20they%20must%20build%20and%20maintain%20a%20semantic%20map%20that%20accurately%20reflects%20the%20current%20state%20of%20the%20environment.%20Existing%20research%20on%20semantic%20exploration%20largely%20focuses%20on%20static%20scenes%20without%20persistent%20object-level%20instance%20tracking.%20In%20this%20work%2C%20we%20propose%20an%20open-vocabulary%2C%20semantic%20exploration%20system%20for%20semi-static%20environments.%20Our%20system%20maintains%20a%20consistent%20map%20by%20building%20a%20probabilistic%20model%20of%20object%20instance%20stationarity%2C%20systematically%20tracking%20semi-static%20changes%2C%20and%20actively%20exploring%20areas%20that%20have%20not%20been%20visited%20for%20an%20extended%20period.%20In%20addition%20to%20active%20map%20maintenance%2C%20our%20approach%20leverages%20the%20map%27s%20semantic%20richness%20with%20large%20language%20model%20%28LLM%29-based%20reasoning%20for%20open-vocabulary%20object-goal%20navigation.%20This%20enables%20the%20robot%20to%20search%20more%20efficiently%20by%20prioritizing%20contextually%20relevant%20areas.%20We%20compare%20our%20approach%20against%20state-of-the-art%20baselines%20using%20publicly%20available%20object%20navigation%20and%20mapping%20datasets%2C%20and%20we%20further%20demonstrate%20real-world%20transferability%20in%20three%20real-world%20environments.%20Our%20approach%20outperforms%20the%20compared%20baselines%20in%20both%20success%20rate%20and%20search%20efficiency%20for%20object-navigation%20tasks%20and%20can%20more%20reliably%20handle%20changes%20in%20mapping%20semi-static%20environments.%20In%20real-world%20experiments%2C%20our%20system%20detects%2095%25%20of%20map%20changes%20on%20average%2C%20improving%20efficiency%20by%20more%20than%2029%25%20as%20compared%20to%20random%20and%20patrol%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520Did%2520I%2520Leave%2520My%2520Glasses%253F%2520Open-Vocabulary%2520Semantic%2520Exploration%2520in%2520Real-World%2520Semi-Static%2520Environments%26entry.906535625%3DBenjamin%2520Bogenberger%2520and%2520Oliver%2520Harrison%2520and%2520Orrin%2520Dahanaggamaarachchi%2520and%2520Lukas%2520Brunke%2520and%2520Jingxing%2520Qian%2520and%2520Siqi%2520Zhou%2520and%2520Angela%2520P.%2520Schoellig%26entry.1292438233%3DRobots%2520deployed%2520in%2520real-world%2520environments%252C%2520such%2520as%2520homes%252C%2520must%2520not%2520only%2520navigate%2520safely%2520but%2520also%2520understand%2520their%2520surroundings%2520and%2520adapt%2520to%2520changes%2520in%2520the%2520environment.%2520To%2520perform%2520tasks%2520efficiently%252C%2520they%2520must%2520build%2520and%2520maintain%2520a%2520semantic%2520map%2520that%2520accurately%2520reflects%2520the%2520current%2520state%2520of%2520the%2520environment.%2520Existing%2520research%2520on%2520semantic%2520exploration%2520largely%2520focuses%2520on%2520static%2520scenes%2520without%2520persistent%2520object-level%2520instance%2520tracking.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520open-vocabulary%252C%2520semantic%2520exploration%2520system%2520for%2520semi-static%2520environments.%2520Our%2520system%2520maintains%2520a%2520consistent%2520map%2520by%2520building%2520a%2520probabilistic%2520model%2520of%2520object%2520instance%2520stationarity%252C%2520systematically%2520tracking%2520semi-static%2520changes%252C%2520and%2520actively%2520exploring%2520areas%2520that%2520have%2520not%2520been%2520visited%2520for%2520an%2520extended%2520period.%2520In%2520addition%2520to%2520active%2520map%2520maintenance%252C%2520our%2520approach%2520leverages%2520the%2520map%2527s%2520semantic%2520richness%2520with%2520large%2520language%2520model%2520%2528LLM%2529-based%2520reasoning%2520for%2520open-vocabulary%2520object-goal%2520navigation.%2520This%2520enables%2520the%2520robot%2520to%2520search%2520more%2520efficiently%2520by%2520prioritizing%2520contextually%2520relevant%2520areas.%2520We%2520compare%2520our%2520approach%2520against%2520state-of-the-art%2520baselines%2520using%2520publicly%2520available%2520object%2520navigation%2520and%2520mapping%2520datasets%252C%2520and%2520we%2520further%2520demonstrate%2520real-world%2520transferability%2520in%2520three%2520real-world%2520environments.%2520Our%2520approach%2520outperforms%2520the%2520compared%2520baselines%2520in%2520both%2520success%2520rate%2520and%2520search%2520efficiency%2520for%2520object-navigation%2520tasks%2520and%2520can%2520more%2520reliably%2520handle%2520changes%2520in%2520mapping%2520semi-static%2520environments.%2520In%2520real-world%2520experiments%252C%2520our%2520system%2520detects%252095%2525%2520of%2520map%2520changes%2520on%2520average%252C%2520improving%2520efficiency%2520by%2520more%2520than%252029%2525%2520as%2520compared%2520to%2520random%2520and%2520patrol%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20Did%20I%20Leave%20My%20Glasses%3F%20Open-Vocabulary%20Semantic%20Exploration%20in%20Real-World%20Semi-Static%20Environments&entry.906535625=Benjamin%20Bogenberger%20and%20Oliver%20Harrison%20and%20Orrin%20Dahanaggamaarachchi%20and%20Lukas%20Brunke%20and%20Jingxing%20Qian%20and%20Siqi%20Zhou%20and%20Angela%20P.%20Schoellig&entry.1292438233=Robots%20deployed%20in%20real-world%20environments%2C%20such%20as%20homes%2C%20must%20not%20only%20navigate%20safely%20but%20also%20understand%20their%20surroundings%20and%20adapt%20to%20changes%20in%20the%20environment.%20To%20perform%20tasks%20efficiently%2C%20they%20must%20build%20and%20maintain%20a%20semantic%20map%20that%20accurately%20reflects%20the%20current%20state%20of%20the%20environment.%20Existing%20research%20on%20semantic%20exploration%20largely%20focuses%20on%20static%20scenes%20without%20persistent%20object-level%20instance%20tracking.%20In%20this%20work%2C%20we%20propose%20an%20open-vocabulary%2C%20semantic%20exploration%20system%20for%20semi-static%20environments.%20Our%20system%20maintains%20a%20consistent%20map%20by%20building%20a%20probabilistic%20model%20of%20object%20instance%20stationarity%2C%20systematically%20tracking%20semi-static%20changes%2C%20and%20actively%20exploring%20areas%20that%20have%20not%20been%20visited%20for%20an%20extended%20period.%20In%20addition%20to%20active%20map%20maintenance%2C%20our%20approach%20leverages%20the%20map%27s%20semantic%20richness%20with%20large%20language%20model%20%28LLM%29-based%20reasoning%20for%20open-vocabulary%20object-goal%20navigation.%20This%20enables%20the%20robot%20to%20search%20more%20efficiently%20by%20prioritizing%20contextually%20relevant%20areas.%20We%20compare%20our%20approach%20against%20state-of-the-art%20baselines%20using%20publicly%20available%20object%20navigation%20and%20mapping%20datasets%2C%20and%20we%20further%20demonstrate%20real-world%20transferability%20in%20three%20real-world%20environments.%20Our%20approach%20outperforms%20the%20compared%20baselines%20in%20both%20success%20rate%20and%20search%20efficiency%20for%20object-navigation%20tasks%20and%20can%20more%20reliably%20handle%20changes%20in%20mapping%20semi-static%20environments.%20In%20real-world%20experiments%2C%20our%20system%20detects%2095%25%20of%20map%20changes%20on%20average%2C%20improving%20efficiency%20by%20more%20than%2029%25%20as%20compared%20to%20random%20and%20patrol%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2509.19851v2&entry.124074799=Read"},
{"title": "Input Convex Kolmogorov Arnold Networks", "author": "Thomas Deschatre and Xavier Warin", "abstract": "This article presents an input convex neural network architecture using Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the first is based on a low-order, linear-by-part, representation of functions, and a universal approximation theorem is provided. The second is based on cubic splines, for which only numerical results support convergence. We demonstrate on simple tests that these networks perform competitively with classical input convex neural networks (ICNNs). In a second part, we use the networks to solve some optimal transport problems needing a convex approximation of functions and demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs produce results similar to those of classical ICNNs.", "link": "http://arxiv.org/abs/2505.21208v2", "date": "2026-01-14", "relevancy": 1.7053, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Input%20Convex%20Kolmogorov%20Arnold%20Networks&body=Title%3A%20Input%20Convex%20Kolmogorov%20Arnold%20Networks%0AAuthor%3A%20Thomas%20Deschatre%20and%20Xavier%20Warin%0AAbstract%3A%20This%20article%20presents%20an%20input%20convex%20neural%20network%20architecture%20using%20Kolmogorov-Arnold%20networks%20%28ICKAN%29.%20Two%20specific%20networks%20are%20presented%3A%20the%20first%20is%20based%20on%20a%20low-order%2C%20linear-by-part%2C%20representation%20of%20functions%2C%20and%20a%20universal%20approximation%20theorem%20is%20provided.%20The%20second%20is%20based%20on%20cubic%20splines%2C%20for%20which%20only%20numerical%20results%20support%20convergence.%20We%20demonstrate%20on%20simple%20tests%20that%20these%20networks%20perform%20competitively%20with%20classical%20input%20convex%20neural%20networks%20%28ICNNs%29.%20In%20a%20second%20part%2C%20we%20use%20the%20networks%20to%20solve%20some%20optimal%20transport%20problems%20needing%20a%20convex%20approximation%20of%20functions%20and%20demonstrate%20their%20effectiveness.%20Comparisons%20with%20ICNNs%20show%20that%20cubic%20ICKANs%20produce%20results%20similar%20to%20those%20of%20classical%20ICNNs.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInput%2520Convex%2520Kolmogorov%2520Arnold%2520Networks%26entry.906535625%3DThomas%2520Deschatre%2520and%2520Xavier%2520Warin%26entry.1292438233%3DThis%2520article%2520presents%2520an%2520input%2520convex%2520neural%2520network%2520architecture%2520using%2520Kolmogorov-Arnold%2520networks%2520%2528ICKAN%2529.%2520Two%2520specific%2520networks%2520are%2520presented%253A%2520the%2520first%2520is%2520based%2520on%2520a%2520low-order%252C%2520linear-by-part%252C%2520representation%2520of%2520functions%252C%2520and%2520a%2520universal%2520approximation%2520theorem%2520is%2520provided.%2520The%2520second%2520is%2520based%2520on%2520cubic%2520splines%252C%2520for%2520which%2520only%2520numerical%2520results%2520support%2520convergence.%2520We%2520demonstrate%2520on%2520simple%2520tests%2520that%2520these%2520networks%2520perform%2520competitively%2520with%2520classical%2520input%2520convex%2520neural%2520networks%2520%2528ICNNs%2529.%2520In%2520a%2520second%2520part%252C%2520we%2520use%2520the%2520networks%2520to%2520solve%2520some%2520optimal%2520transport%2520problems%2520needing%2520a%2520convex%2520approximation%2520of%2520functions%2520and%2520demonstrate%2520their%2520effectiveness.%2520Comparisons%2520with%2520ICNNs%2520show%2520that%2520cubic%2520ICKANs%2520produce%2520results%2520similar%2520to%2520those%2520of%2520classical%2520ICNNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Input%20Convex%20Kolmogorov%20Arnold%20Networks&entry.906535625=Thomas%20Deschatre%20and%20Xavier%20Warin&entry.1292438233=This%20article%20presents%20an%20input%20convex%20neural%20network%20architecture%20using%20Kolmogorov-Arnold%20networks%20%28ICKAN%29.%20Two%20specific%20networks%20are%20presented%3A%20the%20first%20is%20based%20on%20a%20low-order%2C%20linear-by-part%2C%20representation%20of%20functions%2C%20and%20a%20universal%20approximation%20theorem%20is%20provided.%20The%20second%20is%20based%20on%20cubic%20splines%2C%20for%20which%20only%20numerical%20results%20support%20convergence.%20We%20demonstrate%20on%20simple%20tests%20that%20these%20networks%20perform%20competitively%20with%20classical%20input%20convex%20neural%20networks%20%28ICNNs%29.%20In%20a%20second%20part%2C%20we%20use%20the%20networks%20to%20solve%20some%20optimal%20transport%20problems%20needing%20a%20convex%20approximation%20of%20functions%20and%20demonstrate%20their%20effectiveness.%20Comparisons%20with%20ICNNs%20show%20that%20cubic%20ICKANs%20produce%20results%20similar%20to%20those%20of%20classical%20ICNNs.&entry.1838667208=http%3A//arxiv.org/abs/2505.21208v2&entry.124074799=Read"},
{"title": "Analysis of Quantum Image Representations for Supervised Classification", "author": "Marco Parigi and Mehran Khosrojerdi and Filippo Caruso and Leonardo Banchi", "abstract": "In the era of big data and artificial intelligence, the increasing volume of data and the demand to solve more and more complex computational challenges are two driving forces for improving the efficiency of data storage, processing and analysis. Quantum image processing (QIP) is an interdisciplinary field between quantum information science and image processing, which has the potential to alleviate some of these challenges by leveraging the power of quantum computing. In this work, we compare and examine the compression properties of four different Quantum Image Representations (QImRs): namely, Tensor Network Representation (TNR), Flexible Representation of Quantum Image (FRQI), Novel Enhanced Quantum Representation NEQR, and Quantum Probability Image Encoding (QPIE). Our simulations show that FRQI and QPIE perform a higher compression of image information than TNR and NEQR. Furthermore, we investigate the trade-off between accuracy and memory in binary classification problems, evaluating the performance of quantum kernels based on QImRs compared to the classical linear kernel. Our results indicate that quantum kernels provide comparable classification average accuracy but require exponentially fewer resources for image storage.", "link": "http://arxiv.org/abs/2507.22039v2", "date": "2026-01-14", "relevancy": 1.9477, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4989}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Quantum%20Image%20Representations%20for%20Supervised%20Classification&body=Title%3A%20Analysis%20of%20Quantum%20Image%20Representations%20for%20Supervised%20Classification%0AAuthor%3A%20Marco%20Parigi%20and%20Mehran%20Khosrojerdi%20and%20Filippo%20Caruso%20and%20Leonardo%20Banchi%0AAbstract%3A%20In%20the%20era%20of%20big%20data%20and%20artificial%20intelligence%2C%20the%20increasing%20volume%20of%20data%20and%20the%20demand%20to%20solve%20more%20and%20more%20complex%20computational%20challenges%20are%20two%20driving%20forces%20for%20improving%20the%20efficiency%20of%20data%20storage%2C%20processing%20and%20analysis.%20Quantum%20image%20processing%20%28QIP%29%20is%20an%20interdisciplinary%20field%20between%20quantum%20information%20science%20and%20image%20processing%2C%20which%20has%20the%20potential%20to%20alleviate%20some%20of%20these%20challenges%20by%20leveraging%20the%20power%20of%20quantum%20computing.%20In%20this%20work%2C%20we%20compare%20and%20examine%20the%20compression%20properties%20of%20four%20different%20Quantum%20Image%20Representations%20%28QImRs%29%3A%20namely%2C%20Tensor%20Network%20Representation%20%28TNR%29%2C%20Flexible%20Representation%20of%20Quantum%20Image%20%28FRQI%29%2C%20Novel%20Enhanced%20Quantum%20Representation%20NEQR%2C%20and%20Quantum%20Probability%20Image%20Encoding%20%28QPIE%29.%20Our%20simulations%20show%20that%20FRQI%20and%20QPIE%20perform%20a%20higher%20compression%20of%20image%20information%20than%20TNR%20and%20NEQR.%20Furthermore%2C%20we%20investigate%20the%20trade-off%20between%20accuracy%20and%20memory%20in%20binary%20classification%20problems%2C%20evaluating%20the%20performance%20of%20quantum%20kernels%20based%20on%20QImRs%20compared%20to%20the%20classical%20linear%20kernel.%20Our%20results%20indicate%20that%20quantum%20kernels%20provide%20comparable%20classification%20average%20accuracy%20but%20require%20exponentially%20fewer%20resources%20for%20image%20storage.%0ALink%3A%20http%3A//arxiv.org/abs/2507.22039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Quantum%2520Image%2520Representations%2520for%2520Supervised%2520Classification%26entry.906535625%3DMarco%2520Parigi%2520and%2520Mehran%2520Khosrojerdi%2520and%2520Filippo%2520Caruso%2520and%2520Leonardo%2520Banchi%26entry.1292438233%3DIn%2520the%2520era%2520of%2520big%2520data%2520and%2520artificial%2520intelligence%252C%2520the%2520increasing%2520volume%2520of%2520data%2520and%2520the%2520demand%2520to%2520solve%2520more%2520and%2520more%2520complex%2520computational%2520challenges%2520are%2520two%2520driving%2520forces%2520for%2520improving%2520the%2520efficiency%2520of%2520data%2520storage%252C%2520processing%2520and%2520analysis.%2520Quantum%2520image%2520processing%2520%2528QIP%2529%2520is%2520an%2520interdisciplinary%2520field%2520between%2520quantum%2520information%2520science%2520and%2520image%2520processing%252C%2520which%2520has%2520the%2520potential%2520to%2520alleviate%2520some%2520of%2520these%2520challenges%2520by%2520leveraging%2520the%2520power%2520of%2520quantum%2520computing.%2520In%2520this%2520work%252C%2520we%2520compare%2520and%2520examine%2520the%2520compression%2520properties%2520of%2520four%2520different%2520Quantum%2520Image%2520Representations%2520%2528QImRs%2529%253A%2520namely%252C%2520Tensor%2520Network%2520Representation%2520%2528TNR%2529%252C%2520Flexible%2520Representation%2520of%2520Quantum%2520Image%2520%2528FRQI%2529%252C%2520Novel%2520Enhanced%2520Quantum%2520Representation%2520NEQR%252C%2520and%2520Quantum%2520Probability%2520Image%2520Encoding%2520%2528QPIE%2529.%2520Our%2520simulations%2520show%2520that%2520FRQI%2520and%2520QPIE%2520perform%2520a%2520higher%2520compression%2520of%2520image%2520information%2520than%2520TNR%2520and%2520NEQR.%2520Furthermore%252C%2520we%2520investigate%2520the%2520trade-off%2520between%2520accuracy%2520and%2520memory%2520in%2520binary%2520classification%2520problems%252C%2520evaluating%2520the%2520performance%2520of%2520quantum%2520kernels%2520based%2520on%2520QImRs%2520compared%2520to%2520the%2520classical%2520linear%2520kernel.%2520Our%2520results%2520indicate%2520that%2520quantum%2520kernels%2520provide%2520comparable%2520classification%2520average%2520accuracy%2520but%2520require%2520exponentially%2520fewer%2520resources%2520for%2520image%2520storage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Quantum%20Image%20Representations%20for%20Supervised%20Classification&entry.906535625=Marco%20Parigi%20and%20Mehran%20Khosrojerdi%20and%20Filippo%20Caruso%20and%20Leonardo%20Banchi&entry.1292438233=In%20the%20era%20of%20big%20data%20and%20artificial%20intelligence%2C%20the%20increasing%20volume%20of%20data%20and%20the%20demand%20to%20solve%20more%20and%20more%20complex%20computational%20challenges%20are%20two%20driving%20forces%20for%20improving%20the%20efficiency%20of%20data%20storage%2C%20processing%20and%20analysis.%20Quantum%20image%20processing%20%28QIP%29%20is%20an%20interdisciplinary%20field%20between%20quantum%20information%20science%20and%20image%20processing%2C%20which%20has%20the%20potential%20to%20alleviate%20some%20of%20these%20challenges%20by%20leveraging%20the%20power%20of%20quantum%20computing.%20In%20this%20work%2C%20we%20compare%20and%20examine%20the%20compression%20properties%20of%20four%20different%20Quantum%20Image%20Representations%20%28QImRs%29%3A%20namely%2C%20Tensor%20Network%20Representation%20%28TNR%29%2C%20Flexible%20Representation%20of%20Quantum%20Image%20%28FRQI%29%2C%20Novel%20Enhanced%20Quantum%20Representation%20NEQR%2C%20and%20Quantum%20Probability%20Image%20Encoding%20%28QPIE%29.%20Our%20simulations%20show%20that%20FRQI%20and%20QPIE%20perform%20a%20higher%20compression%20of%20image%20information%20than%20TNR%20and%20NEQR.%20Furthermore%2C%20we%20investigate%20the%20trade-off%20between%20accuracy%20and%20memory%20in%20binary%20classification%20problems%2C%20evaluating%20the%20performance%20of%20quantum%20kernels%20based%20on%20QImRs%20compared%20to%20the%20classical%20linear%20kernel.%20Our%20results%20indicate%20that%20quantum%20kernels%20provide%20comparable%20classification%20average%20accuracy%20but%20require%20exponentially%20fewer%20resources%20for%20image%20storage.&entry.1838667208=http%3A//arxiv.org/abs/2507.22039v2&entry.124074799=Read"},
{"title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering", "author": "Shahana Yasmin Chowdhury and Bithi Banik and Md Tamjidul Hoque and Shreya Banerjee", "abstract": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of human-computer interaction (HCI) and the evolution of artificial intelligence (AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions: neutral, happy, sad, angry, fear, disgust, and surprise, which are trained on five datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C). The model achieves high accuracy on individual datasets, including 97.83% on RAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS and EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy, outperforming previously reported results. To our knowledge, no existing study has evaluated a single SER model across all five benchmark datasets (i.e., R+T+S+C+E) simultaneously. In our work, we introduce this comprehensive combination and achieve a remarkable overall accuracy of 93.76%. These results confirm the robustness and generalizability of our DCRF-BiLSTM framework across diverse datasets.", "link": "http://arxiv.org/abs/2507.07046v2", "date": "2026-01-14", "relevancy": 1.7862, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.458}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Hybrid%20Deep%20Learning%20Technique%20for%20Speech%20Emotion%20Detection%20using%20Feature%20Engineering&body=Title%3A%20A%20Novel%20Hybrid%20Deep%20Learning%20Technique%20for%20Speech%20Emotion%20Detection%20using%20Feature%20Engineering%0AAuthor%3A%20Shahana%20Yasmin%20Chowdhury%20and%20Bithi%20Banik%20and%20Md%20Tamjidul%20Hoque%20and%20Shreya%20Banerjee%0AAbstract%3A%20Nowadays%2C%20speech%20emotion%20recognition%20%28SER%29%20plays%20a%20vital%20role%20in%20the%20field%20of%20human-computer%20interaction%20%28HCI%29%20and%20the%20evolution%20of%20artificial%20intelligence%20%28AI%29.%20Our%20proposed%20DCRF-BiLSTM%20model%20is%20used%20to%20recognize%20seven%20emotions%3A%20neutral%2C%20happy%2C%20sad%2C%20angry%2C%20fear%2C%20disgust%2C%20and%20surprise%2C%20which%20are%20trained%20on%20five%20datasets%3A%20RAVDESS%20%28R%29%2C%20TESS%20%28T%29%2C%20SAVEE%20%28S%29%2C%20EmoDB%20%28E%29%2C%20and%20Crema-D%20%28C%29.%20The%20model%20achieves%20high%20accuracy%20on%20individual%20datasets%2C%20including%2097.83%25%20on%20RAVDESS%2C%2097.02%25%20on%20SAVEE%2C%2095.10%25%20for%20CREMA-D%2C%20and%20a%20perfect%20100%25%20on%20both%20TESS%20and%20EMO-DB.%20For%20the%20combined%20%28R%2BT%2BS%29%20datasets%2C%20it%20achieves%2098.82%25%20accuracy%2C%20outperforming%20previously%20reported%20results.%20To%20our%20knowledge%2C%20no%20existing%20study%20has%20evaluated%20a%20single%20SER%20model%20across%20all%20five%20benchmark%20datasets%20%28i.e.%2C%20R%2BT%2BS%2BC%2BE%29%20simultaneously.%20In%20our%20work%2C%20we%20introduce%20this%20comprehensive%20combination%20and%20achieve%20a%20remarkable%20overall%20accuracy%20of%2093.76%25.%20These%20results%20confirm%20the%20robustness%20and%20generalizability%20of%20our%20DCRF-BiLSTM%20framework%20across%20diverse%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2507.07046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Hybrid%2520Deep%2520Learning%2520Technique%2520for%2520Speech%2520Emotion%2520Detection%2520using%2520Feature%2520Engineering%26entry.906535625%3DShahana%2520Yasmin%2520Chowdhury%2520and%2520Bithi%2520Banik%2520and%2520Md%2520Tamjidul%2520Hoque%2520and%2520Shreya%2520Banerjee%26entry.1292438233%3DNowadays%252C%2520speech%2520emotion%2520recognition%2520%2528SER%2529%2520plays%2520a%2520vital%2520role%2520in%2520the%2520field%2520of%2520human-computer%2520interaction%2520%2528HCI%2529%2520and%2520the%2520evolution%2520of%2520artificial%2520intelligence%2520%2528AI%2529.%2520Our%2520proposed%2520DCRF-BiLSTM%2520model%2520is%2520used%2520to%2520recognize%2520seven%2520emotions%253A%2520neutral%252C%2520happy%252C%2520sad%252C%2520angry%252C%2520fear%252C%2520disgust%252C%2520and%2520surprise%252C%2520which%2520are%2520trained%2520on%2520five%2520datasets%253A%2520RAVDESS%2520%2528R%2529%252C%2520TESS%2520%2528T%2529%252C%2520SAVEE%2520%2528S%2529%252C%2520EmoDB%2520%2528E%2529%252C%2520and%2520Crema-D%2520%2528C%2529.%2520The%2520model%2520achieves%2520high%2520accuracy%2520on%2520individual%2520datasets%252C%2520including%252097.83%2525%2520on%2520RAVDESS%252C%252097.02%2525%2520on%2520SAVEE%252C%252095.10%2525%2520for%2520CREMA-D%252C%2520and%2520a%2520perfect%2520100%2525%2520on%2520both%2520TESS%2520and%2520EMO-DB.%2520For%2520the%2520combined%2520%2528R%252BT%252BS%2529%2520datasets%252C%2520it%2520achieves%252098.82%2525%2520accuracy%252C%2520outperforming%2520previously%2520reported%2520results.%2520To%2520our%2520knowledge%252C%2520no%2520existing%2520study%2520has%2520evaluated%2520a%2520single%2520SER%2520model%2520across%2520all%2520five%2520benchmark%2520datasets%2520%2528i.e.%252C%2520R%252BT%252BS%252BC%252BE%2529%2520simultaneously.%2520In%2520our%2520work%252C%2520we%2520introduce%2520this%2520comprehensive%2520combination%2520and%2520achieve%2520a%2520remarkable%2520overall%2520accuracy%2520of%252093.76%2525.%2520These%2520results%2520confirm%2520the%2520robustness%2520and%2520generalizability%2520of%2520our%2520DCRF-BiLSTM%2520framework%2520across%2520diverse%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Hybrid%20Deep%20Learning%20Technique%20for%20Speech%20Emotion%20Detection%20using%20Feature%20Engineering&entry.906535625=Shahana%20Yasmin%20Chowdhury%20and%20Bithi%20Banik%20and%20Md%20Tamjidul%20Hoque%20and%20Shreya%20Banerjee&entry.1292438233=Nowadays%2C%20speech%20emotion%20recognition%20%28SER%29%20plays%20a%20vital%20role%20in%20the%20field%20of%20human-computer%20interaction%20%28HCI%29%20and%20the%20evolution%20of%20artificial%20intelligence%20%28AI%29.%20Our%20proposed%20DCRF-BiLSTM%20model%20is%20used%20to%20recognize%20seven%20emotions%3A%20neutral%2C%20happy%2C%20sad%2C%20angry%2C%20fear%2C%20disgust%2C%20and%20surprise%2C%20which%20are%20trained%20on%20five%20datasets%3A%20RAVDESS%20%28R%29%2C%20TESS%20%28T%29%2C%20SAVEE%20%28S%29%2C%20EmoDB%20%28E%29%2C%20and%20Crema-D%20%28C%29.%20The%20model%20achieves%20high%20accuracy%20on%20individual%20datasets%2C%20including%2097.83%25%20on%20RAVDESS%2C%2097.02%25%20on%20SAVEE%2C%2095.10%25%20for%20CREMA-D%2C%20and%20a%20perfect%20100%25%20on%20both%20TESS%20and%20EMO-DB.%20For%20the%20combined%20%28R%2BT%2BS%29%20datasets%2C%20it%20achieves%2098.82%25%20accuracy%2C%20outperforming%20previously%20reported%20results.%20To%20our%20knowledge%2C%20no%20existing%20study%20has%20evaluated%20a%20single%20SER%20model%20across%20all%20five%20benchmark%20datasets%20%28i.e.%2C%20R%2BT%2BS%2BC%2BE%29%20simultaneously.%20In%20our%20work%2C%20we%20introduce%20this%20comprehensive%20combination%20and%20achieve%20a%20remarkable%20overall%20accuracy%20of%2093.76%25.%20These%20results%20confirm%20the%20robustness%20and%20generalizability%20of%20our%20DCRF-BiLSTM%20framework%20across%20diverse%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2507.07046v2&entry.124074799=Read"},
{"title": "Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?", "author": "David Reid and Ognjen Arandjelovic", "abstract": "Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.", "link": "http://arxiv.org/abs/2601.09433v1", "date": "2026-01-14", "relevancy": 1.9574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Transformers%20Understand%20Ancient%20Roman%20Coin%20Motifs%20Better%20than%20CNNs%3F&body=Title%3A%20Do%20Transformers%20Understand%20Ancient%20Roman%20Coin%20Motifs%20Better%20than%20CNNs%3F%0AAuthor%3A%20David%20Reid%20and%20Ognjen%20Arandjelovic%0AAbstract%3A%20Automated%20analysis%20of%20ancient%20coins%20has%20the%20potential%20to%20help%20researchers%20extract%20more%20historical%20insights%20from%20large%20collections%20of%20coins%20and%20to%20help%20collectors%20understand%20what%20they%20are%20buying%20or%20selling.%20Recent%20research%20in%20this%20area%20has%20shown%20promise%20in%20focusing%20on%20identification%20of%20semantic%20elements%20as%20they%20are%20commonly%20depicted%20on%20ancient%20coins%2C%20by%20using%20convolutional%20neural%20networks%20%28CNNs%29.%20This%20paper%20is%20the%20first%20to%20apply%20the%20recently%20proposed%20Vision%20Transformer%20%28ViT%29%20deep%20learning%20architecture%20to%20the%20task%20of%20identification%20of%20semantic%20elements%20on%20coins%2C%20using%20fully%20automatic%20learning%20from%20multi-modal%20data%20%28images%20and%20unstructured%20text%29.%20This%20article%20summarises%20previous%20research%20in%20the%20area%2C%20discusses%20the%20training%20and%20implementation%20of%20ViT%20and%20CNN%20models%20for%20ancient%20coins%20analysis%20and%20provides%20an%20evaluation%20of%20their%20performance.%20The%20ViT%20models%20were%20found%20to%20outperform%20the%20newly%20trained%20CNN%20models%20in%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Transformers%2520Understand%2520Ancient%2520Roman%2520Coin%2520Motifs%2520Better%2520than%2520CNNs%253F%26entry.906535625%3DDavid%2520Reid%2520and%2520Ognjen%2520Arandjelovic%26entry.1292438233%3DAutomated%2520analysis%2520of%2520ancient%2520coins%2520has%2520the%2520potential%2520to%2520help%2520researchers%2520extract%2520more%2520historical%2520insights%2520from%2520large%2520collections%2520of%2520coins%2520and%2520to%2520help%2520collectors%2520understand%2520what%2520they%2520are%2520buying%2520or%2520selling.%2520Recent%2520research%2520in%2520this%2520area%2520has%2520shown%2520promise%2520in%2520focusing%2520on%2520identification%2520of%2520semantic%2520elements%2520as%2520they%2520are%2520commonly%2520depicted%2520on%2520ancient%2520coins%252C%2520by%2520using%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520This%2520paper%2520is%2520the%2520first%2520to%2520apply%2520the%2520recently%2520proposed%2520Vision%2520Transformer%2520%2528ViT%2529%2520deep%2520learning%2520architecture%2520to%2520the%2520task%2520of%2520identification%2520of%2520semantic%2520elements%2520on%2520coins%252C%2520using%2520fully%2520automatic%2520learning%2520from%2520multi-modal%2520data%2520%2528images%2520and%2520unstructured%2520text%2529.%2520This%2520article%2520summarises%2520previous%2520research%2520in%2520the%2520area%252C%2520discusses%2520the%2520training%2520and%2520implementation%2520of%2520ViT%2520and%2520CNN%2520models%2520for%2520ancient%2520coins%2520analysis%2520and%2520provides%2520an%2520evaluation%2520of%2520their%2520performance.%2520The%2520ViT%2520models%2520were%2520found%2520to%2520outperform%2520the%2520newly%2520trained%2520CNN%2520models%2520in%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Transformers%20Understand%20Ancient%20Roman%20Coin%20Motifs%20Better%20than%20CNNs%3F&entry.906535625=David%20Reid%20and%20Ognjen%20Arandjelovic&entry.1292438233=Automated%20analysis%20of%20ancient%20coins%20has%20the%20potential%20to%20help%20researchers%20extract%20more%20historical%20insights%20from%20large%20collections%20of%20coins%20and%20to%20help%20collectors%20understand%20what%20they%20are%20buying%20or%20selling.%20Recent%20research%20in%20this%20area%20has%20shown%20promise%20in%20focusing%20on%20identification%20of%20semantic%20elements%20as%20they%20are%20commonly%20depicted%20on%20ancient%20coins%2C%20by%20using%20convolutional%20neural%20networks%20%28CNNs%29.%20This%20paper%20is%20the%20first%20to%20apply%20the%20recently%20proposed%20Vision%20Transformer%20%28ViT%29%20deep%20learning%20architecture%20to%20the%20task%20of%20identification%20of%20semantic%20elements%20on%20coins%2C%20using%20fully%20automatic%20learning%20from%20multi-modal%20data%20%28images%20and%20unstructured%20text%29.%20This%20article%20summarises%20previous%20research%20in%20the%20area%2C%20discusses%20the%20training%20and%20implementation%20of%20ViT%20and%20CNN%20models%20for%20ancient%20coins%20analysis%20and%20provides%20an%20evaluation%20of%20their%20performance.%20The%20ViT%20models%20were%20found%20to%20outperform%20the%20newly%20trained%20CNN%20models%20in%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.09433v1&entry.124074799=Read"},
{"title": "ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving", "author": "Xuemei Yao and Xiao Yang and Jianbin Sun and Liuwei Xie and Xuebin Shao and Xiyu Fang and Hang Su and Kewei Yang", "abstract": "Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.", "link": "http://arxiv.org/abs/2601.09377v1", "date": "2026-01-14", "relevancy": 1.5727, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5287}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5231}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReflexDiffusion%3A%20Reflection-Enhanced%20Trajectory%20Planning%20for%20High-lateral-acceleration%20Scenarios%20in%20Autonomous%20Driving&body=Title%3A%20ReflexDiffusion%3A%20Reflection-Enhanced%20Trajectory%20Planning%20for%20High-lateral-acceleration%20Scenarios%20in%20Autonomous%20Driving%0AAuthor%3A%20Xuemei%20Yao%20and%20Xiao%20Yang%20and%20Jianbin%20Sun%20and%20Liuwei%20Xie%20and%20Xuebin%20Shao%20and%20Xiyu%20Fang%20and%20Hang%20Su%20and%20Kewei%20Yang%0AAbstract%3A%20Generating%20safe%20and%20reliable%20trajectories%20for%20autonomous%20vehicles%20in%20long-tail%20scenarios%20remains%20a%20significant%20challenge%2C%20particularly%20for%20high-lateral-acceleration%20maneuvers%20such%20as%20sharp%20turns%2C%20which%20represent%20critical%20safety%20situations.%20Existing%20trajectory%20planners%20exhibit%20systematic%20failures%20in%20these%20scenarios%20due%20to%20data%20imbalance.%20This%20results%20in%20insufficient%20modelling%20of%20vehicle%20dynamics%2C%20road%20geometry%2C%20and%20environmental%20constraints%20in%20high-risk%20situations%2C%20leading%20to%20suboptimal%20or%20unsafe%20trajectory%20prediction%20when%20vehicles%20operate%20near%20their%20physical%20limits.%20In%20this%20paper%2C%20we%20introduce%20ReflexDiffusion%2C%20a%20novel%20inference-stage%20framework%20that%20enhances%20diffusion-based%20trajectory%20planners%20through%20reflective%20adjustment.%20Our%20method%20introduces%20a%20gradient-based%20adjustment%20mechanism%20during%20the%20iterative%20denoising%20process%3A%20after%20each%20standard%20trajectory%20update%2C%20we%20compute%20the%20gradient%20between%20the%20conditional%20and%20unconditional%20noise%20predictions%20to%20explicitly%20amplify%20critical%20conditioning%20signals%2C%20including%20road%20curvature%20and%20lateral%20vehicle%20dynamics.%20This%20amplification%20enforces%20strict%20adherence%20to%20physical%20constraints%2C%20particularly%20improving%20stability%20during%20high-lateral-acceleration%20maneuvers%20where%20precise%20vehicle-road%20interaction%20is%20paramount.%20Evaluated%20on%20the%20nuPlan%20Test14-hard%20benchmark%2C%20ReflexDiffusion%20achieves%20a%2014.1%25%20improvement%20in%20driving%20score%20for%20high-lateral-acceleration%20scenarios%20over%20the%20state-of-the-art%20%28SOTA%29%20methods.%20This%20demonstrates%20that%20inference-time%20trajectory%20optimization%20can%20effectively%20compensate%20for%20training%20data%20sparsity%20by%20dynamically%20reinforcing%20safety-critical%20constraints%20near%20handling%20limits.%20The%20framework%27s%20architecture-agnostic%20design%20enables%20direct%20deployment%20to%20existing%20diffusion-based%20planners%2C%20offering%20a%20practical%20solution%20for%20improving%20autonomous%20vehicle%20safety%20in%20challenging%20driving%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflexDiffusion%253A%2520Reflection-Enhanced%2520Trajectory%2520Planning%2520for%2520High-lateral-acceleration%2520Scenarios%2520in%2520Autonomous%2520Driving%26entry.906535625%3DXuemei%2520Yao%2520and%2520Xiao%2520Yang%2520and%2520Jianbin%2520Sun%2520and%2520Liuwei%2520Xie%2520and%2520Xuebin%2520Shao%2520and%2520Xiyu%2520Fang%2520and%2520Hang%2520Su%2520and%2520Kewei%2520Yang%26entry.1292438233%3DGenerating%2520safe%2520and%2520reliable%2520trajectories%2520for%2520autonomous%2520vehicles%2520in%2520long-tail%2520scenarios%2520remains%2520a%2520significant%2520challenge%252C%2520particularly%2520for%2520high-lateral-acceleration%2520maneuvers%2520such%2520as%2520sharp%2520turns%252C%2520which%2520represent%2520critical%2520safety%2520situations.%2520Existing%2520trajectory%2520planners%2520exhibit%2520systematic%2520failures%2520in%2520these%2520scenarios%2520due%2520to%2520data%2520imbalance.%2520This%2520results%2520in%2520insufficient%2520modelling%2520of%2520vehicle%2520dynamics%252C%2520road%2520geometry%252C%2520and%2520environmental%2520constraints%2520in%2520high-risk%2520situations%252C%2520leading%2520to%2520suboptimal%2520or%2520unsafe%2520trajectory%2520prediction%2520when%2520vehicles%2520operate%2520near%2520their%2520physical%2520limits.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520ReflexDiffusion%252C%2520a%2520novel%2520inference-stage%2520framework%2520that%2520enhances%2520diffusion-based%2520trajectory%2520planners%2520through%2520reflective%2520adjustment.%2520Our%2520method%2520introduces%2520a%2520gradient-based%2520adjustment%2520mechanism%2520during%2520the%2520iterative%2520denoising%2520process%253A%2520after%2520each%2520standard%2520trajectory%2520update%252C%2520we%2520compute%2520the%2520gradient%2520between%2520the%2520conditional%2520and%2520unconditional%2520noise%2520predictions%2520to%2520explicitly%2520amplify%2520critical%2520conditioning%2520signals%252C%2520including%2520road%2520curvature%2520and%2520lateral%2520vehicle%2520dynamics.%2520This%2520amplification%2520enforces%2520strict%2520adherence%2520to%2520physical%2520constraints%252C%2520particularly%2520improving%2520stability%2520during%2520high-lateral-acceleration%2520maneuvers%2520where%2520precise%2520vehicle-road%2520interaction%2520is%2520paramount.%2520Evaluated%2520on%2520the%2520nuPlan%2520Test14-hard%2520benchmark%252C%2520ReflexDiffusion%2520achieves%2520a%252014.1%2525%2520improvement%2520in%2520driving%2520score%2520for%2520high-lateral-acceleration%2520scenarios%2520over%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520methods.%2520This%2520demonstrates%2520that%2520inference-time%2520trajectory%2520optimization%2520can%2520effectively%2520compensate%2520for%2520training%2520data%2520sparsity%2520by%2520dynamically%2520reinforcing%2520safety-critical%2520constraints%2520near%2520handling%2520limits.%2520The%2520framework%2527s%2520architecture-agnostic%2520design%2520enables%2520direct%2520deployment%2520to%2520existing%2520diffusion-based%2520planners%252C%2520offering%2520a%2520practical%2520solution%2520for%2520improving%2520autonomous%2520vehicle%2520safety%2520in%2520challenging%2520driving%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReflexDiffusion%3A%20Reflection-Enhanced%20Trajectory%20Planning%20for%20High-lateral-acceleration%20Scenarios%20in%20Autonomous%20Driving&entry.906535625=Xuemei%20Yao%20and%20Xiao%20Yang%20and%20Jianbin%20Sun%20and%20Liuwei%20Xie%20and%20Xuebin%20Shao%20and%20Xiyu%20Fang%20and%20Hang%20Su%20and%20Kewei%20Yang&entry.1292438233=Generating%20safe%20and%20reliable%20trajectories%20for%20autonomous%20vehicles%20in%20long-tail%20scenarios%20remains%20a%20significant%20challenge%2C%20particularly%20for%20high-lateral-acceleration%20maneuvers%20such%20as%20sharp%20turns%2C%20which%20represent%20critical%20safety%20situations.%20Existing%20trajectory%20planners%20exhibit%20systematic%20failures%20in%20these%20scenarios%20due%20to%20data%20imbalance.%20This%20results%20in%20insufficient%20modelling%20of%20vehicle%20dynamics%2C%20road%20geometry%2C%20and%20environmental%20constraints%20in%20high-risk%20situations%2C%20leading%20to%20suboptimal%20or%20unsafe%20trajectory%20prediction%20when%20vehicles%20operate%20near%20their%20physical%20limits.%20In%20this%20paper%2C%20we%20introduce%20ReflexDiffusion%2C%20a%20novel%20inference-stage%20framework%20that%20enhances%20diffusion-based%20trajectory%20planners%20through%20reflective%20adjustment.%20Our%20method%20introduces%20a%20gradient-based%20adjustment%20mechanism%20during%20the%20iterative%20denoising%20process%3A%20after%20each%20standard%20trajectory%20update%2C%20we%20compute%20the%20gradient%20between%20the%20conditional%20and%20unconditional%20noise%20predictions%20to%20explicitly%20amplify%20critical%20conditioning%20signals%2C%20including%20road%20curvature%20and%20lateral%20vehicle%20dynamics.%20This%20amplification%20enforces%20strict%20adherence%20to%20physical%20constraints%2C%20particularly%20improving%20stability%20during%20high-lateral-acceleration%20maneuvers%20where%20precise%20vehicle-road%20interaction%20is%20paramount.%20Evaluated%20on%20the%20nuPlan%20Test14-hard%20benchmark%2C%20ReflexDiffusion%20achieves%20a%2014.1%25%20improvement%20in%20driving%20score%20for%20high-lateral-acceleration%20scenarios%20over%20the%20state-of-the-art%20%28SOTA%29%20methods.%20This%20demonstrates%20that%20inference-time%20trajectory%20optimization%20can%20effectively%20compensate%20for%20training%20data%20sparsity%20by%20dynamically%20reinforcing%20safety-critical%20constraints%20near%20handling%20limits.%20The%20framework%27s%20architecture-agnostic%20design%20enables%20direct%20deployment%20to%20existing%20diffusion-based%20planners%2C%20offering%20a%20practical%20solution%20for%20improving%20autonomous%20vehicle%20safety%20in%20challenging%20driving%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2601.09377v1&entry.124074799=Read"},
{"title": "Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust", "author": "Pooja Prajod and Hannes Cools and Thomas R\u00f6ggla and Karthikeya Puttur Venkatraj and Amber Kusters and Alia ElKattan and Pablo Cesar and Abdallah El Ali", "abstract": "As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \\textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\\times$2$\\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.", "link": "http://arxiv.org/abs/2601.09620v1", "date": "2026-01-14", "relevancy": 1.5692, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.437}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.386}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Full%20Disclosure%2C%20Less%20Trust%3F%20How%20the%20Level%20of%20Detail%20about%20AI%20Use%20in%20News%20Writing%20Affects%20Readers%27%20Trust&body=Title%3A%20Full%20Disclosure%2C%20Less%20Trust%3F%20How%20the%20Level%20of%20Detail%20about%20AI%20Use%20in%20News%20Writing%20Affects%20Readers%27%20Trust%0AAuthor%3A%20Pooja%20Prajod%20and%20Hannes%20Cools%20and%20Thomas%20R%C3%B6ggla%20and%20Karthikeya%20Puttur%20Venkatraj%20and%20Amber%20Kusters%20and%20Alia%20ElKattan%20and%20Pablo%20Cesar%20and%20Abdallah%20El%20Ali%0AAbstract%3A%20As%20artificial%20intelligence%20%28AI%29%20is%20increasingly%20integrated%20into%20news%20production%2C%20calls%20for%20transparency%20about%20the%20use%20of%20AI%20have%20gained%20considerable%20traction.%20Recent%20studies%20suggest%20that%20AI%20disclosures%20can%20lead%20to%20a%20%60%60transparency%20dilemma%27%27%2C%20where%20disclosure%20reduces%20readers%27%20trust.%20However%2C%20little%20is%20known%20about%20how%20the%20%5Ctextit%7Blevel%20of%20detail%7D%20in%20AI%20disclosures%20influences%20trust%20and%20contributes%20to%20this%20dilemma%20within%20the%20news%20context.%20In%20this%203%24%5Ctimes%242%24%5Ctimes%242%20mixed%20factorial%20study%20with%2040%20participants%2C%20we%20investigate%20how%20three%20levels%20of%20AI%20disclosures%20%28none%2C%20one-line%2C%20detailed%29%20across%20two%20types%20of%20news%20%28politics%20and%20lifestyle%29%20and%20two%20levels%20of%20AI%20involvement%20%28low%20and%20high%29%20affect%20news%20readers%27%20trust.%20We%20measured%20trust%20using%20the%20News%20Media%20Trust%20questionnaire%2C%20along%20with%20two%20decision%20behaviors%3A%20source-checking%20and%20subscription%20decisions.%20Questionnaire%20responses%20and%20subscription%20rates%20showed%20a%20decline%20in%20trust%20only%20for%20detailed%20AI%20disclosures%2C%20whereas%20source-checking%20behavior%20increased%20for%20both%20one-line%20and%20detailed%20disclosures%2C%20with%20the%20effect%20being%20more%20pronounced%20for%20detailed%20disclosures.%20Insights%20from%20semi-structured%20interviews%20suggest%20that%20source-checking%20behavior%20was%20primarily%20driven%20by%20interest%20in%20the%20topic%2C%20followed%20by%20trust%2C%20whereas%20trust%20was%20the%20main%20factor%20influencing%20subscription%20decisions.%20Around%20two-thirds%20of%20participants%20expressed%20a%20preference%20for%20detailed%20disclosures%2C%20while%20most%20participants%20who%20preferred%20one-line%20indicated%20a%20need%20for%20detail-on-demand%20disclosure%20formats.%20Our%20findings%20show%20that%20not%20all%20AI%20disclosures%20lead%20to%20a%20transparency%20dilemma%2C%20but%20instead%20reflect%20a%20trade-off%20between%20readers%27%20desire%20for%20more%20transparency%20and%20their%20trust%20in%20AI-assisted%20news%20content.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFull%2520Disclosure%252C%2520Less%2520Trust%253F%2520How%2520the%2520Level%2520of%2520Detail%2520about%2520AI%2520Use%2520in%2520News%2520Writing%2520Affects%2520Readers%2527%2520Trust%26entry.906535625%3DPooja%2520Prajod%2520and%2520Hannes%2520Cools%2520and%2520Thomas%2520R%25C3%25B6ggla%2520and%2520Karthikeya%2520Puttur%2520Venkatraj%2520and%2520Amber%2520Kusters%2520and%2520Alia%2520ElKattan%2520and%2520Pablo%2520Cesar%2520and%2520Abdallah%2520El%2520Ali%26entry.1292438233%3DAs%2520artificial%2520intelligence%2520%2528AI%2529%2520is%2520increasingly%2520integrated%2520into%2520news%2520production%252C%2520calls%2520for%2520transparency%2520about%2520the%2520use%2520of%2520AI%2520have%2520gained%2520considerable%2520traction.%2520Recent%2520studies%2520suggest%2520that%2520AI%2520disclosures%2520can%2520lead%2520to%2520a%2520%2560%2560transparency%2520dilemma%2527%2527%252C%2520where%2520disclosure%2520reduces%2520readers%2527%2520trust.%2520However%252C%2520little%2520is%2520known%2520about%2520how%2520the%2520%255Ctextit%257Blevel%2520of%2520detail%257D%2520in%2520AI%2520disclosures%2520influences%2520trust%2520and%2520contributes%2520to%2520this%2520dilemma%2520within%2520the%2520news%2520context.%2520In%2520this%25203%2524%255Ctimes%25242%2524%255Ctimes%25242%2520mixed%2520factorial%2520study%2520with%252040%2520participants%252C%2520we%2520investigate%2520how%2520three%2520levels%2520of%2520AI%2520disclosures%2520%2528none%252C%2520one-line%252C%2520detailed%2529%2520across%2520two%2520types%2520of%2520news%2520%2528politics%2520and%2520lifestyle%2529%2520and%2520two%2520levels%2520of%2520AI%2520involvement%2520%2528low%2520and%2520high%2529%2520affect%2520news%2520readers%2527%2520trust.%2520We%2520measured%2520trust%2520using%2520the%2520News%2520Media%2520Trust%2520questionnaire%252C%2520along%2520with%2520two%2520decision%2520behaviors%253A%2520source-checking%2520and%2520subscription%2520decisions.%2520Questionnaire%2520responses%2520and%2520subscription%2520rates%2520showed%2520a%2520decline%2520in%2520trust%2520only%2520for%2520detailed%2520AI%2520disclosures%252C%2520whereas%2520source-checking%2520behavior%2520increased%2520for%2520both%2520one-line%2520and%2520detailed%2520disclosures%252C%2520with%2520the%2520effect%2520being%2520more%2520pronounced%2520for%2520detailed%2520disclosures.%2520Insights%2520from%2520semi-structured%2520interviews%2520suggest%2520that%2520source-checking%2520behavior%2520was%2520primarily%2520driven%2520by%2520interest%2520in%2520the%2520topic%252C%2520followed%2520by%2520trust%252C%2520whereas%2520trust%2520was%2520the%2520main%2520factor%2520influencing%2520subscription%2520decisions.%2520Around%2520two-thirds%2520of%2520participants%2520expressed%2520a%2520preference%2520for%2520detailed%2520disclosures%252C%2520while%2520most%2520participants%2520who%2520preferred%2520one-line%2520indicated%2520a%2520need%2520for%2520detail-on-demand%2520disclosure%2520formats.%2520Our%2520findings%2520show%2520that%2520not%2520all%2520AI%2520disclosures%2520lead%2520to%2520a%2520transparency%2520dilemma%252C%2520but%2520instead%2520reflect%2520a%2520trade-off%2520between%2520readers%2527%2520desire%2520for%2520more%2520transparency%2520and%2520their%2520trust%2520in%2520AI-assisted%2520news%2520content.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Full%20Disclosure%2C%20Less%20Trust%3F%20How%20the%20Level%20of%20Detail%20about%20AI%20Use%20in%20News%20Writing%20Affects%20Readers%27%20Trust&entry.906535625=Pooja%20Prajod%20and%20Hannes%20Cools%20and%20Thomas%20R%C3%B6ggla%20and%20Karthikeya%20Puttur%20Venkatraj%20and%20Amber%20Kusters%20and%20Alia%20ElKattan%20and%20Pablo%20Cesar%20and%20Abdallah%20El%20Ali&entry.1292438233=As%20artificial%20intelligence%20%28AI%29%20is%20increasingly%20integrated%20into%20news%20production%2C%20calls%20for%20transparency%20about%20the%20use%20of%20AI%20have%20gained%20considerable%20traction.%20Recent%20studies%20suggest%20that%20AI%20disclosures%20can%20lead%20to%20a%20%60%60transparency%20dilemma%27%27%2C%20where%20disclosure%20reduces%20readers%27%20trust.%20However%2C%20little%20is%20known%20about%20how%20the%20%5Ctextit%7Blevel%20of%20detail%7D%20in%20AI%20disclosures%20influences%20trust%20and%20contributes%20to%20this%20dilemma%20within%20the%20news%20context.%20In%20this%203%24%5Ctimes%242%24%5Ctimes%242%20mixed%20factorial%20study%20with%2040%20participants%2C%20we%20investigate%20how%20three%20levels%20of%20AI%20disclosures%20%28none%2C%20one-line%2C%20detailed%29%20across%20two%20types%20of%20news%20%28politics%20and%20lifestyle%29%20and%20two%20levels%20of%20AI%20involvement%20%28low%20and%20high%29%20affect%20news%20readers%27%20trust.%20We%20measured%20trust%20using%20the%20News%20Media%20Trust%20questionnaire%2C%20along%20with%20two%20decision%20behaviors%3A%20source-checking%20and%20subscription%20decisions.%20Questionnaire%20responses%20and%20subscription%20rates%20showed%20a%20decline%20in%20trust%20only%20for%20detailed%20AI%20disclosures%2C%20whereas%20source-checking%20behavior%20increased%20for%20both%20one-line%20and%20detailed%20disclosures%2C%20with%20the%20effect%20being%20more%20pronounced%20for%20detailed%20disclosures.%20Insights%20from%20semi-structured%20interviews%20suggest%20that%20source-checking%20behavior%20was%20primarily%20driven%20by%20interest%20in%20the%20topic%2C%20followed%20by%20trust%2C%20whereas%20trust%20was%20the%20main%20factor%20influencing%20subscription%20decisions.%20Around%20two-thirds%20of%20participants%20expressed%20a%20preference%20for%20detailed%20disclosures%2C%20while%20most%20participants%20who%20preferred%20one-line%20indicated%20a%20need%20for%20detail-on-demand%20disclosure%20formats.%20Our%20findings%20show%20that%20not%20all%20AI%20disclosures%20lead%20to%20a%20transparency%20dilemma%2C%20but%20instead%20reflect%20a%20trade-off%20between%20readers%27%20desire%20for%20more%20transparency%20and%20their%20trust%20in%20AI-assisted%20news%20content.&entry.1838667208=http%3A//arxiv.org/abs/2601.09620v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


