<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250224.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting", "author": "Chong Cheng and Gaochao Song and Yiyang Yao and Qinzheng Zhou and Gangjian Zhang and Hao Wang", "abstract": "  This paper investigates an open research challenge of reconstructing\nhigh-quality, large 3D open scenes from images. It is observed existing methods\nhave various limitations, such as requiring precise camera poses for input and\ndense viewpoints for supervision. To perform effective and efficient 3D scene\nreconstruction, we propose a novel graph-guided 3D scene reconstruction\nframework, GraphGS. Specifically, given a set of images captured by RGB cameras\non a scene, we first design a spatial prior-based scene structure estimation\nmethod. This is then used to create a camera graph that includes information\nabout the camera topology. Further, we propose to apply the graph-guided\nmulti-view consistency constraint and adaptive sampling strategy to the 3D\nGaussian Splatting optimization process. This greatly alleviates the issue of\nGaussian points overfitting to specific sparse viewpoints and expedites the 3D\nreconstruction process. We demonstrate GraphGS achieves high-fidelity 3D\nreconstruction from images, which presents state-of-the-art performance through\nquantitative and qualitative evaluation across multiple datasets. Project Page:\nhttps://3dagentworld.github.io/graphgs.\n", "link": "http://arxiv.org/abs/2502.17377v1", "date": "2025-02-24", "relevancy": 3.4885, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7293}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6927}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Guided%20Scene%20Reconstruction%20from%20Images%20with%203D%20Gaussian%20Splatting&body=Title%3A%20Graph-Guided%20Scene%20Reconstruction%20from%20Images%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Chong%20Cheng%20and%20Gaochao%20Song%20and%20Yiyang%20Yao%20and%20Qinzheng%20Zhou%20and%20Gangjian%20Zhang%20and%20Hao%20Wang%0AAbstract%3A%20%20%20This%20paper%20investigates%20an%20open%20research%20challenge%20of%20reconstructing%0Ahigh-quality%2C%20large%203D%20open%20scenes%20from%20images.%20It%20is%20observed%20existing%20methods%0Ahave%20various%20limitations%2C%20such%20as%20requiring%20precise%20camera%20poses%20for%20input%20and%0Adense%20viewpoints%20for%20supervision.%20To%20perform%20effective%20and%20efficient%203D%20scene%0Areconstruction%2C%20we%20propose%20a%20novel%20graph-guided%203D%20scene%20reconstruction%0Aframework%2C%20GraphGS.%20Specifically%2C%20given%20a%20set%20of%20images%20captured%20by%20RGB%20cameras%0Aon%20a%20scene%2C%20we%20first%20design%20a%20spatial%20prior-based%20scene%20structure%20estimation%0Amethod.%20This%20is%20then%20used%20to%20create%20a%20camera%20graph%20that%20includes%20information%0Aabout%20the%20camera%20topology.%20Further%2C%20we%20propose%20to%20apply%20the%20graph-guided%0Amulti-view%20consistency%20constraint%20and%20adaptive%20sampling%20strategy%20to%20the%203D%0AGaussian%20Splatting%20optimization%20process.%20This%20greatly%20alleviates%20the%20issue%20of%0AGaussian%20points%20overfitting%20to%20specific%20sparse%20viewpoints%20and%20expedites%20the%203D%0Areconstruction%20process.%20We%20demonstrate%20GraphGS%20achieves%20high-fidelity%203D%0Areconstruction%20from%20images%2C%20which%20presents%20state-of-the-art%20performance%20through%0Aquantitative%20and%20qualitative%20evaluation%20across%20multiple%20datasets.%20Project%20Page%3A%0Ahttps%3A//3dagentworld.github.io/graphgs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Guided%2520Scene%2520Reconstruction%2520from%2520Images%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DChong%2520Cheng%2520and%2520Gaochao%2520Song%2520and%2520Yiyang%2520Yao%2520and%2520Qinzheng%2520Zhou%2520and%2520Gangjian%2520Zhang%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520an%2520open%2520research%2520challenge%2520of%2520reconstructing%250Ahigh-quality%252C%2520large%25203D%2520open%2520scenes%2520from%2520images.%2520It%2520is%2520observed%2520existing%2520methods%250Ahave%2520various%2520limitations%252C%2520such%2520as%2520requiring%2520precise%2520camera%2520poses%2520for%2520input%2520and%250Adense%2520viewpoints%2520for%2520supervision.%2520To%2520perform%2520effective%2520and%2520efficient%25203D%2520scene%250Areconstruction%252C%2520we%2520propose%2520a%2520novel%2520graph-guided%25203D%2520scene%2520reconstruction%250Aframework%252C%2520GraphGS.%2520Specifically%252C%2520given%2520a%2520set%2520of%2520images%2520captured%2520by%2520RGB%2520cameras%250Aon%2520a%2520scene%252C%2520we%2520first%2520design%2520a%2520spatial%2520prior-based%2520scene%2520structure%2520estimation%250Amethod.%2520This%2520is%2520then%2520used%2520to%2520create%2520a%2520camera%2520graph%2520that%2520includes%2520information%250Aabout%2520the%2520camera%2520topology.%2520Further%252C%2520we%2520propose%2520to%2520apply%2520the%2520graph-guided%250Amulti-view%2520consistency%2520constraint%2520and%2520adaptive%2520sampling%2520strategy%2520to%2520the%25203D%250AGaussian%2520Splatting%2520optimization%2520process.%2520This%2520greatly%2520alleviates%2520the%2520issue%2520of%250AGaussian%2520points%2520overfitting%2520to%2520specific%2520sparse%2520viewpoints%2520and%2520expedites%2520the%25203D%250Areconstruction%2520process.%2520We%2520demonstrate%2520GraphGS%2520achieves%2520high-fidelity%25203D%250Areconstruction%2520from%2520images%252C%2520which%2520presents%2520state-of-the-art%2520performance%2520through%250Aquantitative%2520and%2520qualitative%2520evaluation%2520across%2520multiple%2520datasets.%2520Project%2520Page%253A%250Ahttps%253A//3dagentworld.github.io/graphgs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Guided%20Scene%20Reconstruction%20from%20Images%20with%203D%20Gaussian%20Splatting&entry.906535625=Chong%20Cheng%20and%20Gaochao%20Song%20and%20Yiyang%20Yao%20and%20Qinzheng%20Zhou%20and%20Gangjian%20Zhang%20and%20Hao%20Wang&entry.1292438233=%20%20This%20paper%20investigates%20an%20open%20research%20challenge%20of%20reconstructing%0Ahigh-quality%2C%20large%203D%20open%20scenes%20from%20images.%20It%20is%20observed%20existing%20methods%0Ahave%20various%20limitations%2C%20such%20as%20requiring%20precise%20camera%20poses%20for%20input%20and%0Adense%20viewpoints%20for%20supervision.%20To%20perform%20effective%20and%20efficient%203D%20scene%0Areconstruction%2C%20we%20propose%20a%20novel%20graph-guided%203D%20scene%20reconstruction%0Aframework%2C%20GraphGS.%20Specifically%2C%20given%20a%20set%20of%20images%20captured%20by%20RGB%20cameras%0Aon%20a%20scene%2C%20we%20first%20design%20a%20spatial%20prior-based%20scene%20structure%20estimation%0Amethod.%20This%20is%20then%20used%20to%20create%20a%20camera%20graph%20that%20includes%20information%0Aabout%20the%20camera%20topology.%20Further%2C%20we%20propose%20to%20apply%20the%20graph-guided%0Amulti-view%20consistency%20constraint%20and%20adaptive%20sampling%20strategy%20to%20the%203D%0AGaussian%20Splatting%20optimization%20process.%20This%20greatly%20alleviates%20the%20issue%20of%0AGaussian%20points%20overfitting%20to%20specific%20sparse%20viewpoints%20and%20expedites%20the%203D%0Areconstruction%20process.%20We%20demonstrate%20GraphGS%20achieves%20high-fidelity%203D%0Areconstruction%20from%20images%2C%20which%20presents%20state-of-the-art%20performance%20through%0Aquantitative%20and%20qualitative%20evaluation%20across%20multiple%20datasets.%20Project%20Page%3A%0Ahttps%3A//3dagentworld.github.io/graphgs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17377v1&entry.124074799=Read"},
{"title": "X-Dancer: Expressive Music to Human Dance Video Generation", "author": "Zeyuan Chen and Hongyi Xu and Guoxian Song and You Xie and Chenxu Zhang and Xin Chen and Chao Wang and Di Chang and Linjie Luo", "abstract": "  We present X-Dancer, a novel zero-shot music-driven image animation pipeline\nthat creates diverse and long-range lifelike human dance videos from a single\nstatic image. As its core, we introduce a unified transformer-diffusion\nframework, featuring an autoregressive transformer model that synthesize\nextended and music-synchronized token sequences for 2D body, head and hands\nposes, which then guide a diffusion model to produce coherent and realistic\ndance video frames. Unlike traditional methods that primarily generate human\nmotion in 3D, X-Dancer addresses data limitations and enhances scalability by\nmodeling a wide spectrum of 2D dance motions, capturing their nuanced alignment\nwith musical beats through readily available monocular videos. To achieve this,\nwe first build a spatially compositional token representation from 2D human\npose labels associated with keypoint confidences, encoding both large\narticulated body movements (e.g., upper and lower body) and fine-grained\nmotions (e.g., head and hands). We then design a music-to-motion transformer\nmodel that autoregressively generates music-aligned dance pose token sequences,\nincorporating global attention to both musical style and prior motion context.\nFinally we leverage a diffusion backbone to animate the reference image with\nthese synthesized pose tokens through AdaIN, forming a fully differentiable\nend-to-end framework. Experimental results demonstrate that X-Dancer is able to\nproduce both diverse and characterized dance videos, substantially\noutperforming state-of-the-art methods in term of diversity, expressiveness and\nrealism. Code and model will be available for research purposes.\n", "link": "http://arxiv.org/abs/2502.17414v1", "date": "2025-02-24", "relevancy": 3.2437, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6988}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6304}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Dancer%3A%20Expressive%20Music%20to%20Human%20Dance%20Video%20Generation&body=Title%3A%20X-Dancer%3A%20Expressive%20Music%20to%20Human%20Dance%20Video%20Generation%0AAuthor%3A%20Zeyuan%20Chen%20and%20Hongyi%20Xu%20and%20Guoxian%20Song%20and%20You%20Xie%20and%20Chenxu%20Zhang%20and%20Xin%20Chen%20and%20Chao%20Wang%20and%20Di%20Chang%20and%20Linjie%20Luo%0AAbstract%3A%20%20%20We%20present%20X-Dancer%2C%20a%20novel%20zero-shot%20music-driven%20image%20animation%20pipeline%0Athat%20creates%20diverse%20and%20long-range%20lifelike%20human%20dance%20videos%20from%20a%20single%0Astatic%20image.%20As%20its%20core%2C%20we%20introduce%20a%20unified%20transformer-diffusion%0Aframework%2C%20featuring%20an%20autoregressive%20transformer%20model%20that%20synthesize%0Aextended%20and%20music-synchronized%20token%20sequences%20for%202D%20body%2C%20head%20and%20hands%0Aposes%2C%20which%20then%20guide%20a%20diffusion%20model%20to%20produce%20coherent%20and%20realistic%0Adance%20video%20frames.%20Unlike%20traditional%20methods%20that%20primarily%20generate%20human%0Amotion%20in%203D%2C%20X-Dancer%20addresses%20data%20limitations%20and%20enhances%20scalability%20by%0Amodeling%20a%20wide%20spectrum%20of%202D%20dance%20motions%2C%20capturing%20their%20nuanced%20alignment%0Awith%20musical%20beats%20through%20readily%20available%20monocular%20videos.%20To%20achieve%20this%2C%0Awe%20first%20build%20a%20spatially%20compositional%20token%20representation%20from%202D%20human%0Apose%20labels%20associated%20with%20keypoint%20confidences%2C%20encoding%20both%20large%0Aarticulated%20body%20movements%20%28e.g.%2C%20upper%20and%20lower%20body%29%20and%20fine-grained%0Amotions%20%28e.g.%2C%20head%20and%20hands%29.%20We%20then%20design%20a%20music-to-motion%20transformer%0Amodel%20that%20autoregressively%20generates%20music-aligned%20dance%20pose%20token%20sequences%2C%0Aincorporating%20global%20attention%20to%20both%20musical%20style%20and%20prior%20motion%20context.%0AFinally%20we%20leverage%20a%20diffusion%20backbone%20to%20animate%20the%20reference%20image%20with%0Athese%20synthesized%20pose%20tokens%20through%20AdaIN%2C%20forming%20a%20fully%20differentiable%0Aend-to-end%20framework.%20Experimental%20results%20demonstrate%20that%20X-Dancer%20is%20able%20to%0Aproduce%20both%20diverse%20and%20characterized%20dance%20videos%2C%20substantially%0Aoutperforming%20state-of-the-art%20methods%20in%20term%20of%20diversity%2C%20expressiveness%20and%0Arealism.%20Code%20and%20model%20will%20be%20available%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Dancer%253A%2520Expressive%2520Music%2520to%2520Human%2520Dance%2520Video%2520Generation%26entry.906535625%3DZeyuan%2520Chen%2520and%2520Hongyi%2520Xu%2520and%2520Guoxian%2520Song%2520and%2520You%2520Xie%2520and%2520Chenxu%2520Zhang%2520and%2520Xin%2520Chen%2520and%2520Chao%2520Wang%2520and%2520Di%2520Chang%2520and%2520Linjie%2520Luo%26entry.1292438233%3D%2520%2520We%2520present%2520X-Dancer%252C%2520a%2520novel%2520zero-shot%2520music-driven%2520image%2520animation%2520pipeline%250Athat%2520creates%2520diverse%2520and%2520long-range%2520lifelike%2520human%2520dance%2520videos%2520from%2520a%2520single%250Astatic%2520image.%2520As%2520its%2520core%252C%2520we%2520introduce%2520a%2520unified%2520transformer-diffusion%250Aframework%252C%2520featuring%2520an%2520autoregressive%2520transformer%2520model%2520that%2520synthesize%250Aextended%2520and%2520music-synchronized%2520token%2520sequences%2520for%25202D%2520body%252C%2520head%2520and%2520hands%250Aposes%252C%2520which%2520then%2520guide%2520a%2520diffusion%2520model%2520to%2520produce%2520coherent%2520and%2520realistic%250Adance%2520video%2520frames.%2520Unlike%2520traditional%2520methods%2520that%2520primarily%2520generate%2520human%250Amotion%2520in%25203D%252C%2520X-Dancer%2520addresses%2520data%2520limitations%2520and%2520enhances%2520scalability%2520by%250Amodeling%2520a%2520wide%2520spectrum%2520of%25202D%2520dance%2520motions%252C%2520capturing%2520their%2520nuanced%2520alignment%250Awith%2520musical%2520beats%2520through%2520readily%2520available%2520monocular%2520videos.%2520To%2520achieve%2520this%252C%250Awe%2520first%2520build%2520a%2520spatially%2520compositional%2520token%2520representation%2520from%25202D%2520human%250Apose%2520labels%2520associated%2520with%2520keypoint%2520confidences%252C%2520encoding%2520both%2520large%250Aarticulated%2520body%2520movements%2520%2528e.g.%252C%2520upper%2520and%2520lower%2520body%2529%2520and%2520fine-grained%250Amotions%2520%2528e.g.%252C%2520head%2520and%2520hands%2529.%2520We%2520then%2520design%2520a%2520music-to-motion%2520transformer%250Amodel%2520that%2520autoregressively%2520generates%2520music-aligned%2520dance%2520pose%2520token%2520sequences%252C%250Aincorporating%2520global%2520attention%2520to%2520both%2520musical%2520style%2520and%2520prior%2520motion%2520context.%250AFinally%2520we%2520leverage%2520a%2520diffusion%2520backbone%2520to%2520animate%2520the%2520reference%2520image%2520with%250Athese%2520synthesized%2520pose%2520tokens%2520through%2520AdaIN%252C%2520forming%2520a%2520fully%2520differentiable%250Aend-to-end%2520framework.%2520Experimental%2520results%2520demonstrate%2520that%2520X-Dancer%2520is%2520able%2520to%250Aproduce%2520both%2520diverse%2520and%2520characterized%2520dance%2520videos%252C%2520substantially%250Aoutperforming%2520state-of-the-art%2520methods%2520in%2520term%2520of%2520diversity%252C%2520expressiveness%2520and%250Arealism.%2520Code%2520and%2520model%2520will%2520be%2520available%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Dancer%3A%20Expressive%20Music%20to%20Human%20Dance%20Video%20Generation&entry.906535625=Zeyuan%20Chen%20and%20Hongyi%20Xu%20and%20Guoxian%20Song%20and%20You%20Xie%20and%20Chenxu%20Zhang%20and%20Xin%20Chen%20and%20Chao%20Wang%20and%20Di%20Chang%20and%20Linjie%20Luo&entry.1292438233=%20%20We%20present%20X-Dancer%2C%20a%20novel%20zero-shot%20music-driven%20image%20animation%20pipeline%0Athat%20creates%20diverse%20and%20long-range%20lifelike%20human%20dance%20videos%20from%20a%20single%0Astatic%20image.%20As%20its%20core%2C%20we%20introduce%20a%20unified%20transformer-diffusion%0Aframework%2C%20featuring%20an%20autoregressive%20transformer%20model%20that%20synthesize%0Aextended%20and%20music-synchronized%20token%20sequences%20for%202D%20body%2C%20head%20and%20hands%0Aposes%2C%20which%20then%20guide%20a%20diffusion%20model%20to%20produce%20coherent%20and%20realistic%0Adance%20video%20frames.%20Unlike%20traditional%20methods%20that%20primarily%20generate%20human%0Amotion%20in%203D%2C%20X-Dancer%20addresses%20data%20limitations%20and%20enhances%20scalability%20by%0Amodeling%20a%20wide%20spectrum%20of%202D%20dance%20motions%2C%20capturing%20their%20nuanced%20alignment%0Awith%20musical%20beats%20through%20readily%20available%20monocular%20videos.%20To%20achieve%20this%2C%0Awe%20first%20build%20a%20spatially%20compositional%20token%20representation%20from%202D%20human%0Apose%20labels%20associated%20with%20keypoint%20confidences%2C%20encoding%20both%20large%0Aarticulated%20body%20movements%20%28e.g.%2C%20upper%20and%20lower%20body%29%20and%20fine-grained%0Amotions%20%28e.g.%2C%20head%20and%20hands%29.%20We%20then%20design%20a%20music-to-motion%20transformer%0Amodel%20that%20autoregressively%20generates%20music-aligned%20dance%20pose%20token%20sequences%2C%0Aincorporating%20global%20attention%20to%20both%20musical%20style%20and%20prior%20motion%20context.%0AFinally%20we%20leverage%20a%20diffusion%20backbone%20to%20animate%20the%20reference%20image%20with%0Athese%20synthesized%20pose%20tokens%20through%20AdaIN%2C%20forming%20a%20fully%20differentiable%0Aend-to-end%20framework.%20Experimental%20results%20demonstrate%20that%20X-Dancer%20is%20able%20to%0Aproduce%20both%20diverse%20and%20characterized%20dance%20videos%2C%20substantially%0Aoutperforming%20state-of-the-art%20methods%20in%20term%20of%20diversity%2C%20expressiveness%20and%0Arealism.%20Code%20and%20model%20will%20be%20available%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17414v1&entry.124074799=Read"},
{"title": "Dimitra: Audio-driven Diffusion model for Expressive Talking Head\n  Generation", "author": "Baptiste Chopin and Tashvik Dhamija and Pranav Balaji and Yaohui Wang and Antitza Dantcheva", "abstract": "  We propose Dimitra, a novel framework for audio-driven talking head\ngeneration, streamlined to learn lip motion, facial expression, as well as head\npose motion. Specifically, we train a conditional Motion Diffusion Transformer\n(cMDT) by modeling facial motion sequences with 3D representation. We condition\nthe cMDT with only two input signals, an audio-sequence, as well as a reference\nfacial image. By extracting additional features directly from audio, Dimitra is\nable to increase quality and realism of generated videos. In particular,\nphoneme sequences contribute to the realism of lip motion, whereas text\ntranscript to facial expression and head pose realism. Quantitative and\nqualitative experiments on two widely employed datasets, VoxCeleb2 and HDTF,\nshowcase that Dimitra is able to outperform existing approaches for generating\nrealistic talking heads imparting lip motion, facial expression, and head pose.\n", "link": "http://arxiv.org/abs/2502.17198v1", "date": "2025-02-24", "relevancy": 3.1578, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6676}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6135}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dimitra%3A%20Audio-driven%20Diffusion%20model%20for%20Expressive%20Talking%20Head%0A%20%20Generation&body=Title%3A%20Dimitra%3A%20Audio-driven%20Diffusion%20model%20for%20Expressive%20Talking%20Head%0A%20%20Generation%0AAuthor%3A%20Baptiste%20Chopin%20and%20Tashvik%20Dhamija%20and%20Pranav%20Balaji%20and%20Yaohui%20Wang%20and%20Antitza%20Dantcheva%0AAbstract%3A%20%20%20We%20propose%20Dimitra%2C%20a%20novel%20framework%20for%20audio-driven%20talking%20head%0Ageneration%2C%20streamlined%20to%20learn%20lip%20motion%2C%20facial%20expression%2C%20as%20well%20as%20head%0Apose%20motion.%20Specifically%2C%20we%20train%20a%20conditional%20Motion%20Diffusion%20Transformer%0A%28cMDT%29%20by%20modeling%20facial%20motion%20sequences%20with%203D%20representation.%20We%20condition%0Athe%20cMDT%20with%20only%20two%20input%20signals%2C%20an%20audio-sequence%2C%20as%20well%20as%20a%20reference%0Afacial%20image.%20By%20extracting%20additional%20features%20directly%20from%20audio%2C%20Dimitra%20is%0Aable%20to%20increase%20quality%20and%20realism%20of%20generated%20videos.%20In%20particular%2C%0Aphoneme%20sequences%20contribute%20to%20the%20realism%20of%20lip%20motion%2C%20whereas%20text%0Atranscript%20to%20facial%20expression%20and%20head%20pose%20realism.%20Quantitative%20and%0Aqualitative%20experiments%20on%20two%20widely%20employed%20datasets%2C%20VoxCeleb2%20and%20HDTF%2C%0Ashowcase%20that%20Dimitra%20is%20able%20to%20outperform%20existing%20approaches%20for%20generating%0Arealistic%20talking%20heads%20imparting%20lip%20motion%2C%20facial%20expression%2C%20and%20head%20pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDimitra%253A%2520Audio-driven%2520Diffusion%2520model%2520for%2520Expressive%2520Talking%2520Head%250A%2520%2520Generation%26entry.906535625%3DBaptiste%2520Chopin%2520and%2520Tashvik%2520Dhamija%2520and%2520Pranav%2520Balaji%2520and%2520Yaohui%2520Wang%2520and%2520Antitza%2520Dantcheva%26entry.1292438233%3D%2520%2520We%2520propose%2520Dimitra%252C%2520a%2520novel%2520framework%2520for%2520audio-driven%2520talking%2520head%250Ageneration%252C%2520streamlined%2520to%2520learn%2520lip%2520motion%252C%2520facial%2520expression%252C%2520as%2520well%2520as%2520head%250Apose%2520motion.%2520Specifically%252C%2520we%2520train%2520a%2520conditional%2520Motion%2520Diffusion%2520Transformer%250A%2528cMDT%2529%2520by%2520modeling%2520facial%2520motion%2520sequences%2520with%25203D%2520representation.%2520We%2520condition%250Athe%2520cMDT%2520with%2520only%2520two%2520input%2520signals%252C%2520an%2520audio-sequence%252C%2520as%2520well%2520as%2520a%2520reference%250Afacial%2520image.%2520By%2520extracting%2520additional%2520features%2520directly%2520from%2520audio%252C%2520Dimitra%2520is%250Aable%2520to%2520increase%2520quality%2520and%2520realism%2520of%2520generated%2520videos.%2520In%2520particular%252C%250Aphoneme%2520sequences%2520contribute%2520to%2520the%2520realism%2520of%2520lip%2520motion%252C%2520whereas%2520text%250Atranscript%2520to%2520facial%2520expression%2520and%2520head%2520pose%2520realism.%2520Quantitative%2520and%250Aqualitative%2520experiments%2520on%2520two%2520widely%2520employed%2520datasets%252C%2520VoxCeleb2%2520and%2520HDTF%252C%250Ashowcase%2520that%2520Dimitra%2520is%2520able%2520to%2520outperform%2520existing%2520approaches%2520for%2520generating%250Arealistic%2520talking%2520heads%2520imparting%2520lip%2520motion%252C%2520facial%2520expression%252C%2520and%2520head%2520pose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dimitra%3A%20Audio-driven%20Diffusion%20model%20for%20Expressive%20Talking%20Head%0A%20%20Generation&entry.906535625=Baptiste%20Chopin%20and%20Tashvik%20Dhamija%20and%20Pranav%20Balaji%20and%20Yaohui%20Wang%20and%20Antitza%20Dantcheva&entry.1292438233=%20%20We%20propose%20Dimitra%2C%20a%20novel%20framework%20for%20audio-driven%20talking%20head%0Ageneration%2C%20streamlined%20to%20learn%20lip%20motion%2C%20facial%20expression%2C%20as%20well%20as%20head%0Apose%20motion.%20Specifically%2C%20we%20train%20a%20conditional%20Motion%20Diffusion%20Transformer%0A%28cMDT%29%20by%20modeling%20facial%20motion%20sequences%20with%203D%20representation.%20We%20condition%0Athe%20cMDT%20with%20only%20two%20input%20signals%2C%20an%20audio-sequence%2C%20as%20well%20as%20a%20reference%0Afacial%20image.%20By%20extracting%20additional%20features%20directly%20from%20audio%2C%20Dimitra%20is%0Aable%20to%20increase%20quality%20and%20realism%20of%20generated%20videos.%20In%20particular%2C%0Aphoneme%20sequences%20contribute%20to%20the%20realism%20of%20lip%20motion%2C%20whereas%20text%0Atranscript%20to%20facial%20expression%20and%20head%20pose%20realism.%20Quantitative%20and%0Aqualitative%20experiments%20on%20two%20widely%20employed%20datasets%2C%20VoxCeleb2%20and%20HDTF%2C%0Ashowcase%20that%20Dimitra%20is%20able%20to%20outperform%20existing%20approaches%20for%20generating%0Arealistic%20talking%20heads%20imparting%20lip%20motion%2C%20facial%20expression%2C%20and%20head%20pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17198v1&entry.124074799=Read"},
{"title": "TB-HSU: Hierarchical 3D Scene Understanding with Contextual Affordances", "author": "Wenting Xu and Viorela Ila and Luping Zhou and Craig T. Jin", "abstract": "  The concept of function and affordance is a critical aspect of 3D scene\nunderstanding and supports task-oriented objectives. In this work, we develop a\nmodel that learns to structure and vary functional affordance across a 3D\nhierarchical scene graph representing the spatial organization of a scene. The\nvarying functional affordance is designed to integrate with the varying spatial\ncontext of the graph. More specifically, we develop an algorithm that learns to\nconstruct a 3D hierarchical scene graph (3DHSG) that captures the spatial\norganization of the scene. Starting from segmented object point clouds and\nobject semantic labels, we develop a 3DHSG with a top node that identifies the\nroom label, child nodes that define local spatial regions inside the room with\nregion-specific affordances, and grand-child nodes indicating object locations\nand object-specific affordances. To support this work, we create a custom 3DHSG\ndataset that provides ground truth data for local spatial regions with\nregion-specific affordances and also object-specific affordances for each\nobject. We employ a transformer-based model to learn the 3DHSG. We use a\nmulti-task learning framework that learns both room classification and learns\nto define spatial regions within the room with region-specific affordances. Our\nwork improves on the performance of state-of-the-art baseline models and shows\none approach for applying transformer models to 3D scene understanding and the\ngeneration of 3DHSGs that capture the spatial organization of a room. The code\nand dataset are publicly available.\n", "link": "http://arxiv.org/abs/2412.05596v2", "date": "2025-02-24", "relevancy": 3.0577, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6155}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TB-HSU%3A%20Hierarchical%203D%20Scene%20Understanding%20with%20Contextual%20Affordances&body=Title%3A%20TB-HSU%3A%20Hierarchical%203D%20Scene%20Understanding%20with%20Contextual%20Affordances%0AAuthor%3A%20Wenting%20Xu%20and%20Viorela%20Ila%20and%20Luping%20Zhou%20and%20Craig%20T.%20Jin%0AAbstract%3A%20%20%20The%20concept%20of%20function%20and%20affordance%20is%20a%20critical%20aspect%20of%203D%20scene%0Aunderstanding%20and%20supports%20task-oriented%20objectives.%20In%20this%20work%2C%20we%20develop%20a%0Amodel%20that%20learns%20to%20structure%20and%20vary%20functional%20affordance%20across%20a%203D%0Ahierarchical%20scene%20graph%20representing%20the%20spatial%20organization%20of%20a%20scene.%20The%0Avarying%20functional%20affordance%20is%20designed%20to%20integrate%20with%20the%20varying%20spatial%0Acontext%20of%20the%20graph.%20More%20specifically%2C%20we%20develop%20an%20algorithm%20that%20learns%20to%0Aconstruct%20a%203D%20hierarchical%20scene%20graph%20%283DHSG%29%20that%20captures%20the%20spatial%0Aorganization%20of%20the%20scene.%20Starting%20from%20segmented%20object%20point%20clouds%20and%0Aobject%20semantic%20labels%2C%20we%20develop%20a%203DHSG%20with%20a%20top%20node%20that%20identifies%20the%0Aroom%20label%2C%20child%20nodes%20that%20define%20local%20spatial%20regions%20inside%20the%20room%20with%0Aregion-specific%20affordances%2C%20and%20grand-child%20nodes%20indicating%20object%20locations%0Aand%20object-specific%20affordances.%20To%20support%20this%20work%2C%20we%20create%20a%20custom%203DHSG%0Adataset%20that%20provides%20ground%20truth%20data%20for%20local%20spatial%20regions%20with%0Aregion-specific%20affordances%20and%20also%20object-specific%20affordances%20for%20each%0Aobject.%20We%20employ%20a%20transformer-based%20model%20to%20learn%20the%203DHSG.%20We%20use%20a%0Amulti-task%20learning%20framework%20that%20learns%20both%20room%20classification%20and%20learns%0Ato%20define%20spatial%20regions%20within%20the%20room%20with%20region-specific%20affordances.%20Our%0Awork%20improves%20on%20the%20performance%20of%20state-of-the-art%20baseline%20models%20and%20shows%0Aone%20approach%20for%20applying%20transformer%20models%20to%203D%20scene%20understanding%20and%20the%0Ageneration%20of%203DHSGs%20that%20capture%20the%20spatial%20organization%20of%20a%20room.%20The%20code%0Aand%20dataset%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTB-HSU%253A%2520Hierarchical%25203D%2520Scene%2520Understanding%2520with%2520Contextual%2520Affordances%26entry.906535625%3DWenting%2520Xu%2520and%2520Viorela%2520Ila%2520and%2520Luping%2520Zhou%2520and%2520Craig%2520T.%2520Jin%26entry.1292438233%3D%2520%2520The%2520concept%2520of%2520function%2520and%2520affordance%2520is%2520a%2520critical%2520aspect%2520of%25203D%2520scene%250Aunderstanding%2520and%2520supports%2520task-oriented%2520objectives.%2520In%2520this%2520work%252C%2520we%2520develop%2520a%250Amodel%2520that%2520learns%2520to%2520structure%2520and%2520vary%2520functional%2520affordance%2520across%2520a%25203D%250Ahierarchical%2520scene%2520graph%2520representing%2520the%2520spatial%2520organization%2520of%2520a%2520scene.%2520The%250Avarying%2520functional%2520affordance%2520is%2520designed%2520to%2520integrate%2520with%2520the%2520varying%2520spatial%250Acontext%2520of%2520the%2520graph.%2520More%2520specifically%252C%2520we%2520develop%2520an%2520algorithm%2520that%2520learns%2520to%250Aconstruct%2520a%25203D%2520hierarchical%2520scene%2520graph%2520%25283DHSG%2529%2520that%2520captures%2520the%2520spatial%250Aorganization%2520of%2520the%2520scene.%2520Starting%2520from%2520segmented%2520object%2520point%2520clouds%2520and%250Aobject%2520semantic%2520labels%252C%2520we%2520develop%2520a%25203DHSG%2520with%2520a%2520top%2520node%2520that%2520identifies%2520the%250Aroom%2520label%252C%2520child%2520nodes%2520that%2520define%2520local%2520spatial%2520regions%2520inside%2520the%2520room%2520with%250Aregion-specific%2520affordances%252C%2520and%2520grand-child%2520nodes%2520indicating%2520object%2520locations%250Aand%2520object-specific%2520affordances.%2520To%2520support%2520this%2520work%252C%2520we%2520create%2520a%2520custom%25203DHSG%250Adataset%2520that%2520provides%2520ground%2520truth%2520data%2520for%2520local%2520spatial%2520regions%2520with%250Aregion-specific%2520affordances%2520and%2520also%2520object-specific%2520affordances%2520for%2520each%250Aobject.%2520We%2520employ%2520a%2520transformer-based%2520model%2520to%2520learn%2520the%25203DHSG.%2520We%2520use%2520a%250Amulti-task%2520learning%2520framework%2520that%2520learns%2520both%2520room%2520classification%2520and%2520learns%250Ato%2520define%2520spatial%2520regions%2520within%2520the%2520room%2520with%2520region-specific%2520affordances.%2520Our%250Awork%2520improves%2520on%2520the%2520performance%2520of%2520state-of-the-art%2520baseline%2520models%2520and%2520shows%250Aone%2520approach%2520for%2520applying%2520transformer%2520models%2520to%25203D%2520scene%2520understanding%2520and%2520the%250Ageneration%2520of%25203DHSGs%2520that%2520capture%2520the%2520spatial%2520organization%2520of%2520a%2520room.%2520The%2520code%250Aand%2520dataset%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TB-HSU%3A%20Hierarchical%203D%20Scene%20Understanding%20with%20Contextual%20Affordances&entry.906535625=Wenting%20Xu%20and%20Viorela%20Ila%20and%20Luping%20Zhou%20and%20Craig%20T.%20Jin&entry.1292438233=%20%20The%20concept%20of%20function%20and%20affordance%20is%20a%20critical%20aspect%20of%203D%20scene%0Aunderstanding%20and%20supports%20task-oriented%20objectives.%20In%20this%20work%2C%20we%20develop%20a%0Amodel%20that%20learns%20to%20structure%20and%20vary%20functional%20affordance%20across%20a%203D%0Ahierarchical%20scene%20graph%20representing%20the%20spatial%20organization%20of%20a%20scene.%20The%0Avarying%20functional%20affordance%20is%20designed%20to%20integrate%20with%20the%20varying%20spatial%0Acontext%20of%20the%20graph.%20More%20specifically%2C%20we%20develop%20an%20algorithm%20that%20learns%20to%0Aconstruct%20a%203D%20hierarchical%20scene%20graph%20%283DHSG%29%20that%20captures%20the%20spatial%0Aorganization%20of%20the%20scene.%20Starting%20from%20segmented%20object%20point%20clouds%20and%0Aobject%20semantic%20labels%2C%20we%20develop%20a%203DHSG%20with%20a%20top%20node%20that%20identifies%20the%0Aroom%20label%2C%20child%20nodes%20that%20define%20local%20spatial%20regions%20inside%20the%20room%20with%0Aregion-specific%20affordances%2C%20and%20grand-child%20nodes%20indicating%20object%20locations%0Aand%20object-specific%20affordances.%20To%20support%20this%20work%2C%20we%20create%20a%20custom%203DHSG%0Adataset%20that%20provides%20ground%20truth%20data%20for%20local%20spatial%20regions%20with%0Aregion-specific%20affordances%20and%20also%20object-specific%20affordances%20for%20each%0Aobject.%20We%20employ%20a%20transformer-based%20model%20to%20learn%20the%203DHSG.%20We%20use%20a%0Amulti-task%20learning%20framework%20that%20learns%20both%20room%20classification%20and%20learns%0Ato%20define%20spatial%20regions%20within%20the%20room%20with%20region-specific%20affordances.%20Our%0Awork%20improves%20on%20the%20performance%20of%20state-of-the-art%20baseline%20models%20and%20shows%0Aone%20approach%20for%20applying%20transformer%20models%20to%203D%20scene%20understanding%20and%20the%0Ageneration%20of%203DHSGs%20that%20capture%20the%20spatial%20organization%20of%20a%20room.%20The%20code%0Aand%20dataset%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05596v2&entry.124074799=Read"},
{"title": "GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using\n  Gaussian Splatting and Temporal Flow", "author": "Simon Boeder and Fabian Gigengack and Benjamin Risse", "abstract": "  Occupancy estimation has become a prominent task in 3D computer vision,\nparticularly within the autonomous driving community. In this paper, we present\na novel approach to occupancy estimation, termed GaussianFlowOcc, which is\ninspired by Gaussian Splatting and replaces traditional dense voxel grids with\na sparse 3D Gaussian representation. Our efficient model architecture based on\na Gaussian Transformer significantly reduces computational and memory\nrequirements by eliminating the need for expensive 3D convolutions used with\ninefficient voxel-based representations that predominantly represent empty 3D\nspaces. GaussianFlowOcc effectively captures scene dynamics by estimating\ntemporal flow for each Gaussian during the overall network training process,\noffering a straightforward solution to a complex problem that is often\nneglected by existing methods. Moreover, GaussianFlowOcc is designed for\nscalability, as it employs weak supervision and does not require costly dense\n3D voxel annotations based on additional data (e.g., LiDAR). Through extensive\nexperimentation, we demonstrate that GaussianFlowOcc significantly outperforms\nall previous methods for weakly supervised occupancy estimation on the nuScenes\ndataset while featuring an inference speed that is 50 times faster than current\nSOTA.\n", "link": "http://arxiv.org/abs/2502.17288v1", "date": "2025-02-24", "relevancy": 3.0383, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6339}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.621}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianFlowOcc%3A%20Sparse%20and%20Weakly%20Supervised%20Occupancy%20Estimation%20using%0A%20%20Gaussian%20Splatting%20and%20Temporal%20Flow&body=Title%3A%20GaussianFlowOcc%3A%20Sparse%20and%20Weakly%20Supervised%20Occupancy%20Estimation%20using%0A%20%20Gaussian%20Splatting%20and%20Temporal%20Flow%0AAuthor%3A%20Simon%20Boeder%20and%20Fabian%20Gigengack%20and%20Benjamin%20Risse%0AAbstract%3A%20%20%20Occupancy%20estimation%20has%20become%20a%20prominent%20task%20in%203D%20computer%20vision%2C%0Aparticularly%20within%20the%20autonomous%20driving%20community.%20In%20this%20paper%2C%20we%20present%0Aa%20novel%20approach%20to%20occupancy%20estimation%2C%20termed%20GaussianFlowOcc%2C%20which%20is%0Ainspired%20by%20Gaussian%20Splatting%20and%20replaces%20traditional%20dense%20voxel%20grids%20with%0Aa%20sparse%203D%20Gaussian%20representation.%20Our%20efficient%20model%20architecture%20based%20on%0Aa%20Gaussian%20Transformer%20significantly%20reduces%20computational%20and%20memory%0Arequirements%20by%20eliminating%20the%20need%20for%20expensive%203D%20convolutions%20used%20with%0Ainefficient%20voxel-based%20representations%20that%20predominantly%20represent%20empty%203D%0Aspaces.%20GaussianFlowOcc%20effectively%20captures%20scene%20dynamics%20by%20estimating%0Atemporal%20flow%20for%20each%20Gaussian%20during%20the%20overall%20network%20training%20process%2C%0Aoffering%20a%20straightforward%20solution%20to%20a%20complex%20problem%20that%20is%20often%0Aneglected%20by%20existing%20methods.%20Moreover%2C%20GaussianFlowOcc%20is%20designed%20for%0Ascalability%2C%20as%20it%20employs%20weak%20supervision%20and%20does%20not%20require%20costly%20dense%0A3D%20voxel%20annotations%20based%20on%20additional%20data%20%28e.g.%2C%20LiDAR%29.%20Through%20extensive%0Aexperimentation%2C%20we%20demonstrate%20that%20GaussianFlowOcc%20significantly%20outperforms%0Aall%20previous%20methods%20for%20weakly%20supervised%20occupancy%20estimation%20on%20the%20nuScenes%0Adataset%20while%20featuring%20an%20inference%20speed%20that%20is%2050%20times%20faster%20than%20current%0ASOTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianFlowOcc%253A%2520Sparse%2520and%2520Weakly%2520Supervised%2520Occupancy%2520Estimation%2520using%250A%2520%2520Gaussian%2520Splatting%2520and%2520Temporal%2520Flow%26entry.906535625%3DSimon%2520Boeder%2520and%2520Fabian%2520Gigengack%2520and%2520Benjamin%2520Risse%26entry.1292438233%3D%2520%2520Occupancy%2520estimation%2520has%2520become%2520a%2520prominent%2520task%2520in%25203D%2520computer%2520vision%252C%250Aparticularly%2520within%2520the%2520autonomous%2520driving%2520community.%2520In%2520this%2520paper%252C%2520we%2520present%250Aa%2520novel%2520approach%2520to%2520occupancy%2520estimation%252C%2520termed%2520GaussianFlowOcc%252C%2520which%2520is%250Ainspired%2520by%2520Gaussian%2520Splatting%2520and%2520replaces%2520traditional%2520dense%2520voxel%2520grids%2520with%250Aa%2520sparse%25203D%2520Gaussian%2520representation.%2520Our%2520efficient%2520model%2520architecture%2520based%2520on%250Aa%2520Gaussian%2520Transformer%2520significantly%2520reduces%2520computational%2520and%2520memory%250Arequirements%2520by%2520eliminating%2520the%2520need%2520for%2520expensive%25203D%2520convolutions%2520used%2520with%250Ainefficient%2520voxel-based%2520representations%2520that%2520predominantly%2520represent%2520empty%25203D%250Aspaces.%2520GaussianFlowOcc%2520effectively%2520captures%2520scene%2520dynamics%2520by%2520estimating%250Atemporal%2520flow%2520for%2520each%2520Gaussian%2520during%2520the%2520overall%2520network%2520training%2520process%252C%250Aoffering%2520a%2520straightforward%2520solution%2520to%2520a%2520complex%2520problem%2520that%2520is%2520often%250Aneglected%2520by%2520existing%2520methods.%2520Moreover%252C%2520GaussianFlowOcc%2520is%2520designed%2520for%250Ascalability%252C%2520as%2520it%2520employs%2520weak%2520supervision%2520and%2520does%2520not%2520require%2520costly%2520dense%250A3D%2520voxel%2520annotations%2520based%2520on%2520additional%2520data%2520%2528e.g.%252C%2520LiDAR%2529.%2520Through%2520extensive%250Aexperimentation%252C%2520we%2520demonstrate%2520that%2520GaussianFlowOcc%2520significantly%2520outperforms%250Aall%2520previous%2520methods%2520for%2520weakly%2520supervised%2520occupancy%2520estimation%2520on%2520the%2520nuScenes%250Adataset%2520while%2520featuring%2520an%2520inference%2520speed%2520that%2520is%252050%2520times%2520faster%2520than%2520current%250ASOTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianFlowOcc%3A%20Sparse%20and%20Weakly%20Supervised%20Occupancy%20Estimation%20using%0A%20%20Gaussian%20Splatting%20and%20Temporal%20Flow&entry.906535625=Simon%20Boeder%20and%20Fabian%20Gigengack%20and%20Benjamin%20Risse&entry.1292438233=%20%20Occupancy%20estimation%20has%20become%20a%20prominent%20task%20in%203D%20computer%20vision%2C%0Aparticularly%20within%20the%20autonomous%20driving%20community.%20In%20this%20paper%2C%20we%20present%0Aa%20novel%20approach%20to%20occupancy%20estimation%2C%20termed%20GaussianFlowOcc%2C%20which%20is%0Ainspired%20by%20Gaussian%20Splatting%20and%20replaces%20traditional%20dense%20voxel%20grids%20with%0Aa%20sparse%203D%20Gaussian%20representation.%20Our%20efficient%20model%20architecture%20based%20on%0Aa%20Gaussian%20Transformer%20significantly%20reduces%20computational%20and%20memory%0Arequirements%20by%20eliminating%20the%20need%20for%20expensive%203D%20convolutions%20used%20with%0Ainefficient%20voxel-based%20representations%20that%20predominantly%20represent%20empty%203D%0Aspaces.%20GaussianFlowOcc%20effectively%20captures%20scene%20dynamics%20by%20estimating%0Atemporal%20flow%20for%20each%20Gaussian%20during%20the%20overall%20network%20training%20process%2C%0Aoffering%20a%20straightforward%20solution%20to%20a%20complex%20problem%20that%20is%20often%0Aneglected%20by%20existing%20methods.%20Moreover%2C%20GaussianFlowOcc%20is%20designed%20for%0Ascalability%2C%20as%20it%20employs%20weak%20supervision%20and%20does%20not%20require%20costly%20dense%0A3D%20voxel%20annotations%20based%20on%20additional%20data%20%28e.g.%2C%20LiDAR%29.%20Through%20extensive%0Aexperimentation%2C%20we%20demonstrate%20that%20GaussianFlowOcc%20significantly%20outperforms%0Aall%20previous%20methods%20for%20weakly%20supervised%20occupancy%20estimation%20on%20the%20nuScenes%0Adataset%20while%20featuring%20an%20inference%20speed%20that%20is%2050%20times%20faster%20than%20current%0ASOTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17288v1&entry.124074799=Read"},
{"title": "CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping", "author": "Yufei Lu and Yuetao Li and Zhizhou Jia and Qun Hao and Shaohui Zhang", "abstract": "  In this letter, we propose a color-assisted robust framework for accurate\nLiDAR odometry and mapping (LOAM). Simultaneously receiving data from both the\nLiDAR and the camera, the framework utilizes the color information from the\ncamera images to colorize the LiDAR point clouds and then performs iterative\npose optimization. For each LiDAR scan, the edge and planar features are\nextracted and colored using the corresponding image and then matched to a\nglobal map. Specifically, we adopt a perceptually uniform color difference\nweighting strategy to exclude color correspondence outliers and a robust error\nmetric based on the Welsch's function to mitigate the impact of positional\ncorrespondence outliers during the pose optimization process. As a result, the\nsystem achieves accurate localization and reconstructs dense, accurate, colored\nand three-dimensional (3D) maps of the environment. Thorough experiments with\nchallenging scenarios, including complex forests and a campus, show that our\nmethod provides higher robustness and accuracy compared with current\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2502.17249v1", "date": "2025-02-24", "relevancy": 2.966, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6554}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAR-LOAM%3A%20Color-Assisted%20Robust%20LiDAR%20Odometry%20and%20Mapping&body=Title%3A%20CAR-LOAM%3A%20Color-Assisted%20Robust%20LiDAR%20Odometry%20and%20Mapping%0AAuthor%3A%20Yufei%20Lu%20and%20Yuetao%20Li%20and%20Zhizhou%20Jia%20and%20Qun%20Hao%20and%20Shaohui%20Zhang%0AAbstract%3A%20%20%20In%20this%20letter%2C%20we%20propose%20a%20color-assisted%20robust%20framework%20for%20accurate%0ALiDAR%20odometry%20and%20mapping%20%28LOAM%29.%20Simultaneously%20receiving%20data%20from%20both%20the%0ALiDAR%20and%20the%20camera%2C%20the%20framework%20utilizes%20the%20color%20information%20from%20the%0Acamera%20images%20to%20colorize%20the%20LiDAR%20point%20clouds%20and%20then%20performs%20iterative%0Apose%20optimization.%20For%20each%20LiDAR%20scan%2C%20the%20edge%20and%20planar%20features%20are%0Aextracted%20and%20colored%20using%20the%20corresponding%20image%20and%20then%20matched%20to%20a%0Aglobal%20map.%20Specifically%2C%20we%20adopt%20a%20perceptually%20uniform%20color%20difference%0Aweighting%20strategy%20to%20exclude%20color%20correspondence%20outliers%20and%20a%20robust%20error%0Ametric%20based%20on%20the%20Welsch%27s%20function%20to%20mitigate%20the%20impact%20of%20positional%0Acorrespondence%20outliers%20during%20the%20pose%20optimization%20process.%20As%20a%20result%2C%20the%0Asystem%20achieves%20accurate%20localization%20and%20reconstructs%20dense%2C%20accurate%2C%20colored%0Aand%20three-dimensional%20%283D%29%20maps%20of%20the%20environment.%20Thorough%20experiments%20with%0Achallenging%20scenarios%2C%20including%20complex%20forests%20and%20a%20campus%2C%20show%20that%20our%0Amethod%20provides%20higher%20robustness%20and%20accuracy%20compared%20with%20current%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAR-LOAM%253A%2520Color-Assisted%2520Robust%2520LiDAR%2520Odometry%2520and%2520Mapping%26entry.906535625%3DYufei%2520Lu%2520and%2520Yuetao%2520Li%2520and%2520Zhizhou%2520Jia%2520and%2520Qun%2520Hao%2520and%2520Shaohui%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520letter%252C%2520we%2520propose%2520a%2520color-assisted%2520robust%2520framework%2520for%2520accurate%250ALiDAR%2520odometry%2520and%2520mapping%2520%2528LOAM%2529.%2520Simultaneously%2520receiving%2520data%2520from%2520both%2520the%250ALiDAR%2520and%2520the%2520camera%252C%2520the%2520framework%2520utilizes%2520the%2520color%2520information%2520from%2520the%250Acamera%2520images%2520to%2520colorize%2520the%2520LiDAR%2520point%2520clouds%2520and%2520then%2520performs%2520iterative%250Apose%2520optimization.%2520For%2520each%2520LiDAR%2520scan%252C%2520the%2520edge%2520and%2520planar%2520features%2520are%250Aextracted%2520and%2520colored%2520using%2520the%2520corresponding%2520image%2520and%2520then%2520matched%2520to%2520a%250Aglobal%2520map.%2520Specifically%252C%2520we%2520adopt%2520a%2520perceptually%2520uniform%2520color%2520difference%250Aweighting%2520strategy%2520to%2520exclude%2520color%2520correspondence%2520outliers%2520and%2520a%2520robust%2520error%250Ametric%2520based%2520on%2520the%2520Welsch%2527s%2520function%2520to%2520mitigate%2520the%2520impact%2520of%2520positional%250Acorrespondence%2520outliers%2520during%2520the%2520pose%2520optimization%2520process.%2520As%2520a%2520result%252C%2520the%250Asystem%2520achieves%2520accurate%2520localization%2520and%2520reconstructs%2520dense%252C%2520accurate%252C%2520colored%250Aand%2520three-dimensional%2520%25283D%2529%2520maps%2520of%2520the%2520environment.%2520Thorough%2520experiments%2520with%250Achallenging%2520scenarios%252C%2520including%2520complex%2520forests%2520and%2520a%2520campus%252C%2520show%2520that%2520our%250Amethod%2520provides%2520higher%2520robustness%2520and%2520accuracy%2520compared%2520with%2520current%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAR-LOAM%3A%20Color-Assisted%20Robust%20LiDAR%20Odometry%20and%20Mapping&entry.906535625=Yufei%20Lu%20and%20Yuetao%20Li%20and%20Zhizhou%20Jia%20and%20Qun%20Hao%20and%20Shaohui%20Zhang&entry.1292438233=%20%20In%20this%20letter%2C%20we%20propose%20a%20color-assisted%20robust%20framework%20for%20accurate%0ALiDAR%20odometry%20and%20mapping%20%28LOAM%29.%20Simultaneously%20receiving%20data%20from%20both%20the%0ALiDAR%20and%20the%20camera%2C%20the%20framework%20utilizes%20the%20color%20information%20from%20the%0Acamera%20images%20to%20colorize%20the%20LiDAR%20point%20clouds%20and%20then%20performs%20iterative%0Apose%20optimization.%20For%20each%20LiDAR%20scan%2C%20the%20edge%20and%20planar%20features%20are%0Aextracted%20and%20colored%20using%20the%20corresponding%20image%20and%20then%20matched%20to%20a%0Aglobal%20map.%20Specifically%2C%20we%20adopt%20a%20perceptually%20uniform%20color%20difference%0Aweighting%20strategy%20to%20exclude%20color%20correspondence%20outliers%20and%20a%20robust%20error%0Ametric%20based%20on%20the%20Welsch%27s%20function%20to%20mitigate%20the%20impact%20of%20positional%0Acorrespondence%20outliers%20during%20the%20pose%20optimization%20process.%20As%20a%20result%2C%20the%0Asystem%20achieves%20accurate%20localization%20and%20reconstructs%20dense%2C%20accurate%2C%20colored%0Aand%20three-dimensional%20%283D%29%20maps%20of%20the%20environment.%20Thorough%20experiments%20with%0Achallenging%20scenarios%2C%20including%20complex%20forests%20and%20a%20campus%2C%20show%20that%20our%0Amethod%20provides%20higher%20robustness%20and%20accuracy%20compared%20with%20current%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17249v1&entry.124074799=Read"},
{"title": "Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder", "author": "Antoine Schnepf and Karim Kassab and Jean-Yves Franceschi and Laurent Caraffa and Flavian Vasile and Jeremie Mary and Andrew Comport and Valerie Gouet-Brunet", "abstract": "  While pre-trained image autoencoders are increasingly utilized in computer\nvision, the application of inverse graphics in 2D latent spaces has been\nunder-explored. Yet, besides reducing the training and rendering complexity,\napplying inverse graphics in the latent space enables a valuable\ninteroperability with other latent-based 2D methods. The major challenge is\nthat inverse graphics cannot be directly applied to such image latent spaces\nbecause they lack an underlying 3D geometry. In this paper, we propose an\nInverse Graphics Autoencoder (IG-AE) that specifically addresses this issue. To\nthis end, we regularize an image autoencoder with 3D-geometry by aligning its\nlatent space with jointly trained latent 3D scenes. We utilize the trained\nIG-AE to bring NeRFs to the latent space with a latent NeRF training pipeline,\nwhich we implement in an open-source extension of the Nerfstudio framework,\nthereby unlocking latent scene learning for its supported methods. We\nexperimentally confirm that Latent NeRFs trained with IG-AE present an improved\nquality compared to a standard autoencoder, all while exhibiting training and\nrendering accelerations with respect to NeRFs trained in the image space. Our\nproject page can be found at https://ig-ae.github.io .\n", "link": "http://arxiv.org/abs/2410.22936v2", "date": "2025-02-24", "relevancy": 2.8777, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5949}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5749}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bringing%20NeRFs%20to%20the%20Latent%20Space%3A%20Inverse%20Graphics%20Autoencoder&body=Title%3A%20Bringing%20NeRFs%20to%20the%20Latent%20Space%3A%20Inverse%20Graphics%20Autoencoder%0AAuthor%3A%20Antoine%20Schnepf%20and%20Karim%20Kassab%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Valerie%20Gouet-Brunet%0AAbstract%3A%20%20%20While%20pre-trained%20image%20autoencoders%20are%20increasingly%20utilized%20in%20computer%0Avision%2C%20the%20application%20of%20inverse%20graphics%20in%202D%20latent%20spaces%20has%20been%0Aunder-explored.%20Yet%2C%20besides%20reducing%20the%20training%20and%20rendering%20complexity%2C%0Aapplying%20inverse%20graphics%20in%20the%20latent%20space%20enables%20a%20valuable%0Ainteroperability%20with%20other%20latent-based%202D%20methods.%20The%20major%20challenge%20is%0Athat%20inverse%20graphics%20cannot%20be%20directly%20applied%20to%20such%20image%20latent%20spaces%0Abecause%20they%20lack%20an%20underlying%203D%20geometry.%20In%20this%20paper%2C%20we%20propose%20an%0AInverse%20Graphics%20Autoencoder%20%28IG-AE%29%20that%20specifically%20addresses%20this%20issue.%20To%0Athis%20end%2C%20we%20regularize%20an%20image%20autoencoder%20with%203D-geometry%20by%20aligning%20its%0Alatent%20space%20with%20jointly%20trained%20latent%203D%20scenes.%20We%20utilize%20the%20trained%0AIG-AE%20to%20bring%20NeRFs%20to%20the%20latent%20space%20with%20a%20latent%20NeRF%20training%20pipeline%2C%0Awhich%20we%20implement%20in%20an%20open-source%20extension%20of%20the%20Nerfstudio%20framework%2C%0Athereby%20unlocking%20latent%20scene%20learning%20for%20its%20supported%20methods.%20We%0Aexperimentally%20confirm%20that%20Latent%20NeRFs%20trained%20with%20IG-AE%20present%20an%20improved%0Aquality%20compared%20to%20a%20standard%20autoencoder%2C%20all%20while%20exhibiting%20training%20and%0Arendering%20accelerations%20with%20respect%20to%20NeRFs%20trained%20in%20the%20image%20space.%20Our%0Aproject%20page%20can%20be%20found%20at%20https%3A//ig-ae.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBringing%2520NeRFs%2520to%2520the%2520Latent%2520Space%253A%2520Inverse%2520Graphics%2520Autoencoder%26entry.906535625%3DAntoine%2520Schnepf%2520and%2520Karim%2520Kassab%2520and%2520Jean-Yves%2520Franceschi%2520and%2520Laurent%2520Caraffa%2520and%2520Flavian%2520Vasile%2520and%2520Jeremie%2520Mary%2520and%2520Andrew%2520Comport%2520and%2520Valerie%2520Gouet-Brunet%26entry.1292438233%3D%2520%2520While%2520pre-trained%2520image%2520autoencoders%2520are%2520increasingly%2520utilized%2520in%2520computer%250Avision%252C%2520the%2520application%2520of%2520inverse%2520graphics%2520in%25202D%2520latent%2520spaces%2520has%2520been%250Aunder-explored.%2520Yet%252C%2520besides%2520reducing%2520the%2520training%2520and%2520rendering%2520complexity%252C%250Aapplying%2520inverse%2520graphics%2520in%2520the%2520latent%2520space%2520enables%2520a%2520valuable%250Ainteroperability%2520with%2520other%2520latent-based%25202D%2520methods.%2520The%2520major%2520challenge%2520is%250Athat%2520inverse%2520graphics%2520cannot%2520be%2520directly%2520applied%2520to%2520such%2520image%2520latent%2520spaces%250Abecause%2520they%2520lack%2520an%2520underlying%25203D%2520geometry.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250AInverse%2520Graphics%2520Autoencoder%2520%2528IG-AE%2529%2520that%2520specifically%2520addresses%2520this%2520issue.%2520To%250Athis%2520end%252C%2520we%2520regularize%2520an%2520image%2520autoencoder%2520with%25203D-geometry%2520by%2520aligning%2520its%250Alatent%2520space%2520with%2520jointly%2520trained%2520latent%25203D%2520scenes.%2520We%2520utilize%2520the%2520trained%250AIG-AE%2520to%2520bring%2520NeRFs%2520to%2520the%2520latent%2520space%2520with%2520a%2520latent%2520NeRF%2520training%2520pipeline%252C%250Awhich%2520we%2520implement%2520in%2520an%2520open-source%2520extension%2520of%2520the%2520Nerfstudio%2520framework%252C%250Athereby%2520unlocking%2520latent%2520scene%2520learning%2520for%2520its%2520supported%2520methods.%2520We%250Aexperimentally%2520confirm%2520that%2520Latent%2520NeRFs%2520trained%2520with%2520IG-AE%2520present%2520an%2520improved%250Aquality%2520compared%2520to%2520a%2520standard%2520autoencoder%252C%2520all%2520while%2520exhibiting%2520training%2520and%250Arendering%2520accelerations%2520with%2520respect%2520to%2520NeRFs%2520trained%2520in%2520the%2520image%2520space.%2520Our%250Aproject%2520page%2520can%2520be%2520found%2520at%2520https%253A//ig-ae.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bringing%20NeRFs%20to%20the%20Latent%20Space%3A%20Inverse%20Graphics%20Autoencoder&entry.906535625=Antoine%20Schnepf%20and%20Karim%20Kassab%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Valerie%20Gouet-Brunet&entry.1292438233=%20%20While%20pre-trained%20image%20autoencoders%20are%20increasingly%20utilized%20in%20computer%0Avision%2C%20the%20application%20of%20inverse%20graphics%20in%202D%20latent%20spaces%20has%20been%0Aunder-explored.%20Yet%2C%20besides%20reducing%20the%20training%20and%20rendering%20complexity%2C%0Aapplying%20inverse%20graphics%20in%20the%20latent%20space%20enables%20a%20valuable%0Ainteroperability%20with%20other%20latent-based%202D%20methods.%20The%20major%20challenge%20is%0Athat%20inverse%20graphics%20cannot%20be%20directly%20applied%20to%20such%20image%20latent%20spaces%0Abecause%20they%20lack%20an%20underlying%203D%20geometry.%20In%20this%20paper%2C%20we%20propose%20an%0AInverse%20Graphics%20Autoencoder%20%28IG-AE%29%20that%20specifically%20addresses%20this%20issue.%20To%0Athis%20end%2C%20we%20regularize%20an%20image%20autoencoder%20with%203D-geometry%20by%20aligning%20its%0Alatent%20space%20with%20jointly%20trained%20latent%203D%20scenes.%20We%20utilize%20the%20trained%0AIG-AE%20to%20bring%20NeRFs%20to%20the%20latent%20space%20with%20a%20latent%20NeRF%20training%20pipeline%2C%0Awhich%20we%20implement%20in%20an%20open-source%20extension%20of%20the%20Nerfstudio%20framework%2C%0Athereby%20unlocking%20latent%20scene%20learning%20for%20its%20supported%20methods.%20We%0Aexperimentally%20confirm%20that%20Latent%20NeRFs%20trained%20with%20IG-AE%20present%20an%20improved%0Aquality%20compared%20to%20a%20standard%20autoencoder%2C%20all%20while%20exhibiting%20training%20and%0Arendering%20accelerations%20with%20respect%20to%20NeRFs%20trained%20in%20the%20image%20space.%20Our%0Aproject%20page%20can%20be%20found%20at%20https%3A//ig-ae.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22936v2&entry.124074799=Read"},
{"title": "Unraveling the geometry of visual relational reasoning", "author": "Jiaqi Shang and Gabriel Kreiman and Haim Sompolinsky", "abstract": "  Humans and other animals readily generalize abstract relations, such as\nrecognizing constant in shape or color, whereas neural networks struggle. To\ninvestigate how neural networks generalize abstract relations, we introduce\nSimplifiedRPM, a novel benchmark for systematic evaluation. In parallel, we\nconduct human experiments to benchmark relational difficulty, enabling direct\nmodel-human comparisons. Testing four architectures--ResNet-50, Vision\nTransformer, Wild Relation Network, and Scattering Compositional Learner\n(SCL)--we find that SCL best aligns with human behavior and generalizes best.\nBuilding on a geometric theory of neural representations, we show\nrepresentational geometries that predict generalization. Layer-wise analysis\nreveals distinct relational reasoning strategies across models and suggests a\ntrade-off where unseen rule representations compress into training-shaped\nsubspaces. Guided by our geometric perspective, we propose and evaluate\nSNRloss, a novel objective balancing representation geometry. Our findings\noffer geometric insights into how neural networks generalize abstract\nrelations, paving the way for more human-like visual reasoning in AI.\n", "link": "http://arxiv.org/abs/2502.17382v1", "date": "2025-02-24", "relevancy": 2.8724, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unraveling%20the%20geometry%20of%20visual%20relational%20reasoning&body=Title%3A%20Unraveling%20the%20geometry%20of%20visual%20relational%20reasoning%0AAuthor%3A%20Jiaqi%20Shang%20and%20Gabriel%20Kreiman%20and%20Haim%20Sompolinsky%0AAbstract%3A%20%20%20Humans%20and%20other%20animals%20readily%20generalize%20abstract%20relations%2C%20such%20as%0Arecognizing%20constant%20in%20shape%20or%20color%2C%20whereas%20neural%20networks%20struggle.%20To%0Ainvestigate%20how%20neural%20networks%20generalize%20abstract%20relations%2C%20we%20introduce%0ASimplifiedRPM%2C%20a%20novel%20benchmark%20for%20systematic%20evaluation.%20In%20parallel%2C%20we%0Aconduct%20human%20experiments%20to%20benchmark%20relational%20difficulty%2C%20enabling%20direct%0Amodel-human%20comparisons.%20Testing%20four%20architectures--ResNet-50%2C%20Vision%0ATransformer%2C%20Wild%20Relation%20Network%2C%20and%20Scattering%20Compositional%20Learner%0A%28SCL%29--we%20find%20that%20SCL%20best%20aligns%20with%20human%20behavior%20and%20generalizes%20best.%0ABuilding%20on%20a%20geometric%20theory%20of%20neural%20representations%2C%20we%20show%0Arepresentational%20geometries%20that%20predict%20generalization.%20Layer-wise%20analysis%0Areveals%20distinct%20relational%20reasoning%20strategies%20across%20models%20and%20suggests%20a%0Atrade-off%20where%20unseen%20rule%20representations%20compress%20into%20training-shaped%0Asubspaces.%20Guided%20by%20our%20geometric%20perspective%2C%20we%20propose%20and%20evaluate%0ASNRloss%2C%20a%20novel%20objective%20balancing%20representation%20geometry.%20Our%20findings%0Aoffer%20geometric%20insights%20into%20how%20neural%20networks%20generalize%20abstract%0Arelations%2C%20paving%20the%20way%20for%20more%20human-like%20visual%20reasoning%20in%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnraveling%2520the%2520geometry%2520of%2520visual%2520relational%2520reasoning%26entry.906535625%3DJiaqi%2520Shang%2520and%2520Gabriel%2520Kreiman%2520and%2520Haim%2520Sompolinsky%26entry.1292438233%3D%2520%2520Humans%2520and%2520other%2520animals%2520readily%2520generalize%2520abstract%2520relations%252C%2520such%2520as%250Arecognizing%2520constant%2520in%2520shape%2520or%2520color%252C%2520whereas%2520neural%2520networks%2520struggle.%2520To%250Ainvestigate%2520how%2520neural%2520networks%2520generalize%2520abstract%2520relations%252C%2520we%2520introduce%250ASimplifiedRPM%252C%2520a%2520novel%2520benchmark%2520for%2520systematic%2520evaluation.%2520In%2520parallel%252C%2520we%250Aconduct%2520human%2520experiments%2520to%2520benchmark%2520relational%2520difficulty%252C%2520enabling%2520direct%250Amodel-human%2520comparisons.%2520Testing%2520four%2520architectures--ResNet-50%252C%2520Vision%250ATransformer%252C%2520Wild%2520Relation%2520Network%252C%2520and%2520Scattering%2520Compositional%2520Learner%250A%2528SCL%2529--we%2520find%2520that%2520SCL%2520best%2520aligns%2520with%2520human%2520behavior%2520and%2520generalizes%2520best.%250ABuilding%2520on%2520a%2520geometric%2520theory%2520of%2520neural%2520representations%252C%2520we%2520show%250Arepresentational%2520geometries%2520that%2520predict%2520generalization.%2520Layer-wise%2520analysis%250Areveals%2520distinct%2520relational%2520reasoning%2520strategies%2520across%2520models%2520and%2520suggests%2520a%250Atrade-off%2520where%2520unseen%2520rule%2520representations%2520compress%2520into%2520training-shaped%250Asubspaces.%2520Guided%2520by%2520our%2520geometric%2520perspective%252C%2520we%2520propose%2520and%2520evaluate%250ASNRloss%252C%2520a%2520novel%2520objective%2520balancing%2520representation%2520geometry.%2520Our%2520findings%250Aoffer%2520geometric%2520insights%2520into%2520how%2520neural%2520networks%2520generalize%2520abstract%250Arelations%252C%2520paving%2520the%2520way%2520for%2520more%2520human-like%2520visual%2520reasoning%2520in%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unraveling%20the%20geometry%20of%20visual%20relational%20reasoning&entry.906535625=Jiaqi%20Shang%20and%20Gabriel%20Kreiman%20and%20Haim%20Sompolinsky&entry.1292438233=%20%20Humans%20and%20other%20animals%20readily%20generalize%20abstract%20relations%2C%20such%20as%0Arecognizing%20constant%20in%20shape%20or%20color%2C%20whereas%20neural%20networks%20struggle.%20To%0Ainvestigate%20how%20neural%20networks%20generalize%20abstract%20relations%2C%20we%20introduce%0ASimplifiedRPM%2C%20a%20novel%20benchmark%20for%20systematic%20evaluation.%20In%20parallel%2C%20we%0Aconduct%20human%20experiments%20to%20benchmark%20relational%20difficulty%2C%20enabling%20direct%0Amodel-human%20comparisons.%20Testing%20four%20architectures--ResNet-50%2C%20Vision%0ATransformer%2C%20Wild%20Relation%20Network%2C%20and%20Scattering%20Compositional%20Learner%0A%28SCL%29--we%20find%20that%20SCL%20best%20aligns%20with%20human%20behavior%20and%20generalizes%20best.%0ABuilding%20on%20a%20geometric%20theory%20of%20neural%20representations%2C%20we%20show%0Arepresentational%20geometries%20that%20predict%20generalization.%20Layer-wise%20analysis%0Areveals%20distinct%20relational%20reasoning%20strategies%20across%20models%20and%20suggests%20a%0Atrade-off%20where%20unseen%20rule%20representations%20compress%20into%20training-shaped%0Asubspaces.%20Guided%20by%20our%20geometric%20perspective%2C%20we%20propose%20and%20evaluate%0ASNRloss%2C%20a%20novel%20objective%20balancing%20representation%20geometry.%20Our%20findings%0Aoffer%20geometric%20insights%20into%20how%20neural%20networks%20generalize%20abstract%0Arelations%2C%20paving%20the%20way%20for%20more%20human-like%20visual%20reasoning%20in%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17382v1&entry.124074799=Read"},
{"title": "X Modality Assisting RGBT Object Tracking", "author": "Zhaisheng Ding and Haiyan Li and Ruichao Hou and Yanyu Liu and Shidong Xie", "abstract": "  Developing robust multi-modal feature representations is crucial for\nenhancing object tracking performance. In pursuit of this objective, a novel X\nModality Assisting Network (X-Net) is introduced, which explores the impact of\nthe fusion paradigm by decoupling visual object tracking into three distinct\nlevels, thereby facilitating subsequent processing. Initially, to overcome the\nchallenges associated with feature learning due to significant discrepancies\nbetween RGB and thermal modalities, a plug-and-play pixel-level generation\nmodule (PGM) based on knowledge distillation learning is proposed. This module\neffectively generates the X modality, bridging the gap between the two patterns\nwhile minimizing noise interference. Subsequently, to optimize sample feature\nrepresentation and promote cross-modal interactions, a feature-level\ninteraction module (FIM) is introduced, integrating a mixed feature interaction\ntransformer and a spatial dimensional feature translation strategy. Finally, to\naddress random drifting caused by missing instance features, a flexible online\noptimization strategy called the decision-level refinement module (DRM) is\nproposed, which incorporates optical flow and refinement mechanisms. The\nefficacy of X-Net is validated through experiments on three benchmarks,\ndemonstrating its superiority over state-of-the-art trackers. Notably, X-Net\nachieves performance gains of 0.47%/1.2% in the average of precise rate and\nsuccess rate, respectively. Additionally, the research content, data, and code\nare pledged to be made publicly accessible at\nhttps://github.com/DZSYUNNAN/XNet.\n", "link": "http://arxiv.org/abs/2312.17273v2", "date": "2025-02-24", "relevancy": 2.8609, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X%20Modality%20Assisting%20RGBT%20Object%20Tracking&body=Title%3A%20X%20Modality%20Assisting%20RGBT%20Object%20Tracking%0AAuthor%3A%20Zhaisheng%20Ding%20and%20Haiyan%20Li%20and%20Ruichao%20Hou%20and%20Yanyu%20Liu%20and%20Shidong%20Xie%0AAbstract%3A%20%20%20Developing%20robust%20multi-modal%20feature%20representations%20is%20crucial%20for%0Aenhancing%20object%20tracking%20performance.%20In%20pursuit%20of%20this%20objective%2C%20a%20novel%20X%0AModality%20Assisting%20Network%20%28X-Net%29%20is%20introduced%2C%20which%20explores%20the%20impact%20of%0Athe%20fusion%20paradigm%20by%20decoupling%20visual%20object%20tracking%20into%20three%20distinct%0Alevels%2C%20thereby%20facilitating%20subsequent%20processing.%20Initially%2C%20to%20overcome%20the%0Achallenges%20associated%20with%20feature%20learning%20due%20to%20significant%20discrepancies%0Abetween%20RGB%20and%20thermal%20modalities%2C%20a%20plug-and-play%20pixel-level%20generation%0Amodule%20%28PGM%29%20based%20on%20knowledge%20distillation%20learning%20is%20proposed.%20This%20module%0Aeffectively%20generates%20the%20X%20modality%2C%20bridging%20the%20gap%20between%20the%20two%20patterns%0Awhile%20minimizing%20noise%20interference.%20Subsequently%2C%20to%20optimize%20sample%20feature%0Arepresentation%20and%20promote%20cross-modal%20interactions%2C%20a%20feature-level%0Ainteraction%20module%20%28FIM%29%20is%20introduced%2C%20integrating%20a%20mixed%20feature%20interaction%0Atransformer%20and%20a%20spatial%20dimensional%20feature%20translation%20strategy.%20Finally%2C%20to%0Aaddress%20random%20drifting%20caused%20by%20missing%20instance%20features%2C%20a%20flexible%20online%0Aoptimization%20strategy%20called%20the%20decision-level%20refinement%20module%20%28DRM%29%20is%0Aproposed%2C%20which%20incorporates%20optical%20flow%20and%20refinement%20mechanisms.%20The%0Aefficacy%20of%20X-Net%20is%20validated%20through%20experiments%20on%20three%20benchmarks%2C%0Ademonstrating%20its%20superiority%20over%20state-of-the-art%20trackers.%20Notably%2C%20X-Net%0Aachieves%20performance%20gains%20of%200.47%25/1.2%25%20in%20the%20average%20of%20precise%20rate%20and%0Asuccess%20rate%2C%20respectively.%20Additionally%2C%20the%20research%20content%2C%20data%2C%20and%20code%0Aare%20pledged%20to%20be%20made%20publicly%20accessible%20at%0Ahttps%3A//github.com/DZSYUNNAN/XNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX%2520Modality%2520Assisting%2520RGBT%2520Object%2520Tracking%26entry.906535625%3DZhaisheng%2520Ding%2520and%2520Haiyan%2520Li%2520and%2520Ruichao%2520Hou%2520and%2520Yanyu%2520Liu%2520and%2520Shidong%2520Xie%26entry.1292438233%3D%2520%2520Developing%2520robust%2520multi-modal%2520feature%2520representations%2520is%2520crucial%2520for%250Aenhancing%2520object%2520tracking%2520performance.%2520In%2520pursuit%2520of%2520this%2520objective%252C%2520a%2520novel%2520X%250AModality%2520Assisting%2520Network%2520%2528X-Net%2529%2520is%2520introduced%252C%2520which%2520explores%2520the%2520impact%2520of%250Athe%2520fusion%2520paradigm%2520by%2520decoupling%2520visual%2520object%2520tracking%2520into%2520three%2520distinct%250Alevels%252C%2520thereby%2520facilitating%2520subsequent%2520processing.%2520Initially%252C%2520to%2520overcome%2520the%250Achallenges%2520associated%2520with%2520feature%2520learning%2520due%2520to%2520significant%2520discrepancies%250Abetween%2520RGB%2520and%2520thermal%2520modalities%252C%2520a%2520plug-and-play%2520pixel-level%2520generation%250Amodule%2520%2528PGM%2529%2520based%2520on%2520knowledge%2520distillation%2520learning%2520is%2520proposed.%2520This%2520module%250Aeffectively%2520generates%2520the%2520X%2520modality%252C%2520bridging%2520the%2520gap%2520between%2520the%2520two%2520patterns%250Awhile%2520minimizing%2520noise%2520interference.%2520Subsequently%252C%2520to%2520optimize%2520sample%2520feature%250Arepresentation%2520and%2520promote%2520cross-modal%2520interactions%252C%2520a%2520feature-level%250Ainteraction%2520module%2520%2528FIM%2529%2520is%2520introduced%252C%2520integrating%2520a%2520mixed%2520feature%2520interaction%250Atransformer%2520and%2520a%2520spatial%2520dimensional%2520feature%2520translation%2520strategy.%2520Finally%252C%2520to%250Aaddress%2520random%2520drifting%2520caused%2520by%2520missing%2520instance%2520features%252C%2520a%2520flexible%2520online%250Aoptimization%2520strategy%2520called%2520the%2520decision-level%2520refinement%2520module%2520%2528DRM%2529%2520is%250Aproposed%252C%2520which%2520incorporates%2520optical%2520flow%2520and%2520refinement%2520mechanisms.%2520The%250Aefficacy%2520of%2520X-Net%2520is%2520validated%2520through%2520experiments%2520on%2520three%2520benchmarks%252C%250Ademonstrating%2520its%2520superiority%2520over%2520state-of-the-art%2520trackers.%2520Notably%252C%2520X-Net%250Aachieves%2520performance%2520gains%2520of%25200.47%2525/1.2%2525%2520in%2520the%2520average%2520of%2520precise%2520rate%2520and%250Asuccess%2520rate%252C%2520respectively.%2520Additionally%252C%2520the%2520research%2520content%252C%2520data%252C%2520and%2520code%250Aare%2520pledged%2520to%2520be%2520made%2520publicly%2520accessible%2520at%250Ahttps%253A//github.com/DZSYUNNAN/XNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X%20Modality%20Assisting%20RGBT%20Object%20Tracking&entry.906535625=Zhaisheng%20Ding%20and%20Haiyan%20Li%20and%20Ruichao%20Hou%20and%20Yanyu%20Liu%20and%20Shidong%20Xie&entry.1292438233=%20%20Developing%20robust%20multi-modal%20feature%20representations%20is%20crucial%20for%0Aenhancing%20object%20tracking%20performance.%20In%20pursuit%20of%20this%20objective%2C%20a%20novel%20X%0AModality%20Assisting%20Network%20%28X-Net%29%20is%20introduced%2C%20which%20explores%20the%20impact%20of%0Athe%20fusion%20paradigm%20by%20decoupling%20visual%20object%20tracking%20into%20three%20distinct%0Alevels%2C%20thereby%20facilitating%20subsequent%20processing.%20Initially%2C%20to%20overcome%20the%0Achallenges%20associated%20with%20feature%20learning%20due%20to%20significant%20discrepancies%0Abetween%20RGB%20and%20thermal%20modalities%2C%20a%20plug-and-play%20pixel-level%20generation%0Amodule%20%28PGM%29%20based%20on%20knowledge%20distillation%20learning%20is%20proposed.%20This%20module%0Aeffectively%20generates%20the%20X%20modality%2C%20bridging%20the%20gap%20between%20the%20two%20patterns%0Awhile%20minimizing%20noise%20interference.%20Subsequently%2C%20to%20optimize%20sample%20feature%0Arepresentation%20and%20promote%20cross-modal%20interactions%2C%20a%20feature-level%0Ainteraction%20module%20%28FIM%29%20is%20introduced%2C%20integrating%20a%20mixed%20feature%20interaction%0Atransformer%20and%20a%20spatial%20dimensional%20feature%20translation%20strategy.%20Finally%2C%20to%0Aaddress%20random%20drifting%20caused%20by%20missing%20instance%20features%2C%20a%20flexible%20online%0Aoptimization%20strategy%20called%20the%20decision-level%20refinement%20module%20%28DRM%29%20is%0Aproposed%2C%20which%20incorporates%20optical%20flow%20and%20refinement%20mechanisms.%20The%0Aefficacy%20of%20X-Net%20is%20validated%20through%20experiments%20on%20three%20benchmarks%2C%0Ademonstrating%20its%20superiority%20over%20state-of-the-art%20trackers.%20Notably%2C%20X-Net%0Aachieves%20performance%20gains%20of%200.47%25/1.2%25%20in%20the%20average%20of%20precise%20rate%20and%0Asuccess%20rate%2C%20respectively.%20Additionally%2C%20the%20research%20content%2C%20data%2C%20and%20code%0Aare%20pledged%20to%20be%20made%20publicly%20accessible%20at%0Ahttps%3A//github.com/DZSYUNNAN/XNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17273v2&entry.124074799=Read"},
{"title": "MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs", "author": "Jiarui Zhang and Mahyar Khayatkhoei and Prateek Chhikara and Filip Ilievski", "abstract": "  Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk.\n", "link": "http://arxiv.org/abs/2502.17422v1", "date": "2025-02-24", "relevancy": 2.8415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5734}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLLMs%20Know%20Where%20to%20Look%3A%20Training-free%20Perception%20of%20Small%20Visual%0A%20%20Details%20with%20Multimodal%20LLMs&body=Title%3A%20MLLMs%20Know%20Where%20to%20Look%3A%20Training-free%20Perception%20of%20Small%20Visual%0A%20%20Details%20with%20Multimodal%20LLMs%0AAuthor%3A%20Jiarui%20Zhang%20and%20Mahyar%20Khayatkhoei%20and%20Prateek%20Chhikara%20and%20Filip%20Ilievski%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20experienced%20rapid%20progress%20in%0Avisual%20recognition%20tasks%20in%20recent%20years.%20Given%20their%20potential%20integration%0Ainto%20many%20critical%20applications%2C%20it%20is%20important%20to%20understand%20the%20limitations%0Aof%20their%20visual%20perception.%20In%20this%20work%2C%20we%20study%20whether%20MLLMs%20can%20perceive%0Asmall%20visual%20details%20as%20effectively%20as%20large%20ones%20when%20answering%20questions%0Aabout%20images.%20We%20observe%20that%20their%20performance%20is%20very%20sensitive%20to%20the%20size%0Aof%20the%20visual%20subject%20of%20the%20question%2C%20and%20further%20show%20that%20this%20effect%20is%20in%0Afact%20causal%20by%20conducting%20an%20intervention%20study.%20Next%2C%20we%20study%20the%20attention%0Apatterns%20of%20MLLMs%20when%20answering%20visual%20questions%2C%20and%20intriguingly%20find%20that%0Athey%20consistently%20know%20where%20to%20look%2C%20even%20when%20they%20provide%20the%20wrong%20answer.%0ABased%20on%20these%20findings%2C%20we%20then%20propose%20training-free%20visual%20intervention%0Amethods%20that%20leverage%20the%20internal%20knowledge%20of%20any%20MLLM%20itself%2C%20in%20the%20form%20of%0Aattention%20and%20gradient%20maps%2C%20to%20enhance%20its%20perception%20of%20small%20visual%20details.%0AWe%20evaluate%20our%20proposed%20methods%20on%20two%20widely-used%20MLLMs%20and%20seven%20visual%0Aquestion%20answering%20benchmarks%20and%20show%20that%20they%20can%20significantly%20improve%0AMLLMs%27%20accuracy%20without%20requiring%20any%20training.%20Our%20results%20elucidate%20the%20risk%0Aof%20applying%20MLLMs%20to%20visual%20recognition%20tasks%20concerning%20small%20details%20and%0Aindicate%20that%20visual%20intervention%20using%20the%20model%27s%20internal%20state%20is%20a%0Apromising%20direction%20to%20mitigate%20this%20risk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLLMs%2520Know%2520Where%2520to%2520Look%253A%2520Training-free%2520Perception%2520of%2520Small%2520Visual%250A%2520%2520Details%2520with%2520Multimodal%2520LLMs%26entry.906535625%3DJiarui%2520Zhang%2520and%2520Mahyar%2520Khayatkhoei%2520and%2520Prateek%2520Chhikara%2520and%2520Filip%2520Ilievski%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520experienced%2520rapid%2520progress%2520in%250Avisual%2520recognition%2520tasks%2520in%2520recent%2520years.%2520Given%2520their%2520potential%2520integration%250Ainto%2520many%2520critical%2520applications%252C%2520it%2520is%2520important%2520to%2520understand%2520the%2520limitations%250Aof%2520their%2520visual%2520perception.%2520In%2520this%2520work%252C%2520we%2520study%2520whether%2520MLLMs%2520can%2520perceive%250Asmall%2520visual%2520details%2520as%2520effectively%2520as%2520large%2520ones%2520when%2520answering%2520questions%250Aabout%2520images.%2520We%2520observe%2520that%2520their%2520performance%2520is%2520very%2520sensitive%2520to%2520the%2520size%250Aof%2520the%2520visual%2520subject%2520of%2520the%2520question%252C%2520and%2520further%2520show%2520that%2520this%2520effect%2520is%2520in%250Afact%2520causal%2520by%2520conducting%2520an%2520intervention%2520study.%2520Next%252C%2520we%2520study%2520the%2520attention%250Apatterns%2520of%2520MLLMs%2520when%2520answering%2520visual%2520questions%252C%2520and%2520intriguingly%2520find%2520that%250Athey%2520consistently%2520know%2520where%2520to%2520look%252C%2520even%2520when%2520they%2520provide%2520the%2520wrong%2520answer.%250ABased%2520on%2520these%2520findings%252C%2520we%2520then%2520propose%2520training-free%2520visual%2520intervention%250Amethods%2520that%2520leverage%2520the%2520internal%2520knowledge%2520of%2520any%2520MLLM%2520itself%252C%2520in%2520the%2520form%2520of%250Aattention%2520and%2520gradient%2520maps%252C%2520to%2520enhance%2520its%2520perception%2520of%2520small%2520visual%2520details.%250AWe%2520evaluate%2520our%2520proposed%2520methods%2520on%2520two%2520widely-used%2520MLLMs%2520and%2520seven%2520visual%250Aquestion%2520answering%2520benchmarks%2520and%2520show%2520that%2520they%2520can%2520significantly%2520improve%250AMLLMs%2527%2520accuracy%2520without%2520requiring%2520any%2520training.%2520Our%2520results%2520elucidate%2520the%2520risk%250Aof%2520applying%2520MLLMs%2520to%2520visual%2520recognition%2520tasks%2520concerning%2520small%2520details%2520and%250Aindicate%2520that%2520visual%2520intervention%2520using%2520the%2520model%2527s%2520internal%2520state%2520is%2520a%250Apromising%2520direction%2520to%2520mitigate%2520this%2520risk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLLMs%20Know%20Where%20to%20Look%3A%20Training-free%20Perception%20of%20Small%20Visual%0A%20%20Details%20with%20Multimodal%20LLMs&entry.906535625=Jiarui%20Zhang%20and%20Mahyar%20Khayatkhoei%20and%20Prateek%20Chhikara%20and%20Filip%20Ilievski&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20experienced%20rapid%20progress%20in%0Avisual%20recognition%20tasks%20in%20recent%20years.%20Given%20their%20potential%20integration%0Ainto%20many%20critical%20applications%2C%20it%20is%20important%20to%20understand%20the%20limitations%0Aof%20their%20visual%20perception.%20In%20this%20work%2C%20we%20study%20whether%20MLLMs%20can%20perceive%0Asmall%20visual%20details%20as%20effectively%20as%20large%20ones%20when%20answering%20questions%0Aabout%20images.%20We%20observe%20that%20their%20performance%20is%20very%20sensitive%20to%20the%20size%0Aof%20the%20visual%20subject%20of%20the%20question%2C%20and%20further%20show%20that%20this%20effect%20is%20in%0Afact%20causal%20by%20conducting%20an%20intervention%20study.%20Next%2C%20we%20study%20the%20attention%0Apatterns%20of%20MLLMs%20when%20answering%20visual%20questions%2C%20and%20intriguingly%20find%20that%0Athey%20consistently%20know%20where%20to%20look%2C%20even%20when%20they%20provide%20the%20wrong%20answer.%0ABased%20on%20these%20findings%2C%20we%20then%20propose%20training-free%20visual%20intervention%0Amethods%20that%20leverage%20the%20internal%20knowledge%20of%20any%20MLLM%20itself%2C%20in%20the%20form%20of%0Aattention%20and%20gradient%20maps%2C%20to%20enhance%20its%20perception%20of%20small%20visual%20details.%0AWe%20evaluate%20our%20proposed%20methods%20on%20two%20widely-used%20MLLMs%20and%20seven%20visual%0Aquestion%20answering%20benchmarks%20and%20show%20that%20they%20can%20significantly%20improve%0AMLLMs%27%20accuracy%20without%20requiring%20any%20training.%20Our%20results%20elucidate%20the%20risk%0Aof%20applying%20MLLMs%20to%20visual%20recognition%20tasks%20concerning%20small%20details%20and%0Aindicate%20that%20visual%20intervention%20using%20the%20model%27s%20internal%20state%20is%20a%0Apromising%20direction%20to%20mitigate%20this%20risk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17422v1&entry.124074799=Read"},
{"title": "DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth\n  Observation Applications", "author": "Ibrahim Fayad and Max Zimmer and Martin Schwartz and Philippe Ciais and Fabian Gieseke and Gabriel Belouze and Sarah Brood and Aurelien De Truchis and Alexandre d'Aspremont", "abstract": "  Significant efforts have been directed towards adapting self-supervised\nmultimodal learning for Earth observation applications. However, existing\nmethods produce coarse patch-sized embeddings, limiting their effectiveness and\nintegration with other modalities like LiDAR. To close this gap, we present\nDUNIA, an approach to learn pixel-sized embeddings through cross-modal\nalignment between images and full-waveform LiDAR data. As the model is trained\nin a contrastive manner, the embeddings can be directly leveraged in the\ncontext of a variety of environmental monitoring tasks in a zero-shot setting.\nIn our experiments, we demonstrate the effectiveness of the embeddings for\nseven such tasks (canopy height mapping, fractional canopy cover, land cover\nmapping, tree species identification, plant area index, crop type\nclassification, and per-pixel waveform-based vertical structure mapping). The\nresults show that the embeddings, along with zero-shot classifiers, often\noutperform specialized supervised models, even in low data regimes. In the\nfine-tuning setting, we show strong low-shot capabilities with performances\nnear or better than state-of-the-art on five out of six tasks.\n", "link": "http://arxiv.org/abs/2502.17066v1", "date": "2025-02-24", "relevancy": 2.8177, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5724}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5643}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUNIA%3A%20Pixel-Sized%20Embeddings%20via%20Cross-Modal%20Alignment%20for%20Earth%0A%20%20Observation%20Applications&body=Title%3A%20DUNIA%3A%20Pixel-Sized%20Embeddings%20via%20Cross-Modal%20Alignment%20for%20Earth%0A%20%20Observation%20Applications%0AAuthor%3A%20Ibrahim%20Fayad%20and%20Max%20Zimmer%20and%20Martin%20Schwartz%20and%20Philippe%20Ciais%20and%20Fabian%20Gieseke%20and%20Gabriel%20Belouze%20and%20Sarah%20Brood%20and%20Aurelien%20De%20Truchis%20and%20Alexandre%20d%27Aspremont%0AAbstract%3A%20%20%20Significant%20efforts%20have%20been%20directed%20towards%20adapting%20self-supervised%0Amultimodal%20learning%20for%20Earth%20observation%20applications.%20However%2C%20existing%0Amethods%20produce%20coarse%20patch-sized%20embeddings%2C%20limiting%20their%20effectiveness%20and%0Aintegration%20with%20other%20modalities%20like%20LiDAR.%20To%20close%20this%20gap%2C%20we%20present%0ADUNIA%2C%20an%20approach%20to%20learn%20pixel-sized%20embeddings%20through%20cross-modal%0Aalignment%20between%20images%20and%20full-waveform%20LiDAR%20data.%20As%20the%20model%20is%20trained%0Ain%20a%20contrastive%20manner%2C%20the%20embeddings%20can%20be%20directly%20leveraged%20in%20the%0Acontext%20of%20a%20variety%20of%20environmental%20monitoring%20tasks%20in%20a%20zero-shot%20setting.%0AIn%20our%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20embeddings%20for%0Aseven%20such%20tasks%20%28canopy%20height%20mapping%2C%20fractional%20canopy%20cover%2C%20land%20cover%0Amapping%2C%20tree%20species%20identification%2C%20plant%20area%20index%2C%20crop%20type%0Aclassification%2C%20and%20per-pixel%20waveform-based%20vertical%20structure%20mapping%29.%20The%0Aresults%20show%20that%20the%20embeddings%2C%20along%20with%20zero-shot%20classifiers%2C%20often%0Aoutperform%20specialized%20supervised%20models%2C%20even%20in%20low%20data%20regimes.%20In%20the%0Afine-tuning%20setting%2C%20we%20show%20strong%20low-shot%20capabilities%20with%20performances%0Anear%20or%20better%20than%20state-of-the-art%20on%20five%20out%20of%20six%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUNIA%253A%2520Pixel-Sized%2520Embeddings%2520via%2520Cross-Modal%2520Alignment%2520for%2520Earth%250A%2520%2520Observation%2520Applications%26entry.906535625%3DIbrahim%2520Fayad%2520and%2520Max%2520Zimmer%2520and%2520Martin%2520Schwartz%2520and%2520Philippe%2520Ciais%2520and%2520Fabian%2520Gieseke%2520and%2520Gabriel%2520Belouze%2520and%2520Sarah%2520Brood%2520and%2520Aurelien%2520De%2520Truchis%2520and%2520Alexandre%2520d%2527Aspremont%26entry.1292438233%3D%2520%2520Significant%2520efforts%2520have%2520been%2520directed%2520towards%2520adapting%2520self-supervised%250Amultimodal%2520learning%2520for%2520Earth%2520observation%2520applications.%2520However%252C%2520existing%250Amethods%2520produce%2520coarse%2520patch-sized%2520embeddings%252C%2520limiting%2520their%2520effectiveness%2520and%250Aintegration%2520with%2520other%2520modalities%2520like%2520LiDAR.%2520To%2520close%2520this%2520gap%252C%2520we%2520present%250ADUNIA%252C%2520an%2520approach%2520to%2520learn%2520pixel-sized%2520embeddings%2520through%2520cross-modal%250Aalignment%2520between%2520images%2520and%2520full-waveform%2520LiDAR%2520data.%2520As%2520the%2520model%2520is%2520trained%250Ain%2520a%2520contrastive%2520manner%252C%2520the%2520embeddings%2520can%2520be%2520directly%2520leveraged%2520in%2520the%250Acontext%2520of%2520a%2520variety%2520of%2520environmental%2520monitoring%2520tasks%2520in%2520a%2520zero-shot%2520setting.%250AIn%2520our%2520experiments%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520embeddings%2520for%250Aseven%2520such%2520tasks%2520%2528canopy%2520height%2520mapping%252C%2520fractional%2520canopy%2520cover%252C%2520land%2520cover%250Amapping%252C%2520tree%2520species%2520identification%252C%2520plant%2520area%2520index%252C%2520crop%2520type%250Aclassification%252C%2520and%2520per-pixel%2520waveform-based%2520vertical%2520structure%2520mapping%2529.%2520The%250Aresults%2520show%2520that%2520the%2520embeddings%252C%2520along%2520with%2520zero-shot%2520classifiers%252C%2520often%250Aoutperform%2520specialized%2520supervised%2520models%252C%2520even%2520in%2520low%2520data%2520regimes.%2520In%2520the%250Afine-tuning%2520setting%252C%2520we%2520show%2520strong%2520low-shot%2520capabilities%2520with%2520performances%250Anear%2520or%2520better%2520than%2520state-of-the-art%2520on%2520five%2520out%2520of%2520six%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUNIA%3A%20Pixel-Sized%20Embeddings%20via%20Cross-Modal%20Alignment%20for%20Earth%0A%20%20Observation%20Applications&entry.906535625=Ibrahim%20Fayad%20and%20Max%20Zimmer%20and%20Martin%20Schwartz%20and%20Philippe%20Ciais%20and%20Fabian%20Gieseke%20and%20Gabriel%20Belouze%20and%20Sarah%20Brood%20and%20Aurelien%20De%20Truchis%20and%20Alexandre%20d%27Aspremont&entry.1292438233=%20%20Significant%20efforts%20have%20been%20directed%20towards%20adapting%20self-supervised%0Amultimodal%20learning%20for%20Earth%20observation%20applications.%20However%2C%20existing%0Amethods%20produce%20coarse%20patch-sized%20embeddings%2C%20limiting%20their%20effectiveness%20and%0Aintegration%20with%20other%20modalities%20like%20LiDAR.%20To%20close%20this%20gap%2C%20we%20present%0ADUNIA%2C%20an%20approach%20to%20learn%20pixel-sized%20embeddings%20through%20cross-modal%0Aalignment%20between%20images%20and%20full-waveform%20LiDAR%20data.%20As%20the%20model%20is%20trained%0Ain%20a%20contrastive%20manner%2C%20the%20embeddings%20can%20be%20directly%20leveraged%20in%20the%0Acontext%20of%20a%20variety%20of%20environmental%20monitoring%20tasks%20in%20a%20zero-shot%20setting.%0AIn%20our%20experiments%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%20embeddings%20for%0Aseven%20such%20tasks%20%28canopy%20height%20mapping%2C%20fractional%20canopy%20cover%2C%20land%20cover%0Amapping%2C%20tree%20species%20identification%2C%20plant%20area%20index%2C%20crop%20type%0Aclassification%2C%20and%20per-pixel%20waveform-based%20vertical%20structure%20mapping%29.%20The%0Aresults%20show%20that%20the%20embeddings%2C%20along%20with%20zero-shot%20classifiers%2C%20often%0Aoutperform%20specialized%20supervised%20models%2C%20even%20in%20low%20data%20regimes.%20In%20the%0Afine-tuning%20setting%2C%20we%20show%20strong%20low-shot%20capabilities%20with%20performances%0Anear%20or%20better%20than%20state-of-the-art%20on%20five%20out%20of%20six%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17066v1&entry.124074799=Read"},
{"title": "PointSea: Point Cloud Completion via Self-structure Augmentation", "author": "Zhe Zhu and Honghua Chen and Xing He and Mingqiang Wei", "abstract": "  Point cloud completion is a fundamental yet not well-solved problem in 3D\nvision. Current approaches often rely on 3D coordinate information and/or\nadditional data (e.g., images and scanning viewpoints) to fill in missing\nparts. Unlike these methods, we explore self-structure augmentation and propose\nPointSea for global-to-local point cloud completion. In the global stage,\nconsider how we inspect a defective region of a physical object, we may observe\nit from various perspectives for a better understanding. Inspired by this,\nPointSea augments data representation by leveraging self-projected depth images\nfrom multiple views. To reconstruct a compact global shape from the cross-modal\ninput, we incorporate a feature fusion module to fuse features at both\nintra-view and inter-view levels. In the local stage, to reveal highly detailed\nstructures, we introduce a point generator called the self-structure\ndual-generator. This generator integrates both learned shape priors and\ngeometric self-similarities for shape refinement. Unlike existing efforts that\napply a unified strategy for all points, our dual-path design adapts refinement\nstrategies conditioned on the structural type of each point, addressing the\nspecific incompleteness of each point. Comprehensive experiments on widely-used\nbenchmarks demonstrate that PointSea effectively understands global shapes and\ngenerates local details from incomplete input, showing clear improvements over\nexisting methods.\n", "link": "http://arxiv.org/abs/2502.17053v1", "date": "2025-02-24", "relevancy": 2.7961, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5767}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5547}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PointSea%3A%20Point%20Cloud%20Completion%20via%20Self-structure%20Augmentation&body=Title%3A%20PointSea%3A%20Point%20Cloud%20Completion%20via%20Self-structure%20Augmentation%0AAuthor%3A%20Zhe%20Zhu%20and%20Honghua%20Chen%20and%20Xing%20He%20and%20Mingqiang%20Wei%0AAbstract%3A%20%20%20Point%20cloud%20completion%20is%20a%20fundamental%20yet%20not%20well-solved%20problem%20in%203D%0Avision.%20Current%20approaches%20often%20rely%20on%203D%20coordinate%20information%20and/or%0Aadditional%20data%20%28e.g.%2C%20images%20and%20scanning%20viewpoints%29%20to%20fill%20in%20missing%0Aparts.%20Unlike%20these%20methods%2C%20we%20explore%20self-structure%20augmentation%20and%20propose%0APointSea%20for%20global-to-local%20point%20cloud%20completion.%20In%20the%20global%20stage%2C%0Aconsider%20how%20we%20inspect%20a%20defective%20region%20of%20a%20physical%20object%2C%20we%20may%20observe%0Ait%20from%20various%20perspectives%20for%20a%20better%20understanding.%20Inspired%20by%20this%2C%0APointSea%20augments%20data%20representation%20by%20leveraging%20self-projected%20depth%20images%0Afrom%20multiple%20views.%20To%20reconstruct%20a%20compact%20global%20shape%20from%20the%20cross-modal%0Ainput%2C%20we%20incorporate%20a%20feature%20fusion%20module%20to%20fuse%20features%20at%20both%0Aintra-view%20and%20inter-view%20levels.%20In%20the%20local%20stage%2C%20to%20reveal%20highly%20detailed%0Astructures%2C%20we%20introduce%20a%20point%20generator%20called%20the%20self-structure%0Adual-generator.%20This%20generator%20integrates%20both%20learned%20shape%20priors%20and%0Ageometric%20self-similarities%20for%20shape%20refinement.%20Unlike%20existing%20efforts%20that%0Aapply%20a%20unified%20strategy%20for%20all%20points%2C%20our%20dual-path%20design%20adapts%20refinement%0Astrategies%20conditioned%20on%20the%20structural%20type%20of%20each%20point%2C%20addressing%20the%0Aspecific%20incompleteness%20of%20each%20point.%20Comprehensive%20experiments%20on%20widely-used%0Abenchmarks%20demonstrate%20that%20PointSea%20effectively%20understands%20global%20shapes%20and%0Agenerates%20local%20details%20from%20incomplete%20input%2C%20showing%20clear%20improvements%20over%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointSea%253A%2520Point%2520Cloud%2520Completion%2520via%2520Self-structure%2520Augmentation%26entry.906535625%3DZhe%2520Zhu%2520and%2520Honghua%2520Chen%2520and%2520Xing%2520He%2520and%2520Mingqiang%2520Wei%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520is%2520a%2520fundamental%2520yet%2520not%2520well-solved%2520problem%2520in%25203D%250Avision.%2520Current%2520approaches%2520often%2520rely%2520on%25203D%2520coordinate%2520information%2520and/or%250Aadditional%2520data%2520%2528e.g.%252C%2520images%2520and%2520scanning%2520viewpoints%2529%2520to%2520fill%2520in%2520missing%250Aparts.%2520Unlike%2520these%2520methods%252C%2520we%2520explore%2520self-structure%2520augmentation%2520and%2520propose%250APointSea%2520for%2520global-to-local%2520point%2520cloud%2520completion.%2520In%2520the%2520global%2520stage%252C%250Aconsider%2520how%2520we%2520inspect%2520a%2520defective%2520region%2520of%2520a%2520physical%2520object%252C%2520we%2520may%2520observe%250Ait%2520from%2520various%2520perspectives%2520for%2520a%2520better%2520understanding.%2520Inspired%2520by%2520this%252C%250APointSea%2520augments%2520data%2520representation%2520by%2520leveraging%2520self-projected%2520depth%2520images%250Afrom%2520multiple%2520views.%2520To%2520reconstruct%2520a%2520compact%2520global%2520shape%2520from%2520the%2520cross-modal%250Ainput%252C%2520we%2520incorporate%2520a%2520feature%2520fusion%2520module%2520to%2520fuse%2520features%2520at%2520both%250Aintra-view%2520and%2520inter-view%2520levels.%2520In%2520the%2520local%2520stage%252C%2520to%2520reveal%2520highly%2520detailed%250Astructures%252C%2520we%2520introduce%2520a%2520point%2520generator%2520called%2520the%2520self-structure%250Adual-generator.%2520This%2520generator%2520integrates%2520both%2520learned%2520shape%2520priors%2520and%250Ageometric%2520self-similarities%2520for%2520shape%2520refinement.%2520Unlike%2520existing%2520efforts%2520that%250Aapply%2520a%2520unified%2520strategy%2520for%2520all%2520points%252C%2520our%2520dual-path%2520design%2520adapts%2520refinement%250Astrategies%2520conditioned%2520on%2520the%2520structural%2520type%2520of%2520each%2520point%252C%2520addressing%2520the%250Aspecific%2520incompleteness%2520of%2520each%2520point.%2520Comprehensive%2520experiments%2520on%2520widely-used%250Abenchmarks%2520demonstrate%2520that%2520PointSea%2520effectively%2520understands%2520global%2520shapes%2520and%250Agenerates%2520local%2520details%2520from%2520incomplete%2520input%252C%2520showing%2520clear%2520improvements%2520over%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PointSea%3A%20Point%20Cloud%20Completion%20via%20Self-structure%20Augmentation&entry.906535625=Zhe%20Zhu%20and%20Honghua%20Chen%20and%20Xing%20He%20and%20Mingqiang%20Wei&entry.1292438233=%20%20Point%20cloud%20completion%20is%20a%20fundamental%20yet%20not%20well-solved%20problem%20in%203D%0Avision.%20Current%20approaches%20often%20rely%20on%203D%20coordinate%20information%20and/or%0Aadditional%20data%20%28e.g.%2C%20images%20and%20scanning%20viewpoints%29%20to%20fill%20in%20missing%0Aparts.%20Unlike%20these%20methods%2C%20we%20explore%20self-structure%20augmentation%20and%20propose%0APointSea%20for%20global-to-local%20point%20cloud%20completion.%20In%20the%20global%20stage%2C%0Aconsider%20how%20we%20inspect%20a%20defective%20region%20of%20a%20physical%20object%2C%20we%20may%20observe%0Ait%20from%20various%20perspectives%20for%20a%20better%20understanding.%20Inspired%20by%20this%2C%0APointSea%20augments%20data%20representation%20by%20leveraging%20self-projected%20depth%20images%0Afrom%20multiple%20views.%20To%20reconstruct%20a%20compact%20global%20shape%20from%20the%20cross-modal%0Ainput%2C%20we%20incorporate%20a%20feature%20fusion%20module%20to%20fuse%20features%20at%20both%0Aintra-view%20and%20inter-view%20levels.%20In%20the%20local%20stage%2C%20to%20reveal%20highly%20detailed%0Astructures%2C%20we%20introduce%20a%20point%20generator%20called%20the%20self-structure%0Adual-generator.%20This%20generator%20integrates%20both%20learned%20shape%20priors%20and%0Ageometric%20self-similarities%20for%20shape%20refinement.%20Unlike%20existing%20efforts%20that%0Aapply%20a%20unified%20strategy%20for%20all%20points%2C%20our%20dual-path%20design%20adapts%20refinement%0Astrategies%20conditioned%20on%20the%20structural%20type%20of%20each%20point%2C%20addressing%20the%0Aspecific%20incompleteness%20of%20each%20point.%20Comprehensive%20experiments%20on%20widely-used%0Abenchmarks%20demonstrate%20that%20PointSea%20effectively%20understands%20global%20shapes%20and%0Agenerates%20local%20details%20from%20incomplete%20input%2C%20showing%20clear%20improvements%20over%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17053v1&entry.124074799=Read"},
{"title": "LCV2I: Communication-Efficient and High-Performance Collaborative\n  Perception Framework with Low-Resolution LiDAR", "author": "Xinxin Feng and Haoran Sun and Haifeng Zheng and Huacong Chen and Wenqiang Chen", "abstract": "  Vehicle-to-Infrastructure (V2I) collaborative perception leverages data\ncollected by infrastructure's sensors to enhance vehicle perceptual\ncapabilities. LiDAR, as a commonly used sensor in cooperative perception, is\nwidely equipped in intelligent vehicles and infrastructure. However, its\nsuperior performance comes with a correspondingly high cost. To achieve\nlow-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we study\nadopting low-resolution LiDAR on the vehicle to minimize cost as much as\npossible. However, simply reducing the resolution of vehicle's LiDAR results in\nsparse point clouds, making distant small objects even more blurred.\nAdditionally, traditional communication methods have relatively low bandwidth\nutilization efficiency. These factors pose challenges for us. To balance cost\nand perceptual accuracy, we propose a new collaborative perception framework,\nnamely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDAR\nas input. It also employs feature offset correction modules and regional\nfeature enhancement algorithms to improve feature representation. Finally, we\nuse regional difference map and regional score map to assess the value of\ncollaboration content, thereby improving communication bandwidth efficiency. In\nsummary, our approach achieves high perceptual performance while substantially\nreducing the demand for high-resolution sensors on the vehicle. To evaluate\nthis algorithm, we conduct 3D object detection in the real-world scenario of\nDAIR-V2X, demonstrating that the performance of LCV2I consistently surpasses\ncurrently existing algorithms.\n", "link": "http://arxiv.org/abs/2502.17039v1", "date": "2025-02-24", "relevancy": 2.7838, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCV2I%3A%20Communication-Efficient%20and%20High-Performance%20Collaborative%0A%20%20Perception%20Framework%20with%20Low-Resolution%20LiDAR&body=Title%3A%20LCV2I%3A%20Communication-Efficient%20and%20High-Performance%20Collaborative%0A%20%20Perception%20Framework%20with%20Low-Resolution%20LiDAR%0AAuthor%3A%20Xinxin%20Feng%20and%20Haoran%20Sun%20and%20Haifeng%20Zheng%20and%20Huacong%20Chen%20and%20Wenqiang%20Chen%0AAbstract%3A%20%20%20Vehicle-to-Infrastructure%20%28V2I%29%20collaborative%20perception%20leverages%20data%0Acollected%20by%20infrastructure%27s%20sensors%20to%20enhance%20vehicle%20perceptual%0Acapabilities.%20LiDAR%2C%20as%20a%20commonly%20used%20sensor%20in%20cooperative%20perception%2C%20is%0Awidely%20equipped%20in%20intelligent%20vehicles%20and%20infrastructure.%20However%2C%20its%0Asuperior%20performance%20comes%20with%20a%20correspondingly%20high%20cost.%20To%20achieve%0Alow-cost%20V2I%2C%20reducing%20the%20cost%20of%20LiDAR%20is%20crucial.%20Therefore%2C%20we%20study%0Aadopting%20low-resolution%20LiDAR%20on%20the%20vehicle%20to%20minimize%20cost%20as%20much%20as%0Apossible.%20However%2C%20simply%20reducing%20the%20resolution%20of%20vehicle%27s%20LiDAR%20results%20in%0Asparse%20point%20clouds%2C%20making%20distant%20small%20objects%20even%20more%20blurred.%0AAdditionally%2C%20traditional%20communication%20methods%20have%20relatively%20low%20bandwidth%0Autilization%20efficiency.%20These%20factors%20pose%20challenges%20for%20us.%20To%20balance%20cost%0Aand%20perceptual%20accuracy%2C%20we%20propose%20a%20new%20collaborative%20perception%20framework%2C%0Anamely%20LCV2I.%20LCV2I%20uses%20data%20collected%20from%20cameras%20and%20low-resolution%20LiDAR%0Aas%20input.%20It%20also%20employs%20feature%20offset%20correction%20modules%20and%20regional%0Afeature%20enhancement%20algorithms%20to%20improve%20feature%20representation.%20Finally%2C%20we%0Ause%20regional%20difference%20map%20and%20regional%20score%20map%20to%20assess%20the%20value%20of%0Acollaboration%20content%2C%20thereby%20improving%20communication%20bandwidth%20efficiency.%20In%0Asummary%2C%20our%20approach%20achieves%20high%20perceptual%20performance%20while%20substantially%0Areducing%20the%20demand%20for%20high-resolution%20sensors%20on%20the%20vehicle.%20To%20evaluate%0Athis%20algorithm%2C%20we%20conduct%203D%20object%20detection%20in%20the%20real-world%20scenario%20of%0ADAIR-V2X%2C%20demonstrating%20that%20the%20performance%20of%20LCV2I%20consistently%20surpasses%0Acurrently%20existing%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCV2I%253A%2520Communication-Efficient%2520and%2520High-Performance%2520Collaborative%250A%2520%2520Perception%2520Framework%2520with%2520Low-Resolution%2520LiDAR%26entry.906535625%3DXinxin%2520Feng%2520and%2520Haoran%2520Sun%2520and%2520Haifeng%2520Zheng%2520and%2520Huacong%2520Chen%2520and%2520Wenqiang%2520Chen%26entry.1292438233%3D%2520%2520Vehicle-to-Infrastructure%2520%2528V2I%2529%2520collaborative%2520perception%2520leverages%2520data%250Acollected%2520by%2520infrastructure%2527s%2520sensors%2520to%2520enhance%2520vehicle%2520perceptual%250Acapabilities.%2520LiDAR%252C%2520as%2520a%2520commonly%2520used%2520sensor%2520in%2520cooperative%2520perception%252C%2520is%250Awidely%2520equipped%2520in%2520intelligent%2520vehicles%2520and%2520infrastructure.%2520However%252C%2520its%250Asuperior%2520performance%2520comes%2520with%2520a%2520correspondingly%2520high%2520cost.%2520To%2520achieve%250Alow-cost%2520V2I%252C%2520reducing%2520the%2520cost%2520of%2520LiDAR%2520is%2520crucial.%2520Therefore%252C%2520we%2520study%250Aadopting%2520low-resolution%2520LiDAR%2520on%2520the%2520vehicle%2520to%2520minimize%2520cost%2520as%2520much%2520as%250Apossible.%2520However%252C%2520simply%2520reducing%2520the%2520resolution%2520of%2520vehicle%2527s%2520LiDAR%2520results%2520in%250Asparse%2520point%2520clouds%252C%2520making%2520distant%2520small%2520objects%2520even%2520more%2520blurred.%250AAdditionally%252C%2520traditional%2520communication%2520methods%2520have%2520relatively%2520low%2520bandwidth%250Autilization%2520efficiency.%2520These%2520factors%2520pose%2520challenges%2520for%2520us.%2520To%2520balance%2520cost%250Aand%2520perceptual%2520accuracy%252C%2520we%2520propose%2520a%2520new%2520collaborative%2520perception%2520framework%252C%250Anamely%2520LCV2I.%2520LCV2I%2520uses%2520data%2520collected%2520from%2520cameras%2520and%2520low-resolution%2520LiDAR%250Aas%2520input.%2520It%2520also%2520employs%2520feature%2520offset%2520correction%2520modules%2520and%2520regional%250Afeature%2520enhancement%2520algorithms%2520to%2520improve%2520feature%2520representation.%2520Finally%252C%2520we%250Ause%2520regional%2520difference%2520map%2520and%2520regional%2520score%2520map%2520to%2520assess%2520the%2520value%2520of%250Acollaboration%2520content%252C%2520thereby%2520improving%2520communication%2520bandwidth%2520efficiency.%2520In%250Asummary%252C%2520our%2520approach%2520achieves%2520high%2520perceptual%2520performance%2520while%2520substantially%250Areducing%2520the%2520demand%2520for%2520high-resolution%2520sensors%2520on%2520the%2520vehicle.%2520To%2520evaluate%250Athis%2520algorithm%252C%2520we%2520conduct%25203D%2520object%2520detection%2520in%2520the%2520real-world%2520scenario%2520of%250ADAIR-V2X%252C%2520demonstrating%2520that%2520the%2520performance%2520of%2520LCV2I%2520consistently%2520surpasses%250Acurrently%2520existing%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCV2I%3A%20Communication-Efficient%20and%20High-Performance%20Collaborative%0A%20%20Perception%20Framework%20with%20Low-Resolution%20LiDAR&entry.906535625=Xinxin%20Feng%20and%20Haoran%20Sun%20and%20Haifeng%20Zheng%20and%20Huacong%20Chen%20and%20Wenqiang%20Chen&entry.1292438233=%20%20Vehicle-to-Infrastructure%20%28V2I%29%20collaborative%20perception%20leverages%20data%0Acollected%20by%20infrastructure%27s%20sensors%20to%20enhance%20vehicle%20perceptual%0Acapabilities.%20LiDAR%2C%20as%20a%20commonly%20used%20sensor%20in%20cooperative%20perception%2C%20is%0Awidely%20equipped%20in%20intelligent%20vehicles%20and%20infrastructure.%20However%2C%20its%0Asuperior%20performance%20comes%20with%20a%20correspondingly%20high%20cost.%20To%20achieve%0Alow-cost%20V2I%2C%20reducing%20the%20cost%20of%20LiDAR%20is%20crucial.%20Therefore%2C%20we%20study%0Aadopting%20low-resolution%20LiDAR%20on%20the%20vehicle%20to%20minimize%20cost%20as%20much%20as%0Apossible.%20However%2C%20simply%20reducing%20the%20resolution%20of%20vehicle%27s%20LiDAR%20results%20in%0Asparse%20point%20clouds%2C%20making%20distant%20small%20objects%20even%20more%20blurred.%0AAdditionally%2C%20traditional%20communication%20methods%20have%20relatively%20low%20bandwidth%0Autilization%20efficiency.%20These%20factors%20pose%20challenges%20for%20us.%20To%20balance%20cost%0Aand%20perceptual%20accuracy%2C%20we%20propose%20a%20new%20collaborative%20perception%20framework%2C%0Anamely%20LCV2I.%20LCV2I%20uses%20data%20collected%20from%20cameras%20and%20low-resolution%20LiDAR%0Aas%20input.%20It%20also%20employs%20feature%20offset%20correction%20modules%20and%20regional%0Afeature%20enhancement%20algorithms%20to%20improve%20feature%20representation.%20Finally%2C%20we%0Ause%20regional%20difference%20map%20and%20regional%20score%20map%20to%20assess%20the%20value%20of%0Acollaboration%20content%2C%20thereby%20improving%20communication%20bandwidth%20efficiency.%20In%0Asummary%2C%20our%20approach%20achieves%20high%20perceptual%20performance%20while%20substantially%0Areducing%20the%20demand%20for%20high-resolution%20sensors%20on%20the%20vehicle.%20To%20evaluate%0Athis%20algorithm%2C%20we%20conduct%203D%20object%20detection%20in%20the%20real-world%20scenario%20of%0ADAIR-V2X%2C%20demonstrating%20that%20the%20performance%20of%20LCV2I%20consistently%20surpasses%0Acurrently%20existing%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17039v1&entry.124074799=Read"},
{"title": "Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation", "author": "Mengkang Hu and Tianxing Chen and Yude Zou and Yuheng Lei and Qiguang Chen and Ming Li and Yao Mu and Hongyuan Zhang and Wenqi Shao and Ping Luo", "abstract": "  Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.\n", "link": "http://arxiv.org/abs/2502.13092v2", "date": "2025-02-24", "relevancy": 2.7526, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2World%3A%20Benchmarking%20Large%20Language%20Models%20for%20Symbolic%20World%20Model%0A%20%20Generation&body=Title%3A%20Text2World%3A%20Benchmarking%20Large%20Language%20Models%20for%20Symbolic%20World%20Model%0A%20%20Generation%0AAuthor%3A%20Mengkang%20Hu%20and%20Tianxing%20Chen%20and%20Yude%20Zou%20and%20Yuheng%20Lei%20and%20Qiguang%20Chen%20and%20Ming%20Li%20and%20Yao%20Mu%20and%20Hongyuan%20Zhang%20and%20Wenqi%20Shao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20leveraging%20large%20language%20models%0A%28LLMs%29%20to%20generate%20symbolic%20world%20models%20from%20textual%20descriptions.%20Although%0ALLMs%20have%20been%20extensively%20explored%20in%20the%20context%20of%20world%20modeling%2C%20prior%0Astudies%20encountered%20several%20challenges%2C%20including%20evaluation%20randomness%2C%0Adependence%20on%20indirect%20metrics%2C%20and%20a%20limited%20domain%20scope.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20novel%20benchmark%2C%20Text2World%2C%20based%20on%20planning%0Adomain%20definition%20language%20%28PDDL%29%2C%20featuring%20hundreds%20of%20diverse%20domains%20and%0Aemploying%20multi-criteria%2C%20execution-based%20metrics%20for%20a%20more%20robust%20evaluation.%0AWe%20benchmark%20current%20LLMs%20using%20Text2World%20and%20find%20that%20reasoning%20models%0Atrained%20with%20large-scale%20reinforcement%20learning%20outperform%20others.%20However%2C%0Aeven%20the%20best-performing%20model%20still%20demonstrates%20limited%20capabilities%20in%20world%0Amodeling.%20Building%20on%20these%20insights%2C%20we%20examine%20several%20promising%20strategies%0Ato%20enhance%20the%20world%20modeling%20capabilities%20of%20LLMs%2C%20including%20test-time%0Ascaling%2C%20agent%20training%2C%20and%20more.%20We%20hope%20that%20Text2World%20can%20serve%20as%20a%0Acrucial%20resource%2C%20laying%20the%20groundwork%20for%20future%20research%20in%20leveraging%20LLMs%0Aas%20world%20models.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//text-to-world.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13092v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2World%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520for%2520Symbolic%2520World%2520Model%250A%2520%2520Generation%26entry.906535625%3DMengkang%2520Hu%2520and%2520Tianxing%2520Chen%2520and%2520Yude%2520Zou%2520and%2520Yuheng%2520Lei%2520and%2520Qiguang%2520Chen%2520and%2520Ming%2520Li%2520and%2520Yao%2520Mu%2520and%2520Hongyuan%2520Zhang%2520and%2520Wenqi%2520Shao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520growing%2520interest%2520in%2520leveraging%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520generate%2520symbolic%2520world%2520models%2520from%2520textual%2520descriptions.%2520Although%250ALLMs%2520have%2520been%2520extensively%2520explored%2520in%2520the%2520context%2520of%2520world%2520modeling%252C%2520prior%250Astudies%2520encountered%2520several%2520challenges%252C%2520including%2520evaluation%2520randomness%252C%250Adependence%2520on%2520indirect%2520metrics%252C%2520and%2520a%2520limited%2520domain%2520scope.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%252C%2520Text2World%252C%2520based%2520on%2520planning%250Adomain%2520definition%2520language%2520%2528PDDL%2529%252C%2520featuring%2520hundreds%2520of%2520diverse%2520domains%2520and%250Aemploying%2520multi-criteria%252C%2520execution-based%2520metrics%2520for%2520a%2520more%2520robust%2520evaluation.%250AWe%2520benchmark%2520current%2520LLMs%2520using%2520Text2World%2520and%2520find%2520that%2520reasoning%2520models%250Atrained%2520with%2520large-scale%2520reinforcement%2520learning%2520outperform%2520others.%2520However%252C%250Aeven%2520the%2520best-performing%2520model%2520still%2520demonstrates%2520limited%2520capabilities%2520in%2520world%250Amodeling.%2520Building%2520on%2520these%2520insights%252C%2520we%2520examine%2520several%2520promising%2520strategies%250Ato%2520enhance%2520the%2520world%2520modeling%2520capabilities%2520of%2520LLMs%252C%2520including%2520test-time%250Ascaling%252C%2520agent%2520training%252C%2520and%2520more.%2520We%2520hope%2520that%2520Text2World%2520can%2520serve%2520as%2520a%250Acrucial%2520resource%252C%2520laying%2520the%2520groundwork%2520for%2520future%2520research%2520in%2520leveraging%2520LLMs%250Aas%2520world%2520models.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//text-to-world.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13092v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2World%3A%20Benchmarking%20Large%20Language%20Models%20for%20Symbolic%20World%20Model%0A%20%20Generation&entry.906535625=Mengkang%20Hu%20and%20Tianxing%20Chen%20and%20Yude%20Zou%20and%20Yuheng%20Lei%20and%20Qiguang%20Chen%20and%20Ming%20Li%20and%20Yao%20Mu%20and%20Hongyuan%20Zhang%20and%20Wenqi%20Shao%20and%20Ping%20Luo&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20growing%20interest%20in%20leveraging%20large%20language%20models%0A%28LLMs%29%20to%20generate%20symbolic%20world%20models%20from%20textual%20descriptions.%20Although%0ALLMs%20have%20been%20extensively%20explored%20in%20the%20context%20of%20world%20modeling%2C%20prior%0Astudies%20encountered%20several%20challenges%2C%20including%20evaluation%20randomness%2C%0Adependence%20on%20indirect%20metrics%2C%20and%20a%20limited%20domain%20scope.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20a%20novel%20benchmark%2C%20Text2World%2C%20based%20on%20planning%0Adomain%20definition%20language%20%28PDDL%29%2C%20featuring%20hundreds%20of%20diverse%20domains%20and%0Aemploying%20multi-criteria%2C%20execution-based%20metrics%20for%20a%20more%20robust%20evaluation.%0AWe%20benchmark%20current%20LLMs%20using%20Text2World%20and%20find%20that%20reasoning%20models%0Atrained%20with%20large-scale%20reinforcement%20learning%20outperform%20others.%20However%2C%0Aeven%20the%20best-performing%20model%20still%20demonstrates%20limited%20capabilities%20in%20world%0Amodeling.%20Building%20on%20these%20insights%2C%20we%20examine%20several%20promising%20strategies%0Ato%20enhance%20the%20world%20modeling%20capabilities%20of%20LLMs%2C%20including%20test-time%0Ascaling%2C%20agent%20training%2C%20and%20more.%20We%20hope%20that%20Text2World%20can%20serve%20as%20a%0Acrucial%20resource%2C%20laying%20the%20groundwork%20for%20future%20research%20in%20leveraging%20LLMs%0Aas%20world%20models.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//text-to-world.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13092v2&entry.124074799=Read"},
{"title": "MegaLoc: One Retrieval to Place Them All", "author": "Gabriele Berton and Carlo Masone", "abstract": "  Retrieving images from the same location as a given query is an important\ncomponent of multiple computer vision tasks, like Visual Place Recognition,\nLandmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,\nexisting solutions are built to specifically work for one of these tasks, and\nare known to fail when the requirements slightly change or when they meet\nout-of-distribution data. In this paper we combine a variety of existing\nmethods, training techniques, and datasets to train a retrieval model, called\nMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)\nachieves state of the art on a large number of Visual Place Recognition\ndatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)\nsets a new state of the art for Visual Localization on the LaMAR datasets,\nwhere we only changed the retrieval method to the existing localization\npipeline. The code for MegaLoc is available at\nhttps://github.com/gmberton/MegaLoc\n", "link": "http://arxiv.org/abs/2502.17237v1", "date": "2025-02-24", "relevancy": 2.7266, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5766}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaLoc%3A%20One%20Retrieval%20to%20Place%20Them%20All&body=Title%3A%20MegaLoc%3A%20One%20Retrieval%20to%20Place%20Them%20All%0AAuthor%3A%20Gabriele%20Berton%20and%20Carlo%20Masone%0AAbstract%3A%20%20%20Retrieving%20images%20from%20the%20same%20location%20as%20a%20given%20query%20is%20an%20important%0Acomponent%20of%20multiple%20computer%20vision%20tasks%2C%20like%20Visual%20Place%20Recognition%2C%0ALandmark%20Retrieval%2C%20Visual%20Localization%2C%203D%20reconstruction%2C%20and%20SLAM.%20However%2C%0Aexisting%20solutions%20are%20built%20to%20specifically%20work%20for%20one%20of%20these%20tasks%2C%20and%0Aare%20known%20to%20fail%20when%20the%20requirements%20slightly%20change%20or%20when%20they%20meet%0Aout-of-distribution%20data.%20In%20this%20paper%20we%20combine%20a%20variety%20of%20existing%0Amethods%2C%20training%20techniques%2C%20and%20datasets%20to%20train%20a%20retrieval%20model%2C%20called%0AMegaLoc%2C%20that%20is%20performant%20on%20multiple%20tasks.%20We%20find%20that%20MegaLoc%20%281%29%0Aachieves%20state%20of%20the%20art%20on%20a%20large%20number%20of%20Visual%20Place%20Recognition%0Adatasets%2C%20%282%29%20impressive%20results%20on%20common%20Landmark%20Retrieval%20datasets%2C%20and%20%283%29%0Asets%20a%20new%20state%20of%20the%20art%20for%20Visual%20Localization%20on%20the%20LaMAR%20datasets%2C%0Awhere%20we%20only%20changed%20the%20retrieval%20method%20to%20the%20existing%20localization%0Apipeline.%20The%20code%20for%20MegaLoc%20is%20available%20at%0Ahttps%3A//github.com/gmberton/MegaLoc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17237v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaLoc%253A%2520One%2520Retrieval%2520to%2520Place%2520Them%2520All%26entry.906535625%3DGabriele%2520Berton%2520and%2520Carlo%2520Masone%26entry.1292438233%3D%2520%2520Retrieving%2520images%2520from%2520the%2520same%2520location%2520as%2520a%2520given%2520query%2520is%2520an%2520important%250Acomponent%2520of%2520multiple%2520computer%2520vision%2520tasks%252C%2520like%2520Visual%2520Place%2520Recognition%252C%250ALandmark%2520Retrieval%252C%2520Visual%2520Localization%252C%25203D%2520reconstruction%252C%2520and%2520SLAM.%2520However%252C%250Aexisting%2520solutions%2520are%2520built%2520to%2520specifically%2520work%2520for%2520one%2520of%2520these%2520tasks%252C%2520and%250Aare%2520known%2520to%2520fail%2520when%2520the%2520requirements%2520slightly%2520change%2520or%2520when%2520they%2520meet%250Aout-of-distribution%2520data.%2520In%2520this%2520paper%2520we%2520combine%2520a%2520variety%2520of%2520existing%250Amethods%252C%2520training%2520techniques%252C%2520and%2520datasets%2520to%2520train%2520a%2520retrieval%2520model%252C%2520called%250AMegaLoc%252C%2520that%2520is%2520performant%2520on%2520multiple%2520tasks.%2520We%2520find%2520that%2520MegaLoc%2520%25281%2529%250Aachieves%2520state%2520of%2520the%2520art%2520on%2520a%2520large%2520number%2520of%2520Visual%2520Place%2520Recognition%250Adatasets%252C%2520%25282%2529%2520impressive%2520results%2520on%2520common%2520Landmark%2520Retrieval%2520datasets%252C%2520and%2520%25283%2529%250Asets%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520Visual%2520Localization%2520on%2520the%2520LaMAR%2520datasets%252C%250Awhere%2520we%2520only%2520changed%2520the%2520retrieval%2520method%2520to%2520the%2520existing%2520localization%250Apipeline.%2520The%2520code%2520for%2520MegaLoc%2520is%2520available%2520at%250Ahttps%253A//github.com/gmberton/MegaLoc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17237v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaLoc%3A%20One%20Retrieval%20to%20Place%20Them%20All&entry.906535625=Gabriele%20Berton%20and%20Carlo%20Masone&entry.1292438233=%20%20Retrieving%20images%20from%20the%20same%20location%20as%20a%20given%20query%20is%20an%20important%0Acomponent%20of%20multiple%20computer%20vision%20tasks%2C%20like%20Visual%20Place%20Recognition%2C%0ALandmark%20Retrieval%2C%20Visual%20Localization%2C%203D%20reconstruction%2C%20and%20SLAM.%20However%2C%0Aexisting%20solutions%20are%20built%20to%20specifically%20work%20for%20one%20of%20these%20tasks%2C%20and%0Aare%20known%20to%20fail%20when%20the%20requirements%20slightly%20change%20or%20when%20they%20meet%0Aout-of-distribution%20data.%20In%20this%20paper%20we%20combine%20a%20variety%20of%20existing%0Amethods%2C%20training%20techniques%2C%20and%20datasets%20to%20train%20a%20retrieval%20model%2C%20called%0AMegaLoc%2C%20that%20is%20performant%20on%20multiple%20tasks.%20We%20find%20that%20MegaLoc%20%281%29%0Aachieves%20state%20of%20the%20art%20on%20a%20large%20number%20of%20Visual%20Place%20Recognition%0Adatasets%2C%20%282%29%20impressive%20results%20on%20common%20Landmark%20Retrieval%20datasets%2C%20and%20%283%29%0Asets%20a%20new%20state%20of%20the%20art%20for%20Visual%20Localization%20on%20the%20LaMAR%20datasets%2C%0Awhere%20we%20only%20changed%20the%20retrieval%20method%20to%20the%20existing%20localization%0Apipeline.%20The%20code%20for%20MegaLoc%20is%20available%20at%0Ahttps%3A//github.com/gmberton/MegaLoc%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17237v1&entry.124074799=Read"},
{"title": "Introducing Visual Perception Token into Multimodal Large Language Model", "author": "Runpeng Yu and Xinyin Ma and Xinchao Wang", "abstract": "  To utilize visual information, Multimodal Large Language Model (MLLM) relies\non the perception process of its vision encoder. The completeness and accuracy\nof visual perception significantly influence the precision of spatial\nreasoning, fine-grained understanding, and other tasks. However, MLLM still\nlacks the autonomous capability to control its own visual perception processes,\nfor example, selectively reviewing specific regions of an image or focusing on\ninformation related to specific object categories. In this work, we propose the\nconcept of Visual Perception Token, aiming to empower MLLM with a mechanism to\ncontrol its visual perception processes. We design two types of Visual\nPerception Tokens, termed the Region Selection Token and the Vision Re-Encoding\nToken. MLLMs autonomously generate these tokens, just as they generate text,\nand use them to trigger additional visual perception actions. The Region\nSelection Token explicitly identifies specific regions in an image that require\nfurther perception, while the Vision Re-Encoding Token uses its hidden states\nas control signals to guide additional visual perception processes. Extensive\nexperiments demonstrate the advantages of these tokens in handling spatial\nreasoning, improving fine-grained understanding, and other tasks. On average,\nthe introduction of Visual Perception Tokens improves the performance of a 2B\nmodel by 23.6\\%, increasing its score from 0.572 to 0.708, and even outperforms\na 7B parameter model by 13.4\\% (from 0.624). Please check out our repo\nhttps://github.com/yu-rp/VisualPerceptionToken\n", "link": "http://arxiv.org/abs/2502.17425v1", "date": "2025-02-24", "relevancy": 2.7262, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20Visual%20Perception%20Token%20into%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Introducing%20Visual%20Perception%20Token%20into%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Runpeng%20Yu%20and%20Xinyin%20Ma%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20To%20utilize%20visual%20information%2C%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20relies%0Aon%20the%20perception%20process%20of%20its%20vision%20encoder.%20The%20completeness%20and%20accuracy%0Aof%20visual%20perception%20significantly%20influence%20the%20precision%20of%20spatial%0Areasoning%2C%20fine-grained%20understanding%2C%20and%20other%20tasks.%20However%2C%20MLLM%20still%0Alacks%20the%20autonomous%20capability%20to%20control%20its%20own%20visual%20perception%20processes%2C%0Afor%20example%2C%20selectively%20reviewing%20specific%20regions%20of%20an%20image%20or%20focusing%20on%0Ainformation%20related%20to%20specific%20object%20categories.%20In%20this%20work%2C%20we%20propose%20the%0Aconcept%20of%20Visual%20Perception%20Token%2C%20aiming%20to%20empower%20MLLM%20with%20a%20mechanism%20to%0Acontrol%20its%20visual%20perception%20processes.%20We%20design%20two%20types%20of%20Visual%0APerception%20Tokens%2C%20termed%20the%20Region%20Selection%20Token%20and%20the%20Vision%20Re-Encoding%0AToken.%20MLLMs%20autonomously%20generate%20these%20tokens%2C%20just%20as%20they%20generate%20text%2C%0Aand%20use%20them%20to%20trigger%20additional%20visual%20perception%20actions.%20The%20Region%0ASelection%20Token%20explicitly%20identifies%20specific%20regions%20in%20an%20image%20that%20require%0Afurther%20perception%2C%20while%20the%20Vision%20Re-Encoding%20Token%20uses%20its%20hidden%20states%0Aas%20control%20signals%20to%20guide%20additional%20visual%20perception%20processes.%20Extensive%0Aexperiments%20demonstrate%20the%20advantages%20of%20these%20tokens%20in%20handling%20spatial%0Areasoning%2C%20improving%20fine-grained%20understanding%2C%20and%20other%20tasks.%20On%20average%2C%0Athe%20introduction%20of%20Visual%20Perception%20Tokens%20improves%20the%20performance%20of%20a%202B%0Amodel%20by%2023.6%5C%25%2C%20increasing%20its%20score%20from%200.572%20to%200.708%2C%20and%20even%20outperforms%0Aa%207B%20parameter%20model%20by%2013.4%5C%25%20%28from%200.624%29.%20Please%20check%20out%20our%20repo%0Ahttps%3A//github.com/yu-rp/VisualPerceptionToken%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520Visual%2520Perception%2520Token%2520into%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DRunpeng%2520Yu%2520and%2520Xinyin%2520Ma%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520To%2520utilize%2520visual%2520information%252C%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520relies%250Aon%2520the%2520perception%2520process%2520of%2520its%2520vision%2520encoder.%2520The%2520completeness%2520and%2520accuracy%250Aof%2520visual%2520perception%2520significantly%2520influence%2520the%2520precision%2520of%2520spatial%250Areasoning%252C%2520fine-grained%2520understanding%252C%2520and%2520other%2520tasks.%2520However%252C%2520MLLM%2520still%250Alacks%2520the%2520autonomous%2520capability%2520to%2520control%2520its%2520own%2520visual%2520perception%2520processes%252C%250Afor%2520example%252C%2520selectively%2520reviewing%2520specific%2520regions%2520of%2520an%2520image%2520or%2520focusing%2520on%250Ainformation%2520related%2520to%2520specific%2520object%2520categories.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%250Aconcept%2520of%2520Visual%2520Perception%2520Token%252C%2520aiming%2520to%2520empower%2520MLLM%2520with%2520a%2520mechanism%2520to%250Acontrol%2520its%2520visual%2520perception%2520processes.%2520We%2520design%2520two%2520types%2520of%2520Visual%250APerception%2520Tokens%252C%2520termed%2520the%2520Region%2520Selection%2520Token%2520and%2520the%2520Vision%2520Re-Encoding%250AToken.%2520MLLMs%2520autonomously%2520generate%2520these%2520tokens%252C%2520just%2520as%2520they%2520generate%2520text%252C%250Aand%2520use%2520them%2520to%2520trigger%2520additional%2520visual%2520perception%2520actions.%2520The%2520Region%250ASelection%2520Token%2520explicitly%2520identifies%2520specific%2520regions%2520in%2520an%2520image%2520that%2520require%250Afurther%2520perception%252C%2520while%2520the%2520Vision%2520Re-Encoding%2520Token%2520uses%2520its%2520hidden%2520states%250Aas%2520control%2520signals%2520to%2520guide%2520additional%2520visual%2520perception%2520processes.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520advantages%2520of%2520these%2520tokens%2520in%2520handling%2520spatial%250Areasoning%252C%2520improving%2520fine-grained%2520understanding%252C%2520and%2520other%2520tasks.%2520On%2520average%252C%250Athe%2520introduction%2520of%2520Visual%2520Perception%2520Tokens%2520improves%2520the%2520performance%2520of%2520a%25202B%250Amodel%2520by%252023.6%255C%2525%252C%2520increasing%2520its%2520score%2520from%25200.572%2520to%25200.708%252C%2520and%2520even%2520outperforms%250Aa%25207B%2520parameter%2520model%2520by%252013.4%255C%2525%2520%2528from%25200.624%2529.%2520Please%2520check%2520out%2520our%2520repo%250Ahttps%253A//github.com/yu-rp/VisualPerceptionToken%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20Visual%20Perception%20Token%20into%20Multimodal%20Large%20Language%20Model&entry.906535625=Runpeng%20Yu%20and%20Xinyin%20Ma%20and%20Xinchao%20Wang&entry.1292438233=%20%20To%20utilize%20visual%20information%2C%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20relies%0Aon%20the%20perception%20process%20of%20its%20vision%20encoder.%20The%20completeness%20and%20accuracy%0Aof%20visual%20perception%20significantly%20influence%20the%20precision%20of%20spatial%0Areasoning%2C%20fine-grained%20understanding%2C%20and%20other%20tasks.%20However%2C%20MLLM%20still%0Alacks%20the%20autonomous%20capability%20to%20control%20its%20own%20visual%20perception%20processes%2C%0Afor%20example%2C%20selectively%20reviewing%20specific%20regions%20of%20an%20image%20or%20focusing%20on%0Ainformation%20related%20to%20specific%20object%20categories.%20In%20this%20work%2C%20we%20propose%20the%0Aconcept%20of%20Visual%20Perception%20Token%2C%20aiming%20to%20empower%20MLLM%20with%20a%20mechanism%20to%0Acontrol%20its%20visual%20perception%20processes.%20We%20design%20two%20types%20of%20Visual%0APerception%20Tokens%2C%20termed%20the%20Region%20Selection%20Token%20and%20the%20Vision%20Re-Encoding%0AToken.%20MLLMs%20autonomously%20generate%20these%20tokens%2C%20just%20as%20they%20generate%20text%2C%0Aand%20use%20them%20to%20trigger%20additional%20visual%20perception%20actions.%20The%20Region%0ASelection%20Token%20explicitly%20identifies%20specific%20regions%20in%20an%20image%20that%20require%0Afurther%20perception%2C%20while%20the%20Vision%20Re-Encoding%20Token%20uses%20its%20hidden%20states%0Aas%20control%20signals%20to%20guide%20additional%20visual%20perception%20processes.%20Extensive%0Aexperiments%20demonstrate%20the%20advantages%20of%20these%20tokens%20in%20handling%20spatial%0Areasoning%2C%20improving%20fine-grained%20understanding%2C%20and%20other%20tasks.%20On%20average%2C%0Athe%20introduction%20of%20Visual%20Perception%20Tokens%20improves%20the%20performance%20of%20a%202B%0Amodel%20by%2023.6%5C%25%2C%20increasing%20its%20score%20from%200.572%20to%200.708%2C%20and%20even%20outperforms%0Aa%207B%20parameter%20model%20by%2013.4%5C%25%20%28from%200.624%29.%20Please%20check%20out%20our%20repo%0Ahttps%3A//github.com/yu-rp/VisualPerceptionToken%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17425v1&entry.124074799=Read"},
{"title": "A Neural Material Point Method for Particle-based Emulation", "author": "Omer Rochman Sharabi and Sacha Lewin and Gilles Louppe", "abstract": "  Mesh-free Lagrangian methods are widely used for simulating fluids, solids,\nand their complex interactions due to their ability to handle large\ndeformations and topological changes. These physics simulators, however,\nrequire substantial computational resources for accurate simulations. To\naddress these issues, deep learning emulators promise faster and scalable\nsimulations, yet they often remain expensive and difficult to train, limiting\ntheir practical use. Inspired by the Material Point Method (MPM), we present\nNeuralMPM, a neural emulation framework for particle-based simulations.\nNeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes\nupdates on grid nodes using image-to-image neural networks, and interpolates\nback to the particles. Similarly to MPM, NeuralMPM benefits from the regular\nvoxelized representation to simplify the computation of the state dynamics,\nwhile avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the\nadvantages of NeuralMPM on several datasets, including fluid dynamics and\nfluid-solid interactions. Compared to existing methods, NeuralMPM reduces\ntraining times from days to hours, while achieving comparable or superior\nlong-term accuracy, making it a promising approach for practical forward and\ninverse problems. A project page is available at https://neuralmpm.isach.be\n", "link": "http://arxiv.org/abs/2408.15753v3", "date": "2025-02-24", "relevancy": 2.7206, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5874}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5325}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Neural%20Material%20Point%20Method%20for%20Particle-based%20Emulation&body=Title%3A%20A%20Neural%20Material%20Point%20Method%20for%20Particle-based%20Emulation%0AAuthor%3A%20Omer%20Rochman%20Sharabi%20and%20Sacha%20Lewin%20and%20Gilles%20Louppe%0AAbstract%3A%20%20%20Mesh-free%20Lagrangian%20methods%20are%20widely%20used%20for%20simulating%20fluids%2C%20solids%2C%0Aand%20their%20complex%20interactions%20due%20to%20their%20ability%20to%20handle%20large%0Adeformations%20and%20topological%20changes.%20These%20physics%20simulators%2C%20however%2C%0Arequire%20substantial%20computational%20resources%20for%20accurate%20simulations.%20To%0Aaddress%20these%20issues%2C%20deep%20learning%20emulators%20promise%20faster%20and%20scalable%0Asimulations%2C%20yet%20they%20often%20remain%20expensive%20and%20difficult%20to%20train%2C%20limiting%0Atheir%20practical%20use.%20Inspired%20by%20the%20Material%20Point%20Method%20%28MPM%29%2C%20we%20present%0ANeuralMPM%2C%20a%20neural%20emulation%20framework%20for%20particle-based%20simulations.%0ANeuralMPM%20interpolates%20Lagrangian%20particles%20onto%20a%20fixed-size%20grid%2C%20computes%0Aupdates%20on%20grid%20nodes%20using%20image-to-image%20neural%20networks%2C%20and%20interpolates%0Aback%20to%20the%20particles.%20Similarly%20to%20MPM%2C%20NeuralMPM%20benefits%20from%20the%20regular%0Avoxelized%20representation%20to%20simplify%20the%20computation%20of%20the%20state%20dynamics%2C%0Awhile%20avoiding%20the%20drawbacks%20of%20mesh-based%20Eulerian%20methods.%20We%20demonstrate%20the%0Aadvantages%20of%20NeuralMPM%20on%20several%20datasets%2C%20including%20fluid%20dynamics%20and%0Afluid-solid%20interactions.%20Compared%20to%20existing%20methods%2C%20NeuralMPM%20reduces%0Atraining%20times%20from%20days%20to%20hours%2C%20while%20achieving%20comparable%20or%20superior%0Along-term%20accuracy%2C%20making%20it%20a%20promising%20approach%20for%20practical%20forward%20and%0Ainverse%20problems.%20A%20project%20page%20is%20available%20at%20https%3A//neuralmpm.isach.be%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15753v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Neural%2520Material%2520Point%2520Method%2520for%2520Particle-based%2520Emulation%26entry.906535625%3DOmer%2520Rochman%2520Sharabi%2520and%2520Sacha%2520Lewin%2520and%2520Gilles%2520Louppe%26entry.1292438233%3D%2520%2520Mesh-free%2520Lagrangian%2520methods%2520are%2520widely%2520used%2520for%2520simulating%2520fluids%252C%2520solids%252C%250Aand%2520their%2520complex%2520interactions%2520due%2520to%2520their%2520ability%2520to%2520handle%2520large%250Adeformations%2520and%2520topological%2520changes.%2520These%2520physics%2520simulators%252C%2520however%252C%250Arequire%2520substantial%2520computational%2520resources%2520for%2520accurate%2520simulations.%2520To%250Aaddress%2520these%2520issues%252C%2520deep%2520learning%2520emulators%2520promise%2520faster%2520and%2520scalable%250Asimulations%252C%2520yet%2520they%2520often%2520remain%2520expensive%2520and%2520difficult%2520to%2520train%252C%2520limiting%250Atheir%2520practical%2520use.%2520Inspired%2520by%2520the%2520Material%2520Point%2520Method%2520%2528MPM%2529%252C%2520we%2520present%250ANeuralMPM%252C%2520a%2520neural%2520emulation%2520framework%2520for%2520particle-based%2520simulations.%250ANeuralMPM%2520interpolates%2520Lagrangian%2520particles%2520onto%2520a%2520fixed-size%2520grid%252C%2520computes%250Aupdates%2520on%2520grid%2520nodes%2520using%2520image-to-image%2520neural%2520networks%252C%2520and%2520interpolates%250Aback%2520to%2520the%2520particles.%2520Similarly%2520to%2520MPM%252C%2520NeuralMPM%2520benefits%2520from%2520the%2520regular%250Avoxelized%2520representation%2520to%2520simplify%2520the%2520computation%2520of%2520the%2520state%2520dynamics%252C%250Awhile%2520avoiding%2520the%2520drawbacks%2520of%2520mesh-based%2520Eulerian%2520methods.%2520We%2520demonstrate%2520the%250Aadvantages%2520of%2520NeuralMPM%2520on%2520several%2520datasets%252C%2520including%2520fluid%2520dynamics%2520and%250Afluid-solid%2520interactions.%2520Compared%2520to%2520existing%2520methods%252C%2520NeuralMPM%2520reduces%250Atraining%2520times%2520from%2520days%2520to%2520hours%252C%2520while%2520achieving%2520comparable%2520or%2520superior%250Along-term%2520accuracy%252C%2520making%2520it%2520a%2520promising%2520approach%2520for%2520practical%2520forward%2520and%250Ainverse%2520problems.%2520A%2520project%2520page%2520is%2520available%2520at%2520https%253A//neuralmpm.isach.be%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15753v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Neural%20Material%20Point%20Method%20for%20Particle-based%20Emulation&entry.906535625=Omer%20Rochman%20Sharabi%20and%20Sacha%20Lewin%20and%20Gilles%20Louppe&entry.1292438233=%20%20Mesh-free%20Lagrangian%20methods%20are%20widely%20used%20for%20simulating%20fluids%2C%20solids%2C%0Aand%20their%20complex%20interactions%20due%20to%20their%20ability%20to%20handle%20large%0Adeformations%20and%20topological%20changes.%20These%20physics%20simulators%2C%20however%2C%0Arequire%20substantial%20computational%20resources%20for%20accurate%20simulations.%20To%0Aaddress%20these%20issues%2C%20deep%20learning%20emulators%20promise%20faster%20and%20scalable%0Asimulations%2C%20yet%20they%20often%20remain%20expensive%20and%20difficult%20to%20train%2C%20limiting%0Atheir%20practical%20use.%20Inspired%20by%20the%20Material%20Point%20Method%20%28MPM%29%2C%20we%20present%0ANeuralMPM%2C%20a%20neural%20emulation%20framework%20for%20particle-based%20simulations.%0ANeuralMPM%20interpolates%20Lagrangian%20particles%20onto%20a%20fixed-size%20grid%2C%20computes%0Aupdates%20on%20grid%20nodes%20using%20image-to-image%20neural%20networks%2C%20and%20interpolates%0Aback%20to%20the%20particles.%20Similarly%20to%20MPM%2C%20NeuralMPM%20benefits%20from%20the%20regular%0Avoxelized%20representation%20to%20simplify%20the%20computation%20of%20the%20state%20dynamics%2C%0Awhile%20avoiding%20the%20drawbacks%20of%20mesh-based%20Eulerian%20methods.%20We%20demonstrate%20the%0Aadvantages%20of%20NeuralMPM%20on%20several%20datasets%2C%20including%20fluid%20dynamics%20and%0Afluid-solid%20interactions.%20Compared%20to%20existing%20methods%2C%20NeuralMPM%20reduces%0Atraining%20times%20from%20days%20to%20hours%2C%20while%20achieving%20comparable%20or%20superior%0Along-term%20accuracy%2C%20making%20it%20a%20promising%20approach%20for%20practical%20forward%20and%0Ainverse%20problems.%20A%20project%20page%20is%20available%20at%20https%3A//neuralmpm.isach.be%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15753v3&entry.124074799=Read"},
{"title": "Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI", "author": "Syed Abdul Gaffar Shakhadri and Kruthika KR and Kartik Basavaraj Angadi", "abstract": "  We introduce Shakti VLM, a family of vision-language models in the capacity\nof 1B and 4B parameters designed to address data efficiency challenges in\nmultimodal learning. While recent VLMs achieve strong performance through\nextensive training data, Shakti models leverage architectural innovations to\nattain competitive results with fewer tokens. Key advancements include\nQK-Normalization for attention stability, hybrid normalization techniques, and\nenhanced positional encoding. A three-stage training strategy further optimizes\nlearning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and\nShakti-VLM-4B excel in document understanding, Visual Reasoning, OCR\nextraction, and general multimodal reasoning. Our results highlight that high\nperformance can be achieved through model design and training strategy rather\nthan sheer data volume, making Shakti an efficient solution for\nenterprise-scale multimodal tasks.\n", "link": "http://arxiv.org/abs/2502.17092v1", "date": "2025-02-24", "relevancy": 2.6571, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shakti-VLMs%3A%20Scalable%20Vision-Language%20Models%20for%20Enterprise%20AI&body=Title%3A%20Shakti-VLMs%3A%20Scalable%20Vision-Language%20Models%20for%20Enterprise%20AI%0AAuthor%3A%20Syed%20Abdul%20Gaffar%20Shakhadri%20and%20Kruthika%20KR%20and%20Kartik%20Basavaraj%20Angadi%0AAbstract%3A%20%20%20We%20introduce%20Shakti%20VLM%2C%20a%20family%20of%20vision-language%20models%20in%20the%20capacity%0Aof%201B%20and%204B%20parameters%20designed%20to%20address%20data%20efficiency%20challenges%20in%0Amultimodal%20learning.%20While%20recent%20VLMs%20achieve%20strong%20performance%20through%0Aextensive%20training%20data%2C%20Shakti%20models%20leverage%20architectural%20innovations%20to%0Aattain%20competitive%20results%20with%20fewer%20tokens.%20Key%20advancements%20include%0AQK-Normalization%20for%20attention%20stability%2C%20hybrid%20normalization%20techniques%2C%20and%0Aenhanced%20positional%20encoding.%20A%20three-stage%20training%20strategy%20further%20optimizes%0Alearning%20efficiency.%20Evaluations%20show%20that%20Shakti-Shakti-VLM-1B%20and%0AShakti-VLM-4B%20excel%20in%20document%20understanding%2C%20Visual%20Reasoning%2C%20OCR%0Aextraction%2C%20and%20general%20multimodal%20reasoning.%20Our%20results%20highlight%20that%20high%0Aperformance%20can%20be%20achieved%20through%20model%20design%20and%20training%20strategy%20rather%0Athan%20sheer%20data%20volume%2C%20making%20Shakti%20an%20efficient%20solution%20for%0Aenterprise-scale%20multimodal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShakti-VLMs%253A%2520Scalable%2520Vision-Language%2520Models%2520for%2520Enterprise%2520AI%26entry.906535625%3DSyed%2520Abdul%2520Gaffar%2520Shakhadri%2520and%2520Kruthika%2520KR%2520and%2520Kartik%2520Basavaraj%2520Angadi%26entry.1292438233%3D%2520%2520We%2520introduce%2520Shakti%2520VLM%252C%2520a%2520family%2520of%2520vision-language%2520models%2520in%2520the%2520capacity%250Aof%25201B%2520and%25204B%2520parameters%2520designed%2520to%2520address%2520data%2520efficiency%2520challenges%2520in%250Amultimodal%2520learning.%2520While%2520recent%2520VLMs%2520achieve%2520strong%2520performance%2520through%250Aextensive%2520training%2520data%252C%2520Shakti%2520models%2520leverage%2520architectural%2520innovations%2520to%250Aattain%2520competitive%2520results%2520with%2520fewer%2520tokens.%2520Key%2520advancements%2520include%250AQK-Normalization%2520for%2520attention%2520stability%252C%2520hybrid%2520normalization%2520techniques%252C%2520and%250Aenhanced%2520positional%2520encoding.%2520A%2520three-stage%2520training%2520strategy%2520further%2520optimizes%250Alearning%2520efficiency.%2520Evaluations%2520show%2520that%2520Shakti-Shakti-VLM-1B%2520and%250AShakti-VLM-4B%2520excel%2520in%2520document%2520understanding%252C%2520Visual%2520Reasoning%252C%2520OCR%250Aextraction%252C%2520and%2520general%2520multimodal%2520reasoning.%2520Our%2520results%2520highlight%2520that%2520high%250Aperformance%2520can%2520be%2520achieved%2520through%2520model%2520design%2520and%2520training%2520strategy%2520rather%250Athan%2520sheer%2520data%2520volume%252C%2520making%2520Shakti%2520an%2520efficient%2520solution%2520for%250Aenterprise-scale%2520multimodal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shakti-VLMs%3A%20Scalable%20Vision-Language%20Models%20for%20Enterprise%20AI&entry.906535625=Syed%20Abdul%20Gaffar%20Shakhadri%20and%20Kruthika%20KR%20and%20Kartik%20Basavaraj%20Angadi&entry.1292438233=%20%20We%20introduce%20Shakti%20VLM%2C%20a%20family%20of%20vision-language%20models%20in%20the%20capacity%0Aof%201B%20and%204B%20parameters%20designed%20to%20address%20data%20efficiency%20challenges%20in%0Amultimodal%20learning.%20While%20recent%20VLMs%20achieve%20strong%20performance%20through%0Aextensive%20training%20data%2C%20Shakti%20models%20leverage%20architectural%20innovations%20to%0Aattain%20competitive%20results%20with%20fewer%20tokens.%20Key%20advancements%20include%0AQK-Normalization%20for%20attention%20stability%2C%20hybrid%20normalization%20techniques%2C%20and%0Aenhanced%20positional%20encoding.%20A%20three-stage%20training%20strategy%20further%20optimizes%0Alearning%20efficiency.%20Evaluations%20show%20that%20Shakti-Shakti-VLM-1B%20and%0AShakti-VLM-4B%20excel%20in%20document%20understanding%2C%20Visual%20Reasoning%2C%20OCR%0Aextraction%2C%20and%20general%20multimodal%20reasoning.%20Our%20results%20highlight%20that%20high%0Aperformance%20can%20be%20achieved%20through%20model%20design%20and%20training%20strategy%20rather%0Athan%20sheer%20data%20volume%2C%20making%20Shakti%20an%20efficient%20solution%20for%0Aenterprise-scale%20multimodal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17092v1&entry.124074799=Read"},
{"title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model", "author": "Guoqing Ma and Haoyang Huang and Kun Yan and Liangyu Chen and Nan Duan and Shengming Yin and Changyi Wan and Ranchen Ming and Xiaoniu Song and Xing Chen and Yu Zhou and Deshan Sun and Deyu Zhou and Jian Zhou and Kaijun Tan and Kang An and Mei Chen and Wei Ji and Qiling Wu and Wen Sun and Xin Han and Yanan Wei and Zheng Ge and Aojie Li and Bin Wang and Bizhu Huang and Bo Wang and Brian Li and Changxing Miao and Chen Xu and Chenfei Wu and Chenguang Yu and Dapeng Shi and Dingyuan Hu and Enle Liu and Gang Yu and Ge Yang and Guanzhe Huang and Gulin Yan and Haiyang Feng and Hao Nie and Haonan Jia and Hanpeng Hu and Hanqi Chen and Haolong Yan and Heng Wang and Hongcheng Guo and Huilin Xiong and Huixin Xiong and Jiahao Gong and Jianchang Wu and Jiaoren Wu and Jie Wu and Jie Yang and Jiashuai Liu and Jiashuo Li and Jingyang Zhang and Junjing Guo and Junzhe Lin and Kaixiang Li and Lei Liu and Lei Xia and Liang Zhao and Liguo Tan and Liwen Huang and Liying Shi and Ming Li and Mingliang Li and Muhua Cheng and Na Wang and Qiaohui Chen and Qinglin He and Qiuyan Liang and Quan Sun and Ran Sun and Rui Wang and Shaoliang Pang and Shiliang Yang and Sitong Liu and Siqi Liu and Shuli Gao and Tiancheng Cao and Tianyu Wang and Weipeng Ming and Wenqing He and Xu Zhao and Xuelin Zhang and Xianfang Zeng and Xiaojia Liu and Xuan Yang and Yaqi Dai and Yanbo Yu and Yang Li and Yineng Deng and Yingming Wang and Yilei Wang and Yuanwei Lu and Yu Chen and Yu Luo and Yuchu Luo and Yuhe Yin and Yuheng Feng and Yuxiang Yang and Zecheng Tang and Zekai Zhang and Zidong Yang and Binxing Jiao and Jiansheng Chen and Jing Li and Shuchang Zhou and Xiangyu Zhang and Xinhao Zhang and Yibo Zhu and Heung-Yeung Shum and Daxin Jiang", "abstract": "  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n", "link": "http://arxiv.org/abs/2502.10248v3", "date": "2025-02-24", "relevancy": 2.5919, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6653}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6573}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-Video-T2V%20Technical%20Report%3A%20The%20Practice%2C%20Challenges%2C%20and%20Future%20of%0A%20%20Video%20Foundation%20Model&body=Title%3A%20Step-Video-T2V%20Technical%20Report%3A%20The%20Practice%2C%20Challenges%2C%20and%20Future%20of%0A%20%20Video%20Foundation%20Model%0AAuthor%3A%20Guoqing%20Ma%20and%20Haoyang%20Huang%20and%20Kun%20Yan%20and%20Liangyu%20Chen%20and%20Nan%20Duan%20and%20Shengming%20Yin%20and%20Changyi%20Wan%20and%20Ranchen%20Ming%20and%20Xiaoniu%20Song%20and%20Xing%20Chen%20and%20Yu%20Zhou%20and%20Deshan%20Sun%20and%20Deyu%20Zhou%20and%20Jian%20Zhou%20and%20Kaijun%20Tan%20and%20Kang%20An%20and%20Mei%20Chen%20and%20Wei%20Ji%20and%20Qiling%20Wu%20and%20Wen%20Sun%20and%20Xin%20Han%20and%20Yanan%20Wei%20and%20Zheng%20Ge%20and%20Aojie%20Li%20and%20Bin%20Wang%20and%20Bizhu%20Huang%20and%20Bo%20Wang%20and%20Brian%20Li%20and%20Changxing%20Miao%20and%20Chen%20Xu%20and%20Chenfei%20Wu%20and%20Chenguang%20Yu%20and%20Dapeng%20Shi%20and%20Dingyuan%20Hu%20and%20Enle%20Liu%20and%20Gang%20Yu%20and%20Ge%20Yang%20and%20Guanzhe%20Huang%20and%20Gulin%20Yan%20and%20Haiyang%20Feng%20and%20Hao%20Nie%20and%20Haonan%20Jia%20and%20Hanpeng%20Hu%20and%20Hanqi%20Chen%20and%20Haolong%20Yan%20and%20Heng%20Wang%20and%20Hongcheng%20Guo%20and%20Huilin%20Xiong%20and%20Huixin%20Xiong%20and%20Jiahao%20Gong%20and%20Jianchang%20Wu%20and%20Jiaoren%20Wu%20and%20Jie%20Wu%20and%20Jie%20Yang%20and%20Jiashuai%20Liu%20and%20Jiashuo%20Li%20and%20Jingyang%20Zhang%20and%20Junjing%20Guo%20and%20Junzhe%20Lin%20and%20Kaixiang%20Li%20and%20Lei%20Liu%20and%20Lei%20Xia%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liwen%20Huang%20and%20Liying%20Shi%20and%20Ming%20Li%20and%20Mingliang%20Li%20and%20Muhua%20Cheng%20and%20Na%20Wang%20and%20Qiaohui%20Chen%20and%20Qinglin%20He%20and%20Qiuyan%20Liang%20and%20Quan%20Sun%20and%20Ran%20Sun%20and%20Rui%20Wang%20and%20Shaoliang%20Pang%20and%20Shiliang%20Yang%20and%20Sitong%20Liu%20and%20Siqi%20Liu%20and%20Shuli%20Gao%20and%20Tiancheng%20Cao%20and%20Tianyu%20Wang%20and%20Weipeng%20Ming%20and%20Wenqing%20He%20and%20Xu%20Zhao%20and%20Xuelin%20Zhang%20and%20Xianfang%20Zeng%20and%20Xiaojia%20Liu%20and%20Xuan%20Yang%20and%20Yaqi%20Dai%20and%20Yanbo%20Yu%20and%20Yang%20Li%20and%20Yineng%20Deng%20and%20Yingming%20Wang%20and%20Yilei%20Wang%20and%20Yuanwei%20Lu%20and%20Yu%20Chen%20and%20Yu%20Luo%20and%20Yuchu%20Luo%20and%20Yuhe%20Yin%20and%20Yuheng%20Feng%20and%20Yuxiang%20Yang%20and%20Zecheng%20Tang%20and%20Zekai%20Zhang%20and%20Zidong%20Yang%20and%20Binxing%20Jiao%20and%20Jiansheng%20Chen%20and%20Jing%20Li%20and%20Shuchang%20Zhou%20and%20Xiangyu%20Zhang%20and%20Xinhao%20Zhang%20and%20Yibo%20Zhu%20and%20Heung-Yeung%20Shum%20and%20Daxin%20Jiang%0AAbstract%3A%20%20%20We%20present%20Step-Video-T2V%2C%20a%20state-of-the-art%20text-to-video%20pre-trained%20model%0Awith%2030B%20parameters%20and%20the%20ability%20to%20generate%20videos%20up%20to%20204%20frames%20in%0Alength.%20A%20deep%20compression%20Variational%20Autoencoder%2C%20Video-VAE%2C%20is%20designed%20for%0Avideo%20generation%20tasks%2C%20achieving%2016x16%20spatial%20and%208x%20temporal%20compression%0Aratios%2C%20while%20maintaining%20exceptional%20video%20reconstruction%20quality.%20User%0Aprompts%20are%20encoded%20using%20two%20bilingual%20text%20encoders%20to%20handle%20both%20English%0Aand%20Chinese.%20A%20DiT%20with%203D%20full%20attention%20is%20trained%20using%20Flow%20Matching%20and%20is%0Aemployed%20to%20denoise%20input%20noise%20into%20latent%20frames.%20A%20video-based%20DPO%20approach%2C%0AVideo-DPO%2C%20is%20applied%20to%20reduce%20artifacts%20and%20improve%20the%20visual%20quality%20of%20the%0Agenerated%20videos.%20We%20also%20detail%20our%20training%20strategies%20and%20share%20key%0Aobservations%20and%20insights.%20Step-Video-T2V%27s%20performance%20is%20evaluated%20on%20a%20novel%0Avideo%20generation%20benchmark%2C%20Step-Video-T2V-Eval%2C%20demonstrating%20its%0Astate-of-the-art%20text-to-video%20quality%20when%20compared%20with%20both%20open-source%20and%0Acommercial%20engines.%20Additionally%2C%20we%20discuss%20the%20limitations%20of%20current%0Adiffusion-based%20model%20paradigm%20and%20outline%20future%20directions%20for%20video%0Afoundation%20models.%20We%20make%20both%20Step-Video-T2V%20and%20Step-Video-T2V-Eval%0Aavailable%20at%20https%3A//github.com/stepfun-ai/Step-Video-T2V.%20The%20online%20version%0Acan%20be%20accessed%20from%20https%3A//yuewen.cn/videos%20as%20well.%20Our%20goal%20is%20to%0Aaccelerate%20the%20innovation%20of%20video%20foundation%20models%20and%20empower%20video%20content%0Acreators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10248v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-Video-T2V%2520Technical%2520Report%253A%2520The%2520Practice%252C%2520Challenges%252C%2520and%2520Future%2520of%250A%2520%2520Video%2520Foundation%2520Model%26entry.906535625%3DGuoqing%2520Ma%2520and%2520Haoyang%2520Huang%2520and%2520Kun%2520Yan%2520and%2520Liangyu%2520Chen%2520and%2520Nan%2520Duan%2520and%2520Shengming%2520Yin%2520and%2520Changyi%2520Wan%2520and%2520Ranchen%2520Ming%2520and%2520Xiaoniu%2520Song%2520and%2520Xing%2520Chen%2520and%2520Yu%2520Zhou%2520and%2520Deshan%2520Sun%2520and%2520Deyu%2520Zhou%2520and%2520Jian%2520Zhou%2520and%2520Kaijun%2520Tan%2520and%2520Kang%2520An%2520and%2520Mei%2520Chen%2520and%2520Wei%2520Ji%2520and%2520Qiling%2520Wu%2520and%2520Wen%2520Sun%2520and%2520Xin%2520Han%2520and%2520Yanan%2520Wei%2520and%2520Zheng%2520Ge%2520and%2520Aojie%2520Li%2520and%2520Bin%2520Wang%2520and%2520Bizhu%2520Huang%2520and%2520Bo%2520Wang%2520and%2520Brian%2520Li%2520and%2520Changxing%2520Miao%2520and%2520Chen%2520Xu%2520and%2520Chenfei%2520Wu%2520and%2520Chenguang%2520Yu%2520and%2520Dapeng%2520Shi%2520and%2520Dingyuan%2520Hu%2520and%2520Enle%2520Liu%2520and%2520Gang%2520Yu%2520and%2520Ge%2520Yang%2520and%2520Guanzhe%2520Huang%2520and%2520Gulin%2520Yan%2520and%2520Haiyang%2520Feng%2520and%2520Hao%2520Nie%2520and%2520Haonan%2520Jia%2520and%2520Hanpeng%2520Hu%2520and%2520Hanqi%2520Chen%2520and%2520Haolong%2520Yan%2520and%2520Heng%2520Wang%2520and%2520Hongcheng%2520Guo%2520and%2520Huilin%2520Xiong%2520and%2520Huixin%2520Xiong%2520and%2520Jiahao%2520Gong%2520and%2520Jianchang%2520Wu%2520and%2520Jiaoren%2520Wu%2520and%2520Jie%2520Wu%2520and%2520Jie%2520Yang%2520and%2520Jiashuai%2520Liu%2520and%2520Jiashuo%2520Li%2520and%2520Jingyang%2520Zhang%2520and%2520Junjing%2520Guo%2520and%2520Junzhe%2520Lin%2520and%2520Kaixiang%2520Li%2520and%2520Lei%2520Liu%2520and%2520Lei%2520Xia%2520and%2520Liang%2520Zhao%2520and%2520Liguo%2520Tan%2520and%2520Liwen%2520Huang%2520and%2520Liying%2520Shi%2520and%2520Ming%2520Li%2520and%2520Mingliang%2520Li%2520and%2520Muhua%2520Cheng%2520and%2520Na%2520Wang%2520and%2520Qiaohui%2520Chen%2520and%2520Qinglin%2520He%2520and%2520Qiuyan%2520Liang%2520and%2520Quan%2520Sun%2520and%2520Ran%2520Sun%2520and%2520Rui%2520Wang%2520and%2520Shaoliang%2520Pang%2520and%2520Shiliang%2520Yang%2520and%2520Sitong%2520Liu%2520and%2520Siqi%2520Liu%2520and%2520Shuli%2520Gao%2520and%2520Tiancheng%2520Cao%2520and%2520Tianyu%2520Wang%2520and%2520Weipeng%2520Ming%2520and%2520Wenqing%2520He%2520and%2520Xu%2520Zhao%2520and%2520Xuelin%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Xiaojia%2520Liu%2520and%2520Xuan%2520Yang%2520and%2520Yaqi%2520Dai%2520and%2520Yanbo%2520Yu%2520and%2520Yang%2520Li%2520and%2520Yineng%2520Deng%2520and%2520Yingming%2520Wang%2520and%2520Yilei%2520Wang%2520and%2520Yuanwei%2520Lu%2520and%2520Yu%2520Chen%2520and%2520Yu%2520Luo%2520and%2520Yuchu%2520Luo%2520and%2520Yuhe%2520Yin%2520and%2520Yuheng%2520Feng%2520and%2520Yuxiang%2520Yang%2520and%2520Zecheng%2520Tang%2520and%2520Zekai%2520Zhang%2520and%2520Zidong%2520Yang%2520and%2520Binxing%2520Jiao%2520and%2520Jiansheng%2520Chen%2520and%2520Jing%2520Li%2520and%2520Shuchang%2520Zhou%2520and%2520Xiangyu%2520Zhang%2520and%2520Xinhao%2520Zhang%2520and%2520Yibo%2520Zhu%2520and%2520Heung-Yeung%2520Shum%2520and%2520Daxin%2520Jiang%26entry.1292438233%3D%2520%2520We%2520present%2520Step-Video-T2V%252C%2520a%2520state-of-the-art%2520text-to-video%2520pre-trained%2520model%250Awith%252030B%2520parameters%2520and%2520the%2520ability%2520to%2520generate%2520videos%2520up%2520to%2520204%2520frames%2520in%250Alength.%2520A%2520deep%2520compression%2520Variational%2520Autoencoder%252C%2520Video-VAE%252C%2520is%2520designed%2520for%250Avideo%2520generation%2520tasks%252C%2520achieving%252016x16%2520spatial%2520and%25208x%2520temporal%2520compression%250Aratios%252C%2520while%2520maintaining%2520exceptional%2520video%2520reconstruction%2520quality.%2520User%250Aprompts%2520are%2520encoded%2520using%2520two%2520bilingual%2520text%2520encoders%2520to%2520handle%2520both%2520English%250Aand%2520Chinese.%2520A%2520DiT%2520with%25203D%2520full%2520attention%2520is%2520trained%2520using%2520Flow%2520Matching%2520and%2520is%250Aemployed%2520to%2520denoise%2520input%2520noise%2520into%2520latent%2520frames.%2520A%2520video-based%2520DPO%2520approach%252C%250AVideo-DPO%252C%2520is%2520applied%2520to%2520reduce%2520artifacts%2520and%2520improve%2520the%2520visual%2520quality%2520of%2520the%250Agenerated%2520videos.%2520We%2520also%2520detail%2520our%2520training%2520strategies%2520and%2520share%2520key%250Aobservations%2520and%2520insights.%2520Step-Video-T2V%2527s%2520performance%2520is%2520evaluated%2520on%2520a%2520novel%250Avideo%2520generation%2520benchmark%252C%2520Step-Video-T2V-Eval%252C%2520demonstrating%2520its%250Astate-of-the-art%2520text-to-video%2520quality%2520when%2520compared%2520with%2520both%2520open-source%2520and%250Acommercial%2520engines.%2520Additionally%252C%2520we%2520discuss%2520the%2520limitations%2520of%2520current%250Adiffusion-based%2520model%2520paradigm%2520and%2520outline%2520future%2520directions%2520for%2520video%250Afoundation%2520models.%2520We%2520make%2520both%2520Step-Video-T2V%2520and%2520Step-Video-T2V-Eval%250Aavailable%2520at%2520https%253A//github.com/stepfun-ai/Step-Video-T2V.%2520The%2520online%2520version%250Acan%2520be%2520accessed%2520from%2520https%253A//yuewen.cn/videos%2520as%2520well.%2520Our%2520goal%2520is%2520to%250Aaccelerate%2520the%2520innovation%2520of%2520video%2520foundation%2520models%2520and%2520empower%2520video%2520content%250Acreators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10248v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-Video-T2V%20Technical%20Report%3A%20The%20Practice%2C%20Challenges%2C%20and%20Future%20of%0A%20%20Video%20Foundation%20Model&entry.906535625=Guoqing%20Ma%20and%20Haoyang%20Huang%20and%20Kun%20Yan%20and%20Liangyu%20Chen%20and%20Nan%20Duan%20and%20Shengming%20Yin%20and%20Changyi%20Wan%20and%20Ranchen%20Ming%20and%20Xiaoniu%20Song%20and%20Xing%20Chen%20and%20Yu%20Zhou%20and%20Deshan%20Sun%20and%20Deyu%20Zhou%20and%20Jian%20Zhou%20and%20Kaijun%20Tan%20and%20Kang%20An%20and%20Mei%20Chen%20and%20Wei%20Ji%20and%20Qiling%20Wu%20and%20Wen%20Sun%20and%20Xin%20Han%20and%20Yanan%20Wei%20and%20Zheng%20Ge%20and%20Aojie%20Li%20and%20Bin%20Wang%20and%20Bizhu%20Huang%20and%20Bo%20Wang%20and%20Brian%20Li%20and%20Changxing%20Miao%20and%20Chen%20Xu%20and%20Chenfei%20Wu%20and%20Chenguang%20Yu%20and%20Dapeng%20Shi%20and%20Dingyuan%20Hu%20and%20Enle%20Liu%20and%20Gang%20Yu%20and%20Ge%20Yang%20and%20Guanzhe%20Huang%20and%20Gulin%20Yan%20and%20Haiyang%20Feng%20and%20Hao%20Nie%20and%20Haonan%20Jia%20and%20Hanpeng%20Hu%20and%20Hanqi%20Chen%20and%20Haolong%20Yan%20and%20Heng%20Wang%20and%20Hongcheng%20Guo%20and%20Huilin%20Xiong%20and%20Huixin%20Xiong%20and%20Jiahao%20Gong%20and%20Jianchang%20Wu%20and%20Jiaoren%20Wu%20and%20Jie%20Wu%20and%20Jie%20Yang%20and%20Jiashuai%20Liu%20and%20Jiashuo%20Li%20and%20Jingyang%20Zhang%20and%20Junjing%20Guo%20and%20Junzhe%20Lin%20and%20Kaixiang%20Li%20and%20Lei%20Liu%20and%20Lei%20Xia%20and%20Liang%20Zhao%20and%20Liguo%20Tan%20and%20Liwen%20Huang%20and%20Liying%20Shi%20and%20Ming%20Li%20and%20Mingliang%20Li%20and%20Muhua%20Cheng%20and%20Na%20Wang%20and%20Qiaohui%20Chen%20and%20Qinglin%20He%20and%20Qiuyan%20Liang%20and%20Quan%20Sun%20and%20Ran%20Sun%20and%20Rui%20Wang%20and%20Shaoliang%20Pang%20and%20Shiliang%20Yang%20and%20Sitong%20Liu%20and%20Siqi%20Liu%20and%20Shuli%20Gao%20and%20Tiancheng%20Cao%20and%20Tianyu%20Wang%20and%20Weipeng%20Ming%20and%20Wenqing%20He%20and%20Xu%20Zhao%20and%20Xuelin%20Zhang%20and%20Xianfang%20Zeng%20and%20Xiaojia%20Liu%20and%20Xuan%20Yang%20and%20Yaqi%20Dai%20and%20Yanbo%20Yu%20and%20Yang%20Li%20and%20Yineng%20Deng%20and%20Yingming%20Wang%20and%20Yilei%20Wang%20and%20Yuanwei%20Lu%20and%20Yu%20Chen%20and%20Yu%20Luo%20and%20Yuchu%20Luo%20and%20Yuhe%20Yin%20and%20Yuheng%20Feng%20and%20Yuxiang%20Yang%20and%20Zecheng%20Tang%20and%20Zekai%20Zhang%20and%20Zidong%20Yang%20and%20Binxing%20Jiao%20and%20Jiansheng%20Chen%20and%20Jing%20Li%20and%20Shuchang%20Zhou%20and%20Xiangyu%20Zhang%20and%20Xinhao%20Zhang%20and%20Yibo%20Zhu%20and%20Heung-Yeung%20Shum%20and%20Daxin%20Jiang&entry.1292438233=%20%20We%20present%20Step-Video-T2V%2C%20a%20state-of-the-art%20text-to-video%20pre-trained%20model%0Awith%2030B%20parameters%20and%20the%20ability%20to%20generate%20videos%20up%20to%20204%20frames%20in%0Alength.%20A%20deep%20compression%20Variational%20Autoencoder%2C%20Video-VAE%2C%20is%20designed%20for%0Avideo%20generation%20tasks%2C%20achieving%2016x16%20spatial%20and%208x%20temporal%20compression%0Aratios%2C%20while%20maintaining%20exceptional%20video%20reconstruction%20quality.%20User%0Aprompts%20are%20encoded%20using%20two%20bilingual%20text%20encoders%20to%20handle%20both%20English%0Aand%20Chinese.%20A%20DiT%20with%203D%20full%20attention%20is%20trained%20using%20Flow%20Matching%20and%20is%0Aemployed%20to%20denoise%20input%20noise%20into%20latent%20frames.%20A%20video-based%20DPO%20approach%2C%0AVideo-DPO%2C%20is%20applied%20to%20reduce%20artifacts%20and%20improve%20the%20visual%20quality%20of%20the%0Agenerated%20videos.%20We%20also%20detail%20our%20training%20strategies%20and%20share%20key%0Aobservations%20and%20insights.%20Step-Video-T2V%27s%20performance%20is%20evaluated%20on%20a%20novel%0Avideo%20generation%20benchmark%2C%20Step-Video-T2V-Eval%2C%20demonstrating%20its%0Astate-of-the-art%20text-to-video%20quality%20when%20compared%20with%20both%20open-source%20and%0Acommercial%20engines.%20Additionally%2C%20we%20discuss%20the%20limitations%20of%20current%0Adiffusion-based%20model%20paradigm%20and%20outline%20future%20directions%20for%20video%0Afoundation%20models.%20We%20make%20both%20Step-Video-T2V%20and%20Step-Video-T2V-Eval%0Aavailable%20at%20https%3A//github.com/stepfun-ai/Step-Video-T2V.%20The%20online%20version%0Acan%20be%20accessed%20from%20https%3A//yuewen.cn/videos%20as%20well.%20Our%20goal%20is%20to%0Aaccelerate%20the%20innovation%20of%20video%20foundation%20models%20and%20empower%20video%20content%0Acreators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10248v3&entry.124074799=Read"},
{"title": "DIS-CO: Discovering Copyrighted Content in VLMs Training Data", "author": "Andr\u00e9 V. Duarte and Xuandong Zhao and Arlindo L. Oliveira and Lei Li", "abstract": "  How can we verify whether copyrighted content was used to train a large\nvision-language model (VLM) without direct access to its training data?\nMotivated by the hypothesis that a VLM is able to recognize images from its\ntraining corpus, we propose DIS-CO, a novel approach to infer the inclusion of\ncopyrighted content during the model's development. By repeatedly querying a\nVLM with specific frames from targeted copyrighted material, DIS-CO extracts\nthe content's identity through free-form text completions. To assess its\neffectiveness, we introduce MovieTection, a benchmark comprising 14,000 frames\npaired with detailed captions, drawn from films released both before and after\na model's training cutoff. Our results show that DIS-CO significantly improves\ndetection performance, nearly doubling the average AUC of the best prior method\non models with logits available. Our findings also highlight a broader concern:\nall tested models appear to have been exposed to some extent to copyrighted\ncontent. Our code and data are available at\nhttps://github.com/avduarte333/DIS-CO\n", "link": "http://arxiv.org/abs/2502.17358v1", "date": "2025-02-24", "relevancy": 2.5817, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIS-CO%3A%20Discovering%20Copyrighted%20Content%20in%20VLMs%20Training%20Data&body=Title%3A%20DIS-CO%3A%20Discovering%20Copyrighted%20Content%20in%20VLMs%20Training%20Data%0AAuthor%3A%20Andr%C3%A9%20V.%20Duarte%20and%20Xuandong%20Zhao%20and%20Arlindo%20L.%20Oliveira%20and%20Lei%20Li%0AAbstract%3A%20%20%20How%20can%20we%20verify%20whether%20copyrighted%20content%20was%20used%20to%20train%20a%20large%0Avision-language%20model%20%28VLM%29%20without%20direct%20access%20to%20its%20training%20data%3F%0AMotivated%20by%20the%20hypothesis%20that%20a%20VLM%20is%20able%20to%20recognize%20images%20from%20its%0Atraining%20corpus%2C%20we%20propose%20DIS-CO%2C%20a%20novel%20approach%20to%20infer%20the%20inclusion%20of%0Acopyrighted%20content%20during%20the%20model%27s%20development.%20By%20repeatedly%20querying%20a%0AVLM%20with%20specific%20frames%20from%20targeted%20copyrighted%20material%2C%20DIS-CO%20extracts%0Athe%20content%27s%20identity%20through%20free-form%20text%20completions.%20To%20assess%20its%0Aeffectiveness%2C%20we%20introduce%20MovieTection%2C%20a%20benchmark%20comprising%2014%2C000%20frames%0Apaired%20with%20detailed%20captions%2C%20drawn%20from%20films%20released%20both%20before%20and%20after%0Aa%20model%27s%20training%20cutoff.%20Our%20results%20show%20that%20DIS-CO%20significantly%20improves%0Adetection%20performance%2C%20nearly%20doubling%20the%20average%20AUC%20of%20the%20best%20prior%20method%0Aon%20models%20with%20logits%20available.%20Our%20findings%20also%20highlight%20a%20broader%20concern%3A%0Aall%20tested%20models%20appear%20to%20have%20been%20exposed%20to%20some%20extent%20to%20copyrighted%0Acontent.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/avduarte333/DIS-CO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIS-CO%253A%2520Discovering%2520Copyrighted%2520Content%2520in%2520VLMs%2520Training%2520Data%26entry.906535625%3DAndr%25C3%25A9%2520V.%2520Duarte%2520and%2520Xuandong%2520Zhao%2520and%2520Arlindo%2520L.%2520Oliveira%2520and%2520Lei%2520Li%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520verify%2520whether%2520copyrighted%2520content%2520was%2520used%2520to%2520train%2520a%2520large%250Avision-language%2520model%2520%2528VLM%2529%2520without%2520direct%2520access%2520to%2520its%2520training%2520data%253F%250AMotivated%2520by%2520the%2520hypothesis%2520that%2520a%2520VLM%2520is%2520able%2520to%2520recognize%2520images%2520from%2520its%250Atraining%2520corpus%252C%2520we%2520propose%2520DIS-CO%252C%2520a%2520novel%2520approach%2520to%2520infer%2520the%2520inclusion%2520of%250Acopyrighted%2520content%2520during%2520the%2520model%2527s%2520development.%2520By%2520repeatedly%2520querying%2520a%250AVLM%2520with%2520specific%2520frames%2520from%2520targeted%2520copyrighted%2520material%252C%2520DIS-CO%2520extracts%250Athe%2520content%2527s%2520identity%2520through%2520free-form%2520text%2520completions.%2520To%2520assess%2520its%250Aeffectiveness%252C%2520we%2520introduce%2520MovieTection%252C%2520a%2520benchmark%2520comprising%252014%252C000%2520frames%250Apaired%2520with%2520detailed%2520captions%252C%2520drawn%2520from%2520films%2520released%2520both%2520before%2520and%2520after%250Aa%2520model%2527s%2520training%2520cutoff.%2520Our%2520results%2520show%2520that%2520DIS-CO%2520significantly%2520improves%250Adetection%2520performance%252C%2520nearly%2520doubling%2520the%2520average%2520AUC%2520of%2520the%2520best%2520prior%2520method%250Aon%2520models%2520with%2520logits%2520available.%2520Our%2520findings%2520also%2520highlight%2520a%2520broader%2520concern%253A%250Aall%2520tested%2520models%2520appear%2520to%2520have%2520been%2520exposed%2520to%2520some%2520extent%2520to%2520copyrighted%250Acontent.%2520Our%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/avduarte333/DIS-CO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIS-CO%3A%20Discovering%20Copyrighted%20Content%20in%20VLMs%20Training%20Data&entry.906535625=Andr%C3%A9%20V.%20Duarte%20and%20Xuandong%20Zhao%20and%20Arlindo%20L.%20Oliveira%20and%20Lei%20Li&entry.1292438233=%20%20How%20can%20we%20verify%20whether%20copyrighted%20content%20was%20used%20to%20train%20a%20large%0Avision-language%20model%20%28VLM%29%20without%20direct%20access%20to%20its%20training%20data%3F%0AMotivated%20by%20the%20hypothesis%20that%20a%20VLM%20is%20able%20to%20recognize%20images%20from%20its%0Atraining%20corpus%2C%20we%20propose%20DIS-CO%2C%20a%20novel%20approach%20to%20infer%20the%20inclusion%20of%0Acopyrighted%20content%20during%20the%20model%27s%20development.%20By%20repeatedly%20querying%20a%0AVLM%20with%20specific%20frames%20from%20targeted%20copyrighted%20material%2C%20DIS-CO%20extracts%0Athe%20content%27s%20identity%20through%20free-form%20text%20completions.%20To%20assess%20its%0Aeffectiveness%2C%20we%20introduce%20MovieTection%2C%20a%20benchmark%20comprising%2014%2C000%20frames%0Apaired%20with%20detailed%20captions%2C%20drawn%20from%20films%20released%20both%20before%20and%20after%0Aa%20model%27s%20training%20cutoff.%20Our%20results%20show%20that%20DIS-CO%20significantly%20improves%0Adetection%20performance%2C%20nearly%20doubling%20the%20average%20AUC%20of%20the%20best%20prior%20method%0Aon%20models%20with%20logits%20available.%20Our%20findings%20also%20highlight%20a%20broader%20concern%3A%0Aall%20tested%20models%20appear%20to%20have%20been%20exposed%20to%20some%20extent%20to%20copyrighted%0Acontent.%20Our%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/avduarte333/DIS-CO%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17358v1&entry.124074799=Read"},
{"title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference", "author": "Yaohua Tang and Zhicheng Hu and Kun Cheng and Fan Mo and Qiheng Lv and Hua Wang and Zhi Chen", "abstract": "  The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.\n", "link": "http://arxiv.org/abs/2502.15294v2", "date": "2025-02-24", "relevancy": 2.5788, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Round%20Attention%3A%20A%20Novel%20Round-Level%20Attention%20Mechanism%20to%20Accelerate%0A%20%20LLM%20Inference&body=Title%3A%20Round%20Attention%3A%20A%20Novel%20Round-Level%20Attention%20Mechanism%20to%20Accelerate%0A%20%20LLM%20Inference%0AAuthor%3A%20Yaohua%20Tang%20and%20Zhicheng%20Hu%20and%20Kun%20Cheng%20and%20Fan%20Mo%20and%20Qiheng%20Lv%20and%20Hua%20Wang%20and%20Zhi%20Chen%0AAbstract%3A%20%20%20The%20increasing%20context%20window%20size%20in%20large%20language%20models%20%28LLMs%29%20has%0Aimproved%20their%20ability%20to%20handle%20complex%2C%20long-text%20tasks.%20However%2C%20as%20the%0Aconversation%20rounds%20continue%2C%20it%20is%20required%20to%20store%20a%20large%20amount%20of%20KV%0Acache%20in%20GPU%20memory%2C%20which%20significantly%20affects%20the%20efficiency%20and%20even%0Aavailability%20of%20the%20model%20serving%20systems.%20This%20paper%20analyzes%20dialogue%20data%0Afrom%20real%20users%20and%20discovers%20that%20the%20LLM%20inference%20manifests%20a%20watershed%0Alayer%2C%20after%20which%20the%20distribution%20of%20round-level%20attention%20shows%20notable%0Asimilarity.%20We%20propose%20Round%20Attention%2C%20a%20novel%20round-level%20attention%20mechanism%0Athat%20only%20recalls%20and%20computes%20the%20KV%20cache%20of%20the%20most%20relevant%20rounds.%20The%0Aexperiments%20show%20that%20our%20method%20saves%2055%5C%25%20memory%20usage%20without%20compromising%0Amodel%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRound%2520Attention%253A%2520A%2520Novel%2520Round-Level%2520Attention%2520Mechanism%2520to%2520Accelerate%250A%2520%2520LLM%2520Inference%26entry.906535625%3DYaohua%2520Tang%2520and%2520Zhicheng%2520Hu%2520and%2520Kun%2520Cheng%2520and%2520Fan%2520Mo%2520and%2520Qiheng%2520Lv%2520and%2520Hua%2520Wang%2520and%2520Zhi%2520Chen%26entry.1292438233%3D%2520%2520The%2520increasing%2520context%2520window%2520size%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%250Aimproved%2520their%2520ability%2520to%2520handle%2520complex%252C%2520long-text%2520tasks.%2520However%252C%2520as%2520the%250Aconversation%2520rounds%2520continue%252C%2520it%2520is%2520required%2520to%2520store%2520a%2520large%2520amount%2520of%2520KV%250Acache%2520in%2520GPU%2520memory%252C%2520which%2520significantly%2520affects%2520the%2520efficiency%2520and%2520even%250Aavailability%2520of%2520the%2520model%2520serving%2520systems.%2520This%2520paper%2520analyzes%2520dialogue%2520data%250Afrom%2520real%2520users%2520and%2520discovers%2520that%2520the%2520LLM%2520inference%2520manifests%2520a%2520watershed%250Alayer%252C%2520after%2520which%2520the%2520distribution%2520of%2520round-level%2520attention%2520shows%2520notable%250Asimilarity.%2520We%2520propose%2520Round%2520Attention%252C%2520a%2520novel%2520round-level%2520attention%2520mechanism%250Athat%2520only%2520recalls%2520and%2520computes%2520the%2520KV%2520cache%2520of%2520the%2520most%2520relevant%2520rounds.%2520The%250Aexperiments%2520show%2520that%2520our%2520method%2520saves%252055%255C%2525%2520memory%2520usage%2520without%2520compromising%250Amodel%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Round%20Attention%3A%20A%20Novel%20Round-Level%20Attention%20Mechanism%20to%20Accelerate%0A%20%20LLM%20Inference&entry.906535625=Yaohua%20Tang%20and%20Zhicheng%20Hu%20and%20Kun%20Cheng%20and%20Fan%20Mo%20and%20Qiheng%20Lv%20and%20Hua%20Wang%20and%20Zhi%20Chen&entry.1292438233=%20%20The%20increasing%20context%20window%20size%20in%20large%20language%20models%20%28LLMs%29%20has%0Aimproved%20their%20ability%20to%20handle%20complex%2C%20long-text%20tasks.%20However%2C%20as%20the%0Aconversation%20rounds%20continue%2C%20it%20is%20required%20to%20store%20a%20large%20amount%20of%20KV%0Acache%20in%20GPU%20memory%2C%20which%20significantly%20affects%20the%20efficiency%20and%20even%0Aavailability%20of%20the%20model%20serving%20systems.%20This%20paper%20analyzes%20dialogue%20data%0Afrom%20real%20users%20and%20discovers%20that%20the%20LLM%20inference%20manifests%20a%20watershed%0Alayer%2C%20after%20which%20the%20distribution%20of%20round-level%20attention%20shows%20notable%0Asimilarity.%20We%20propose%20Round%20Attention%2C%20a%20novel%20round-level%20attention%20mechanism%0Athat%20only%20recalls%20and%20computes%20the%20KV%20cache%20of%20the%20most%20relevant%20rounds.%20The%0Aexperiments%20show%20that%20our%20method%20saves%2055%5C%25%20memory%20usage%20without%20compromising%0Amodel%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15294v2&entry.124074799=Read"},
{"title": "Tokenized SAEs: Disentangling SAE Reconstructions", "author": "Thomas Dooms and Daniel Wilhelm", "abstract": "  Sparse auto-encoders (SAEs) have become a prevalent tool for interpreting\nlanguage models' inner workings. However, it is unknown how tightly SAE\nfeatures correspond to computationally important directions in the model. This\nwork empirically shows that many RES-JB SAE features predominantly correspond\nto simple input statistics. We hypothesize this is caused by a large class\nimbalance in training data combined with a lack of complex error signals. To\nreduce this behavior, we propose a method that disentangles token\nreconstruction from feature reconstruction. This improvement is achieved by\nintroducing a per-token bias, which provides an enhanced baseline for\ninteresting reconstruction. As a result, significantly more interesting\nfeatures and improved reconstruction in sparse regimes are learned.\n", "link": "http://arxiv.org/abs/2502.17332v1", "date": "2025-02-24", "relevancy": 2.5616, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5218}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5108}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenized%20SAEs%3A%20Disentangling%20SAE%20Reconstructions&body=Title%3A%20Tokenized%20SAEs%3A%20Disentangling%20SAE%20Reconstructions%0AAuthor%3A%20Thomas%20Dooms%20and%20Daniel%20Wilhelm%0AAbstract%3A%20%20%20Sparse%20auto-encoders%20%28SAEs%29%20have%20become%20a%20prevalent%20tool%20for%20interpreting%0Alanguage%20models%27%20inner%20workings.%20However%2C%20it%20is%20unknown%20how%20tightly%20SAE%0Afeatures%20correspond%20to%20computationally%20important%20directions%20in%20the%20model.%20This%0Awork%20empirically%20shows%20that%20many%20RES-JB%20SAE%20features%20predominantly%20correspond%0Ato%20simple%20input%20statistics.%20We%20hypothesize%20this%20is%20caused%20by%20a%20large%20class%0Aimbalance%20in%20training%20data%20combined%20with%20a%20lack%20of%20complex%20error%20signals.%20To%0Areduce%20this%20behavior%2C%20we%20propose%20a%20method%20that%20disentangles%20token%0Areconstruction%20from%20feature%20reconstruction.%20This%20improvement%20is%20achieved%20by%0Aintroducing%20a%20per-token%20bias%2C%20which%20provides%20an%20enhanced%20baseline%20for%0Ainteresting%20reconstruction.%20As%20a%20result%2C%20significantly%20more%20interesting%0Afeatures%20and%20improved%20reconstruction%20in%20sparse%20regimes%20are%20learned.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenized%2520SAEs%253A%2520Disentangling%2520SAE%2520Reconstructions%26entry.906535625%3DThomas%2520Dooms%2520and%2520Daniel%2520Wilhelm%26entry.1292438233%3D%2520%2520Sparse%2520auto-encoders%2520%2528SAEs%2529%2520have%2520become%2520a%2520prevalent%2520tool%2520for%2520interpreting%250Alanguage%2520models%2527%2520inner%2520workings.%2520However%252C%2520it%2520is%2520unknown%2520how%2520tightly%2520SAE%250Afeatures%2520correspond%2520to%2520computationally%2520important%2520directions%2520in%2520the%2520model.%2520This%250Awork%2520empirically%2520shows%2520that%2520many%2520RES-JB%2520SAE%2520features%2520predominantly%2520correspond%250Ato%2520simple%2520input%2520statistics.%2520We%2520hypothesize%2520this%2520is%2520caused%2520by%2520a%2520large%2520class%250Aimbalance%2520in%2520training%2520data%2520combined%2520with%2520a%2520lack%2520of%2520complex%2520error%2520signals.%2520To%250Areduce%2520this%2520behavior%252C%2520we%2520propose%2520a%2520method%2520that%2520disentangles%2520token%250Areconstruction%2520from%2520feature%2520reconstruction.%2520This%2520improvement%2520is%2520achieved%2520by%250Aintroducing%2520a%2520per-token%2520bias%252C%2520which%2520provides%2520an%2520enhanced%2520baseline%2520for%250Ainteresting%2520reconstruction.%2520As%2520a%2520result%252C%2520significantly%2520more%2520interesting%250Afeatures%2520and%2520improved%2520reconstruction%2520in%2520sparse%2520regimes%2520are%2520learned.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenized%20SAEs%3A%20Disentangling%20SAE%20Reconstructions&entry.906535625=Thomas%20Dooms%20and%20Daniel%20Wilhelm&entry.1292438233=%20%20Sparse%20auto-encoders%20%28SAEs%29%20have%20become%20a%20prevalent%20tool%20for%20interpreting%0Alanguage%20models%27%20inner%20workings.%20However%2C%20it%20is%20unknown%20how%20tightly%20SAE%0Afeatures%20correspond%20to%20computationally%20important%20directions%20in%20the%20model.%20This%0Awork%20empirically%20shows%20that%20many%20RES-JB%20SAE%20features%20predominantly%20correspond%0Ato%20simple%20input%20statistics.%20We%20hypothesize%20this%20is%20caused%20by%20a%20large%20class%0Aimbalance%20in%20training%20data%20combined%20with%20a%20lack%20of%20complex%20error%20signals.%20To%0Areduce%20this%20behavior%2C%20we%20propose%20a%20method%20that%20disentangles%20token%0Areconstruction%20from%20feature%20reconstruction.%20This%20improvement%20is%20achieved%20by%0Aintroducing%20a%20per-token%20bias%2C%20which%20provides%20an%20enhanced%20baseline%20for%0Ainteresting%20reconstruction.%20As%20a%20result%2C%20significantly%20more%20interesting%0Afeatures%20and%20improved%20reconstruction%20in%20sparse%20regimes%20are%20learned.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17332v1&entry.124074799=Read"},
{"title": "Fast and Effective GNN Training through Sequences of Random Path Graphs", "author": "Francesco Bonchi and Claudio Gentile and Francesco Paolo Nerini and Andr\u00e9 Panisson and Fabio Vitale", "abstract": "  We present GERN, a novel scalable framework for training GNNs in node\nclassification tasks, based on effective resistance, a standard tool in\nspectral graph theory. Our method progressively refines the GNN weights on a\nsequence of random spanning trees suitably transformed into path graphs which,\ndespite their simplicity, are shown to retain essential topological and node\ninformation of the original input graph. The sparse nature of these path graphs\nsubstantially lightens the computational burden of GNN training. This not only\nenhances scalability but also improves accuracy in subsequent test phases,\nespecially under small training set regimes, which are of great practical\nimportance, as in many real-world scenarios labels may be hard to obtain. In\nthese settings, our framework yields very good results as it effectively\ncounters the training deterioration caused by overfitting when the training set\nis small. Our method also addresses common issues like over-squashing and\nover-smoothing while avoiding under-reaching phenomena.\n  Although our framework is flexible and can be deployed in several types of\nGNNs, in this paper we focus on graph convolutional networks and carry out an\nextensive experimental investigation on a number of real-world graph\nbenchmarks, where we achieve simultaneous improvement of training speed and\ntest accuracy over a wide pool of representative baselines.\n", "link": "http://arxiv.org/abs/2306.04828v4", "date": "2025-02-24", "relevancy": 2.5542, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5362}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5044}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20Effective%20GNN%20Training%20through%20Sequences%20of%20Random%20Path%20Graphs&body=Title%3A%20Fast%20and%20Effective%20GNN%20Training%20through%20Sequences%20of%20Random%20Path%20Graphs%0AAuthor%3A%20Francesco%20Bonchi%20and%20Claudio%20Gentile%20and%20Francesco%20Paolo%20Nerini%20and%20Andr%C3%A9%20Panisson%20and%20Fabio%20Vitale%0AAbstract%3A%20%20%20We%20present%20GERN%2C%20a%20novel%20scalable%20framework%20for%20training%20GNNs%20in%20node%0Aclassification%20tasks%2C%20based%20on%20effective%20resistance%2C%20a%20standard%20tool%20in%0Aspectral%20graph%20theory.%20Our%20method%20progressively%20refines%20the%20GNN%20weights%20on%20a%0Asequence%20of%20random%20spanning%20trees%20suitably%20transformed%20into%20path%20graphs%20which%2C%0Adespite%20their%20simplicity%2C%20are%20shown%20to%20retain%20essential%20topological%20and%20node%0Ainformation%20of%20the%20original%20input%20graph.%20The%20sparse%20nature%20of%20these%20path%20graphs%0Asubstantially%20lightens%20the%20computational%20burden%20of%20GNN%20training.%20This%20not%20only%0Aenhances%20scalability%20but%20also%20improves%20accuracy%20in%20subsequent%20test%20phases%2C%0Aespecially%20under%20small%20training%20set%20regimes%2C%20which%20are%20of%20great%20practical%0Aimportance%2C%20as%20in%20many%20real-world%20scenarios%20labels%20may%20be%20hard%20to%20obtain.%20In%0Athese%20settings%2C%20our%20framework%20yields%20very%20good%20results%20as%20it%20effectively%0Acounters%20the%20training%20deterioration%20caused%20by%20overfitting%20when%20the%20training%20set%0Ais%20small.%20Our%20method%20also%20addresses%20common%20issues%20like%20over-squashing%20and%0Aover-smoothing%20while%20avoiding%20under-reaching%20phenomena.%0A%20%20Although%20our%20framework%20is%20flexible%20and%20can%20be%20deployed%20in%20several%20types%20of%0AGNNs%2C%20in%20this%20paper%20we%20focus%20on%20graph%20convolutional%20networks%20and%20carry%20out%20an%0Aextensive%20experimental%20investigation%20on%20a%20number%20of%20real-world%20graph%0Abenchmarks%2C%20where%20we%20achieve%20simultaneous%20improvement%20of%20training%20speed%20and%0Atest%20accuracy%20over%20a%20wide%20pool%20of%20representative%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.04828v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520Effective%2520GNN%2520Training%2520through%2520Sequences%2520of%2520Random%2520Path%2520Graphs%26entry.906535625%3DFrancesco%2520Bonchi%2520and%2520Claudio%2520Gentile%2520and%2520Francesco%2520Paolo%2520Nerini%2520and%2520Andr%25C3%25A9%2520Panisson%2520and%2520Fabio%2520Vitale%26entry.1292438233%3D%2520%2520We%2520present%2520GERN%252C%2520a%2520novel%2520scalable%2520framework%2520for%2520training%2520GNNs%2520in%2520node%250Aclassification%2520tasks%252C%2520based%2520on%2520effective%2520resistance%252C%2520a%2520standard%2520tool%2520in%250Aspectral%2520graph%2520theory.%2520Our%2520method%2520progressively%2520refines%2520the%2520GNN%2520weights%2520on%2520a%250Asequence%2520of%2520random%2520spanning%2520trees%2520suitably%2520transformed%2520into%2520path%2520graphs%2520which%252C%250Adespite%2520their%2520simplicity%252C%2520are%2520shown%2520to%2520retain%2520essential%2520topological%2520and%2520node%250Ainformation%2520of%2520the%2520original%2520input%2520graph.%2520The%2520sparse%2520nature%2520of%2520these%2520path%2520graphs%250Asubstantially%2520lightens%2520the%2520computational%2520burden%2520of%2520GNN%2520training.%2520This%2520not%2520only%250Aenhances%2520scalability%2520but%2520also%2520improves%2520accuracy%2520in%2520subsequent%2520test%2520phases%252C%250Aespecially%2520under%2520small%2520training%2520set%2520regimes%252C%2520which%2520are%2520of%2520great%2520practical%250Aimportance%252C%2520as%2520in%2520many%2520real-world%2520scenarios%2520labels%2520may%2520be%2520hard%2520to%2520obtain.%2520In%250Athese%2520settings%252C%2520our%2520framework%2520yields%2520very%2520good%2520results%2520as%2520it%2520effectively%250Acounters%2520the%2520training%2520deterioration%2520caused%2520by%2520overfitting%2520when%2520the%2520training%2520set%250Ais%2520small.%2520Our%2520method%2520also%2520addresses%2520common%2520issues%2520like%2520over-squashing%2520and%250Aover-smoothing%2520while%2520avoiding%2520under-reaching%2520phenomena.%250A%2520%2520Although%2520our%2520framework%2520is%2520flexible%2520and%2520can%2520be%2520deployed%2520in%2520several%2520types%2520of%250AGNNs%252C%2520in%2520this%2520paper%2520we%2520focus%2520on%2520graph%2520convolutional%2520networks%2520and%2520carry%2520out%2520an%250Aextensive%2520experimental%2520investigation%2520on%2520a%2520number%2520of%2520real-world%2520graph%250Abenchmarks%252C%2520where%2520we%2520achieve%2520simultaneous%2520improvement%2520of%2520training%2520speed%2520and%250Atest%2520accuracy%2520over%2520a%2520wide%2520pool%2520of%2520representative%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.04828v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20Effective%20GNN%20Training%20through%20Sequences%20of%20Random%20Path%20Graphs&entry.906535625=Francesco%20Bonchi%20and%20Claudio%20Gentile%20and%20Francesco%20Paolo%20Nerini%20and%20Andr%C3%A9%20Panisson%20and%20Fabio%20Vitale&entry.1292438233=%20%20We%20present%20GERN%2C%20a%20novel%20scalable%20framework%20for%20training%20GNNs%20in%20node%0Aclassification%20tasks%2C%20based%20on%20effective%20resistance%2C%20a%20standard%20tool%20in%0Aspectral%20graph%20theory.%20Our%20method%20progressively%20refines%20the%20GNN%20weights%20on%20a%0Asequence%20of%20random%20spanning%20trees%20suitably%20transformed%20into%20path%20graphs%20which%2C%0Adespite%20their%20simplicity%2C%20are%20shown%20to%20retain%20essential%20topological%20and%20node%0Ainformation%20of%20the%20original%20input%20graph.%20The%20sparse%20nature%20of%20these%20path%20graphs%0Asubstantially%20lightens%20the%20computational%20burden%20of%20GNN%20training.%20This%20not%20only%0Aenhances%20scalability%20but%20also%20improves%20accuracy%20in%20subsequent%20test%20phases%2C%0Aespecially%20under%20small%20training%20set%20regimes%2C%20which%20are%20of%20great%20practical%0Aimportance%2C%20as%20in%20many%20real-world%20scenarios%20labels%20may%20be%20hard%20to%20obtain.%20In%0Athese%20settings%2C%20our%20framework%20yields%20very%20good%20results%20as%20it%20effectively%0Acounters%20the%20training%20deterioration%20caused%20by%20overfitting%20when%20the%20training%20set%0Ais%20small.%20Our%20method%20also%20addresses%20common%20issues%20like%20over-squashing%20and%0Aover-smoothing%20while%20avoiding%20under-reaching%20phenomena.%0A%20%20Although%20our%20framework%20is%20flexible%20and%20can%20be%20deployed%20in%20several%20types%20of%0AGNNs%2C%20in%20this%20paper%20we%20focus%20on%20graph%20convolutional%20networks%20and%20carry%20out%20an%0Aextensive%20experimental%20investigation%20on%20a%20number%20of%20real-world%20graph%0Abenchmarks%2C%20where%20we%20achieve%20simultaneous%20improvement%20of%20training%20speed%20and%0Atest%20accuracy%20over%20a%20wide%20pool%20of%20representative%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.04828v4&entry.124074799=Read"},
{"title": "A Survey of Geometric Graph Neural Networks: Data Structures, Models and\n  Applications", "author": "Jiaqi Han and Jiacheng Cen and Liming Wu and Zongzhao Li and Xiangzhe Kong and Rui Jiao and Ziyang Yu and Tingyang Xu and Fandi Wu and Zihe Wang and Hongteng Xu and Zhewei Wei and Deli Zhao and Yang Liu and Yu Rong and Wenbing Huang", "abstract": "  Geometric graphs are a special kind of graph with geometric features, which\nare vital to model many scientific problems. Unlike generic graphs, geometric\ngraphs often exhibit physical symmetries of translations, rotations, and\nreflections, making them ineffectively processed by current Graph Neural\nNetworks (GNNs). To address this issue, researchers proposed a variety of\ngeometric GNNs equipped with invariant/equivariant properties to better\ncharacterize the geometry and topology of geometric graphs. Given the current\nprogress in this field, it is imperative to conduct a comprehensive survey of\ndata structures, models, and applications related to geometric GNNs. In this\npaper, based on the necessary but concise mathematical preliminaries, we\nformalize geometric graph as the data structure, on top of which we provide a\nunified view of existing models from the geometric message passing perspective.\nAdditionally, we summarize the applications as well as the related datasets to\nfacilitate later research for methodology development and experimental\nevaluation. We also discuss the challenges and future potential directions of\ngeometric GNNs at the end of this survey.\n", "link": "http://arxiv.org/abs/2403.00485v2", "date": "2025-02-24", "relevancy": 2.5452, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5261}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5044}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Geometric%20Graph%20Neural%20Networks%3A%20Data%20Structures%2C%20Models%20and%0A%20%20Applications&body=Title%3A%20A%20Survey%20of%20Geometric%20Graph%20Neural%20Networks%3A%20Data%20Structures%2C%20Models%20and%0A%20%20Applications%0AAuthor%3A%20Jiaqi%20Han%20and%20Jiacheng%20Cen%20and%20Liming%20Wu%20and%20Zongzhao%20Li%20and%20Xiangzhe%20Kong%20and%20Rui%20Jiao%20and%20Ziyang%20Yu%20and%20Tingyang%20Xu%20and%20Fandi%20Wu%20and%20Zihe%20Wang%20and%20Hongteng%20Xu%20and%20Zhewei%20Wei%20and%20Deli%20Zhao%20and%20Yang%20Liu%20and%20Yu%20Rong%20and%20Wenbing%20Huang%0AAbstract%3A%20%20%20Geometric%20graphs%20are%20a%20special%20kind%20of%20graph%20with%20geometric%20features%2C%20which%0Aare%20vital%20to%20model%20many%20scientific%20problems.%20Unlike%20generic%20graphs%2C%20geometric%0Agraphs%20often%20exhibit%20physical%20symmetries%20of%20translations%2C%20rotations%2C%20and%0Areflections%2C%20making%20them%20ineffectively%20processed%20by%20current%20Graph%20Neural%0ANetworks%20%28GNNs%29.%20To%20address%20this%20issue%2C%20researchers%20proposed%20a%20variety%20of%0Ageometric%20GNNs%20equipped%20with%20invariant/equivariant%20properties%20to%20better%0Acharacterize%20the%20geometry%20and%20topology%20of%20geometric%20graphs.%20Given%20the%20current%0Aprogress%20in%20this%20field%2C%20it%20is%20imperative%20to%20conduct%20a%20comprehensive%20survey%20of%0Adata%20structures%2C%20models%2C%20and%20applications%20related%20to%20geometric%20GNNs.%20In%20this%0Apaper%2C%20based%20on%20the%20necessary%20but%20concise%20mathematical%20preliminaries%2C%20we%0Aformalize%20geometric%20graph%20as%20the%20data%20structure%2C%20on%20top%20of%20which%20we%20provide%20a%0Aunified%20view%20of%20existing%20models%20from%20the%20geometric%20message%20passing%20perspective.%0AAdditionally%2C%20we%20summarize%20the%20applications%20as%20well%20as%20the%20related%20datasets%20to%0Afacilitate%20later%20research%20for%20methodology%20development%20and%20experimental%0Aevaluation.%20We%20also%20discuss%20the%20challenges%20and%20future%20potential%20directions%20of%0Ageometric%20GNNs%20at%20the%20end%20of%20this%20survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Geometric%2520Graph%2520Neural%2520Networks%253A%2520Data%2520Structures%252C%2520Models%2520and%250A%2520%2520Applications%26entry.906535625%3DJiaqi%2520Han%2520and%2520Jiacheng%2520Cen%2520and%2520Liming%2520Wu%2520and%2520Zongzhao%2520Li%2520and%2520Xiangzhe%2520Kong%2520and%2520Rui%2520Jiao%2520and%2520Ziyang%2520Yu%2520and%2520Tingyang%2520Xu%2520and%2520Fandi%2520Wu%2520and%2520Zihe%2520Wang%2520and%2520Hongteng%2520Xu%2520and%2520Zhewei%2520Wei%2520and%2520Deli%2520Zhao%2520and%2520Yang%2520Liu%2520and%2520Yu%2520Rong%2520and%2520Wenbing%2520Huang%26entry.1292438233%3D%2520%2520Geometric%2520graphs%2520are%2520a%2520special%2520kind%2520of%2520graph%2520with%2520geometric%2520features%252C%2520which%250Aare%2520vital%2520to%2520model%2520many%2520scientific%2520problems.%2520Unlike%2520generic%2520graphs%252C%2520geometric%250Agraphs%2520often%2520exhibit%2520physical%2520symmetries%2520of%2520translations%252C%2520rotations%252C%2520and%250Areflections%252C%2520making%2520them%2520ineffectively%2520processed%2520by%2520current%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529.%2520To%2520address%2520this%2520issue%252C%2520researchers%2520proposed%2520a%2520variety%2520of%250Ageometric%2520GNNs%2520equipped%2520with%2520invariant/equivariant%2520properties%2520to%2520better%250Acharacterize%2520the%2520geometry%2520and%2520topology%2520of%2520geometric%2520graphs.%2520Given%2520the%2520current%250Aprogress%2520in%2520this%2520field%252C%2520it%2520is%2520imperative%2520to%2520conduct%2520a%2520comprehensive%2520survey%2520of%250Adata%2520structures%252C%2520models%252C%2520and%2520applications%2520related%2520to%2520geometric%2520GNNs.%2520In%2520this%250Apaper%252C%2520based%2520on%2520the%2520necessary%2520but%2520concise%2520mathematical%2520preliminaries%252C%2520we%250Aformalize%2520geometric%2520graph%2520as%2520the%2520data%2520structure%252C%2520on%2520top%2520of%2520which%2520we%2520provide%2520a%250Aunified%2520view%2520of%2520existing%2520models%2520from%2520the%2520geometric%2520message%2520passing%2520perspective.%250AAdditionally%252C%2520we%2520summarize%2520the%2520applications%2520as%2520well%2520as%2520the%2520related%2520datasets%2520to%250Afacilitate%2520later%2520research%2520for%2520methodology%2520development%2520and%2520experimental%250Aevaluation.%2520We%2520also%2520discuss%2520the%2520challenges%2520and%2520future%2520potential%2520directions%2520of%250Ageometric%2520GNNs%2520at%2520the%2520end%2520of%2520this%2520survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Geometric%20Graph%20Neural%20Networks%3A%20Data%20Structures%2C%20Models%20and%0A%20%20Applications&entry.906535625=Jiaqi%20Han%20and%20Jiacheng%20Cen%20and%20Liming%20Wu%20and%20Zongzhao%20Li%20and%20Xiangzhe%20Kong%20and%20Rui%20Jiao%20and%20Ziyang%20Yu%20and%20Tingyang%20Xu%20and%20Fandi%20Wu%20and%20Zihe%20Wang%20and%20Hongteng%20Xu%20and%20Zhewei%20Wei%20and%20Deli%20Zhao%20and%20Yang%20Liu%20and%20Yu%20Rong%20and%20Wenbing%20Huang&entry.1292438233=%20%20Geometric%20graphs%20are%20a%20special%20kind%20of%20graph%20with%20geometric%20features%2C%20which%0Aare%20vital%20to%20model%20many%20scientific%20problems.%20Unlike%20generic%20graphs%2C%20geometric%0Agraphs%20often%20exhibit%20physical%20symmetries%20of%20translations%2C%20rotations%2C%20and%0Areflections%2C%20making%20them%20ineffectively%20processed%20by%20current%20Graph%20Neural%0ANetworks%20%28GNNs%29.%20To%20address%20this%20issue%2C%20researchers%20proposed%20a%20variety%20of%0Ageometric%20GNNs%20equipped%20with%20invariant/equivariant%20properties%20to%20better%0Acharacterize%20the%20geometry%20and%20topology%20of%20geometric%20graphs.%20Given%20the%20current%0Aprogress%20in%20this%20field%2C%20it%20is%20imperative%20to%20conduct%20a%20comprehensive%20survey%20of%0Adata%20structures%2C%20models%2C%20and%20applications%20related%20to%20geometric%20GNNs.%20In%20this%0Apaper%2C%20based%20on%20the%20necessary%20but%20concise%20mathematical%20preliminaries%2C%20we%0Aformalize%20geometric%20graph%20as%20the%20data%20structure%2C%20on%20top%20of%20which%20we%20provide%20a%0Aunified%20view%20of%20existing%20models%20from%20the%20geometric%20message%20passing%20perspective.%0AAdditionally%2C%20we%20summarize%20the%20applications%20as%20well%20as%20the%20related%20datasets%20to%0Afacilitate%20later%20research%20for%20methodology%20development%20and%20experimental%0Aevaluation.%20We%20also%20discuss%20the%20challenges%20and%20future%20potential%20directions%20of%0Ageometric%20GNNs%20at%20the%20end%20of%20this%20survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00485v2&entry.124074799=Read"},
{"title": "Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence", "author": "Wenzhe Yin and Zehao Xiao and Pan Zhou and Shujian Yu and Jiayi Shen and Jan-Jakob Sonke and Efstratios Gavves", "abstract": "  Multimodal alignment is crucial for various downstream tasks such as\ncross-modal generation and retrieval. Previous multimodal approaches like CLIP\nmaximize the mutual information mainly by aligning pairwise samples across\nmodalities while overlooking the distributional differences, leading to\nsuboptimal alignment with modality gaps. In this paper, to overcome the\nlimitation, we propose CS-Aligner, a novel and straightforward framework that\nperforms distributional vision-language alignment by integrating Cauchy-Schwarz\n(CS) divergence with mutual information. In the proposed framework, we find\nthat the CS divergence and mutual information serve complementary roles in\nmultimodal alignment, capturing both the global distribution information of\neach modality and the pairwise semantic relationships, yielding tighter and\nmore precise alignment. Moreover, CS-Aligher enables incorporating additional\ninformation from unpaired data and token-level representations, enhancing\nflexible and fine-grained alignment in practice. Experiments on text-to-image\ngeneration and cross-modality retrieval tasks demonstrate the effectiveness of\nour method on vision-language alignment.\n", "link": "http://arxiv.org/abs/2502.17028v1", "date": "2025-02-24", "relevancy": 2.5452, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributional%20Vision-Language%20Alignment%20by%20Cauchy-Schwarz%20Divergence&body=Title%3A%20Distributional%20Vision-Language%20Alignment%20by%20Cauchy-Schwarz%20Divergence%0AAuthor%3A%20Wenzhe%20Yin%20and%20Zehao%20Xiao%20and%20Pan%20Zhou%20and%20Shujian%20Yu%20and%20Jiayi%20Shen%20and%20Jan-Jakob%20Sonke%20and%20Efstratios%20Gavves%0AAbstract%3A%20%20%20Multimodal%20alignment%20is%20crucial%20for%20various%20downstream%20tasks%20such%20as%0Across-modal%20generation%20and%20retrieval.%20Previous%20multimodal%20approaches%20like%20CLIP%0Amaximize%20the%20mutual%20information%20mainly%20by%20aligning%20pairwise%20samples%20across%0Amodalities%20while%20overlooking%20the%20distributional%20differences%2C%20leading%20to%0Asuboptimal%20alignment%20with%20modality%20gaps.%20In%20this%20paper%2C%20to%20overcome%20the%0Alimitation%2C%20we%20propose%20CS-Aligner%2C%20a%20novel%20and%20straightforward%20framework%20that%0Aperforms%20distributional%20vision-language%20alignment%20by%20integrating%20Cauchy-Schwarz%0A%28CS%29%20divergence%20with%20mutual%20information.%20In%20the%20proposed%20framework%2C%20we%20find%0Athat%20the%20CS%20divergence%20and%20mutual%20information%20serve%20complementary%20roles%20in%0Amultimodal%20alignment%2C%20capturing%20both%20the%20global%20distribution%20information%20of%0Aeach%20modality%20and%20the%20pairwise%20semantic%20relationships%2C%20yielding%20tighter%20and%0Amore%20precise%20alignment.%20Moreover%2C%20CS-Aligher%20enables%20incorporating%20additional%0Ainformation%20from%20unpaired%20data%20and%20token-level%20representations%2C%20enhancing%0Aflexible%20and%20fine-grained%20alignment%20in%20practice.%20Experiments%20on%20text-to-image%0Ageneration%20and%20cross-modality%20retrieval%20tasks%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20on%20vision-language%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributional%2520Vision-Language%2520Alignment%2520by%2520Cauchy-Schwarz%2520Divergence%26entry.906535625%3DWenzhe%2520Yin%2520and%2520Zehao%2520Xiao%2520and%2520Pan%2520Zhou%2520and%2520Shujian%2520Yu%2520and%2520Jiayi%2520Shen%2520and%2520Jan-Jakob%2520Sonke%2520and%2520Efstratios%2520Gavves%26entry.1292438233%3D%2520%2520Multimodal%2520alignment%2520is%2520crucial%2520for%2520various%2520downstream%2520tasks%2520such%2520as%250Across-modal%2520generation%2520and%2520retrieval.%2520Previous%2520multimodal%2520approaches%2520like%2520CLIP%250Amaximize%2520the%2520mutual%2520information%2520mainly%2520by%2520aligning%2520pairwise%2520samples%2520across%250Amodalities%2520while%2520overlooking%2520the%2520distributional%2520differences%252C%2520leading%2520to%250Asuboptimal%2520alignment%2520with%2520modality%2520gaps.%2520In%2520this%2520paper%252C%2520to%2520overcome%2520the%250Alimitation%252C%2520we%2520propose%2520CS-Aligner%252C%2520a%2520novel%2520and%2520straightforward%2520framework%2520that%250Aperforms%2520distributional%2520vision-language%2520alignment%2520by%2520integrating%2520Cauchy-Schwarz%250A%2528CS%2529%2520divergence%2520with%2520mutual%2520information.%2520In%2520the%2520proposed%2520framework%252C%2520we%2520find%250Athat%2520the%2520CS%2520divergence%2520and%2520mutual%2520information%2520serve%2520complementary%2520roles%2520in%250Amultimodal%2520alignment%252C%2520capturing%2520both%2520the%2520global%2520distribution%2520information%2520of%250Aeach%2520modality%2520and%2520the%2520pairwise%2520semantic%2520relationships%252C%2520yielding%2520tighter%2520and%250Amore%2520precise%2520alignment.%2520Moreover%252C%2520CS-Aligher%2520enables%2520incorporating%2520additional%250Ainformation%2520from%2520unpaired%2520data%2520and%2520token-level%2520representations%252C%2520enhancing%250Aflexible%2520and%2520fine-grained%2520alignment%2520in%2520practice.%2520Experiments%2520on%2520text-to-image%250Ageneration%2520and%2520cross-modality%2520retrieval%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520method%2520on%2520vision-language%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributional%20Vision-Language%20Alignment%20by%20Cauchy-Schwarz%20Divergence&entry.906535625=Wenzhe%20Yin%20and%20Zehao%20Xiao%20and%20Pan%20Zhou%20and%20Shujian%20Yu%20and%20Jiayi%20Shen%20and%20Jan-Jakob%20Sonke%20and%20Efstratios%20Gavves&entry.1292438233=%20%20Multimodal%20alignment%20is%20crucial%20for%20various%20downstream%20tasks%20such%20as%0Across-modal%20generation%20and%20retrieval.%20Previous%20multimodal%20approaches%20like%20CLIP%0Amaximize%20the%20mutual%20information%20mainly%20by%20aligning%20pairwise%20samples%20across%0Amodalities%20while%20overlooking%20the%20distributional%20differences%2C%20leading%20to%0Asuboptimal%20alignment%20with%20modality%20gaps.%20In%20this%20paper%2C%20to%20overcome%20the%0Alimitation%2C%20we%20propose%20CS-Aligner%2C%20a%20novel%20and%20straightforward%20framework%20that%0Aperforms%20distributional%20vision-language%20alignment%20by%20integrating%20Cauchy-Schwarz%0A%28CS%29%20divergence%20with%20mutual%20information.%20In%20the%20proposed%20framework%2C%20we%20find%0Athat%20the%20CS%20divergence%20and%20mutual%20information%20serve%20complementary%20roles%20in%0Amultimodal%20alignment%2C%20capturing%20both%20the%20global%20distribution%20information%20of%0Aeach%20modality%20and%20the%20pairwise%20semantic%20relationships%2C%20yielding%20tighter%20and%0Amore%20precise%20alignment.%20Moreover%2C%20CS-Aligher%20enables%20incorporating%20additional%0Ainformation%20from%20unpaired%20data%20and%20token-level%20representations%2C%20enhancing%0Aflexible%20and%20fine-grained%20alignment%20in%20practice.%20Experiments%20on%20text-to-image%0Ageneration%20and%20cross-modality%20retrieval%20tasks%20demonstrate%20the%20effectiveness%20of%0Aour%20method%20on%20vision-language%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17028v1&entry.124074799=Read"},
{"title": "Leveraging Procedural Knowledge and Task Hierarchies for Efficient\n  Instructional Video Pre-training", "author": "Karan Samel and Nitish Sontakke and Irfan Essa", "abstract": "  Instructional videos provide a convenient modality to learn new tasks (ex.\ncooking a recipe, or assembling furniture). A viewer will want to find a\ncorresponding video that reflects both the overall task they are interested in\nas well as contains the relevant steps they need to carry out the task. To\nperform this, an instructional video model should be capable of inferring both\nthe tasks and the steps that occur in an input video. Doing this efficiently\nand in a generalizable fashion is key when compute or relevant video topics\nused to train this model are limited. To address these requirements we\nexplicitly mine task hierarchies and the procedural steps associated with\ninstructional videos. We use this prior knowledge to pre-train our model,\n$\\texttt{Pivot}$, for step and task prediction. During pre-training, we also\nprovide video augmentation and early stopping strategies to optimally identify\nwhich model to use for downstream tasks. We test this pre-trained model on task\nrecognition, step recognition, and step prediction tasks on two downstream\ndatasets. When pre-training data and compute are limited, we outperform\nprevious baselines along these tasks. Therefore, leveraging prior task and step\nstructures enables efficient training of $\\texttt{Pivot}$ for instructional\nvideo recommendation.\n", "link": "http://arxiv.org/abs/2502.17352v1", "date": "2025-02-24", "relevancy": 2.5268, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Procedural%20Knowledge%20and%20Task%20Hierarchies%20for%20Efficient%0A%20%20Instructional%20Video%20Pre-training&body=Title%3A%20Leveraging%20Procedural%20Knowledge%20and%20Task%20Hierarchies%20for%20Efficient%0A%20%20Instructional%20Video%20Pre-training%0AAuthor%3A%20Karan%20Samel%20and%20Nitish%20Sontakke%20and%20Irfan%20Essa%0AAbstract%3A%20%20%20Instructional%20videos%20provide%20a%20convenient%20modality%20to%20learn%20new%20tasks%20%28ex.%0Acooking%20a%20recipe%2C%20or%20assembling%20furniture%29.%20A%20viewer%20will%20want%20to%20find%20a%0Acorresponding%20video%20that%20reflects%20both%20the%20overall%20task%20they%20are%20interested%20in%0Aas%20well%20as%20contains%20the%20relevant%20steps%20they%20need%20to%20carry%20out%20the%20task.%20To%0Aperform%20this%2C%20an%20instructional%20video%20model%20should%20be%20capable%20of%20inferring%20both%0Athe%20tasks%20and%20the%20steps%20that%20occur%20in%20an%20input%20video.%20Doing%20this%20efficiently%0Aand%20in%20a%20generalizable%20fashion%20is%20key%20when%20compute%20or%20relevant%20video%20topics%0Aused%20to%20train%20this%20model%20are%20limited.%20To%20address%20these%20requirements%20we%0Aexplicitly%20mine%20task%20hierarchies%20and%20the%20procedural%20steps%20associated%20with%0Ainstructional%20videos.%20We%20use%20this%20prior%20knowledge%20to%20pre-train%20our%20model%2C%0A%24%5Ctexttt%7BPivot%7D%24%2C%20for%20step%20and%20task%20prediction.%20During%20pre-training%2C%20we%20also%0Aprovide%20video%20augmentation%20and%20early%20stopping%20strategies%20to%20optimally%20identify%0Awhich%20model%20to%20use%20for%20downstream%20tasks.%20We%20test%20this%20pre-trained%20model%20on%20task%0Arecognition%2C%20step%20recognition%2C%20and%20step%20prediction%20tasks%20on%20two%20downstream%0Adatasets.%20When%20pre-training%20data%20and%20compute%20are%20limited%2C%20we%20outperform%0Aprevious%20baselines%20along%20these%20tasks.%20Therefore%2C%20leveraging%20prior%20task%20and%20step%0Astructures%20enables%20efficient%20training%20of%20%24%5Ctexttt%7BPivot%7D%24%20for%20instructional%0Avideo%20recommendation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Procedural%2520Knowledge%2520and%2520Task%2520Hierarchies%2520for%2520Efficient%250A%2520%2520Instructional%2520Video%2520Pre-training%26entry.906535625%3DKaran%2520Samel%2520and%2520Nitish%2520Sontakke%2520and%2520Irfan%2520Essa%26entry.1292438233%3D%2520%2520Instructional%2520videos%2520provide%2520a%2520convenient%2520modality%2520to%2520learn%2520new%2520tasks%2520%2528ex.%250Acooking%2520a%2520recipe%252C%2520or%2520assembling%2520furniture%2529.%2520A%2520viewer%2520will%2520want%2520to%2520find%2520a%250Acorresponding%2520video%2520that%2520reflects%2520both%2520the%2520overall%2520task%2520they%2520are%2520interested%2520in%250Aas%2520well%2520as%2520contains%2520the%2520relevant%2520steps%2520they%2520need%2520to%2520carry%2520out%2520the%2520task.%2520To%250Aperform%2520this%252C%2520an%2520instructional%2520video%2520model%2520should%2520be%2520capable%2520of%2520inferring%2520both%250Athe%2520tasks%2520and%2520the%2520steps%2520that%2520occur%2520in%2520an%2520input%2520video.%2520Doing%2520this%2520efficiently%250Aand%2520in%2520a%2520generalizable%2520fashion%2520is%2520key%2520when%2520compute%2520or%2520relevant%2520video%2520topics%250Aused%2520to%2520train%2520this%2520model%2520are%2520limited.%2520To%2520address%2520these%2520requirements%2520we%250Aexplicitly%2520mine%2520task%2520hierarchies%2520and%2520the%2520procedural%2520steps%2520associated%2520with%250Ainstructional%2520videos.%2520We%2520use%2520this%2520prior%2520knowledge%2520to%2520pre-train%2520our%2520model%252C%250A%2524%255Ctexttt%257BPivot%257D%2524%252C%2520for%2520step%2520and%2520task%2520prediction.%2520During%2520pre-training%252C%2520we%2520also%250Aprovide%2520video%2520augmentation%2520and%2520early%2520stopping%2520strategies%2520to%2520optimally%2520identify%250Awhich%2520model%2520to%2520use%2520for%2520downstream%2520tasks.%2520We%2520test%2520this%2520pre-trained%2520model%2520on%2520task%250Arecognition%252C%2520step%2520recognition%252C%2520and%2520step%2520prediction%2520tasks%2520on%2520two%2520downstream%250Adatasets.%2520When%2520pre-training%2520data%2520and%2520compute%2520are%2520limited%252C%2520we%2520outperform%250Aprevious%2520baselines%2520along%2520these%2520tasks.%2520Therefore%252C%2520leveraging%2520prior%2520task%2520and%2520step%250Astructures%2520enables%2520efficient%2520training%2520of%2520%2524%255Ctexttt%257BPivot%257D%2524%2520for%2520instructional%250Avideo%2520recommendation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Procedural%20Knowledge%20and%20Task%20Hierarchies%20for%20Efficient%0A%20%20Instructional%20Video%20Pre-training&entry.906535625=Karan%20Samel%20and%20Nitish%20Sontakke%20and%20Irfan%20Essa&entry.1292438233=%20%20Instructional%20videos%20provide%20a%20convenient%20modality%20to%20learn%20new%20tasks%20%28ex.%0Acooking%20a%20recipe%2C%20or%20assembling%20furniture%29.%20A%20viewer%20will%20want%20to%20find%20a%0Acorresponding%20video%20that%20reflects%20both%20the%20overall%20task%20they%20are%20interested%20in%0Aas%20well%20as%20contains%20the%20relevant%20steps%20they%20need%20to%20carry%20out%20the%20task.%20To%0Aperform%20this%2C%20an%20instructional%20video%20model%20should%20be%20capable%20of%20inferring%20both%0Athe%20tasks%20and%20the%20steps%20that%20occur%20in%20an%20input%20video.%20Doing%20this%20efficiently%0Aand%20in%20a%20generalizable%20fashion%20is%20key%20when%20compute%20or%20relevant%20video%20topics%0Aused%20to%20train%20this%20model%20are%20limited.%20To%20address%20these%20requirements%20we%0Aexplicitly%20mine%20task%20hierarchies%20and%20the%20procedural%20steps%20associated%20with%0Ainstructional%20videos.%20We%20use%20this%20prior%20knowledge%20to%20pre-train%20our%20model%2C%0A%24%5Ctexttt%7BPivot%7D%24%2C%20for%20step%20and%20task%20prediction.%20During%20pre-training%2C%20we%20also%0Aprovide%20video%20augmentation%20and%20early%20stopping%20strategies%20to%20optimally%20identify%0Awhich%20model%20to%20use%20for%20downstream%20tasks.%20We%20test%20this%20pre-trained%20model%20on%20task%0Arecognition%2C%20step%20recognition%2C%20and%20step%20prediction%20tasks%20on%20two%20downstream%0Adatasets.%20When%20pre-training%20data%20and%20compute%20are%20limited%2C%20we%20outperform%0Aprevious%20baselines%20along%20these%20tasks.%20Therefore%2C%20leveraging%20prior%20task%20and%20step%0Astructures%20enables%20efficient%20training%20of%20%24%5Ctexttt%7BPivot%7D%24%20for%20instructional%0Avideo%20recommendation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17352v1&entry.124074799=Read"},
{"title": "COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of\n  LLMs", "author": "Liming Liu and Zhenghao Xu and Zixuan Zhang and Hao Kang and Zichong Li and Chen Liang and Weizhu Chen and Tuo Zhao", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains, yet their optimization remains a significant challenge due to\nthe complex and high-dimensional loss landscapes they inhabit. While adaptive\noptimizers such as AdamW are widely used, they suffer from critical\nlimitations, including an inability to capture interdependencies between\ncoordinates and high memory consumption. Subsequent research, exemplified by\nSOAP, attempts to better capture coordinate interdependence but incurs greater\nmemory overhead, limiting scalability for massive LLMs. An alternative approach\naims to reduce memory consumption through low-dimensional projection, but this\nleads to substantial approximation errors, resulting in less effective\noptimization (e.g., in terms of per-token efficiency). In this paper, we\npropose COSMOS, a novel hybrid optimizer that leverages the varying importance\nof eigensubspaces in the gradient matrix to achieve memory efficiency without\ncompromising optimization performance. The design of COSMOS is motivated by our\nempirical insights and practical considerations. Specifically, COSMOS applies\nSOAP to the leading eigensubspace, which captures the primary optimization\ndynamics, and MUON to the remaining eigensubspace, which is less critical but\ncomputationally expensive to handle with SOAP. This hybrid strategy\nsignificantly reduces memory consumption while maintaining robust optimization\nperformance, making it particularly suitable for massive LLMs. Numerical\nexperiments on various datasets and transformer architectures are provided to\ndemonstrate the effectiveness of COSMOS. Our code is available at\nhttps://github.com/lliu606/COSMOS.\n", "link": "http://arxiv.org/abs/2502.17410v1", "date": "2025-02-24", "relevancy": 2.5185, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COSMOS%3A%20A%20Hybrid%20Adaptive%20Optimizer%20for%20Memory-Efficient%20Training%20of%0A%20%20LLMs&body=Title%3A%20COSMOS%3A%20A%20Hybrid%20Adaptive%20Optimizer%20for%20Memory-Efficient%20Training%20of%0A%20%20LLMs%0AAuthor%3A%20Liming%20Liu%20and%20Zhenghao%20Xu%20and%20Zixuan%20Zhang%20and%20Hao%20Kang%20and%20Zichong%20Li%20and%20Chen%20Liang%20and%20Weizhu%20Chen%20and%20Tuo%20Zhao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20success%20across%0Avarious%20domains%2C%20yet%20their%20optimization%20remains%20a%20significant%20challenge%20due%20to%0Athe%20complex%20and%20high-dimensional%20loss%20landscapes%20they%20inhabit.%20While%20adaptive%0Aoptimizers%20such%20as%20AdamW%20are%20widely%20used%2C%20they%20suffer%20from%20critical%0Alimitations%2C%20including%20an%20inability%20to%20capture%20interdependencies%20between%0Acoordinates%20and%20high%20memory%20consumption.%20Subsequent%20research%2C%20exemplified%20by%0ASOAP%2C%20attempts%20to%20better%20capture%20coordinate%20interdependence%20but%20incurs%20greater%0Amemory%20overhead%2C%20limiting%20scalability%20for%20massive%20LLMs.%20An%20alternative%20approach%0Aaims%20to%20reduce%20memory%20consumption%20through%20low-dimensional%20projection%2C%20but%20this%0Aleads%20to%20substantial%20approximation%20errors%2C%20resulting%20in%20less%20effective%0Aoptimization%20%28e.g.%2C%20in%20terms%20of%20per-token%20efficiency%29.%20In%20this%20paper%2C%20we%0Apropose%20COSMOS%2C%20a%20novel%20hybrid%20optimizer%20that%20leverages%20the%20varying%20importance%0Aof%20eigensubspaces%20in%20the%20gradient%20matrix%20to%20achieve%20memory%20efficiency%20without%0Acompromising%20optimization%20performance.%20The%20design%20of%20COSMOS%20is%20motivated%20by%20our%0Aempirical%20insights%20and%20practical%20considerations.%20Specifically%2C%20COSMOS%20applies%0ASOAP%20to%20the%20leading%20eigensubspace%2C%20which%20captures%20the%20primary%20optimization%0Adynamics%2C%20and%20MUON%20to%20the%20remaining%20eigensubspace%2C%20which%20is%20less%20critical%20but%0Acomputationally%20expensive%20to%20handle%20with%20SOAP.%20This%20hybrid%20strategy%0Asignificantly%20reduces%20memory%20consumption%20while%20maintaining%20robust%20optimization%0Aperformance%2C%20making%20it%20particularly%20suitable%20for%20massive%20LLMs.%20Numerical%0Aexperiments%20on%20various%20datasets%20and%20transformer%20architectures%20are%20provided%20to%0Ademonstrate%20the%20effectiveness%20of%20COSMOS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lliu606/COSMOS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOSMOS%253A%2520A%2520Hybrid%2520Adaptive%2520Optimizer%2520for%2520Memory-Efficient%2520Training%2520of%250A%2520%2520LLMs%26entry.906535625%3DLiming%2520Liu%2520and%2520Zhenghao%2520Xu%2520and%2520Zixuan%2520Zhang%2520and%2520Hao%2520Kang%2520and%2520Zichong%2520Li%2520and%2520Chen%2520Liang%2520and%2520Weizhu%2520Chen%2520and%2520Tuo%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520success%2520across%250Avarious%2520domains%252C%2520yet%2520their%2520optimization%2520remains%2520a%2520significant%2520challenge%2520due%2520to%250Athe%2520complex%2520and%2520high-dimensional%2520loss%2520landscapes%2520they%2520inhabit.%2520While%2520adaptive%250Aoptimizers%2520such%2520as%2520AdamW%2520are%2520widely%2520used%252C%2520they%2520suffer%2520from%2520critical%250Alimitations%252C%2520including%2520an%2520inability%2520to%2520capture%2520interdependencies%2520between%250Acoordinates%2520and%2520high%2520memory%2520consumption.%2520Subsequent%2520research%252C%2520exemplified%2520by%250ASOAP%252C%2520attempts%2520to%2520better%2520capture%2520coordinate%2520interdependence%2520but%2520incurs%2520greater%250Amemory%2520overhead%252C%2520limiting%2520scalability%2520for%2520massive%2520LLMs.%2520An%2520alternative%2520approach%250Aaims%2520to%2520reduce%2520memory%2520consumption%2520through%2520low-dimensional%2520projection%252C%2520but%2520this%250Aleads%2520to%2520substantial%2520approximation%2520errors%252C%2520resulting%2520in%2520less%2520effective%250Aoptimization%2520%2528e.g.%252C%2520in%2520terms%2520of%2520per-token%2520efficiency%2529.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520COSMOS%252C%2520a%2520novel%2520hybrid%2520optimizer%2520that%2520leverages%2520the%2520varying%2520importance%250Aof%2520eigensubspaces%2520in%2520the%2520gradient%2520matrix%2520to%2520achieve%2520memory%2520efficiency%2520without%250Acompromising%2520optimization%2520performance.%2520The%2520design%2520of%2520COSMOS%2520is%2520motivated%2520by%2520our%250Aempirical%2520insights%2520and%2520practical%2520considerations.%2520Specifically%252C%2520COSMOS%2520applies%250ASOAP%2520to%2520the%2520leading%2520eigensubspace%252C%2520which%2520captures%2520the%2520primary%2520optimization%250Adynamics%252C%2520and%2520MUON%2520to%2520the%2520remaining%2520eigensubspace%252C%2520which%2520is%2520less%2520critical%2520but%250Acomputationally%2520expensive%2520to%2520handle%2520with%2520SOAP.%2520This%2520hybrid%2520strategy%250Asignificantly%2520reduces%2520memory%2520consumption%2520while%2520maintaining%2520robust%2520optimization%250Aperformance%252C%2520making%2520it%2520particularly%2520suitable%2520for%2520massive%2520LLMs.%2520Numerical%250Aexperiments%2520on%2520various%2520datasets%2520and%2520transformer%2520architectures%2520are%2520provided%2520to%250Ademonstrate%2520the%2520effectiveness%2520of%2520COSMOS.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lliu606/COSMOS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COSMOS%3A%20A%20Hybrid%20Adaptive%20Optimizer%20for%20Memory-Efficient%20Training%20of%0A%20%20LLMs&entry.906535625=Liming%20Liu%20and%20Zhenghao%20Xu%20and%20Zixuan%20Zhang%20and%20Hao%20Kang%20and%20Zichong%20Li%20and%20Chen%20Liang%20and%20Weizhu%20Chen%20and%20Tuo%20Zhao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20success%20across%0Avarious%20domains%2C%20yet%20their%20optimization%20remains%20a%20significant%20challenge%20due%20to%0Athe%20complex%20and%20high-dimensional%20loss%20landscapes%20they%20inhabit.%20While%20adaptive%0Aoptimizers%20such%20as%20AdamW%20are%20widely%20used%2C%20they%20suffer%20from%20critical%0Alimitations%2C%20including%20an%20inability%20to%20capture%20interdependencies%20between%0Acoordinates%20and%20high%20memory%20consumption.%20Subsequent%20research%2C%20exemplified%20by%0ASOAP%2C%20attempts%20to%20better%20capture%20coordinate%20interdependence%20but%20incurs%20greater%0Amemory%20overhead%2C%20limiting%20scalability%20for%20massive%20LLMs.%20An%20alternative%20approach%0Aaims%20to%20reduce%20memory%20consumption%20through%20low-dimensional%20projection%2C%20but%20this%0Aleads%20to%20substantial%20approximation%20errors%2C%20resulting%20in%20less%20effective%0Aoptimization%20%28e.g.%2C%20in%20terms%20of%20per-token%20efficiency%29.%20In%20this%20paper%2C%20we%0Apropose%20COSMOS%2C%20a%20novel%20hybrid%20optimizer%20that%20leverages%20the%20varying%20importance%0Aof%20eigensubspaces%20in%20the%20gradient%20matrix%20to%20achieve%20memory%20efficiency%20without%0Acompromising%20optimization%20performance.%20The%20design%20of%20COSMOS%20is%20motivated%20by%20our%0Aempirical%20insights%20and%20practical%20considerations.%20Specifically%2C%20COSMOS%20applies%0ASOAP%20to%20the%20leading%20eigensubspace%2C%20which%20captures%20the%20primary%20optimization%0Adynamics%2C%20and%20MUON%20to%20the%20remaining%20eigensubspace%2C%20which%20is%20less%20critical%20but%0Acomputationally%20expensive%20to%20handle%20with%20SOAP.%20This%20hybrid%20strategy%0Asignificantly%20reduces%20memory%20consumption%20while%20maintaining%20robust%20optimization%0Aperformance%2C%20making%20it%20particularly%20suitable%20for%20massive%20LLMs.%20Numerical%0Aexperiments%20on%20various%20datasets%20and%20transformer%20architectures%20are%20provided%20to%0Ademonstrate%20the%20effectiveness%20of%20COSMOS.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lliu606/COSMOS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17410v1&entry.124074799=Read"},
{"title": "Compactly-supported nonstationary kernels for computing exact Gaussian\n  processes on big data", "author": "Mark D. Risser and Marcus M. Noack and Hengrui Luo and Ronald Pandolfi", "abstract": "  The Gaussian process (GP) is a widely used probabilistic machine learning\nmethod with implicit uncertainty characterization for stochastic function\napproximation, stochastic modeling, and analyzing real-world measurements of\nnonlinear processes. Traditional implementations of GPs involve stationary\nkernels (also termed covariance functions) that limit their flexibility, and\nexact methods for inference that prevent application to data sets with more\nthan about ten thousand points. Modern approaches to address stationarity\nassumptions generally fail to accommodate large data sets, while all attempts\nto address scalability focus on approximating the Gaussian likelihood, which\ncan involve subjectivity and lead to inaccuracies. In this work, we explicitly\nderive an alternative kernel that can discover and encode both sparsity and\nnonstationarity. We embed the kernel within a fully Bayesian GP model and\nleverage high-performance computing resources to enable the analysis of massive\ndata sets. We demonstrate the favorable performance of our novel kernel\nrelative to existing exact and approximate GP methods across a variety of\nsynthetic data examples. Furthermore, we conduct space-time prediction based on\nmore than one million measurements of daily maximum temperature and verify that\nour results outperform state-of-the-art methods in the Earth sciences. More\nbroadly, having access to exact GPs that use ultra-scalable,\nsparsity-discovering, nonstationary kernels allows GP methods to truly compete\nwith a wide variety of machine learning methods.\n", "link": "http://arxiv.org/abs/2411.05869v2", "date": "2025-02-24", "relevancy": 2.4981, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5078}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.498}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compactly-supported%20nonstationary%20kernels%20for%20computing%20exact%20Gaussian%0A%20%20processes%20on%20big%20data&body=Title%3A%20Compactly-supported%20nonstationary%20kernels%20for%20computing%20exact%20Gaussian%0A%20%20processes%20on%20big%20data%0AAuthor%3A%20Mark%20D.%20Risser%20and%20Marcus%20M.%20Noack%20and%20Hengrui%20Luo%20and%20Ronald%20Pandolfi%0AAbstract%3A%20%20%20The%20Gaussian%20process%20%28GP%29%20is%20a%20widely%20used%20probabilistic%20machine%20learning%0Amethod%20with%20implicit%20uncertainty%20characterization%20for%20stochastic%20function%0Aapproximation%2C%20stochastic%20modeling%2C%20and%20analyzing%20real-world%20measurements%20of%0Anonlinear%20processes.%20Traditional%20implementations%20of%20GPs%20involve%20stationary%0Akernels%20%28also%20termed%20covariance%20functions%29%20that%20limit%20their%20flexibility%2C%20and%0Aexact%20methods%20for%20inference%20that%20prevent%20application%20to%20data%20sets%20with%20more%0Athan%20about%20ten%20thousand%20points.%20Modern%20approaches%20to%20address%20stationarity%0Aassumptions%20generally%20fail%20to%20accommodate%20large%20data%20sets%2C%20while%20all%20attempts%0Ato%20address%20scalability%20focus%20on%20approximating%20the%20Gaussian%20likelihood%2C%20which%0Acan%20involve%20subjectivity%20and%20lead%20to%20inaccuracies.%20In%20this%20work%2C%20we%20explicitly%0Aderive%20an%20alternative%20kernel%20that%20can%20discover%20and%20encode%20both%20sparsity%20and%0Anonstationarity.%20We%20embed%20the%20kernel%20within%20a%20fully%20Bayesian%20GP%20model%20and%0Aleverage%20high-performance%20computing%20resources%20to%20enable%20the%20analysis%20of%20massive%0Adata%20sets.%20We%20demonstrate%20the%20favorable%20performance%20of%20our%20novel%20kernel%0Arelative%20to%20existing%20exact%20and%20approximate%20GP%20methods%20across%20a%20variety%20of%0Asynthetic%20data%20examples.%20Furthermore%2C%20we%20conduct%20space-time%20prediction%20based%20on%0Amore%20than%20one%20million%20measurements%20of%20daily%20maximum%20temperature%20and%20verify%20that%0Aour%20results%20outperform%20state-of-the-art%20methods%20in%20the%20Earth%20sciences.%20More%0Abroadly%2C%20having%20access%20to%20exact%20GPs%20that%20use%20ultra-scalable%2C%0Asparsity-discovering%2C%20nonstationary%20kernels%20allows%20GP%20methods%20to%20truly%20compete%0Awith%20a%20wide%20variety%20of%20machine%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05869v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompactly-supported%2520nonstationary%2520kernels%2520for%2520computing%2520exact%2520Gaussian%250A%2520%2520processes%2520on%2520big%2520data%26entry.906535625%3DMark%2520D.%2520Risser%2520and%2520Marcus%2520M.%2520Noack%2520and%2520Hengrui%2520Luo%2520and%2520Ronald%2520Pandolfi%26entry.1292438233%3D%2520%2520The%2520Gaussian%2520process%2520%2528GP%2529%2520is%2520a%2520widely%2520used%2520probabilistic%2520machine%2520learning%250Amethod%2520with%2520implicit%2520uncertainty%2520characterization%2520for%2520stochastic%2520function%250Aapproximation%252C%2520stochastic%2520modeling%252C%2520and%2520analyzing%2520real-world%2520measurements%2520of%250Anonlinear%2520processes.%2520Traditional%2520implementations%2520of%2520GPs%2520involve%2520stationary%250Akernels%2520%2528also%2520termed%2520covariance%2520functions%2529%2520that%2520limit%2520their%2520flexibility%252C%2520and%250Aexact%2520methods%2520for%2520inference%2520that%2520prevent%2520application%2520to%2520data%2520sets%2520with%2520more%250Athan%2520about%2520ten%2520thousand%2520points.%2520Modern%2520approaches%2520to%2520address%2520stationarity%250Aassumptions%2520generally%2520fail%2520to%2520accommodate%2520large%2520data%2520sets%252C%2520while%2520all%2520attempts%250Ato%2520address%2520scalability%2520focus%2520on%2520approximating%2520the%2520Gaussian%2520likelihood%252C%2520which%250Acan%2520involve%2520subjectivity%2520and%2520lead%2520to%2520inaccuracies.%2520In%2520this%2520work%252C%2520we%2520explicitly%250Aderive%2520an%2520alternative%2520kernel%2520that%2520can%2520discover%2520and%2520encode%2520both%2520sparsity%2520and%250Anonstationarity.%2520We%2520embed%2520the%2520kernel%2520within%2520a%2520fully%2520Bayesian%2520GP%2520model%2520and%250Aleverage%2520high-performance%2520computing%2520resources%2520to%2520enable%2520the%2520analysis%2520of%2520massive%250Adata%2520sets.%2520We%2520demonstrate%2520the%2520favorable%2520performance%2520of%2520our%2520novel%2520kernel%250Arelative%2520to%2520existing%2520exact%2520and%2520approximate%2520GP%2520methods%2520across%2520a%2520variety%2520of%250Asynthetic%2520data%2520examples.%2520Furthermore%252C%2520we%2520conduct%2520space-time%2520prediction%2520based%2520on%250Amore%2520than%2520one%2520million%2520measurements%2520of%2520daily%2520maximum%2520temperature%2520and%2520verify%2520that%250Aour%2520results%2520outperform%2520state-of-the-art%2520methods%2520in%2520the%2520Earth%2520sciences.%2520More%250Abroadly%252C%2520having%2520access%2520to%2520exact%2520GPs%2520that%2520use%2520ultra-scalable%252C%250Asparsity-discovering%252C%2520nonstationary%2520kernels%2520allows%2520GP%2520methods%2520to%2520truly%2520compete%250Awith%2520a%2520wide%2520variety%2520of%2520machine%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05869v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compactly-supported%20nonstationary%20kernels%20for%20computing%20exact%20Gaussian%0A%20%20processes%20on%20big%20data&entry.906535625=Mark%20D.%20Risser%20and%20Marcus%20M.%20Noack%20and%20Hengrui%20Luo%20and%20Ronald%20Pandolfi&entry.1292438233=%20%20The%20Gaussian%20process%20%28GP%29%20is%20a%20widely%20used%20probabilistic%20machine%20learning%0Amethod%20with%20implicit%20uncertainty%20characterization%20for%20stochastic%20function%0Aapproximation%2C%20stochastic%20modeling%2C%20and%20analyzing%20real-world%20measurements%20of%0Anonlinear%20processes.%20Traditional%20implementations%20of%20GPs%20involve%20stationary%0Akernels%20%28also%20termed%20covariance%20functions%29%20that%20limit%20their%20flexibility%2C%20and%0Aexact%20methods%20for%20inference%20that%20prevent%20application%20to%20data%20sets%20with%20more%0Athan%20about%20ten%20thousand%20points.%20Modern%20approaches%20to%20address%20stationarity%0Aassumptions%20generally%20fail%20to%20accommodate%20large%20data%20sets%2C%20while%20all%20attempts%0Ato%20address%20scalability%20focus%20on%20approximating%20the%20Gaussian%20likelihood%2C%20which%0Acan%20involve%20subjectivity%20and%20lead%20to%20inaccuracies.%20In%20this%20work%2C%20we%20explicitly%0Aderive%20an%20alternative%20kernel%20that%20can%20discover%20and%20encode%20both%20sparsity%20and%0Anonstationarity.%20We%20embed%20the%20kernel%20within%20a%20fully%20Bayesian%20GP%20model%20and%0Aleverage%20high-performance%20computing%20resources%20to%20enable%20the%20analysis%20of%20massive%0Adata%20sets.%20We%20demonstrate%20the%20favorable%20performance%20of%20our%20novel%20kernel%0Arelative%20to%20existing%20exact%20and%20approximate%20GP%20methods%20across%20a%20variety%20of%0Asynthetic%20data%20examples.%20Furthermore%2C%20we%20conduct%20space-time%20prediction%20based%20on%0Amore%20than%20one%20million%20measurements%20of%20daily%20maximum%20temperature%20and%20verify%20that%0Aour%20results%20outperform%20state-of-the-art%20methods%20in%20the%20Earth%20sciences.%20More%0Abroadly%2C%20having%20access%20to%20exact%20GPs%20that%20use%20ultra-scalable%2C%0Asparsity-discovering%2C%20nonstationary%20kernels%20allows%20GP%20methods%20to%20truly%20compete%0Awith%20a%20wide%20variety%20of%20machine%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05869v2&entry.124074799=Read"},
{"title": "Language Model Re-rankers are Steered by Lexical Similarities", "author": "Lovisa Hagstr\u00f6m and Ercong Nie and Ruben Halifa and Helmut Schmid and Richard Johansson and Alexander Junge", "abstract": "  Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information.\nTo understand whether LM re-rankers always live up to this assumption, we\nevaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our\nresults show that LM re-rankers struggle to outperform a simple BM25 re-ranker\non DRUID. Leveraging a novel separation metric based on BM25 scores, we explain\nand identify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation.\n", "link": "http://arxiv.org/abs/2502.17036v1", "date": "2025-02-24", "relevancy": 2.4887, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Model%20Re-rankers%20are%20Steered%20by%20Lexical%20Similarities&body=Title%3A%20Language%20Model%20Re-rankers%20are%20Steered%20by%20Lexical%20Similarities%0AAuthor%3A%20Lovisa%20Hagstr%C3%B6m%20and%20Ercong%20Nie%20and%20Ruben%20Halifa%20and%20Helmut%20Schmid%20and%20Richard%20Johansson%20and%20Alexander%20Junge%0AAbstract%3A%20%20%20Language%20model%20%28LM%29%20re-rankers%20are%20used%20to%20refine%20retrieval%20results%20for%0Aretrieval-augmented%20generation%20%28RAG%29.%20They%20are%20more%20expensive%20than%20lexical%0Amatching%20methods%20like%20BM25%20but%20assumed%20to%20better%20process%20semantic%20information.%0ATo%20understand%20whether%20LM%20re-rankers%20always%20live%20up%20to%20this%20assumption%2C%20we%0Aevaluate%206%20different%20LM%20re-rankers%20on%20the%20NQ%2C%20LitQA2%20and%20DRUID%20datasets.%20Our%0Aresults%20show%20that%20LM%20re-rankers%20struggle%20to%20outperform%20a%20simple%20BM25%20re-ranker%0Aon%20DRUID.%20Leveraging%20a%20novel%20separation%20metric%20based%20on%20BM25%20scores%2C%20we%20explain%0Aand%20identify%20re-ranker%20errors%20stemming%20from%20lexical%20dissimilarities.%20We%20also%0Ainvestigate%20different%20methods%20to%20improve%20LM%20re-ranker%20performance%20and%20find%0Athese%20methods%20mainly%20useful%20for%20NQ.%20Taken%20together%2C%20our%20work%20identifies%20and%0Aexplains%20weaknesses%20of%20LM%20re-rankers%20and%20points%20to%20the%20need%20for%20more%0Aadversarial%20and%20realistic%20datasets%20for%20their%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Model%2520Re-rankers%2520are%2520Steered%2520by%2520Lexical%2520Similarities%26entry.906535625%3DLovisa%2520Hagstr%25C3%25B6m%2520and%2520Ercong%2520Nie%2520and%2520Ruben%2520Halifa%2520and%2520Helmut%2520Schmid%2520and%2520Richard%2520Johansson%2520and%2520Alexander%2520Junge%26entry.1292438233%3D%2520%2520Language%2520model%2520%2528LM%2529%2520re-rankers%2520are%2520used%2520to%2520refine%2520retrieval%2520results%2520for%250Aretrieval-augmented%2520generation%2520%2528RAG%2529.%2520They%2520are%2520more%2520expensive%2520than%2520lexical%250Amatching%2520methods%2520like%2520BM25%2520but%2520assumed%2520to%2520better%2520process%2520semantic%2520information.%250ATo%2520understand%2520whether%2520LM%2520re-rankers%2520always%2520live%2520up%2520to%2520this%2520assumption%252C%2520we%250Aevaluate%25206%2520different%2520LM%2520re-rankers%2520on%2520the%2520NQ%252C%2520LitQA2%2520and%2520DRUID%2520datasets.%2520Our%250Aresults%2520show%2520that%2520LM%2520re-rankers%2520struggle%2520to%2520outperform%2520a%2520simple%2520BM25%2520re-ranker%250Aon%2520DRUID.%2520Leveraging%2520a%2520novel%2520separation%2520metric%2520based%2520on%2520BM25%2520scores%252C%2520we%2520explain%250Aand%2520identify%2520re-ranker%2520errors%2520stemming%2520from%2520lexical%2520dissimilarities.%2520We%2520also%250Ainvestigate%2520different%2520methods%2520to%2520improve%2520LM%2520re-ranker%2520performance%2520and%2520find%250Athese%2520methods%2520mainly%2520useful%2520for%2520NQ.%2520Taken%2520together%252C%2520our%2520work%2520identifies%2520and%250Aexplains%2520weaknesses%2520of%2520LM%2520re-rankers%2520and%2520points%2520to%2520the%2520need%2520for%2520more%250Aadversarial%2520and%2520realistic%2520datasets%2520for%2520their%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Model%20Re-rankers%20are%20Steered%20by%20Lexical%20Similarities&entry.906535625=Lovisa%20Hagstr%C3%B6m%20and%20Ercong%20Nie%20and%20Ruben%20Halifa%20and%20Helmut%20Schmid%20and%20Richard%20Johansson%20and%20Alexander%20Junge&entry.1292438233=%20%20Language%20model%20%28LM%29%20re-rankers%20are%20used%20to%20refine%20retrieval%20results%20for%0Aretrieval-augmented%20generation%20%28RAG%29.%20They%20are%20more%20expensive%20than%20lexical%0Amatching%20methods%20like%20BM25%20but%20assumed%20to%20better%20process%20semantic%20information.%0ATo%20understand%20whether%20LM%20re-rankers%20always%20live%20up%20to%20this%20assumption%2C%20we%0Aevaluate%206%20different%20LM%20re-rankers%20on%20the%20NQ%2C%20LitQA2%20and%20DRUID%20datasets.%20Our%0Aresults%20show%20that%20LM%20re-rankers%20struggle%20to%20outperform%20a%20simple%20BM25%20re-ranker%0Aon%20DRUID.%20Leveraging%20a%20novel%20separation%20metric%20based%20on%20BM25%20scores%2C%20we%20explain%0Aand%20identify%20re-ranker%20errors%20stemming%20from%20lexical%20dissimilarities.%20We%20also%0Ainvestigate%20different%20methods%20to%20improve%20LM%20re-ranker%20performance%20and%20find%0Athese%20methods%20mainly%20useful%20for%20NQ.%20Taken%20together%2C%20our%20work%20identifies%20and%0Aexplains%20weaknesses%20of%20LM%20re-rankers%20and%20points%20to%20the%20need%20for%20more%0Aadversarial%20and%20realistic%20datasets%20for%20their%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17036v1&entry.124074799=Read"},
{"title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator", "author": "Guoxuan Chen and Han Shi and Jiawei Li and Yihang Gao and Xiaozhe Ren and Yimeng Chen and Xin Jiang and Zhenguo Li and Weiyang Liu and Chao Huang", "abstract": "  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n", "link": "http://arxiv.org/abs/2412.12094v5", "date": "2025-02-24", "relevancy": 2.4748, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&body=Title%3A%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator%0AAuthor%3A%20Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20separator%20tokens%20%28i.e.%2C%0Apunctuations%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12094v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSepLLM%253A%2520Accelerate%2520Large%2520Language%2520Models%2520by%2520Compressing%2520One%2520Segment%2520into%250A%2520%2520One%2520Separator%26entry.906535625%3DGuoxuan%2520Chen%2520and%2520Han%2520Shi%2520and%2520Jiawei%2520Li%2520and%2520Yihang%2520Gao%2520and%2520Xiaozhe%2520Ren%2520and%2520Yimeng%2520Chen%2520and%2520Xin%2520Jiang%2520and%2520Zhenguo%2520Li%2520and%2520Weiyang%2520Liu%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520exceptional%2520performance%2520across%2520a%250Aspectrum%2520of%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520their%2520substantial%2520sizes%250Apose%2520considerable%2520challenges%252C%2520particularly%2520in%2520computational%2520demands%2520and%250Ainference%2520speed%252C%2520due%2520to%2520their%2520quadratic%2520complexity.%2520In%2520this%2520work%252C%2520we%2520have%250Aidentified%2520a%2520key%2520pattern%253A%2520certain%2520seemingly%2520meaningless%2520separator%2520tokens%2520%2528i.e.%252C%250Apunctuations%2529%2520contribute%2520disproportionately%2520to%2520attention%2520scores%2520compared%2520to%250Asemantically%2520meaningful%2520tokens.%2520This%2520observation%2520suggests%2520that%2520information%2520of%250Athe%2520segments%2520between%2520these%2520separator%2520tokens%2520can%2520be%2520effectively%2520condensed%2520into%250Athe%2520separator%2520tokens%2520themselves%2520without%2520significant%2520information%2520loss.%2520Guided%2520by%250Athis%2520insight%252C%2520we%2520introduce%2520SepLLM%252C%2520a%2520plug-and-play%2520framework%2520that%2520accelerates%250Ainference%2520by%2520compressing%2520these%2520segments%2520and%2520eliminating%2520redundant%2520tokens.%250AAdditionally%252C%2520we%2520implement%2520efficient%2520kernels%2520for%2520training%2520acceleration.%250AExperimental%2520results%2520across%2520training-free%252C%2520training-from-scratch%252C%2520and%250Apost-training%2520settings%2520demonstrate%2520SepLLM%2527s%2520effectiveness.%2520Notably%252C%2520using%2520the%250ALlama-3-8B%2520backbone%252C%2520SepLLM%2520achieves%2520over%252050%2525%2520reduction%2520in%2520KV%2520cache%2520on%2520the%250AGSM8K-CoT%2520benchmark%2520while%2520maintaining%2520comparable%2520performance.%2520Furthermore%252C%2520in%250Astreaming%2520settings%252C%2520SepLLM%2520effectively%2520processes%2520sequences%2520of%2520up%2520to%25204%2520million%250Atokens%2520or%2520more%2520while%2520maintaining%2520consistent%2520language%2520modeling%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12094v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&entry.906535625=Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20separator%20tokens%20%28i.e.%2C%0Apunctuations%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12094v5&entry.124074799=Read"},
{"title": "Aligned at the Start: Conceptual Groupings in LLM Embeddings", "author": "Mehrdad Khatir and Sanchit Kabra and Chandan K. Reddy", "abstract": "  This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.\n", "link": "http://arxiv.org/abs/2406.05315v3", "date": "2025-02-24", "relevancy": 2.4725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligned%20at%20the%20Start%3A%20Conceptual%20Groupings%20in%20LLM%20Embeddings&body=Title%3A%20Aligned%20at%20the%20Start%3A%20Conceptual%20Groupings%20in%20LLM%20Embeddings%0AAuthor%3A%20Mehrdad%20Khatir%20and%20Sanchit%20Kabra%20and%20Chandan%20K.%20Reddy%0AAbstract%3A%20%20%20This%20paper%20shifts%20focus%20to%20the%20often-overlooked%20input%20embeddings%20-%20the%0Ainitial%20representations%20fed%20into%20transformer%20blocks.%20Using%20fuzzy%20graph%2C%0Ak-nearest%20neighbor%20%28k-NN%29%2C%20and%20community%20detection%2C%20we%20analyze%20embeddings%20from%0Adiverse%20LLMs%2C%20finding%20significant%20categorical%20community%20structure%20aligned%20with%0Apredefined%20concepts%20and%20categories%20aligned%20with%20humans.%20We%20observe%20these%0Agroupings%20exhibit%20within-cluster%20organization%20%28such%20as%20hierarchies%2C%20topological%0Aordering%2C%20etc.%29%2C%20hypothesizing%20a%20fundamental%20structure%20that%20precedes%20contextual%0Aprocessing.%20To%20further%20investigate%20the%20conceptual%20nature%20of%20these%20groupings%2C%20we%0Aexplore%20cross-model%20alignments%20across%20different%20LLM%20categories%20within%20their%0Ainput%20embeddings%2C%20observing%20a%20medium%20to%20high%20degree%20of%20alignment.%20Furthermore%2C%0Aprovide%20evidence%20that%20manipulating%20these%20groupings%20can%20play%20a%20functional%20role%0Ain%20mitigating%20ethnicity%20bias%20in%20LLM%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05315v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligned%2520at%2520the%2520Start%253A%2520Conceptual%2520Groupings%2520in%2520LLM%2520Embeddings%26entry.906535625%3DMehrdad%2520Khatir%2520and%2520Sanchit%2520Kabra%2520and%2520Chandan%2520K.%2520Reddy%26entry.1292438233%3D%2520%2520This%2520paper%2520shifts%2520focus%2520to%2520the%2520often-overlooked%2520input%2520embeddings%2520-%2520the%250Ainitial%2520representations%2520fed%2520into%2520transformer%2520blocks.%2520Using%2520fuzzy%2520graph%252C%250Ak-nearest%2520neighbor%2520%2528k-NN%2529%252C%2520and%2520community%2520detection%252C%2520we%2520analyze%2520embeddings%2520from%250Adiverse%2520LLMs%252C%2520finding%2520significant%2520categorical%2520community%2520structure%2520aligned%2520with%250Apredefined%2520concepts%2520and%2520categories%2520aligned%2520with%2520humans.%2520We%2520observe%2520these%250Agroupings%2520exhibit%2520within-cluster%2520organization%2520%2528such%2520as%2520hierarchies%252C%2520topological%250Aordering%252C%2520etc.%2529%252C%2520hypothesizing%2520a%2520fundamental%2520structure%2520that%2520precedes%2520contextual%250Aprocessing.%2520To%2520further%2520investigate%2520the%2520conceptual%2520nature%2520of%2520these%2520groupings%252C%2520we%250Aexplore%2520cross-model%2520alignments%2520across%2520different%2520LLM%2520categories%2520within%2520their%250Ainput%2520embeddings%252C%2520observing%2520a%2520medium%2520to%2520high%2520degree%2520of%2520alignment.%2520Furthermore%252C%250Aprovide%2520evidence%2520that%2520manipulating%2520these%2520groupings%2520can%2520play%2520a%2520functional%2520role%250Ain%2520mitigating%2520ethnicity%2520bias%2520in%2520LLM%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05315v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligned%20at%20the%20Start%3A%20Conceptual%20Groupings%20in%20LLM%20Embeddings&entry.906535625=Mehrdad%20Khatir%20and%20Sanchit%20Kabra%20and%20Chandan%20K.%20Reddy&entry.1292438233=%20%20This%20paper%20shifts%20focus%20to%20the%20often-overlooked%20input%20embeddings%20-%20the%0Ainitial%20representations%20fed%20into%20transformer%20blocks.%20Using%20fuzzy%20graph%2C%0Ak-nearest%20neighbor%20%28k-NN%29%2C%20and%20community%20detection%2C%20we%20analyze%20embeddings%20from%0Adiverse%20LLMs%2C%20finding%20significant%20categorical%20community%20structure%20aligned%20with%0Apredefined%20concepts%20and%20categories%20aligned%20with%20humans.%20We%20observe%20these%0Agroupings%20exhibit%20within-cluster%20organization%20%28such%20as%20hierarchies%2C%20topological%0Aordering%2C%20etc.%29%2C%20hypothesizing%20a%20fundamental%20structure%20that%20precedes%20contextual%0Aprocessing.%20To%20further%20investigate%20the%20conceptual%20nature%20of%20these%20groupings%2C%20we%0Aexplore%20cross-model%20alignments%20across%20different%20LLM%20categories%20within%20their%0Ainput%20embeddings%2C%20observing%20a%20medium%20to%20high%20degree%20of%20alignment.%20Furthermore%2C%0Aprovide%20evidence%20that%20manipulating%20these%20groupings%20can%20play%20a%20functional%20role%0Ain%20mitigating%20ethnicity%20bias%20in%20LLM%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05315v3&entry.124074799=Read"},
{"title": "Deep Learning-Powered Electrical Brain Signals Analysis: Advancing\n  Neurological Diagnostics", "author": "Jiahe Li and Xin Chen and Fanqi Shen and Junru Chen and Yuxin Liu and Daoze Zhang and Zhizhang Yuan and Fang Zhao and Meng Li and Yang Yang", "abstract": "  Neurological disorders represent significant global health challenges,\ndriving the advancement of brain signal analysis methods. Scalp\nelectroencephalography (EEG) and intracranial electroencephalography (iEEG) are\nwidely used to diagnose and monitor neurological conditions. However, dataset\nheterogeneity and task variations pose challenges in developing robust deep\nlearning solutions. This review systematically examines recent advances in deep\nlearning approaches for EEG/iEEG-based neurological diagnostics, focusing on\napplications across 7 neurological conditions using 46 datasets. We explore\ntrends in data utilization, model design, and task-specific adaptations,\nhighlighting the importance of pre-trained multi-task models for scalable,\ngeneralizable solutions. To advance research, we propose a standardized\nbenchmark for evaluating models across diverse datasets to enhance\nreproducibility. This survey emphasizes how recent innovations can transform\nneurological diagnostics and enable the development of intelligent, adaptable\nhealthcare solutions.\n", "link": "http://arxiv.org/abs/2502.17213v1", "date": "2025-02-24", "relevancy": 2.4673, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-Powered%20Electrical%20Brain%20Signals%20Analysis%3A%20Advancing%0A%20%20Neurological%20Diagnostics&body=Title%3A%20Deep%20Learning-Powered%20Electrical%20Brain%20Signals%20Analysis%3A%20Advancing%0A%20%20Neurological%20Diagnostics%0AAuthor%3A%20Jiahe%20Li%20and%20Xin%20Chen%20and%20Fanqi%20Shen%20and%20Junru%20Chen%20and%20Yuxin%20Liu%20and%20Daoze%20Zhang%20and%20Zhizhang%20Yuan%20and%20Fang%20Zhao%20and%20Meng%20Li%20and%20Yang%20Yang%0AAbstract%3A%20%20%20Neurological%20disorders%20represent%20significant%20global%20health%20challenges%2C%0Adriving%20the%20advancement%20of%20brain%20signal%20analysis%20methods.%20Scalp%0Aelectroencephalography%20%28EEG%29%20and%20intracranial%20electroencephalography%20%28iEEG%29%20are%0Awidely%20used%20to%20diagnose%20and%20monitor%20neurological%20conditions.%20However%2C%20dataset%0Aheterogeneity%20and%20task%20variations%20pose%20challenges%20in%20developing%20robust%20deep%0Alearning%20solutions.%20This%20review%20systematically%20examines%20recent%20advances%20in%20deep%0Alearning%20approaches%20for%20EEG/iEEG-based%20neurological%20diagnostics%2C%20focusing%20on%0Aapplications%20across%207%20neurological%20conditions%20using%2046%20datasets.%20We%20explore%0Atrends%20in%20data%20utilization%2C%20model%20design%2C%20and%20task-specific%20adaptations%2C%0Ahighlighting%20the%20importance%20of%20pre-trained%20multi-task%20models%20for%20scalable%2C%0Ageneralizable%20solutions.%20To%20advance%20research%2C%20we%20propose%20a%20standardized%0Abenchmark%20for%20evaluating%20models%20across%20diverse%20datasets%20to%20enhance%0Areproducibility.%20This%20survey%20emphasizes%20how%20recent%20innovations%20can%20transform%0Aneurological%20diagnostics%20and%20enable%20the%20development%20of%20intelligent%2C%20adaptable%0Ahealthcare%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-Powered%2520Electrical%2520Brain%2520Signals%2520Analysis%253A%2520Advancing%250A%2520%2520Neurological%2520Diagnostics%26entry.906535625%3DJiahe%2520Li%2520and%2520Xin%2520Chen%2520and%2520Fanqi%2520Shen%2520and%2520Junru%2520Chen%2520and%2520Yuxin%2520Liu%2520and%2520Daoze%2520Zhang%2520and%2520Zhizhang%2520Yuan%2520and%2520Fang%2520Zhao%2520and%2520Meng%2520Li%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520Neurological%2520disorders%2520represent%2520significant%2520global%2520health%2520challenges%252C%250Adriving%2520the%2520advancement%2520of%2520brain%2520signal%2520analysis%2520methods.%2520Scalp%250Aelectroencephalography%2520%2528EEG%2529%2520and%2520intracranial%2520electroencephalography%2520%2528iEEG%2529%2520are%250Awidely%2520used%2520to%2520diagnose%2520and%2520monitor%2520neurological%2520conditions.%2520However%252C%2520dataset%250Aheterogeneity%2520and%2520task%2520variations%2520pose%2520challenges%2520in%2520developing%2520robust%2520deep%250Alearning%2520solutions.%2520This%2520review%2520systematically%2520examines%2520recent%2520advances%2520in%2520deep%250Alearning%2520approaches%2520for%2520EEG/iEEG-based%2520neurological%2520diagnostics%252C%2520focusing%2520on%250Aapplications%2520across%25207%2520neurological%2520conditions%2520using%252046%2520datasets.%2520We%2520explore%250Atrends%2520in%2520data%2520utilization%252C%2520model%2520design%252C%2520and%2520task-specific%2520adaptations%252C%250Ahighlighting%2520the%2520importance%2520of%2520pre-trained%2520multi-task%2520models%2520for%2520scalable%252C%250Ageneralizable%2520solutions.%2520To%2520advance%2520research%252C%2520we%2520propose%2520a%2520standardized%250Abenchmark%2520for%2520evaluating%2520models%2520across%2520diverse%2520datasets%2520to%2520enhance%250Areproducibility.%2520This%2520survey%2520emphasizes%2520how%2520recent%2520innovations%2520can%2520transform%250Aneurological%2520diagnostics%2520and%2520enable%2520the%2520development%2520of%2520intelligent%252C%2520adaptable%250Ahealthcare%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-Powered%20Electrical%20Brain%20Signals%20Analysis%3A%20Advancing%0A%20%20Neurological%20Diagnostics&entry.906535625=Jiahe%20Li%20and%20Xin%20Chen%20and%20Fanqi%20Shen%20and%20Junru%20Chen%20and%20Yuxin%20Liu%20and%20Daoze%20Zhang%20and%20Zhizhang%20Yuan%20and%20Fang%20Zhao%20and%20Meng%20Li%20and%20Yang%20Yang&entry.1292438233=%20%20Neurological%20disorders%20represent%20significant%20global%20health%20challenges%2C%0Adriving%20the%20advancement%20of%20brain%20signal%20analysis%20methods.%20Scalp%0Aelectroencephalography%20%28EEG%29%20and%20intracranial%20electroencephalography%20%28iEEG%29%20are%0Awidely%20used%20to%20diagnose%20and%20monitor%20neurological%20conditions.%20However%2C%20dataset%0Aheterogeneity%20and%20task%20variations%20pose%20challenges%20in%20developing%20robust%20deep%0Alearning%20solutions.%20This%20review%20systematically%20examines%20recent%20advances%20in%20deep%0Alearning%20approaches%20for%20EEG/iEEG-based%20neurological%20diagnostics%2C%20focusing%20on%0Aapplications%20across%207%20neurological%20conditions%20using%2046%20datasets.%20We%20explore%0Atrends%20in%20data%20utilization%2C%20model%20design%2C%20and%20task-specific%20adaptations%2C%0Ahighlighting%20the%20importance%20of%20pre-trained%20multi-task%20models%20for%20scalable%2C%0Ageneralizable%20solutions.%20To%20advance%20research%2C%20we%20propose%20a%20standardized%0Abenchmark%20for%20evaluating%20models%20across%20diverse%20datasets%20to%20enhance%0Areproducibility.%20This%20survey%20emphasizes%20how%20recent%20innovations%20can%20transform%0Aneurological%20diagnostics%20and%20enable%20the%20development%20of%20intelligent%2C%20adaptable%0Ahealthcare%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17213v1&entry.124074799=Read"},
{"title": "Stable-SPAM: How to Train in 4-Bit More Stably than 16-Bit Adam", "author": "Tianjin Huang and Haotian Hu and Zhenyu Zhang and Gaojie Jin and Xiang Li and Li Shen and Tianlong Chen and Lu Liu and Qingsong Wen and Zhangyang Wang and Shiwei Liu", "abstract": "  This paper comprehensively evaluates several recently proposed optimizers for\n4-bit training, revealing that low-bit precision amplifies sensitivity to\nlearning rates and often causes unstable gradient norms, leading to divergence\nat higher learning rates. Among these, SPAM, a recent optimizer featuring\nmomentum reset and spike-aware gradient clipping, achieves the best performance\nacross various bit levels, but struggles to stabilize gradient norms, requiring\ncareful learning rate tuning. To address these limitations, we propose\nStable-SPAM, which incorporates enhanced gradient normalization and clipping\ntechniques. In particular, Stable-SPAM (1) adaptively updates the clipping\nthreshold for spiked gradients by tracking their historical maxima; (2)\nnormalizes the entire gradient matrix based on its historical $l_2$-norm\nstatistics; and $(3)$ inherits momentum reset from SPAM to periodically reset\nthe first and second moments of Adam, mitigating the accumulation of spiked\ngradients. Extensive experiments show that Stable-SPAM effectively stabilizes\ngradient norms in 4-bit LLM training, delivering superior performance compared\nto Adam and SPAM. Notably, our 4-bit LLaMA-1B model trained with Stable-SPAM\noutperforms the BF16 LLaMA-1B trained with Adam by up to $2$ perplexity.\nFurthermore, when both models are trained in 4-bit, Stable-SPAM achieves the\nsame loss as Adam while requiring only about half the training steps. Code is\navailable at https://github.com/TianjinYellow/StableSPAM.git.\n", "link": "http://arxiv.org/abs/2502.17055v1", "date": "2025-02-24", "relevancy": 2.4631, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5279}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4961}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable-SPAM%3A%20How%20to%20Train%20in%204-Bit%20More%20Stably%20than%2016-Bit%20Adam&body=Title%3A%20Stable-SPAM%3A%20How%20to%20Train%20in%204-Bit%20More%20Stably%20than%2016-Bit%20Adam%0AAuthor%3A%20Tianjin%20Huang%20and%20Haotian%20Hu%20and%20Zhenyu%20Zhang%20and%20Gaojie%20Jin%20and%20Xiang%20Li%20and%20Li%20Shen%20and%20Tianlong%20Chen%20and%20Lu%20Liu%20and%20Qingsong%20Wen%20and%20Zhangyang%20Wang%20and%20Shiwei%20Liu%0AAbstract%3A%20%20%20This%20paper%20comprehensively%20evaluates%20several%20recently%20proposed%20optimizers%20for%0A4-bit%20training%2C%20revealing%20that%20low-bit%20precision%20amplifies%20sensitivity%20to%0Alearning%20rates%20and%20often%20causes%20unstable%20gradient%20norms%2C%20leading%20to%20divergence%0Aat%20higher%20learning%20rates.%20Among%20these%2C%20SPAM%2C%20a%20recent%20optimizer%20featuring%0Amomentum%20reset%20and%20spike-aware%20gradient%20clipping%2C%20achieves%20the%20best%20performance%0Aacross%20various%20bit%20levels%2C%20but%20struggles%20to%20stabilize%20gradient%20norms%2C%20requiring%0Acareful%20learning%20rate%20tuning.%20To%20address%20these%20limitations%2C%20we%20propose%0AStable-SPAM%2C%20which%20incorporates%20enhanced%20gradient%20normalization%20and%20clipping%0Atechniques.%20In%20particular%2C%20Stable-SPAM%20%281%29%20adaptively%20updates%20the%20clipping%0Athreshold%20for%20spiked%20gradients%20by%20tracking%20their%20historical%20maxima%3B%20%282%29%0Anormalizes%20the%20entire%20gradient%20matrix%20based%20on%20its%20historical%20%24l_2%24-norm%0Astatistics%3B%20and%20%24%283%29%24%20inherits%20momentum%20reset%20from%20SPAM%20to%20periodically%20reset%0Athe%20first%20and%20second%20moments%20of%20Adam%2C%20mitigating%20the%20accumulation%20of%20spiked%0Agradients.%20Extensive%20experiments%20show%20that%20Stable-SPAM%20effectively%20stabilizes%0Agradient%20norms%20in%204-bit%20LLM%20training%2C%20delivering%20superior%20performance%20compared%0Ato%20Adam%20and%20SPAM.%20Notably%2C%20our%204-bit%20LLaMA-1B%20model%20trained%20with%20Stable-SPAM%0Aoutperforms%20the%20BF16%20LLaMA-1B%20trained%20with%20Adam%20by%20up%20to%20%242%24%20perplexity.%0AFurthermore%2C%20when%20both%20models%20are%20trained%20in%204-bit%2C%20Stable-SPAM%20achieves%20the%0Asame%20loss%20as%20Adam%20while%20requiring%20only%20about%20half%20the%20training%20steps.%20Code%20is%0Aavailable%20at%20https%3A//github.com/TianjinYellow/StableSPAM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable-SPAM%253A%2520How%2520to%2520Train%2520in%25204-Bit%2520More%2520Stably%2520than%252016-Bit%2520Adam%26entry.906535625%3DTianjin%2520Huang%2520and%2520Haotian%2520Hu%2520and%2520Zhenyu%2520Zhang%2520and%2520Gaojie%2520Jin%2520and%2520Xiang%2520Li%2520and%2520Li%2520Shen%2520and%2520Tianlong%2520Chen%2520and%2520Lu%2520Liu%2520and%2520Qingsong%2520Wen%2520and%2520Zhangyang%2520Wang%2520and%2520Shiwei%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520comprehensively%2520evaluates%2520several%2520recently%2520proposed%2520optimizers%2520for%250A4-bit%2520training%252C%2520revealing%2520that%2520low-bit%2520precision%2520amplifies%2520sensitivity%2520to%250Alearning%2520rates%2520and%2520often%2520causes%2520unstable%2520gradient%2520norms%252C%2520leading%2520to%2520divergence%250Aat%2520higher%2520learning%2520rates.%2520Among%2520these%252C%2520SPAM%252C%2520a%2520recent%2520optimizer%2520featuring%250Amomentum%2520reset%2520and%2520spike-aware%2520gradient%2520clipping%252C%2520achieves%2520the%2520best%2520performance%250Aacross%2520various%2520bit%2520levels%252C%2520but%2520struggles%2520to%2520stabilize%2520gradient%2520norms%252C%2520requiring%250Acareful%2520learning%2520rate%2520tuning.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AStable-SPAM%252C%2520which%2520incorporates%2520enhanced%2520gradient%2520normalization%2520and%2520clipping%250Atechniques.%2520In%2520particular%252C%2520Stable-SPAM%2520%25281%2529%2520adaptively%2520updates%2520the%2520clipping%250Athreshold%2520for%2520spiked%2520gradients%2520by%2520tracking%2520their%2520historical%2520maxima%253B%2520%25282%2529%250Anormalizes%2520the%2520entire%2520gradient%2520matrix%2520based%2520on%2520its%2520historical%2520%2524l_2%2524-norm%250Astatistics%253B%2520and%2520%2524%25283%2529%2524%2520inherits%2520momentum%2520reset%2520from%2520SPAM%2520to%2520periodically%2520reset%250Athe%2520first%2520and%2520second%2520moments%2520of%2520Adam%252C%2520mitigating%2520the%2520accumulation%2520of%2520spiked%250Agradients.%2520Extensive%2520experiments%2520show%2520that%2520Stable-SPAM%2520effectively%2520stabilizes%250Agradient%2520norms%2520in%25204-bit%2520LLM%2520training%252C%2520delivering%2520superior%2520performance%2520compared%250Ato%2520Adam%2520and%2520SPAM.%2520Notably%252C%2520our%25204-bit%2520LLaMA-1B%2520model%2520trained%2520with%2520Stable-SPAM%250Aoutperforms%2520the%2520BF16%2520LLaMA-1B%2520trained%2520with%2520Adam%2520by%2520up%2520to%2520%25242%2524%2520perplexity.%250AFurthermore%252C%2520when%2520both%2520models%2520are%2520trained%2520in%25204-bit%252C%2520Stable-SPAM%2520achieves%2520the%250Asame%2520loss%2520as%2520Adam%2520while%2520requiring%2520only%2520about%2520half%2520the%2520training%2520steps.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/TianjinYellow/StableSPAM.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable-SPAM%3A%20How%20to%20Train%20in%204-Bit%20More%20Stably%20than%2016-Bit%20Adam&entry.906535625=Tianjin%20Huang%20and%20Haotian%20Hu%20and%20Zhenyu%20Zhang%20and%20Gaojie%20Jin%20and%20Xiang%20Li%20and%20Li%20Shen%20and%20Tianlong%20Chen%20and%20Lu%20Liu%20and%20Qingsong%20Wen%20and%20Zhangyang%20Wang%20and%20Shiwei%20Liu&entry.1292438233=%20%20This%20paper%20comprehensively%20evaluates%20several%20recently%20proposed%20optimizers%20for%0A4-bit%20training%2C%20revealing%20that%20low-bit%20precision%20amplifies%20sensitivity%20to%0Alearning%20rates%20and%20often%20causes%20unstable%20gradient%20norms%2C%20leading%20to%20divergence%0Aat%20higher%20learning%20rates.%20Among%20these%2C%20SPAM%2C%20a%20recent%20optimizer%20featuring%0Amomentum%20reset%20and%20spike-aware%20gradient%20clipping%2C%20achieves%20the%20best%20performance%0Aacross%20various%20bit%20levels%2C%20but%20struggles%20to%20stabilize%20gradient%20norms%2C%20requiring%0Acareful%20learning%20rate%20tuning.%20To%20address%20these%20limitations%2C%20we%20propose%0AStable-SPAM%2C%20which%20incorporates%20enhanced%20gradient%20normalization%20and%20clipping%0Atechniques.%20In%20particular%2C%20Stable-SPAM%20%281%29%20adaptively%20updates%20the%20clipping%0Athreshold%20for%20spiked%20gradients%20by%20tracking%20their%20historical%20maxima%3B%20%282%29%0Anormalizes%20the%20entire%20gradient%20matrix%20based%20on%20its%20historical%20%24l_2%24-norm%0Astatistics%3B%20and%20%24%283%29%24%20inherits%20momentum%20reset%20from%20SPAM%20to%20periodically%20reset%0Athe%20first%20and%20second%20moments%20of%20Adam%2C%20mitigating%20the%20accumulation%20of%20spiked%0Agradients.%20Extensive%20experiments%20show%20that%20Stable-SPAM%20effectively%20stabilizes%0Agradient%20norms%20in%204-bit%20LLM%20training%2C%20delivering%20superior%20performance%20compared%0Ato%20Adam%20and%20SPAM.%20Notably%2C%20our%204-bit%20LLaMA-1B%20model%20trained%20with%20Stable-SPAM%0Aoutperforms%20the%20BF16%20LLaMA-1B%20trained%20with%20Adam%20by%20up%20to%20%242%24%20perplexity.%0AFurthermore%2C%20when%20both%20models%20are%20trained%20in%204-bit%2C%20Stable-SPAM%20achieves%20the%0Asame%20loss%20as%20Adam%20while%20requiring%20only%20about%20half%20the%20training%20steps.%20Code%20is%0Aavailable%20at%20https%3A//github.com/TianjinYellow/StableSPAM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17055v1&entry.124074799=Read"},
{"title": "Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization", "author": "Zixuan Gong and Xiaolin Hu and Huayi Tang and Yong Liu", "abstract": "  Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.\n", "link": "http://arxiv.org/abs/2502.17024v1", "date": "2025-02-24", "relevancy": 2.4576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4944}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Auto-Regressive%20Next-Token%20Prediction%3A%20In-Context%20Learning%0A%20%20Emerges%20from%20Generalization&body=Title%3A%20Towards%20Auto-Regressive%20Next-Token%20Prediction%3A%20In-Context%20Learning%0A%20%20Emerges%20from%20Generalization%0AAuthor%3A%20Zixuan%20Gong%20and%20Xiaolin%20Hu%20and%20Huayi%20Tang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20in-context%20learning%0A%28ICL%29%20abilities.%20However%2C%20existing%20theoretical%20analysis%20of%20ICL%20primarily%0Aexhibits%20two%20limitations%3A%20%28a%29%20Limited%20i.i.d.%20Setting.%20Most%20studies%20focus%20on%0Asupervised%20function%20learning%20tasks%20where%20prompts%20are%20constructed%20with%20i.i.d.%0Ainput-label%20pairs.%20This%20i.i.d.%20assumption%20diverges%20significantly%20from%20real%0Alanguage%20learning%20scenarios%20where%20prompt%20tokens%20are%20interdependent.%20%28b%29%20Lack%20of%0AEmergence%20Explanation.%20Most%20literature%20answers%20what%20ICL%20does%20from%20an%20implicit%0Aoptimization%20perspective%20but%20falls%20short%20in%20elucidating%20how%20ICL%20emerges%20and%20the%0Aimpact%20of%20pre-training%20phase%20on%20ICL.%20In%20our%20paper%2C%20to%20extend%20%28a%29%2C%20we%20adopt%20a%0Amore%20practical%20paradigm%2C%20auto-regressive%20next-token%20prediction%20%28AR-NTP%29%2C%20which%0Aclosely%20aligns%20with%20the%20actual%20training%20of%20language%20models.%20Specifically%2C%0Awithin%20AR-NTP%2C%20we%20emphasize%20prompt%20token-dependency%2C%20which%20involves%20predicting%0Aeach%20subsequent%20token%20based%20on%20the%20preceding%20sequence.%20To%20address%20%28b%29%2C%20we%0Aformalize%20a%20systematic%20pre-training%20and%20ICL%20framework%2C%20highlighting%20the%0Alayer-wise%20structure%20of%20sequences%20and%20topics%2C%20alongside%20a%20two-level%0Aexpectation.%20In%20conclusion%2C%20we%20present%20data-dependent%2C%20topic-dependent%20and%0Aoptimization-dependent%20PAC-Bayesian%20generalization%20bounds%20for%20pre-trained%20LLMs%2C%0Ainvestigating%20that%20ICL%20emerges%20from%20the%20generalization%20of%20sequences%20and%20topics.%0AOur%20theory%20is%20supported%20by%20experiments%20on%20numerical%20linear%20dynamic%20systems%2C%0Asynthetic%20GINC%20and%20real-world%20language%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Auto-Regressive%2520Next-Token%2520Prediction%253A%2520In-Context%2520Learning%250A%2520%2520Emerges%2520from%2520Generalization%26entry.906535625%3DZixuan%2520Gong%2520and%2520Xiaolin%2520Hu%2520and%2520Huayi%2520Tang%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520in-context%2520learning%250A%2528ICL%2529%2520abilities.%2520However%252C%2520existing%2520theoretical%2520analysis%2520of%2520ICL%2520primarily%250Aexhibits%2520two%2520limitations%253A%2520%2528a%2529%2520Limited%2520i.i.d.%2520Setting.%2520Most%2520studies%2520focus%2520on%250Asupervised%2520function%2520learning%2520tasks%2520where%2520prompts%2520are%2520constructed%2520with%2520i.i.d.%250Ainput-label%2520pairs.%2520This%2520i.i.d.%2520assumption%2520diverges%2520significantly%2520from%2520real%250Alanguage%2520learning%2520scenarios%2520where%2520prompt%2520tokens%2520are%2520interdependent.%2520%2528b%2529%2520Lack%2520of%250AEmergence%2520Explanation.%2520Most%2520literature%2520answers%2520what%2520ICL%2520does%2520from%2520an%2520implicit%250Aoptimization%2520perspective%2520but%2520falls%2520short%2520in%2520elucidating%2520how%2520ICL%2520emerges%2520and%2520the%250Aimpact%2520of%2520pre-training%2520phase%2520on%2520ICL.%2520In%2520our%2520paper%252C%2520to%2520extend%2520%2528a%2529%252C%2520we%2520adopt%2520a%250Amore%2520practical%2520paradigm%252C%2520auto-regressive%2520next-token%2520prediction%2520%2528AR-NTP%2529%252C%2520which%250Aclosely%2520aligns%2520with%2520the%2520actual%2520training%2520of%2520language%2520models.%2520Specifically%252C%250Awithin%2520AR-NTP%252C%2520we%2520emphasize%2520prompt%2520token-dependency%252C%2520which%2520involves%2520predicting%250Aeach%2520subsequent%2520token%2520based%2520on%2520the%2520preceding%2520sequence.%2520To%2520address%2520%2528b%2529%252C%2520we%250Aformalize%2520a%2520systematic%2520pre-training%2520and%2520ICL%2520framework%252C%2520highlighting%2520the%250Alayer-wise%2520structure%2520of%2520sequences%2520and%2520topics%252C%2520alongside%2520a%2520two-level%250Aexpectation.%2520In%2520conclusion%252C%2520we%2520present%2520data-dependent%252C%2520topic-dependent%2520and%250Aoptimization-dependent%2520PAC-Bayesian%2520generalization%2520bounds%2520for%2520pre-trained%2520LLMs%252C%250Ainvestigating%2520that%2520ICL%2520emerges%2520from%2520the%2520generalization%2520of%2520sequences%2520and%2520topics.%250AOur%2520theory%2520is%2520supported%2520by%2520experiments%2520on%2520numerical%2520linear%2520dynamic%2520systems%252C%250Asynthetic%2520GINC%2520and%2520real-world%2520language%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Auto-Regressive%20Next-Token%20Prediction%3A%20In-Context%20Learning%0A%20%20Emerges%20from%20Generalization&entry.906535625=Zixuan%20Gong%20and%20Xiaolin%20Hu%20and%20Huayi%20Tang%20and%20Yong%20Liu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20in-context%20learning%0A%28ICL%29%20abilities.%20However%2C%20existing%20theoretical%20analysis%20of%20ICL%20primarily%0Aexhibits%20two%20limitations%3A%20%28a%29%20Limited%20i.i.d.%20Setting.%20Most%20studies%20focus%20on%0Asupervised%20function%20learning%20tasks%20where%20prompts%20are%20constructed%20with%20i.i.d.%0Ainput-label%20pairs.%20This%20i.i.d.%20assumption%20diverges%20significantly%20from%20real%0Alanguage%20learning%20scenarios%20where%20prompt%20tokens%20are%20interdependent.%20%28b%29%20Lack%20of%0AEmergence%20Explanation.%20Most%20literature%20answers%20what%20ICL%20does%20from%20an%20implicit%0Aoptimization%20perspective%20but%20falls%20short%20in%20elucidating%20how%20ICL%20emerges%20and%20the%0Aimpact%20of%20pre-training%20phase%20on%20ICL.%20In%20our%20paper%2C%20to%20extend%20%28a%29%2C%20we%20adopt%20a%0Amore%20practical%20paradigm%2C%20auto-regressive%20next-token%20prediction%20%28AR-NTP%29%2C%20which%0Aclosely%20aligns%20with%20the%20actual%20training%20of%20language%20models.%20Specifically%2C%0Awithin%20AR-NTP%2C%20we%20emphasize%20prompt%20token-dependency%2C%20which%20involves%20predicting%0Aeach%20subsequent%20token%20based%20on%20the%20preceding%20sequence.%20To%20address%20%28b%29%2C%20we%0Aformalize%20a%20systematic%20pre-training%20and%20ICL%20framework%2C%20highlighting%20the%0Alayer-wise%20structure%20of%20sequences%20and%20topics%2C%20alongside%20a%20two-level%0Aexpectation.%20In%20conclusion%2C%20we%20present%20data-dependent%2C%20topic-dependent%20and%0Aoptimization-dependent%20PAC-Bayesian%20generalization%20bounds%20for%20pre-trained%20LLMs%2C%0Ainvestigating%20that%20ICL%20emerges%20from%20the%20generalization%20of%20sequences%20and%20topics.%0AOur%20theory%20is%20supported%20by%20experiments%20on%20numerical%20linear%20dynamic%20systems%2C%0Asynthetic%20GINC%20and%20real-world%20language%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17024v1&entry.124074799=Read"},
{"title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning", "author": "Zhen Li and Yupeng Su and Runming Yang and Congkai Xie and Zheng Wang and Zhongwei Xie and Ngai Wong and Hongxia Yang", "abstract": "  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n", "link": "http://arxiv.org/abs/2501.03035v4", "date": "2025-02-24", "relevancy": 2.4566, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5037}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&body=Title%3A%20Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning%0AAuthor%3A%20Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Congkai%20Xie%20and%20Zheng%20Wang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20Our%20results%20demonstrate%20that%20aggressive%0Aquantization%20methods%20like%20AWQ%20and%20GPTQ%20introduce%20up%20to%2032.39%25%20accuracy%0Adegradation%20%28average%2011.31%25%29%20on%20Llama-3%20models%2C%20particularly%20in%20numerical%0Acomputation%20and%20reasoning%20planning.%20To%20address%20this%2C%20we%20introduce%20a%0Amultidimensional%20evaluation%20framework%20combining%20qualitative%20capability%20analysis%0Aand%20quantitative%20error%20assessment.%20We%20further%20develop%20targeted%20recovery%0Astrategies%2C%20showing%20that%20fine-tuning%20quantized%20models%20on%20only%20545%20task-specific%0Aexamples%20for%203%20minutes%20on%204%20GPUs%20effectively%20restores%20reasoning%20capabilities%20to%0Anear%20full-precision%20levels.%20Additionally%2C%20our%20error%20assessment%20pipeline%0Aachieves%2098.9%25%20accuracy%20in%20diagnosing%20and%20localizing%20errors%20across%203%2C366%0Afailure%20cases%2C%20providing%20actionable%20insights%20for%20mitigating%0Aquantization-induced%20degradation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03035v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantization%2520Meets%2520Reasoning%253A%2520Exploring%2520LLM%2520Low-Bit%2520Quantization%250A%2520%2520Degradation%2520for%2520Mathematical%2520Reasoning%26entry.906535625%3DZhen%2520Li%2520and%2520Yupeng%2520Su%2520and%2520Runming%2520Yang%2520and%2520Congkai%2520Xie%2520and%2520Zheng%2520Wang%2520and%2520Zhongwei%2520Xie%2520and%2520Ngai%2520Wong%2520and%2520Hongxia%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520achieved%2520significant%2520advancements%2520in%2520complex%250Amathematical%2520reasoning%2520benchmarks%252C%2520such%2520as%2520MATH.%2520However%252C%2520their%2520substantial%250Acomputational%2520requirements%2520present%2520challenges%2520for%2520practical%2520deployment.%2520Model%250Aquantization%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520to%2520reduce%2520memory%2520usage%2520and%250Acomputational%2520costs%2520by%2520employing%2520lower%2520precision%2520and%2520bit-width%2520representations.%250AIn%2520this%2520study%252C%2520we%2520systematically%2520evaluate%2520the%2520impact%2520of%2520quantization%2520on%250Amathematical%2520reasoning%2520tasks.%2520Our%2520results%2520demonstrate%2520that%2520aggressive%250Aquantization%2520methods%2520like%2520AWQ%2520and%2520GPTQ%2520introduce%2520up%2520to%252032.39%2525%2520accuracy%250Adegradation%2520%2528average%252011.31%2525%2529%2520on%2520Llama-3%2520models%252C%2520particularly%2520in%2520numerical%250Acomputation%2520and%2520reasoning%2520planning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%250Amultidimensional%2520evaluation%2520framework%2520combining%2520qualitative%2520capability%2520analysis%250Aand%2520quantitative%2520error%2520assessment.%2520We%2520further%2520develop%2520targeted%2520recovery%250Astrategies%252C%2520showing%2520that%2520fine-tuning%2520quantized%2520models%2520on%2520only%2520545%2520task-specific%250Aexamples%2520for%25203%2520minutes%2520on%25204%2520GPUs%2520effectively%2520restores%2520reasoning%2520capabilities%2520to%250Anear%2520full-precision%2520levels.%2520Additionally%252C%2520our%2520error%2520assessment%2520pipeline%250Aachieves%252098.9%2525%2520accuracy%2520in%2520diagnosing%2520and%2520localizing%2520errors%2520across%25203%252C366%250Afailure%2520cases%252C%2520providing%2520actionable%2520insights%2520for%2520mitigating%250Aquantization-induced%2520degradation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03035v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantization%20Meets%20Reasoning%3A%20Exploring%20LLM%20Low-Bit%20Quantization%0A%20%20Degradation%20for%20Mathematical%20Reasoning&entry.906535625=Zhen%20Li%20and%20Yupeng%20Su%20and%20Runming%20Yang%20and%20Congkai%20Xie%20and%20Zheng%20Wang%20and%20Zhongwei%20Xie%20and%20Ngai%20Wong%20and%20Hongxia%20Yang&entry.1292438233=%20%20Large%20language%20models%20have%20achieved%20significant%20advancements%20in%20complex%0Amathematical%20reasoning%20benchmarks%2C%20such%20as%20MATH.%20However%2C%20their%20substantial%0Acomputational%20requirements%20present%20challenges%20for%20practical%20deployment.%20Model%0Aquantization%20has%20emerged%20as%20an%20effective%20strategy%20to%20reduce%20memory%20usage%20and%0Acomputational%20costs%20by%20employing%20lower%20precision%20and%20bit-width%20representations.%0AIn%20this%20study%2C%20we%20systematically%20evaluate%20the%20impact%20of%20quantization%20on%0Amathematical%20reasoning%20tasks.%20Our%20results%20demonstrate%20that%20aggressive%0Aquantization%20methods%20like%20AWQ%20and%20GPTQ%20introduce%20up%20to%2032.39%25%20accuracy%0Adegradation%20%28average%2011.31%25%29%20on%20Llama-3%20models%2C%20particularly%20in%20numerical%0Acomputation%20and%20reasoning%20planning.%20To%20address%20this%2C%20we%20introduce%20a%0Amultidimensional%20evaluation%20framework%20combining%20qualitative%20capability%20analysis%0Aand%20quantitative%20error%20assessment.%20We%20further%20develop%20targeted%20recovery%0Astrategies%2C%20showing%20that%20fine-tuning%20quantized%20models%20on%20only%20545%20task-specific%0Aexamples%20for%203%20minutes%20on%204%20GPUs%20effectively%20restores%20reasoning%20capabilities%20to%0Anear%20full-precision%20levels.%20Additionally%2C%20our%20error%20assessment%20pipeline%0Aachieves%2098.9%25%20accuracy%20in%20diagnosing%20and%20localizing%20errors%20across%203%2C366%0Afailure%20cases%2C%20providing%20actionable%20insights%20for%20mitigating%0Aquantization-induced%20degradation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03035v4&entry.124074799=Read"},
{"title": "Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?", "author": "Uli Sauerland and Celia Matthaei and Felix Salfner", "abstract": "  We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance.\n", "link": "http://arxiv.org/abs/2502.17304v1", "date": "2025-02-24", "relevancy": 2.4565, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5036}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Child%20vs.%20machine%20language%20learning%3A%20Can%20the%20logical%20structure%20of%20human%0A%20%20language%20unleash%20LLMs%3F&body=Title%3A%20Child%20vs.%20machine%20language%20learning%3A%20Can%20the%20logical%20structure%20of%20human%0A%20%20language%20unleash%20LLMs%3F%0AAuthor%3A%20Uli%20Sauerland%20and%20Celia%20Matthaei%20and%20Felix%20Salfner%0AAbstract%3A%20%20%20We%20argue%20that%20human%20language%20learning%20proceeds%20in%20a%20manner%20that%20is%20different%0Ain%20nature%20from%20current%20approaches%20to%20training%20LLMs%2C%20predicting%20a%20difference%20in%0Alearning%20biases.%20We%20then%20present%20evidence%20from%20German%20plural%20formation%20by%20LLMs%0Athat%20confirm%20our%20hypothesis%20that%20even%20very%20powerful%20implementations%20produce%0Aresults%20that%20miss%20aspects%20of%20the%20logic%20inherent%20to%20language%20that%20humans%20have%20no%0Aproblem%20with.%20We%20conclude%20that%20attention%20to%20the%20different%20structures%20of%20human%0Alanguage%20and%20artificial%20neural%20networks%20is%20likely%20to%20be%20an%20avenue%20to%20improve%0ALLM%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChild%2520vs.%2520machine%2520language%2520learning%253A%2520Can%2520the%2520logical%2520structure%2520of%2520human%250A%2520%2520language%2520unleash%2520LLMs%253F%26entry.906535625%3DUli%2520Sauerland%2520and%2520Celia%2520Matthaei%2520and%2520Felix%2520Salfner%26entry.1292438233%3D%2520%2520We%2520argue%2520that%2520human%2520language%2520learning%2520proceeds%2520in%2520a%2520manner%2520that%2520is%2520different%250Ain%2520nature%2520from%2520current%2520approaches%2520to%2520training%2520LLMs%252C%2520predicting%2520a%2520difference%2520in%250Alearning%2520biases.%2520We%2520then%2520present%2520evidence%2520from%2520German%2520plural%2520formation%2520by%2520LLMs%250Athat%2520confirm%2520our%2520hypothesis%2520that%2520even%2520very%2520powerful%2520implementations%2520produce%250Aresults%2520that%2520miss%2520aspects%2520of%2520the%2520logic%2520inherent%2520to%2520language%2520that%2520humans%2520have%2520no%250Aproblem%2520with.%2520We%2520conclude%2520that%2520attention%2520to%2520the%2520different%2520structures%2520of%2520human%250Alanguage%2520and%2520artificial%2520neural%2520networks%2520is%2520likely%2520to%2520be%2520an%2520avenue%2520to%2520improve%250ALLM%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Child%20vs.%20machine%20language%20learning%3A%20Can%20the%20logical%20structure%20of%20human%0A%20%20language%20unleash%20LLMs%3F&entry.906535625=Uli%20Sauerland%20and%20Celia%20Matthaei%20and%20Felix%20Salfner&entry.1292438233=%20%20We%20argue%20that%20human%20language%20learning%20proceeds%20in%20a%20manner%20that%20is%20different%0Ain%20nature%20from%20current%20approaches%20to%20training%20LLMs%2C%20predicting%20a%20difference%20in%0Alearning%20biases.%20We%20then%20present%20evidence%20from%20German%20plural%20formation%20by%20LLMs%0Athat%20confirm%20our%20hypothesis%20that%20even%20very%20powerful%20implementations%20produce%0Aresults%20that%20miss%20aspects%20of%20the%20logic%20inherent%20to%20language%20that%20humans%20have%20no%0Aproblem%20with.%20We%20conclude%20that%20attention%20to%20the%20different%20structures%20of%20human%0Alanguage%20and%20artificial%20neural%20networks%20is%20likely%20to%20be%20an%20avenue%20to%20improve%0ALLM%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17304v1&entry.124074799=Read"},
{"title": "Learning to Reason at the Frontier of Learnability", "author": "Thomas Foster and Jakob Foerster", "abstract": "  Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.\n", "link": "http://arxiv.org/abs/2502.12272v3", "date": "2025-02-24", "relevancy": 2.4545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Reason%20at%20the%20Frontier%20of%20Learnability&body=Title%3A%20Learning%20to%20Reason%20at%20the%20Frontier%20of%20Learnability%0AAuthor%3A%20Thomas%20Foster%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Reinforcement%20learning%20is%20now%20widely%20adopted%20as%20the%20final%20stage%20of%20large%0Alanguage%20model%20training%2C%20especially%20for%20reasoning-style%20tasks%20such%20as%20maths%0Aproblems.%20Typically%2C%20models%20attempt%20each%20question%20many%20times%20during%20a%20single%0Atraining%20step%20and%20attempt%20to%20learn%20from%20their%20successes%20and%20failures.%20However%2C%0Awe%20demonstrate%20that%20throughout%20training%20with%20two%20popular%20algorithms%20%28PPO%20and%0AVinePPO%29%20on%20two%20widely%20used%20datasets%2C%20many%20questions%20are%20either%20solved%20by%20all%0Aattempts%20-%20meaning%20they%20are%20already%20learned%20-%20or%20by%20none%20-%20providing%20no%0Ameaningful%20training%20signal.%20To%20address%20this%2C%20we%20adapt%20a%20method%20from%20the%0Areinforcement%20learning%20literature%20-%20sampling%20for%20learnability%20-%20and%20apply%20it%20to%0Athe%20reinforcement%20learning%20stage%20of%20LLM%20training.%20Our%20curriculum%20prioritises%0Aquestions%20with%20high%20variance%20of%20success%2C%20i.e.%20those%20where%20the%20agent%20sometimes%0Asucceeds%2C%20but%20not%20always.%20Our%20findings%20demonstrate%20that%20this%20curriculum%0Aconsistently%20boosts%20training%20performance%20across%20multiple%20algorithms%20and%0Adatasets%2C%20paving%20the%20way%20for%20more%20efficient%20and%20effective%20reinforcement%0Alearning%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12272v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Reason%2520at%2520the%2520Frontier%2520of%2520Learnability%26entry.906535625%3DThomas%2520Foster%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520is%2520now%2520widely%2520adopted%2520as%2520the%2520final%2520stage%2520of%2520large%250Alanguage%2520model%2520training%252C%2520especially%2520for%2520reasoning-style%2520tasks%2520such%2520as%2520maths%250Aproblems.%2520Typically%252C%2520models%2520attempt%2520each%2520question%2520many%2520times%2520during%2520a%2520single%250Atraining%2520step%2520and%2520attempt%2520to%2520learn%2520from%2520their%2520successes%2520and%2520failures.%2520However%252C%250Awe%2520demonstrate%2520that%2520throughout%2520training%2520with%2520two%2520popular%2520algorithms%2520%2528PPO%2520and%250AVinePPO%2529%2520on%2520two%2520widely%2520used%2520datasets%252C%2520many%2520questions%2520are%2520either%2520solved%2520by%2520all%250Aattempts%2520-%2520meaning%2520they%2520are%2520already%2520learned%2520-%2520or%2520by%2520none%2520-%2520providing%2520no%250Ameaningful%2520training%2520signal.%2520To%2520address%2520this%252C%2520we%2520adapt%2520a%2520method%2520from%2520the%250Areinforcement%2520learning%2520literature%2520-%2520sampling%2520for%2520learnability%2520-%2520and%2520apply%2520it%2520to%250Athe%2520reinforcement%2520learning%2520stage%2520of%2520LLM%2520training.%2520Our%2520curriculum%2520prioritises%250Aquestions%2520with%2520high%2520variance%2520of%2520success%252C%2520i.e.%2520those%2520where%2520the%2520agent%2520sometimes%250Asucceeds%252C%2520but%2520not%2520always.%2520Our%2520findings%2520demonstrate%2520that%2520this%2520curriculum%250Aconsistently%2520boosts%2520training%2520performance%2520across%2520multiple%2520algorithms%2520and%250Adatasets%252C%2520paving%2520the%2520way%2520for%2520more%2520efficient%2520and%2520effective%2520reinforcement%250Alearning%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12272v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Reason%20at%20the%20Frontier%20of%20Learnability&entry.906535625=Thomas%20Foster%20and%20Jakob%20Foerster&entry.1292438233=%20%20Reinforcement%20learning%20is%20now%20widely%20adopted%20as%20the%20final%20stage%20of%20large%0Alanguage%20model%20training%2C%20especially%20for%20reasoning-style%20tasks%20such%20as%20maths%0Aproblems.%20Typically%2C%20models%20attempt%20each%20question%20many%20times%20during%20a%20single%0Atraining%20step%20and%20attempt%20to%20learn%20from%20their%20successes%20and%20failures.%20However%2C%0Awe%20demonstrate%20that%20throughout%20training%20with%20two%20popular%20algorithms%20%28PPO%20and%0AVinePPO%29%20on%20two%20widely%20used%20datasets%2C%20many%20questions%20are%20either%20solved%20by%20all%0Aattempts%20-%20meaning%20they%20are%20already%20learned%20-%20or%20by%20none%20-%20providing%20no%0Ameaningful%20training%20signal.%20To%20address%20this%2C%20we%20adapt%20a%20method%20from%20the%0Areinforcement%20learning%20literature%20-%20sampling%20for%20learnability%20-%20and%20apply%20it%20to%0Athe%20reinforcement%20learning%20stage%20of%20LLM%20training.%20Our%20curriculum%20prioritises%0Aquestions%20with%20high%20variance%20of%20success%2C%20i.e.%20those%20where%20the%20agent%20sometimes%0Asucceeds%2C%20but%20not%20always.%20Our%20findings%20demonstrate%20that%20this%20curriculum%0Aconsistently%20boosts%20training%20performance%20across%20multiple%20algorithms%20and%0Adatasets%2C%20paving%20the%20way%20for%20more%20efficient%20and%20effective%20reinforcement%0Alearning%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12272v3&entry.124074799=Read"},
{"title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition\n  and Translation", "author": "Qiuming Zhao and Guangzhi Sun and Chao Zhang and Mingxing Xu and Thomas Fang Zheng", "abstract": "  Language diversity presents a significant challenge in speech-to-text (S2T)\ntasks, such as automatic speech recognition and translation. Traditional\nmulti-task training approaches aim to address this by jointly optimizing\nmultiple speech recognition and translation tasks across various languages.\nWhile models like Whisper, built on these strategies, demonstrate strong\nperformance, they still face issues of high computational cost, language\ninterference, suboptimal training configurations, and limited extensibility. To\novercome these challenges, we introduce LoRS-Merging (low-rank and sparse model\nmerging), a novel technique designed to efficiently integrate models trained on\ndifferent languages or tasks while preserving performance and reducing\ncomputational overhead. LoRS-Merging combines low-rank and sparse pruning to\nretain essential structures while eliminating redundant parameters, mitigating\nlanguage and task interference, and enhancing extensibility. Experimental\nresults across a range of languages demonstrate that LoRS-Merging significantly\noutperforms conventional multi-lingual multi-task training baselines. Our\nfindings suggest that model merging, particularly LoRS-Merging, is a scalable\nand effective complement to traditional multi-lingual training strategies for\nS2T applications.\n", "link": "http://arxiv.org/abs/2502.17380v1", "date": "2025-02-24", "relevancy": 2.4491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Rank%20and%20Sparse%20Model%20Merging%20for%20Multi-Lingual%20Speech%20Recognition%0A%20%20and%20Translation&body=Title%3A%20Low-Rank%20and%20Sparse%20Model%20Merging%20for%20Multi-Lingual%20Speech%20Recognition%0A%20%20and%20Translation%0AAuthor%3A%20Qiuming%20Zhao%20and%20Guangzhi%20Sun%20and%20Chao%20Zhang%20and%20Mingxing%20Xu%20and%20Thomas%20Fang%20Zheng%0AAbstract%3A%20%20%20Language%20diversity%20presents%20a%20significant%20challenge%20in%20speech-to-text%20%28S2T%29%0Atasks%2C%20such%20as%20automatic%20speech%20recognition%20and%20translation.%20Traditional%0Amulti-task%20training%20approaches%20aim%20to%20address%20this%20by%20jointly%20optimizing%0Amultiple%20speech%20recognition%20and%20translation%20tasks%20across%20various%20languages.%0AWhile%20models%20like%20Whisper%2C%20built%20on%20these%20strategies%2C%20demonstrate%20strong%0Aperformance%2C%20they%20still%20face%20issues%20of%20high%20computational%20cost%2C%20language%0Ainterference%2C%20suboptimal%20training%20configurations%2C%20and%20limited%20extensibility.%20To%0Aovercome%20these%20challenges%2C%20we%20introduce%20LoRS-Merging%20%28low-rank%20and%20sparse%20model%0Amerging%29%2C%20a%20novel%20technique%20designed%20to%20efficiently%20integrate%20models%20trained%20on%0Adifferent%20languages%20or%20tasks%20while%20preserving%20performance%20and%20reducing%0Acomputational%20overhead.%20LoRS-Merging%20combines%20low-rank%20and%20sparse%20pruning%20to%0Aretain%20essential%20structures%20while%20eliminating%20redundant%20parameters%2C%20mitigating%0Alanguage%20and%20task%20interference%2C%20and%20enhancing%20extensibility.%20Experimental%0Aresults%20across%20a%20range%20of%20languages%20demonstrate%20that%20LoRS-Merging%20significantly%0Aoutperforms%20conventional%20multi-lingual%20multi-task%20training%20baselines.%20Our%0Afindings%20suggest%20that%20model%20merging%2C%20particularly%20LoRS-Merging%2C%20is%20a%20scalable%0Aand%20effective%20complement%20to%20traditional%20multi-lingual%20training%20strategies%20for%0AS2T%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Rank%2520and%2520Sparse%2520Model%2520Merging%2520for%2520Multi-Lingual%2520Speech%2520Recognition%250A%2520%2520and%2520Translation%26entry.906535625%3DQiuming%2520Zhao%2520and%2520Guangzhi%2520Sun%2520and%2520Chao%2520Zhang%2520and%2520Mingxing%2520Xu%2520and%2520Thomas%2520Fang%2520Zheng%26entry.1292438233%3D%2520%2520Language%2520diversity%2520presents%2520a%2520significant%2520challenge%2520in%2520speech-to-text%2520%2528S2T%2529%250Atasks%252C%2520such%2520as%2520automatic%2520speech%2520recognition%2520and%2520translation.%2520Traditional%250Amulti-task%2520training%2520approaches%2520aim%2520to%2520address%2520this%2520by%2520jointly%2520optimizing%250Amultiple%2520speech%2520recognition%2520and%2520translation%2520tasks%2520across%2520various%2520languages.%250AWhile%2520models%2520like%2520Whisper%252C%2520built%2520on%2520these%2520strategies%252C%2520demonstrate%2520strong%250Aperformance%252C%2520they%2520still%2520face%2520issues%2520of%2520high%2520computational%2520cost%252C%2520language%250Ainterference%252C%2520suboptimal%2520training%2520configurations%252C%2520and%2520limited%2520extensibility.%2520To%250Aovercome%2520these%2520challenges%252C%2520we%2520introduce%2520LoRS-Merging%2520%2528low-rank%2520and%2520sparse%2520model%250Amerging%2529%252C%2520a%2520novel%2520technique%2520designed%2520to%2520efficiently%2520integrate%2520models%2520trained%2520on%250Adifferent%2520languages%2520or%2520tasks%2520while%2520preserving%2520performance%2520and%2520reducing%250Acomputational%2520overhead.%2520LoRS-Merging%2520combines%2520low-rank%2520and%2520sparse%2520pruning%2520to%250Aretain%2520essential%2520structures%2520while%2520eliminating%2520redundant%2520parameters%252C%2520mitigating%250Alanguage%2520and%2520task%2520interference%252C%2520and%2520enhancing%2520extensibility.%2520Experimental%250Aresults%2520across%2520a%2520range%2520of%2520languages%2520demonstrate%2520that%2520LoRS-Merging%2520significantly%250Aoutperforms%2520conventional%2520multi-lingual%2520multi-task%2520training%2520baselines.%2520Our%250Afindings%2520suggest%2520that%2520model%2520merging%252C%2520particularly%2520LoRS-Merging%252C%2520is%2520a%2520scalable%250Aand%2520effective%2520complement%2520to%2520traditional%2520multi-lingual%2520training%2520strategies%2520for%250AS2T%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Rank%20and%20Sparse%20Model%20Merging%20for%20Multi-Lingual%20Speech%20Recognition%0A%20%20and%20Translation&entry.906535625=Qiuming%20Zhao%20and%20Guangzhi%20Sun%20and%20Chao%20Zhang%20and%20Mingxing%20Xu%20and%20Thomas%20Fang%20Zheng&entry.1292438233=%20%20Language%20diversity%20presents%20a%20significant%20challenge%20in%20speech-to-text%20%28S2T%29%0Atasks%2C%20such%20as%20automatic%20speech%20recognition%20and%20translation.%20Traditional%0Amulti-task%20training%20approaches%20aim%20to%20address%20this%20by%20jointly%20optimizing%0Amultiple%20speech%20recognition%20and%20translation%20tasks%20across%20various%20languages.%0AWhile%20models%20like%20Whisper%2C%20built%20on%20these%20strategies%2C%20demonstrate%20strong%0Aperformance%2C%20they%20still%20face%20issues%20of%20high%20computational%20cost%2C%20language%0Ainterference%2C%20suboptimal%20training%20configurations%2C%20and%20limited%20extensibility.%20To%0Aovercome%20these%20challenges%2C%20we%20introduce%20LoRS-Merging%20%28low-rank%20and%20sparse%20model%0Amerging%29%2C%20a%20novel%20technique%20designed%20to%20efficiently%20integrate%20models%20trained%20on%0Adifferent%20languages%20or%20tasks%20while%20preserving%20performance%20and%20reducing%0Acomputational%20overhead.%20LoRS-Merging%20combines%20low-rank%20and%20sparse%20pruning%20to%0Aretain%20essential%20structures%20while%20eliminating%20redundant%20parameters%2C%20mitigating%0Alanguage%20and%20task%20interference%2C%20and%20enhancing%20extensibility.%20Experimental%0Aresults%20across%20a%20range%20of%20languages%20demonstrate%20that%20LoRS-Merging%20significantly%0Aoutperforms%20conventional%20multi-lingual%20multi-task%20training%20baselines.%20Our%0Afindings%20suggest%20that%20model%20merging%2C%20particularly%20LoRS-Merging%2C%20is%20a%20scalable%0Aand%20effective%20complement%20to%20traditional%20multi-lingual%20training%20strategies%20for%0AS2T%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17380v1&entry.124074799=Read"},
{"title": "Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine\n  Perception", "author": "Martin Feick and Xuxin Tang and Raul Garcia-Martin and Alexandru Luchianov and Roderick Wei Xiao Huang and Chang Xiao and Alexa Siu and Mustafa Doga Dogan", "abstract": "  Hybrid paper interfaces leverage augmented reality to combine the desired\ntangibility of paper documents with the affordances of interactive digital\nmedia. Typically, virtual content can be embedded through direct links (e.g.,\nQR codes); however, this impacts the aesthetics of the paper print and limits\nthe available visual content space. To address this problem, we present\nImprinto, an infrared inkjet watermarking technique that allows for invisible\ncontent embeddings only by using off-the-shelf IR inks and a camera. Imprinto\nwas established through a psychophysical experiment, studying how much IR ink\ncan be used while remaining invisible to users regardless of background color.\nWe demonstrate that we can detect invisible IR content through our machine\nlearning pipeline, and we developed an authoring tool that optimizes the amount\nof IR ink on the color regions of an input document for machine and human\ndetectability. Finally, we demonstrate several applications, including\naugmenting paper documents and objects.\n", "link": "http://arxiv.org/abs/2502.17089v1", "date": "2025-02-24", "relevancy": 2.4404, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5052}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4921}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imprinto%3A%20Enhancing%20Infrared%20Inkjet%20Watermarking%20for%20Human%20and%20Machine%0A%20%20Perception&body=Title%3A%20Imprinto%3A%20Enhancing%20Infrared%20Inkjet%20Watermarking%20for%20Human%20and%20Machine%0A%20%20Perception%0AAuthor%3A%20Martin%20Feick%20and%20Xuxin%20Tang%20and%20Raul%20Garcia-Martin%20and%20Alexandru%20Luchianov%20and%20Roderick%20Wei%20Xiao%20Huang%20and%20Chang%20Xiao%20and%20Alexa%20Siu%20and%20Mustafa%20Doga%20Dogan%0AAbstract%3A%20%20%20Hybrid%20paper%20interfaces%20leverage%20augmented%20reality%20to%20combine%20the%20desired%0Atangibility%20of%20paper%20documents%20with%20the%20affordances%20of%20interactive%20digital%0Amedia.%20Typically%2C%20virtual%20content%20can%20be%20embedded%20through%20direct%20links%20%28e.g.%2C%0AQR%20codes%29%3B%20however%2C%20this%20impacts%20the%20aesthetics%20of%20the%20paper%20print%20and%20limits%0Athe%20available%20visual%20content%20space.%20To%20address%20this%20problem%2C%20we%20present%0AImprinto%2C%20an%20infrared%20inkjet%20watermarking%20technique%20that%20allows%20for%20invisible%0Acontent%20embeddings%20only%20by%20using%20off-the-shelf%20IR%20inks%20and%20a%20camera.%20Imprinto%0Awas%20established%20through%20a%20psychophysical%20experiment%2C%20studying%20how%20much%20IR%20ink%0Acan%20be%20used%20while%20remaining%20invisible%20to%20users%20regardless%20of%20background%20color.%0AWe%20demonstrate%20that%20we%20can%20detect%20invisible%20IR%20content%20through%20our%20machine%0Alearning%20pipeline%2C%20and%20we%20developed%20an%20authoring%20tool%20that%20optimizes%20the%20amount%0Aof%20IR%20ink%20on%20the%20color%20regions%20of%20an%20input%20document%20for%20machine%20and%20human%0Adetectability.%20Finally%2C%20we%20demonstrate%20several%20applications%2C%20including%0Aaugmenting%20paper%20documents%20and%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprinto%253A%2520Enhancing%2520Infrared%2520Inkjet%2520Watermarking%2520for%2520Human%2520and%2520Machine%250A%2520%2520Perception%26entry.906535625%3DMartin%2520Feick%2520and%2520Xuxin%2520Tang%2520and%2520Raul%2520Garcia-Martin%2520and%2520Alexandru%2520Luchianov%2520and%2520Roderick%2520Wei%2520Xiao%2520Huang%2520and%2520Chang%2520Xiao%2520and%2520Alexa%2520Siu%2520and%2520Mustafa%2520Doga%2520Dogan%26entry.1292438233%3D%2520%2520Hybrid%2520paper%2520interfaces%2520leverage%2520augmented%2520reality%2520to%2520combine%2520the%2520desired%250Atangibility%2520of%2520paper%2520documents%2520with%2520the%2520affordances%2520of%2520interactive%2520digital%250Amedia.%2520Typically%252C%2520virtual%2520content%2520can%2520be%2520embedded%2520through%2520direct%2520links%2520%2528e.g.%252C%250AQR%2520codes%2529%253B%2520however%252C%2520this%2520impacts%2520the%2520aesthetics%2520of%2520the%2520paper%2520print%2520and%2520limits%250Athe%2520available%2520visual%2520content%2520space.%2520To%2520address%2520this%2520problem%252C%2520we%2520present%250AImprinto%252C%2520an%2520infrared%2520inkjet%2520watermarking%2520technique%2520that%2520allows%2520for%2520invisible%250Acontent%2520embeddings%2520only%2520by%2520using%2520off-the-shelf%2520IR%2520inks%2520and%2520a%2520camera.%2520Imprinto%250Awas%2520established%2520through%2520a%2520psychophysical%2520experiment%252C%2520studying%2520how%2520much%2520IR%2520ink%250Acan%2520be%2520used%2520while%2520remaining%2520invisible%2520to%2520users%2520regardless%2520of%2520background%2520color.%250AWe%2520demonstrate%2520that%2520we%2520can%2520detect%2520invisible%2520IR%2520content%2520through%2520our%2520machine%250Alearning%2520pipeline%252C%2520and%2520we%2520developed%2520an%2520authoring%2520tool%2520that%2520optimizes%2520the%2520amount%250Aof%2520IR%2520ink%2520on%2520the%2520color%2520regions%2520of%2520an%2520input%2520document%2520for%2520machine%2520and%2520human%250Adetectability.%2520Finally%252C%2520we%2520demonstrate%2520several%2520applications%252C%2520including%250Aaugmenting%2520paper%2520documents%2520and%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imprinto%3A%20Enhancing%20Infrared%20Inkjet%20Watermarking%20for%20Human%20and%20Machine%0A%20%20Perception&entry.906535625=Martin%20Feick%20and%20Xuxin%20Tang%20and%20Raul%20Garcia-Martin%20and%20Alexandru%20Luchianov%20and%20Roderick%20Wei%20Xiao%20Huang%20and%20Chang%20Xiao%20and%20Alexa%20Siu%20and%20Mustafa%20Doga%20Dogan&entry.1292438233=%20%20Hybrid%20paper%20interfaces%20leverage%20augmented%20reality%20to%20combine%20the%20desired%0Atangibility%20of%20paper%20documents%20with%20the%20affordances%20of%20interactive%20digital%0Amedia.%20Typically%2C%20virtual%20content%20can%20be%20embedded%20through%20direct%20links%20%28e.g.%2C%0AQR%20codes%29%3B%20however%2C%20this%20impacts%20the%20aesthetics%20of%20the%20paper%20print%20and%20limits%0Athe%20available%20visual%20content%20space.%20To%20address%20this%20problem%2C%20we%20present%0AImprinto%2C%20an%20infrared%20inkjet%20watermarking%20technique%20that%20allows%20for%20invisible%0Acontent%20embeddings%20only%20by%20using%20off-the-shelf%20IR%20inks%20and%20a%20camera.%20Imprinto%0Awas%20established%20through%20a%20psychophysical%20experiment%2C%20studying%20how%20much%20IR%20ink%0Acan%20be%20used%20while%20remaining%20invisible%20to%20users%20regardless%20of%20background%20color.%0AWe%20demonstrate%20that%20we%20can%20detect%20invisible%20IR%20content%20through%20our%20machine%0Alearning%20pipeline%2C%20and%20we%20developed%20an%20authoring%20tool%20that%20optimizes%20the%20amount%0Aof%20IR%20ink%20on%20the%20color%20regions%20of%20an%20input%20document%20for%20machine%20and%20human%0Adetectability.%20Finally%2C%20we%20demonstrate%20several%20applications%2C%20including%0Aaugmenting%20paper%20documents%20and%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17089v1&entry.124074799=Read"},
{"title": "PersonalLLM: Tailoring LLMs to Individual Preferences", "author": "Thomas P. Zollo and Andrew Wei Tung Siah and Naimeng Ye and Ang Li and Hongseok Namkoong", "abstract": "  As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM\n", "link": "http://arxiv.org/abs/2409.20296v2", "date": "2025-02-24", "relevancy": 2.4048, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4824}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PersonalLLM%3A%20Tailoring%20LLMs%20to%20Individual%20Preferences&body=Title%3A%20PersonalLLM%3A%20Tailoring%20LLMs%20to%20Individual%20Preferences%0AAuthor%3A%20Thomas%20P.%20Zollo%20and%20Andrew%20Wei%20Tung%20Siah%20and%20Naimeng%20Ye%20and%20Ang%20Li%20and%20Hongseok%20Namkoong%0AAbstract%3A%20%20%20As%20LLMs%20become%20capable%20of%20complex%20tasks%2C%20there%20is%20growing%20potential%20for%0Apersonalized%20interactions%20tailored%20to%20the%20subtle%20and%20idiosyncratic%20preferences%0Aof%20the%20user.%20We%20present%20a%20public%20benchmark%2C%20PersonalLLM%2C%20focusing%20on%20adapting%0ALLMs%20to%20provide%20maximal%20benefits%20for%20a%20particular%20user.%20Departing%20from%20existing%0Aalignment%20benchmarks%20that%20implicitly%20assume%20uniform%20preferences%2C%20we%20curate%0Aopen-ended%20prompts%20paired%20with%20many%20high-quality%20answers%20over%20which%20users%20would%0Abe%20expected%20to%20display%20heterogeneous%20latent%20preferences.%20Instead%20of%0Apersona-prompting%20LLMs%20based%20on%20high-level%20attributes%20%28e.g.%2C%20user%27s%20race%20or%0Aresponse%20length%29%2C%20which%20yields%20homogeneous%20preferences%20relative%20to%20humans%2C%20we%0Adevelop%20a%20method%20that%20can%20simulate%20a%20large%20user%20base%20with%20diverse%20preferences%0Afrom%20a%20set%20of%20pre-trained%20reward%20models.%20Our%20dataset%20and%20generated%0Apersonalities%20offer%20an%20innovative%20testbed%20for%20developing%20personalization%0Aalgorithms%20that%20grapple%20with%20continual%20data%20sparsity--few%20relevant%20feedback%0Afrom%20the%20particular%20user--by%20leveraging%20historical%20data%20from%20other%20%28similar%29%0Ausers.%20We%20explore%20basic%20in-context%20learning%20and%20meta-learning%20baselines%20to%0Aillustrate%20the%20utility%20of%20PersonalLLM%20and%20highlight%20the%20need%20for%20future%0Amethodological%20development.%20Our%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/namkoong-lab/PersonalLLM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.20296v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalLLM%253A%2520Tailoring%2520LLMs%2520to%2520Individual%2520Preferences%26entry.906535625%3DThomas%2520P.%2520Zollo%2520and%2520Andrew%2520Wei%2520Tung%2520Siah%2520and%2520Naimeng%2520Ye%2520and%2520Ang%2520Li%2520and%2520Hongseok%2520Namkoong%26entry.1292438233%3D%2520%2520As%2520LLMs%2520become%2520capable%2520of%2520complex%2520tasks%252C%2520there%2520is%2520growing%2520potential%2520for%250Apersonalized%2520interactions%2520tailored%2520to%2520the%2520subtle%2520and%2520idiosyncratic%2520preferences%250Aof%2520the%2520user.%2520We%2520present%2520a%2520public%2520benchmark%252C%2520PersonalLLM%252C%2520focusing%2520on%2520adapting%250ALLMs%2520to%2520provide%2520maximal%2520benefits%2520for%2520a%2520particular%2520user.%2520Departing%2520from%2520existing%250Aalignment%2520benchmarks%2520that%2520implicitly%2520assume%2520uniform%2520preferences%252C%2520we%2520curate%250Aopen-ended%2520prompts%2520paired%2520with%2520many%2520high-quality%2520answers%2520over%2520which%2520users%2520would%250Abe%2520expected%2520to%2520display%2520heterogeneous%2520latent%2520preferences.%2520Instead%2520of%250Apersona-prompting%2520LLMs%2520based%2520on%2520high-level%2520attributes%2520%2528e.g.%252C%2520user%2527s%2520race%2520or%250Aresponse%2520length%2529%252C%2520which%2520yields%2520homogeneous%2520preferences%2520relative%2520to%2520humans%252C%2520we%250Adevelop%2520a%2520method%2520that%2520can%2520simulate%2520a%2520large%2520user%2520base%2520with%2520diverse%2520preferences%250Afrom%2520a%2520set%2520of%2520pre-trained%2520reward%2520models.%2520Our%2520dataset%2520and%2520generated%250Apersonalities%2520offer%2520an%2520innovative%2520testbed%2520for%2520developing%2520personalization%250Aalgorithms%2520that%2520grapple%2520with%2520continual%2520data%2520sparsity--few%2520relevant%2520feedback%250Afrom%2520the%2520particular%2520user--by%2520leveraging%2520historical%2520data%2520from%2520other%2520%2528similar%2529%250Ausers.%2520We%2520explore%2520basic%2520in-context%2520learning%2520and%2520meta-learning%2520baselines%2520to%250Aillustrate%2520the%2520utility%2520of%2520PersonalLLM%2520and%2520highlight%2520the%2520need%2520for%2520future%250Amethodological%2520development.%2520Our%2520dataset%2520is%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/namkoong-lab/PersonalLLM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20296v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PersonalLLM%3A%20Tailoring%20LLMs%20to%20Individual%20Preferences&entry.906535625=Thomas%20P.%20Zollo%20and%20Andrew%20Wei%20Tung%20Siah%20and%20Naimeng%20Ye%20and%20Ang%20Li%20and%20Hongseok%20Namkoong&entry.1292438233=%20%20As%20LLMs%20become%20capable%20of%20complex%20tasks%2C%20there%20is%20growing%20potential%20for%0Apersonalized%20interactions%20tailored%20to%20the%20subtle%20and%20idiosyncratic%20preferences%0Aof%20the%20user.%20We%20present%20a%20public%20benchmark%2C%20PersonalLLM%2C%20focusing%20on%20adapting%0ALLMs%20to%20provide%20maximal%20benefits%20for%20a%20particular%20user.%20Departing%20from%20existing%0Aalignment%20benchmarks%20that%20implicitly%20assume%20uniform%20preferences%2C%20we%20curate%0Aopen-ended%20prompts%20paired%20with%20many%20high-quality%20answers%20over%20which%20users%20would%0Abe%20expected%20to%20display%20heterogeneous%20latent%20preferences.%20Instead%20of%0Apersona-prompting%20LLMs%20based%20on%20high-level%20attributes%20%28e.g.%2C%20user%27s%20race%20or%0Aresponse%20length%29%2C%20which%20yields%20homogeneous%20preferences%20relative%20to%20humans%2C%20we%0Adevelop%20a%20method%20that%20can%20simulate%20a%20large%20user%20base%20with%20diverse%20preferences%0Afrom%20a%20set%20of%20pre-trained%20reward%20models.%20Our%20dataset%20and%20generated%0Apersonalities%20offer%20an%20innovative%20testbed%20for%20developing%20personalization%0Aalgorithms%20that%20grapple%20with%20continual%20data%20sparsity--few%20relevant%20feedback%0Afrom%20the%20particular%20user--by%20leveraging%20historical%20data%20from%20other%20%28similar%29%0Ausers.%20We%20explore%20basic%20in-context%20learning%20and%20meta-learning%20baselines%20to%0Aillustrate%20the%20utility%20of%20PersonalLLM%20and%20highlight%20the%20need%20for%20future%0Amethodological%20development.%20Our%20dataset%20is%20available%20at%0Ahttps%3A//huggingface.co/datasets/namkoong-lab/PersonalLLM%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.20296v2&entry.124074799=Read"},
{"title": "Pleno-Generation: A Scalable Generative Face Video Compression Framework\n  with Bandwidth Intelligence", "author": "Bolin Chen and Hanwei Zhu and Shanzhi Yin and Lingyu Zhu and Jie Chen and Ru-Ling Liao and Shiqi Wang and Yan Ye", "abstract": "  Generative model based compact video compression is typically operated within\na relative narrow range of bitrates, and often with an emphasis on ultra-low\nrate applications. There has been an increasing consensus in the video\ncommunication industry that full bitrate coverage should be enabled by\ngenerative coding. However, this is an extremely difficult task, largely\nbecause generation and compression, although related, have distinct goals and\ntrade-offs. The proposed Pleno-Generation (PGen) framework distinguishes itself\nthrough its exceptional capabilities in ensuring the robustness of video coding\nby utilizing a wider range of bandwidth for generation via bandwidth\nintelligence. In particular, we initiate our research of PGen with face video\ncoding, and PGen offers a paradigm shift that prioritizes high-fidelity\nreconstruction over pursuing compact bitstream. The novel PGen framework\nleverages scalable representation and layered reconstruction for Generative\nFace Video Compression (GFVC), in an attempt to imbue the bitstream with\nintelligence in different granularity. Experimental results illustrate that the\nproposed PGen framework can facilitate existing GFVC algorithms to better\ndeliver high-fidelity and faithful face videos. In addition, the proposed\nframework can allow a greater space of flexibility for coding applications and\nshow superior RD performance with a much wider bitrate range in terms of\nvarious quality evaluations. Moreover, in comparison with the latest Versatile\nVideo Coding (VVC) codec, the proposed scheme achieves competitive\nBj{\\o}ntegaard-delta-rate savings for perceptual-level evaluations.\n", "link": "http://arxiv.org/abs/2502.17085v1", "date": "2025-02-24", "relevancy": 2.4011, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6121}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5924}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pleno-Generation%3A%20A%20Scalable%20Generative%20Face%20Video%20Compression%20Framework%0A%20%20with%20Bandwidth%20Intelligence&body=Title%3A%20Pleno-Generation%3A%20A%20Scalable%20Generative%20Face%20Video%20Compression%20Framework%0A%20%20with%20Bandwidth%20Intelligence%0AAuthor%3A%20Bolin%20Chen%20and%20Hanwei%20Zhu%20and%20Shanzhi%20Yin%20and%20Lingyu%20Zhu%20and%20Jie%20Chen%20and%20Ru-Ling%20Liao%20and%20Shiqi%20Wang%20and%20Yan%20Ye%0AAbstract%3A%20%20%20Generative%20model%20based%20compact%20video%20compression%20is%20typically%20operated%20within%0Aa%20relative%20narrow%20range%20of%20bitrates%2C%20and%20often%20with%20an%20emphasis%20on%20ultra-low%0Arate%20applications.%20There%20has%20been%20an%20increasing%20consensus%20in%20the%20video%0Acommunication%20industry%20that%20full%20bitrate%20coverage%20should%20be%20enabled%20by%0Agenerative%20coding.%20However%2C%20this%20is%20an%20extremely%20difficult%20task%2C%20largely%0Abecause%20generation%20and%20compression%2C%20although%20related%2C%20have%20distinct%20goals%20and%0Atrade-offs.%20The%20proposed%20Pleno-Generation%20%28PGen%29%20framework%20distinguishes%20itself%0Athrough%20its%20exceptional%20capabilities%20in%20ensuring%20the%20robustness%20of%20video%20coding%0Aby%20utilizing%20a%20wider%20range%20of%20bandwidth%20for%20generation%20via%20bandwidth%0Aintelligence.%20In%20particular%2C%20we%20initiate%20our%20research%20of%20PGen%20with%20face%20video%0Acoding%2C%20and%20PGen%20offers%20a%20paradigm%20shift%20that%20prioritizes%20high-fidelity%0Areconstruction%20over%20pursuing%20compact%20bitstream.%20The%20novel%20PGen%20framework%0Aleverages%20scalable%20representation%20and%20layered%20reconstruction%20for%20Generative%0AFace%20Video%20Compression%20%28GFVC%29%2C%20in%20an%20attempt%20to%20imbue%20the%20bitstream%20with%0Aintelligence%20in%20different%20granularity.%20Experimental%20results%20illustrate%20that%20the%0Aproposed%20PGen%20framework%20can%20facilitate%20existing%20GFVC%20algorithms%20to%20better%0Adeliver%20high-fidelity%20and%20faithful%20face%20videos.%20In%20addition%2C%20the%20proposed%0Aframework%20can%20allow%20a%20greater%20space%20of%20flexibility%20for%20coding%20applications%20and%0Ashow%20superior%20RD%20performance%20with%20a%20much%20wider%20bitrate%20range%20in%20terms%20of%0Avarious%20quality%20evaluations.%20Moreover%2C%20in%20comparison%20with%20the%20latest%20Versatile%0AVideo%20Coding%20%28VVC%29%20codec%2C%20the%20proposed%20scheme%20achieves%20competitive%0ABj%7B%5Co%7Dntegaard-delta-rate%20savings%20for%20perceptual-level%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPleno-Generation%253A%2520A%2520Scalable%2520Generative%2520Face%2520Video%2520Compression%2520Framework%250A%2520%2520with%2520Bandwidth%2520Intelligence%26entry.906535625%3DBolin%2520Chen%2520and%2520Hanwei%2520Zhu%2520and%2520Shanzhi%2520Yin%2520and%2520Lingyu%2520Zhu%2520and%2520Jie%2520Chen%2520and%2520Ru-Ling%2520Liao%2520and%2520Shiqi%2520Wang%2520and%2520Yan%2520Ye%26entry.1292438233%3D%2520%2520Generative%2520model%2520based%2520compact%2520video%2520compression%2520is%2520typically%2520operated%2520within%250Aa%2520relative%2520narrow%2520range%2520of%2520bitrates%252C%2520and%2520often%2520with%2520an%2520emphasis%2520on%2520ultra-low%250Arate%2520applications.%2520There%2520has%2520been%2520an%2520increasing%2520consensus%2520in%2520the%2520video%250Acommunication%2520industry%2520that%2520full%2520bitrate%2520coverage%2520should%2520be%2520enabled%2520by%250Agenerative%2520coding.%2520However%252C%2520this%2520is%2520an%2520extremely%2520difficult%2520task%252C%2520largely%250Abecause%2520generation%2520and%2520compression%252C%2520although%2520related%252C%2520have%2520distinct%2520goals%2520and%250Atrade-offs.%2520The%2520proposed%2520Pleno-Generation%2520%2528PGen%2529%2520framework%2520distinguishes%2520itself%250Athrough%2520its%2520exceptional%2520capabilities%2520in%2520ensuring%2520the%2520robustness%2520of%2520video%2520coding%250Aby%2520utilizing%2520a%2520wider%2520range%2520of%2520bandwidth%2520for%2520generation%2520via%2520bandwidth%250Aintelligence.%2520In%2520particular%252C%2520we%2520initiate%2520our%2520research%2520of%2520PGen%2520with%2520face%2520video%250Acoding%252C%2520and%2520PGen%2520offers%2520a%2520paradigm%2520shift%2520that%2520prioritizes%2520high-fidelity%250Areconstruction%2520over%2520pursuing%2520compact%2520bitstream.%2520The%2520novel%2520PGen%2520framework%250Aleverages%2520scalable%2520representation%2520and%2520layered%2520reconstruction%2520for%2520Generative%250AFace%2520Video%2520Compression%2520%2528GFVC%2529%252C%2520in%2520an%2520attempt%2520to%2520imbue%2520the%2520bitstream%2520with%250Aintelligence%2520in%2520different%2520granularity.%2520Experimental%2520results%2520illustrate%2520that%2520the%250Aproposed%2520PGen%2520framework%2520can%2520facilitate%2520existing%2520GFVC%2520algorithms%2520to%2520better%250Adeliver%2520high-fidelity%2520and%2520faithful%2520face%2520videos.%2520In%2520addition%252C%2520the%2520proposed%250Aframework%2520can%2520allow%2520a%2520greater%2520space%2520of%2520flexibility%2520for%2520coding%2520applications%2520and%250Ashow%2520superior%2520RD%2520performance%2520with%2520a%2520much%2520wider%2520bitrate%2520range%2520in%2520terms%2520of%250Avarious%2520quality%2520evaluations.%2520Moreover%252C%2520in%2520comparison%2520with%2520the%2520latest%2520Versatile%250AVideo%2520Coding%2520%2528VVC%2529%2520codec%252C%2520the%2520proposed%2520scheme%2520achieves%2520competitive%250ABj%257B%255Co%257Dntegaard-delta-rate%2520savings%2520for%2520perceptual-level%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pleno-Generation%3A%20A%20Scalable%20Generative%20Face%20Video%20Compression%20Framework%0A%20%20with%20Bandwidth%20Intelligence&entry.906535625=Bolin%20Chen%20and%20Hanwei%20Zhu%20and%20Shanzhi%20Yin%20and%20Lingyu%20Zhu%20and%20Jie%20Chen%20and%20Ru-Ling%20Liao%20and%20Shiqi%20Wang%20and%20Yan%20Ye&entry.1292438233=%20%20Generative%20model%20based%20compact%20video%20compression%20is%20typically%20operated%20within%0Aa%20relative%20narrow%20range%20of%20bitrates%2C%20and%20often%20with%20an%20emphasis%20on%20ultra-low%0Arate%20applications.%20There%20has%20been%20an%20increasing%20consensus%20in%20the%20video%0Acommunication%20industry%20that%20full%20bitrate%20coverage%20should%20be%20enabled%20by%0Agenerative%20coding.%20However%2C%20this%20is%20an%20extremely%20difficult%20task%2C%20largely%0Abecause%20generation%20and%20compression%2C%20although%20related%2C%20have%20distinct%20goals%20and%0Atrade-offs.%20The%20proposed%20Pleno-Generation%20%28PGen%29%20framework%20distinguishes%20itself%0Athrough%20its%20exceptional%20capabilities%20in%20ensuring%20the%20robustness%20of%20video%20coding%0Aby%20utilizing%20a%20wider%20range%20of%20bandwidth%20for%20generation%20via%20bandwidth%0Aintelligence.%20In%20particular%2C%20we%20initiate%20our%20research%20of%20PGen%20with%20face%20video%0Acoding%2C%20and%20PGen%20offers%20a%20paradigm%20shift%20that%20prioritizes%20high-fidelity%0Areconstruction%20over%20pursuing%20compact%20bitstream.%20The%20novel%20PGen%20framework%0Aleverages%20scalable%20representation%20and%20layered%20reconstruction%20for%20Generative%0AFace%20Video%20Compression%20%28GFVC%29%2C%20in%20an%20attempt%20to%20imbue%20the%20bitstream%20with%0Aintelligence%20in%20different%20granularity.%20Experimental%20results%20illustrate%20that%20the%0Aproposed%20PGen%20framework%20can%20facilitate%20existing%20GFVC%20algorithms%20to%20better%0Adeliver%20high-fidelity%20and%20faithful%20face%20videos.%20In%20addition%2C%20the%20proposed%0Aframework%20can%20allow%20a%20greater%20space%20of%20flexibility%20for%20coding%20applications%20and%0Ashow%20superior%20RD%20performance%20with%20a%20much%20wider%20bitrate%20range%20in%20terms%20of%0Avarious%20quality%20evaluations.%20Moreover%2C%20in%20comparison%20with%20the%20latest%20Versatile%0AVideo%20Coding%20%28VVC%29%20codec%2C%20the%20proposed%20scheme%20achieves%20competitive%0ABj%7B%5Co%7Dntegaard-delta-rate%20savings%20for%20perceptual-level%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17085v1&entry.124074799=Read"},
{"title": "CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation", "author": "Vishal Thengane and Jean Lahoud and Hisham Cholakkal and Rao Muhammad Anwer and Lu Yin and Xiatian Zhu and Salman Khan", "abstract": "  While 3D instance segmentation has made significant progress, current methods\nstruggle to address realistic scenarios where new categories emerge over time\nwith natural class imbalance. This limitation stems from existing datasets,\nwhich typically feature few well-balanced classes. Although few datasets\ninclude unbalanced class annotations, they lack the diverse incremental\nscenarios necessary for evaluating methods under incremental settings.\nAddressing these challenges requires frameworks that handle both incremental\nlearning and class imbalance. However, existing methods for 3D incremental\nsegmentation rely heavily on large exemplar replay, focusing only on\nincremental learning while neglecting class imbalance. Moreover,\nfrequency-based tuning for balanced learning is impractical in these setups due\nto the lack of prior class statistics. To overcome these limitations, we\npropose a framework to tackle both \\textbf{C}ontinual \\textbf{L}earning and\nclass \\textbf{Imb}alance for \\textbf{3D} instance segmentation\n(\\textbf{CLIMB-3D}). Our proposed approach combines Exemplar Replay (ER),\nKnowledge Distillation (KD), and a novel Imbalance Correction (IC) module.\nUnlike prior methods, our framework minimizes ER usage, with KD preventing\nforgetting and supporting the IC module in compiling past class statistics to\nbalance learning of rare classes during incremental updates. To evaluate our\nframework, we design three incremental scenarios based on class frequency,\nsemantic similarity, and random grouping that aim to mirror real-world dynamics\nin 3D environments. Experimental results show that our proposed framework\nachieves state-of-the-art performance, with an increase of up to 16.76\\% in mAP\ncompared to the baseline. Code will be available at:\n\\href{https://github.com/vgthengane/CLIMB3D}{https://github.com/vgthengane/CLIMB3D}\n", "link": "http://arxiv.org/abs/2502.17429v1", "date": "2025-02-24", "relevancy": 2.3968, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6261}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5844}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation&body=Title%3A%20CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation%0AAuthor%3A%20Vishal%20Thengane%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Lu%20Yin%20and%20Xiatian%20Zhu%20and%20Salman%20Khan%0AAbstract%3A%20%20%20While%203D%20instance%20segmentation%20has%20made%20significant%20progress%2C%20current%20methods%0Astruggle%20to%20address%20realistic%20scenarios%20where%20new%20categories%20emerge%20over%20time%0Awith%20natural%20class%20imbalance.%20This%20limitation%20stems%20from%20existing%20datasets%2C%0Awhich%20typically%20feature%20few%20well-balanced%20classes.%20Although%20few%20datasets%0Ainclude%20unbalanced%20class%20annotations%2C%20they%20lack%20the%20diverse%20incremental%0Ascenarios%20necessary%20for%20evaluating%20methods%20under%20incremental%20settings.%0AAddressing%20these%20challenges%20requires%20frameworks%20that%20handle%20both%20incremental%0Alearning%20and%20class%20imbalance.%20However%2C%20existing%20methods%20for%203D%20incremental%0Asegmentation%20rely%20heavily%20on%20large%20exemplar%20replay%2C%20focusing%20only%20on%0Aincremental%20learning%20while%20neglecting%20class%20imbalance.%20Moreover%2C%0Afrequency-based%20tuning%20for%20balanced%20learning%20is%20impractical%20in%20these%20setups%20due%0Ato%20the%20lack%20of%20prior%20class%20statistics.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20framework%20to%20tackle%20both%20%5Ctextbf%7BC%7Dontinual%20%5Ctextbf%7BL%7Dearning%20and%0Aclass%20%5Ctextbf%7BImb%7Dalance%20for%20%5Ctextbf%7B3D%7D%20instance%20segmentation%0A%28%5Ctextbf%7BCLIMB-3D%7D%29.%20Our%20proposed%20approach%20combines%20Exemplar%20Replay%20%28ER%29%2C%0AKnowledge%20Distillation%20%28KD%29%2C%20and%20a%20novel%20Imbalance%20Correction%20%28IC%29%20module.%0AUnlike%20prior%20methods%2C%20our%20framework%20minimizes%20ER%20usage%2C%20with%20KD%20preventing%0Aforgetting%20and%20supporting%20the%20IC%20module%20in%20compiling%20past%20class%20statistics%20to%0Abalance%20learning%20of%20rare%20classes%20during%20incremental%20updates.%20To%20evaluate%20our%0Aframework%2C%20we%20design%20three%20incremental%20scenarios%20based%20on%20class%20frequency%2C%0Asemantic%20similarity%2C%20and%20random%20grouping%20that%20aim%20to%20mirror%20real-world%20dynamics%0Ain%203D%20environments.%20Experimental%20results%20show%20that%20our%20proposed%20framework%0Aachieves%20state-of-the-art%20performance%2C%20with%20an%20increase%20of%20up%20to%2016.76%5C%25%20in%20mAP%0Acompared%20to%20the%20baseline.%20Code%20will%20be%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/vgthengane/CLIMB3D%7D%7Bhttps%3A//github.com/vgthengane/CLIMB3D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIMB-3D%253A%2520Continual%2520Learning%2520for%2520Imbalanced%25203D%2520Instance%2520Segmentation%26entry.906535625%3DVishal%2520Thengane%2520and%2520Jean%2520Lahoud%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Lu%2520Yin%2520and%2520Xiatian%2520Zhu%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520While%25203D%2520instance%2520segmentation%2520has%2520made%2520significant%2520progress%252C%2520current%2520methods%250Astruggle%2520to%2520address%2520realistic%2520scenarios%2520where%2520new%2520categories%2520emerge%2520over%2520time%250Awith%2520natural%2520class%2520imbalance.%2520This%2520limitation%2520stems%2520from%2520existing%2520datasets%252C%250Awhich%2520typically%2520feature%2520few%2520well-balanced%2520classes.%2520Although%2520few%2520datasets%250Ainclude%2520unbalanced%2520class%2520annotations%252C%2520they%2520lack%2520the%2520diverse%2520incremental%250Ascenarios%2520necessary%2520for%2520evaluating%2520methods%2520under%2520incremental%2520settings.%250AAddressing%2520these%2520challenges%2520requires%2520frameworks%2520that%2520handle%2520both%2520incremental%250Alearning%2520and%2520class%2520imbalance.%2520However%252C%2520existing%2520methods%2520for%25203D%2520incremental%250Asegmentation%2520rely%2520heavily%2520on%2520large%2520exemplar%2520replay%252C%2520focusing%2520only%2520on%250Aincremental%2520learning%2520while%2520neglecting%2520class%2520imbalance.%2520Moreover%252C%250Afrequency-based%2520tuning%2520for%2520balanced%2520learning%2520is%2520impractical%2520in%2520these%2520setups%2520due%250Ato%2520the%2520lack%2520of%2520prior%2520class%2520statistics.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520framework%2520to%2520tackle%2520both%2520%255Ctextbf%257BC%257Dontinual%2520%255Ctextbf%257BL%257Dearning%2520and%250Aclass%2520%255Ctextbf%257BImb%257Dalance%2520for%2520%255Ctextbf%257B3D%257D%2520instance%2520segmentation%250A%2528%255Ctextbf%257BCLIMB-3D%257D%2529.%2520Our%2520proposed%2520approach%2520combines%2520Exemplar%2520Replay%2520%2528ER%2529%252C%250AKnowledge%2520Distillation%2520%2528KD%2529%252C%2520and%2520a%2520novel%2520Imbalance%2520Correction%2520%2528IC%2529%2520module.%250AUnlike%2520prior%2520methods%252C%2520our%2520framework%2520minimizes%2520ER%2520usage%252C%2520with%2520KD%2520preventing%250Aforgetting%2520and%2520supporting%2520the%2520IC%2520module%2520in%2520compiling%2520past%2520class%2520statistics%2520to%250Abalance%2520learning%2520of%2520rare%2520classes%2520during%2520incremental%2520updates.%2520To%2520evaluate%2520our%250Aframework%252C%2520we%2520design%2520three%2520incremental%2520scenarios%2520based%2520on%2520class%2520frequency%252C%250Asemantic%2520similarity%252C%2520and%2520random%2520grouping%2520that%2520aim%2520to%2520mirror%2520real-world%2520dynamics%250Ain%25203D%2520environments.%2520Experimental%2520results%2520show%2520that%2520our%2520proposed%2520framework%250Aachieves%2520state-of-the-art%2520performance%252C%2520with%2520an%2520increase%2520of%2520up%2520to%252016.76%255C%2525%2520in%2520mAP%250Acompared%2520to%2520the%2520baseline.%2520Code%2520will%2520be%2520available%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/vgthengane/CLIMB3D%257D%257Bhttps%253A//github.com/vgthengane/CLIMB3D%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIMB-3D%3A%20Continual%20Learning%20for%20Imbalanced%203D%20Instance%20Segmentation&entry.906535625=Vishal%20Thengane%20and%20Jean%20Lahoud%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Lu%20Yin%20and%20Xiatian%20Zhu%20and%20Salman%20Khan&entry.1292438233=%20%20While%203D%20instance%20segmentation%20has%20made%20significant%20progress%2C%20current%20methods%0Astruggle%20to%20address%20realistic%20scenarios%20where%20new%20categories%20emerge%20over%20time%0Awith%20natural%20class%20imbalance.%20This%20limitation%20stems%20from%20existing%20datasets%2C%0Awhich%20typically%20feature%20few%20well-balanced%20classes.%20Although%20few%20datasets%0Ainclude%20unbalanced%20class%20annotations%2C%20they%20lack%20the%20diverse%20incremental%0Ascenarios%20necessary%20for%20evaluating%20methods%20under%20incremental%20settings.%0AAddressing%20these%20challenges%20requires%20frameworks%20that%20handle%20both%20incremental%0Alearning%20and%20class%20imbalance.%20However%2C%20existing%20methods%20for%203D%20incremental%0Asegmentation%20rely%20heavily%20on%20large%20exemplar%20replay%2C%20focusing%20only%20on%0Aincremental%20learning%20while%20neglecting%20class%20imbalance.%20Moreover%2C%0Afrequency-based%20tuning%20for%20balanced%20learning%20is%20impractical%20in%20these%20setups%20due%0Ato%20the%20lack%20of%20prior%20class%20statistics.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20framework%20to%20tackle%20both%20%5Ctextbf%7BC%7Dontinual%20%5Ctextbf%7BL%7Dearning%20and%0Aclass%20%5Ctextbf%7BImb%7Dalance%20for%20%5Ctextbf%7B3D%7D%20instance%20segmentation%0A%28%5Ctextbf%7BCLIMB-3D%7D%29.%20Our%20proposed%20approach%20combines%20Exemplar%20Replay%20%28ER%29%2C%0AKnowledge%20Distillation%20%28KD%29%2C%20and%20a%20novel%20Imbalance%20Correction%20%28IC%29%20module.%0AUnlike%20prior%20methods%2C%20our%20framework%20minimizes%20ER%20usage%2C%20with%20KD%20preventing%0Aforgetting%20and%20supporting%20the%20IC%20module%20in%20compiling%20past%20class%20statistics%20to%0Abalance%20learning%20of%20rare%20classes%20during%20incremental%20updates.%20To%20evaluate%20our%0Aframework%2C%20we%20design%20three%20incremental%20scenarios%20based%20on%20class%20frequency%2C%0Asemantic%20similarity%2C%20and%20random%20grouping%20that%20aim%20to%20mirror%20real-world%20dynamics%0Ain%203D%20environments.%20Experimental%20results%20show%20that%20our%20proposed%20framework%0Aachieves%20state-of-the-art%20performance%2C%20with%20an%20increase%20of%20up%20to%2016.76%5C%25%20in%20mAP%0Acompared%20to%20the%20baseline.%20Code%20will%20be%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/vgthengane/CLIMB3D%7D%7Bhttps%3A//github.com/vgthengane/CLIMB3D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17429v1&entry.124074799=Read"},
{"title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video\n  Editing", "author": "Xiangpeng Yang and Linchao Zhu and Hehe Fan and Yi Yang", "abstract": "  Recent advancements in diffusion models have significantly improved video\ngeneration and editing capabilities. However, multi-grained video editing,\nwhich encompasses class-level, instance-level, and part-level modifications,\nremains a formidable challenge. The major difficulties in multi-grained editing\ninclude semantic misalignment of text-to-region control and feature coupling\nwithin the diffusion model. To address these difficulties, we present\nVideoGrain, a zero-shot approach that modulates space-time (cross- and self-)\nattention mechanisms to achieve fine-grained control over video content. We\nenhance text-to-region control by amplifying each local prompt's attention to\nits corresponding spatial-disentangled region while minimizing interactions\nwith irrelevant areas in cross-attention. Additionally, we improve feature\nseparation by increasing intra-region awareness and reducing inter-region\ninterference in self-attention. Extensive experiments demonstrate our method\nachieves state-of-the-art performance in real-world scenarios. Our code, data,\nand demos are available at https://knightyxp.github.io/VideoGrain_project_page/\n", "link": "http://arxiv.org/abs/2502.17258v1", "date": "2025-02-24", "relevancy": 2.3894, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6499}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6064}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoGrain%3A%20Modulating%20Space-Time%20Attention%20for%20Multi-grained%20Video%0A%20%20Editing&body=Title%3A%20VideoGrain%3A%20Modulating%20Space-Time%20Attention%20for%20Multi-grained%20Video%0A%20%20Editing%0AAuthor%3A%20Xiangpeng%20Yang%20and%20Linchao%20Zhu%20and%20Hehe%20Fan%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20diffusion%20models%20have%20significantly%20improved%20video%0Ageneration%20and%20editing%20capabilities.%20However%2C%20multi-grained%20video%20editing%2C%0Awhich%20encompasses%20class-level%2C%20instance-level%2C%20and%20part-level%20modifications%2C%0Aremains%20a%20formidable%20challenge.%20The%20major%20difficulties%20in%20multi-grained%20editing%0Ainclude%20semantic%20misalignment%20of%20text-to-region%20control%20and%20feature%20coupling%0Awithin%20the%20diffusion%20model.%20To%20address%20these%20difficulties%2C%20we%20present%0AVideoGrain%2C%20a%20zero-shot%20approach%20that%20modulates%20space-time%20%28cross-%20and%20self-%29%0Aattention%20mechanisms%20to%20achieve%20fine-grained%20control%20over%20video%20content.%20We%0Aenhance%20text-to-region%20control%20by%20amplifying%20each%20local%20prompt%27s%20attention%20to%0Aits%20corresponding%20spatial-disentangled%20region%20while%20minimizing%20interactions%0Awith%20irrelevant%20areas%20in%20cross-attention.%20Additionally%2C%20we%20improve%20feature%0Aseparation%20by%20increasing%20intra-region%20awareness%20and%20reducing%20inter-region%0Ainterference%20in%20self-attention.%20Extensive%20experiments%20demonstrate%20our%20method%0Aachieves%20state-of-the-art%20performance%20in%20real-world%20scenarios.%20Our%20code%2C%20data%2C%0Aand%20demos%20are%20available%20at%20https%3A//knightyxp.github.io/VideoGrain_project_page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17258v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoGrain%253A%2520Modulating%2520Space-Time%2520Attention%2520for%2520Multi-grained%2520Video%250A%2520%2520Editing%26entry.906535625%3DXiangpeng%2520Yang%2520and%2520Linchao%2520Zhu%2520and%2520Hehe%2520Fan%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520diffusion%2520models%2520have%2520significantly%2520improved%2520video%250Ageneration%2520and%2520editing%2520capabilities.%2520However%252C%2520multi-grained%2520video%2520editing%252C%250Awhich%2520encompasses%2520class-level%252C%2520instance-level%252C%2520and%2520part-level%2520modifications%252C%250Aremains%2520a%2520formidable%2520challenge.%2520The%2520major%2520difficulties%2520in%2520multi-grained%2520editing%250Ainclude%2520semantic%2520misalignment%2520of%2520text-to-region%2520control%2520and%2520feature%2520coupling%250Awithin%2520the%2520diffusion%2520model.%2520To%2520address%2520these%2520difficulties%252C%2520we%2520present%250AVideoGrain%252C%2520a%2520zero-shot%2520approach%2520that%2520modulates%2520space-time%2520%2528cross-%2520and%2520self-%2529%250Aattention%2520mechanisms%2520to%2520achieve%2520fine-grained%2520control%2520over%2520video%2520content.%2520We%250Aenhance%2520text-to-region%2520control%2520by%2520amplifying%2520each%2520local%2520prompt%2527s%2520attention%2520to%250Aits%2520corresponding%2520spatial-disentangled%2520region%2520while%2520minimizing%2520interactions%250Awith%2520irrelevant%2520areas%2520in%2520cross-attention.%2520Additionally%252C%2520we%2520improve%2520feature%250Aseparation%2520by%2520increasing%2520intra-region%2520awareness%2520and%2520reducing%2520inter-region%250Ainterference%2520in%2520self-attention.%2520Extensive%2520experiments%2520demonstrate%2520our%2520method%250Aachieves%2520state-of-the-art%2520performance%2520in%2520real-world%2520scenarios.%2520Our%2520code%252C%2520data%252C%250Aand%2520demos%2520are%2520available%2520at%2520https%253A//knightyxp.github.io/VideoGrain_project_page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17258v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGrain%3A%20Modulating%20Space-Time%20Attention%20for%20Multi-grained%20Video%0A%20%20Editing&entry.906535625=Xiangpeng%20Yang%20and%20Linchao%20Zhu%20and%20Hehe%20Fan%20and%20Yi%20Yang&entry.1292438233=%20%20Recent%20advancements%20in%20diffusion%20models%20have%20significantly%20improved%20video%0Ageneration%20and%20editing%20capabilities.%20However%2C%20multi-grained%20video%20editing%2C%0Awhich%20encompasses%20class-level%2C%20instance-level%2C%20and%20part-level%20modifications%2C%0Aremains%20a%20formidable%20challenge.%20The%20major%20difficulties%20in%20multi-grained%20editing%0Ainclude%20semantic%20misalignment%20of%20text-to-region%20control%20and%20feature%20coupling%0Awithin%20the%20diffusion%20model.%20To%20address%20these%20difficulties%2C%20we%20present%0AVideoGrain%2C%20a%20zero-shot%20approach%20that%20modulates%20space-time%20%28cross-%20and%20self-%29%0Aattention%20mechanisms%20to%20achieve%20fine-grained%20control%20over%20video%20content.%20We%0Aenhance%20text-to-region%20control%20by%20amplifying%20each%20local%20prompt%27s%20attention%20to%0Aits%20corresponding%20spatial-disentangled%20region%20while%20minimizing%20interactions%0Awith%20irrelevant%20areas%20in%20cross-attention.%20Additionally%2C%20we%20improve%20feature%0Aseparation%20by%20increasing%20intra-region%20awareness%20and%20reducing%20inter-region%0Ainterference%20in%20self-attention.%20Extensive%20experiments%20demonstrate%20our%20method%0Aachieves%20state-of-the-art%20performance%20in%20real-world%20scenarios.%20Our%20code%2C%20data%2C%0Aand%20demos%20are%20available%20at%20https%3A//knightyxp.github.io/VideoGrain_project_page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17258v1&entry.124074799=Read"},
{"title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive,\n  Distributional, and Semantic Objective", "author": "Simon Geisler and Tom Wollschl\u00e4ger and M. H. I. Abdalla and Vincent Cohen-Addad and Johannes Gasteiger and Stephan G\u00fcnnemann", "abstract": "  To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense.\n", "link": "http://arxiv.org/abs/2502.17254v1", "date": "2025-02-24", "relevancy": 2.381, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4983}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4667}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REINFORCE%20Adversarial%20Attacks%20on%20Large%20Language%20Models%3A%20An%20Adaptive%2C%0A%20%20Distributional%2C%20and%20Semantic%20Objective&body=Title%3A%20REINFORCE%20Adversarial%20Attacks%20on%20Large%20Language%20Models%3A%20An%20Adaptive%2C%0A%20%20Distributional%2C%20and%20Semantic%20Objective%0AAuthor%3A%20Simon%20Geisler%20and%20Tom%20Wollschl%C3%A4ger%20and%20M.%20H.%20I.%20Abdalla%20and%20Vincent%20Cohen-Addad%20and%20Johannes%20Gasteiger%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20To%20circumvent%20the%20alignment%20of%20large%20language%20models%20%28LLMs%29%2C%20current%0Aoptimization-based%20adversarial%20attacks%20usually%20craft%20adversarial%20prompts%20by%0Amaximizing%20the%20likelihood%20of%20a%20so-called%20affirmative%20response.%20An%20affirmative%0Aresponse%20is%20a%20manually%20designed%20start%20of%20a%20harmful%20answer%20to%20an%20inappropriate%0Arequest.%20While%20it%20is%20often%20easy%20to%20craft%20prompts%20that%20yield%20a%20substantial%0Alikelihood%20for%20the%20affirmative%20response%2C%20the%20attacked%20model%20frequently%20does%20not%0Acomplete%20the%20response%20in%20a%20harmful%20manner.%20Moreover%2C%20the%20affirmative%20objective%0Ais%20usually%20not%20adapted%20to%20model-specific%20preferences%20and%20essentially%20ignores%0Athe%20fact%20that%20LLMs%20output%20a%20distribution%20over%20responses.%20If%20low%20attack%20success%0Aunder%20such%20an%20objective%20is%20taken%20as%20a%20measure%20of%20robustness%2C%20the%20true%0Arobustness%20might%20be%20grossly%20overestimated.%20To%20alleviate%20these%20flaws%2C%20we%20propose%0Aan%20adaptive%20and%20semantic%20optimization%20problem%20over%20the%20population%20of%20responses.%0AWe%20derive%20a%20generally%20applicable%20objective%20via%20the%20REINFORCE%20policy-gradient%0Aformalism%20and%20demonstrate%20its%20efficacy%20with%20the%20state-of-the-art%20jailbreak%0Aalgorithms%20Greedy%20Coordinate%20Gradient%20%28GCG%29%20and%20Projected%20Gradient%20Descent%0A%28PGD%29.%20For%20example%2C%20our%20objective%20doubles%20the%20attack%20success%20rate%20%28ASR%29%20on%0ALlama3%20and%20increases%20the%20ASR%20from%202%25%20to%2050%25%20with%20circuit%20breaker%20defense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREINFORCE%2520Adversarial%2520Attacks%2520on%2520Large%2520Language%2520Models%253A%2520An%2520Adaptive%252C%250A%2520%2520Distributional%252C%2520and%2520Semantic%2520Objective%26entry.906535625%3DSimon%2520Geisler%2520and%2520Tom%2520Wollschl%25C3%25A4ger%2520and%2520M.%2520H.%2520I.%2520Abdalla%2520and%2520Vincent%2520Cohen-Addad%2520and%2520Johannes%2520Gasteiger%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520To%2520circumvent%2520the%2520alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520current%250Aoptimization-based%2520adversarial%2520attacks%2520usually%2520craft%2520adversarial%2520prompts%2520by%250Amaximizing%2520the%2520likelihood%2520of%2520a%2520so-called%2520affirmative%2520response.%2520An%2520affirmative%250Aresponse%2520is%2520a%2520manually%2520designed%2520start%2520of%2520a%2520harmful%2520answer%2520to%2520an%2520inappropriate%250Arequest.%2520While%2520it%2520is%2520often%2520easy%2520to%2520craft%2520prompts%2520that%2520yield%2520a%2520substantial%250Alikelihood%2520for%2520the%2520affirmative%2520response%252C%2520the%2520attacked%2520model%2520frequently%2520does%2520not%250Acomplete%2520the%2520response%2520in%2520a%2520harmful%2520manner.%2520Moreover%252C%2520the%2520affirmative%2520objective%250Ais%2520usually%2520not%2520adapted%2520to%2520model-specific%2520preferences%2520and%2520essentially%2520ignores%250Athe%2520fact%2520that%2520LLMs%2520output%2520a%2520distribution%2520over%2520responses.%2520If%2520low%2520attack%2520success%250Aunder%2520such%2520an%2520objective%2520is%2520taken%2520as%2520a%2520measure%2520of%2520robustness%252C%2520the%2520true%250Arobustness%2520might%2520be%2520grossly%2520overestimated.%2520To%2520alleviate%2520these%2520flaws%252C%2520we%2520propose%250Aan%2520adaptive%2520and%2520semantic%2520optimization%2520problem%2520over%2520the%2520population%2520of%2520responses.%250AWe%2520derive%2520a%2520generally%2520applicable%2520objective%2520via%2520the%2520REINFORCE%2520policy-gradient%250Aformalism%2520and%2520demonstrate%2520its%2520efficacy%2520with%2520the%2520state-of-the-art%2520jailbreak%250Aalgorithms%2520Greedy%2520Coordinate%2520Gradient%2520%2528GCG%2529%2520and%2520Projected%2520Gradient%2520Descent%250A%2528PGD%2529.%2520For%2520example%252C%2520our%2520objective%2520doubles%2520the%2520attack%2520success%2520rate%2520%2528ASR%2529%2520on%250ALlama3%2520and%2520increases%2520the%2520ASR%2520from%25202%2525%2520to%252050%2525%2520with%2520circuit%2520breaker%2520defense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REINFORCE%20Adversarial%20Attacks%20on%20Large%20Language%20Models%3A%20An%20Adaptive%2C%0A%20%20Distributional%2C%20and%20Semantic%20Objective&entry.906535625=Simon%20Geisler%20and%20Tom%20Wollschl%C3%A4ger%20and%20M.%20H.%20I.%20Abdalla%20and%20Vincent%20Cohen-Addad%20and%20Johannes%20Gasteiger%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20To%20circumvent%20the%20alignment%20of%20large%20language%20models%20%28LLMs%29%2C%20current%0Aoptimization-based%20adversarial%20attacks%20usually%20craft%20adversarial%20prompts%20by%0Amaximizing%20the%20likelihood%20of%20a%20so-called%20affirmative%20response.%20An%20affirmative%0Aresponse%20is%20a%20manually%20designed%20start%20of%20a%20harmful%20answer%20to%20an%20inappropriate%0Arequest.%20While%20it%20is%20often%20easy%20to%20craft%20prompts%20that%20yield%20a%20substantial%0Alikelihood%20for%20the%20affirmative%20response%2C%20the%20attacked%20model%20frequently%20does%20not%0Acomplete%20the%20response%20in%20a%20harmful%20manner.%20Moreover%2C%20the%20affirmative%20objective%0Ais%20usually%20not%20adapted%20to%20model-specific%20preferences%20and%20essentially%20ignores%0Athe%20fact%20that%20LLMs%20output%20a%20distribution%20over%20responses.%20If%20low%20attack%20success%0Aunder%20such%20an%20objective%20is%20taken%20as%20a%20measure%20of%20robustness%2C%20the%20true%0Arobustness%20might%20be%20grossly%20overestimated.%20To%20alleviate%20these%20flaws%2C%20we%20propose%0Aan%20adaptive%20and%20semantic%20optimization%20problem%20over%20the%20population%20of%20responses.%0AWe%20derive%20a%20generally%20applicable%20objective%20via%20the%20REINFORCE%20policy-gradient%0Aformalism%20and%20demonstrate%20its%20efficacy%20with%20the%20state-of-the-art%20jailbreak%0Aalgorithms%20Greedy%20Coordinate%20Gradient%20%28GCG%29%20and%20Projected%20Gradient%20Descent%0A%28PGD%29.%20For%20example%2C%20our%20objective%20doubles%20the%20attack%20success%20rate%20%28ASR%29%20on%0ALlama3%20and%20increases%20the%20ASR%20from%202%25%20to%2050%25%20with%20circuit%20breaker%20defense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17254v1&entry.124074799=Read"},
{"title": "Adversaries With Incentives: A Strategic Alternative to Adversarial\n  Robustness", "author": "Maayan Ehrenberg and Roy Ganz and Nir Rosenfeld", "abstract": "  Adversarial training aims to defend against *adversaries*: malicious\nopponents whose sole aim is to harm predictive performance in any way possible\n- a rather harsh perspective, which we assert results in unnecessarily\nconservative models. Instead, we propose to model opponents as simply pursuing\ntheir own goals, rather than working directly against the classifier. Employing\ntools from strategic modeling, our approach uses knowledge or beliefs regarding\nthe opponent's possible incentives as inductive bias for learning. Our method\nof *strategic training* is designed to defend against opponents within an\n*incentive uncertainty set*: this resorts to adversarial learning when the set\nis maximal, but offers potential gains when it can be appropriately reduced. We\nconduct a series of experiments that show how even mild knowledge regarding the\nadversary's incentives can be useful, and that the degree of potential gains\ndepends on how incentives relate to the structure of the learning task.\n", "link": "http://arxiv.org/abs/2406.11458v2", "date": "2025-02-24", "relevancy": 2.3785, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4881}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness&body=Title%3A%20Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness%0AAuthor%3A%20Maayan%20Ehrenberg%20and%20Roy%20Ganz%20and%20Nir%20Rosenfeld%0AAbstract%3A%20%20%20Adversarial%20training%20aims%20to%20defend%20against%20%2Aadversaries%2A%3A%20malicious%0Aopponents%20whose%20sole%20aim%20is%20to%20harm%20predictive%20performance%20in%20any%20way%20possible%0A-%20a%20rather%20harsh%20perspective%2C%20which%20we%20assert%20results%20in%20unnecessarily%0Aconservative%20models.%20Instead%2C%20we%20propose%20to%20model%20opponents%20as%20simply%20pursuing%0Atheir%20own%20goals%2C%20rather%20than%20working%20directly%20against%20the%20classifier.%20Employing%0Atools%20from%20strategic%20modeling%2C%20our%20approach%20uses%20knowledge%20or%20beliefs%20regarding%0Athe%20opponent%27s%20possible%20incentives%20as%20inductive%20bias%20for%20learning.%20Our%20method%0Aof%20%2Astrategic%20training%2A%20is%20designed%20to%20defend%20against%20opponents%20within%20an%0A%2Aincentive%20uncertainty%20set%2A%3A%20this%20resorts%20to%20adversarial%20learning%20when%20the%20set%0Ais%20maximal%2C%20but%20offers%20potential%20gains%20when%20it%20can%20be%20appropriately%20reduced.%20We%0Aconduct%20a%20series%20of%20experiments%20that%20show%20how%20even%20mild%20knowledge%20regarding%20the%0Aadversary%27s%20incentives%20can%20be%20useful%2C%20and%20that%20the%20degree%20of%20potential%20gains%0Adepends%20on%20how%20incentives%20relate%20to%20the%20structure%20of%20the%20learning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversaries%2520With%2520Incentives%253A%2520A%2520Strategic%2520Alternative%2520to%2520Adversarial%250A%2520%2520Robustness%26entry.906535625%3DMaayan%2520Ehrenberg%2520and%2520Roy%2520Ganz%2520and%2520Nir%2520Rosenfeld%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520aims%2520to%2520defend%2520against%2520%252Aadversaries%252A%253A%2520malicious%250Aopponents%2520whose%2520sole%2520aim%2520is%2520to%2520harm%2520predictive%2520performance%2520in%2520any%2520way%2520possible%250A-%2520a%2520rather%2520harsh%2520perspective%252C%2520which%2520we%2520assert%2520results%2520in%2520unnecessarily%250Aconservative%2520models.%2520Instead%252C%2520we%2520propose%2520to%2520model%2520opponents%2520as%2520simply%2520pursuing%250Atheir%2520own%2520goals%252C%2520rather%2520than%2520working%2520directly%2520against%2520the%2520classifier.%2520Employing%250Atools%2520from%2520strategic%2520modeling%252C%2520our%2520approach%2520uses%2520knowledge%2520or%2520beliefs%2520regarding%250Athe%2520opponent%2527s%2520possible%2520incentives%2520as%2520inductive%2520bias%2520for%2520learning.%2520Our%2520method%250Aof%2520%252Astrategic%2520training%252A%2520is%2520designed%2520to%2520defend%2520against%2520opponents%2520within%2520an%250A%252Aincentive%2520uncertainty%2520set%252A%253A%2520this%2520resorts%2520to%2520adversarial%2520learning%2520when%2520the%2520set%250Ais%2520maximal%252C%2520but%2520offers%2520potential%2520gains%2520when%2520it%2520can%2520be%2520appropriately%2520reduced.%2520We%250Aconduct%2520a%2520series%2520of%2520experiments%2520that%2520show%2520how%2520even%2520mild%2520knowledge%2520regarding%2520the%250Aadversary%2527s%2520incentives%2520can%2520be%2520useful%252C%2520and%2520that%2520the%2520degree%2520of%2520potential%2520gains%250Adepends%2520on%2520how%2520incentives%2520relate%2520to%2520the%2520structure%2520of%2520the%2520learning%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness&entry.906535625=Maayan%20Ehrenberg%20and%20Roy%20Ganz%20and%20Nir%20Rosenfeld&entry.1292438233=%20%20Adversarial%20training%20aims%20to%20defend%20against%20%2Aadversaries%2A%3A%20malicious%0Aopponents%20whose%20sole%20aim%20is%20to%20harm%20predictive%20performance%20in%20any%20way%20possible%0A-%20a%20rather%20harsh%20perspective%2C%20which%20we%20assert%20results%20in%20unnecessarily%0Aconservative%20models.%20Instead%2C%20we%20propose%20to%20model%20opponents%20as%20simply%20pursuing%0Atheir%20own%20goals%2C%20rather%20than%20working%20directly%20against%20the%20classifier.%20Employing%0Atools%20from%20strategic%20modeling%2C%20our%20approach%20uses%20knowledge%20or%20beliefs%20regarding%0Athe%20opponent%27s%20possible%20incentives%20as%20inductive%20bias%20for%20learning.%20Our%20method%0Aof%20%2Astrategic%20training%2A%20is%20designed%20to%20defend%20against%20opponents%20within%20an%0A%2Aincentive%20uncertainty%20set%2A%3A%20this%20resorts%20to%20adversarial%20learning%20when%20the%20set%0Ais%20maximal%2C%20but%20offers%20potential%20gains%20when%20it%20can%20be%20appropriately%20reduced.%20We%0Aconduct%20a%20series%20of%20experiments%20that%20show%20how%20even%20mild%20knowledge%20regarding%20the%0Aadversary%27s%20incentives%20can%20be%20useful%2C%20and%20that%20the%20degree%20of%20potential%20gains%0Adepends%20on%20how%20incentives%20relate%20to%20the%20structure%20of%20the%20learning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11458v2&entry.124074799=Read"},
{"title": "V-HOP: Visuo-Haptic 6D Object Pose Tracking", "author": "Hongyu Li and Mingxi Jia and Tuluhan Akbulut and Yu Xiang and George Konidaris and Srinath Sridhar", "abstract": "  Humans naturally integrate vision and haptics for robust object perception\nduring manipulation. The loss of either modality significantly degrades\nperformance. Inspired by this multisensory integration, prior object pose\nestimation research has attempted to combine visual and haptic/tactile\nfeedback. Although these works demonstrate improvements in controlled\nenvironments or synthetic datasets, they often underperform vision-only\napproaches in real-world settings due to poor generalization across diverse\ngrippers, sensor layouts, or sim-to-real environments. Furthermore, they\ntypically estimate the object pose for each frame independently, resulting in\nless coherent tracking over sequences in real-world deployments. To address\nthese limitations, we introduce a novel unified haptic representation that\neffectively handles multiple gripper embodiments. Building on this\nrepresentation, we introduce a new visuo-haptic transformer-based object pose\ntracker that seamlessly integrates visual and haptic input. We validate our\nframework in our dataset and the Feelsight dataset, demonstrating significant\nperformance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and\nsensor types (both taxel-based and vision-based tactile sensors). In real-world\nexperiments, we demonstrate that our approach outperforms state-of-the-art\nvisual trackers by a large margin. We further show that we can achieve precise\nmanipulation tasks by incorporating our real-time object tracking result into\nmotion plans, underscoring the advantages of visuo-haptic perception. Our model\nand dataset will be made open source upon acceptance of the paper. Project\nwebsite: https://lhy.xyz/projects/v-hop/\n", "link": "http://arxiv.org/abs/2502.17434v1", "date": "2025-02-24", "relevancy": 2.3782, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5971}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5928}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-HOP%3A%20Visuo-Haptic%206D%20Object%20Pose%20Tracking&body=Title%3A%20V-HOP%3A%20Visuo-Haptic%206D%20Object%20Pose%20Tracking%0AAuthor%3A%20Hongyu%20Li%20and%20Mingxi%20Jia%20and%20Tuluhan%20Akbulut%20and%20Yu%20Xiang%20and%20George%20Konidaris%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Humans%20naturally%20integrate%20vision%20and%20haptics%20for%20robust%20object%20perception%0Aduring%20manipulation.%20The%20loss%20of%20either%20modality%20significantly%20degrades%0Aperformance.%20Inspired%20by%20this%20multisensory%20integration%2C%20prior%20object%20pose%0Aestimation%20research%20has%20attempted%20to%20combine%20visual%20and%20haptic/tactile%0Afeedback.%20Although%20these%20works%20demonstrate%20improvements%20in%20controlled%0Aenvironments%20or%20synthetic%20datasets%2C%20they%20often%20underperform%20vision-only%0Aapproaches%20in%20real-world%20settings%20due%20to%20poor%20generalization%20across%20diverse%0Agrippers%2C%20sensor%20layouts%2C%20or%20sim-to-real%20environments.%20Furthermore%2C%20they%0Atypically%20estimate%20the%20object%20pose%20for%20each%20frame%20independently%2C%20resulting%20in%0Aless%20coherent%20tracking%20over%20sequences%20in%20real-world%20deployments.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20a%20novel%20unified%20haptic%20representation%20that%0Aeffectively%20handles%20multiple%20gripper%20embodiments.%20Building%20on%20this%0Arepresentation%2C%20we%20introduce%20a%20new%20visuo-haptic%20transformer-based%20object%20pose%0Atracker%20that%20seamlessly%20integrates%20visual%20and%20haptic%20input.%20We%20validate%20our%0Aframework%20in%20our%20dataset%20and%20the%20Feelsight%20dataset%2C%20demonstrating%20significant%0Aperformance%20improvement%20on%20challenging%20sequences.%20Notably%2C%20our%20method%20achieves%0Asuperior%20generalization%20and%20robustness%20across%20novel%20embodiments%2C%20objects%2C%20and%0Asensor%20types%20%28both%20taxel-based%20and%20vision-based%20tactile%20sensors%29.%20In%20real-world%0Aexperiments%2C%20we%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%0Avisual%20trackers%20by%20a%20large%20margin.%20We%20further%20show%20that%20we%20can%20achieve%20precise%0Amanipulation%20tasks%20by%20incorporating%20our%20real-time%20object%20tracking%20result%20into%0Amotion%20plans%2C%20underscoring%20the%20advantages%20of%20visuo-haptic%20perception.%20Our%20model%0Aand%20dataset%20will%20be%20made%20open%20source%20upon%20acceptance%20of%20the%20paper.%20Project%0Awebsite%3A%20https%3A//lhy.xyz/projects/v-hop/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-HOP%253A%2520Visuo-Haptic%25206D%2520Object%2520Pose%2520Tracking%26entry.906535625%3DHongyu%2520Li%2520and%2520Mingxi%2520Jia%2520and%2520Tuluhan%2520Akbulut%2520and%2520Yu%2520Xiang%2520and%2520George%2520Konidaris%2520and%2520Srinath%2520Sridhar%26entry.1292438233%3D%2520%2520Humans%2520naturally%2520integrate%2520vision%2520and%2520haptics%2520for%2520robust%2520object%2520perception%250Aduring%2520manipulation.%2520The%2520loss%2520of%2520either%2520modality%2520significantly%2520degrades%250Aperformance.%2520Inspired%2520by%2520this%2520multisensory%2520integration%252C%2520prior%2520object%2520pose%250Aestimation%2520research%2520has%2520attempted%2520to%2520combine%2520visual%2520and%2520haptic/tactile%250Afeedback.%2520Although%2520these%2520works%2520demonstrate%2520improvements%2520in%2520controlled%250Aenvironments%2520or%2520synthetic%2520datasets%252C%2520they%2520often%2520underperform%2520vision-only%250Aapproaches%2520in%2520real-world%2520settings%2520due%2520to%2520poor%2520generalization%2520across%2520diverse%250Agrippers%252C%2520sensor%2520layouts%252C%2520or%2520sim-to-real%2520environments.%2520Furthermore%252C%2520they%250Atypically%2520estimate%2520the%2520object%2520pose%2520for%2520each%2520frame%2520independently%252C%2520resulting%2520in%250Aless%2520coherent%2520tracking%2520over%2520sequences%2520in%2520real-world%2520deployments.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520introduce%2520a%2520novel%2520unified%2520haptic%2520representation%2520that%250Aeffectively%2520handles%2520multiple%2520gripper%2520embodiments.%2520Building%2520on%2520this%250Arepresentation%252C%2520we%2520introduce%2520a%2520new%2520visuo-haptic%2520transformer-based%2520object%2520pose%250Atracker%2520that%2520seamlessly%2520integrates%2520visual%2520and%2520haptic%2520input.%2520We%2520validate%2520our%250Aframework%2520in%2520our%2520dataset%2520and%2520the%2520Feelsight%2520dataset%252C%2520demonstrating%2520significant%250Aperformance%2520improvement%2520on%2520challenging%2520sequences.%2520Notably%252C%2520our%2520method%2520achieves%250Asuperior%2520generalization%2520and%2520robustness%2520across%2520novel%2520embodiments%252C%2520objects%252C%2520and%250Asensor%2520types%2520%2528both%2520taxel-based%2520and%2520vision-based%2520tactile%2520sensors%2529.%2520In%2520real-world%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%250Avisual%2520trackers%2520by%2520a%2520large%2520margin.%2520We%2520further%2520show%2520that%2520we%2520can%2520achieve%2520precise%250Amanipulation%2520tasks%2520by%2520incorporating%2520our%2520real-time%2520object%2520tracking%2520result%2520into%250Amotion%2520plans%252C%2520underscoring%2520the%2520advantages%2520of%2520visuo-haptic%2520perception.%2520Our%2520model%250Aand%2520dataset%2520will%2520be%2520made%2520open%2520source%2520upon%2520acceptance%2520of%2520the%2520paper.%2520Project%250Awebsite%253A%2520https%253A//lhy.xyz/projects/v-hop/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-HOP%3A%20Visuo-Haptic%206D%20Object%20Pose%20Tracking&entry.906535625=Hongyu%20Li%20and%20Mingxi%20Jia%20and%20Tuluhan%20Akbulut%20and%20Yu%20Xiang%20and%20George%20Konidaris%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Humans%20naturally%20integrate%20vision%20and%20haptics%20for%20robust%20object%20perception%0Aduring%20manipulation.%20The%20loss%20of%20either%20modality%20significantly%20degrades%0Aperformance.%20Inspired%20by%20this%20multisensory%20integration%2C%20prior%20object%20pose%0Aestimation%20research%20has%20attempted%20to%20combine%20visual%20and%20haptic/tactile%0Afeedback.%20Although%20these%20works%20demonstrate%20improvements%20in%20controlled%0Aenvironments%20or%20synthetic%20datasets%2C%20they%20often%20underperform%20vision-only%0Aapproaches%20in%20real-world%20settings%20due%20to%20poor%20generalization%20across%20diverse%0Agrippers%2C%20sensor%20layouts%2C%20or%20sim-to-real%20environments.%20Furthermore%2C%20they%0Atypically%20estimate%20the%20object%20pose%20for%20each%20frame%20independently%2C%20resulting%20in%0Aless%20coherent%20tracking%20over%20sequences%20in%20real-world%20deployments.%20To%20address%0Athese%20limitations%2C%20we%20introduce%20a%20novel%20unified%20haptic%20representation%20that%0Aeffectively%20handles%20multiple%20gripper%20embodiments.%20Building%20on%20this%0Arepresentation%2C%20we%20introduce%20a%20new%20visuo-haptic%20transformer-based%20object%20pose%0Atracker%20that%20seamlessly%20integrates%20visual%20and%20haptic%20input.%20We%20validate%20our%0Aframework%20in%20our%20dataset%20and%20the%20Feelsight%20dataset%2C%20demonstrating%20significant%0Aperformance%20improvement%20on%20challenging%20sequences.%20Notably%2C%20our%20method%20achieves%0Asuperior%20generalization%20and%20robustness%20across%20novel%20embodiments%2C%20objects%2C%20and%0Asensor%20types%20%28both%20taxel-based%20and%20vision-based%20tactile%20sensors%29.%20In%20real-world%0Aexperiments%2C%20we%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%0Avisual%20trackers%20by%20a%20large%20margin.%20We%20further%20show%20that%20we%20can%20achieve%20precise%0Amanipulation%20tasks%20by%20incorporating%20our%20real-time%20object%20tracking%20result%20into%0Amotion%20plans%2C%20underscoring%20the%20advantages%20of%20visuo-haptic%20perception.%20Our%20model%0Aand%20dataset%20will%20be%20made%20open%20source%20upon%20acceptance%20of%20the%20paper.%20Project%0Awebsite%3A%20https%3A//lhy.xyz/projects/v-hop/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17434v1&entry.124074799=Read"},
{"title": "AnyTop: Character Animation Diffusion with Any Topology", "author": "Inbar Gat and Sigal Raab and Guy Tevet and Yuval Reshef and Amit H. Bermano and Daniel Cohen-Or", "abstract": "  Generating motion for arbitrary skeletons is a longstanding challenge in\ncomputer graphics, remaining largely unexplored due to the scarcity of diverse\ndatasets and the irregular nature of the data. In this work, we introduce\nAnyTop, a diffusion model that generates motions for diverse characters with\ndistinct motion dynamics, using only their skeletal structure as input. Our\nwork features a transformer-based denoising network, tailored for arbitrary\nskeleton learning, integrating topology information into the traditional\nattention mechanism. Additionally, by incorporating textual joint descriptions\ninto the latent feature representation, AnyTop learns semantic correspondences\nbetween joints across diverse skeletons. Our evaluation demonstrates that\nAnyTop generalizes well, even with as few as three training examples per\ntopology, and can produce motions for unseen skeletons as well. Furthermore,\nour model's latent space is highly informative, enabling downstream tasks such\nas joint correspondence, temporal segmentation and motion editing. Our webpage,\nhttps://anytop2025.github.io/Anytop-page, includes links to videos and code.\n", "link": "http://arxiv.org/abs/2502.17327v1", "date": "2025-02-24", "relevancy": 2.3563, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6161}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5712}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyTop%3A%20Character%20Animation%20Diffusion%20with%20Any%20Topology&body=Title%3A%20AnyTop%3A%20Character%20Animation%20Diffusion%20with%20Any%20Topology%0AAuthor%3A%20Inbar%20Gat%20and%20Sigal%20Raab%20and%20Guy%20Tevet%20and%20Yuval%20Reshef%20and%20Amit%20H.%20Bermano%20and%20Daniel%20Cohen-Or%0AAbstract%3A%20%20%20Generating%20motion%20for%20arbitrary%20skeletons%20is%20a%20longstanding%20challenge%20in%0Acomputer%20graphics%2C%20remaining%20largely%20unexplored%20due%20to%20the%20scarcity%20of%20diverse%0Adatasets%20and%20the%20irregular%20nature%20of%20the%20data.%20In%20this%20work%2C%20we%20introduce%0AAnyTop%2C%20a%20diffusion%20model%20that%20generates%20motions%20for%20diverse%20characters%20with%0Adistinct%20motion%20dynamics%2C%20using%20only%20their%20skeletal%20structure%20as%20input.%20Our%0Awork%20features%20a%20transformer-based%20denoising%20network%2C%20tailored%20for%20arbitrary%0Askeleton%20learning%2C%20integrating%20topology%20information%20into%20the%20traditional%0Aattention%20mechanism.%20Additionally%2C%20by%20incorporating%20textual%20joint%20descriptions%0Ainto%20the%20latent%20feature%20representation%2C%20AnyTop%20learns%20semantic%20correspondences%0Abetween%20joints%20across%20diverse%20skeletons.%20Our%20evaluation%20demonstrates%20that%0AAnyTop%20generalizes%20well%2C%20even%20with%20as%20few%20as%20three%20training%20examples%20per%0Atopology%2C%20and%20can%20produce%20motions%20for%20unseen%20skeletons%20as%20well.%20Furthermore%2C%0Aour%20model%27s%20latent%20space%20is%20highly%20informative%2C%20enabling%20downstream%20tasks%20such%0Aas%20joint%20correspondence%2C%20temporal%20segmentation%20and%20motion%20editing.%20Our%20webpage%2C%0Ahttps%3A//anytop2025.github.io/Anytop-page%2C%20includes%20links%20to%20videos%20and%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyTop%253A%2520Character%2520Animation%2520Diffusion%2520with%2520Any%2520Topology%26entry.906535625%3DInbar%2520Gat%2520and%2520Sigal%2520Raab%2520and%2520Guy%2520Tevet%2520and%2520Yuval%2520Reshef%2520and%2520Amit%2520H.%2520Bermano%2520and%2520Daniel%2520Cohen-Or%26entry.1292438233%3D%2520%2520Generating%2520motion%2520for%2520arbitrary%2520skeletons%2520is%2520a%2520longstanding%2520challenge%2520in%250Acomputer%2520graphics%252C%2520remaining%2520largely%2520unexplored%2520due%2520to%2520the%2520scarcity%2520of%2520diverse%250Adatasets%2520and%2520the%2520irregular%2520nature%2520of%2520the%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%250AAnyTop%252C%2520a%2520diffusion%2520model%2520that%2520generates%2520motions%2520for%2520diverse%2520characters%2520with%250Adistinct%2520motion%2520dynamics%252C%2520using%2520only%2520their%2520skeletal%2520structure%2520as%2520input.%2520Our%250Awork%2520features%2520a%2520transformer-based%2520denoising%2520network%252C%2520tailored%2520for%2520arbitrary%250Askeleton%2520learning%252C%2520integrating%2520topology%2520information%2520into%2520the%2520traditional%250Aattention%2520mechanism.%2520Additionally%252C%2520by%2520incorporating%2520textual%2520joint%2520descriptions%250Ainto%2520the%2520latent%2520feature%2520representation%252C%2520AnyTop%2520learns%2520semantic%2520correspondences%250Abetween%2520joints%2520across%2520diverse%2520skeletons.%2520Our%2520evaluation%2520demonstrates%2520that%250AAnyTop%2520generalizes%2520well%252C%2520even%2520with%2520as%2520few%2520as%2520three%2520training%2520examples%2520per%250Atopology%252C%2520and%2520can%2520produce%2520motions%2520for%2520unseen%2520skeletons%2520as%2520well.%2520Furthermore%252C%250Aour%2520model%2527s%2520latent%2520space%2520is%2520highly%2520informative%252C%2520enabling%2520downstream%2520tasks%2520such%250Aas%2520joint%2520correspondence%252C%2520temporal%2520segmentation%2520and%2520motion%2520editing.%2520Our%2520webpage%252C%250Ahttps%253A//anytop2025.github.io/Anytop-page%252C%2520includes%2520links%2520to%2520videos%2520and%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyTop%3A%20Character%20Animation%20Diffusion%20with%20Any%20Topology&entry.906535625=Inbar%20Gat%20and%20Sigal%20Raab%20and%20Guy%20Tevet%20and%20Yuval%20Reshef%20and%20Amit%20H.%20Bermano%20and%20Daniel%20Cohen-Or&entry.1292438233=%20%20Generating%20motion%20for%20arbitrary%20skeletons%20is%20a%20longstanding%20challenge%20in%0Acomputer%20graphics%2C%20remaining%20largely%20unexplored%20due%20to%20the%20scarcity%20of%20diverse%0Adatasets%20and%20the%20irregular%20nature%20of%20the%20data.%20In%20this%20work%2C%20we%20introduce%0AAnyTop%2C%20a%20diffusion%20model%20that%20generates%20motions%20for%20diverse%20characters%20with%0Adistinct%20motion%20dynamics%2C%20using%20only%20their%20skeletal%20structure%20as%20input.%20Our%0Awork%20features%20a%20transformer-based%20denoising%20network%2C%20tailored%20for%20arbitrary%0Askeleton%20learning%2C%20integrating%20topology%20information%20into%20the%20traditional%0Aattention%20mechanism.%20Additionally%2C%20by%20incorporating%20textual%20joint%20descriptions%0Ainto%20the%20latent%20feature%20representation%2C%20AnyTop%20learns%20semantic%20correspondences%0Abetween%20joints%20across%20diverse%20skeletons.%20Our%20evaluation%20demonstrates%20that%0AAnyTop%20generalizes%20well%2C%20even%20with%20as%20few%20as%20three%20training%20examples%20per%0Atopology%2C%20and%20can%20produce%20motions%20for%20unseen%20skeletons%20as%20well.%20Furthermore%2C%0Aour%20model%27s%20latent%20space%20is%20highly%20informative%2C%20enabling%20downstream%20tasks%20such%0Aas%20joint%20correspondence%2C%20temporal%20segmentation%20and%20motion%20editing.%20Our%20webpage%2C%0Ahttps%3A//anytop2025.github.io/Anytop-page%2C%20includes%20links%20to%20videos%20and%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17327v1&entry.124074799=Read"},
{"title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch", "author": "Xueru Wen and Jie Lou and Zichao Li and Yaojie Lu and Xing Yu and Yuqiu Ji and Guohai Xu and Hongyu Lin and Ben He and Xianpei Han and Le Sun and Debing Zhang", "abstract": "  Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.\n", "link": "http://arxiv.org/abs/2502.17173v1", "date": "2025-02-24", "relevancy": 2.352, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4715}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cheems%3A%20A%20Practical%20Guidance%20for%20Building%20and%20Evaluating%20Chinese%20Reward%0A%20%20Models%20from%20Scratch&body=Title%3A%20Cheems%3A%20A%20Practical%20Guidance%20for%20Building%20and%20Evaluating%20Chinese%20Reward%0A%20%20Models%20from%20Scratch%0AAuthor%3A%20Xueru%20Wen%20and%20Jie%20Lou%20and%20Zichao%20Li%20and%20Yaojie%20Lu%20and%20Xing%20Yu%20and%20Yuqiu%20Ji%20and%20Guohai%20Xu%20and%20Hongyu%20Lin%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun%20and%20Debing%20Zhang%0AAbstract%3A%20%20%20Reward%20models%20%28RMs%29%20are%20crucial%20for%20aligning%20large%20language%20models%20%28LLMs%29%0Awith%20human%20preferences.%20However%2C%20most%20RM%20research%20is%20centered%20on%20English%20and%0Arelies%20heavily%20on%20synthetic%20resources%2C%20which%20leads%20to%20limited%20and%20less%20reliable%0Adatasets%20and%20benchmarks%20for%20Chinese.%20To%20address%20this%20gap%2C%20we%20introduce%0ACheemsBench%2C%20a%20fully%20human-annotated%20RM%20evaluation%20benchmark%20within%20Chinese%0Acontexts%2C%20and%20CheemsPreference%2C%20a%20large-scale%20and%20diverse%20preference%20dataset%0Aannotated%20through%20human-machine%20collaboration%20to%20support%20Chinese%20RM%20training.%0AWe%20systematically%20evaluate%20open-source%20discriminative%20and%20generative%20RMs%20on%0ACheemsBench%20and%20observe%20significant%20limitations%20in%20their%20ability%20to%20capture%0Ahuman%20preferences%20in%20Chinese%20scenarios.%20Additionally%2C%20based%20on%0ACheemsPreference%2C%20we%20construct%20an%20RM%20that%20achieves%20state-of-the-art%20performance%0Aon%20CheemsBench%2C%20demonstrating%20the%20necessity%20of%20human%20supervision%20in%20RM%0Atraining.%20Our%20findings%20reveal%20that%20scaled%20AI-generated%20data%20struggles%20to%20fully%0Acapture%20human%20preferences%2C%20emphasizing%20the%20importance%20of%20high-quality%20human%0Asupervision%20in%20RM%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCheems%253A%2520A%2520Practical%2520Guidance%2520for%2520Building%2520and%2520Evaluating%2520Chinese%2520Reward%250A%2520%2520Models%2520from%2520Scratch%26entry.906535625%3DXueru%2520Wen%2520and%2520Jie%2520Lou%2520and%2520Zichao%2520Li%2520and%2520Yaojie%2520Lu%2520and%2520Xing%2520Yu%2520and%2520Yuqiu%2520Ji%2520and%2520Guohai%2520Xu%2520and%2520Hongyu%2520Lin%2520and%2520Ben%2520He%2520and%2520Xianpei%2520Han%2520and%2520Le%2520Sun%2520and%2520Debing%2520Zhang%26entry.1292438233%3D%2520%2520Reward%2520models%2520%2528RMs%2529%2520are%2520crucial%2520for%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%250Awith%2520human%2520preferences.%2520However%252C%2520most%2520RM%2520research%2520is%2520centered%2520on%2520English%2520and%250Arelies%2520heavily%2520on%2520synthetic%2520resources%252C%2520which%2520leads%2520to%2520limited%2520and%2520less%2520reliable%250Adatasets%2520and%2520benchmarks%2520for%2520Chinese.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ACheemsBench%252C%2520a%2520fully%2520human-annotated%2520RM%2520evaluation%2520benchmark%2520within%2520Chinese%250Acontexts%252C%2520and%2520CheemsPreference%252C%2520a%2520large-scale%2520and%2520diverse%2520preference%2520dataset%250Aannotated%2520through%2520human-machine%2520collaboration%2520to%2520support%2520Chinese%2520RM%2520training.%250AWe%2520systematically%2520evaluate%2520open-source%2520discriminative%2520and%2520generative%2520RMs%2520on%250ACheemsBench%2520and%2520observe%2520significant%2520limitations%2520in%2520their%2520ability%2520to%2520capture%250Ahuman%2520preferences%2520in%2520Chinese%2520scenarios.%2520Additionally%252C%2520based%2520on%250ACheemsPreference%252C%2520we%2520construct%2520an%2520RM%2520that%2520achieves%2520state-of-the-art%2520performance%250Aon%2520CheemsBench%252C%2520demonstrating%2520the%2520necessity%2520of%2520human%2520supervision%2520in%2520RM%250Atraining.%2520Our%2520findings%2520reveal%2520that%2520scaled%2520AI-generated%2520data%2520struggles%2520to%2520fully%250Acapture%2520human%2520preferences%252C%2520emphasizing%2520the%2520importance%2520of%2520high-quality%2520human%250Asupervision%2520in%2520RM%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cheems%3A%20A%20Practical%20Guidance%20for%20Building%20and%20Evaluating%20Chinese%20Reward%0A%20%20Models%20from%20Scratch&entry.906535625=Xueru%20Wen%20and%20Jie%20Lou%20and%20Zichao%20Li%20and%20Yaojie%20Lu%20and%20Xing%20Yu%20and%20Yuqiu%20Ji%20and%20Guohai%20Xu%20and%20Hongyu%20Lin%20and%20Ben%20He%20and%20Xianpei%20Han%20and%20Le%20Sun%20and%20Debing%20Zhang&entry.1292438233=%20%20Reward%20models%20%28RMs%29%20are%20crucial%20for%20aligning%20large%20language%20models%20%28LLMs%29%0Awith%20human%20preferences.%20However%2C%20most%20RM%20research%20is%20centered%20on%20English%20and%0Arelies%20heavily%20on%20synthetic%20resources%2C%20which%20leads%20to%20limited%20and%20less%20reliable%0Adatasets%20and%20benchmarks%20for%20Chinese.%20To%20address%20this%20gap%2C%20we%20introduce%0ACheemsBench%2C%20a%20fully%20human-annotated%20RM%20evaluation%20benchmark%20within%20Chinese%0Acontexts%2C%20and%20CheemsPreference%2C%20a%20large-scale%20and%20diverse%20preference%20dataset%0Aannotated%20through%20human-machine%20collaboration%20to%20support%20Chinese%20RM%20training.%0AWe%20systematically%20evaluate%20open-source%20discriminative%20and%20generative%20RMs%20on%0ACheemsBench%20and%20observe%20significant%20limitations%20in%20their%20ability%20to%20capture%0Ahuman%20preferences%20in%20Chinese%20scenarios.%20Additionally%2C%20based%20on%0ACheemsPreference%2C%20we%20construct%20an%20RM%20that%20achieves%20state-of-the-art%20performance%0Aon%20CheemsBench%2C%20demonstrating%20the%20necessity%20of%20human%20supervision%20in%20RM%0Atraining.%20Our%20findings%20reveal%20that%20scaled%20AI-generated%20data%20struggles%20to%20fully%0Acapture%20human%20preferences%2C%20emphasizing%20the%20importance%20of%20high-quality%20human%0Asupervision%20in%20RM%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17173v1&entry.124074799=Read"},
{"title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization", "author": "Yen-Ju Lu and Ting-Yao Hu and Hema Swetha Koppula and Hadi Pouransari and Jen-Hao Rick Chang and Yin Xia and Xiang Kong and Qi Zhu and Simon Wang and Oncel Tuzel and Raviteja Vemulapalli", "abstract": "  In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks.\n", "link": "http://arxiv.org/abs/2502.17328v1", "date": "2025-02-24", "relevancy": 2.3499, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mutual%20Reinforcement%20of%20LLM%20Dialogue%20Synthesis%20and%20Summarization%0A%20%20Capabilities%20for%20Few-Shot%20Dialogue%20Summarization&body=Title%3A%20Mutual%20Reinforcement%20of%20LLM%20Dialogue%20Synthesis%20and%20Summarization%0A%20%20Capabilities%20for%20Few-Shot%20Dialogue%20Summarization%0AAuthor%3A%20Yen-Ju%20Lu%20and%20Ting-Yao%20Hu%20and%20Hema%20Swetha%20Koppula%20and%20Hadi%20Pouransari%20and%20Jen-Hao%20Rick%20Chang%20and%20Yin%20Xia%20and%20Xiang%20Kong%20and%20Qi%20Zhu%20and%20Simon%20Wang%20and%20Oncel%20Tuzel%20and%20Raviteja%20Vemulapalli%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20Mutual%20Reinforcing%20Data%20Synthesis%20%28MRDS%29%20within%20LLMs%0Ato%20improve%20few-shot%20dialogue%20summarization%20task.%20Unlike%20prior%20methods%20that%0Arequire%20external%20knowledge%2C%20we%20mutually%20reinforce%20the%20LLM%5C%27s%20dialogue%20synthesis%0Aand%20summarization%20capabilities%2C%20allowing%20them%20to%20complement%20each%20other%20during%0Atraining%20and%20enhance%20overall%20performances.%20The%20dialogue%20synthesis%20capability%20is%0Aenhanced%20by%20directed%20preference%20optimization%20with%20preference%20scoring%20from%0Asummarization%20capability.%20The%20summarization%20capability%20is%20enhanced%20by%20the%0Aadditional%20high%20quality%20dialogue-summary%20paired%20data%20produced%20by%20the%20dialogue%0Asynthesis%20capability.%20By%20leveraging%20the%20proposed%20MRDS%20mechanism%2C%20we%20elicit%20the%0Ainternal%20knowledge%20of%20LLM%20in%20the%20format%20of%20synthetic%20data%2C%20and%20use%20it%20to%0Aaugment%20the%20few-shot%20real%20training%20dataset.%20Empirical%20results%20demonstrate%20that%0Aour%20method%20improves%20dialogue%20summarization%2C%20achieving%20a%201.5%25%20increase%20in%20ROUGE%0Ascores%20and%20a%200.3%25%20improvement%20in%20BERT%20scores%20in%20few-shot%20settings.%20Furthermore%2C%0Aour%20method%20attains%20the%20highest%20average%20scores%20in%20human%20evaluations%2C%20surpassing%0Aboth%20the%20pre-trained%20models%20and%20the%20baselines%20fine-tuned%20solely%20for%0Asummarization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMutual%2520Reinforcement%2520of%2520LLM%2520Dialogue%2520Synthesis%2520and%2520Summarization%250A%2520%2520Capabilities%2520for%2520Few-Shot%2520Dialogue%2520Summarization%26entry.906535625%3DYen-Ju%2520Lu%2520and%2520Ting-Yao%2520Hu%2520and%2520Hema%2520Swetha%2520Koppula%2520and%2520Hadi%2520Pouransari%2520and%2520Jen-Hao%2520Rick%2520Chang%2520and%2520Yin%2520Xia%2520and%2520Xiang%2520Kong%2520and%2520Qi%2520Zhu%2520and%2520Simon%2520Wang%2520and%2520Oncel%2520Tuzel%2520and%2520Raviteja%2520Vemulapalli%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520Mutual%2520Reinforcing%2520Data%2520Synthesis%2520%2528MRDS%2529%2520within%2520LLMs%250Ato%2520improve%2520few-shot%2520dialogue%2520summarization%2520task.%2520Unlike%2520prior%2520methods%2520that%250Arequire%2520external%2520knowledge%252C%2520we%2520mutually%2520reinforce%2520the%2520LLM%255C%2527s%2520dialogue%2520synthesis%250Aand%2520summarization%2520capabilities%252C%2520allowing%2520them%2520to%2520complement%2520each%2520other%2520during%250Atraining%2520and%2520enhance%2520overall%2520performances.%2520The%2520dialogue%2520synthesis%2520capability%2520is%250Aenhanced%2520by%2520directed%2520preference%2520optimization%2520with%2520preference%2520scoring%2520from%250Asummarization%2520capability.%2520The%2520summarization%2520capability%2520is%2520enhanced%2520by%2520the%250Aadditional%2520high%2520quality%2520dialogue-summary%2520paired%2520data%2520produced%2520by%2520the%2520dialogue%250Asynthesis%2520capability.%2520By%2520leveraging%2520the%2520proposed%2520MRDS%2520mechanism%252C%2520we%2520elicit%2520the%250Ainternal%2520knowledge%2520of%2520LLM%2520in%2520the%2520format%2520of%2520synthetic%2520data%252C%2520and%2520use%2520it%2520to%250Aaugment%2520the%2520few-shot%2520real%2520training%2520dataset.%2520Empirical%2520results%2520demonstrate%2520that%250Aour%2520method%2520improves%2520dialogue%2520summarization%252C%2520achieving%2520a%25201.5%2525%2520increase%2520in%2520ROUGE%250Ascores%2520and%2520a%25200.3%2525%2520improvement%2520in%2520BERT%2520scores%2520in%2520few-shot%2520settings.%2520Furthermore%252C%250Aour%2520method%2520attains%2520the%2520highest%2520average%2520scores%2520in%2520human%2520evaluations%252C%2520surpassing%250Aboth%2520the%2520pre-trained%2520models%2520and%2520the%2520baselines%2520fine-tuned%2520solely%2520for%250Asummarization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mutual%20Reinforcement%20of%20LLM%20Dialogue%20Synthesis%20and%20Summarization%0A%20%20Capabilities%20for%20Few-Shot%20Dialogue%20Summarization&entry.906535625=Yen-Ju%20Lu%20and%20Ting-Yao%20Hu%20and%20Hema%20Swetha%20Koppula%20and%20Hadi%20Pouransari%20and%20Jen-Hao%20Rick%20Chang%20and%20Yin%20Xia%20and%20Xiang%20Kong%20and%20Qi%20Zhu%20and%20Simon%20Wang%20and%20Oncel%20Tuzel%20and%20Raviteja%20Vemulapalli&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20Mutual%20Reinforcing%20Data%20Synthesis%20%28MRDS%29%20within%20LLMs%0Ato%20improve%20few-shot%20dialogue%20summarization%20task.%20Unlike%20prior%20methods%20that%0Arequire%20external%20knowledge%2C%20we%20mutually%20reinforce%20the%20LLM%5C%27s%20dialogue%20synthesis%0Aand%20summarization%20capabilities%2C%20allowing%20them%20to%20complement%20each%20other%20during%0Atraining%20and%20enhance%20overall%20performances.%20The%20dialogue%20synthesis%20capability%20is%0Aenhanced%20by%20directed%20preference%20optimization%20with%20preference%20scoring%20from%0Asummarization%20capability.%20The%20summarization%20capability%20is%20enhanced%20by%20the%0Aadditional%20high%20quality%20dialogue-summary%20paired%20data%20produced%20by%20the%20dialogue%0Asynthesis%20capability.%20By%20leveraging%20the%20proposed%20MRDS%20mechanism%2C%20we%20elicit%20the%0Ainternal%20knowledge%20of%20LLM%20in%20the%20format%20of%20synthetic%20data%2C%20and%20use%20it%20to%0Aaugment%20the%20few-shot%20real%20training%20dataset.%20Empirical%20results%20demonstrate%20that%0Aour%20method%20improves%20dialogue%20summarization%2C%20achieving%20a%201.5%25%20increase%20in%20ROUGE%0Ascores%20and%20a%200.3%25%20improvement%20in%20BERT%20scores%20in%20few-shot%20settings.%20Furthermore%2C%0Aour%20method%20attains%20the%20highest%20average%20scores%20in%20human%20evaluations%2C%20surpassing%0Aboth%20the%20pre-trained%20models%20and%20the%20baselines%20fine-tuned%20solely%20for%0Asummarization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17328v1&entry.124074799=Read"},
{"title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design", "author": "Muhammad Haris Khan and Artyom Myshlyaev and Artyom Lykov and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "  We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability.\n", "link": "http://arxiv.org/abs/2502.17034v1", "date": "2025-02-24", "relevancy": 2.3465, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5855}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%206.0%3A%20Evolving%20Robotic%20Capabilities%20Through%20Generative%20Design&body=Title%3A%20Evolution%206.0%3A%20Evolving%20Robotic%20Capabilities%20Through%20Generative%20Design%0AAuthor%3A%20Muhammad%20Haris%20Khan%20and%20Artyom%20Myshlyaev%20and%20Artyom%20Lykov%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20We%20propose%20a%20new%20concept%2C%20Evolution%206.0%2C%20which%20represents%20the%20evolution%20of%0Arobotics%20driven%20by%20Generative%20AI.%20When%20a%20robot%20lacks%20the%20necessary%20tools%20to%0Aaccomplish%20a%20task%20requested%20by%20a%20human%2C%20it%20autonomously%20designs%20the%20required%0Ainstruments%20and%20learns%20how%20to%20use%20them%20to%20achieve%20the%20goal.%20Evolution%206.0%20is%20an%0Aautonomous%20robotic%20system%20powered%20by%20Vision-Language%20Models%20%28VLMs%29%2C%0AVision-Language%20Action%20%28VLA%29%20models%2C%20and%20Text-to-3D%20generative%20models%20for%20tool%0Adesign%20and%20task%20execution.%20The%20system%20comprises%20two%20key%20modules%3A%20the%20Tool%0AGeneration%20Module%2C%20which%20fabricates%20task-specific%20tools%20from%20visual%20and%20textual%0Adata%2C%20and%20the%20Action%20Generation%20Module%2C%20which%20converts%20natural%20language%0Ainstructions%20into%20robotic%20actions.%20It%20integrates%20QwenVLM%20for%20environmental%0Aunderstanding%2C%20OpenVLA%20for%20task%20execution%2C%20and%20Llama-Mesh%20for%203D%20tool%0Ageneration.%20Evaluation%20results%20demonstrate%20a%2090%25%20success%20rate%20for%20tool%0Ageneration%20with%20a%2010-second%20inference%20time%2C%20and%20action%20generation%20achieving%0A83.5%25%20in%20physical%20and%20visual%20generalization%2C%2070%25%20in%20motion%20generalization%2C%20and%0A37%25%20in%20semantic%20generalization.%20Future%20improvements%20will%20focus%20on%20bimanual%0Amanipulation%2C%20expanded%20task%20capabilities%2C%20and%20enhanced%20environmental%0Ainterpretation%20to%20improve%20real-world%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%25206.0%253A%2520Evolving%2520Robotic%2520Capabilities%2520Through%2520Generative%2520Design%26entry.906535625%3DMuhammad%2520Haris%2520Khan%2520and%2520Artyom%2520Myshlyaev%2520and%2520Artyom%2520Lykov%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520concept%252C%2520Evolution%25206.0%252C%2520which%2520represents%2520the%2520evolution%2520of%250Arobotics%2520driven%2520by%2520Generative%2520AI.%2520When%2520a%2520robot%2520lacks%2520the%2520necessary%2520tools%2520to%250Aaccomplish%2520a%2520task%2520requested%2520by%2520a%2520human%252C%2520it%2520autonomously%2520designs%2520the%2520required%250Ainstruments%2520and%2520learns%2520how%2520to%2520use%2520them%2520to%2520achieve%2520the%2520goal.%2520Evolution%25206.0%2520is%2520an%250Aautonomous%2520robotic%2520system%2520powered%2520by%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%250AVision-Language%2520Action%2520%2528VLA%2529%2520models%252C%2520and%2520Text-to-3D%2520generative%2520models%2520for%2520tool%250Adesign%2520and%2520task%2520execution.%2520The%2520system%2520comprises%2520two%2520key%2520modules%253A%2520the%2520Tool%250AGeneration%2520Module%252C%2520which%2520fabricates%2520task-specific%2520tools%2520from%2520visual%2520and%2520textual%250Adata%252C%2520and%2520the%2520Action%2520Generation%2520Module%252C%2520which%2520converts%2520natural%2520language%250Ainstructions%2520into%2520robotic%2520actions.%2520It%2520integrates%2520QwenVLM%2520for%2520environmental%250Aunderstanding%252C%2520OpenVLA%2520for%2520task%2520execution%252C%2520and%2520Llama-Mesh%2520for%25203D%2520tool%250Ageneration.%2520Evaluation%2520results%2520demonstrate%2520a%252090%2525%2520success%2520rate%2520for%2520tool%250Ageneration%2520with%2520a%252010-second%2520inference%2520time%252C%2520and%2520action%2520generation%2520achieving%250A83.5%2525%2520in%2520physical%2520and%2520visual%2520generalization%252C%252070%2525%2520in%2520motion%2520generalization%252C%2520and%250A37%2525%2520in%2520semantic%2520generalization.%2520Future%2520improvements%2520will%2520focus%2520on%2520bimanual%250Amanipulation%252C%2520expanded%2520task%2520capabilities%252C%2520and%2520enhanced%2520environmental%250Ainterpretation%2520to%2520improve%2520real-world%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%206.0%3A%20Evolving%20Robotic%20Capabilities%20Through%20Generative%20Design&entry.906535625=Muhammad%20Haris%20Khan%20and%20Artyom%20Myshlyaev%20and%20Artyom%20Lykov%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20We%20propose%20a%20new%20concept%2C%20Evolution%206.0%2C%20which%20represents%20the%20evolution%20of%0Arobotics%20driven%20by%20Generative%20AI.%20When%20a%20robot%20lacks%20the%20necessary%20tools%20to%0Aaccomplish%20a%20task%20requested%20by%20a%20human%2C%20it%20autonomously%20designs%20the%20required%0Ainstruments%20and%20learns%20how%20to%20use%20them%20to%20achieve%20the%20goal.%20Evolution%206.0%20is%20an%0Aautonomous%20robotic%20system%20powered%20by%20Vision-Language%20Models%20%28VLMs%29%2C%0AVision-Language%20Action%20%28VLA%29%20models%2C%20and%20Text-to-3D%20generative%20models%20for%20tool%0Adesign%20and%20task%20execution.%20The%20system%20comprises%20two%20key%20modules%3A%20the%20Tool%0AGeneration%20Module%2C%20which%20fabricates%20task-specific%20tools%20from%20visual%20and%20textual%0Adata%2C%20and%20the%20Action%20Generation%20Module%2C%20which%20converts%20natural%20language%0Ainstructions%20into%20robotic%20actions.%20It%20integrates%20QwenVLM%20for%20environmental%0Aunderstanding%2C%20OpenVLA%20for%20task%20execution%2C%20and%20Llama-Mesh%20for%203D%20tool%0Ageneration.%20Evaluation%20results%20demonstrate%20a%2090%25%20success%20rate%20for%20tool%0Ageneration%20with%20a%2010-second%20inference%20time%2C%20and%20action%20generation%20achieving%0A83.5%25%20in%20physical%20and%20visual%20generalization%2C%2070%25%20in%20motion%20generalization%2C%20and%0A37%25%20in%20semantic%20generalization.%20Future%20improvements%20will%20focus%20on%20bimanual%0Amanipulation%2C%20expanded%20task%20capabilities%2C%20and%20enhanced%20environmental%0Ainterpretation%20to%20improve%20real-world%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17034v1&entry.124074799=Read"},
{"title": "Sustainable Greenhouse Management: A Comparative Analysis of Recurrent\n  and Graph Neural Networks", "author": "Emiliano Seri and Marcello Petitta and Cristina Cornaro", "abstract": "  The integration of photovoltaic (PV) systems into greenhouses not only\noptimizes land use but also enhances sustainable agricultural practices by\nenabling dual benefits of food production and renewable energy generation.\nHowever, accurate prediction of internal environmental conditions is crucial to\nensure optimal crop growth while maximizing energy production. This study\nintroduces a novel application of Spatio-Temporal Graph Neural Networks\n(STGNNs) to greenhouse microclimate modeling, comparing their performance with\ntraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal\npattern recognition, they cannot explicitly model the directional relationships\nbetween environmental variables. Our STGNN approach addresses this limitation\nby representing these relationships as directed graphs, enabling the model to\ncapture both spatial dependencies and their directionality. Using\nhigh-frequency data collected at 15-minute intervals from a greenhouse in\nVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter\nconditions (R^2 = 0.985) but show limitations during summer cooling system\noperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),\ntheir architecture offers greater potential for integrating additional\nvariables such as PV generation and crop growth indicators.\n", "link": "http://arxiv.org/abs/2502.17371v1", "date": "2025-02-24", "relevancy": 2.3336, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4754}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4697}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sustainable%20Greenhouse%20Management%3A%20A%20Comparative%20Analysis%20of%20Recurrent%0A%20%20and%20Graph%20Neural%20Networks&body=Title%3A%20Sustainable%20Greenhouse%20Management%3A%20A%20Comparative%20Analysis%20of%20Recurrent%0A%20%20and%20Graph%20Neural%20Networks%0AAuthor%3A%20Emiliano%20Seri%20and%20Marcello%20Petitta%20and%20Cristina%20Cornaro%0AAbstract%3A%20%20%20The%20integration%20of%20photovoltaic%20%28PV%29%20systems%20into%20greenhouses%20not%20only%0Aoptimizes%20land%20use%20but%20also%20enhances%20sustainable%20agricultural%20practices%20by%0Aenabling%20dual%20benefits%20of%20food%20production%20and%20renewable%20energy%20generation.%0AHowever%2C%20accurate%20prediction%20of%20internal%20environmental%20conditions%20is%20crucial%20to%0Aensure%20optimal%20crop%20growth%20while%20maximizing%20energy%20production.%20This%20study%0Aintroduces%20a%20novel%20application%20of%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%28STGNNs%29%20to%20greenhouse%20microclimate%20modeling%2C%20comparing%20their%20performance%20with%0Atraditional%20Recurrent%20Neural%20Networks%20%28RNNs%29.%20While%20RNNs%20excel%20at%20temporal%0Apattern%20recognition%2C%20they%20cannot%20explicitly%20model%20the%20directional%20relationships%0Abetween%20environmental%20variables.%20Our%20STGNN%20approach%20addresses%20this%20limitation%0Aby%20representing%20these%20relationships%20as%20directed%20graphs%2C%20enabling%20the%20model%20to%0Acapture%20both%20spatial%20dependencies%20and%20their%20directionality.%20Using%0Ahigh-frequency%20data%20collected%20at%2015-minute%20intervals%20from%20a%20greenhouse%20in%0AVolos%2C%20Greece%2C%20we%20demonstrate%20that%20RNNs%20achieve%20exceptional%20accuracy%20in%20winter%0Aconditions%20%28R%5E2%20%3D%200.985%29%20but%20show%20limitations%20during%20summer%20cooling%20system%0Aoperation.%20Though%20STGNNs%20currently%20show%20lower%20performance%20%28winter%20R%5E2%20%3D%200.947%29%2C%0Atheir%20architecture%20offers%20greater%20potential%20for%20integrating%20additional%0Avariables%20such%20as%20PV%20generation%20and%20crop%20growth%20indicators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSustainable%2520Greenhouse%2520Management%253A%2520A%2520Comparative%2520Analysis%2520of%2520Recurrent%250A%2520%2520and%2520Graph%2520Neural%2520Networks%26entry.906535625%3DEmiliano%2520Seri%2520and%2520Marcello%2520Petitta%2520and%2520Cristina%2520Cornaro%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520photovoltaic%2520%2528PV%2529%2520systems%2520into%2520greenhouses%2520not%2520only%250Aoptimizes%2520land%2520use%2520but%2520also%2520enhances%2520sustainable%2520agricultural%2520practices%2520by%250Aenabling%2520dual%2520benefits%2520of%2520food%2520production%2520and%2520renewable%2520energy%2520generation.%250AHowever%252C%2520accurate%2520prediction%2520of%2520internal%2520environmental%2520conditions%2520is%2520crucial%2520to%250Aensure%2520optimal%2520crop%2520growth%2520while%2520maximizing%2520energy%2520production.%2520This%2520study%250Aintroduces%2520a%2520novel%2520application%2520of%2520Spatio-Temporal%2520Graph%2520Neural%2520Networks%250A%2528STGNNs%2529%2520to%2520greenhouse%2520microclimate%2520modeling%252C%2520comparing%2520their%2520performance%2520with%250Atraditional%2520Recurrent%2520Neural%2520Networks%2520%2528RNNs%2529.%2520While%2520RNNs%2520excel%2520at%2520temporal%250Apattern%2520recognition%252C%2520they%2520cannot%2520explicitly%2520model%2520the%2520directional%2520relationships%250Abetween%2520environmental%2520variables.%2520Our%2520STGNN%2520approach%2520addresses%2520this%2520limitation%250Aby%2520representing%2520these%2520relationships%2520as%2520directed%2520graphs%252C%2520enabling%2520the%2520model%2520to%250Acapture%2520both%2520spatial%2520dependencies%2520and%2520their%2520directionality.%2520Using%250Ahigh-frequency%2520data%2520collected%2520at%252015-minute%2520intervals%2520from%2520a%2520greenhouse%2520in%250AVolos%252C%2520Greece%252C%2520we%2520demonstrate%2520that%2520RNNs%2520achieve%2520exceptional%2520accuracy%2520in%2520winter%250Aconditions%2520%2528R%255E2%2520%253D%25200.985%2529%2520but%2520show%2520limitations%2520during%2520summer%2520cooling%2520system%250Aoperation.%2520Though%2520STGNNs%2520currently%2520show%2520lower%2520performance%2520%2528winter%2520R%255E2%2520%253D%25200.947%2529%252C%250Atheir%2520architecture%2520offers%2520greater%2520potential%2520for%2520integrating%2520additional%250Avariables%2520such%2520as%2520PV%2520generation%2520and%2520crop%2520growth%2520indicators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sustainable%20Greenhouse%20Management%3A%20A%20Comparative%20Analysis%20of%20Recurrent%0A%20%20and%20Graph%20Neural%20Networks&entry.906535625=Emiliano%20Seri%20and%20Marcello%20Petitta%20and%20Cristina%20Cornaro&entry.1292438233=%20%20The%20integration%20of%20photovoltaic%20%28PV%29%20systems%20into%20greenhouses%20not%20only%0Aoptimizes%20land%20use%20but%20also%20enhances%20sustainable%20agricultural%20practices%20by%0Aenabling%20dual%20benefits%20of%20food%20production%20and%20renewable%20energy%20generation.%0AHowever%2C%20accurate%20prediction%20of%20internal%20environmental%20conditions%20is%20crucial%20to%0Aensure%20optimal%20crop%20growth%20while%20maximizing%20energy%20production.%20This%20study%0Aintroduces%20a%20novel%20application%20of%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%28STGNNs%29%20to%20greenhouse%20microclimate%20modeling%2C%20comparing%20their%20performance%20with%0Atraditional%20Recurrent%20Neural%20Networks%20%28RNNs%29.%20While%20RNNs%20excel%20at%20temporal%0Apattern%20recognition%2C%20they%20cannot%20explicitly%20model%20the%20directional%20relationships%0Abetween%20environmental%20variables.%20Our%20STGNN%20approach%20addresses%20this%20limitation%0Aby%20representing%20these%20relationships%20as%20directed%20graphs%2C%20enabling%20the%20model%20to%0Acapture%20both%20spatial%20dependencies%20and%20their%20directionality.%20Using%0Ahigh-frequency%20data%20collected%20at%2015-minute%20intervals%20from%20a%20greenhouse%20in%0AVolos%2C%20Greece%2C%20we%20demonstrate%20that%20RNNs%20achieve%20exceptional%20accuracy%20in%20winter%0Aconditions%20%28R%5E2%20%3D%200.985%29%20but%20show%20limitations%20during%20summer%20cooling%20system%0Aoperation.%20Though%20STGNNs%20currently%20show%20lower%20performance%20%28winter%20R%5E2%20%3D%200.947%29%2C%0Atheir%20architecture%20offers%20greater%20potential%20for%20integrating%20additional%0Avariables%20such%20as%20PV%20generation%20and%20crop%20growth%20indicators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17371v1&entry.124074799=Read"},
{"title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models", "author": "Gokul Karthik Kumar and Iheb Chaabane and Kebin Wu", "abstract": "  Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a 'leaky modality mix', where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.\n", "link": "http://arxiv.org/abs/2502.10250v2", "date": "2025-02-24", "relevancy": 2.3327, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5952}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisCon-100K%3A%20Leveraging%20Contextual%20Web%20Data%20for%20Fine-tuning%20Vision%0A%20%20Language%20Models&body=Title%3A%20VisCon-100K%3A%20Leveraging%20Contextual%20Web%20Data%20for%20Fine-tuning%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Gokul%20Karthik%20Kumar%20and%20Iheb%20Chaabane%20and%20Kebin%20Wu%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20various%20visual%20benchmarks%20but%20are%0Aoften%20constrained%20by%20the%20lack%20of%20high-quality%20visual%20fine-tuning%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20VisCon-100K%2C%20a%20novel%20dataset%20derived%20from%0Ainterleaved%20image-text%20web%20documents.%20Our%20approach%20transforms%2045K%20web%20documents%0Afrom%20the%20OBELICS%20dataset%20into%20100K%20image%20conversation%20samples.%20We%20utilize%0AGPT-4V%20to%20generate%20image-contextual%20captions%20and%20OpenChat%203.5%20model%20to%20convert%0Athese%20captions%20into%20diverse%20free-form%20and%20multiple-choice%20question-answer%0Apairs.%20Integrating%20this%20dataset%20for%20fine-tuning%20considerably%20enhances%20VLM%0Aperformance%20across%20multiple%20benchmarks.%20Unlike%20methods%20that%20focus%20solely%20on%0Afine-grained%20visual%20content%2C%20our%20approach%20leverages%20accompanying%20web%20context%2C%0Ayielding%20superior%20results.%20We%20also%20discover%20that%20a%20%27leaky%20modality%20mix%27%2C%20where%0Aconversation%20samples%20contain%20questions%20answerable%20from%20both%20the%20image%20and%20its%0Acontextual%20caption%2C%20outperforms%20non-leaky%20combinations%20of%20captions%20and%20Q%26A%0Apairs.%20VisCon-100k%20dataset%20shows%20strong%20performance%20with%20two%20popular%20VLM%0Aapproaches%3A%20text-only%20large%20language%20model%20%28LLM%29%20aligned%20with%20a%20vision%20encoder%0Ausing%20image%20captions%20data%20%28ShareGPT4V-7b%29%20and%20multimodally%20pretrained%20LLM%0A%28IDEFICS2-8b%29%20using%20interleaved%20image-text%20data.%20In%20addition%20to%20releasing%20the%0AVisCon-100K%20dataset%2C%20we%20provide%20a%20contextual%20captioner%20trained%20on%20this%20dataset%2C%0Afacilitating%20scalable%20fine-tuning%20data%20generation%20for%20future%20research%20and%0Aopen-source%20applications.%20Using%20the%20same%20pipeline%2C%20but%20substituting%20our%20trained%0Acontextual%20captioner%20for%20GPT-4V%2C%20we%20also%20release%20the%20larger%20VisCon-1M%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10250v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisCon-100K%253A%2520Leveraging%2520Contextual%2520Web%2520Data%2520for%2520Fine-tuning%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DGokul%2520Karthik%2520Kumar%2520and%2520Iheb%2520Chaabane%2520and%2520Kebin%2520Wu%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520excel%2520in%2520various%2520visual%2520benchmarks%2520but%2520are%250Aoften%2520constrained%2520by%2520the%2520lack%2520of%2520high-quality%2520visual%2520fine-tuning%2520data.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520introduce%2520VisCon-100K%252C%2520a%2520novel%2520dataset%2520derived%2520from%250Ainterleaved%2520image-text%2520web%2520documents.%2520Our%2520approach%2520transforms%252045K%2520web%2520documents%250Afrom%2520the%2520OBELICS%2520dataset%2520into%2520100K%2520image%2520conversation%2520samples.%2520We%2520utilize%250AGPT-4V%2520to%2520generate%2520image-contextual%2520captions%2520and%2520OpenChat%25203.5%2520model%2520to%2520convert%250Athese%2520captions%2520into%2520diverse%2520free-form%2520and%2520multiple-choice%2520question-answer%250Apairs.%2520Integrating%2520this%2520dataset%2520for%2520fine-tuning%2520considerably%2520enhances%2520VLM%250Aperformance%2520across%2520multiple%2520benchmarks.%2520Unlike%2520methods%2520that%2520focus%2520solely%2520on%250Afine-grained%2520visual%2520content%252C%2520our%2520approach%2520leverages%2520accompanying%2520web%2520context%252C%250Ayielding%2520superior%2520results.%2520We%2520also%2520discover%2520that%2520a%2520%2527leaky%2520modality%2520mix%2527%252C%2520where%250Aconversation%2520samples%2520contain%2520questions%2520answerable%2520from%2520both%2520the%2520image%2520and%2520its%250Acontextual%2520caption%252C%2520outperforms%2520non-leaky%2520combinations%2520of%2520captions%2520and%2520Q%2526A%250Apairs.%2520VisCon-100k%2520dataset%2520shows%2520strong%2520performance%2520with%2520two%2520popular%2520VLM%250Aapproaches%253A%2520text-only%2520large%2520language%2520model%2520%2528LLM%2529%2520aligned%2520with%2520a%2520vision%2520encoder%250Ausing%2520image%2520captions%2520data%2520%2528ShareGPT4V-7b%2529%2520and%2520multimodally%2520pretrained%2520LLM%250A%2528IDEFICS2-8b%2529%2520using%2520interleaved%2520image-text%2520data.%2520In%2520addition%2520to%2520releasing%2520the%250AVisCon-100K%2520dataset%252C%2520we%2520provide%2520a%2520contextual%2520captioner%2520trained%2520on%2520this%2520dataset%252C%250Afacilitating%2520scalable%2520fine-tuning%2520data%2520generation%2520for%2520future%2520research%2520and%250Aopen-source%2520applications.%2520Using%2520the%2520same%2520pipeline%252C%2520but%2520substituting%2520our%2520trained%250Acontextual%2520captioner%2520for%2520GPT-4V%252C%2520we%2520also%2520release%2520the%2520larger%2520VisCon-1M%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10250v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisCon-100K%3A%20Leveraging%20Contextual%20Web%20Data%20for%20Fine-tuning%20Vision%0A%20%20Language%20Models&entry.906535625=Gokul%20Karthik%20Kumar%20and%20Iheb%20Chaabane%20and%20Kebin%20Wu&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20various%20visual%20benchmarks%20but%20are%0Aoften%20constrained%20by%20the%20lack%20of%20high-quality%20visual%20fine-tuning%20data.%20To%0Aaddress%20this%20challenge%2C%20we%20introduce%20VisCon-100K%2C%20a%20novel%20dataset%20derived%20from%0Ainterleaved%20image-text%20web%20documents.%20Our%20approach%20transforms%2045K%20web%20documents%0Afrom%20the%20OBELICS%20dataset%20into%20100K%20image%20conversation%20samples.%20We%20utilize%0AGPT-4V%20to%20generate%20image-contextual%20captions%20and%20OpenChat%203.5%20model%20to%20convert%0Athese%20captions%20into%20diverse%20free-form%20and%20multiple-choice%20question-answer%0Apairs.%20Integrating%20this%20dataset%20for%20fine-tuning%20considerably%20enhances%20VLM%0Aperformance%20across%20multiple%20benchmarks.%20Unlike%20methods%20that%20focus%20solely%20on%0Afine-grained%20visual%20content%2C%20our%20approach%20leverages%20accompanying%20web%20context%2C%0Ayielding%20superior%20results.%20We%20also%20discover%20that%20a%20%27leaky%20modality%20mix%27%2C%20where%0Aconversation%20samples%20contain%20questions%20answerable%20from%20both%20the%20image%20and%20its%0Acontextual%20caption%2C%20outperforms%20non-leaky%20combinations%20of%20captions%20and%20Q%26A%0Apairs.%20VisCon-100k%20dataset%20shows%20strong%20performance%20with%20two%20popular%20VLM%0Aapproaches%3A%20text-only%20large%20language%20model%20%28LLM%29%20aligned%20with%20a%20vision%20encoder%0Ausing%20image%20captions%20data%20%28ShareGPT4V-7b%29%20and%20multimodally%20pretrained%20LLM%0A%28IDEFICS2-8b%29%20using%20interleaved%20image-text%20data.%20In%20addition%20to%20releasing%20the%0AVisCon-100K%20dataset%2C%20we%20provide%20a%20contextual%20captioner%20trained%20on%20this%20dataset%2C%0Afacilitating%20scalable%20fine-tuning%20data%20generation%20for%20future%20research%20and%0Aopen-source%20applications.%20Using%20the%20same%20pipeline%2C%20but%20substituting%20our%20trained%0Acontextual%20captioner%20for%20GPT-4V%2C%20we%20also%20release%20the%20larger%20VisCon-1M%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10250v2&entry.124074799=Read"},
{"title": "SFLD: Reducing the content bias for AI-generated Image Detection", "author": "Seoyeon Gye and Junwon Ko and Hyounguk Shon and Minchan Kwon and Junmo Kim", "abstract": "  Identifying AI-generated content is critical for the safe and ethical use of\ngenerative AI. Recent research has focused on developing detectors that\ngeneralize to unknown generators, with popular methods relying either on\nhigh-level features or low-level fingerprints. However, these methods have\nclear limitations: biased towards unseen content, or vulnerable to common image\ndegradations, such as JPEG compression. To address these issues, we propose a\nnovel approach, SFLD, which incorporates PatchShuffle to integrate high-level\nsemantic and low-level textural information. SFLD applies PatchShuffle at\nmultiple levels, improving robustness and generalization across various\ngenerative models. Additionally, current benchmarks face challenges such as low\nimage quality, insufficient content preservation, and limited class diversity.\nIn response, we introduce TwinSynths, a new benchmark generation methodology\nthat constructs visually near-identical pairs of real and synthetic images to\nensure high quality and content preservation. Our extensive experiments and\nanalysis show that SFLD outperforms existing methods on detecting a wide\nvariety of fake images sourced from GANs, diffusion models, and TwinSynths,\ndemonstrating the state-of-the-art performance and generalization capabilities\nto novel generative models.\n", "link": "http://arxiv.org/abs/2502.17105v1", "date": "2025-02-24", "relevancy": 2.3155, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5807}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5798}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFLD%3A%20Reducing%20the%20content%20bias%20for%20AI-generated%20Image%20Detection&body=Title%3A%20SFLD%3A%20Reducing%20the%20content%20bias%20for%20AI-generated%20Image%20Detection%0AAuthor%3A%20Seoyeon%20Gye%20and%20Junwon%20Ko%20and%20Hyounguk%20Shon%20and%20Minchan%20Kwon%20and%20Junmo%20Kim%0AAbstract%3A%20%20%20Identifying%20AI-generated%20content%20is%20critical%20for%20the%20safe%20and%20ethical%20use%20of%0Agenerative%20AI.%20Recent%20research%20has%20focused%20on%20developing%20detectors%20that%0Ageneralize%20to%20unknown%20generators%2C%20with%20popular%20methods%20relying%20either%20on%0Ahigh-level%20features%20or%20low-level%20fingerprints.%20However%2C%20these%20methods%20have%0Aclear%20limitations%3A%20biased%20towards%20unseen%20content%2C%20or%20vulnerable%20to%20common%20image%0Adegradations%2C%20such%20as%20JPEG%20compression.%20To%20address%20these%20issues%2C%20we%20propose%20a%0Anovel%20approach%2C%20SFLD%2C%20which%20incorporates%20PatchShuffle%20to%20integrate%20high-level%0Asemantic%20and%20low-level%20textural%20information.%20SFLD%20applies%20PatchShuffle%20at%0Amultiple%20levels%2C%20improving%20robustness%20and%20generalization%20across%20various%0Agenerative%20models.%20Additionally%2C%20current%20benchmarks%20face%20challenges%20such%20as%20low%0Aimage%20quality%2C%20insufficient%20content%20preservation%2C%20and%20limited%20class%20diversity.%0AIn%20response%2C%20we%20introduce%20TwinSynths%2C%20a%20new%20benchmark%20generation%20methodology%0Athat%20constructs%20visually%20near-identical%20pairs%20of%20real%20and%20synthetic%20images%20to%0Aensure%20high%20quality%20and%20content%20preservation.%20Our%20extensive%20experiments%20and%0Aanalysis%20show%20that%20SFLD%20outperforms%20existing%20methods%20on%20detecting%20a%20wide%0Avariety%20of%20fake%20images%20sourced%20from%20GANs%2C%20diffusion%20models%2C%20and%20TwinSynths%2C%0Ademonstrating%20the%20state-of-the-art%20performance%20and%20generalization%20capabilities%0Ato%20novel%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFLD%253A%2520Reducing%2520the%2520content%2520bias%2520for%2520AI-generated%2520Image%2520Detection%26entry.906535625%3DSeoyeon%2520Gye%2520and%2520Junwon%2520Ko%2520and%2520Hyounguk%2520Shon%2520and%2520Minchan%2520Kwon%2520and%2520Junmo%2520Kim%26entry.1292438233%3D%2520%2520Identifying%2520AI-generated%2520content%2520is%2520critical%2520for%2520the%2520safe%2520and%2520ethical%2520use%2520of%250Agenerative%2520AI.%2520Recent%2520research%2520has%2520focused%2520on%2520developing%2520detectors%2520that%250Ageneralize%2520to%2520unknown%2520generators%252C%2520with%2520popular%2520methods%2520relying%2520either%2520on%250Ahigh-level%2520features%2520or%2520low-level%2520fingerprints.%2520However%252C%2520these%2520methods%2520have%250Aclear%2520limitations%253A%2520biased%2520towards%2520unseen%2520content%252C%2520or%2520vulnerable%2520to%2520common%2520image%250Adegradations%252C%2520such%2520as%2520JPEG%2520compression.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%250Anovel%2520approach%252C%2520SFLD%252C%2520which%2520incorporates%2520PatchShuffle%2520to%2520integrate%2520high-level%250Asemantic%2520and%2520low-level%2520textural%2520information.%2520SFLD%2520applies%2520PatchShuffle%2520at%250Amultiple%2520levels%252C%2520improving%2520robustness%2520and%2520generalization%2520across%2520various%250Agenerative%2520models.%2520Additionally%252C%2520current%2520benchmarks%2520face%2520challenges%2520such%2520as%2520low%250Aimage%2520quality%252C%2520insufficient%2520content%2520preservation%252C%2520and%2520limited%2520class%2520diversity.%250AIn%2520response%252C%2520we%2520introduce%2520TwinSynths%252C%2520a%2520new%2520benchmark%2520generation%2520methodology%250Athat%2520constructs%2520visually%2520near-identical%2520pairs%2520of%2520real%2520and%2520synthetic%2520images%2520to%250Aensure%2520high%2520quality%2520and%2520content%2520preservation.%2520Our%2520extensive%2520experiments%2520and%250Aanalysis%2520show%2520that%2520SFLD%2520outperforms%2520existing%2520methods%2520on%2520detecting%2520a%2520wide%250Avariety%2520of%2520fake%2520images%2520sourced%2520from%2520GANs%252C%2520diffusion%2520models%252C%2520and%2520TwinSynths%252C%250Ademonstrating%2520the%2520state-of-the-art%2520performance%2520and%2520generalization%2520capabilities%250Ato%2520novel%2520generative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFLD%3A%20Reducing%20the%20content%20bias%20for%20AI-generated%20Image%20Detection&entry.906535625=Seoyeon%20Gye%20and%20Junwon%20Ko%20and%20Hyounguk%20Shon%20and%20Minchan%20Kwon%20and%20Junmo%20Kim&entry.1292438233=%20%20Identifying%20AI-generated%20content%20is%20critical%20for%20the%20safe%20and%20ethical%20use%20of%0Agenerative%20AI.%20Recent%20research%20has%20focused%20on%20developing%20detectors%20that%0Ageneralize%20to%20unknown%20generators%2C%20with%20popular%20methods%20relying%20either%20on%0Ahigh-level%20features%20or%20low-level%20fingerprints.%20However%2C%20these%20methods%20have%0Aclear%20limitations%3A%20biased%20towards%20unseen%20content%2C%20or%20vulnerable%20to%20common%20image%0Adegradations%2C%20such%20as%20JPEG%20compression.%20To%20address%20these%20issues%2C%20we%20propose%20a%0Anovel%20approach%2C%20SFLD%2C%20which%20incorporates%20PatchShuffle%20to%20integrate%20high-level%0Asemantic%20and%20low-level%20textural%20information.%20SFLD%20applies%20PatchShuffle%20at%0Amultiple%20levels%2C%20improving%20robustness%20and%20generalization%20across%20various%0Agenerative%20models.%20Additionally%2C%20current%20benchmarks%20face%20challenges%20such%20as%20low%0Aimage%20quality%2C%20insufficient%20content%20preservation%2C%20and%20limited%20class%20diversity.%0AIn%20response%2C%20we%20introduce%20TwinSynths%2C%20a%20new%20benchmark%20generation%20methodology%0Athat%20constructs%20visually%20near-identical%20pairs%20of%20real%20and%20synthetic%20images%20to%0Aensure%20high%20quality%20and%20content%20preservation.%20Our%20extensive%20experiments%20and%0Aanalysis%20show%20that%20SFLD%20outperforms%20existing%20methods%20on%20detecting%20a%20wide%0Avariety%20of%20fake%20images%20sourced%20from%20GANs%2C%20diffusion%20models%2C%20and%20TwinSynths%2C%0Ademonstrating%20the%20state-of-the-art%20performance%20and%20generalization%20capabilities%0Ato%20novel%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17105v1&entry.124074799=Read"},
{"title": "Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative\n  Refinement", "author": "Rui Liu", "abstract": "  Real-world image matting is essential for applications in content creation\nand augmented reality. However, it remains challenging due to the complex\nnature of scenes and the scarcity of high-quality datasets. To address these\nlimitations, we introduce Mask2Alpha, an iterative refinement framework\ndesigned to enhance semantic comprehension, instance awareness, and fine-detail\nrecovery in image matting. Our framework leverages self-supervised Vision\nTransformer features as semantic priors, strengthening contextual understanding\nin complex scenarios. To further improve instance differentiation, we implement\na mask-guided feature selection module, enabling precise targeting of objects\nin multi-instance settings. Additionally, a sparse convolution-based\noptimization scheme allows Mask2Alpha to recover high-resolution details\nthrough progressive refinement,from low-resolution semantic passes to\nhigh-resolution sparse reconstructions. Benchmarking across various real-world\ndatasets, Mask2Alpha consistently achieves state-of-the-art results, showcasing\nits effectiveness in accurate and efficient image matting.\n", "link": "http://arxiv.org/abs/2502.17093v1", "date": "2025-02-24", "relevancy": 2.2997, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5952}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Image%20Matting%20in%20Real-World%20Scenes%20with%20Mask-Guided%20Iterative%0A%20%20Refinement&body=Title%3A%20Enhancing%20Image%20Matting%20in%20Real-World%20Scenes%20with%20Mask-Guided%20Iterative%0A%20%20Refinement%0AAuthor%3A%20Rui%20Liu%0AAbstract%3A%20%20%20Real-world%20image%20matting%20is%20essential%20for%20applications%20in%20content%20creation%0Aand%20augmented%20reality.%20However%2C%20it%20remains%20challenging%20due%20to%20the%20complex%0Anature%20of%20scenes%20and%20the%20scarcity%20of%20high-quality%20datasets.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20Mask2Alpha%2C%20an%20iterative%20refinement%20framework%0Adesigned%20to%20enhance%20semantic%20comprehension%2C%20instance%20awareness%2C%20and%20fine-detail%0Arecovery%20in%20image%20matting.%20Our%20framework%20leverages%20self-supervised%20Vision%0ATransformer%20features%20as%20semantic%20priors%2C%20strengthening%20contextual%20understanding%0Ain%20complex%20scenarios.%20To%20further%20improve%20instance%20differentiation%2C%20we%20implement%0Aa%20mask-guided%20feature%20selection%20module%2C%20enabling%20precise%20targeting%20of%20objects%0Ain%20multi-instance%20settings.%20Additionally%2C%20a%20sparse%20convolution-based%0Aoptimization%20scheme%20allows%20Mask2Alpha%20to%20recover%20high-resolution%20details%0Athrough%20progressive%20refinement%2Cfrom%20low-resolution%20semantic%20passes%20to%0Ahigh-resolution%20sparse%20reconstructions.%20Benchmarking%20across%20various%20real-world%0Adatasets%2C%20Mask2Alpha%20consistently%20achieves%20state-of-the-art%20results%2C%20showcasing%0Aits%20effectiveness%20in%20accurate%20and%20efficient%20image%20matting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Image%2520Matting%2520in%2520Real-World%2520Scenes%2520with%2520Mask-Guided%2520Iterative%250A%2520%2520Refinement%26entry.906535625%3DRui%2520Liu%26entry.1292438233%3D%2520%2520Real-world%2520image%2520matting%2520is%2520essential%2520for%2520applications%2520in%2520content%2520creation%250Aand%2520augmented%2520reality.%2520However%252C%2520it%2520remains%2520challenging%2520due%2520to%2520the%2520complex%250Anature%2520of%2520scenes%2520and%2520the%2520scarcity%2520of%2520high-quality%2520datasets.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520introduce%2520Mask2Alpha%252C%2520an%2520iterative%2520refinement%2520framework%250Adesigned%2520to%2520enhance%2520semantic%2520comprehension%252C%2520instance%2520awareness%252C%2520and%2520fine-detail%250Arecovery%2520in%2520image%2520matting.%2520Our%2520framework%2520leverages%2520self-supervised%2520Vision%250ATransformer%2520features%2520as%2520semantic%2520priors%252C%2520strengthening%2520contextual%2520understanding%250Ain%2520complex%2520scenarios.%2520To%2520further%2520improve%2520instance%2520differentiation%252C%2520we%2520implement%250Aa%2520mask-guided%2520feature%2520selection%2520module%252C%2520enabling%2520precise%2520targeting%2520of%2520objects%250Ain%2520multi-instance%2520settings.%2520Additionally%252C%2520a%2520sparse%2520convolution-based%250Aoptimization%2520scheme%2520allows%2520Mask2Alpha%2520to%2520recover%2520high-resolution%2520details%250Athrough%2520progressive%2520refinement%252Cfrom%2520low-resolution%2520semantic%2520passes%2520to%250Ahigh-resolution%2520sparse%2520reconstructions.%2520Benchmarking%2520across%2520various%2520real-world%250Adatasets%252C%2520Mask2Alpha%2520consistently%2520achieves%2520state-of-the-art%2520results%252C%2520showcasing%250Aits%2520effectiveness%2520in%2520accurate%2520and%2520efficient%2520image%2520matting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Image%20Matting%20in%20Real-World%20Scenes%20with%20Mask-Guided%20Iterative%0A%20%20Refinement&entry.906535625=Rui%20Liu&entry.1292438233=%20%20Real-world%20image%20matting%20is%20essential%20for%20applications%20in%20content%20creation%0Aand%20augmented%20reality.%20However%2C%20it%20remains%20challenging%20due%20to%20the%20complex%0Anature%20of%20scenes%20and%20the%20scarcity%20of%20high-quality%20datasets.%20To%20address%20these%0Alimitations%2C%20we%20introduce%20Mask2Alpha%2C%20an%20iterative%20refinement%20framework%0Adesigned%20to%20enhance%20semantic%20comprehension%2C%20instance%20awareness%2C%20and%20fine-detail%0Arecovery%20in%20image%20matting.%20Our%20framework%20leverages%20self-supervised%20Vision%0ATransformer%20features%20as%20semantic%20priors%2C%20strengthening%20contextual%20understanding%0Ain%20complex%20scenarios.%20To%20further%20improve%20instance%20differentiation%2C%20we%20implement%0Aa%20mask-guided%20feature%20selection%20module%2C%20enabling%20precise%20targeting%20of%20objects%0Ain%20multi-instance%20settings.%20Additionally%2C%20a%20sparse%20convolution-based%0Aoptimization%20scheme%20allows%20Mask2Alpha%20to%20recover%20high-resolution%20details%0Athrough%20progressive%20refinement%2Cfrom%20low-resolution%20semantic%20passes%20to%0Ahigh-resolution%20sparse%20reconstructions.%20Benchmarking%20across%20various%20real-world%0Adatasets%2C%20Mask2Alpha%20consistently%20achieves%20state-of-the-art%20results%2C%20showcasing%0Aits%20effectiveness%20in%20accurate%20and%20efficient%20image%20matting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17093v1&entry.124074799=Read"},
{"title": "Enhancing Ground-to-Aerial Image Matching for Visual Misinformation\n  Detection Using Semantic Segmentation", "author": "Emanuele Mule and Matteo Pannacci and Ali Ghasemi Goudarzi and Francesco Pro and Lorenzo Papa and Luca Maiano and Irene Amerini", "abstract": "  The recent advancements in generative AI techniques, which have significantly\nincreased the online dissemination of altered images and videos, have raised\nserious concerns about the credibility of digital media available on the\nInternet and distributed through information channels and social networks. This\nissue particularly affects domains that rely heavily on trustworthy data, such\nas journalism, forensic analysis, and Earth observation. To address these\nconcerns, the ability to geolocate a non-geo-tagged ground-view image without\nexternal information, such as GPS coordinates, has become increasingly\ncritical. This study tackles the challenge of linking a ground-view image,\npotentially exhibiting varying fields of view (FoV), to its corresponding\nsatellite image without the aid of GPS data. To achieve this, we propose a\nnovel four-stream Siamese-like architecture, the Quadruple Semantic Align Net\n(SAN-QUAD), which extends previous state-of-the-art (SOTA) approaches by\nleveraging semantic segmentation applied to both ground and satellite imagery.\nExperimental results on a subset of the CVUSA dataset demonstrate significant\nimprovements of up to 9.8% over prior methods across various FoV settings.\n", "link": "http://arxiv.org/abs/2502.06288v3", "date": "2025-02-24", "relevancy": 2.2963, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5985}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5686}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Ground-to-Aerial%20Image%20Matching%20for%20Visual%20Misinformation%0A%20%20Detection%20Using%20Semantic%20Segmentation&body=Title%3A%20Enhancing%20Ground-to-Aerial%20Image%20Matching%20for%20Visual%20Misinformation%0A%20%20Detection%20Using%20Semantic%20Segmentation%0AAuthor%3A%20Emanuele%20Mule%20and%20Matteo%20Pannacci%20and%20Ali%20Ghasemi%20Goudarzi%20and%20Francesco%20Pro%20and%20Lorenzo%20Papa%20and%20Luca%20Maiano%20and%20Irene%20Amerini%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20generative%20AI%20techniques%2C%20which%20have%20significantly%0Aincreased%20the%20online%20dissemination%20of%20altered%20images%20and%20videos%2C%20have%20raised%0Aserious%20concerns%20about%20the%20credibility%20of%20digital%20media%20available%20on%20the%0AInternet%20and%20distributed%20through%20information%20channels%20and%20social%20networks.%20This%0Aissue%20particularly%20affects%20domains%20that%20rely%20heavily%20on%20trustworthy%20data%2C%20such%0Aas%20journalism%2C%20forensic%20analysis%2C%20and%20Earth%20observation.%20To%20address%20these%0Aconcerns%2C%20the%20ability%20to%20geolocate%20a%20non-geo-tagged%20ground-view%20image%20without%0Aexternal%20information%2C%20such%20as%20GPS%20coordinates%2C%20has%20become%20increasingly%0Acritical.%20This%20study%20tackles%20the%20challenge%20of%20linking%20a%20ground-view%20image%2C%0Apotentially%20exhibiting%20varying%20fields%20of%20view%20%28FoV%29%2C%20to%20its%20corresponding%0Asatellite%20image%20without%20the%20aid%20of%20GPS%20data.%20To%20achieve%20this%2C%20we%20propose%20a%0Anovel%20four-stream%20Siamese-like%20architecture%2C%20the%20Quadruple%20Semantic%20Align%20Net%0A%28SAN-QUAD%29%2C%20which%20extends%20previous%20state-of-the-art%20%28SOTA%29%20approaches%20by%0Aleveraging%20semantic%20segmentation%20applied%20to%20both%20ground%20and%20satellite%20imagery.%0AExperimental%20results%20on%20a%20subset%20of%20the%20CVUSA%20dataset%20demonstrate%20significant%0Aimprovements%20of%20up%20to%209.8%25%20over%20prior%20methods%20across%20various%20FoV%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06288v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Ground-to-Aerial%2520Image%2520Matching%2520for%2520Visual%2520Misinformation%250A%2520%2520Detection%2520Using%2520Semantic%2520Segmentation%26entry.906535625%3DEmanuele%2520Mule%2520and%2520Matteo%2520Pannacci%2520and%2520Ali%2520Ghasemi%2520Goudarzi%2520and%2520Francesco%2520Pro%2520and%2520Lorenzo%2520Papa%2520and%2520Luca%2520Maiano%2520and%2520Irene%2520Amerini%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520generative%2520AI%2520techniques%252C%2520which%2520have%2520significantly%250Aincreased%2520the%2520online%2520dissemination%2520of%2520altered%2520images%2520and%2520videos%252C%2520have%2520raised%250Aserious%2520concerns%2520about%2520the%2520credibility%2520of%2520digital%2520media%2520available%2520on%2520the%250AInternet%2520and%2520distributed%2520through%2520information%2520channels%2520and%2520social%2520networks.%2520This%250Aissue%2520particularly%2520affects%2520domains%2520that%2520rely%2520heavily%2520on%2520trustworthy%2520data%252C%2520such%250Aas%2520journalism%252C%2520forensic%2520analysis%252C%2520and%2520Earth%2520observation.%2520To%2520address%2520these%250Aconcerns%252C%2520the%2520ability%2520to%2520geolocate%2520a%2520non-geo-tagged%2520ground-view%2520image%2520without%250Aexternal%2520information%252C%2520such%2520as%2520GPS%2520coordinates%252C%2520has%2520become%2520increasingly%250Acritical.%2520This%2520study%2520tackles%2520the%2520challenge%2520of%2520linking%2520a%2520ground-view%2520image%252C%250Apotentially%2520exhibiting%2520varying%2520fields%2520of%2520view%2520%2528FoV%2529%252C%2520to%2520its%2520corresponding%250Asatellite%2520image%2520without%2520the%2520aid%2520of%2520GPS%2520data.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%250Anovel%2520four-stream%2520Siamese-like%2520architecture%252C%2520the%2520Quadruple%2520Semantic%2520Align%2520Net%250A%2528SAN-QUAD%2529%252C%2520which%2520extends%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520approaches%2520by%250Aleveraging%2520semantic%2520segmentation%2520applied%2520to%2520both%2520ground%2520and%2520satellite%2520imagery.%250AExperimental%2520results%2520on%2520a%2520subset%2520of%2520the%2520CVUSA%2520dataset%2520demonstrate%2520significant%250Aimprovements%2520of%2520up%2520to%25209.8%2525%2520over%2520prior%2520methods%2520across%2520various%2520FoV%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06288v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Ground-to-Aerial%20Image%20Matching%20for%20Visual%20Misinformation%0A%20%20Detection%20Using%20Semantic%20Segmentation&entry.906535625=Emanuele%20Mule%20and%20Matteo%20Pannacci%20and%20Ali%20Ghasemi%20Goudarzi%20and%20Francesco%20Pro%20and%20Lorenzo%20Papa%20and%20Luca%20Maiano%20and%20Irene%20Amerini&entry.1292438233=%20%20The%20recent%20advancements%20in%20generative%20AI%20techniques%2C%20which%20have%20significantly%0Aincreased%20the%20online%20dissemination%20of%20altered%20images%20and%20videos%2C%20have%20raised%0Aserious%20concerns%20about%20the%20credibility%20of%20digital%20media%20available%20on%20the%0AInternet%20and%20distributed%20through%20information%20channels%20and%20social%20networks.%20This%0Aissue%20particularly%20affects%20domains%20that%20rely%20heavily%20on%20trustworthy%20data%2C%20such%0Aas%20journalism%2C%20forensic%20analysis%2C%20and%20Earth%20observation.%20To%20address%20these%0Aconcerns%2C%20the%20ability%20to%20geolocate%20a%20non-geo-tagged%20ground-view%20image%20without%0Aexternal%20information%2C%20such%20as%20GPS%20coordinates%2C%20has%20become%20increasingly%0Acritical.%20This%20study%20tackles%20the%20challenge%20of%20linking%20a%20ground-view%20image%2C%0Apotentially%20exhibiting%20varying%20fields%20of%20view%20%28FoV%29%2C%20to%20its%20corresponding%0Asatellite%20image%20without%20the%20aid%20of%20GPS%20data.%20To%20achieve%20this%2C%20we%20propose%20a%0Anovel%20four-stream%20Siamese-like%20architecture%2C%20the%20Quadruple%20Semantic%20Align%20Net%0A%28SAN-QUAD%29%2C%20which%20extends%20previous%20state-of-the-art%20%28SOTA%29%20approaches%20by%0Aleveraging%20semantic%20segmentation%20applied%20to%20both%20ground%20and%20satellite%20imagery.%0AExperimental%20results%20on%20a%20subset%20of%20the%20CVUSA%20dataset%20demonstrate%20significant%0Aimprovements%20of%20up%20to%209.8%25%20over%20prior%20methods%20across%20various%20FoV%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06288v3&entry.124074799=Read"},
{"title": "Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver\n  Assistance Systems", "author": "Federico Scar\u00ec and Nitin Jonathan Myers and Chen Quan and Arkady Zgonnikov", "abstract": "  Accurate environmental perception is critical for advanced driver assistance\nsystems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role\nin ADAS; they can reliably detect obstacles and help ensure traffic safety.\nExisting research on LiDAR sensing has demonstrated that adapting the LiDAR's\nresolution and range based on environmental characteristics can improve machine\nperception. However, current adaptive LiDAR approaches for ADAS have not\nexplored the possibility of combining the perception abilities of the vehicle\nand the human driver, which can potentially further enhance the detection\nperformance. In this paper, we propose a novel system that adapts LiDAR\ncharacteristics to human driver's visual perception to enhance LiDAR sensing\noutside human's field of view. We develop a proof-of-concept prototype of the\nsystem in the virtual environment CARLA. Our system integrates real-time data\non the driver's gaze to identify regions in the environment that the driver is\nmonitoring. This allows the system to optimize LiDAR resources by dynamically\nincreasing the LiDAR's range and resolution in peripheral areas that the driver\nmay not be attending to. Our simulations show that this gaze-aware LiDAR\nenhances detection performance compared to a baseline standalone LiDAR,\nparticularly in challenging environmental conditions like fog. Our hybrid\nhuman-machine sensing approach potentially offers improved safety and\nsituational awareness in real-time driving scenarios for ADAS applications.\n", "link": "http://arxiv.org/abs/2502.17309v1", "date": "2025-02-24", "relevancy": 2.2927, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5786}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5779}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Human-Machine%20Perception%20via%20Adaptive%20LiDAR%20for%20Advanced%20Driver%0A%20%20Assistance%20Systems&body=Title%3A%20Hybrid%20Human-Machine%20Perception%20via%20Adaptive%20LiDAR%20for%20Advanced%20Driver%0A%20%20Assistance%20Systems%0AAuthor%3A%20Federico%20Scar%C3%AC%20and%20Nitin%20Jonathan%20Myers%20and%20Chen%20Quan%20and%20Arkady%20Zgonnikov%0AAbstract%3A%20%20%20Accurate%20environmental%20perception%20is%20critical%20for%20advanced%20driver%20assistance%0Asystems%20%28ADAS%29.%20Light%20detection%20and%20ranging%20%28LiDAR%29%20systems%20play%20a%20crucial%20role%0Ain%20ADAS%3B%20they%20can%20reliably%20detect%20obstacles%20and%20help%20ensure%20traffic%20safety.%0AExisting%20research%20on%20LiDAR%20sensing%20has%20demonstrated%20that%20adapting%20the%20LiDAR%27s%0Aresolution%20and%20range%20based%20on%20environmental%20characteristics%20can%20improve%20machine%0Aperception.%20However%2C%20current%20adaptive%20LiDAR%20approaches%20for%20ADAS%20have%20not%0Aexplored%20the%20possibility%20of%20combining%20the%20perception%20abilities%20of%20the%20vehicle%0Aand%20the%20human%20driver%2C%20which%20can%20potentially%20further%20enhance%20the%20detection%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20system%20that%20adapts%20LiDAR%0Acharacteristics%20to%20human%20driver%27s%20visual%20perception%20to%20enhance%20LiDAR%20sensing%0Aoutside%20human%27s%20field%20of%20view.%20We%20develop%20a%20proof-of-concept%20prototype%20of%20the%0Asystem%20in%20the%20virtual%20environment%20CARLA.%20Our%20system%20integrates%20real-time%20data%0Aon%20the%20driver%27s%20gaze%20to%20identify%20regions%20in%20the%20environment%20that%20the%20driver%20is%0Amonitoring.%20This%20allows%20the%20system%20to%20optimize%20LiDAR%20resources%20by%20dynamically%0Aincreasing%20the%20LiDAR%27s%20range%20and%20resolution%20in%20peripheral%20areas%20that%20the%20driver%0Amay%20not%20be%20attending%20to.%20Our%20simulations%20show%20that%20this%20gaze-aware%20LiDAR%0Aenhances%20detection%20performance%20compared%20to%20a%20baseline%20standalone%20LiDAR%2C%0Aparticularly%20in%20challenging%20environmental%20conditions%20like%20fog.%20Our%20hybrid%0Ahuman-machine%20sensing%20approach%20potentially%20offers%20improved%20safety%20and%0Asituational%20awareness%20in%20real-time%20driving%20scenarios%20for%20ADAS%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17309v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Human-Machine%2520Perception%2520via%2520Adaptive%2520LiDAR%2520for%2520Advanced%2520Driver%250A%2520%2520Assistance%2520Systems%26entry.906535625%3DFederico%2520Scar%25C3%25AC%2520and%2520Nitin%2520Jonathan%2520Myers%2520and%2520Chen%2520Quan%2520and%2520Arkady%2520Zgonnikov%26entry.1292438233%3D%2520%2520Accurate%2520environmental%2520perception%2520is%2520critical%2520for%2520advanced%2520driver%2520assistance%250Asystems%2520%2528ADAS%2529.%2520Light%2520detection%2520and%2520ranging%2520%2528LiDAR%2529%2520systems%2520play%2520a%2520crucial%2520role%250Ain%2520ADAS%253B%2520they%2520can%2520reliably%2520detect%2520obstacles%2520and%2520help%2520ensure%2520traffic%2520safety.%250AExisting%2520research%2520on%2520LiDAR%2520sensing%2520has%2520demonstrated%2520that%2520adapting%2520the%2520LiDAR%2527s%250Aresolution%2520and%2520range%2520based%2520on%2520environmental%2520characteristics%2520can%2520improve%2520machine%250Aperception.%2520However%252C%2520current%2520adaptive%2520LiDAR%2520approaches%2520for%2520ADAS%2520have%2520not%250Aexplored%2520the%2520possibility%2520of%2520combining%2520the%2520perception%2520abilities%2520of%2520the%2520vehicle%250Aand%2520the%2520human%2520driver%252C%2520which%2520can%2520potentially%2520further%2520enhance%2520the%2520detection%250Aperformance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520system%2520that%2520adapts%2520LiDAR%250Acharacteristics%2520to%2520human%2520driver%2527s%2520visual%2520perception%2520to%2520enhance%2520LiDAR%2520sensing%250Aoutside%2520human%2527s%2520field%2520of%2520view.%2520We%2520develop%2520a%2520proof-of-concept%2520prototype%2520of%2520the%250Asystem%2520in%2520the%2520virtual%2520environment%2520CARLA.%2520Our%2520system%2520integrates%2520real-time%2520data%250Aon%2520the%2520driver%2527s%2520gaze%2520to%2520identify%2520regions%2520in%2520the%2520environment%2520that%2520the%2520driver%2520is%250Amonitoring.%2520This%2520allows%2520the%2520system%2520to%2520optimize%2520LiDAR%2520resources%2520by%2520dynamically%250Aincreasing%2520the%2520LiDAR%2527s%2520range%2520and%2520resolution%2520in%2520peripheral%2520areas%2520that%2520the%2520driver%250Amay%2520not%2520be%2520attending%2520to.%2520Our%2520simulations%2520show%2520that%2520this%2520gaze-aware%2520LiDAR%250Aenhances%2520detection%2520performance%2520compared%2520to%2520a%2520baseline%2520standalone%2520LiDAR%252C%250Aparticularly%2520in%2520challenging%2520environmental%2520conditions%2520like%2520fog.%2520Our%2520hybrid%250Ahuman-machine%2520sensing%2520approach%2520potentially%2520offers%2520improved%2520safety%2520and%250Asituational%2520awareness%2520in%2520real-time%2520driving%2520scenarios%2520for%2520ADAS%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17309v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Human-Machine%20Perception%20via%20Adaptive%20LiDAR%20for%20Advanced%20Driver%0A%20%20Assistance%20Systems&entry.906535625=Federico%20Scar%C3%AC%20and%20Nitin%20Jonathan%20Myers%20and%20Chen%20Quan%20and%20Arkady%20Zgonnikov&entry.1292438233=%20%20Accurate%20environmental%20perception%20is%20critical%20for%20advanced%20driver%20assistance%0Asystems%20%28ADAS%29.%20Light%20detection%20and%20ranging%20%28LiDAR%29%20systems%20play%20a%20crucial%20role%0Ain%20ADAS%3B%20they%20can%20reliably%20detect%20obstacles%20and%20help%20ensure%20traffic%20safety.%0AExisting%20research%20on%20LiDAR%20sensing%20has%20demonstrated%20that%20adapting%20the%20LiDAR%27s%0Aresolution%20and%20range%20based%20on%20environmental%20characteristics%20can%20improve%20machine%0Aperception.%20However%2C%20current%20adaptive%20LiDAR%20approaches%20for%20ADAS%20have%20not%0Aexplored%20the%20possibility%20of%20combining%20the%20perception%20abilities%20of%20the%20vehicle%0Aand%20the%20human%20driver%2C%20which%20can%20potentially%20further%20enhance%20the%20detection%0Aperformance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20system%20that%20adapts%20LiDAR%0Acharacteristics%20to%20human%20driver%27s%20visual%20perception%20to%20enhance%20LiDAR%20sensing%0Aoutside%20human%27s%20field%20of%20view.%20We%20develop%20a%20proof-of-concept%20prototype%20of%20the%0Asystem%20in%20the%20virtual%20environment%20CARLA.%20Our%20system%20integrates%20real-time%20data%0Aon%20the%20driver%27s%20gaze%20to%20identify%20regions%20in%20the%20environment%20that%20the%20driver%20is%0Amonitoring.%20This%20allows%20the%20system%20to%20optimize%20LiDAR%20resources%20by%20dynamically%0Aincreasing%20the%20LiDAR%27s%20range%20and%20resolution%20in%20peripheral%20areas%20that%20the%20driver%0Amay%20not%20be%20attending%20to.%20Our%20simulations%20show%20that%20this%20gaze-aware%20LiDAR%0Aenhances%20detection%20performance%20compared%20to%20a%20baseline%20standalone%20LiDAR%2C%0Aparticularly%20in%20challenging%20environmental%20conditions%20like%20fog.%20Our%20hybrid%0Ahuman-machine%20sensing%20approach%20potentially%20offers%20improved%20safety%20and%0Asituational%20awareness%20in%20real-time%20driving%20scenarios%20for%20ADAS%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17309v1&entry.124074799=Read"},
{"title": "Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared\n  Autonomy Framework", "author": "Federico Vasile and Elisa Maiettini and Giulia Pasquale and Nicol\u00f2 Boccardo and Lorenzo Natale", "abstract": "  Most control techniques for prosthetic grasping focus on dexterous fingers\ncontrol, but overlook the wrist motion. This forces the user to perform\ncompensatory movements with the elbow, shoulder and hip to adapt the wrist for\ngrasping. We propose a computer vision-based system that leverages the\ncollaboration between the user and an automatic system in a shared autonomy\nframework, to perform continuous control of the wrist degrees of freedom in a\nprosthetic arm, promoting a more natural approach-to-grasp motion. Our pipeline\nallows to seamlessly control the prosthetic wrist to follow the target object\nand finally orient it for grasping according to the user intent. We assess the\neffectiveness of each system component through quantitative analysis and\nfinally deploy our method on the Hannes prosthetic arm. Code and videos:\nhttps://hsp-iit.github.io/hannes-wrist-control.\n", "link": "http://arxiv.org/abs/2502.17265v1", "date": "2025-02-24", "relevancy": 2.2892, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5317}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Wrist%20Control%20on%20the%20Hannes%20Prosthesis%3A%20a%20Vision-based%20Shared%0A%20%20Autonomy%20Framework&body=Title%3A%20Continuous%20Wrist%20Control%20on%20the%20Hannes%20Prosthesis%3A%20a%20Vision-based%20Shared%0A%20%20Autonomy%20Framework%0AAuthor%3A%20Federico%20Vasile%20and%20Elisa%20Maiettini%20and%20Giulia%20Pasquale%20and%20Nicol%C3%B2%20Boccardo%20and%20Lorenzo%20Natale%0AAbstract%3A%20%20%20Most%20control%20techniques%20for%20prosthetic%20grasping%20focus%20on%20dexterous%20fingers%0Acontrol%2C%20but%20overlook%20the%20wrist%20motion.%20This%20forces%20the%20user%20to%20perform%0Acompensatory%20movements%20with%20the%20elbow%2C%20shoulder%20and%20hip%20to%20adapt%20the%20wrist%20for%0Agrasping.%20We%20propose%20a%20computer%20vision-based%20system%20that%20leverages%20the%0Acollaboration%20between%20the%20user%20and%20an%20automatic%20system%20in%20a%20shared%20autonomy%0Aframework%2C%20to%20perform%20continuous%20control%20of%20the%20wrist%20degrees%20of%20freedom%20in%20a%0Aprosthetic%20arm%2C%20promoting%20a%20more%20natural%20approach-to-grasp%20motion.%20Our%20pipeline%0Aallows%20to%20seamlessly%20control%20the%20prosthetic%20wrist%20to%20follow%20the%20target%20object%0Aand%20finally%20orient%20it%20for%20grasping%20according%20to%20the%20user%20intent.%20We%20assess%20the%0Aeffectiveness%20of%20each%20system%20component%20through%20quantitative%20analysis%20and%0Afinally%20deploy%20our%20method%20on%20the%20Hannes%20prosthetic%20arm.%20Code%20and%20videos%3A%0Ahttps%3A//hsp-iit.github.io/hannes-wrist-control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17265v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Wrist%2520Control%2520on%2520the%2520Hannes%2520Prosthesis%253A%2520a%2520Vision-based%2520Shared%250A%2520%2520Autonomy%2520Framework%26entry.906535625%3DFederico%2520Vasile%2520and%2520Elisa%2520Maiettini%2520and%2520Giulia%2520Pasquale%2520and%2520Nicol%25C3%25B2%2520Boccardo%2520and%2520Lorenzo%2520Natale%26entry.1292438233%3D%2520%2520Most%2520control%2520techniques%2520for%2520prosthetic%2520grasping%2520focus%2520on%2520dexterous%2520fingers%250Acontrol%252C%2520but%2520overlook%2520the%2520wrist%2520motion.%2520This%2520forces%2520the%2520user%2520to%2520perform%250Acompensatory%2520movements%2520with%2520the%2520elbow%252C%2520shoulder%2520and%2520hip%2520to%2520adapt%2520the%2520wrist%2520for%250Agrasping.%2520We%2520propose%2520a%2520computer%2520vision-based%2520system%2520that%2520leverages%2520the%250Acollaboration%2520between%2520the%2520user%2520and%2520an%2520automatic%2520system%2520in%2520a%2520shared%2520autonomy%250Aframework%252C%2520to%2520perform%2520continuous%2520control%2520of%2520the%2520wrist%2520degrees%2520of%2520freedom%2520in%2520a%250Aprosthetic%2520arm%252C%2520promoting%2520a%2520more%2520natural%2520approach-to-grasp%2520motion.%2520Our%2520pipeline%250Aallows%2520to%2520seamlessly%2520control%2520the%2520prosthetic%2520wrist%2520to%2520follow%2520the%2520target%2520object%250Aand%2520finally%2520orient%2520it%2520for%2520grasping%2520according%2520to%2520the%2520user%2520intent.%2520We%2520assess%2520the%250Aeffectiveness%2520of%2520each%2520system%2520component%2520through%2520quantitative%2520analysis%2520and%250Afinally%2520deploy%2520our%2520method%2520on%2520the%2520Hannes%2520prosthetic%2520arm.%2520Code%2520and%2520videos%253A%250Ahttps%253A//hsp-iit.github.io/hannes-wrist-control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17265v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Wrist%20Control%20on%20the%20Hannes%20Prosthesis%3A%20a%20Vision-based%20Shared%0A%20%20Autonomy%20Framework&entry.906535625=Federico%20Vasile%20and%20Elisa%20Maiettini%20and%20Giulia%20Pasquale%20and%20Nicol%C3%B2%20Boccardo%20and%20Lorenzo%20Natale&entry.1292438233=%20%20Most%20control%20techniques%20for%20prosthetic%20grasping%20focus%20on%20dexterous%20fingers%0Acontrol%2C%20but%20overlook%20the%20wrist%20motion.%20This%20forces%20the%20user%20to%20perform%0Acompensatory%20movements%20with%20the%20elbow%2C%20shoulder%20and%20hip%20to%20adapt%20the%20wrist%20for%0Agrasping.%20We%20propose%20a%20computer%20vision-based%20system%20that%20leverages%20the%0Acollaboration%20between%20the%20user%20and%20an%20automatic%20system%20in%20a%20shared%20autonomy%0Aframework%2C%20to%20perform%20continuous%20control%20of%20the%20wrist%20degrees%20of%20freedom%20in%20a%0Aprosthetic%20arm%2C%20promoting%20a%20more%20natural%20approach-to-grasp%20motion.%20Our%20pipeline%0Aallows%20to%20seamlessly%20control%20the%20prosthetic%20wrist%20to%20follow%20the%20target%20object%0Aand%20finally%20orient%20it%20for%20grasping%20according%20to%20the%20user%20intent.%20We%20assess%20the%0Aeffectiveness%20of%20each%20system%20component%20through%20quantitative%20analysis%20and%0Afinally%20deploy%20our%20method%20on%20the%20Hannes%20prosthetic%20arm.%20Code%20and%20videos%3A%0Ahttps%3A//hsp-iit.github.io/hannes-wrist-control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17265v1&entry.124074799=Read"},
{"title": "AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven\n  Data Generation", "author": "Datao Tang and Xiangyong Cao and Xuan Wu and Jialin Li and Jing Yao and Xueru Bai and Dongsheng Jiang and Yin Li and Deyu Meng", "abstract": "  Remote sensing image object detection (RSIOD) aims to identify and locate\nspecific objects within satellite or aerial imagery. However, there is a\nscarcity of labeled data in current RSIOD datasets, which significantly limits\nthe performance of current detection algorithms. Although existing techniques,\ne.g., data augmentation and semi-supervised learning, can mitigate this\nscarcity issue to some extent, they are heavily dependent on high-quality\nlabeled data and perform worse in rare object classes. To address this issue,\nthis paper proposes a layout-controllable diffusion generative model (i.e.\nAeroGen) tailored for RSIOD. To our knowledge, AeroGen is the first model to\nsimultaneously support horizontal and rotated bounding box condition\ngeneration, thus enabling the generation of high-quality synthetic images that\nmeet specific layout and object category requirements. Additionally, we propose\nan end-to-end data augmentation framework that integrates a\ndiversity-conditioned generator and a filtering mechanism to enhance both the\ndiversity and quality of generated data. Experimental results demonstrate that\nthe synthetic data produced by our method are of high quality and diversity.\nFurthermore, the synthetic RSIOD data can significantly improve the detection\nperformance of existing RSIOD models, i.e., the mAP metrics on DIOR, DIOR-R,\nand HRSC datasets are improved by 3.7%, 4.3%, and 2.43%, respectively. The code\nis available at https://github.com/Sonettoo/AeroGen.\n", "link": "http://arxiv.org/abs/2411.15497v3", "date": "2025-02-24", "relevancy": 2.2851, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6106}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AeroGen%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20with%20Diffusion-Driven%0A%20%20Data%20Generation&body=Title%3A%20AeroGen%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20with%20Diffusion-Driven%0A%20%20Data%20Generation%0AAuthor%3A%20Datao%20Tang%20and%20Xiangyong%20Cao%20and%20Xuan%20Wu%20and%20Jialin%20Li%20and%20Jing%20Yao%20and%20Xueru%20Bai%20and%20Dongsheng%20Jiang%20and%20Yin%20Li%20and%20Deyu%20Meng%0AAbstract%3A%20%20%20Remote%20sensing%20image%20object%20detection%20%28RSIOD%29%20aims%20to%20identify%20and%20locate%0Aspecific%20objects%20within%20satellite%20or%20aerial%20imagery.%20However%2C%20there%20is%20a%0Ascarcity%20of%20labeled%20data%20in%20current%20RSIOD%20datasets%2C%20which%20significantly%20limits%0Athe%20performance%20of%20current%20detection%20algorithms.%20Although%20existing%20techniques%2C%0Ae.g.%2C%20data%20augmentation%20and%20semi-supervised%20learning%2C%20can%20mitigate%20this%0Ascarcity%20issue%20to%20some%20extent%2C%20they%20are%20heavily%20dependent%20on%20high-quality%0Alabeled%20data%20and%20perform%20worse%20in%20rare%20object%20classes.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20layout-controllable%20diffusion%20generative%20model%20%28i.e.%0AAeroGen%29%20tailored%20for%20RSIOD.%20To%20our%20knowledge%2C%20AeroGen%20is%20the%20first%20model%20to%0Asimultaneously%20support%20horizontal%20and%20rotated%20bounding%20box%20condition%0Ageneration%2C%20thus%20enabling%20the%20generation%20of%20high-quality%20synthetic%20images%20that%0Ameet%20specific%20layout%20and%20object%20category%20requirements.%20Additionally%2C%20we%20propose%0Aan%20end-to-end%20data%20augmentation%20framework%20that%20integrates%20a%0Adiversity-conditioned%20generator%20and%20a%20filtering%20mechanism%20to%20enhance%20both%20the%0Adiversity%20and%20quality%20of%20generated%20data.%20Experimental%20results%20demonstrate%20that%0Athe%20synthetic%20data%20produced%20by%20our%20method%20are%20of%20high%20quality%20and%20diversity.%0AFurthermore%2C%20the%20synthetic%20RSIOD%20data%20can%20significantly%20improve%20the%20detection%0Aperformance%20of%20existing%20RSIOD%20models%2C%20i.e.%2C%20the%20mAP%20metrics%20on%20DIOR%2C%20DIOR-R%2C%0Aand%20HRSC%20datasets%20are%20improved%20by%203.7%25%2C%204.3%25%2C%20and%202.43%25%2C%20respectively.%20The%20code%0Ais%20available%20at%20https%3A//github.com/Sonettoo/AeroGen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.15497v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAeroGen%253A%2520Enhancing%2520Remote%2520Sensing%2520Object%2520Detection%2520with%2520Diffusion-Driven%250A%2520%2520Data%2520Generation%26entry.906535625%3DDatao%2520Tang%2520and%2520Xiangyong%2520Cao%2520and%2520Xuan%2520Wu%2520and%2520Jialin%2520Li%2520and%2520Jing%2520Yao%2520and%2520Xueru%2520Bai%2520and%2520Dongsheng%2520Jiang%2520and%2520Yin%2520Li%2520and%2520Deyu%2520Meng%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520object%2520detection%2520%2528RSIOD%2529%2520aims%2520to%2520identify%2520and%2520locate%250Aspecific%2520objects%2520within%2520satellite%2520or%2520aerial%2520imagery.%2520However%252C%2520there%2520is%2520a%250Ascarcity%2520of%2520labeled%2520data%2520in%2520current%2520RSIOD%2520datasets%252C%2520which%2520significantly%2520limits%250Athe%2520performance%2520of%2520current%2520detection%2520algorithms.%2520Although%2520existing%2520techniques%252C%250Ae.g.%252C%2520data%2520augmentation%2520and%2520semi-supervised%2520learning%252C%2520can%2520mitigate%2520this%250Ascarcity%2520issue%2520to%2520some%2520extent%252C%2520they%2520are%2520heavily%2520dependent%2520on%2520high-quality%250Alabeled%2520data%2520and%2520perform%2520worse%2520in%2520rare%2520object%2520classes.%2520To%2520address%2520this%2520issue%252C%250Athis%2520paper%2520proposes%2520a%2520layout-controllable%2520diffusion%2520generative%2520model%2520%2528i.e.%250AAeroGen%2529%2520tailored%2520for%2520RSIOD.%2520To%2520our%2520knowledge%252C%2520AeroGen%2520is%2520the%2520first%2520model%2520to%250Asimultaneously%2520support%2520horizontal%2520and%2520rotated%2520bounding%2520box%2520condition%250Ageneration%252C%2520thus%2520enabling%2520the%2520generation%2520of%2520high-quality%2520synthetic%2520images%2520that%250Ameet%2520specific%2520layout%2520and%2520object%2520category%2520requirements.%2520Additionally%252C%2520we%2520propose%250Aan%2520end-to-end%2520data%2520augmentation%2520framework%2520that%2520integrates%2520a%250Adiversity-conditioned%2520generator%2520and%2520a%2520filtering%2520mechanism%2520to%2520enhance%2520both%2520the%250Adiversity%2520and%2520quality%2520of%2520generated%2520data.%2520Experimental%2520results%2520demonstrate%2520that%250Athe%2520synthetic%2520data%2520produced%2520by%2520our%2520method%2520are%2520of%2520high%2520quality%2520and%2520diversity.%250AFurthermore%252C%2520the%2520synthetic%2520RSIOD%2520data%2520can%2520significantly%2520improve%2520the%2520detection%250Aperformance%2520of%2520existing%2520RSIOD%2520models%252C%2520i.e.%252C%2520the%2520mAP%2520metrics%2520on%2520DIOR%252C%2520DIOR-R%252C%250Aand%2520HRSC%2520datasets%2520are%2520improved%2520by%25203.7%2525%252C%25204.3%2525%252C%2520and%25202.43%2525%252C%2520respectively.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/Sonettoo/AeroGen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.15497v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AeroGen%3A%20Enhancing%20Remote%20Sensing%20Object%20Detection%20with%20Diffusion-Driven%0A%20%20Data%20Generation&entry.906535625=Datao%20Tang%20and%20Xiangyong%20Cao%20and%20Xuan%20Wu%20and%20Jialin%20Li%20and%20Jing%20Yao%20and%20Xueru%20Bai%20and%20Dongsheng%20Jiang%20and%20Yin%20Li%20and%20Deyu%20Meng&entry.1292438233=%20%20Remote%20sensing%20image%20object%20detection%20%28RSIOD%29%20aims%20to%20identify%20and%20locate%0Aspecific%20objects%20within%20satellite%20or%20aerial%20imagery.%20However%2C%20there%20is%20a%0Ascarcity%20of%20labeled%20data%20in%20current%20RSIOD%20datasets%2C%20which%20significantly%20limits%0Athe%20performance%20of%20current%20detection%20algorithms.%20Although%20existing%20techniques%2C%0Ae.g.%2C%20data%20augmentation%20and%20semi-supervised%20learning%2C%20can%20mitigate%20this%0Ascarcity%20issue%20to%20some%20extent%2C%20they%20are%20heavily%20dependent%20on%20high-quality%0Alabeled%20data%20and%20perform%20worse%20in%20rare%20object%20classes.%20To%20address%20this%20issue%2C%0Athis%20paper%20proposes%20a%20layout-controllable%20diffusion%20generative%20model%20%28i.e.%0AAeroGen%29%20tailored%20for%20RSIOD.%20To%20our%20knowledge%2C%20AeroGen%20is%20the%20first%20model%20to%0Asimultaneously%20support%20horizontal%20and%20rotated%20bounding%20box%20condition%0Ageneration%2C%20thus%20enabling%20the%20generation%20of%20high-quality%20synthetic%20images%20that%0Ameet%20specific%20layout%20and%20object%20category%20requirements.%20Additionally%2C%20we%20propose%0Aan%20end-to-end%20data%20augmentation%20framework%20that%20integrates%20a%0Adiversity-conditioned%20generator%20and%20a%20filtering%20mechanism%20to%20enhance%20both%20the%0Adiversity%20and%20quality%20of%20generated%20data.%20Experimental%20results%20demonstrate%20that%0Athe%20synthetic%20data%20produced%20by%20our%20method%20are%20of%20high%20quality%20and%20diversity.%0AFurthermore%2C%20the%20synthetic%20RSIOD%20data%20can%20significantly%20improve%20the%20detection%0Aperformance%20of%20existing%20RSIOD%20models%2C%20i.e.%2C%20the%20mAP%20metrics%20on%20DIOR%2C%20DIOR-R%2C%0Aand%20HRSC%20datasets%20are%20improved%20by%203.7%25%2C%204.3%25%2C%20and%202.43%25%2C%20respectively.%20The%20code%0Ais%20available%20at%20https%3A//github.com/Sonettoo/AeroGen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.15497v3&entry.124074799=Read"},
{"title": "Fast Shortest Path Polyline Smoothing With $G^1$ Continuity and Bounded\n  Curvature", "author": "Patrick Pastorelli and Simone Dagnino and Enrico Saccon and Marco Frego and Luigi Palopoli", "abstract": "  In this work, we propose a novel and efficient method for smoothing polylines\nin motion planning tasks. The algorithm applies to motion planning of vehicles\nwith bounded curvature. In the paper, we show that the generated path: 1) has\nminimal length, 2) is $G^1$ continuous, and 3) is collision-free by\nconstruction, if the hypotheses are respected. We compare our solution with the\nstate-of.the-art and show its convenience both in terms of computation time and\nof length of the compute path.\n", "link": "http://arxiv.org/abs/2409.09816v2", "date": "2025-02-24", "relevancy": 2.2633, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4738}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4488}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Shortest%20Path%20Polyline%20Smoothing%20With%20%24G%5E1%24%20Continuity%20and%20Bounded%0A%20%20Curvature&body=Title%3A%20Fast%20Shortest%20Path%20Polyline%20Smoothing%20With%20%24G%5E1%24%20Continuity%20and%20Bounded%0A%20%20Curvature%0AAuthor%3A%20Patrick%20Pastorelli%20and%20Simone%20Dagnino%20and%20Enrico%20Saccon%20and%20Marco%20Frego%20and%20Luigi%20Palopoli%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20and%20efficient%20method%20for%20smoothing%20polylines%0Ain%20motion%20planning%20tasks.%20The%20algorithm%20applies%20to%20motion%20planning%20of%20vehicles%0Awith%20bounded%20curvature.%20In%20the%20paper%2C%20we%20show%20that%20the%20generated%20path%3A%201%29%20has%0Aminimal%20length%2C%202%29%20is%20%24G%5E1%24%20continuous%2C%20and%203%29%20is%20collision-free%20by%0Aconstruction%2C%20if%20the%20hypotheses%20are%20respected.%20We%20compare%20our%20solution%20with%20the%0Astate-of.the-art%20and%20show%20its%20convenience%20both%20in%20terms%20of%20computation%20time%20and%0Aof%20length%20of%20the%20compute%20path.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Shortest%2520Path%2520Polyline%2520Smoothing%2520With%2520%2524G%255E1%2524%2520Continuity%2520and%2520Bounded%250A%2520%2520Curvature%26entry.906535625%3DPatrick%2520Pastorelli%2520and%2520Simone%2520Dagnino%2520and%2520Enrico%2520Saccon%2520and%2520Marco%2520Frego%2520and%2520Luigi%2520Palopoli%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520and%2520efficient%2520method%2520for%2520smoothing%2520polylines%250Ain%2520motion%2520planning%2520tasks.%2520The%2520algorithm%2520applies%2520to%2520motion%2520planning%2520of%2520vehicles%250Awith%2520bounded%2520curvature.%2520In%2520the%2520paper%252C%2520we%2520show%2520that%2520the%2520generated%2520path%253A%25201%2529%2520has%250Aminimal%2520length%252C%25202%2529%2520is%2520%2524G%255E1%2524%2520continuous%252C%2520and%25203%2529%2520is%2520collision-free%2520by%250Aconstruction%252C%2520if%2520the%2520hypotheses%2520are%2520respected.%2520We%2520compare%2520our%2520solution%2520with%2520the%250Astate-of.the-art%2520and%2520show%2520its%2520convenience%2520both%2520in%2520terms%2520of%2520computation%2520time%2520and%250Aof%2520length%2520of%2520the%2520compute%2520path.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Shortest%20Path%20Polyline%20Smoothing%20With%20%24G%5E1%24%20Continuity%20and%20Bounded%0A%20%20Curvature&entry.906535625=Patrick%20Pastorelli%20and%20Simone%20Dagnino%20and%20Enrico%20Saccon%20and%20Marco%20Frego%20and%20Luigi%20Palopoli&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20novel%20and%20efficient%20method%20for%20smoothing%20polylines%0Ain%20motion%20planning%20tasks.%20The%20algorithm%20applies%20to%20motion%20planning%20of%20vehicles%0Awith%20bounded%20curvature.%20In%20the%20paper%2C%20we%20show%20that%20the%20generated%20path%3A%201%29%20has%0Aminimal%20length%2C%202%29%20is%20%24G%5E1%24%20continuous%2C%20and%203%29%20is%20collision-free%20by%0Aconstruction%2C%20if%20the%20hypotheses%20are%20respected.%20We%20compare%20our%20solution%20with%20the%0Astate-of.the-art%20and%20show%20its%20convenience%20both%20in%20terms%20of%20computation%20time%20and%0Aof%20length%20of%20the%20compute%20path.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09816v2&entry.124074799=Read"},
{"title": "GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales", "author": "Yuehong Cassandra Tai and Khushi Navin Patni and Nicholas Daniel Hemauer and Bruce Desmarais and Yu-Ru Lin", "abstract": "  Despite recent advances in understanding the capabilities and limits of\ngenerative artificial intelligence (GenAI) models, we are just beginning to\nunderstand their capacity to assess and reason about the veracity of content.\nWe evaluate multiple GenAI models across tasks that involve the rating of, and\nperceived reasoning about, the credibility of information. The information in\nour experiments comes from content that subnational U.S. politicians post to\nFacebook. We find that GPT-4o, one of the most used AI models in consumer\napplications, outperforms other models, but all models exhibit only moderate\nagreement with human coders. Importantly, even when GenAI models accurately\nidentify low-credibility content, their reasoning relies heavily on linguistic\nfeatures and ``hard'' criteria, such as the level of detail, source\nreliability, and language formality, rather than an understanding of veracity.\nWe also assess the effectiveness of summarized versus full content inputs,\nfinding that summarized content holds promise for improving efficiency without\nsacrificing accuracy. While GenAI has the potential to support human\nfact-checkers in scaling misinformation detection, our results caution against\nrelying solely on these models.\n", "link": "http://arxiv.org/abs/2502.14943v2", "date": "2025-02-24", "relevancy": 2.2613, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAI%20vs.%20Human%20Fact-Checkers%3A%20Accurate%20Ratings%2C%20Flawed%20Rationales&body=Title%3A%20GenAI%20vs.%20Human%20Fact-Checkers%3A%20Accurate%20Ratings%2C%20Flawed%20Rationales%0AAuthor%3A%20Yuehong%20Cassandra%20Tai%20and%20Khushi%20Navin%20Patni%20and%20Nicholas%20Daniel%20Hemauer%20and%20Bruce%20Desmarais%20and%20Yu-Ru%20Lin%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20understanding%20the%20capabilities%20and%20limits%20of%0Agenerative%20artificial%20intelligence%20%28GenAI%29%20models%2C%20we%20are%20just%20beginning%20to%0Aunderstand%20their%20capacity%20to%20assess%20and%20reason%20about%20the%20veracity%20of%20content.%0AWe%20evaluate%20multiple%20GenAI%20models%20across%20tasks%20that%20involve%20the%20rating%20of%2C%20and%0Aperceived%20reasoning%20about%2C%20the%20credibility%20of%20information.%20The%20information%20in%0Aour%20experiments%20comes%20from%20content%20that%20subnational%20U.S.%20politicians%20post%20to%0AFacebook.%20We%20find%20that%20GPT-4o%2C%20one%20of%20the%20most%20used%20AI%20models%20in%20consumer%0Aapplications%2C%20outperforms%20other%20models%2C%20but%20all%20models%20exhibit%20only%20moderate%0Aagreement%20with%20human%20coders.%20Importantly%2C%20even%20when%20GenAI%20models%20accurately%0Aidentify%20low-credibility%20content%2C%20their%20reasoning%20relies%20heavily%20on%20linguistic%0Afeatures%20and%20%60%60hard%27%27%20criteria%2C%20such%20as%20the%20level%20of%20detail%2C%20source%0Areliability%2C%20and%20language%20formality%2C%20rather%20than%20an%20understanding%20of%20veracity.%0AWe%20also%20assess%20the%20effectiveness%20of%20summarized%20versus%20full%20content%20inputs%2C%0Afinding%20that%20summarized%20content%20holds%20promise%20for%20improving%20efficiency%20without%0Asacrificing%20accuracy.%20While%20GenAI%20has%20the%20potential%20to%20support%20human%0Afact-checkers%20in%20scaling%20misinformation%20detection%2C%20our%20results%20caution%20against%0Arelying%20solely%20on%20these%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAI%2520vs.%2520Human%2520Fact-Checkers%253A%2520Accurate%2520Ratings%252C%2520Flawed%2520Rationales%26entry.906535625%3DYuehong%2520Cassandra%2520Tai%2520and%2520Khushi%2520Navin%2520Patni%2520and%2520Nicholas%2520Daniel%2520Hemauer%2520and%2520Bruce%2520Desmarais%2520and%2520Yu-Ru%2520Lin%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520understanding%2520the%2520capabilities%2520and%2520limits%2520of%250Agenerative%2520artificial%2520intelligence%2520%2528GenAI%2529%2520models%252C%2520we%2520are%2520just%2520beginning%2520to%250Aunderstand%2520their%2520capacity%2520to%2520assess%2520and%2520reason%2520about%2520the%2520veracity%2520of%2520content.%250AWe%2520evaluate%2520multiple%2520GenAI%2520models%2520across%2520tasks%2520that%2520involve%2520the%2520rating%2520of%252C%2520and%250Aperceived%2520reasoning%2520about%252C%2520the%2520credibility%2520of%2520information.%2520The%2520information%2520in%250Aour%2520experiments%2520comes%2520from%2520content%2520that%2520subnational%2520U.S.%2520politicians%2520post%2520to%250AFacebook.%2520We%2520find%2520that%2520GPT-4o%252C%2520one%2520of%2520the%2520most%2520used%2520AI%2520models%2520in%2520consumer%250Aapplications%252C%2520outperforms%2520other%2520models%252C%2520but%2520all%2520models%2520exhibit%2520only%2520moderate%250Aagreement%2520with%2520human%2520coders.%2520Importantly%252C%2520even%2520when%2520GenAI%2520models%2520accurately%250Aidentify%2520low-credibility%2520content%252C%2520their%2520reasoning%2520relies%2520heavily%2520on%2520linguistic%250Afeatures%2520and%2520%2560%2560hard%2527%2527%2520criteria%252C%2520such%2520as%2520the%2520level%2520of%2520detail%252C%2520source%250Areliability%252C%2520and%2520language%2520formality%252C%2520rather%2520than%2520an%2520understanding%2520of%2520veracity.%250AWe%2520also%2520assess%2520the%2520effectiveness%2520of%2520summarized%2520versus%2520full%2520content%2520inputs%252C%250Afinding%2520that%2520summarized%2520content%2520holds%2520promise%2520for%2520improving%2520efficiency%2520without%250Asacrificing%2520accuracy.%2520While%2520GenAI%2520has%2520the%2520potential%2520to%2520support%2520human%250Afact-checkers%2520in%2520scaling%2520misinformation%2520detection%252C%2520our%2520results%2520caution%2520against%250Arelying%2520solely%2520on%2520these%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAI%20vs.%20Human%20Fact-Checkers%3A%20Accurate%20Ratings%2C%20Flawed%20Rationales&entry.906535625=Yuehong%20Cassandra%20Tai%20and%20Khushi%20Navin%20Patni%20and%20Nicholas%20Daniel%20Hemauer%20and%20Bruce%20Desmarais%20and%20Yu-Ru%20Lin&entry.1292438233=%20%20Despite%20recent%20advances%20in%20understanding%20the%20capabilities%20and%20limits%20of%0Agenerative%20artificial%20intelligence%20%28GenAI%29%20models%2C%20we%20are%20just%20beginning%20to%0Aunderstand%20their%20capacity%20to%20assess%20and%20reason%20about%20the%20veracity%20of%20content.%0AWe%20evaluate%20multiple%20GenAI%20models%20across%20tasks%20that%20involve%20the%20rating%20of%2C%20and%0Aperceived%20reasoning%20about%2C%20the%20credibility%20of%20information.%20The%20information%20in%0Aour%20experiments%20comes%20from%20content%20that%20subnational%20U.S.%20politicians%20post%20to%0AFacebook.%20We%20find%20that%20GPT-4o%2C%20one%20of%20the%20most%20used%20AI%20models%20in%20consumer%0Aapplications%2C%20outperforms%20other%20models%2C%20but%20all%20models%20exhibit%20only%20moderate%0Aagreement%20with%20human%20coders.%20Importantly%2C%20even%20when%20GenAI%20models%20accurately%0Aidentify%20low-credibility%20content%2C%20their%20reasoning%20relies%20heavily%20on%20linguistic%0Afeatures%20and%20%60%60hard%27%27%20criteria%2C%20such%20as%20the%20level%20of%20detail%2C%20source%0Areliability%2C%20and%20language%20formality%2C%20rather%20than%20an%20understanding%20of%20veracity.%0AWe%20also%20assess%20the%20effectiveness%20of%20summarized%20versus%20full%20content%20inputs%2C%0Afinding%20that%20summarized%20content%20holds%20promise%20for%20improving%20efficiency%20without%0Asacrificing%20accuracy.%20While%20GenAI%20has%20the%20potential%20to%20support%20human%0Afact-checkers%20in%20scaling%20misinformation%20detection%2C%20our%20results%20caution%20against%0Arelying%20solely%20on%20these%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14943v2&entry.124074799=Read"},
{"title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in\n  Transformer Models", "author": "Andrew DiGiugno and Ausif Mahmood", "abstract": "  Transformer models typically calculate attention matrices using dot products,\nwhich have limitations when capturing nonlinear relationships between embedding\nvectors. We propose Neural Attention, a technique that replaces dot products\nwith feed-forward networks, enabling a more expressive representation of\nrelationships between tokens. This approach modifies only the attention matrix\ncalculation while preserving the matrix dimensions, making it easily adaptable\nto existing transformer-based architectures. We provide a detailed mathematical\njustification for why Neural Attention increases representational capacity and\nconduct controlled experiments to validate this claim. When comparing Neural\nAttention and Dot-Product Attention, NLP experiments on WikiText-103 show a\nreduction in perplexity of over 5 percent. Similarly, experiments on CIFAR-10\nand CIFAR-100 show comparable improvements for image classification tasks.\nWhile Neural Attention introduces higher computational demands, we develop\ntechniques to mitigate these challenges, ensuring practical usability without\nsacrificing the increased expressivity it provides. This work establishes\nNeural Attention as an effective means of enhancing the predictive capabilities\nof transformer models across a variety of applications.\n", "link": "http://arxiv.org/abs/2502.17206v1", "date": "2025-02-24", "relevancy": 2.2348, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6095}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5683}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Attention%3A%20A%20Novel%20Mechanism%20for%20Enhanced%20Expressive%20Power%20in%0A%20%20Transformer%20Models&body=Title%3A%20Neural%20Attention%3A%20A%20Novel%20Mechanism%20for%20Enhanced%20Expressive%20Power%20in%0A%20%20Transformer%20Models%0AAuthor%3A%20Andrew%20DiGiugno%20and%20Ausif%20Mahmood%0AAbstract%3A%20%20%20Transformer%20models%20typically%20calculate%20attention%20matrices%20using%20dot%20products%2C%0Awhich%20have%20limitations%20when%20capturing%20nonlinear%20relationships%20between%20embedding%0Avectors.%20We%20propose%20Neural%20Attention%2C%20a%20technique%20that%20replaces%20dot%20products%0Awith%20feed-forward%20networks%2C%20enabling%20a%20more%20expressive%20representation%20of%0Arelationships%20between%20tokens.%20This%20approach%20modifies%20only%20the%20attention%20matrix%0Acalculation%20while%20preserving%20the%20matrix%20dimensions%2C%20making%20it%20easily%20adaptable%0Ato%20existing%20transformer-based%20architectures.%20We%20provide%20a%20detailed%20mathematical%0Ajustification%20for%20why%20Neural%20Attention%20increases%20representational%20capacity%20and%0Aconduct%20controlled%20experiments%20to%20validate%20this%20claim.%20When%20comparing%20Neural%0AAttention%20and%20Dot-Product%20Attention%2C%20NLP%20experiments%20on%20WikiText-103%20show%20a%0Areduction%20in%20perplexity%20of%20over%205%20percent.%20Similarly%2C%20experiments%20on%20CIFAR-10%0Aand%20CIFAR-100%20show%20comparable%20improvements%20for%20image%20classification%20tasks.%0AWhile%20Neural%20Attention%20introduces%20higher%20computational%20demands%2C%20we%20develop%0Atechniques%20to%20mitigate%20these%20challenges%2C%20ensuring%20practical%20usability%20without%0Asacrificing%20the%20increased%20expressivity%20it%20provides.%20This%20work%20establishes%0ANeural%20Attention%20as%20an%20effective%20means%20of%20enhancing%20the%20predictive%20capabilities%0Aof%20transformer%20models%20across%20a%20variety%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Attention%253A%2520A%2520Novel%2520Mechanism%2520for%2520Enhanced%2520Expressive%2520Power%2520in%250A%2520%2520Transformer%2520Models%26entry.906535625%3DAndrew%2520DiGiugno%2520and%2520Ausif%2520Mahmood%26entry.1292438233%3D%2520%2520Transformer%2520models%2520typically%2520calculate%2520attention%2520matrices%2520using%2520dot%2520products%252C%250Awhich%2520have%2520limitations%2520when%2520capturing%2520nonlinear%2520relationships%2520between%2520embedding%250Avectors.%2520We%2520propose%2520Neural%2520Attention%252C%2520a%2520technique%2520that%2520replaces%2520dot%2520products%250Awith%2520feed-forward%2520networks%252C%2520enabling%2520a%2520more%2520expressive%2520representation%2520of%250Arelationships%2520between%2520tokens.%2520This%2520approach%2520modifies%2520only%2520the%2520attention%2520matrix%250Acalculation%2520while%2520preserving%2520the%2520matrix%2520dimensions%252C%2520making%2520it%2520easily%2520adaptable%250Ato%2520existing%2520transformer-based%2520architectures.%2520We%2520provide%2520a%2520detailed%2520mathematical%250Ajustification%2520for%2520why%2520Neural%2520Attention%2520increases%2520representational%2520capacity%2520and%250Aconduct%2520controlled%2520experiments%2520to%2520validate%2520this%2520claim.%2520When%2520comparing%2520Neural%250AAttention%2520and%2520Dot-Product%2520Attention%252C%2520NLP%2520experiments%2520on%2520WikiText-103%2520show%2520a%250Areduction%2520in%2520perplexity%2520of%2520over%25205%2520percent.%2520Similarly%252C%2520experiments%2520on%2520CIFAR-10%250Aand%2520CIFAR-100%2520show%2520comparable%2520improvements%2520for%2520image%2520classification%2520tasks.%250AWhile%2520Neural%2520Attention%2520introduces%2520higher%2520computational%2520demands%252C%2520we%2520develop%250Atechniques%2520to%2520mitigate%2520these%2520challenges%252C%2520ensuring%2520practical%2520usability%2520without%250Asacrificing%2520the%2520increased%2520expressivity%2520it%2520provides.%2520This%2520work%2520establishes%250ANeural%2520Attention%2520as%2520an%2520effective%2520means%2520of%2520enhancing%2520the%2520predictive%2520capabilities%250Aof%2520transformer%2520models%2520across%2520a%2520variety%2520of%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Attention%3A%20A%20Novel%20Mechanism%20for%20Enhanced%20Expressive%20Power%20in%0A%20%20Transformer%20Models&entry.906535625=Andrew%20DiGiugno%20and%20Ausif%20Mahmood&entry.1292438233=%20%20Transformer%20models%20typically%20calculate%20attention%20matrices%20using%20dot%20products%2C%0Awhich%20have%20limitations%20when%20capturing%20nonlinear%20relationships%20between%20embedding%0Avectors.%20We%20propose%20Neural%20Attention%2C%20a%20technique%20that%20replaces%20dot%20products%0Awith%20feed-forward%20networks%2C%20enabling%20a%20more%20expressive%20representation%20of%0Arelationships%20between%20tokens.%20This%20approach%20modifies%20only%20the%20attention%20matrix%0Acalculation%20while%20preserving%20the%20matrix%20dimensions%2C%20making%20it%20easily%20adaptable%0Ato%20existing%20transformer-based%20architectures.%20We%20provide%20a%20detailed%20mathematical%0Ajustification%20for%20why%20Neural%20Attention%20increases%20representational%20capacity%20and%0Aconduct%20controlled%20experiments%20to%20validate%20this%20claim.%20When%20comparing%20Neural%0AAttention%20and%20Dot-Product%20Attention%2C%20NLP%20experiments%20on%20WikiText-103%20show%20a%0Areduction%20in%20perplexity%20of%20over%205%20percent.%20Similarly%2C%20experiments%20on%20CIFAR-10%0Aand%20CIFAR-100%20show%20comparable%20improvements%20for%20image%20classification%20tasks.%0AWhile%20Neural%20Attention%20introduces%20higher%20computational%20demands%2C%20we%20develop%0Atechniques%20to%20mitigate%20these%20challenges%2C%20ensuring%20practical%20usability%20without%0Asacrificing%20the%20increased%20expressivity%20it%20provides.%20This%20work%20establishes%0ANeural%20Attention%20as%20an%20effective%20means%20of%20enhancing%20the%20predictive%20capabilities%0Aof%20transformer%20models%20across%20a%20variety%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17206v1&entry.124074799=Read"},
{"title": "Which Transformer to Favor: A Comparative Analysis of Efficiency in\n  Vision Transformers", "author": "Tobias Christian Nauen and Sebastian Palacio and Federico Raue and Andreas Dengel", "abstract": "  Self-attention in Transformers comes with a high computational cost because\nof their quadratic computational complexity, but their effectiveness in\naddressing problems in language and vision has sparked extensive research aimed\nat enhancing their efficiency. However, diverse experimental conditions,\nspanning multiple input domains, prevent a fair comparison based solely on\nreported results, posing challenges for model selection. To address this gap in\ncomparability, we perform a large-scale benchmark of more than 45 models for\nimage classification, evaluating key efficiency aspects, including accuracy,\nspeed, and memory usage. Our benchmark provides a standardized baseline for\nefficiency-oriented transformers. We analyze the results based on the Pareto\nfront -- the boundary of optimal models. Surprisingly, despite claims of other\nmodels being more efficient, ViT remains Pareto optimal across multiple\nmetrics. We observe that hybrid attention-CNN models exhibit remarkable\ninference memory- and parameter-efficiency. Moreover, our benchmark shows that\nusing a larger model in general is more efficient than using higher resolution\nimages. Thanks to our holistic evaluation, we provide a centralized resource\nfor practitioners and researchers, facilitating informed decisions when\nselecting or developing efficient transformers.\n", "link": "http://arxiv.org/abs/2308.09372v4", "date": "2025-02-24", "relevancy": 2.2205, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Which%20Transformer%20to%20Favor%3A%20A%20Comparative%20Analysis%20of%20Efficiency%20in%0A%20%20Vision%20Transformers&body=Title%3A%20Which%20Transformer%20to%20Favor%3A%20A%20Comparative%20Analysis%20of%20Efficiency%20in%0A%20%20Vision%20Transformers%0AAuthor%3A%20Tobias%20Christian%20Nauen%20and%20Sebastian%20Palacio%20and%20Federico%20Raue%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Self-attention%20in%20Transformers%20comes%20with%20a%20high%20computational%20cost%20because%0Aof%20their%20quadratic%20computational%20complexity%2C%20but%20their%20effectiveness%20in%0Aaddressing%20problems%20in%20language%20and%20vision%20has%20sparked%20extensive%20research%20aimed%0Aat%20enhancing%20their%20efficiency.%20However%2C%20diverse%20experimental%20conditions%2C%0Aspanning%20multiple%20input%20domains%2C%20prevent%20a%20fair%20comparison%20based%20solely%20on%0Areported%20results%2C%20posing%20challenges%20for%20model%20selection.%20To%20address%20this%20gap%20in%0Acomparability%2C%20we%20perform%20a%20large-scale%20benchmark%20of%20more%20than%2045%20models%20for%0Aimage%20classification%2C%20evaluating%20key%20efficiency%20aspects%2C%20including%20accuracy%2C%0Aspeed%2C%20and%20memory%20usage.%20Our%20benchmark%20provides%20a%20standardized%20baseline%20for%0Aefficiency-oriented%20transformers.%20We%20analyze%20the%20results%20based%20on%20the%20Pareto%0Afront%20--%20the%20boundary%20of%20optimal%20models.%20Surprisingly%2C%20despite%20claims%20of%20other%0Amodels%20being%20more%20efficient%2C%20ViT%20remains%20Pareto%20optimal%20across%20multiple%0Ametrics.%20We%20observe%20that%20hybrid%20attention-CNN%20models%20exhibit%20remarkable%0Ainference%20memory-%20and%20parameter-efficiency.%20Moreover%2C%20our%20benchmark%20shows%20that%0Ausing%20a%20larger%20model%20in%20general%20is%20more%20efficient%20than%20using%20higher%20resolution%0Aimages.%20Thanks%20to%20our%20holistic%20evaluation%2C%20we%20provide%20a%20centralized%20resource%0Afor%20practitioners%20and%20researchers%2C%20facilitating%20informed%20decisions%20when%0Aselecting%20or%20developing%20efficient%20transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.09372v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhich%2520Transformer%2520to%2520Favor%253A%2520A%2520Comparative%2520Analysis%2520of%2520Efficiency%2520in%250A%2520%2520Vision%2520Transformers%26entry.906535625%3DTobias%2520Christian%2520Nauen%2520and%2520Sebastian%2520Palacio%2520and%2520Federico%2520Raue%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Self-attention%2520in%2520Transformers%2520comes%2520with%2520a%2520high%2520computational%2520cost%2520because%250Aof%2520their%2520quadratic%2520computational%2520complexity%252C%2520but%2520their%2520effectiveness%2520in%250Aaddressing%2520problems%2520in%2520language%2520and%2520vision%2520has%2520sparked%2520extensive%2520research%2520aimed%250Aat%2520enhancing%2520their%2520efficiency.%2520However%252C%2520diverse%2520experimental%2520conditions%252C%250Aspanning%2520multiple%2520input%2520domains%252C%2520prevent%2520a%2520fair%2520comparison%2520based%2520solely%2520on%250Areported%2520results%252C%2520posing%2520challenges%2520for%2520model%2520selection.%2520To%2520address%2520this%2520gap%2520in%250Acomparability%252C%2520we%2520perform%2520a%2520large-scale%2520benchmark%2520of%2520more%2520than%252045%2520models%2520for%250Aimage%2520classification%252C%2520evaluating%2520key%2520efficiency%2520aspects%252C%2520including%2520accuracy%252C%250Aspeed%252C%2520and%2520memory%2520usage.%2520Our%2520benchmark%2520provides%2520a%2520standardized%2520baseline%2520for%250Aefficiency-oriented%2520transformers.%2520We%2520analyze%2520the%2520results%2520based%2520on%2520the%2520Pareto%250Afront%2520--%2520the%2520boundary%2520of%2520optimal%2520models.%2520Surprisingly%252C%2520despite%2520claims%2520of%2520other%250Amodels%2520being%2520more%2520efficient%252C%2520ViT%2520remains%2520Pareto%2520optimal%2520across%2520multiple%250Ametrics.%2520We%2520observe%2520that%2520hybrid%2520attention-CNN%2520models%2520exhibit%2520remarkable%250Ainference%2520memory-%2520and%2520parameter-efficiency.%2520Moreover%252C%2520our%2520benchmark%2520shows%2520that%250Ausing%2520a%2520larger%2520model%2520in%2520general%2520is%2520more%2520efficient%2520than%2520using%2520higher%2520resolution%250Aimages.%2520Thanks%2520to%2520our%2520holistic%2520evaluation%252C%2520we%2520provide%2520a%2520centralized%2520resource%250Afor%2520practitioners%2520and%2520researchers%252C%2520facilitating%2520informed%2520decisions%2520when%250Aselecting%2520or%2520developing%2520efficient%2520transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.09372v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Transformer%20to%20Favor%3A%20A%20Comparative%20Analysis%20of%20Efficiency%20in%0A%20%20Vision%20Transformers&entry.906535625=Tobias%20Christian%20Nauen%20and%20Sebastian%20Palacio%20and%20Federico%20Raue%20and%20Andreas%20Dengel&entry.1292438233=%20%20Self-attention%20in%20Transformers%20comes%20with%20a%20high%20computational%20cost%20because%0Aof%20their%20quadratic%20computational%20complexity%2C%20but%20their%20effectiveness%20in%0Aaddressing%20problems%20in%20language%20and%20vision%20has%20sparked%20extensive%20research%20aimed%0Aat%20enhancing%20their%20efficiency.%20However%2C%20diverse%20experimental%20conditions%2C%0Aspanning%20multiple%20input%20domains%2C%20prevent%20a%20fair%20comparison%20based%20solely%20on%0Areported%20results%2C%20posing%20challenges%20for%20model%20selection.%20To%20address%20this%20gap%20in%0Acomparability%2C%20we%20perform%20a%20large-scale%20benchmark%20of%20more%20than%2045%20models%20for%0Aimage%20classification%2C%20evaluating%20key%20efficiency%20aspects%2C%20including%20accuracy%2C%0Aspeed%2C%20and%20memory%20usage.%20Our%20benchmark%20provides%20a%20standardized%20baseline%20for%0Aefficiency-oriented%20transformers.%20We%20analyze%20the%20results%20based%20on%20the%20Pareto%0Afront%20--%20the%20boundary%20of%20optimal%20models.%20Surprisingly%2C%20despite%20claims%20of%20other%0Amodels%20being%20more%20efficient%2C%20ViT%20remains%20Pareto%20optimal%20across%20multiple%0Ametrics.%20We%20observe%20that%20hybrid%20attention-CNN%20models%20exhibit%20remarkable%0Ainference%20memory-%20and%20parameter-efficiency.%20Moreover%2C%20our%20benchmark%20shows%20that%0Ausing%20a%20larger%20model%20in%20general%20is%20more%20efficient%20than%20using%20higher%20resolution%0Aimages.%20Thanks%20to%20our%20holistic%20evaluation%2C%20we%20provide%20a%20centralized%20resource%0Afor%20practitioners%20and%20researchers%2C%20facilitating%20informed%20decisions%20when%0Aselecting%20or%20developing%20efficient%20transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.09372v4&entry.124074799=Read"},
{"title": "M3DA: Benchmark for Unsupervised Domain Adaptation in 3D Medical Image\n  Segmentation", "author": "Boris Shirokikh and Anvar Kurmukov and Mariia Donskova and Valentin Samokhin and Mikhail Belyaev and Ivan Oseledets", "abstract": "  Domain shift presents a significant challenge in applying Deep Learning to\nthe segmentation of 3D medical images from sources like Magnetic Resonance\nImaging (MRI) and Computed Tomography (CT). Although numerous Domain Adaptation\nmethods have been developed to address this issue, they are often evaluated\nunder impractical data shift scenarios. Specifically, the medical imaging\ndatasets used are often either private, too small for robust training and\nevaluation, or limited to single or synthetic tasks. To overcome these\nlimitations, we introduce a M3DA /\"mEd@/ benchmark comprising four publicly\navailable, multiclass segmentation datasets. We have designed eight domain\npairs featuring diverse and practically relevant distribution shifts. These\ninclude inter-modality shifts between MRI and CT and intra-modality shifts\namong various MRI acquisition parameters, different CT radiation doses, and\npresence/absence of contrast enhancement in images. Within the proposed\nbenchmark, we evaluate more than ten existing domain adaptation methods. Our\nresults show that none of them can consistently close the performance gap\nbetween the domains. For instance, the most effective method reduces the\nperformance gap by about 62% across the tasks. This highlights the need for\ndeveloping novel domain adaptation algorithms to enhance the robustness and\nscalability of deep learning models in medical imaging. We made our M3DA\nbenchmark publicly available: https://github.com/BorisShirokikh/M3DA.\n", "link": "http://arxiv.org/abs/2502.17029v1", "date": "2025-02-24", "relevancy": 2.1969, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5772}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M3DA%3A%20Benchmark%20for%20Unsupervised%20Domain%20Adaptation%20in%203D%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20M3DA%3A%20Benchmark%20for%20Unsupervised%20Domain%20Adaptation%20in%203D%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Boris%20Shirokikh%20and%20Anvar%20Kurmukov%20and%20Mariia%20Donskova%20and%20Valentin%20Samokhin%20and%20Mikhail%20Belyaev%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20Domain%20shift%20presents%20a%20significant%20challenge%20in%20applying%20Deep%20Learning%20to%0Athe%20segmentation%20of%203D%20medical%20images%20from%20sources%20like%20Magnetic%20Resonance%0AImaging%20%28MRI%29%20and%20Computed%20Tomography%20%28CT%29.%20Although%20numerous%20Domain%20Adaptation%0Amethods%20have%20been%20developed%20to%20address%20this%20issue%2C%20they%20are%20often%20evaluated%0Aunder%20impractical%20data%20shift%20scenarios.%20Specifically%2C%20the%20medical%20imaging%0Adatasets%20used%20are%20often%20either%20private%2C%20too%20small%20for%20robust%20training%20and%0Aevaluation%2C%20or%20limited%20to%20single%20or%20synthetic%20tasks.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20a%20M3DA%20/%22mEd%40/%20benchmark%20comprising%20four%20publicly%0Aavailable%2C%20multiclass%20segmentation%20datasets.%20We%20have%20designed%20eight%20domain%0Apairs%20featuring%20diverse%20and%20practically%20relevant%20distribution%20shifts.%20These%0Ainclude%20inter-modality%20shifts%20between%20MRI%20and%20CT%20and%20intra-modality%20shifts%0Aamong%20various%20MRI%20acquisition%20parameters%2C%20different%20CT%20radiation%20doses%2C%20and%0Apresence/absence%20of%20contrast%20enhancement%20in%20images.%20Within%20the%20proposed%0Abenchmark%2C%20we%20evaluate%20more%20than%20ten%20existing%20domain%20adaptation%20methods.%20Our%0Aresults%20show%20that%20none%20of%20them%20can%20consistently%20close%20the%20performance%20gap%0Abetween%20the%20domains.%20For%20instance%2C%20the%20most%20effective%20method%20reduces%20the%0Aperformance%20gap%20by%20about%2062%25%20across%20the%20tasks.%20This%20highlights%20the%20need%20for%0Adeveloping%20novel%20domain%20adaptation%20algorithms%20to%20enhance%20the%20robustness%20and%0Ascalability%20of%20deep%20learning%20models%20in%20medical%20imaging.%20We%20made%20our%20M3DA%0Abenchmark%20publicly%20available%3A%20https%3A//github.com/BorisShirokikh/M3DA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM3DA%253A%2520Benchmark%2520for%2520Unsupervised%2520Domain%2520Adaptation%2520in%25203D%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DBoris%2520Shirokikh%2520and%2520Anvar%2520Kurmukov%2520and%2520Mariia%2520Donskova%2520and%2520Valentin%2520Samokhin%2520and%2520Mikhail%2520Belyaev%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520Domain%2520shift%2520presents%2520a%2520significant%2520challenge%2520in%2520applying%2520Deep%2520Learning%2520to%250Athe%2520segmentation%2520of%25203D%2520medical%2520images%2520from%2520sources%2520like%2520Magnetic%2520Resonance%250AImaging%2520%2528MRI%2529%2520and%2520Computed%2520Tomography%2520%2528CT%2529.%2520Although%2520numerous%2520Domain%2520Adaptation%250Amethods%2520have%2520been%2520developed%2520to%2520address%2520this%2520issue%252C%2520they%2520are%2520often%2520evaluated%250Aunder%2520impractical%2520data%2520shift%2520scenarios.%2520Specifically%252C%2520the%2520medical%2520imaging%250Adatasets%2520used%2520are%2520often%2520either%2520private%252C%2520too%2520small%2520for%2520robust%2520training%2520and%250Aevaluation%252C%2520or%2520limited%2520to%2520single%2520or%2520synthetic%2520tasks.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520introduce%2520a%2520M3DA%2520/%2522mEd%2540/%2520benchmark%2520comprising%2520four%2520publicly%250Aavailable%252C%2520multiclass%2520segmentation%2520datasets.%2520We%2520have%2520designed%2520eight%2520domain%250Apairs%2520featuring%2520diverse%2520and%2520practically%2520relevant%2520distribution%2520shifts.%2520These%250Ainclude%2520inter-modality%2520shifts%2520between%2520MRI%2520and%2520CT%2520and%2520intra-modality%2520shifts%250Aamong%2520various%2520MRI%2520acquisition%2520parameters%252C%2520different%2520CT%2520radiation%2520doses%252C%2520and%250Apresence/absence%2520of%2520contrast%2520enhancement%2520in%2520images.%2520Within%2520the%2520proposed%250Abenchmark%252C%2520we%2520evaluate%2520more%2520than%2520ten%2520existing%2520domain%2520adaptation%2520methods.%2520Our%250Aresults%2520show%2520that%2520none%2520of%2520them%2520can%2520consistently%2520close%2520the%2520performance%2520gap%250Abetween%2520the%2520domains.%2520For%2520instance%252C%2520the%2520most%2520effective%2520method%2520reduces%2520the%250Aperformance%2520gap%2520by%2520about%252062%2525%2520across%2520the%2520tasks.%2520This%2520highlights%2520the%2520need%2520for%250Adeveloping%2520novel%2520domain%2520adaptation%2520algorithms%2520to%2520enhance%2520the%2520robustness%2520and%250Ascalability%2520of%2520deep%2520learning%2520models%2520in%2520medical%2520imaging.%2520We%2520made%2520our%2520M3DA%250Abenchmark%2520publicly%2520available%253A%2520https%253A//github.com/BorisShirokikh/M3DA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M3DA%3A%20Benchmark%20for%20Unsupervised%20Domain%20Adaptation%20in%203D%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Boris%20Shirokikh%20and%20Anvar%20Kurmukov%20and%20Mariia%20Donskova%20and%20Valentin%20Samokhin%20and%20Mikhail%20Belyaev%20and%20Ivan%20Oseledets&entry.1292438233=%20%20Domain%20shift%20presents%20a%20significant%20challenge%20in%20applying%20Deep%20Learning%20to%0Athe%20segmentation%20of%203D%20medical%20images%20from%20sources%20like%20Magnetic%20Resonance%0AImaging%20%28MRI%29%20and%20Computed%20Tomography%20%28CT%29.%20Although%20numerous%20Domain%20Adaptation%0Amethods%20have%20been%20developed%20to%20address%20this%20issue%2C%20they%20are%20often%20evaluated%0Aunder%20impractical%20data%20shift%20scenarios.%20Specifically%2C%20the%20medical%20imaging%0Adatasets%20used%20are%20often%20either%20private%2C%20too%20small%20for%20robust%20training%20and%0Aevaluation%2C%20or%20limited%20to%20single%20or%20synthetic%20tasks.%20To%20overcome%20these%0Alimitations%2C%20we%20introduce%20a%20M3DA%20/%22mEd%40/%20benchmark%20comprising%20four%20publicly%0Aavailable%2C%20multiclass%20segmentation%20datasets.%20We%20have%20designed%20eight%20domain%0Apairs%20featuring%20diverse%20and%20practically%20relevant%20distribution%20shifts.%20These%0Ainclude%20inter-modality%20shifts%20between%20MRI%20and%20CT%20and%20intra-modality%20shifts%0Aamong%20various%20MRI%20acquisition%20parameters%2C%20different%20CT%20radiation%20doses%2C%20and%0Apresence/absence%20of%20contrast%20enhancement%20in%20images.%20Within%20the%20proposed%0Abenchmark%2C%20we%20evaluate%20more%20than%20ten%20existing%20domain%20adaptation%20methods.%20Our%0Aresults%20show%20that%20none%20of%20them%20can%20consistently%20close%20the%20performance%20gap%0Abetween%20the%20domains.%20For%20instance%2C%20the%20most%20effective%20method%20reduces%20the%0Aperformance%20gap%20by%20about%2062%25%20across%20the%20tasks.%20This%20highlights%20the%20need%20for%0Adeveloping%20novel%20domain%20adaptation%20algorithms%20to%20enhance%20the%20robustness%20and%0Ascalability%20of%20deep%20learning%20models%20in%20medical%20imaging.%20We%20made%20our%20M3DA%0Abenchmark%20publicly%20available%3A%20https%3A//github.com/BorisShirokikh/M3DA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17029v1&entry.124074799=Read"},
{"title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation", "author": "Tianrui Zhu and Shiyi Zhang and Jiawei Shao and Yansong Tang", "abstract": "  Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit\n", "link": "http://arxiv.org/abs/2502.17363v1", "date": "2025-02-24", "relevancy": 2.1958, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5604}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5448}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KV-Edit%3A%20Training-Free%20Image%20Editing%20for%20Precise%20Background%20Preservation&body=Title%3A%20KV-Edit%3A%20Training-Free%20Image%20Editing%20for%20Precise%20Background%20Preservation%0AAuthor%3A%20Tianrui%20Zhu%20and%20Shiyi%20Zhang%20and%20Jiawei%20Shao%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20Background%20consistency%20remains%20a%20significant%20challenge%20in%20image%20editing%0Atasks.%20Despite%20extensive%20developments%2C%20existing%20works%20still%20face%20a%20trade-off%0Abetween%20maintaining%20similarity%20to%20the%20original%20image%20and%20generating%20content%0Athat%20aligns%20with%20the%20target.%20Here%2C%20we%20propose%20KV-Edit%2C%20a%20training-free%20approach%0Athat%20uses%20KV%20cache%20in%20DiTs%20to%20maintain%20background%20consistency%2C%20where%20background%0Atokens%20are%20preserved%20rather%20than%20regenerated%2C%20eliminating%20the%20need%20for%20complex%0Amechanisms%20or%20expensive%20training%2C%20ultimately%20generating%20new%20content%20that%0Aseamlessly%20integrates%20with%20the%20background%20within%20user-provided%20regions.%20We%0Afurther%20explore%20the%20memory%20consumption%20of%20the%20KV%20cache%20during%20editing%20and%0Aoptimize%20the%20space%20complexity%20to%20%24O%281%29%24%20using%20an%20inversion-free%20method.%20Our%0Aapproach%20is%20compatible%20with%20any%20DiT-based%20generative%20model%20without%20additional%0Atraining.%20Experiments%20demonstrate%20that%20KV-Edit%20significantly%20outperforms%0Aexisting%20approaches%20in%20terms%20of%20both%20background%20and%20image%20quality%2C%20even%0Asurpassing%20training-based%20methods.%20Project%20webpage%20is%20available%20at%0Ahttps%3A//xilluill.github.io/projectpages/KV-Edit%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKV-Edit%253A%2520Training-Free%2520Image%2520Editing%2520for%2520Precise%2520Background%2520Preservation%26entry.906535625%3DTianrui%2520Zhu%2520and%2520Shiyi%2520Zhang%2520and%2520Jiawei%2520Shao%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520Background%2520consistency%2520remains%2520a%2520significant%2520challenge%2520in%2520image%2520editing%250Atasks.%2520Despite%2520extensive%2520developments%252C%2520existing%2520works%2520still%2520face%2520a%2520trade-off%250Abetween%2520maintaining%2520similarity%2520to%2520the%2520original%2520image%2520and%2520generating%2520content%250Athat%2520aligns%2520with%2520the%2520target.%2520Here%252C%2520we%2520propose%2520KV-Edit%252C%2520a%2520training-free%2520approach%250Athat%2520uses%2520KV%2520cache%2520in%2520DiTs%2520to%2520maintain%2520background%2520consistency%252C%2520where%2520background%250Atokens%2520are%2520preserved%2520rather%2520than%2520regenerated%252C%2520eliminating%2520the%2520need%2520for%2520complex%250Amechanisms%2520or%2520expensive%2520training%252C%2520ultimately%2520generating%2520new%2520content%2520that%250Aseamlessly%2520integrates%2520with%2520the%2520background%2520within%2520user-provided%2520regions.%2520We%250Afurther%2520explore%2520the%2520memory%2520consumption%2520of%2520the%2520KV%2520cache%2520during%2520editing%2520and%250Aoptimize%2520the%2520space%2520complexity%2520to%2520%2524O%25281%2529%2524%2520using%2520an%2520inversion-free%2520method.%2520Our%250Aapproach%2520is%2520compatible%2520with%2520any%2520DiT-based%2520generative%2520model%2520without%2520additional%250Atraining.%2520Experiments%2520demonstrate%2520that%2520KV-Edit%2520significantly%2520outperforms%250Aexisting%2520approaches%2520in%2520terms%2520of%2520both%2520background%2520and%2520image%2520quality%252C%2520even%250Asurpassing%2520training-based%2520methods.%2520Project%2520webpage%2520is%2520available%2520at%250Ahttps%253A//xilluill.github.io/projectpages/KV-Edit%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KV-Edit%3A%20Training-Free%20Image%20Editing%20for%20Precise%20Background%20Preservation&entry.906535625=Tianrui%20Zhu%20and%20Shiyi%20Zhang%20and%20Jiawei%20Shao%20and%20Yansong%20Tang&entry.1292438233=%20%20Background%20consistency%20remains%20a%20significant%20challenge%20in%20image%20editing%0Atasks.%20Despite%20extensive%20developments%2C%20existing%20works%20still%20face%20a%20trade-off%0Abetween%20maintaining%20similarity%20to%20the%20original%20image%20and%20generating%20content%0Athat%20aligns%20with%20the%20target.%20Here%2C%20we%20propose%20KV-Edit%2C%20a%20training-free%20approach%0Athat%20uses%20KV%20cache%20in%20DiTs%20to%20maintain%20background%20consistency%2C%20where%20background%0Atokens%20are%20preserved%20rather%20than%20regenerated%2C%20eliminating%20the%20need%20for%20complex%0Amechanisms%20or%20expensive%20training%2C%20ultimately%20generating%20new%20content%20that%0Aseamlessly%20integrates%20with%20the%20background%20within%20user-provided%20regions.%20We%0Afurther%20explore%20the%20memory%20consumption%20of%20the%20KV%20cache%20during%20editing%20and%0Aoptimize%20the%20space%20complexity%20to%20%24O%281%29%24%20using%20an%20inversion-free%20method.%20Our%0Aapproach%20is%20compatible%20with%20any%20DiT-based%20generative%20model%20without%20additional%0Atraining.%20Experiments%20demonstrate%20that%20KV-Edit%20significantly%20outperforms%0Aexisting%20approaches%20in%20terms%20of%20both%20background%20and%20image%20quality%2C%20even%0Asurpassing%20training-based%20methods.%20Project%20webpage%20is%20available%20at%0Ahttps%3A//xilluill.github.io/projectpages/KV-Edit%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17363v1&entry.124074799=Read"},
{"title": "Fast Summation of Radial Kernels via QMC Slicing", "author": "Johannes Hertrich and Tim Jahn and Michael Quellmalz", "abstract": "  The fast computation of large kernel sums is a challenging task, which arises\nas a subproblem in any kernel method. We approach the problem by slicing, which\nrelies on random projections to one-dimensional subspaces and fast Fourier\nsummation. We prove bounds for the slicing error and propose a quasi-Monte\nCarlo (QMC) approach for selecting the projections based on spherical\nquadrature rules. Numerical examples demonstrate that our QMC-slicing approach\nsignificantly outperforms existing methods like (QMC-)random Fourier features,\northogonal Fourier features or non-QMC slicing on standard test datasets.\n", "link": "http://arxiv.org/abs/2410.01316v2", "date": "2025-02-24", "relevancy": 2.1946, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4645}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4427}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Summation%20of%20Radial%20Kernels%20via%20QMC%20Slicing&body=Title%3A%20Fast%20Summation%20of%20Radial%20Kernels%20via%20QMC%20Slicing%0AAuthor%3A%20Johannes%20Hertrich%20and%20Tim%20Jahn%20and%20Michael%20Quellmalz%0AAbstract%3A%20%20%20The%20fast%20computation%20of%20large%20kernel%20sums%20is%20a%20challenging%20task%2C%20which%20arises%0Aas%20a%20subproblem%20in%20any%20kernel%20method.%20We%20approach%20the%20problem%20by%20slicing%2C%20which%0Arelies%20on%20random%20projections%20to%20one-dimensional%20subspaces%20and%20fast%20Fourier%0Asummation.%20We%20prove%20bounds%20for%20the%20slicing%20error%20and%20propose%20a%20quasi-Monte%0ACarlo%20%28QMC%29%20approach%20for%20selecting%20the%20projections%20based%20on%20spherical%0Aquadrature%20rules.%20Numerical%20examples%20demonstrate%20that%20our%20QMC-slicing%20approach%0Asignificantly%20outperforms%20existing%20methods%20like%20%28QMC-%29random%20Fourier%20features%2C%0Aorthogonal%20Fourier%20features%20or%20non-QMC%20slicing%20on%20standard%20test%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Summation%2520of%2520Radial%2520Kernels%2520via%2520QMC%2520Slicing%26entry.906535625%3DJohannes%2520Hertrich%2520and%2520Tim%2520Jahn%2520and%2520Michael%2520Quellmalz%26entry.1292438233%3D%2520%2520The%2520fast%2520computation%2520of%2520large%2520kernel%2520sums%2520is%2520a%2520challenging%2520task%252C%2520which%2520arises%250Aas%2520a%2520subproblem%2520in%2520any%2520kernel%2520method.%2520We%2520approach%2520the%2520problem%2520by%2520slicing%252C%2520which%250Arelies%2520on%2520random%2520projections%2520to%2520one-dimensional%2520subspaces%2520and%2520fast%2520Fourier%250Asummation.%2520We%2520prove%2520bounds%2520for%2520the%2520slicing%2520error%2520and%2520propose%2520a%2520quasi-Monte%250ACarlo%2520%2528QMC%2529%2520approach%2520for%2520selecting%2520the%2520projections%2520based%2520on%2520spherical%250Aquadrature%2520rules.%2520Numerical%2520examples%2520demonstrate%2520that%2520our%2520QMC-slicing%2520approach%250Asignificantly%2520outperforms%2520existing%2520methods%2520like%2520%2528QMC-%2529random%2520Fourier%2520features%252C%250Aorthogonal%2520Fourier%2520features%2520or%2520non-QMC%2520slicing%2520on%2520standard%2520test%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Summation%20of%20Radial%20Kernels%20via%20QMC%20Slicing&entry.906535625=Johannes%20Hertrich%20and%20Tim%20Jahn%20and%20Michael%20Quellmalz&entry.1292438233=%20%20The%20fast%20computation%20of%20large%20kernel%20sums%20is%20a%20challenging%20task%2C%20which%20arises%0Aas%20a%20subproblem%20in%20any%20kernel%20method.%20We%20approach%20the%20problem%20by%20slicing%2C%20which%0Arelies%20on%20random%20projections%20to%20one-dimensional%20subspaces%20and%20fast%20Fourier%0Asummation.%20We%20prove%20bounds%20for%20the%20slicing%20error%20and%20propose%20a%20quasi-Monte%0ACarlo%20%28QMC%29%20approach%20for%20selecting%20the%20projections%20based%20on%20spherical%0Aquadrature%20rules.%20Numerical%20examples%20demonstrate%20that%20our%20QMC-slicing%20approach%0Asignificantly%20outperforms%20existing%20methods%20like%20%28QMC-%29random%20Fourier%20features%2C%0Aorthogonal%20Fourier%20features%20or%20non-QMC%20slicing%20on%20standard%20test%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01316v2&entry.124074799=Read"},
{"title": "RELICT: A Replica Detection Framework for Medical Image Generation", "author": "Orhun Utku Aydin and Alexander Koch and Adam Hilbert and Jana Rieger and Felix Lohrke and Fujimaro Ishida and Satoru Tanioka and Dietmar Frey", "abstract": "  Despite the potential of synthetic medical data for augmenting and improving\nthe generalizability of deep learning models, memorization in generative models\ncan lead to unintended leakage of sensitive patient information and limit model\nutility. Thus, the use of memorizing generative models in the medical domain\ncan jeopardize patient privacy. We propose a framework for identifying\nreplicas, i.e. nearly identical copies of the training data, in synthetic\nmedical image datasets. Our REpLIca deteCTion (RELICT) framework for medical\nimage generative models evaluates image similarity using three complementary\napproaches: (1) voxel-level analysis, (2) feature-level analysis by a\npretrained medical foundation model, and (3) segmentation-level analysis. Two\nclinically relevant 3D generative modelling use cases were investigated:\nnon-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flight\nMR angiography of the Circle of Willis (N=1,782). Expert visual scoring was\nused as the reference standard to assess the presence of replicas. We report\nthe balanced accuracy at the optimal threshold to assess replica classification\nperformance. The reference visual rating identified 45 of 50 and 5 of 50\ngenerated images as replicas for the NCCT and TOF-MRA use cases, respectively.\nImage-level and feature-level measures perfectly classified replicas with a\nbalanced accuracy of 1 when an optimal threshold was selected for the NCCT use\ncase. A perfect classification of replicas for the TOF-MRA case was not\npossible at any threshold, with the segmentation-level analysis achieving a\nbalanced accuracy of 0.79. Replica detection is a crucial but neglected\nvalidation step for the development of generative models in medical imaging.\nThe proposed RELICT framework provides a standardized, easy-to-use tool for\nreplica detection and aims to facilitate responsible and ethical medical image\nsynthesis.\n", "link": "http://arxiv.org/abs/2502.17360v1", "date": "2025-02-24", "relevancy": 2.1856, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5615}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5374}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RELICT%3A%20A%20Replica%20Detection%20Framework%20for%20Medical%20Image%20Generation&body=Title%3A%20RELICT%3A%20A%20Replica%20Detection%20Framework%20for%20Medical%20Image%20Generation%0AAuthor%3A%20Orhun%20Utku%20Aydin%20and%20Alexander%20Koch%20and%20Adam%20Hilbert%20and%20Jana%20Rieger%20and%20Felix%20Lohrke%20and%20Fujimaro%20Ishida%20and%20Satoru%20Tanioka%20and%20Dietmar%20Frey%0AAbstract%3A%20%20%20Despite%20the%20potential%20of%20synthetic%20medical%20data%20for%20augmenting%20and%20improving%0Athe%20generalizability%20of%20deep%20learning%20models%2C%20memorization%20in%20generative%20models%0Acan%20lead%20to%20unintended%20leakage%20of%20sensitive%20patient%20information%20and%20limit%20model%0Autility.%20Thus%2C%20the%20use%20of%20memorizing%20generative%20models%20in%20the%20medical%20domain%0Acan%20jeopardize%20patient%20privacy.%20We%20propose%20a%20framework%20for%20identifying%0Areplicas%2C%20i.e.%20nearly%20identical%20copies%20of%20the%20training%20data%2C%20in%20synthetic%0Amedical%20image%20datasets.%20Our%20REpLIca%20deteCTion%20%28RELICT%29%20framework%20for%20medical%0Aimage%20generative%20models%20evaluates%20image%20similarity%20using%20three%20complementary%0Aapproaches%3A%20%281%29%20voxel-level%20analysis%2C%20%282%29%20feature-level%20analysis%20by%20a%0Apretrained%20medical%20foundation%20model%2C%20and%20%283%29%20segmentation-level%20analysis.%20Two%0Aclinically%20relevant%203D%20generative%20modelling%20use%20cases%20were%20investigated%3A%0Anon-contrast%20head%20CT%20with%20intracerebral%20hemorrhage%20%28N%3D774%29%20and%20time-of-flight%0AMR%20angiography%20of%20the%20Circle%20of%20Willis%20%28N%3D1%2C782%29.%20Expert%20visual%20scoring%20was%0Aused%20as%20the%20reference%20standard%20to%20assess%20the%20presence%20of%20replicas.%20We%20report%0Athe%20balanced%20accuracy%20at%20the%20optimal%20threshold%20to%20assess%20replica%20classification%0Aperformance.%20The%20reference%20visual%20rating%20identified%2045%20of%2050%20and%205%20of%2050%0Agenerated%20images%20as%20replicas%20for%20the%20NCCT%20and%20TOF-MRA%20use%20cases%2C%20respectively.%0AImage-level%20and%20feature-level%20measures%20perfectly%20classified%20replicas%20with%20a%0Abalanced%20accuracy%20of%201%20when%20an%20optimal%20threshold%20was%20selected%20for%20the%20NCCT%20use%0Acase.%20A%20perfect%20classification%20of%20replicas%20for%20the%20TOF-MRA%20case%20was%20not%0Apossible%20at%20any%20threshold%2C%20with%20the%20segmentation-level%20analysis%20achieving%20a%0Abalanced%20accuracy%20of%200.79.%20Replica%20detection%20is%20a%20crucial%20but%20neglected%0Avalidation%20step%20for%20the%20development%20of%20generative%20models%20in%20medical%20imaging.%0AThe%20proposed%20RELICT%20framework%20provides%20a%20standardized%2C%20easy-to-use%20tool%20for%0Areplica%20detection%20and%20aims%20to%20facilitate%20responsible%20and%20ethical%20medical%20image%0Asynthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRELICT%253A%2520A%2520Replica%2520Detection%2520Framework%2520for%2520Medical%2520Image%2520Generation%26entry.906535625%3DOrhun%2520Utku%2520Aydin%2520and%2520Alexander%2520Koch%2520and%2520Adam%2520Hilbert%2520and%2520Jana%2520Rieger%2520and%2520Felix%2520Lohrke%2520and%2520Fujimaro%2520Ishida%2520and%2520Satoru%2520Tanioka%2520and%2520Dietmar%2520Frey%26entry.1292438233%3D%2520%2520Despite%2520the%2520potential%2520of%2520synthetic%2520medical%2520data%2520for%2520augmenting%2520and%2520improving%250Athe%2520generalizability%2520of%2520deep%2520learning%2520models%252C%2520memorization%2520in%2520generative%2520models%250Acan%2520lead%2520to%2520unintended%2520leakage%2520of%2520sensitive%2520patient%2520information%2520and%2520limit%2520model%250Autility.%2520Thus%252C%2520the%2520use%2520of%2520memorizing%2520generative%2520models%2520in%2520the%2520medical%2520domain%250Acan%2520jeopardize%2520patient%2520privacy.%2520We%2520propose%2520a%2520framework%2520for%2520identifying%250Areplicas%252C%2520i.e.%2520nearly%2520identical%2520copies%2520of%2520the%2520training%2520data%252C%2520in%2520synthetic%250Amedical%2520image%2520datasets.%2520Our%2520REpLIca%2520deteCTion%2520%2528RELICT%2529%2520framework%2520for%2520medical%250Aimage%2520generative%2520models%2520evaluates%2520image%2520similarity%2520using%2520three%2520complementary%250Aapproaches%253A%2520%25281%2529%2520voxel-level%2520analysis%252C%2520%25282%2529%2520feature-level%2520analysis%2520by%2520a%250Apretrained%2520medical%2520foundation%2520model%252C%2520and%2520%25283%2529%2520segmentation-level%2520analysis.%2520Two%250Aclinically%2520relevant%25203D%2520generative%2520modelling%2520use%2520cases%2520were%2520investigated%253A%250Anon-contrast%2520head%2520CT%2520with%2520intracerebral%2520hemorrhage%2520%2528N%253D774%2529%2520and%2520time-of-flight%250AMR%2520angiography%2520of%2520the%2520Circle%2520of%2520Willis%2520%2528N%253D1%252C782%2529.%2520Expert%2520visual%2520scoring%2520was%250Aused%2520as%2520the%2520reference%2520standard%2520to%2520assess%2520the%2520presence%2520of%2520replicas.%2520We%2520report%250Athe%2520balanced%2520accuracy%2520at%2520the%2520optimal%2520threshold%2520to%2520assess%2520replica%2520classification%250Aperformance.%2520The%2520reference%2520visual%2520rating%2520identified%252045%2520of%252050%2520and%25205%2520of%252050%250Agenerated%2520images%2520as%2520replicas%2520for%2520the%2520NCCT%2520and%2520TOF-MRA%2520use%2520cases%252C%2520respectively.%250AImage-level%2520and%2520feature-level%2520measures%2520perfectly%2520classified%2520replicas%2520with%2520a%250Abalanced%2520accuracy%2520of%25201%2520when%2520an%2520optimal%2520threshold%2520was%2520selected%2520for%2520the%2520NCCT%2520use%250Acase.%2520A%2520perfect%2520classification%2520of%2520replicas%2520for%2520the%2520TOF-MRA%2520case%2520was%2520not%250Apossible%2520at%2520any%2520threshold%252C%2520with%2520the%2520segmentation-level%2520analysis%2520achieving%2520a%250Abalanced%2520accuracy%2520of%25200.79.%2520Replica%2520detection%2520is%2520a%2520crucial%2520but%2520neglected%250Avalidation%2520step%2520for%2520the%2520development%2520of%2520generative%2520models%2520in%2520medical%2520imaging.%250AThe%2520proposed%2520RELICT%2520framework%2520provides%2520a%2520standardized%252C%2520easy-to-use%2520tool%2520for%250Areplica%2520detection%2520and%2520aims%2520to%2520facilitate%2520responsible%2520and%2520ethical%2520medical%2520image%250Asynthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RELICT%3A%20A%20Replica%20Detection%20Framework%20for%20Medical%20Image%20Generation&entry.906535625=Orhun%20Utku%20Aydin%20and%20Alexander%20Koch%20and%20Adam%20Hilbert%20and%20Jana%20Rieger%20and%20Felix%20Lohrke%20and%20Fujimaro%20Ishida%20and%20Satoru%20Tanioka%20and%20Dietmar%20Frey&entry.1292438233=%20%20Despite%20the%20potential%20of%20synthetic%20medical%20data%20for%20augmenting%20and%20improving%0Athe%20generalizability%20of%20deep%20learning%20models%2C%20memorization%20in%20generative%20models%0Acan%20lead%20to%20unintended%20leakage%20of%20sensitive%20patient%20information%20and%20limit%20model%0Autility.%20Thus%2C%20the%20use%20of%20memorizing%20generative%20models%20in%20the%20medical%20domain%0Acan%20jeopardize%20patient%20privacy.%20We%20propose%20a%20framework%20for%20identifying%0Areplicas%2C%20i.e.%20nearly%20identical%20copies%20of%20the%20training%20data%2C%20in%20synthetic%0Amedical%20image%20datasets.%20Our%20REpLIca%20deteCTion%20%28RELICT%29%20framework%20for%20medical%0Aimage%20generative%20models%20evaluates%20image%20similarity%20using%20three%20complementary%0Aapproaches%3A%20%281%29%20voxel-level%20analysis%2C%20%282%29%20feature-level%20analysis%20by%20a%0Apretrained%20medical%20foundation%20model%2C%20and%20%283%29%20segmentation-level%20analysis.%20Two%0Aclinically%20relevant%203D%20generative%20modelling%20use%20cases%20were%20investigated%3A%0Anon-contrast%20head%20CT%20with%20intracerebral%20hemorrhage%20%28N%3D774%29%20and%20time-of-flight%0AMR%20angiography%20of%20the%20Circle%20of%20Willis%20%28N%3D1%2C782%29.%20Expert%20visual%20scoring%20was%0Aused%20as%20the%20reference%20standard%20to%20assess%20the%20presence%20of%20replicas.%20We%20report%0Athe%20balanced%20accuracy%20at%20the%20optimal%20threshold%20to%20assess%20replica%20classification%0Aperformance.%20The%20reference%20visual%20rating%20identified%2045%20of%2050%20and%205%20of%2050%0Agenerated%20images%20as%20replicas%20for%20the%20NCCT%20and%20TOF-MRA%20use%20cases%2C%20respectively.%0AImage-level%20and%20feature-level%20measures%20perfectly%20classified%20replicas%20with%20a%0Abalanced%20accuracy%20of%201%20when%20an%20optimal%20threshold%20was%20selected%20for%20the%20NCCT%20use%0Acase.%20A%20perfect%20classification%20of%20replicas%20for%20the%20TOF-MRA%20case%20was%20not%0Apossible%20at%20any%20threshold%2C%20with%20the%20segmentation-level%20analysis%20achieving%20a%0Abalanced%20accuracy%20of%200.79.%20Replica%20detection%20is%20a%20crucial%20but%20neglected%0Avalidation%20step%20for%20the%20development%20of%20generative%20models%20in%20medical%20imaging.%0AThe%20proposed%20RELICT%20framework%20provides%20a%20standardized%2C%20easy-to-use%20tool%20for%0Areplica%20detection%20and%20aims%20to%20facilitate%20responsible%20and%20ethical%20medical%20image%0Asynthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17360v1&entry.124074799=Read"},
{"title": "A Physics-Informed Machine Learning Framework for Safe and Optimal\n  Control of Autonomous Systems", "author": "Manan Tayal and Aditya Singh and Shishir Kolathaya and Somil Bansal", "abstract": "  As autonomous systems become more ubiquitous in daily life, ensuring high\nperformance with guaranteed safety is crucial. However, safety and performance\ncould be competing objectives, which makes their co-optimization difficult.\nLearning-based methods, such as Constrained Reinforcement Learning (CRL),\nachieve strong performance but lack formal safety guarantees due to safety\nbeing enforced as soft constraints, limiting their use in safety-critical\nsettings. Conversely, formal methods such as Hamilton-Jacobi (HJ) Reachability\nAnalysis and Control Barrier Functions (CBFs) provide rigorous safety\nassurances but often neglect performance, resulting in overly conservative\ncontrollers. To bridge this gap, we formulate the co-optimization of safety and\nperformance as a state-constrained optimal control problem, where performance\nobjectives are encoded via a cost function and safety requirements are imposed\nas state constraints. We demonstrate that the resultant value function\nsatisfies a Hamilton-Jacobi-Bellman (HJB) equation, which we approximate\nefficiently using a novel physics-informed machine learning framework. In\naddition, we introduce a conformal prediction-based verification strategy to\nquantify the learning errors, recovering a high-confidence safety value\nfunction, along with a probabilistic error bound on performance degradation.\nThrough several case studies, we demonstrate the efficacy of the proposed\nframework in enabling scalable learning of safe and performant controllers for\ncomplex, high-dimensional autonomous systems.\n", "link": "http://arxiv.org/abs/2502.11057v2", "date": "2025-02-24", "relevancy": 2.1813, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5862}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5554}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Physics-Informed%20Machine%20Learning%20Framework%20for%20Safe%20and%20Optimal%0A%20%20Control%20of%20Autonomous%20Systems&body=Title%3A%20A%20Physics-Informed%20Machine%20Learning%20Framework%20for%20Safe%20and%20Optimal%0A%20%20Control%20of%20Autonomous%20Systems%0AAuthor%3A%20Manan%20Tayal%20and%20Aditya%20Singh%20and%20Shishir%20Kolathaya%20and%20Somil%20Bansal%0AAbstract%3A%20%20%20As%20autonomous%20systems%20become%20more%20ubiquitous%20in%20daily%20life%2C%20ensuring%20high%0Aperformance%20with%20guaranteed%20safety%20is%20crucial.%20However%2C%20safety%20and%20performance%0Acould%20be%20competing%20objectives%2C%20which%20makes%20their%20co-optimization%20difficult.%0ALearning-based%20methods%2C%20such%20as%20Constrained%20Reinforcement%20Learning%20%28CRL%29%2C%0Aachieve%20strong%20performance%20but%20lack%20formal%20safety%20guarantees%20due%20to%20safety%0Abeing%20enforced%20as%20soft%20constraints%2C%20limiting%20their%20use%20in%20safety-critical%0Asettings.%20Conversely%2C%20formal%20methods%20such%20as%20Hamilton-Jacobi%20%28HJ%29%20Reachability%0AAnalysis%20and%20Control%20Barrier%20Functions%20%28CBFs%29%20provide%20rigorous%20safety%0Aassurances%20but%20often%20neglect%20performance%2C%20resulting%20in%20overly%20conservative%0Acontrollers.%20To%20bridge%20this%20gap%2C%20we%20formulate%20the%20co-optimization%20of%20safety%20and%0Aperformance%20as%20a%20state-constrained%20optimal%20control%20problem%2C%20where%20performance%0Aobjectives%20are%20encoded%20via%20a%20cost%20function%20and%20safety%20requirements%20are%20imposed%0Aas%20state%20constraints.%20We%20demonstrate%20that%20the%20resultant%20value%20function%0Asatisfies%20a%20Hamilton-Jacobi-Bellman%20%28HJB%29%20equation%2C%20which%20we%20approximate%0Aefficiently%20using%20a%20novel%20physics-informed%20machine%20learning%20framework.%20In%0Aaddition%2C%20we%20introduce%20a%20conformal%20prediction-based%20verification%20strategy%20to%0Aquantify%20the%20learning%20errors%2C%20recovering%20a%20high-confidence%20safety%20value%0Afunction%2C%20along%20with%20a%20probabilistic%20error%20bound%20on%20performance%20degradation.%0AThrough%20several%20case%20studies%2C%20we%20demonstrate%20the%20efficacy%20of%20the%20proposed%0Aframework%20in%20enabling%20scalable%20learning%20of%20safe%20and%20performant%20controllers%20for%0Acomplex%2C%20high-dimensional%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11057v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Physics-Informed%2520Machine%2520Learning%2520Framework%2520for%2520Safe%2520and%2520Optimal%250A%2520%2520Control%2520of%2520Autonomous%2520Systems%26entry.906535625%3DManan%2520Tayal%2520and%2520Aditya%2520Singh%2520and%2520Shishir%2520Kolathaya%2520and%2520Somil%2520Bansal%26entry.1292438233%3D%2520%2520As%2520autonomous%2520systems%2520become%2520more%2520ubiquitous%2520in%2520daily%2520life%252C%2520ensuring%2520high%250Aperformance%2520with%2520guaranteed%2520safety%2520is%2520crucial.%2520However%252C%2520safety%2520and%2520performance%250Acould%2520be%2520competing%2520objectives%252C%2520which%2520makes%2520their%2520co-optimization%2520difficult.%250ALearning-based%2520methods%252C%2520such%2520as%2520Constrained%2520Reinforcement%2520Learning%2520%2528CRL%2529%252C%250Aachieve%2520strong%2520performance%2520but%2520lack%2520formal%2520safety%2520guarantees%2520due%2520to%2520safety%250Abeing%2520enforced%2520as%2520soft%2520constraints%252C%2520limiting%2520their%2520use%2520in%2520safety-critical%250Asettings.%2520Conversely%252C%2520formal%2520methods%2520such%2520as%2520Hamilton-Jacobi%2520%2528HJ%2529%2520Reachability%250AAnalysis%2520and%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529%2520provide%2520rigorous%2520safety%250Aassurances%2520but%2520often%2520neglect%2520performance%252C%2520resulting%2520in%2520overly%2520conservative%250Acontrollers.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520formulate%2520the%2520co-optimization%2520of%2520safety%2520and%250Aperformance%2520as%2520a%2520state-constrained%2520optimal%2520control%2520problem%252C%2520where%2520performance%250Aobjectives%2520are%2520encoded%2520via%2520a%2520cost%2520function%2520and%2520safety%2520requirements%2520are%2520imposed%250Aas%2520state%2520constraints.%2520We%2520demonstrate%2520that%2520the%2520resultant%2520value%2520function%250Asatisfies%2520a%2520Hamilton-Jacobi-Bellman%2520%2528HJB%2529%2520equation%252C%2520which%2520we%2520approximate%250Aefficiently%2520using%2520a%2520novel%2520physics-informed%2520machine%2520learning%2520framework.%2520In%250Aaddition%252C%2520we%2520introduce%2520a%2520conformal%2520prediction-based%2520verification%2520strategy%2520to%250Aquantify%2520the%2520learning%2520errors%252C%2520recovering%2520a%2520high-confidence%2520safety%2520value%250Afunction%252C%2520along%2520with%2520a%2520probabilistic%2520error%2520bound%2520on%2520performance%2520degradation.%250AThrough%2520several%2520case%2520studies%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520proposed%250Aframework%2520in%2520enabling%2520scalable%2520learning%2520of%2520safe%2520and%2520performant%2520controllers%2520for%250Acomplex%252C%2520high-dimensional%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11057v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Physics-Informed%20Machine%20Learning%20Framework%20for%20Safe%20and%20Optimal%0A%20%20Control%20of%20Autonomous%20Systems&entry.906535625=Manan%20Tayal%20and%20Aditya%20Singh%20and%20Shishir%20Kolathaya%20and%20Somil%20Bansal&entry.1292438233=%20%20As%20autonomous%20systems%20become%20more%20ubiquitous%20in%20daily%20life%2C%20ensuring%20high%0Aperformance%20with%20guaranteed%20safety%20is%20crucial.%20However%2C%20safety%20and%20performance%0Acould%20be%20competing%20objectives%2C%20which%20makes%20their%20co-optimization%20difficult.%0ALearning-based%20methods%2C%20such%20as%20Constrained%20Reinforcement%20Learning%20%28CRL%29%2C%0Aachieve%20strong%20performance%20but%20lack%20formal%20safety%20guarantees%20due%20to%20safety%0Abeing%20enforced%20as%20soft%20constraints%2C%20limiting%20their%20use%20in%20safety-critical%0Asettings.%20Conversely%2C%20formal%20methods%20such%20as%20Hamilton-Jacobi%20%28HJ%29%20Reachability%0AAnalysis%20and%20Control%20Barrier%20Functions%20%28CBFs%29%20provide%20rigorous%20safety%0Aassurances%20but%20often%20neglect%20performance%2C%20resulting%20in%20overly%20conservative%0Acontrollers.%20To%20bridge%20this%20gap%2C%20we%20formulate%20the%20co-optimization%20of%20safety%20and%0Aperformance%20as%20a%20state-constrained%20optimal%20control%20problem%2C%20where%20performance%0Aobjectives%20are%20encoded%20via%20a%20cost%20function%20and%20safety%20requirements%20are%20imposed%0Aas%20state%20constraints.%20We%20demonstrate%20that%20the%20resultant%20value%20function%0Asatisfies%20a%20Hamilton-Jacobi-Bellman%20%28HJB%29%20equation%2C%20which%20we%20approximate%0Aefficiently%20using%20a%20novel%20physics-informed%20machine%20learning%20framework.%20In%0Aaddition%2C%20we%20introduce%20a%20conformal%20prediction-based%20verification%20strategy%20to%0Aquantify%20the%20learning%20errors%2C%20recovering%20a%20high-confidence%20safety%20value%0Afunction%2C%20along%20with%20a%20probabilistic%20error%20bound%20on%20performance%20degradation.%0AThrough%20several%20case%20studies%2C%20we%20demonstrate%20the%20efficacy%20of%20the%20proposed%0Aframework%20in%20enabling%20scalable%20learning%20of%20safe%20and%20performant%20controllers%20for%0Acomplex%2C%20high-dimensional%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11057v2&entry.124074799=Read"},
{"title": "Large Language Model as a Teacher for Zero-shot Tagging at Extreme\n  Scales", "author": "Jinbin Zhang and Nasib Ullah and Rohit Babbar", "abstract": "  Extreme Multi-label Text Classification (XMC) entails selecting the most\nrelevant labels for an instance from a vast label set. Extreme Zero-shot XMC\n(EZ-XMC) extends this challenge by operating without annotated data, relying\nonly on raw text instances and a predefined label set, making it particularly\ncritical for addressing cold-start problems in large-scale recommendation and\ncategorization systems. State-of-the-art methods, such as MACLR and RTS,\nleverage lightweight bi-encoders but rely on suboptimal pseudo labels for\ntraining, such as document titles (MACLR) or document segments (RTS), which may\nnot align well with the intended tagging or categorization tasks. On the other\nhand, LLM-based approaches, like ICXML, achieve better label-instance alignment\nbut are computationally expensive and impractical for real-world EZ-XMC\napplications due to their heavy inference costs. In this paper, we introduce\nLMTX (Large language Model as Teacher for eXtreme classification), a novel\nframework that bridges the gap between these two approaches. LMTX utilizes an\nLLM to identify high-quality pseudo labels during training, while employing a\nlightweight bi-encoder for efficient inference. This design eliminates the need\nfor LLMs at inference time, offering the benefits of improved label alignment\nwithout sacrificing computational efficiency. Our approach achieves superior\nperformance and efficiency over both LLM and non-LLM based approaches,\nestablishing a new state-of-the-art in EZ-XMC.\n", "link": "http://arxiv.org/abs/2406.09288v2", "date": "2025-02-24", "relevancy": 2.1779, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20as%20a%20Teacher%20for%20Zero-shot%20Tagging%20at%20Extreme%0A%20%20Scales&body=Title%3A%20Large%20Language%20Model%20as%20a%20Teacher%20for%20Zero-shot%20Tagging%20at%20Extreme%0A%20%20Scales%0AAuthor%3A%20Jinbin%20Zhang%20and%20Nasib%20Ullah%20and%20Rohit%20Babbar%0AAbstract%3A%20%20%20Extreme%20Multi-label%20Text%20Classification%20%28XMC%29%20entails%20selecting%20the%20most%0Arelevant%20labels%20for%20an%20instance%20from%20a%20vast%20label%20set.%20Extreme%20Zero-shot%20XMC%0A%28EZ-XMC%29%20extends%20this%20challenge%20by%20operating%20without%20annotated%20data%2C%20relying%0Aonly%20on%20raw%20text%20instances%20and%20a%20predefined%20label%20set%2C%20making%20it%20particularly%0Acritical%20for%20addressing%20cold-start%20problems%20in%20large-scale%20recommendation%20and%0Acategorization%20systems.%20State-of-the-art%20methods%2C%20such%20as%20MACLR%20and%20RTS%2C%0Aleverage%20lightweight%20bi-encoders%20but%20rely%20on%20suboptimal%20pseudo%20labels%20for%0Atraining%2C%20such%20as%20document%20titles%20%28MACLR%29%20or%20document%20segments%20%28RTS%29%2C%20which%20may%0Anot%20align%20well%20with%20the%20intended%20tagging%20or%20categorization%20tasks.%20On%20the%20other%0Ahand%2C%20LLM-based%20approaches%2C%20like%20ICXML%2C%20achieve%20better%20label-instance%20alignment%0Abut%20are%20computationally%20expensive%20and%20impractical%20for%20real-world%20EZ-XMC%0Aapplications%20due%20to%20their%20heavy%20inference%20costs.%20In%20this%20paper%2C%20we%20introduce%0ALMTX%20%28Large%20language%20Model%20as%20Teacher%20for%20eXtreme%20classification%29%2C%20a%20novel%0Aframework%20that%20bridges%20the%20gap%20between%20these%20two%20approaches.%20LMTX%20utilizes%20an%0ALLM%20to%20identify%20high-quality%20pseudo%20labels%20during%20training%2C%20while%20employing%20a%0Alightweight%20bi-encoder%20for%20efficient%20inference.%20This%20design%20eliminates%20the%20need%0Afor%20LLMs%20at%20inference%20time%2C%20offering%20the%20benefits%20of%20improved%20label%20alignment%0Awithout%20sacrificing%20computational%20efficiency.%20Our%20approach%20achieves%20superior%0Aperformance%20and%20efficiency%20over%20both%20LLM%20and%20non-LLM%20based%20approaches%2C%0Aestablishing%20a%20new%20state-of-the-art%20in%20EZ-XMC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520as%2520a%2520Teacher%2520for%2520Zero-shot%2520Tagging%2520at%2520Extreme%250A%2520%2520Scales%26entry.906535625%3DJinbin%2520Zhang%2520and%2520Nasib%2520Ullah%2520and%2520Rohit%2520Babbar%26entry.1292438233%3D%2520%2520Extreme%2520Multi-label%2520Text%2520Classification%2520%2528XMC%2529%2520entails%2520selecting%2520the%2520most%250Arelevant%2520labels%2520for%2520an%2520instance%2520from%2520a%2520vast%2520label%2520set.%2520Extreme%2520Zero-shot%2520XMC%250A%2528EZ-XMC%2529%2520extends%2520this%2520challenge%2520by%2520operating%2520without%2520annotated%2520data%252C%2520relying%250Aonly%2520on%2520raw%2520text%2520instances%2520and%2520a%2520predefined%2520label%2520set%252C%2520making%2520it%2520particularly%250Acritical%2520for%2520addressing%2520cold-start%2520problems%2520in%2520large-scale%2520recommendation%2520and%250Acategorization%2520systems.%2520State-of-the-art%2520methods%252C%2520such%2520as%2520MACLR%2520and%2520RTS%252C%250Aleverage%2520lightweight%2520bi-encoders%2520but%2520rely%2520on%2520suboptimal%2520pseudo%2520labels%2520for%250Atraining%252C%2520such%2520as%2520document%2520titles%2520%2528MACLR%2529%2520or%2520document%2520segments%2520%2528RTS%2529%252C%2520which%2520may%250Anot%2520align%2520well%2520with%2520the%2520intended%2520tagging%2520or%2520categorization%2520tasks.%2520On%2520the%2520other%250Ahand%252C%2520LLM-based%2520approaches%252C%2520like%2520ICXML%252C%2520achieve%2520better%2520label-instance%2520alignment%250Abut%2520are%2520computationally%2520expensive%2520and%2520impractical%2520for%2520real-world%2520EZ-XMC%250Aapplications%2520due%2520to%2520their%2520heavy%2520inference%2520costs.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALMTX%2520%2528Large%2520language%2520Model%2520as%2520Teacher%2520for%2520eXtreme%2520classification%2529%252C%2520a%2520novel%250Aframework%2520that%2520bridges%2520the%2520gap%2520between%2520these%2520two%2520approaches.%2520LMTX%2520utilizes%2520an%250ALLM%2520to%2520identify%2520high-quality%2520pseudo%2520labels%2520during%2520training%252C%2520while%2520employing%2520a%250Alightweight%2520bi-encoder%2520for%2520efficient%2520inference.%2520This%2520design%2520eliminates%2520the%2520need%250Afor%2520LLMs%2520at%2520inference%2520time%252C%2520offering%2520the%2520benefits%2520of%2520improved%2520label%2520alignment%250Awithout%2520sacrificing%2520computational%2520efficiency.%2520Our%2520approach%2520achieves%2520superior%250Aperformance%2520and%2520efficiency%2520over%2520both%2520LLM%2520and%2520non-LLM%2520based%2520approaches%252C%250Aestablishing%2520a%2520new%2520state-of-the-art%2520in%2520EZ-XMC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20as%20a%20Teacher%20for%20Zero-shot%20Tagging%20at%20Extreme%0A%20%20Scales&entry.906535625=Jinbin%20Zhang%20and%20Nasib%20Ullah%20and%20Rohit%20Babbar&entry.1292438233=%20%20Extreme%20Multi-label%20Text%20Classification%20%28XMC%29%20entails%20selecting%20the%20most%0Arelevant%20labels%20for%20an%20instance%20from%20a%20vast%20label%20set.%20Extreme%20Zero-shot%20XMC%0A%28EZ-XMC%29%20extends%20this%20challenge%20by%20operating%20without%20annotated%20data%2C%20relying%0Aonly%20on%20raw%20text%20instances%20and%20a%20predefined%20label%20set%2C%20making%20it%20particularly%0Acritical%20for%20addressing%20cold-start%20problems%20in%20large-scale%20recommendation%20and%0Acategorization%20systems.%20State-of-the-art%20methods%2C%20such%20as%20MACLR%20and%20RTS%2C%0Aleverage%20lightweight%20bi-encoders%20but%20rely%20on%20suboptimal%20pseudo%20labels%20for%0Atraining%2C%20such%20as%20document%20titles%20%28MACLR%29%20or%20document%20segments%20%28RTS%29%2C%20which%20may%0Anot%20align%20well%20with%20the%20intended%20tagging%20or%20categorization%20tasks.%20On%20the%20other%0Ahand%2C%20LLM-based%20approaches%2C%20like%20ICXML%2C%20achieve%20better%20label-instance%20alignment%0Abut%20are%20computationally%20expensive%20and%20impractical%20for%20real-world%20EZ-XMC%0Aapplications%20due%20to%20their%20heavy%20inference%20costs.%20In%20this%20paper%2C%20we%20introduce%0ALMTX%20%28Large%20language%20Model%20as%20Teacher%20for%20eXtreme%20classification%29%2C%20a%20novel%0Aframework%20that%20bridges%20the%20gap%20between%20these%20two%20approaches.%20LMTX%20utilizes%20an%0ALLM%20to%20identify%20high-quality%20pseudo%20labels%20during%20training%2C%20while%20employing%20a%0Alightweight%20bi-encoder%20for%20efficient%20inference.%20This%20design%20eliminates%20the%20need%0Afor%20LLMs%20at%20inference%20time%2C%20offering%20the%20benefits%20of%20improved%20label%20alignment%0Awithout%20sacrificing%20computational%20efficiency.%20Our%20approach%20achieves%20superior%0Aperformance%20and%20efficiency%20over%20both%20LLM%20and%20non-LLM%20based%20approaches%2C%0Aestablishing%20a%20new%20state-of-the-art%20in%20EZ-XMC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09288v2&entry.124074799=Read"},
{"title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of\n  Multimodal Large Language Models", "author": "Jiahao Huo and Yibo Yan and Xu Zheng and Yuanhuiyi Lyu and Xin Zou and Zhihua Wei and Xuming Hu", "abstract": "  Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient descent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2502.11051v2", "date": "2025-02-24", "relevancy": 2.1758, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMUnlearner%3A%20Reformulating%20Multimodal%20Machine%20Unlearning%20in%20the%20Era%20of%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20MMUnlearner%3A%20Reformulating%20Multimodal%20Machine%20Unlearning%20in%20the%20Era%20of%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Jiahao%20Huo%20and%20Yibo%20Yan%20and%20Xu%20Zheng%20and%20Yuanhuiyi%20Lyu%20and%20Xin%20Zou%20and%20Zhihua%20Wei%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Recent%20progress%20in%20Machine%20Unlearning%20%28MU%29%20has%20introduced%20solutions%20for%20the%0Aselective%20removal%20of%20private%20or%20sensitive%20information%20encoded%20within%20deep%0Aneural%20networks.%20Nonetheless%2C%20MU%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Aremains%20in%20its%20nascent%20phase.%20Therefore%2C%20we%20propose%20to%20reformulate%20the%20task%20of%0Amultimodal%20MU%20in%20the%20era%20of%20MLLMs%2C%20which%20aims%20to%20erase%20only%20the%20visual%20patterns%0Aassociated%20with%20a%20given%20entity%20while%20preserving%20the%20corresponding%20textual%0Aknowledge%20encoded%20within%20the%20original%20parameters%20of%20the%20language%20model%0Abackbone.%20Furthermore%2C%20we%20develop%20a%20novel%20geometry-constrained%20gradient%20descent%0Amethod%20MMUnlearner.%20It%20updates%20the%20weights%20of%20MLLMs%20with%20a%20weight%20saliency%20map%0Ajointly%20restricted%20by%20the%20remaining%20concepts%20and%20textual%20knowledge%20during%0Aunlearning%2C%20thereby%20preserving%20parameters%20essential%20for%20non-target%20knowledge.%0AExtensive%20experiments%20demonstrate%20that%20MMUnlearner%20surpasses%20baselines%20that%0Afinetuning%20MLLMs%20with%20VQA%20data%20directly%20through%20Gradient%20Ascent%20%28GA%29%20or%0ANegative%20Preference%20Optimization%20%28NPO%29%2C%20across%20all%20evaluation%20dimensions.%20Our%0Acode%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11051v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMUnlearner%253A%2520Reformulating%2520Multimodal%2520Machine%2520Unlearning%2520in%2520the%2520Era%2520of%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DJiahao%2520Huo%2520and%2520Yibo%2520Yan%2520and%2520Xu%2520Zheng%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Xin%2520Zou%2520and%2520Zhihua%2520Wei%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Machine%2520Unlearning%2520%2528MU%2529%2520has%2520introduced%2520solutions%2520for%2520the%250Aselective%2520removal%2520of%2520private%2520or%2520sensitive%2520information%2520encoded%2520within%2520deep%250Aneural%2520networks.%2520Nonetheless%252C%2520MU%2520for%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%250Aremains%2520in%2520its%2520nascent%2520phase.%2520Therefore%252C%2520we%2520propose%2520to%2520reformulate%2520the%2520task%2520of%250Amultimodal%2520MU%2520in%2520the%2520era%2520of%2520MLLMs%252C%2520which%2520aims%2520to%2520erase%2520only%2520the%2520visual%2520patterns%250Aassociated%2520with%2520a%2520given%2520entity%2520while%2520preserving%2520the%2520corresponding%2520textual%250Aknowledge%2520encoded%2520within%2520the%2520original%2520parameters%2520of%2520the%2520language%2520model%250Abackbone.%2520Furthermore%252C%2520we%2520develop%2520a%2520novel%2520geometry-constrained%2520gradient%2520descent%250Amethod%2520MMUnlearner.%2520It%2520updates%2520the%2520weights%2520of%2520MLLMs%2520with%2520a%2520weight%2520saliency%2520map%250Ajointly%2520restricted%2520by%2520the%2520remaining%2520concepts%2520and%2520textual%2520knowledge%2520during%250Aunlearning%252C%2520thereby%2520preserving%2520parameters%2520essential%2520for%2520non-target%2520knowledge.%250AExtensive%2520experiments%2520demonstrate%2520that%2520MMUnlearner%2520surpasses%2520baselines%2520that%250Afinetuning%2520MLLMs%2520with%2520VQA%2520data%2520directly%2520through%2520Gradient%2520Ascent%2520%2528GA%2529%2520or%250ANegative%2520Preference%2520Optimization%2520%2528NPO%2529%252C%2520across%2520all%2520evaluation%2520dimensions.%2520Our%250Acode%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11051v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMUnlearner%3A%20Reformulating%20Multimodal%20Machine%20Unlearning%20in%20the%20Era%20of%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Jiahao%20Huo%20and%20Yibo%20Yan%20and%20Xu%20Zheng%20and%20Yuanhuiyi%20Lyu%20and%20Xin%20Zou%20and%20Zhihua%20Wei%20and%20Xuming%20Hu&entry.1292438233=%20%20Recent%20progress%20in%20Machine%20Unlearning%20%28MU%29%20has%20introduced%20solutions%20for%20the%0Aselective%20removal%20of%20private%20or%20sensitive%20information%20encoded%20within%20deep%0Aneural%20networks.%20Nonetheless%2C%20MU%20for%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%0Aremains%20in%20its%20nascent%20phase.%20Therefore%2C%20we%20propose%20to%20reformulate%20the%20task%20of%0Amultimodal%20MU%20in%20the%20era%20of%20MLLMs%2C%20which%20aims%20to%20erase%20only%20the%20visual%20patterns%0Aassociated%20with%20a%20given%20entity%20while%20preserving%20the%20corresponding%20textual%0Aknowledge%20encoded%20within%20the%20original%20parameters%20of%20the%20language%20model%0Abackbone.%20Furthermore%2C%20we%20develop%20a%20novel%20geometry-constrained%20gradient%20descent%0Amethod%20MMUnlearner.%20It%20updates%20the%20weights%20of%20MLLMs%20with%20a%20weight%20saliency%20map%0Ajointly%20restricted%20by%20the%20remaining%20concepts%20and%20textual%20knowledge%20during%0Aunlearning%2C%20thereby%20preserving%20parameters%20essential%20for%20non-target%20knowledge.%0AExtensive%20experiments%20demonstrate%20that%20MMUnlearner%20surpasses%20baselines%20that%0Afinetuning%20MLLMs%20with%20VQA%20data%20directly%20through%20Gradient%20Ascent%20%28GA%29%20or%0ANegative%20Preference%20Optimization%20%28NPO%29%2C%20across%20all%20evaluation%20dimensions.%20Our%0Acode%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11051v2&entry.124074799=Read"},
{"title": "ContextualStory: Consistent Visual Storytelling with Spatially-Enhanced\n  and Storyline Context", "author": "Sixiao Zheng and Yanwei Fu", "abstract": "  Visual storytelling involves generating a sequence of coherent frames from a\ntextual storyline while maintaining consistency in characters and scenes.\nExisting autoregressive methods, which rely on previous frame-sentence pairs,\nstruggle with high memory usage, slow generation speeds, and limited context\nintegration. To address these issues, we propose ContextualStory, a novel\nframework designed to generate coherent story frames and extend frames for\nvisual storytelling. ContextualStory utilizes Spatially-Enhanced Temporal\nAttention to capture spatial and temporal dependencies, handling significant\ncharacter movements effectively. Additionally, we introduce a Storyline\nContextualizer to enrich context in storyline embedding, and a StoryFlow\nAdapter to measure scene changes between frames for guiding the model.\nExtensive experiments on PororoSV and FlintstonesSV datasets demonstrate that\nContextualStory significantly outperforms existing SOTA methods in both story\nvisualization and continuation. Code is available at\nhttps://github.com/sixiaozheng/ContextualStory.\n", "link": "http://arxiv.org/abs/2407.09774v3", "date": "2025-02-24", "relevancy": 2.1727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextualStory%3A%20Consistent%20Visual%20Storytelling%20with%20Spatially-Enhanced%0A%20%20and%20Storyline%20Context&body=Title%3A%20ContextualStory%3A%20Consistent%20Visual%20Storytelling%20with%20Spatially-Enhanced%0A%20%20and%20Storyline%20Context%0AAuthor%3A%20Sixiao%20Zheng%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Visual%20storytelling%20involves%20generating%20a%20sequence%20of%20coherent%20frames%20from%20a%0Atextual%20storyline%20while%20maintaining%20consistency%20in%20characters%20and%20scenes.%0AExisting%20autoregressive%20methods%2C%20which%20rely%20on%20previous%20frame-sentence%20pairs%2C%0Astruggle%20with%20high%20memory%20usage%2C%20slow%20generation%20speeds%2C%20and%20limited%20context%0Aintegration.%20To%20address%20these%20issues%2C%20we%20propose%20ContextualStory%2C%20a%20novel%0Aframework%20designed%20to%20generate%20coherent%20story%20frames%20and%20extend%20frames%20for%0Avisual%20storytelling.%20ContextualStory%20utilizes%20Spatially-Enhanced%20Temporal%0AAttention%20to%20capture%20spatial%20and%20temporal%20dependencies%2C%20handling%20significant%0Acharacter%20movements%20effectively.%20Additionally%2C%20we%20introduce%20a%20Storyline%0AContextualizer%20to%20enrich%20context%20in%20storyline%20embedding%2C%20and%20a%20StoryFlow%0AAdapter%20to%20measure%20scene%20changes%20between%20frames%20for%20guiding%20the%20model.%0AExtensive%20experiments%20on%20PororoSV%20and%20FlintstonesSV%20datasets%20demonstrate%20that%0AContextualStory%20significantly%20outperforms%20existing%20SOTA%20methods%20in%20both%20story%0Avisualization%20and%20continuation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sixiaozheng/ContextualStory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09774v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextualStory%253A%2520Consistent%2520Visual%2520Storytelling%2520with%2520Spatially-Enhanced%250A%2520%2520and%2520Storyline%2520Context%26entry.906535625%3DSixiao%2520Zheng%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Visual%2520storytelling%2520involves%2520generating%2520a%2520sequence%2520of%2520coherent%2520frames%2520from%2520a%250Atextual%2520storyline%2520while%2520maintaining%2520consistency%2520in%2520characters%2520and%2520scenes.%250AExisting%2520autoregressive%2520methods%252C%2520which%2520rely%2520on%2520previous%2520frame-sentence%2520pairs%252C%250Astruggle%2520with%2520high%2520memory%2520usage%252C%2520slow%2520generation%2520speeds%252C%2520and%2520limited%2520context%250Aintegration.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520ContextualStory%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520generate%2520coherent%2520story%2520frames%2520and%2520extend%2520frames%2520for%250Avisual%2520storytelling.%2520ContextualStory%2520utilizes%2520Spatially-Enhanced%2520Temporal%250AAttention%2520to%2520capture%2520spatial%2520and%2520temporal%2520dependencies%252C%2520handling%2520significant%250Acharacter%2520movements%2520effectively.%2520Additionally%252C%2520we%2520introduce%2520a%2520Storyline%250AContextualizer%2520to%2520enrich%2520context%2520in%2520storyline%2520embedding%252C%2520and%2520a%2520StoryFlow%250AAdapter%2520to%2520measure%2520scene%2520changes%2520between%2520frames%2520for%2520guiding%2520the%2520model.%250AExtensive%2520experiments%2520on%2520PororoSV%2520and%2520FlintstonesSV%2520datasets%2520demonstrate%2520that%250AContextualStory%2520significantly%2520outperforms%2520existing%2520SOTA%2520methods%2520in%2520both%2520story%250Avisualization%2520and%2520continuation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/sixiaozheng/ContextualStory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09774v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextualStory%3A%20Consistent%20Visual%20Storytelling%20with%20Spatially-Enhanced%0A%20%20and%20Storyline%20Context&entry.906535625=Sixiao%20Zheng%20and%20Yanwei%20Fu&entry.1292438233=%20%20Visual%20storytelling%20involves%20generating%20a%20sequence%20of%20coherent%20frames%20from%20a%0Atextual%20storyline%20while%20maintaining%20consistency%20in%20characters%20and%20scenes.%0AExisting%20autoregressive%20methods%2C%20which%20rely%20on%20previous%20frame-sentence%20pairs%2C%0Astruggle%20with%20high%20memory%20usage%2C%20slow%20generation%20speeds%2C%20and%20limited%20context%0Aintegration.%20To%20address%20these%20issues%2C%20we%20propose%20ContextualStory%2C%20a%20novel%0Aframework%20designed%20to%20generate%20coherent%20story%20frames%20and%20extend%20frames%20for%0Avisual%20storytelling.%20ContextualStory%20utilizes%20Spatially-Enhanced%20Temporal%0AAttention%20to%20capture%20spatial%20and%20temporal%20dependencies%2C%20handling%20significant%0Acharacter%20movements%20effectively.%20Additionally%2C%20we%20introduce%20a%20Storyline%0AContextualizer%20to%20enrich%20context%20in%20storyline%20embedding%2C%20and%20a%20StoryFlow%0AAdapter%20to%20measure%20scene%20changes%20between%20frames%20for%20guiding%20the%20model.%0AExtensive%20experiments%20on%20PororoSV%20and%20FlintstonesSV%20datasets%20demonstrate%20that%0AContextualStory%20significantly%20outperforms%20existing%20SOTA%20methods%20in%20both%20story%0Avisualization%20and%20continuation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/sixiaozheng/ContextualStory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09774v3&entry.124074799=Read"},
{"title": "From System 1 to System 2: A Survey of Reasoning Large Language Models", "author": "Zhong-Zhi Li and Duzhen Zhang and Ming-Liang Zhang and Jiaxin Zhang and Zengyan Liu and Yuxuan Yao and Haotian Xu and Junhao Zheng and Pei-Jie Wang and Xiuyi Chen and Yingying Zhang and Fei Yin and Jiahua Dong and Zhijiang Guo and Le Song and Cheng-Lin Liu", "abstract": "  Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.\n", "link": "http://arxiv.org/abs/2502.17419v1", "date": "2025-02-24", "relevancy": 2.1563, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5498}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20System%201%20to%20System%202%3A%20A%20Survey%20of%20Reasoning%20Large%20Language%20Models&body=Title%3A%20From%20System%201%20to%20System%202%3A%20A%20Survey%20of%20Reasoning%20Large%20Language%20Models%0AAuthor%3A%20Zhong-Zhi%20Li%20and%20Duzhen%20Zhang%20and%20Ming-Liang%20Zhang%20and%20Jiaxin%20Zhang%20and%20Zengyan%20Liu%20and%20Yuxuan%20Yao%20and%20Haotian%20Xu%20and%20Junhao%20Zheng%20and%20Pei-Jie%20Wang%20and%20Xiuyi%20Chen%20and%20Yingying%20Zhang%20and%20Fei%20Yin%20and%20Jiahua%20Dong%20and%20Zhijiang%20Guo%20and%20Le%20Song%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Achieving%20human-level%20intelligence%20requires%20refining%20the%20transition%20from%20the%0Afast%2C%20intuitive%20System%201%20to%20the%20slower%2C%20more%20deliberate%20System%202%20reasoning.%0AWhile%20System%201%20excels%20in%20quick%2C%20heuristic%20decisions%2C%20System%202%20relies%20on%20logical%0Areasoning%20for%20more%20accurate%20judgments%20and%20reduced%20biases.%20Foundational%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20at%20fast%20decision-making%20but%20lack%20the%20depth%20for%0Acomplex%20reasoning%2C%20as%20they%20have%20not%20yet%20fully%20embraced%20the%20step-by-step%0Aanalysis%20characteristic%20of%20true%20System%202%20thinking.%20Recently%2C%20reasoning%20LLMs%0Alike%20OpenAI%27s%20o1/o3%20and%20DeepSeek%27s%20R1%20have%20demonstrated%20expert-level%0Aperformance%20in%20fields%20such%20as%20mathematics%20and%20coding%2C%20closely%20mimicking%20the%0Adeliberate%20reasoning%20of%20System%202%20and%20showcasing%20human-like%20cognitive%20abilities.%0AThis%20survey%20begins%20with%20a%20brief%20overview%20of%20the%20progress%20in%20foundational%20LLMs%0Aand%20the%20early%20development%20of%20System%202%20technologies%2C%20exploring%20how%20their%0Acombination%20has%20paved%20the%20way%20for%20reasoning%20LLMs.%20Next%2C%20we%20discuss%20how%20to%0Aconstruct%20reasoning%20LLMs%2C%20analyzing%20their%20features%2C%20the%20core%20methods%20enabling%0Aadvanced%20reasoning%2C%20and%20the%20evolution%20of%20various%20reasoning%20LLMs.%20Additionally%2C%0Awe%20provide%20an%20overview%20of%20reasoning%20benchmarks%2C%20offering%20an%20in-depth%20comparison%0Aof%20the%20performance%20of%20representative%20reasoning%20LLMs.%20Finally%2C%20we%20explore%0Apromising%20directions%20for%20advancing%20reasoning%20LLMs%20and%20maintain%20a%20real-time%0A%5Chref%7Bhttps%3A//github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub%0ARepository%7D%20to%20track%20the%20latest%20developments.%20We%20hope%20this%20survey%20will%20serve%20as%0Aa%20valuable%20resource%20to%20inspire%20innovation%20and%20drive%20progress%20in%20this%20rapidly%0Aevolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520System%25201%2520to%2520System%25202%253A%2520A%2520Survey%2520of%2520Reasoning%2520Large%2520Language%2520Models%26entry.906535625%3DZhong-Zhi%2520Li%2520and%2520Duzhen%2520Zhang%2520and%2520Ming-Liang%2520Zhang%2520and%2520Jiaxin%2520Zhang%2520and%2520Zengyan%2520Liu%2520and%2520Yuxuan%2520Yao%2520and%2520Haotian%2520Xu%2520and%2520Junhao%2520Zheng%2520and%2520Pei-Jie%2520Wang%2520and%2520Xiuyi%2520Chen%2520and%2520Yingying%2520Zhang%2520and%2520Fei%2520Yin%2520and%2520Jiahua%2520Dong%2520and%2520Zhijiang%2520Guo%2520and%2520Le%2520Song%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Achieving%2520human-level%2520intelligence%2520requires%2520refining%2520the%2520transition%2520from%2520the%250Afast%252C%2520intuitive%2520System%25201%2520to%2520the%2520slower%252C%2520more%2520deliberate%2520System%25202%2520reasoning.%250AWhile%2520System%25201%2520excels%2520in%2520quick%252C%2520heuristic%2520decisions%252C%2520System%25202%2520relies%2520on%2520logical%250Areasoning%2520for%2520more%2520accurate%2520judgments%2520and%2520reduced%2520biases.%2520Foundational%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520fast%2520decision-making%2520but%2520lack%2520the%2520depth%2520for%250Acomplex%2520reasoning%252C%2520as%2520they%2520have%2520not%2520yet%2520fully%2520embraced%2520the%2520step-by-step%250Aanalysis%2520characteristic%2520of%2520true%2520System%25202%2520thinking.%2520Recently%252C%2520reasoning%2520LLMs%250Alike%2520OpenAI%2527s%2520o1/o3%2520and%2520DeepSeek%2527s%2520R1%2520have%2520demonstrated%2520expert-level%250Aperformance%2520in%2520fields%2520such%2520as%2520mathematics%2520and%2520coding%252C%2520closely%2520mimicking%2520the%250Adeliberate%2520reasoning%2520of%2520System%25202%2520and%2520showcasing%2520human-like%2520cognitive%2520abilities.%250AThis%2520survey%2520begins%2520with%2520a%2520brief%2520overview%2520of%2520the%2520progress%2520in%2520foundational%2520LLMs%250Aand%2520the%2520early%2520development%2520of%2520System%25202%2520technologies%252C%2520exploring%2520how%2520their%250Acombination%2520has%2520paved%2520the%2520way%2520for%2520reasoning%2520LLMs.%2520Next%252C%2520we%2520discuss%2520how%2520to%250Aconstruct%2520reasoning%2520LLMs%252C%2520analyzing%2520their%2520features%252C%2520the%2520core%2520methods%2520enabling%250Aadvanced%2520reasoning%252C%2520and%2520the%2520evolution%2520of%2520various%2520reasoning%2520LLMs.%2520Additionally%252C%250Awe%2520provide%2520an%2520overview%2520of%2520reasoning%2520benchmarks%252C%2520offering%2520an%2520in-depth%2520comparison%250Aof%2520the%2520performance%2520of%2520representative%2520reasoning%2520LLMs.%2520Finally%252C%2520we%2520explore%250Apromising%2520directions%2520for%2520advancing%2520reasoning%2520LLMs%2520and%2520maintain%2520a%2520real-time%250A%255Chref%257Bhttps%253A//github.com/zzli2022/Awesome-Slow-Reason-System%257D%257BGitHub%250ARepository%257D%2520to%2520track%2520the%2520latest%2520developments.%2520We%2520hope%2520this%2520survey%2520will%2520serve%2520as%250Aa%2520valuable%2520resource%2520to%2520inspire%2520innovation%2520and%2520drive%2520progress%2520in%2520this%2520rapidly%250Aevolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20System%201%20to%20System%202%3A%20A%20Survey%20of%20Reasoning%20Large%20Language%20Models&entry.906535625=Zhong-Zhi%20Li%20and%20Duzhen%20Zhang%20and%20Ming-Liang%20Zhang%20and%20Jiaxin%20Zhang%20and%20Zengyan%20Liu%20and%20Yuxuan%20Yao%20and%20Haotian%20Xu%20and%20Junhao%20Zheng%20and%20Pei-Jie%20Wang%20and%20Xiuyi%20Chen%20and%20Yingying%20Zhang%20and%20Fei%20Yin%20and%20Jiahua%20Dong%20and%20Zhijiang%20Guo%20and%20Le%20Song%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Achieving%20human-level%20intelligence%20requires%20refining%20the%20transition%20from%20the%0Afast%2C%20intuitive%20System%201%20to%20the%20slower%2C%20more%20deliberate%20System%202%20reasoning.%0AWhile%20System%201%20excels%20in%20quick%2C%20heuristic%20decisions%2C%20System%202%20relies%20on%20logical%0Areasoning%20for%20more%20accurate%20judgments%20and%20reduced%20biases.%20Foundational%20Large%0ALanguage%20Models%20%28LLMs%29%20excel%20at%20fast%20decision-making%20but%20lack%20the%20depth%20for%0Acomplex%20reasoning%2C%20as%20they%20have%20not%20yet%20fully%20embraced%20the%20step-by-step%0Aanalysis%20characteristic%20of%20true%20System%202%20thinking.%20Recently%2C%20reasoning%20LLMs%0Alike%20OpenAI%27s%20o1/o3%20and%20DeepSeek%27s%20R1%20have%20demonstrated%20expert-level%0Aperformance%20in%20fields%20such%20as%20mathematics%20and%20coding%2C%20closely%20mimicking%20the%0Adeliberate%20reasoning%20of%20System%202%20and%20showcasing%20human-like%20cognitive%20abilities.%0AThis%20survey%20begins%20with%20a%20brief%20overview%20of%20the%20progress%20in%20foundational%20LLMs%0Aand%20the%20early%20development%20of%20System%202%20technologies%2C%20exploring%20how%20their%0Acombination%20has%20paved%20the%20way%20for%20reasoning%20LLMs.%20Next%2C%20we%20discuss%20how%20to%0Aconstruct%20reasoning%20LLMs%2C%20analyzing%20their%20features%2C%20the%20core%20methods%20enabling%0Aadvanced%20reasoning%2C%20and%20the%20evolution%20of%20various%20reasoning%20LLMs.%20Additionally%2C%0Awe%20provide%20an%20overview%20of%20reasoning%20benchmarks%2C%20offering%20an%20in-depth%20comparison%0Aof%20the%20performance%20of%20representative%20reasoning%20LLMs.%20Finally%2C%20we%20explore%0Apromising%20directions%20for%20advancing%20reasoning%20LLMs%20and%20maintain%20a%20real-time%0A%5Chref%7Bhttps%3A//github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub%0ARepository%7D%20to%20track%20the%20latest%20developments.%20We%20hope%20this%20survey%20will%20serve%20as%0Aa%20valuable%20resource%20to%20inspire%20innovation%20and%20drive%20progress%20in%20this%20rapidly%0Aevolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17419v1&entry.124074799=Read"},
{"title": "VR-Pipe: Streamlining Hardware Graphics Pipeline for Volume Rendering", "author": "Junseo Lee and Jaisung Kim and Junyong Park and Jaewoong Sim", "abstract": "  Graphics rendering that builds on machine learning and radiance fields is\ngaining significant attention due to its outstanding quality and speed in\ngenerating photorealistic images from novel viewpoints. However, prior work has\nprimarily focused on evaluating its performance through software-based\nrendering on programmable shader cores, leaving its performance when exploiting\nfixed-function graphics units largely unexplored.\n  In this paper, we investigate the performance implications of performing\nradiance field rendering on the hardware graphics pipeline. In doing so, we\nimplement the state-of-the-art radiance field method, 3D Gaussian splatting,\nusing graphics APIs and evaluate it across synthetic and real-world scenes on\ntoday's graphics hardware. Based on our analysis, we present VR-Pipe, which\nseamlessly integrates two innovations into graphics hardware to streamline the\nhardware pipeline for volume rendering, such as radiance field methods. First,\nwe introduce native hardware support for early termination by repurposing\nexisting special-purpose hardware in modern GPUs. Second, we propose\nmulti-granular tile binning with quad merging, which opportunistically blends\nfragments in shader cores before passing them to fixed-function blending units.\nOur evaluation shows that VR-Pipe greatly improves rendering performance,\nachieving up to a 2.78x speedup over the conventional graphics pipeline with\nnegligible hardware overhead.\n", "link": "http://arxiv.org/abs/2502.17078v1", "date": "2025-02-24", "relevancy": 2.1478, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5805}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5148}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VR-Pipe%3A%20Streamlining%20Hardware%20Graphics%20Pipeline%20for%20Volume%20Rendering&body=Title%3A%20VR-Pipe%3A%20Streamlining%20Hardware%20Graphics%20Pipeline%20for%20Volume%20Rendering%0AAuthor%3A%20Junseo%20Lee%20and%20Jaisung%20Kim%20and%20Junyong%20Park%20and%20Jaewoong%20Sim%0AAbstract%3A%20%20%20Graphics%20rendering%20that%20builds%20on%20machine%20learning%20and%20radiance%20fields%20is%0Againing%20significant%20attention%20due%20to%20its%20outstanding%20quality%20and%20speed%20in%0Agenerating%20photorealistic%20images%20from%20novel%20viewpoints.%20However%2C%20prior%20work%20has%0Aprimarily%20focused%20on%20evaluating%20its%20performance%20through%20software-based%0Arendering%20on%20programmable%20shader%20cores%2C%20leaving%20its%20performance%20when%20exploiting%0Afixed-function%20graphics%20units%20largely%20unexplored.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20performance%20implications%20of%20performing%0Aradiance%20field%20rendering%20on%20the%20hardware%20graphics%20pipeline.%20In%20doing%20so%2C%20we%0Aimplement%20the%20state-of-the-art%20radiance%20field%20method%2C%203D%20Gaussian%20splatting%2C%0Ausing%20graphics%20APIs%20and%20evaluate%20it%20across%20synthetic%20and%20real-world%20scenes%20on%0Atoday%27s%20graphics%20hardware.%20Based%20on%20our%20analysis%2C%20we%20present%20VR-Pipe%2C%20which%0Aseamlessly%20integrates%20two%20innovations%20into%20graphics%20hardware%20to%20streamline%20the%0Ahardware%20pipeline%20for%20volume%20rendering%2C%20such%20as%20radiance%20field%20methods.%20First%2C%0Awe%20introduce%20native%20hardware%20support%20for%20early%20termination%20by%20repurposing%0Aexisting%20special-purpose%20hardware%20in%20modern%20GPUs.%20Second%2C%20we%20propose%0Amulti-granular%20tile%20binning%20with%20quad%20merging%2C%20which%20opportunistically%20blends%0Afragments%20in%20shader%20cores%20before%20passing%20them%20to%20fixed-function%20blending%20units.%0AOur%20evaluation%20shows%20that%20VR-Pipe%20greatly%20improves%20rendering%20performance%2C%0Aachieving%20up%20to%20a%202.78x%20speedup%20over%20the%20conventional%20graphics%20pipeline%20with%0Anegligible%20hardware%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVR-Pipe%253A%2520Streamlining%2520Hardware%2520Graphics%2520Pipeline%2520for%2520Volume%2520Rendering%26entry.906535625%3DJunseo%2520Lee%2520and%2520Jaisung%2520Kim%2520and%2520Junyong%2520Park%2520and%2520Jaewoong%2520Sim%26entry.1292438233%3D%2520%2520Graphics%2520rendering%2520that%2520builds%2520on%2520machine%2520learning%2520and%2520radiance%2520fields%2520is%250Againing%2520significant%2520attention%2520due%2520to%2520its%2520outstanding%2520quality%2520and%2520speed%2520in%250Agenerating%2520photorealistic%2520images%2520from%2520novel%2520viewpoints.%2520However%252C%2520prior%2520work%2520has%250Aprimarily%2520focused%2520on%2520evaluating%2520its%2520performance%2520through%2520software-based%250Arendering%2520on%2520programmable%2520shader%2520cores%252C%2520leaving%2520its%2520performance%2520when%2520exploiting%250Afixed-function%2520graphics%2520units%2520largely%2520unexplored.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520performance%2520implications%2520of%2520performing%250Aradiance%2520field%2520rendering%2520on%2520the%2520hardware%2520graphics%2520pipeline.%2520In%2520doing%2520so%252C%2520we%250Aimplement%2520the%2520state-of-the-art%2520radiance%2520field%2520method%252C%25203D%2520Gaussian%2520splatting%252C%250Ausing%2520graphics%2520APIs%2520and%2520evaluate%2520it%2520across%2520synthetic%2520and%2520real-world%2520scenes%2520on%250Atoday%2527s%2520graphics%2520hardware.%2520Based%2520on%2520our%2520analysis%252C%2520we%2520present%2520VR-Pipe%252C%2520which%250Aseamlessly%2520integrates%2520two%2520innovations%2520into%2520graphics%2520hardware%2520to%2520streamline%2520the%250Ahardware%2520pipeline%2520for%2520volume%2520rendering%252C%2520such%2520as%2520radiance%2520field%2520methods.%2520First%252C%250Awe%2520introduce%2520native%2520hardware%2520support%2520for%2520early%2520termination%2520by%2520repurposing%250Aexisting%2520special-purpose%2520hardware%2520in%2520modern%2520GPUs.%2520Second%252C%2520we%2520propose%250Amulti-granular%2520tile%2520binning%2520with%2520quad%2520merging%252C%2520which%2520opportunistically%2520blends%250Afragments%2520in%2520shader%2520cores%2520before%2520passing%2520them%2520to%2520fixed-function%2520blending%2520units.%250AOur%2520evaluation%2520shows%2520that%2520VR-Pipe%2520greatly%2520improves%2520rendering%2520performance%252C%250Aachieving%2520up%2520to%2520a%25202.78x%2520speedup%2520over%2520the%2520conventional%2520graphics%2520pipeline%2520with%250Anegligible%2520hardware%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VR-Pipe%3A%20Streamlining%20Hardware%20Graphics%20Pipeline%20for%20Volume%20Rendering&entry.906535625=Junseo%20Lee%20and%20Jaisung%20Kim%20and%20Junyong%20Park%20and%20Jaewoong%20Sim&entry.1292438233=%20%20Graphics%20rendering%20that%20builds%20on%20machine%20learning%20and%20radiance%20fields%20is%0Againing%20significant%20attention%20due%20to%20its%20outstanding%20quality%20and%20speed%20in%0Agenerating%20photorealistic%20images%20from%20novel%20viewpoints.%20However%2C%20prior%20work%20has%0Aprimarily%20focused%20on%20evaluating%20its%20performance%20through%20software-based%0Arendering%20on%20programmable%20shader%20cores%2C%20leaving%20its%20performance%20when%20exploiting%0Afixed-function%20graphics%20units%20largely%20unexplored.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20performance%20implications%20of%20performing%0Aradiance%20field%20rendering%20on%20the%20hardware%20graphics%20pipeline.%20In%20doing%20so%2C%20we%0Aimplement%20the%20state-of-the-art%20radiance%20field%20method%2C%203D%20Gaussian%20splatting%2C%0Ausing%20graphics%20APIs%20and%20evaluate%20it%20across%20synthetic%20and%20real-world%20scenes%20on%0Atoday%27s%20graphics%20hardware.%20Based%20on%20our%20analysis%2C%20we%20present%20VR-Pipe%2C%20which%0Aseamlessly%20integrates%20two%20innovations%20into%20graphics%20hardware%20to%20streamline%20the%0Ahardware%20pipeline%20for%20volume%20rendering%2C%20such%20as%20radiance%20field%20methods.%20First%2C%0Awe%20introduce%20native%20hardware%20support%20for%20early%20termination%20by%20repurposing%0Aexisting%20special-purpose%20hardware%20in%20modern%20GPUs.%20Second%2C%20we%20propose%0Amulti-granular%20tile%20binning%20with%20quad%20merging%2C%20which%20opportunistically%20blends%0Afragments%20in%20shader%20cores%20before%20passing%20them%20to%20fixed-function%20blending%20units.%0AOur%20evaluation%20shows%20that%20VR-Pipe%20greatly%20improves%20rendering%20performance%2C%0Aachieving%20up%20to%20a%202.78x%20speedup%20over%20the%20conventional%20graphics%20pipeline%20with%0Anegligible%20hardware%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17078v1&entry.124074799=Read"},
{"title": "TV-based Deep 3D Self Super-Resolution for fMRI", "author": "Fernando P\u00e9rez-Bueno and Hongwei Bran Li and Matthew S. Rosen and Shahin Nasr and Cesar Caballero-Gaudes and Juan Eugenio Iglesias", "abstract": "  While functional Magnetic Resonance Imaging (fMRI) offers valuable insights\ninto cognitive processes, its inherent spatial limitations pose challenges for\ndetailed analysis of the fine-grained functional architecture of the brain.\nMore specifically, MRI scanner and sequence specifications impose a trade-off\nbetween temporal resolution, spatial resolution, signal-to-noise ratio, and\nscan time. Deep Learning (DL) Super-Resolution (SR) methods have emerged as a\npromising solution to enhance fMRI resolution, generating high-resolution (HR)\nimages from low-resolution (LR) images typically acquired with lower scanning\ntimes. However, most existing SR approaches depend on supervised DL techniques,\nwhich require training ground truth (GT) HR data, which is often difficult to\nacquire and simultaneously sets a bound for how far SR can go. In this paper,\nwe introduce a novel self-supervised DL SR model that combines a DL network\nwith an analytical approach and Total Variation (TV) regularization. Our method\neliminates the need for external GT images, achieving competitive performance\ncompared to supervised DL techniques and preserving the functional maps.\n", "link": "http://arxiv.org/abs/2410.04097v2", "date": "2025-02-24", "relevancy": 2.1444, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5445}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TV-based%20Deep%203D%20Self%20Super-Resolution%20for%20fMRI&body=Title%3A%20TV-based%20Deep%203D%20Self%20Super-Resolution%20for%20fMRI%0AAuthor%3A%20Fernando%20P%C3%A9rez-Bueno%20and%20Hongwei%20Bran%20Li%20and%20Matthew%20S.%20Rosen%20and%20Shahin%20Nasr%20and%20Cesar%20Caballero-Gaudes%20and%20Juan%20Eugenio%20Iglesias%0AAbstract%3A%20%20%20While%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20offers%20valuable%20insights%0Ainto%20cognitive%20processes%2C%20its%20inherent%20spatial%20limitations%20pose%20challenges%20for%0Adetailed%20analysis%20of%20the%20fine-grained%20functional%20architecture%20of%20the%20brain.%0AMore%20specifically%2C%20MRI%20scanner%20and%20sequence%20specifications%20impose%20a%20trade-off%0Abetween%20temporal%20resolution%2C%20spatial%20resolution%2C%20signal-to-noise%20ratio%2C%20and%0Ascan%20time.%20Deep%20Learning%20%28DL%29%20Super-Resolution%20%28SR%29%20methods%20have%20emerged%20as%20a%0Apromising%20solution%20to%20enhance%20fMRI%20resolution%2C%20generating%20high-resolution%20%28HR%29%0Aimages%20from%20low-resolution%20%28LR%29%20images%20typically%20acquired%20with%20lower%20scanning%0Atimes.%20However%2C%20most%20existing%20SR%20approaches%20depend%20on%20supervised%20DL%20techniques%2C%0Awhich%20require%20training%20ground%20truth%20%28GT%29%20HR%20data%2C%20which%20is%20often%20difficult%20to%0Aacquire%20and%20simultaneously%20sets%20a%20bound%20for%20how%20far%20SR%20can%20go.%20In%20this%20paper%2C%0Awe%20introduce%20a%20novel%20self-supervised%20DL%20SR%20model%20that%20combines%20a%20DL%20network%0Awith%20an%20analytical%20approach%20and%20Total%20Variation%20%28TV%29%20regularization.%20Our%20method%0Aeliminates%20the%20need%20for%20external%20GT%20images%2C%20achieving%20competitive%20performance%0Acompared%20to%20supervised%20DL%20techniques%20and%20preserving%20the%20functional%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTV-based%2520Deep%25203D%2520Self%2520Super-Resolution%2520for%2520fMRI%26entry.906535625%3DFernando%2520P%25C3%25A9rez-Bueno%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Matthew%2520S.%2520Rosen%2520and%2520Shahin%2520Nasr%2520and%2520Cesar%2520Caballero-Gaudes%2520and%2520Juan%2520Eugenio%2520Iglesias%26entry.1292438233%3D%2520%2520While%2520functional%2520Magnetic%2520Resonance%2520Imaging%2520%2528fMRI%2529%2520offers%2520valuable%2520insights%250Ainto%2520cognitive%2520processes%252C%2520its%2520inherent%2520spatial%2520limitations%2520pose%2520challenges%2520for%250Adetailed%2520analysis%2520of%2520the%2520fine-grained%2520functional%2520architecture%2520of%2520the%2520brain.%250AMore%2520specifically%252C%2520MRI%2520scanner%2520and%2520sequence%2520specifications%2520impose%2520a%2520trade-off%250Abetween%2520temporal%2520resolution%252C%2520spatial%2520resolution%252C%2520signal-to-noise%2520ratio%252C%2520and%250Ascan%2520time.%2520Deep%2520Learning%2520%2528DL%2529%2520Super-Resolution%2520%2528SR%2529%2520methods%2520have%2520emerged%2520as%2520a%250Apromising%2520solution%2520to%2520enhance%2520fMRI%2520resolution%252C%2520generating%2520high-resolution%2520%2528HR%2529%250Aimages%2520from%2520low-resolution%2520%2528LR%2529%2520images%2520typically%2520acquired%2520with%2520lower%2520scanning%250Atimes.%2520However%252C%2520most%2520existing%2520SR%2520approaches%2520depend%2520on%2520supervised%2520DL%2520techniques%252C%250Awhich%2520require%2520training%2520ground%2520truth%2520%2528GT%2529%2520HR%2520data%252C%2520which%2520is%2520often%2520difficult%2520to%250Aacquire%2520and%2520simultaneously%2520sets%2520a%2520bound%2520for%2520how%2520far%2520SR%2520can%2520go.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520a%2520novel%2520self-supervised%2520DL%2520SR%2520model%2520that%2520combines%2520a%2520DL%2520network%250Awith%2520an%2520analytical%2520approach%2520and%2520Total%2520Variation%2520%2528TV%2529%2520regularization.%2520Our%2520method%250Aeliminates%2520the%2520need%2520for%2520external%2520GT%2520images%252C%2520achieving%2520competitive%2520performance%250Acompared%2520to%2520supervised%2520DL%2520techniques%2520and%2520preserving%2520the%2520functional%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TV-based%20Deep%203D%20Self%20Super-Resolution%20for%20fMRI&entry.906535625=Fernando%20P%C3%A9rez-Bueno%20and%20Hongwei%20Bran%20Li%20and%20Matthew%20S.%20Rosen%20and%20Shahin%20Nasr%20and%20Cesar%20Caballero-Gaudes%20and%20Juan%20Eugenio%20Iglesias&entry.1292438233=%20%20While%20functional%20Magnetic%20Resonance%20Imaging%20%28fMRI%29%20offers%20valuable%20insights%0Ainto%20cognitive%20processes%2C%20its%20inherent%20spatial%20limitations%20pose%20challenges%20for%0Adetailed%20analysis%20of%20the%20fine-grained%20functional%20architecture%20of%20the%20brain.%0AMore%20specifically%2C%20MRI%20scanner%20and%20sequence%20specifications%20impose%20a%20trade-off%0Abetween%20temporal%20resolution%2C%20spatial%20resolution%2C%20signal-to-noise%20ratio%2C%20and%0Ascan%20time.%20Deep%20Learning%20%28DL%29%20Super-Resolution%20%28SR%29%20methods%20have%20emerged%20as%20a%0Apromising%20solution%20to%20enhance%20fMRI%20resolution%2C%20generating%20high-resolution%20%28HR%29%0Aimages%20from%20low-resolution%20%28LR%29%20images%20typically%20acquired%20with%20lower%20scanning%0Atimes.%20However%2C%20most%20existing%20SR%20approaches%20depend%20on%20supervised%20DL%20techniques%2C%0Awhich%20require%20training%20ground%20truth%20%28GT%29%20HR%20data%2C%20which%20is%20often%20difficult%20to%0Aacquire%20and%20simultaneously%20sets%20a%20bound%20for%20how%20far%20SR%20can%20go.%20In%20this%20paper%2C%0Awe%20introduce%20a%20novel%20self-supervised%20DL%20SR%20model%20that%20combines%20a%20DL%20network%0Awith%20an%20analytical%20approach%20and%20Total%20Variation%20%28TV%29%20regularization.%20Our%20method%0Aeliminates%20the%20need%20for%20external%20GT%20images%2C%20achieving%20competitive%20performance%0Acompared%20to%20supervised%20DL%20techniques%20and%20preserving%20the%20functional%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04097v2&entry.124074799=Read"},
{"title": "Inverse Kinematics on Guiding Vector Fields for Robot Path Following", "author": "Yu Zhou and Jes\u00fas Bautista and Weijia Yao and H\u00e9ctor Garc\u00eda de Marina", "abstract": "  Inverse kinematics is a fundamental technique for motion and positioning\ncontrol in robotics, typically applied to end-effectors. In this paper, we\nextend the concept of inverse kinematics to guiding vector fields for path\nfollowing in autonomous mobile robots. The desired path is defined by its\nimplicit equation, i.e., by a collection of points belonging to one or more\nzero-level sets. These level sets serve as a reference to construct an error\nsignal that drives the guiding vector field toward the desired path, enabling\nthe robot to converge and travel along the path by following such a vector\nfield. We start with the formal exposition on how inverse kinematics can be\napplied to guiding vector fields for single-integrator robots in an\nm-dimensional Euclidean space. Then, we leverage inverse kinematics to ensure\nthat the level-set error signal behaves as a linear system, facilitating\ncontrol over the robot's transient motion toward the desired path and allowing\nfor the injection of feed-forward signals to induce precise motion behavior\nalong the path. We then propose solutions to the theoretical and practical\nchallenges of applying this technique to unicycles with constant speeds to\nfollow 2D paths with precise transient control. We finish by validating the\npredicted theoretical results through real flights with fixed-wing drones.\n", "link": "http://arxiv.org/abs/2502.17313v1", "date": "2025-02-24", "relevancy": 2.1382, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5794}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Kinematics%20on%20Guiding%20Vector%20Fields%20for%20Robot%20Path%20Following&body=Title%3A%20Inverse%20Kinematics%20on%20Guiding%20Vector%20Fields%20for%20Robot%20Path%20Following%0AAuthor%3A%20Yu%20Zhou%20and%20Jes%C3%BAs%20Bautista%20and%20Weijia%20Yao%20and%20H%C3%A9ctor%20Garc%C3%ADa%20de%20Marina%0AAbstract%3A%20%20%20Inverse%20kinematics%20is%20a%20fundamental%20technique%20for%20motion%20and%20positioning%0Acontrol%20in%20robotics%2C%20typically%20applied%20to%20end-effectors.%20In%20this%20paper%2C%20we%0Aextend%20the%20concept%20of%20inverse%20kinematics%20to%20guiding%20vector%20fields%20for%20path%0Afollowing%20in%20autonomous%20mobile%20robots.%20The%20desired%20path%20is%20defined%20by%20its%0Aimplicit%20equation%2C%20i.e.%2C%20by%20a%20collection%20of%20points%20belonging%20to%20one%20or%20more%0Azero-level%20sets.%20These%20level%20sets%20serve%20as%20a%20reference%20to%20construct%20an%20error%0Asignal%20that%20drives%20the%20guiding%20vector%20field%20toward%20the%20desired%20path%2C%20enabling%0Athe%20robot%20to%20converge%20and%20travel%20along%20the%20path%20by%20following%20such%20a%20vector%0Afield.%20We%20start%20with%20the%20formal%20exposition%20on%20how%20inverse%20kinematics%20can%20be%0Aapplied%20to%20guiding%20vector%20fields%20for%20single-integrator%20robots%20in%20an%0Am-dimensional%20Euclidean%20space.%20Then%2C%20we%20leverage%20inverse%20kinematics%20to%20ensure%0Athat%20the%20level-set%20error%20signal%20behaves%20as%20a%20linear%20system%2C%20facilitating%0Acontrol%20over%20the%20robot%27s%20transient%20motion%20toward%20the%20desired%20path%20and%20allowing%0Afor%20the%20injection%20of%20feed-forward%20signals%20to%20induce%20precise%20motion%20behavior%0Aalong%20the%20path.%20We%20then%20propose%20solutions%20to%20the%20theoretical%20and%20practical%0Achallenges%20of%20applying%20this%20technique%20to%20unicycles%20with%20constant%20speeds%20to%0Afollow%202D%20paths%20with%20precise%20transient%20control.%20We%20finish%20by%20validating%20the%0Apredicted%20theoretical%20results%20through%20real%20flights%20with%20fixed-wing%20drones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Kinematics%2520on%2520Guiding%2520Vector%2520Fields%2520for%2520Robot%2520Path%2520Following%26entry.906535625%3DYu%2520Zhou%2520and%2520Jes%25C3%25BAs%2520Bautista%2520and%2520Weijia%2520Yao%2520and%2520H%25C3%25A9ctor%2520Garc%25C3%25ADa%2520de%2520Marina%26entry.1292438233%3D%2520%2520Inverse%2520kinematics%2520is%2520a%2520fundamental%2520technique%2520for%2520motion%2520and%2520positioning%250Acontrol%2520in%2520robotics%252C%2520typically%2520applied%2520to%2520end-effectors.%2520In%2520this%2520paper%252C%2520we%250Aextend%2520the%2520concept%2520of%2520inverse%2520kinematics%2520to%2520guiding%2520vector%2520fields%2520for%2520path%250Afollowing%2520in%2520autonomous%2520mobile%2520robots.%2520The%2520desired%2520path%2520is%2520defined%2520by%2520its%250Aimplicit%2520equation%252C%2520i.e.%252C%2520by%2520a%2520collection%2520of%2520points%2520belonging%2520to%2520one%2520or%2520more%250Azero-level%2520sets.%2520These%2520level%2520sets%2520serve%2520as%2520a%2520reference%2520to%2520construct%2520an%2520error%250Asignal%2520that%2520drives%2520the%2520guiding%2520vector%2520field%2520toward%2520the%2520desired%2520path%252C%2520enabling%250Athe%2520robot%2520to%2520converge%2520and%2520travel%2520along%2520the%2520path%2520by%2520following%2520such%2520a%2520vector%250Afield.%2520We%2520start%2520with%2520the%2520formal%2520exposition%2520on%2520how%2520inverse%2520kinematics%2520can%2520be%250Aapplied%2520to%2520guiding%2520vector%2520fields%2520for%2520single-integrator%2520robots%2520in%2520an%250Am-dimensional%2520Euclidean%2520space.%2520Then%252C%2520we%2520leverage%2520inverse%2520kinematics%2520to%2520ensure%250Athat%2520the%2520level-set%2520error%2520signal%2520behaves%2520as%2520a%2520linear%2520system%252C%2520facilitating%250Acontrol%2520over%2520the%2520robot%2527s%2520transient%2520motion%2520toward%2520the%2520desired%2520path%2520and%2520allowing%250Afor%2520the%2520injection%2520of%2520feed-forward%2520signals%2520to%2520induce%2520precise%2520motion%2520behavior%250Aalong%2520the%2520path.%2520We%2520then%2520propose%2520solutions%2520to%2520the%2520theoretical%2520and%2520practical%250Achallenges%2520of%2520applying%2520this%2520technique%2520to%2520unicycles%2520with%2520constant%2520speeds%2520to%250Afollow%25202D%2520paths%2520with%2520precise%2520transient%2520control.%2520We%2520finish%2520by%2520validating%2520the%250Apredicted%2520theoretical%2520results%2520through%2520real%2520flights%2520with%2520fixed-wing%2520drones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Kinematics%20on%20Guiding%20Vector%20Fields%20for%20Robot%20Path%20Following&entry.906535625=Yu%20Zhou%20and%20Jes%C3%BAs%20Bautista%20and%20Weijia%20Yao%20and%20H%C3%A9ctor%20Garc%C3%ADa%20de%20Marina&entry.1292438233=%20%20Inverse%20kinematics%20is%20a%20fundamental%20technique%20for%20motion%20and%20positioning%0Acontrol%20in%20robotics%2C%20typically%20applied%20to%20end-effectors.%20In%20this%20paper%2C%20we%0Aextend%20the%20concept%20of%20inverse%20kinematics%20to%20guiding%20vector%20fields%20for%20path%0Afollowing%20in%20autonomous%20mobile%20robots.%20The%20desired%20path%20is%20defined%20by%20its%0Aimplicit%20equation%2C%20i.e.%2C%20by%20a%20collection%20of%20points%20belonging%20to%20one%20or%20more%0Azero-level%20sets.%20These%20level%20sets%20serve%20as%20a%20reference%20to%20construct%20an%20error%0Asignal%20that%20drives%20the%20guiding%20vector%20field%20toward%20the%20desired%20path%2C%20enabling%0Athe%20robot%20to%20converge%20and%20travel%20along%20the%20path%20by%20following%20such%20a%20vector%0Afield.%20We%20start%20with%20the%20formal%20exposition%20on%20how%20inverse%20kinematics%20can%20be%0Aapplied%20to%20guiding%20vector%20fields%20for%20single-integrator%20robots%20in%20an%0Am-dimensional%20Euclidean%20space.%20Then%2C%20we%20leverage%20inverse%20kinematics%20to%20ensure%0Athat%20the%20level-set%20error%20signal%20behaves%20as%20a%20linear%20system%2C%20facilitating%0Acontrol%20over%20the%20robot%27s%20transient%20motion%20toward%20the%20desired%20path%20and%20allowing%0Afor%20the%20injection%20of%20feed-forward%20signals%20to%20induce%20precise%20motion%20behavior%0Aalong%20the%20path.%20We%20then%20propose%20solutions%20to%20the%20theoretical%20and%20practical%0Achallenges%20of%20applying%20this%20technique%20to%20unicycles%20with%20constant%20speeds%20to%0Afollow%202D%20paths%20with%20precise%20transient%20control.%20We%20finish%20by%20validating%20the%0Apredicted%20theoretical%20results%20through%20real%20flights%20with%20fixed-wing%20drones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17313v1&entry.124074799=Read"},
{"title": "FoMo: Multi-Modal, Multi-Scale and Multi-Task Remote Sensing Foundation\n  Models for Forest Monitoring", "author": "Nikolaos Ioannis Bountos and Arthur Ouaknine and Ioannis Papoutsis and David Rolnick", "abstract": "  Forests are vital to ecosystems, supporting biodiversity and essential\nservices, but are rapidly changing due to land use and climate change.\nUnderstanding and mitigating negative effects requires parsing data on forests\nat global scale from a broad array of sensory modalities, and using them in\ndiverse forest monitoring applications. Such diversity in data and applications\ncan be effectively addressed through the development of a large, pre-trained\nfoundation model that serves as a versatile base for various downstream tasks.\nHowever, remote sensing modalities, which are an excellent fit for several\nforest management tasks, are particularly challenging considering the variation\nin environmental conditions, object scales, image acquisition modes,\nspatio-temporal resolutions, etc. With that in mind, we present the first\nunified Forest Monitoring Benchmark (FoMo-Bench), carefully constructed to\nevaluate foundation models with such flexibility. FoMo-Bench consists of 15\ndiverse datasets encompassing satellite, aerial, and inventory data, covering a\nvariety of geographical regions, and including multispectral, red-green-blue,\nsynthetic aperture radar and LiDAR data with various temporal, spatial and\nspectral resolutions. FoMo-Bench includes multiple types of forest-monitoring\ntasks, spanning classification, segmentation, and object detection. To enhance\ntask and geographic diversity in FoMo-Bench, we introduce TalloS, a global\ndataset combining satellite imagery with ground-based annotations for tree\nspecies classification across 1,000+ categories and hierarchical taxonomic\nlevels. Finally, we propose FoMo-Net, a pre-training framework to develop\nfoundation models with the capacity to process any combination of commonly used\nmodalities and spectral bands in remote sensing.\n", "link": "http://arxiv.org/abs/2312.10114v3", "date": "2025-02-24", "relevancy": 2.1337, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FoMo%3A%20Multi-Modal%2C%20Multi-Scale%20and%20Multi-Task%20Remote%20Sensing%20Foundation%0A%20%20Models%20for%20Forest%20Monitoring&body=Title%3A%20FoMo%3A%20Multi-Modal%2C%20Multi-Scale%20and%20Multi-Task%20Remote%20Sensing%20Foundation%0A%20%20Models%20for%20Forest%20Monitoring%0AAuthor%3A%20Nikolaos%20Ioannis%20Bountos%20and%20Arthur%20Ouaknine%20and%20Ioannis%20Papoutsis%20and%20David%20Rolnick%0AAbstract%3A%20%20%20Forests%20are%20vital%20to%20ecosystems%2C%20supporting%20biodiversity%20and%20essential%0Aservices%2C%20but%20are%20rapidly%20changing%20due%20to%20land%20use%20and%20climate%20change.%0AUnderstanding%20and%20mitigating%20negative%20effects%20requires%20parsing%20data%20on%20forests%0Aat%20global%20scale%20from%20a%20broad%20array%20of%20sensory%20modalities%2C%20and%20using%20them%20in%0Adiverse%20forest%20monitoring%20applications.%20Such%20diversity%20in%20data%20and%20applications%0Acan%20be%20effectively%20addressed%20through%20the%20development%20of%20a%20large%2C%20pre-trained%0Afoundation%20model%20that%20serves%20as%20a%20versatile%20base%20for%20various%20downstream%20tasks.%0AHowever%2C%20remote%20sensing%20modalities%2C%20which%20are%20an%20excellent%20fit%20for%20several%0Aforest%20management%20tasks%2C%20are%20particularly%20challenging%20considering%20the%20variation%0Ain%20environmental%20conditions%2C%20object%20scales%2C%20image%20acquisition%20modes%2C%0Aspatio-temporal%20resolutions%2C%20etc.%20With%20that%20in%20mind%2C%20we%20present%20the%20first%0Aunified%20Forest%20Monitoring%20Benchmark%20%28FoMo-Bench%29%2C%20carefully%20constructed%20to%0Aevaluate%20foundation%20models%20with%20such%20flexibility.%20FoMo-Bench%20consists%20of%2015%0Adiverse%20datasets%20encompassing%20satellite%2C%20aerial%2C%20and%20inventory%20data%2C%20covering%20a%0Avariety%20of%20geographical%20regions%2C%20and%20including%20multispectral%2C%20red-green-blue%2C%0Asynthetic%20aperture%20radar%20and%20LiDAR%20data%20with%20various%20temporal%2C%20spatial%20and%0Aspectral%20resolutions.%20FoMo-Bench%20includes%20multiple%20types%20of%20forest-monitoring%0Atasks%2C%20spanning%20classification%2C%20segmentation%2C%20and%20object%20detection.%20To%20enhance%0Atask%20and%20geographic%20diversity%20in%20FoMo-Bench%2C%20we%20introduce%20TalloS%2C%20a%20global%0Adataset%20combining%20satellite%20imagery%20with%20ground-based%20annotations%20for%20tree%0Aspecies%20classification%20across%201%2C000%2B%20categories%20and%20hierarchical%20taxonomic%0Alevels.%20Finally%2C%20we%20propose%20FoMo-Net%2C%20a%20pre-training%20framework%20to%20develop%0Afoundation%20models%20with%20the%20capacity%20to%20process%20any%20combination%20of%20commonly%20used%0Amodalities%20and%20spectral%20bands%20in%20remote%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10114v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoMo%253A%2520Multi-Modal%252C%2520Multi-Scale%2520and%2520Multi-Task%2520Remote%2520Sensing%2520Foundation%250A%2520%2520Models%2520for%2520Forest%2520Monitoring%26entry.906535625%3DNikolaos%2520Ioannis%2520Bountos%2520and%2520Arthur%2520Ouaknine%2520and%2520Ioannis%2520Papoutsis%2520and%2520David%2520Rolnick%26entry.1292438233%3D%2520%2520Forests%2520are%2520vital%2520to%2520ecosystems%252C%2520supporting%2520biodiversity%2520and%2520essential%250Aservices%252C%2520but%2520are%2520rapidly%2520changing%2520due%2520to%2520land%2520use%2520and%2520climate%2520change.%250AUnderstanding%2520and%2520mitigating%2520negative%2520effects%2520requires%2520parsing%2520data%2520on%2520forests%250Aat%2520global%2520scale%2520from%2520a%2520broad%2520array%2520of%2520sensory%2520modalities%252C%2520and%2520using%2520them%2520in%250Adiverse%2520forest%2520monitoring%2520applications.%2520Such%2520diversity%2520in%2520data%2520and%2520applications%250Acan%2520be%2520effectively%2520addressed%2520through%2520the%2520development%2520of%2520a%2520large%252C%2520pre-trained%250Afoundation%2520model%2520that%2520serves%2520as%2520a%2520versatile%2520base%2520for%2520various%2520downstream%2520tasks.%250AHowever%252C%2520remote%2520sensing%2520modalities%252C%2520which%2520are%2520an%2520excellent%2520fit%2520for%2520several%250Aforest%2520management%2520tasks%252C%2520are%2520particularly%2520challenging%2520considering%2520the%2520variation%250Ain%2520environmental%2520conditions%252C%2520object%2520scales%252C%2520image%2520acquisition%2520modes%252C%250Aspatio-temporal%2520resolutions%252C%2520etc.%2520With%2520that%2520in%2520mind%252C%2520we%2520present%2520the%2520first%250Aunified%2520Forest%2520Monitoring%2520Benchmark%2520%2528FoMo-Bench%2529%252C%2520carefully%2520constructed%2520to%250Aevaluate%2520foundation%2520models%2520with%2520such%2520flexibility.%2520FoMo-Bench%2520consists%2520of%252015%250Adiverse%2520datasets%2520encompassing%2520satellite%252C%2520aerial%252C%2520and%2520inventory%2520data%252C%2520covering%2520a%250Avariety%2520of%2520geographical%2520regions%252C%2520and%2520including%2520multispectral%252C%2520red-green-blue%252C%250Asynthetic%2520aperture%2520radar%2520and%2520LiDAR%2520data%2520with%2520various%2520temporal%252C%2520spatial%2520and%250Aspectral%2520resolutions.%2520FoMo-Bench%2520includes%2520multiple%2520types%2520of%2520forest-monitoring%250Atasks%252C%2520spanning%2520classification%252C%2520segmentation%252C%2520and%2520object%2520detection.%2520To%2520enhance%250Atask%2520and%2520geographic%2520diversity%2520in%2520FoMo-Bench%252C%2520we%2520introduce%2520TalloS%252C%2520a%2520global%250Adataset%2520combining%2520satellite%2520imagery%2520with%2520ground-based%2520annotations%2520for%2520tree%250Aspecies%2520classification%2520across%25201%252C000%252B%2520categories%2520and%2520hierarchical%2520taxonomic%250Alevels.%2520Finally%252C%2520we%2520propose%2520FoMo-Net%252C%2520a%2520pre-training%2520framework%2520to%2520develop%250Afoundation%2520models%2520with%2520the%2520capacity%2520to%2520process%2520any%2520combination%2520of%2520commonly%2520used%250Amodalities%2520and%2520spectral%2520bands%2520in%2520remote%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10114v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FoMo%3A%20Multi-Modal%2C%20Multi-Scale%20and%20Multi-Task%20Remote%20Sensing%20Foundation%0A%20%20Models%20for%20Forest%20Monitoring&entry.906535625=Nikolaos%20Ioannis%20Bountos%20and%20Arthur%20Ouaknine%20and%20Ioannis%20Papoutsis%20and%20David%20Rolnick&entry.1292438233=%20%20Forests%20are%20vital%20to%20ecosystems%2C%20supporting%20biodiversity%20and%20essential%0Aservices%2C%20but%20are%20rapidly%20changing%20due%20to%20land%20use%20and%20climate%20change.%0AUnderstanding%20and%20mitigating%20negative%20effects%20requires%20parsing%20data%20on%20forests%0Aat%20global%20scale%20from%20a%20broad%20array%20of%20sensory%20modalities%2C%20and%20using%20them%20in%0Adiverse%20forest%20monitoring%20applications.%20Such%20diversity%20in%20data%20and%20applications%0Acan%20be%20effectively%20addressed%20through%20the%20development%20of%20a%20large%2C%20pre-trained%0Afoundation%20model%20that%20serves%20as%20a%20versatile%20base%20for%20various%20downstream%20tasks.%0AHowever%2C%20remote%20sensing%20modalities%2C%20which%20are%20an%20excellent%20fit%20for%20several%0Aforest%20management%20tasks%2C%20are%20particularly%20challenging%20considering%20the%20variation%0Ain%20environmental%20conditions%2C%20object%20scales%2C%20image%20acquisition%20modes%2C%0Aspatio-temporal%20resolutions%2C%20etc.%20With%20that%20in%20mind%2C%20we%20present%20the%20first%0Aunified%20Forest%20Monitoring%20Benchmark%20%28FoMo-Bench%29%2C%20carefully%20constructed%20to%0Aevaluate%20foundation%20models%20with%20such%20flexibility.%20FoMo-Bench%20consists%20of%2015%0Adiverse%20datasets%20encompassing%20satellite%2C%20aerial%2C%20and%20inventory%20data%2C%20covering%20a%0Avariety%20of%20geographical%20regions%2C%20and%20including%20multispectral%2C%20red-green-blue%2C%0Asynthetic%20aperture%20radar%20and%20LiDAR%20data%20with%20various%20temporal%2C%20spatial%20and%0Aspectral%20resolutions.%20FoMo-Bench%20includes%20multiple%20types%20of%20forest-monitoring%0Atasks%2C%20spanning%20classification%2C%20segmentation%2C%20and%20object%20detection.%20To%20enhance%0Atask%20and%20geographic%20diversity%20in%20FoMo-Bench%2C%20we%20introduce%20TalloS%2C%20a%20global%0Adataset%20combining%20satellite%20imagery%20with%20ground-based%20annotations%20for%20tree%0Aspecies%20classification%20across%201%2C000%2B%20categories%20and%20hierarchical%20taxonomic%0Alevels.%20Finally%2C%20we%20propose%20FoMo-Net%2C%20a%20pre-training%20framework%20to%20develop%0Afoundation%20models%20with%20the%20capacity%20to%20process%20any%20combination%20of%20commonly%20used%0Amodalities%20and%20spectral%20bands%20in%20remote%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10114v3&entry.124074799=Read"},
{"title": "ELFS: Label-Free Coreset Selection with Proxy Training Dynamics", "author": "Haizhong Zheng and Elisa Tsai and Yifu Lu and Jiachen Sun and Brian R. Bartoldson and Bhavya Kailkhura and Atul Prakash", "abstract": "  High-quality human-annotated data is crucial for modern deep learning\npipelines, yet the human annotation process is both costly and time-consuming.\nGiven a constrained human labeling budget, selecting an informative and\nrepresentative data subset for labeling can significantly reduce human\nannotation effort. Well-performing state-of-the-art (SOTA) coreset selection\nmethods require ground truth labels over the whole dataset, failing to reduce\nthe human labeling burden. Meanwhile, SOTA label-free coreset selection methods\ndeliver inferior performance due to poor geometry-based difficulty scores. In\nthis paper, we introduce ELFS (Effective Label-Free Coreset Selection), a novel\nlabel-free coreset selection method. ELFS significantly improves label-free\ncoreset selection by addressing two challenges: 1) ELFS utilizes deep\nclustering to estimate training dynamics-based data difficulty scores without\nground truth labels; 2) Pseudo-labels introduce a distribution shift in the\ndata difficulty scores, and we propose a simple but effective double-end\npruning method to mitigate bias on calculated scores. We evaluate ELFS on four\nvision benchmarks and show that, given the same vision encoder, ELFS\nconsistently outperforms SOTA label-free baselines. For instance, when using\nSwAV as the encoder, ELFS outperforms D2 by up to 10.2% in accuracy on\nImageNet-1K. We make our code publicly available on GitHub.\n", "link": "http://arxiv.org/abs/2406.04273v2", "date": "2025-02-24", "relevancy": 2.1277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5545}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5387}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELFS%3A%20Label-Free%20Coreset%20Selection%20with%20Proxy%20Training%20Dynamics&body=Title%3A%20ELFS%3A%20Label-Free%20Coreset%20Selection%20with%20Proxy%20Training%20Dynamics%0AAuthor%3A%20Haizhong%20Zheng%20and%20Elisa%20Tsai%20and%20Yifu%20Lu%20and%20Jiachen%20Sun%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Atul%20Prakash%0AAbstract%3A%20%20%20High-quality%20human-annotated%20data%20is%20crucial%20for%20modern%20deep%20learning%0Apipelines%2C%20yet%20the%20human%20annotation%20process%20is%20both%20costly%20and%20time-consuming.%0AGiven%20a%20constrained%20human%20labeling%20budget%2C%20selecting%20an%20informative%20and%0Arepresentative%20data%20subset%20for%20labeling%20can%20significantly%20reduce%20human%0Aannotation%20effort.%20Well-performing%20state-of-the-art%20%28SOTA%29%20coreset%20selection%0Amethods%20require%20ground%20truth%20labels%20over%20the%20whole%20dataset%2C%20failing%20to%20reduce%0Athe%20human%20labeling%20burden.%20Meanwhile%2C%20SOTA%20label-free%20coreset%20selection%20methods%0Adeliver%20inferior%20performance%20due%20to%20poor%20geometry-based%20difficulty%20scores.%20In%0Athis%20paper%2C%20we%20introduce%20ELFS%20%28Effective%20Label-Free%20Coreset%20Selection%29%2C%20a%20novel%0Alabel-free%20coreset%20selection%20method.%20ELFS%20significantly%20improves%20label-free%0Acoreset%20selection%20by%20addressing%20two%20challenges%3A%201%29%20ELFS%20utilizes%20deep%0Aclustering%20to%20estimate%20training%20dynamics-based%20data%20difficulty%20scores%20without%0Aground%20truth%20labels%3B%202%29%20Pseudo-labels%20introduce%20a%20distribution%20shift%20in%20the%0Adata%20difficulty%20scores%2C%20and%20we%20propose%20a%20simple%20but%20effective%20double-end%0Apruning%20method%20to%20mitigate%20bias%20on%20calculated%20scores.%20We%20evaluate%20ELFS%20on%20four%0Avision%20benchmarks%20and%20show%20that%2C%20given%20the%20same%20vision%20encoder%2C%20ELFS%0Aconsistently%20outperforms%20SOTA%20label-free%20baselines.%20For%20instance%2C%20when%20using%0ASwAV%20as%20the%20encoder%2C%20ELFS%20outperforms%20D2%20by%20up%20to%2010.2%25%20in%20accuracy%20on%0AImageNet-1K.%20We%20make%20our%20code%20publicly%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELFS%253A%2520Label-Free%2520Coreset%2520Selection%2520with%2520Proxy%2520Training%2520Dynamics%26entry.906535625%3DHaizhong%2520Zheng%2520and%2520Elisa%2520Tsai%2520and%2520Yifu%2520Lu%2520and%2520Jiachen%2520Sun%2520and%2520Brian%2520R.%2520Bartoldson%2520and%2520Bhavya%2520Kailkhura%2520and%2520Atul%2520Prakash%26entry.1292438233%3D%2520%2520High-quality%2520human-annotated%2520data%2520is%2520crucial%2520for%2520modern%2520deep%2520learning%250Apipelines%252C%2520yet%2520the%2520human%2520annotation%2520process%2520is%2520both%2520costly%2520and%2520time-consuming.%250AGiven%2520a%2520constrained%2520human%2520labeling%2520budget%252C%2520selecting%2520an%2520informative%2520and%250Arepresentative%2520data%2520subset%2520for%2520labeling%2520can%2520significantly%2520reduce%2520human%250Aannotation%2520effort.%2520Well-performing%2520state-of-the-art%2520%2528SOTA%2529%2520coreset%2520selection%250Amethods%2520require%2520ground%2520truth%2520labels%2520over%2520the%2520whole%2520dataset%252C%2520failing%2520to%2520reduce%250Athe%2520human%2520labeling%2520burden.%2520Meanwhile%252C%2520SOTA%2520label-free%2520coreset%2520selection%2520methods%250Adeliver%2520inferior%2520performance%2520due%2520to%2520poor%2520geometry-based%2520difficulty%2520scores.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520ELFS%2520%2528Effective%2520Label-Free%2520Coreset%2520Selection%2529%252C%2520a%2520novel%250Alabel-free%2520coreset%2520selection%2520method.%2520ELFS%2520significantly%2520improves%2520label-free%250Acoreset%2520selection%2520by%2520addressing%2520two%2520challenges%253A%25201%2529%2520ELFS%2520utilizes%2520deep%250Aclustering%2520to%2520estimate%2520training%2520dynamics-based%2520data%2520difficulty%2520scores%2520without%250Aground%2520truth%2520labels%253B%25202%2529%2520Pseudo-labels%2520introduce%2520a%2520distribution%2520shift%2520in%2520the%250Adata%2520difficulty%2520scores%252C%2520and%2520we%2520propose%2520a%2520simple%2520but%2520effective%2520double-end%250Apruning%2520method%2520to%2520mitigate%2520bias%2520on%2520calculated%2520scores.%2520We%2520evaluate%2520ELFS%2520on%2520four%250Avision%2520benchmarks%2520and%2520show%2520that%252C%2520given%2520the%2520same%2520vision%2520encoder%252C%2520ELFS%250Aconsistently%2520outperforms%2520SOTA%2520label-free%2520baselines.%2520For%2520instance%252C%2520when%2520using%250ASwAV%2520as%2520the%2520encoder%252C%2520ELFS%2520outperforms%2520D2%2520by%2520up%2520to%252010.2%2525%2520in%2520accuracy%2520on%250AImageNet-1K.%2520We%2520make%2520our%2520code%2520publicly%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELFS%3A%20Label-Free%20Coreset%20Selection%20with%20Proxy%20Training%20Dynamics&entry.906535625=Haizhong%20Zheng%20and%20Elisa%20Tsai%20and%20Yifu%20Lu%20and%20Jiachen%20Sun%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Atul%20Prakash&entry.1292438233=%20%20High-quality%20human-annotated%20data%20is%20crucial%20for%20modern%20deep%20learning%0Apipelines%2C%20yet%20the%20human%20annotation%20process%20is%20both%20costly%20and%20time-consuming.%0AGiven%20a%20constrained%20human%20labeling%20budget%2C%20selecting%20an%20informative%20and%0Arepresentative%20data%20subset%20for%20labeling%20can%20significantly%20reduce%20human%0Aannotation%20effort.%20Well-performing%20state-of-the-art%20%28SOTA%29%20coreset%20selection%0Amethods%20require%20ground%20truth%20labels%20over%20the%20whole%20dataset%2C%20failing%20to%20reduce%0Athe%20human%20labeling%20burden.%20Meanwhile%2C%20SOTA%20label-free%20coreset%20selection%20methods%0Adeliver%20inferior%20performance%20due%20to%20poor%20geometry-based%20difficulty%20scores.%20In%0Athis%20paper%2C%20we%20introduce%20ELFS%20%28Effective%20Label-Free%20Coreset%20Selection%29%2C%20a%20novel%0Alabel-free%20coreset%20selection%20method.%20ELFS%20significantly%20improves%20label-free%0Acoreset%20selection%20by%20addressing%20two%20challenges%3A%201%29%20ELFS%20utilizes%20deep%0Aclustering%20to%20estimate%20training%20dynamics-based%20data%20difficulty%20scores%20without%0Aground%20truth%20labels%3B%202%29%20Pseudo-labels%20introduce%20a%20distribution%20shift%20in%20the%0Adata%20difficulty%20scores%2C%20and%20we%20propose%20a%20simple%20but%20effective%20double-end%0Apruning%20method%20to%20mitigate%20bias%20on%20calculated%20scores.%20We%20evaluate%20ELFS%20on%20four%0Avision%20benchmarks%20and%20show%20that%2C%20given%20the%20same%20vision%20encoder%2C%20ELFS%0Aconsistently%20outperforms%20SOTA%20label-free%20baselines.%20For%20instance%2C%20when%20using%0ASwAV%20as%20the%20encoder%2C%20ELFS%20outperforms%20D2%20by%20up%20to%2010.2%25%20in%20accuracy%20on%0AImageNet-1K.%20We%20make%20our%20code%20publicly%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04273v2&entry.124074799=Read"},
{"title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification", "author": "Penghui Yang and Cunxiao Du and Fengzhuo Zhang and Haonan Wang and Tianyu Pang and Chao Du and Bo An", "abstract": "  Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.\n", "link": "http://arxiv.org/abs/2502.17421v1", "date": "2025-02-24", "relevancy": 2.1202, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongSpec%3A%20Long-Context%20Speculative%20Decoding%20with%20Efficient%20Drafting%20and%0A%20%20Verification&body=Title%3A%20LongSpec%3A%20Long-Context%20Speculative%20Decoding%20with%20Efficient%20Drafting%20and%0A%20%20Verification%0AAuthor%3A%20Penghui%20Yang%20and%20Cunxiao%20Du%20and%20Fengzhuo%20Zhang%20and%20Haonan%20Wang%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Bo%20An%0AAbstract%3A%20%20%20Speculative%20decoding%20has%20become%20a%20promising%20technique%20to%20mitigate%20the%20high%0Ainference%20latency%20of%20autoregressive%20decoding%20in%20Large%20Language%20Models%20%28LLMs%29.%0ADespite%20its%20promise%2C%20the%20effective%20application%20of%20speculative%20decoding%20in%20LLMs%0Astill%20confronts%20three%20key%20challenges%3A%20the%20increasing%20memory%20demands%20of%20the%0Adraft%20model%2C%20the%20distribution%20shift%20between%20the%20short-training%20corpora%20and%0Along-context%20inference%2C%20and%20inefficiencies%20in%20attention%20implementation.%20In%20this%0Awork%2C%20we%20enhance%20the%20performance%20of%20speculative%20decoding%20in%20long-context%0Asettings%20by%20addressing%20these%20challenges.%20First%2C%20we%20propose%20a%20memory-efficient%0Adraft%20model%20with%20a%20constant-sized%20Key-Value%20%28KV%29%20cache.%20Second%2C%20we%20introduce%0Anovel%20position%20indices%20for%20short-training%20data%2C%20enabling%20seamless%20adaptation%0Afrom%20short-context%20training%20to%20long-context%20inference.%20Finally%2C%20we%20present%20an%0Ainnovative%20attention%20aggregation%20method%20that%20combines%20fast%20implementations%20for%0Aprefix%20computation%20with%20standard%20attention%20for%20tree%20mask%20handling%2C%20effectively%0Aresolving%20the%20latency%20and%20memory%20inefficiencies%20of%20tree%20decoding.%20Our%20approach%0Aachieves%20strong%20results%20on%20various%20long-context%20tasks%2C%20including%0Arepository-level%20code%20completion%2C%20long-context%20summarization%2C%20and%20o1-like%20long%0Areasoning%20tasks%2C%20demonstrating%20significant%20improvements%20in%20latency%20reduction.%0AThe%20code%20is%20available%20at%20https%3A//github.com/sail-sg/LongSpec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongSpec%253A%2520Long-Context%2520Speculative%2520Decoding%2520with%2520Efficient%2520Drafting%2520and%250A%2520%2520Verification%26entry.906535625%3DPenghui%2520Yang%2520and%2520Cunxiao%2520Du%2520and%2520Fengzhuo%2520Zhang%2520and%2520Haonan%2520Wang%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Speculative%2520decoding%2520has%2520become%2520a%2520promising%2520technique%2520to%2520mitigate%2520the%2520high%250Ainference%2520latency%2520of%2520autoregressive%2520decoding%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250ADespite%2520its%2520promise%252C%2520the%2520effective%2520application%2520of%2520speculative%2520decoding%2520in%2520LLMs%250Astill%2520confronts%2520three%2520key%2520challenges%253A%2520the%2520increasing%2520memory%2520demands%2520of%2520the%250Adraft%2520model%252C%2520the%2520distribution%2520shift%2520between%2520the%2520short-training%2520corpora%2520and%250Along-context%2520inference%252C%2520and%2520inefficiencies%2520in%2520attention%2520implementation.%2520In%2520this%250Awork%252C%2520we%2520enhance%2520the%2520performance%2520of%2520speculative%2520decoding%2520in%2520long-context%250Asettings%2520by%2520addressing%2520these%2520challenges.%2520First%252C%2520we%2520propose%2520a%2520memory-efficient%250Adraft%2520model%2520with%2520a%2520constant-sized%2520Key-Value%2520%2528KV%2529%2520cache.%2520Second%252C%2520we%2520introduce%250Anovel%2520position%2520indices%2520for%2520short-training%2520data%252C%2520enabling%2520seamless%2520adaptation%250Afrom%2520short-context%2520training%2520to%2520long-context%2520inference.%2520Finally%252C%2520we%2520present%2520an%250Ainnovative%2520attention%2520aggregation%2520method%2520that%2520combines%2520fast%2520implementations%2520for%250Aprefix%2520computation%2520with%2520standard%2520attention%2520for%2520tree%2520mask%2520handling%252C%2520effectively%250Aresolving%2520the%2520latency%2520and%2520memory%2520inefficiencies%2520of%2520tree%2520decoding.%2520Our%2520approach%250Aachieves%2520strong%2520results%2520on%2520various%2520long-context%2520tasks%252C%2520including%250Arepository-level%2520code%2520completion%252C%2520long-context%2520summarization%252C%2520and%2520o1-like%2520long%250Areasoning%2520tasks%252C%2520demonstrating%2520significant%2520improvements%2520in%2520latency%2520reduction.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/sail-sg/LongSpec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongSpec%3A%20Long-Context%20Speculative%20Decoding%20with%20Efficient%20Drafting%20and%0A%20%20Verification&entry.906535625=Penghui%20Yang%20and%20Cunxiao%20Du%20and%20Fengzhuo%20Zhang%20and%20Haonan%20Wang%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Bo%20An&entry.1292438233=%20%20Speculative%20decoding%20has%20become%20a%20promising%20technique%20to%20mitigate%20the%20high%0Ainference%20latency%20of%20autoregressive%20decoding%20in%20Large%20Language%20Models%20%28LLMs%29.%0ADespite%20its%20promise%2C%20the%20effective%20application%20of%20speculative%20decoding%20in%20LLMs%0Astill%20confronts%20three%20key%20challenges%3A%20the%20increasing%20memory%20demands%20of%20the%0Adraft%20model%2C%20the%20distribution%20shift%20between%20the%20short-training%20corpora%20and%0Along-context%20inference%2C%20and%20inefficiencies%20in%20attention%20implementation.%20In%20this%0Awork%2C%20we%20enhance%20the%20performance%20of%20speculative%20decoding%20in%20long-context%0Asettings%20by%20addressing%20these%20challenges.%20First%2C%20we%20propose%20a%20memory-efficient%0Adraft%20model%20with%20a%20constant-sized%20Key-Value%20%28KV%29%20cache.%20Second%2C%20we%20introduce%0Anovel%20position%20indices%20for%20short-training%20data%2C%20enabling%20seamless%20adaptation%0Afrom%20short-context%20training%20to%20long-context%20inference.%20Finally%2C%20we%20present%20an%0Ainnovative%20attention%20aggregation%20method%20that%20combines%20fast%20implementations%20for%0Aprefix%20computation%20with%20standard%20attention%20for%20tree%20mask%20handling%2C%20effectively%0Aresolving%20the%20latency%20and%20memory%20inefficiencies%20of%20tree%20decoding.%20Our%20approach%0Aachieves%20strong%20results%20on%20various%20long-context%20tasks%2C%20including%0Arepository-level%20code%20completion%2C%20long-context%20summarization%2C%20and%20o1-like%20long%0Areasoning%20tasks%2C%20demonstrating%20significant%20improvements%20in%20latency%20reduction.%0AThe%20code%20is%20available%20at%20https%3A//github.com/sail-sg/LongSpec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17421v1&entry.124074799=Read"},
{"title": "On the Dichotomy Between Privacy and Traceability in $\\ell_p$ Stochastic\n  Convex Optimization", "author": "Sasha Voitovych and Mahdi Haghifam and Idan Attias and Gintare Karolina Dziugaite and Roi Livni and Daniel M. Roy", "abstract": "  In this paper, we investigate the necessity of memorization in stochastic\nconvex optimization (SCO) under $\\ell_p$ geometries. Informally, we say a\nlearning algorithm memorizes $m$ samples (or is $m$-traceable) if, by analyzing\nits output, it is possible to identify at least $m$ of its training samples.\nOur main results uncover a fundamental tradeoff between traceability and excess\nrisk in SCO. For every $p\\in [1,\\infty)$, we establish the existence of a risk\nthreshold below which any sample-efficient learner must memorize a \\em{constant\nfraction} of its sample. For $p\\in [1,2]$, this threshold coincides with best\nrisk of differentially private (DP) algorithms, i.e., above this threshold,\nthere are algorithms that do not memorize even a single sample. This\nestablishes a sharp dichotomy between privacy and traceability for $p \\in\n[1,2]$. For $p \\in (2,\\infty)$, this threshold instead gives novel lower bounds\nfor DP learning, partially closing an open problem in this setup. En route of\nproving these results, we introduce a complexity notion we term \\em{trace\nvalue} of a problem, which unifies privacy lower bounds and traceability\nresults, and prove a sparse variant of the fingerprinting lemma.\n", "link": "http://arxiv.org/abs/2502.17384v1", "date": "2025-02-24", "relevancy": 2.1176, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.439}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4169}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Dichotomy%20Between%20Privacy%20and%20Traceability%20in%20%24%5Cell_p%24%20Stochastic%0A%20%20Convex%20Optimization&body=Title%3A%20On%20the%20Dichotomy%20Between%20Privacy%20and%20Traceability%20in%20%24%5Cell_p%24%20Stochastic%0A%20%20Convex%20Optimization%0AAuthor%3A%20Sasha%20Voitovych%20and%20Mahdi%20Haghifam%20and%20Idan%20Attias%20and%20Gintare%20Karolina%20Dziugaite%20and%20Roi%20Livni%20and%20Daniel%20M.%20Roy%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20investigate%20the%20necessity%20of%20memorization%20in%20stochastic%0Aconvex%20optimization%20%28SCO%29%20under%20%24%5Cell_p%24%20geometries.%20Informally%2C%20we%20say%20a%0Alearning%20algorithm%20memorizes%20%24m%24%20samples%20%28or%20is%20%24m%24-traceable%29%20if%2C%20by%20analyzing%0Aits%20output%2C%20it%20is%20possible%20to%20identify%20at%20least%20%24m%24%20of%20its%20training%20samples.%0AOur%20main%20results%20uncover%20a%20fundamental%20tradeoff%20between%20traceability%20and%20excess%0Arisk%20in%20SCO.%20For%20every%20%24p%5Cin%20%5B1%2C%5Cinfty%29%24%2C%20we%20establish%20the%20existence%20of%20a%20risk%0Athreshold%20below%20which%20any%20sample-efficient%20learner%20must%20memorize%20a%20%5Cem%7Bconstant%0Afraction%7D%20of%20its%20sample.%20For%20%24p%5Cin%20%5B1%2C2%5D%24%2C%20this%20threshold%20coincides%20with%20best%0Arisk%20of%20differentially%20private%20%28DP%29%20algorithms%2C%20i.e.%2C%20above%20this%20threshold%2C%0Athere%20are%20algorithms%20that%20do%20not%20memorize%20even%20a%20single%20sample.%20This%0Aestablishes%20a%20sharp%20dichotomy%20between%20privacy%20and%20traceability%20for%20%24p%20%5Cin%0A%5B1%2C2%5D%24.%20For%20%24p%20%5Cin%20%282%2C%5Cinfty%29%24%2C%20this%20threshold%20instead%20gives%20novel%20lower%20bounds%0Afor%20DP%20learning%2C%20partially%20closing%20an%20open%20problem%20in%20this%20setup.%20En%20route%20of%0Aproving%20these%20results%2C%20we%20introduce%20a%20complexity%20notion%20we%20term%20%5Cem%7Btrace%0Avalue%7D%20of%20a%20problem%2C%20which%20unifies%20privacy%20lower%20bounds%20and%20traceability%0Aresults%2C%20and%20prove%20a%20sparse%20variant%20of%20the%20fingerprinting%20lemma.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Dichotomy%2520Between%2520Privacy%2520and%2520Traceability%2520in%2520%2524%255Cell_p%2524%2520Stochastic%250A%2520%2520Convex%2520Optimization%26entry.906535625%3DSasha%2520Voitovych%2520and%2520Mahdi%2520Haghifam%2520and%2520Idan%2520Attias%2520and%2520Gintare%2520Karolina%2520Dziugaite%2520and%2520Roi%2520Livni%2520and%2520Daniel%2520M.%2520Roy%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520necessity%2520of%2520memorization%2520in%2520stochastic%250Aconvex%2520optimization%2520%2528SCO%2529%2520under%2520%2524%255Cell_p%2524%2520geometries.%2520Informally%252C%2520we%2520say%2520a%250Alearning%2520algorithm%2520memorizes%2520%2524m%2524%2520samples%2520%2528or%2520is%2520%2524m%2524-traceable%2529%2520if%252C%2520by%2520analyzing%250Aits%2520output%252C%2520it%2520is%2520possible%2520to%2520identify%2520at%2520least%2520%2524m%2524%2520of%2520its%2520training%2520samples.%250AOur%2520main%2520results%2520uncover%2520a%2520fundamental%2520tradeoff%2520between%2520traceability%2520and%2520excess%250Arisk%2520in%2520SCO.%2520For%2520every%2520%2524p%255Cin%2520%255B1%252C%255Cinfty%2529%2524%252C%2520we%2520establish%2520the%2520existence%2520of%2520a%2520risk%250Athreshold%2520below%2520which%2520any%2520sample-efficient%2520learner%2520must%2520memorize%2520a%2520%255Cem%257Bconstant%250Afraction%257D%2520of%2520its%2520sample.%2520For%2520%2524p%255Cin%2520%255B1%252C2%255D%2524%252C%2520this%2520threshold%2520coincides%2520with%2520best%250Arisk%2520of%2520differentially%2520private%2520%2528DP%2529%2520algorithms%252C%2520i.e.%252C%2520above%2520this%2520threshold%252C%250Athere%2520are%2520algorithms%2520that%2520do%2520not%2520memorize%2520even%2520a%2520single%2520sample.%2520This%250Aestablishes%2520a%2520sharp%2520dichotomy%2520between%2520privacy%2520and%2520traceability%2520for%2520%2524p%2520%255Cin%250A%255B1%252C2%255D%2524.%2520For%2520%2524p%2520%255Cin%2520%25282%252C%255Cinfty%2529%2524%252C%2520this%2520threshold%2520instead%2520gives%2520novel%2520lower%2520bounds%250Afor%2520DP%2520learning%252C%2520partially%2520closing%2520an%2520open%2520problem%2520in%2520this%2520setup.%2520En%2520route%2520of%250Aproving%2520these%2520results%252C%2520we%2520introduce%2520a%2520complexity%2520notion%2520we%2520term%2520%255Cem%257Btrace%250Avalue%257D%2520of%2520a%2520problem%252C%2520which%2520unifies%2520privacy%2520lower%2520bounds%2520and%2520traceability%250Aresults%252C%2520and%2520prove%2520a%2520sparse%2520variant%2520of%2520the%2520fingerprinting%2520lemma.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Dichotomy%20Between%20Privacy%20and%20Traceability%20in%20%24%5Cell_p%24%20Stochastic%0A%20%20Convex%20Optimization&entry.906535625=Sasha%20Voitovych%20and%20Mahdi%20Haghifam%20and%20Idan%20Attias%20and%20Gintare%20Karolina%20Dziugaite%20and%20Roi%20Livni%20and%20Daniel%20M.%20Roy&entry.1292438233=%20%20In%20this%20paper%2C%20we%20investigate%20the%20necessity%20of%20memorization%20in%20stochastic%0Aconvex%20optimization%20%28SCO%29%20under%20%24%5Cell_p%24%20geometries.%20Informally%2C%20we%20say%20a%0Alearning%20algorithm%20memorizes%20%24m%24%20samples%20%28or%20is%20%24m%24-traceable%29%20if%2C%20by%20analyzing%0Aits%20output%2C%20it%20is%20possible%20to%20identify%20at%20least%20%24m%24%20of%20its%20training%20samples.%0AOur%20main%20results%20uncover%20a%20fundamental%20tradeoff%20between%20traceability%20and%20excess%0Arisk%20in%20SCO.%20For%20every%20%24p%5Cin%20%5B1%2C%5Cinfty%29%24%2C%20we%20establish%20the%20existence%20of%20a%20risk%0Athreshold%20below%20which%20any%20sample-efficient%20learner%20must%20memorize%20a%20%5Cem%7Bconstant%0Afraction%7D%20of%20its%20sample.%20For%20%24p%5Cin%20%5B1%2C2%5D%24%2C%20this%20threshold%20coincides%20with%20best%0Arisk%20of%20differentially%20private%20%28DP%29%20algorithms%2C%20i.e.%2C%20above%20this%20threshold%2C%0Athere%20are%20algorithms%20that%20do%20not%20memorize%20even%20a%20single%20sample.%20This%0Aestablishes%20a%20sharp%20dichotomy%20between%20privacy%20and%20traceability%20for%20%24p%20%5Cin%0A%5B1%2C2%5D%24.%20For%20%24p%20%5Cin%20%282%2C%5Cinfty%29%24%2C%20this%20threshold%20instead%20gives%20novel%20lower%20bounds%0Afor%20DP%20learning%2C%20partially%20closing%20an%20open%20problem%20in%20this%20setup.%20En%20route%20of%0Aproving%20these%20results%2C%20we%20introduce%20a%20complexity%20notion%20we%20term%20%5Cem%7Btrace%0Avalue%7D%20of%20a%20problem%2C%20which%20unifies%20privacy%20lower%20bounds%20and%20traceability%0Aresults%2C%20and%20prove%20a%20sparse%20variant%20of%20the%20fingerprinting%20lemma.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17384v1&entry.124074799=Read"},
{"title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning", "author": "Dayuan Fu and Keqing He and Yejie Wang and Wentao Hong and Zhuoma Gongque and Weihao Zeng and Wei Wang and Jingang Wang and Xunliang Cai and Weiran Xu", "abstract": "  Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.\n", "link": "http://arxiv.org/abs/2501.01702v2", "date": "2025-02-24", "relevancy": 2.1007, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5538}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5175}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentRefine%3A%20Enhancing%20Agent%20Generalization%20through%20Refinement%20Tuning&body=Title%3A%20AgentRefine%3A%20Enhancing%20Agent%20Generalization%20through%20Refinement%20Tuning%0AAuthor%3A%20Dayuan%20Fu%20and%20Keqing%20He%20and%20Yejie%20Wang%20and%20Wentao%20Hong%20and%20Zhuoma%20Gongque%20and%20Weihao%20Zeng%20and%20Wei%20Wang%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Weiran%20Xu%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20based%20agents%20have%20proved%20their%20ability%20to%20perform%0Acomplex%20tasks%20like%20humans.%20However%2C%20there%20is%20still%20a%20large%20gap%20between%0Aopen-sourced%20LLMs%20and%20commercial%20models%20like%20the%20GPT%20series.%20In%20this%20paper%2C%20we%0Afocus%20on%20improving%20the%20agent%20generalization%20capabilities%20of%20LLMs%20via%0Ainstruction%20tuning.%20We%20first%20observe%20that%20the%20existing%20agent%20training%20corpus%0Aexhibits%20satisfactory%20results%20on%20held-in%20evaluation%20sets%20but%20fails%20to%0Ageneralize%20to%20held-out%20sets.%20These%20agent-tuning%20works%20face%20severe%20formatting%0Aerrors%20and%20are%20frequently%20stuck%20in%20the%20same%20mistake%20for%20a%20long%20while.%20We%0Aanalyze%20that%20the%20poor%20generalization%20ability%20comes%20from%20overfitting%20to%20several%0Amanual%20agent%20environments%20and%20a%20lack%20of%20adaptation%20to%20new%20situations.%20They%0Astruggle%20with%20the%20wrong%20action%20steps%20and%20can%20not%20learn%20from%20the%20experience%20but%0Ajust%20memorize%20existing%20observation-action%20relations.%20Inspired%20by%20the%20insight%2C%0Awe%20propose%20a%20novel%20AgentRefine%20framework%20for%20agent-tuning.%20The%20core%20idea%20is%20to%0Aenable%20the%20model%20to%20learn%20to%20correct%20its%20mistakes%20via%20observation%20in%20the%0Atrajectory.%20Specifically%2C%20we%20propose%20an%20agent%20synthesis%20framework%20to%20encompass%0Aa%20diverse%20array%20of%20environments%20and%20tasks%20and%20prompt%20a%20strong%20LLM%20to%20refine%20its%0Aerror%20action%20according%20to%20the%20environment%20feedback.%20AgentRefine%20significantly%0Aoutperforms%20state-of-the-art%20agent-tuning%20work%20in%20terms%20of%20generalization%0Aability%20on%20diverse%20agent%20tasks.%20It%20also%20has%20better%20robustness%20facing%0Aperturbation%20and%20can%20generate%20diversified%20thought%20in%20inference.%20Our%20findings%0Aestablish%20the%20correlation%20between%20agent%20generalization%20and%20self-refinement%20and%0Aprovide%20a%20new%20paradigm%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentRefine%253A%2520Enhancing%2520Agent%2520Generalization%2520through%2520Refinement%2520Tuning%26entry.906535625%3DDayuan%2520Fu%2520and%2520Keqing%2520He%2520and%2520Yejie%2520Wang%2520and%2520Wentao%2520Hong%2520and%2520Zhuoma%2520Gongque%2520and%2520Weihao%2520Zeng%2520and%2520Wei%2520Wang%2520and%2520Jingang%2520Wang%2520and%2520Xunliang%2520Cai%2520and%2520Weiran%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520based%2520agents%2520have%2520proved%2520their%2520ability%2520to%2520perform%250Acomplex%2520tasks%2520like%2520humans.%2520However%252C%2520there%2520is%2520still%2520a%2520large%2520gap%2520between%250Aopen-sourced%2520LLMs%2520and%2520commercial%2520models%2520like%2520the%2520GPT%2520series.%2520In%2520this%2520paper%252C%2520we%250Afocus%2520on%2520improving%2520the%2520agent%2520generalization%2520capabilities%2520of%2520LLMs%2520via%250Ainstruction%2520tuning.%2520We%2520first%2520observe%2520that%2520the%2520existing%2520agent%2520training%2520corpus%250Aexhibits%2520satisfactory%2520results%2520on%2520held-in%2520evaluation%2520sets%2520but%2520fails%2520to%250Ageneralize%2520to%2520held-out%2520sets.%2520These%2520agent-tuning%2520works%2520face%2520severe%2520formatting%250Aerrors%2520and%2520are%2520frequently%2520stuck%2520in%2520the%2520same%2520mistake%2520for%2520a%2520long%2520while.%2520We%250Aanalyze%2520that%2520the%2520poor%2520generalization%2520ability%2520comes%2520from%2520overfitting%2520to%2520several%250Amanual%2520agent%2520environments%2520and%2520a%2520lack%2520of%2520adaptation%2520to%2520new%2520situations.%2520They%250Astruggle%2520with%2520the%2520wrong%2520action%2520steps%2520and%2520can%2520not%2520learn%2520from%2520the%2520experience%2520but%250Ajust%2520memorize%2520existing%2520observation-action%2520relations.%2520Inspired%2520by%2520the%2520insight%252C%250Awe%2520propose%2520a%2520novel%2520AgentRefine%2520framework%2520for%2520agent-tuning.%2520The%2520core%2520idea%2520is%2520to%250Aenable%2520the%2520model%2520to%2520learn%2520to%2520correct%2520its%2520mistakes%2520via%2520observation%2520in%2520the%250Atrajectory.%2520Specifically%252C%2520we%2520propose%2520an%2520agent%2520synthesis%2520framework%2520to%2520encompass%250Aa%2520diverse%2520array%2520of%2520environments%2520and%2520tasks%2520and%2520prompt%2520a%2520strong%2520LLM%2520to%2520refine%2520its%250Aerror%2520action%2520according%2520to%2520the%2520environment%2520feedback.%2520AgentRefine%2520significantly%250Aoutperforms%2520state-of-the-art%2520agent-tuning%2520work%2520in%2520terms%2520of%2520generalization%250Aability%2520on%2520diverse%2520agent%2520tasks.%2520It%2520also%2520has%2520better%2520robustness%2520facing%250Aperturbation%2520and%2520can%2520generate%2520diversified%2520thought%2520in%2520inference.%2520Our%2520findings%250Aestablish%2520the%2520correlation%2520between%2520agent%2520generalization%2520and%2520self-refinement%2520and%250Aprovide%2520a%2520new%2520paradigm%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentRefine%3A%20Enhancing%20Agent%20Generalization%20through%20Refinement%20Tuning&entry.906535625=Dayuan%20Fu%20and%20Keqing%20He%20and%20Yejie%20Wang%20and%20Wentao%20Hong%20and%20Zhuoma%20Gongque%20and%20Weihao%20Zeng%20and%20Wei%20Wang%20and%20Jingang%20Wang%20and%20Xunliang%20Cai%20and%20Weiran%20Xu&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20based%20agents%20have%20proved%20their%20ability%20to%20perform%0Acomplex%20tasks%20like%20humans.%20However%2C%20there%20is%20still%20a%20large%20gap%20between%0Aopen-sourced%20LLMs%20and%20commercial%20models%20like%20the%20GPT%20series.%20In%20this%20paper%2C%20we%0Afocus%20on%20improving%20the%20agent%20generalization%20capabilities%20of%20LLMs%20via%0Ainstruction%20tuning.%20We%20first%20observe%20that%20the%20existing%20agent%20training%20corpus%0Aexhibits%20satisfactory%20results%20on%20held-in%20evaluation%20sets%20but%20fails%20to%0Ageneralize%20to%20held-out%20sets.%20These%20agent-tuning%20works%20face%20severe%20formatting%0Aerrors%20and%20are%20frequently%20stuck%20in%20the%20same%20mistake%20for%20a%20long%20while.%20We%0Aanalyze%20that%20the%20poor%20generalization%20ability%20comes%20from%20overfitting%20to%20several%0Amanual%20agent%20environments%20and%20a%20lack%20of%20adaptation%20to%20new%20situations.%20They%0Astruggle%20with%20the%20wrong%20action%20steps%20and%20can%20not%20learn%20from%20the%20experience%20but%0Ajust%20memorize%20existing%20observation-action%20relations.%20Inspired%20by%20the%20insight%2C%0Awe%20propose%20a%20novel%20AgentRefine%20framework%20for%20agent-tuning.%20The%20core%20idea%20is%20to%0Aenable%20the%20model%20to%20learn%20to%20correct%20its%20mistakes%20via%20observation%20in%20the%0Atrajectory.%20Specifically%2C%20we%20propose%20an%20agent%20synthesis%20framework%20to%20encompass%0Aa%20diverse%20array%20of%20environments%20and%20tasks%20and%20prompt%20a%20strong%20LLM%20to%20refine%20its%0Aerror%20action%20according%20to%20the%20environment%20feedback.%20AgentRefine%20significantly%0Aoutperforms%20state-of-the-art%20agent-tuning%20work%20in%20terms%20of%20generalization%0Aability%20on%20diverse%20agent%20tasks.%20It%20also%20has%20better%20robustness%20facing%0Aperturbation%20and%20can%20generate%20diversified%20thought%20in%20inference.%20Our%20findings%0Aestablish%20the%20correlation%20between%20agent%20generalization%20and%20self-refinement%20and%0Aprovide%20a%20new%20paradigm%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01702v2&entry.124074799=Read"},
{"title": "Large Language Models are Powerful EHR Encoders", "author": "Stefan Hegselmann and Georg von Arnim and Tillmann Rheude and Noel Kronenberg and David Sontag and Gerhard Hindricks and Roland Eils and Benjamin Wild", "abstract": "  Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.\n", "link": "http://arxiv.org/abs/2502.17403v1", "date": "2025-02-24", "relevancy": 2.0994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20are%20Powerful%20EHR%20Encoders&body=Title%3A%20Large%20Language%20Models%20are%20Powerful%20EHR%20Encoders%0AAuthor%3A%20Stefan%20Hegselmann%20and%20Georg%20von%20Arnim%20and%20Tillmann%20Rheude%20and%20Noel%20Kronenberg%20and%20David%20Sontag%20and%20Gerhard%20Hindricks%20and%20Roland%20Eils%20and%20Benjamin%20Wild%0AAbstract%3A%20%20%20Electronic%20Health%20Records%20%28EHRs%29%20offer%20rich%20potential%20for%20clinical%0Aprediction%2C%20yet%20their%20inherent%20complexity%20and%20heterogeneity%20pose%20significant%0Achallenges%20for%20traditional%20machine%20learning%20approaches.%20Domain-specific%20EHR%0Afoundation%20models%20trained%20on%20large%20collections%20of%20unlabeled%20EHR%20data%20have%0Ademonstrated%20promising%20improvements%20in%20predictive%20accuracy%20and%20generalization%3B%0Ahowever%2C%20their%20training%20is%20constrained%20by%20limited%20access%20to%20diverse%2C%0Ahigh-quality%20datasets%20and%20inconsistencies%20in%20coding%20standards%20and%20healthcare%0Apractices.%20In%20this%20study%2C%20we%20explore%20the%20possibility%20of%20using%20general-purpose%0ALarge%20Language%20Models%20%28LLMs%29%20based%20embedding%20methods%20as%20EHR%20encoders.%20By%0Aserializing%20patient%20records%20into%20structured%20Markdown%20text%2C%20transforming%20codes%0Ainto%20human-readable%20descriptors%2C%20we%20leverage%20the%20extensive%20generalization%0Acapabilities%20of%20LLMs%20pretrained%20on%20vast%20public%20corpora%2C%20thereby%20bypassing%20the%0Aneed%20for%20proprietary%20medical%20datasets.%20We%20systematically%20evaluate%20two%0Astate-of-the-art%20LLM-embedding%20models%2C%20GTE-Qwen2-7B-Instruct%20and%0ALLM2Vec-Llama3.1-8B-Instruct%2C%20across%2015%20diverse%20clinical%20prediction%20tasks%20from%0Athe%20EHRSHOT%20benchmark%2C%20comparing%20their%20performance%20to%20an%20EHRspecific%20foundation%0Amodel%2C%20CLIMBR-T-Base%2C%20and%20traditional%20machine%20learning%20baselines.%20Our%20results%0Ademonstrate%20that%20LLM-based%20embeddings%20frequently%20match%20or%20exceed%20the%0Aperformance%20of%20specialized%20models%2C%20even%20in%20few-shot%20settings%2C%20and%20that%20their%0Aeffectiveness%20scales%20with%20the%20size%20of%20the%20underlying%20LLM%20and%20the%20available%0Acontext%20window.%20Overall%2C%20our%20findings%20demonstrate%20that%20repurposing%20LLMs%20for%20EHR%0Aencoding%20offers%20a%20scalable%20and%20effective%20approach%20for%20clinical%20prediction%2C%0Acapable%20of%20overcoming%20the%20limitations%20of%20traditional%20EHR%20modeling%20and%0Afacilitating%20more%20interoperable%20and%20generalizable%20healthcare%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520are%2520Powerful%2520EHR%2520Encoders%26entry.906535625%3DStefan%2520Hegselmann%2520and%2520Georg%2520von%2520Arnim%2520and%2520Tillmann%2520Rheude%2520and%2520Noel%2520Kronenberg%2520and%2520David%2520Sontag%2520and%2520Gerhard%2520Hindricks%2520and%2520Roland%2520Eils%2520and%2520Benjamin%2520Wild%26entry.1292438233%3D%2520%2520Electronic%2520Health%2520Records%2520%2528EHRs%2529%2520offer%2520rich%2520potential%2520for%2520clinical%250Aprediction%252C%2520yet%2520their%2520inherent%2520complexity%2520and%2520heterogeneity%2520pose%2520significant%250Achallenges%2520for%2520traditional%2520machine%2520learning%2520approaches.%2520Domain-specific%2520EHR%250Afoundation%2520models%2520trained%2520on%2520large%2520collections%2520of%2520unlabeled%2520EHR%2520data%2520have%250Ademonstrated%2520promising%2520improvements%2520in%2520predictive%2520accuracy%2520and%2520generalization%253B%250Ahowever%252C%2520their%2520training%2520is%2520constrained%2520by%2520limited%2520access%2520to%2520diverse%252C%250Ahigh-quality%2520datasets%2520and%2520inconsistencies%2520in%2520coding%2520standards%2520and%2520healthcare%250Apractices.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%2520possibility%2520of%2520using%2520general-purpose%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520based%2520embedding%2520methods%2520as%2520EHR%2520encoders.%2520By%250Aserializing%2520patient%2520records%2520into%2520structured%2520Markdown%2520text%252C%2520transforming%2520codes%250Ainto%2520human-readable%2520descriptors%252C%2520we%2520leverage%2520the%2520extensive%2520generalization%250Acapabilities%2520of%2520LLMs%2520pretrained%2520on%2520vast%2520public%2520corpora%252C%2520thereby%2520bypassing%2520the%250Aneed%2520for%2520proprietary%2520medical%2520datasets.%2520We%2520systematically%2520evaluate%2520two%250Astate-of-the-art%2520LLM-embedding%2520models%252C%2520GTE-Qwen2-7B-Instruct%2520and%250ALLM2Vec-Llama3.1-8B-Instruct%252C%2520across%252015%2520diverse%2520clinical%2520prediction%2520tasks%2520from%250Athe%2520EHRSHOT%2520benchmark%252C%2520comparing%2520their%2520performance%2520to%2520an%2520EHRspecific%2520foundation%250Amodel%252C%2520CLIMBR-T-Base%252C%2520and%2520traditional%2520machine%2520learning%2520baselines.%2520Our%2520results%250Ademonstrate%2520that%2520LLM-based%2520embeddings%2520frequently%2520match%2520or%2520exceed%2520the%250Aperformance%2520of%2520specialized%2520models%252C%2520even%2520in%2520few-shot%2520settings%252C%2520and%2520that%2520their%250Aeffectiveness%2520scales%2520with%2520the%2520size%2520of%2520the%2520underlying%2520LLM%2520and%2520the%2520available%250Acontext%2520window.%2520Overall%252C%2520our%2520findings%2520demonstrate%2520that%2520repurposing%2520LLMs%2520for%2520EHR%250Aencoding%2520offers%2520a%2520scalable%2520and%2520effective%2520approach%2520for%2520clinical%2520prediction%252C%250Acapable%2520of%2520overcoming%2520the%2520limitations%2520of%2520traditional%2520EHR%2520modeling%2520and%250Afacilitating%2520more%2520interoperable%2520and%2520generalizable%2520healthcare%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20are%20Powerful%20EHR%20Encoders&entry.906535625=Stefan%20Hegselmann%20and%20Georg%20von%20Arnim%20and%20Tillmann%20Rheude%20and%20Noel%20Kronenberg%20and%20David%20Sontag%20and%20Gerhard%20Hindricks%20and%20Roland%20Eils%20and%20Benjamin%20Wild&entry.1292438233=%20%20Electronic%20Health%20Records%20%28EHRs%29%20offer%20rich%20potential%20for%20clinical%0Aprediction%2C%20yet%20their%20inherent%20complexity%20and%20heterogeneity%20pose%20significant%0Achallenges%20for%20traditional%20machine%20learning%20approaches.%20Domain-specific%20EHR%0Afoundation%20models%20trained%20on%20large%20collections%20of%20unlabeled%20EHR%20data%20have%0Ademonstrated%20promising%20improvements%20in%20predictive%20accuracy%20and%20generalization%3B%0Ahowever%2C%20their%20training%20is%20constrained%20by%20limited%20access%20to%20diverse%2C%0Ahigh-quality%20datasets%20and%20inconsistencies%20in%20coding%20standards%20and%20healthcare%0Apractices.%20In%20this%20study%2C%20we%20explore%20the%20possibility%20of%20using%20general-purpose%0ALarge%20Language%20Models%20%28LLMs%29%20based%20embedding%20methods%20as%20EHR%20encoders.%20By%0Aserializing%20patient%20records%20into%20structured%20Markdown%20text%2C%20transforming%20codes%0Ainto%20human-readable%20descriptors%2C%20we%20leverage%20the%20extensive%20generalization%0Acapabilities%20of%20LLMs%20pretrained%20on%20vast%20public%20corpora%2C%20thereby%20bypassing%20the%0Aneed%20for%20proprietary%20medical%20datasets.%20We%20systematically%20evaluate%20two%0Astate-of-the-art%20LLM-embedding%20models%2C%20GTE-Qwen2-7B-Instruct%20and%0ALLM2Vec-Llama3.1-8B-Instruct%2C%20across%2015%20diverse%20clinical%20prediction%20tasks%20from%0Athe%20EHRSHOT%20benchmark%2C%20comparing%20their%20performance%20to%20an%20EHRspecific%20foundation%0Amodel%2C%20CLIMBR-T-Base%2C%20and%20traditional%20machine%20learning%20baselines.%20Our%20results%0Ademonstrate%20that%20LLM-based%20embeddings%20frequently%20match%20or%20exceed%20the%0Aperformance%20of%20specialized%20models%2C%20even%20in%20few-shot%20settings%2C%20and%20that%20their%0Aeffectiveness%20scales%20with%20the%20size%20of%20the%20underlying%20LLM%20and%20the%20available%0Acontext%20window.%20Overall%2C%20our%20findings%20demonstrate%20that%20repurposing%20LLMs%20for%20EHR%0Aencoding%20offers%20a%20scalable%20and%20effective%20approach%20for%20clinical%20prediction%2C%0Acapable%20of%20overcoming%20the%20limitations%20of%20traditional%20EHR%20modeling%20and%0Afacilitating%20more%20interoperable%20and%20generalizable%20healthcare%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17403v1&entry.124074799=Read"},
{"title": "An Equivariant Pretrained Transformer for Unified 3D Molecular\n  Representation Learning", "author": "Rui Jiao and Xiangzhe Kong and Li Zhang and Ziyang Yu and Fangyuan Ren and Wenjuan Tan and Wenbing Huang and Yang Liu", "abstract": "  Pretraining on a large number of unlabeled 3D molecules has showcased\nsuperiority in various scientific applications. However, prior efforts\ntypically focus on pretraining models in a specific domain, either proteins or\nsmall molecules, missing the opportunity to leverage cross-domain knowledge. To\nmitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), an\nall-atom foundation model that can be pretrained from multiple domain 3D\nmolecules. Built upon an E(3)-equivariant transformer, EPT is able to not only\nprocess atom-level information but also incorporate block-level features (e.g.\nresiduals in proteins). Additionally, we employ a block-level denoising task,\nrather than the conventional atom-level denoising, as the pretraining\nobjective. To pretrain EPT, we construct a large-scale dataset of 5.89M\nentries, comprising small molecules, proteins, protein-protein complexes, and\nprotein-molecule complexes. Experimental evaluations on downstream tasks\nincluding ligand binding affinity prediction, protein property prediction, and\nmolecular property prediction, show that EPT significantly outperforms previous\nstate-of-the-art methods in the first task and achieves competitively superior\nperformance for the remaining two tasks. Furthermore, we demonstrate the\npotential of EPT in identifying small molecule drug candidates targeting 3CL\nprotease, a critical target in the replication of SARS-CoV-2. Among 1,978\nFDA-approved drugs, EPT ranks 7 out of 8 known anti-COVID-19 drugs in the top\n200, indicating the high recall of EPT. By using Molecular Dynamics (MD)\nsimulations, EPT further discoveries 7 novel compounds whose binding affinities\nare higher than that of the top-ranked known anti-COVID-19 drug, showcasing its\npowerful capabilities in drug discovery.\n", "link": "http://arxiv.org/abs/2402.12714v2", "date": "2025-02-24", "relevancy": 2.094, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5686}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Equivariant%20Pretrained%20Transformer%20for%20Unified%203D%20Molecular%0A%20%20Representation%20Learning&body=Title%3A%20An%20Equivariant%20Pretrained%20Transformer%20for%20Unified%203D%20Molecular%0A%20%20Representation%20Learning%0AAuthor%3A%20Rui%20Jiao%20and%20Xiangzhe%20Kong%20and%20Li%20Zhang%20and%20Ziyang%20Yu%20and%20Fangyuan%20Ren%20and%20Wenjuan%20Tan%20and%20Wenbing%20Huang%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Pretraining%20on%20a%20large%20number%20of%20unlabeled%203D%20molecules%20has%20showcased%0Asuperiority%20in%20various%20scientific%20applications.%20However%2C%20prior%20efforts%0Atypically%20focus%20on%20pretraining%20models%20in%20a%20specific%20domain%2C%20either%20proteins%20or%0Asmall%20molecules%2C%20missing%20the%20opportunity%20to%20leverage%20cross-domain%20knowledge.%20To%0Amitigate%20this%20gap%2C%20we%20introduce%20Equivariant%20Pretrained%20Transformer%20%28EPT%29%2C%20an%0Aall-atom%20foundation%20model%20that%20can%20be%20pretrained%20from%20multiple%20domain%203D%0Amolecules.%20Built%20upon%20an%20E%283%29-equivariant%20transformer%2C%20EPT%20is%20able%20to%20not%20only%0Aprocess%20atom-level%20information%20but%20also%20incorporate%20block-level%20features%20%28e.g.%0Aresiduals%20in%20proteins%29.%20Additionally%2C%20we%20employ%20a%20block-level%20denoising%20task%2C%0Arather%20than%20the%20conventional%20atom-level%20denoising%2C%20as%20the%20pretraining%0Aobjective.%20To%20pretrain%20EPT%2C%20we%20construct%20a%20large-scale%20dataset%20of%205.89M%0Aentries%2C%20comprising%20small%20molecules%2C%20proteins%2C%20protein-protein%20complexes%2C%20and%0Aprotein-molecule%20complexes.%20Experimental%20evaluations%20on%20downstream%20tasks%0Aincluding%20ligand%20binding%20affinity%20prediction%2C%20protein%20property%20prediction%2C%20and%0Amolecular%20property%20prediction%2C%20show%20that%20EPT%20significantly%20outperforms%20previous%0Astate-of-the-art%20methods%20in%20the%20first%20task%20and%20achieves%20competitively%20superior%0Aperformance%20for%20the%20remaining%20two%20tasks.%20Furthermore%2C%20we%20demonstrate%20the%0Apotential%20of%20EPT%20in%20identifying%20small%20molecule%20drug%20candidates%20targeting%203CL%0Aprotease%2C%20a%20critical%20target%20in%20the%20replication%20of%20SARS-CoV-2.%20Among%201%2C978%0AFDA-approved%20drugs%2C%20EPT%20ranks%207%20out%20of%208%20known%20anti-COVID-19%20drugs%20in%20the%20top%0A200%2C%20indicating%20the%20high%20recall%20of%20EPT.%20By%20using%20Molecular%20Dynamics%20%28MD%29%0Asimulations%2C%20EPT%20further%20discoveries%207%20novel%20compounds%20whose%20binding%20affinities%0Aare%20higher%20than%20that%20of%20the%20top-ranked%20known%20anti-COVID-19%20drug%2C%20showcasing%20its%0Apowerful%20capabilities%20in%20drug%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12714v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Equivariant%2520Pretrained%2520Transformer%2520for%2520Unified%25203D%2520Molecular%250A%2520%2520Representation%2520Learning%26entry.906535625%3DRui%2520Jiao%2520and%2520Xiangzhe%2520Kong%2520and%2520Li%2520Zhang%2520and%2520Ziyang%2520Yu%2520and%2520Fangyuan%2520Ren%2520and%2520Wenjuan%2520Tan%2520and%2520Wenbing%2520Huang%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Pretraining%2520on%2520a%2520large%2520number%2520of%2520unlabeled%25203D%2520molecules%2520has%2520showcased%250Asuperiority%2520in%2520various%2520scientific%2520applications.%2520However%252C%2520prior%2520efforts%250Atypically%2520focus%2520on%2520pretraining%2520models%2520in%2520a%2520specific%2520domain%252C%2520either%2520proteins%2520or%250Asmall%2520molecules%252C%2520missing%2520the%2520opportunity%2520to%2520leverage%2520cross-domain%2520knowledge.%2520To%250Amitigate%2520this%2520gap%252C%2520we%2520introduce%2520Equivariant%2520Pretrained%2520Transformer%2520%2528EPT%2529%252C%2520an%250Aall-atom%2520foundation%2520model%2520that%2520can%2520be%2520pretrained%2520from%2520multiple%2520domain%25203D%250Amolecules.%2520Built%2520upon%2520an%2520E%25283%2529-equivariant%2520transformer%252C%2520EPT%2520is%2520able%2520to%2520not%2520only%250Aprocess%2520atom-level%2520information%2520but%2520also%2520incorporate%2520block-level%2520features%2520%2528e.g.%250Aresiduals%2520in%2520proteins%2529.%2520Additionally%252C%2520we%2520employ%2520a%2520block-level%2520denoising%2520task%252C%250Arather%2520than%2520the%2520conventional%2520atom-level%2520denoising%252C%2520as%2520the%2520pretraining%250Aobjective.%2520To%2520pretrain%2520EPT%252C%2520we%2520construct%2520a%2520large-scale%2520dataset%2520of%25205.89M%250Aentries%252C%2520comprising%2520small%2520molecules%252C%2520proteins%252C%2520protein-protein%2520complexes%252C%2520and%250Aprotein-molecule%2520complexes.%2520Experimental%2520evaluations%2520on%2520downstream%2520tasks%250Aincluding%2520ligand%2520binding%2520affinity%2520prediction%252C%2520protein%2520property%2520prediction%252C%2520and%250Amolecular%2520property%2520prediction%252C%2520show%2520that%2520EPT%2520significantly%2520outperforms%2520previous%250Astate-of-the-art%2520methods%2520in%2520the%2520first%2520task%2520and%2520achieves%2520competitively%2520superior%250Aperformance%2520for%2520the%2520remaining%2520two%2520tasks.%2520Furthermore%252C%2520we%2520demonstrate%2520the%250Apotential%2520of%2520EPT%2520in%2520identifying%2520small%2520molecule%2520drug%2520candidates%2520targeting%25203CL%250Aprotease%252C%2520a%2520critical%2520target%2520in%2520the%2520replication%2520of%2520SARS-CoV-2.%2520Among%25201%252C978%250AFDA-approved%2520drugs%252C%2520EPT%2520ranks%25207%2520out%2520of%25208%2520known%2520anti-COVID-19%2520drugs%2520in%2520the%2520top%250A200%252C%2520indicating%2520the%2520high%2520recall%2520of%2520EPT.%2520By%2520using%2520Molecular%2520Dynamics%2520%2528MD%2529%250Asimulations%252C%2520EPT%2520further%2520discoveries%25207%2520novel%2520compounds%2520whose%2520binding%2520affinities%250Aare%2520higher%2520than%2520that%2520of%2520the%2520top-ranked%2520known%2520anti-COVID-19%2520drug%252C%2520showcasing%2520its%250Apowerful%2520capabilities%2520in%2520drug%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12714v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Equivariant%20Pretrained%20Transformer%20for%20Unified%203D%20Molecular%0A%20%20Representation%20Learning&entry.906535625=Rui%20Jiao%20and%20Xiangzhe%20Kong%20and%20Li%20Zhang%20and%20Ziyang%20Yu%20and%20Fangyuan%20Ren%20and%20Wenjuan%20Tan%20and%20Wenbing%20Huang%20and%20Yang%20Liu&entry.1292438233=%20%20Pretraining%20on%20a%20large%20number%20of%20unlabeled%203D%20molecules%20has%20showcased%0Asuperiority%20in%20various%20scientific%20applications.%20However%2C%20prior%20efforts%0Atypically%20focus%20on%20pretraining%20models%20in%20a%20specific%20domain%2C%20either%20proteins%20or%0Asmall%20molecules%2C%20missing%20the%20opportunity%20to%20leverage%20cross-domain%20knowledge.%20To%0Amitigate%20this%20gap%2C%20we%20introduce%20Equivariant%20Pretrained%20Transformer%20%28EPT%29%2C%20an%0Aall-atom%20foundation%20model%20that%20can%20be%20pretrained%20from%20multiple%20domain%203D%0Amolecules.%20Built%20upon%20an%20E%283%29-equivariant%20transformer%2C%20EPT%20is%20able%20to%20not%20only%0Aprocess%20atom-level%20information%20but%20also%20incorporate%20block-level%20features%20%28e.g.%0Aresiduals%20in%20proteins%29.%20Additionally%2C%20we%20employ%20a%20block-level%20denoising%20task%2C%0Arather%20than%20the%20conventional%20atom-level%20denoising%2C%20as%20the%20pretraining%0Aobjective.%20To%20pretrain%20EPT%2C%20we%20construct%20a%20large-scale%20dataset%20of%205.89M%0Aentries%2C%20comprising%20small%20molecules%2C%20proteins%2C%20protein-protein%20complexes%2C%20and%0Aprotein-molecule%20complexes.%20Experimental%20evaluations%20on%20downstream%20tasks%0Aincluding%20ligand%20binding%20affinity%20prediction%2C%20protein%20property%20prediction%2C%20and%0Amolecular%20property%20prediction%2C%20show%20that%20EPT%20significantly%20outperforms%20previous%0Astate-of-the-art%20methods%20in%20the%20first%20task%20and%20achieves%20competitively%20superior%0Aperformance%20for%20the%20remaining%20two%20tasks.%20Furthermore%2C%20we%20demonstrate%20the%0Apotential%20of%20EPT%20in%20identifying%20small%20molecule%20drug%20candidates%20targeting%203CL%0Aprotease%2C%20a%20critical%20target%20in%20the%20replication%20of%20SARS-CoV-2.%20Among%201%2C978%0AFDA-approved%20drugs%2C%20EPT%20ranks%207%20out%20of%208%20known%20anti-COVID-19%20drugs%20in%20the%20top%0A200%2C%20indicating%20the%20high%20recall%20of%20EPT.%20By%20using%20Molecular%20Dynamics%20%28MD%29%0Asimulations%2C%20EPT%20further%20discoveries%207%20novel%20compounds%20whose%20binding%20affinities%0Aare%20higher%20than%20that%20of%20the%20top-ranked%20known%20anti-COVID-19%20drug%2C%20showcasing%20its%0Apowerful%20capabilities%20in%20drug%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12714v2&entry.124074799=Read"},
{"title": "FACTR: Force-Attending Curriculum Training for Contact-Rich Policy\n  Learning", "author": "Jason Jingzhou Liu and Yulong Li and Kenneth Shaw and Tony Tao and Ruslan Salakhutdinov and Deepak Pathak", "abstract": "  Many contact-rich tasks humans perform, such as box pickup or rolling dough,\nrely on force feedback for reliable execution. However, this force information,\nwhich is readily available in most robot arms, is not commonly used in\nteleoperation and policy learning. Consequently, robot behavior is often\nlimited to quasi-static kinematic tasks that do not require intricate\nforce-feedback. In this paper, we first present a low-cost, intuitive,\nbilateral teleoperation setup that relays external forces of the follower arm\nback to the teacher arm, facilitating data collection for complex, contact-rich\ntasks. We then introduce FACTR, a policy learning method that employs a\ncurriculum which corrupts the visual input with decreasing intensity throughout\ntraining. The curriculum prevents our transformer-based policy from\nover-fitting to the visual input and guides the policy to properly attend to\nthe force modality. We demonstrate that by fully utilizing the force\ninformation, our method significantly improves generalization to unseen objects\nby 43\\% compared to baseline approaches without a curriculum. Video results and\ninstructions at https://jasonjzliu.com/factr/\n", "link": "http://arxiv.org/abs/2502.17432v1", "date": "2025-02-24", "relevancy": 2.0843, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5481}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.516}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FACTR%3A%20Force-Attending%20Curriculum%20Training%20for%20Contact-Rich%20Policy%0A%20%20Learning&body=Title%3A%20FACTR%3A%20Force-Attending%20Curriculum%20Training%20for%20Contact-Rich%20Policy%0A%20%20Learning%0AAuthor%3A%20Jason%20Jingzhou%20Liu%20and%20Yulong%20Li%20and%20Kenneth%20Shaw%20and%20Tony%20Tao%20and%20Ruslan%20Salakhutdinov%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20Many%20contact-rich%20tasks%20humans%20perform%2C%20such%20as%20box%20pickup%20or%20rolling%20dough%2C%0Arely%20on%20force%20feedback%20for%20reliable%20execution.%20However%2C%20this%20force%20information%2C%0Awhich%20is%20readily%20available%20in%20most%20robot%20arms%2C%20is%20not%20commonly%20used%20in%0Ateleoperation%20and%20policy%20learning.%20Consequently%2C%20robot%20behavior%20is%20often%0Alimited%20to%20quasi-static%20kinematic%20tasks%20that%20do%20not%20require%20intricate%0Aforce-feedback.%20In%20this%20paper%2C%20we%20first%20present%20a%20low-cost%2C%20intuitive%2C%0Abilateral%20teleoperation%20setup%20that%20relays%20external%20forces%20of%20the%20follower%20arm%0Aback%20to%20the%20teacher%20arm%2C%20facilitating%20data%20collection%20for%20complex%2C%20contact-rich%0Atasks.%20We%20then%20introduce%20FACTR%2C%20a%20policy%20learning%20method%20that%20employs%20a%0Acurriculum%20which%20corrupts%20the%20visual%20input%20with%20decreasing%20intensity%20throughout%0Atraining.%20The%20curriculum%20prevents%20our%20transformer-based%20policy%20from%0Aover-fitting%20to%20the%20visual%20input%20and%20guides%20the%20policy%20to%20properly%20attend%20to%0Athe%20force%20modality.%20We%20demonstrate%20that%20by%20fully%20utilizing%20the%20force%0Ainformation%2C%20our%20method%20significantly%20improves%20generalization%20to%20unseen%20objects%0Aby%2043%5C%25%20compared%20to%20baseline%20approaches%20without%20a%20curriculum.%20Video%20results%20and%0Ainstructions%20at%20https%3A//jasonjzliu.com/factr/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFACTR%253A%2520Force-Attending%2520Curriculum%2520Training%2520for%2520Contact-Rich%2520Policy%250A%2520%2520Learning%26entry.906535625%3DJason%2520Jingzhou%2520Liu%2520and%2520Yulong%2520Li%2520and%2520Kenneth%2520Shaw%2520and%2520Tony%2520Tao%2520and%2520Ruslan%2520Salakhutdinov%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520Many%2520contact-rich%2520tasks%2520humans%2520perform%252C%2520such%2520as%2520box%2520pickup%2520or%2520rolling%2520dough%252C%250Arely%2520on%2520force%2520feedback%2520for%2520reliable%2520execution.%2520However%252C%2520this%2520force%2520information%252C%250Awhich%2520is%2520readily%2520available%2520in%2520most%2520robot%2520arms%252C%2520is%2520not%2520commonly%2520used%2520in%250Ateleoperation%2520and%2520policy%2520learning.%2520Consequently%252C%2520robot%2520behavior%2520is%2520often%250Alimited%2520to%2520quasi-static%2520kinematic%2520tasks%2520that%2520do%2520not%2520require%2520intricate%250Aforce-feedback.%2520In%2520this%2520paper%252C%2520we%2520first%2520present%2520a%2520low-cost%252C%2520intuitive%252C%250Abilateral%2520teleoperation%2520setup%2520that%2520relays%2520external%2520forces%2520of%2520the%2520follower%2520arm%250Aback%2520to%2520the%2520teacher%2520arm%252C%2520facilitating%2520data%2520collection%2520for%2520complex%252C%2520contact-rich%250Atasks.%2520We%2520then%2520introduce%2520FACTR%252C%2520a%2520policy%2520learning%2520method%2520that%2520employs%2520a%250Acurriculum%2520which%2520corrupts%2520the%2520visual%2520input%2520with%2520decreasing%2520intensity%2520throughout%250Atraining.%2520The%2520curriculum%2520prevents%2520our%2520transformer-based%2520policy%2520from%250Aover-fitting%2520to%2520the%2520visual%2520input%2520and%2520guides%2520the%2520policy%2520to%2520properly%2520attend%2520to%250Athe%2520force%2520modality.%2520We%2520demonstrate%2520that%2520by%2520fully%2520utilizing%2520the%2520force%250Ainformation%252C%2520our%2520method%2520significantly%2520improves%2520generalization%2520to%2520unseen%2520objects%250Aby%252043%255C%2525%2520compared%2520to%2520baseline%2520approaches%2520without%2520a%2520curriculum.%2520Video%2520results%2520and%250Ainstructions%2520at%2520https%253A//jasonjzliu.com/factr/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FACTR%3A%20Force-Attending%20Curriculum%20Training%20for%20Contact-Rich%20Policy%0A%20%20Learning&entry.906535625=Jason%20Jingzhou%20Liu%20and%20Yulong%20Li%20and%20Kenneth%20Shaw%20and%20Tony%20Tao%20and%20Ruslan%20Salakhutdinov%20and%20Deepak%20Pathak&entry.1292438233=%20%20Many%20contact-rich%20tasks%20humans%20perform%2C%20such%20as%20box%20pickup%20or%20rolling%20dough%2C%0Arely%20on%20force%20feedback%20for%20reliable%20execution.%20However%2C%20this%20force%20information%2C%0Awhich%20is%20readily%20available%20in%20most%20robot%20arms%2C%20is%20not%20commonly%20used%20in%0Ateleoperation%20and%20policy%20learning.%20Consequently%2C%20robot%20behavior%20is%20often%0Alimited%20to%20quasi-static%20kinematic%20tasks%20that%20do%20not%20require%20intricate%0Aforce-feedback.%20In%20this%20paper%2C%20we%20first%20present%20a%20low-cost%2C%20intuitive%2C%0Abilateral%20teleoperation%20setup%20that%20relays%20external%20forces%20of%20the%20follower%20arm%0Aback%20to%20the%20teacher%20arm%2C%20facilitating%20data%20collection%20for%20complex%2C%20contact-rich%0Atasks.%20We%20then%20introduce%20FACTR%2C%20a%20policy%20learning%20method%20that%20employs%20a%0Acurriculum%20which%20corrupts%20the%20visual%20input%20with%20decreasing%20intensity%20throughout%0Atraining.%20The%20curriculum%20prevents%20our%20transformer-based%20policy%20from%0Aover-fitting%20to%20the%20visual%20input%20and%20guides%20the%20policy%20to%20properly%20attend%20to%0Athe%20force%20modality.%20We%20demonstrate%20that%20by%20fully%20utilizing%20the%20force%0Ainformation%2C%20our%20method%20significantly%20improves%20generalization%20to%20unseen%20objects%0Aby%2043%5C%25%20compared%20to%20baseline%20approaches%20without%20a%20curriculum.%20Video%20results%20and%0Ainstructions%20at%20https%3A//jasonjzliu.com/factr/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17432v1&entry.124074799=Read"},
{"title": "Evaluation of Deep Audio Representations for Hearables", "author": "Fabian Gr\u00f6ger and Pascal Baumann and Ludovic Amruthalingam and Laurent Simon and Ruksana Giurda and Simone Lionetti", "abstract": "  Effectively steering hearable devices requires understanding the acoustic\nenvironment around the user. In the computational analysis of sound scenes,\nfoundation models have emerged as the state of the art to produce\nhigh-performance, robust, multi-purpose audio representations. We introduce and\nrelease Deep Evaluation of Audio Representations (DEAR), the first dataset and\nbenchmark to evaluate the efficacy of foundation models in capturing essential\nacoustic properties for hearables. The dataset includes 1,158 audio tracks,\neach 30 seconds long, created by spatially mixing proprietary monologues with\ncommercial, high-quality recordings of everyday acoustic scenes. Our benchmark\nencompasses eight tasks that assess the general context, speech sources, and\ntechnical acoustic properties of the audio scenes. Through our evaluation of\nfour general-purpose audio representation models, we demonstrate that the BEATs\nmodel significantly surpasses its counterparts. This superiority underscores\nthe advantage of models trained on diverse audio collections, confirming their\napplicability to a wide array of auditory tasks, including encoding the\nenvironment properties necessary for hearable steering. The DEAR dataset and\nassociated code are available at https://dear-dataset.github.io.\n", "link": "http://arxiv.org/abs/2502.06664v2", "date": "2025-02-24", "relevancy": 2.0797, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5307}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Deep%20Audio%20Representations%20for%20Hearables&body=Title%3A%20Evaluation%20of%20Deep%20Audio%20Representations%20for%20Hearables%0AAuthor%3A%20Fabian%20Gr%C3%B6ger%20and%20Pascal%20Baumann%20and%20Ludovic%20Amruthalingam%20and%20Laurent%20Simon%20and%20Ruksana%20Giurda%20and%20Simone%20Lionetti%0AAbstract%3A%20%20%20Effectively%20steering%20hearable%20devices%20requires%20understanding%20the%20acoustic%0Aenvironment%20around%20the%20user.%20In%20the%20computational%20analysis%20of%20sound%20scenes%2C%0Afoundation%20models%20have%20emerged%20as%20the%20state%20of%20the%20art%20to%20produce%0Ahigh-performance%2C%20robust%2C%20multi-purpose%20audio%20representations.%20We%20introduce%20and%0Arelease%20Deep%20Evaluation%20of%20Audio%20Representations%20%28DEAR%29%2C%20the%20first%20dataset%20and%0Abenchmark%20to%20evaluate%20the%20efficacy%20of%20foundation%20models%20in%20capturing%20essential%0Aacoustic%20properties%20for%20hearables.%20The%20dataset%20includes%201%2C158%20audio%20tracks%2C%0Aeach%2030%20seconds%20long%2C%20created%20by%20spatially%20mixing%20proprietary%20monologues%20with%0Acommercial%2C%20high-quality%20recordings%20of%20everyday%20acoustic%20scenes.%20Our%20benchmark%0Aencompasses%20eight%20tasks%20that%20assess%20the%20general%20context%2C%20speech%20sources%2C%20and%0Atechnical%20acoustic%20properties%20of%20the%20audio%20scenes.%20Through%20our%20evaluation%20of%0Afour%20general-purpose%20audio%20representation%20models%2C%20we%20demonstrate%20that%20the%20BEATs%0Amodel%20significantly%20surpasses%20its%20counterparts.%20This%20superiority%20underscores%0Athe%20advantage%20of%20models%20trained%20on%20diverse%20audio%20collections%2C%20confirming%20their%0Aapplicability%20to%20a%20wide%20array%20of%20auditory%20tasks%2C%20including%20encoding%20the%0Aenvironment%20properties%20necessary%20for%20hearable%20steering.%20The%20DEAR%20dataset%20and%0Aassociated%20code%20are%20available%20at%20https%3A//dear-dataset.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Deep%2520Audio%2520Representations%2520for%2520Hearables%26entry.906535625%3DFabian%2520Gr%25C3%25B6ger%2520and%2520Pascal%2520Baumann%2520and%2520Ludovic%2520Amruthalingam%2520and%2520Laurent%2520Simon%2520and%2520Ruksana%2520Giurda%2520and%2520Simone%2520Lionetti%26entry.1292438233%3D%2520%2520Effectively%2520steering%2520hearable%2520devices%2520requires%2520understanding%2520the%2520acoustic%250Aenvironment%2520around%2520the%2520user.%2520In%2520the%2520computational%2520analysis%2520of%2520sound%2520scenes%252C%250Afoundation%2520models%2520have%2520emerged%2520as%2520the%2520state%2520of%2520the%2520art%2520to%2520produce%250Ahigh-performance%252C%2520robust%252C%2520multi-purpose%2520audio%2520representations.%2520We%2520introduce%2520and%250Arelease%2520Deep%2520Evaluation%2520of%2520Audio%2520Representations%2520%2528DEAR%2529%252C%2520the%2520first%2520dataset%2520and%250Abenchmark%2520to%2520evaluate%2520the%2520efficacy%2520of%2520foundation%2520models%2520in%2520capturing%2520essential%250Aacoustic%2520properties%2520for%2520hearables.%2520The%2520dataset%2520includes%25201%252C158%2520audio%2520tracks%252C%250Aeach%252030%2520seconds%2520long%252C%2520created%2520by%2520spatially%2520mixing%2520proprietary%2520monologues%2520with%250Acommercial%252C%2520high-quality%2520recordings%2520of%2520everyday%2520acoustic%2520scenes.%2520Our%2520benchmark%250Aencompasses%2520eight%2520tasks%2520that%2520assess%2520the%2520general%2520context%252C%2520speech%2520sources%252C%2520and%250Atechnical%2520acoustic%2520properties%2520of%2520the%2520audio%2520scenes.%2520Through%2520our%2520evaluation%2520of%250Afour%2520general-purpose%2520audio%2520representation%2520models%252C%2520we%2520demonstrate%2520that%2520the%2520BEATs%250Amodel%2520significantly%2520surpasses%2520its%2520counterparts.%2520This%2520superiority%2520underscores%250Athe%2520advantage%2520of%2520models%2520trained%2520on%2520diverse%2520audio%2520collections%252C%2520confirming%2520their%250Aapplicability%2520to%2520a%2520wide%2520array%2520of%2520auditory%2520tasks%252C%2520including%2520encoding%2520the%250Aenvironment%2520properties%2520necessary%2520for%2520hearable%2520steering.%2520The%2520DEAR%2520dataset%2520and%250Aassociated%2520code%2520are%2520available%2520at%2520https%253A//dear-dataset.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Deep%20Audio%20Representations%20for%20Hearables&entry.906535625=Fabian%20Gr%C3%B6ger%20and%20Pascal%20Baumann%20and%20Ludovic%20Amruthalingam%20and%20Laurent%20Simon%20and%20Ruksana%20Giurda%20and%20Simone%20Lionetti&entry.1292438233=%20%20Effectively%20steering%20hearable%20devices%20requires%20understanding%20the%20acoustic%0Aenvironment%20around%20the%20user.%20In%20the%20computational%20analysis%20of%20sound%20scenes%2C%0Afoundation%20models%20have%20emerged%20as%20the%20state%20of%20the%20art%20to%20produce%0Ahigh-performance%2C%20robust%2C%20multi-purpose%20audio%20representations.%20We%20introduce%20and%0Arelease%20Deep%20Evaluation%20of%20Audio%20Representations%20%28DEAR%29%2C%20the%20first%20dataset%20and%0Abenchmark%20to%20evaluate%20the%20efficacy%20of%20foundation%20models%20in%20capturing%20essential%0Aacoustic%20properties%20for%20hearables.%20The%20dataset%20includes%201%2C158%20audio%20tracks%2C%0Aeach%2030%20seconds%20long%2C%20created%20by%20spatially%20mixing%20proprietary%20monologues%20with%0Acommercial%2C%20high-quality%20recordings%20of%20everyday%20acoustic%20scenes.%20Our%20benchmark%0Aencompasses%20eight%20tasks%20that%20assess%20the%20general%20context%2C%20speech%20sources%2C%20and%0Atechnical%20acoustic%20properties%20of%20the%20audio%20scenes.%20Through%20our%20evaluation%20of%0Afour%20general-purpose%20audio%20representation%20models%2C%20we%20demonstrate%20that%20the%20BEATs%0Amodel%20significantly%20surpasses%20its%20counterparts.%20This%20superiority%20underscores%0Athe%20advantage%20of%20models%20trained%20on%20diverse%20audio%20collections%2C%20confirming%20their%0Aapplicability%20to%20a%20wide%20array%20of%20auditory%20tasks%2C%20including%20encoding%20the%0Aenvironment%20properties%20necessary%20for%20hearable%20steering.%20The%20DEAR%20dataset%20and%0Aassociated%20code%20are%20available%20at%20https%3A//dear-dataset.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06664v2&entry.124074799=Read"},
{"title": "GCC: Generative Color Constancy via Diffusing a Color Checker", "author": "Chen-Wei Chang and Cheng-De Fan and Chia-Che Chang and Yi-Chen Lo and Yu-Chee Tseng and Jiun-Long Huang and Yu-Lun Liu", "abstract": "  Color constancy methods often struggle to generalize across different camera\nsensors due to varying spectral sensitivities. We present GCC, which leverages\ndiffusion models to inpaint color checkers into images for illumination\nestimation. Our key innovations include (1) a single-step deterministic\ninference approach that inpaints color checkers reflecting scene illumination,\n(2) a Laplacian decomposition technique that preserves checker structure while\nallowing illumination-dependent color adaptation, and (3) a mask-based data\naugmentation strategy for handling imprecise color checker annotations. GCC\ndemonstrates superior robustness in cross-camera scenarios, achieving\nstate-of-the-art worst-25% error rates of 5.15{\\deg} and 4.32{\\deg} in\nbi-directional evaluations. These results highlight our method's stability and\ngeneralization capability across different camera characteristics without\nrequiring sensor-specific training, making it a versatile solution for\nreal-world applications.\n", "link": "http://arxiv.org/abs/2502.17435v1", "date": "2025-02-24", "relevancy": 2.0793, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.565}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5119}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCC%3A%20Generative%20Color%20Constancy%20via%20Diffusing%20a%20Color%20Checker&body=Title%3A%20GCC%3A%20Generative%20Color%20Constancy%20via%20Diffusing%20a%20Color%20Checker%0AAuthor%3A%20Chen-Wei%20Chang%20and%20Cheng-De%20Fan%20and%20Chia-Che%20Chang%20and%20Yi-Chen%20Lo%20and%20Yu-Chee%20Tseng%20and%20Jiun-Long%20Huang%20and%20Yu-Lun%20Liu%0AAbstract%3A%20%20%20Color%20constancy%20methods%20often%20struggle%20to%20generalize%20across%20different%20camera%0Asensors%20due%20to%20varying%20spectral%20sensitivities.%20We%20present%20GCC%2C%20which%20leverages%0Adiffusion%20models%20to%20inpaint%20color%20checkers%20into%20images%20for%20illumination%0Aestimation.%20Our%20key%20innovations%20include%20%281%29%20a%20single-step%20deterministic%0Ainference%20approach%20that%20inpaints%20color%20checkers%20reflecting%20scene%20illumination%2C%0A%282%29%20a%20Laplacian%20decomposition%20technique%20that%20preserves%20checker%20structure%20while%0Aallowing%20illumination-dependent%20color%20adaptation%2C%20and%20%283%29%20a%20mask-based%20data%0Aaugmentation%20strategy%20for%20handling%20imprecise%20color%20checker%20annotations.%20GCC%0Ademonstrates%20superior%20robustness%20in%20cross-camera%20scenarios%2C%20achieving%0Astate-of-the-art%20worst-25%25%20error%20rates%20of%205.15%7B%5Cdeg%7D%20and%204.32%7B%5Cdeg%7D%20in%0Abi-directional%20evaluations.%20These%20results%20highlight%20our%20method%27s%20stability%20and%0Ageneralization%20capability%20across%20different%20camera%20characteristics%20without%0Arequiring%20sensor-specific%20training%2C%20making%20it%20a%20versatile%20solution%20for%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCC%253A%2520Generative%2520Color%2520Constancy%2520via%2520Diffusing%2520a%2520Color%2520Checker%26entry.906535625%3DChen-Wei%2520Chang%2520and%2520Cheng-De%2520Fan%2520and%2520Chia-Che%2520Chang%2520and%2520Yi-Chen%2520Lo%2520and%2520Yu-Chee%2520Tseng%2520and%2520Jiun-Long%2520Huang%2520and%2520Yu-Lun%2520Liu%26entry.1292438233%3D%2520%2520Color%2520constancy%2520methods%2520often%2520struggle%2520to%2520generalize%2520across%2520different%2520camera%250Asensors%2520due%2520to%2520varying%2520spectral%2520sensitivities.%2520We%2520present%2520GCC%252C%2520which%2520leverages%250Adiffusion%2520models%2520to%2520inpaint%2520color%2520checkers%2520into%2520images%2520for%2520illumination%250Aestimation.%2520Our%2520key%2520innovations%2520include%2520%25281%2529%2520a%2520single-step%2520deterministic%250Ainference%2520approach%2520that%2520inpaints%2520color%2520checkers%2520reflecting%2520scene%2520illumination%252C%250A%25282%2529%2520a%2520Laplacian%2520decomposition%2520technique%2520that%2520preserves%2520checker%2520structure%2520while%250Aallowing%2520illumination-dependent%2520color%2520adaptation%252C%2520and%2520%25283%2529%2520a%2520mask-based%2520data%250Aaugmentation%2520strategy%2520for%2520handling%2520imprecise%2520color%2520checker%2520annotations.%2520GCC%250Ademonstrates%2520superior%2520robustness%2520in%2520cross-camera%2520scenarios%252C%2520achieving%250Astate-of-the-art%2520worst-25%2525%2520error%2520rates%2520of%25205.15%257B%255Cdeg%257D%2520and%25204.32%257B%255Cdeg%257D%2520in%250Abi-directional%2520evaluations.%2520These%2520results%2520highlight%2520our%2520method%2527s%2520stability%2520and%250Ageneralization%2520capability%2520across%2520different%2520camera%2520characteristics%2520without%250Arequiring%2520sensor-specific%2520training%252C%2520making%2520it%2520a%2520versatile%2520solution%2520for%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCC%3A%20Generative%20Color%20Constancy%20via%20Diffusing%20a%20Color%20Checker&entry.906535625=Chen-Wei%20Chang%20and%20Cheng-De%20Fan%20and%20Chia-Che%20Chang%20and%20Yi-Chen%20Lo%20and%20Yu-Chee%20Tseng%20and%20Jiun-Long%20Huang%20and%20Yu-Lun%20Liu&entry.1292438233=%20%20Color%20constancy%20methods%20often%20struggle%20to%20generalize%20across%20different%20camera%0Asensors%20due%20to%20varying%20spectral%20sensitivities.%20We%20present%20GCC%2C%20which%20leverages%0Adiffusion%20models%20to%20inpaint%20color%20checkers%20into%20images%20for%20illumination%0Aestimation.%20Our%20key%20innovations%20include%20%281%29%20a%20single-step%20deterministic%0Ainference%20approach%20that%20inpaints%20color%20checkers%20reflecting%20scene%20illumination%2C%0A%282%29%20a%20Laplacian%20decomposition%20technique%20that%20preserves%20checker%20structure%20while%0Aallowing%20illumination-dependent%20color%20adaptation%2C%20and%20%283%29%20a%20mask-based%20data%0Aaugmentation%20strategy%20for%20handling%20imprecise%20color%20checker%20annotations.%20GCC%0Ademonstrates%20superior%20robustness%20in%20cross-camera%20scenarios%2C%20achieving%0Astate-of-the-art%20worst-25%25%20error%20rates%20of%205.15%7B%5Cdeg%7D%20and%204.32%7B%5Cdeg%7D%20in%0Abi-directional%20evaluations.%20These%20results%20highlight%20our%20method%27s%20stability%20and%0Ageneralization%20capability%20across%20different%20camera%20characteristics%20without%0Arequiring%20sensor-specific%20training%2C%20making%20it%20a%20versatile%20solution%20for%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17435v1&entry.124074799=Read"},
{"title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical\n  Systems", "author": "Maksim Zhdanov and Max Welling and Jan-Willem van de Meent", "abstract": "  Large-scale physical systems defined on irregular grids pose significant\nscalability challenges for deep learning methods, especially in the presence of\nlong-range interactions and multi-scale coupling. Traditional approaches that\ncompute all pairwise interactions, such as attention, become computationally\nprohibitive as they scale quadratically with the number of nodes. We present\nErwin, a hierarchical transformer inspired by methods from computational\nmany-body physics, which combines the efficiency of tree-based algorithms with\nthe expressivity of attention mechanisms. Erwin employs ball tree partitioning\nto organize computation, which enables linear-time attention by processing\nnodes in parallel within local neighborhoods of fixed size. Through progressive\ncoarsening and refinement of the ball tree structure, complemented by a novel\ncross-ball interaction mechanism, it captures both fine-grained local details\nand global features. We demonstrate Erwin's effectiveness across multiple\ndomains, including cosmology, molecular dynamics, and particle fluid dynamics,\nwhere it consistently outperforms baseline methods both in accuracy and\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2502.17019v1", "date": "2025-02-24", "relevancy": 2.0762, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5689}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5214}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Erwin%3A%20A%20Tree-based%20Hierarchical%20Transformer%20for%20Large-scale%20Physical%0A%20%20Systems&body=Title%3A%20Erwin%3A%20A%20Tree-based%20Hierarchical%20Transformer%20for%20Large-scale%20Physical%0A%20%20Systems%0AAuthor%3A%20Maksim%20Zhdanov%20and%20Max%20Welling%20and%20Jan-Willem%20van%20de%20Meent%0AAbstract%3A%20%20%20Large-scale%20physical%20systems%20defined%20on%20irregular%20grids%20pose%20significant%0Ascalability%20challenges%20for%20deep%20learning%20methods%2C%20especially%20in%20the%20presence%20of%0Along-range%20interactions%20and%20multi-scale%20coupling.%20Traditional%20approaches%20that%0Acompute%20all%20pairwise%20interactions%2C%20such%20as%20attention%2C%20become%20computationally%0Aprohibitive%20as%20they%20scale%20quadratically%20with%20the%20number%20of%20nodes.%20We%20present%0AErwin%2C%20a%20hierarchical%20transformer%20inspired%20by%20methods%20from%20computational%0Amany-body%20physics%2C%20which%20combines%20the%20efficiency%20of%20tree-based%20algorithms%20with%0Athe%20expressivity%20of%20attention%20mechanisms.%20Erwin%20employs%20ball%20tree%20partitioning%0Ato%20organize%20computation%2C%20which%20enables%20linear-time%20attention%20by%20processing%0Anodes%20in%20parallel%20within%20local%20neighborhoods%20of%20fixed%20size.%20Through%20progressive%0Acoarsening%20and%20refinement%20of%20the%20ball%20tree%20structure%2C%20complemented%20by%20a%20novel%0Across-ball%20interaction%20mechanism%2C%20it%20captures%20both%20fine-grained%20local%20details%0Aand%20global%20features.%20We%20demonstrate%20Erwin%27s%20effectiveness%20across%20multiple%0Adomains%2C%20including%20cosmology%2C%20molecular%20dynamics%2C%20and%20particle%20fluid%20dynamics%2C%0Awhere%20it%20consistently%20outperforms%20baseline%20methods%20both%20in%20accuracy%20and%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DErwin%253A%2520A%2520Tree-based%2520Hierarchical%2520Transformer%2520for%2520Large-scale%2520Physical%250A%2520%2520Systems%26entry.906535625%3DMaksim%2520Zhdanov%2520and%2520Max%2520Welling%2520and%2520Jan-Willem%2520van%2520de%2520Meent%26entry.1292438233%3D%2520%2520Large-scale%2520physical%2520systems%2520defined%2520on%2520irregular%2520grids%2520pose%2520significant%250Ascalability%2520challenges%2520for%2520deep%2520learning%2520methods%252C%2520especially%2520in%2520the%2520presence%2520of%250Along-range%2520interactions%2520and%2520multi-scale%2520coupling.%2520Traditional%2520approaches%2520that%250Acompute%2520all%2520pairwise%2520interactions%252C%2520such%2520as%2520attention%252C%2520become%2520computationally%250Aprohibitive%2520as%2520they%2520scale%2520quadratically%2520with%2520the%2520number%2520of%2520nodes.%2520We%2520present%250AErwin%252C%2520a%2520hierarchical%2520transformer%2520inspired%2520by%2520methods%2520from%2520computational%250Amany-body%2520physics%252C%2520which%2520combines%2520the%2520efficiency%2520of%2520tree-based%2520algorithms%2520with%250Athe%2520expressivity%2520of%2520attention%2520mechanisms.%2520Erwin%2520employs%2520ball%2520tree%2520partitioning%250Ato%2520organize%2520computation%252C%2520which%2520enables%2520linear-time%2520attention%2520by%2520processing%250Anodes%2520in%2520parallel%2520within%2520local%2520neighborhoods%2520of%2520fixed%2520size.%2520Through%2520progressive%250Acoarsening%2520and%2520refinement%2520of%2520the%2520ball%2520tree%2520structure%252C%2520complemented%2520by%2520a%2520novel%250Across-ball%2520interaction%2520mechanism%252C%2520it%2520captures%2520both%2520fine-grained%2520local%2520details%250Aand%2520global%2520features.%2520We%2520demonstrate%2520Erwin%2527s%2520effectiveness%2520across%2520multiple%250Adomains%252C%2520including%2520cosmology%252C%2520molecular%2520dynamics%252C%2520and%2520particle%2520fluid%2520dynamics%252C%250Awhere%2520it%2520consistently%2520outperforms%2520baseline%2520methods%2520both%2520in%2520accuracy%2520and%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Erwin%3A%20A%20Tree-based%20Hierarchical%20Transformer%20for%20Large-scale%20Physical%0A%20%20Systems&entry.906535625=Maksim%20Zhdanov%20and%20Max%20Welling%20and%20Jan-Willem%20van%20de%20Meent&entry.1292438233=%20%20Large-scale%20physical%20systems%20defined%20on%20irregular%20grids%20pose%20significant%0Ascalability%20challenges%20for%20deep%20learning%20methods%2C%20especially%20in%20the%20presence%20of%0Along-range%20interactions%20and%20multi-scale%20coupling.%20Traditional%20approaches%20that%0Acompute%20all%20pairwise%20interactions%2C%20such%20as%20attention%2C%20become%20computationally%0Aprohibitive%20as%20they%20scale%20quadratically%20with%20the%20number%20of%20nodes.%20We%20present%0AErwin%2C%20a%20hierarchical%20transformer%20inspired%20by%20methods%20from%20computational%0Amany-body%20physics%2C%20which%20combines%20the%20efficiency%20of%20tree-based%20algorithms%20with%0Athe%20expressivity%20of%20attention%20mechanisms.%20Erwin%20employs%20ball%20tree%20partitioning%0Ato%20organize%20computation%2C%20which%20enables%20linear-time%20attention%20by%20processing%0Anodes%20in%20parallel%20within%20local%20neighborhoods%20of%20fixed%20size.%20Through%20progressive%0Acoarsening%20and%20refinement%20of%20the%20ball%20tree%20structure%2C%20complemented%20by%20a%20novel%0Across-ball%20interaction%20mechanism%2C%20it%20captures%20both%20fine-grained%20local%20details%0Aand%20global%20features.%20We%20demonstrate%20Erwin%27s%20effectiveness%20across%20multiple%0Adomains%2C%20including%20cosmology%2C%20molecular%20dynamics%2C%20and%20particle%20fluid%20dynamics%2C%0Awhere%20it%20consistently%20outperforms%20baseline%20methods%20both%20in%20accuracy%20and%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17019v1&entry.124074799=Read"},
{"title": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages", "author": "Ashutosh Bajpai and Tanmoy Chakraborty", "abstract": "  The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages.\n", "link": "http://arxiv.org/abs/2412.08090v2", "date": "2025-02-24", "relevancy": 2.0707, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5499}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20LLMs%20Inherently%20Reward%20In-Language%20Time-Sensitive%20Semantic%0A%20%20Alignment%20for%20Low-Resource%20Languages&body=Title%3A%20Multilingual%20LLMs%20Inherently%20Reward%20In-Language%20Time-Sensitive%20Semantic%0A%20%20Alignment%20for%20Low-Resource%20Languages%0AAuthor%3A%20Ashutosh%20Bajpai%20and%20Tanmoy%20Chakraborty%0AAbstract%3A%20%20%20The%20unwavering%20disparity%20in%20labeled%20resources%20between%20resource-rich%20languages%0Aand%20those%20considered%20low-resource%20remains%20a%20significant%20impediment%20for%20Large%0ALanguage%20Models%20%28LLMs%29.%20Recent%20strides%20in%20cross-lingual%20in-context%20learning%0A%28X-ICL%29%2C%20mainly%20through%20semantically%20aligned%20examples%20retrieved%20from%0Amultilingual%20pre-trained%20transformers%2C%20have%20shown%20promise%20in%20mitigating%20this%0Aissue.%20However%2C%20our%20investigation%20reveals%20that%20LLMs%20intrinsically%20reward%0Ain-language%20semantically%20aligned%20cross-lingual%20instances%20over%20direct%0Across-lingual%20semantic%20alignments%2C%20with%20a%20pronounced%20disparity%20in%20handling%0Atime-sensitive%20queries%20in%20the%20X-ICL%20setup.%20Such%20queries%20demand%20sound%20temporal%0Areasoning%20ability%20from%20LLMs%2C%20yet%20the%20advancements%20have%20predominantly%20focused%20on%0AEnglish.%20This%20study%20aims%20to%20bridge%20this%20gap%20by%20improving%20temporal%20reasoning%0Acapabilities%20in%20low-resource%20languages.%20To%20this%20end%2C%20we%20introduce%20mTEMPREASON%2C%0Aa%20temporal%20reasoning%20dataset%20aimed%20at%20the%20varied%20degrees%20of%20low-resource%0Alanguages%20and%20propose%20Cross-Lingual%20Time-Sensitive%20Semantic%20Alignment%0A%28CLiTSSA%29%2C%20a%20novel%20method%20to%20improve%20temporal%20reasoning%20in%20these%20contexts.%20To%0Afacilitate%20this%2C%20we%20construct%20an%20extension%20of%20mTEMPREASON%20comprising%20pairs%20of%0Aparallel%20cross-language%20temporal%20queries%20along%20with%20their%20anticipated%0Ain-language%20semantic%20similarity%20scores.%20Our%20empirical%20evidence%20underscores%20the%0Asuperior%20performance%20of%20CLiTSSA%20compared%20to%20established%20baselines%20across%20three%0Alanguages%20--%20Romanian%2C%20German%2C%20and%20French%2C%20encompassing%20three%20temporal%20tasks%0Aand%20including%20a%20diverse%20set%20of%20four%20contemporaneous%20LLMs.%20This%20marks%20a%0Asignificant%20step%20forward%20in%20addressing%20resource%20disparity%20in%20the%20context%20of%0Atemporal%20reasoning%20across%20languages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520LLMs%2520Inherently%2520Reward%2520In-Language%2520Time-Sensitive%2520Semantic%250A%2520%2520Alignment%2520for%2520Low-Resource%2520Languages%26entry.906535625%3DAshutosh%2520Bajpai%2520and%2520Tanmoy%2520Chakraborty%26entry.1292438233%3D%2520%2520The%2520unwavering%2520disparity%2520in%2520labeled%2520resources%2520between%2520resource-rich%2520languages%250Aand%2520those%2520considered%2520low-resource%2520remains%2520a%2520significant%2520impediment%2520for%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529.%2520Recent%2520strides%2520in%2520cross-lingual%2520in-context%2520learning%250A%2528X-ICL%2529%252C%2520mainly%2520through%2520semantically%2520aligned%2520examples%2520retrieved%2520from%250Amultilingual%2520pre-trained%2520transformers%252C%2520have%2520shown%2520promise%2520in%2520mitigating%2520this%250Aissue.%2520However%252C%2520our%2520investigation%2520reveals%2520that%2520LLMs%2520intrinsically%2520reward%250Ain-language%2520semantically%2520aligned%2520cross-lingual%2520instances%2520over%2520direct%250Across-lingual%2520semantic%2520alignments%252C%2520with%2520a%2520pronounced%2520disparity%2520in%2520handling%250Atime-sensitive%2520queries%2520in%2520the%2520X-ICL%2520setup.%2520Such%2520queries%2520demand%2520sound%2520temporal%250Areasoning%2520ability%2520from%2520LLMs%252C%2520yet%2520the%2520advancements%2520have%2520predominantly%2520focused%2520on%250AEnglish.%2520This%2520study%2520aims%2520to%2520bridge%2520this%2520gap%2520by%2520improving%2520temporal%2520reasoning%250Acapabilities%2520in%2520low-resource%2520languages.%2520To%2520this%2520end%252C%2520we%2520introduce%2520mTEMPREASON%252C%250Aa%2520temporal%2520reasoning%2520dataset%2520aimed%2520at%2520the%2520varied%2520degrees%2520of%2520low-resource%250Alanguages%2520and%2520propose%2520Cross-Lingual%2520Time-Sensitive%2520Semantic%2520Alignment%250A%2528CLiTSSA%2529%252C%2520a%2520novel%2520method%2520to%2520improve%2520temporal%2520reasoning%2520in%2520these%2520contexts.%2520To%250Afacilitate%2520this%252C%2520we%2520construct%2520an%2520extension%2520of%2520mTEMPREASON%2520comprising%2520pairs%2520of%250Aparallel%2520cross-language%2520temporal%2520queries%2520along%2520with%2520their%2520anticipated%250Ain-language%2520semantic%2520similarity%2520scores.%2520Our%2520empirical%2520evidence%2520underscores%2520the%250Asuperior%2520performance%2520of%2520CLiTSSA%2520compared%2520to%2520established%2520baselines%2520across%2520three%250Alanguages%2520--%2520Romanian%252C%2520German%252C%2520and%2520French%252C%2520encompassing%2520three%2520temporal%2520tasks%250Aand%2520including%2520a%2520diverse%2520set%2520of%2520four%2520contemporaneous%2520LLMs.%2520This%2520marks%2520a%250Asignificant%2520step%2520forward%2520in%2520addressing%2520resource%2520disparity%2520in%2520the%2520context%2520of%250Atemporal%2520reasoning%2520across%2520languages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20LLMs%20Inherently%20Reward%20In-Language%20Time-Sensitive%20Semantic%0A%20%20Alignment%20for%20Low-Resource%20Languages&entry.906535625=Ashutosh%20Bajpai%20and%20Tanmoy%20Chakraborty&entry.1292438233=%20%20The%20unwavering%20disparity%20in%20labeled%20resources%20between%20resource-rich%20languages%0Aand%20those%20considered%20low-resource%20remains%20a%20significant%20impediment%20for%20Large%0ALanguage%20Models%20%28LLMs%29.%20Recent%20strides%20in%20cross-lingual%20in-context%20learning%0A%28X-ICL%29%2C%20mainly%20through%20semantically%20aligned%20examples%20retrieved%20from%0Amultilingual%20pre-trained%20transformers%2C%20have%20shown%20promise%20in%20mitigating%20this%0Aissue.%20However%2C%20our%20investigation%20reveals%20that%20LLMs%20intrinsically%20reward%0Ain-language%20semantically%20aligned%20cross-lingual%20instances%20over%20direct%0Across-lingual%20semantic%20alignments%2C%20with%20a%20pronounced%20disparity%20in%20handling%0Atime-sensitive%20queries%20in%20the%20X-ICL%20setup.%20Such%20queries%20demand%20sound%20temporal%0Areasoning%20ability%20from%20LLMs%2C%20yet%20the%20advancements%20have%20predominantly%20focused%20on%0AEnglish.%20This%20study%20aims%20to%20bridge%20this%20gap%20by%20improving%20temporal%20reasoning%0Acapabilities%20in%20low-resource%20languages.%20To%20this%20end%2C%20we%20introduce%20mTEMPREASON%2C%0Aa%20temporal%20reasoning%20dataset%20aimed%20at%20the%20varied%20degrees%20of%20low-resource%0Alanguages%20and%20propose%20Cross-Lingual%20Time-Sensitive%20Semantic%20Alignment%0A%28CLiTSSA%29%2C%20a%20novel%20method%20to%20improve%20temporal%20reasoning%20in%20these%20contexts.%20To%0Afacilitate%20this%2C%20we%20construct%20an%20extension%20of%20mTEMPREASON%20comprising%20pairs%20of%0Aparallel%20cross-language%20temporal%20queries%20along%20with%20their%20anticipated%0Ain-language%20semantic%20similarity%20scores.%20Our%20empirical%20evidence%20underscores%20the%0Asuperior%20performance%20of%20CLiTSSA%20compared%20to%20established%20baselines%20across%20three%0Alanguages%20--%20Romanian%2C%20German%2C%20and%20French%2C%20encompassing%20three%20temporal%20tasks%0Aand%20including%20a%20diverse%20set%20of%20four%20contemporaneous%20LLMs.%20This%20marks%20a%0Asignificant%20step%20forward%20in%20addressing%20resource%20disparity%20in%20the%20context%20of%0Atemporal%20reasoning%20across%20languages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08090v2&entry.124074799=Read"},
{"title": "Disentangling Visual Transformers: Patch-level Interpretability for\n  Image Classification", "author": "Guillaume Jeanneret and Lo\u00efc Simon and Fr\u00e9d\u00e9ric Jurie", "abstract": "  Visual transformers have achieved remarkable performance in image\nclassification tasks, but this performance gain has come at the cost of\ninterpretability. One of the main obstacles to the interpretation of\ntransformers is the self-attention mechanism, which mixes visual information\nacross the whole image in a complex way. In this paper, we propose Hindered\nTransformer (HiT), a novel interpretable by design architecture inspired by\nvisual transformers. Our proposed architecture rethinks the design of\ntransformers to better disentangle patch influences at the classification\nstage. Ultimately, HiT can be interpreted as a linear combination of\npatch-level information. We show that the advantages of our approach in terms\nof explicability come with a reasonable trade-off in performance, making it an\nattractive alternative for applications where interpretability is paramount.\n", "link": "http://arxiv.org/abs/2502.17196v1", "date": "2025-02-24", "relevancy": 2.0681, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5181}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Visual%20Transformers%3A%20Patch-level%20Interpretability%20for%0A%20%20Image%20Classification&body=Title%3A%20Disentangling%20Visual%20Transformers%3A%20Patch-level%20Interpretability%20for%0A%20%20Image%20Classification%0AAuthor%3A%20Guillaume%20Jeanneret%20and%20Lo%C3%AFc%20Simon%20and%20Fr%C3%A9d%C3%A9ric%20Jurie%0AAbstract%3A%20%20%20Visual%20transformers%20have%20achieved%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20but%20this%20performance%20gain%20has%20come%20at%20the%20cost%20of%0Ainterpretability.%20One%20of%20the%20main%20obstacles%20to%20the%20interpretation%20of%0Atransformers%20is%20the%20self-attention%20mechanism%2C%20which%20mixes%20visual%20information%0Aacross%20the%20whole%20image%20in%20a%20complex%20way.%20In%20this%20paper%2C%20we%20propose%20Hindered%0ATransformer%20%28HiT%29%2C%20a%20novel%20interpretable%20by%20design%20architecture%20inspired%20by%0Avisual%20transformers.%20Our%20proposed%20architecture%20rethinks%20the%20design%20of%0Atransformers%20to%20better%20disentangle%20patch%20influences%20at%20the%20classification%0Astage.%20Ultimately%2C%20HiT%20can%20be%20interpreted%20as%20a%20linear%20combination%20of%0Apatch-level%20information.%20We%20show%20that%20the%20advantages%20of%20our%20approach%20in%20terms%0Aof%20explicability%20come%20with%20a%20reasonable%20trade-off%20in%20performance%2C%20making%20it%20an%0Aattractive%20alternative%20for%20applications%20where%20interpretability%20is%20paramount.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17196v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Visual%2520Transformers%253A%2520Patch-level%2520Interpretability%2520for%250A%2520%2520Image%2520Classification%26entry.906535625%3DGuillaume%2520Jeanneret%2520and%2520Lo%25C3%25AFc%2520Simon%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Jurie%26entry.1292438233%3D%2520%2520Visual%2520transformers%2520have%2520achieved%2520remarkable%2520performance%2520in%2520image%250Aclassification%2520tasks%252C%2520but%2520this%2520performance%2520gain%2520has%2520come%2520at%2520the%2520cost%2520of%250Ainterpretability.%2520One%2520of%2520the%2520main%2520obstacles%2520to%2520the%2520interpretation%2520of%250Atransformers%2520is%2520the%2520self-attention%2520mechanism%252C%2520which%2520mixes%2520visual%2520information%250Aacross%2520the%2520whole%2520image%2520in%2520a%2520complex%2520way.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Hindered%250ATransformer%2520%2528HiT%2529%252C%2520a%2520novel%2520interpretable%2520by%2520design%2520architecture%2520inspired%2520by%250Avisual%2520transformers.%2520Our%2520proposed%2520architecture%2520rethinks%2520the%2520design%2520of%250Atransformers%2520to%2520better%2520disentangle%2520patch%2520influences%2520at%2520the%2520classification%250Astage.%2520Ultimately%252C%2520HiT%2520can%2520be%2520interpreted%2520as%2520a%2520linear%2520combination%2520of%250Apatch-level%2520information.%2520We%2520show%2520that%2520the%2520advantages%2520of%2520our%2520approach%2520in%2520terms%250Aof%2520explicability%2520come%2520with%2520a%2520reasonable%2520trade-off%2520in%2520performance%252C%2520making%2520it%2520an%250Aattractive%2520alternative%2520for%2520applications%2520where%2520interpretability%2520is%2520paramount.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17196v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Visual%20Transformers%3A%20Patch-level%20Interpretability%20for%0A%20%20Image%20Classification&entry.906535625=Guillaume%20Jeanneret%20and%20Lo%C3%AFc%20Simon%20and%20Fr%C3%A9d%C3%A9ric%20Jurie&entry.1292438233=%20%20Visual%20transformers%20have%20achieved%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20but%20this%20performance%20gain%20has%20come%20at%20the%20cost%20of%0Ainterpretability.%20One%20of%20the%20main%20obstacles%20to%20the%20interpretation%20of%0Atransformers%20is%20the%20self-attention%20mechanism%2C%20which%20mixes%20visual%20information%0Aacross%20the%20whole%20image%20in%20a%20complex%20way.%20In%20this%20paper%2C%20we%20propose%20Hindered%0ATransformer%20%28HiT%29%2C%20a%20novel%20interpretable%20by%20design%20architecture%20inspired%20by%0Avisual%20transformers.%20Our%20proposed%20architecture%20rethinks%20the%20design%20of%0Atransformers%20to%20better%20disentangle%20patch%20influences%20at%20the%20classification%0Astage.%20Ultimately%2C%20HiT%20can%20be%20interpreted%20as%20a%20linear%20combination%20of%0Apatch-level%20information.%20We%20show%20that%20the%20advantages%20of%20our%20approach%20in%20terms%0Aof%20explicability%20come%20with%20a%20reasonable%20trade-off%20in%20performance%2C%20making%20it%20an%0Aattractive%20alternative%20for%20applications%20where%20interpretability%20is%20paramount.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17196v1&entry.124074799=Read"},
{"title": "CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought", "author": "Boxuan Zhang and Ruqi Zhang", "abstract": "  Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ.\n", "link": "http://arxiv.org/abs/2502.17214v1", "date": "2025-02-24", "relevancy": 2.0653, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5805}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5382}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-UQ%3A%20Improving%20Response-wise%20Uncertainty%20Quantification%20in%20LLMs%20with%0A%20%20Chain-of-Thought&body=Title%3A%20CoT-UQ%3A%20Improving%20Response-wise%20Uncertainty%20Quantification%20in%20LLMs%20with%0A%20%20Chain-of-Thought%0AAuthor%3A%20Boxuan%20Zhang%20and%20Ruqi%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20many%20tasks%20but%20struggle%20to%20accurately%0Aquantify%20uncertainty%20in%20their%20generated%20responses.%20This%20limitation%20makes%20it%0Achallenging%20to%20detect%20misinformation%20and%20ensure%20reliable%20decision-making.%0AExisting%20uncertainty%20quantification%20%28UQ%29%20methods%20for%20LLMs%20are%20primarily%0Aprompt-wise%20rather%20than%20response-wise%2C%20often%20requiring%20multiple%20response%0Asamples%2C%20which%20incurs%20high%20computational%20costs.%20Moreover%2C%20LLMs%20have%20been%20shown%0Ato%20be%20overconfident%2C%20particularly%20when%20using%20reasoning%20steps%20to%20derive%20their%0Aanswers.%20In%20this%20work%2C%20we%20propose%20CoT-UQ%2C%20a%20response-wise%20UQ%20framework%20that%0Aintegrates%20LLMs%27%20inherent%20reasoning%20capabilities%20through%20Chain-of-Thought%20%28CoT%29%0Ainto%20the%20UQ%20process.%20CoT-UQ%20captures%20critical%20information%20during%20inference%20by%0Aextracting%20keywords%20from%20each%20reasoning%20step%20and%20assessing%20their%20importance%20to%0Athe%20final%20answer.%20This%20key%20reasoning%20information%20is%20then%20aggregated%20to%20produce%0Aa%20final%20uncertainty%20estimate.%20We%20conduct%20extensive%20experiments%20based%20on%20LLaMA%0AFamily%20with%20model%20sizes%20varying%20from%208B%20to%2013B%20across%20logical%20and%20mathematical%0Areasoning%20tasks.%20Experimental%20results%20demonstrate%20that%20CoT-UQ%20significantly%0Aoutperforms%20existing%20UQ%20methods%2C%20achieving%20an%20average%20improvement%20of%205.9%25%20AUROC%0Acompared%20to%20current%20UQ%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZBox1005/CoT-UQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-UQ%253A%2520Improving%2520Response-wise%2520Uncertainty%2520Quantification%2520in%2520LLMs%2520with%250A%2520%2520Chain-of-Thought%26entry.906535625%3DBoxuan%2520Zhang%2520and%2520Ruqi%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520in%2520many%2520tasks%2520but%2520struggle%2520to%2520accurately%250Aquantify%2520uncertainty%2520in%2520their%2520generated%2520responses.%2520This%2520limitation%2520makes%2520it%250Achallenging%2520to%2520detect%2520misinformation%2520and%2520ensure%2520reliable%2520decision-making.%250AExisting%2520uncertainty%2520quantification%2520%2528UQ%2529%2520methods%2520for%2520LLMs%2520are%2520primarily%250Aprompt-wise%2520rather%2520than%2520response-wise%252C%2520often%2520requiring%2520multiple%2520response%250Asamples%252C%2520which%2520incurs%2520high%2520computational%2520costs.%2520Moreover%252C%2520LLMs%2520have%2520been%2520shown%250Ato%2520be%2520overconfident%252C%2520particularly%2520when%2520using%2520reasoning%2520steps%2520to%2520derive%2520their%250Aanswers.%2520In%2520this%2520work%252C%2520we%2520propose%2520CoT-UQ%252C%2520a%2520response-wise%2520UQ%2520framework%2520that%250Aintegrates%2520LLMs%2527%2520inherent%2520reasoning%2520capabilities%2520through%2520Chain-of-Thought%2520%2528CoT%2529%250Ainto%2520the%2520UQ%2520process.%2520CoT-UQ%2520captures%2520critical%2520information%2520during%2520inference%2520by%250Aextracting%2520keywords%2520from%2520each%2520reasoning%2520step%2520and%2520assessing%2520their%2520importance%2520to%250Athe%2520final%2520answer.%2520This%2520key%2520reasoning%2520information%2520is%2520then%2520aggregated%2520to%2520produce%250Aa%2520final%2520uncertainty%2520estimate.%2520We%2520conduct%2520extensive%2520experiments%2520based%2520on%2520LLaMA%250AFamily%2520with%2520model%2520sizes%2520varying%2520from%25208B%2520to%252013B%2520across%2520logical%2520and%2520mathematical%250Areasoning%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520CoT-UQ%2520significantly%250Aoutperforms%2520existing%2520UQ%2520methods%252C%2520achieving%2520an%2520average%2520improvement%2520of%25205.9%2525%2520AUROC%250Acompared%2520to%2520current%2520UQ%2520methods.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ZBox1005/CoT-UQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-UQ%3A%20Improving%20Response-wise%20Uncertainty%20Quantification%20in%20LLMs%20with%0A%20%20Chain-of-Thought&entry.906535625=Boxuan%20Zhang%20and%20Ruqi%20Zhang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20in%20many%20tasks%20but%20struggle%20to%20accurately%0Aquantify%20uncertainty%20in%20their%20generated%20responses.%20This%20limitation%20makes%20it%0Achallenging%20to%20detect%20misinformation%20and%20ensure%20reliable%20decision-making.%0AExisting%20uncertainty%20quantification%20%28UQ%29%20methods%20for%20LLMs%20are%20primarily%0Aprompt-wise%20rather%20than%20response-wise%2C%20often%20requiring%20multiple%20response%0Asamples%2C%20which%20incurs%20high%20computational%20costs.%20Moreover%2C%20LLMs%20have%20been%20shown%0Ato%20be%20overconfident%2C%20particularly%20when%20using%20reasoning%20steps%20to%20derive%20their%0Aanswers.%20In%20this%20work%2C%20we%20propose%20CoT-UQ%2C%20a%20response-wise%20UQ%20framework%20that%0Aintegrates%20LLMs%27%20inherent%20reasoning%20capabilities%20through%20Chain-of-Thought%20%28CoT%29%0Ainto%20the%20UQ%20process.%20CoT-UQ%20captures%20critical%20information%20during%20inference%20by%0Aextracting%20keywords%20from%20each%20reasoning%20step%20and%20assessing%20their%20importance%20to%0Athe%20final%20answer.%20This%20key%20reasoning%20information%20is%20then%20aggregated%20to%20produce%0Aa%20final%20uncertainty%20estimate.%20We%20conduct%20extensive%20experiments%20based%20on%20LLaMA%0AFamily%20with%20model%20sizes%20varying%20from%208B%20to%2013B%20across%20logical%20and%20mathematical%0Areasoning%20tasks.%20Experimental%20results%20demonstrate%20that%20CoT-UQ%20significantly%0Aoutperforms%20existing%20UQ%20methods%2C%20achieving%20an%20average%20improvement%20of%205.9%25%20AUROC%0Acompared%20to%20current%20UQ%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/ZBox1005/CoT-UQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17214v1&entry.124074799=Read"},
{"title": "Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches", "author": "Alexander Beiser and David Penz", "abstract": "  Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset.\n", "link": "http://arxiv.org/abs/2502.17216v1", "date": "2025-02-24", "relevancy": 2.0627, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5251}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Making%20LLMs%20Reason%3F%20The%20Intermediate%20Language%20Problem%20in%20Neurosymbolic%0A%20%20Approaches&body=Title%3A%20Making%20LLMs%20Reason%3F%20The%20Intermediate%20Language%20Problem%20in%20Neurosymbolic%0A%20%20Approaches%0AAuthor%3A%20Alexander%20Beiser%20and%20David%20Penz%0AAbstract%3A%20%20%20Logical%20reasoning%20tasks%20manifest%20themselves%20as%20a%20challenge%20to%20Large%20Language%0AModels%20%28LLMs%29.%20Neurosymbolic%20approaches%20use%20LLMs%20to%20translate%20logical%20reasoning%0Aproblems%20formulated%20in%20natural%20language%20into%20a%20formal%20intermediate%20language.%0ASubsequently%2C%20the%20usage%20of%20symbolic%20reasoners%20yields%20reliable%20solving%20thereof.%0AHowever%2C%20LLMs%20often%20fail%20in%20translation%20due%20to%20poorly%20chosen%20intermediate%0Alanguages.%0A%20%20We%20introduce%20the%20intermediate%20language%20problem%2C%20which%20is%20the%20problem%20of%0Achoosing%20a%20suitable%20formal%20language%20representation%20for%20neurosymbolic%0Aapproaches.%20Theoretically%2C%20we%20argue%20that%20its%20origins%20lie%20in%20the%20inability%20of%0ALLMs%20to%20distinguish%20syntax%20from%20semantics%20and%20the%20relative%20independence%20of%20the%0Aproblem%20from%20its%20representation.%20We%20showcase%20its%20existence%20experimentally%20by%0Acontrasting%20two%20intermediate%20languages%2C%20Answer%20Set%20Programming%20and%20the%20Python%0AKnowledge%20Engine.%20In%20addition%2C%20we%20demonstrate%20the%20effects%20of%20varying%20degrees%20of%0Asupplementary%20context%20information.%20Our%20results%20show%20a%20maximum%20difference%20in%0Aoverall-accuracy%20of%2053.20%25%20and%2049.26%25%20in%20execution-accuracy.%20When%20using%20the%0AGPT4o-mini%20LLM%20we%20beat%20the%20state-of-the-art%20in%20overall-accuracy%20on%20the%20ProntoQA%0Adataset%20by%2021.20%25%20and%20by%2050.50%25%20on%20the%20ProofWriter%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaking%2520LLMs%2520Reason%253F%2520The%2520Intermediate%2520Language%2520Problem%2520in%2520Neurosymbolic%250A%2520%2520Approaches%26entry.906535625%3DAlexander%2520Beiser%2520and%2520David%2520Penz%26entry.1292438233%3D%2520%2520Logical%2520reasoning%2520tasks%2520manifest%2520themselves%2520as%2520a%2520challenge%2520to%2520Large%2520Language%250AModels%2520%2528LLMs%2529.%2520Neurosymbolic%2520approaches%2520use%2520LLMs%2520to%2520translate%2520logical%2520reasoning%250Aproblems%2520formulated%2520in%2520natural%2520language%2520into%2520a%2520formal%2520intermediate%2520language.%250ASubsequently%252C%2520the%2520usage%2520of%2520symbolic%2520reasoners%2520yields%2520reliable%2520solving%2520thereof.%250AHowever%252C%2520LLMs%2520often%2520fail%2520in%2520translation%2520due%2520to%2520poorly%2520chosen%2520intermediate%250Alanguages.%250A%2520%2520We%2520introduce%2520the%2520intermediate%2520language%2520problem%252C%2520which%2520is%2520the%2520problem%2520of%250Achoosing%2520a%2520suitable%2520formal%2520language%2520representation%2520for%2520neurosymbolic%250Aapproaches.%2520Theoretically%252C%2520we%2520argue%2520that%2520its%2520origins%2520lie%2520in%2520the%2520inability%2520of%250ALLMs%2520to%2520distinguish%2520syntax%2520from%2520semantics%2520and%2520the%2520relative%2520independence%2520of%2520the%250Aproblem%2520from%2520its%2520representation.%2520We%2520showcase%2520its%2520existence%2520experimentally%2520by%250Acontrasting%2520two%2520intermediate%2520languages%252C%2520Answer%2520Set%2520Programming%2520and%2520the%2520Python%250AKnowledge%2520Engine.%2520In%2520addition%252C%2520we%2520demonstrate%2520the%2520effects%2520of%2520varying%2520degrees%2520of%250Asupplementary%2520context%2520information.%2520Our%2520results%2520show%2520a%2520maximum%2520difference%2520in%250Aoverall-accuracy%2520of%252053.20%2525%2520and%252049.26%2525%2520in%2520execution-accuracy.%2520When%2520using%2520the%250AGPT4o-mini%2520LLM%2520we%2520beat%2520the%2520state-of-the-art%2520in%2520overall-accuracy%2520on%2520the%2520ProntoQA%250Adataset%2520by%252021.20%2525%2520and%2520by%252050.50%2525%2520on%2520the%2520ProofWriter%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20LLMs%20Reason%3F%20The%20Intermediate%20Language%20Problem%20in%20Neurosymbolic%0A%20%20Approaches&entry.906535625=Alexander%20Beiser%20and%20David%20Penz&entry.1292438233=%20%20Logical%20reasoning%20tasks%20manifest%20themselves%20as%20a%20challenge%20to%20Large%20Language%0AModels%20%28LLMs%29.%20Neurosymbolic%20approaches%20use%20LLMs%20to%20translate%20logical%20reasoning%0Aproblems%20formulated%20in%20natural%20language%20into%20a%20formal%20intermediate%20language.%0ASubsequently%2C%20the%20usage%20of%20symbolic%20reasoners%20yields%20reliable%20solving%20thereof.%0AHowever%2C%20LLMs%20often%20fail%20in%20translation%20due%20to%20poorly%20chosen%20intermediate%0Alanguages.%0A%20%20We%20introduce%20the%20intermediate%20language%20problem%2C%20which%20is%20the%20problem%20of%0Achoosing%20a%20suitable%20formal%20language%20representation%20for%20neurosymbolic%0Aapproaches.%20Theoretically%2C%20we%20argue%20that%20its%20origins%20lie%20in%20the%20inability%20of%0ALLMs%20to%20distinguish%20syntax%20from%20semantics%20and%20the%20relative%20independence%20of%20the%0Aproblem%20from%20its%20representation.%20We%20showcase%20its%20existence%20experimentally%20by%0Acontrasting%20two%20intermediate%20languages%2C%20Answer%20Set%20Programming%20and%20the%20Python%0AKnowledge%20Engine.%20In%20addition%2C%20we%20demonstrate%20the%20effects%20of%20varying%20degrees%20of%0Asupplementary%20context%20information.%20Our%20results%20show%20a%20maximum%20difference%20in%0Aoverall-accuracy%20of%2053.20%25%20and%2049.26%25%20in%20execution-accuracy.%20When%20using%20the%0AGPT4o-mini%20LLM%20we%20beat%20the%20state-of-the-art%20in%20overall-accuracy%20on%20the%20ProntoQA%0Adataset%20by%2021.20%25%20and%20by%2050.50%25%20on%20the%20ProofWriter%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17216v1&entry.124074799=Read"},
{"title": "LLM-QE: Improving Query Expansion by Aligning Large Language Models with\n  Ranking Preferences", "author": "Sijia Yao and Pengcheng Huang and Zhenghao Liu and Yu Gu and Yukun Yan and Shi Yu and Ge Yu", "abstract": "  Query expansion plays a crucial role in information retrieval, which aims to\nbridge the semantic gap between queries and documents to improve matching\nperformance. This paper introduces LLM-QE, a novel approach that leverages\nLarge Language Models (LLMs) to generate document-based query expansions,\nthereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE\ndesigns both rank-based and answer-based rewards and uses these reward models\nto optimize LLMs to align with the ranking preferences of both retrievers and\nLLMs, thus mitigating the hallucination of LLMs during query expansion. Our\nexperiments on the zero-shot dense retrieval model, Contriever, demonstrate the\neffectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by\nincorporating answer-based reward modeling, LLM-QE generates more relevant and\nprecise information related to the documents, rather than simply producing\nredundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves\nthe training process of dense retrievers, achieving a more than 5% improvement\nafter fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE.\n", "link": "http://arxiv.org/abs/2502.17057v1", "date": "2025-02-24", "relevancy": 2.0545, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-QE%3A%20Improving%20Query%20Expansion%20by%20Aligning%20Large%20Language%20Models%20with%0A%20%20Ranking%20Preferences&body=Title%3A%20LLM-QE%3A%20Improving%20Query%20Expansion%20by%20Aligning%20Large%20Language%20Models%20with%0A%20%20Ranking%20Preferences%0AAuthor%3A%20Sijia%20Yao%20and%20Pengcheng%20Huang%20and%20Zhenghao%20Liu%20and%20Yu%20Gu%20and%20Yukun%20Yan%20and%20Shi%20Yu%20and%20Ge%20Yu%0AAbstract%3A%20%20%20Query%20expansion%20plays%20a%20crucial%20role%20in%20information%20retrieval%2C%20which%20aims%20to%0Abridge%20the%20semantic%20gap%20between%20queries%20and%20documents%20to%20improve%20matching%0Aperformance.%20This%20paper%20introduces%20LLM-QE%2C%20a%20novel%20approach%20that%20leverages%0ALarge%20Language%20Models%20%28LLMs%29%20to%20generate%20document-based%20query%20expansions%2C%0Athereby%20enhancing%20dense%20retrieval%20models.%20Unlike%20traditional%20methods%2C%20LLM-QE%0Adesigns%20both%20rank-based%20and%20answer-based%20rewards%20and%20uses%20these%20reward%20models%0Ato%20optimize%20LLMs%20to%20align%20with%20the%20ranking%20preferences%20of%20both%20retrievers%20and%0ALLMs%2C%20thus%20mitigating%20the%20hallucination%20of%20LLMs%20during%20query%20expansion.%20Our%0Aexperiments%20on%20the%20zero-shot%20dense%20retrieval%20model%2C%20Contriever%2C%20demonstrate%20the%0Aeffectiveness%20of%20LLM-QE%2C%20achieving%20an%20improvement%20of%20over%208%25.%20Furthermore%2C%20by%0Aincorporating%20answer-based%20reward%20modeling%2C%20LLM-QE%20generates%20more%20relevant%20and%0Aprecise%20information%20related%20to%20the%20documents%2C%20rather%20than%20simply%20producing%0Aredundant%20tokens%20to%20maximize%20rank-based%20rewards.%20Notably%2C%20LLM-QE%20also%20improves%0Athe%20training%20process%20of%20dense%20retrievers%2C%20achieving%20a%20more%20than%205%25%20improvement%0Aafter%20fine-tuning.%20All%20codes%20are%20available%20at%20https%3A//github.com/NEUIR/LLM-QE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17057v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-QE%253A%2520Improving%2520Query%2520Expansion%2520by%2520Aligning%2520Large%2520Language%2520Models%2520with%250A%2520%2520Ranking%2520Preferences%26entry.906535625%3DSijia%2520Yao%2520and%2520Pengcheng%2520Huang%2520and%2520Zhenghao%2520Liu%2520and%2520Yu%2520Gu%2520and%2520Yukun%2520Yan%2520and%2520Shi%2520Yu%2520and%2520Ge%2520Yu%26entry.1292438233%3D%2520%2520Query%2520expansion%2520plays%2520a%2520crucial%2520role%2520in%2520information%2520retrieval%252C%2520which%2520aims%2520to%250Abridge%2520the%2520semantic%2520gap%2520between%2520queries%2520and%2520documents%2520to%2520improve%2520matching%250Aperformance.%2520This%2520paper%2520introduces%2520LLM-QE%252C%2520a%2520novel%2520approach%2520that%2520leverages%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520generate%2520document-based%2520query%2520expansions%252C%250Athereby%2520enhancing%2520dense%2520retrieval%2520models.%2520Unlike%2520traditional%2520methods%252C%2520LLM-QE%250Adesigns%2520both%2520rank-based%2520and%2520answer-based%2520rewards%2520and%2520uses%2520these%2520reward%2520models%250Ato%2520optimize%2520LLMs%2520to%2520align%2520with%2520the%2520ranking%2520preferences%2520of%2520both%2520retrievers%2520and%250ALLMs%252C%2520thus%2520mitigating%2520the%2520hallucination%2520of%2520LLMs%2520during%2520query%2520expansion.%2520Our%250Aexperiments%2520on%2520the%2520zero-shot%2520dense%2520retrieval%2520model%252C%2520Contriever%252C%2520demonstrate%2520the%250Aeffectiveness%2520of%2520LLM-QE%252C%2520achieving%2520an%2520improvement%2520of%2520over%25208%2525.%2520Furthermore%252C%2520by%250Aincorporating%2520answer-based%2520reward%2520modeling%252C%2520LLM-QE%2520generates%2520more%2520relevant%2520and%250Aprecise%2520information%2520related%2520to%2520the%2520documents%252C%2520rather%2520than%2520simply%2520producing%250Aredundant%2520tokens%2520to%2520maximize%2520rank-based%2520rewards.%2520Notably%252C%2520LLM-QE%2520also%2520improves%250Athe%2520training%2520process%2520of%2520dense%2520retrievers%252C%2520achieving%2520a%2520more%2520than%25205%2525%2520improvement%250Aafter%2520fine-tuning.%2520All%2520codes%2520are%2520available%2520at%2520https%253A//github.com/NEUIR/LLM-QE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17057v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-QE%3A%20Improving%20Query%20Expansion%20by%20Aligning%20Large%20Language%20Models%20with%0A%20%20Ranking%20Preferences&entry.906535625=Sijia%20Yao%20and%20Pengcheng%20Huang%20and%20Zhenghao%20Liu%20and%20Yu%20Gu%20and%20Yukun%20Yan%20and%20Shi%20Yu%20and%20Ge%20Yu&entry.1292438233=%20%20Query%20expansion%20plays%20a%20crucial%20role%20in%20information%20retrieval%2C%20which%20aims%20to%0Abridge%20the%20semantic%20gap%20between%20queries%20and%20documents%20to%20improve%20matching%0Aperformance.%20This%20paper%20introduces%20LLM-QE%2C%20a%20novel%20approach%20that%20leverages%0ALarge%20Language%20Models%20%28LLMs%29%20to%20generate%20document-based%20query%20expansions%2C%0Athereby%20enhancing%20dense%20retrieval%20models.%20Unlike%20traditional%20methods%2C%20LLM-QE%0Adesigns%20both%20rank-based%20and%20answer-based%20rewards%20and%20uses%20these%20reward%20models%0Ato%20optimize%20LLMs%20to%20align%20with%20the%20ranking%20preferences%20of%20both%20retrievers%20and%0ALLMs%2C%20thus%20mitigating%20the%20hallucination%20of%20LLMs%20during%20query%20expansion.%20Our%0Aexperiments%20on%20the%20zero-shot%20dense%20retrieval%20model%2C%20Contriever%2C%20demonstrate%20the%0Aeffectiveness%20of%20LLM-QE%2C%20achieving%20an%20improvement%20of%20over%208%25.%20Furthermore%2C%20by%0Aincorporating%20answer-based%20reward%20modeling%2C%20LLM-QE%20generates%20more%20relevant%20and%0Aprecise%20information%20related%20to%20the%20documents%2C%20rather%20than%20simply%20producing%0Aredundant%20tokens%20to%20maximize%20rank-based%20rewards.%20Notably%2C%20LLM-QE%20also%20improves%0Athe%20training%20process%20of%20dense%20retrievers%2C%20achieving%20a%20more%20than%205%25%20improvement%0Aafter%20fine-tuning.%20All%20codes%20are%20available%20at%20https%3A//github.com/NEUIR/LLM-QE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17057v1&entry.124074799=Read"},
{"title": "Delta Decompression for MoE-based LLMs Compression", "author": "Hao Gu and Wei Li and Lujun Li and Qiyuan Zhu and Mark Lee and Shengjie Sun and Wei Xue and Yike Guo", "abstract": "  Mixture-of-Experts (MoE) architectures in large language models (LLMs)\nachieve exceptional performance, but face prohibitive storage and memory\nrequirements. To address these challenges, we present $D^2$-MoE, a new delta\ndecompression compressor for reducing the parameters of MoE LLMs. Based on\nobservations of expert diversity, we decompose their weights into a shared base\nweight and unique delta weights. Specifically, our method first merges each\nexpert's weight into the base weight using the Fisher information matrix to\ncapture shared components. Then, we compress delta weights through Singular\nValue Decomposition (SVD) by exploiting their low-rank properties. Finally, we\nintroduce a semi-dynamical structured pruning strategy for the base weights,\ncombining static and dynamic redundancy analysis to achieve further parameter\nreduction while maintaining input adaptivity. In this way, our $D^2$-MoE\nsuccessfully compact MoE LLMs to high compression ratios without additional\ntraining. Extensive experiments highlight the superiority of our approach, with\nover 13% performance gains than other compressors on\nMixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes\nare available in https://github.com/lliai/D2MoE.\n", "link": "http://arxiv.org/abs/2502.17298v1", "date": "2025-02-24", "relevancy": 1.9596, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5039}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delta%20Decompression%20for%20MoE-based%20LLMs%20Compression&body=Title%3A%20Delta%20Decompression%20for%20MoE-based%20LLMs%20Compression%0AAuthor%3A%20Hao%20Gu%20and%20Wei%20Li%20and%20Lujun%20Li%20and%20Qiyuan%20Zhu%20and%20Mark%20Lee%20and%20Shengjie%20Sun%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20architectures%20in%20large%20language%20models%20%28LLMs%29%0Aachieve%20exceptional%20performance%2C%20but%20face%20prohibitive%20storage%20and%20memory%0Arequirements.%20To%20address%20these%20challenges%2C%20we%20present%20%24D%5E2%24-MoE%2C%20a%20new%20delta%0Adecompression%20compressor%20for%20reducing%20the%20parameters%20of%20MoE%20LLMs.%20Based%20on%0Aobservations%20of%20expert%20diversity%2C%20we%20decompose%20their%20weights%20into%20a%20shared%20base%0Aweight%20and%20unique%20delta%20weights.%20Specifically%2C%20our%20method%20first%20merges%20each%0Aexpert%27s%20weight%20into%20the%20base%20weight%20using%20the%20Fisher%20information%20matrix%20to%0Acapture%20shared%20components.%20Then%2C%20we%20compress%20delta%20weights%20through%20Singular%0AValue%20Decomposition%20%28SVD%29%20by%20exploiting%20their%20low-rank%20properties.%20Finally%2C%20we%0Aintroduce%20a%20semi-dynamical%20structured%20pruning%20strategy%20for%20the%20base%20weights%2C%0Acombining%20static%20and%20dynamic%20redundancy%20analysis%20to%20achieve%20further%20parameter%0Areduction%20while%20maintaining%20input%20adaptivity.%20In%20this%20way%2C%20our%20%24D%5E2%24-MoE%0Asuccessfully%20compact%20MoE%20LLMs%20to%20high%20compression%20ratios%20without%20additional%0Atraining.%20Extensive%20experiments%20highlight%20the%20superiority%20of%20our%20approach%2C%20with%0Aover%2013%25%20performance%20gains%20than%20other%20compressors%20on%0AMixtral%7CPhi-3.5%7CDeepSeek%7CQwen2%20MoE%20LLMs%20at%2040%24%5Csim%2460%25%20compression%20rates.%20Codes%0Aare%20available%20in%20https%3A//github.com/lliai/D2MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelta%2520Decompression%2520for%2520MoE-based%2520LLMs%2520Compression%26entry.906535625%3DHao%2520Gu%2520and%2520Wei%2520Li%2520and%2520Lujun%2520Li%2520and%2520Qiyuan%2520Zhu%2520and%2520Mark%2520Lee%2520and%2520Shengjie%2520Sun%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520architectures%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%250Aachieve%2520exceptional%2520performance%252C%2520but%2520face%2520prohibitive%2520storage%2520and%2520memory%250Arequirements.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520%2524D%255E2%2524-MoE%252C%2520a%2520new%2520delta%250Adecompression%2520compressor%2520for%2520reducing%2520the%2520parameters%2520of%2520MoE%2520LLMs.%2520Based%2520on%250Aobservations%2520of%2520expert%2520diversity%252C%2520we%2520decompose%2520their%2520weights%2520into%2520a%2520shared%2520base%250Aweight%2520and%2520unique%2520delta%2520weights.%2520Specifically%252C%2520our%2520method%2520first%2520merges%2520each%250Aexpert%2527s%2520weight%2520into%2520the%2520base%2520weight%2520using%2520the%2520Fisher%2520information%2520matrix%2520to%250Acapture%2520shared%2520components.%2520Then%252C%2520we%2520compress%2520delta%2520weights%2520through%2520Singular%250AValue%2520Decomposition%2520%2528SVD%2529%2520by%2520exploiting%2520their%2520low-rank%2520properties.%2520Finally%252C%2520we%250Aintroduce%2520a%2520semi-dynamical%2520structured%2520pruning%2520strategy%2520for%2520the%2520base%2520weights%252C%250Acombining%2520static%2520and%2520dynamic%2520redundancy%2520analysis%2520to%2520achieve%2520further%2520parameter%250Areduction%2520while%2520maintaining%2520input%2520adaptivity.%2520In%2520this%2520way%252C%2520our%2520%2524D%255E2%2524-MoE%250Asuccessfully%2520compact%2520MoE%2520LLMs%2520to%2520high%2520compression%2520ratios%2520without%2520additional%250Atraining.%2520Extensive%2520experiments%2520highlight%2520the%2520superiority%2520of%2520our%2520approach%252C%2520with%250Aover%252013%2525%2520performance%2520gains%2520than%2520other%2520compressors%2520on%250AMixtral%257CPhi-3.5%257CDeepSeek%257CQwen2%2520MoE%2520LLMs%2520at%252040%2524%255Csim%252460%2525%2520compression%2520rates.%2520Codes%250Aare%2520available%2520in%2520https%253A//github.com/lliai/D2MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delta%20Decompression%20for%20MoE-based%20LLMs%20Compression&entry.906535625=Hao%20Gu%20and%20Wei%20Li%20and%20Lujun%20Li%20and%20Qiyuan%20Zhu%20and%20Mark%20Lee%20and%20Shengjie%20Sun%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20architectures%20in%20large%20language%20models%20%28LLMs%29%0Aachieve%20exceptional%20performance%2C%20but%20face%20prohibitive%20storage%20and%20memory%0Arequirements.%20To%20address%20these%20challenges%2C%20we%20present%20%24D%5E2%24-MoE%2C%20a%20new%20delta%0Adecompression%20compressor%20for%20reducing%20the%20parameters%20of%20MoE%20LLMs.%20Based%20on%0Aobservations%20of%20expert%20diversity%2C%20we%20decompose%20their%20weights%20into%20a%20shared%20base%0Aweight%20and%20unique%20delta%20weights.%20Specifically%2C%20our%20method%20first%20merges%20each%0Aexpert%27s%20weight%20into%20the%20base%20weight%20using%20the%20Fisher%20information%20matrix%20to%0Acapture%20shared%20components.%20Then%2C%20we%20compress%20delta%20weights%20through%20Singular%0AValue%20Decomposition%20%28SVD%29%20by%20exploiting%20their%20low-rank%20properties.%20Finally%2C%20we%0Aintroduce%20a%20semi-dynamical%20structured%20pruning%20strategy%20for%20the%20base%20weights%2C%0Acombining%20static%20and%20dynamic%20redundancy%20analysis%20to%20achieve%20further%20parameter%0Areduction%20while%20maintaining%20input%20adaptivity.%20In%20this%20way%2C%20our%20%24D%5E2%24-MoE%0Asuccessfully%20compact%20MoE%20LLMs%20to%20high%20compression%20ratios%20without%20additional%0Atraining.%20Extensive%20experiments%20highlight%20the%20superiority%20of%20our%20approach%2C%20with%0Aover%2013%25%20performance%20gains%20than%20other%20compressors%20on%0AMixtral%7CPhi-3.5%7CDeepSeek%7CQwen2%20MoE%20LLMs%20at%2040%24%5Csim%2460%25%20compression%20rates.%20Codes%0Aare%20available%20in%20https%3A//github.com/lliai/D2MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17298v1&entry.124074799=Read"},
{"title": "Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras", "author": "Nektarios A. Valous and Eckhard Hitzer and Drago\u015f Du\u015fe and Rodrigo Rojas Moraleda and Ferdinand Popp and Meggy Suarez-Carmona and Anna Berthel and Ismini Papageorgiou and Carlo Fremd and Alexander R\u00f6lle and Christina C. Westhoff and B\u00e9n\u00e9dicte Lenoir and Niels Halama and Inka Z\u00f6rnig and Dirk J\u00e4ger", "abstract": "  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n", "link": "http://arxiv.org/abs/2502.07758v3", "date": "2025-02-24", "relevancy": 1.5521, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.531}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5176}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras&body=Title%3A%20Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras%0AAuthor%3A%20Nektarios%20A.%20Valous%20and%20Eckhard%20Hitzer%20and%20Drago%C5%9F%20Du%C5%9Fe%20and%20Rodrigo%20Rojas%20Moraleda%20and%20Ferdinand%20Popp%20and%20Meggy%20Suarez-Carmona%20and%20Anna%20Berthel%20and%20Ismini%20Papageorgiou%20and%20Carlo%20Fremd%20and%20Alexander%20R%C3%B6lle%20and%20Christina%20C.%20Westhoff%20and%20B%C3%A9n%C3%A9dicte%20Lenoir%20and%20Niels%20Halama%20and%20Inka%20Z%C3%B6rnig%20and%20Dirk%20J%C3%A4ger%0AAbstract%3A%20%20%20Hypercomplex%20image%20processing%20extends%20conventional%20techniques%20in%20a%20unified%0Aparadigm%20encompassing%20algebraic%20and%20geometric%20principles.%20This%20work%20leverages%0Aquaternions%20and%20the%20two-dimensional%20orthogonal%20planes%20split%20framework%0A%28splitting%20of%20a%20quaternion%20-%20representing%20a%20pixel%20-%20into%20pairs%20of%20orthogonal%202D%0Aplanes%29%20for%20natural/biomedical%20image%20analysis%20through%20the%20following%0Acomputational%20workflows%20and%20outcomes%3A%20natural/biomedical%20image%20re-colorization%2C%0Anatural%20image%20de-colorization%2C%20natural/biomedical%20image%20contrast%20enhancement%2C%0Acomputational%20re-staining%20and%20stain%20separation%20in%20histological%20images%2C%20and%0Aperformance%20gains%20in%20machine/deep%20learning%20pipelines%20for%20histological%20images.%0AThe%20workflows%20are%20analyzed%20separately%20for%20natural%20and%20biomedical%20images%20to%0Ashowcase%20the%20effectiveness%20of%20the%20proposed%20approaches.%20The%20proposed%20workflows%0Acan%20regulate%20color%20appearance%20%28e.g.%20with%20alternative%20renditions%20and%20grayscale%0Aconversion%29%20and%20image%20contrast%2C%20be%20part%20of%20automated%20image%20processing%20pipelines%0A%28e.g.%20isolating%20stain%20components%2C%20boosting%20learning%20models%29%2C%20and%20assist%20in%0Adigital%20pathology%20applications%20%28e.g.%20enhancing%20biomarker%20visibility%2C%20enabling%0Acolorblind-friendly%20renditions%29.%20Employing%20only%20basic%20arithmetic%20and%20matrix%0Aoperations%2C%20this%20work%20offers%20a%20computationally%20accessible%20methodology%20-%20in%20the%0Ahypercomplex%20domain%20-%20that%20showcases%20versatility%20and%20consistency%20across%20image%0Aprocessing%20tasks%20and%20a%20range%20of%20computer%20vision%20and%20biomedical%20applications.%0AThe%20proposed%20non-data-driven%20methods%20achieve%20comparable%20or%20better%20results%0A%28particularly%20in%20cases%20involving%20well-known%20methods%29%20to%20those%20reported%20in%20the%0Aliterature%2C%20showcasing%20the%20potential%20of%20robust%20theoretical%20frameworks%20with%0Apractical%20effectiveness.%20Results%2C%20methods%2C%20and%20limitations%20are%20detailed%0Aalongside%20discussion%20of%20promising%20extensions%2C%20emphasizing%20the%20potential%20of%0Afeature-rich%20mathematical/computational%20frameworks%20for%20natural%20and%20biomedical%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07758v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520computational%2520workflows%2520for%2520natural%2520and%2520biomedical%2520image%250A%2520%2520processing%2520based%2520on%2520hypercomplex%2520algebras%26entry.906535625%3DNektarios%2520A.%2520Valous%2520and%2520Eckhard%2520Hitzer%2520and%2520Drago%25C5%259F%2520Du%25C5%259Fe%2520and%2520Rodrigo%2520Rojas%2520Moraleda%2520and%2520Ferdinand%2520Popp%2520and%2520Meggy%2520Suarez-Carmona%2520and%2520Anna%2520Berthel%2520and%2520Ismini%2520Papageorgiou%2520and%2520Carlo%2520Fremd%2520and%2520Alexander%2520R%25C3%25B6lle%2520and%2520Christina%2520C.%2520Westhoff%2520and%2520B%25C3%25A9n%25C3%25A9dicte%2520Lenoir%2520and%2520Niels%2520Halama%2520and%2520Inka%2520Z%25C3%25B6rnig%2520and%2520Dirk%2520J%25C3%25A4ger%26entry.1292438233%3D%2520%2520Hypercomplex%2520image%2520processing%2520extends%2520conventional%2520techniques%2520in%2520a%2520unified%250Aparadigm%2520encompassing%2520algebraic%2520and%2520geometric%2520principles.%2520This%2520work%2520leverages%250Aquaternions%2520and%2520the%2520two-dimensional%2520orthogonal%2520planes%2520split%2520framework%250A%2528splitting%2520of%2520a%2520quaternion%2520-%2520representing%2520a%2520pixel%2520-%2520into%2520pairs%2520of%2520orthogonal%25202D%250Aplanes%2529%2520for%2520natural/biomedical%2520image%2520analysis%2520through%2520the%2520following%250Acomputational%2520workflows%2520and%2520outcomes%253A%2520natural/biomedical%2520image%2520re-colorization%252C%250Anatural%2520image%2520de-colorization%252C%2520natural/biomedical%2520image%2520contrast%2520enhancement%252C%250Acomputational%2520re-staining%2520and%2520stain%2520separation%2520in%2520histological%2520images%252C%2520and%250Aperformance%2520gains%2520in%2520machine/deep%2520learning%2520pipelines%2520for%2520histological%2520images.%250AThe%2520workflows%2520are%2520analyzed%2520separately%2520for%2520natural%2520and%2520biomedical%2520images%2520to%250Ashowcase%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approaches.%2520The%2520proposed%2520workflows%250Acan%2520regulate%2520color%2520appearance%2520%2528e.g.%2520with%2520alternative%2520renditions%2520and%2520grayscale%250Aconversion%2529%2520and%2520image%2520contrast%252C%2520be%2520part%2520of%2520automated%2520image%2520processing%2520pipelines%250A%2528e.g.%2520isolating%2520stain%2520components%252C%2520boosting%2520learning%2520models%2529%252C%2520and%2520assist%2520in%250Adigital%2520pathology%2520applications%2520%2528e.g.%2520enhancing%2520biomarker%2520visibility%252C%2520enabling%250Acolorblind-friendly%2520renditions%2529.%2520Employing%2520only%2520basic%2520arithmetic%2520and%2520matrix%250Aoperations%252C%2520this%2520work%2520offers%2520a%2520computationally%2520accessible%2520methodology%2520-%2520in%2520the%250Ahypercomplex%2520domain%2520-%2520that%2520showcases%2520versatility%2520and%2520consistency%2520across%2520image%250Aprocessing%2520tasks%2520and%2520a%2520range%2520of%2520computer%2520vision%2520and%2520biomedical%2520applications.%250AThe%2520proposed%2520non-data-driven%2520methods%2520achieve%2520comparable%2520or%2520better%2520results%250A%2528particularly%2520in%2520cases%2520involving%2520well-known%2520methods%2529%2520to%2520those%2520reported%2520in%2520the%250Aliterature%252C%2520showcasing%2520the%2520potential%2520of%2520robust%2520theoretical%2520frameworks%2520with%250Apractical%2520effectiveness.%2520Results%252C%2520methods%252C%2520and%2520limitations%2520are%2520detailed%250Aalongside%2520discussion%2520of%2520promising%2520extensions%252C%2520emphasizing%2520the%2520potential%2520of%250Afeature-rich%2520mathematical/computational%2520frameworks%2520for%2520natural%2520and%2520biomedical%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07758v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20computational%20workflows%20for%20natural%20and%20biomedical%20image%0A%20%20processing%20based%20on%20hypercomplex%20algebras&entry.906535625=Nektarios%20A.%20Valous%20and%20Eckhard%20Hitzer%20and%20Drago%C5%9F%20Du%C5%9Fe%20and%20Rodrigo%20Rojas%20Moraleda%20and%20Ferdinand%20Popp%20and%20Meggy%20Suarez-Carmona%20and%20Anna%20Berthel%20and%20Ismini%20Papageorgiou%20and%20Carlo%20Fremd%20and%20Alexander%20R%C3%B6lle%20and%20Christina%20C.%20Westhoff%20and%20B%C3%A9n%C3%A9dicte%20Lenoir%20and%20Niels%20Halama%20and%20Inka%20Z%C3%B6rnig%20and%20Dirk%20J%C3%A4ger&entry.1292438233=%20%20Hypercomplex%20image%20processing%20extends%20conventional%20techniques%20in%20a%20unified%0Aparadigm%20encompassing%20algebraic%20and%20geometric%20principles.%20This%20work%20leverages%0Aquaternions%20and%20the%20two-dimensional%20orthogonal%20planes%20split%20framework%0A%28splitting%20of%20a%20quaternion%20-%20representing%20a%20pixel%20-%20into%20pairs%20of%20orthogonal%202D%0Aplanes%29%20for%20natural/biomedical%20image%20analysis%20through%20the%20following%0Acomputational%20workflows%20and%20outcomes%3A%20natural/biomedical%20image%20re-colorization%2C%0Anatural%20image%20de-colorization%2C%20natural/biomedical%20image%20contrast%20enhancement%2C%0Acomputational%20re-staining%20and%20stain%20separation%20in%20histological%20images%2C%20and%0Aperformance%20gains%20in%20machine/deep%20learning%20pipelines%20for%20histological%20images.%0AThe%20workflows%20are%20analyzed%20separately%20for%20natural%20and%20biomedical%20images%20to%0Ashowcase%20the%20effectiveness%20of%20the%20proposed%20approaches.%20The%20proposed%20workflows%0Acan%20regulate%20color%20appearance%20%28e.g.%20with%20alternative%20renditions%20and%20grayscale%0Aconversion%29%20and%20image%20contrast%2C%20be%20part%20of%20automated%20image%20processing%20pipelines%0A%28e.g.%20isolating%20stain%20components%2C%20boosting%20learning%20models%29%2C%20and%20assist%20in%0Adigital%20pathology%20applications%20%28e.g.%20enhancing%20biomarker%20visibility%2C%20enabling%0Acolorblind-friendly%20renditions%29.%20Employing%20only%20basic%20arithmetic%20and%20matrix%0Aoperations%2C%20this%20work%20offers%20a%20computationally%20accessible%20methodology%20-%20in%20the%0Ahypercomplex%20domain%20-%20that%20showcases%20versatility%20and%20consistency%20across%20image%0Aprocessing%20tasks%20and%20a%20range%20of%20computer%20vision%20and%20biomedical%20applications.%0AThe%20proposed%20non-data-driven%20methods%20achieve%20comparable%20or%20better%20results%0A%28particularly%20in%20cases%20involving%20well-known%20methods%29%20to%20those%20reported%20in%20the%0Aliterature%2C%20showcasing%20the%20potential%20of%20robust%20theoretical%20frameworks%20with%0Apractical%20effectiveness.%20Results%2C%20methods%2C%20and%20limitations%20are%20detailed%0Aalongside%20discussion%20of%20promising%20extensions%2C%20emphasizing%20the%20potential%20of%0Afeature-rich%20mathematical/computational%20frameworks%20for%20natural%20and%20biomedical%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07758v3&entry.124074799=Read"},
{"title": "A Closer Look at TabPFN v2: Strength, Limitation, and Extension", "author": "Han-Jia Ye and Si-Yang Liu and Wei-Lun Chao", "abstract": "  Tabular datasets are inherently heterogeneous, posing significant challenges\nfor developing pre-trained foundation models. The recently introduced\ntransformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves\nunprecedented in-context learning accuracy across multiple tabular datasets,\nmarking a pivotal advancement in tabular foundation models. In this paper, we\ncomprehensively evaluate TabPFN v2 on over 300 datasets, confirming its\nexceptional generalization capabilities on small- to medium-scale tasks. Our\nanalysis identifies randomized feature tokens as a key factor behind TabPFN\nv2's success, as they unify heterogeneous datasets into a fixed-dimensional\nrepresentation, enabling more effective training and inference. To further\nunderstand TabPFN v2's predictions, we propose a leave-one-fold-out approach,\ntransforming TabPFN v2 into a feature extractor and revealing its capability to\nsimplify data distributions and boost accuracy. Lastly, to address TabPFN v2's\nlimitations in high-dimensional, large-scale, and many-category tasks, we\nintroduce a divide-and-conquer mechanism inspired by Chain-of-Thought\nprompting, enabling scalable inference. By uncovering the mechanisms behind\nTabPFN v2's success and introducing strategies to expand its applicability,\nthis study provides key insights into the future of tabular foundation models.\n", "link": "http://arxiv.org/abs/2502.17361v1", "date": "2025-02-24", "relevancy": 1.9781, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5164}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4936}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Closer%20Look%20at%20TabPFN%20v2%3A%20Strength%2C%20Limitation%2C%20and%20Extension&body=Title%3A%20A%20Closer%20Look%20at%20TabPFN%20v2%3A%20Strength%2C%20Limitation%2C%20and%20Extension%0AAuthor%3A%20Han-Jia%20Ye%20and%20Si-Yang%20Liu%20and%20Wei-Lun%20Chao%0AAbstract%3A%20%20%20Tabular%20datasets%20are%20inherently%20heterogeneous%2C%20posing%20significant%20challenges%0Afor%20developing%20pre-trained%20foundation%20models.%20The%20recently%20introduced%0Atransformer-based%20Tabular%20Prior-data%20Fitted%20Network%20v2%20%28TabPFN%20v2%29%20achieves%0Aunprecedented%20in-context%20learning%20accuracy%20across%20multiple%20tabular%20datasets%2C%0Amarking%20a%20pivotal%20advancement%20in%20tabular%20foundation%20models.%20In%20this%20paper%2C%20we%0Acomprehensively%20evaluate%20TabPFN%20v2%20on%20over%20300%20datasets%2C%20confirming%20its%0Aexceptional%20generalization%20capabilities%20on%20small-%20to%20medium-scale%20tasks.%20Our%0Aanalysis%20identifies%20randomized%20feature%20tokens%20as%20a%20key%20factor%20behind%20TabPFN%0Av2%27s%20success%2C%20as%20they%20unify%20heterogeneous%20datasets%20into%20a%20fixed-dimensional%0Arepresentation%2C%20enabling%20more%20effective%20training%20and%20inference.%20To%20further%0Aunderstand%20TabPFN%20v2%27s%20predictions%2C%20we%20propose%20a%20leave-one-fold-out%20approach%2C%0Atransforming%20TabPFN%20v2%20into%20a%20feature%20extractor%20and%20revealing%20its%20capability%20to%0Asimplify%20data%20distributions%20and%20boost%20accuracy.%20Lastly%2C%20to%20address%20TabPFN%20v2%27s%0Alimitations%20in%20high-dimensional%2C%20large-scale%2C%20and%20many-category%20tasks%2C%20we%0Aintroduce%20a%20divide-and-conquer%20mechanism%20inspired%20by%20Chain-of-Thought%0Aprompting%2C%20enabling%20scalable%20inference.%20By%20uncovering%20the%20mechanisms%20behind%0ATabPFN%20v2%27s%20success%20and%20introducing%20strategies%20to%20expand%20its%20applicability%2C%0Athis%20study%20provides%20key%20insights%20into%20the%20future%20of%20tabular%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Closer%2520Look%2520at%2520TabPFN%2520v2%253A%2520Strength%252C%2520Limitation%252C%2520and%2520Extension%26entry.906535625%3DHan-Jia%2520Ye%2520and%2520Si-Yang%2520Liu%2520and%2520Wei-Lun%2520Chao%26entry.1292438233%3D%2520%2520Tabular%2520datasets%2520are%2520inherently%2520heterogeneous%252C%2520posing%2520significant%2520challenges%250Afor%2520developing%2520pre-trained%2520foundation%2520models.%2520The%2520recently%2520introduced%250Atransformer-based%2520Tabular%2520Prior-data%2520Fitted%2520Network%2520v2%2520%2528TabPFN%2520v2%2529%2520achieves%250Aunprecedented%2520in-context%2520learning%2520accuracy%2520across%2520multiple%2520tabular%2520datasets%252C%250Amarking%2520a%2520pivotal%2520advancement%2520in%2520tabular%2520foundation%2520models.%2520In%2520this%2520paper%252C%2520we%250Acomprehensively%2520evaluate%2520TabPFN%2520v2%2520on%2520over%2520300%2520datasets%252C%2520confirming%2520its%250Aexceptional%2520generalization%2520capabilities%2520on%2520small-%2520to%2520medium-scale%2520tasks.%2520Our%250Aanalysis%2520identifies%2520randomized%2520feature%2520tokens%2520as%2520a%2520key%2520factor%2520behind%2520TabPFN%250Av2%2527s%2520success%252C%2520as%2520they%2520unify%2520heterogeneous%2520datasets%2520into%2520a%2520fixed-dimensional%250Arepresentation%252C%2520enabling%2520more%2520effective%2520training%2520and%2520inference.%2520To%2520further%250Aunderstand%2520TabPFN%2520v2%2527s%2520predictions%252C%2520we%2520propose%2520a%2520leave-one-fold-out%2520approach%252C%250Atransforming%2520TabPFN%2520v2%2520into%2520a%2520feature%2520extractor%2520and%2520revealing%2520its%2520capability%2520to%250Asimplify%2520data%2520distributions%2520and%2520boost%2520accuracy.%2520Lastly%252C%2520to%2520address%2520TabPFN%2520v2%2527s%250Alimitations%2520in%2520high-dimensional%252C%2520large-scale%252C%2520and%2520many-category%2520tasks%252C%2520we%250Aintroduce%2520a%2520divide-and-conquer%2520mechanism%2520inspired%2520by%2520Chain-of-Thought%250Aprompting%252C%2520enabling%2520scalable%2520inference.%2520By%2520uncovering%2520the%2520mechanisms%2520behind%250ATabPFN%2520v2%2527s%2520success%2520and%2520introducing%2520strategies%2520to%2520expand%2520its%2520applicability%252C%250Athis%2520study%2520provides%2520key%2520insights%2520into%2520the%2520future%2520of%2520tabular%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Closer%20Look%20at%20TabPFN%20v2%3A%20Strength%2C%20Limitation%2C%20and%20Extension&entry.906535625=Han-Jia%20Ye%20and%20Si-Yang%20Liu%20and%20Wei-Lun%20Chao&entry.1292438233=%20%20Tabular%20datasets%20are%20inherently%20heterogeneous%2C%20posing%20significant%20challenges%0Afor%20developing%20pre-trained%20foundation%20models.%20The%20recently%20introduced%0Atransformer-based%20Tabular%20Prior-data%20Fitted%20Network%20v2%20%28TabPFN%20v2%29%20achieves%0Aunprecedented%20in-context%20learning%20accuracy%20across%20multiple%20tabular%20datasets%2C%0Amarking%20a%20pivotal%20advancement%20in%20tabular%20foundation%20models.%20In%20this%20paper%2C%20we%0Acomprehensively%20evaluate%20TabPFN%20v2%20on%20over%20300%20datasets%2C%20confirming%20its%0Aexceptional%20generalization%20capabilities%20on%20small-%20to%20medium-scale%20tasks.%20Our%0Aanalysis%20identifies%20randomized%20feature%20tokens%20as%20a%20key%20factor%20behind%20TabPFN%0Av2%27s%20success%2C%20as%20they%20unify%20heterogeneous%20datasets%20into%20a%20fixed-dimensional%0Arepresentation%2C%20enabling%20more%20effective%20training%20and%20inference.%20To%20further%0Aunderstand%20TabPFN%20v2%27s%20predictions%2C%20we%20propose%20a%20leave-one-fold-out%20approach%2C%0Atransforming%20TabPFN%20v2%20into%20a%20feature%20extractor%20and%20revealing%20its%20capability%20to%0Asimplify%20data%20distributions%20and%20boost%20accuracy.%20Lastly%2C%20to%20address%20TabPFN%20v2%27s%0Alimitations%20in%20high-dimensional%2C%20large-scale%2C%20and%20many-category%20tasks%2C%20we%0Aintroduce%20a%20divide-and-conquer%20mechanism%20inspired%20by%20Chain-of-Thought%0Aprompting%2C%20enabling%20scalable%20inference.%20By%20uncovering%20the%20mechanisms%20behind%0ATabPFN%20v2%27s%20success%20and%20introducing%20strategies%20to%20expand%20its%20applicability%2C%0Athis%20study%20provides%20key%20insights%20into%20the%20future%20of%20tabular%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17361v1&entry.124074799=Read"},
{"title": "Systematic Weight Evaluation for Pruning Large Language Models:\n  Enhancing Performance and Sustainability", "author": "Ashhadul Islam and Samir Brahim Belhaouari and Amine Bermak", "abstract": "  The exponential growth of large language models (LLMs) like ChatGPT has\nrevolutionized artificial intelligence, offering unprecedented capabilities in\nnatural language processing. However, the extensive computational resources\nrequired for training these models have significant environmental implications,\nincluding high carbon emissions, energy consumption, and water usage. This\nresearch presents a novel approach to LLM pruning, focusing on the systematic\nevaluation of individual weight importance throughout the training process. By\nmonitoring parameter evolution over time, we propose a method that effectively\nreduces model size without compromising performance. Extensive experiments with\nboth a scaled-down LLM and a large multimodal model reveal that moderate\npruning enhances efficiency and reduces loss, while excessive pruning\ndrastically deteriorates model performance. These findings highlight the\ncritical need for optimized AI models to ensure sustainable development,\nbalancing technological advancement with environmental responsibility.\n", "link": "http://arxiv.org/abs/2502.17071v1", "date": "2025-02-24", "relevancy": 1.9072, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4948}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Systematic%20Weight%20Evaluation%20for%20Pruning%20Large%20Language%20Models%3A%0A%20%20Enhancing%20Performance%20and%20Sustainability&body=Title%3A%20Systematic%20Weight%20Evaluation%20for%20Pruning%20Large%20Language%20Models%3A%0A%20%20Enhancing%20Performance%20and%20Sustainability%0AAuthor%3A%20Ashhadul%20Islam%20and%20Samir%20Brahim%20Belhaouari%20and%20Amine%20Bermak%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20has%0Arevolutionized%20artificial%20intelligence%2C%20offering%20unprecedented%20capabilities%20in%0Anatural%20language%20processing.%20However%2C%20the%20extensive%20computational%20resources%0Arequired%20for%20training%20these%20models%20have%20significant%20environmental%20implications%2C%0Aincluding%20high%20carbon%20emissions%2C%20energy%20consumption%2C%20and%20water%20usage.%20This%0Aresearch%20presents%20a%20novel%20approach%20to%20LLM%20pruning%2C%20focusing%20on%20the%20systematic%0Aevaluation%20of%20individual%20weight%20importance%20throughout%20the%20training%20process.%20By%0Amonitoring%20parameter%20evolution%20over%20time%2C%20we%20propose%20a%20method%20that%20effectively%0Areduces%20model%20size%20without%20compromising%20performance.%20Extensive%20experiments%20with%0Aboth%20a%20scaled-down%20LLM%20and%20a%20large%20multimodal%20model%20reveal%20that%20moderate%0Apruning%20enhances%20efficiency%20and%20reduces%20loss%2C%20while%20excessive%20pruning%0Adrastically%20deteriorates%20model%20performance.%20These%20findings%20highlight%20the%0Acritical%20need%20for%20optimized%20AI%20models%20to%20ensure%20sustainable%20development%2C%0Abalancing%20technological%20advancement%20with%20environmental%20responsibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSystematic%2520Weight%2520Evaluation%2520for%2520Pruning%2520Large%2520Language%2520Models%253A%250A%2520%2520Enhancing%2520Performance%2520and%2520Sustainability%26entry.906535625%3DAshhadul%2520Islam%2520and%2520Samir%2520Brahim%2520Belhaouari%2520and%2520Amine%2520Bermak%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520like%2520ChatGPT%2520has%250Arevolutionized%2520artificial%2520intelligence%252C%2520offering%2520unprecedented%2520capabilities%2520in%250Anatural%2520language%2520processing.%2520However%252C%2520the%2520extensive%2520computational%2520resources%250Arequired%2520for%2520training%2520these%2520models%2520have%2520significant%2520environmental%2520implications%252C%250Aincluding%2520high%2520carbon%2520emissions%252C%2520energy%2520consumption%252C%2520and%2520water%2520usage.%2520This%250Aresearch%2520presents%2520a%2520novel%2520approach%2520to%2520LLM%2520pruning%252C%2520focusing%2520on%2520the%2520systematic%250Aevaluation%2520of%2520individual%2520weight%2520importance%2520throughout%2520the%2520training%2520process.%2520By%250Amonitoring%2520parameter%2520evolution%2520over%2520time%252C%2520we%2520propose%2520a%2520method%2520that%2520effectively%250Areduces%2520model%2520size%2520without%2520compromising%2520performance.%2520Extensive%2520experiments%2520with%250Aboth%2520a%2520scaled-down%2520LLM%2520and%2520a%2520large%2520multimodal%2520model%2520reveal%2520that%2520moderate%250Apruning%2520enhances%2520efficiency%2520and%2520reduces%2520loss%252C%2520while%2520excessive%2520pruning%250Adrastically%2520deteriorates%2520model%2520performance.%2520These%2520findings%2520highlight%2520the%250Acritical%2520need%2520for%2520optimized%2520AI%2520models%2520to%2520ensure%2520sustainable%2520development%252C%250Abalancing%2520technological%2520advancement%2520with%2520environmental%2520responsibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Systematic%20Weight%20Evaluation%20for%20Pruning%20Large%20Language%20Models%3A%0A%20%20Enhancing%20Performance%20and%20Sustainability&entry.906535625=Ashhadul%20Islam%20and%20Samir%20Brahim%20Belhaouari%20and%20Amine%20Bermak&entry.1292438233=%20%20The%20exponential%20growth%20of%20large%20language%20models%20%28LLMs%29%20like%20ChatGPT%20has%0Arevolutionized%20artificial%20intelligence%2C%20offering%20unprecedented%20capabilities%20in%0Anatural%20language%20processing.%20However%2C%20the%20extensive%20computational%20resources%0Arequired%20for%20training%20these%20models%20have%20significant%20environmental%20implications%2C%0Aincluding%20high%20carbon%20emissions%2C%20energy%20consumption%2C%20and%20water%20usage.%20This%0Aresearch%20presents%20a%20novel%20approach%20to%20LLM%20pruning%2C%20focusing%20on%20the%20systematic%0Aevaluation%20of%20individual%20weight%20importance%20throughout%20the%20training%20process.%20By%0Amonitoring%20parameter%20evolution%20over%20time%2C%20we%20propose%20a%20method%20that%20effectively%0Areduces%20model%20size%20without%20compromising%20performance.%20Extensive%20experiments%20with%0Aboth%20a%20scaled-down%20LLM%20and%20a%20large%20multimodal%20model%20reveal%20that%20moderate%0Apruning%20enhances%20efficiency%20and%20reduces%20loss%2C%20while%20excessive%20pruning%0Adrastically%20deteriorates%20model%20performance.%20These%20findings%20highlight%20the%0Acritical%20need%20for%20optimized%20AI%20models%20to%20ensure%20sustainable%20development%2C%0Abalancing%20technological%20advancement%20with%20environmental%20responsibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17071v1&entry.124074799=Read"},
{"title": "SpecDM: Hyperspectral Dataset Synthesis with Pixel-level Semantic\n  Annotations", "author": "Wendi Liu and Pei Yang and Wenhui Hong and Xiaoguang Mei and Jiayi Ma", "abstract": "  In hyperspectral remote sensing field, some downstream dense prediction\ntasks, such as semantic segmentation (SS) and change detection (CD), rely on\nsupervised learning to improve model performance and require a large amount of\nmanually annotated data for training. However, due to the needs of specific\nequipment and special application scenarios, the acquisition and annotation of\nhyperspectral images (HSIs) are often costly and time-consuming. To this end,\nour work explores the potential of generative diffusion model in synthesizing\nHSIs with pixel-level annotations. The main idea is to utilize a two-stream VAE\nto learn the latent representations of images and corresponding masks\nrespectively, learn their joint distribution during the diffusion model\ntraining, and finally obtain the image and mask through their respective\ndecoders. To the best of our knowledge, it is the first work to generate\nhigh-dimensional HSIs with annotations. Our proposed approach can be applied in\nvarious kinds of dataset generation. We select two of the most widely used\ndense prediction tasks: semantic segmentation and change detection, and\ngenerate datasets suitable for these tasks. Experiments demonstrate that our\nsynthetic datasets have a positive impact on the improvement of these\ndownstream tasks.\n", "link": "http://arxiv.org/abs/2502.17056v1", "date": "2025-02-24", "relevancy": 1.8032, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.612}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5986}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecDM%3A%20Hyperspectral%20Dataset%20Synthesis%20with%20Pixel-level%20Semantic%0A%20%20Annotations&body=Title%3A%20SpecDM%3A%20Hyperspectral%20Dataset%20Synthesis%20with%20Pixel-level%20Semantic%0A%20%20Annotations%0AAuthor%3A%20Wendi%20Liu%20and%20Pei%20Yang%20and%20Wenhui%20Hong%20and%20Xiaoguang%20Mei%20and%20Jiayi%20Ma%0AAbstract%3A%20%20%20In%20hyperspectral%20remote%20sensing%20field%2C%20some%20downstream%20dense%20prediction%0Atasks%2C%20such%20as%20semantic%20segmentation%20%28SS%29%20and%20change%20detection%20%28CD%29%2C%20rely%20on%0Asupervised%20learning%20to%20improve%20model%20performance%20and%20require%20a%20large%20amount%20of%0Amanually%20annotated%20data%20for%20training.%20However%2C%20due%20to%20the%20needs%20of%20specific%0Aequipment%20and%20special%20application%20scenarios%2C%20the%20acquisition%20and%20annotation%20of%0Ahyperspectral%20images%20%28HSIs%29%20are%20often%20costly%20and%20time-consuming.%20To%20this%20end%2C%0Aour%20work%20explores%20the%20potential%20of%20generative%20diffusion%20model%20in%20synthesizing%0AHSIs%20with%20pixel-level%20annotations.%20The%20main%20idea%20is%20to%20utilize%20a%20two-stream%20VAE%0Ato%20learn%20the%20latent%20representations%20of%20images%20and%20corresponding%20masks%0Arespectively%2C%20learn%20their%20joint%20distribution%20during%20the%20diffusion%20model%0Atraining%2C%20and%20finally%20obtain%20the%20image%20and%20mask%20through%20their%20respective%0Adecoders.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20work%20to%20generate%0Ahigh-dimensional%20HSIs%20with%20annotations.%20Our%20proposed%20approach%20can%20be%20applied%20in%0Avarious%20kinds%20of%20dataset%20generation.%20We%20select%20two%20of%20the%20most%20widely%20used%0Adense%20prediction%20tasks%3A%20semantic%20segmentation%20and%20change%20detection%2C%20and%0Agenerate%20datasets%20suitable%20for%20these%20tasks.%20Experiments%20demonstrate%20that%20our%0Asynthetic%20datasets%20have%20a%20positive%20impact%20on%20the%20improvement%20of%20these%0Adownstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecDM%253A%2520Hyperspectral%2520Dataset%2520Synthesis%2520with%2520Pixel-level%2520Semantic%250A%2520%2520Annotations%26entry.906535625%3DWendi%2520Liu%2520and%2520Pei%2520Yang%2520and%2520Wenhui%2520Hong%2520and%2520Xiaoguang%2520Mei%2520and%2520Jiayi%2520Ma%26entry.1292438233%3D%2520%2520In%2520hyperspectral%2520remote%2520sensing%2520field%252C%2520some%2520downstream%2520dense%2520prediction%250Atasks%252C%2520such%2520as%2520semantic%2520segmentation%2520%2528SS%2529%2520and%2520change%2520detection%2520%2528CD%2529%252C%2520rely%2520on%250Asupervised%2520learning%2520to%2520improve%2520model%2520performance%2520and%2520require%2520a%2520large%2520amount%2520of%250Amanually%2520annotated%2520data%2520for%2520training.%2520However%252C%2520due%2520to%2520the%2520needs%2520of%2520specific%250Aequipment%2520and%2520special%2520application%2520scenarios%252C%2520the%2520acquisition%2520and%2520annotation%2520of%250Ahyperspectral%2520images%2520%2528HSIs%2529%2520are%2520often%2520costly%2520and%2520time-consuming.%2520To%2520this%2520end%252C%250Aour%2520work%2520explores%2520the%2520potential%2520of%2520generative%2520diffusion%2520model%2520in%2520synthesizing%250AHSIs%2520with%2520pixel-level%2520annotations.%2520The%2520main%2520idea%2520is%2520to%2520utilize%2520a%2520two-stream%2520VAE%250Ato%2520learn%2520the%2520latent%2520representations%2520of%2520images%2520and%2520corresponding%2520masks%250Arespectively%252C%2520learn%2520their%2520joint%2520distribution%2520during%2520the%2520diffusion%2520model%250Atraining%252C%2520and%2520finally%2520obtain%2520the%2520image%2520and%2520mask%2520through%2520their%2520respective%250Adecoders.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520it%2520is%2520the%2520first%2520work%2520to%2520generate%250Ahigh-dimensional%2520HSIs%2520with%2520annotations.%2520Our%2520proposed%2520approach%2520can%2520be%2520applied%2520in%250Avarious%2520kinds%2520of%2520dataset%2520generation.%2520We%2520select%2520two%2520of%2520the%2520most%2520widely%2520used%250Adense%2520prediction%2520tasks%253A%2520semantic%2520segmentation%2520and%2520change%2520detection%252C%2520and%250Agenerate%2520datasets%2520suitable%2520for%2520these%2520tasks.%2520Experiments%2520demonstrate%2520that%2520our%250Asynthetic%2520datasets%2520have%2520a%2520positive%2520impact%2520on%2520the%2520improvement%2520of%2520these%250Adownstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecDM%3A%20Hyperspectral%20Dataset%20Synthesis%20with%20Pixel-level%20Semantic%0A%20%20Annotations&entry.906535625=Wendi%20Liu%20and%20Pei%20Yang%20and%20Wenhui%20Hong%20and%20Xiaoguang%20Mei%20and%20Jiayi%20Ma&entry.1292438233=%20%20In%20hyperspectral%20remote%20sensing%20field%2C%20some%20downstream%20dense%20prediction%0Atasks%2C%20such%20as%20semantic%20segmentation%20%28SS%29%20and%20change%20detection%20%28CD%29%2C%20rely%20on%0Asupervised%20learning%20to%20improve%20model%20performance%20and%20require%20a%20large%20amount%20of%0Amanually%20annotated%20data%20for%20training.%20However%2C%20due%20to%20the%20needs%20of%20specific%0Aequipment%20and%20special%20application%20scenarios%2C%20the%20acquisition%20and%20annotation%20of%0Ahyperspectral%20images%20%28HSIs%29%20are%20often%20costly%20and%20time-consuming.%20To%20this%20end%2C%0Aour%20work%20explores%20the%20potential%20of%20generative%20diffusion%20model%20in%20synthesizing%0AHSIs%20with%20pixel-level%20annotations.%20The%20main%20idea%20is%20to%20utilize%20a%20two-stream%20VAE%0Ato%20learn%20the%20latent%20representations%20of%20images%20and%20corresponding%20masks%0Arespectively%2C%20learn%20their%20joint%20distribution%20during%20the%20diffusion%20model%0Atraining%2C%20and%20finally%20obtain%20the%20image%20and%20mask%20through%20their%20respective%0Adecoders.%20To%20the%20best%20of%20our%20knowledge%2C%20it%20is%20the%20first%20work%20to%20generate%0Ahigh-dimensional%20HSIs%20with%20annotations.%20Our%20proposed%20approach%20can%20be%20applied%20in%0Avarious%20kinds%20of%20dataset%20generation.%20We%20select%20two%20of%20the%20most%20widely%20used%0Adense%20prediction%20tasks%3A%20semantic%20segmentation%20and%20change%20detection%2C%20and%0Agenerate%20datasets%20suitable%20for%20these%20tasks.%20Experiments%20demonstrate%20that%20our%0Asynthetic%20datasets%20have%20a%20positive%20impact%20on%20the%20improvement%20of%20these%0Adownstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17056v1&entry.124074799=Read"},
{"title": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints", "author": "Meiyi Zhu and Matteo Zecchin and Sangwoo Park and Caili Guo and Chunyan Feng and Petar Popovski and Osvaldo Simeone", "abstract": "  This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks.\n", "link": "http://arxiv.org/abs/2409.07902v3", "date": "2025-02-24", "relevancy": 1.9452, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4934}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Distributed%20Remote%20Inference%20in%20Sensor%20Networks%20Under%0A%20%20Reliability%20and%20Communication%20Constraints&body=Title%3A%20Conformal%20Distributed%20Remote%20Inference%20in%20Sensor%20Networks%20Under%0A%20%20Reliability%20and%20Communication%20Constraints%0AAuthor%3A%20Meiyi%20Zhu%20and%20Matteo%20Zecchin%20and%20Sangwoo%20Park%20and%20Caili%20Guo%20and%20Chunyan%20Feng%20and%20Petar%20Popovski%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20This%20paper%20presents%20communication-constrained%20distributed%20conformal%20risk%0Acontrol%20%28CD-CRC%29%20framework%2C%20a%20novel%20decision-making%20framework%20for%20sensor%0Anetworks%20under%20communication%20constraints.%20Targeting%20multi-label%20classification%0Aproblems%2C%20such%20as%20segmentation%2C%20CD-CRC%20dynamically%20adjusts%20local%20and%20global%0Athresholds%20used%20to%20identify%20significant%20labels%20with%20the%20goal%20of%20ensuring%20a%0Atarget%20false%20negative%20rate%20%28FNR%29%2C%20while%20adhering%20to%20communication%20capacity%0Alimits.%20CD-CRC%20builds%20on%20online%20exponentiated%20gradient%20descent%20to%20estimate%20the%0Arelative%20quality%20of%20the%20observations%20of%20different%20sensors%2C%20and%20on%20online%0Aconformal%20risk%20control%20%28CRC%29%20as%20a%20mechanism%20to%20control%20local%20and%20global%0Athresholds.%20CD-CRC%20is%20proved%20to%20offer%20deterministic%20worst-case%20performance%0Aguarantees%20in%20terms%20of%20FNR%20and%20communication%20overhead%2C%20while%20the%20regret%0Aperformance%20in%20terms%20of%20false%20positive%20rate%20%28FPR%29%20is%20characterized%20as%20a%0Afunction%20of%20the%20key%20hyperparameters.%20Simulation%20results%20highlight%20the%0Aeffectiveness%20of%20CD-CRC%2C%20particularly%20in%20communication%20resource-constrained%0Aenvironments%2C%20making%20it%20a%20valuable%20tool%20for%20enhancing%20the%20performance%20and%0Areliability%20of%20distributed%20sensor%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07902v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Distributed%2520Remote%2520Inference%2520in%2520Sensor%2520Networks%2520Under%250A%2520%2520Reliability%2520and%2520Communication%2520Constraints%26entry.906535625%3DMeiyi%2520Zhu%2520and%2520Matteo%2520Zecchin%2520and%2520Sangwoo%2520Park%2520and%2520Caili%2520Guo%2520and%2520Chunyan%2520Feng%2520and%2520Petar%2520Popovski%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520communication-constrained%2520distributed%2520conformal%2520risk%250Acontrol%2520%2528CD-CRC%2529%2520framework%252C%2520a%2520novel%2520decision-making%2520framework%2520for%2520sensor%250Anetworks%2520under%2520communication%2520constraints.%2520Targeting%2520multi-label%2520classification%250Aproblems%252C%2520such%2520as%2520segmentation%252C%2520CD-CRC%2520dynamically%2520adjusts%2520local%2520and%2520global%250Athresholds%2520used%2520to%2520identify%2520significant%2520labels%2520with%2520the%2520goal%2520of%2520ensuring%2520a%250Atarget%2520false%2520negative%2520rate%2520%2528FNR%2529%252C%2520while%2520adhering%2520to%2520communication%2520capacity%250Alimits.%2520CD-CRC%2520builds%2520on%2520online%2520exponentiated%2520gradient%2520descent%2520to%2520estimate%2520the%250Arelative%2520quality%2520of%2520the%2520observations%2520of%2520different%2520sensors%252C%2520and%2520on%2520online%250Aconformal%2520risk%2520control%2520%2528CRC%2529%2520as%2520a%2520mechanism%2520to%2520control%2520local%2520and%2520global%250Athresholds.%2520CD-CRC%2520is%2520proved%2520to%2520offer%2520deterministic%2520worst-case%2520performance%250Aguarantees%2520in%2520terms%2520of%2520FNR%2520and%2520communication%2520overhead%252C%2520while%2520the%2520regret%250Aperformance%2520in%2520terms%2520of%2520false%2520positive%2520rate%2520%2528FPR%2529%2520is%2520characterized%2520as%2520a%250Afunction%2520of%2520the%2520key%2520hyperparameters.%2520Simulation%2520results%2520highlight%2520the%250Aeffectiveness%2520of%2520CD-CRC%252C%2520particularly%2520in%2520communication%2520resource-constrained%250Aenvironments%252C%2520making%2520it%2520a%2520valuable%2520tool%2520for%2520enhancing%2520the%2520performance%2520and%250Areliability%2520of%2520distributed%2520sensor%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07902v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Distributed%20Remote%20Inference%20in%20Sensor%20Networks%20Under%0A%20%20Reliability%20and%20Communication%20Constraints&entry.906535625=Meiyi%20Zhu%20and%20Matteo%20Zecchin%20and%20Sangwoo%20Park%20and%20Caili%20Guo%20and%20Chunyan%20Feng%20and%20Petar%20Popovski%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20This%20paper%20presents%20communication-constrained%20distributed%20conformal%20risk%0Acontrol%20%28CD-CRC%29%20framework%2C%20a%20novel%20decision-making%20framework%20for%20sensor%0Anetworks%20under%20communication%20constraints.%20Targeting%20multi-label%20classification%0Aproblems%2C%20such%20as%20segmentation%2C%20CD-CRC%20dynamically%20adjusts%20local%20and%20global%0Athresholds%20used%20to%20identify%20significant%20labels%20with%20the%20goal%20of%20ensuring%20a%0Atarget%20false%20negative%20rate%20%28FNR%29%2C%20while%20adhering%20to%20communication%20capacity%0Alimits.%20CD-CRC%20builds%20on%20online%20exponentiated%20gradient%20descent%20to%20estimate%20the%0Arelative%20quality%20of%20the%20observations%20of%20different%20sensors%2C%20and%20on%20online%0Aconformal%20risk%20control%20%28CRC%29%20as%20a%20mechanism%20to%20control%20local%20and%20global%0Athresholds.%20CD-CRC%20is%20proved%20to%20offer%20deterministic%20worst-case%20performance%0Aguarantees%20in%20terms%20of%20FNR%20and%20communication%20overhead%2C%20while%20the%20regret%0Aperformance%20in%20terms%20of%20false%20positive%20rate%20%28FPR%29%20is%20characterized%20as%20a%0Afunction%20of%20the%20key%20hyperparameters.%20Simulation%20results%20highlight%20the%0Aeffectiveness%20of%20CD-CRC%2C%20particularly%20in%20communication%20resource-constrained%0Aenvironments%2C%20making%20it%20a%20valuable%20tool%20for%20enhancing%20the%20performance%20and%0Areliability%20of%20distributed%20sensor%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07902v3&entry.124074799=Read"},
{"title": "Institutional Platform for Secure Self-Service Large Language Model\n  Exploration", "author": "V. K. Cody Bumgardner and Mitchell A. Klusty and W. Vaiden Logan and Samuel E. Armstrong and Caroline N. Leach and Kenneth L. Calvert and Caylin Hickey and Jeff Talbert", "abstract": "  This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery.\n", "link": "http://arxiv.org/abs/2402.00913v3", "date": "2025-02-24", "relevancy": 1.9601, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Institutional%20Platform%20for%20Secure%20Self-Service%20Large%20Language%20Model%0A%20%20Exploration&body=Title%3A%20Institutional%20Platform%20for%20Secure%20Self-Service%20Large%20Language%20Model%0A%20%20Exploration%0AAuthor%3A%20V.%20K.%20Cody%20Bumgardner%20and%20Mitchell%20A.%20Klusty%20and%20W.%20Vaiden%20Logan%20and%20Samuel%20E.%20Armstrong%20and%20Caroline%20N.%20Leach%20and%20Kenneth%20L.%20Calvert%20and%20Caylin%20Hickey%20and%20Jeff%20Talbert%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20user-friendly%20platform%20developed%20by%20the%20University%20of%0AKentucky%20Center%20for%20Applied%20AI%2C%20designed%20to%20make%20large%2C%20customized%20language%0Amodels%20%28LLMs%29%20more%20accessible.%20By%20capitalizing%20on%20recent%20advancements%20in%0Amulti-LoRA%20inference%2C%20the%20system%20efficiently%20accommodates%20custom%20adapters%20for%20a%0Adiverse%20range%20of%20users%20and%20projects.%20The%20paper%20outlines%20the%20system%27s%0Aarchitecture%20and%20key%20features%2C%20encompassing%20dataset%20curation%2C%20model%20training%2C%0Asecure%20inference%2C%20and%20text-based%20feature%20extraction.%0A%20%20We%20illustrate%20the%20establishment%20of%20a%20tenant-aware%20computational%20network%20using%0Aagent-based%20methods%2C%20securely%20utilizing%20islands%20of%20isolated%20resources%20as%20a%0Aunified%20system.%20The%20platform%20strives%20to%20deliver%20secure%20LLM%20services%2C%0Aemphasizing%20process%20and%20data%20isolation%2C%20end-to-end%20encryption%2C%20and%20role-based%0Aresource%20authentication.%20This%20contribution%20aligns%20with%20the%20overarching%20goal%20of%0Aenabling%20simplified%20access%20to%20cutting-edge%20AI%20models%20and%20technology%20in%20support%0Aof%20scientific%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00913v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstitutional%2520Platform%2520for%2520Secure%2520Self-Service%2520Large%2520Language%2520Model%250A%2520%2520Exploration%26entry.906535625%3DV.%2520K.%2520Cody%2520Bumgardner%2520and%2520Mitchell%2520A.%2520Klusty%2520and%2520W.%2520Vaiden%2520Logan%2520and%2520Samuel%2520E.%2520Armstrong%2520and%2520Caroline%2520N.%2520Leach%2520and%2520Kenneth%2520L.%2520Calvert%2520and%2520Caylin%2520Hickey%2520and%2520Jeff%2520Talbert%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520user-friendly%2520platform%2520developed%2520by%2520the%2520University%2520of%250AKentucky%2520Center%2520for%2520Applied%2520AI%252C%2520designed%2520to%2520make%2520large%252C%2520customized%2520language%250Amodels%2520%2528LLMs%2529%2520more%2520accessible.%2520By%2520capitalizing%2520on%2520recent%2520advancements%2520in%250Amulti-LoRA%2520inference%252C%2520the%2520system%2520efficiently%2520accommodates%2520custom%2520adapters%2520for%2520a%250Adiverse%2520range%2520of%2520users%2520and%2520projects.%2520The%2520paper%2520outlines%2520the%2520system%2527s%250Aarchitecture%2520and%2520key%2520features%252C%2520encompassing%2520dataset%2520curation%252C%2520model%2520training%252C%250Asecure%2520inference%252C%2520and%2520text-based%2520feature%2520extraction.%250A%2520%2520We%2520illustrate%2520the%2520establishment%2520of%2520a%2520tenant-aware%2520computational%2520network%2520using%250Aagent-based%2520methods%252C%2520securely%2520utilizing%2520islands%2520of%2520isolated%2520resources%2520as%2520a%250Aunified%2520system.%2520The%2520platform%2520strives%2520to%2520deliver%2520secure%2520LLM%2520services%252C%250Aemphasizing%2520process%2520and%2520data%2520isolation%252C%2520end-to-end%2520encryption%252C%2520and%2520role-based%250Aresource%2520authentication.%2520This%2520contribution%2520aligns%2520with%2520the%2520overarching%2520goal%2520of%250Aenabling%2520simplified%2520access%2520to%2520cutting-edge%2520AI%2520models%2520and%2520technology%2520in%2520support%250Aof%2520scientific%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00913v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Institutional%20Platform%20for%20Secure%20Self-Service%20Large%20Language%20Model%0A%20%20Exploration&entry.906535625=V.%20K.%20Cody%20Bumgardner%20and%20Mitchell%20A.%20Klusty%20and%20W.%20Vaiden%20Logan%20and%20Samuel%20E.%20Armstrong%20and%20Caroline%20N.%20Leach%20and%20Kenneth%20L.%20Calvert%20and%20Caylin%20Hickey%20and%20Jeff%20Talbert&entry.1292438233=%20%20This%20paper%20introduces%20a%20user-friendly%20platform%20developed%20by%20the%20University%20of%0AKentucky%20Center%20for%20Applied%20AI%2C%20designed%20to%20make%20large%2C%20customized%20language%0Amodels%20%28LLMs%29%20more%20accessible.%20By%20capitalizing%20on%20recent%20advancements%20in%0Amulti-LoRA%20inference%2C%20the%20system%20efficiently%20accommodates%20custom%20adapters%20for%20a%0Adiverse%20range%20of%20users%20and%20projects.%20The%20paper%20outlines%20the%20system%27s%0Aarchitecture%20and%20key%20features%2C%20encompassing%20dataset%20curation%2C%20model%20training%2C%0Asecure%20inference%2C%20and%20text-based%20feature%20extraction.%0A%20%20We%20illustrate%20the%20establishment%20of%20a%20tenant-aware%20computational%20network%20using%0Aagent-based%20methods%2C%20securely%20utilizing%20islands%20of%20isolated%20resources%20as%20a%0Aunified%20system.%20The%20platform%20strives%20to%20deliver%20secure%20LLM%20services%2C%0Aemphasizing%20process%20and%20data%20isolation%2C%20end-to-end%20encryption%2C%20and%20role-based%0Aresource%20authentication.%20This%20contribution%20aligns%20with%20the%20overarching%20goal%20of%0Aenabling%20simplified%20access%20to%20cutting-edge%20AI%20models%20and%20technology%20in%20support%0Aof%20scientific%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00913v3&entry.124074799=Read"},
{"title": "An Explainable AI Model for Binary LJ Fluids", "author": "Israrul H Hashmi and Rahul Karmakar and Marripelli Maniteja and Kumar Ayush and Tarak K. Patra", "abstract": "  Lennard-Jones (LJ) fluids serve as an important theoretical framework for\nunderstanding molecular interactions. Binary LJ fluids, where two distinct\nspecies of particles interact based on the LJ potential, exhibit rich phase\nbehavior and provide valuable insights of complex fluid mixtures. Here we\nreport the construction and utility of an artificial intelligence (AI) model\nfor binary LJ fluids, focusing on their effectiveness in predicting radial\ndistribution functions (RDFs) across a range of conditions. The RDFs of a\nbinary mixture with varying compositions and temperatures are collected from\nmolecular dynamics (MD) simulations to establish and validate the AI model. In\nthis AI pipeline, RDFs are discretized in order to reduce the output dimension\nof the model. This, in turn, improves the efficacy, and reduce the complexity\nof an AI RDF model. The model is shown to predict RDFs for many unknown\nmixtures very accurately, especially outside the training temperature range.\nOur analysis suggests that the particle size ratio has a higher order impact on\nthe microstructure of a binary mixture. We also highlight the areas where the\nfidelity of the AI model is low when encountering new regimes with different\nunderlying physics.\n", "link": "http://arxiv.org/abs/2502.17357v1", "date": "2025-02-24", "relevancy": 1.3827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4937}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.457}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Explainable%20AI%20Model%20for%20Binary%20LJ%20Fluids&body=Title%3A%20An%20Explainable%20AI%20Model%20for%20Binary%20LJ%20Fluids%0AAuthor%3A%20Israrul%20H%20Hashmi%20and%20Rahul%20Karmakar%20and%20Marripelli%20Maniteja%20and%20Kumar%20Ayush%20and%20Tarak%20K.%20Patra%0AAbstract%3A%20%20%20Lennard-Jones%20%28LJ%29%20fluids%20serve%20as%20an%20important%20theoretical%20framework%20for%0Aunderstanding%20molecular%20interactions.%20Binary%20LJ%20fluids%2C%20where%20two%20distinct%0Aspecies%20of%20particles%20interact%20based%20on%20the%20LJ%20potential%2C%20exhibit%20rich%20phase%0Abehavior%20and%20provide%20valuable%20insights%20of%20complex%20fluid%20mixtures.%20Here%20we%0Areport%20the%20construction%20and%20utility%20of%20an%20artificial%20intelligence%20%28AI%29%20model%0Afor%20binary%20LJ%20fluids%2C%20focusing%20on%20their%20effectiveness%20in%20predicting%20radial%0Adistribution%20functions%20%28RDFs%29%20across%20a%20range%20of%20conditions.%20The%20RDFs%20of%20a%0Abinary%20mixture%20with%20varying%20compositions%20and%20temperatures%20are%20collected%20from%0Amolecular%20dynamics%20%28MD%29%20simulations%20to%20establish%20and%20validate%20the%20AI%20model.%20In%0Athis%20AI%20pipeline%2C%20RDFs%20are%20discretized%20in%20order%20to%20reduce%20the%20output%20dimension%0Aof%20the%20model.%20This%2C%20in%20turn%2C%20improves%20the%20efficacy%2C%20and%20reduce%20the%20complexity%0Aof%20an%20AI%20RDF%20model.%20The%20model%20is%20shown%20to%20predict%20RDFs%20for%20many%20unknown%0Amixtures%20very%20accurately%2C%20especially%20outside%20the%20training%20temperature%20range.%0AOur%20analysis%20suggests%20that%20the%20particle%20size%20ratio%20has%20a%20higher%20order%20impact%20on%0Athe%20microstructure%20of%20a%20binary%20mixture.%20We%20also%20highlight%20the%20areas%20where%20the%0Afidelity%20of%20the%20AI%20model%20is%20low%20when%20encountering%20new%20regimes%20with%20different%0Aunderlying%20physics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Explainable%2520AI%2520Model%2520for%2520Binary%2520LJ%2520Fluids%26entry.906535625%3DIsrarul%2520H%2520Hashmi%2520and%2520Rahul%2520Karmakar%2520and%2520Marripelli%2520Maniteja%2520and%2520Kumar%2520Ayush%2520and%2520Tarak%2520K.%2520Patra%26entry.1292438233%3D%2520%2520Lennard-Jones%2520%2528LJ%2529%2520fluids%2520serve%2520as%2520an%2520important%2520theoretical%2520framework%2520for%250Aunderstanding%2520molecular%2520interactions.%2520Binary%2520LJ%2520fluids%252C%2520where%2520two%2520distinct%250Aspecies%2520of%2520particles%2520interact%2520based%2520on%2520the%2520LJ%2520potential%252C%2520exhibit%2520rich%2520phase%250Abehavior%2520and%2520provide%2520valuable%2520insights%2520of%2520complex%2520fluid%2520mixtures.%2520Here%2520we%250Areport%2520the%2520construction%2520and%2520utility%2520of%2520an%2520artificial%2520intelligence%2520%2528AI%2529%2520model%250Afor%2520binary%2520LJ%2520fluids%252C%2520focusing%2520on%2520their%2520effectiveness%2520in%2520predicting%2520radial%250Adistribution%2520functions%2520%2528RDFs%2529%2520across%2520a%2520range%2520of%2520conditions.%2520The%2520RDFs%2520of%2520a%250Abinary%2520mixture%2520with%2520varying%2520compositions%2520and%2520temperatures%2520are%2520collected%2520from%250Amolecular%2520dynamics%2520%2528MD%2529%2520simulations%2520to%2520establish%2520and%2520validate%2520the%2520AI%2520model.%2520In%250Athis%2520AI%2520pipeline%252C%2520RDFs%2520are%2520discretized%2520in%2520order%2520to%2520reduce%2520the%2520output%2520dimension%250Aof%2520the%2520model.%2520This%252C%2520in%2520turn%252C%2520improves%2520the%2520efficacy%252C%2520and%2520reduce%2520the%2520complexity%250Aof%2520an%2520AI%2520RDF%2520model.%2520The%2520model%2520is%2520shown%2520to%2520predict%2520RDFs%2520for%2520many%2520unknown%250Amixtures%2520very%2520accurately%252C%2520especially%2520outside%2520the%2520training%2520temperature%2520range.%250AOur%2520analysis%2520suggests%2520that%2520the%2520particle%2520size%2520ratio%2520has%2520a%2520higher%2520order%2520impact%2520on%250Athe%2520microstructure%2520of%2520a%2520binary%2520mixture.%2520We%2520also%2520highlight%2520the%2520areas%2520where%2520the%250Afidelity%2520of%2520the%2520AI%2520model%2520is%2520low%2520when%2520encountering%2520new%2520regimes%2520with%2520different%250Aunderlying%2520physics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Explainable%20AI%20Model%20for%20Binary%20LJ%20Fluids&entry.906535625=Israrul%20H%20Hashmi%20and%20Rahul%20Karmakar%20and%20Marripelli%20Maniteja%20and%20Kumar%20Ayush%20and%20Tarak%20K.%20Patra&entry.1292438233=%20%20Lennard-Jones%20%28LJ%29%20fluids%20serve%20as%20an%20important%20theoretical%20framework%20for%0Aunderstanding%20molecular%20interactions.%20Binary%20LJ%20fluids%2C%20where%20two%20distinct%0Aspecies%20of%20particles%20interact%20based%20on%20the%20LJ%20potential%2C%20exhibit%20rich%20phase%0Abehavior%20and%20provide%20valuable%20insights%20of%20complex%20fluid%20mixtures.%20Here%20we%0Areport%20the%20construction%20and%20utility%20of%20an%20artificial%20intelligence%20%28AI%29%20model%0Afor%20binary%20LJ%20fluids%2C%20focusing%20on%20their%20effectiveness%20in%20predicting%20radial%0Adistribution%20functions%20%28RDFs%29%20across%20a%20range%20of%20conditions.%20The%20RDFs%20of%20a%0Abinary%20mixture%20with%20varying%20compositions%20and%20temperatures%20are%20collected%20from%0Amolecular%20dynamics%20%28MD%29%20simulations%20to%20establish%20and%20validate%20the%20AI%20model.%20In%0Athis%20AI%20pipeline%2C%20RDFs%20are%20discretized%20in%20order%20to%20reduce%20the%20output%20dimension%0Aof%20the%20model.%20This%2C%20in%20turn%2C%20improves%20the%20efficacy%2C%20and%20reduce%20the%20complexity%0Aof%20an%20AI%20RDF%20model.%20The%20model%20is%20shown%20to%20predict%20RDFs%20for%20many%20unknown%0Amixtures%20very%20accurately%2C%20especially%20outside%20the%20training%20temperature%20range.%0AOur%20analysis%20suggests%20that%20the%20particle%20size%20ratio%20has%20a%20higher%20order%20impact%20on%0Athe%20microstructure%20of%20a%20binary%20mixture.%20We%20also%20highlight%20the%20areas%20where%20the%0Afidelity%20of%20the%20AI%20model%20is%20low%20when%20encountering%20new%20regimes%20with%20different%0Aunderlying%20physics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17357v1&entry.124074799=Read"},
{"title": "Implicit Repair with Reinforcement Learning in Emergent Communication", "author": "F\u00e1bio Vital and Alberto Sardinha and Francisco S. Melo", "abstract": "  Conversational repair is a mechanism used to detect and resolve\nmiscommunication and misinformation problems when two or more agents interact.\nOne particular and underexplored form of repair in emergent communication is\nthe implicit repair mechanism, where the interlocutor purposely conveys the\ndesired information in such a way as to prevent misinformation from any other\ninterlocutor. This work explores how redundancy can modify the emergent\ncommunication protocol to continue conveying the necessary information to\ncomplete the underlying task, even with additional external environmental\npressures such as noise. We focus on extending the signaling game, called the\nLewis Game, by adding noise in the communication channel and inputs received by\nthe agents. Our analysis shows that agents add redundancy to the transmitted\nmessages as an outcome to prevent the negative impact of noise on the task\nsuccess. Additionally, we observe that the emerging communication protocol's\ngeneralization capabilities remain equivalent to architectures employed in\nsimpler games that are entirely deterministic. Additionally, our method is the\nonly one suitable for producing robust communication protocols that can handle\ncases with and without noise while maintaining increased generalization\nperformance levels.\n", "link": "http://arxiv.org/abs/2502.12624v2", "date": "2025-02-24", "relevancy": 1.1949, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4061}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3982}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Repair%20with%20Reinforcement%20Learning%20in%20Emergent%20Communication&body=Title%3A%20Implicit%20Repair%20with%20Reinforcement%20Learning%20in%20Emergent%20Communication%0AAuthor%3A%20F%C3%A1bio%20Vital%20and%20Alberto%20Sardinha%20and%20Francisco%20S.%20Melo%0AAbstract%3A%20%20%20Conversational%20repair%20is%20a%20mechanism%20used%20to%20detect%20and%20resolve%0Amiscommunication%20and%20misinformation%20problems%20when%20two%20or%20more%20agents%20interact.%0AOne%20particular%20and%20underexplored%20form%20of%20repair%20in%20emergent%20communication%20is%0Athe%20implicit%20repair%20mechanism%2C%20where%20the%20interlocutor%20purposely%20conveys%20the%0Adesired%20information%20in%20such%20a%20way%20as%20to%20prevent%20misinformation%20from%20any%20other%0Ainterlocutor.%20This%20work%20explores%20how%20redundancy%20can%20modify%20the%20emergent%0Acommunication%20protocol%20to%20continue%20conveying%20the%20necessary%20information%20to%0Acomplete%20the%20underlying%20task%2C%20even%20with%20additional%20external%20environmental%0Apressures%20such%20as%20noise.%20We%20focus%20on%20extending%20the%20signaling%20game%2C%20called%20the%0ALewis%20Game%2C%20by%20adding%20noise%20in%20the%20communication%20channel%20and%20inputs%20received%20by%0Athe%20agents.%20Our%20analysis%20shows%20that%20agents%20add%20redundancy%20to%20the%20transmitted%0Amessages%20as%20an%20outcome%20to%20prevent%20the%20negative%20impact%20of%20noise%20on%20the%20task%0Asuccess.%20Additionally%2C%20we%20observe%20that%20the%20emerging%20communication%20protocol%27s%0Ageneralization%20capabilities%20remain%20equivalent%20to%20architectures%20employed%20in%0Asimpler%20games%20that%20are%20entirely%20deterministic.%20Additionally%2C%20our%20method%20is%20the%0Aonly%20one%20suitable%20for%20producing%20robust%20communication%20protocols%20that%20can%20handle%0Acases%20with%20and%20without%20noise%20while%20maintaining%20increased%20generalization%0Aperformance%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Repair%2520with%2520Reinforcement%2520Learning%2520in%2520Emergent%2520Communication%26entry.906535625%3DF%25C3%25A1bio%2520Vital%2520and%2520Alberto%2520Sardinha%2520and%2520Francisco%2520S.%2520Melo%26entry.1292438233%3D%2520%2520Conversational%2520repair%2520is%2520a%2520mechanism%2520used%2520to%2520detect%2520and%2520resolve%250Amiscommunication%2520and%2520misinformation%2520problems%2520when%2520two%2520or%2520more%2520agents%2520interact.%250AOne%2520particular%2520and%2520underexplored%2520form%2520of%2520repair%2520in%2520emergent%2520communication%2520is%250Athe%2520implicit%2520repair%2520mechanism%252C%2520where%2520the%2520interlocutor%2520purposely%2520conveys%2520the%250Adesired%2520information%2520in%2520such%2520a%2520way%2520as%2520to%2520prevent%2520misinformation%2520from%2520any%2520other%250Ainterlocutor.%2520This%2520work%2520explores%2520how%2520redundancy%2520can%2520modify%2520the%2520emergent%250Acommunication%2520protocol%2520to%2520continue%2520conveying%2520the%2520necessary%2520information%2520to%250Acomplete%2520the%2520underlying%2520task%252C%2520even%2520with%2520additional%2520external%2520environmental%250Apressures%2520such%2520as%2520noise.%2520We%2520focus%2520on%2520extending%2520the%2520signaling%2520game%252C%2520called%2520the%250ALewis%2520Game%252C%2520by%2520adding%2520noise%2520in%2520the%2520communication%2520channel%2520and%2520inputs%2520received%2520by%250Athe%2520agents.%2520Our%2520analysis%2520shows%2520that%2520agents%2520add%2520redundancy%2520to%2520the%2520transmitted%250Amessages%2520as%2520an%2520outcome%2520to%2520prevent%2520the%2520negative%2520impact%2520of%2520noise%2520on%2520the%2520task%250Asuccess.%2520Additionally%252C%2520we%2520observe%2520that%2520the%2520emerging%2520communication%2520protocol%2527s%250Ageneralization%2520capabilities%2520remain%2520equivalent%2520to%2520architectures%2520employed%2520in%250Asimpler%2520games%2520that%2520are%2520entirely%2520deterministic.%2520Additionally%252C%2520our%2520method%2520is%2520the%250Aonly%2520one%2520suitable%2520for%2520producing%2520robust%2520communication%2520protocols%2520that%2520can%2520handle%250Acases%2520with%2520and%2520without%2520noise%2520while%2520maintaining%2520increased%2520generalization%250Aperformance%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Repair%20with%20Reinforcement%20Learning%20in%20Emergent%20Communication&entry.906535625=F%C3%A1bio%20Vital%20and%20Alberto%20Sardinha%20and%20Francisco%20S.%20Melo&entry.1292438233=%20%20Conversational%20repair%20is%20a%20mechanism%20used%20to%20detect%20and%20resolve%0Amiscommunication%20and%20misinformation%20problems%20when%20two%20or%20more%20agents%20interact.%0AOne%20particular%20and%20underexplored%20form%20of%20repair%20in%20emergent%20communication%20is%0Athe%20implicit%20repair%20mechanism%2C%20where%20the%20interlocutor%20purposely%20conveys%20the%0Adesired%20information%20in%20such%20a%20way%20as%20to%20prevent%20misinformation%20from%20any%20other%0Ainterlocutor.%20This%20work%20explores%20how%20redundancy%20can%20modify%20the%20emergent%0Acommunication%20protocol%20to%20continue%20conveying%20the%20necessary%20information%20to%0Acomplete%20the%20underlying%20task%2C%20even%20with%20additional%20external%20environmental%0Apressures%20such%20as%20noise.%20We%20focus%20on%20extending%20the%20signaling%20game%2C%20called%20the%0ALewis%20Game%2C%20by%20adding%20noise%20in%20the%20communication%20channel%20and%20inputs%20received%20by%0Athe%20agents.%20Our%20analysis%20shows%20that%20agents%20add%20redundancy%20to%20the%20transmitted%0Amessages%20as%20an%20outcome%20to%20prevent%20the%20negative%20impact%20of%20noise%20on%20the%20task%0Asuccess.%20Additionally%2C%20we%20observe%20that%20the%20emerging%20communication%20protocol%27s%0Ageneralization%20capabilities%20remain%20equivalent%20to%20architectures%20employed%20in%0Asimpler%20games%20that%20are%20entirely%20deterministic.%20Additionally%2C%20our%20method%20is%20the%0Aonly%20one%20suitable%20for%20producing%20robust%20communication%20protocols%20that%20can%20handle%0Acases%20with%20and%20without%20noise%20while%20maintaining%20increased%20generalization%0Aperformance%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12624v2&entry.124074799=Read"},
{"title": "On Quantile Regression Forests for Modelling Mixed-Frequency and\n  Longitudinal Data", "author": "Mila Andreani", "abstract": "  The aim of this thesis is to extend the applications of the Quantile\nRegression Forest (QRF) algorithm to handle mixed-frequency and longitudinal\ndata. To this end, standard statistical approaches have been exploited to build\ntwo novel algorithms: the Mixed- Frequency Quantile Regression Forest\n(MIDAS-QRF) and the Finite Mixture Quantile Regression Forest (FM-QRF). The\nMIDAS-QRF combines the flexibility of QRF with the Mixed Data Sampling (MIDAS)\napproach, enabling non-parametric quantile estimation with variables observed\nat different frequencies. FM-QRF, on the other hand, extends random effects\nmachine learning algorithms to a QR framework, allowing for conditional\nquantile estimation in a longitudinal data setting. The contributions of this\ndissertation lie both methodologically and empirically. Methodologically, the\nMIDAS-QRF and the FM-QRF represent two novel approaches for handling\nmixed-frequency and longitudinal data in QR machine learning framework.\nEmpirically, the application of the proposed models in financial risk\nmanagement and climate-change impact evaluation demonstrates their validity as\naccurate and flexible models to be applied in complex empirical settings.\n", "link": "http://arxiv.org/abs/2502.17137v1", "date": "2025-02-24", "relevancy": 1.2848, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4569}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4209}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Quantile%20Regression%20Forests%20for%20Modelling%20Mixed-Frequency%20and%0A%20%20Longitudinal%20Data&body=Title%3A%20On%20Quantile%20Regression%20Forests%20for%20Modelling%20Mixed-Frequency%20and%0A%20%20Longitudinal%20Data%0AAuthor%3A%20Mila%20Andreani%0AAbstract%3A%20%20%20The%20aim%20of%20this%20thesis%20is%20to%20extend%20the%20applications%20of%20the%20Quantile%0ARegression%20Forest%20%28QRF%29%20algorithm%20to%20handle%20mixed-frequency%20and%20longitudinal%0Adata.%20To%20this%20end%2C%20standard%20statistical%20approaches%20have%20been%20exploited%20to%20build%0Atwo%20novel%20algorithms%3A%20the%20Mixed-%20Frequency%20Quantile%20Regression%20Forest%0A%28MIDAS-QRF%29%20and%20the%20Finite%20Mixture%20Quantile%20Regression%20Forest%20%28FM-QRF%29.%20The%0AMIDAS-QRF%20combines%20the%20flexibility%20of%20QRF%20with%20the%20Mixed%20Data%20Sampling%20%28MIDAS%29%0Aapproach%2C%20enabling%20non-parametric%20quantile%20estimation%20with%20variables%20observed%0Aat%20different%20frequencies.%20FM-QRF%2C%20on%20the%20other%20hand%2C%20extends%20random%20effects%0Amachine%20learning%20algorithms%20to%20a%20QR%20framework%2C%20allowing%20for%20conditional%0Aquantile%20estimation%20in%20a%20longitudinal%20data%20setting.%20The%20contributions%20of%20this%0Adissertation%20lie%20both%20methodologically%20and%20empirically.%20Methodologically%2C%20the%0AMIDAS-QRF%20and%20the%20FM-QRF%20represent%20two%20novel%20approaches%20for%20handling%0Amixed-frequency%20and%20longitudinal%20data%20in%20QR%20machine%20learning%20framework.%0AEmpirically%2C%20the%20application%20of%20the%20proposed%20models%20in%20financial%20risk%0Amanagement%20and%20climate-change%20impact%20evaluation%20demonstrates%20their%20validity%20as%0Aaccurate%20and%20flexible%20models%20to%20be%20applied%20in%20complex%20empirical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.17137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Quantile%2520Regression%2520Forests%2520for%2520Modelling%2520Mixed-Frequency%2520and%250A%2520%2520Longitudinal%2520Data%26entry.906535625%3DMila%2520Andreani%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520this%2520thesis%2520is%2520to%2520extend%2520the%2520applications%2520of%2520the%2520Quantile%250ARegression%2520Forest%2520%2528QRF%2529%2520algorithm%2520to%2520handle%2520mixed-frequency%2520and%2520longitudinal%250Adata.%2520To%2520this%2520end%252C%2520standard%2520statistical%2520approaches%2520have%2520been%2520exploited%2520to%2520build%250Atwo%2520novel%2520algorithms%253A%2520the%2520Mixed-%2520Frequency%2520Quantile%2520Regression%2520Forest%250A%2528MIDAS-QRF%2529%2520and%2520the%2520Finite%2520Mixture%2520Quantile%2520Regression%2520Forest%2520%2528FM-QRF%2529.%2520The%250AMIDAS-QRF%2520combines%2520the%2520flexibility%2520of%2520QRF%2520with%2520the%2520Mixed%2520Data%2520Sampling%2520%2528MIDAS%2529%250Aapproach%252C%2520enabling%2520non-parametric%2520quantile%2520estimation%2520with%2520variables%2520observed%250Aat%2520different%2520frequencies.%2520FM-QRF%252C%2520on%2520the%2520other%2520hand%252C%2520extends%2520random%2520effects%250Amachine%2520learning%2520algorithms%2520to%2520a%2520QR%2520framework%252C%2520allowing%2520for%2520conditional%250Aquantile%2520estimation%2520in%2520a%2520longitudinal%2520data%2520setting.%2520The%2520contributions%2520of%2520this%250Adissertation%2520lie%2520both%2520methodologically%2520and%2520empirically.%2520Methodologically%252C%2520the%250AMIDAS-QRF%2520and%2520the%2520FM-QRF%2520represent%2520two%2520novel%2520approaches%2520for%2520handling%250Amixed-frequency%2520and%2520longitudinal%2520data%2520in%2520QR%2520machine%2520learning%2520framework.%250AEmpirically%252C%2520the%2520application%2520of%2520the%2520proposed%2520models%2520in%2520financial%2520risk%250Amanagement%2520and%2520climate-change%2520impact%2520evaluation%2520demonstrates%2520their%2520validity%2520as%250Aaccurate%2520and%2520flexible%2520models%2520to%2520be%2520applied%2520in%2520complex%2520empirical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.17137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Quantile%20Regression%20Forests%20for%20Modelling%20Mixed-Frequency%20and%0A%20%20Longitudinal%20Data&entry.906535625=Mila%20Andreani&entry.1292438233=%20%20The%20aim%20of%20this%20thesis%20is%20to%20extend%20the%20applications%20of%20the%20Quantile%0ARegression%20Forest%20%28QRF%29%20algorithm%20to%20handle%20mixed-frequency%20and%20longitudinal%0Adata.%20To%20this%20end%2C%20standard%20statistical%20approaches%20have%20been%20exploited%20to%20build%0Atwo%20novel%20algorithms%3A%20the%20Mixed-%20Frequency%20Quantile%20Regression%20Forest%0A%28MIDAS-QRF%29%20and%20the%20Finite%20Mixture%20Quantile%20Regression%20Forest%20%28FM-QRF%29.%20The%0AMIDAS-QRF%20combines%20the%20flexibility%20of%20QRF%20with%20the%20Mixed%20Data%20Sampling%20%28MIDAS%29%0Aapproach%2C%20enabling%20non-parametric%20quantile%20estimation%20with%20variables%20observed%0Aat%20different%20frequencies.%20FM-QRF%2C%20on%20the%20other%20hand%2C%20extends%20random%20effects%0Amachine%20learning%20algorithms%20to%20a%20QR%20framework%2C%20allowing%20for%20conditional%0Aquantile%20estimation%20in%20a%20longitudinal%20data%20setting.%20The%20contributions%20of%20this%0Adissertation%20lie%20both%20methodologically%20and%20empirically.%20Methodologically%2C%20the%0AMIDAS-QRF%20and%20the%20FM-QRF%20represent%20two%20novel%20approaches%20for%20handling%0Amixed-frequency%20and%20longitudinal%20data%20in%20QR%20machine%20learning%20framework.%0AEmpirically%2C%20the%20application%20of%20the%20proposed%20models%20in%20financial%20risk%0Amanagement%20and%20climate-change%20impact%20evaluation%20demonstrates%20their%20validity%20as%0Aaccurate%20and%20flexible%20models%20to%20be%20applied%20in%20complex%20empirical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.17137v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


