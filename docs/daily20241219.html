<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241218.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians", "author": "Xiaobao Wei and Peng Chen and Ming Lu and Hui Chen and Feng Tian", "abstract": "  Rendering photorealistic head avatars from arbitrary viewpoints is crucial\nfor various applications like virtual reality. Although previous methods based\non Neural Radiance Fields (NeRF) can achieve impressive results, they lack\nfidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have\nimproved rendering quality and real-time performance but still require\nsignificant storage overhead. In this paper, we introduce a method called\nGraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians\nfor the head avatar. Specifically, GraphAvatar trains a geometric GNN and an\nappearance GNN to generate the attributes of the 3D Gaussians from the tracked\nmesh. Therefore, our method can store the GNN models instead of the 3D\nGaussians, significantly reducing the storage overhead to just 10MB. To reduce\nthe impact of face-tracking errors, we also present a novel graph-guided\noptimization module to refine face-tracking parameters during training.\nFinally, we introduce a 3D-aware enhancer for post-processing to enhance the\nrendering quality. We conduct comprehensive experiments to demonstrate the\nadvantages of GraphAvatar, surpassing existing methods in visual fidelity and\nstorage consumption. The ablation study sheds light on the trade-offs between\nrendering quality and model size. The code will be released at:\nhttps://github.com/ucwxb/GraphAvatar\n", "link": "http://arxiv.org/abs/2412.13983v1", "date": "2024-12-18", "relevancy": 3.5587, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7519}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7519}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphAvatar%3A%20Compact%20Head%20Avatars%20with%20GNN-Generated%203D%20Gaussians&body=Title%3A%20GraphAvatar%3A%20Compact%20Head%20Avatars%20with%20GNN-Generated%203D%20Gaussians%0AAuthor%3A%20Xiaobao%20Wei%20and%20Peng%20Chen%20and%20Ming%20Lu%20and%20Hui%20Chen%20and%20Feng%20Tian%0AAbstract%3A%20%20%20Rendering%20photorealistic%20head%20avatars%20from%20arbitrary%20viewpoints%20is%20crucial%0Afor%20various%20applications%20like%20virtual%20reality.%20Although%20previous%20methods%20based%0Aon%20Neural%20Radiance%20Fields%20%28NeRF%29%20can%20achieve%20impressive%20results%2C%20they%20lack%0Afidelity%20and%20efficiency.%20Recent%20methods%20using%203D%20Gaussian%20Splatting%20%283DGS%29%20have%0Aimproved%20rendering%20quality%20and%20real-time%20performance%20but%20still%20require%0Asignificant%20storage%20overhead.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20called%0AGraphAvatar%20that%20utilizes%20Graph%20Neural%20Networks%20%28GNN%29%20to%20generate%203D%20Gaussians%0Afor%20the%20head%20avatar.%20Specifically%2C%20GraphAvatar%20trains%20a%20geometric%20GNN%20and%20an%0Aappearance%20GNN%20to%20generate%20the%20attributes%20of%20the%203D%20Gaussians%20from%20the%20tracked%0Amesh.%20Therefore%2C%20our%20method%20can%20store%20the%20GNN%20models%20instead%20of%20the%203D%0AGaussians%2C%20significantly%20reducing%20the%20storage%20overhead%20to%20just%2010MB.%20To%20reduce%0Athe%20impact%20of%20face-tracking%20errors%2C%20we%20also%20present%20a%20novel%20graph-guided%0Aoptimization%20module%20to%20refine%20face-tracking%20parameters%20during%20training.%0AFinally%2C%20we%20introduce%20a%203D-aware%20enhancer%20for%20post-processing%20to%20enhance%20the%0Arendering%20quality.%20We%20conduct%20comprehensive%20experiments%20to%20demonstrate%20the%0Aadvantages%20of%20GraphAvatar%2C%20surpassing%20existing%20methods%20in%20visual%20fidelity%20and%0Astorage%20consumption.%20The%20ablation%20study%20sheds%20light%20on%20the%20trade-offs%20between%0Arendering%20quality%20and%20model%20size.%20The%20code%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/ucwxb/GraphAvatar%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphAvatar%253A%2520Compact%2520Head%2520Avatars%2520with%2520GNN-Generated%25203D%2520Gaussians%26entry.906535625%3DXiaobao%2520Wei%2520and%2520Peng%2520Chen%2520and%2520Ming%2520Lu%2520and%2520Hui%2520Chen%2520and%2520Feng%2520Tian%26entry.1292438233%3D%2520%2520Rendering%2520photorealistic%2520head%2520avatars%2520from%2520arbitrary%2520viewpoints%2520is%2520crucial%250Afor%2520various%2520applications%2520like%2520virtual%2520reality.%2520Although%2520previous%2520methods%2520based%250Aon%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520can%2520achieve%2520impressive%2520results%252C%2520they%2520lack%250Afidelity%2520and%2520efficiency.%2520Recent%2520methods%2520using%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%250Aimproved%2520rendering%2520quality%2520and%2520real-time%2520performance%2520but%2520still%2520require%250Asignificant%2520storage%2520overhead.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520method%2520called%250AGraphAvatar%2520that%2520utilizes%2520Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520to%2520generate%25203D%2520Gaussians%250Afor%2520the%2520head%2520avatar.%2520Specifically%252C%2520GraphAvatar%2520trains%2520a%2520geometric%2520GNN%2520and%2520an%250Aappearance%2520GNN%2520to%2520generate%2520the%2520attributes%2520of%2520the%25203D%2520Gaussians%2520from%2520the%2520tracked%250Amesh.%2520Therefore%252C%2520our%2520method%2520can%2520store%2520the%2520GNN%2520models%2520instead%2520of%2520the%25203D%250AGaussians%252C%2520significantly%2520reducing%2520the%2520storage%2520overhead%2520to%2520just%252010MB.%2520To%2520reduce%250Athe%2520impact%2520of%2520face-tracking%2520errors%252C%2520we%2520also%2520present%2520a%2520novel%2520graph-guided%250Aoptimization%2520module%2520to%2520refine%2520face-tracking%2520parameters%2520during%2520training.%250AFinally%252C%2520we%2520introduce%2520a%25203D-aware%2520enhancer%2520for%2520post-processing%2520to%2520enhance%2520the%250Arendering%2520quality.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%2520demonstrate%2520the%250Aadvantages%2520of%2520GraphAvatar%252C%2520surpassing%2520existing%2520methods%2520in%2520visual%2520fidelity%2520and%250Astorage%2520consumption.%2520The%2520ablation%2520study%2520sheds%2520light%2520on%2520the%2520trade-offs%2520between%250Arendering%2520quality%2520and%2520model%2520size.%2520The%2520code%2520will%2520be%2520released%2520at%253A%250Ahttps%253A//github.com/ucwxb/GraphAvatar%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphAvatar%3A%20Compact%20Head%20Avatars%20with%20GNN-Generated%203D%20Gaussians&entry.906535625=Xiaobao%20Wei%20and%20Peng%20Chen%20and%20Ming%20Lu%20and%20Hui%20Chen%20and%20Feng%20Tian&entry.1292438233=%20%20Rendering%20photorealistic%20head%20avatars%20from%20arbitrary%20viewpoints%20is%20crucial%0Afor%20various%20applications%20like%20virtual%20reality.%20Although%20previous%20methods%20based%0Aon%20Neural%20Radiance%20Fields%20%28NeRF%29%20can%20achieve%20impressive%20results%2C%20they%20lack%0Afidelity%20and%20efficiency.%20Recent%20methods%20using%203D%20Gaussian%20Splatting%20%283DGS%29%20have%0Aimproved%20rendering%20quality%20and%20real-time%20performance%20but%20still%20require%0Asignificant%20storage%20overhead.%20In%20this%20paper%2C%20we%20introduce%20a%20method%20called%0AGraphAvatar%20that%20utilizes%20Graph%20Neural%20Networks%20%28GNN%29%20to%20generate%203D%20Gaussians%0Afor%20the%20head%20avatar.%20Specifically%2C%20GraphAvatar%20trains%20a%20geometric%20GNN%20and%20an%0Aappearance%20GNN%20to%20generate%20the%20attributes%20of%20the%203D%20Gaussians%20from%20the%20tracked%0Amesh.%20Therefore%2C%20our%20method%20can%20store%20the%20GNN%20models%20instead%20of%20the%203D%0AGaussians%2C%20significantly%20reducing%20the%20storage%20overhead%20to%20just%2010MB.%20To%20reduce%0Athe%20impact%20of%20face-tracking%20errors%2C%20we%20also%20present%20a%20novel%20graph-guided%0Aoptimization%20module%20to%20refine%20face-tracking%20parameters%20during%20training.%0AFinally%2C%20we%20introduce%20a%203D-aware%20enhancer%20for%20post-processing%20to%20enhance%20the%0Arendering%20quality.%20We%20conduct%20comprehensive%20experiments%20to%20demonstrate%20the%0Aadvantages%20of%20GraphAvatar%2C%20surpassing%20existing%20methods%20in%20visual%20fidelity%20and%0Astorage%20consumption.%20The%20ablation%20study%20sheds%20light%20on%20the%20trade-offs%20between%0Arendering%20quality%20and%20model%20size.%20The%20code%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/ucwxb/GraphAvatar%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13983v1&entry.124074799=Read"},
{"title": "MCMat: Multiview-Consistent and Physically Accurate PBR Material\n  Generation", "author": "Shenhao Zhu and Lingteng Qiu and Xiaodong Gu and Zhengyi Zhao and Chao Xu and Yuxiao He and Zhe Li and Xiaoguang Han and Yao Yao and Xun Cao and Siyu Zhu and Weihao Yuan and Zilong Dong and Hao Zhu", "abstract": "  Existing 2D methods utilize UNet-based diffusion models to generate\nmulti-view physically-based rendering (PBR) maps but struggle with multi-view\ninconsistency, while some 3D methods directly generate UV maps, encountering\ngeneralization issues due to the limited 3D data. To address these problems, we\npropose a two-stage approach, including multi-view generation and UV materials\nrefinement. In the generation stage, we adopt a Diffusion Transformer (DiT)\nmodel to generate PBR materials, where both the specially designed multi-branch\nDiT and reference-based DiT blocks adopt a global attention mechanism to\npromote feature interaction and fusion between different views, thereby\nimproving multi-view consistency. In addition, we adopt a PBR-based diffusion\nloss to ensure that the generated materials align with realistic physical\nprinciples. In the refinement stage, we propose a material-refined DiT that\nperforms inpainting in empty areas and enhances details in UV space. Except for\nthe normal condition, this refinement also takes the material map from the\ngeneration stage as an additional condition to reduce the learning difficulty\nand improve generalization. Extensive experiments show that our method achieves\nstate-of-the-art performance in texturing 3D objects with PBR materials and\nprovides significant advantages for graphics relighting applications. Project\nPage: https://lingtengqiu.github.io/2024/MCMat/\n", "link": "http://arxiv.org/abs/2412.14148v1", "date": "2024-12-18", "relevancy": 3.0652, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6225}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCMat%3A%20Multiview-Consistent%20and%20Physically%20Accurate%20PBR%20Material%0A%20%20Generation&body=Title%3A%20MCMat%3A%20Multiview-Consistent%20and%20Physically%20Accurate%20PBR%20Material%0A%20%20Generation%0AAuthor%3A%20Shenhao%20Zhu%20and%20Lingteng%20Qiu%20and%20Xiaodong%20Gu%20and%20Zhengyi%20Zhao%20and%20Chao%20Xu%20and%20Yuxiao%20He%20and%20Zhe%20Li%20and%20Xiaoguang%20Han%20and%20Yao%20Yao%20and%20Xun%20Cao%20and%20Siyu%20Zhu%20and%20Weihao%20Yuan%20and%20Zilong%20Dong%20and%20Hao%20Zhu%0AAbstract%3A%20%20%20Existing%202D%20methods%20utilize%20UNet-based%20diffusion%20models%20to%20generate%0Amulti-view%20physically-based%20rendering%20%28PBR%29%20maps%20but%20struggle%20with%20multi-view%0Ainconsistency%2C%20while%20some%203D%20methods%20directly%20generate%20UV%20maps%2C%20encountering%0Ageneralization%20issues%20due%20to%20the%20limited%203D%20data.%20To%20address%20these%20problems%2C%20we%0Apropose%20a%20two-stage%20approach%2C%20including%20multi-view%20generation%20and%20UV%20materials%0Arefinement.%20In%20the%20generation%20stage%2C%20we%20adopt%20a%20Diffusion%20Transformer%20%28DiT%29%0Amodel%20to%20generate%20PBR%20materials%2C%20where%20both%20the%20specially%20designed%20multi-branch%0ADiT%20and%20reference-based%20DiT%20blocks%20adopt%20a%20global%20attention%20mechanism%20to%0Apromote%20feature%20interaction%20and%20fusion%20between%20different%20views%2C%20thereby%0Aimproving%20multi-view%20consistency.%20In%20addition%2C%20we%20adopt%20a%20PBR-based%20diffusion%0Aloss%20to%20ensure%20that%20the%20generated%20materials%20align%20with%20realistic%20physical%0Aprinciples.%20In%20the%20refinement%20stage%2C%20we%20propose%20a%20material-refined%20DiT%20that%0Aperforms%20inpainting%20in%20empty%20areas%20and%20enhances%20details%20in%20UV%20space.%20Except%20for%0Athe%20normal%20condition%2C%20this%20refinement%20also%20takes%20the%20material%20map%20from%20the%0Ageneration%20stage%20as%20an%20additional%20condition%20to%20reduce%20the%20learning%20difficulty%0Aand%20improve%20generalization.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20texturing%203D%20objects%20with%20PBR%20materials%20and%0Aprovides%20significant%20advantages%20for%20graphics%20relighting%20applications.%20Project%0APage%3A%20https%3A//lingtengqiu.github.io/2024/MCMat/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCMat%253A%2520Multiview-Consistent%2520and%2520Physically%2520Accurate%2520PBR%2520Material%250A%2520%2520Generation%26entry.906535625%3DShenhao%2520Zhu%2520and%2520Lingteng%2520Qiu%2520and%2520Xiaodong%2520Gu%2520and%2520Zhengyi%2520Zhao%2520and%2520Chao%2520Xu%2520and%2520Yuxiao%2520He%2520and%2520Zhe%2520Li%2520and%2520Xiaoguang%2520Han%2520and%2520Yao%2520Yao%2520and%2520Xun%2520Cao%2520and%2520Siyu%2520Zhu%2520and%2520Weihao%2520Yuan%2520and%2520Zilong%2520Dong%2520and%2520Hao%2520Zhu%26entry.1292438233%3D%2520%2520Existing%25202D%2520methods%2520utilize%2520UNet-based%2520diffusion%2520models%2520to%2520generate%250Amulti-view%2520physically-based%2520rendering%2520%2528PBR%2529%2520maps%2520but%2520struggle%2520with%2520multi-view%250Ainconsistency%252C%2520while%2520some%25203D%2520methods%2520directly%2520generate%2520UV%2520maps%252C%2520encountering%250Ageneralization%2520issues%2520due%2520to%2520the%2520limited%25203D%2520data.%2520To%2520address%2520these%2520problems%252C%2520we%250Apropose%2520a%2520two-stage%2520approach%252C%2520including%2520multi-view%2520generation%2520and%2520UV%2520materials%250Arefinement.%2520In%2520the%2520generation%2520stage%252C%2520we%2520adopt%2520a%2520Diffusion%2520Transformer%2520%2528DiT%2529%250Amodel%2520to%2520generate%2520PBR%2520materials%252C%2520where%2520both%2520the%2520specially%2520designed%2520multi-branch%250ADiT%2520and%2520reference-based%2520DiT%2520blocks%2520adopt%2520a%2520global%2520attention%2520mechanism%2520to%250Apromote%2520feature%2520interaction%2520and%2520fusion%2520between%2520different%2520views%252C%2520thereby%250Aimproving%2520multi-view%2520consistency.%2520In%2520addition%252C%2520we%2520adopt%2520a%2520PBR-based%2520diffusion%250Aloss%2520to%2520ensure%2520that%2520the%2520generated%2520materials%2520align%2520with%2520realistic%2520physical%250Aprinciples.%2520In%2520the%2520refinement%2520stage%252C%2520we%2520propose%2520a%2520material-refined%2520DiT%2520that%250Aperforms%2520inpainting%2520in%2520empty%2520areas%2520and%2520enhances%2520details%2520in%2520UV%2520space.%2520Except%2520for%250Athe%2520normal%2520condition%252C%2520this%2520refinement%2520also%2520takes%2520the%2520material%2520map%2520from%2520the%250Ageneration%2520stage%2520as%2520an%2520additional%2520condition%2520to%2520reduce%2520the%2520learning%2520difficulty%250Aand%2520improve%2520generalization.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520in%2520texturing%25203D%2520objects%2520with%2520PBR%2520materials%2520and%250Aprovides%2520significant%2520advantages%2520for%2520graphics%2520relighting%2520applications.%2520Project%250APage%253A%2520https%253A//lingtengqiu.github.io/2024/MCMat/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCMat%3A%20Multiview-Consistent%20and%20Physically%20Accurate%20PBR%20Material%0A%20%20Generation&entry.906535625=Shenhao%20Zhu%20and%20Lingteng%20Qiu%20and%20Xiaodong%20Gu%20and%20Zhengyi%20Zhao%20and%20Chao%20Xu%20and%20Yuxiao%20He%20and%20Zhe%20Li%20and%20Xiaoguang%20Han%20and%20Yao%20Yao%20and%20Xun%20Cao%20and%20Siyu%20Zhu%20and%20Weihao%20Yuan%20and%20Zilong%20Dong%20and%20Hao%20Zhu&entry.1292438233=%20%20Existing%202D%20methods%20utilize%20UNet-based%20diffusion%20models%20to%20generate%0Amulti-view%20physically-based%20rendering%20%28PBR%29%20maps%20but%20struggle%20with%20multi-view%0Ainconsistency%2C%20while%20some%203D%20methods%20directly%20generate%20UV%20maps%2C%20encountering%0Ageneralization%20issues%20due%20to%20the%20limited%203D%20data.%20To%20address%20these%20problems%2C%20we%0Apropose%20a%20two-stage%20approach%2C%20including%20multi-view%20generation%20and%20UV%20materials%0Arefinement.%20In%20the%20generation%20stage%2C%20we%20adopt%20a%20Diffusion%20Transformer%20%28DiT%29%0Amodel%20to%20generate%20PBR%20materials%2C%20where%20both%20the%20specially%20designed%20multi-branch%0ADiT%20and%20reference-based%20DiT%20blocks%20adopt%20a%20global%20attention%20mechanism%20to%0Apromote%20feature%20interaction%20and%20fusion%20between%20different%20views%2C%20thereby%0Aimproving%20multi-view%20consistency.%20In%20addition%2C%20we%20adopt%20a%20PBR-based%20diffusion%0Aloss%20to%20ensure%20that%20the%20generated%20materials%20align%20with%20realistic%20physical%0Aprinciples.%20In%20the%20refinement%20stage%2C%20we%20propose%20a%20material-refined%20DiT%20that%0Aperforms%20inpainting%20in%20empty%20areas%20and%20enhances%20details%20in%20UV%20space.%20Except%20for%0Athe%20normal%20condition%2C%20this%20refinement%20also%20takes%20the%20material%20map%20from%20the%0Ageneration%20stage%20as%20an%20additional%20condition%20to%20reduce%20the%20learning%20difficulty%0Aand%20improve%20generalization.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%20in%20texturing%203D%20objects%20with%20PBR%20materials%20and%0Aprovides%20significant%20advantages%20for%20graphics%20relighting%20applications.%20Project%0APage%3A%20https%3A//lingtengqiu.github.io/2024/MCMat/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14148v1&entry.124074799=Read"},
{"title": "DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion\n  Priors", "author": "Tianyu Huang and Haoze Zhang and Yihan Zeng and Zhilu Zhang and Hui Li and Wangmeng Zuo and Rynson W. H. Lau", "abstract": "  Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physics priors. In this work, to\ncombine the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. In addition, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motions than\nstate-of-the-arts do.\n", "link": "http://arxiv.org/abs/2406.01476v3", "date": "2024-12-18", "relevancy": 3.0431, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6558}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamPhysics%3A%20Learning%20Physics-Based%203D%20Dynamics%20with%20Video%20Diffusion%0A%20%20Priors&body=Title%3A%20DreamPhysics%3A%20Learning%20Physics-Based%203D%20Dynamics%20with%20Video%20Diffusion%0A%20%20Priors%0AAuthor%3A%20Tianyu%20Huang%20and%20Haoze%20Zhang%20and%20Yihan%20Zeng%20and%20Zhilu%20Zhang%20and%20Hui%20Li%20and%20Wangmeng%20Zuo%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Dynamic%203D%20interaction%20has%20been%20attracting%20a%20lot%20of%20attention%20recently.%0AHowever%2C%20creating%20such%204D%20content%20remains%20challenging.%20One%20solution%20is%20to%0Aanimate%203D%20scenes%20with%20physics-based%20simulation%2C%20which%20requires%20manually%0Aassigning%20precise%20physical%20properties%20to%20the%20object%20or%20the%20simulated%20results%0Awould%20become%20unnatural.%20Another%20solution%20is%20to%20learn%20the%20deformation%20of%203D%0Aobjects%20with%20the%20distillation%20of%20video%20generative%20models%2C%20which%2C%20however%2C%20tends%0Ato%20produce%203D%20videos%20with%20small%20and%20discontinuous%20motions%20due%20to%20the%0Ainappropriate%20extraction%20and%20application%20of%20physics%20priors.%20In%20this%20work%2C%20to%0Acombine%20the%20strengths%20and%20complementing%20shortcomings%20of%20the%20above%20two%0Asolutions%2C%20we%20propose%20to%20learn%20the%20physical%20properties%20of%20a%20material%20field%20with%0Avideo%20diffusion%20priors%2C%20and%20then%20utilize%20a%20physics-based%20Material-Point-Method%0A%28MPM%29%20simulator%20to%20generate%204D%20content%20with%20realistic%20motions.%20In%20particular%2C%0Awe%20propose%20motion%20distillation%20sampling%20to%20emphasize%20video%20motion%20information%0Aduring%20distillation.%20In%20addition%2C%20to%20facilitate%20the%20optimization%2C%20we%20further%0Apropose%20a%20KAN-based%20material%20field%20with%20frame%20boosting.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20enjoys%20more%20realistic%20motions%20than%0Astate-of-the-arts%20do.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01476v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamPhysics%253A%2520Learning%2520Physics-Based%25203D%2520Dynamics%2520with%2520Video%2520Diffusion%250A%2520%2520Priors%26entry.906535625%3DTianyu%2520Huang%2520and%2520Haoze%2520Zhang%2520and%2520Yihan%2520Zeng%2520and%2520Zhilu%2520Zhang%2520and%2520Hui%2520Li%2520and%2520Wangmeng%2520Zuo%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Dynamic%25203D%2520interaction%2520has%2520been%2520attracting%2520a%2520lot%2520of%2520attention%2520recently.%250AHowever%252C%2520creating%2520such%25204D%2520content%2520remains%2520challenging.%2520One%2520solution%2520is%2520to%250Aanimate%25203D%2520scenes%2520with%2520physics-based%2520simulation%252C%2520which%2520requires%2520manually%250Aassigning%2520precise%2520physical%2520properties%2520to%2520the%2520object%2520or%2520the%2520simulated%2520results%250Awould%2520become%2520unnatural.%2520Another%2520solution%2520is%2520to%2520learn%2520the%2520deformation%2520of%25203D%250Aobjects%2520with%2520the%2520distillation%2520of%2520video%2520generative%2520models%252C%2520which%252C%2520however%252C%2520tends%250Ato%2520produce%25203D%2520videos%2520with%2520small%2520and%2520discontinuous%2520motions%2520due%2520to%2520the%250Ainappropriate%2520extraction%2520and%2520application%2520of%2520physics%2520priors.%2520In%2520this%2520work%252C%2520to%250Acombine%2520the%2520strengths%2520and%2520complementing%2520shortcomings%2520of%2520the%2520above%2520two%250Asolutions%252C%2520we%2520propose%2520to%2520learn%2520the%2520physical%2520properties%2520of%2520a%2520material%2520field%2520with%250Avideo%2520diffusion%2520priors%252C%2520and%2520then%2520utilize%2520a%2520physics-based%2520Material-Point-Method%250A%2528MPM%2529%2520simulator%2520to%2520generate%25204D%2520content%2520with%2520realistic%2520motions.%2520In%2520particular%252C%250Awe%2520propose%2520motion%2520distillation%2520sampling%2520to%2520emphasize%2520video%2520motion%2520information%250Aduring%2520distillation.%2520In%2520addition%252C%2520to%2520facilitate%2520the%2520optimization%252C%2520we%2520further%250Apropose%2520a%2520KAN-based%2520material%2520field%2520with%2520frame%2520boosting.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520method%2520enjoys%2520more%2520realistic%2520motions%2520than%250Astate-of-the-arts%2520do.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01476v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamPhysics%3A%20Learning%20Physics-Based%203D%20Dynamics%20with%20Video%20Diffusion%0A%20%20Priors&entry.906535625=Tianyu%20Huang%20and%20Haoze%20Zhang%20and%20Yihan%20Zeng%20and%20Zhilu%20Zhang%20and%20Hui%20Li%20and%20Wangmeng%20Zuo%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Dynamic%203D%20interaction%20has%20been%20attracting%20a%20lot%20of%20attention%20recently.%0AHowever%2C%20creating%20such%204D%20content%20remains%20challenging.%20One%20solution%20is%20to%0Aanimate%203D%20scenes%20with%20physics-based%20simulation%2C%20which%20requires%20manually%0Aassigning%20precise%20physical%20properties%20to%20the%20object%20or%20the%20simulated%20results%0Awould%20become%20unnatural.%20Another%20solution%20is%20to%20learn%20the%20deformation%20of%203D%0Aobjects%20with%20the%20distillation%20of%20video%20generative%20models%2C%20which%2C%20however%2C%20tends%0Ato%20produce%203D%20videos%20with%20small%20and%20discontinuous%20motions%20due%20to%20the%0Ainappropriate%20extraction%20and%20application%20of%20physics%20priors.%20In%20this%20work%2C%20to%0Acombine%20the%20strengths%20and%20complementing%20shortcomings%20of%20the%20above%20two%0Asolutions%2C%20we%20propose%20to%20learn%20the%20physical%20properties%20of%20a%20material%20field%20with%0Avideo%20diffusion%20priors%2C%20and%20then%20utilize%20a%20physics-based%20Material-Point-Method%0A%28MPM%29%20simulator%20to%20generate%204D%20content%20with%20realistic%20motions.%20In%20particular%2C%0Awe%20propose%20motion%20distillation%20sampling%20to%20emphasize%20video%20motion%20information%0Aduring%20distillation.%20In%20addition%2C%20to%20facilitate%20the%20optimization%2C%20we%20further%0Apropose%20a%20KAN-based%20material%20field%20with%20frame%20boosting.%20Experimental%20results%0Ademonstrate%20that%20our%20method%20enjoys%20more%20realistic%20motions%20than%0Astate-of-the-arts%20do.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01476v3&entry.124074799=Read"},
{"title": "A Concept-Centric Approach to Multi-Modality Learning", "author": "Yuchong Geng and Ao Tang", "abstract": "  In an effort to create a more efficient AI system, we introduce a new\nmulti-modality learning framework that leverages a modality-agnostic concept\nspace possessing abstract knowledge and a set of modality-specific projection\nmodels tailored to process distinct modality inputs and map them onto the\nconcept space. Decoupled from specific modalities and their associated\nprojection models, the concept space focuses on learning abstract knowledge\nthat is universally applicable across modalities. Subsequently, the knowledge\nembedded into the concept space streamlines the learning processes of\nmodality-specific projection models. We evaluate our framework on two popular\ntasks: Image-Text Matching and Visual Question Answering. Our framework\nachieves performance on par with benchmark models while demonstrating more\nefficient learning curves.\n", "link": "http://arxiv.org/abs/2412.13847v1", "date": "2024-12-18", "relevancy": 3.0162, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Concept-Centric%20Approach%20to%20Multi-Modality%20Learning&body=Title%3A%20A%20Concept-Centric%20Approach%20to%20Multi-Modality%20Learning%0AAuthor%3A%20Yuchong%20Geng%20and%20Ao%20Tang%0AAbstract%3A%20%20%20In%20an%20effort%20to%20create%20a%20more%20efficient%20AI%20system%2C%20we%20introduce%20a%20new%0Amulti-modality%20learning%20framework%20that%20leverages%20a%20modality-agnostic%20concept%0Aspace%20possessing%20abstract%20knowledge%20and%20a%20set%20of%20modality-specific%20projection%0Amodels%20tailored%20to%20process%20distinct%20modality%20inputs%20and%20map%20them%20onto%20the%0Aconcept%20space.%20Decoupled%20from%20specific%20modalities%20and%20their%20associated%0Aprojection%20models%2C%20the%20concept%20space%20focuses%20on%20learning%20abstract%20knowledge%0Athat%20is%20universally%20applicable%20across%20modalities.%20Subsequently%2C%20the%20knowledge%0Aembedded%20into%20the%20concept%20space%20streamlines%20the%20learning%20processes%20of%0Amodality-specific%20projection%20models.%20We%20evaluate%20our%20framework%20on%20two%20popular%0Atasks%3A%20Image-Text%20Matching%20and%20Visual%20Question%20Answering.%20Our%20framework%0Aachieves%20performance%20on%20par%20with%20benchmark%20models%20while%20demonstrating%20more%0Aefficient%20learning%20curves.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Concept-Centric%2520Approach%2520to%2520Multi-Modality%2520Learning%26entry.906535625%3DYuchong%2520Geng%2520and%2520Ao%2520Tang%26entry.1292438233%3D%2520%2520In%2520an%2520effort%2520to%2520create%2520a%2520more%2520efficient%2520AI%2520system%252C%2520we%2520introduce%2520a%2520new%250Amulti-modality%2520learning%2520framework%2520that%2520leverages%2520a%2520modality-agnostic%2520concept%250Aspace%2520possessing%2520abstract%2520knowledge%2520and%2520a%2520set%2520of%2520modality-specific%2520projection%250Amodels%2520tailored%2520to%2520process%2520distinct%2520modality%2520inputs%2520and%2520map%2520them%2520onto%2520the%250Aconcept%2520space.%2520Decoupled%2520from%2520specific%2520modalities%2520and%2520their%2520associated%250Aprojection%2520models%252C%2520the%2520concept%2520space%2520focuses%2520on%2520learning%2520abstract%2520knowledge%250Athat%2520is%2520universally%2520applicable%2520across%2520modalities.%2520Subsequently%252C%2520the%2520knowledge%250Aembedded%2520into%2520the%2520concept%2520space%2520streamlines%2520the%2520learning%2520processes%2520of%250Amodality-specific%2520projection%2520models.%2520We%2520evaluate%2520our%2520framework%2520on%2520two%2520popular%250Atasks%253A%2520Image-Text%2520Matching%2520and%2520Visual%2520Question%2520Answering.%2520Our%2520framework%250Aachieves%2520performance%2520on%2520par%2520with%2520benchmark%2520models%2520while%2520demonstrating%2520more%250Aefficient%2520learning%2520curves.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Concept-Centric%20Approach%20to%20Multi-Modality%20Learning&entry.906535625=Yuchong%20Geng%20and%20Ao%20Tang&entry.1292438233=%20%20In%20an%20effort%20to%20create%20a%20more%20efficient%20AI%20system%2C%20we%20introduce%20a%20new%0Amulti-modality%20learning%20framework%20that%20leverages%20a%20modality-agnostic%20concept%0Aspace%20possessing%20abstract%20knowledge%20and%20a%20set%20of%20modality-specific%20projection%0Amodels%20tailored%20to%20process%20distinct%20modality%20inputs%20and%20map%20them%20onto%20the%0Aconcept%20space.%20Decoupled%20from%20specific%20modalities%20and%20their%20associated%0Aprojection%20models%2C%20the%20concept%20space%20focuses%20on%20learning%20abstract%20knowledge%0Athat%20is%20universally%20applicable%20across%20modalities.%20Subsequently%2C%20the%20knowledge%0Aembedded%20into%20the%20concept%20space%20streamlines%20the%20learning%20processes%20of%0Amodality-specific%20projection%20models.%20We%20evaluate%20our%20framework%20on%20two%20popular%0Atasks%3A%20Image-Text%20Matching%20and%20Visual%20Question%20Answering.%20Our%20framework%0Aachieves%20performance%20on%20par%20with%20benchmark%20models%20while%20demonstrating%20more%0Aefficient%20learning%20curves.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13847v1&entry.124074799=Read"},
{"title": "Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration", "author": "Ziheng Zhou and Jinxing Zhou and Wei Qian and Shengeng Tang and Xiaojun Chang and Dan Guo", "abstract": "  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, untrimmed videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel CCNet, comprising two core modules: the\nCross-Modal Consistency Collaboration (CMCC) and the Multi-Temporal Granularity\nCollaboration (MTGC). Specifically, the CMCC module contains two branches: a\ncross-modal interaction branch and a temporal consistency-gated branch. The\nformer branch facilitates the aggregation of consistent event semantics across\nmodalities through the encoding of audio-visual relations, while the latter\nbranch guides one modality's focus to pivotal event-relevant temporal areas as\ndiscerned in the other modality. The MTGC module includes a coarse-to-fine\ncollaboration block and a fine-to-coarse collaboration block, providing\nbidirectional support among coarse- and fine-grained temporal features.\nExtensive experiments on the UnAV-100 dataset validate our module design,\nresulting in a new state-of-the-art performance in dense audio-visual event\nlocalization. The code is available at\nhttps://github.com/zzhhfut/CCNet-AAAI2025.\n", "link": "http://arxiv.org/abs/2412.12628v2", "date": "2024-12-18", "relevancy": 2.9902, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Audio-Visual%20Event%20Localization%20under%20Cross-Modal%20Consistency%20and%0A%20%20Multi-Temporal%20Granularity%20Collaboration&body=Title%3A%20Dense%20Audio-Visual%20Event%20Localization%20under%20Cross-Modal%20Consistency%20and%0A%20%20Multi-Temporal%20Granularity%20Collaboration%0AAuthor%3A%20Ziheng%20Zhou%20and%20Jinxing%20Zhou%20and%20Wei%20Qian%20and%20Shengeng%20Tang%20and%20Xiaojun%20Chang%20and%20Dan%20Guo%0AAbstract%3A%20%20%20In%20the%20field%20of%20audio-visual%20learning%2C%20most%20research%20tasks%20focus%20exclusively%0Aon%20short%20videos.%20This%20paper%20focuses%20on%20the%20more%20practical%20Dense%20Audio-Visual%0AEvent%20Localization%20%28DAVEL%29%20task%2C%20advancing%20audio-visual%20scene%20understanding%20for%0Alonger%2C%20untrimmed%20videos.%20This%20task%20seeks%20to%20identify%20and%20temporally%20pinpoint%0Aall%20events%20simultaneously%20occurring%20in%20both%20audio%20and%20visual%20streams.%0ATypically%2C%20each%20video%20encompasses%20dense%20events%20of%20multiple%20classes%2C%20which%20may%0Aoverlap%20on%20the%20timeline%2C%20each%20exhibiting%20varied%20durations.%20Given%20these%0Achallenges%2C%20effectively%20exploiting%20the%20audio-visual%20relations%20and%20the%20temporal%0Afeatures%20encoded%20at%20various%20granularities%20becomes%20crucial.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20CCNet%2C%20comprising%20two%20core%20modules%3A%20the%0ACross-Modal%20Consistency%20Collaboration%20%28CMCC%29%20and%20the%20Multi-Temporal%20Granularity%0ACollaboration%20%28MTGC%29.%20Specifically%2C%20the%20CMCC%20module%20contains%20two%20branches%3A%20a%0Across-modal%20interaction%20branch%20and%20a%20temporal%20consistency-gated%20branch.%20The%0Aformer%20branch%20facilitates%20the%20aggregation%20of%20consistent%20event%20semantics%20across%0Amodalities%20through%20the%20encoding%20of%20audio-visual%20relations%2C%20while%20the%20latter%0Abranch%20guides%20one%20modality%27s%20focus%20to%20pivotal%20event-relevant%20temporal%20areas%20as%0Adiscerned%20in%20the%20other%20modality.%20The%20MTGC%20module%20includes%20a%20coarse-to-fine%0Acollaboration%20block%20and%20a%20fine-to-coarse%20collaboration%20block%2C%20providing%0Abidirectional%20support%20among%20coarse-%20and%20fine-grained%20temporal%20features.%0AExtensive%20experiments%20on%20the%20UnAV-100%20dataset%20validate%20our%20module%20design%2C%0Aresulting%20in%20a%20new%20state-of-the-art%20performance%20in%20dense%20audio-visual%20event%0Alocalization.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zzhhfut/CCNet-AAAI2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Audio-Visual%2520Event%2520Localization%2520under%2520Cross-Modal%2520Consistency%2520and%250A%2520%2520Multi-Temporal%2520Granularity%2520Collaboration%26entry.906535625%3DZiheng%2520Zhou%2520and%2520Jinxing%2520Zhou%2520and%2520Wei%2520Qian%2520and%2520Shengeng%2520Tang%2520and%2520Xiaojun%2520Chang%2520and%2520Dan%2520Guo%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520audio-visual%2520learning%252C%2520most%2520research%2520tasks%2520focus%2520exclusively%250Aon%2520short%2520videos.%2520This%2520paper%2520focuses%2520on%2520the%2520more%2520practical%2520Dense%2520Audio-Visual%250AEvent%2520Localization%2520%2528DAVEL%2529%2520task%252C%2520advancing%2520audio-visual%2520scene%2520understanding%2520for%250Alonger%252C%2520untrimmed%2520videos.%2520This%2520task%2520seeks%2520to%2520identify%2520and%2520temporally%2520pinpoint%250Aall%2520events%2520simultaneously%2520occurring%2520in%2520both%2520audio%2520and%2520visual%2520streams.%250ATypically%252C%2520each%2520video%2520encompasses%2520dense%2520events%2520of%2520multiple%2520classes%252C%2520which%2520may%250Aoverlap%2520on%2520the%2520timeline%252C%2520each%2520exhibiting%2520varied%2520durations.%2520Given%2520these%250Achallenges%252C%2520effectively%2520exploiting%2520the%2520audio-visual%2520relations%2520and%2520the%2520temporal%250Afeatures%2520encoded%2520at%2520various%2520granularities%2520becomes%2520crucial.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520novel%2520CCNet%252C%2520comprising%2520two%2520core%2520modules%253A%2520the%250ACross-Modal%2520Consistency%2520Collaboration%2520%2528CMCC%2529%2520and%2520the%2520Multi-Temporal%2520Granularity%250ACollaboration%2520%2528MTGC%2529.%2520Specifically%252C%2520the%2520CMCC%2520module%2520contains%2520two%2520branches%253A%2520a%250Across-modal%2520interaction%2520branch%2520and%2520a%2520temporal%2520consistency-gated%2520branch.%2520The%250Aformer%2520branch%2520facilitates%2520the%2520aggregation%2520of%2520consistent%2520event%2520semantics%2520across%250Amodalities%2520through%2520the%2520encoding%2520of%2520audio-visual%2520relations%252C%2520while%2520the%2520latter%250Abranch%2520guides%2520one%2520modality%2527s%2520focus%2520to%2520pivotal%2520event-relevant%2520temporal%2520areas%2520as%250Adiscerned%2520in%2520the%2520other%2520modality.%2520The%2520MTGC%2520module%2520includes%2520a%2520coarse-to-fine%250Acollaboration%2520block%2520and%2520a%2520fine-to-coarse%2520collaboration%2520block%252C%2520providing%250Abidirectional%2520support%2520among%2520coarse-%2520and%2520fine-grained%2520temporal%2520features.%250AExtensive%2520experiments%2520on%2520the%2520UnAV-100%2520dataset%2520validate%2520our%2520module%2520design%252C%250Aresulting%2520in%2520a%2520new%2520state-of-the-art%2520performance%2520in%2520dense%2520audio-visual%2520event%250Alocalization.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zzhhfut/CCNet-AAAI2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Audio-Visual%20Event%20Localization%20under%20Cross-Modal%20Consistency%20and%0A%20%20Multi-Temporal%20Granularity%20Collaboration&entry.906535625=Ziheng%20Zhou%20and%20Jinxing%20Zhou%20and%20Wei%20Qian%20and%20Shengeng%20Tang%20and%20Xiaojun%20Chang%20and%20Dan%20Guo&entry.1292438233=%20%20In%20the%20field%20of%20audio-visual%20learning%2C%20most%20research%20tasks%20focus%20exclusively%0Aon%20short%20videos.%20This%20paper%20focuses%20on%20the%20more%20practical%20Dense%20Audio-Visual%0AEvent%20Localization%20%28DAVEL%29%20task%2C%20advancing%20audio-visual%20scene%20understanding%20for%0Alonger%2C%20untrimmed%20videos.%20This%20task%20seeks%20to%20identify%20and%20temporally%20pinpoint%0Aall%20events%20simultaneously%20occurring%20in%20both%20audio%20and%20visual%20streams.%0ATypically%2C%20each%20video%20encompasses%20dense%20events%20of%20multiple%20classes%2C%20which%20may%0Aoverlap%20on%20the%20timeline%2C%20each%20exhibiting%20varied%20durations.%20Given%20these%0Achallenges%2C%20effectively%20exploiting%20the%20audio-visual%20relations%20and%20the%20temporal%0Afeatures%20encoded%20at%20various%20granularities%20becomes%20crucial.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20CCNet%2C%20comprising%20two%20core%20modules%3A%20the%0ACross-Modal%20Consistency%20Collaboration%20%28CMCC%29%20and%20the%20Multi-Temporal%20Granularity%0ACollaboration%20%28MTGC%29.%20Specifically%2C%20the%20CMCC%20module%20contains%20two%20branches%3A%20a%0Across-modal%20interaction%20branch%20and%20a%20temporal%20consistency-gated%20branch.%20The%0Aformer%20branch%20facilitates%20the%20aggregation%20of%20consistent%20event%20semantics%20across%0Amodalities%20through%20the%20encoding%20of%20audio-visual%20relations%2C%20while%20the%20latter%0Abranch%20guides%20one%20modality%27s%20focus%20to%20pivotal%20event-relevant%20temporal%20areas%20as%0Adiscerned%20in%20the%20other%20modality.%20The%20MTGC%20module%20includes%20a%20coarse-to-fine%0Acollaboration%20block%20and%20a%20fine-to-coarse%20collaboration%20block%2C%20providing%0Abidirectional%20support%20among%20coarse-%20and%20fine-grained%20temporal%20features.%0AExtensive%20experiments%20on%20the%20UnAV-100%20dataset%20validate%20our%20module%20design%2C%0Aresulting%20in%20a%20new%20state-of-the-art%20performance%20in%20dense%20audio-visual%20event%0Alocalization.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zzhhfut/CCNet-AAAI2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12628v2&entry.124074799=Read"},
{"title": "Incorporating Feature Pyramid Tokenization and Open Vocabulary Semantic\n  Segmentation", "author": "Jianyu Zhang and Li Zhang and Shijian Li", "abstract": "  The visual understanding are often approached from 3 granular levels: image,\npatch and pixel. Visual Tokenization, trained by self-supervised reconstructive\nlearning, compresses visual data by codebook in patch-level with marginal\ninformation loss, but the visual tokens does not have semantic meaning. Open\nVocabulary semantic segmentation benefits from the evolving Vision-Language\nmodels (VLMs) with strong image zero-shot capability, but transferring\nimage-level to pixel-level understanding remains an imminent challenge. In this\npaper, we treat segmentation as tokenizing pixels and study a united perceptual\nand semantic token compression for all granular understanding and consequently\nfacilitate open vocabulary semantic segmentation. Referring to the cognitive\nprocess of pretrained VLM where the low-level features are progressively\ncomposed to high-level semantics, we propose Feature Pyramid Tokenization (PAT)\nto cluster and represent multi-resolution feature by learnable codebooks and\nthen decode them by joint learning pixel reconstruction and semantic\nsegmentation. We design loosely coupled pixel and semantic learning branches.\nThe pixel branch simulates bottom-up composition and top-down visualization of\ncodebook tokens, while the semantic branch collectively fuse hierarchical\ncodebooks as auxiliary segmentation guidance. Our experiments show that PAT\nenhances the semantic intuition of VLM feature pyramid, improves performance\nover the baseline segmentation model and achieves competitive performance on\nopen vocabulary semantic segmentation benchmark. Our model is\nparameter-efficient for VLM integration and flexible for the independent\ntokenization. We hope to give inspiration not only on improving segmentation\nbut also on semantic visual token utilization.\n", "link": "http://arxiv.org/abs/2412.14145v1", "date": "2024-12-18", "relevancy": 2.9355, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5962}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incorporating%20Feature%20Pyramid%20Tokenization%20and%20Open%20Vocabulary%20Semantic%0A%20%20Segmentation&body=Title%3A%20Incorporating%20Feature%20Pyramid%20Tokenization%20and%20Open%20Vocabulary%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jianyu%20Zhang%20and%20Li%20Zhang%20and%20Shijian%20Li%0AAbstract%3A%20%20%20The%20visual%20understanding%20are%20often%20approached%20from%203%20granular%20levels%3A%20image%2C%0Apatch%20and%20pixel.%20Visual%20Tokenization%2C%20trained%20by%20self-supervised%20reconstructive%0Alearning%2C%20compresses%20visual%20data%20by%20codebook%20in%20patch-level%20with%20marginal%0Ainformation%20loss%2C%20but%20the%20visual%20tokens%20does%20not%20have%20semantic%20meaning.%20Open%0AVocabulary%20semantic%20segmentation%20benefits%20from%20the%20evolving%20Vision-Language%0Amodels%20%28VLMs%29%20with%20strong%20image%20zero-shot%20capability%2C%20but%20transferring%0Aimage-level%20to%20pixel-level%20understanding%20remains%20an%20imminent%20challenge.%20In%20this%0Apaper%2C%20we%20treat%20segmentation%20as%20tokenizing%20pixels%20and%20study%20a%20united%20perceptual%0Aand%20semantic%20token%20compression%20for%20all%20granular%20understanding%20and%20consequently%0Afacilitate%20open%20vocabulary%20semantic%20segmentation.%20Referring%20to%20the%20cognitive%0Aprocess%20of%20pretrained%20VLM%20where%20the%20low-level%20features%20are%20progressively%0Acomposed%20to%20high-level%20semantics%2C%20we%20propose%20Feature%20Pyramid%20Tokenization%20%28PAT%29%0Ato%20cluster%20and%20represent%20multi-resolution%20feature%20by%20learnable%20codebooks%20and%0Athen%20decode%20them%20by%20joint%20learning%20pixel%20reconstruction%20and%20semantic%0Asegmentation.%20We%20design%20loosely%20coupled%20pixel%20and%20semantic%20learning%20branches.%0AThe%20pixel%20branch%20simulates%20bottom-up%20composition%20and%20top-down%20visualization%20of%0Acodebook%20tokens%2C%20while%20the%20semantic%20branch%20collectively%20fuse%20hierarchical%0Acodebooks%20as%20auxiliary%20segmentation%20guidance.%20Our%20experiments%20show%20that%20PAT%0Aenhances%20the%20semantic%20intuition%20of%20VLM%20feature%20pyramid%2C%20improves%20performance%0Aover%20the%20baseline%20segmentation%20model%20and%20achieves%20competitive%20performance%20on%0Aopen%20vocabulary%20semantic%20segmentation%20benchmark.%20Our%20model%20is%0Aparameter-efficient%20for%20VLM%20integration%20and%20flexible%20for%20the%20independent%0Atokenization.%20We%20hope%20to%20give%20inspiration%20not%20only%20on%20improving%20segmentation%0Abut%20also%20on%20semantic%20visual%20token%20utilization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncorporating%2520Feature%2520Pyramid%2520Tokenization%2520and%2520Open%2520Vocabulary%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DJianyu%2520Zhang%2520and%2520Li%2520Zhang%2520and%2520Shijian%2520Li%26entry.1292438233%3D%2520%2520The%2520visual%2520understanding%2520are%2520often%2520approached%2520from%25203%2520granular%2520levels%253A%2520image%252C%250Apatch%2520and%2520pixel.%2520Visual%2520Tokenization%252C%2520trained%2520by%2520self-supervised%2520reconstructive%250Alearning%252C%2520compresses%2520visual%2520data%2520by%2520codebook%2520in%2520patch-level%2520with%2520marginal%250Ainformation%2520loss%252C%2520but%2520the%2520visual%2520tokens%2520does%2520not%2520have%2520semantic%2520meaning.%2520Open%250AVocabulary%2520semantic%2520segmentation%2520benefits%2520from%2520the%2520evolving%2520Vision-Language%250Amodels%2520%2528VLMs%2529%2520with%2520strong%2520image%2520zero-shot%2520capability%252C%2520but%2520transferring%250Aimage-level%2520to%2520pixel-level%2520understanding%2520remains%2520an%2520imminent%2520challenge.%2520In%2520this%250Apaper%252C%2520we%2520treat%2520segmentation%2520as%2520tokenizing%2520pixels%2520and%2520study%2520a%2520united%2520perceptual%250Aand%2520semantic%2520token%2520compression%2520for%2520all%2520granular%2520understanding%2520and%2520consequently%250Afacilitate%2520open%2520vocabulary%2520semantic%2520segmentation.%2520Referring%2520to%2520the%2520cognitive%250Aprocess%2520of%2520pretrained%2520VLM%2520where%2520the%2520low-level%2520features%2520are%2520progressively%250Acomposed%2520to%2520high-level%2520semantics%252C%2520we%2520propose%2520Feature%2520Pyramid%2520Tokenization%2520%2528PAT%2529%250Ato%2520cluster%2520and%2520represent%2520multi-resolution%2520feature%2520by%2520learnable%2520codebooks%2520and%250Athen%2520decode%2520them%2520by%2520joint%2520learning%2520pixel%2520reconstruction%2520and%2520semantic%250Asegmentation.%2520We%2520design%2520loosely%2520coupled%2520pixel%2520and%2520semantic%2520learning%2520branches.%250AThe%2520pixel%2520branch%2520simulates%2520bottom-up%2520composition%2520and%2520top-down%2520visualization%2520of%250Acodebook%2520tokens%252C%2520while%2520the%2520semantic%2520branch%2520collectively%2520fuse%2520hierarchical%250Acodebooks%2520as%2520auxiliary%2520segmentation%2520guidance.%2520Our%2520experiments%2520show%2520that%2520PAT%250Aenhances%2520the%2520semantic%2520intuition%2520of%2520VLM%2520feature%2520pyramid%252C%2520improves%2520performance%250Aover%2520the%2520baseline%2520segmentation%2520model%2520and%2520achieves%2520competitive%2520performance%2520on%250Aopen%2520vocabulary%2520semantic%2520segmentation%2520benchmark.%2520Our%2520model%2520is%250Aparameter-efficient%2520for%2520VLM%2520integration%2520and%2520flexible%2520for%2520the%2520independent%250Atokenization.%2520We%2520hope%2520to%2520give%2520inspiration%2520not%2520only%2520on%2520improving%2520segmentation%250Abut%2520also%2520on%2520semantic%2520visual%2520token%2520utilization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incorporating%20Feature%20Pyramid%20Tokenization%20and%20Open%20Vocabulary%20Semantic%0A%20%20Segmentation&entry.906535625=Jianyu%20Zhang%20and%20Li%20Zhang%20and%20Shijian%20Li&entry.1292438233=%20%20The%20visual%20understanding%20are%20often%20approached%20from%203%20granular%20levels%3A%20image%2C%0Apatch%20and%20pixel.%20Visual%20Tokenization%2C%20trained%20by%20self-supervised%20reconstructive%0Alearning%2C%20compresses%20visual%20data%20by%20codebook%20in%20patch-level%20with%20marginal%0Ainformation%20loss%2C%20but%20the%20visual%20tokens%20does%20not%20have%20semantic%20meaning.%20Open%0AVocabulary%20semantic%20segmentation%20benefits%20from%20the%20evolving%20Vision-Language%0Amodels%20%28VLMs%29%20with%20strong%20image%20zero-shot%20capability%2C%20but%20transferring%0Aimage-level%20to%20pixel-level%20understanding%20remains%20an%20imminent%20challenge.%20In%20this%0Apaper%2C%20we%20treat%20segmentation%20as%20tokenizing%20pixels%20and%20study%20a%20united%20perceptual%0Aand%20semantic%20token%20compression%20for%20all%20granular%20understanding%20and%20consequently%0Afacilitate%20open%20vocabulary%20semantic%20segmentation.%20Referring%20to%20the%20cognitive%0Aprocess%20of%20pretrained%20VLM%20where%20the%20low-level%20features%20are%20progressively%0Acomposed%20to%20high-level%20semantics%2C%20we%20propose%20Feature%20Pyramid%20Tokenization%20%28PAT%29%0Ato%20cluster%20and%20represent%20multi-resolution%20feature%20by%20learnable%20codebooks%20and%0Athen%20decode%20them%20by%20joint%20learning%20pixel%20reconstruction%20and%20semantic%0Asegmentation.%20We%20design%20loosely%20coupled%20pixel%20and%20semantic%20learning%20branches.%0AThe%20pixel%20branch%20simulates%20bottom-up%20composition%20and%20top-down%20visualization%20of%0Acodebook%20tokens%2C%20while%20the%20semantic%20branch%20collectively%20fuse%20hierarchical%0Acodebooks%20as%20auxiliary%20segmentation%20guidance.%20Our%20experiments%20show%20that%20PAT%0Aenhances%20the%20semantic%20intuition%20of%20VLM%20feature%20pyramid%2C%20improves%20performance%0Aover%20the%20baseline%20segmentation%20model%20and%20achieves%20competitive%20performance%20on%0Aopen%20vocabulary%20semantic%20segmentation%20benchmark.%20Our%20model%20is%0Aparameter-efficient%20for%20VLM%20integration%20and%20flexible%20for%20the%20independent%0Atokenization.%20We%20hope%20to%20give%20inspiration%20not%20only%20on%20improving%20segmentation%0Abut%20also%20on%20semantic%20visual%20token%20utilization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14145v1&entry.124074799=Read"},
{"title": "Do Language Models Understand Time?", "author": "Xi Ding and Lei Wang", "abstract": "  Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.\n", "link": "http://arxiv.org/abs/2412.13845v1", "date": "2024-12-18", "relevancy": 2.8919, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6128}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Language%20Models%20Understand%20Time%3F&body=Title%3A%20Do%20Language%20Models%20Understand%20Time%3F%0AAuthor%3A%20Xi%20Ding%20and%20Lei%20Wang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20video-based%20computer%20vision%0Aapplications%2C%20including%20action%20recognition%2C%20anomaly%20detection%2C%20and%20video%0Asummarization.%20Videos%20inherently%20pose%20unique%20challenges%2C%20combining%20spatial%0Acomplexity%20with%20temporal%20dynamics%20that%20are%20absent%20in%20static%20images%20or%20textual%0Adata.%20Current%20approaches%20to%20video%20understanding%20with%20LLMs%20often%20rely%20on%0Apretrained%20video%20encoders%20to%20extract%20spatiotemporal%20features%20and%20text%20encoders%0Ato%20capture%20semantic%20meaning.%20These%20representations%20are%20integrated%20within%20LLM%0Aframeworks%2C%20enabling%20multimodal%20reasoning%20across%20diverse%20video%20tasks.%20However%2C%0Athe%20critical%20question%20persists%3A%20Can%20LLMs%20truly%20understand%20the%20concept%20of%20time%2C%0Aand%20how%20effectively%20can%20they%20reason%20about%20temporal%20relationships%20in%20videos%3F%0AThis%20work%20critically%20examines%20the%20role%20of%20LLMs%20in%20video%20processing%2C%20with%20a%0Aspecific%20focus%20on%20their%20temporal%20reasoning%20capabilities.%20We%20identify%20key%0Alimitations%20in%20the%20interaction%20between%20LLMs%20and%20pretrained%20encoders%2C%20revealing%0Agaps%20in%20their%20ability%20to%20model%20long-term%20dependencies%20and%20abstract%20temporal%0Aconcepts%20such%20as%20causality%20and%20event%20progression.%20Furthermore%2C%20we%20analyze%0Achallenges%20posed%20by%20existing%20video%20datasets%2C%20including%20biases%2C%20lack%20of%20temporal%0Aannotations%2C%20and%20domain-specific%20limitations%20that%20constrain%20the%20temporal%0Aunderstanding%20of%20LLMs.%20To%20address%20these%20gaps%2C%20we%20explore%20promising%20future%0Adirections%2C%20including%20the%20co-evolution%20of%20LLMs%20and%20encoders%2C%20the%20development%20of%0Aenriched%20datasets%20with%20explicit%20temporal%20labels%2C%20and%20innovative%20architectures%0Afor%20integrating%20spatial%2C%20temporal%2C%20and%20semantic%20reasoning.%20By%20addressing%20these%0Achallenges%2C%20we%20aim%20to%20advance%20the%20temporal%20comprehension%20of%20LLMs%2C%20unlocking%0Atheir%20full%20potential%20in%20video%20analysis%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Language%2520Models%2520Understand%2520Time%253F%26entry.906535625%3DXi%2520Ding%2520and%2520Lei%2520Wang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520video-based%2520computer%2520vision%250Aapplications%252C%2520including%2520action%2520recognition%252C%2520anomaly%2520detection%252C%2520and%2520video%250Asummarization.%2520Videos%2520inherently%2520pose%2520unique%2520challenges%252C%2520combining%2520spatial%250Acomplexity%2520with%2520temporal%2520dynamics%2520that%2520are%2520absent%2520in%2520static%2520images%2520or%2520textual%250Adata.%2520Current%2520approaches%2520to%2520video%2520understanding%2520with%2520LLMs%2520often%2520rely%2520on%250Apretrained%2520video%2520encoders%2520to%2520extract%2520spatiotemporal%2520features%2520and%2520text%2520encoders%250Ato%2520capture%2520semantic%2520meaning.%2520These%2520representations%2520are%2520integrated%2520within%2520LLM%250Aframeworks%252C%2520enabling%2520multimodal%2520reasoning%2520across%2520diverse%2520video%2520tasks.%2520However%252C%250Athe%2520critical%2520question%2520persists%253A%2520Can%2520LLMs%2520truly%2520understand%2520the%2520concept%2520of%2520time%252C%250Aand%2520how%2520effectively%2520can%2520they%2520reason%2520about%2520temporal%2520relationships%2520in%2520videos%253F%250AThis%2520work%2520critically%2520examines%2520the%2520role%2520of%2520LLMs%2520in%2520video%2520processing%252C%2520with%2520a%250Aspecific%2520focus%2520on%2520their%2520temporal%2520reasoning%2520capabilities.%2520We%2520identify%2520key%250Alimitations%2520in%2520the%2520interaction%2520between%2520LLMs%2520and%2520pretrained%2520encoders%252C%2520revealing%250Agaps%2520in%2520their%2520ability%2520to%2520model%2520long-term%2520dependencies%2520and%2520abstract%2520temporal%250Aconcepts%2520such%2520as%2520causality%2520and%2520event%2520progression.%2520Furthermore%252C%2520we%2520analyze%250Achallenges%2520posed%2520by%2520existing%2520video%2520datasets%252C%2520including%2520biases%252C%2520lack%2520of%2520temporal%250Aannotations%252C%2520and%2520domain-specific%2520limitations%2520that%2520constrain%2520the%2520temporal%250Aunderstanding%2520of%2520LLMs.%2520To%2520address%2520these%2520gaps%252C%2520we%2520explore%2520promising%2520future%250Adirections%252C%2520including%2520the%2520co-evolution%2520of%2520LLMs%2520and%2520encoders%252C%2520the%2520development%2520of%250Aenriched%2520datasets%2520with%2520explicit%2520temporal%2520labels%252C%2520and%2520innovative%2520architectures%250Afor%2520integrating%2520spatial%252C%2520temporal%252C%2520and%2520semantic%2520reasoning.%2520By%2520addressing%2520these%250Achallenges%252C%2520we%2520aim%2520to%2520advance%2520the%2520temporal%2520comprehension%2520of%2520LLMs%252C%2520unlocking%250Atheir%2520full%2520potential%2520in%2520video%2520analysis%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Language%20Models%20Understand%20Time%3F&entry.906535625=Xi%20Ding%20and%20Lei%20Wang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20video-based%20computer%20vision%0Aapplications%2C%20including%20action%20recognition%2C%20anomaly%20detection%2C%20and%20video%0Asummarization.%20Videos%20inherently%20pose%20unique%20challenges%2C%20combining%20spatial%0Acomplexity%20with%20temporal%20dynamics%20that%20are%20absent%20in%20static%20images%20or%20textual%0Adata.%20Current%20approaches%20to%20video%20understanding%20with%20LLMs%20often%20rely%20on%0Apretrained%20video%20encoders%20to%20extract%20spatiotemporal%20features%20and%20text%20encoders%0Ato%20capture%20semantic%20meaning.%20These%20representations%20are%20integrated%20within%20LLM%0Aframeworks%2C%20enabling%20multimodal%20reasoning%20across%20diverse%20video%20tasks.%20However%2C%0Athe%20critical%20question%20persists%3A%20Can%20LLMs%20truly%20understand%20the%20concept%20of%20time%2C%0Aand%20how%20effectively%20can%20they%20reason%20about%20temporal%20relationships%20in%20videos%3F%0AThis%20work%20critically%20examines%20the%20role%20of%20LLMs%20in%20video%20processing%2C%20with%20a%0Aspecific%20focus%20on%20their%20temporal%20reasoning%20capabilities.%20We%20identify%20key%0Alimitations%20in%20the%20interaction%20between%20LLMs%20and%20pretrained%20encoders%2C%20revealing%0Agaps%20in%20their%20ability%20to%20model%20long-term%20dependencies%20and%20abstract%20temporal%0Aconcepts%20such%20as%20causality%20and%20event%20progression.%20Furthermore%2C%20we%20analyze%0Achallenges%20posed%20by%20existing%20video%20datasets%2C%20including%20biases%2C%20lack%20of%20temporal%0Aannotations%2C%20and%20domain-specific%20limitations%20that%20constrain%20the%20temporal%0Aunderstanding%20of%20LLMs.%20To%20address%20these%20gaps%2C%20we%20explore%20promising%20future%0Adirections%2C%20including%20the%20co-evolution%20of%20LLMs%20and%20encoders%2C%20the%20development%20of%0Aenriched%20datasets%20with%20explicit%20temporal%20labels%2C%20and%20innovative%20architectures%0Afor%20integrating%20spatial%2C%20temporal%2C%20and%20semantic%20reasoning.%20By%20addressing%20these%0Achallenges%2C%20we%20aim%20to%20advance%20the%20temporal%20comprehension%20of%20LLMs%2C%20unlocking%0Atheir%20full%20potential%20in%20video%20analysis%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13845v1&entry.124074799=Read"},
{"title": "MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning", "author": "Shengbang Tong and David Fan and Jiachen Zhu and Yunyang Xiong and Xinlei Chen and Koustuv Sinha and Michael Rabbat and Yann LeCun and Saining Xie and Zhuang Liu", "abstract": "  In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.\n", "link": "http://arxiv.org/abs/2412.14164v1", "date": "2024-12-18", "relevancy": 2.8883, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5828}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaMorph%3A%20Multimodal%20Understanding%20and%20Generation%20via%20Instruction%0A%20%20Tuning&body=Title%3A%20MetaMorph%3A%20Multimodal%20Understanding%20and%20Generation%20via%20Instruction%0A%20%20Tuning%0AAuthor%3A%20Shengbang%20Tong%20and%20David%20Fan%20and%20Jiachen%20Zhu%20and%20Yunyang%20Xiong%20and%20Xinlei%20Chen%20and%20Koustuv%20Sinha%20and%20Michael%20Rabbat%20and%20Yann%20LeCun%20and%20Saining%20Xie%20and%20Zhuang%20Liu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20Visual-Predictive%20Instruction%20Tuning%20%28VPiT%29%20-%20a%0Asimple%20and%20effective%20extension%20to%20visual%20instruction%20tuning%20that%20enables%20a%0Apretrained%20LLM%20to%20quickly%20morph%20into%20an%20unified%20autoregressive%20model%20capable%20of%0Agenerating%20both%20text%20and%20visual%20tokens.%20VPiT%20teaches%20an%20LLM%20to%20predict%20discrete%0Atext%20tokens%20and%20continuous%20visual%20tokens%20from%20any%20input%20sequence%20of%20image%20and%0Atext%20data%20curated%20in%20an%20instruction-following%20format.%20Our%20empirical%0Ainvestigation%20reveals%20several%20intriguing%20properties%20of%20VPiT%3A%20%281%29%20visual%0Ageneration%20ability%20emerges%20as%20a%20natural%20byproduct%20of%20improved%20visual%0Aunderstanding%2C%20and%20can%20be%20unlocked%20efficiently%20with%20a%20small%20amount%20of%0Ageneration%20data%3B%20%282%29%20while%20we%20find%20understanding%20and%20generation%20to%20be%20mutually%0Abeneficial%2C%20understanding%20data%20contributes%20to%20both%20capabilities%20more%0Aeffectively%20than%20generation%20data.%20Building%20upon%20these%20findings%2C%20we%20train%20our%0AMetaMorph%20model%20and%20achieve%20competitive%20performance%20on%20both%20visual%0Aunderstanding%20and%20generation.%20In%20visual%20generation%2C%20MetaMorph%20can%20leverage%20the%0Aworld%20knowledge%20and%20reasoning%20abilities%20gained%20from%20LLM%20pretraining%2C%20and%0Aovercome%20common%20failure%20modes%20exhibited%20by%20other%20generation%20models.%20Our%20results%0Asuggest%20that%20LLMs%20may%20have%20strong%20%22prior%22%20vision%20capabilities%20that%20can%20be%0Aefficiently%20adapted%20to%20both%20visual%20understanding%20and%20generation%20with%20a%0Arelatively%20simple%20instruction%20tuning%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaMorph%253A%2520Multimodal%2520Understanding%2520and%2520Generation%2520via%2520Instruction%250A%2520%2520Tuning%26entry.906535625%3DShengbang%2520Tong%2520and%2520David%2520Fan%2520and%2520Jiachen%2520Zhu%2520and%2520Yunyang%2520Xiong%2520and%2520Xinlei%2520Chen%2520and%2520Koustuv%2520Sinha%2520and%2520Michael%2520Rabbat%2520and%2520Yann%2520LeCun%2520and%2520Saining%2520Xie%2520and%2520Zhuang%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520Visual-Predictive%2520Instruction%2520Tuning%2520%2528VPiT%2529%2520-%2520a%250Asimple%2520and%2520effective%2520extension%2520to%2520visual%2520instruction%2520tuning%2520that%2520enables%2520a%250Apretrained%2520LLM%2520to%2520quickly%2520morph%2520into%2520an%2520unified%2520autoregressive%2520model%2520capable%2520of%250Agenerating%2520both%2520text%2520and%2520visual%2520tokens.%2520VPiT%2520teaches%2520an%2520LLM%2520to%2520predict%2520discrete%250Atext%2520tokens%2520and%2520continuous%2520visual%2520tokens%2520from%2520any%2520input%2520sequence%2520of%2520image%2520and%250Atext%2520data%2520curated%2520in%2520an%2520instruction-following%2520format.%2520Our%2520empirical%250Ainvestigation%2520reveals%2520several%2520intriguing%2520properties%2520of%2520VPiT%253A%2520%25281%2529%2520visual%250Ageneration%2520ability%2520emerges%2520as%2520a%2520natural%2520byproduct%2520of%2520improved%2520visual%250Aunderstanding%252C%2520and%2520can%2520be%2520unlocked%2520efficiently%2520with%2520a%2520small%2520amount%2520of%250Ageneration%2520data%253B%2520%25282%2529%2520while%2520we%2520find%2520understanding%2520and%2520generation%2520to%2520be%2520mutually%250Abeneficial%252C%2520understanding%2520data%2520contributes%2520to%2520both%2520capabilities%2520more%250Aeffectively%2520than%2520generation%2520data.%2520Building%2520upon%2520these%2520findings%252C%2520we%2520train%2520our%250AMetaMorph%2520model%2520and%2520achieve%2520competitive%2520performance%2520on%2520both%2520visual%250Aunderstanding%2520and%2520generation.%2520In%2520visual%2520generation%252C%2520MetaMorph%2520can%2520leverage%2520the%250Aworld%2520knowledge%2520and%2520reasoning%2520abilities%2520gained%2520from%2520LLM%2520pretraining%252C%2520and%250Aovercome%2520common%2520failure%2520modes%2520exhibited%2520by%2520other%2520generation%2520models.%2520Our%2520results%250Asuggest%2520that%2520LLMs%2520may%2520have%2520strong%2520%2522prior%2522%2520vision%2520capabilities%2520that%2520can%2520be%250Aefficiently%2520adapted%2520to%2520both%2520visual%2520understanding%2520and%2520generation%2520with%2520a%250Arelatively%2520simple%2520instruction%2520tuning%2520process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaMorph%3A%20Multimodal%20Understanding%20and%20Generation%20via%20Instruction%0A%20%20Tuning&entry.906535625=Shengbang%20Tong%20and%20David%20Fan%20and%20Jiachen%20Zhu%20and%20Yunyang%20Xiong%20and%20Xinlei%20Chen%20and%20Koustuv%20Sinha%20and%20Michael%20Rabbat%20and%20Yann%20LeCun%20and%20Saining%20Xie%20and%20Zhuang%20Liu&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20Visual-Predictive%20Instruction%20Tuning%20%28VPiT%29%20-%20a%0Asimple%20and%20effective%20extension%20to%20visual%20instruction%20tuning%20that%20enables%20a%0Apretrained%20LLM%20to%20quickly%20morph%20into%20an%20unified%20autoregressive%20model%20capable%20of%0Agenerating%20both%20text%20and%20visual%20tokens.%20VPiT%20teaches%20an%20LLM%20to%20predict%20discrete%0Atext%20tokens%20and%20continuous%20visual%20tokens%20from%20any%20input%20sequence%20of%20image%20and%0Atext%20data%20curated%20in%20an%20instruction-following%20format.%20Our%20empirical%0Ainvestigation%20reveals%20several%20intriguing%20properties%20of%20VPiT%3A%20%281%29%20visual%0Ageneration%20ability%20emerges%20as%20a%20natural%20byproduct%20of%20improved%20visual%0Aunderstanding%2C%20and%20can%20be%20unlocked%20efficiently%20with%20a%20small%20amount%20of%0Ageneration%20data%3B%20%282%29%20while%20we%20find%20understanding%20and%20generation%20to%20be%20mutually%0Abeneficial%2C%20understanding%20data%20contributes%20to%20both%20capabilities%20more%0Aeffectively%20than%20generation%20data.%20Building%20upon%20these%20findings%2C%20we%20train%20our%0AMetaMorph%20model%20and%20achieve%20competitive%20performance%20on%20both%20visual%0Aunderstanding%20and%20generation.%20In%20visual%20generation%2C%20MetaMorph%20can%20leverage%20the%0Aworld%20knowledge%20and%20reasoning%20abilities%20gained%20from%20LLM%20pretraining%2C%20and%0Aovercome%20common%20failure%20modes%20exhibited%20by%20other%20generation%20models.%20Our%20results%0Asuggest%20that%20LLMs%20may%20have%20strong%20%22prior%22%20vision%20capabilities%20that%20can%20be%0Aefficiently%20adapted%20to%20both%20visual%20understanding%20and%20generation%20with%20a%0Arelatively%20simple%20instruction%20tuning%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14164v1&entry.124074799=Read"},
{"title": "MOT-DETR: 3D Single Shot Detection and Tracking with Transformers to\n  build 3D representations for Agro-Food Robots", "author": "David Rapado-Rincon and Henk Nap and Katarina Smolenova and Eldert J. van Henten and Gert Kootstra", "abstract": "  In the current demand for automation in the agro-food industry, accurately\ndetecting and localizing relevant objects in 3D is essential for successful\nrobotic operations. However, this is a challenge due the presence of\nocclusions. Multi-view perception approaches allow robots to overcome\nocclusions, but a tracking component is needed to associate the objects\ndetected by the robot over multiple viewpoints. Most multi-object tracking\n(MOT) algorithms are designed for high frame rate sequences and struggle with\nthe occlusions generated by robots' motions and 3D environments. In this paper,\nwe introduce MOT-DETR, a novel approach to detect and track objects in 3D over\ntime using a combination of convolutional networks and transformers. Our method\nprocesses 2D and 3D data, and employs a transformer architecture to perform\ndata fusion. We show that MOT-DETR outperforms state-of-the-art multi-object\ntracking methods. Furthermore, we prove that MOT-DETR can leverage 3D data to\ndeal with long-term occlusions and large frame-to-frame distances better than\nstate-of-the-art methods. Finally, we show how our method is resilient to\ncamera pose noise that can affect the accuracy of point clouds. The\nimplementation of MOT-DETR can be found here:\nhttps://github.com/drapado/mot-detr\n", "link": "http://arxiv.org/abs/2311.15674v3", "date": "2024-12-18", "relevancy": 2.8753, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5816}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5771}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOT-DETR%3A%203D%20Single%20Shot%20Detection%20and%20Tracking%20with%20Transformers%20to%0A%20%20build%203D%20representations%20for%20Agro-Food%20Robots&body=Title%3A%20MOT-DETR%3A%203D%20Single%20Shot%20Detection%20and%20Tracking%20with%20Transformers%20to%0A%20%20build%203D%20representations%20for%20Agro-Food%20Robots%0AAuthor%3A%20David%20Rapado-Rincon%20and%20Henk%20Nap%20and%20Katarina%20Smolenova%20and%20Eldert%20J.%20van%20Henten%20and%20Gert%20Kootstra%0AAbstract%3A%20%20%20In%20the%20current%20demand%20for%20automation%20in%20the%20agro-food%20industry%2C%20accurately%0Adetecting%20and%20localizing%20relevant%20objects%20in%203D%20is%20essential%20for%20successful%0Arobotic%20operations.%20However%2C%20this%20is%20a%20challenge%20due%20the%20presence%20of%0Aocclusions.%20Multi-view%20perception%20approaches%20allow%20robots%20to%20overcome%0Aocclusions%2C%20but%20a%20tracking%20component%20is%20needed%20to%20associate%20the%20objects%0Adetected%20by%20the%20robot%20over%20multiple%20viewpoints.%20Most%20multi-object%20tracking%0A%28MOT%29%20algorithms%20are%20designed%20for%20high%20frame%20rate%20sequences%20and%20struggle%20with%0Athe%20occlusions%20generated%20by%20robots%27%20motions%20and%203D%20environments.%20In%20this%20paper%2C%0Awe%20introduce%20MOT-DETR%2C%20a%20novel%20approach%20to%20detect%20and%20track%20objects%20in%203D%20over%0Atime%20using%20a%20combination%20of%20convolutional%20networks%20and%20transformers.%20Our%20method%0Aprocesses%202D%20and%203D%20data%2C%20and%20employs%20a%20transformer%20architecture%20to%20perform%0Adata%20fusion.%20We%20show%20that%20MOT-DETR%20outperforms%20state-of-the-art%20multi-object%0Atracking%20methods.%20Furthermore%2C%20we%20prove%20that%20MOT-DETR%20can%20leverage%203D%20data%20to%0Adeal%20with%20long-term%20occlusions%20and%20large%20frame-to-frame%20distances%20better%20than%0Astate-of-the-art%20methods.%20Finally%2C%20we%20show%20how%20our%20method%20is%20resilient%20to%0Acamera%20pose%20noise%20that%20can%20affect%20the%20accuracy%20of%20point%20clouds.%20The%0Aimplementation%20of%20MOT-DETR%20can%20be%20found%20here%3A%0Ahttps%3A//github.com/drapado/mot-detr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15674v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOT-DETR%253A%25203D%2520Single%2520Shot%2520Detection%2520and%2520Tracking%2520with%2520Transformers%2520to%250A%2520%2520build%25203D%2520representations%2520for%2520Agro-Food%2520Robots%26entry.906535625%3DDavid%2520Rapado-Rincon%2520and%2520Henk%2520Nap%2520and%2520Katarina%2520Smolenova%2520and%2520Eldert%2520J.%2520van%2520Henten%2520and%2520Gert%2520Kootstra%26entry.1292438233%3D%2520%2520In%2520the%2520current%2520demand%2520for%2520automation%2520in%2520the%2520agro-food%2520industry%252C%2520accurately%250Adetecting%2520and%2520localizing%2520relevant%2520objects%2520in%25203D%2520is%2520essential%2520for%2520successful%250Arobotic%2520operations.%2520However%252C%2520this%2520is%2520a%2520challenge%2520due%2520the%2520presence%2520of%250Aocclusions.%2520Multi-view%2520perception%2520approaches%2520allow%2520robots%2520to%2520overcome%250Aocclusions%252C%2520but%2520a%2520tracking%2520component%2520is%2520needed%2520to%2520associate%2520the%2520objects%250Adetected%2520by%2520the%2520robot%2520over%2520multiple%2520viewpoints.%2520Most%2520multi-object%2520tracking%250A%2528MOT%2529%2520algorithms%2520are%2520designed%2520for%2520high%2520frame%2520rate%2520sequences%2520and%2520struggle%2520with%250Athe%2520occlusions%2520generated%2520by%2520robots%2527%2520motions%2520and%25203D%2520environments.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520MOT-DETR%252C%2520a%2520novel%2520approach%2520to%2520detect%2520and%2520track%2520objects%2520in%25203D%2520over%250Atime%2520using%2520a%2520combination%2520of%2520convolutional%2520networks%2520and%2520transformers.%2520Our%2520method%250Aprocesses%25202D%2520and%25203D%2520data%252C%2520and%2520employs%2520a%2520transformer%2520architecture%2520to%2520perform%250Adata%2520fusion.%2520We%2520show%2520that%2520MOT-DETR%2520outperforms%2520state-of-the-art%2520multi-object%250Atracking%2520methods.%2520Furthermore%252C%2520we%2520prove%2520that%2520MOT-DETR%2520can%2520leverage%25203D%2520data%2520to%250Adeal%2520with%2520long-term%2520occlusions%2520and%2520large%2520frame-to-frame%2520distances%2520better%2520than%250Astate-of-the-art%2520methods.%2520Finally%252C%2520we%2520show%2520how%2520our%2520method%2520is%2520resilient%2520to%250Acamera%2520pose%2520noise%2520that%2520can%2520affect%2520the%2520accuracy%2520of%2520point%2520clouds.%2520The%250Aimplementation%2520of%2520MOT-DETR%2520can%2520be%2520found%2520here%253A%250Ahttps%253A//github.com/drapado/mot-detr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15674v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOT-DETR%3A%203D%20Single%20Shot%20Detection%20and%20Tracking%20with%20Transformers%20to%0A%20%20build%203D%20representations%20for%20Agro-Food%20Robots&entry.906535625=David%20Rapado-Rincon%20and%20Henk%20Nap%20and%20Katarina%20Smolenova%20and%20Eldert%20J.%20van%20Henten%20and%20Gert%20Kootstra&entry.1292438233=%20%20In%20the%20current%20demand%20for%20automation%20in%20the%20agro-food%20industry%2C%20accurately%0Adetecting%20and%20localizing%20relevant%20objects%20in%203D%20is%20essential%20for%20successful%0Arobotic%20operations.%20However%2C%20this%20is%20a%20challenge%20due%20the%20presence%20of%0Aocclusions.%20Multi-view%20perception%20approaches%20allow%20robots%20to%20overcome%0Aocclusions%2C%20but%20a%20tracking%20component%20is%20needed%20to%20associate%20the%20objects%0Adetected%20by%20the%20robot%20over%20multiple%20viewpoints.%20Most%20multi-object%20tracking%0A%28MOT%29%20algorithms%20are%20designed%20for%20high%20frame%20rate%20sequences%20and%20struggle%20with%0Athe%20occlusions%20generated%20by%20robots%27%20motions%20and%203D%20environments.%20In%20this%20paper%2C%0Awe%20introduce%20MOT-DETR%2C%20a%20novel%20approach%20to%20detect%20and%20track%20objects%20in%203D%20over%0Atime%20using%20a%20combination%20of%20convolutional%20networks%20and%20transformers.%20Our%20method%0Aprocesses%202D%20and%203D%20data%2C%20and%20employs%20a%20transformer%20architecture%20to%20perform%0Adata%20fusion.%20We%20show%20that%20MOT-DETR%20outperforms%20state-of-the-art%20multi-object%0Atracking%20methods.%20Furthermore%2C%20we%20prove%20that%20MOT-DETR%20can%20leverage%203D%20data%20to%0Adeal%20with%20long-term%20occlusions%20and%20large%20frame-to-frame%20distances%20better%20than%0Astate-of-the-art%20methods.%20Finally%2C%20we%20show%20how%20our%20method%20is%20resilient%20to%0Acamera%20pose%20noise%20that%20can%20affect%20the%20accuracy%20of%20point%20clouds.%20The%0Aimplementation%20of%20MOT-DETR%20can%20be%20found%20here%3A%0Ahttps%3A//github.com/drapado/mot-detr%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15674v3&entry.124074799=Read"},
{"title": "Real-Time Position-Aware View Synthesis from Single-View Input", "author": "Manu Gond and Emin Zerman and Sebastian Knorr and M\u00e5rten Sj\u00f6str\u00f6m", "abstract": "  Recent advancements in view synthesis have significantly enhanced immersive\nexperiences across various computer graphics and multimedia applications,\nincluding telepresence, and entertainment. By enabling the generation of new\nperspectives from a single input view, view synthesis allows users to better\nperceive and interact with their environment. However, many state-of-the-art\nmethods, while achieving high visual quality, face limitations in real-time\nperformance, which makes them less suitable for live applications where low\nlatency is critical. In this paper, we present a lightweight, position-aware\nnetwork designed for real-time view synthesis from a single input image and a\ntarget camera pose. The proposed framework consists of a Position Aware\nEmbedding, modeled with a multi-layer perceptron, which efficiently maps\npositional information from the target pose to generate high dimensional\nfeature maps. These feature maps, along with the input image, are fed into a\nRendering Network that merges features from dual encoder branches to resolve\nboth high level semantics and low level details, producing a realistic new view\nof the scene. Experimental results demonstrate that our method achieves\nsuperior efficiency and visual quality compared to existing approaches,\nparticularly in handling complex translational movements without explicit\ngeometric operations like warping. This work marks a step toward enabling\nreal-time view synthesis from a single image for live and interactive\napplications.\n", "link": "http://arxiv.org/abs/2412.14005v1", "date": "2024-12-18", "relevancy": 2.8692, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5781}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Position-Aware%20View%20Synthesis%20from%20Single-View%20Input&body=Title%3A%20Real-Time%20Position-Aware%20View%20Synthesis%20from%20Single-View%20Input%0AAuthor%3A%20Manu%20Gond%20and%20Emin%20Zerman%20and%20Sebastian%20Knorr%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m%0AAbstract%3A%20%20%20Recent%20advancements%20in%20view%20synthesis%20have%20significantly%20enhanced%20immersive%0Aexperiences%20across%20various%20computer%20graphics%20and%20multimedia%20applications%2C%0Aincluding%20telepresence%2C%20and%20entertainment.%20By%20enabling%20the%20generation%20of%20new%0Aperspectives%20from%20a%20single%20input%20view%2C%20view%20synthesis%20allows%20users%20to%20better%0Aperceive%20and%20interact%20with%20their%20environment.%20However%2C%20many%20state-of-the-art%0Amethods%2C%20while%20achieving%20high%20visual%20quality%2C%20face%20limitations%20in%20real-time%0Aperformance%2C%20which%20makes%20them%20less%20suitable%20for%20live%20applications%20where%20low%0Alatency%20is%20critical.%20In%20this%20paper%2C%20we%20present%20a%20lightweight%2C%20position-aware%0Anetwork%20designed%20for%20real-time%20view%20synthesis%20from%20a%20single%20input%20image%20and%20a%0Atarget%20camera%20pose.%20The%20proposed%20framework%20consists%20of%20a%20Position%20Aware%0AEmbedding%2C%20modeled%20with%20a%20multi-layer%20perceptron%2C%20which%20efficiently%20maps%0Apositional%20information%20from%20the%20target%20pose%20to%20generate%20high%20dimensional%0Afeature%20maps.%20These%20feature%20maps%2C%20along%20with%20the%20input%20image%2C%20are%20fed%20into%20a%0ARendering%20Network%20that%20merges%20features%20from%20dual%20encoder%20branches%20to%20resolve%0Aboth%20high%20level%20semantics%20and%20low%20level%20details%2C%20producing%20a%20realistic%20new%20view%0Aof%20the%20scene.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20efficiency%20and%20visual%20quality%20compared%20to%20existing%20approaches%2C%0Aparticularly%20in%20handling%20complex%20translational%20movements%20without%20explicit%0Ageometric%20operations%20like%20warping.%20This%20work%20marks%20a%20step%20toward%20enabling%0Areal-time%20view%20synthesis%20from%20a%20single%20image%20for%20live%20and%20interactive%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Position-Aware%2520View%2520Synthesis%2520from%2520Single-View%2520Input%26entry.906535625%3DManu%2520Gond%2520and%2520Emin%2520Zerman%2520and%2520Sebastian%2520Knorr%2520and%2520M%25C3%25A5rten%2520Sj%25C3%25B6str%25C3%25B6m%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520view%2520synthesis%2520have%2520significantly%2520enhanced%2520immersive%250Aexperiences%2520across%2520various%2520computer%2520graphics%2520and%2520multimedia%2520applications%252C%250Aincluding%2520telepresence%252C%2520and%2520entertainment.%2520By%2520enabling%2520the%2520generation%2520of%2520new%250Aperspectives%2520from%2520a%2520single%2520input%2520view%252C%2520view%2520synthesis%2520allows%2520users%2520to%2520better%250Aperceive%2520and%2520interact%2520with%2520their%2520environment.%2520However%252C%2520many%2520state-of-the-art%250Amethods%252C%2520while%2520achieving%2520high%2520visual%2520quality%252C%2520face%2520limitations%2520in%2520real-time%250Aperformance%252C%2520which%2520makes%2520them%2520less%2520suitable%2520for%2520live%2520applications%2520where%2520low%250Alatency%2520is%2520critical.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520lightweight%252C%2520position-aware%250Anetwork%2520designed%2520for%2520real-time%2520view%2520synthesis%2520from%2520a%2520single%2520input%2520image%2520and%2520a%250Atarget%2520camera%2520pose.%2520The%2520proposed%2520framework%2520consists%2520of%2520a%2520Position%2520Aware%250AEmbedding%252C%2520modeled%2520with%2520a%2520multi-layer%2520perceptron%252C%2520which%2520efficiently%2520maps%250Apositional%2520information%2520from%2520the%2520target%2520pose%2520to%2520generate%2520high%2520dimensional%250Afeature%2520maps.%2520These%2520feature%2520maps%252C%2520along%2520with%2520the%2520input%2520image%252C%2520are%2520fed%2520into%2520a%250ARendering%2520Network%2520that%2520merges%2520features%2520from%2520dual%2520encoder%2520branches%2520to%2520resolve%250Aboth%2520high%2520level%2520semantics%2520and%2520low%2520level%2520details%252C%2520producing%2520a%2520realistic%2520new%2520view%250Aof%2520the%2520scene.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520achieves%250Asuperior%2520efficiency%2520and%2520visual%2520quality%2520compared%2520to%2520existing%2520approaches%252C%250Aparticularly%2520in%2520handling%2520complex%2520translational%2520movements%2520without%2520explicit%250Ageometric%2520operations%2520like%2520warping.%2520This%2520work%2520marks%2520a%2520step%2520toward%2520enabling%250Areal-time%2520view%2520synthesis%2520from%2520a%2520single%2520image%2520for%2520live%2520and%2520interactive%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Position-Aware%20View%20Synthesis%20from%20Single-View%20Input&entry.906535625=Manu%20Gond%20and%20Emin%20Zerman%20and%20Sebastian%20Knorr%20and%20M%C3%A5rten%20Sj%C3%B6str%C3%B6m&entry.1292438233=%20%20Recent%20advancements%20in%20view%20synthesis%20have%20significantly%20enhanced%20immersive%0Aexperiences%20across%20various%20computer%20graphics%20and%20multimedia%20applications%2C%0Aincluding%20telepresence%2C%20and%20entertainment.%20By%20enabling%20the%20generation%20of%20new%0Aperspectives%20from%20a%20single%20input%20view%2C%20view%20synthesis%20allows%20users%20to%20better%0Aperceive%20and%20interact%20with%20their%20environment.%20However%2C%20many%20state-of-the-art%0Amethods%2C%20while%20achieving%20high%20visual%20quality%2C%20face%20limitations%20in%20real-time%0Aperformance%2C%20which%20makes%20them%20less%20suitable%20for%20live%20applications%20where%20low%0Alatency%20is%20critical.%20In%20this%20paper%2C%20we%20present%20a%20lightweight%2C%20position-aware%0Anetwork%20designed%20for%20real-time%20view%20synthesis%20from%20a%20single%20input%20image%20and%20a%0Atarget%20camera%20pose.%20The%20proposed%20framework%20consists%20of%20a%20Position%20Aware%0AEmbedding%2C%20modeled%20with%20a%20multi-layer%20perceptron%2C%20which%20efficiently%20maps%0Apositional%20information%20from%20the%20target%20pose%20to%20generate%20high%20dimensional%0Afeature%20maps.%20These%20feature%20maps%2C%20along%20with%20the%20input%20image%2C%20are%20fed%20into%20a%0ARendering%20Network%20that%20merges%20features%20from%20dual%20encoder%20branches%20to%20resolve%0Aboth%20high%20level%20semantics%20and%20low%20level%20details%2C%20producing%20a%20realistic%20new%20view%0Aof%20the%20scene.%20Experimental%20results%20demonstrate%20that%20our%20method%20achieves%0Asuperior%20efficiency%20and%20visual%20quality%20compared%20to%20existing%20approaches%2C%0Aparticularly%20in%20handling%20complex%20translational%20movements%20without%20explicit%0Ageometric%20operations%20like%20warping.%20This%20work%20marks%20a%20step%20toward%20enabling%0Areal-time%20view%20synthesis%20from%20a%20single%20image%20for%20live%20and%20interactive%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14005v1&entry.124074799=Read"},
{"title": "InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal\n  Large Language Models", "author": "Cong Wei and Yujie Zhong and Haoxian Tan and Yingsen Zeng and Yong Liu and Zheng Zhao and Yujiu Yang", "abstract": "  Boosted by Multi-modal Large Language Models (MLLMs), text-guided universal\nsegmentation models for the image and video domains have made rapid progress\nrecently. However, these methods are often developed separately for specific\ndomains, overlooking the similarities in task settings and solutions across\nthese two areas. In this paper, we define the union of referring segmentation\nand reasoning segmentation at both the image and video levels as Instructed\nVisual Segmentation (IVS). Correspondingly, we propose InstructSeg, an\nend-to-end segmentation pipeline equipped with MLLMs for IVS. Specifically, we\nemploy an object-aware video perceiver to extract temporal and object\ninformation from reference frames, facilitating comprehensive video\nunderstanding. Additionally, we introduce vision-guided multi-granularity text\nfusion to better integrate global and detailed text information with\nfine-grained visual guidance. By leveraging multi-task and end-to-end training,\nInstructSeg demonstrates superior performance across diverse image and video\nsegmentation tasks, surpassing both segmentation specialists and MLLM-based\nmethods with a single model. Our code is available at\nhttps://github.com/congvvc/InstructSeg.\n", "link": "http://arxiv.org/abs/2412.14006v1", "date": "2024-12-18", "relevancy": 2.8608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructSeg%3A%20Unifying%20Instructed%20Visual%20Segmentation%20with%20Multi-modal%0A%20%20Large%20Language%20Models&body=Title%3A%20InstructSeg%3A%20Unifying%20Instructed%20Visual%20Segmentation%20with%20Multi-modal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Cong%20Wei%20and%20Yujie%20Zhong%20and%20Haoxian%20Tan%20and%20Yingsen%20Zeng%20and%20Yong%20Liu%20and%20Zheng%20Zhao%20and%20Yujiu%20Yang%0AAbstract%3A%20%20%20Boosted%20by%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20text-guided%20universal%0Asegmentation%20models%20for%20the%20image%20and%20video%20domains%20have%20made%20rapid%20progress%0Arecently.%20However%2C%20these%20methods%20are%20often%20developed%20separately%20for%20specific%0Adomains%2C%20overlooking%20the%20similarities%20in%20task%20settings%20and%20solutions%20across%0Athese%20two%20areas.%20In%20this%20paper%2C%20we%20define%20the%20union%20of%20referring%20segmentation%0Aand%20reasoning%20segmentation%20at%20both%20the%20image%20and%20video%20levels%20as%20Instructed%0AVisual%20Segmentation%20%28IVS%29.%20Correspondingly%2C%20we%20propose%20InstructSeg%2C%20an%0Aend-to-end%20segmentation%20pipeline%20equipped%20with%20MLLMs%20for%20IVS.%20Specifically%2C%20we%0Aemploy%20an%20object-aware%20video%20perceiver%20to%20extract%20temporal%20and%20object%0Ainformation%20from%20reference%20frames%2C%20facilitating%20comprehensive%20video%0Aunderstanding.%20Additionally%2C%20we%20introduce%20vision-guided%20multi-granularity%20text%0Afusion%20to%20better%20integrate%20global%20and%20detailed%20text%20information%20with%0Afine-grained%20visual%20guidance.%20By%20leveraging%20multi-task%20and%20end-to-end%20training%2C%0AInstructSeg%20demonstrates%20superior%20performance%20across%20diverse%20image%20and%20video%0Asegmentation%20tasks%2C%20surpassing%20both%20segmentation%20specialists%20and%20MLLM-based%0Amethods%20with%20a%20single%20model.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/congvvc/InstructSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructSeg%253A%2520Unifying%2520Instructed%2520Visual%2520Segmentation%2520with%2520Multi-modal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DCong%2520Wei%2520and%2520Yujie%2520Zhong%2520and%2520Haoxian%2520Tan%2520and%2520Yingsen%2520Zeng%2520and%2520Yong%2520Liu%2520and%2520Zheng%2520Zhao%2520and%2520Yujiu%2520Yang%26entry.1292438233%3D%2520%2520Boosted%2520by%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520text-guided%2520universal%250Asegmentation%2520models%2520for%2520the%2520image%2520and%2520video%2520domains%2520have%2520made%2520rapid%2520progress%250Arecently.%2520However%252C%2520these%2520methods%2520are%2520often%2520developed%2520separately%2520for%2520specific%250Adomains%252C%2520overlooking%2520the%2520similarities%2520in%2520task%2520settings%2520and%2520solutions%2520across%250Athese%2520two%2520areas.%2520In%2520this%2520paper%252C%2520we%2520define%2520the%2520union%2520of%2520referring%2520segmentation%250Aand%2520reasoning%2520segmentation%2520at%2520both%2520the%2520image%2520and%2520video%2520levels%2520as%2520Instructed%250AVisual%2520Segmentation%2520%2528IVS%2529.%2520Correspondingly%252C%2520we%2520propose%2520InstructSeg%252C%2520an%250Aend-to-end%2520segmentation%2520pipeline%2520equipped%2520with%2520MLLMs%2520for%2520IVS.%2520Specifically%252C%2520we%250Aemploy%2520an%2520object-aware%2520video%2520perceiver%2520to%2520extract%2520temporal%2520and%2520object%250Ainformation%2520from%2520reference%2520frames%252C%2520facilitating%2520comprehensive%2520video%250Aunderstanding.%2520Additionally%252C%2520we%2520introduce%2520vision-guided%2520multi-granularity%2520text%250Afusion%2520to%2520better%2520integrate%2520global%2520and%2520detailed%2520text%2520information%2520with%250Afine-grained%2520visual%2520guidance.%2520By%2520leveraging%2520multi-task%2520and%2520end-to-end%2520training%252C%250AInstructSeg%2520demonstrates%2520superior%2520performance%2520across%2520diverse%2520image%2520and%2520video%250Asegmentation%2520tasks%252C%2520surpassing%2520both%2520segmentation%2520specialists%2520and%2520MLLM-based%250Amethods%2520with%2520a%2520single%2520model.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/congvvc/InstructSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructSeg%3A%20Unifying%20Instructed%20Visual%20Segmentation%20with%20Multi-modal%0A%20%20Large%20Language%20Models&entry.906535625=Cong%20Wei%20and%20Yujie%20Zhong%20and%20Haoxian%20Tan%20and%20Yingsen%20Zeng%20and%20Yong%20Liu%20and%20Zheng%20Zhao%20and%20Yujiu%20Yang&entry.1292438233=%20%20Boosted%20by%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20text-guided%20universal%0Asegmentation%20models%20for%20the%20image%20and%20video%20domains%20have%20made%20rapid%20progress%0Arecently.%20However%2C%20these%20methods%20are%20often%20developed%20separately%20for%20specific%0Adomains%2C%20overlooking%20the%20similarities%20in%20task%20settings%20and%20solutions%20across%0Athese%20two%20areas.%20In%20this%20paper%2C%20we%20define%20the%20union%20of%20referring%20segmentation%0Aand%20reasoning%20segmentation%20at%20both%20the%20image%20and%20video%20levels%20as%20Instructed%0AVisual%20Segmentation%20%28IVS%29.%20Correspondingly%2C%20we%20propose%20InstructSeg%2C%20an%0Aend-to-end%20segmentation%20pipeline%20equipped%20with%20MLLMs%20for%20IVS.%20Specifically%2C%20we%0Aemploy%20an%20object-aware%20video%20perceiver%20to%20extract%20temporal%20and%20object%0Ainformation%20from%20reference%20frames%2C%20facilitating%20comprehensive%20video%0Aunderstanding.%20Additionally%2C%20we%20introduce%20vision-guided%20multi-granularity%20text%0Afusion%20to%20better%20integrate%20global%20and%20detailed%20text%20information%20with%0Afine-grained%20visual%20guidance.%20By%20leveraging%20multi-task%20and%20end-to-end%20training%2C%0AInstructSeg%20demonstrates%20superior%20performance%20across%20diverse%20image%20and%20video%0Asegmentation%20tasks%2C%20surpassing%20both%20segmentation%20specialists%20and%20MLLM-based%0Amethods%20with%20a%20single%20model.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/congvvc/InstructSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14006v1&entry.124074799=Read"},
{"title": "Real Classification by Description: Extending CLIP's Limits of Part\n  Attributes Recognition", "author": "Ethan Baron and Idan Tankel and Peter Tu and Guy Ben-Yosef", "abstract": "  In this study, we define and tackle zero shot \"real\" classification by\ndescription, a novel task that evaluates the ability of Vision-Language Models\n(VLMs) like CLIP to classify objects based solely on descriptive attributes,\nexcluding object class names. This approach highlights the current limitations\nof VLMs in understanding intricate object descriptions, pushing these models\nbeyond mere object recognition. To facilitate this exploration, we introduce a\nnew challenge and release description data for six popular fine-grained\nbenchmarks, which omit object names to encourage genuine zero-shot learning\nwithin the research community. Additionally, we propose a method to enhance\nCLIP's attribute detection capabilities through targeted training using\nImageNet21k's diverse object categories, paired with rich attribute\ndescriptions generated by large language models. Furthermore, we introduce a\nmodified CLIP architecture that leverages multiple resolutions to improve the\ndetection of fine-grained part attributes. Through these efforts, we broaden\nthe understanding of part-attribute recognition in CLIP, improving its\nperformance in fine-grained classification tasks across six popular benchmarks,\nas well as in the PACO dataset, a widely used benchmark for object-attribute\nrecognition. Code is available at:\nhttps://github.com/ethanbar11/grounding_ge_public.\n", "link": "http://arxiv.org/abs/2412.13947v1", "date": "2024-12-18", "relevancy": 2.8551, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real%20Classification%20by%20Description%3A%20Extending%20CLIP%27s%20Limits%20of%20Part%0A%20%20Attributes%20Recognition&body=Title%3A%20Real%20Classification%20by%20Description%3A%20Extending%20CLIP%27s%20Limits%20of%20Part%0A%20%20Attributes%20Recognition%0AAuthor%3A%20Ethan%20Baron%20and%20Idan%20Tankel%20and%20Peter%20Tu%20and%20Guy%20Ben-Yosef%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20define%20and%20tackle%20zero%20shot%20%22real%22%20classification%20by%0Adescription%2C%20a%20novel%20task%20that%20evaluates%20the%20ability%20of%20Vision-Language%20Models%0A%28VLMs%29%20like%20CLIP%20to%20classify%20objects%20based%20solely%20on%20descriptive%20attributes%2C%0Aexcluding%20object%20class%20names.%20This%20approach%20highlights%20the%20current%20limitations%0Aof%20VLMs%20in%20understanding%20intricate%20object%20descriptions%2C%20pushing%20these%20models%0Abeyond%20mere%20object%20recognition.%20To%20facilitate%20this%20exploration%2C%20we%20introduce%20a%0Anew%20challenge%20and%20release%20description%20data%20for%20six%20popular%20fine-grained%0Abenchmarks%2C%20which%20omit%20object%20names%20to%20encourage%20genuine%20zero-shot%20learning%0Awithin%20the%20research%20community.%20Additionally%2C%20we%20propose%20a%20method%20to%20enhance%0ACLIP%27s%20attribute%20detection%20capabilities%20through%20targeted%20training%20using%0AImageNet21k%27s%20diverse%20object%20categories%2C%20paired%20with%20rich%20attribute%0Adescriptions%20generated%20by%20large%20language%20models.%20Furthermore%2C%20we%20introduce%20a%0Amodified%20CLIP%20architecture%20that%20leverages%20multiple%20resolutions%20to%20improve%20the%0Adetection%20of%20fine-grained%20part%20attributes.%20Through%20these%20efforts%2C%20we%20broaden%0Athe%20understanding%20of%20part-attribute%20recognition%20in%20CLIP%2C%20improving%20its%0Aperformance%20in%20fine-grained%20classification%20tasks%20across%20six%20popular%20benchmarks%2C%0Aas%20well%20as%20in%20the%20PACO%20dataset%2C%20a%20widely%20used%20benchmark%20for%20object-attribute%0Arecognition.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ethanbar11/grounding_ge_public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal%2520Classification%2520by%2520Description%253A%2520Extending%2520CLIP%2527s%2520Limits%2520of%2520Part%250A%2520%2520Attributes%2520Recognition%26entry.906535625%3DEthan%2520Baron%2520and%2520Idan%2520Tankel%2520and%2520Peter%2520Tu%2520and%2520Guy%2520Ben-Yosef%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520define%2520and%2520tackle%2520zero%2520shot%2520%2522real%2522%2520classification%2520by%250Adescription%252C%2520a%2520novel%2520task%2520that%2520evaluates%2520the%2520ability%2520of%2520Vision-Language%2520Models%250A%2528VLMs%2529%2520like%2520CLIP%2520to%2520classify%2520objects%2520based%2520solely%2520on%2520descriptive%2520attributes%252C%250Aexcluding%2520object%2520class%2520names.%2520This%2520approach%2520highlights%2520the%2520current%2520limitations%250Aof%2520VLMs%2520in%2520understanding%2520intricate%2520object%2520descriptions%252C%2520pushing%2520these%2520models%250Abeyond%2520mere%2520object%2520recognition.%2520To%2520facilitate%2520this%2520exploration%252C%2520we%2520introduce%2520a%250Anew%2520challenge%2520and%2520release%2520description%2520data%2520for%2520six%2520popular%2520fine-grained%250Abenchmarks%252C%2520which%2520omit%2520object%2520names%2520to%2520encourage%2520genuine%2520zero-shot%2520learning%250Awithin%2520the%2520research%2520community.%2520Additionally%252C%2520we%2520propose%2520a%2520method%2520to%2520enhance%250ACLIP%2527s%2520attribute%2520detection%2520capabilities%2520through%2520targeted%2520training%2520using%250AImageNet21k%2527s%2520diverse%2520object%2520categories%252C%2520paired%2520with%2520rich%2520attribute%250Adescriptions%2520generated%2520by%2520large%2520language%2520models.%2520Furthermore%252C%2520we%2520introduce%2520a%250Amodified%2520CLIP%2520architecture%2520that%2520leverages%2520multiple%2520resolutions%2520to%2520improve%2520the%250Adetection%2520of%2520fine-grained%2520part%2520attributes.%2520Through%2520these%2520efforts%252C%2520we%2520broaden%250Athe%2520understanding%2520of%2520part-attribute%2520recognition%2520in%2520CLIP%252C%2520improving%2520its%250Aperformance%2520in%2520fine-grained%2520classification%2520tasks%2520across%2520six%2520popular%2520benchmarks%252C%250Aas%2520well%2520as%2520in%2520the%2520PACO%2520dataset%252C%2520a%2520widely%2520used%2520benchmark%2520for%2520object-attribute%250Arecognition.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ethanbar11/grounding_ge_public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20Classification%20by%20Description%3A%20Extending%20CLIP%27s%20Limits%20of%20Part%0A%20%20Attributes%20Recognition&entry.906535625=Ethan%20Baron%20and%20Idan%20Tankel%20and%20Peter%20Tu%20and%20Guy%20Ben-Yosef&entry.1292438233=%20%20In%20this%20study%2C%20we%20define%20and%20tackle%20zero%20shot%20%22real%22%20classification%20by%0Adescription%2C%20a%20novel%20task%20that%20evaluates%20the%20ability%20of%20Vision-Language%20Models%0A%28VLMs%29%20like%20CLIP%20to%20classify%20objects%20based%20solely%20on%20descriptive%20attributes%2C%0Aexcluding%20object%20class%20names.%20This%20approach%20highlights%20the%20current%20limitations%0Aof%20VLMs%20in%20understanding%20intricate%20object%20descriptions%2C%20pushing%20these%20models%0Abeyond%20mere%20object%20recognition.%20To%20facilitate%20this%20exploration%2C%20we%20introduce%20a%0Anew%20challenge%20and%20release%20description%20data%20for%20six%20popular%20fine-grained%0Abenchmarks%2C%20which%20omit%20object%20names%20to%20encourage%20genuine%20zero-shot%20learning%0Awithin%20the%20research%20community.%20Additionally%2C%20we%20propose%20a%20method%20to%20enhance%0ACLIP%27s%20attribute%20detection%20capabilities%20through%20targeted%20training%20using%0AImageNet21k%27s%20diverse%20object%20categories%2C%20paired%20with%20rich%20attribute%0Adescriptions%20generated%20by%20large%20language%20models.%20Furthermore%2C%20we%20introduce%20a%0Amodified%20CLIP%20architecture%20that%20leverages%20multiple%20resolutions%20to%20improve%20the%0Adetection%20of%20fine-grained%20part%20attributes.%20Through%20these%20efforts%2C%20we%20broaden%0Athe%20understanding%20of%20part-attribute%20recognition%20in%20CLIP%2C%20improving%20its%0Aperformance%20in%20fine-grained%20classification%20tasks%20across%20six%20popular%20benchmarks%2C%0Aas%20well%20as%20in%20the%20PACO%20dataset%2C%20a%20widely%20used%20benchmark%20for%20object-attribute%0Arecognition.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ethanbar11/grounding_ge_public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13947v1&entry.124074799=Read"},
{"title": "Interpreting and Analysing CLIP's Zero-Shot Image Classification via\n  Mutual Knowledge", "author": "Fawaz Sammani and Nikos Deligiannis", "abstract": "  Contrastive Language-Image Pretraining (CLIP) performs zero-shot image\nclassification by mapping images and textual class representation into a shared\nembedding space, then retrieving the class closest to the image. This work\nprovides a new approach for interpreting CLIP models for image classification\nfrom the lens of mutual knowledge between the two modalities. Specifically, we\nask: what concepts do both vision and language CLIP encoders learn in common\nthat influence the joint embedding space, causing points to be closer or\nfurther apart? We answer this question via an approach of textual concept-based\nexplanations, showing their effectiveness, and perform an analysis encompassing\na pool of 13 CLIP models varying in architecture, size and pretraining\ndatasets. We explore those different aspects in relation to mutual knowledge,\nand analyze zero-shot predictions. Our approach demonstrates an effective and\nhuman-friendly way of understanding zero-shot classification decisions with\nCLIP.\n", "link": "http://arxiv.org/abs/2410.13016v3", "date": "2024-12-18", "relevancy": 2.8496, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20and%20Analysing%20CLIP%27s%20Zero-Shot%20Image%20Classification%20via%0A%20%20Mutual%20Knowledge&body=Title%3A%20Interpreting%20and%20Analysing%20CLIP%27s%20Zero-Shot%20Image%20Classification%20via%0A%20%20Mutual%20Knowledge%0AAuthor%3A%20Fawaz%20Sammani%20and%20Nikos%20Deligiannis%0AAbstract%3A%20%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20performs%20zero-shot%20image%0Aclassification%20by%20mapping%20images%20and%20textual%20class%20representation%20into%20a%20shared%0Aembedding%20space%2C%20then%20retrieving%20the%20class%20closest%20to%20the%20image.%20This%20work%0Aprovides%20a%20new%20approach%20for%20interpreting%20CLIP%20models%20for%20image%20classification%0Afrom%20the%20lens%20of%20mutual%20knowledge%20between%20the%20two%20modalities.%20Specifically%2C%20we%0Aask%3A%20what%20concepts%20do%20both%20vision%20and%20language%20CLIP%20encoders%20learn%20in%20common%0Athat%20influence%20the%20joint%20embedding%20space%2C%20causing%20points%20to%20be%20closer%20or%0Afurther%20apart%3F%20We%20answer%20this%20question%20via%20an%20approach%20of%20textual%20concept-based%0Aexplanations%2C%20showing%20their%20effectiveness%2C%20and%20perform%20an%20analysis%20encompassing%0Aa%20pool%20of%2013%20CLIP%20models%20varying%20in%20architecture%2C%20size%20and%20pretraining%0Adatasets.%20We%20explore%20those%20different%20aspects%20in%20relation%20to%20mutual%20knowledge%2C%0Aand%20analyze%20zero-shot%20predictions.%20Our%20approach%20demonstrates%20an%20effective%20and%0Ahuman-friendly%20way%20of%20understanding%20zero-shot%20classification%20decisions%20with%0ACLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13016v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520and%2520Analysing%2520CLIP%2527s%2520Zero-Shot%2520Image%2520Classification%2520via%250A%2520%2520Mutual%2520Knowledge%26entry.906535625%3DFawaz%2520Sammani%2520and%2520Nikos%2520Deligiannis%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529%2520performs%2520zero-shot%2520image%250Aclassification%2520by%2520mapping%2520images%2520and%2520textual%2520class%2520representation%2520into%2520a%2520shared%250Aembedding%2520space%252C%2520then%2520retrieving%2520the%2520class%2520closest%2520to%2520the%2520image.%2520This%2520work%250Aprovides%2520a%2520new%2520approach%2520for%2520interpreting%2520CLIP%2520models%2520for%2520image%2520classification%250Afrom%2520the%2520lens%2520of%2520mutual%2520knowledge%2520between%2520the%2520two%2520modalities.%2520Specifically%252C%2520we%250Aask%253A%2520what%2520concepts%2520do%2520both%2520vision%2520and%2520language%2520CLIP%2520encoders%2520learn%2520in%2520common%250Athat%2520influence%2520the%2520joint%2520embedding%2520space%252C%2520causing%2520points%2520to%2520be%2520closer%2520or%250Afurther%2520apart%253F%2520We%2520answer%2520this%2520question%2520via%2520an%2520approach%2520of%2520textual%2520concept-based%250Aexplanations%252C%2520showing%2520their%2520effectiveness%252C%2520and%2520perform%2520an%2520analysis%2520encompassing%250Aa%2520pool%2520of%252013%2520CLIP%2520models%2520varying%2520in%2520architecture%252C%2520size%2520and%2520pretraining%250Adatasets.%2520We%2520explore%2520those%2520different%2520aspects%2520in%2520relation%2520to%2520mutual%2520knowledge%252C%250Aand%2520analyze%2520zero-shot%2520predictions.%2520Our%2520approach%2520demonstrates%2520an%2520effective%2520and%250Ahuman-friendly%2520way%2520of%2520understanding%2520zero-shot%2520classification%2520decisions%2520with%250ACLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13016v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20and%20Analysing%20CLIP%27s%20Zero-Shot%20Image%20Classification%20via%0A%20%20Mutual%20Knowledge&entry.906535625=Fawaz%20Sammani%20and%20Nikos%20Deligiannis&entry.1292438233=%20%20Contrastive%20Language-Image%20Pretraining%20%28CLIP%29%20performs%20zero-shot%20image%0Aclassification%20by%20mapping%20images%20and%20textual%20class%20representation%20into%20a%20shared%0Aembedding%20space%2C%20then%20retrieving%20the%20class%20closest%20to%20the%20image.%20This%20work%0Aprovides%20a%20new%20approach%20for%20interpreting%20CLIP%20models%20for%20image%20classification%0Afrom%20the%20lens%20of%20mutual%20knowledge%20between%20the%20two%20modalities.%20Specifically%2C%20we%0Aask%3A%20what%20concepts%20do%20both%20vision%20and%20language%20CLIP%20encoders%20learn%20in%20common%0Athat%20influence%20the%20joint%20embedding%20space%2C%20causing%20points%20to%20be%20closer%20or%0Afurther%20apart%3F%20We%20answer%20this%20question%20via%20an%20approach%20of%20textual%20concept-based%0Aexplanations%2C%20showing%20their%20effectiveness%2C%20and%20perform%20an%20analysis%20encompassing%0Aa%20pool%20of%2013%20CLIP%20models%20varying%20in%20architecture%2C%20size%20and%20pretraining%0Adatasets.%20We%20explore%20those%20different%20aspects%20in%20relation%20to%20mutual%20knowledge%2C%0Aand%20analyze%20zero-shot%20predictions.%20Our%20approach%20demonstrates%20an%20effective%20and%0Ahuman-friendly%20way%20of%20understanding%20zero-shot%20classification%20decisions%20with%0ACLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13016v3&entry.124074799=Read"},
{"title": "Standardizing Generative Face Video Compression using Supplemental\n  Enhancement Information", "author": "Bolin Chen and Yan Ye and Jie Chen and Ru-Ling Liao and Shanzhi Yin and Shiqi Wang and Kaifa Yang and Yue Li and Yiling Xu and Ye-Kui Wang and Shiv Gehlot and Guan-Ming Su and Peng Yin and Sean McCarthy and Gary J. Sullivan", "abstract": "  This paper proposes a Generative Face Video Compression (GFVC) approach using\nSupplemental Enhancement Information (SEI), where a series of compact spatial\nand temporal representations of a face video signal (i.e., 2D/3D key-points,\nfacial semantics and compact features) can be coded using SEI message and\ninserted into the coded video bitstream. At the time of writing, the proposed\nGFVC approach using SEI messages has been adopted into the official working\ndraft of Versatile Supplemental Enhancement Information (VSEI) standard by the\nJoint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which\nwill be standardized as a new version for \"ITU-T H.274 | ISO/IEC 23002-7\". To\nthe best of the authors' knowledge, the JVET work on the proposed SEI-based\nGFVC approach is the first standardization activity for generative video\ncompression. The proposed SEI approach has not only advanced the reconstruction\nquality of early-day Model-Based Coding (MBC) via the state-of-the-art\ngenerative technique, but also established a new SEI definition for future GFVC\napplications and deployment. Experimental results illustrate that the proposed\nSEI-based GFVC approach can achieve remarkable rate-distortion performance\ncompared with the latest Versatile Video Coding (VVC) standard, whilst also\npotentially enabling a wide variety of functionalities including user-specified\nanimation/filtering and metaverse-related applications.\n", "link": "http://arxiv.org/abs/2410.15105v2", "date": "2024-12-18", "relevancy": 2.8477, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6127}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5561}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Standardizing%20Generative%20Face%20Video%20Compression%20using%20Supplemental%0A%20%20Enhancement%20Information&body=Title%3A%20Standardizing%20Generative%20Face%20Video%20Compression%20using%20Supplemental%0A%20%20Enhancement%20Information%0AAuthor%3A%20Bolin%20Chen%20and%20Yan%20Ye%20and%20Jie%20Chen%20and%20Ru-Ling%20Liao%20and%20Shanzhi%20Yin%20and%20Shiqi%20Wang%20and%20Kaifa%20Yang%20and%20Yue%20Li%20and%20Yiling%20Xu%20and%20Ye-Kui%20Wang%20and%20Shiv%20Gehlot%20and%20Guan-Ming%20Su%20and%20Peng%20Yin%20and%20Sean%20McCarthy%20and%20Gary%20J.%20Sullivan%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20Generative%20Face%20Video%20Compression%20%28GFVC%29%20approach%20using%0ASupplemental%20Enhancement%20Information%20%28SEI%29%2C%20where%20a%20series%20of%20compact%20spatial%0Aand%20temporal%20representations%20of%20a%20face%20video%20signal%20%28i.e.%2C%202D/3D%20key-points%2C%0Afacial%20semantics%20and%20compact%20features%29%20can%20be%20coded%20using%20SEI%20message%20and%0Ainserted%20into%20the%20coded%20video%20bitstream.%20At%20the%20time%20of%20writing%2C%20the%20proposed%0AGFVC%20approach%20using%20SEI%20messages%20has%20been%20adopted%20into%20the%20official%20working%0Adraft%20of%20Versatile%20Supplemental%20Enhancement%20Information%20%28VSEI%29%20standard%20by%20the%0AJoint%20Video%20Experts%20Team%20%28JVET%29%20of%20ISO/IEC%20JTC%201/SC%2029%20and%20ITU-T%20SG16%2C%20which%0Awill%20be%20standardized%20as%20a%20new%20version%20for%20%22ITU-T%20H.274%20%7C%20ISO/IEC%2023002-7%22.%20To%0Athe%20best%20of%20the%20authors%27%20knowledge%2C%20the%20JVET%20work%20on%20the%20proposed%20SEI-based%0AGFVC%20approach%20is%20the%20first%20standardization%20activity%20for%20generative%20video%0Acompression.%20The%20proposed%20SEI%20approach%20has%20not%20only%20advanced%20the%20reconstruction%0Aquality%20of%20early-day%20Model-Based%20Coding%20%28MBC%29%20via%20the%20state-of-the-art%0Agenerative%20technique%2C%20but%20also%20established%20a%20new%20SEI%20definition%20for%20future%20GFVC%0Aapplications%20and%20deployment.%20Experimental%20results%20illustrate%20that%20the%20proposed%0ASEI-based%20GFVC%20approach%20can%20achieve%20remarkable%20rate-distortion%20performance%0Acompared%20with%20the%20latest%20Versatile%20Video%20Coding%20%28VVC%29%20standard%2C%20whilst%20also%0Apotentially%20enabling%20a%20wide%20variety%20of%20functionalities%20including%20user-specified%0Aanimation/filtering%20and%20metaverse-related%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15105v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStandardizing%2520Generative%2520Face%2520Video%2520Compression%2520using%2520Supplemental%250A%2520%2520Enhancement%2520Information%26entry.906535625%3DBolin%2520Chen%2520and%2520Yan%2520Ye%2520and%2520Jie%2520Chen%2520and%2520Ru-Ling%2520Liao%2520and%2520Shanzhi%2520Yin%2520and%2520Shiqi%2520Wang%2520and%2520Kaifa%2520Yang%2520and%2520Yue%2520Li%2520and%2520Yiling%2520Xu%2520and%2520Ye-Kui%2520Wang%2520and%2520Shiv%2520Gehlot%2520and%2520Guan-Ming%2520Su%2520and%2520Peng%2520Yin%2520and%2520Sean%2520McCarthy%2520and%2520Gary%2520J.%2520Sullivan%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520Generative%2520Face%2520Video%2520Compression%2520%2528GFVC%2529%2520approach%2520using%250ASupplemental%2520Enhancement%2520Information%2520%2528SEI%2529%252C%2520where%2520a%2520series%2520of%2520compact%2520spatial%250Aand%2520temporal%2520representations%2520of%2520a%2520face%2520video%2520signal%2520%2528i.e.%252C%25202D/3D%2520key-points%252C%250Afacial%2520semantics%2520and%2520compact%2520features%2529%2520can%2520be%2520coded%2520using%2520SEI%2520message%2520and%250Ainserted%2520into%2520the%2520coded%2520video%2520bitstream.%2520At%2520the%2520time%2520of%2520writing%252C%2520the%2520proposed%250AGFVC%2520approach%2520using%2520SEI%2520messages%2520has%2520been%2520adopted%2520into%2520the%2520official%2520working%250Adraft%2520of%2520Versatile%2520Supplemental%2520Enhancement%2520Information%2520%2528VSEI%2529%2520standard%2520by%2520the%250AJoint%2520Video%2520Experts%2520Team%2520%2528JVET%2529%2520of%2520ISO/IEC%2520JTC%25201/SC%252029%2520and%2520ITU-T%2520SG16%252C%2520which%250Awill%2520be%2520standardized%2520as%2520a%2520new%2520version%2520for%2520%2522ITU-T%2520H.274%2520%257C%2520ISO/IEC%252023002-7%2522.%2520To%250Athe%2520best%2520of%2520the%2520authors%2527%2520knowledge%252C%2520the%2520JVET%2520work%2520on%2520the%2520proposed%2520SEI-based%250AGFVC%2520approach%2520is%2520the%2520first%2520standardization%2520activity%2520for%2520generative%2520video%250Acompression.%2520The%2520proposed%2520SEI%2520approach%2520has%2520not%2520only%2520advanced%2520the%2520reconstruction%250Aquality%2520of%2520early-day%2520Model-Based%2520Coding%2520%2528MBC%2529%2520via%2520the%2520state-of-the-art%250Agenerative%2520technique%252C%2520but%2520also%2520established%2520a%2520new%2520SEI%2520definition%2520for%2520future%2520GFVC%250Aapplications%2520and%2520deployment.%2520Experimental%2520results%2520illustrate%2520that%2520the%2520proposed%250ASEI-based%2520GFVC%2520approach%2520can%2520achieve%2520remarkable%2520rate-distortion%2520performance%250Acompared%2520with%2520the%2520latest%2520Versatile%2520Video%2520Coding%2520%2528VVC%2529%2520standard%252C%2520whilst%2520also%250Apotentially%2520enabling%2520a%2520wide%2520variety%2520of%2520functionalities%2520including%2520user-specified%250Aanimation/filtering%2520and%2520metaverse-related%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15105v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Standardizing%20Generative%20Face%20Video%20Compression%20using%20Supplemental%0A%20%20Enhancement%20Information&entry.906535625=Bolin%20Chen%20and%20Yan%20Ye%20and%20Jie%20Chen%20and%20Ru-Ling%20Liao%20and%20Shanzhi%20Yin%20and%20Shiqi%20Wang%20and%20Kaifa%20Yang%20and%20Yue%20Li%20and%20Yiling%20Xu%20and%20Ye-Kui%20Wang%20and%20Shiv%20Gehlot%20and%20Guan-Ming%20Su%20and%20Peng%20Yin%20and%20Sean%20McCarthy%20and%20Gary%20J.%20Sullivan&entry.1292438233=%20%20This%20paper%20proposes%20a%20Generative%20Face%20Video%20Compression%20%28GFVC%29%20approach%20using%0ASupplemental%20Enhancement%20Information%20%28SEI%29%2C%20where%20a%20series%20of%20compact%20spatial%0Aand%20temporal%20representations%20of%20a%20face%20video%20signal%20%28i.e.%2C%202D/3D%20key-points%2C%0Afacial%20semantics%20and%20compact%20features%29%20can%20be%20coded%20using%20SEI%20message%20and%0Ainserted%20into%20the%20coded%20video%20bitstream.%20At%20the%20time%20of%20writing%2C%20the%20proposed%0AGFVC%20approach%20using%20SEI%20messages%20has%20been%20adopted%20into%20the%20official%20working%0Adraft%20of%20Versatile%20Supplemental%20Enhancement%20Information%20%28VSEI%29%20standard%20by%20the%0AJoint%20Video%20Experts%20Team%20%28JVET%29%20of%20ISO/IEC%20JTC%201/SC%2029%20and%20ITU-T%20SG16%2C%20which%0Awill%20be%20standardized%20as%20a%20new%20version%20for%20%22ITU-T%20H.274%20%7C%20ISO/IEC%2023002-7%22.%20To%0Athe%20best%20of%20the%20authors%27%20knowledge%2C%20the%20JVET%20work%20on%20the%20proposed%20SEI-based%0AGFVC%20approach%20is%20the%20first%20standardization%20activity%20for%20generative%20video%0Acompression.%20The%20proposed%20SEI%20approach%20has%20not%20only%20advanced%20the%20reconstruction%0Aquality%20of%20early-day%20Model-Based%20Coding%20%28MBC%29%20via%20the%20state-of-the-art%0Agenerative%20technique%2C%20but%20also%20established%20a%20new%20SEI%20definition%20for%20future%20GFVC%0Aapplications%20and%20deployment.%20Experimental%20results%20illustrate%20that%20the%20proposed%0ASEI-based%20GFVC%20approach%20can%20achieve%20remarkable%20rate-distortion%20performance%0Acompared%20with%20the%20latest%20Versatile%20Video%20Coding%20%28VVC%29%20standard%2C%20whilst%20also%0Apotentially%20enabling%20a%20wide%20variety%20of%20functionalities%20including%20user-specified%0Aanimation/filtering%20and%20metaverse-related%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15105v2&entry.124074799=Read"},
{"title": "LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via\n  Hierarchical Window Transformer", "author": "Yipeng Zhang and Yifan Liu and Zonghao Guo and Yidan Zhang and Xuesong Yang and Chi Chen and Jun Song and Bo Zheng and Yuan Yao and Zhiyuan Liu and Tat-Seng Chua and Maosong Sun", "abstract": "  In multimodal large language models (MLLMs), vision transformers (ViTs) are\nwidely employed for visual encoding. However, their performance in solving\nuniversal MLLM tasks is not satisfactory. We attribute it to a lack of\ninformation from diverse visual levels, impeding alignment with the various\nsemantic granularity required for language generation. To address this issue,\nwe present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window\ntransformer that enables capturing diverse visual granularity by constructing\nand integrating a high-resolution feature pyramid. As a vision-language\nprojector, Hiwin transformer comprises two primary modules: (i) an inverse\nfeature pyramid, constructed by a ViT-derived feature up-sampling process\nutilizing high-frequency details from an image pyramid, and (ii) hierarchical\nwindow attention, focusing on a set of key sampling features within cross-scale\nwindows to condense multi-level feature maps. Extensive experiments demonstrate\nthat LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular\nbenchmarks. Notably, our design brings an average boost of 3.7% across 14\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. We\nmake all the data, model checkpoint, and code publicly available to facilitate\nfuture research.\n", "link": "http://arxiv.org/abs/2412.13871v1", "date": "2024-12-18", "relevancy": 2.84, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5755}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-UHD%20v2%3A%20an%20MLLM%20Integrating%20High-Resolution%20Feature%20Pyramid%20via%0A%20%20Hierarchical%20Window%20Transformer&body=Title%3A%20LLaVA-UHD%20v2%3A%20an%20MLLM%20Integrating%20High-Resolution%20Feature%20Pyramid%20via%0A%20%20Hierarchical%20Window%20Transformer%0AAuthor%3A%20Yipeng%20Zhang%20and%20Yifan%20Liu%20and%20Zonghao%20Guo%20and%20Yidan%20Zhang%20and%20Xuesong%20Yang%20and%20Chi%20Chen%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Yuan%20Yao%20and%20Zhiyuan%20Liu%20and%20Tat-Seng%20Chua%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20In%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20vision%20transformers%20%28ViTs%29%20are%0Awidely%20employed%20for%20visual%20encoding.%20However%2C%20their%20performance%20in%20solving%0Auniversal%20MLLM%20tasks%20is%20not%20satisfactory.%20We%20attribute%20it%20to%20a%20lack%20of%0Ainformation%20from%20diverse%20visual%20levels%2C%20impeding%20alignment%20with%20the%20various%0Asemantic%20granularity%20required%20for%20language%20generation.%20To%20address%20this%20issue%2C%0Awe%20present%20LLaVA-UHD%20v2%2C%20an%20advanced%20MLLM%20centered%20around%20a%20Hierarchical%20window%0Atransformer%20that%20enables%20capturing%20diverse%20visual%20granularity%20by%20constructing%0Aand%20integrating%20a%20high-resolution%20feature%20pyramid.%20As%20a%20vision-language%0Aprojector%2C%20Hiwin%20transformer%20comprises%20two%20primary%20modules%3A%20%28i%29%20an%20inverse%0Afeature%20pyramid%2C%20constructed%20by%20a%20ViT-derived%20feature%20up-sampling%20process%0Autilizing%20high-frequency%20details%20from%20an%20image%20pyramid%2C%20and%20%28ii%29%20hierarchical%0Awindow%20attention%2C%20focusing%20on%20a%20set%20of%20key%20sampling%20features%20within%20cross-scale%0Awindows%20to%20condense%20multi-level%20feature%20maps.%20Extensive%20experiments%20demonstrate%0Athat%20LLaVA-UHD%20v2%20achieves%20superior%20performance%20over%20existing%20MLLMs%20on%20popular%0Abenchmarks.%20Notably%2C%20our%20design%20brings%20an%20average%20boost%20of%203.7%25%20across%2014%0Abenchmarks%20compared%20with%20the%20baseline%20method%2C%209.3%25%20on%20DocVQA%20for%20instance.%20We%0Amake%20all%20the%20data%2C%20model%20checkpoint%2C%20and%20code%20publicly%20available%20to%20facilitate%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-UHD%2520v2%253A%2520an%2520MLLM%2520Integrating%2520High-Resolution%2520Feature%2520Pyramid%2520via%250A%2520%2520Hierarchical%2520Window%2520Transformer%26entry.906535625%3DYipeng%2520Zhang%2520and%2520Yifan%2520Liu%2520and%2520Zonghao%2520Guo%2520and%2520Yidan%2520Zhang%2520and%2520Xuesong%2520Yang%2520and%2520Chi%2520Chen%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%2520and%2520Yuan%2520Yao%2520and%2520Zhiyuan%2520Liu%2520and%2520Tat-Seng%2520Chua%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520In%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520vision%2520transformers%2520%2528ViTs%2529%2520are%250Awidely%2520employed%2520for%2520visual%2520encoding.%2520However%252C%2520their%2520performance%2520in%2520solving%250Auniversal%2520MLLM%2520tasks%2520is%2520not%2520satisfactory.%2520We%2520attribute%2520it%2520to%2520a%2520lack%2520of%250Ainformation%2520from%2520diverse%2520visual%2520levels%252C%2520impeding%2520alignment%2520with%2520the%2520various%250Asemantic%2520granularity%2520required%2520for%2520language%2520generation.%2520To%2520address%2520this%2520issue%252C%250Awe%2520present%2520LLaVA-UHD%2520v2%252C%2520an%2520advanced%2520MLLM%2520centered%2520around%2520a%2520Hierarchical%2520window%250Atransformer%2520that%2520enables%2520capturing%2520diverse%2520visual%2520granularity%2520by%2520constructing%250Aand%2520integrating%2520a%2520high-resolution%2520feature%2520pyramid.%2520As%2520a%2520vision-language%250Aprojector%252C%2520Hiwin%2520transformer%2520comprises%2520two%2520primary%2520modules%253A%2520%2528i%2529%2520an%2520inverse%250Afeature%2520pyramid%252C%2520constructed%2520by%2520a%2520ViT-derived%2520feature%2520up-sampling%2520process%250Autilizing%2520high-frequency%2520details%2520from%2520an%2520image%2520pyramid%252C%2520and%2520%2528ii%2529%2520hierarchical%250Awindow%2520attention%252C%2520focusing%2520on%2520a%2520set%2520of%2520key%2520sampling%2520features%2520within%2520cross-scale%250Awindows%2520to%2520condense%2520multi-level%2520feature%2520maps.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520LLaVA-UHD%2520v2%2520achieves%2520superior%2520performance%2520over%2520existing%2520MLLMs%2520on%2520popular%250Abenchmarks.%2520Notably%252C%2520our%2520design%2520brings%2520an%2520average%2520boost%2520of%25203.7%2525%2520across%252014%250Abenchmarks%2520compared%2520with%2520the%2520baseline%2520method%252C%25209.3%2525%2520on%2520DocVQA%2520for%2520instance.%2520We%250Amake%2520all%2520the%2520data%252C%2520model%2520checkpoint%252C%2520and%2520code%2520publicly%2520available%2520to%2520facilitate%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-UHD%20v2%3A%20an%20MLLM%20Integrating%20High-Resolution%20Feature%20Pyramid%20via%0A%20%20Hierarchical%20Window%20Transformer&entry.906535625=Yipeng%20Zhang%20and%20Yifan%20Liu%20and%20Zonghao%20Guo%20and%20Yidan%20Zhang%20and%20Xuesong%20Yang%20and%20Chi%20Chen%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Yuan%20Yao%20and%20Zhiyuan%20Liu%20and%20Tat-Seng%20Chua%20and%20Maosong%20Sun&entry.1292438233=%20%20In%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20vision%20transformers%20%28ViTs%29%20are%0Awidely%20employed%20for%20visual%20encoding.%20However%2C%20their%20performance%20in%20solving%0Auniversal%20MLLM%20tasks%20is%20not%20satisfactory.%20We%20attribute%20it%20to%20a%20lack%20of%0Ainformation%20from%20diverse%20visual%20levels%2C%20impeding%20alignment%20with%20the%20various%0Asemantic%20granularity%20required%20for%20language%20generation.%20To%20address%20this%20issue%2C%0Awe%20present%20LLaVA-UHD%20v2%2C%20an%20advanced%20MLLM%20centered%20around%20a%20Hierarchical%20window%0Atransformer%20that%20enables%20capturing%20diverse%20visual%20granularity%20by%20constructing%0Aand%20integrating%20a%20high-resolution%20feature%20pyramid.%20As%20a%20vision-language%0Aprojector%2C%20Hiwin%20transformer%20comprises%20two%20primary%20modules%3A%20%28i%29%20an%20inverse%0Afeature%20pyramid%2C%20constructed%20by%20a%20ViT-derived%20feature%20up-sampling%20process%0Autilizing%20high-frequency%20details%20from%20an%20image%20pyramid%2C%20and%20%28ii%29%20hierarchical%0Awindow%20attention%2C%20focusing%20on%20a%20set%20of%20key%20sampling%20features%20within%20cross-scale%0Awindows%20to%20condense%20multi-level%20feature%20maps.%20Extensive%20experiments%20demonstrate%0Athat%20LLaVA-UHD%20v2%20achieves%20superior%20performance%20over%20existing%20MLLMs%20on%20popular%0Abenchmarks.%20Notably%2C%20our%20design%20brings%20an%20average%20boost%20of%203.7%25%20across%2014%0Abenchmarks%20compared%20with%20the%20baseline%20method%2C%209.3%25%20on%20DocVQA%20for%20instance.%20We%0Amake%20all%20the%20data%2C%20model%20checkpoint%2C%20and%20code%20publicly%20available%20to%20facilitate%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13871v1&entry.124074799=Read"},
{"title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence", "author": "Jinghan He and Kuan Zhu and Haiyun Guo and Junfeng Fang and Zhenglin Hua and Yuheng Jia and Ming Tang and Tat-Seng Chua and Jinqiao Wang", "abstract": "  Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.\n", "link": "http://arxiv.org/abs/2412.13949v1", "date": "2024-12-18", "relevancy": 2.8369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5917}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cracking%20the%20Code%20of%20Hallucination%20in%20LVLMs%20with%20Vision-aware%20Head%0A%20%20Divergence&body=Title%3A%20Cracking%20the%20Code%20of%20Hallucination%20in%20LVLMs%20with%20Vision-aware%20Head%0A%20%20Divergence%0AAuthor%3A%20Jinghan%20He%20and%20Kuan%20Zhu%20and%20Haiyun%20Guo%20and%20Junfeng%20Fang%20and%20Zhenglin%20Hua%20and%20Yuheng%20Jia%20and%20Ming%20Tang%20and%20Tat-Seng%20Chua%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20made%20substantial%20progress%20in%0Aintegrating%20large%20language%20models%20%28LLMs%29%20with%20visual%20inputs%2C%20enabling%20advanced%0Amultimodal%20reasoning.%20Despite%20their%20success%2C%20a%20persistent%20challenge%20is%0Ahallucination-where%20generated%20text%20fails%20to%20accurately%20reflect%20visual%0Acontent-undermining%20both%20accuracy%20and%20reliability.%20Existing%20methods%20focus%20on%0Aalignment%20training%20or%20decoding%20refinements%20but%20primarily%20address%20symptoms%20at%0Athe%20generation%20stage%20without%20probing%20the%20underlying%20causes.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20internal%20mechanisms%20driving%20hallucination%20in%20LVLMs%2C%20with%20an%0Aemphasis%20on%20the%20multi-head%20attention%20module.%20Specifically%2C%20we%20introduce%0AVision-aware%20Head%20Divergence%20%28VHD%29%2C%20a%20metric%20that%20quantifies%20the%20sensitivity%20of%0Aattention%20head%20outputs%20to%20visual%20context.%20Based%20on%20this%2C%20our%20findings%20reveal%0Athe%20presence%20of%20vision-aware%20attention%20heads%20that%20are%20more%20attuned%20to%20visual%0Ainformation%3B%20however%2C%20the%20model%27s%20overreliance%20on%20its%20prior%20language%20patterns%0Ais%20closely%20related%20to%20hallucinations.%20Building%20on%20these%20insights%2C%20we%20propose%0AVision-aware%20Head%20Reinforcement%20%28VHR%29%2C%20a%20training-free%20approach%20to%20mitigate%0Ahallucination%20by%20enhancing%20the%20role%20of%20vision-aware%20attention%20heads.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%20compared%0Ato%20state-of-the-art%20approaches%20in%20mitigating%20hallucinations%2C%20while%20maintaining%0Ahigh%20efficiency%20with%20negligible%20additional%20time%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCracking%2520the%2520Code%2520of%2520Hallucination%2520in%2520LVLMs%2520with%2520Vision-aware%2520Head%250A%2520%2520Divergence%26entry.906535625%3DJinghan%2520He%2520and%2520Kuan%2520Zhu%2520and%2520Haiyun%2520Guo%2520and%2520Junfeng%2520Fang%2520and%2520Zhenglin%2520Hua%2520and%2520Yuheng%2520Jia%2520and%2520Ming%2520Tang%2520and%2520Tat-Seng%2520Chua%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520made%2520substantial%2520progress%2520in%250Aintegrating%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520visual%2520inputs%252C%2520enabling%2520advanced%250Amultimodal%2520reasoning.%2520Despite%2520their%2520success%252C%2520a%2520persistent%2520challenge%2520is%250Ahallucination-where%2520generated%2520text%2520fails%2520to%2520accurately%2520reflect%2520visual%250Acontent-undermining%2520both%2520accuracy%2520and%2520reliability.%2520Existing%2520methods%2520focus%2520on%250Aalignment%2520training%2520or%2520decoding%2520refinements%2520but%2520primarily%2520address%2520symptoms%2520at%250Athe%2520generation%2520stage%2520without%2520probing%2520the%2520underlying%2520causes.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520internal%2520mechanisms%2520driving%2520hallucination%2520in%2520LVLMs%252C%2520with%2520an%250Aemphasis%2520on%2520the%2520multi-head%2520attention%2520module.%2520Specifically%252C%2520we%2520introduce%250AVision-aware%2520Head%2520Divergence%2520%2528VHD%2529%252C%2520a%2520metric%2520that%2520quantifies%2520the%2520sensitivity%2520of%250Aattention%2520head%2520outputs%2520to%2520visual%2520context.%2520Based%2520on%2520this%252C%2520our%2520findings%2520reveal%250Athe%2520presence%2520of%2520vision-aware%2520attention%2520heads%2520that%2520are%2520more%2520attuned%2520to%2520visual%250Ainformation%253B%2520however%252C%2520the%2520model%2527s%2520overreliance%2520on%2520its%2520prior%2520language%2520patterns%250Ais%2520closely%2520related%2520to%2520hallucinations.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%250AVision-aware%2520Head%2520Reinforcement%2520%2528VHR%2529%252C%2520a%2520training-free%2520approach%2520to%2520mitigate%250Ahallucination%2520by%2520enhancing%2520the%2520role%2520of%2520vision-aware%2520attention%2520heads.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance%2520compared%250Ato%2520state-of-the-art%2520approaches%2520in%2520mitigating%2520hallucinations%252C%2520while%2520maintaining%250Ahigh%2520efficiency%2520with%2520negligible%2520additional%2520time%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cracking%20the%20Code%20of%20Hallucination%20in%20LVLMs%20with%20Vision-aware%20Head%0A%20%20Divergence&entry.906535625=Jinghan%20He%20and%20Kuan%20Zhu%20and%20Haiyun%20Guo%20and%20Junfeng%20Fang%20and%20Zhenglin%20Hua%20and%20Yuheng%20Jia%20and%20Ming%20Tang%20and%20Tat-Seng%20Chua%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Large%20vision-language%20models%20%28LVLMs%29%20have%20made%20substantial%20progress%20in%0Aintegrating%20large%20language%20models%20%28LLMs%29%20with%20visual%20inputs%2C%20enabling%20advanced%0Amultimodal%20reasoning.%20Despite%20their%20success%2C%20a%20persistent%20challenge%20is%0Ahallucination-where%20generated%20text%20fails%20to%20accurately%20reflect%20visual%0Acontent-undermining%20both%20accuracy%20and%20reliability.%20Existing%20methods%20focus%20on%0Aalignment%20training%20or%20decoding%20refinements%20but%20primarily%20address%20symptoms%20at%0Athe%20generation%20stage%20without%20probing%20the%20underlying%20causes.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20internal%20mechanisms%20driving%20hallucination%20in%20LVLMs%2C%20with%20an%0Aemphasis%20on%20the%20multi-head%20attention%20module.%20Specifically%2C%20we%20introduce%0AVision-aware%20Head%20Divergence%20%28VHD%29%2C%20a%20metric%20that%20quantifies%20the%20sensitivity%20of%0Aattention%20head%20outputs%20to%20visual%20context.%20Based%20on%20this%2C%20our%20findings%20reveal%0Athe%20presence%20of%20vision-aware%20attention%20heads%20that%20are%20more%20attuned%20to%20visual%0Ainformation%3B%20however%2C%20the%20model%27s%20overreliance%20on%20its%20prior%20language%20patterns%0Ais%20closely%20related%20to%20hallucinations.%20Building%20on%20these%20insights%2C%20we%20propose%0AVision-aware%20Head%20Reinforcement%20%28VHR%29%2C%20a%20training-free%20approach%20to%20mitigate%0Ahallucination%20by%20enhancing%20the%20role%20of%20vision-aware%20attention%20heads.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%20compared%0Ato%20state-of-the-art%20approaches%20in%20mitigating%20hallucinations%2C%20while%20maintaining%0Ahigh%20efficiency%20with%20negligible%20additional%20time%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13949v1&entry.124074799=Read"},
{"title": "Mesoscopic Insights: Orchestrating Multi-scale & Hybrid Architecture for\n  Image Manipulation Localization", "author": "Xuekang Zhu and Xiaochen Ma and Lei Su and Zhuohang Jiang and Bo Du and Xiwen Wang and Zeyu Lei and Wentao Feng and Chi-Man Pun and Jizhe Zhou", "abstract": "  The mesoscopic level serves as a bridge between the macroscopic and\nmicroscopic worlds, addressing gaps overlooked by both. Image manipulation\nlocalization (IML), a crucial technique to pursue truth from fake images, has\nlong relied on low-level (microscopic-level) traces. However, in practice, most\ntampering aims to deceive the audience by altering image semantics. As a\nresult, manipulation commonly occurs at the object level (macroscopic level),\nwhich is equally important as microscopic traces. Therefore, integrating these\ntwo levels into the mesoscopic level presents a new perspective for IML\nresearch. Inspired by this, our paper explores how to simultaneously construct\nmesoscopic representations of micro and macro information for IML and\nintroduces the Mesorch architecture to orchestrate both. Specifically, this\narchitecture i) combines Transformers and CNNs in parallel, with Transformers\nextracting macro information and CNNs capturing micro details, and ii) explores\nacross different scales, assessing micro and macro information seamlessly.\nAdditionally, based on the Mesorch architecture, the paper introduces two\nbaseline models aimed at solving IML tasks through mesoscopic representation.\nExtensive experiments across four datasets have demonstrated that our models\nsurpass the current state-of-the-art in terms of performance, computational\ncomplexity, and robustness.\n", "link": "http://arxiv.org/abs/2412.13753v1", "date": "2024-12-18", "relevancy": 2.8237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5846}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mesoscopic%20Insights%3A%20Orchestrating%20Multi-scale%20%26%20Hybrid%20Architecture%20for%0A%20%20Image%20Manipulation%20Localization&body=Title%3A%20Mesoscopic%20Insights%3A%20Orchestrating%20Multi-scale%20%26%20Hybrid%20Architecture%20for%0A%20%20Image%20Manipulation%20Localization%0AAuthor%3A%20Xuekang%20Zhu%20and%20Xiaochen%20Ma%20and%20Lei%20Su%20and%20Zhuohang%20Jiang%20and%20Bo%20Du%20and%20Xiwen%20Wang%20and%20Zeyu%20Lei%20and%20Wentao%20Feng%20and%20Chi-Man%20Pun%20and%20Jizhe%20Zhou%0AAbstract%3A%20%20%20The%20mesoscopic%20level%20serves%20as%20a%20bridge%20between%20the%20macroscopic%20and%0Amicroscopic%20worlds%2C%20addressing%20gaps%20overlooked%20by%20both.%20Image%20manipulation%0Alocalization%20%28IML%29%2C%20a%20crucial%20technique%20to%20pursue%20truth%20from%20fake%20images%2C%20has%0Along%20relied%20on%20low-level%20%28microscopic-level%29%20traces.%20However%2C%20in%20practice%2C%20most%0Atampering%20aims%20to%20deceive%20the%20audience%20by%20altering%20image%20semantics.%20As%20a%0Aresult%2C%20manipulation%20commonly%20occurs%20at%20the%20object%20level%20%28macroscopic%20level%29%2C%0Awhich%20is%20equally%20important%20as%20microscopic%20traces.%20Therefore%2C%20integrating%20these%0Atwo%20levels%20into%20the%20mesoscopic%20level%20presents%20a%20new%20perspective%20for%20IML%0Aresearch.%20Inspired%20by%20this%2C%20our%20paper%20explores%20how%20to%20simultaneously%20construct%0Amesoscopic%20representations%20of%20micro%20and%20macro%20information%20for%20IML%20and%0Aintroduces%20the%20Mesorch%20architecture%20to%20orchestrate%20both.%20Specifically%2C%20this%0Aarchitecture%20i%29%20combines%20Transformers%20and%20CNNs%20in%20parallel%2C%20with%20Transformers%0Aextracting%20macro%20information%20and%20CNNs%20capturing%20micro%20details%2C%20and%20ii%29%20explores%0Aacross%20different%20scales%2C%20assessing%20micro%20and%20macro%20information%20seamlessly.%0AAdditionally%2C%20based%20on%20the%20Mesorch%20architecture%2C%20the%20paper%20introduces%20two%0Abaseline%20models%20aimed%20at%20solving%20IML%20tasks%20through%20mesoscopic%20representation.%0AExtensive%20experiments%20across%20four%20datasets%20have%20demonstrated%20that%20our%20models%0Asurpass%20the%20current%20state-of-the-art%20in%20terms%20of%20performance%2C%20computational%0Acomplexity%2C%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMesoscopic%2520Insights%253A%2520Orchestrating%2520Multi-scale%2520%2526%2520Hybrid%2520Architecture%2520for%250A%2520%2520Image%2520Manipulation%2520Localization%26entry.906535625%3DXuekang%2520Zhu%2520and%2520Xiaochen%2520Ma%2520and%2520Lei%2520Su%2520and%2520Zhuohang%2520Jiang%2520and%2520Bo%2520Du%2520and%2520Xiwen%2520Wang%2520and%2520Zeyu%2520Lei%2520and%2520Wentao%2520Feng%2520and%2520Chi-Man%2520Pun%2520and%2520Jizhe%2520Zhou%26entry.1292438233%3D%2520%2520The%2520mesoscopic%2520level%2520serves%2520as%2520a%2520bridge%2520between%2520the%2520macroscopic%2520and%250Amicroscopic%2520worlds%252C%2520addressing%2520gaps%2520overlooked%2520by%2520both.%2520Image%2520manipulation%250Alocalization%2520%2528IML%2529%252C%2520a%2520crucial%2520technique%2520to%2520pursue%2520truth%2520from%2520fake%2520images%252C%2520has%250Along%2520relied%2520on%2520low-level%2520%2528microscopic-level%2529%2520traces.%2520However%252C%2520in%2520practice%252C%2520most%250Atampering%2520aims%2520to%2520deceive%2520the%2520audience%2520by%2520altering%2520image%2520semantics.%2520As%2520a%250Aresult%252C%2520manipulation%2520commonly%2520occurs%2520at%2520the%2520object%2520level%2520%2528macroscopic%2520level%2529%252C%250Awhich%2520is%2520equally%2520important%2520as%2520microscopic%2520traces.%2520Therefore%252C%2520integrating%2520these%250Atwo%2520levels%2520into%2520the%2520mesoscopic%2520level%2520presents%2520a%2520new%2520perspective%2520for%2520IML%250Aresearch.%2520Inspired%2520by%2520this%252C%2520our%2520paper%2520explores%2520how%2520to%2520simultaneously%2520construct%250Amesoscopic%2520representations%2520of%2520micro%2520and%2520macro%2520information%2520for%2520IML%2520and%250Aintroduces%2520the%2520Mesorch%2520architecture%2520to%2520orchestrate%2520both.%2520Specifically%252C%2520this%250Aarchitecture%2520i%2529%2520combines%2520Transformers%2520and%2520CNNs%2520in%2520parallel%252C%2520with%2520Transformers%250Aextracting%2520macro%2520information%2520and%2520CNNs%2520capturing%2520micro%2520details%252C%2520and%2520ii%2529%2520explores%250Aacross%2520different%2520scales%252C%2520assessing%2520micro%2520and%2520macro%2520information%2520seamlessly.%250AAdditionally%252C%2520based%2520on%2520the%2520Mesorch%2520architecture%252C%2520the%2520paper%2520introduces%2520two%250Abaseline%2520models%2520aimed%2520at%2520solving%2520IML%2520tasks%2520through%2520mesoscopic%2520representation.%250AExtensive%2520experiments%2520across%2520four%2520datasets%2520have%2520demonstrated%2520that%2520our%2520models%250Asurpass%2520the%2520current%2520state-of-the-art%2520in%2520terms%2520of%2520performance%252C%2520computational%250Acomplexity%252C%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mesoscopic%20Insights%3A%20Orchestrating%20Multi-scale%20%26%20Hybrid%20Architecture%20for%0A%20%20Image%20Manipulation%20Localization&entry.906535625=Xuekang%20Zhu%20and%20Xiaochen%20Ma%20and%20Lei%20Su%20and%20Zhuohang%20Jiang%20and%20Bo%20Du%20and%20Xiwen%20Wang%20and%20Zeyu%20Lei%20and%20Wentao%20Feng%20and%20Chi-Man%20Pun%20and%20Jizhe%20Zhou&entry.1292438233=%20%20The%20mesoscopic%20level%20serves%20as%20a%20bridge%20between%20the%20macroscopic%20and%0Amicroscopic%20worlds%2C%20addressing%20gaps%20overlooked%20by%20both.%20Image%20manipulation%0Alocalization%20%28IML%29%2C%20a%20crucial%20technique%20to%20pursue%20truth%20from%20fake%20images%2C%20has%0Along%20relied%20on%20low-level%20%28microscopic-level%29%20traces.%20However%2C%20in%20practice%2C%20most%0Atampering%20aims%20to%20deceive%20the%20audience%20by%20altering%20image%20semantics.%20As%20a%0Aresult%2C%20manipulation%20commonly%20occurs%20at%20the%20object%20level%20%28macroscopic%20level%29%2C%0Awhich%20is%20equally%20important%20as%20microscopic%20traces.%20Therefore%2C%20integrating%20these%0Atwo%20levels%20into%20the%20mesoscopic%20level%20presents%20a%20new%20perspective%20for%20IML%0Aresearch.%20Inspired%20by%20this%2C%20our%20paper%20explores%20how%20to%20simultaneously%20construct%0Amesoscopic%20representations%20of%20micro%20and%20macro%20information%20for%20IML%20and%0Aintroduces%20the%20Mesorch%20architecture%20to%20orchestrate%20both.%20Specifically%2C%20this%0Aarchitecture%20i%29%20combines%20Transformers%20and%20CNNs%20in%20parallel%2C%20with%20Transformers%0Aextracting%20macro%20information%20and%20CNNs%20capturing%20micro%20details%2C%20and%20ii%29%20explores%0Aacross%20different%20scales%2C%20assessing%20micro%20and%20macro%20information%20seamlessly.%0AAdditionally%2C%20based%20on%20the%20Mesorch%20architecture%2C%20the%20paper%20introduces%20two%0Abaseline%20models%20aimed%20at%20solving%20IML%20tasks%20through%20mesoscopic%20representation.%0AExtensive%20experiments%20across%20four%20datasets%20have%20demonstrated%20that%20our%20models%0Asurpass%20the%20current%20state-of-the-art%20in%20terms%20of%20performance%2C%20computational%0Acomplexity%2C%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13753v1&entry.124074799=Read"},
{"title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth\n  Estimation", "author": "Haotong Lin and Sida Peng and Jingxiao Chen and Songyou Peng and Jiaming Sun and Minghuan Liu and Hujun Bao and Jiashi Feng and Xiaowei Zhou and Bingyi Kang", "abstract": "  Prompts play a critical role in unleashing the power of language and vision\nfoundation models for specific tasks. For the first time, we introduce\nprompting into depth foundation models, creating a new paradigm for metric\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\nfusion design that integrates the LiDAR at multiple scales within the depth\ndecoder. To address training challenges posed by limited datasets containing\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\nScanNet++ datasets and benefits downstream applications, including 3D\nreconstruction and generalized robotic grasping.\n", "link": "http://arxiv.org/abs/2412.14015v1", "date": "2024-12-18", "relevancy": 2.818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5809}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompting%20Depth%20Anything%20for%204K%20Resolution%20Accurate%20Metric%20Depth%0A%20%20Estimation&body=Title%3A%20Prompting%20Depth%20Anything%20for%204K%20Resolution%20Accurate%20Metric%20Depth%0A%20%20Estimation%0AAuthor%3A%20Haotong%20Lin%20and%20Sida%20Peng%20and%20Jingxiao%20Chen%20and%20Songyou%20Peng%20and%20Jiaming%20Sun%20and%20Minghuan%20Liu%20and%20Hujun%20Bao%20and%20Jiashi%20Feng%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang%0AAbstract%3A%20%20%20Prompts%20play%20a%20critical%20role%20in%20unleashing%20the%20power%20of%20language%20and%20vision%0Afoundation%20models%20for%20specific%20tasks.%20For%20the%20first%20time%2C%20we%20introduce%0Aprompting%20into%20depth%20foundation%20models%2C%20creating%20a%20new%20paradigm%20for%20metric%0Adepth%20estimation%20termed%20Prompt%20Depth%20Anything.%20Specifically%2C%20we%20use%20a%20low-cost%0ALiDAR%20as%20the%20prompt%20to%20guide%20the%20Depth%20Anything%20model%20for%20accurate%20metric%20depth%0Aoutput%2C%20achieving%20up%20to%204K%20resolution.%20Our%20approach%20centers%20on%20a%20concise%20prompt%0Afusion%20design%20that%20integrates%20the%20LiDAR%20at%20multiple%20scales%20within%20the%20depth%0Adecoder.%20To%20address%20training%20challenges%20posed%20by%20limited%20datasets%20containing%0Aboth%20LiDAR%20depth%20and%20precise%20GT%20depth%2C%20we%20propose%20a%20scalable%20data%20pipeline%20that%0Aincludes%20synthetic%20data%20LiDAR%20simulation%20and%20real%20data%20pseudo%20GT%20depth%0Ageneration.%20Our%20approach%20sets%20new%20state-of-the-arts%20on%20the%20ARKitScenes%20and%0AScanNet%2B%2B%20datasets%20and%20benefits%20downstream%20applications%2C%20including%203D%0Areconstruction%20and%20generalized%20robotic%20grasping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompting%2520Depth%2520Anything%2520for%25204K%2520Resolution%2520Accurate%2520Metric%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DHaotong%2520Lin%2520and%2520Sida%2520Peng%2520and%2520Jingxiao%2520Chen%2520and%2520Songyou%2520Peng%2520and%2520Jiaming%2520Sun%2520and%2520Minghuan%2520Liu%2520and%2520Hujun%2520Bao%2520and%2520Jiashi%2520Feng%2520and%2520Xiaowei%2520Zhou%2520and%2520Bingyi%2520Kang%26entry.1292438233%3D%2520%2520Prompts%2520play%2520a%2520critical%2520role%2520in%2520unleashing%2520the%2520power%2520of%2520language%2520and%2520vision%250Afoundation%2520models%2520for%2520specific%2520tasks.%2520For%2520the%2520first%2520time%252C%2520we%2520introduce%250Aprompting%2520into%2520depth%2520foundation%2520models%252C%2520creating%2520a%2520new%2520paradigm%2520for%2520metric%250Adepth%2520estimation%2520termed%2520Prompt%2520Depth%2520Anything.%2520Specifically%252C%2520we%2520use%2520a%2520low-cost%250ALiDAR%2520as%2520the%2520prompt%2520to%2520guide%2520the%2520Depth%2520Anything%2520model%2520for%2520accurate%2520metric%2520depth%250Aoutput%252C%2520achieving%2520up%2520to%25204K%2520resolution.%2520Our%2520approach%2520centers%2520on%2520a%2520concise%2520prompt%250Afusion%2520design%2520that%2520integrates%2520the%2520LiDAR%2520at%2520multiple%2520scales%2520within%2520the%2520depth%250Adecoder.%2520To%2520address%2520training%2520challenges%2520posed%2520by%2520limited%2520datasets%2520containing%250Aboth%2520LiDAR%2520depth%2520and%2520precise%2520GT%2520depth%252C%2520we%2520propose%2520a%2520scalable%2520data%2520pipeline%2520that%250Aincludes%2520synthetic%2520data%2520LiDAR%2520simulation%2520and%2520real%2520data%2520pseudo%2520GT%2520depth%250Ageneration.%2520Our%2520approach%2520sets%2520new%2520state-of-the-arts%2520on%2520the%2520ARKitScenes%2520and%250AScanNet%252B%252B%2520datasets%2520and%2520benefits%2520downstream%2520applications%252C%2520including%25203D%250Areconstruction%2520and%2520generalized%2520robotic%2520grasping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompting%20Depth%20Anything%20for%204K%20Resolution%20Accurate%20Metric%20Depth%0A%20%20Estimation&entry.906535625=Haotong%20Lin%20and%20Sida%20Peng%20and%20Jingxiao%20Chen%20and%20Songyou%20Peng%20and%20Jiaming%20Sun%20and%20Minghuan%20Liu%20and%20Hujun%20Bao%20and%20Jiashi%20Feng%20and%20Xiaowei%20Zhou%20and%20Bingyi%20Kang&entry.1292438233=%20%20Prompts%20play%20a%20critical%20role%20in%20unleashing%20the%20power%20of%20language%20and%20vision%0Afoundation%20models%20for%20specific%20tasks.%20For%20the%20first%20time%2C%20we%20introduce%0Aprompting%20into%20depth%20foundation%20models%2C%20creating%20a%20new%20paradigm%20for%20metric%0Adepth%20estimation%20termed%20Prompt%20Depth%20Anything.%20Specifically%2C%20we%20use%20a%20low-cost%0ALiDAR%20as%20the%20prompt%20to%20guide%20the%20Depth%20Anything%20model%20for%20accurate%20metric%20depth%0Aoutput%2C%20achieving%20up%20to%204K%20resolution.%20Our%20approach%20centers%20on%20a%20concise%20prompt%0Afusion%20design%20that%20integrates%20the%20LiDAR%20at%20multiple%20scales%20within%20the%20depth%0Adecoder.%20To%20address%20training%20challenges%20posed%20by%20limited%20datasets%20containing%0Aboth%20LiDAR%20depth%20and%20precise%20GT%20depth%2C%20we%20propose%20a%20scalable%20data%20pipeline%20that%0Aincludes%20synthetic%20data%20LiDAR%20simulation%20and%20real%20data%20pseudo%20GT%20depth%0Ageneration.%20Our%20approach%20sets%20new%20state-of-the-arts%20on%20the%20ARKitScenes%20and%0AScanNet%2B%2B%20datasets%20and%20benefits%20downstream%20applications%2C%20including%203D%0Areconstruction%20and%20generalized%20robotic%20grasping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14015v1&entry.124074799=Read"},
{"title": "Foundation Models Meet Low-Cost Sensors: Test-Time Adaptation for\n  Rescaling Disparity for Zero-Shot Metric Depth Estimation", "author": "R\u00e9mi Marsal and Alexandre Chapoutot and Philippe Xu and David Filliat", "abstract": "  The recent development of foundation models for monocular depth estimation\nsuch as Depth Anything paved the way to zero-shot monocular depth estimation.\nSince it returns an affine-invariant disparity map, the favored technique to\nrecover the metric depth consists in fine-tuning the model. However, this stage\nis costly to perform because of the training but also due to the creation of\nthe dataset. It must contain images captured by the camera that will be used at\ntest time and the corresponding ground truth. Moreover, the fine-tuning may\nalso degrade the generalizing capacity of the original model. Instead, we\npropose in this paper a new method to rescale Depth Anything predictions using\n3D points provided by low-cost sensors or techniques such as low-resolution\nLiDAR, stereo camera, structure-from-motion where poses are given by an IMU.\nThus, this approach avoids fine-tuning and preserves the generalizing power of\nthe original depth estimation model while being robust to the noise of the\nsensor or of the depth model. Our experiments highlight improvements relative\nto other metric depth estimation methods and competitive results compared to\nfine-tuned approaches. Code available at\nhttps://gitlab.ensta.fr/ssh/monocular-depth-rescaling.\n", "link": "http://arxiv.org/abs/2412.14103v1", "date": "2024-12-18", "relevancy": 2.8043, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20Meet%20Low-Cost%20Sensors%3A%20Test-Time%20Adaptation%20for%0A%20%20Rescaling%20Disparity%20for%20Zero-Shot%20Metric%20Depth%20Estimation&body=Title%3A%20Foundation%20Models%20Meet%20Low-Cost%20Sensors%3A%20Test-Time%20Adaptation%20for%0A%20%20Rescaling%20Disparity%20for%20Zero-Shot%20Metric%20Depth%20Estimation%0AAuthor%3A%20R%C3%A9mi%20Marsal%20and%20Alexandre%20Chapoutot%20and%20Philippe%20Xu%20and%20David%20Filliat%0AAbstract%3A%20%20%20The%20recent%20development%20of%20foundation%20models%20for%20monocular%20depth%20estimation%0Asuch%20as%20Depth%20Anything%20paved%20the%20way%20to%20zero-shot%20monocular%20depth%20estimation.%0ASince%20it%20returns%20an%20affine-invariant%20disparity%20map%2C%20the%20favored%20technique%20to%0Arecover%20the%20metric%20depth%20consists%20in%20fine-tuning%20the%20model.%20However%2C%20this%20stage%0Ais%20costly%20to%20perform%20because%20of%20the%20training%20but%20also%20due%20to%20the%20creation%20of%0Athe%20dataset.%20It%20must%20contain%20images%20captured%20by%20the%20camera%20that%20will%20be%20used%20at%0Atest%20time%20and%20the%20corresponding%20ground%20truth.%20Moreover%2C%20the%20fine-tuning%20may%0Aalso%20degrade%20the%20generalizing%20capacity%20of%20the%20original%20model.%20Instead%2C%20we%0Apropose%20in%20this%20paper%20a%20new%20method%20to%20rescale%20Depth%20Anything%20predictions%20using%0A3D%20points%20provided%20by%20low-cost%20sensors%20or%20techniques%20such%20as%20low-resolution%0ALiDAR%2C%20stereo%20camera%2C%20structure-from-motion%20where%20poses%20are%20given%20by%20an%20IMU.%0AThus%2C%20this%20approach%20avoids%20fine-tuning%20and%20preserves%20the%20generalizing%20power%20of%0Athe%20original%20depth%20estimation%20model%20while%20being%20robust%20to%20the%20noise%20of%20the%0Asensor%20or%20of%20the%20depth%20model.%20Our%20experiments%20highlight%20improvements%20relative%0Ato%20other%20metric%20depth%20estimation%20methods%20and%20competitive%20results%20compared%20to%0Afine-tuned%20approaches.%20Code%20available%20at%0Ahttps%3A//gitlab.ensta.fr/ssh/monocular-depth-rescaling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520Meet%2520Low-Cost%2520Sensors%253A%2520Test-Time%2520Adaptation%2520for%250A%2520%2520Rescaling%2520Disparity%2520for%2520Zero-Shot%2520Metric%2520Depth%2520Estimation%26entry.906535625%3DR%25C3%25A9mi%2520Marsal%2520and%2520Alexandre%2520Chapoutot%2520and%2520Philippe%2520Xu%2520and%2520David%2520Filliat%26entry.1292438233%3D%2520%2520The%2520recent%2520development%2520of%2520foundation%2520models%2520for%2520monocular%2520depth%2520estimation%250Asuch%2520as%2520Depth%2520Anything%2520paved%2520the%2520way%2520to%2520zero-shot%2520monocular%2520depth%2520estimation.%250ASince%2520it%2520returns%2520an%2520affine-invariant%2520disparity%2520map%252C%2520the%2520favored%2520technique%2520to%250Arecover%2520the%2520metric%2520depth%2520consists%2520in%2520fine-tuning%2520the%2520model.%2520However%252C%2520this%2520stage%250Ais%2520costly%2520to%2520perform%2520because%2520of%2520the%2520training%2520but%2520also%2520due%2520to%2520the%2520creation%2520of%250Athe%2520dataset.%2520It%2520must%2520contain%2520images%2520captured%2520by%2520the%2520camera%2520that%2520will%2520be%2520used%2520at%250Atest%2520time%2520and%2520the%2520corresponding%2520ground%2520truth.%2520Moreover%252C%2520the%2520fine-tuning%2520may%250Aalso%2520degrade%2520the%2520generalizing%2520capacity%2520of%2520the%2520original%2520model.%2520Instead%252C%2520we%250Apropose%2520in%2520this%2520paper%2520a%2520new%2520method%2520to%2520rescale%2520Depth%2520Anything%2520predictions%2520using%250A3D%2520points%2520provided%2520by%2520low-cost%2520sensors%2520or%2520techniques%2520such%2520as%2520low-resolution%250ALiDAR%252C%2520stereo%2520camera%252C%2520structure-from-motion%2520where%2520poses%2520are%2520given%2520by%2520an%2520IMU.%250AThus%252C%2520this%2520approach%2520avoids%2520fine-tuning%2520and%2520preserves%2520the%2520generalizing%2520power%2520of%250Athe%2520original%2520depth%2520estimation%2520model%2520while%2520being%2520robust%2520to%2520the%2520noise%2520of%2520the%250Asensor%2520or%2520of%2520the%2520depth%2520model.%2520Our%2520experiments%2520highlight%2520improvements%2520relative%250Ato%2520other%2520metric%2520depth%2520estimation%2520methods%2520and%2520competitive%2520results%2520compared%2520to%250Afine-tuned%2520approaches.%2520Code%2520available%2520at%250Ahttps%253A//gitlab.ensta.fr/ssh/monocular-depth-rescaling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20Meet%20Low-Cost%20Sensors%3A%20Test-Time%20Adaptation%20for%0A%20%20Rescaling%20Disparity%20for%20Zero-Shot%20Metric%20Depth%20Estimation&entry.906535625=R%C3%A9mi%20Marsal%20and%20Alexandre%20Chapoutot%20and%20Philippe%20Xu%20and%20David%20Filliat&entry.1292438233=%20%20The%20recent%20development%20of%20foundation%20models%20for%20monocular%20depth%20estimation%0Asuch%20as%20Depth%20Anything%20paved%20the%20way%20to%20zero-shot%20monocular%20depth%20estimation.%0ASince%20it%20returns%20an%20affine-invariant%20disparity%20map%2C%20the%20favored%20technique%20to%0Arecover%20the%20metric%20depth%20consists%20in%20fine-tuning%20the%20model.%20However%2C%20this%20stage%0Ais%20costly%20to%20perform%20because%20of%20the%20training%20but%20also%20due%20to%20the%20creation%20of%0Athe%20dataset.%20It%20must%20contain%20images%20captured%20by%20the%20camera%20that%20will%20be%20used%20at%0Atest%20time%20and%20the%20corresponding%20ground%20truth.%20Moreover%2C%20the%20fine-tuning%20may%0Aalso%20degrade%20the%20generalizing%20capacity%20of%20the%20original%20model.%20Instead%2C%20we%0Apropose%20in%20this%20paper%20a%20new%20method%20to%20rescale%20Depth%20Anything%20predictions%20using%0A3D%20points%20provided%20by%20low-cost%20sensors%20or%20techniques%20such%20as%20low-resolution%0ALiDAR%2C%20stereo%20camera%2C%20structure-from-motion%20where%20poses%20are%20given%20by%20an%20IMU.%0AThus%2C%20this%20approach%20avoids%20fine-tuning%20and%20preserves%20the%20generalizing%20power%20of%0Athe%20original%20depth%20estimation%20model%20while%20being%20robust%20to%20the%20noise%20of%20the%0Asensor%20or%20of%20the%20depth%20model.%20Our%20experiments%20highlight%20improvements%20relative%0Ato%20other%20metric%20depth%20estimation%20methods%20and%20competitive%20results%20compared%20to%0Afine-tuned%20approaches.%20Code%20available%20at%0Ahttps%3A//gitlab.ensta.fr/ssh/monocular-depth-rescaling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14103v1&entry.124074799=Read"},
{"title": "Memorizing SAM: 3D Medical Segment Anything Model with Memorizing\n  Transformer", "author": "Xinyuan Shao and Yiqing Shen and Mathias Unberath", "abstract": "  Segment Anything Models (SAMs) have gained increasing attention in medical\nimage analysis due to their zero-shot generalization capability in segmenting\nobjects of unseen classes and domains when provided with appropriate user\nprompts. Addressing this performance gap is important to fully leverage the\npre-trained weights of SAMs, particularly in the domain of volumetric medical\nimage segmentation, where accuracy is important but well-annotated 3D medical\ndata for fine-tuning is limited. In this work, we investigate whether\nintroducing the memory mechanism as a plug-in, specifically the ability to\nmemorize and recall internal representations of past inputs, can improve the\nperformance of SAM with limited computation cost. To this end, we propose\nMemorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer\nas a plug-in. Unlike conventional memorizing Transformers that save the\ninternal representation during training or inference, our Memorizing SAM\nutilizes existing highly accurate internal representation as the memory source\nto ensure the quality of memory. We evaluate the performance of Memorizing SAM\nin 33 categories from the TotalSegmentator dataset, which indicates that\nMemorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D\nwith an average Dice increase of 11.36% at the cost of only 4.38 millisecond\nincrease in inference time. The source code is publicly available at\nhttps://github.com/swedfr/memorizingSAM\n", "link": "http://arxiv.org/abs/2412.13908v1", "date": "2024-12-18", "relevancy": 2.7838, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6145}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5418}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memorizing%20SAM%3A%203D%20Medical%20Segment%20Anything%20Model%20with%20Memorizing%0A%20%20Transformer&body=Title%3A%20Memorizing%20SAM%3A%203D%20Medical%20Segment%20Anything%20Model%20with%20Memorizing%0A%20%20Transformer%0AAuthor%3A%20Xinyuan%20Shao%20and%20Yiqing%20Shen%20and%20Mathias%20Unberath%0AAbstract%3A%20%20%20Segment%20Anything%20Models%20%28SAMs%29%20have%20gained%20increasing%20attention%20in%20medical%0Aimage%20analysis%20due%20to%20their%20zero-shot%20generalization%20capability%20in%20segmenting%0Aobjects%20of%20unseen%20classes%20and%20domains%20when%20provided%20with%20appropriate%20user%0Aprompts.%20Addressing%20this%20performance%20gap%20is%20important%20to%20fully%20leverage%20the%0Apre-trained%20weights%20of%20SAMs%2C%20particularly%20in%20the%20domain%20of%20volumetric%20medical%0Aimage%20segmentation%2C%20where%20accuracy%20is%20important%20but%20well-annotated%203D%20medical%0Adata%20for%20fine-tuning%20is%20limited.%20In%20this%20work%2C%20we%20investigate%20whether%0Aintroducing%20the%20memory%20mechanism%20as%20a%20plug-in%2C%20specifically%20the%20ability%20to%0Amemorize%20and%20recall%20internal%20representations%20of%20past%20inputs%2C%20can%20improve%20the%0Aperformance%20of%20SAM%20with%20limited%20computation%20cost.%20To%20this%20end%2C%20we%20propose%0AMemorizing%20SAM%2C%20a%20novel%203D%20SAM%20architecture%20incorporating%20a%20memory%20Transformer%0Aas%20a%20plug-in.%20Unlike%20conventional%20memorizing%20Transformers%20that%20save%20the%0Ainternal%20representation%20during%20training%20or%20inference%2C%20our%20Memorizing%20SAM%0Autilizes%20existing%20highly%20accurate%20internal%20representation%20as%20the%20memory%20source%0Ato%20ensure%20the%20quality%20of%20memory.%20We%20evaluate%20the%20performance%20of%20Memorizing%20SAM%0Ain%2033%20categories%20from%20the%20TotalSegmentator%20dataset%2C%20which%20indicates%20that%0AMemorizing%20SAM%20can%20outperform%20state-of-the-art%203D%20SAM%20variant%20i.e.%2C%20FastSAM3D%0Awith%20an%20average%20Dice%20increase%20of%2011.36%25%20at%20the%20cost%20of%20only%204.38%20millisecond%0Aincrease%20in%20inference%20time.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/swedfr/memorizingSAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemorizing%2520SAM%253A%25203D%2520Medical%2520Segment%2520Anything%2520Model%2520with%2520Memorizing%250A%2520%2520Transformer%26entry.906535625%3DXinyuan%2520Shao%2520and%2520Yiqing%2520Shen%2520and%2520Mathias%2520Unberath%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Models%2520%2528SAMs%2529%2520have%2520gained%2520increasing%2520attention%2520in%2520medical%250Aimage%2520analysis%2520due%2520to%2520their%2520zero-shot%2520generalization%2520capability%2520in%2520segmenting%250Aobjects%2520of%2520unseen%2520classes%2520and%2520domains%2520when%2520provided%2520with%2520appropriate%2520user%250Aprompts.%2520Addressing%2520this%2520performance%2520gap%2520is%2520important%2520to%2520fully%2520leverage%2520the%250Apre-trained%2520weights%2520of%2520SAMs%252C%2520particularly%2520in%2520the%2520domain%2520of%2520volumetric%2520medical%250Aimage%2520segmentation%252C%2520where%2520accuracy%2520is%2520important%2520but%2520well-annotated%25203D%2520medical%250Adata%2520for%2520fine-tuning%2520is%2520limited.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%250Aintroducing%2520the%2520memory%2520mechanism%2520as%2520a%2520plug-in%252C%2520specifically%2520the%2520ability%2520to%250Amemorize%2520and%2520recall%2520internal%2520representations%2520of%2520past%2520inputs%252C%2520can%2520improve%2520the%250Aperformance%2520of%2520SAM%2520with%2520limited%2520computation%2520cost.%2520To%2520this%2520end%252C%2520we%2520propose%250AMemorizing%2520SAM%252C%2520a%2520novel%25203D%2520SAM%2520architecture%2520incorporating%2520a%2520memory%2520Transformer%250Aas%2520a%2520plug-in.%2520Unlike%2520conventional%2520memorizing%2520Transformers%2520that%2520save%2520the%250Ainternal%2520representation%2520during%2520training%2520or%2520inference%252C%2520our%2520Memorizing%2520SAM%250Autilizes%2520existing%2520highly%2520accurate%2520internal%2520representation%2520as%2520the%2520memory%2520source%250Ato%2520ensure%2520the%2520quality%2520of%2520memory.%2520We%2520evaluate%2520the%2520performance%2520of%2520Memorizing%2520SAM%250Ain%252033%2520categories%2520from%2520the%2520TotalSegmentator%2520dataset%252C%2520which%2520indicates%2520that%250AMemorizing%2520SAM%2520can%2520outperform%2520state-of-the-art%25203D%2520SAM%2520variant%2520i.e.%252C%2520FastSAM3D%250Awith%2520an%2520average%2520Dice%2520increase%2520of%252011.36%2525%2520at%2520the%2520cost%2520of%2520only%25204.38%2520millisecond%250Aincrease%2520in%2520inference%2520time.%2520The%2520source%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/swedfr/memorizingSAM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memorizing%20SAM%3A%203D%20Medical%20Segment%20Anything%20Model%20with%20Memorizing%0A%20%20Transformer&entry.906535625=Xinyuan%20Shao%20and%20Yiqing%20Shen%20and%20Mathias%20Unberath&entry.1292438233=%20%20Segment%20Anything%20Models%20%28SAMs%29%20have%20gained%20increasing%20attention%20in%20medical%0Aimage%20analysis%20due%20to%20their%20zero-shot%20generalization%20capability%20in%20segmenting%0Aobjects%20of%20unseen%20classes%20and%20domains%20when%20provided%20with%20appropriate%20user%0Aprompts.%20Addressing%20this%20performance%20gap%20is%20important%20to%20fully%20leverage%20the%0Apre-trained%20weights%20of%20SAMs%2C%20particularly%20in%20the%20domain%20of%20volumetric%20medical%0Aimage%20segmentation%2C%20where%20accuracy%20is%20important%20but%20well-annotated%203D%20medical%0Adata%20for%20fine-tuning%20is%20limited.%20In%20this%20work%2C%20we%20investigate%20whether%0Aintroducing%20the%20memory%20mechanism%20as%20a%20plug-in%2C%20specifically%20the%20ability%20to%0Amemorize%20and%20recall%20internal%20representations%20of%20past%20inputs%2C%20can%20improve%20the%0Aperformance%20of%20SAM%20with%20limited%20computation%20cost.%20To%20this%20end%2C%20we%20propose%0AMemorizing%20SAM%2C%20a%20novel%203D%20SAM%20architecture%20incorporating%20a%20memory%20Transformer%0Aas%20a%20plug-in.%20Unlike%20conventional%20memorizing%20Transformers%20that%20save%20the%0Ainternal%20representation%20during%20training%20or%20inference%2C%20our%20Memorizing%20SAM%0Autilizes%20existing%20highly%20accurate%20internal%20representation%20as%20the%20memory%20source%0Ato%20ensure%20the%20quality%20of%20memory.%20We%20evaluate%20the%20performance%20of%20Memorizing%20SAM%0Ain%2033%20categories%20from%20the%20TotalSegmentator%20dataset%2C%20which%20indicates%20that%0AMemorizing%20SAM%20can%20outperform%20state-of-the-art%203D%20SAM%20variant%20i.e.%2C%20FastSAM3D%0Awith%20an%20average%20Dice%20increase%20of%2011.36%25%20at%20the%20cost%20of%20only%204.38%20millisecond%0Aincrease%20in%20inference%20time.%20The%20source%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/swedfr/memorizingSAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13908v1&entry.124074799=Read"},
{"title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds", "author": "Danila Rukhovich and Elona Dupont and Dimitrios Mallis and Kseniya Cherenkova and Anis Kacem and Djamila Aouada", "abstract": "  Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.\n", "link": "http://arxiv.org/abs/2412.14042v1", "date": "2024-12-18", "relevancy": 2.7411, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5504}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5504}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Recode%3A%20Reverse%20Engineering%20CAD%20Code%20from%20Point%20Clouds&body=Title%3A%20CAD-Recode%3A%20Reverse%20Engineering%20CAD%20Code%20from%20Point%20Clouds%0AAuthor%3A%20Danila%20Rukhovich%20and%20Elona%20Dupont%20and%20Dimitrios%20Mallis%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Computer-Aided%20Design%20%28CAD%29%20models%20are%20typically%20constructed%20by%20sequentially%0Adrawing%20parametric%20sketches%20and%20applying%20CAD%20operations%20to%20obtain%20a%203D%20model.%0AThe%20problem%20of%203D%20CAD%20reverse%20engineering%20consists%20of%20reconstructing%20the%20sketch%0Aand%20CAD%20operation%20sequences%20from%203D%20representations%20such%20as%20point%20clouds.%20In%0Athis%20paper%2C%20we%20address%20this%20challenge%20through%20novel%20contributions%20across%20three%0Alevels%3A%20CAD%20sequence%20representation%2C%20network%20design%2C%20and%20dataset.%20In%0Aparticular%2C%20we%20represent%20CAD%20sketch-extrude%20sequences%20as%20Python%20code.%20The%0Aproposed%20CAD-Recode%20translates%20a%20point%20cloud%20into%20Python%20code%20that%2C%20when%0Aexecuted%2C%20reconstructs%20the%20CAD%20model.%20Taking%20advantage%20of%20the%20exposure%20of%0Apre-trained%20Large%20Language%20Models%20%28LLMs%29%20to%20Python%20code%2C%20we%20leverage%20a%0Arelatively%20small%20LLM%20as%20a%20decoder%20for%20CAD-Recode%20and%20combine%20it%20with%20a%0Alightweight%20point%20cloud%20projector.%20CAD-Recode%20is%20trained%20solely%20on%20a%20proposed%0Asynthetic%20dataset%20of%20one%20million%20diverse%20CAD%20sequences.%20CAD-Recode%0Asignificantly%20outperforms%20existing%20methods%20across%20three%20datasets%20while%0Arequiring%20fewer%20input%20points.%20Notably%2C%20it%20achieves%2010%20times%20lower%20mean%20Chamfer%0Adistance%20than%20state-of-the-art%20methods%20on%20DeepCAD%20and%20Fusion360%20datasets.%0AFurthermore%2C%20we%20show%20that%20our%20CAD%20Python%20code%20output%20is%20interpretable%20by%0Aoff-the-shelf%20LLMs%2C%20enabling%20CAD%20editing%20and%20CAD-specific%20question%20answering%0Afrom%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Recode%253A%2520Reverse%2520Engineering%2520CAD%2520Code%2520from%2520Point%2520Clouds%26entry.906535625%3DDanila%2520Rukhovich%2520and%2520Elona%2520Dupont%2520and%2520Dimitrios%2520Mallis%2520and%2520Kseniya%2520Cherenkova%2520and%2520Anis%2520Kacem%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520models%2520are%2520typically%2520constructed%2520by%2520sequentially%250Adrawing%2520parametric%2520sketches%2520and%2520applying%2520CAD%2520operations%2520to%2520obtain%2520a%25203D%2520model.%250AThe%2520problem%2520of%25203D%2520CAD%2520reverse%2520engineering%2520consists%2520of%2520reconstructing%2520the%2520sketch%250Aand%2520CAD%2520operation%2520sequences%2520from%25203D%2520representations%2520such%2520as%2520point%2520clouds.%2520In%250Athis%2520paper%252C%2520we%2520address%2520this%2520challenge%2520through%2520novel%2520contributions%2520across%2520three%250Alevels%253A%2520CAD%2520sequence%2520representation%252C%2520network%2520design%252C%2520and%2520dataset.%2520In%250Aparticular%252C%2520we%2520represent%2520CAD%2520sketch-extrude%2520sequences%2520as%2520Python%2520code.%2520The%250Aproposed%2520CAD-Recode%2520translates%2520a%2520point%2520cloud%2520into%2520Python%2520code%2520that%252C%2520when%250Aexecuted%252C%2520reconstructs%2520the%2520CAD%2520model.%2520Taking%2520advantage%2520of%2520the%2520exposure%2520of%250Apre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520Python%2520code%252C%2520we%2520leverage%2520a%250Arelatively%2520small%2520LLM%2520as%2520a%2520decoder%2520for%2520CAD-Recode%2520and%2520combine%2520it%2520with%2520a%250Alightweight%2520point%2520cloud%2520projector.%2520CAD-Recode%2520is%2520trained%2520solely%2520on%2520a%2520proposed%250Asynthetic%2520dataset%2520of%2520one%2520million%2520diverse%2520CAD%2520sequences.%2520CAD-Recode%250Asignificantly%2520outperforms%2520existing%2520methods%2520across%2520three%2520datasets%2520while%250Arequiring%2520fewer%2520input%2520points.%2520Notably%252C%2520it%2520achieves%252010%2520times%2520lower%2520mean%2520Chamfer%250Adistance%2520than%2520state-of-the-art%2520methods%2520on%2520DeepCAD%2520and%2520Fusion360%2520datasets.%250AFurthermore%252C%2520we%2520show%2520that%2520our%2520CAD%2520Python%2520code%2520output%2520is%2520interpretable%2520by%250Aoff-the-shelf%2520LLMs%252C%2520enabling%2520CAD%2520editing%2520and%2520CAD-specific%2520question%2520answering%250Afrom%2520point%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Recode%3A%20Reverse%20Engineering%20CAD%20Code%20from%20Point%20Clouds&entry.906535625=Danila%20Rukhovich%20and%20Elona%20Dupont%20and%20Dimitrios%20Mallis%20and%20Kseniya%20Cherenkova%20and%20Anis%20Kacem%20and%20Djamila%20Aouada&entry.1292438233=%20%20Computer-Aided%20Design%20%28CAD%29%20models%20are%20typically%20constructed%20by%20sequentially%0Adrawing%20parametric%20sketches%20and%20applying%20CAD%20operations%20to%20obtain%20a%203D%20model.%0AThe%20problem%20of%203D%20CAD%20reverse%20engineering%20consists%20of%20reconstructing%20the%20sketch%0Aand%20CAD%20operation%20sequences%20from%203D%20representations%20such%20as%20point%20clouds.%20In%0Athis%20paper%2C%20we%20address%20this%20challenge%20through%20novel%20contributions%20across%20three%0Alevels%3A%20CAD%20sequence%20representation%2C%20network%20design%2C%20and%20dataset.%20In%0Aparticular%2C%20we%20represent%20CAD%20sketch-extrude%20sequences%20as%20Python%20code.%20The%0Aproposed%20CAD-Recode%20translates%20a%20point%20cloud%20into%20Python%20code%20that%2C%20when%0Aexecuted%2C%20reconstructs%20the%20CAD%20model.%20Taking%20advantage%20of%20the%20exposure%20of%0Apre-trained%20Large%20Language%20Models%20%28LLMs%29%20to%20Python%20code%2C%20we%20leverage%20a%0Arelatively%20small%20LLM%20as%20a%20decoder%20for%20CAD-Recode%20and%20combine%20it%20with%20a%0Alightweight%20point%20cloud%20projector.%20CAD-Recode%20is%20trained%20solely%20on%20a%20proposed%0Asynthetic%20dataset%20of%20one%20million%20diverse%20CAD%20sequences.%20CAD-Recode%0Asignificantly%20outperforms%20existing%20methods%20across%20three%20datasets%20while%0Arequiring%20fewer%20input%20points.%20Notably%2C%20it%20achieves%2010%20times%20lower%20mean%20Chamfer%0Adistance%20than%20state-of-the-art%20methods%20on%20DeepCAD%20and%20Fusion360%20datasets.%0AFurthermore%2C%20we%20show%20that%20our%20CAD%20Python%20code%20output%20is%20interpretable%20by%0Aoff-the-shelf%20LLMs%2C%20enabling%20CAD%20editing%20and%20CAD-specific%20question%20answering%0Afrom%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14042v1&entry.124074799=Read"},
{"title": "ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models", "author": "Mingrui Wu and Xinyue Cai and Jiayi Ji and Jiale Li and Oucheng Huang and Gen Luo and Hao Fei and Guannan Jiang and Xiaoshuai Sun and Rongrong Ji", "abstract": "  In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.\n", "link": "http://arxiv.org/abs/2407.21534v4", "date": "2024-12-18", "relevancy": 2.7405, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5502}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlMLLM%3A%20Training-Free%20Visual%20Prompt%20Learning%20for%20Multimodal%20Large%0A%20%20Language%20Models&body=Title%3A%20ControlMLLM%3A%20Training-Free%20Visual%20Prompt%20Learning%20for%20Multimodal%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Mingrui%20Wu%20and%20Xinyue%20Cai%20and%20Jiayi%20Ji%20and%20Jiale%20Li%20and%20Oucheng%20Huang%20and%20Gen%20Luo%20and%20Hao%20Fei%20and%20Guannan%20Jiang%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20training-free%20method%20to%20inject%20visual%20referring%0Ainto%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20through%20learnable%20visual%20token%0Aoptimization.%20We%20observe%20the%20relationship%20between%20text%20prompt%20tokens%20and%20visual%0Atokens%20in%20MLLMs%2C%20where%20attention%20layers%20model%20the%20connection%20between%20them.%20Our%0Aapproach%20involves%20adjusting%20visual%20tokens%20from%20the%20MLP%20output%20during%20inference%2C%0Acontrolling%20which%20text%20prompt%20tokens%20attend%20to%20which%20visual%20tokens.%20We%20optimize%0Aa%20learnable%20visual%20token%20based%20on%20an%20energy%20function%2C%20enhancing%20the%20strength%20of%0Areferential%20regions%20in%20the%20attention%20map.%20This%20enables%20detailed%20region%0Adescription%20and%20reasoning%20without%20the%20need%20for%20substantial%20training%20costs%20or%0Amodel%20retraining.%20Our%20method%20offers%20a%20promising%20direction%20for%20integrating%0Areferential%20abilities%20into%20MLLMs.%20Our%20method%20support%20referring%20with%20box%2C%20mask%2C%0Ascribble%20and%20point.%20The%20results%20demonstrate%20that%20our%20method%20exhibits%0Acontrollability%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21534v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlMLLM%253A%2520Training-Free%2520Visual%2520Prompt%2520Learning%2520for%2520Multimodal%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMingrui%2520Wu%2520and%2520Xinyue%2520Cai%2520and%2520Jiayi%2520Ji%2520and%2520Jiale%2520Li%2520and%2520Oucheng%2520Huang%2520and%2520Gen%2520Luo%2520and%2520Hao%2520Fei%2520and%2520Guannan%2520Jiang%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520training-free%2520method%2520to%2520inject%2520visual%2520referring%250Ainto%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520through%2520learnable%2520visual%2520token%250Aoptimization.%2520We%2520observe%2520the%2520relationship%2520between%2520text%2520prompt%2520tokens%2520and%2520visual%250Atokens%2520in%2520MLLMs%252C%2520where%2520attention%2520layers%2520model%2520the%2520connection%2520between%2520them.%2520Our%250Aapproach%2520involves%2520adjusting%2520visual%2520tokens%2520from%2520the%2520MLP%2520output%2520during%2520inference%252C%250Acontrolling%2520which%2520text%2520prompt%2520tokens%2520attend%2520to%2520which%2520visual%2520tokens.%2520We%2520optimize%250Aa%2520learnable%2520visual%2520token%2520based%2520on%2520an%2520energy%2520function%252C%2520enhancing%2520the%2520strength%2520of%250Areferential%2520regions%2520in%2520the%2520attention%2520map.%2520This%2520enables%2520detailed%2520region%250Adescription%2520and%2520reasoning%2520without%2520the%2520need%2520for%2520substantial%2520training%2520costs%2520or%250Amodel%2520retraining.%2520Our%2520method%2520offers%2520a%2520promising%2520direction%2520for%2520integrating%250Areferential%2520abilities%2520into%2520MLLMs.%2520Our%2520method%2520support%2520referring%2520with%2520box%252C%2520mask%252C%250Ascribble%2520and%2520point.%2520The%2520results%2520demonstrate%2520that%2520our%2520method%2520exhibits%250Acontrollability%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21534v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlMLLM%3A%20Training-Free%20Visual%20Prompt%20Learning%20for%20Multimodal%20Large%0A%20%20Language%20Models&entry.906535625=Mingrui%20Wu%20and%20Xinyue%20Cai%20and%20Jiayi%20Ji%20and%20Jiale%20Li%20and%20Oucheng%20Huang%20and%20Gen%20Luo%20and%20Hao%20Fei%20and%20Guannan%20Jiang%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20training-free%20method%20to%20inject%20visual%20referring%0Ainto%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20through%20learnable%20visual%20token%0Aoptimization.%20We%20observe%20the%20relationship%20between%20text%20prompt%20tokens%20and%20visual%0Atokens%20in%20MLLMs%2C%20where%20attention%20layers%20model%20the%20connection%20between%20them.%20Our%0Aapproach%20involves%20adjusting%20visual%20tokens%20from%20the%20MLP%20output%20during%20inference%2C%0Acontrolling%20which%20text%20prompt%20tokens%20attend%20to%20which%20visual%20tokens.%20We%20optimize%0Aa%20learnable%20visual%20token%20based%20on%20an%20energy%20function%2C%20enhancing%20the%20strength%20of%0Areferential%20regions%20in%20the%20attention%20map.%20This%20enables%20detailed%20region%0Adescription%20and%20reasoning%20without%20the%20need%20for%20substantial%20training%20costs%20or%0Amodel%20retraining.%20Our%20method%20offers%20a%20promising%20direction%20for%20integrating%0Areferential%20abilities%20into%20MLLMs.%20Our%20method%20support%20referring%20with%20box%2C%20mask%2C%0Ascribble%20and%20point.%20The%20results%20demonstrate%20that%20our%20method%20exhibits%0Acontrollability%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21534v4&entry.124074799=Read"},
{"title": "Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization\n  Perspective", "author": "Semih Cayci", "abstract": "  We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nfactor yields robustness to ill-conditioned kernels, analogous to the\nunderparameterized regime. These findings demonstrate the potential of\nGauss-Newton methods for efficiently optimizing neural networks, particularly\nin ill-conditioned problems where kernel and Gram matrices have small singular\nvalues.\n", "link": "http://arxiv.org/abs/2412.14031v1", "date": "2024-12-18", "relevancy": 2.7126, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.566}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5544}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gauss-Newton%20Dynamics%20for%20Neural%20Networks%3A%20A%20Riemannian%20Optimization%0A%20%20Perspective&body=Title%3A%20Gauss-Newton%20Dynamics%20for%20Neural%20Networks%3A%20A%20Riemannian%20Optimization%0A%20%20Perspective%0AAuthor%3A%20Semih%20Cayci%0AAbstract%3A%20%20%20We%20analyze%20the%20convergence%20of%20Gauss-Newton%20dynamics%20for%20training%20neural%0Anetworks%20with%20smooth%20activation%20functions.%20In%20the%20underparameterized%20regime%2C%0Athe%20Gauss-Newton%20gradient%20flow%20induces%20a%20Riemannian%20gradient%20flow%20on%20a%0Alow-dimensional%2C%20smooth%2C%20embedded%20submanifold%20of%20the%20Euclidean%20output%20space.%0AUsing%20tools%20from%20Riemannian%20optimization%2C%20we%20prove%20%5Cemph%7Blast-iterate%7D%0Aconvergence%20of%20the%20Riemannian%20gradient%20flow%20to%20the%20optimal%20in-class%20predictor%0Aat%20an%20%5Cemph%7Bexponential%20rate%7D%20that%20is%20independent%20of%20the%20conditioning%20of%20the%0AGram%20matrix%2C%20%5Cemph%7Bwithout%7D%20requiring%20explicit%20regularization.%20We%20further%0Acharacterize%20the%20critical%20impacts%20of%20the%20neural%20network%20scaling%20factor%20and%20the%0Ainitialization%20on%20the%20convergence%20behavior.%20In%20the%20overparameterized%20regime%2C%20we%0Ashow%20that%20the%20Levenberg-Marquardt%20dynamics%20with%20an%20appropriately%20chosen%20damping%0Afactor%20yields%20robustness%20to%20ill-conditioned%20kernels%2C%20analogous%20to%20the%0Aunderparameterized%20regime.%20These%20findings%20demonstrate%20the%20potential%20of%0AGauss-Newton%20methods%20for%20efficiently%20optimizing%20neural%20networks%2C%20particularly%0Ain%20ill-conditioned%20problems%20where%20kernel%20and%20Gram%20matrices%20have%20small%20singular%0Avalues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGauss-Newton%2520Dynamics%2520for%2520Neural%2520Networks%253A%2520A%2520Riemannian%2520Optimization%250A%2520%2520Perspective%26entry.906535625%3DSemih%2520Cayci%26entry.1292438233%3D%2520%2520We%2520analyze%2520the%2520convergence%2520of%2520Gauss-Newton%2520dynamics%2520for%2520training%2520neural%250Anetworks%2520with%2520smooth%2520activation%2520functions.%2520In%2520the%2520underparameterized%2520regime%252C%250Athe%2520Gauss-Newton%2520gradient%2520flow%2520induces%2520a%2520Riemannian%2520gradient%2520flow%2520on%2520a%250Alow-dimensional%252C%2520smooth%252C%2520embedded%2520submanifold%2520of%2520the%2520Euclidean%2520output%2520space.%250AUsing%2520tools%2520from%2520Riemannian%2520optimization%252C%2520we%2520prove%2520%255Cemph%257Blast-iterate%257D%250Aconvergence%2520of%2520the%2520Riemannian%2520gradient%2520flow%2520to%2520the%2520optimal%2520in-class%2520predictor%250Aat%2520an%2520%255Cemph%257Bexponential%2520rate%257D%2520that%2520is%2520independent%2520of%2520the%2520conditioning%2520of%2520the%250AGram%2520matrix%252C%2520%255Cemph%257Bwithout%257D%2520requiring%2520explicit%2520regularization.%2520We%2520further%250Acharacterize%2520the%2520critical%2520impacts%2520of%2520the%2520neural%2520network%2520scaling%2520factor%2520and%2520the%250Ainitialization%2520on%2520the%2520convergence%2520behavior.%2520In%2520the%2520overparameterized%2520regime%252C%2520we%250Ashow%2520that%2520the%2520Levenberg-Marquardt%2520dynamics%2520with%2520an%2520appropriately%2520chosen%2520damping%250Afactor%2520yields%2520robustness%2520to%2520ill-conditioned%2520kernels%252C%2520analogous%2520to%2520the%250Aunderparameterized%2520regime.%2520These%2520findings%2520demonstrate%2520the%2520potential%2520of%250AGauss-Newton%2520methods%2520for%2520efficiently%2520optimizing%2520neural%2520networks%252C%2520particularly%250Ain%2520ill-conditioned%2520problems%2520where%2520kernel%2520and%2520Gram%2520matrices%2520have%2520small%2520singular%250Avalues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gauss-Newton%20Dynamics%20for%20Neural%20Networks%3A%20A%20Riemannian%20Optimization%0A%20%20Perspective&entry.906535625=Semih%20Cayci&entry.1292438233=%20%20We%20analyze%20the%20convergence%20of%20Gauss-Newton%20dynamics%20for%20training%20neural%0Anetworks%20with%20smooth%20activation%20functions.%20In%20the%20underparameterized%20regime%2C%0Athe%20Gauss-Newton%20gradient%20flow%20induces%20a%20Riemannian%20gradient%20flow%20on%20a%0Alow-dimensional%2C%20smooth%2C%20embedded%20submanifold%20of%20the%20Euclidean%20output%20space.%0AUsing%20tools%20from%20Riemannian%20optimization%2C%20we%20prove%20%5Cemph%7Blast-iterate%7D%0Aconvergence%20of%20the%20Riemannian%20gradient%20flow%20to%20the%20optimal%20in-class%20predictor%0Aat%20an%20%5Cemph%7Bexponential%20rate%7D%20that%20is%20independent%20of%20the%20conditioning%20of%20the%0AGram%20matrix%2C%20%5Cemph%7Bwithout%7D%20requiring%20explicit%20regularization.%20We%20further%0Acharacterize%20the%20critical%20impacts%20of%20the%20neural%20network%20scaling%20factor%20and%20the%0Ainitialization%20on%20the%20convergence%20behavior.%20In%20the%20overparameterized%20regime%2C%20we%0Ashow%20that%20the%20Levenberg-Marquardt%20dynamics%20with%20an%20appropriately%20chosen%20damping%0Afactor%20yields%20robustness%20to%20ill-conditioned%20kernels%2C%20analogous%20to%20the%0Aunderparameterized%20regime.%20These%20findings%20demonstrate%20the%20potential%20of%0AGauss-Newton%20methods%20for%20efficiently%20optimizing%20neural%20networks%2C%20particularly%0Ain%20ill-conditioned%20problems%20where%20kernel%20and%20Gram%20matrices%20have%20small%20singular%0Avalues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14031v1&entry.124074799=Read"},
{"title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning", "author": "Julia Witte Zimmerman and Denis Hudon and Kathryn Cramer and Alejandro J. Ruiz and Calla Beauregard and Ashley Fehr and Mikaela Irene Fudolig and Bradford Demarest and Yoshi Meke Bird and Milo Z. Trujillo and Christopher M. Danforth and Peter Sheridan Dodds", "abstract": "  Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DM) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens motivates\nlinguistically-informed interventions in existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokenization pretraining can be a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being meaningfully\ninsulated from the main system intelligence.\n", "link": "http://arxiv.org/abs/2412.10924v2", "date": "2024-12-18", "relevancy": 2.711, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5747}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning&body=Title%3A%20Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning%0AAuthor%3A%20Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Alejandro%20J.%20Ruiz%20and%20Calla%20Beauregard%20and%20Ashley%20Fehr%20and%20Mikaela%20Irene%20Fudolig%20and%20Bradford%20Demarest%20and%20Yoshi%20Meke%20Bird%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds%0AAbstract%3A%20%20%20Tokenization%20is%20a%20necessary%20component%20within%20the%20current%20architecture%20of%20many%0Alanguage%20models%2C%20including%20the%20transformer-based%20large%20language%20models%20%28LLMs%29%0Aof%20Generative%20AI%2C%20yet%20its%20impact%20on%20the%20model%27s%20cognition%20is%20often%20overlooked.%0AWe%20argue%20that%20LLMs%20demonstrate%20that%20the%20Distributional%20Hypothesis%20%28DM%29%20is%0Asufficient%20for%20reasonably%20human-like%20language%20performance%2C%20and%20that%20the%0Aemergence%20of%20human-meaningful%20linguistic%20units%20among%20tokens%20motivates%0Alinguistically-informed%20interventions%20in%20existing%2C%20linguistically-agnostic%0Atokenization%20techniques%2C%20particularly%20with%20respect%20to%20their%20roles%20as%20%281%29%0Asemantic%20primitives%20and%20as%20%282%29%20vehicles%20for%20conveying%20salient%20distributional%0Apatterns%20from%20human%20language%20to%20the%20model.%20We%20explore%20tokenizations%20from%20a%20BPE%0Atokenizer%3B%20extant%20model%20vocabularies%20obtained%20from%20Hugging%20Face%20and%20tiktoken%3B%0Aand%20the%20information%20in%20exemplar%20token%20vectors%20as%20they%20move%20through%20the%20layers%0Aof%20a%20RoBERTa%20%28large%29%20model.%20Besides%20creating%20sub-optimal%20semantic%20building%0Ablocks%20and%20obscuring%20the%20model%27s%20access%20to%20the%20necessary%20distributional%0Apatterns%2C%20we%20describe%20how%20tokenization%20pretraining%20can%20be%20a%20backdoor%20for%20bias%0Aand%20other%20unwanted%20content%2C%20which%20current%20alignment%20practices%20may%20not%0Aremediate.%20Additionally%2C%20we%20relay%20evidence%20that%20the%20tokenization%20algorithm%27s%0Aobjective%20function%20impacts%20the%20LLM%27s%20cognition%2C%20despite%20being%20meaningfully%0Ainsulated%20from%20the%20main%20system%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokens%252C%2520the%2520oft-overlooked%2520appetizer%253A%2520Large%2520language%2520models%252C%2520the%250A%2520%2520distributional%2520hypothesis%252C%2520and%2520meaning%26entry.906535625%3DJulia%2520Witte%2520Zimmerman%2520and%2520Denis%2520Hudon%2520and%2520Kathryn%2520Cramer%2520and%2520Alejandro%2520J.%2520Ruiz%2520and%2520Calla%2520Beauregard%2520and%2520Ashley%2520Fehr%2520and%2520Mikaela%2520Irene%2520Fudolig%2520and%2520Bradford%2520Demarest%2520and%2520Yoshi%2520Meke%2520Bird%2520and%2520Milo%2520Z.%2520Trujillo%2520and%2520Christopher%2520M.%2520Danforth%2520and%2520Peter%2520Sheridan%2520Dodds%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520a%2520necessary%2520component%2520within%2520the%2520current%2520architecture%2520of%2520many%250Alanguage%2520models%252C%2520including%2520the%2520transformer-based%2520large%2520language%2520models%2520%2528LLMs%2529%250Aof%2520Generative%2520AI%252C%2520yet%2520its%2520impact%2520on%2520the%2520model%2527s%2520cognition%2520is%2520often%2520overlooked.%250AWe%2520argue%2520that%2520LLMs%2520demonstrate%2520that%2520the%2520Distributional%2520Hypothesis%2520%2528DM%2529%2520is%250Asufficient%2520for%2520reasonably%2520human-like%2520language%2520performance%252C%2520and%2520that%2520the%250Aemergence%2520of%2520human-meaningful%2520linguistic%2520units%2520among%2520tokens%2520motivates%250Alinguistically-informed%2520interventions%2520in%2520existing%252C%2520linguistically-agnostic%250Atokenization%2520techniques%252C%2520particularly%2520with%2520respect%2520to%2520their%2520roles%2520as%2520%25281%2529%250Asemantic%2520primitives%2520and%2520as%2520%25282%2529%2520vehicles%2520for%2520conveying%2520salient%2520distributional%250Apatterns%2520from%2520human%2520language%2520to%2520the%2520model.%2520We%2520explore%2520tokenizations%2520from%2520a%2520BPE%250Atokenizer%253B%2520extant%2520model%2520vocabularies%2520obtained%2520from%2520Hugging%2520Face%2520and%2520tiktoken%253B%250Aand%2520the%2520information%2520in%2520exemplar%2520token%2520vectors%2520as%2520they%2520move%2520through%2520the%2520layers%250Aof%2520a%2520RoBERTa%2520%2528large%2529%2520model.%2520Besides%2520creating%2520sub-optimal%2520semantic%2520building%250Ablocks%2520and%2520obscuring%2520the%2520model%2527s%2520access%2520to%2520the%2520necessary%2520distributional%250Apatterns%252C%2520we%2520describe%2520how%2520tokenization%2520pretraining%2520can%2520be%2520a%2520backdoor%2520for%2520bias%250Aand%2520other%2520unwanted%2520content%252C%2520which%2520current%2520alignment%2520practices%2520may%2520not%250Aremediate.%2520Additionally%252C%2520we%2520relay%2520evidence%2520that%2520the%2520tokenization%2520algorithm%2527s%250Aobjective%2520function%2520impacts%2520the%2520LLM%2527s%2520cognition%252C%2520despite%2520being%2520meaningfully%250Ainsulated%2520from%2520the%2520main%2520system%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokens%2C%20the%20oft-overlooked%20appetizer%3A%20Large%20language%20models%2C%20the%0A%20%20distributional%20hypothesis%2C%20and%20meaning&entry.906535625=Julia%20Witte%20Zimmerman%20and%20Denis%20Hudon%20and%20Kathryn%20Cramer%20and%20Alejandro%20J.%20Ruiz%20and%20Calla%20Beauregard%20and%20Ashley%20Fehr%20and%20Mikaela%20Irene%20Fudolig%20and%20Bradford%20Demarest%20and%20Yoshi%20Meke%20Bird%20and%20Milo%20Z.%20Trujillo%20and%20Christopher%20M.%20Danforth%20and%20Peter%20Sheridan%20Dodds&entry.1292438233=%20%20Tokenization%20is%20a%20necessary%20component%20within%20the%20current%20architecture%20of%20many%0Alanguage%20models%2C%20including%20the%20transformer-based%20large%20language%20models%20%28LLMs%29%0Aof%20Generative%20AI%2C%20yet%20its%20impact%20on%20the%20model%27s%20cognition%20is%20often%20overlooked.%0AWe%20argue%20that%20LLMs%20demonstrate%20that%20the%20Distributional%20Hypothesis%20%28DM%29%20is%0Asufficient%20for%20reasonably%20human-like%20language%20performance%2C%20and%20that%20the%0Aemergence%20of%20human-meaningful%20linguistic%20units%20among%20tokens%20motivates%0Alinguistically-informed%20interventions%20in%20existing%2C%20linguistically-agnostic%0Atokenization%20techniques%2C%20particularly%20with%20respect%20to%20their%20roles%20as%20%281%29%0Asemantic%20primitives%20and%20as%20%282%29%20vehicles%20for%20conveying%20salient%20distributional%0Apatterns%20from%20human%20language%20to%20the%20model.%20We%20explore%20tokenizations%20from%20a%20BPE%0Atokenizer%3B%20extant%20model%20vocabularies%20obtained%20from%20Hugging%20Face%20and%20tiktoken%3B%0Aand%20the%20information%20in%20exemplar%20token%20vectors%20as%20they%20move%20through%20the%20layers%0Aof%20a%20RoBERTa%20%28large%29%20model.%20Besides%20creating%20sub-optimal%20semantic%20building%0Ablocks%20and%20obscuring%20the%20model%27s%20access%20to%20the%20necessary%20distributional%0Apatterns%2C%20we%20describe%20how%20tokenization%20pretraining%20can%20be%20a%20backdoor%20for%20bias%0Aand%20other%20unwanted%20content%2C%20which%20current%20alignment%20practices%20may%20not%0Aremediate.%20Additionally%2C%20we%20relay%20evidence%20that%20the%20tokenization%20algorithm%27s%0Aobjective%20function%20impacts%20the%20LLM%27s%20cognition%2C%20despite%20being%20meaningfully%0Ainsulated%20from%20the%20main%20system%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10924v2&entry.124074799=Read"},
{"title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models", "author": "Anna Scius-Bertrand and Michael Jungo and Lars V\u00f6gtlin and Jean-Marc Spat and Andreas Fischer", "abstract": "  Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.\n", "link": "http://arxiv.org/abs/2412.13859v1", "date": "2024-12-18", "relevancy": 2.7099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5472}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Prompting%20and%20Few-Shot%20Fine-Tuning%3A%20Revisiting%20Document%20Image%0A%20%20Classification%20Using%20Large%20Language%20Models&body=Title%3A%20Zero-Shot%20Prompting%20and%20Few-Shot%20Fine-Tuning%3A%20Revisiting%20Document%20Image%0A%20%20Classification%20Using%20Large%20Language%20Models%0AAuthor%3A%20Anna%20Scius-Bertrand%20and%20Michael%20Jungo%20and%20Lars%20V%C3%B6gtlin%20and%20Jean-Marc%20Spat%20and%20Andreas%20Fischer%0AAbstract%3A%20%20%20Classifying%20scanned%20documents%20is%20a%20challenging%20problem%20that%20involves%20image%2C%0Alayout%2C%20and%20text%20analysis%20for%20document%20understanding.%20Nevertheless%2C%20for%20certain%0Abenchmark%20datasets%2C%20notably%20RVL-CDIP%2C%20the%20state%20of%20the%20art%20is%20closing%20in%20to%0Anear-perfect%20performance%20when%20considering%20hundreds%20of%20thousands%20of%20training%0Asamples.%20With%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%2C%20which%20are%20excellent%0Afew-shot%20learners%2C%20the%20question%20arises%20to%20what%20extent%20the%20document%0Aclassification%20problem%20can%20be%20addressed%20with%20only%20a%20few%20training%20samples%2C%20or%0Aeven%20none%20at%20all.%20In%20this%20paper%2C%20we%20investigate%20this%20question%20in%20the%20context%20of%0Azero-shot%20prompting%20and%20few-shot%20model%20fine-tuning%2C%20with%20the%20aim%20of%20reducing%0Athe%20need%20for%20human-annotated%20training%20samples%20as%20much%20as%20possible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Prompting%2520and%2520Few-Shot%2520Fine-Tuning%253A%2520Revisiting%2520Document%2520Image%250A%2520%2520Classification%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DAnna%2520Scius-Bertrand%2520and%2520Michael%2520Jungo%2520and%2520Lars%2520V%25C3%25B6gtlin%2520and%2520Jean-Marc%2520Spat%2520and%2520Andreas%2520Fischer%26entry.1292438233%3D%2520%2520Classifying%2520scanned%2520documents%2520is%2520a%2520challenging%2520problem%2520that%2520involves%2520image%252C%250Alayout%252C%2520and%2520text%2520analysis%2520for%2520document%2520understanding.%2520Nevertheless%252C%2520for%2520certain%250Abenchmark%2520datasets%252C%2520notably%2520RVL-CDIP%252C%2520the%2520state%2520of%2520the%2520art%2520is%2520closing%2520in%2520to%250Anear-perfect%2520performance%2520when%2520considering%2520hundreds%2520of%2520thousands%2520of%2520training%250Asamples.%2520With%2520the%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520are%2520excellent%250Afew-shot%2520learners%252C%2520the%2520question%2520arises%2520to%2520what%2520extent%2520the%2520document%250Aclassification%2520problem%2520can%2520be%2520addressed%2520with%2520only%2520a%2520few%2520training%2520samples%252C%2520or%250Aeven%2520none%2520at%2520all.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520this%2520question%2520in%2520the%2520context%2520of%250Azero-shot%2520prompting%2520and%2520few-shot%2520model%2520fine-tuning%252C%2520with%2520the%2520aim%2520of%2520reducing%250Athe%2520need%2520for%2520human-annotated%2520training%2520samples%2520as%2520much%2520as%2520possible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Prompting%20and%20Few-Shot%20Fine-Tuning%3A%20Revisiting%20Document%20Image%0A%20%20Classification%20Using%20Large%20Language%20Models&entry.906535625=Anna%20Scius-Bertrand%20and%20Michael%20Jungo%20and%20Lars%20V%C3%B6gtlin%20and%20Jean-Marc%20Spat%20and%20Andreas%20Fischer&entry.1292438233=%20%20Classifying%20scanned%20documents%20is%20a%20challenging%20problem%20that%20involves%20image%2C%0Alayout%2C%20and%20text%20analysis%20for%20document%20understanding.%20Nevertheless%2C%20for%20certain%0Abenchmark%20datasets%2C%20notably%20RVL-CDIP%2C%20the%20state%20of%20the%20art%20is%20closing%20in%20to%0Anear-perfect%20performance%20when%20considering%20hundreds%20of%20thousands%20of%20training%0Asamples.%20With%20the%20advent%20of%20large%20language%20models%20%28LLMs%29%2C%20which%20are%20excellent%0Afew-shot%20learners%2C%20the%20question%20arises%20to%20what%20extent%20the%20document%0Aclassification%20problem%20can%20be%20addressed%20with%20only%20a%20few%20training%20samples%2C%20or%0Aeven%20none%20at%20all.%20In%20this%20paper%2C%20we%20investigate%20this%20question%20in%20the%20context%20of%0Azero-shot%20prompting%20and%20few-shot%20model%20fine-tuning%2C%20with%20the%20aim%20of%20reducing%0Athe%20need%20for%20human-annotated%20training%20samples%20as%20much%20as%20possible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13859v1&entry.124074799=Read"},
{"title": "3D Registration in 30 Years: A Survey", "author": "Jiaqi Yang and Chu'ai Zhang and Zhengbao Wang and Xinyue Cao and Xuan Ouyang and Xiyu Zhang and Zhenxuan Zeng and Zhao Zeng and Borui Lu and Zhiyi Xia and Qian Zhang and Yulan Guo and Yanning Zhang", "abstract": "  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n", "link": "http://arxiv.org/abs/2412.13735v1", "date": "2024-12-18", "relevancy": 2.7034, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5421}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Registration%20in%2030%20Years%3A%20A%20Survey&body=Title%3A%203D%20Registration%20in%2030%20Years%3A%20A%20Survey%0AAuthor%3A%20Jiaqi%20Yang%20and%20Chu%27ai%20Zhang%20and%20Zhengbao%20Wang%20and%20Xinyue%20Cao%20and%20Xuan%20Ouyang%20and%20Xiyu%20Zhang%20and%20Zhenxuan%20Zeng%20and%20Zhao%20Zeng%20and%20Borui%20Lu%20and%20Zhiyi%20Xia%20and%20Qian%20Zhang%20and%20Yulan%20Guo%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%203D%20point%20cloud%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%2C%0Acomputer%20graphics%2C%20robotics%2C%20remote%20sensing%2C%20and%20etc.%20Over%20the%20last%20thirty%0Ayears%2C%20we%20have%20witnessed%20the%20amazing%20advancement%20in%20this%20area%20with%20numerous%0Akinds%20of%20solutions.%20Although%20a%20handful%20of%20relevant%20surveys%20have%20been%20conducted%2C%0Atheir%20coverage%20is%20still%20limited.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%0Asurvey%20on%203D%20point%20cloud%20registration%2C%20covering%20a%20set%20of%20sub-areas%20such%20as%0Apairwise%20coarse%20registration%2C%20pairwise%20fine%20registration%2C%20multi-view%0Aregistration%2C%20cross-scale%20registration%2C%20and%20multi-instance%20registration.%20The%0Adatasets%2C%20evaluation%20metrics%2C%20method%20taxonomy%2C%20discussions%20of%20the%20merits%20and%0Ademerits%2C%20insightful%20thoughts%20of%20future%20directions%20are%20comprehensively%0Apresented%20in%20this%20survey.%20The%20regularly%20updated%20project%20page%20of%20the%20survey%20is%0Aavailable%20at%20https%3A//github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Registration%2520in%252030%2520Years%253A%2520A%2520Survey%26entry.906535625%3DJiaqi%2520Yang%2520and%2520Chu%2527ai%2520Zhang%2520and%2520Zhengbao%2520Wang%2520and%2520Xinyue%2520Cao%2520and%2520Xuan%2520Ouyang%2520and%2520Xiyu%2520Zhang%2520and%2520Zhenxuan%2520Zeng%2520and%2520Zhao%2520Zeng%2520and%2520Borui%2520Lu%2520and%2520Zhiyi%2520Xia%2520and%2520Qian%2520Zhang%2520and%2520Yulan%2520Guo%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%25203D%2520point%2520cloud%2520registration%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%252C%250Acomputer%2520graphics%252C%2520robotics%252C%2520remote%2520sensing%252C%2520and%2520etc.%2520Over%2520the%2520last%2520thirty%250Ayears%252C%2520we%2520have%2520witnessed%2520the%2520amazing%2520advancement%2520in%2520this%2520area%2520with%2520numerous%250Akinds%2520of%2520solutions.%2520Although%2520a%2520handful%2520of%2520relevant%2520surveys%2520have%2520been%2520conducted%252C%250Atheir%2520coverage%2520is%2520still%2520limited.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520comprehensive%250Asurvey%2520on%25203D%2520point%2520cloud%2520registration%252C%2520covering%2520a%2520set%2520of%2520sub-areas%2520such%2520as%250Apairwise%2520coarse%2520registration%252C%2520pairwise%2520fine%2520registration%252C%2520multi-view%250Aregistration%252C%2520cross-scale%2520registration%252C%2520and%2520multi-instance%2520registration.%2520The%250Adatasets%252C%2520evaluation%2520metrics%252C%2520method%2520taxonomy%252C%2520discussions%2520of%2520the%2520merits%2520and%250Ademerits%252C%2520insightful%2520thoughts%2520of%2520future%2520directions%2520are%2520comprehensively%250Apresented%2520in%2520this%2520survey.%2520The%2520regularly%2520updated%2520project%2520page%2520of%2520the%2520survey%2520is%250Aavailable%2520at%2520https%253A//github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Registration%20in%2030%20Years%3A%20A%20Survey&entry.906535625=Jiaqi%20Yang%20and%20Chu%27ai%20Zhang%20and%20Zhengbao%20Wang%20and%20Xinyue%20Cao%20and%20Xuan%20Ouyang%20and%20Xiyu%20Zhang%20and%20Zhenxuan%20Zeng%20and%20Zhao%20Zeng%20and%20Borui%20Lu%20and%20Zhiyi%20Xia%20and%20Qian%20Zhang%20and%20Yulan%20Guo%20and%20Yanning%20Zhang&entry.1292438233=%20%203D%20point%20cloud%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%2C%0Acomputer%20graphics%2C%20robotics%2C%20remote%20sensing%2C%20and%20etc.%20Over%20the%20last%20thirty%0Ayears%2C%20we%20have%20witnessed%20the%20amazing%20advancement%20in%20this%20area%20with%20numerous%0Akinds%20of%20solutions.%20Although%20a%20handful%20of%20relevant%20surveys%20have%20been%20conducted%2C%0Atheir%20coverage%20is%20still%20limited.%20In%20this%20work%2C%20we%20present%20a%20comprehensive%0Asurvey%20on%203D%20point%20cloud%20registration%2C%20covering%20a%20set%20of%20sub-areas%20such%20as%0Apairwise%20coarse%20registration%2C%20pairwise%20fine%20registration%2C%20multi-view%0Aregistration%2C%20cross-scale%20registration%2C%20and%20multi-instance%20registration.%20The%0Adatasets%2C%20evaluation%20metrics%2C%20method%20taxonomy%2C%20discussions%20of%20the%20merits%20and%0Ademerits%2C%20insightful%20thoughts%20of%20future%20directions%20are%20comprehensively%0Apresented%20in%20this%20survey.%20The%20regularly%20updated%20project%20page%20of%20the%20survey%20is%0Aavailable%20at%20https%3A//github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13735v1&entry.124074799=Read"},
{"title": "Signal Reconstruction from Samples at Unknown Locations with Application\n  to 2D Unknown View Tomography", "author": "Sheel Shah and Kaishva Shah and Karthik S. Gurumoorthy and Ajit Rajwade", "abstract": "  It is well known that a band-limited signal can be reconstructed from its\nuniformly spaced samples if the sampling rate is sufficiently high. More\nrecently, it has been proved that one can reconstruct a 1D band-limited signal\neven if the exact sample locations are unknown, but given a uniform\ndistribution of the sample locations and their ordering in 1D. In this work, we\nextend the analytical error bounds in such scenarios for quasi-bandlimited\n(QBL) signals, and for the case of arbitrary but known sampling distributions.\nWe also prove that such reconstruction methods are resilient to a certain\nproportion of errors in the specification of the sample location ordering. We\nthen express the problem of tomographic reconstruction of 2D images from 1D\nRadon projections under unknown angles (2D UVT) with known angle distribution,\nas a special case for reconstruction of QBL signals from samples at unknown\nlocations with known distribution. Building upon our theoretical background, we\npresent asymptotic bounds for 2D QBL image reconstruction from 1D Radon\nprojections in the unknown angles setting, and present an extensive set of\nsimulations to verify these bounds in varied parameter regimes. To the best of\nour knowledge, this is the first piece of work to perform such an analysis for\n2D UVT and explicitly relate it to advances in sampling theory, even though the\nassociated reconstruction algorithms have been known for a long time.\n", "link": "http://arxiv.org/abs/2304.06376v2", "date": "2024-12-18", "relevancy": 2.7028, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5475}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5371}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Signal%20Reconstruction%20from%20Samples%20at%20Unknown%20Locations%20with%20Application%0A%20%20to%202D%20Unknown%20View%20Tomography&body=Title%3A%20Signal%20Reconstruction%20from%20Samples%20at%20Unknown%20Locations%20with%20Application%0A%20%20to%202D%20Unknown%20View%20Tomography%0AAuthor%3A%20Sheel%20Shah%20and%20Kaishva%20Shah%20and%20Karthik%20S.%20Gurumoorthy%20and%20Ajit%20Rajwade%0AAbstract%3A%20%20%20It%20is%20well%20known%20that%20a%20band-limited%20signal%20can%20be%20reconstructed%20from%20its%0Auniformly%20spaced%20samples%20if%20the%20sampling%20rate%20is%20sufficiently%20high.%20More%0Arecently%2C%20it%20has%20been%20proved%20that%20one%20can%20reconstruct%20a%201D%20band-limited%20signal%0Aeven%20if%20the%20exact%20sample%20locations%20are%20unknown%2C%20but%20given%20a%20uniform%0Adistribution%20of%20the%20sample%20locations%20and%20their%20ordering%20in%201D.%20In%20this%20work%2C%20we%0Aextend%20the%20analytical%20error%20bounds%20in%20such%20scenarios%20for%20quasi-bandlimited%0A%28QBL%29%20signals%2C%20and%20for%20the%20case%20of%20arbitrary%20but%20known%20sampling%20distributions.%0AWe%20also%20prove%20that%20such%20reconstruction%20methods%20are%20resilient%20to%20a%20certain%0Aproportion%20of%20errors%20in%20the%20specification%20of%20the%20sample%20location%20ordering.%20We%0Athen%20express%20the%20problem%20of%20tomographic%20reconstruction%20of%202D%20images%20from%201D%0ARadon%20projections%20under%20unknown%20angles%20%282D%20UVT%29%20with%20known%20angle%20distribution%2C%0Aas%20a%20special%20case%20for%20reconstruction%20of%20QBL%20signals%20from%20samples%20at%20unknown%0Alocations%20with%20known%20distribution.%20Building%20upon%20our%20theoretical%20background%2C%20we%0Apresent%20asymptotic%20bounds%20for%202D%20QBL%20image%20reconstruction%20from%201D%20Radon%0Aprojections%20in%20the%20unknown%20angles%20setting%2C%20and%20present%20an%20extensive%20set%20of%0Asimulations%20to%20verify%20these%20bounds%20in%20varied%20parameter%20regimes.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20piece%20of%20work%20to%20perform%20such%20an%20analysis%20for%0A2D%20UVT%20and%20explicitly%20relate%20it%20to%20advances%20in%20sampling%20theory%2C%20even%20though%20the%0Aassociated%20reconstruction%20algorithms%20have%20been%20known%20for%20a%20long%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignal%2520Reconstruction%2520from%2520Samples%2520at%2520Unknown%2520Locations%2520with%2520Application%250A%2520%2520to%25202D%2520Unknown%2520View%2520Tomography%26entry.906535625%3DSheel%2520Shah%2520and%2520Kaishva%2520Shah%2520and%2520Karthik%2520S.%2520Gurumoorthy%2520and%2520Ajit%2520Rajwade%26entry.1292438233%3D%2520%2520It%2520is%2520well%2520known%2520that%2520a%2520band-limited%2520signal%2520can%2520be%2520reconstructed%2520from%2520its%250Auniformly%2520spaced%2520samples%2520if%2520the%2520sampling%2520rate%2520is%2520sufficiently%2520high.%2520More%250Arecently%252C%2520it%2520has%2520been%2520proved%2520that%2520one%2520can%2520reconstruct%2520a%25201D%2520band-limited%2520signal%250Aeven%2520if%2520the%2520exact%2520sample%2520locations%2520are%2520unknown%252C%2520but%2520given%2520a%2520uniform%250Adistribution%2520of%2520the%2520sample%2520locations%2520and%2520their%2520ordering%2520in%25201D.%2520In%2520this%2520work%252C%2520we%250Aextend%2520the%2520analytical%2520error%2520bounds%2520in%2520such%2520scenarios%2520for%2520quasi-bandlimited%250A%2528QBL%2529%2520signals%252C%2520and%2520for%2520the%2520case%2520of%2520arbitrary%2520but%2520known%2520sampling%2520distributions.%250AWe%2520also%2520prove%2520that%2520such%2520reconstruction%2520methods%2520are%2520resilient%2520to%2520a%2520certain%250Aproportion%2520of%2520errors%2520in%2520the%2520specification%2520of%2520the%2520sample%2520location%2520ordering.%2520We%250Athen%2520express%2520the%2520problem%2520of%2520tomographic%2520reconstruction%2520of%25202D%2520images%2520from%25201D%250ARadon%2520projections%2520under%2520unknown%2520angles%2520%25282D%2520UVT%2529%2520with%2520known%2520angle%2520distribution%252C%250Aas%2520a%2520special%2520case%2520for%2520reconstruction%2520of%2520QBL%2520signals%2520from%2520samples%2520at%2520unknown%250Alocations%2520with%2520known%2520distribution.%2520Building%2520upon%2520our%2520theoretical%2520background%252C%2520we%250Apresent%2520asymptotic%2520bounds%2520for%25202D%2520QBL%2520image%2520reconstruction%2520from%25201D%2520Radon%250Aprojections%2520in%2520the%2520unknown%2520angles%2520setting%252C%2520and%2520present%2520an%2520extensive%2520set%2520of%250Asimulations%2520to%2520verify%2520these%2520bounds%2520in%2520varied%2520parameter%2520regimes.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520piece%2520of%2520work%2520to%2520perform%2520such%2520an%2520analysis%2520for%250A2D%2520UVT%2520and%2520explicitly%2520relate%2520it%2520to%2520advances%2520in%2520sampling%2520theory%252C%2520even%2520though%2520the%250Aassociated%2520reconstruction%2520algorithms%2520have%2520been%2520known%2520for%2520a%2520long%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Signal%20Reconstruction%20from%20Samples%20at%20Unknown%20Locations%20with%20Application%0A%20%20to%202D%20Unknown%20View%20Tomography&entry.906535625=Sheel%20Shah%20and%20Kaishva%20Shah%20and%20Karthik%20S.%20Gurumoorthy%20and%20Ajit%20Rajwade&entry.1292438233=%20%20It%20is%20well%20known%20that%20a%20band-limited%20signal%20can%20be%20reconstructed%20from%20its%0Auniformly%20spaced%20samples%20if%20the%20sampling%20rate%20is%20sufficiently%20high.%20More%0Arecently%2C%20it%20has%20been%20proved%20that%20one%20can%20reconstruct%20a%201D%20band-limited%20signal%0Aeven%20if%20the%20exact%20sample%20locations%20are%20unknown%2C%20but%20given%20a%20uniform%0Adistribution%20of%20the%20sample%20locations%20and%20their%20ordering%20in%201D.%20In%20this%20work%2C%20we%0Aextend%20the%20analytical%20error%20bounds%20in%20such%20scenarios%20for%20quasi-bandlimited%0A%28QBL%29%20signals%2C%20and%20for%20the%20case%20of%20arbitrary%20but%20known%20sampling%20distributions.%0AWe%20also%20prove%20that%20such%20reconstruction%20methods%20are%20resilient%20to%20a%20certain%0Aproportion%20of%20errors%20in%20the%20specification%20of%20the%20sample%20location%20ordering.%20We%0Athen%20express%20the%20problem%20of%20tomographic%20reconstruction%20of%202D%20images%20from%201D%0ARadon%20projections%20under%20unknown%20angles%20%282D%20UVT%29%20with%20known%20angle%20distribution%2C%0Aas%20a%20special%20case%20for%20reconstruction%20of%20QBL%20signals%20from%20samples%20at%20unknown%0Alocations%20with%20known%20distribution.%20Building%20upon%20our%20theoretical%20background%2C%20we%0Apresent%20asymptotic%20bounds%20for%202D%20QBL%20image%20reconstruction%20from%201D%20Radon%0Aprojections%20in%20the%20unknown%20angles%20setting%2C%20and%20present%20an%20extensive%20set%20of%0Asimulations%20to%20verify%20these%20bounds%20in%20varied%20parameter%20regimes.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20piece%20of%20work%20to%20perform%20such%20an%20analysis%20for%0A2D%20UVT%20and%20explicitly%20relate%20it%20to%20advances%20in%20sampling%20theory%2C%20even%20though%20the%0Aassociated%20reconstruction%20algorithms%20have%20been%20known%20for%20a%20long%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06376v2&entry.124074799=Read"},
{"title": "CREST: An Efficient Conjointly-trained Spike-driven Framework for\n  Event-based Object Detection Exploiting Spatiotemporal Dynamics", "author": "Ruixin Mao and Aoyu Shen and Lin Tang and Jun Zhou", "abstract": "  Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.\n", "link": "http://arxiv.org/abs/2412.12525v2", "date": "2024-12-18", "relevancy": 2.6591, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5582}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5208}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREST%3A%20An%20Efficient%20Conjointly-trained%20Spike-driven%20Framework%20for%0A%20%20Event-based%20Object%20Detection%20Exploiting%20Spatiotemporal%20Dynamics&body=Title%3A%20CREST%3A%20An%20Efficient%20Conjointly-trained%20Spike-driven%20Framework%20for%0A%20%20Event-based%20Object%20Detection%20Exploiting%20Spatiotemporal%20Dynamics%0AAuthor%3A%20Ruixin%20Mao%20and%20Aoyu%20Shen%20and%20Lin%20Tang%20and%20Jun%20Zhou%0AAbstract%3A%20%20%20Event-based%20cameras%20feature%20high%20temporal%20resolution%2C%20wide%20dynamic%20range%2C%20and%0Alow%20power%20consumption%2C%20which%20is%20ideal%20for%20high-speed%20and%20low-light%20object%0Adetection.%20Spiking%20neural%20networks%20%28SNNs%29%20are%20promising%20for%20event-based%20object%0Arecognition%20and%20detection%20due%20to%20their%20spiking%20nature%20but%20lack%20efficient%0Atraining%20methods%2C%20leading%20to%20gradient%20vanishing%20and%20high%20computational%0Acomplexity%2C%20especially%20in%20deep%20SNNs.%20Additionally%2C%20existing%20SNN%20frameworks%0Aoften%20fail%20to%20effectively%20handle%20multi-scale%20spatiotemporal%20features%2C%20leading%0Ato%20increased%20data%20redundancy%20and%20reduced%20accuracy.%20To%20address%20these%20issues%2C%20we%0Apropose%20CREST%2C%20a%20novel%20conjointly-trained%20spike-driven%20framework%20to%20exploit%0Aspatiotemporal%20dynamics%20in%20event-based%20object%20detection.%20We%20introduce%20the%0Aconjoint%20learning%20rule%20to%20accelerate%20SNN%20learning%20and%20alleviate%20gradient%0Avanishing.%20It%20also%20supports%20dual%20operation%20modes%20for%20efficient%20and%20flexible%0Aimplementation%20on%20different%20hardware%20types.%20Additionally%2C%20CREST%20features%20a%0Afully%20spike-driven%20framework%20with%20a%20multi-scale%20spatiotemporal%20event%20integrator%0A%28MESTOR%29%20and%20a%20spatiotemporal-IoU%20%28ST-IoU%29%20loss.%20Our%20approach%20achieves%20superior%0Aobject%20recognition%20%26%20detection%20performance%20and%20up%20to%20100X%20energy%20efficiency%0Acompared%20with%20state-of-the-art%20SNN%20algorithms%20on%20three%20datasets%2C%20providing%20an%0Aefficient%20solution%20for%20event-based%20object%20detection%20algorithms%20suitable%20for%20SNN%0Ahardware%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12525v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREST%253A%2520An%2520Efficient%2520Conjointly-trained%2520Spike-driven%2520Framework%2520for%250A%2520%2520Event-based%2520Object%2520Detection%2520Exploiting%2520Spatiotemporal%2520Dynamics%26entry.906535625%3DRuixin%2520Mao%2520and%2520Aoyu%2520Shen%2520and%2520Lin%2520Tang%2520and%2520Jun%2520Zhou%26entry.1292438233%3D%2520%2520Event-based%2520cameras%2520feature%2520high%2520temporal%2520resolution%252C%2520wide%2520dynamic%2520range%252C%2520and%250Alow%2520power%2520consumption%252C%2520which%2520is%2520ideal%2520for%2520high-speed%2520and%2520low-light%2520object%250Adetection.%2520Spiking%2520neural%2520networks%2520%2528SNNs%2529%2520are%2520promising%2520for%2520event-based%2520object%250Arecognition%2520and%2520detection%2520due%2520to%2520their%2520spiking%2520nature%2520but%2520lack%2520efficient%250Atraining%2520methods%252C%2520leading%2520to%2520gradient%2520vanishing%2520and%2520high%2520computational%250Acomplexity%252C%2520especially%2520in%2520deep%2520SNNs.%2520Additionally%252C%2520existing%2520SNN%2520frameworks%250Aoften%2520fail%2520to%2520effectively%2520handle%2520multi-scale%2520spatiotemporal%2520features%252C%2520leading%250Ato%2520increased%2520data%2520redundancy%2520and%2520reduced%2520accuracy.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520CREST%252C%2520a%2520novel%2520conjointly-trained%2520spike-driven%2520framework%2520to%2520exploit%250Aspatiotemporal%2520dynamics%2520in%2520event-based%2520object%2520detection.%2520We%2520introduce%2520the%250Aconjoint%2520learning%2520rule%2520to%2520accelerate%2520SNN%2520learning%2520and%2520alleviate%2520gradient%250Avanishing.%2520It%2520also%2520supports%2520dual%2520operation%2520modes%2520for%2520efficient%2520and%2520flexible%250Aimplementation%2520on%2520different%2520hardware%2520types.%2520Additionally%252C%2520CREST%2520features%2520a%250Afully%2520spike-driven%2520framework%2520with%2520a%2520multi-scale%2520spatiotemporal%2520event%2520integrator%250A%2528MESTOR%2529%2520and%2520a%2520spatiotemporal-IoU%2520%2528ST-IoU%2529%2520loss.%2520Our%2520approach%2520achieves%2520superior%250Aobject%2520recognition%2520%2526%2520detection%2520performance%2520and%2520up%2520to%2520100X%2520energy%2520efficiency%250Acompared%2520with%2520state-of-the-art%2520SNN%2520algorithms%2520on%2520three%2520datasets%252C%2520providing%2520an%250Aefficient%2520solution%2520for%2520event-based%2520object%2520detection%2520algorithms%2520suitable%2520for%2520SNN%250Ahardware%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12525v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREST%3A%20An%20Efficient%20Conjointly-trained%20Spike-driven%20Framework%20for%0A%20%20Event-based%20Object%20Detection%20Exploiting%20Spatiotemporal%20Dynamics&entry.906535625=Ruixin%20Mao%20and%20Aoyu%20Shen%20and%20Lin%20Tang%20and%20Jun%20Zhou&entry.1292438233=%20%20Event-based%20cameras%20feature%20high%20temporal%20resolution%2C%20wide%20dynamic%20range%2C%20and%0Alow%20power%20consumption%2C%20which%20is%20ideal%20for%20high-speed%20and%20low-light%20object%0Adetection.%20Spiking%20neural%20networks%20%28SNNs%29%20are%20promising%20for%20event-based%20object%0Arecognition%20and%20detection%20due%20to%20their%20spiking%20nature%20but%20lack%20efficient%0Atraining%20methods%2C%20leading%20to%20gradient%20vanishing%20and%20high%20computational%0Acomplexity%2C%20especially%20in%20deep%20SNNs.%20Additionally%2C%20existing%20SNN%20frameworks%0Aoften%20fail%20to%20effectively%20handle%20multi-scale%20spatiotemporal%20features%2C%20leading%0Ato%20increased%20data%20redundancy%20and%20reduced%20accuracy.%20To%20address%20these%20issues%2C%20we%0Apropose%20CREST%2C%20a%20novel%20conjointly-trained%20spike-driven%20framework%20to%20exploit%0Aspatiotemporal%20dynamics%20in%20event-based%20object%20detection.%20We%20introduce%20the%0Aconjoint%20learning%20rule%20to%20accelerate%20SNN%20learning%20and%20alleviate%20gradient%0Avanishing.%20It%20also%20supports%20dual%20operation%20modes%20for%20efficient%20and%20flexible%0Aimplementation%20on%20different%20hardware%20types.%20Additionally%2C%20CREST%20features%20a%0Afully%20spike-driven%20framework%20with%20a%20multi-scale%20spatiotemporal%20event%20integrator%0A%28MESTOR%29%20and%20a%20spatiotemporal-IoU%20%28ST-IoU%29%20loss.%20Our%20approach%20achieves%20superior%0Aobject%20recognition%20%26%20detection%20performance%20and%20up%20to%20100X%20energy%20efficiency%0Acompared%20with%20state-of-the-art%20SNN%20algorithms%20on%20three%20datasets%2C%20providing%20an%0Aefficient%20solution%20for%20event-based%20object%20detection%20algorithms%20suitable%20for%20SNN%0Ahardware%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12525v2&entry.124074799=Read"},
{"title": "Vocabulary Expansion of Chat Models with Unlabeled Target Language Data", "author": "Atsuki Yamaguchi and Terufumi Morishita and Aline Villavicencio and Nikolaos Aletras", "abstract": "  Chat models (i.e. language models trained to follow instructions through\nconversation with humans) outperform base models (i.e. trained solely on\nunlabeled data) in both conversation and general task-solving abilities. These\nmodels are generally English-centric and require further adaptation for\nlanguages that are underrepresented in or absent from their training data. A\ncommon technique for adapting base models is to extend the model's vocabulary\nwith target language tokens, i.e. vocabulary expansion (VE), and then\ncontinually pre-train it on language-specific data. Using chat data is ideal\nfor chat model adaptation, but often, either this does not exist or is costly\nto construct. Alternatively, adapting chat models with unlabeled data is a\npossible solution, but it could result in catastrophic forgetting. In this\npaper, we investigate the impact of using unlabeled target language data for VE\non chat models for the first time. We first show that off-the-shelf VE\ngenerally performs well across target language tasks and models in 71% of\ncases, though it underperforms in scenarios where source chat models are\nalready strong. To further improve adapted models, we propose post-hoc\ntechniques that inject information from the source model without requiring any\nfurther training. Experiments reveal the effectiveness of our methods, helping\nthe adapted models to achieve performance improvements in 87% of cases.\n", "link": "http://arxiv.org/abs/2412.11704v2", "date": "2024-12-18", "relevancy": 2.6564, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vocabulary%20Expansion%20of%20Chat%20Models%20with%20Unlabeled%20Target%20Language%20Data&body=Title%3A%20Vocabulary%20Expansion%20of%20Chat%20Models%20with%20Unlabeled%20Target%20Language%20Data%0AAuthor%3A%20Atsuki%20Yamaguchi%20and%20Terufumi%20Morishita%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras%0AAbstract%3A%20%20%20Chat%20models%20%28i.e.%20language%20models%20trained%20to%20follow%20instructions%20through%0Aconversation%20with%20humans%29%20outperform%20base%20models%20%28i.e.%20trained%20solely%20on%0Aunlabeled%20data%29%20in%20both%20conversation%20and%20general%20task-solving%20abilities.%20These%0Amodels%20are%20generally%20English-centric%20and%20require%20further%20adaptation%20for%0Alanguages%20that%20are%20underrepresented%20in%20or%20absent%20from%20their%20training%20data.%20A%0Acommon%20technique%20for%20adapting%20base%20models%20is%20to%20extend%20the%20model%27s%20vocabulary%0Awith%20target%20language%20tokens%2C%20i.e.%20vocabulary%20expansion%20%28VE%29%2C%20and%20then%0Acontinually%20pre-train%20it%20on%20language-specific%20data.%20Using%20chat%20data%20is%20ideal%0Afor%20chat%20model%20adaptation%2C%20but%20often%2C%20either%20this%20does%20not%20exist%20or%20is%20costly%0Ato%20construct.%20Alternatively%2C%20adapting%20chat%20models%20with%20unlabeled%20data%20is%20a%0Apossible%20solution%2C%20but%20it%20could%20result%20in%20catastrophic%20forgetting.%20In%20this%0Apaper%2C%20we%20investigate%20the%20impact%20of%20using%20unlabeled%20target%20language%20data%20for%20VE%0Aon%20chat%20models%20for%20the%20first%20time.%20We%20first%20show%20that%20off-the-shelf%20VE%0Agenerally%20performs%20well%20across%20target%20language%20tasks%20and%20models%20in%2071%25%20of%0Acases%2C%20though%20it%20underperforms%20in%20scenarios%20where%20source%20chat%20models%20are%0Aalready%20strong.%20To%20further%20improve%20adapted%20models%2C%20we%20propose%20post-hoc%0Atechniques%20that%20inject%20information%20from%20the%20source%20model%20without%20requiring%20any%0Afurther%20training.%20Experiments%20reveal%20the%20effectiveness%20of%20our%20methods%2C%20helping%0Athe%20adapted%20models%20to%20achieve%20performance%20improvements%20in%2087%25%20of%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVocabulary%2520Expansion%2520of%2520Chat%2520Models%2520with%2520Unlabeled%2520Target%2520Language%2520Data%26entry.906535625%3DAtsuki%2520Yamaguchi%2520and%2520Terufumi%2520Morishita%2520and%2520Aline%2520Villavicencio%2520and%2520Nikolaos%2520Aletras%26entry.1292438233%3D%2520%2520Chat%2520models%2520%2528i.e.%2520language%2520models%2520trained%2520to%2520follow%2520instructions%2520through%250Aconversation%2520with%2520humans%2529%2520outperform%2520base%2520models%2520%2528i.e.%2520trained%2520solely%2520on%250Aunlabeled%2520data%2529%2520in%2520both%2520conversation%2520and%2520general%2520task-solving%2520abilities.%2520These%250Amodels%2520are%2520generally%2520English-centric%2520and%2520require%2520further%2520adaptation%2520for%250Alanguages%2520that%2520are%2520underrepresented%2520in%2520or%2520absent%2520from%2520their%2520training%2520data.%2520A%250Acommon%2520technique%2520for%2520adapting%2520base%2520models%2520is%2520to%2520extend%2520the%2520model%2527s%2520vocabulary%250Awith%2520target%2520language%2520tokens%252C%2520i.e.%2520vocabulary%2520expansion%2520%2528VE%2529%252C%2520and%2520then%250Acontinually%2520pre-train%2520it%2520on%2520language-specific%2520data.%2520Using%2520chat%2520data%2520is%2520ideal%250Afor%2520chat%2520model%2520adaptation%252C%2520but%2520often%252C%2520either%2520this%2520does%2520not%2520exist%2520or%2520is%2520costly%250Ato%2520construct.%2520Alternatively%252C%2520adapting%2520chat%2520models%2520with%2520unlabeled%2520data%2520is%2520a%250Apossible%2520solution%252C%2520but%2520it%2520could%2520result%2520in%2520catastrophic%2520forgetting.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520impact%2520of%2520using%2520unlabeled%2520target%2520language%2520data%2520for%2520VE%250Aon%2520chat%2520models%2520for%2520the%2520first%2520time.%2520We%2520first%2520show%2520that%2520off-the-shelf%2520VE%250Agenerally%2520performs%2520well%2520across%2520target%2520language%2520tasks%2520and%2520models%2520in%252071%2525%2520of%250Acases%252C%2520though%2520it%2520underperforms%2520in%2520scenarios%2520where%2520source%2520chat%2520models%2520are%250Aalready%2520strong.%2520To%2520further%2520improve%2520adapted%2520models%252C%2520we%2520propose%2520post-hoc%250Atechniques%2520that%2520inject%2520information%2520from%2520the%2520source%2520model%2520without%2520requiring%2520any%250Afurther%2520training.%2520Experiments%2520reveal%2520the%2520effectiveness%2520of%2520our%2520methods%252C%2520helping%250Athe%2520adapted%2520models%2520to%2520achieve%2520performance%2520improvements%2520in%252087%2525%2520of%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vocabulary%20Expansion%20of%20Chat%20Models%20with%20Unlabeled%20Target%20Language%20Data&entry.906535625=Atsuki%20Yamaguchi%20and%20Terufumi%20Morishita%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras&entry.1292438233=%20%20Chat%20models%20%28i.e.%20language%20models%20trained%20to%20follow%20instructions%20through%0Aconversation%20with%20humans%29%20outperform%20base%20models%20%28i.e.%20trained%20solely%20on%0Aunlabeled%20data%29%20in%20both%20conversation%20and%20general%20task-solving%20abilities.%20These%0Amodels%20are%20generally%20English-centric%20and%20require%20further%20adaptation%20for%0Alanguages%20that%20are%20underrepresented%20in%20or%20absent%20from%20their%20training%20data.%20A%0Acommon%20technique%20for%20adapting%20base%20models%20is%20to%20extend%20the%20model%27s%20vocabulary%0Awith%20target%20language%20tokens%2C%20i.e.%20vocabulary%20expansion%20%28VE%29%2C%20and%20then%0Acontinually%20pre-train%20it%20on%20language-specific%20data.%20Using%20chat%20data%20is%20ideal%0Afor%20chat%20model%20adaptation%2C%20but%20often%2C%20either%20this%20does%20not%20exist%20or%20is%20costly%0Ato%20construct.%20Alternatively%2C%20adapting%20chat%20models%20with%20unlabeled%20data%20is%20a%0Apossible%20solution%2C%20but%20it%20could%20result%20in%20catastrophic%20forgetting.%20In%20this%0Apaper%2C%20we%20investigate%20the%20impact%20of%20using%20unlabeled%20target%20language%20data%20for%20VE%0Aon%20chat%20models%20for%20the%20first%20time.%20We%20first%20show%20that%20off-the-shelf%20VE%0Agenerally%20performs%20well%20across%20target%20language%20tasks%20and%20models%20in%2071%25%20of%0Acases%2C%20though%20it%20underperforms%20in%20scenarios%20where%20source%20chat%20models%20are%0Aalready%20strong.%20To%20further%20improve%20adapted%20models%2C%20we%20propose%20post-hoc%0Atechniques%20that%20inject%20information%20from%20the%20source%20model%20without%20requiring%20any%0Afurther%20training.%20Experiments%20reveal%20the%20effectiveness%20of%20our%20methods%2C%20helping%0Athe%20adapted%20models%20to%20achieve%20performance%20improvements%20in%2087%25%20of%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11704v2&entry.124074799=Read"},
{"title": "Understanding Key Point Cloud Features for Development Three-dimensional\n  Adversarial Attacks", "author": "Hanieh Naderi and Chinthaka Dinesh and Ivan V. Bajic and Shohreh Kasaei", "abstract": "  Adversarial attacks pose serious challenges for deep neural network\n(DNN)-based analysis of various input signals. In the case of three-dimensional\npoint clouds, methods have been developed to identify points that play a key\nrole in network decision, and these become crucial in generating existing\nadversarial attacks. For example, a saliency map approach is a popular method\nfor identifying adversarial drop points, whose removal would significantly\nimpact the network decision. This paper seeks to enhance the understanding of\nthree-dimensional adversarial attacks by exploring which point cloud features\nare most important for predicting adversarial points. Specifically, Fourteen\nkey point cloud features such as edge intensity and distance from the centroid\nare defined, and multiple linear regression is employed to assess their\npredictive power for adversarial points. Based on critical feature selection\ninsights, a new attack method has been developed to evaluate whether the\nselected features can generate an attack successfully. Unlike traditional\nattack methods that rely on model-specific vulnerabilities, this approach\nfocuses on the intrinsic characteristics of the point clouds themselves. It is\ndemonstrated that these features can predict adversarial points across four\ndifferent DNN architectures, Point Network (PointNet), PointNet++, Dynamic\nGraph Convolutional Neural Networks (DGCNN), and Point Convolutional Network\n(PointConv) outperforming random guessing and achieving results comparable to\nsaliency map-based attacks. This study has important engineering applications,\nsuch as enhancing the security and robustness of three-dimensional point\ncloud-based systems in fields like robotics and autonomous driving.\n", "link": "http://arxiv.org/abs/2210.14164v4", "date": "2024-12-18", "relevancy": 2.6527, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5703}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Key%20Point%20Cloud%20Features%20for%20Development%20Three-dimensional%0A%20%20Adversarial%20Attacks&body=Title%3A%20Understanding%20Key%20Point%20Cloud%20Features%20for%20Development%20Three-dimensional%0A%20%20Adversarial%20Attacks%0AAuthor%3A%20Hanieh%20Naderi%20and%20Chinthaka%20Dinesh%20and%20Ivan%20V.%20Bajic%20and%20Shohreh%20Kasaei%0AAbstract%3A%20%20%20Adversarial%20attacks%20pose%20serious%20challenges%20for%20deep%20neural%20network%0A%28DNN%29-based%20analysis%20of%20various%20input%20signals.%20In%20the%20case%20of%20three-dimensional%0Apoint%20clouds%2C%20methods%20have%20been%20developed%20to%20identify%20points%20that%20play%20a%20key%0Arole%20in%20network%20decision%2C%20and%20these%20become%20crucial%20in%20generating%20existing%0Aadversarial%20attacks.%20For%20example%2C%20a%20saliency%20map%20approach%20is%20a%20popular%20method%0Afor%20identifying%20adversarial%20drop%20points%2C%20whose%20removal%20would%20significantly%0Aimpact%20the%20network%20decision.%20This%20paper%20seeks%20to%20enhance%20the%20understanding%20of%0Athree-dimensional%20adversarial%20attacks%20by%20exploring%20which%20point%20cloud%20features%0Aare%20most%20important%20for%20predicting%20adversarial%20points.%20Specifically%2C%20Fourteen%0Akey%20point%20cloud%20features%20such%20as%20edge%20intensity%20and%20distance%20from%20the%20centroid%0Aare%20defined%2C%20and%20multiple%20linear%20regression%20is%20employed%20to%20assess%20their%0Apredictive%20power%20for%20adversarial%20points.%20Based%20on%20critical%20feature%20selection%0Ainsights%2C%20a%20new%20attack%20method%20has%20been%20developed%20to%20evaluate%20whether%20the%0Aselected%20features%20can%20generate%20an%20attack%20successfully.%20Unlike%20traditional%0Aattack%20methods%20that%20rely%20on%20model-specific%20vulnerabilities%2C%20this%20approach%0Afocuses%20on%20the%20intrinsic%20characteristics%20of%20the%20point%20clouds%20themselves.%20It%20is%0Ademonstrated%20that%20these%20features%20can%20predict%20adversarial%20points%20across%20four%0Adifferent%20DNN%20architectures%2C%20Point%20Network%20%28PointNet%29%2C%20PointNet%2B%2B%2C%20Dynamic%0AGraph%20Convolutional%20Neural%20Networks%20%28DGCNN%29%2C%20and%20Point%20Convolutional%20Network%0A%28PointConv%29%20outperforming%20random%20guessing%20and%20achieving%20results%20comparable%20to%0Asaliency%20map-based%20attacks.%20This%20study%20has%20important%20engineering%20applications%2C%0Asuch%20as%20enhancing%20the%20security%20and%20robustness%20of%20three-dimensional%20point%0Acloud-based%20systems%20in%20fields%20like%20robotics%20and%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.14164v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Key%2520Point%2520Cloud%2520Features%2520for%2520Development%2520Three-dimensional%250A%2520%2520Adversarial%2520Attacks%26entry.906535625%3DHanieh%2520Naderi%2520and%2520Chinthaka%2520Dinesh%2520and%2520Ivan%2520V.%2520Bajic%2520and%2520Shohreh%2520Kasaei%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520pose%2520serious%2520challenges%2520for%2520deep%2520neural%2520network%250A%2528DNN%2529-based%2520analysis%2520of%2520various%2520input%2520signals.%2520In%2520the%2520case%2520of%2520three-dimensional%250Apoint%2520clouds%252C%2520methods%2520have%2520been%2520developed%2520to%2520identify%2520points%2520that%2520play%2520a%2520key%250Arole%2520in%2520network%2520decision%252C%2520and%2520these%2520become%2520crucial%2520in%2520generating%2520existing%250Aadversarial%2520attacks.%2520For%2520example%252C%2520a%2520saliency%2520map%2520approach%2520is%2520a%2520popular%2520method%250Afor%2520identifying%2520adversarial%2520drop%2520points%252C%2520whose%2520removal%2520would%2520significantly%250Aimpact%2520the%2520network%2520decision.%2520This%2520paper%2520seeks%2520to%2520enhance%2520the%2520understanding%2520of%250Athree-dimensional%2520adversarial%2520attacks%2520by%2520exploring%2520which%2520point%2520cloud%2520features%250Aare%2520most%2520important%2520for%2520predicting%2520adversarial%2520points.%2520Specifically%252C%2520Fourteen%250Akey%2520point%2520cloud%2520features%2520such%2520as%2520edge%2520intensity%2520and%2520distance%2520from%2520the%2520centroid%250Aare%2520defined%252C%2520and%2520multiple%2520linear%2520regression%2520is%2520employed%2520to%2520assess%2520their%250Apredictive%2520power%2520for%2520adversarial%2520points.%2520Based%2520on%2520critical%2520feature%2520selection%250Ainsights%252C%2520a%2520new%2520attack%2520method%2520has%2520been%2520developed%2520to%2520evaluate%2520whether%2520the%250Aselected%2520features%2520can%2520generate%2520an%2520attack%2520successfully.%2520Unlike%2520traditional%250Aattack%2520methods%2520that%2520rely%2520on%2520model-specific%2520vulnerabilities%252C%2520this%2520approach%250Afocuses%2520on%2520the%2520intrinsic%2520characteristics%2520of%2520the%2520point%2520clouds%2520themselves.%2520It%2520is%250Ademonstrated%2520that%2520these%2520features%2520can%2520predict%2520adversarial%2520points%2520across%2520four%250Adifferent%2520DNN%2520architectures%252C%2520Point%2520Network%2520%2528PointNet%2529%252C%2520PointNet%252B%252B%252C%2520Dynamic%250AGraph%2520Convolutional%2520Neural%2520Networks%2520%2528DGCNN%2529%252C%2520and%2520Point%2520Convolutional%2520Network%250A%2528PointConv%2529%2520outperforming%2520random%2520guessing%2520and%2520achieving%2520results%2520comparable%2520to%250Asaliency%2520map-based%2520attacks.%2520This%2520study%2520has%2520important%2520engineering%2520applications%252C%250Asuch%2520as%2520enhancing%2520the%2520security%2520and%2520robustness%2520of%2520three-dimensional%2520point%250Acloud-based%2520systems%2520in%2520fields%2520like%2520robotics%2520and%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.14164v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Key%20Point%20Cloud%20Features%20for%20Development%20Three-dimensional%0A%20%20Adversarial%20Attacks&entry.906535625=Hanieh%20Naderi%20and%20Chinthaka%20Dinesh%20and%20Ivan%20V.%20Bajic%20and%20Shohreh%20Kasaei&entry.1292438233=%20%20Adversarial%20attacks%20pose%20serious%20challenges%20for%20deep%20neural%20network%0A%28DNN%29-based%20analysis%20of%20various%20input%20signals.%20In%20the%20case%20of%20three-dimensional%0Apoint%20clouds%2C%20methods%20have%20been%20developed%20to%20identify%20points%20that%20play%20a%20key%0Arole%20in%20network%20decision%2C%20and%20these%20become%20crucial%20in%20generating%20existing%0Aadversarial%20attacks.%20For%20example%2C%20a%20saliency%20map%20approach%20is%20a%20popular%20method%0Afor%20identifying%20adversarial%20drop%20points%2C%20whose%20removal%20would%20significantly%0Aimpact%20the%20network%20decision.%20This%20paper%20seeks%20to%20enhance%20the%20understanding%20of%0Athree-dimensional%20adversarial%20attacks%20by%20exploring%20which%20point%20cloud%20features%0Aare%20most%20important%20for%20predicting%20adversarial%20points.%20Specifically%2C%20Fourteen%0Akey%20point%20cloud%20features%20such%20as%20edge%20intensity%20and%20distance%20from%20the%20centroid%0Aare%20defined%2C%20and%20multiple%20linear%20regression%20is%20employed%20to%20assess%20their%0Apredictive%20power%20for%20adversarial%20points.%20Based%20on%20critical%20feature%20selection%0Ainsights%2C%20a%20new%20attack%20method%20has%20been%20developed%20to%20evaluate%20whether%20the%0Aselected%20features%20can%20generate%20an%20attack%20successfully.%20Unlike%20traditional%0Aattack%20methods%20that%20rely%20on%20model-specific%20vulnerabilities%2C%20this%20approach%0Afocuses%20on%20the%20intrinsic%20characteristics%20of%20the%20point%20clouds%20themselves.%20It%20is%0Ademonstrated%20that%20these%20features%20can%20predict%20adversarial%20points%20across%20four%0Adifferent%20DNN%20architectures%2C%20Point%20Network%20%28PointNet%29%2C%20PointNet%2B%2B%2C%20Dynamic%0AGraph%20Convolutional%20Neural%20Networks%20%28DGCNN%29%2C%20and%20Point%20Convolutional%20Network%0A%28PointConv%29%20outperforming%20random%20guessing%20and%20achieving%20results%20comparable%20to%0Asaliency%20map-based%20attacks.%20This%20study%20has%20important%20engineering%20applications%2C%0Asuch%20as%20enhancing%20the%20security%20and%20robustness%20of%20three-dimensional%20point%0Acloud-based%20systems%20in%20fields%20like%20robotics%20and%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.14164v4&entry.124074799=Read"},
{"title": "Model Decides How to Tokenize: Adaptive DNA Sequence Tokenization with\n  MxDNA", "author": "Lifeng Qiao and Peng Ye and Yuchen Ren and Weiqiang Bai and Chaoqi Liang and Xinzhu Ma and Nanqing Dong and Wanli Ouyang", "abstract": "  Foundation models have made significant strides in understanding the genomic\nlanguage of DNA sequences. However, previous models typically adopt the\ntokenization methods designed for natural language, which are unsuitable for\nDNA sequences due to their unique characteristics. In addition, the optimal\napproach to tokenize DNA remains largely under-explored, and may not be\nintuitively understood by humans even if discovered. To address these\nchallenges, we introduce MxDNA, a novel framework where the model autonomously\nlearns an effective DNA tokenization strategy through gradient decent. MxDNA\nemploys a sparse Mixture of Convolution Experts coupled with a deformable\nconvolution to model the tokenization process, with the discontinuous,\noverlapping, and ambiguous nature of meaningful genomic segments explicitly\nconsidered. On Nucleotide Transformer Benchmarks and Genomic Benchmarks, MxDNA\ndemonstrates superior performance to existing methods with less pretraining\ndata and time, highlighting its effectiveness. Finally, we show that MxDNA\nlearns unique tokenization strategy distinct to those of previous methods and\ncaptures genomic functionalities at a token level during self-supervised\npretraining. Our MxDNA aims to provide a new perspective on DNA tokenization,\npotentially offering broad applications in various domains and yielding\nprofound insights.\n", "link": "http://arxiv.org/abs/2412.13716v1", "date": "2024-12-18", "relevancy": 2.6474, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Decides%20How%20to%20Tokenize%3A%20Adaptive%20DNA%20Sequence%20Tokenization%20with%0A%20%20MxDNA&body=Title%3A%20Model%20Decides%20How%20to%20Tokenize%3A%20Adaptive%20DNA%20Sequence%20Tokenization%20with%0A%20%20MxDNA%0AAuthor%3A%20Lifeng%20Qiao%20and%20Peng%20Ye%20and%20Yuchen%20Ren%20and%20Weiqiang%20Bai%20and%20Chaoqi%20Liang%20and%20Xinzhu%20Ma%20and%20Nanqing%20Dong%20and%20Wanli%20Ouyang%0AAbstract%3A%20%20%20Foundation%20models%20have%20made%20significant%20strides%20in%20understanding%20the%20genomic%0Alanguage%20of%20DNA%20sequences.%20However%2C%20previous%20models%20typically%20adopt%20the%0Atokenization%20methods%20designed%20for%20natural%20language%2C%20which%20are%20unsuitable%20for%0ADNA%20sequences%20due%20to%20their%20unique%20characteristics.%20In%20addition%2C%20the%20optimal%0Aapproach%20to%20tokenize%20DNA%20remains%20largely%20under-explored%2C%20and%20may%20not%20be%0Aintuitively%20understood%20by%20humans%20even%20if%20discovered.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20MxDNA%2C%20a%20novel%20framework%20where%20the%20model%20autonomously%0Alearns%20an%20effective%20DNA%20tokenization%20strategy%20through%20gradient%20decent.%20MxDNA%0Aemploys%20a%20sparse%20Mixture%20of%20Convolution%20Experts%20coupled%20with%20a%20deformable%0Aconvolution%20to%20model%20the%20tokenization%20process%2C%20with%20the%20discontinuous%2C%0Aoverlapping%2C%20and%20ambiguous%20nature%20of%20meaningful%20genomic%20segments%20explicitly%0Aconsidered.%20On%20Nucleotide%20Transformer%20Benchmarks%20and%20Genomic%20Benchmarks%2C%20MxDNA%0Ademonstrates%20superior%20performance%20to%20existing%20methods%20with%20less%20pretraining%0Adata%20and%20time%2C%20highlighting%20its%20effectiveness.%20Finally%2C%20we%20show%20that%20MxDNA%0Alearns%20unique%20tokenization%20strategy%20distinct%20to%20those%20of%20previous%20methods%20and%0Acaptures%20genomic%20functionalities%20at%20a%20token%20level%20during%20self-supervised%0Apretraining.%20Our%20MxDNA%20aims%20to%20provide%20a%20new%20perspective%20on%20DNA%20tokenization%2C%0Apotentially%20offering%20broad%20applications%20in%20various%20domains%20and%20yielding%0Aprofound%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Decides%2520How%2520to%2520Tokenize%253A%2520Adaptive%2520DNA%2520Sequence%2520Tokenization%2520with%250A%2520%2520MxDNA%26entry.906535625%3DLifeng%2520Qiao%2520and%2520Peng%2520Ye%2520and%2520Yuchen%2520Ren%2520and%2520Weiqiang%2520Bai%2520and%2520Chaoqi%2520Liang%2520and%2520Xinzhu%2520Ma%2520and%2520Nanqing%2520Dong%2520and%2520Wanli%2520Ouyang%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520made%2520significant%2520strides%2520in%2520understanding%2520the%2520genomic%250Alanguage%2520of%2520DNA%2520sequences.%2520However%252C%2520previous%2520models%2520typically%2520adopt%2520the%250Atokenization%2520methods%2520designed%2520for%2520natural%2520language%252C%2520which%2520are%2520unsuitable%2520for%250ADNA%2520sequences%2520due%2520to%2520their%2520unique%2520characteristics.%2520In%2520addition%252C%2520the%2520optimal%250Aapproach%2520to%2520tokenize%2520DNA%2520remains%2520largely%2520under-explored%252C%2520and%2520may%2520not%2520be%250Aintuitively%2520understood%2520by%2520humans%2520even%2520if%2520discovered.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520MxDNA%252C%2520a%2520novel%2520framework%2520where%2520the%2520model%2520autonomously%250Alearns%2520an%2520effective%2520DNA%2520tokenization%2520strategy%2520through%2520gradient%2520decent.%2520MxDNA%250Aemploys%2520a%2520sparse%2520Mixture%2520of%2520Convolution%2520Experts%2520coupled%2520with%2520a%2520deformable%250Aconvolution%2520to%2520model%2520the%2520tokenization%2520process%252C%2520with%2520the%2520discontinuous%252C%250Aoverlapping%252C%2520and%2520ambiguous%2520nature%2520of%2520meaningful%2520genomic%2520segments%2520explicitly%250Aconsidered.%2520On%2520Nucleotide%2520Transformer%2520Benchmarks%2520and%2520Genomic%2520Benchmarks%252C%2520MxDNA%250Ademonstrates%2520superior%2520performance%2520to%2520existing%2520methods%2520with%2520less%2520pretraining%250Adata%2520and%2520time%252C%2520highlighting%2520its%2520effectiveness.%2520Finally%252C%2520we%2520show%2520that%2520MxDNA%250Alearns%2520unique%2520tokenization%2520strategy%2520distinct%2520to%2520those%2520of%2520previous%2520methods%2520and%250Acaptures%2520genomic%2520functionalities%2520at%2520a%2520token%2520level%2520during%2520self-supervised%250Apretraining.%2520Our%2520MxDNA%2520aims%2520to%2520provide%2520a%2520new%2520perspective%2520on%2520DNA%2520tokenization%252C%250Apotentially%2520offering%2520broad%2520applications%2520in%2520various%2520domains%2520and%2520yielding%250Aprofound%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Decides%20How%20to%20Tokenize%3A%20Adaptive%20DNA%20Sequence%20Tokenization%20with%0A%20%20MxDNA&entry.906535625=Lifeng%20Qiao%20and%20Peng%20Ye%20and%20Yuchen%20Ren%20and%20Weiqiang%20Bai%20and%20Chaoqi%20Liang%20and%20Xinzhu%20Ma%20and%20Nanqing%20Dong%20and%20Wanli%20Ouyang&entry.1292438233=%20%20Foundation%20models%20have%20made%20significant%20strides%20in%20understanding%20the%20genomic%0Alanguage%20of%20DNA%20sequences.%20However%2C%20previous%20models%20typically%20adopt%20the%0Atokenization%20methods%20designed%20for%20natural%20language%2C%20which%20are%20unsuitable%20for%0ADNA%20sequences%20due%20to%20their%20unique%20characteristics.%20In%20addition%2C%20the%20optimal%0Aapproach%20to%20tokenize%20DNA%20remains%20largely%20under-explored%2C%20and%20may%20not%20be%0Aintuitively%20understood%20by%20humans%20even%20if%20discovered.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20MxDNA%2C%20a%20novel%20framework%20where%20the%20model%20autonomously%0Alearns%20an%20effective%20DNA%20tokenization%20strategy%20through%20gradient%20decent.%20MxDNA%0Aemploys%20a%20sparse%20Mixture%20of%20Convolution%20Experts%20coupled%20with%20a%20deformable%0Aconvolution%20to%20model%20the%20tokenization%20process%2C%20with%20the%20discontinuous%2C%0Aoverlapping%2C%20and%20ambiguous%20nature%20of%20meaningful%20genomic%20segments%20explicitly%0Aconsidered.%20On%20Nucleotide%20Transformer%20Benchmarks%20and%20Genomic%20Benchmarks%2C%20MxDNA%0Ademonstrates%20superior%20performance%20to%20existing%20methods%20with%20less%20pretraining%0Adata%20and%20time%2C%20highlighting%20its%20effectiveness.%20Finally%2C%20we%20show%20that%20MxDNA%0Alearns%20unique%20tokenization%20strategy%20distinct%20to%20those%20of%20previous%20methods%20and%0Acaptures%20genomic%20functionalities%20at%20a%20token%20level%20during%20self-supervised%0Apretraining.%20Our%20MxDNA%20aims%20to%20provide%20a%20new%20perspective%20on%20DNA%20tokenization%2C%0Apotentially%20offering%20broad%20applications%20in%20various%20domains%20and%20yielding%0Aprofound%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13716v1&entry.124074799=Read"},
{"title": "SSE-SAM: Balancing Head and Tail Classes Gradually through Stage-Wise\n  SAM", "author": "Xingyu Lyu and Qianqian Xu and Zhiyong Yang and Shaojie Lyu and Qingming Huang", "abstract": "  Real-world datasets often exhibit a long-tailed distribution, where vast\nmajority of classes known as tail classes have only few samples. Traditional\nmethods tend to overfit on these tail classes. Recently, a new approach called\nImbalanced SAM (ImbSAM) is proposed to leverage the generalization benefits of\nSharpness-Aware Minimization (SAM) for long-tailed distributions. The main\nstrategy is to merely enhance the smoothness of the loss function for tail\nclasses. However, we argue that improving generalization in long-tail scenarios\nrequires a careful balance between head and tail classes. We show that neither\nSAM nor ImbSAM alone can fully achieve this balance. For SAM, we prove that\nalthough it enhances the model's generalization ability by escaping saddle\npoint in the overall loss landscape, it does not effectively address this for\ntail-class losses. Conversely, while ImbSAM is more effective at avoiding\nsaddle points in tail classes, the head classes are trained insufficiently,\nresulting in significant performance drops. Based on these insights, we propose\nStage-wise Saddle Escaping SAM (SSE-SAM), which uses complementary strengths of\nImbSAM and SAM in a phased approach. Initially, SSE-SAM follows the majority\nsample to avoid saddle points of the head-class loss. During the later phase,\nit focuses on tail-classes to help them escape saddle points. Our experiments\nconfirm that SSE-SAM has better ability in escaping saddles both on head and\ntail classes, and shows performance improvements.\n", "link": "http://arxiv.org/abs/2412.13715v1", "date": "2024-12-18", "relevancy": 2.6471, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.6345}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSE-SAM%3A%20Balancing%20Head%20and%20Tail%20Classes%20Gradually%20through%20Stage-Wise%0A%20%20SAM&body=Title%3A%20SSE-SAM%3A%20Balancing%20Head%20and%20Tail%20Classes%20Gradually%20through%20Stage-Wise%0A%20%20SAM%0AAuthor%3A%20Xingyu%20Lyu%20and%20Qianqian%20Xu%20and%20Zhiyong%20Yang%20and%20Shaojie%20Lyu%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Real-world%20datasets%20often%20exhibit%20a%20long-tailed%20distribution%2C%20where%20vast%0Amajority%20of%20classes%20known%20as%20tail%20classes%20have%20only%20few%20samples.%20Traditional%0Amethods%20tend%20to%20overfit%20on%20these%20tail%20classes.%20Recently%2C%20a%20new%20approach%20called%0AImbalanced%20SAM%20%28ImbSAM%29%20is%20proposed%20to%20leverage%20the%20generalization%20benefits%20of%0ASharpness-Aware%20Minimization%20%28SAM%29%20for%20long-tailed%20distributions.%20The%20main%0Astrategy%20is%20to%20merely%20enhance%20the%20smoothness%20of%20the%20loss%20function%20for%20tail%0Aclasses.%20However%2C%20we%20argue%20that%20improving%20generalization%20in%20long-tail%20scenarios%0Arequires%20a%20careful%20balance%20between%20head%20and%20tail%20classes.%20We%20show%20that%20neither%0ASAM%20nor%20ImbSAM%20alone%20can%20fully%20achieve%20this%20balance.%20For%20SAM%2C%20we%20prove%20that%0Aalthough%20it%20enhances%20the%20model%27s%20generalization%20ability%20by%20escaping%20saddle%0Apoint%20in%20the%20overall%20loss%20landscape%2C%20it%20does%20not%20effectively%20address%20this%20for%0Atail-class%20losses.%20Conversely%2C%20while%20ImbSAM%20is%20more%20effective%20at%20avoiding%0Asaddle%20points%20in%20tail%20classes%2C%20the%20head%20classes%20are%20trained%20insufficiently%2C%0Aresulting%20in%20significant%20performance%20drops.%20Based%20on%20these%20insights%2C%20we%20propose%0AStage-wise%20Saddle%20Escaping%20SAM%20%28SSE-SAM%29%2C%20which%20uses%20complementary%20strengths%20of%0AImbSAM%20and%20SAM%20in%20a%20phased%20approach.%20Initially%2C%20SSE-SAM%20follows%20the%20majority%0Asample%20to%20avoid%20saddle%20points%20of%20the%20head-class%20loss.%20During%20the%20later%20phase%2C%0Ait%20focuses%20on%20tail-classes%20to%20help%20them%20escape%20saddle%20points.%20Our%20experiments%0Aconfirm%20that%20SSE-SAM%20has%20better%20ability%20in%20escaping%20saddles%20both%20on%20head%20and%0Atail%20classes%2C%20and%20shows%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSE-SAM%253A%2520Balancing%2520Head%2520and%2520Tail%2520Classes%2520Gradually%2520through%2520Stage-Wise%250A%2520%2520SAM%26entry.906535625%3DXingyu%2520Lyu%2520and%2520Qianqian%2520Xu%2520and%2520Zhiyong%2520Yang%2520and%2520Shaojie%2520Lyu%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Real-world%2520datasets%2520often%2520exhibit%2520a%2520long-tailed%2520distribution%252C%2520where%2520vast%250Amajority%2520of%2520classes%2520known%2520as%2520tail%2520classes%2520have%2520only%2520few%2520samples.%2520Traditional%250Amethods%2520tend%2520to%2520overfit%2520on%2520these%2520tail%2520classes.%2520Recently%252C%2520a%2520new%2520approach%2520called%250AImbalanced%2520SAM%2520%2528ImbSAM%2529%2520is%2520proposed%2520to%2520leverage%2520the%2520generalization%2520benefits%2520of%250ASharpness-Aware%2520Minimization%2520%2528SAM%2529%2520for%2520long-tailed%2520distributions.%2520The%2520main%250Astrategy%2520is%2520to%2520merely%2520enhance%2520the%2520smoothness%2520of%2520the%2520loss%2520function%2520for%2520tail%250Aclasses.%2520However%252C%2520we%2520argue%2520that%2520improving%2520generalization%2520in%2520long-tail%2520scenarios%250Arequires%2520a%2520careful%2520balance%2520between%2520head%2520and%2520tail%2520classes.%2520We%2520show%2520that%2520neither%250ASAM%2520nor%2520ImbSAM%2520alone%2520can%2520fully%2520achieve%2520this%2520balance.%2520For%2520SAM%252C%2520we%2520prove%2520that%250Aalthough%2520it%2520enhances%2520the%2520model%2527s%2520generalization%2520ability%2520by%2520escaping%2520saddle%250Apoint%2520in%2520the%2520overall%2520loss%2520landscape%252C%2520it%2520does%2520not%2520effectively%2520address%2520this%2520for%250Atail-class%2520losses.%2520Conversely%252C%2520while%2520ImbSAM%2520is%2520more%2520effective%2520at%2520avoiding%250Asaddle%2520points%2520in%2520tail%2520classes%252C%2520the%2520head%2520classes%2520are%2520trained%2520insufficiently%252C%250Aresulting%2520in%2520significant%2520performance%2520drops.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%250AStage-wise%2520Saddle%2520Escaping%2520SAM%2520%2528SSE-SAM%2529%252C%2520which%2520uses%2520complementary%2520strengths%2520of%250AImbSAM%2520and%2520SAM%2520in%2520a%2520phased%2520approach.%2520Initially%252C%2520SSE-SAM%2520follows%2520the%2520majority%250Asample%2520to%2520avoid%2520saddle%2520points%2520of%2520the%2520head-class%2520loss.%2520During%2520the%2520later%2520phase%252C%250Ait%2520focuses%2520on%2520tail-classes%2520to%2520help%2520them%2520escape%2520saddle%2520points.%2520Our%2520experiments%250Aconfirm%2520that%2520SSE-SAM%2520has%2520better%2520ability%2520in%2520escaping%2520saddles%2520both%2520on%2520head%2520and%250Atail%2520classes%252C%2520and%2520shows%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSE-SAM%3A%20Balancing%20Head%20and%20Tail%20Classes%20Gradually%20through%20Stage-Wise%0A%20%20SAM&entry.906535625=Xingyu%20Lyu%20and%20Qianqian%20Xu%20and%20Zhiyong%20Yang%20and%20Shaojie%20Lyu%20and%20Qingming%20Huang&entry.1292438233=%20%20Real-world%20datasets%20often%20exhibit%20a%20long-tailed%20distribution%2C%20where%20vast%0Amajority%20of%20classes%20known%20as%20tail%20classes%20have%20only%20few%20samples.%20Traditional%0Amethods%20tend%20to%20overfit%20on%20these%20tail%20classes.%20Recently%2C%20a%20new%20approach%20called%0AImbalanced%20SAM%20%28ImbSAM%29%20is%20proposed%20to%20leverage%20the%20generalization%20benefits%20of%0ASharpness-Aware%20Minimization%20%28SAM%29%20for%20long-tailed%20distributions.%20The%20main%0Astrategy%20is%20to%20merely%20enhance%20the%20smoothness%20of%20the%20loss%20function%20for%20tail%0Aclasses.%20However%2C%20we%20argue%20that%20improving%20generalization%20in%20long-tail%20scenarios%0Arequires%20a%20careful%20balance%20between%20head%20and%20tail%20classes.%20We%20show%20that%20neither%0ASAM%20nor%20ImbSAM%20alone%20can%20fully%20achieve%20this%20balance.%20For%20SAM%2C%20we%20prove%20that%0Aalthough%20it%20enhances%20the%20model%27s%20generalization%20ability%20by%20escaping%20saddle%0Apoint%20in%20the%20overall%20loss%20landscape%2C%20it%20does%20not%20effectively%20address%20this%20for%0Atail-class%20losses.%20Conversely%2C%20while%20ImbSAM%20is%20more%20effective%20at%20avoiding%0Asaddle%20points%20in%20tail%20classes%2C%20the%20head%20classes%20are%20trained%20insufficiently%2C%0Aresulting%20in%20significant%20performance%20drops.%20Based%20on%20these%20insights%2C%20we%20propose%0AStage-wise%20Saddle%20Escaping%20SAM%20%28SSE-SAM%29%2C%20which%20uses%20complementary%20strengths%20of%0AImbSAM%20and%20SAM%20in%20a%20phased%20approach.%20Initially%2C%20SSE-SAM%20follows%20the%20majority%0Asample%20to%20avoid%20saddle%20points%20of%20the%20head-class%20loss.%20During%20the%20later%20phase%2C%0Ait%20focuses%20on%20tail-classes%20to%20help%20them%20escape%20saddle%20points.%20Our%20experiments%0Aconfirm%20that%20SSE-SAM%20has%20better%20ability%20in%20escaping%20saddles%20both%20on%20head%20and%0Atail%20classes%2C%20and%20shows%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13715v1&entry.124074799=Read"},
{"title": "A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future", "author": "Shilin Sun and Wenbin An and Feng Tian and Fang Nan and Qidong Liu and Jun Liu and Nazaraf Shah and Ping Chen", "abstract": "  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n", "link": "http://arxiv.org/abs/2412.14056v1", "date": "2024-12-18", "relevancy": 2.6093, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Multimodal%20Explainable%20Artificial%20Intelligence%3A%20Past%2C%0A%20%20Present%20and%20Future&body=Title%3A%20A%20Review%20of%20Multimodal%20Explainable%20Artificial%20Intelligence%3A%20Past%2C%0A%20%20Present%20and%20Future%0AAuthor%3A%20Shilin%20Sun%20and%20Wenbin%20An%20and%20Feng%20Tian%20and%20Fang%20Nan%20and%20Qidong%20Liu%20and%20Jun%20Liu%20and%20Nazaraf%20Shah%20and%20Ping%20Chen%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20has%20rapidly%20developed%20through%20advancements%20in%0Acomputational%20power%20and%20the%20growth%20of%20massive%20datasets.%20However%2C%20this%20progress%0Ahas%20also%20heightened%20challenges%20in%20interpreting%20the%20%22black-box%22%20nature%20of%20AI%0Amodels.%20To%20address%20these%20concerns%2C%20eXplainable%20AI%20%28XAI%29%20has%20emerged%20with%20a%0Afocus%20on%20transparency%20and%20interpretability%20to%20enhance%20human%20understanding%20and%0Atrust%20in%20AI%20decision-making%20processes.%20In%20the%20context%20of%20multimodal%20data%20fusion%0Aand%20complex%20reasoning%20scenarios%2C%20the%20proposal%20of%20Multimodal%20eXplainable%20AI%0A%28MXAI%29%20integrates%20multiple%20modalities%20for%20prediction%20and%20explanation%20tasks.%0AMeanwhile%2C%20the%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20remarkable%0Abreakthroughs%20in%20natural%20language%20processing%2C%20yet%20their%20complexity%20has%20further%0Aexacerbated%20the%20issue%20of%20MXAI.%20To%20gain%20key%20insights%20into%20the%20development%20of%0AMXAI%20methods%20and%20provide%20crucial%20guidance%20for%20building%20more%20transparent%2C%20fair%2C%0Aand%20trustworthy%20AI%20systems%2C%20we%20review%20the%20MXAI%20methods%20from%20a%20historical%0Aperspective%20and%20categorize%20them%20across%20four%20eras%3A%20traditional%20machine%20learning%2C%0Adeep%20learning%2C%20discriminative%20foundation%20models%2C%20and%20generative%20LLMs.%20We%20also%0Areview%20evaluation%20metrics%20and%20datasets%20used%20in%20MXAI%20research%2C%20concluding%20with%20a%0Adiscussion%20of%20future%20challenges%20and%20directions.%20A%20project%20related%20to%20this%0Areview%20has%20been%20created%20at%20https%3A//github.com/ShilinSun/mxai_review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Multimodal%2520Explainable%2520Artificial%2520Intelligence%253A%2520Past%252C%250A%2520%2520Present%2520and%2520Future%26entry.906535625%3DShilin%2520Sun%2520and%2520Wenbin%2520An%2520and%2520Feng%2520Tian%2520and%2520Fang%2520Nan%2520and%2520Qidong%2520Liu%2520and%2520Jun%2520Liu%2520and%2520Nazaraf%2520Shah%2520and%2520Ping%2520Chen%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520has%2520rapidly%2520developed%2520through%2520advancements%2520in%250Acomputational%2520power%2520and%2520the%2520growth%2520of%2520massive%2520datasets.%2520However%252C%2520this%2520progress%250Ahas%2520also%2520heightened%2520challenges%2520in%2520interpreting%2520the%2520%2522black-box%2522%2520nature%2520of%2520AI%250Amodels.%2520To%2520address%2520these%2520concerns%252C%2520eXplainable%2520AI%2520%2528XAI%2529%2520has%2520emerged%2520with%2520a%250Afocus%2520on%2520transparency%2520and%2520interpretability%2520to%2520enhance%2520human%2520understanding%2520and%250Atrust%2520in%2520AI%2520decision-making%2520processes.%2520In%2520the%2520context%2520of%2520multimodal%2520data%2520fusion%250Aand%2520complex%2520reasoning%2520scenarios%252C%2520the%2520proposal%2520of%2520Multimodal%2520eXplainable%2520AI%250A%2528MXAI%2529%2520integrates%2520multiple%2520modalities%2520for%2520prediction%2520and%2520explanation%2520tasks.%250AMeanwhile%252C%2520the%2520advent%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520led%2520to%2520remarkable%250Abreakthroughs%2520in%2520natural%2520language%2520processing%252C%2520yet%2520their%2520complexity%2520has%2520further%250Aexacerbated%2520the%2520issue%2520of%2520MXAI.%2520To%2520gain%2520key%2520insights%2520into%2520the%2520development%2520of%250AMXAI%2520methods%2520and%2520provide%2520crucial%2520guidance%2520for%2520building%2520more%2520transparent%252C%2520fair%252C%250Aand%2520trustworthy%2520AI%2520systems%252C%2520we%2520review%2520the%2520MXAI%2520methods%2520from%2520a%2520historical%250Aperspective%2520and%2520categorize%2520them%2520across%2520four%2520eras%253A%2520traditional%2520machine%2520learning%252C%250Adeep%2520learning%252C%2520discriminative%2520foundation%2520models%252C%2520and%2520generative%2520LLMs.%2520We%2520also%250Areview%2520evaluation%2520metrics%2520and%2520datasets%2520used%2520in%2520MXAI%2520research%252C%2520concluding%2520with%2520a%250Adiscussion%2520of%2520future%2520challenges%2520and%2520directions.%2520A%2520project%2520related%2520to%2520this%250Areview%2520has%2520been%2520created%2520at%2520https%253A//github.com/ShilinSun/mxai_review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Multimodal%20Explainable%20Artificial%20Intelligence%3A%20Past%2C%0A%20%20Present%20and%20Future&entry.906535625=Shilin%20Sun%20and%20Wenbin%20An%20and%20Feng%20Tian%20and%20Fang%20Nan%20and%20Qidong%20Liu%20and%20Jun%20Liu%20and%20Nazaraf%20Shah%20and%20Ping%20Chen&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20has%20rapidly%20developed%20through%20advancements%20in%0Acomputational%20power%20and%20the%20growth%20of%20massive%20datasets.%20However%2C%20this%20progress%0Ahas%20also%20heightened%20challenges%20in%20interpreting%20the%20%22black-box%22%20nature%20of%20AI%0Amodels.%20To%20address%20these%20concerns%2C%20eXplainable%20AI%20%28XAI%29%20has%20emerged%20with%20a%0Afocus%20on%20transparency%20and%20interpretability%20to%20enhance%20human%20understanding%20and%0Atrust%20in%20AI%20decision-making%20processes.%20In%20the%20context%20of%20multimodal%20data%20fusion%0Aand%20complex%20reasoning%20scenarios%2C%20the%20proposal%20of%20Multimodal%20eXplainable%20AI%0A%28MXAI%29%20integrates%20multiple%20modalities%20for%20prediction%20and%20explanation%20tasks.%0AMeanwhile%2C%20the%20advent%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20led%20to%20remarkable%0Abreakthroughs%20in%20natural%20language%20processing%2C%20yet%20their%20complexity%20has%20further%0Aexacerbated%20the%20issue%20of%20MXAI.%20To%20gain%20key%20insights%20into%20the%20development%20of%0AMXAI%20methods%20and%20provide%20crucial%20guidance%20for%20building%20more%20transparent%2C%20fair%2C%0Aand%20trustworthy%20AI%20systems%2C%20we%20review%20the%20MXAI%20methods%20from%20a%20historical%0Aperspective%20and%20categorize%20them%20across%20four%20eras%3A%20traditional%20machine%20learning%2C%0Adeep%20learning%2C%20discriminative%20foundation%20models%2C%20and%20generative%20LLMs.%20We%20also%0Areview%20evaluation%20metrics%20and%20datasets%20used%20in%20MXAI%20research%2C%20concluding%20with%20a%0Adiscussion%20of%20future%20challenges%20and%20directions.%20A%20project%20related%20to%20this%0Areview%20has%20been%20created%20at%20https%3A//github.com/ShilinSun/mxai_review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14056v1&entry.124074799=Read"},
{"title": "FashionComposer: Compositional Fashion Image Generation", "author": "Sihui Ji and Yiyang Wang and Xi Chen and Xiaogang Xu and Hao Luo and Hengshuang Zhao", "abstract": "  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n", "link": "http://arxiv.org/abs/2412.14168v1", "date": "2024-12-18", "relevancy": 2.5828, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6915}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6581}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FashionComposer%3A%20Compositional%20Fashion%20Image%20Generation&body=Title%3A%20FashionComposer%3A%20Compositional%20Fashion%20Image%20Generation%0AAuthor%3A%20Sihui%20Ji%20and%20Yiyang%20Wang%20and%20Xi%20Chen%20and%20Xiaogang%20Xu%20and%20Hao%20Luo%20and%20Hengshuang%20Zhao%0AAbstract%3A%20%20%20We%20present%20FashionComposer%20for%20compositional%20fashion%20image%20generation.%20Unlike%0Aprevious%20methods%2C%20FashionComposer%20is%20highly%20flexible.%20It%20takes%20multi-modal%0Ainput%20%28i.e.%2C%20text%20prompt%2C%20parametric%20human%20model%2C%20garment%20image%2C%20and%20face%0Aimage%29%20and%20supports%20personalizing%20the%20appearance%2C%20pose%2C%20and%20figure%20of%20the%20human%0Aand%20assigning%20multiple%20garments%20in%20one%20pass.%20To%20achieve%20this%2C%20we%20first%20develop%0Aa%20universal%20framework%20capable%20of%20handling%20diverse%20input%20modalities.%20We%0Aconstruct%20scaled%20training%20data%20to%20enhance%20the%20model%27s%20robust%20compositional%0Acapabilities.%20To%20accommodate%20multiple%20reference%20images%20%28garments%20and%20faces%29%0Aseamlessly%2C%20we%20organize%20these%20references%20in%20a%20single%20image%20as%20an%20%22asset%0Alibrary%22%20and%20employ%20a%20reference%20UNet%20to%20extract%20appearance%20features.%20To%20inject%0Athe%20appearance%20features%20into%20the%20correct%20pixels%20in%20the%20generated%20result%2C%20we%0Apropose%20subject-binding%20attention.%20It%20binds%20the%20appearance%20features%20from%0Adifferent%20%22assets%22%20with%20the%20corresponding%20text%20features.%20In%20this%20way%2C%20the%20model%0Acould%20understand%20each%20asset%20according%20to%20their%20semantics%2C%20supporting%20arbitrary%0Anumbers%20and%20types%20of%20reference%20images.%20As%20a%20comprehensive%20solution%2C%0AFashionComposer%20also%20supports%20many%20other%20applications%20like%20human%20album%0Ageneration%2C%20diverse%20virtual%20try-on%20tasks%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFashionComposer%253A%2520Compositional%2520Fashion%2520Image%2520Generation%26entry.906535625%3DSihui%2520Ji%2520and%2520Yiyang%2520Wang%2520and%2520Xi%2520Chen%2520and%2520Xiaogang%2520Xu%2520and%2520Hao%2520Luo%2520and%2520Hengshuang%2520Zhao%26entry.1292438233%3D%2520%2520We%2520present%2520FashionComposer%2520for%2520compositional%2520fashion%2520image%2520generation.%2520Unlike%250Aprevious%2520methods%252C%2520FashionComposer%2520is%2520highly%2520flexible.%2520It%2520takes%2520multi-modal%250Ainput%2520%2528i.e.%252C%2520text%2520prompt%252C%2520parametric%2520human%2520model%252C%2520garment%2520image%252C%2520and%2520face%250Aimage%2529%2520and%2520supports%2520personalizing%2520the%2520appearance%252C%2520pose%252C%2520and%2520figure%2520of%2520the%2520human%250Aand%2520assigning%2520multiple%2520garments%2520in%2520one%2520pass.%2520To%2520achieve%2520this%252C%2520we%2520first%2520develop%250Aa%2520universal%2520framework%2520capable%2520of%2520handling%2520diverse%2520input%2520modalities.%2520We%250Aconstruct%2520scaled%2520training%2520data%2520to%2520enhance%2520the%2520model%2527s%2520robust%2520compositional%250Acapabilities.%2520To%2520accommodate%2520multiple%2520reference%2520images%2520%2528garments%2520and%2520faces%2529%250Aseamlessly%252C%2520we%2520organize%2520these%2520references%2520in%2520a%2520single%2520image%2520as%2520an%2520%2522asset%250Alibrary%2522%2520and%2520employ%2520a%2520reference%2520UNet%2520to%2520extract%2520appearance%2520features.%2520To%2520inject%250Athe%2520appearance%2520features%2520into%2520the%2520correct%2520pixels%2520in%2520the%2520generated%2520result%252C%2520we%250Apropose%2520subject-binding%2520attention.%2520It%2520binds%2520the%2520appearance%2520features%2520from%250Adifferent%2520%2522assets%2522%2520with%2520the%2520corresponding%2520text%2520features.%2520In%2520this%2520way%252C%2520the%2520model%250Acould%2520understand%2520each%2520asset%2520according%2520to%2520their%2520semantics%252C%2520supporting%2520arbitrary%250Anumbers%2520and%2520types%2520of%2520reference%2520images.%2520As%2520a%2520comprehensive%2520solution%252C%250AFashionComposer%2520also%2520supports%2520many%2520other%2520applications%2520like%2520human%2520album%250Ageneration%252C%2520diverse%2520virtual%2520try-on%2520tasks%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FashionComposer%3A%20Compositional%20Fashion%20Image%20Generation&entry.906535625=Sihui%20Ji%20and%20Yiyang%20Wang%20and%20Xi%20Chen%20and%20Xiaogang%20Xu%20and%20Hao%20Luo%20and%20Hengshuang%20Zhao&entry.1292438233=%20%20We%20present%20FashionComposer%20for%20compositional%20fashion%20image%20generation.%20Unlike%0Aprevious%20methods%2C%20FashionComposer%20is%20highly%20flexible.%20It%20takes%20multi-modal%0Ainput%20%28i.e.%2C%20text%20prompt%2C%20parametric%20human%20model%2C%20garment%20image%2C%20and%20face%0Aimage%29%20and%20supports%20personalizing%20the%20appearance%2C%20pose%2C%20and%20figure%20of%20the%20human%0Aand%20assigning%20multiple%20garments%20in%20one%20pass.%20To%20achieve%20this%2C%20we%20first%20develop%0Aa%20universal%20framework%20capable%20of%20handling%20diverse%20input%20modalities.%20We%0Aconstruct%20scaled%20training%20data%20to%20enhance%20the%20model%27s%20robust%20compositional%0Acapabilities.%20To%20accommodate%20multiple%20reference%20images%20%28garments%20and%20faces%29%0Aseamlessly%2C%20we%20organize%20these%20references%20in%20a%20single%20image%20as%20an%20%22asset%0Alibrary%22%20and%20employ%20a%20reference%20UNet%20to%20extract%20appearance%20features.%20To%20inject%0Athe%20appearance%20features%20into%20the%20correct%20pixels%20in%20the%20generated%20result%2C%20we%0Apropose%20subject-binding%20attention.%20It%20binds%20the%20appearance%20features%20from%0Adifferent%20%22assets%22%20with%20the%20corresponding%20text%20features.%20In%20this%20way%2C%20the%20model%0Acould%20understand%20each%20asset%20according%20to%20their%20semantics%2C%20supporting%20arbitrary%0Anumbers%20and%20types%20of%20reference%20images.%20As%20a%20comprehensive%20solution%2C%0AFashionComposer%20also%20supports%20many%20other%20applications%20like%20human%20album%0Ageneration%2C%20diverse%20virtual%20try-on%20tasks%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14168v1&entry.124074799=Read"},
{"title": "KlF: Knowledge Localization and Fusion for Language Model Continual\n  Learning", "author": "Yujie Feng and Xu Chu and Yongxin Xu and Zexin Lu and Bo Liu and Philip S. Yu and Xiao-Ming Wu", "abstract": "  Language model continual learning (CL) has recently attracted significant\ninterest for its ability to adapt large language models (LLMs) to dynamic\nreal-world scenarios without retraining. A major challenge in this domain is\ncatastrophic forgetting, where models lose previously acquired knowledge upon\nlearning new tasks. Existing approaches commonly utilize multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge, yet these methods are inefficient and fail to leverage potential\nknowledge transfer across tasks. In this paper, we introduce a novel CL\nframework for language models, named Knowledge Localization and Fusion (KlF),\nwhich boosts knowledge transfer without depending on memory replay. KlF\ninitially segregates the model into 'skill units' based on parameter\ndependencies, allowing for more precise control. Subsequently, it employs a\nnovel group-wise knowledge localization technique to ascertain the importance\ndistribution of skill units for a new task. By comparing this importance\ndistribution with those from previous tasks, we implement a fine-grained\nknowledge fusion strategy that retains task-specific knowledge, thereby\npreventing forgetting, and updates task-shared knowledge, which facilitates\nbi-directional knowledge transfer. As a result, KlF achieves an optimal balance\nbetween retaining prior knowledge and excelling in new tasks. KlF also\ndemonstrates strong generalizability, making it suitable for various base\nmodels and adaptable to PEFT methods like LoRA. Furthermore, it offers notable\nextensibility, supporting enhancements through integration with memory replay\ntechniques. Comprehensive experiments conducted on two CL benchmarks, involving\nmodels ranging from 220M to 7B parameters, affirm the effectiveness of KlF and\nits variants across different settings.\n", "link": "http://arxiv.org/abs/2408.05200v3", "date": "2024-12-18", "relevancy": 2.5718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KlF%3A%20Knowledge%20Localization%20and%20Fusion%20for%20Language%20Model%20Continual%0A%20%20Learning&body=Title%3A%20KlF%3A%20Knowledge%20Localization%20and%20Fusion%20for%20Language%20Model%20Continual%0A%20%20Learning%0AAuthor%3A%20Yujie%20Feng%20and%20Xu%20Chu%20and%20Yongxin%20Xu%20and%20Zexin%20Lu%20and%20Bo%20Liu%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20%20%20Language%20model%20continual%20learning%20%28CL%29%20has%20recently%20attracted%20significant%0Ainterest%20for%20its%20ability%20to%20adapt%20large%20language%20models%20%28LLMs%29%20to%20dynamic%0Areal-world%20scenarios%20without%20retraining.%20A%20major%20challenge%20in%20this%20domain%20is%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20previously%20acquired%20knowledge%20upon%0Alearning%20new%20tasks.%20Existing%20approaches%20commonly%20utilize%20multiple%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20blocks%20to%20acquire%20task-specific%0Aknowledge%2C%20yet%20these%20methods%20are%20inefficient%20and%20fail%20to%20leverage%20potential%0Aknowledge%20transfer%20across%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20CL%0Aframework%20for%20language%20models%2C%20named%20Knowledge%20Localization%20and%20Fusion%20%28KlF%29%2C%0Awhich%20boosts%20knowledge%20transfer%20without%20depending%20on%20memory%20replay.%20KlF%0Ainitially%20segregates%20the%20model%20into%20%27skill%20units%27%20based%20on%20parameter%0Adependencies%2C%20allowing%20for%20more%20precise%20control.%20Subsequently%2C%20it%20employs%20a%0Anovel%20group-wise%20knowledge%20localization%20technique%20to%20ascertain%20the%20importance%0Adistribution%20of%20skill%20units%20for%20a%20new%20task.%20By%20comparing%20this%20importance%0Adistribution%20with%20those%20from%20previous%20tasks%2C%20we%20implement%20a%20fine-grained%0Aknowledge%20fusion%20strategy%20that%20retains%20task-specific%20knowledge%2C%20thereby%0Apreventing%20forgetting%2C%20and%20updates%20task-shared%20knowledge%2C%20which%20facilitates%0Abi-directional%20knowledge%20transfer.%20As%20a%20result%2C%20KlF%20achieves%20an%20optimal%20balance%0Abetween%20retaining%20prior%20knowledge%20and%20excelling%20in%20new%20tasks.%20KlF%20also%0Ademonstrates%20strong%20generalizability%2C%20making%20it%20suitable%20for%20various%20base%0Amodels%20and%20adaptable%20to%20PEFT%20methods%20like%20LoRA.%20Furthermore%2C%20it%20offers%20notable%0Aextensibility%2C%20supporting%20enhancements%20through%20integration%20with%20memory%20replay%0Atechniques.%20Comprehensive%20experiments%20conducted%20on%20two%20CL%20benchmarks%2C%20involving%0Amodels%20ranging%20from%20220M%20to%207B%20parameters%2C%20affirm%20the%20effectiveness%20of%20KlF%20and%0Aits%20variants%20across%20different%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKlF%253A%2520Knowledge%2520Localization%2520and%2520Fusion%2520for%2520Language%2520Model%2520Continual%250A%2520%2520Learning%26entry.906535625%3DYujie%2520Feng%2520and%2520Xu%2520Chu%2520and%2520Yongxin%2520Xu%2520and%2520Zexin%2520Lu%2520and%2520Bo%2520Liu%2520and%2520Philip%2520S.%2520Yu%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3D%2520%2520Language%2520model%2520continual%2520learning%2520%2528CL%2529%2520has%2520recently%2520attracted%2520significant%250Ainterest%2520for%2520its%2520ability%2520to%2520adapt%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520dynamic%250Areal-world%2520scenarios%2520without%2520retraining.%2520A%2520major%2520challenge%2520in%2520this%2520domain%2520is%250Acatastrophic%2520forgetting%252C%2520where%2520models%2520lose%2520previously%2520acquired%2520knowledge%2520upon%250Alearning%2520new%2520tasks.%2520Existing%2520approaches%2520commonly%2520utilize%2520multiple%250Aparameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520blocks%2520to%2520acquire%2520task-specific%250Aknowledge%252C%2520yet%2520these%2520methods%2520are%2520inefficient%2520and%2520fail%2520to%2520leverage%2520potential%250Aknowledge%2520transfer%2520across%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520CL%250Aframework%2520for%2520language%2520models%252C%2520named%2520Knowledge%2520Localization%2520and%2520Fusion%2520%2528KlF%2529%252C%250Awhich%2520boosts%2520knowledge%2520transfer%2520without%2520depending%2520on%2520memory%2520replay.%2520KlF%250Ainitially%2520segregates%2520the%2520model%2520into%2520%2527skill%2520units%2527%2520based%2520on%2520parameter%250Adependencies%252C%2520allowing%2520for%2520more%2520precise%2520control.%2520Subsequently%252C%2520it%2520employs%2520a%250Anovel%2520group-wise%2520knowledge%2520localization%2520technique%2520to%2520ascertain%2520the%2520importance%250Adistribution%2520of%2520skill%2520units%2520for%2520a%2520new%2520task.%2520By%2520comparing%2520this%2520importance%250Adistribution%2520with%2520those%2520from%2520previous%2520tasks%252C%2520we%2520implement%2520a%2520fine-grained%250Aknowledge%2520fusion%2520strategy%2520that%2520retains%2520task-specific%2520knowledge%252C%2520thereby%250Apreventing%2520forgetting%252C%2520and%2520updates%2520task-shared%2520knowledge%252C%2520which%2520facilitates%250Abi-directional%2520knowledge%2520transfer.%2520As%2520a%2520result%252C%2520KlF%2520achieves%2520an%2520optimal%2520balance%250Abetween%2520retaining%2520prior%2520knowledge%2520and%2520excelling%2520in%2520new%2520tasks.%2520KlF%2520also%250Ademonstrates%2520strong%2520generalizability%252C%2520making%2520it%2520suitable%2520for%2520various%2520base%250Amodels%2520and%2520adaptable%2520to%2520PEFT%2520methods%2520like%2520LoRA.%2520Furthermore%252C%2520it%2520offers%2520notable%250Aextensibility%252C%2520supporting%2520enhancements%2520through%2520integration%2520with%2520memory%2520replay%250Atechniques.%2520Comprehensive%2520experiments%2520conducted%2520on%2520two%2520CL%2520benchmarks%252C%2520involving%250Amodels%2520ranging%2520from%2520220M%2520to%25207B%2520parameters%252C%2520affirm%2520the%2520effectiveness%2520of%2520KlF%2520and%250Aits%2520variants%2520across%2520different%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KlF%3A%20Knowledge%20Localization%20and%20Fusion%20for%20Language%20Model%20Continual%0A%20%20Learning&entry.906535625=Yujie%20Feng%20and%20Xu%20Chu%20and%20Yongxin%20Xu%20and%20Zexin%20Lu%20and%20Bo%20Liu%20and%20Philip%20S.%20Yu%20and%20Xiao-Ming%20Wu&entry.1292438233=%20%20Language%20model%20continual%20learning%20%28CL%29%20has%20recently%20attracted%20significant%0Ainterest%20for%20its%20ability%20to%20adapt%20large%20language%20models%20%28LLMs%29%20to%20dynamic%0Areal-world%20scenarios%20without%20retraining.%20A%20major%20challenge%20in%20this%20domain%20is%0Acatastrophic%20forgetting%2C%20where%20models%20lose%20previously%20acquired%20knowledge%20upon%0Alearning%20new%20tasks.%20Existing%20approaches%20commonly%20utilize%20multiple%0Aparameter-efficient%20fine-tuning%20%28PEFT%29%20blocks%20to%20acquire%20task-specific%0Aknowledge%2C%20yet%20these%20methods%20are%20inefficient%20and%20fail%20to%20leverage%20potential%0Aknowledge%20transfer%20across%20tasks.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20CL%0Aframework%20for%20language%20models%2C%20named%20Knowledge%20Localization%20and%20Fusion%20%28KlF%29%2C%0Awhich%20boosts%20knowledge%20transfer%20without%20depending%20on%20memory%20replay.%20KlF%0Ainitially%20segregates%20the%20model%20into%20%27skill%20units%27%20based%20on%20parameter%0Adependencies%2C%20allowing%20for%20more%20precise%20control.%20Subsequently%2C%20it%20employs%20a%0Anovel%20group-wise%20knowledge%20localization%20technique%20to%20ascertain%20the%20importance%0Adistribution%20of%20skill%20units%20for%20a%20new%20task.%20By%20comparing%20this%20importance%0Adistribution%20with%20those%20from%20previous%20tasks%2C%20we%20implement%20a%20fine-grained%0Aknowledge%20fusion%20strategy%20that%20retains%20task-specific%20knowledge%2C%20thereby%0Apreventing%20forgetting%2C%20and%20updates%20task-shared%20knowledge%2C%20which%20facilitates%0Abi-directional%20knowledge%20transfer.%20As%20a%20result%2C%20KlF%20achieves%20an%20optimal%20balance%0Abetween%20retaining%20prior%20knowledge%20and%20excelling%20in%20new%20tasks.%20KlF%20also%0Ademonstrates%20strong%20generalizability%2C%20making%20it%20suitable%20for%20various%20base%0Amodels%20and%20adaptable%20to%20PEFT%20methods%20like%20LoRA.%20Furthermore%2C%20it%20offers%20notable%0Aextensibility%2C%20supporting%20enhancements%20through%20integration%20with%20memory%20replay%0Atechniques.%20Comprehensive%20experiments%20conducted%20on%20two%20CL%20benchmarks%2C%20involving%0Amodels%20ranging%20from%20220M%20to%207B%20parameters%2C%20affirm%20the%20effectiveness%20of%20KlF%20and%0Aits%20variants%20across%20different%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05200v3&entry.124074799=Read"},
{"title": "Parameter-efficient Fine-tuning for improved Convolutional Baseline for\n  Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset", "author": "Bijay Adhikari and Pratibha Kulung and Jakesh Bohaju and Laxmi Kanta Poudel and Confidence Raymond and Dong Zhang and Udunna C Anazodo and Bishesh Khanal and Mahesh Shakya", "abstract": "  Automating brain tumor segmentation using deep learning methods is an ongoing\nchallenge in medical imaging. Multiple lingering issues exist including\ndomain-shift and applications in low-resource settings which brings a unique\nset of challenges including scarcity of data. As a step towards solving these\nspecific problems, we propose Convolutional adapter-inspired\nParameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our\nidea, we show our method performs comparable to full fine-tuning with the added\nbenefit of reduced training compute using BraTS-2021 as pre-training dataset\nand BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small\ndataset (60 train / 35 validation) from the Sub-Saharan African population with\nmarked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We\nfirst show that models trained on BraTS-2021 dataset do not generalize well to\nBraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation\nsamples. Then, we show that PEFT can leverage both the BraTS-2021 and\nBraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained\nonly on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in\ncomparable performance to full fine-tuning (0.77 mean dice) which may show PEFT\nto be better on average but the boxplots show that full finetuning results is\nmuch lesser variance in performance. Nevertheless, on disaggregation of the\ndice metrics, we find that the model has tendency to oversegment as shown by\nhigh specificity (0.99) compared to relatively low sensitivity(0.75). The\nsource code is available at\nhttps://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt\n", "link": "http://arxiv.org/abs/2412.14100v1", "date": "2024-12-18", "relevancy": 2.5513, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5228}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5081}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-efficient%20Fine-tuning%20for%20improved%20Convolutional%20Baseline%20for%0A%20%20Brain%20Tumor%20Segmentation%20in%20Sub-Saharan%20Africa%20Adult%20Glioma%20Dataset&body=Title%3A%20Parameter-efficient%20Fine-tuning%20for%20improved%20Convolutional%20Baseline%20for%0A%20%20Brain%20Tumor%20Segmentation%20in%20Sub-Saharan%20Africa%20Adult%20Glioma%20Dataset%0AAuthor%3A%20Bijay%20Adhikari%20and%20Pratibha%20Kulung%20and%20Jakesh%20Bohaju%20and%20Laxmi%20Kanta%20Poudel%20and%20Confidence%20Raymond%20and%20Dong%20Zhang%20and%20Udunna%20C%20Anazodo%20and%20Bishesh%20Khanal%20and%20Mahesh%20Shakya%0AAbstract%3A%20%20%20Automating%20brain%20tumor%20segmentation%20using%20deep%20learning%20methods%20is%20an%20ongoing%0Achallenge%20in%20medical%20imaging.%20Multiple%20lingering%20issues%20exist%20including%0Adomain-shift%20and%20applications%20in%20low-resource%20settings%20which%20brings%20a%20unique%0Aset%20of%20challenges%20including%20scarcity%20of%20data.%20As%20a%20step%20towards%20solving%20these%0Aspecific%20problems%2C%20we%20propose%20Convolutional%20adapter-inspired%0AParameter-efficient%20Fine-tuning%20%28PEFT%29%20of%20MedNeXt%20architecture.%20To%20validate%20our%0Aidea%2C%20we%20show%20our%20method%20performs%20comparable%20to%20full%20fine-tuning%20with%20the%20added%0Abenefit%20of%20reduced%20training%20compute%20using%20BraTS-2021%20as%20pre-training%20dataset%0Aand%20BraTS-Africa%20as%20the%20fine-tuning%20dataset.%20BraTS-Africa%20consists%20of%20a%20small%0Adataset%20%2860%20train%20/%2035%20validation%29%20from%20the%20Sub-Saharan%20African%20population%20with%0Amarked%20shift%20in%20the%20MRI%20quality%20compared%20to%20BraTS-2021%20%281251%20train%20samples%29.%20We%0Afirst%20show%20that%20models%20trained%20on%20BraTS-2021%20dataset%20do%20not%20generalize%20well%20to%0ABraTS-Africa%20as%20shown%20by%2020%25%20reduction%20in%20mean%20dice%20on%20BraTS-Africa%20validation%0Asamples.%20Then%2C%20we%20show%20that%20PEFT%20can%20leverage%20both%20the%20BraTS-2021%20and%0ABraTS-Africa%20dataset%20to%20obtain%20mean%20dice%20of%200.8%20compared%20to%200.72%20when%20trained%0Aonly%20on%20BraTS-Africa.%20Finally%2C%20We%20show%20that%20PEFT%20%280.80%20mean%20dice%29%20results%20in%0Acomparable%20performance%20to%20full%20fine-tuning%20%280.77%20mean%20dice%29%20which%20may%20show%20PEFT%0Ato%20be%20better%20on%20average%20but%20the%20boxplots%20show%20that%20full%20finetuning%20results%20is%0Amuch%20lesser%20variance%20in%20performance.%20Nevertheless%2C%20on%20disaggregation%20of%20the%0Adice%20metrics%2C%20we%20find%20that%20the%20model%20has%20tendency%20to%20oversegment%20as%20shown%20by%0Ahigh%20specificity%20%280.99%29%20compared%20to%20relatively%20low%20sensitivity%280.75%29.%20The%0Asource%20code%20is%20available%20at%0Ahttps%3A//github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-efficient%2520Fine-tuning%2520for%2520improved%2520Convolutional%2520Baseline%2520for%250A%2520%2520Brain%2520Tumor%2520Segmentation%2520in%2520Sub-Saharan%2520Africa%2520Adult%2520Glioma%2520Dataset%26entry.906535625%3DBijay%2520Adhikari%2520and%2520Pratibha%2520Kulung%2520and%2520Jakesh%2520Bohaju%2520and%2520Laxmi%2520Kanta%2520Poudel%2520and%2520Confidence%2520Raymond%2520and%2520Dong%2520Zhang%2520and%2520Udunna%2520C%2520Anazodo%2520and%2520Bishesh%2520Khanal%2520and%2520Mahesh%2520Shakya%26entry.1292438233%3D%2520%2520Automating%2520brain%2520tumor%2520segmentation%2520using%2520deep%2520learning%2520methods%2520is%2520an%2520ongoing%250Achallenge%2520in%2520medical%2520imaging.%2520Multiple%2520lingering%2520issues%2520exist%2520including%250Adomain-shift%2520and%2520applications%2520in%2520low-resource%2520settings%2520which%2520brings%2520a%2520unique%250Aset%2520of%2520challenges%2520including%2520scarcity%2520of%2520data.%2520As%2520a%2520step%2520towards%2520solving%2520these%250Aspecific%2520problems%252C%2520we%2520propose%2520Convolutional%2520adapter-inspired%250AParameter-efficient%2520Fine-tuning%2520%2528PEFT%2529%2520of%2520MedNeXt%2520architecture.%2520To%2520validate%2520our%250Aidea%252C%2520we%2520show%2520our%2520method%2520performs%2520comparable%2520to%2520full%2520fine-tuning%2520with%2520the%2520added%250Abenefit%2520of%2520reduced%2520training%2520compute%2520using%2520BraTS-2021%2520as%2520pre-training%2520dataset%250Aand%2520BraTS-Africa%2520as%2520the%2520fine-tuning%2520dataset.%2520BraTS-Africa%2520consists%2520of%2520a%2520small%250Adataset%2520%252860%2520train%2520/%252035%2520validation%2529%2520from%2520the%2520Sub-Saharan%2520African%2520population%2520with%250Amarked%2520shift%2520in%2520the%2520MRI%2520quality%2520compared%2520to%2520BraTS-2021%2520%25281251%2520train%2520samples%2529.%2520We%250Afirst%2520show%2520that%2520models%2520trained%2520on%2520BraTS-2021%2520dataset%2520do%2520not%2520generalize%2520well%2520to%250ABraTS-Africa%2520as%2520shown%2520by%252020%2525%2520reduction%2520in%2520mean%2520dice%2520on%2520BraTS-Africa%2520validation%250Asamples.%2520Then%252C%2520we%2520show%2520that%2520PEFT%2520can%2520leverage%2520both%2520the%2520BraTS-2021%2520and%250ABraTS-Africa%2520dataset%2520to%2520obtain%2520mean%2520dice%2520of%25200.8%2520compared%2520to%25200.72%2520when%2520trained%250Aonly%2520on%2520BraTS-Africa.%2520Finally%252C%2520We%2520show%2520that%2520PEFT%2520%25280.80%2520mean%2520dice%2529%2520results%2520in%250Acomparable%2520performance%2520to%2520full%2520fine-tuning%2520%25280.77%2520mean%2520dice%2529%2520which%2520may%2520show%2520PEFT%250Ato%2520be%2520better%2520on%2520average%2520but%2520the%2520boxplots%2520show%2520that%2520full%2520finetuning%2520results%2520is%250Amuch%2520lesser%2520variance%2520in%2520performance.%2520Nevertheless%252C%2520on%2520disaggregation%2520of%2520the%250Adice%2520metrics%252C%2520we%2520find%2520that%2520the%2520model%2520has%2520tendency%2520to%2520oversegment%2520as%2520shown%2520by%250Ahigh%2520specificity%2520%25280.99%2529%2520compared%2520to%2520relatively%2520low%2520sensitivity%25280.75%2529.%2520The%250Asource%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-efficient%20Fine-tuning%20for%20improved%20Convolutional%20Baseline%20for%0A%20%20Brain%20Tumor%20Segmentation%20in%20Sub-Saharan%20Africa%20Adult%20Glioma%20Dataset&entry.906535625=Bijay%20Adhikari%20and%20Pratibha%20Kulung%20and%20Jakesh%20Bohaju%20and%20Laxmi%20Kanta%20Poudel%20and%20Confidence%20Raymond%20and%20Dong%20Zhang%20and%20Udunna%20C%20Anazodo%20and%20Bishesh%20Khanal%20and%20Mahesh%20Shakya&entry.1292438233=%20%20Automating%20brain%20tumor%20segmentation%20using%20deep%20learning%20methods%20is%20an%20ongoing%0Achallenge%20in%20medical%20imaging.%20Multiple%20lingering%20issues%20exist%20including%0Adomain-shift%20and%20applications%20in%20low-resource%20settings%20which%20brings%20a%20unique%0Aset%20of%20challenges%20including%20scarcity%20of%20data.%20As%20a%20step%20towards%20solving%20these%0Aspecific%20problems%2C%20we%20propose%20Convolutional%20adapter-inspired%0AParameter-efficient%20Fine-tuning%20%28PEFT%29%20of%20MedNeXt%20architecture.%20To%20validate%20our%0Aidea%2C%20we%20show%20our%20method%20performs%20comparable%20to%20full%20fine-tuning%20with%20the%20added%0Abenefit%20of%20reduced%20training%20compute%20using%20BraTS-2021%20as%20pre-training%20dataset%0Aand%20BraTS-Africa%20as%20the%20fine-tuning%20dataset.%20BraTS-Africa%20consists%20of%20a%20small%0Adataset%20%2860%20train%20/%2035%20validation%29%20from%20the%20Sub-Saharan%20African%20population%20with%0Amarked%20shift%20in%20the%20MRI%20quality%20compared%20to%20BraTS-2021%20%281251%20train%20samples%29.%20We%0Afirst%20show%20that%20models%20trained%20on%20BraTS-2021%20dataset%20do%20not%20generalize%20well%20to%0ABraTS-Africa%20as%20shown%20by%2020%25%20reduction%20in%20mean%20dice%20on%20BraTS-Africa%20validation%0Asamples.%20Then%2C%20we%20show%20that%20PEFT%20can%20leverage%20both%20the%20BraTS-2021%20and%0ABraTS-Africa%20dataset%20to%20obtain%20mean%20dice%20of%200.8%20compared%20to%200.72%20when%20trained%0Aonly%20on%20BraTS-Africa.%20Finally%2C%20We%20show%20that%20PEFT%20%280.80%20mean%20dice%29%20results%20in%0Acomparable%20performance%20to%20full%20fine-tuning%20%280.77%20mean%20dice%29%20which%20may%20show%20PEFT%0Ato%20be%20better%20on%20average%20but%20the%20boxplots%20show%20that%20full%20finetuning%20results%20is%0Amuch%20lesser%20variance%20in%20performance.%20Nevertheless%2C%20on%20disaggregation%20of%20the%0Adice%20metrics%2C%20we%20find%20that%20the%20model%20has%20tendency%20to%20oversegment%20as%20shown%20by%0Ahigh%20specificity%20%280.99%29%20compared%20to%20relatively%20low%20sensitivity%280.75%29.%20The%0Asource%20code%20is%20available%20at%0Ahttps%3A//github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14100v1&entry.124074799=Read"},
{"title": "Understanding and Analyzing Model Robustness and Knowledge-Transfer in\n  Multilingual Neural Machine Translation using TX-Ray", "author": "Vageesh Saxena and Sharid Lo\u00e1iciga and Nils Rethmeier", "abstract": "  Neural networks have demonstrated significant advancements in Neural Machine\nTranslation (NMT) compared to conventional phrase-based approaches. However,\nMultilingual Neural Machine Translation (MNMT) in extremely low-resource\nsettings remains underexplored. This research investigates how knowledge\ntransfer across languages can enhance MNMT in such scenarios. Using the Tatoeba\ntranslation challenge dataset from Helsinki NLP, we perform English-German,\nEnglish-French, and English-Spanish translations, leveraging minimal parallel\ndata to establish cross-lingual mappings. Unlike conventional methods relying\non extensive pre-training for specific language pairs, we pre-train our model\non English-English translations, setting English as the source language for all\ntasks. The model is fine-tuned on target language pairs using joint multi-task\nand sequential transfer learning strategies. Our work addresses three key\nquestions: (1) How can knowledge transfer across languages improve MNMT in\nextremely low-resource scenarios? (2) How does pruning neuron knowledge affect\nmodel generalization, robustness, and catastrophic forgetting? (3) How can\nTX-Ray interpret and quantify knowledge transfer in trained models? Evaluation\nusing BLEU-4 scores demonstrates that sequential transfer learning outperforms\nbaselines on a 40k parallel sentence corpus, showcasing its efficacy. However,\npruning neuron knowledge degrades performance, increases catastrophic\nforgetting, and fails to improve robustness or generalization. Our findings\nprovide valuable insights into the potential and limitations of knowledge\ntransfer and pruning in MNMT for extremely low-resource settings.\n", "link": "http://arxiv.org/abs/2412.13881v1", "date": "2024-12-18", "relevancy": 2.5378, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5098}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Analyzing%20Model%20Robustness%20and%20Knowledge-Transfer%20in%0A%20%20Multilingual%20Neural%20Machine%20Translation%20using%20TX-Ray&body=Title%3A%20Understanding%20and%20Analyzing%20Model%20Robustness%20and%20Knowledge-Transfer%20in%0A%20%20Multilingual%20Neural%20Machine%20Translation%20using%20TX-Ray%0AAuthor%3A%20Vageesh%20Saxena%20and%20Sharid%20Lo%C3%A1iciga%20and%20Nils%20Rethmeier%0AAbstract%3A%20%20%20Neural%20networks%20have%20demonstrated%20significant%20advancements%20in%20Neural%20Machine%0ATranslation%20%28NMT%29%20compared%20to%20conventional%20phrase-based%20approaches.%20However%2C%0AMultilingual%20Neural%20Machine%20Translation%20%28MNMT%29%20in%20extremely%20low-resource%0Asettings%20remains%20underexplored.%20This%20research%20investigates%20how%20knowledge%0Atransfer%20across%20languages%20can%20enhance%20MNMT%20in%20such%20scenarios.%20Using%20the%20Tatoeba%0Atranslation%20challenge%20dataset%20from%20Helsinki%20NLP%2C%20we%20perform%20English-German%2C%0AEnglish-French%2C%20and%20English-Spanish%20translations%2C%20leveraging%20minimal%20parallel%0Adata%20to%20establish%20cross-lingual%20mappings.%20Unlike%20conventional%20methods%20relying%0Aon%20extensive%20pre-training%20for%20specific%20language%20pairs%2C%20we%20pre-train%20our%20model%0Aon%20English-English%20translations%2C%20setting%20English%20as%20the%20source%20language%20for%20all%0Atasks.%20The%20model%20is%20fine-tuned%20on%20target%20language%20pairs%20using%20joint%20multi-task%0Aand%20sequential%20transfer%20learning%20strategies.%20Our%20work%20addresses%20three%20key%0Aquestions%3A%20%281%29%20How%20can%20knowledge%20transfer%20across%20languages%20improve%20MNMT%20in%0Aextremely%20low-resource%20scenarios%3F%20%282%29%20How%20does%20pruning%20neuron%20knowledge%20affect%0Amodel%20generalization%2C%20robustness%2C%20and%20catastrophic%20forgetting%3F%20%283%29%20How%20can%0ATX-Ray%20interpret%20and%20quantify%20knowledge%20transfer%20in%20trained%20models%3F%20Evaluation%0Ausing%20BLEU-4%20scores%20demonstrates%20that%20sequential%20transfer%20learning%20outperforms%0Abaselines%20on%20a%2040k%20parallel%20sentence%20corpus%2C%20showcasing%20its%20efficacy.%20However%2C%0Apruning%20neuron%20knowledge%20degrades%20performance%2C%20increases%20catastrophic%0Aforgetting%2C%20and%20fails%20to%20improve%20robustness%20or%20generalization.%20Our%20findings%0Aprovide%20valuable%20insights%20into%20the%20potential%20and%20limitations%20of%20knowledge%0Atransfer%20and%20pruning%20in%20MNMT%20for%20extremely%20low-resource%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Analyzing%2520Model%2520Robustness%2520and%2520Knowledge-Transfer%2520in%250A%2520%2520Multilingual%2520Neural%2520Machine%2520Translation%2520using%2520TX-Ray%26entry.906535625%3DVageesh%2520Saxena%2520and%2520Sharid%2520Lo%25C3%25A1iciga%2520and%2520Nils%2520Rethmeier%26entry.1292438233%3D%2520%2520Neural%2520networks%2520have%2520demonstrated%2520significant%2520advancements%2520in%2520Neural%2520Machine%250ATranslation%2520%2528NMT%2529%2520compared%2520to%2520conventional%2520phrase-based%2520approaches.%2520However%252C%250AMultilingual%2520Neural%2520Machine%2520Translation%2520%2528MNMT%2529%2520in%2520extremely%2520low-resource%250Asettings%2520remains%2520underexplored.%2520This%2520research%2520investigates%2520how%2520knowledge%250Atransfer%2520across%2520languages%2520can%2520enhance%2520MNMT%2520in%2520such%2520scenarios.%2520Using%2520the%2520Tatoeba%250Atranslation%2520challenge%2520dataset%2520from%2520Helsinki%2520NLP%252C%2520we%2520perform%2520English-German%252C%250AEnglish-French%252C%2520and%2520English-Spanish%2520translations%252C%2520leveraging%2520minimal%2520parallel%250Adata%2520to%2520establish%2520cross-lingual%2520mappings.%2520Unlike%2520conventional%2520methods%2520relying%250Aon%2520extensive%2520pre-training%2520for%2520specific%2520language%2520pairs%252C%2520we%2520pre-train%2520our%2520model%250Aon%2520English-English%2520translations%252C%2520setting%2520English%2520as%2520the%2520source%2520language%2520for%2520all%250Atasks.%2520The%2520model%2520is%2520fine-tuned%2520on%2520target%2520language%2520pairs%2520using%2520joint%2520multi-task%250Aand%2520sequential%2520transfer%2520learning%2520strategies.%2520Our%2520work%2520addresses%2520three%2520key%250Aquestions%253A%2520%25281%2529%2520How%2520can%2520knowledge%2520transfer%2520across%2520languages%2520improve%2520MNMT%2520in%250Aextremely%2520low-resource%2520scenarios%253F%2520%25282%2529%2520How%2520does%2520pruning%2520neuron%2520knowledge%2520affect%250Amodel%2520generalization%252C%2520robustness%252C%2520and%2520catastrophic%2520forgetting%253F%2520%25283%2529%2520How%2520can%250ATX-Ray%2520interpret%2520and%2520quantify%2520knowledge%2520transfer%2520in%2520trained%2520models%253F%2520Evaluation%250Ausing%2520BLEU-4%2520scores%2520demonstrates%2520that%2520sequential%2520transfer%2520learning%2520outperforms%250Abaselines%2520on%2520a%252040k%2520parallel%2520sentence%2520corpus%252C%2520showcasing%2520its%2520efficacy.%2520However%252C%250Apruning%2520neuron%2520knowledge%2520degrades%2520performance%252C%2520increases%2520catastrophic%250Aforgetting%252C%2520and%2520fails%2520to%2520improve%2520robustness%2520or%2520generalization.%2520Our%2520findings%250Aprovide%2520valuable%2520insights%2520into%2520the%2520potential%2520and%2520limitations%2520of%2520knowledge%250Atransfer%2520and%2520pruning%2520in%2520MNMT%2520for%2520extremely%2520low-resource%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Analyzing%20Model%20Robustness%20and%20Knowledge-Transfer%20in%0A%20%20Multilingual%20Neural%20Machine%20Translation%20using%20TX-Ray&entry.906535625=Vageesh%20Saxena%20and%20Sharid%20Lo%C3%A1iciga%20and%20Nils%20Rethmeier&entry.1292438233=%20%20Neural%20networks%20have%20demonstrated%20significant%20advancements%20in%20Neural%20Machine%0ATranslation%20%28NMT%29%20compared%20to%20conventional%20phrase-based%20approaches.%20However%2C%0AMultilingual%20Neural%20Machine%20Translation%20%28MNMT%29%20in%20extremely%20low-resource%0Asettings%20remains%20underexplored.%20This%20research%20investigates%20how%20knowledge%0Atransfer%20across%20languages%20can%20enhance%20MNMT%20in%20such%20scenarios.%20Using%20the%20Tatoeba%0Atranslation%20challenge%20dataset%20from%20Helsinki%20NLP%2C%20we%20perform%20English-German%2C%0AEnglish-French%2C%20and%20English-Spanish%20translations%2C%20leveraging%20minimal%20parallel%0Adata%20to%20establish%20cross-lingual%20mappings.%20Unlike%20conventional%20methods%20relying%0Aon%20extensive%20pre-training%20for%20specific%20language%20pairs%2C%20we%20pre-train%20our%20model%0Aon%20English-English%20translations%2C%20setting%20English%20as%20the%20source%20language%20for%20all%0Atasks.%20The%20model%20is%20fine-tuned%20on%20target%20language%20pairs%20using%20joint%20multi-task%0Aand%20sequential%20transfer%20learning%20strategies.%20Our%20work%20addresses%20three%20key%0Aquestions%3A%20%281%29%20How%20can%20knowledge%20transfer%20across%20languages%20improve%20MNMT%20in%0Aextremely%20low-resource%20scenarios%3F%20%282%29%20How%20does%20pruning%20neuron%20knowledge%20affect%0Amodel%20generalization%2C%20robustness%2C%20and%20catastrophic%20forgetting%3F%20%283%29%20How%20can%0ATX-Ray%20interpret%20and%20quantify%20knowledge%20transfer%20in%20trained%20models%3F%20Evaluation%0Ausing%20BLEU-4%20scores%20demonstrates%20that%20sequential%20transfer%20learning%20outperforms%0Abaselines%20on%20a%2040k%20parallel%20sentence%20corpus%2C%20showcasing%20its%20efficacy.%20However%2C%0Apruning%20neuron%20knowledge%20degrades%20performance%2C%20increases%20catastrophic%0Aforgetting%2C%20and%20fails%20to%20improve%20robustness%20or%20generalization.%20Our%20findings%0Aprovide%20valuable%20insights%20into%20the%20potential%20and%20limitations%20of%20knowledge%0Atransfer%20and%20pruning%20in%20MNMT%20for%20extremely%20low-resource%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13881v1&entry.124074799=Read"},
{"title": "Autoregressive Video Generation without Vector Quantization", "author": "Haoge Deng and Ting Pan and Haiwen Diao and Zhengxiong Luo and Yufeng Cui and Huchuan Lu and Shiguang Shan and Yonggang Qi and Xinlong Wang", "abstract": "  This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA.\n", "link": "http://arxiv.org/abs/2412.14169v1", "date": "2024-12-18", "relevancy": 2.5374, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6405}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6343}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Video%20Generation%20without%20Vector%20Quantization&body=Title%3A%20Autoregressive%20Video%20Generation%20without%20Vector%20Quantization%0AAuthor%3A%20Haoge%20Deng%20and%20Ting%20Pan%20and%20Haiwen%20Diao%20and%20Zhengxiong%20Luo%20and%20Yufeng%20Cui%20and%20Huchuan%20Lu%20and%20Shiguang%20Shan%20and%20Yonggang%20Qi%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20that%20enables%20autoregressive%20video%0Ageneration%20with%20high%20efficiency.%20We%20propose%20to%20reformulate%20the%20video%20generation%0Aproblem%20as%20a%20non-quantized%20autoregressive%20modeling%20of%20temporal%20frame-by-frame%0Aprediction%20and%20spatial%20set-by-set%20prediction.%20Unlike%20raster-scan%20prediction%20in%0Aprior%20autoregressive%20models%20or%20joint%20distribution%20modeling%20of%20fixed-length%0Atokens%20in%20diffusion%20models%2C%20our%20approach%20maintains%20the%20causal%20property%20of%0AGPT-style%20models%20for%20flexible%20in-context%20capabilities%2C%20while%20leveraging%0Abidirectional%20modeling%20within%20individual%20frames%20for%20efficiency.%20With%20the%0Aproposed%20approach%2C%20we%20train%20a%20novel%20video%20autoregressive%20model%20without%20vector%0Aquantization%2C%20termed%20NOVA.%20Our%20results%20demonstrate%20that%20NOVA%20surpasses%20prior%0Aautoregressive%20video%20models%20in%20data%20efficiency%2C%20inference%20speed%2C%20visual%0Afidelity%2C%20and%20video%20fluency%2C%20even%20with%20a%20much%20smaller%20model%20capacity%2C%20i.e.%2C%0A0.6B%20parameters.%20NOVA%20also%20outperforms%20state-of-the-art%20image%20diffusion%20models%0Ain%20text-to-image%20generation%20tasks%2C%20with%20a%20significantly%20lower%20training%20cost.%0AAdditionally%2C%20NOVA%20generalizes%20well%20across%20extended%20video%20durations%20and%20enables%0Adiverse%20zero-shot%20applications%20in%20one%20unified%20model.%20Code%20and%20models%20are%0Apublicly%20available%20at%20https%3A//github.com/baaivision/NOVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14169v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Video%2520Generation%2520without%2520Vector%2520Quantization%26entry.906535625%3DHaoge%2520Deng%2520and%2520Ting%2520Pan%2520and%2520Haiwen%2520Diao%2520and%2520Zhengxiong%2520Luo%2520and%2520Yufeng%2520Cui%2520and%2520Huchuan%2520Lu%2520and%2520Shiguang%2520Shan%2520and%2520Yonggang%2520Qi%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520that%2520enables%2520autoregressive%2520video%250Ageneration%2520with%2520high%2520efficiency.%2520We%2520propose%2520to%2520reformulate%2520the%2520video%2520generation%250Aproblem%2520as%2520a%2520non-quantized%2520autoregressive%2520modeling%2520of%2520temporal%2520frame-by-frame%250Aprediction%2520and%2520spatial%2520set-by-set%2520prediction.%2520Unlike%2520raster-scan%2520prediction%2520in%250Aprior%2520autoregressive%2520models%2520or%2520joint%2520distribution%2520modeling%2520of%2520fixed-length%250Atokens%2520in%2520diffusion%2520models%252C%2520our%2520approach%2520maintains%2520the%2520causal%2520property%2520of%250AGPT-style%2520models%2520for%2520flexible%2520in-context%2520capabilities%252C%2520while%2520leveraging%250Abidirectional%2520modeling%2520within%2520individual%2520frames%2520for%2520efficiency.%2520With%2520the%250Aproposed%2520approach%252C%2520we%2520train%2520a%2520novel%2520video%2520autoregressive%2520model%2520without%2520vector%250Aquantization%252C%2520termed%2520NOVA.%2520Our%2520results%2520demonstrate%2520that%2520NOVA%2520surpasses%2520prior%250Aautoregressive%2520video%2520models%2520in%2520data%2520efficiency%252C%2520inference%2520speed%252C%2520visual%250Afidelity%252C%2520and%2520video%2520fluency%252C%2520even%2520with%2520a%2520much%2520smaller%2520model%2520capacity%252C%2520i.e.%252C%250A0.6B%2520parameters.%2520NOVA%2520also%2520outperforms%2520state-of-the-art%2520image%2520diffusion%2520models%250Ain%2520text-to-image%2520generation%2520tasks%252C%2520with%2520a%2520significantly%2520lower%2520training%2520cost.%250AAdditionally%252C%2520NOVA%2520generalizes%2520well%2520across%2520extended%2520video%2520durations%2520and%2520enables%250Adiverse%2520zero-shot%2520applications%2520in%2520one%2520unified%2520model.%2520Code%2520and%2520models%2520are%250Apublicly%2520available%2520at%2520https%253A//github.com/baaivision/NOVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14169v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Video%20Generation%20without%20Vector%20Quantization&entry.906535625=Haoge%20Deng%20and%20Ting%20Pan%20and%20Haiwen%20Diao%20and%20Zhengxiong%20Luo%20and%20Yufeng%20Cui%20and%20Huchuan%20Lu%20and%20Shiguang%20Shan%20and%20Yonggang%20Qi%20and%20Xinlong%20Wang&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20that%20enables%20autoregressive%20video%0Ageneration%20with%20high%20efficiency.%20We%20propose%20to%20reformulate%20the%20video%20generation%0Aproblem%20as%20a%20non-quantized%20autoregressive%20modeling%20of%20temporal%20frame-by-frame%0Aprediction%20and%20spatial%20set-by-set%20prediction.%20Unlike%20raster-scan%20prediction%20in%0Aprior%20autoregressive%20models%20or%20joint%20distribution%20modeling%20of%20fixed-length%0Atokens%20in%20diffusion%20models%2C%20our%20approach%20maintains%20the%20causal%20property%20of%0AGPT-style%20models%20for%20flexible%20in-context%20capabilities%2C%20while%20leveraging%0Abidirectional%20modeling%20within%20individual%20frames%20for%20efficiency.%20With%20the%0Aproposed%20approach%2C%20we%20train%20a%20novel%20video%20autoregressive%20model%20without%20vector%0Aquantization%2C%20termed%20NOVA.%20Our%20results%20demonstrate%20that%20NOVA%20surpasses%20prior%0Aautoregressive%20video%20models%20in%20data%20efficiency%2C%20inference%20speed%2C%20visual%0Afidelity%2C%20and%20video%20fluency%2C%20even%20with%20a%20much%20smaller%20model%20capacity%2C%20i.e.%2C%0A0.6B%20parameters.%20NOVA%20also%20outperforms%20state-of-the-art%20image%20diffusion%20models%0Ain%20text-to-image%20generation%20tasks%2C%20with%20a%20significantly%20lower%20training%20cost.%0AAdditionally%2C%20NOVA%20generalizes%20well%20across%20extended%20video%20durations%20and%20enables%0Adiverse%20zero-shot%20applications%20in%20one%20unified%20model.%20Code%20and%20models%20are%0Apublicly%20available%20at%20https%3A//github.com/baaivision/NOVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14169v1&entry.124074799=Read"},
{"title": "Denoising Nearest Neighbor Graph via Continuous CRF for Visual\n  Re-ranking without Fine-tuning", "author": "Jaeyoon Kim and Yoonki Cho and Taeyong Kim and Sung-Eui Yoon", "abstract": "  Visual re-ranking using Nearest Neighbor graph~(NN graph) has been adapted to\nyield high retrieval accuracy, since it is beneficial to exploring an\nhigh-dimensional manifold and applicable without additional fine-tuning. The\nquality of visual re-ranking using NN graph, however, is limited to that of\nconnectivity, i.e., edges of the NN graph. Some edges can be misconnected with\nnegative images. This is known as a noisy edge problem, resulting in a\ndegradation of the retrieval quality. To address this, we propose a\ncomplementary denoising method based on Continuous Conditional Random Field\n(C-CRF) that uses a statistical distance of our similarity-based distribution.\nThis method employs the concept of cliques to make the process computationally\nfeasible. We demonstrate the complementarity of our method through its\napplication to three visual re-ranking methods, observing quality boosts in\nlandmark retrieval and person re-identification (re-ID).\n", "link": "http://arxiv.org/abs/2412.13875v1", "date": "2024-12-18", "relevancy": 2.5321, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5413}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4921}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20Nearest%20Neighbor%20Graph%20via%20Continuous%20CRF%20for%20Visual%0A%20%20Re-ranking%20without%20Fine-tuning&body=Title%3A%20Denoising%20Nearest%20Neighbor%20Graph%20via%20Continuous%20CRF%20for%20Visual%0A%20%20Re-ranking%20without%20Fine-tuning%0AAuthor%3A%20Jaeyoon%20Kim%20and%20Yoonki%20Cho%20and%20Taeyong%20Kim%20and%20Sung-Eui%20Yoon%0AAbstract%3A%20%20%20Visual%20re-ranking%20using%20Nearest%20Neighbor%20graph~%28NN%20graph%29%20has%20been%20adapted%20to%0Ayield%20high%20retrieval%20accuracy%2C%20since%20it%20is%20beneficial%20to%20exploring%20an%0Ahigh-dimensional%20manifold%20and%20applicable%20without%20additional%20fine-tuning.%20The%0Aquality%20of%20visual%20re-ranking%20using%20NN%20graph%2C%20however%2C%20is%20limited%20to%20that%20of%0Aconnectivity%2C%20i.e.%2C%20edges%20of%20the%20NN%20graph.%20Some%20edges%20can%20be%20misconnected%20with%0Anegative%20images.%20This%20is%20known%20as%20a%20noisy%20edge%20problem%2C%20resulting%20in%20a%0Adegradation%20of%20the%20retrieval%20quality.%20To%20address%20this%2C%20we%20propose%20a%0Acomplementary%20denoising%20method%20based%20on%20Continuous%20Conditional%20Random%20Field%0A%28C-CRF%29%20that%20uses%20a%20statistical%20distance%20of%20our%20similarity-based%20distribution.%0AThis%20method%20employs%20the%20concept%20of%20cliques%20to%20make%20the%20process%20computationally%0Afeasible.%20We%20demonstrate%20the%20complementarity%20of%20our%20method%20through%20its%0Aapplication%20to%20three%20visual%20re-ranking%20methods%2C%20observing%20quality%20boosts%20in%0Alandmark%20retrieval%20and%20person%20re-identification%20%28re-ID%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520Nearest%2520Neighbor%2520Graph%2520via%2520Continuous%2520CRF%2520for%2520Visual%250A%2520%2520Re-ranking%2520without%2520Fine-tuning%26entry.906535625%3DJaeyoon%2520Kim%2520and%2520Yoonki%2520Cho%2520and%2520Taeyong%2520Kim%2520and%2520Sung-Eui%2520Yoon%26entry.1292438233%3D%2520%2520Visual%2520re-ranking%2520using%2520Nearest%2520Neighbor%2520graph~%2528NN%2520graph%2529%2520has%2520been%2520adapted%2520to%250Ayield%2520high%2520retrieval%2520accuracy%252C%2520since%2520it%2520is%2520beneficial%2520to%2520exploring%2520an%250Ahigh-dimensional%2520manifold%2520and%2520applicable%2520without%2520additional%2520fine-tuning.%2520The%250Aquality%2520of%2520visual%2520re-ranking%2520using%2520NN%2520graph%252C%2520however%252C%2520is%2520limited%2520to%2520that%2520of%250Aconnectivity%252C%2520i.e.%252C%2520edges%2520of%2520the%2520NN%2520graph.%2520Some%2520edges%2520can%2520be%2520misconnected%2520with%250Anegative%2520images.%2520This%2520is%2520known%2520as%2520a%2520noisy%2520edge%2520problem%252C%2520resulting%2520in%2520a%250Adegradation%2520of%2520the%2520retrieval%2520quality.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Acomplementary%2520denoising%2520method%2520based%2520on%2520Continuous%2520Conditional%2520Random%2520Field%250A%2528C-CRF%2529%2520that%2520uses%2520a%2520statistical%2520distance%2520of%2520our%2520similarity-based%2520distribution.%250AThis%2520method%2520employs%2520the%2520concept%2520of%2520cliques%2520to%2520make%2520the%2520process%2520computationally%250Afeasible.%2520We%2520demonstrate%2520the%2520complementarity%2520of%2520our%2520method%2520through%2520its%250Aapplication%2520to%2520three%2520visual%2520re-ranking%2520methods%252C%2520observing%2520quality%2520boosts%2520in%250Alandmark%2520retrieval%2520and%2520person%2520re-identification%2520%2528re-ID%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20Nearest%20Neighbor%20Graph%20via%20Continuous%20CRF%20for%20Visual%0A%20%20Re-ranking%20without%20Fine-tuning&entry.906535625=Jaeyoon%20Kim%20and%20Yoonki%20Cho%20and%20Taeyong%20Kim%20and%20Sung-Eui%20Yoon&entry.1292438233=%20%20Visual%20re-ranking%20using%20Nearest%20Neighbor%20graph~%28NN%20graph%29%20has%20been%20adapted%20to%0Ayield%20high%20retrieval%20accuracy%2C%20since%20it%20is%20beneficial%20to%20exploring%20an%0Ahigh-dimensional%20manifold%20and%20applicable%20without%20additional%20fine-tuning.%20The%0Aquality%20of%20visual%20re-ranking%20using%20NN%20graph%2C%20however%2C%20is%20limited%20to%20that%20of%0Aconnectivity%2C%20i.e.%2C%20edges%20of%20the%20NN%20graph.%20Some%20edges%20can%20be%20misconnected%20with%0Anegative%20images.%20This%20is%20known%20as%20a%20noisy%20edge%20problem%2C%20resulting%20in%20a%0Adegradation%20of%20the%20retrieval%20quality.%20To%20address%20this%2C%20we%20propose%20a%0Acomplementary%20denoising%20method%20based%20on%20Continuous%20Conditional%20Random%20Field%0A%28C-CRF%29%20that%20uses%20a%20statistical%20distance%20of%20our%20similarity-based%20distribution.%0AThis%20method%20employs%20the%20concept%20of%20cliques%20to%20make%20the%20process%20computationally%0Afeasible.%20We%20demonstrate%20the%20complementarity%20of%20our%20method%20through%20its%0Aapplication%20to%20three%20visual%20re-ranking%20methods%2C%20observing%20quality%20boosts%20in%0Alandmark%20retrieval%20and%20person%20re-identification%20%28re-ID%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13875v1&entry.124074799=Read"},
{"title": "AnchorInv: Few-Shot Class-Incremental Learning of Physiological Signals\n  via Representation Space Guided Inversion", "author": "Chenqi Li and Boyan Gao and Gabriel Jones and Timothy Denison and Tingting Zhu", "abstract": "  Deep learning models have demonstrated exceptional performance in a variety\nof real-world applications. These successes are often attributed to strong base\nmodels that can generalize to novel tasks with limited supporting data while\nkeeping prior knowledge intact. However, these impressive results are based on\nthe availability of a large amount of high-quality data, which is often lacking\nin specialized biomedical applications. In such fields, models are usually\ndeveloped with limited data that arrive incrementally with novel categories.\nThis requires the model to adapt to new information while preserving existing\nknowledge. Few-Shot Class-Incremental Learning (FSCIL) methods offer a\npromising approach to addressing these challenges, but they also depend on\nstrong base models that face the same aforementioned limitations. To overcome\nthese constraints, we propose AnchorInv following the straightforward and\nefficient buffer-replay strategy. Instead of selecting and storing raw data,\nAnchorInv generates synthetic samples guided by anchor points in the feature\nspace. This approach protects privacy and regularizes the model for adaptation.\nWhen evaluated on three public physiological time series datasets, AnchorInv\nexhibits efficient knowledge forgetting prevention and improved adaptation to\nnovel classes, surpassing state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2412.13714v1", "date": "2024-12-18", "relevancy": 2.5034, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5008}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnchorInv%3A%20Few-Shot%20Class-Incremental%20Learning%20of%20Physiological%20Signals%0A%20%20via%20Representation%20Space%20Guided%20Inversion&body=Title%3A%20AnchorInv%3A%20Few-Shot%20Class-Incremental%20Learning%20of%20Physiological%20Signals%0A%20%20via%20Representation%20Space%20Guided%20Inversion%0AAuthor%3A%20Chenqi%20Li%20and%20Boyan%20Gao%20and%20Gabriel%20Jones%20and%20Timothy%20Denison%20and%20Tingting%20Zhu%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20demonstrated%20exceptional%20performance%20in%20a%20variety%0Aof%20real-world%20applications.%20These%20successes%20are%20often%20attributed%20to%20strong%20base%0Amodels%20that%20can%20generalize%20to%20novel%20tasks%20with%20limited%20supporting%20data%20while%0Akeeping%20prior%20knowledge%20intact.%20However%2C%20these%20impressive%20results%20are%20based%20on%0Athe%20availability%20of%20a%20large%20amount%20of%20high-quality%20data%2C%20which%20is%20often%20lacking%0Ain%20specialized%20biomedical%20applications.%20In%20such%20fields%2C%20models%20are%20usually%0Adeveloped%20with%20limited%20data%20that%20arrive%20incrementally%20with%20novel%20categories.%0AThis%20requires%20the%20model%20to%20adapt%20to%20new%20information%20while%20preserving%20existing%0Aknowledge.%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20methods%20offer%20a%0Apromising%20approach%20to%20addressing%20these%20challenges%2C%20but%20they%20also%20depend%20on%0Astrong%20base%20models%20that%20face%20the%20same%20aforementioned%20limitations.%20To%20overcome%0Athese%20constraints%2C%20we%20propose%20AnchorInv%20following%20the%20straightforward%20and%0Aefficient%20buffer-replay%20strategy.%20Instead%20of%20selecting%20and%20storing%20raw%20data%2C%0AAnchorInv%20generates%20synthetic%20samples%20guided%20by%20anchor%20points%20in%20the%20feature%0Aspace.%20This%20approach%20protects%20privacy%20and%20regularizes%20the%20model%20for%20adaptation.%0AWhen%20evaluated%20on%20three%20public%20physiological%20time%20series%20datasets%2C%20AnchorInv%0Aexhibits%20efficient%20knowledge%20forgetting%20prevention%20and%20improved%20adaptation%20to%0Anovel%20classes%2C%20surpassing%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnchorInv%253A%2520Few-Shot%2520Class-Incremental%2520Learning%2520of%2520Physiological%2520Signals%250A%2520%2520via%2520Representation%2520Space%2520Guided%2520Inversion%26entry.906535625%3DChenqi%2520Li%2520and%2520Boyan%2520Gao%2520and%2520Gabriel%2520Jones%2520and%2520Timothy%2520Denison%2520and%2520Tingting%2520Zhu%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520demonstrated%2520exceptional%2520performance%2520in%2520a%2520variety%250Aof%2520real-world%2520applications.%2520These%2520successes%2520are%2520often%2520attributed%2520to%2520strong%2520base%250Amodels%2520that%2520can%2520generalize%2520to%2520novel%2520tasks%2520with%2520limited%2520supporting%2520data%2520while%250Akeeping%2520prior%2520knowledge%2520intact.%2520However%252C%2520these%2520impressive%2520results%2520are%2520based%2520on%250Athe%2520availability%2520of%2520a%2520large%2520amount%2520of%2520high-quality%2520data%252C%2520which%2520is%2520often%2520lacking%250Ain%2520specialized%2520biomedical%2520applications.%2520In%2520such%2520fields%252C%2520models%2520are%2520usually%250Adeveloped%2520with%2520limited%2520data%2520that%2520arrive%2520incrementally%2520with%2520novel%2520categories.%250AThis%2520requires%2520the%2520model%2520to%2520adapt%2520to%2520new%2520information%2520while%2520preserving%2520existing%250Aknowledge.%2520Few-Shot%2520Class-Incremental%2520Learning%2520%2528FSCIL%2529%2520methods%2520offer%2520a%250Apromising%2520approach%2520to%2520addressing%2520these%2520challenges%252C%2520but%2520they%2520also%2520depend%2520on%250Astrong%2520base%2520models%2520that%2520face%2520the%2520same%2520aforementioned%2520limitations.%2520To%2520overcome%250Athese%2520constraints%252C%2520we%2520propose%2520AnchorInv%2520following%2520the%2520straightforward%2520and%250Aefficient%2520buffer-replay%2520strategy.%2520Instead%2520of%2520selecting%2520and%2520storing%2520raw%2520data%252C%250AAnchorInv%2520generates%2520synthetic%2520samples%2520guided%2520by%2520anchor%2520points%2520in%2520the%2520feature%250Aspace.%2520This%2520approach%2520protects%2520privacy%2520and%2520regularizes%2520the%2520model%2520for%2520adaptation.%250AWhen%2520evaluated%2520on%2520three%2520public%2520physiological%2520time%2520series%2520datasets%252C%2520AnchorInv%250Aexhibits%2520efficient%2520knowledge%2520forgetting%2520prevention%2520and%2520improved%2520adaptation%2520to%250Anovel%2520classes%252C%2520surpassing%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnchorInv%3A%20Few-Shot%20Class-Incremental%20Learning%20of%20Physiological%20Signals%0A%20%20via%20Representation%20Space%20Guided%20Inversion&entry.906535625=Chenqi%20Li%20and%20Boyan%20Gao%20and%20Gabriel%20Jones%20and%20Timothy%20Denison%20and%20Tingting%20Zhu&entry.1292438233=%20%20Deep%20learning%20models%20have%20demonstrated%20exceptional%20performance%20in%20a%20variety%0Aof%20real-world%20applications.%20These%20successes%20are%20often%20attributed%20to%20strong%20base%0Amodels%20that%20can%20generalize%20to%20novel%20tasks%20with%20limited%20supporting%20data%20while%0Akeeping%20prior%20knowledge%20intact.%20However%2C%20these%20impressive%20results%20are%20based%20on%0Athe%20availability%20of%20a%20large%20amount%20of%20high-quality%20data%2C%20which%20is%20often%20lacking%0Ain%20specialized%20biomedical%20applications.%20In%20such%20fields%2C%20models%20are%20usually%0Adeveloped%20with%20limited%20data%20that%20arrive%20incrementally%20with%20novel%20categories.%0AThis%20requires%20the%20model%20to%20adapt%20to%20new%20information%20while%20preserving%20existing%0Aknowledge.%20Few-Shot%20Class-Incremental%20Learning%20%28FSCIL%29%20methods%20offer%20a%0Apromising%20approach%20to%20addressing%20these%20challenges%2C%20but%20they%20also%20depend%20on%0Astrong%20base%20models%20that%20face%20the%20same%20aforementioned%20limitations.%20To%20overcome%0Athese%20constraints%2C%20we%20propose%20AnchorInv%20following%20the%20straightforward%20and%0Aefficient%20buffer-replay%20strategy.%20Instead%20of%20selecting%20and%20storing%20raw%20data%2C%0AAnchorInv%20generates%20synthetic%20samples%20guided%20by%20anchor%20points%20in%20the%20feature%0Aspace.%20This%20approach%20protects%20privacy%20and%20regularizes%20the%20model%20for%20adaptation.%0AWhen%20evaluated%20on%20three%20public%20physiological%20time%20series%20datasets%2C%20AnchorInv%0Aexhibits%20efficient%20knowledge%20forgetting%20prevention%20and%20improved%20adaptation%20to%0Anovel%20classes%2C%20surpassing%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13714v1&entry.124074799=Read"},
{"title": "Neuro-Symbolic Embedding for Short and Effective Feature Selection via\n  Autoregressive Generation", "author": "Nanxu Gong and Wangyang Ying and Dongjie Wang and Yanjie Fu", "abstract": "  Feature selection aims to identify the optimal feature subset for enhancing\ndownstream models. Effective feature selection can remove redundant features,\nsave computational resources, accelerate the model learning process, and\nimprove the model overall performance. However, existing works are often\ntime-intensive to identify the effective feature subset within high-dimensional\nfeature spaces. Meanwhile, these methods mainly utilize a single downstream\ntask performance as the selection criterion, leading to the selected subsets\nthat are not only redundant but also lack generalizability. To bridge these\ngaps, we reformulate feature selection through a neuro-symbolic lens and\nintroduce a novel generative framework aimed at identifying short and effective\nfeature subsets. More specifically, we found that feature ID tokens of the\nselected subset can be formulated as symbols to reflect the intricate\ncorrelations among features. Thus, in this framework, we first create a data\ncollector to automatically collect numerous feature selection samples\nconsisting of feature ID tokens, model performance, and the measurement of\nfeature subset redundancy. Building on the collected data, an\nencoder-decoder-evaluator learning paradigm is developed to preserve the\nintelligence of feature selection into a continuous embedding space for\nefficient search. Within the learned embedding space, we leverage a\nmulti-gradient search algorithm to find more robust and generalized embeddings\nwith the objective of improving model performance and reducing feature subset\nredundancy. These embeddings are then utilized to reconstruct the feature ID\ntokens for executing the final feature selection. Ultimately, comprehensive\nexperiments and case studies are conducted to validate the effectiveness of the\nproposed framework.\n", "link": "http://arxiv.org/abs/2404.17157v2", "date": "2024-12-18", "relevancy": 2.4997, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5061}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5003}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neuro-Symbolic%20Embedding%20for%20Short%20and%20Effective%20Feature%20Selection%20via%0A%20%20Autoregressive%20Generation&body=Title%3A%20Neuro-Symbolic%20Embedding%20for%20Short%20and%20Effective%20Feature%20Selection%20via%0A%20%20Autoregressive%20Generation%0AAuthor%3A%20Nanxu%20Gong%20and%20Wangyang%20Ying%20and%20Dongjie%20Wang%20and%20Yanjie%20Fu%0AAbstract%3A%20%20%20Feature%20selection%20aims%20to%20identify%20the%20optimal%20feature%20subset%20for%20enhancing%0Adownstream%20models.%20Effective%20feature%20selection%20can%20remove%20redundant%20features%2C%0Asave%20computational%20resources%2C%20accelerate%20the%20model%20learning%20process%2C%20and%0Aimprove%20the%20model%20overall%20performance.%20However%2C%20existing%20works%20are%20often%0Atime-intensive%20to%20identify%20the%20effective%20feature%20subset%20within%20high-dimensional%0Afeature%20spaces.%20Meanwhile%2C%20these%20methods%20mainly%20utilize%20a%20single%20downstream%0Atask%20performance%20as%20the%20selection%20criterion%2C%20leading%20to%20the%20selected%20subsets%0Athat%20are%20not%20only%20redundant%20but%20also%20lack%20generalizability.%20To%20bridge%20these%0Agaps%2C%20we%20reformulate%20feature%20selection%20through%20a%20neuro-symbolic%20lens%20and%0Aintroduce%20a%20novel%20generative%20framework%20aimed%20at%20identifying%20short%20and%20effective%0Afeature%20subsets.%20More%20specifically%2C%20we%20found%20that%20feature%20ID%20tokens%20of%20the%0Aselected%20subset%20can%20be%20formulated%20as%20symbols%20to%20reflect%20the%20intricate%0Acorrelations%20among%20features.%20Thus%2C%20in%20this%20framework%2C%20we%20first%20create%20a%20data%0Acollector%20to%20automatically%20collect%20numerous%20feature%20selection%20samples%0Aconsisting%20of%20feature%20ID%20tokens%2C%20model%20performance%2C%20and%20the%20measurement%20of%0Afeature%20subset%20redundancy.%20Building%20on%20the%20collected%20data%2C%20an%0Aencoder-decoder-evaluator%20learning%20paradigm%20is%20developed%20to%20preserve%20the%0Aintelligence%20of%20feature%20selection%20into%20a%20continuous%20embedding%20space%20for%0Aefficient%20search.%20Within%20the%20learned%20embedding%20space%2C%20we%20leverage%20a%0Amulti-gradient%20search%20algorithm%20to%20find%20more%20robust%20and%20generalized%20embeddings%0Awith%20the%20objective%20of%20improving%20model%20performance%20and%20reducing%20feature%20subset%0Aredundancy.%20These%20embeddings%20are%20then%20utilized%20to%20reconstruct%20the%20feature%20ID%0Atokens%20for%20executing%20the%20final%20feature%20selection.%20Ultimately%2C%20comprehensive%0Aexperiments%20and%20case%20studies%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20the%0Aproposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuro-Symbolic%2520Embedding%2520for%2520Short%2520and%2520Effective%2520Feature%2520Selection%2520via%250A%2520%2520Autoregressive%2520Generation%26entry.906535625%3DNanxu%2520Gong%2520and%2520Wangyang%2520Ying%2520and%2520Dongjie%2520Wang%2520and%2520Yanjie%2520Fu%26entry.1292438233%3D%2520%2520Feature%2520selection%2520aims%2520to%2520identify%2520the%2520optimal%2520feature%2520subset%2520for%2520enhancing%250Adownstream%2520models.%2520Effective%2520feature%2520selection%2520can%2520remove%2520redundant%2520features%252C%250Asave%2520computational%2520resources%252C%2520accelerate%2520the%2520model%2520learning%2520process%252C%2520and%250Aimprove%2520the%2520model%2520overall%2520performance.%2520However%252C%2520existing%2520works%2520are%2520often%250Atime-intensive%2520to%2520identify%2520the%2520effective%2520feature%2520subset%2520within%2520high-dimensional%250Afeature%2520spaces.%2520Meanwhile%252C%2520these%2520methods%2520mainly%2520utilize%2520a%2520single%2520downstream%250Atask%2520performance%2520as%2520the%2520selection%2520criterion%252C%2520leading%2520to%2520the%2520selected%2520subsets%250Athat%2520are%2520not%2520only%2520redundant%2520but%2520also%2520lack%2520generalizability.%2520To%2520bridge%2520these%250Agaps%252C%2520we%2520reformulate%2520feature%2520selection%2520through%2520a%2520neuro-symbolic%2520lens%2520and%250Aintroduce%2520a%2520novel%2520generative%2520framework%2520aimed%2520at%2520identifying%2520short%2520and%2520effective%250Afeature%2520subsets.%2520More%2520specifically%252C%2520we%2520found%2520that%2520feature%2520ID%2520tokens%2520of%2520the%250Aselected%2520subset%2520can%2520be%2520formulated%2520as%2520symbols%2520to%2520reflect%2520the%2520intricate%250Acorrelations%2520among%2520features.%2520Thus%252C%2520in%2520this%2520framework%252C%2520we%2520first%2520create%2520a%2520data%250Acollector%2520to%2520automatically%2520collect%2520numerous%2520feature%2520selection%2520samples%250Aconsisting%2520of%2520feature%2520ID%2520tokens%252C%2520model%2520performance%252C%2520and%2520the%2520measurement%2520of%250Afeature%2520subset%2520redundancy.%2520Building%2520on%2520the%2520collected%2520data%252C%2520an%250Aencoder-decoder-evaluator%2520learning%2520paradigm%2520is%2520developed%2520to%2520preserve%2520the%250Aintelligence%2520of%2520feature%2520selection%2520into%2520a%2520continuous%2520embedding%2520space%2520for%250Aefficient%2520search.%2520Within%2520the%2520learned%2520embedding%2520space%252C%2520we%2520leverage%2520a%250Amulti-gradient%2520search%2520algorithm%2520to%2520find%2520more%2520robust%2520and%2520generalized%2520embeddings%250Awith%2520the%2520objective%2520of%2520improving%2520model%2520performance%2520and%2520reducing%2520feature%2520subset%250Aredundancy.%2520These%2520embeddings%2520are%2520then%2520utilized%2520to%2520reconstruct%2520the%2520feature%2520ID%250Atokens%2520for%2520executing%2520the%2520final%2520feature%2520selection.%2520Ultimately%252C%2520comprehensive%250Aexperiments%2520and%2520case%2520studies%2520are%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neuro-Symbolic%20Embedding%20for%20Short%20and%20Effective%20Feature%20Selection%20via%0A%20%20Autoregressive%20Generation&entry.906535625=Nanxu%20Gong%20and%20Wangyang%20Ying%20and%20Dongjie%20Wang%20and%20Yanjie%20Fu&entry.1292438233=%20%20Feature%20selection%20aims%20to%20identify%20the%20optimal%20feature%20subset%20for%20enhancing%0Adownstream%20models.%20Effective%20feature%20selection%20can%20remove%20redundant%20features%2C%0Asave%20computational%20resources%2C%20accelerate%20the%20model%20learning%20process%2C%20and%0Aimprove%20the%20model%20overall%20performance.%20However%2C%20existing%20works%20are%20often%0Atime-intensive%20to%20identify%20the%20effective%20feature%20subset%20within%20high-dimensional%0Afeature%20spaces.%20Meanwhile%2C%20these%20methods%20mainly%20utilize%20a%20single%20downstream%0Atask%20performance%20as%20the%20selection%20criterion%2C%20leading%20to%20the%20selected%20subsets%0Athat%20are%20not%20only%20redundant%20but%20also%20lack%20generalizability.%20To%20bridge%20these%0Agaps%2C%20we%20reformulate%20feature%20selection%20through%20a%20neuro-symbolic%20lens%20and%0Aintroduce%20a%20novel%20generative%20framework%20aimed%20at%20identifying%20short%20and%20effective%0Afeature%20subsets.%20More%20specifically%2C%20we%20found%20that%20feature%20ID%20tokens%20of%20the%0Aselected%20subset%20can%20be%20formulated%20as%20symbols%20to%20reflect%20the%20intricate%0Acorrelations%20among%20features.%20Thus%2C%20in%20this%20framework%2C%20we%20first%20create%20a%20data%0Acollector%20to%20automatically%20collect%20numerous%20feature%20selection%20samples%0Aconsisting%20of%20feature%20ID%20tokens%2C%20model%20performance%2C%20and%20the%20measurement%20of%0Afeature%20subset%20redundancy.%20Building%20on%20the%20collected%20data%2C%20an%0Aencoder-decoder-evaluator%20learning%20paradigm%20is%20developed%20to%20preserve%20the%0Aintelligence%20of%20feature%20selection%20into%20a%20continuous%20embedding%20space%20for%0Aefficient%20search.%20Within%20the%20learned%20embedding%20space%2C%20we%20leverage%20a%0Amulti-gradient%20search%20algorithm%20to%20find%20more%20robust%20and%20generalized%20embeddings%0Awith%20the%20objective%20of%20improving%20model%20performance%20and%20reducing%20feature%20subset%0Aredundancy.%20These%20embeddings%20are%20then%20utilized%20to%20reconstruct%20the%20feature%20ID%0Atokens%20for%20executing%20the%20final%20feature%20selection.%20Ultimately%2C%20comprehensive%0Aexperiments%20and%20case%20studies%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20the%0Aproposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17157v2&entry.124074799=Read"},
{"title": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali", "author": "Sharad Duwal and Suraj Prasai and Suresh Manandhar", "abstract": "  Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali.\n", "link": "http://arxiv.org/abs/2412.13860v1", "date": "2024-12-18", "relevancy": 2.4832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-adaptative%20Continual%20Learning%20for%20Low-resource%20Tasks%3A%20Evaluation%0A%20%20on%20Nepali&body=Title%3A%20Domain-adaptative%20Continual%20Learning%20for%20Low-resource%20Tasks%3A%20Evaluation%0A%20%20on%20Nepali%0AAuthor%3A%20Sharad%20Duwal%20and%20Suraj%20Prasai%20and%20Suresh%20Manandhar%0AAbstract%3A%20%20%20Continual%20learning%20has%20emerged%20as%20an%20important%20research%20direction%20due%20to%20the%0Ainfeasibility%20of%20retraining%20large%20language%20models%20%28LLMs%29%20from%20scratch%20in%20the%0Aevent%20of%20new%20data%20availability.%20Of%20great%20interest%20is%20the%20domain-adaptive%0Apre-training%20%28DAPT%29%20paradigm%2C%20which%20focuses%20on%20continually%20training%20a%0Apre-trained%20language%20model%20to%20adapt%20it%20to%20a%20domain%20it%20was%20not%20originally%0Atrained%20on.%20In%20this%20work%2C%20we%20evaluate%20the%20feasibility%20of%20DAPT%20in%20a%20low-resource%0Asetting%2C%20namely%20the%20Nepali%20language.%20We%20use%20synthetic%20data%20to%20continue%20training%0ALlama%203%208B%20to%20adapt%20it%20to%20the%20Nepali%20language%20in%20a%204-bit%20QLoRA%20setting.%20We%0Aevaluate%20the%20adapted%20model%20on%20its%20performance%2C%20forgetting%2C%20and%20knowledge%0Aacquisition.%20We%20compare%20the%20base%20model%20and%20the%20final%20model%20on%20their%20Nepali%0Ageneration%20abilities%2C%20their%20performance%20on%20popular%20benchmarks%2C%20and%20run%0Acase-studies%20to%20probe%20their%20linguistic%20knowledge%20in%20Nepali.%20We%20see%20some%0Aunsurprising%20forgetting%20in%20the%20final%20model%2C%20but%20also%20surprisingly%20find%20that%0Aincreasing%20the%20number%20of%20shots%20during%20evaluation%20yields%20better%20percent%0Aincreases%20in%20the%20final%20model%20%28as%20high%20as%2019.29%25%20increase%29%20compared%20to%20the%20base%0Amodel%20%284.98%25%29%2C%20suggesting%20latent%20retention.%20We%20also%20explore%20layer-head%0Aself-attention%20heatmaps%20to%20establish%20dependency%20resolution%20abilities%20of%20the%0Afinal%20model%20in%20Nepali.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-adaptative%2520Continual%2520Learning%2520for%2520Low-resource%2520Tasks%253A%2520Evaluation%250A%2520%2520on%2520Nepali%26entry.906535625%3DSharad%2520Duwal%2520and%2520Suraj%2520Prasai%2520and%2520Suresh%2520Manandhar%26entry.1292438233%3D%2520%2520Continual%2520learning%2520has%2520emerged%2520as%2520an%2520important%2520research%2520direction%2520due%2520to%2520the%250Ainfeasibility%2520of%2520retraining%2520large%2520language%2520models%2520%2528LLMs%2529%2520from%2520scratch%2520in%2520the%250Aevent%2520of%2520new%2520data%2520availability.%2520Of%2520great%2520interest%2520is%2520the%2520domain-adaptive%250Apre-training%2520%2528DAPT%2529%2520paradigm%252C%2520which%2520focuses%2520on%2520continually%2520training%2520a%250Apre-trained%2520language%2520model%2520to%2520adapt%2520it%2520to%2520a%2520domain%2520it%2520was%2520not%2520originally%250Atrained%2520on.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520the%2520feasibility%2520of%2520DAPT%2520in%2520a%2520low-resource%250Asetting%252C%2520namely%2520the%2520Nepali%2520language.%2520We%2520use%2520synthetic%2520data%2520to%2520continue%2520training%250ALlama%25203%25208B%2520to%2520adapt%2520it%2520to%2520the%2520Nepali%2520language%2520in%2520a%25204-bit%2520QLoRA%2520setting.%2520We%250Aevaluate%2520the%2520adapted%2520model%2520on%2520its%2520performance%252C%2520forgetting%252C%2520and%2520knowledge%250Aacquisition.%2520We%2520compare%2520the%2520base%2520model%2520and%2520the%2520final%2520model%2520on%2520their%2520Nepali%250Ageneration%2520abilities%252C%2520their%2520performance%2520on%2520popular%2520benchmarks%252C%2520and%2520run%250Acase-studies%2520to%2520probe%2520their%2520linguistic%2520knowledge%2520in%2520Nepali.%2520We%2520see%2520some%250Aunsurprising%2520forgetting%2520in%2520the%2520final%2520model%252C%2520but%2520also%2520surprisingly%2520find%2520that%250Aincreasing%2520the%2520number%2520of%2520shots%2520during%2520evaluation%2520yields%2520better%2520percent%250Aincreases%2520in%2520the%2520final%2520model%2520%2528as%2520high%2520as%252019.29%2525%2520increase%2529%2520compared%2520to%2520the%2520base%250Amodel%2520%25284.98%2525%2529%252C%2520suggesting%2520latent%2520retention.%2520We%2520also%2520explore%2520layer-head%250Aself-attention%2520heatmaps%2520to%2520establish%2520dependency%2520resolution%2520abilities%2520of%2520the%250Afinal%2520model%2520in%2520Nepali.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-adaptative%20Continual%20Learning%20for%20Low-resource%20Tasks%3A%20Evaluation%0A%20%20on%20Nepali&entry.906535625=Sharad%20Duwal%20and%20Suraj%20Prasai%20and%20Suresh%20Manandhar&entry.1292438233=%20%20Continual%20learning%20has%20emerged%20as%20an%20important%20research%20direction%20due%20to%20the%0Ainfeasibility%20of%20retraining%20large%20language%20models%20%28LLMs%29%20from%20scratch%20in%20the%0Aevent%20of%20new%20data%20availability.%20Of%20great%20interest%20is%20the%20domain-adaptive%0Apre-training%20%28DAPT%29%20paradigm%2C%20which%20focuses%20on%20continually%20training%20a%0Apre-trained%20language%20model%20to%20adapt%20it%20to%20a%20domain%20it%20was%20not%20originally%0Atrained%20on.%20In%20this%20work%2C%20we%20evaluate%20the%20feasibility%20of%20DAPT%20in%20a%20low-resource%0Asetting%2C%20namely%20the%20Nepali%20language.%20We%20use%20synthetic%20data%20to%20continue%20training%0ALlama%203%208B%20to%20adapt%20it%20to%20the%20Nepali%20language%20in%20a%204-bit%20QLoRA%20setting.%20We%0Aevaluate%20the%20adapted%20model%20on%20its%20performance%2C%20forgetting%2C%20and%20knowledge%0Aacquisition.%20We%20compare%20the%20base%20model%20and%20the%20final%20model%20on%20their%20Nepali%0Ageneration%20abilities%2C%20their%20performance%20on%20popular%20benchmarks%2C%20and%20run%0Acase-studies%20to%20probe%20their%20linguistic%20knowledge%20in%20Nepali.%20We%20see%20some%0Aunsurprising%20forgetting%20in%20the%20final%20model%2C%20but%20also%20surprisingly%20find%20that%0Aincreasing%20the%20number%20of%20shots%20during%20evaluation%20yields%20better%20percent%0Aincreases%20in%20the%20final%20model%20%28as%20high%20as%2019.29%25%20increase%29%20compared%20to%20the%20base%0Amodel%20%284.98%25%29%2C%20suggesting%20latent%20retention.%20We%20also%20explore%20layer-head%0Aself-attention%20heatmaps%20to%20establish%20dependency%20resolution%20abilities%20of%20the%0Afinal%20model%20in%20Nepali.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13860v1&entry.124074799=Read"},
{"title": "M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation", "author": "Zixuan Chen and Jiaxin Li and Liming Tan and Yejie Guo and Junxuan Liang and Cewu Lu and Yonglu Li", "abstract": "  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M3-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M3-VOS, yielding several key insights. Notably, current appearance\nbased approaches show significant room for improvement when handling objects\nwith phase transitions. The inherent changes in disorder suggest that the\npredictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-and-play model that improves its performance by\nreversal refinement. Our data and code will be publicly available\n", "link": "http://arxiv.org/abs/2412.13803v1", "date": "2024-12-18", "relevancy": 2.4763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E3%24-VOS%3A%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%20Object%0A%20%20Segmentation&body=Title%3A%20M%24%5E3%24-VOS%3A%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%20Object%0A%20%20Segmentation%0AAuthor%3A%20Zixuan%20Chen%20and%20Jiaxin%20Li%20and%20Liming%20Tan%20and%20Yejie%20Guo%20and%20Junxuan%20Liang%20and%20Cewu%20Lu%20and%20Yonglu%20Li%0AAbstract%3A%20%20%20Intelligent%20robots%20need%20to%20interact%20with%20diverse%20objects%20across%20various%0Aenvironments.%20The%20appearance%20and%20state%20of%20objects%20frequently%20undergo%20complex%0Atransformations%20depending%20on%20the%20object%20properties%2C%20e.g.%2C%20phase%20transitions.%0AHowever%2C%20in%20the%20vision%20community%2C%20segmenting%20dynamic%20objects%20with%20phase%0Atransitions%20is%20overlooked.%20In%20light%20of%20this%2C%20we%20introduce%20the%20concept%20of%20phase%0Ain%20segmentation%2C%20which%20categorizes%20real-world%20objects%20based%20on%20their%20visual%0Acharacteristics%20and%20potential%20morphological%20and%20appearance%20changes.%20Then%2C%20we%0Apresent%20a%20new%20benchmark%2C%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%0AObject%20Segmentation%20%28M3-VOS%29%2C%20to%20verify%20the%20ability%20of%20models%20to%20understand%0Aobject%20phases%2C%20which%20consists%20of%20479%20high-resolution%20videos%20spanning%20over%2010%0Adistinct%20everyday%20scenarios.%20It%20provides%20dense%20instance%20mask%20annotations%20that%0Acapture%20both%20object%20phases%20and%20their%20transitions.%20We%20evaluate%20state-of-the-art%0Amethods%20on%20M3-VOS%2C%20yielding%20several%20key%20insights.%20Notably%2C%20current%20appearance%0Abased%20approaches%20show%20significant%20room%20for%20improvement%20when%20handling%20objects%0Awith%20phase%20transitions.%20The%20inherent%20changes%20in%20disorder%20suggest%20that%20the%0Apredictive%20performance%20of%20the%20forward%20entropy-increasing%20process%20can%20be%0Aimproved%20through%20a%20reverse%20entropy-reducing%20process.%20These%20findings%20lead%20us%20to%0Apropose%20ReVOS%2C%20a%20new%20plug-and-play%20model%20that%20improves%20its%20performance%20by%0Areversal%20refinement.%20Our%20data%20and%20code%20will%20be%20publicly%20available%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E3%2524-VOS%253A%2520Multi-Phase%252C%2520Multi-Transition%252C%2520and%2520Multi-Scenery%2520Video%2520Object%250A%2520%2520Segmentation%26entry.906535625%3DZixuan%2520Chen%2520and%2520Jiaxin%2520Li%2520and%2520Liming%2520Tan%2520and%2520Yejie%2520Guo%2520and%2520Junxuan%2520Liang%2520and%2520Cewu%2520Lu%2520and%2520Yonglu%2520Li%26entry.1292438233%3D%2520%2520Intelligent%2520robots%2520need%2520to%2520interact%2520with%2520diverse%2520objects%2520across%2520various%250Aenvironments.%2520The%2520appearance%2520and%2520state%2520of%2520objects%2520frequently%2520undergo%2520complex%250Atransformations%2520depending%2520on%2520the%2520object%2520properties%252C%2520e.g.%252C%2520phase%2520transitions.%250AHowever%252C%2520in%2520the%2520vision%2520community%252C%2520segmenting%2520dynamic%2520objects%2520with%2520phase%250Atransitions%2520is%2520overlooked.%2520In%2520light%2520of%2520this%252C%2520we%2520introduce%2520the%2520concept%2520of%2520phase%250Ain%2520segmentation%252C%2520which%2520categorizes%2520real-world%2520objects%2520based%2520on%2520their%2520visual%250Acharacteristics%2520and%2520potential%2520morphological%2520and%2520appearance%2520changes.%2520Then%252C%2520we%250Apresent%2520a%2520new%2520benchmark%252C%2520Multi-Phase%252C%2520Multi-Transition%252C%2520and%2520Multi-Scenery%2520Video%250AObject%2520Segmentation%2520%2528M3-VOS%2529%252C%2520to%2520verify%2520the%2520ability%2520of%2520models%2520to%2520understand%250Aobject%2520phases%252C%2520which%2520consists%2520of%2520479%2520high-resolution%2520videos%2520spanning%2520over%252010%250Adistinct%2520everyday%2520scenarios.%2520It%2520provides%2520dense%2520instance%2520mask%2520annotations%2520that%250Acapture%2520both%2520object%2520phases%2520and%2520their%2520transitions.%2520We%2520evaluate%2520state-of-the-art%250Amethods%2520on%2520M3-VOS%252C%2520yielding%2520several%2520key%2520insights.%2520Notably%252C%2520current%2520appearance%250Abased%2520approaches%2520show%2520significant%2520room%2520for%2520improvement%2520when%2520handling%2520objects%250Awith%2520phase%2520transitions.%2520The%2520inherent%2520changes%2520in%2520disorder%2520suggest%2520that%2520the%250Apredictive%2520performance%2520of%2520the%2520forward%2520entropy-increasing%2520process%2520can%2520be%250Aimproved%2520through%2520a%2520reverse%2520entropy-reducing%2520process.%2520These%2520findings%2520lead%2520us%2520to%250Apropose%2520ReVOS%252C%2520a%2520new%2520plug-and-play%2520model%2520that%2520improves%2520its%2520performance%2520by%250Areversal%2520refinement.%2520Our%2520data%2520and%2520code%2520will%2520be%2520publicly%2520available%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E3%24-VOS%3A%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%20Object%0A%20%20Segmentation&entry.906535625=Zixuan%20Chen%20and%20Jiaxin%20Li%20and%20Liming%20Tan%20and%20Yejie%20Guo%20and%20Junxuan%20Liang%20and%20Cewu%20Lu%20and%20Yonglu%20Li&entry.1292438233=%20%20Intelligent%20robots%20need%20to%20interact%20with%20diverse%20objects%20across%20various%0Aenvironments.%20The%20appearance%20and%20state%20of%20objects%20frequently%20undergo%20complex%0Atransformations%20depending%20on%20the%20object%20properties%2C%20e.g.%2C%20phase%20transitions.%0AHowever%2C%20in%20the%20vision%20community%2C%20segmenting%20dynamic%20objects%20with%20phase%0Atransitions%20is%20overlooked.%20In%20light%20of%20this%2C%20we%20introduce%20the%20concept%20of%20phase%0Ain%20segmentation%2C%20which%20categorizes%20real-world%20objects%20based%20on%20their%20visual%0Acharacteristics%20and%20potential%20morphological%20and%20appearance%20changes.%20Then%2C%20we%0Apresent%20a%20new%20benchmark%2C%20Multi-Phase%2C%20Multi-Transition%2C%20and%20Multi-Scenery%20Video%0AObject%20Segmentation%20%28M3-VOS%29%2C%20to%20verify%20the%20ability%20of%20models%20to%20understand%0Aobject%20phases%2C%20which%20consists%20of%20479%20high-resolution%20videos%20spanning%20over%2010%0Adistinct%20everyday%20scenarios.%20It%20provides%20dense%20instance%20mask%20annotations%20that%0Acapture%20both%20object%20phases%20and%20their%20transitions.%20We%20evaluate%20state-of-the-art%0Amethods%20on%20M3-VOS%2C%20yielding%20several%20key%20insights.%20Notably%2C%20current%20appearance%0Abased%20approaches%20show%20significant%20room%20for%20improvement%20when%20handling%20objects%0Awith%20phase%20transitions.%20The%20inherent%20changes%20in%20disorder%20suggest%20that%20the%0Apredictive%20performance%20of%20the%20forward%20entropy-increasing%20process%20can%20be%0Aimproved%20through%20a%20reverse%20entropy-reducing%20process.%20These%20findings%20lead%20us%20to%0Apropose%20ReVOS%2C%20a%20new%20plug-and-play%20model%20that%20improves%20its%20performance%20by%0Areversal%20refinement.%20Our%20data%20and%20code%20will%20be%20publicly%20available%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13803v1&entry.124074799=Read"},
{"title": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters", "author": "Daniil Gurgurov and Mareike Hartmann and Simon Ostermann", "abstract": "  This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.\n", "link": "http://arxiv.org/abs/2407.01406v3", "date": "2024-12-18", "relevancy": 2.4749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.544}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4715}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Multilingual%20LLMs%20to%20Low-Resource%20Languages%20with%20Knowledge%0A%20%20Graphs%20via%20Adapters&body=Title%3A%20Adapting%20Multilingual%20LLMs%20to%20Low-Resource%20Languages%20with%20Knowledge%0A%20%20Graphs%20via%20Adapters%0AAuthor%3A%20Daniil%20Gurgurov%20and%20Mareike%20Hartmann%20and%20Simon%20Ostermann%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20integration%20of%20graph%20knowledge%20from%20linguistic%0Aontologies%20into%20multilingual%20Large%20Language%20Models%20%28LLMs%29%20using%20adapters%20to%0Aimprove%20performance%20for%20low-resource%20languages%20%28LRLs%29%20in%20sentiment%20analysis%0A%28SA%29%20and%20named%20entity%20recognition%20%28NER%29.%20Building%20upon%20successful%0Aparameter-efficient%20fine-tuning%20techniques%2C%20such%20as%20K-ADAPTER%20and%20MAD-X%2C%20we%0Apropose%20a%20similar%20approach%20for%20incorporating%20knowledge%20from%20multilingual%0Agraphs%2C%20connecting%20concepts%20in%20various%20languages%20with%20each%20other%20through%0Alinguistic%20relationships%2C%20into%20multilingual%20LLMs%20for%20LRLs.%20Specifically%2C%20we%0Afocus%20on%20eight%20LRLs%20--%20Maltese%2C%20Bulgarian%2C%20Indonesian%2C%20Nepali%2C%20Javanese%2C%0AUyghur%2C%20Tibetan%2C%20and%20Sinhala%20--%20and%20employ%20language-specific%20adapters%0Afine-tuned%20on%20data%20extracted%20from%20the%20language-specific%20section%20of%20ConceptNet%2C%0Aaiming%20to%20enable%20knowledge%20transfer%20across%20the%20languages%20covered%20by%20the%0Aknowledge%20graph.%20We%20compare%20various%20fine-tuning%20objectives%2C%20including%20standard%0AMasked%20Language%20Modeling%20%28MLM%29%2C%20MLM%20with%20full-word%20masking%2C%20and%20MLM%20with%0Atargeted%20masking%2C%20to%20analyse%20their%20effectiveness%20in%20learning%20and%20integrating%0Athe%20extracted%20graph%20data.%20Through%20empirical%20evaluation%20on%20language-specific%0Atasks%2C%20we%20assess%20how%20structured%20graph%20knowledge%20affects%20the%20performance%20of%0Amultilingual%20LLMs%20for%20LRLs%20in%20SA%20and%20NER%2C%20providing%20insights%20into%20the%20potential%0Abenefits%20of%20adapting%20language%20models%20for%20low-resource%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01406v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Multilingual%2520LLMs%2520to%2520Low-Resource%2520Languages%2520with%2520Knowledge%250A%2520%2520Graphs%2520via%2520Adapters%26entry.906535625%3DDaniil%2520Gurgurov%2520and%2520Mareike%2520Hartmann%2520and%2520Simon%2520Ostermann%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520integration%2520of%2520graph%2520knowledge%2520from%2520linguistic%250Aontologies%2520into%2520multilingual%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520using%2520adapters%2520to%250Aimprove%2520performance%2520for%2520low-resource%2520languages%2520%2528LRLs%2529%2520in%2520sentiment%2520analysis%250A%2528SA%2529%2520and%2520named%2520entity%2520recognition%2520%2528NER%2529.%2520Building%2520upon%2520successful%250Aparameter-efficient%2520fine-tuning%2520techniques%252C%2520such%2520as%2520K-ADAPTER%2520and%2520MAD-X%252C%2520we%250Apropose%2520a%2520similar%2520approach%2520for%2520incorporating%2520knowledge%2520from%2520multilingual%250Agraphs%252C%2520connecting%2520concepts%2520in%2520various%2520languages%2520with%2520each%2520other%2520through%250Alinguistic%2520relationships%252C%2520into%2520multilingual%2520LLMs%2520for%2520LRLs.%2520Specifically%252C%2520we%250Afocus%2520on%2520eight%2520LRLs%2520--%2520Maltese%252C%2520Bulgarian%252C%2520Indonesian%252C%2520Nepali%252C%2520Javanese%252C%250AUyghur%252C%2520Tibetan%252C%2520and%2520Sinhala%2520--%2520and%2520employ%2520language-specific%2520adapters%250Afine-tuned%2520on%2520data%2520extracted%2520from%2520the%2520language-specific%2520section%2520of%2520ConceptNet%252C%250Aaiming%2520to%2520enable%2520knowledge%2520transfer%2520across%2520the%2520languages%2520covered%2520by%2520the%250Aknowledge%2520graph.%2520We%2520compare%2520various%2520fine-tuning%2520objectives%252C%2520including%2520standard%250AMasked%2520Language%2520Modeling%2520%2528MLM%2529%252C%2520MLM%2520with%2520full-word%2520masking%252C%2520and%2520MLM%2520with%250Atargeted%2520masking%252C%2520to%2520analyse%2520their%2520effectiveness%2520in%2520learning%2520and%2520integrating%250Athe%2520extracted%2520graph%2520data.%2520Through%2520empirical%2520evaluation%2520on%2520language-specific%250Atasks%252C%2520we%2520assess%2520how%2520structured%2520graph%2520knowledge%2520affects%2520the%2520performance%2520of%250Amultilingual%2520LLMs%2520for%2520LRLs%2520in%2520SA%2520and%2520NER%252C%2520providing%2520insights%2520into%2520the%2520potential%250Abenefits%2520of%2520adapting%2520language%2520models%2520for%2520low-resource%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01406v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Multilingual%20LLMs%20to%20Low-Resource%20Languages%20with%20Knowledge%0A%20%20Graphs%20via%20Adapters&entry.906535625=Daniil%20Gurgurov%20and%20Mareike%20Hartmann%20and%20Simon%20Ostermann&entry.1292438233=%20%20This%20paper%20explores%20the%20integration%20of%20graph%20knowledge%20from%20linguistic%0Aontologies%20into%20multilingual%20Large%20Language%20Models%20%28LLMs%29%20using%20adapters%20to%0Aimprove%20performance%20for%20low-resource%20languages%20%28LRLs%29%20in%20sentiment%20analysis%0A%28SA%29%20and%20named%20entity%20recognition%20%28NER%29.%20Building%20upon%20successful%0Aparameter-efficient%20fine-tuning%20techniques%2C%20such%20as%20K-ADAPTER%20and%20MAD-X%2C%20we%0Apropose%20a%20similar%20approach%20for%20incorporating%20knowledge%20from%20multilingual%0Agraphs%2C%20connecting%20concepts%20in%20various%20languages%20with%20each%20other%20through%0Alinguistic%20relationships%2C%20into%20multilingual%20LLMs%20for%20LRLs.%20Specifically%2C%20we%0Afocus%20on%20eight%20LRLs%20--%20Maltese%2C%20Bulgarian%2C%20Indonesian%2C%20Nepali%2C%20Javanese%2C%0AUyghur%2C%20Tibetan%2C%20and%20Sinhala%20--%20and%20employ%20language-specific%20adapters%0Afine-tuned%20on%20data%20extracted%20from%20the%20language-specific%20section%20of%20ConceptNet%2C%0Aaiming%20to%20enable%20knowledge%20transfer%20across%20the%20languages%20covered%20by%20the%0Aknowledge%20graph.%20We%20compare%20various%20fine-tuning%20objectives%2C%20including%20standard%0AMasked%20Language%20Modeling%20%28MLM%29%2C%20MLM%20with%20full-word%20masking%2C%20and%20MLM%20with%0Atargeted%20masking%2C%20to%20analyse%20their%20effectiveness%20in%20learning%20and%20integrating%0Athe%20extracted%20graph%20data.%20Through%20empirical%20evaluation%20on%20language-specific%0Atasks%2C%20we%20assess%20how%20structured%20graph%20knowledge%20affects%20the%20performance%20of%0Amultilingual%20LLMs%20for%20LRLs%20in%20SA%20and%20NER%2C%20providing%20insights%20into%20the%20potential%0Abenefits%20of%20adapting%20language%20models%20for%20low-resource%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01406v3&entry.124074799=Read"},
{"title": "Toward Efficient Data-Free Unlearning", "author": "Chenhao Zhang and Shaofei Shen and Weitong Chen and Miao Xu", "abstract": "  Machine unlearning without access to real data distribution is challenging.\nThe existing method based on data-free distillation achieved unlearning by\nfiltering out synthetic samples containing forgetting information but struggled\nto distill the retaining-related knowledge efficiently. In this work, we\nanalyze that such a problem is due to over-filtering, which reduces the\nsynthesized retaining-related information. We propose a novel method, Inhibited\nSynthetic PostFilter (ISPF), to tackle this challenge from two perspectives:\nFirst, the Inhibited Synthetic, by reducing the synthesized forgetting\ninformation; Second, the PostFilter, by fully utilizing the retaining-related\ninformation in synthesized samples. Experimental results demonstrate that the\nproposed ISPF effectively tackles the challenge and outperforms existing\nmethods.\n", "link": "http://arxiv.org/abs/2412.13790v1", "date": "2024-12-18", "relevancy": 2.447, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4968}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4894}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Efficient%20Data-Free%20Unlearning&body=Title%3A%20Toward%20Efficient%20Data-Free%20Unlearning%0AAuthor%3A%20Chenhao%20Zhang%20and%20Shaofei%20Shen%20and%20Weitong%20Chen%20and%20Miao%20Xu%0AAbstract%3A%20%20%20Machine%20unlearning%20without%20access%20to%20real%20data%20distribution%20is%20challenging.%0AThe%20existing%20method%20based%20on%20data-free%20distillation%20achieved%20unlearning%20by%0Afiltering%20out%20synthetic%20samples%20containing%20forgetting%20information%20but%20struggled%0Ato%20distill%20the%20retaining-related%20knowledge%20efficiently.%20In%20this%20work%2C%20we%0Aanalyze%20that%20such%20a%20problem%20is%20due%20to%20over-filtering%2C%20which%20reduces%20the%0Asynthesized%20retaining-related%20information.%20We%20propose%20a%20novel%20method%2C%20Inhibited%0ASynthetic%20PostFilter%20%28ISPF%29%2C%20to%20tackle%20this%20challenge%20from%20two%20perspectives%3A%0AFirst%2C%20the%20Inhibited%20Synthetic%2C%20by%20reducing%20the%20synthesized%20forgetting%0Ainformation%3B%20Second%2C%20the%20PostFilter%2C%20by%20fully%20utilizing%20the%20retaining-related%0Ainformation%20in%20synthesized%20samples.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20ISPF%20effectively%20tackles%20the%20challenge%20and%20outperforms%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Efficient%2520Data-Free%2520Unlearning%26entry.906535625%3DChenhao%2520Zhang%2520and%2520Shaofei%2520Shen%2520and%2520Weitong%2520Chen%2520and%2520Miao%2520Xu%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520without%2520access%2520to%2520real%2520data%2520distribution%2520is%2520challenging.%250AThe%2520existing%2520method%2520based%2520on%2520data-free%2520distillation%2520achieved%2520unlearning%2520by%250Afiltering%2520out%2520synthetic%2520samples%2520containing%2520forgetting%2520information%2520but%2520struggled%250Ato%2520distill%2520the%2520retaining-related%2520knowledge%2520efficiently.%2520In%2520this%2520work%252C%2520we%250Aanalyze%2520that%2520such%2520a%2520problem%2520is%2520due%2520to%2520over-filtering%252C%2520which%2520reduces%2520the%250Asynthesized%2520retaining-related%2520information.%2520We%2520propose%2520a%2520novel%2520method%252C%2520Inhibited%250ASynthetic%2520PostFilter%2520%2528ISPF%2529%252C%2520to%2520tackle%2520this%2520challenge%2520from%2520two%2520perspectives%253A%250AFirst%252C%2520the%2520Inhibited%2520Synthetic%252C%2520by%2520reducing%2520the%2520synthesized%2520forgetting%250Ainformation%253B%2520Second%252C%2520the%2520PostFilter%252C%2520by%2520fully%2520utilizing%2520the%2520retaining-related%250Ainformation%2520in%2520synthesized%2520samples.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520ISPF%2520effectively%2520tackles%2520the%2520challenge%2520and%2520outperforms%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Efficient%20Data-Free%20Unlearning&entry.906535625=Chenhao%20Zhang%20and%20Shaofei%20Shen%20and%20Weitong%20Chen%20and%20Miao%20Xu&entry.1292438233=%20%20Machine%20unlearning%20without%20access%20to%20real%20data%20distribution%20is%20challenging.%0AThe%20existing%20method%20based%20on%20data-free%20distillation%20achieved%20unlearning%20by%0Afiltering%20out%20synthetic%20samples%20containing%20forgetting%20information%20but%20struggled%0Ato%20distill%20the%20retaining-related%20knowledge%20efficiently.%20In%20this%20work%2C%20we%0Aanalyze%20that%20such%20a%20problem%20is%20due%20to%20over-filtering%2C%20which%20reduces%20the%0Asynthesized%20retaining-related%20information.%20We%20propose%20a%20novel%20method%2C%20Inhibited%0ASynthetic%20PostFilter%20%28ISPF%29%2C%20to%20tackle%20this%20challenge%20from%20two%20perspectives%3A%0AFirst%2C%20the%20Inhibited%20Synthetic%2C%20by%20reducing%20the%20synthesized%20forgetting%0Ainformation%3B%20Second%2C%20the%20PostFilter%2C%20by%20fully%20utilizing%20the%20retaining-related%0Ainformation%20in%20synthesized%20samples.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20ISPF%20effectively%20tackles%20the%20challenge%20and%20outperforms%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13790v1&entry.124074799=Read"},
{"title": "A Data-Centric Perspective on Evaluating Machine Learning Models for\n  Tabular Data", "author": "Andrej Tschalzev and Sascha Marton and Stefan L\u00fcdtke and Christian Bartelt and Heiner Stuckenschmidt", "abstract": "  Tabular data is prevalent in real-world machine learning applications, and\nnew models for supervised learning of tabular data are frequently proposed.\nComparative studies assessing the performance of models typically consist of\nmodel-centric evaluation setups with overly standardized data preprocessing.\nThis paper demonstrates that such model-centric evaluations are biased, as\nreal-world modeling pipelines often require dataset-specific preprocessing and\nfeature engineering. Therefore, we propose a data-centric evaluation framework.\nWe select 10 relevant datasets from Kaggle competitions and implement\nexpert-level preprocessing pipelines for each dataset. We conduct experiments\nwith different preprocessing pipelines and hyperparameter optimization (HPO)\nregimes to quantify the impact of model selection, HPO, feature engineering,\nand test-time adaptation. Our main findings are: 1. After dataset-specific\nfeature engineering, model rankings change considerably, performance\ndifferences decrease, and the importance of model selection reduces. 2. Recent\nmodels, despite their measurable progress, still significantly benefit from\nmanual feature engineering. This holds true for both tree-based models and\nneural networks. 3. While tabular data is typically considered static, samples\nare often collected over time, and adapting to distribution shifts can be\nimportant even in supposedly static data. These insights suggest that research\nefforts should be directed toward a data-centric perspective, acknowledging\nthat tabular data requires feature engineering and often exhibits temporal\ncharacteristics. Our framework is available under:\nhttps://github.com/atschalz/dc_tabeval.\n", "link": "http://arxiv.org/abs/2407.02112v3", "date": "2024-12-18", "relevancy": 2.4336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Data-Centric%20Perspective%20on%20Evaluating%20Machine%20Learning%20Models%20for%0A%20%20Tabular%20Data&body=Title%3A%20A%20Data-Centric%20Perspective%20on%20Evaluating%20Machine%20Learning%20Models%20for%0A%20%20Tabular%20Data%0AAuthor%3A%20Andrej%20Tschalzev%20and%20Sascha%20Marton%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt%0AAbstract%3A%20%20%20Tabular%20data%20is%20prevalent%20in%20real-world%20machine%20learning%20applications%2C%20and%0Anew%20models%20for%20supervised%20learning%20of%20tabular%20data%20are%20frequently%20proposed.%0AComparative%20studies%20assessing%20the%20performance%20of%20models%20typically%20consist%20of%0Amodel-centric%20evaluation%20setups%20with%20overly%20standardized%20data%20preprocessing.%0AThis%20paper%20demonstrates%20that%20such%20model-centric%20evaluations%20are%20biased%2C%20as%0Areal-world%20modeling%20pipelines%20often%20require%20dataset-specific%20preprocessing%20and%0Afeature%20engineering.%20Therefore%2C%20we%20propose%20a%20data-centric%20evaluation%20framework.%0AWe%20select%2010%20relevant%20datasets%20from%20Kaggle%20competitions%20and%20implement%0Aexpert-level%20preprocessing%20pipelines%20for%20each%20dataset.%20We%20conduct%20experiments%0Awith%20different%20preprocessing%20pipelines%20and%20hyperparameter%20optimization%20%28HPO%29%0Aregimes%20to%20quantify%20the%20impact%20of%20model%20selection%2C%20HPO%2C%20feature%20engineering%2C%0Aand%20test-time%20adaptation.%20Our%20main%20findings%20are%3A%201.%20After%20dataset-specific%0Afeature%20engineering%2C%20model%20rankings%20change%20considerably%2C%20performance%0Adifferences%20decrease%2C%20and%20the%20importance%20of%20model%20selection%20reduces.%202.%20Recent%0Amodels%2C%20despite%20their%20measurable%20progress%2C%20still%20significantly%20benefit%20from%0Amanual%20feature%20engineering.%20This%20holds%20true%20for%20both%20tree-based%20models%20and%0Aneural%20networks.%203.%20While%20tabular%20data%20is%20typically%20considered%20static%2C%20samples%0Aare%20often%20collected%20over%20time%2C%20and%20adapting%20to%20distribution%20shifts%20can%20be%0Aimportant%20even%20in%20supposedly%20static%20data.%20These%20insights%20suggest%20that%20research%0Aefforts%20should%20be%20directed%20toward%20a%20data-centric%20perspective%2C%20acknowledging%0Athat%20tabular%20data%20requires%20feature%20engineering%20and%20often%20exhibits%20temporal%0Acharacteristics.%20Our%20framework%20is%20available%20under%3A%0Ahttps%3A//github.com/atschalz/dc_tabeval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02112v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Data-Centric%2520Perspective%2520on%2520Evaluating%2520Machine%2520Learning%2520Models%2520for%250A%2520%2520Tabular%2520Data%26entry.906535625%3DAndrej%2520Tschalzev%2520and%2520Sascha%2520Marton%2520and%2520Stefan%2520L%25C3%25BCdtke%2520and%2520Christian%2520Bartelt%2520and%2520Heiner%2520Stuckenschmidt%26entry.1292438233%3D%2520%2520Tabular%2520data%2520is%2520prevalent%2520in%2520real-world%2520machine%2520learning%2520applications%252C%2520and%250Anew%2520models%2520for%2520supervised%2520learning%2520of%2520tabular%2520data%2520are%2520frequently%2520proposed.%250AComparative%2520studies%2520assessing%2520the%2520performance%2520of%2520models%2520typically%2520consist%2520of%250Amodel-centric%2520evaluation%2520setups%2520with%2520overly%2520standardized%2520data%2520preprocessing.%250AThis%2520paper%2520demonstrates%2520that%2520such%2520model-centric%2520evaluations%2520are%2520biased%252C%2520as%250Areal-world%2520modeling%2520pipelines%2520often%2520require%2520dataset-specific%2520preprocessing%2520and%250Afeature%2520engineering.%2520Therefore%252C%2520we%2520propose%2520a%2520data-centric%2520evaluation%2520framework.%250AWe%2520select%252010%2520relevant%2520datasets%2520from%2520Kaggle%2520competitions%2520and%2520implement%250Aexpert-level%2520preprocessing%2520pipelines%2520for%2520each%2520dataset.%2520We%2520conduct%2520experiments%250Awith%2520different%2520preprocessing%2520pipelines%2520and%2520hyperparameter%2520optimization%2520%2528HPO%2529%250Aregimes%2520to%2520quantify%2520the%2520impact%2520of%2520model%2520selection%252C%2520HPO%252C%2520feature%2520engineering%252C%250Aand%2520test-time%2520adaptation.%2520Our%2520main%2520findings%2520are%253A%25201.%2520After%2520dataset-specific%250Afeature%2520engineering%252C%2520model%2520rankings%2520change%2520considerably%252C%2520performance%250Adifferences%2520decrease%252C%2520and%2520the%2520importance%2520of%2520model%2520selection%2520reduces.%25202.%2520Recent%250Amodels%252C%2520despite%2520their%2520measurable%2520progress%252C%2520still%2520significantly%2520benefit%2520from%250Amanual%2520feature%2520engineering.%2520This%2520holds%2520true%2520for%2520both%2520tree-based%2520models%2520and%250Aneural%2520networks.%25203.%2520While%2520tabular%2520data%2520is%2520typically%2520considered%2520static%252C%2520samples%250Aare%2520often%2520collected%2520over%2520time%252C%2520and%2520adapting%2520to%2520distribution%2520shifts%2520can%2520be%250Aimportant%2520even%2520in%2520supposedly%2520static%2520data.%2520These%2520insights%2520suggest%2520that%2520research%250Aefforts%2520should%2520be%2520directed%2520toward%2520a%2520data-centric%2520perspective%252C%2520acknowledging%250Athat%2520tabular%2520data%2520requires%2520feature%2520engineering%2520and%2520often%2520exhibits%2520temporal%250Acharacteristics.%2520Our%2520framework%2520is%2520available%2520under%253A%250Ahttps%253A//github.com/atschalz/dc_tabeval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02112v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Data-Centric%20Perspective%20on%20Evaluating%20Machine%20Learning%20Models%20for%0A%20%20Tabular%20Data&entry.906535625=Andrej%20Tschalzev%20and%20Sascha%20Marton%20and%20Stefan%20L%C3%BCdtke%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt&entry.1292438233=%20%20Tabular%20data%20is%20prevalent%20in%20real-world%20machine%20learning%20applications%2C%20and%0Anew%20models%20for%20supervised%20learning%20of%20tabular%20data%20are%20frequently%20proposed.%0AComparative%20studies%20assessing%20the%20performance%20of%20models%20typically%20consist%20of%0Amodel-centric%20evaluation%20setups%20with%20overly%20standardized%20data%20preprocessing.%0AThis%20paper%20demonstrates%20that%20such%20model-centric%20evaluations%20are%20biased%2C%20as%0Areal-world%20modeling%20pipelines%20often%20require%20dataset-specific%20preprocessing%20and%0Afeature%20engineering.%20Therefore%2C%20we%20propose%20a%20data-centric%20evaluation%20framework.%0AWe%20select%2010%20relevant%20datasets%20from%20Kaggle%20competitions%20and%20implement%0Aexpert-level%20preprocessing%20pipelines%20for%20each%20dataset.%20We%20conduct%20experiments%0Awith%20different%20preprocessing%20pipelines%20and%20hyperparameter%20optimization%20%28HPO%29%0Aregimes%20to%20quantify%20the%20impact%20of%20model%20selection%2C%20HPO%2C%20feature%20engineering%2C%0Aand%20test-time%20adaptation.%20Our%20main%20findings%20are%3A%201.%20After%20dataset-specific%0Afeature%20engineering%2C%20model%20rankings%20change%20considerably%2C%20performance%0Adifferences%20decrease%2C%20and%20the%20importance%20of%20model%20selection%20reduces.%202.%20Recent%0Amodels%2C%20despite%20their%20measurable%20progress%2C%20still%20significantly%20benefit%20from%0Amanual%20feature%20engineering.%20This%20holds%20true%20for%20both%20tree-based%20models%20and%0Aneural%20networks.%203.%20While%20tabular%20data%20is%20typically%20considered%20static%2C%20samples%0Aare%20often%20collected%20over%20time%2C%20and%20adapting%20to%20distribution%20shifts%20can%20be%0Aimportant%20even%20in%20supposedly%20static%20data.%20These%20insights%20suggest%20that%20research%0Aefforts%20should%20be%20directed%20toward%20a%20data-centric%20perspective%2C%20acknowledging%0Athat%20tabular%20data%20requires%20feature%20engineering%20and%20often%20exhibits%20temporal%0Acharacteristics.%20Our%20framework%20is%20available%20under%3A%0Ahttps%3A//github.com/atschalz/dc_tabeval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02112v3&entry.124074799=Read"},
{"title": "CNNtention: Can CNNs do better with Attention?", "author": "Julian Glattki and Nikhil Kapila and Tejas Rathi", "abstract": "  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n", "link": "http://arxiv.org/abs/2412.11657v2", "date": "2024-12-18", "relevancy": 2.4316, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5249}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4727}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNNtention%3A%20Can%20CNNs%20do%20better%20with%20Attention%3F&body=Title%3A%20CNNtention%3A%20Can%20CNNs%20do%20better%20with%20Attention%3F%0AAuthor%3A%20Julian%20Glattki%20and%20Nikhil%20Kapila%20and%20Tejas%20Rathi%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20the%20standard%20for%20image%0Aclassification%20tasks%20for%20a%20long%20time%2C%20but%20more%20recently%20attention-based%0Amechanisms%20have%20gained%20traction.%20This%20project%20aims%20to%20compare%20traditional%20CNNs%0Awith%20attention-augmented%20CNNs%20across%20an%20image%20classification%20task.%20By%0Aevaluating%20and%20comparing%20their%20performance%2C%20accuracy%20and%20computational%0Aefficiency%2C%20the%20project%20will%20highlight%20benefits%20and%20trade-off%20of%20the%20localized%0Afeature%20extraction%20of%20traditional%20CNNs%20and%20the%20global%20context%20capture%20in%0Aattention-augmented%20CNNs.%20By%20doing%20this%2C%20we%20can%20reveal%20further%20insights%20into%0Atheir%20respective%20strengths%20and%20weaknesses%2C%20guide%20the%20selection%20of%20models%20based%0Aon%20specific%20application%20needs%20and%20ultimately%2C%20enhance%20understanding%20of%20these%0Aarchitectures%20in%20the%20deep%20learning%20community.%0A%20%20This%20was%20our%20final%20project%20for%20CS7643%20Deep%20Learning%20course%20at%20Georgia%20Tech.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNNtention%253A%2520Can%2520CNNs%2520do%2520better%2520with%2520Attention%253F%26entry.906535625%3DJulian%2520Glattki%2520and%2520Nikhil%2520Kapila%2520and%2520Tejas%2520Rathi%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520been%2520the%2520standard%2520for%2520image%250Aclassification%2520tasks%2520for%2520a%2520long%2520time%252C%2520but%2520more%2520recently%2520attention-based%250Amechanisms%2520have%2520gained%2520traction.%2520This%2520project%2520aims%2520to%2520compare%2520traditional%2520CNNs%250Awith%2520attention-augmented%2520CNNs%2520across%2520an%2520image%2520classification%2520task.%2520By%250Aevaluating%2520and%2520comparing%2520their%2520performance%252C%2520accuracy%2520and%2520computational%250Aefficiency%252C%2520the%2520project%2520will%2520highlight%2520benefits%2520and%2520trade-off%2520of%2520the%2520localized%250Afeature%2520extraction%2520of%2520traditional%2520CNNs%2520and%2520the%2520global%2520context%2520capture%2520in%250Aattention-augmented%2520CNNs.%2520By%2520doing%2520this%252C%2520we%2520can%2520reveal%2520further%2520insights%2520into%250Atheir%2520respective%2520strengths%2520and%2520weaknesses%252C%2520guide%2520the%2520selection%2520of%2520models%2520based%250Aon%2520specific%2520application%2520needs%2520and%2520ultimately%252C%2520enhance%2520understanding%2520of%2520these%250Aarchitectures%2520in%2520the%2520deep%2520learning%2520community.%250A%2520%2520This%2520was%2520our%2520final%2520project%2520for%2520CS7643%2520Deep%2520Learning%2520course%2520at%2520Georgia%2520Tech.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNNtention%3A%20Can%20CNNs%20do%20better%20with%20Attention%3F&entry.906535625=Julian%20Glattki%20and%20Nikhil%20Kapila%20and%20Tejas%20Rathi&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20the%20standard%20for%20image%0Aclassification%20tasks%20for%20a%20long%20time%2C%20but%20more%20recently%20attention-based%0Amechanisms%20have%20gained%20traction.%20This%20project%20aims%20to%20compare%20traditional%20CNNs%0Awith%20attention-augmented%20CNNs%20across%20an%20image%20classification%20task.%20By%0Aevaluating%20and%20comparing%20their%20performance%2C%20accuracy%20and%20computational%0Aefficiency%2C%20the%20project%20will%20highlight%20benefits%20and%20trade-off%20of%20the%20localized%0Afeature%20extraction%20of%20traditional%20CNNs%20and%20the%20global%20context%20capture%20in%0Aattention-augmented%20CNNs.%20By%20doing%20this%2C%20we%20can%20reveal%20further%20insights%20into%0Atheir%20respective%20strengths%20and%20weaknesses%2C%20guide%20the%20selection%20of%20models%20based%0Aon%20specific%20application%20needs%20and%20ultimately%2C%20enhance%20understanding%20of%20these%0Aarchitectures%20in%20the%20deep%20learning%20community.%0A%20%20This%20was%20our%20final%20project%20for%20CS7643%20Deep%20Learning%20course%20at%20Georgia%20Tech.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11657v2&entry.124074799=Read"},
{"title": "Towards Deployable OCR models for Indic languages", "author": "Minesh Mathew and Ajoy Mondal and CV Jawahar", "abstract": "  Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/\n", "link": "http://arxiv.org/abs/2205.06740v2", "date": "2024-12-18", "relevancy": 2.4175, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Deployable%20OCR%20models%20for%20Indic%20languages&body=Title%3A%20Towards%20Deployable%20OCR%20models%20for%20Indic%20languages%0AAuthor%3A%20Minesh%20Mathew%20and%20Ajoy%20Mondal%20and%20CV%20Jawahar%0AAbstract%3A%20%20%20Recognition%20of%20text%20on%20word%20or%20line%20images%2C%20without%20the%20need%20for%20sub-word%0Asegmentation%20has%20become%20the%20mainstream%20of%20research%20and%20development%20of%20text%0Arecognition%20for%20Indian%20languages.%20Modelling%20unsegmented%20sequences%20using%0AConnectionist%20Temporal%20Classification%20%28CTC%29%20is%20the%20most%20commonly%20used%20approach%0Afor%20segmentation-free%20OCR.%20In%20this%20work%20we%20present%20a%20comprehensive%20empirical%0Astudy%20of%20various%20neural%20network%20models%20that%20uses%20CTC%20for%20transcribing%20step-wise%0Apredictions%20in%20the%20neural%20network%20output%20to%20a%20Unicode%20sequence.%20The%20study%20is%0Aconducted%20for%2013%20Indian%20languages%2C%20using%20an%20internal%20dataset%20that%20has%20around%0A1000%20pages%20per%20language.%20We%20study%20the%20choice%20of%20line%20vs%20word%20as%20the%20recognition%0Aunit%2C%20and%20use%20of%20synthetic%20data%20to%20train%20the%20models.%20We%20compare%20our%20models%20with%0Apopular%20publicly%20available%20OCR%20tools%20for%20end-to-end%20document%20image%20recognition.%0AOur%20end-to-end%20pipeline%20that%20employ%20our%20recognition%20models%20and%20existing%20text%0Asegmentation%20tools%20outperform%20these%20public%20OCR%20tools%20for%208%20out%20of%20the%2013%0Alanguages.%20We%20also%20introduce%20a%20new%20public%20dataset%20called%20Mozhi%20for%20word%20and%0Aline%20recognition%20in%20Indian%20language.%20The%20dataset%20contains%20more%20than%201.2%20million%0Aannotated%20word%20images%20%28120%20thousand%20text%20lines%29%20across%2013%20Indian%20languages.%20Our%0Acode%2C%20trained%20models%20and%20the%20Mozhi%20dataset%20will%20be%20made%20available%20at%0Ahttp%3A//cvit.iiit.ac.in/research/projects/cvit-projects/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.06740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Deployable%2520OCR%2520models%2520for%2520Indic%2520languages%26entry.906535625%3DMinesh%2520Mathew%2520and%2520Ajoy%2520Mondal%2520and%2520CV%2520Jawahar%26entry.1292438233%3D%2520%2520Recognition%2520of%2520text%2520on%2520word%2520or%2520line%2520images%252C%2520without%2520the%2520need%2520for%2520sub-word%250Asegmentation%2520has%2520become%2520the%2520mainstream%2520of%2520research%2520and%2520development%2520of%2520text%250Arecognition%2520for%2520Indian%2520languages.%2520Modelling%2520unsegmented%2520sequences%2520using%250AConnectionist%2520Temporal%2520Classification%2520%2528CTC%2529%2520is%2520the%2520most%2520commonly%2520used%2520approach%250Afor%2520segmentation-free%2520OCR.%2520In%2520this%2520work%2520we%2520present%2520a%2520comprehensive%2520empirical%250Astudy%2520of%2520various%2520neural%2520network%2520models%2520that%2520uses%2520CTC%2520for%2520transcribing%2520step-wise%250Apredictions%2520in%2520the%2520neural%2520network%2520output%2520to%2520a%2520Unicode%2520sequence.%2520The%2520study%2520is%250Aconducted%2520for%252013%2520Indian%2520languages%252C%2520using%2520an%2520internal%2520dataset%2520that%2520has%2520around%250A1000%2520pages%2520per%2520language.%2520We%2520study%2520the%2520choice%2520of%2520line%2520vs%2520word%2520as%2520the%2520recognition%250Aunit%252C%2520and%2520use%2520of%2520synthetic%2520data%2520to%2520train%2520the%2520models.%2520We%2520compare%2520our%2520models%2520with%250Apopular%2520publicly%2520available%2520OCR%2520tools%2520for%2520end-to-end%2520document%2520image%2520recognition.%250AOur%2520end-to-end%2520pipeline%2520that%2520employ%2520our%2520recognition%2520models%2520and%2520existing%2520text%250Asegmentation%2520tools%2520outperform%2520these%2520public%2520OCR%2520tools%2520for%25208%2520out%2520of%2520the%252013%250Alanguages.%2520We%2520also%2520introduce%2520a%2520new%2520public%2520dataset%2520called%2520Mozhi%2520for%2520word%2520and%250Aline%2520recognition%2520in%2520Indian%2520language.%2520The%2520dataset%2520contains%2520more%2520than%25201.2%2520million%250Aannotated%2520word%2520images%2520%2528120%2520thousand%2520text%2520lines%2529%2520across%252013%2520Indian%2520languages.%2520Our%250Acode%252C%2520trained%2520models%2520and%2520the%2520Mozhi%2520dataset%2520will%2520be%2520made%2520available%2520at%250Ahttp%253A//cvit.iiit.ac.in/research/projects/cvit-projects/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.06740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Deployable%20OCR%20models%20for%20Indic%20languages&entry.906535625=Minesh%20Mathew%20and%20Ajoy%20Mondal%20and%20CV%20Jawahar&entry.1292438233=%20%20Recognition%20of%20text%20on%20word%20or%20line%20images%2C%20without%20the%20need%20for%20sub-word%0Asegmentation%20has%20become%20the%20mainstream%20of%20research%20and%20development%20of%20text%0Arecognition%20for%20Indian%20languages.%20Modelling%20unsegmented%20sequences%20using%0AConnectionist%20Temporal%20Classification%20%28CTC%29%20is%20the%20most%20commonly%20used%20approach%0Afor%20segmentation-free%20OCR.%20In%20this%20work%20we%20present%20a%20comprehensive%20empirical%0Astudy%20of%20various%20neural%20network%20models%20that%20uses%20CTC%20for%20transcribing%20step-wise%0Apredictions%20in%20the%20neural%20network%20output%20to%20a%20Unicode%20sequence.%20The%20study%20is%0Aconducted%20for%2013%20Indian%20languages%2C%20using%20an%20internal%20dataset%20that%20has%20around%0A1000%20pages%20per%20language.%20We%20study%20the%20choice%20of%20line%20vs%20word%20as%20the%20recognition%0Aunit%2C%20and%20use%20of%20synthetic%20data%20to%20train%20the%20models.%20We%20compare%20our%20models%20with%0Apopular%20publicly%20available%20OCR%20tools%20for%20end-to-end%20document%20image%20recognition.%0AOur%20end-to-end%20pipeline%20that%20employ%20our%20recognition%20models%20and%20existing%20text%0Asegmentation%20tools%20outperform%20these%20public%20OCR%20tools%20for%208%20out%20of%20the%2013%0Alanguages.%20We%20also%20introduce%20a%20new%20public%20dataset%20called%20Mozhi%20for%20word%20and%0Aline%20recognition%20in%20Indian%20language.%20The%20dataset%20contains%20more%20than%201.2%20million%0Aannotated%20word%20images%20%28120%20thousand%20text%20lines%29%20across%2013%20Indian%20languages.%20Our%0Acode%2C%20trained%20models%20and%20the%20Mozhi%20dataset%20will%20be%20made%20available%20at%0Ahttp%3A//cvit.iiit.ac.in/research/projects/cvit-projects/%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.06740v2&entry.124074799=Read"},
{"title": "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation", "author": "Runtao Liu and Haoyu Wu and Zheng Ziqiang and Chen Wei and Yingqing He and Renjie Pi and Qifeng Chen", "abstract": "  Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.\n", "link": "http://arxiv.org/abs/2412.14167v1", "date": "2024-12-18", "relevancy": 2.4025, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6339}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6258}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoDPO%3A%20Omni-Preference%20Alignment%20for%20Video%20Diffusion%20Generation&body=Title%3A%20VideoDPO%3A%20Omni-Preference%20Alignment%20for%20Video%20Diffusion%20Generation%0AAuthor%3A%20Runtao%20Liu%20and%20Haoyu%20Wu%20and%20Zheng%20Ziqiang%20and%20Chen%20Wei%20and%20Yingqing%20He%20and%20Renjie%20Pi%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20Recent%20progress%20in%20generative%20diffusion%20models%20has%20greatly%20advanced%0Atext-to-video%20generation.%20While%20text-to-video%20models%20trained%20on%20large-scale%2C%0Adiverse%20datasets%20can%20produce%20varied%20outputs%2C%20these%20generations%20often%20deviate%0Afrom%20user%20preferences%2C%20highlighting%20the%20need%20for%20preference%20alignment%20on%0Apre-trained%20models.%20Although%20Direct%20Preference%20Optimization%20%28DPO%29%20has%0Ademonstrated%20significant%20improvements%20in%20language%20and%20image%20generation%2C%20we%0Apioneer%20its%20adaptation%20to%20video%20diffusion%20models%20and%20propose%20a%20VideoDPO%0Apipeline%20by%20making%20several%20key%20adjustments.%20Unlike%20previous%20image%20alignment%0Amethods%20that%20focus%20solely%20on%20either%20%28i%29%20visual%20quality%20or%20%28ii%29%20semantic%0Aalignment%20between%20text%20and%20videos%2C%20we%20comprehensively%20consider%20both%20dimensions%0Aand%20construct%20a%20preference%20score%20accordingly%2C%20which%20we%20term%20the%20OmniScore.%20We%0Adesign%20a%20pipeline%20to%20automatically%20collect%20preference%20pair%20data%20based%20on%20the%0Aproposed%20OmniScore%20and%20discover%20that%20re-weighting%20these%20pairs%20based%20on%20the%0Ascore%20significantly%20impacts%20overall%20preference%20alignment.%20Our%20experiments%0Ademonstrate%20substantial%20improvements%20in%20both%20visual%20quality%20and%20semantic%0Aalignment%2C%20ensuring%20that%20no%20preference%20aspect%20is%20neglected.%20Code%20and%20data%20will%0Abe%20shared%20at%20https%3A//videodpo.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoDPO%253A%2520Omni-Preference%2520Alignment%2520for%2520Video%2520Diffusion%2520Generation%26entry.906535625%3DRuntao%2520Liu%2520and%2520Haoyu%2520Wu%2520and%2520Zheng%2520Ziqiang%2520and%2520Chen%2520Wei%2520and%2520Yingqing%2520He%2520and%2520Renjie%2520Pi%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520generative%2520diffusion%2520models%2520has%2520greatly%2520advanced%250Atext-to-video%2520generation.%2520While%2520text-to-video%2520models%2520trained%2520on%2520large-scale%252C%250Adiverse%2520datasets%2520can%2520produce%2520varied%2520outputs%252C%2520these%2520generations%2520often%2520deviate%250Afrom%2520user%2520preferences%252C%2520highlighting%2520the%2520need%2520for%2520preference%2520alignment%2520on%250Apre-trained%2520models.%2520Although%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%250Ademonstrated%2520significant%2520improvements%2520in%2520language%2520and%2520image%2520generation%252C%2520we%250Apioneer%2520its%2520adaptation%2520to%2520video%2520diffusion%2520models%2520and%2520propose%2520a%2520VideoDPO%250Apipeline%2520by%2520making%2520several%2520key%2520adjustments.%2520Unlike%2520previous%2520image%2520alignment%250Amethods%2520that%2520focus%2520solely%2520on%2520either%2520%2528i%2529%2520visual%2520quality%2520or%2520%2528ii%2529%2520semantic%250Aalignment%2520between%2520text%2520and%2520videos%252C%2520we%2520comprehensively%2520consider%2520both%2520dimensions%250Aand%2520construct%2520a%2520preference%2520score%2520accordingly%252C%2520which%2520we%2520term%2520the%2520OmniScore.%2520We%250Adesign%2520a%2520pipeline%2520to%2520automatically%2520collect%2520preference%2520pair%2520data%2520based%2520on%2520the%250Aproposed%2520OmniScore%2520and%2520discover%2520that%2520re-weighting%2520these%2520pairs%2520based%2520on%2520the%250Ascore%2520significantly%2520impacts%2520overall%2520preference%2520alignment.%2520Our%2520experiments%250Ademonstrate%2520substantial%2520improvements%2520in%2520both%2520visual%2520quality%2520and%2520semantic%250Aalignment%252C%2520ensuring%2520that%2520no%2520preference%2520aspect%2520is%2520neglected.%2520Code%2520and%2520data%2520will%250Abe%2520shared%2520at%2520https%253A//videodpo.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoDPO%3A%20Omni-Preference%20Alignment%20for%20Video%20Diffusion%20Generation&entry.906535625=Runtao%20Liu%20and%20Haoyu%20Wu%20and%20Zheng%20Ziqiang%20and%20Chen%20Wei%20and%20Yingqing%20He%20and%20Renjie%20Pi%20and%20Qifeng%20Chen&entry.1292438233=%20%20Recent%20progress%20in%20generative%20diffusion%20models%20has%20greatly%20advanced%0Atext-to-video%20generation.%20While%20text-to-video%20models%20trained%20on%20large-scale%2C%0Adiverse%20datasets%20can%20produce%20varied%20outputs%2C%20these%20generations%20often%20deviate%0Afrom%20user%20preferences%2C%20highlighting%20the%20need%20for%20preference%20alignment%20on%0Apre-trained%20models.%20Although%20Direct%20Preference%20Optimization%20%28DPO%29%20has%0Ademonstrated%20significant%20improvements%20in%20language%20and%20image%20generation%2C%20we%0Apioneer%20its%20adaptation%20to%20video%20diffusion%20models%20and%20propose%20a%20VideoDPO%0Apipeline%20by%20making%20several%20key%20adjustments.%20Unlike%20previous%20image%20alignment%0Amethods%20that%20focus%20solely%20on%20either%20%28i%29%20visual%20quality%20or%20%28ii%29%20semantic%0Aalignment%20between%20text%20and%20videos%2C%20we%20comprehensively%20consider%20both%20dimensions%0Aand%20construct%20a%20preference%20score%20accordingly%2C%20which%20we%20term%20the%20OmniScore.%20We%0Adesign%20a%20pipeline%20to%20automatically%20collect%20preference%20pair%20data%20based%20on%20the%0Aproposed%20OmniScore%20and%20discover%20that%20re-weighting%20these%20pairs%20based%20on%20the%0Ascore%20significantly%20impacts%20overall%20preference%20alignment.%20Our%20experiments%0Ademonstrate%20substantial%20improvements%20in%20both%20visual%20quality%20and%20semantic%0Aalignment%2C%20ensuring%20that%20no%20preference%20aspect%20is%20neglected.%20Code%20and%20data%20will%0Abe%20shared%20at%20https%3A//videodpo.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14167v1&entry.124074799=Read"},
{"title": "Prompt Categories Cluster for Weakly Supervised Semantic Segmentation", "author": "Wangyu Wu and Xianglin Qiu and Siqi Song and Xiaowei Huang and Fei Ma and Jimin Xiao", "abstract": "  Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.\n", "link": "http://arxiv.org/abs/2412.13823v1", "date": "2024-12-18", "relevancy": 2.4019, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4924}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt%20Categories%20Cluster%20for%20Weakly%20Supervised%20Semantic%20Segmentation&body=Title%3A%20Prompt%20Categories%20Cluster%20for%20Weakly%20Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Wangyu%20Wu%20and%20Xianglin%20Qiu%20and%20Siqi%20Song%20and%20Xiaowei%20Huang%20and%20Fei%20Ma%20and%20Jimin%20Xiao%0AAbstract%3A%20%20%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%2C%20which%20leverages%20image-level%0Alabels%2C%20has%20garnered%20significant%20attention%20due%20to%20its%20cost-effectiveness.%20The%0Aprevious%20methods%20mainly%20strengthen%20the%20inter-class%20differences%20to%20avoid%20class%0Asemantic%20ambiguity%20which%20may%20lead%20to%20erroneous%20activation.%20However%2C%20they%0Aoverlook%20the%20positive%20function%20of%20some%20shared%20information%20between%20similar%0Aclasses.%20Categories%20within%20the%20same%20cluster%20share%20some%20similar%20features.%0AAllowing%20the%20model%20to%20recognize%20these%20features%20can%20further%20relieve%20the%20semantic%0Aambiguity%20between%20these%20classes.%20To%20effectively%20identify%20and%20utilize%20this%0Ashared%20information%2C%20in%20this%20paper%2C%20we%20introduce%20a%20novel%20WSSS%20framework%20called%0APrompt%20Categories%20Clustering%20%28PCC%29.%20Specifically%2C%20we%20explore%20the%20ability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20derive%20category%20clusters%20through%20prompts.%20These%0Aclusters%20effectively%20represent%20the%20intrinsic%20relationships%20between%20categories.%0ABy%20integrating%20this%20relational%20information%20into%20the%20training%20network%2C%20our%20model%0Ais%20able%20to%20better%20learn%20the%20hidden%20connections%20between%20categories.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20its%20ability%20to%0Aenhance%20performance%20on%20the%20PASCAL%20VOC%202012%20dataset%20and%20surpass%20existing%0Astate-of-the-art%20methods%20in%20WSSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt%2520Categories%2520Cluster%2520for%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DWangyu%2520Wu%2520and%2520Xianglin%2520Qiu%2520and%2520Siqi%2520Song%2520and%2520Xiaowei%2520Huang%2520and%2520Fei%2520Ma%2520and%2520Jimin%2520Xiao%26entry.1292438233%3D%2520%2520Weakly%2520Supervised%2520Semantic%2520Segmentation%2520%2528WSSS%2529%252C%2520which%2520leverages%2520image-level%250Alabels%252C%2520has%2520garnered%2520significant%2520attention%2520due%2520to%2520its%2520cost-effectiveness.%2520The%250Aprevious%2520methods%2520mainly%2520strengthen%2520the%2520inter-class%2520differences%2520to%2520avoid%2520class%250Asemantic%2520ambiguity%2520which%2520may%2520lead%2520to%2520erroneous%2520activation.%2520However%252C%2520they%250Aoverlook%2520the%2520positive%2520function%2520of%2520some%2520shared%2520information%2520between%2520similar%250Aclasses.%2520Categories%2520within%2520the%2520same%2520cluster%2520share%2520some%2520similar%2520features.%250AAllowing%2520the%2520model%2520to%2520recognize%2520these%2520features%2520can%2520further%2520relieve%2520the%2520semantic%250Aambiguity%2520between%2520these%2520classes.%2520To%2520effectively%2520identify%2520and%2520utilize%2520this%250Ashared%2520information%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520WSSS%2520framework%2520called%250APrompt%2520Categories%2520Clustering%2520%2528PCC%2529.%2520Specifically%252C%2520we%2520explore%2520the%2520ability%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520derive%2520category%2520clusters%2520through%2520prompts.%2520These%250Aclusters%2520effectively%2520represent%2520the%2520intrinsic%2520relationships%2520between%2520categories.%250ABy%2520integrating%2520this%2520relational%2520information%2520into%2520the%2520training%2520network%252C%2520our%2520model%250Ais%2520able%2520to%2520better%2520learn%2520the%2520hidden%2520connections%2520between%2520categories.%2520Experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520showing%2520its%2520ability%2520to%250Aenhance%2520performance%2520on%2520the%2520PASCAL%2520VOC%25202012%2520dataset%2520and%2520surpass%2520existing%250Astate-of-the-art%2520methods%2520in%2520WSSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt%20Categories%20Cluster%20for%20Weakly%20Supervised%20Semantic%20Segmentation&entry.906535625=Wangyu%20Wu%20and%20Xianglin%20Qiu%20and%20Siqi%20Song%20and%20Xiaowei%20Huang%20and%20Fei%20Ma%20and%20Jimin%20Xiao&entry.1292438233=%20%20Weakly%20Supervised%20Semantic%20Segmentation%20%28WSSS%29%2C%20which%20leverages%20image-level%0Alabels%2C%20has%20garnered%20significant%20attention%20due%20to%20its%20cost-effectiveness.%20The%0Aprevious%20methods%20mainly%20strengthen%20the%20inter-class%20differences%20to%20avoid%20class%0Asemantic%20ambiguity%20which%20may%20lead%20to%20erroneous%20activation.%20However%2C%20they%0Aoverlook%20the%20positive%20function%20of%20some%20shared%20information%20between%20similar%0Aclasses.%20Categories%20within%20the%20same%20cluster%20share%20some%20similar%20features.%0AAllowing%20the%20model%20to%20recognize%20these%20features%20can%20further%20relieve%20the%20semantic%0Aambiguity%20between%20these%20classes.%20To%20effectively%20identify%20and%20utilize%20this%0Ashared%20information%2C%20in%20this%20paper%2C%20we%20introduce%20a%20novel%20WSSS%20framework%20called%0APrompt%20Categories%20Clustering%20%28PCC%29.%20Specifically%2C%20we%20explore%20the%20ability%20of%0ALarge%20Language%20Models%20%28LLMs%29%20to%20derive%20category%20clusters%20through%20prompts.%20These%0Aclusters%20effectively%20represent%20the%20intrinsic%20relationships%20between%20categories.%0ABy%20integrating%20this%20relational%20information%20into%20the%20training%20network%2C%20our%20model%0Ais%20able%20to%20better%20learn%20the%20hidden%20connections%20between%20categories.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20showing%20its%20ability%20to%0Aenhance%20performance%20on%20the%20PASCAL%20VOC%202012%20dataset%20and%20surpass%20existing%0Astate-of-the-art%20methods%20in%20WSSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13823v1&entry.124074799=Read"},
{"title": "Optimal Exact Recovery in Semi-Supervised Learning: A Study of Spectral\n  Methods and Graph Convolutional Networks", "author": "Hai-Xiao Wang and Zhichao Wang", "abstract": "  We delve into the challenge of semi-supervised node classification on the\nContextual Stochastic Block Model (CSBM) dataset. Here, nodes from the\ntwo-cluster Stochastic Block Model (SBM) are coupled with feature vectors,\nwhich are derived from a Gaussian Mixture Model (GMM) that corresponds to their\nrespective node labels. With only a subset of the CSBM node labels accessible\nfor training, our primary objective becomes the accurate classification of the\nremaining nodes. Venturing into the transductive learning landscape, we, for\nthe first time, pinpoint the information-theoretical threshold for the exact\nrecovery of all test nodes in CSBM. Concurrently, we design an optimal spectral\nestimator inspired by Principal Component Analysis (PCA) with the training\nlabels and essential data from both the adjacency matrix and feature vectors.\nWe also evaluate the efficacy of graph ridge regression and Graph Convolutional\nNetworks (GCN) on this synthetic dataset. Our findings underscore that graph\nridge regression and GCN possess the ability to achieve the information\nthreshold of exact recovery in a manner akin to the optimal estimator when\nusing the optimal weighted self-loops. This highlights the potential role of\nfeature learning in augmenting the proficiency of GCN, especially in the realm\nof semi-supervised learning.\n", "link": "http://arxiv.org/abs/2412.13754v1", "date": "2024-12-18", "relevancy": 2.4008, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5049}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4805}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Exact%20Recovery%20in%20Semi-Supervised%20Learning%3A%20A%20Study%20of%20Spectral%0A%20%20Methods%20and%20Graph%20Convolutional%20Networks&body=Title%3A%20Optimal%20Exact%20Recovery%20in%20Semi-Supervised%20Learning%3A%20A%20Study%20of%20Spectral%0A%20%20Methods%20and%20Graph%20Convolutional%20Networks%0AAuthor%3A%20Hai-Xiao%20Wang%20and%20Zhichao%20Wang%0AAbstract%3A%20%20%20We%20delve%20into%20the%20challenge%20of%20semi-supervised%20node%20classification%20on%20the%0AContextual%20Stochastic%20Block%20Model%20%28CSBM%29%20dataset.%20Here%2C%20nodes%20from%20the%0Atwo-cluster%20Stochastic%20Block%20Model%20%28SBM%29%20are%20coupled%20with%20feature%20vectors%2C%0Awhich%20are%20derived%20from%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20that%20corresponds%20to%20their%0Arespective%20node%20labels.%20With%20only%20a%20subset%20of%20the%20CSBM%20node%20labels%20accessible%0Afor%20training%2C%20our%20primary%20objective%20becomes%20the%20accurate%20classification%20of%20the%0Aremaining%20nodes.%20Venturing%20into%20the%20transductive%20learning%20landscape%2C%20we%2C%20for%0Athe%20first%20time%2C%20pinpoint%20the%20information-theoretical%20threshold%20for%20the%20exact%0Arecovery%20of%20all%20test%20nodes%20in%20CSBM.%20Concurrently%2C%20we%20design%20an%20optimal%20spectral%0Aestimator%20inspired%20by%20Principal%20Component%20Analysis%20%28PCA%29%20with%20the%20training%0Alabels%20and%20essential%20data%20from%20both%20the%20adjacency%20matrix%20and%20feature%20vectors.%0AWe%20also%20evaluate%20the%20efficacy%20of%20graph%20ridge%20regression%20and%20Graph%20Convolutional%0ANetworks%20%28GCN%29%20on%20this%20synthetic%20dataset.%20Our%20findings%20underscore%20that%20graph%0Aridge%20regression%20and%20GCN%20possess%20the%20ability%20to%20achieve%20the%20information%0Athreshold%20of%20exact%20recovery%20in%20a%20manner%20akin%20to%20the%20optimal%20estimator%20when%0Ausing%20the%20optimal%20weighted%20self-loops.%20This%20highlights%20the%20potential%20role%20of%0Afeature%20learning%20in%20augmenting%20the%20proficiency%20of%20GCN%2C%20especially%20in%20the%20realm%0Aof%20semi-supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Exact%2520Recovery%2520in%2520Semi-Supervised%2520Learning%253A%2520A%2520Study%2520of%2520Spectral%250A%2520%2520Methods%2520and%2520Graph%2520Convolutional%2520Networks%26entry.906535625%3DHai-Xiao%2520Wang%2520and%2520Zhichao%2520Wang%26entry.1292438233%3D%2520%2520We%2520delve%2520into%2520the%2520challenge%2520of%2520semi-supervised%2520node%2520classification%2520on%2520the%250AContextual%2520Stochastic%2520Block%2520Model%2520%2528CSBM%2529%2520dataset.%2520Here%252C%2520nodes%2520from%2520the%250Atwo-cluster%2520Stochastic%2520Block%2520Model%2520%2528SBM%2529%2520are%2520coupled%2520with%2520feature%2520vectors%252C%250Awhich%2520are%2520derived%2520from%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520that%2520corresponds%2520to%2520their%250Arespective%2520node%2520labels.%2520With%2520only%2520a%2520subset%2520of%2520the%2520CSBM%2520node%2520labels%2520accessible%250Afor%2520training%252C%2520our%2520primary%2520objective%2520becomes%2520the%2520accurate%2520classification%2520of%2520the%250Aremaining%2520nodes.%2520Venturing%2520into%2520the%2520transductive%2520learning%2520landscape%252C%2520we%252C%2520for%250Athe%2520first%2520time%252C%2520pinpoint%2520the%2520information-theoretical%2520threshold%2520for%2520the%2520exact%250Arecovery%2520of%2520all%2520test%2520nodes%2520in%2520CSBM.%2520Concurrently%252C%2520we%2520design%2520an%2520optimal%2520spectral%250Aestimator%2520inspired%2520by%2520Principal%2520Component%2520Analysis%2520%2528PCA%2529%2520with%2520the%2520training%250Alabels%2520and%2520essential%2520data%2520from%2520both%2520the%2520adjacency%2520matrix%2520and%2520feature%2520vectors.%250AWe%2520also%2520evaluate%2520the%2520efficacy%2520of%2520graph%2520ridge%2520regression%2520and%2520Graph%2520Convolutional%250ANetworks%2520%2528GCN%2529%2520on%2520this%2520synthetic%2520dataset.%2520Our%2520findings%2520underscore%2520that%2520graph%250Aridge%2520regression%2520and%2520GCN%2520possess%2520the%2520ability%2520to%2520achieve%2520the%2520information%250Athreshold%2520of%2520exact%2520recovery%2520in%2520a%2520manner%2520akin%2520to%2520the%2520optimal%2520estimator%2520when%250Ausing%2520the%2520optimal%2520weighted%2520self-loops.%2520This%2520highlights%2520the%2520potential%2520role%2520of%250Afeature%2520learning%2520in%2520augmenting%2520the%2520proficiency%2520of%2520GCN%252C%2520especially%2520in%2520the%2520realm%250Aof%2520semi-supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Exact%20Recovery%20in%20Semi-Supervised%20Learning%3A%20A%20Study%20of%20Spectral%0A%20%20Methods%20and%20Graph%20Convolutional%20Networks&entry.906535625=Hai-Xiao%20Wang%20and%20Zhichao%20Wang&entry.1292438233=%20%20We%20delve%20into%20the%20challenge%20of%20semi-supervised%20node%20classification%20on%20the%0AContextual%20Stochastic%20Block%20Model%20%28CSBM%29%20dataset.%20Here%2C%20nodes%20from%20the%0Atwo-cluster%20Stochastic%20Block%20Model%20%28SBM%29%20are%20coupled%20with%20feature%20vectors%2C%0Awhich%20are%20derived%20from%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20that%20corresponds%20to%20their%0Arespective%20node%20labels.%20With%20only%20a%20subset%20of%20the%20CSBM%20node%20labels%20accessible%0Afor%20training%2C%20our%20primary%20objective%20becomes%20the%20accurate%20classification%20of%20the%0Aremaining%20nodes.%20Venturing%20into%20the%20transductive%20learning%20landscape%2C%20we%2C%20for%0Athe%20first%20time%2C%20pinpoint%20the%20information-theoretical%20threshold%20for%20the%20exact%0Arecovery%20of%20all%20test%20nodes%20in%20CSBM.%20Concurrently%2C%20we%20design%20an%20optimal%20spectral%0Aestimator%20inspired%20by%20Principal%20Component%20Analysis%20%28PCA%29%20with%20the%20training%0Alabels%20and%20essential%20data%20from%20both%20the%20adjacency%20matrix%20and%20feature%20vectors.%0AWe%20also%20evaluate%20the%20efficacy%20of%20graph%20ridge%20regression%20and%20Graph%20Convolutional%0ANetworks%20%28GCN%29%20on%20this%20synthetic%20dataset.%20Our%20findings%20underscore%20that%20graph%0Aridge%20regression%20and%20GCN%20possess%20the%20ability%20to%20achieve%20the%20information%0Athreshold%20of%20exact%20recovery%20in%20a%20manner%20akin%20to%20the%20optimal%20estimator%20when%0Ausing%20the%20optimal%20weighted%20self-loops.%20This%20highlights%20the%20potential%20role%20of%0Afeature%20learning%20in%20augmenting%20the%20proficiency%20of%20GCN%2C%20especially%20in%20the%20realm%0Aof%20semi-supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13754v1&entry.124074799=Read"},
{"title": "RadField3D: A Data Generator and Data Format for Deep Learning in\n  Radiation-Protection Dosimetry for Medical Applications", "author": "Felix Lehner and Pasquale Lombardo and Susana Castillo and Oliver Hupe and Marcus Magnor", "abstract": "  In this research work, we present our open-source Geant4-based Monte-Carlo\nsimulation application, called RadField3D, for generating threedimensional\nradiation field datasets for dosimetry. Accompanying, we introduce a fast,\nmachine-interpretable data format with a Python API for easy integration into\nneural network research, that we call RadFiled3D. Both developments are\nintended to be used to research alternative radiation simulation methods using\ndeep learning.\n", "link": "http://arxiv.org/abs/2412.13852v1", "date": "2024-12-18", "relevancy": 2.399, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4808}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4808}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadField3D%3A%20A%20Data%20Generator%20and%20Data%20Format%20for%20Deep%20Learning%20in%0A%20%20Radiation-Protection%20Dosimetry%20for%20Medical%20Applications&body=Title%3A%20RadField3D%3A%20A%20Data%20Generator%20and%20Data%20Format%20for%20Deep%20Learning%20in%0A%20%20Radiation-Protection%20Dosimetry%20for%20Medical%20Applications%0AAuthor%3A%20Felix%20Lehner%20and%20Pasquale%20Lombardo%20and%20Susana%20Castillo%20and%20Oliver%20Hupe%20and%20Marcus%20Magnor%0AAbstract%3A%20%20%20In%20this%20research%20work%2C%20we%20present%20our%20open-source%20Geant4-based%20Monte-Carlo%0Asimulation%20application%2C%20called%20RadField3D%2C%20for%20generating%20threedimensional%0Aradiation%20field%20datasets%20for%20dosimetry.%20Accompanying%2C%20we%20introduce%20a%20fast%2C%0Amachine-interpretable%20data%20format%20with%20a%20Python%20API%20for%20easy%20integration%20into%0Aneural%20network%20research%2C%20that%20we%20call%20RadFiled3D.%20Both%20developments%20are%0Aintended%20to%20be%20used%20to%20research%20alternative%20radiation%20simulation%20methods%20using%0Adeep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadField3D%253A%2520A%2520Data%2520Generator%2520and%2520Data%2520Format%2520for%2520Deep%2520Learning%2520in%250A%2520%2520Radiation-Protection%2520Dosimetry%2520for%2520Medical%2520Applications%26entry.906535625%3DFelix%2520Lehner%2520and%2520Pasquale%2520Lombardo%2520and%2520Susana%2520Castillo%2520and%2520Oliver%2520Hupe%2520and%2520Marcus%2520Magnor%26entry.1292438233%3D%2520%2520In%2520this%2520research%2520work%252C%2520we%2520present%2520our%2520open-source%2520Geant4-based%2520Monte-Carlo%250Asimulation%2520application%252C%2520called%2520RadField3D%252C%2520for%2520generating%2520threedimensional%250Aradiation%2520field%2520datasets%2520for%2520dosimetry.%2520Accompanying%252C%2520we%2520introduce%2520a%2520fast%252C%250Amachine-interpretable%2520data%2520format%2520with%2520a%2520Python%2520API%2520for%2520easy%2520integration%2520into%250Aneural%2520network%2520research%252C%2520that%2520we%2520call%2520RadFiled3D.%2520Both%2520developments%2520are%250Aintended%2520to%2520be%2520used%2520to%2520research%2520alternative%2520radiation%2520simulation%2520methods%2520using%250Adeep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadField3D%3A%20A%20Data%20Generator%20and%20Data%20Format%20for%20Deep%20Learning%20in%0A%20%20Radiation-Protection%20Dosimetry%20for%20Medical%20Applications&entry.906535625=Felix%20Lehner%20and%20Pasquale%20Lombardo%20and%20Susana%20Castillo%20and%20Oliver%20Hupe%20and%20Marcus%20Magnor&entry.1292438233=%20%20In%20this%20research%20work%2C%20we%20present%20our%20open-source%20Geant4-based%20Monte-Carlo%0Asimulation%20application%2C%20called%20RadField3D%2C%20for%20generating%20threedimensional%0Aradiation%20field%20datasets%20for%20dosimetry.%20Accompanying%2C%20we%20introduce%20a%20fast%2C%0Amachine-interpretable%20data%20format%20with%20a%20Python%20API%20for%20easy%20integration%20into%0Aneural%20network%20research%2C%20that%20we%20call%20RadFiled3D.%20Both%20developments%20are%0Aintended%20to%20be%20used%20to%20research%20alternative%20radiation%20simulation%20methods%20using%0Adeep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13852v1&entry.124074799=Read"},
{"title": "Two Layer Walk: A Community-Aware Graph Embedding", "author": "He Yu and Jing Liu", "abstract": "  Community structures are critical for understanding the mesoscopic\norganization of networks, bridging local and global patterns. While methods\nsuch as DeepWalk and node2vec capture local positional information through\nrandom walks, they fail to preserve community structures. Other approaches like\nmodularized nonnegative matrix factorization and evolutionary algorithms\naddress this gap but are computationally expensive and unsuitable for\nlarge-scale networks. To overcome these limitations, we propose Two Layer Walk\n(TLWalk), a novel graph embedding algorithm that incorporates hierarchical\ncommunity structures. TLWalk balances intra- and inter-community relationships\nthrough a community-aware random walk mechanism without requiring additional\nparameters. Theoretical analysis demonstrates that TLWalk effectively mitigates\nlocality bias. Experiments on benchmark datasets show that TLWalk outperforms\nstate-of-the-art methods, achieving up to 3.2% accuracy gains for link\nprediction tasks. By encoding dense local and sparse global structures, TLWalk\nproves robust and scalable across diverse networks, offering an efficient\nsolution for network analysis.\n", "link": "http://arxiv.org/abs/2412.12933v2", "date": "2024-12-18", "relevancy": 2.3928, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4904}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4727}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two%20Layer%20Walk%3A%20A%20Community-Aware%20Graph%20Embedding&body=Title%3A%20Two%20Layer%20Walk%3A%20A%20Community-Aware%20Graph%20Embedding%0AAuthor%3A%20He%20Yu%20and%20Jing%20Liu%0AAbstract%3A%20%20%20Community%20structures%20are%20critical%20for%20understanding%20the%20mesoscopic%0Aorganization%20of%20networks%2C%20bridging%20local%20and%20global%20patterns.%20While%20methods%0Asuch%20as%20DeepWalk%20and%20node2vec%20capture%20local%20positional%20information%20through%0Arandom%20walks%2C%20they%20fail%20to%20preserve%20community%20structures.%20Other%20approaches%20like%0Amodularized%20nonnegative%20matrix%20factorization%20and%20evolutionary%20algorithms%0Aaddress%20this%20gap%20but%20are%20computationally%20expensive%20and%20unsuitable%20for%0Alarge-scale%20networks.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Two%20Layer%20Walk%0A%28TLWalk%29%2C%20a%20novel%20graph%20embedding%20algorithm%20that%20incorporates%20hierarchical%0Acommunity%20structures.%20TLWalk%20balances%20intra-%20and%20inter-community%20relationships%0Athrough%20a%20community-aware%20random%20walk%20mechanism%20without%20requiring%20additional%0Aparameters.%20Theoretical%20analysis%20demonstrates%20that%20TLWalk%20effectively%20mitigates%0Alocality%20bias.%20Experiments%20on%20benchmark%20datasets%20show%20that%20TLWalk%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20up%20to%203.2%25%20accuracy%20gains%20for%20link%0Aprediction%20tasks.%20By%20encoding%20dense%20local%20and%20sparse%20global%20structures%2C%20TLWalk%0Aproves%20robust%20and%20scalable%20across%20diverse%20networks%2C%20offering%20an%20efficient%0Asolution%20for%20network%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12933v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo%2520Layer%2520Walk%253A%2520A%2520Community-Aware%2520Graph%2520Embedding%26entry.906535625%3DHe%2520Yu%2520and%2520Jing%2520Liu%26entry.1292438233%3D%2520%2520Community%2520structures%2520are%2520critical%2520for%2520understanding%2520the%2520mesoscopic%250Aorganization%2520of%2520networks%252C%2520bridging%2520local%2520and%2520global%2520patterns.%2520While%2520methods%250Asuch%2520as%2520DeepWalk%2520and%2520node2vec%2520capture%2520local%2520positional%2520information%2520through%250Arandom%2520walks%252C%2520they%2520fail%2520to%2520preserve%2520community%2520structures.%2520Other%2520approaches%2520like%250Amodularized%2520nonnegative%2520matrix%2520factorization%2520and%2520evolutionary%2520algorithms%250Aaddress%2520this%2520gap%2520but%2520are%2520computationally%2520expensive%2520and%2520unsuitable%2520for%250Alarge-scale%2520networks.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Two%2520Layer%2520Walk%250A%2528TLWalk%2529%252C%2520a%2520novel%2520graph%2520embedding%2520algorithm%2520that%2520incorporates%2520hierarchical%250Acommunity%2520structures.%2520TLWalk%2520balances%2520intra-%2520and%2520inter-community%2520relationships%250Athrough%2520a%2520community-aware%2520random%2520walk%2520mechanism%2520without%2520requiring%2520additional%250Aparameters.%2520Theoretical%2520analysis%2520demonstrates%2520that%2520TLWalk%2520effectively%2520mitigates%250Alocality%2520bias.%2520Experiments%2520on%2520benchmark%2520datasets%2520show%2520that%2520TLWalk%2520outperforms%250Astate-of-the-art%2520methods%252C%2520achieving%2520up%2520to%25203.2%2525%2520accuracy%2520gains%2520for%2520link%250Aprediction%2520tasks.%2520By%2520encoding%2520dense%2520local%2520and%2520sparse%2520global%2520structures%252C%2520TLWalk%250Aproves%2520robust%2520and%2520scalable%2520across%2520diverse%2520networks%252C%2520offering%2520an%2520efficient%250Asolution%2520for%2520network%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12933v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two%20Layer%20Walk%3A%20A%20Community-Aware%20Graph%20Embedding&entry.906535625=He%20Yu%20and%20Jing%20Liu&entry.1292438233=%20%20Community%20structures%20are%20critical%20for%20understanding%20the%20mesoscopic%0Aorganization%20of%20networks%2C%20bridging%20local%20and%20global%20patterns.%20While%20methods%0Asuch%20as%20DeepWalk%20and%20node2vec%20capture%20local%20positional%20information%20through%0Arandom%20walks%2C%20they%20fail%20to%20preserve%20community%20structures.%20Other%20approaches%20like%0Amodularized%20nonnegative%20matrix%20factorization%20and%20evolutionary%20algorithms%0Aaddress%20this%20gap%20but%20are%20computationally%20expensive%20and%20unsuitable%20for%0Alarge-scale%20networks.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Two%20Layer%20Walk%0A%28TLWalk%29%2C%20a%20novel%20graph%20embedding%20algorithm%20that%20incorporates%20hierarchical%0Acommunity%20structures.%20TLWalk%20balances%20intra-%20and%20inter-community%20relationships%0Athrough%20a%20community-aware%20random%20walk%20mechanism%20without%20requiring%20additional%0Aparameters.%20Theoretical%20analysis%20demonstrates%20that%20TLWalk%20effectively%20mitigates%0Alocality%20bias.%20Experiments%20on%20benchmark%20datasets%20show%20that%20TLWalk%20outperforms%0Astate-of-the-art%20methods%2C%20achieving%20up%20to%203.2%25%20accuracy%20gains%20for%20link%0Aprediction%20tasks.%20By%20encoding%20dense%20local%20and%20sparse%20global%20structures%2C%20TLWalk%0Aproves%20robust%20and%20scalable%20across%20diverse%20networks%2C%20offering%20an%20efficient%0Asolution%20for%20network%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12933v2&entry.124074799=Read"},
{"title": "Certification of Speaker Recognition Models to Additive Perturbations", "author": "Dmitrii Korzh and Elvir Karimov and Mikhail Pautov and Oleg Y. Rogov and Ivan Oseledets", "abstract": "  Speaker recognition technology is applied to various tasks, from personal\nvirtual assistants to secure access systems. However, the robustness of these\nsystems against adversarial attacks, particularly to additive perturbations,\nremains a significant challenge. In this paper, we pioneer applying robustness\ncertification techniques to speaker recognition, initially developed for the\nimage domain. Our work covers this gap by transferring and improving randomized\nsmoothing certification techniques against norm-bounded additive perturbations\nfor classification and few-shot learning tasks to speaker recognition. We\ndemonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for\nseveral models. We expect this work to improve the robustness of voice\nbiometrics and accelerate the research of certification methods in the audio\ndomain.\n", "link": "http://arxiv.org/abs/2404.18791v2", "date": "2024-12-18", "relevancy": 2.3855, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4853}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4745}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certification%20of%20Speaker%20Recognition%20Models%20to%20Additive%20Perturbations&body=Title%3A%20Certification%20of%20Speaker%20Recognition%20Models%20to%20Additive%20Perturbations%0AAuthor%3A%20Dmitrii%20Korzh%20and%20Elvir%20Karimov%20and%20Mikhail%20Pautov%20and%20Oleg%20Y.%20Rogov%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20Speaker%20recognition%20technology%20is%20applied%20to%20various%20tasks%2C%20from%20personal%0Avirtual%20assistants%20to%20secure%20access%20systems.%20However%2C%20the%20robustness%20of%20these%0Asystems%20against%20adversarial%20attacks%2C%20particularly%20to%20additive%20perturbations%2C%0Aremains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20pioneer%20applying%20robustness%0Acertification%20techniques%20to%20speaker%20recognition%2C%20initially%20developed%20for%20the%0Aimage%20domain.%20Our%20work%20covers%20this%20gap%20by%20transferring%20and%20improving%20randomized%0Asmoothing%20certification%20techniques%20against%20norm-bounded%20additive%20perturbations%0Afor%20classification%20and%20few-shot%20learning%20tasks%20to%20speaker%20recognition.%20We%0Ademonstrate%20the%20effectiveness%20of%20these%20methods%20on%20VoxCeleb%201%20and%202%20datasets%20for%0Aseveral%20models.%20We%20expect%20this%20work%20to%20improve%20the%20robustness%20of%20voice%0Abiometrics%20and%20accelerate%20the%20research%20of%20certification%20methods%20in%20the%20audio%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertification%2520of%2520Speaker%2520Recognition%2520Models%2520to%2520Additive%2520Perturbations%26entry.906535625%3DDmitrii%2520Korzh%2520and%2520Elvir%2520Karimov%2520and%2520Mikhail%2520Pautov%2520and%2520Oleg%2520Y.%2520Rogov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520Speaker%2520recognition%2520technology%2520is%2520applied%2520to%2520various%2520tasks%252C%2520from%2520personal%250Avirtual%2520assistants%2520to%2520secure%2520access%2520systems.%2520However%252C%2520the%2520robustness%2520of%2520these%250Asystems%2520against%2520adversarial%2520attacks%252C%2520particularly%2520to%2520additive%2520perturbations%252C%250Aremains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520pioneer%2520applying%2520robustness%250Acertification%2520techniques%2520to%2520speaker%2520recognition%252C%2520initially%2520developed%2520for%2520the%250Aimage%2520domain.%2520Our%2520work%2520covers%2520this%2520gap%2520by%2520transferring%2520and%2520improving%2520randomized%250Asmoothing%2520certification%2520techniques%2520against%2520norm-bounded%2520additive%2520perturbations%250Afor%2520classification%2520and%2520few-shot%2520learning%2520tasks%2520to%2520speaker%2520recognition.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520these%2520methods%2520on%2520VoxCeleb%25201%2520and%25202%2520datasets%2520for%250Aseveral%2520models.%2520We%2520expect%2520this%2520work%2520to%2520improve%2520the%2520robustness%2520of%2520voice%250Abiometrics%2520and%2520accelerate%2520the%2520research%2520of%2520certification%2520methods%2520in%2520the%2520audio%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certification%20of%20Speaker%20Recognition%20Models%20to%20Additive%20Perturbations&entry.906535625=Dmitrii%20Korzh%20and%20Elvir%20Karimov%20and%20Mikhail%20Pautov%20and%20Oleg%20Y.%20Rogov%20and%20Ivan%20Oseledets&entry.1292438233=%20%20Speaker%20recognition%20technology%20is%20applied%20to%20various%20tasks%2C%20from%20personal%0Avirtual%20assistants%20to%20secure%20access%20systems.%20However%2C%20the%20robustness%20of%20these%0Asystems%20against%20adversarial%20attacks%2C%20particularly%20to%20additive%20perturbations%2C%0Aremains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20pioneer%20applying%20robustness%0Acertification%20techniques%20to%20speaker%20recognition%2C%20initially%20developed%20for%20the%0Aimage%20domain.%20Our%20work%20covers%20this%20gap%20by%20transferring%20and%20improving%20randomized%0Asmoothing%20certification%20techniques%20against%20norm-bounded%20additive%20perturbations%0Afor%20classification%20and%20few-shot%20learning%20tasks%20to%20speaker%20recognition.%20We%0Ademonstrate%20the%20effectiveness%20of%20these%20methods%20on%20VoxCeleb%201%20and%202%20datasets%20for%0Aseveral%20models.%20We%20expect%20this%20work%20to%20improve%20the%20robustness%20of%20voice%0Abiometrics%20and%20accelerate%20the%20research%20of%20certification%20methods%20in%20the%20audio%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18791v2&entry.124074799=Read"},
{"title": "Photoacoustic Iterative Optimization Algorithm with Shape Prior\n  Regularization", "author": "Yu Zhang and Shuang Li and Yibing Wang and Yu Sun and Wenyi Xiang", "abstract": "  Photoacoustic imaging (PAI) suffers from inherent limitations that can\ndegrade the quality of reconstructed results, such as noise, artifacts and\nincomplete data acquisition caused by sparse sampling or partial array\ndetection. In this study, we proposed a new optimization method for both\ntwo-dimensional (2D) and three-dimensional (3D) PAI reconstruction results,\ncalled the regularized iteration method with shape prior. The shape prior is a\nprobability matrix derived from the reconstruction results of multiple sets of\nrandom partial array signals in a computational imaging system using any\nreconstruction algorithm, such as Delay-and-Sum (DAS) and Back-Projection (BP).\nIn the probability matrix, high-probability locations indicate high consistency\namong multiple reconstruction results at those positions, suggesting a high\nlikelihood of representing the true imaging results. In contrast,\nlow-probability locations indicate higher randomness, leaning more towards\nnoise or artifacts. As a shape prior, this probability matrix guides the\niteration and regularization of the entire array signal reconstruction results\nusing the original reconstruction algorithm (the same algorithm for processing\nrandom partial array signals). The method takes advantage of the property that\nthe similarity of the object to be imitated is higher than that of noise or\nartifact in the results reconstructed by multiple sets of random partial array\nsignals of the entire imaging system. The probability matrix is taken as a\nprerequisite for improving the original reconstruction results, and the\noptimizer is used to further iterate the imaging results to remove noise and\nartifacts and improve the imaging fidelity. Especially in the case involving\nsparse view which brings more artifacts, the effect is remarkable. Simulation\nand real experiments have both demonstrated the superiority of this method.\n", "link": "http://arxiv.org/abs/2412.00705v2", "date": "2024-12-18", "relevancy": 2.3827, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4862}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4744}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Photoacoustic%20Iterative%20Optimization%20Algorithm%20with%20Shape%20Prior%0A%20%20Regularization&body=Title%3A%20Photoacoustic%20Iterative%20Optimization%20Algorithm%20with%20Shape%20Prior%0A%20%20Regularization%0AAuthor%3A%20Yu%20Zhang%20and%20Shuang%20Li%20and%20Yibing%20Wang%20and%20Yu%20Sun%20and%20Wenyi%20Xiang%0AAbstract%3A%20%20%20Photoacoustic%20imaging%20%28PAI%29%20suffers%20from%20inherent%20limitations%20that%20can%0Adegrade%20the%20quality%20of%20reconstructed%20results%2C%20such%20as%20noise%2C%20artifacts%20and%0Aincomplete%20data%20acquisition%20caused%20by%20sparse%20sampling%20or%20partial%20array%0Adetection.%20In%20this%20study%2C%20we%20proposed%20a%20new%20optimization%20method%20for%20both%0Atwo-dimensional%20%282D%29%20and%20three-dimensional%20%283D%29%20PAI%20reconstruction%20results%2C%0Acalled%20the%20regularized%20iteration%20method%20with%20shape%20prior.%20The%20shape%20prior%20is%20a%0Aprobability%20matrix%20derived%20from%20the%20reconstruction%20results%20of%20multiple%20sets%20of%0Arandom%20partial%20array%20signals%20in%20a%20computational%20imaging%20system%20using%20any%0Areconstruction%20algorithm%2C%20such%20as%20Delay-and-Sum%20%28DAS%29%20and%20Back-Projection%20%28BP%29.%0AIn%20the%20probability%20matrix%2C%20high-probability%20locations%20indicate%20high%20consistency%0Aamong%20multiple%20reconstruction%20results%20at%20those%20positions%2C%20suggesting%20a%20high%0Alikelihood%20of%20representing%20the%20true%20imaging%20results.%20In%20contrast%2C%0Alow-probability%20locations%20indicate%20higher%20randomness%2C%20leaning%20more%20towards%0Anoise%20or%20artifacts.%20As%20a%20shape%20prior%2C%20this%20probability%20matrix%20guides%20the%0Aiteration%20and%20regularization%20of%20the%20entire%20array%20signal%20reconstruction%20results%0Ausing%20the%20original%20reconstruction%20algorithm%20%28the%20same%20algorithm%20for%20processing%0Arandom%20partial%20array%20signals%29.%20The%20method%20takes%20advantage%20of%20the%20property%20that%0Athe%20similarity%20of%20the%20object%20to%20be%20imitated%20is%20higher%20than%20that%20of%20noise%20or%0Aartifact%20in%20the%20results%20reconstructed%20by%20multiple%20sets%20of%20random%20partial%20array%0Asignals%20of%20the%20entire%20imaging%20system.%20The%20probability%20matrix%20is%20taken%20as%20a%0Aprerequisite%20for%20improving%20the%20original%20reconstruction%20results%2C%20and%20the%0Aoptimizer%20is%20used%20to%20further%20iterate%20the%20imaging%20results%20to%20remove%20noise%20and%0Aartifacts%20and%20improve%20the%20imaging%20fidelity.%20Especially%20in%20the%20case%20involving%0Asparse%20view%20which%20brings%20more%20artifacts%2C%20the%20effect%20is%20remarkable.%20Simulation%0Aand%20real%20experiments%20have%20both%20demonstrated%20the%20superiority%20of%20this%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.00705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhotoacoustic%2520Iterative%2520Optimization%2520Algorithm%2520with%2520Shape%2520Prior%250A%2520%2520Regularization%26entry.906535625%3DYu%2520Zhang%2520and%2520Shuang%2520Li%2520and%2520Yibing%2520Wang%2520and%2520Yu%2520Sun%2520and%2520Wenyi%2520Xiang%26entry.1292438233%3D%2520%2520Photoacoustic%2520imaging%2520%2528PAI%2529%2520suffers%2520from%2520inherent%2520limitations%2520that%2520can%250Adegrade%2520the%2520quality%2520of%2520reconstructed%2520results%252C%2520such%2520as%2520noise%252C%2520artifacts%2520and%250Aincomplete%2520data%2520acquisition%2520caused%2520by%2520sparse%2520sampling%2520or%2520partial%2520array%250Adetection.%2520In%2520this%2520study%252C%2520we%2520proposed%2520a%2520new%2520optimization%2520method%2520for%2520both%250Atwo-dimensional%2520%25282D%2529%2520and%2520three-dimensional%2520%25283D%2529%2520PAI%2520reconstruction%2520results%252C%250Acalled%2520the%2520regularized%2520iteration%2520method%2520with%2520shape%2520prior.%2520The%2520shape%2520prior%2520is%2520a%250Aprobability%2520matrix%2520derived%2520from%2520the%2520reconstruction%2520results%2520of%2520multiple%2520sets%2520of%250Arandom%2520partial%2520array%2520signals%2520in%2520a%2520computational%2520imaging%2520system%2520using%2520any%250Areconstruction%2520algorithm%252C%2520such%2520as%2520Delay-and-Sum%2520%2528DAS%2529%2520and%2520Back-Projection%2520%2528BP%2529.%250AIn%2520the%2520probability%2520matrix%252C%2520high-probability%2520locations%2520indicate%2520high%2520consistency%250Aamong%2520multiple%2520reconstruction%2520results%2520at%2520those%2520positions%252C%2520suggesting%2520a%2520high%250Alikelihood%2520of%2520representing%2520the%2520true%2520imaging%2520results.%2520In%2520contrast%252C%250Alow-probability%2520locations%2520indicate%2520higher%2520randomness%252C%2520leaning%2520more%2520towards%250Anoise%2520or%2520artifacts.%2520As%2520a%2520shape%2520prior%252C%2520this%2520probability%2520matrix%2520guides%2520the%250Aiteration%2520and%2520regularization%2520of%2520the%2520entire%2520array%2520signal%2520reconstruction%2520results%250Ausing%2520the%2520original%2520reconstruction%2520algorithm%2520%2528the%2520same%2520algorithm%2520for%2520processing%250Arandom%2520partial%2520array%2520signals%2529.%2520The%2520method%2520takes%2520advantage%2520of%2520the%2520property%2520that%250Athe%2520similarity%2520of%2520the%2520object%2520to%2520be%2520imitated%2520is%2520higher%2520than%2520that%2520of%2520noise%2520or%250Aartifact%2520in%2520the%2520results%2520reconstructed%2520by%2520multiple%2520sets%2520of%2520random%2520partial%2520array%250Asignals%2520of%2520the%2520entire%2520imaging%2520system.%2520The%2520probability%2520matrix%2520is%2520taken%2520as%2520a%250Aprerequisite%2520for%2520improving%2520the%2520original%2520reconstruction%2520results%252C%2520and%2520the%250Aoptimizer%2520is%2520used%2520to%2520further%2520iterate%2520the%2520imaging%2520results%2520to%2520remove%2520noise%2520and%250Aartifacts%2520and%2520improve%2520the%2520imaging%2520fidelity.%2520Especially%2520in%2520the%2520case%2520involving%250Asparse%2520view%2520which%2520brings%2520more%2520artifacts%252C%2520the%2520effect%2520is%2520remarkable.%2520Simulation%250Aand%2520real%2520experiments%2520have%2520both%2520demonstrated%2520the%2520superiority%2520of%2520this%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.00705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photoacoustic%20Iterative%20Optimization%20Algorithm%20with%20Shape%20Prior%0A%20%20Regularization&entry.906535625=Yu%20Zhang%20and%20Shuang%20Li%20and%20Yibing%20Wang%20and%20Yu%20Sun%20and%20Wenyi%20Xiang&entry.1292438233=%20%20Photoacoustic%20imaging%20%28PAI%29%20suffers%20from%20inherent%20limitations%20that%20can%0Adegrade%20the%20quality%20of%20reconstructed%20results%2C%20such%20as%20noise%2C%20artifacts%20and%0Aincomplete%20data%20acquisition%20caused%20by%20sparse%20sampling%20or%20partial%20array%0Adetection.%20In%20this%20study%2C%20we%20proposed%20a%20new%20optimization%20method%20for%20both%0Atwo-dimensional%20%282D%29%20and%20three-dimensional%20%283D%29%20PAI%20reconstruction%20results%2C%0Acalled%20the%20regularized%20iteration%20method%20with%20shape%20prior.%20The%20shape%20prior%20is%20a%0Aprobability%20matrix%20derived%20from%20the%20reconstruction%20results%20of%20multiple%20sets%20of%0Arandom%20partial%20array%20signals%20in%20a%20computational%20imaging%20system%20using%20any%0Areconstruction%20algorithm%2C%20such%20as%20Delay-and-Sum%20%28DAS%29%20and%20Back-Projection%20%28BP%29.%0AIn%20the%20probability%20matrix%2C%20high-probability%20locations%20indicate%20high%20consistency%0Aamong%20multiple%20reconstruction%20results%20at%20those%20positions%2C%20suggesting%20a%20high%0Alikelihood%20of%20representing%20the%20true%20imaging%20results.%20In%20contrast%2C%0Alow-probability%20locations%20indicate%20higher%20randomness%2C%20leaning%20more%20towards%0Anoise%20or%20artifacts.%20As%20a%20shape%20prior%2C%20this%20probability%20matrix%20guides%20the%0Aiteration%20and%20regularization%20of%20the%20entire%20array%20signal%20reconstruction%20results%0Ausing%20the%20original%20reconstruction%20algorithm%20%28the%20same%20algorithm%20for%20processing%0Arandom%20partial%20array%20signals%29.%20The%20method%20takes%20advantage%20of%20the%20property%20that%0Athe%20similarity%20of%20the%20object%20to%20be%20imitated%20is%20higher%20than%20that%20of%20noise%20or%0Aartifact%20in%20the%20results%20reconstructed%20by%20multiple%20sets%20of%20random%20partial%20array%0Asignals%20of%20the%20entire%20imaging%20system.%20The%20probability%20matrix%20is%20taken%20as%20a%0Aprerequisite%20for%20improving%20the%20original%20reconstruction%20results%2C%20and%20the%0Aoptimizer%20is%20used%20to%20further%20iterate%20the%20imaging%20results%20to%20remove%20noise%20and%0Aartifacts%20and%20improve%20the%20imaging%20fidelity.%20Especially%20in%20the%20case%20involving%0Asparse%20view%20which%20brings%20more%20artifacts%2C%20the%20effect%20is%20remarkable.%20Simulation%0Aand%20real%20experiments%20have%20both%20demonstrated%20the%20superiority%20of%20this%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.00705v2&entry.124074799=Read"},
{"title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning", "author": "Jie-Jing Shao and Xiao-Wen Yang and Bo-Wen Zhang and Baizhi Chen and Wen-Da Wei and Lan-Zhe Guo and Yu-feng Li", "abstract": "  Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.\n", "link": "http://arxiv.org/abs/2412.13682v1", "date": "2024-12-18", "relevancy": 2.376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4754}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChinaTravel%3A%20A%20Real-World%20Benchmark%20for%20Language%20Agents%20in%20Chinese%0A%20%20Travel%20Planning&body=Title%3A%20ChinaTravel%3A%20A%20Real-World%20Benchmark%20for%20Language%20Agents%20in%20Chinese%0A%20%20Travel%20Planning%0AAuthor%3A%20Jie-Jing%20Shao%20and%20Xiao-Wen%20Yang%20and%20Bo-Wen%20Zhang%20and%20Baizhi%20Chen%20and%20Wen-Da%20Wei%20and%20Lan-Zhe%20Guo%20and%20Yu-feng%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20LLMs%2C%20particularly%20in%20language%20reasoning%20and%20tool%0Aintegration%2C%20have%20rapidly%20sparked%20the%20real-world%20development%20of%20Language%0AAgents.%20Among%20these%2C%20travel%20planning%20represents%20a%20prominent%20domain%2C%20combining%0Aacademic%20challenges%20with%20practical%20value%20due%20to%20its%20complexity%20and%20market%0Ademand.%20However%2C%20existing%20benchmarks%20fail%20to%20reflect%20the%20diverse%2C%20real-world%0Arequirements%20crucial%20for%20deployment.%20To%20address%20this%20gap%2C%20we%20introduce%0AChinaTravel%2C%20a%20benchmark%20specifically%20designed%20for%20authentic%20Chinese%20travel%0Aplanning%20scenarios.%20We%20collect%20the%20travel%20requirements%20from%20questionnaires%20and%0Apropose%20a%20compositionally%20generalizable%20domain-specific%20language%20that%20enables%20a%0Ascalable%20evaluation%20process%2C%20covering%20feasibility%2C%20constraint%20satisfaction%2C%20and%0Apreference%20comparison.%20Empirical%20studies%20reveal%20the%20potential%20of%20neuro-symbolic%0Aagents%20in%20travel%20planning%2C%20achieving%20a%20constraint%20satisfaction%20rate%20of%2027.9%25%2C%0Asignificantly%20surpassing%20purely%20neural%20models%20at%202.6%25.%20Moreover%2C%20we%20identify%0Akey%20challenges%20in%20real-world%20travel%20planning%20deployments%2C%20including%20open%0Alanguage%20reasoning%20and%20unseen%20concept%20composition.%20These%20findings%20highlight%20the%0Asignificance%20of%20ChinaTravel%20as%20a%20pivotal%20milestone%20for%20advancing%20language%0Aagents%20in%20complex%2C%20real-world%20planning%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChinaTravel%253A%2520A%2520Real-World%2520Benchmark%2520for%2520Language%2520Agents%2520in%2520Chinese%250A%2520%2520Travel%2520Planning%26entry.906535625%3DJie-Jing%2520Shao%2520and%2520Xiao-Wen%2520Yang%2520and%2520Bo-Wen%2520Zhang%2520and%2520Baizhi%2520Chen%2520and%2520Wen-Da%2520Wei%2520and%2520Lan-Zhe%2520Guo%2520and%2520Yu-feng%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520LLMs%252C%2520particularly%2520in%2520language%2520reasoning%2520and%2520tool%250Aintegration%252C%2520have%2520rapidly%2520sparked%2520the%2520real-world%2520development%2520of%2520Language%250AAgents.%2520Among%2520these%252C%2520travel%2520planning%2520represents%2520a%2520prominent%2520domain%252C%2520combining%250Aacademic%2520challenges%2520with%2520practical%2520value%2520due%2520to%2520its%2520complexity%2520and%2520market%250Ademand.%2520However%252C%2520existing%2520benchmarks%2520fail%2520to%2520reflect%2520the%2520diverse%252C%2520real-world%250Arequirements%2520crucial%2520for%2520deployment.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AChinaTravel%252C%2520a%2520benchmark%2520specifically%2520designed%2520for%2520authentic%2520Chinese%2520travel%250Aplanning%2520scenarios.%2520We%2520collect%2520the%2520travel%2520requirements%2520from%2520questionnaires%2520and%250Apropose%2520a%2520compositionally%2520generalizable%2520domain-specific%2520language%2520that%2520enables%2520a%250Ascalable%2520evaluation%2520process%252C%2520covering%2520feasibility%252C%2520constraint%2520satisfaction%252C%2520and%250Apreference%2520comparison.%2520Empirical%2520studies%2520reveal%2520the%2520potential%2520of%2520neuro-symbolic%250Aagents%2520in%2520travel%2520planning%252C%2520achieving%2520a%2520constraint%2520satisfaction%2520rate%2520of%252027.9%2525%252C%250Asignificantly%2520surpassing%2520purely%2520neural%2520models%2520at%25202.6%2525.%2520Moreover%252C%2520we%2520identify%250Akey%2520challenges%2520in%2520real-world%2520travel%2520planning%2520deployments%252C%2520including%2520open%250Alanguage%2520reasoning%2520and%2520unseen%2520concept%2520composition.%2520These%2520findings%2520highlight%2520the%250Asignificance%2520of%2520ChinaTravel%2520as%2520a%2520pivotal%2520milestone%2520for%2520advancing%2520language%250Aagents%2520in%2520complex%252C%2520real-world%2520planning%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChinaTravel%3A%20A%20Real-World%20Benchmark%20for%20Language%20Agents%20in%20Chinese%0A%20%20Travel%20Planning&entry.906535625=Jie-Jing%20Shao%20and%20Xiao-Wen%20Yang%20and%20Bo-Wen%20Zhang%20and%20Baizhi%20Chen%20and%20Wen-Da%20Wei%20and%20Lan-Zhe%20Guo%20and%20Yu-feng%20Li&entry.1292438233=%20%20Recent%20advances%20in%20LLMs%2C%20particularly%20in%20language%20reasoning%20and%20tool%0Aintegration%2C%20have%20rapidly%20sparked%20the%20real-world%20development%20of%20Language%0AAgents.%20Among%20these%2C%20travel%20planning%20represents%20a%20prominent%20domain%2C%20combining%0Aacademic%20challenges%20with%20practical%20value%20due%20to%20its%20complexity%20and%20market%0Ademand.%20However%2C%20existing%20benchmarks%20fail%20to%20reflect%20the%20diverse%2C%20real-world%0Arequirements%20crucial%20for%20deployment.%20To%20address%20this%20gap%2C%20we%20introduce%0AChinaTravel%2C%20a%20benchmark%20specifically%20designed%20for%20authentic%20Chinese%20travel%0Aplanning%20scenarios.%20We%20collect%20the%20travel%20requirements%20from%20questionnaires%20and%0Apropose%20a%20compositionally%20generalizable%20domain-specific%20language%20that%20enables%20a%0Ascalable%20evaluation%20process%2C%20covering%20feasibility%2C%20constraint%20satisfaction%2C%20and%0Apreference%20comparison.%20Empirical%20studies%20reveal%20the%20potential%20of%20neuro-symbolic%0Aagents%20in%20travel%20planning%2C%20achieving%20a%20constraint%20satisfaction%20rate%20of%2027.9%25%2C%0Asignificantly%20surpassing%20purely%20neural%20models%20at%202.6%25.%20Moreover%2C%20we%20identify%0Akey%20challenges%20in%20real-world%20travel%20planning%20deployments%2C%20including%20open%0Alanguage%20reasoning%20and%20unseen%20concept%20composition.%20These%20findings%20highlight%20the%0Asignificance%20of%20ChinaTravel%20as%20a%20pivotal%20milestone%20for%20advancing%20language%0Aagents%20in%20complex%2C%20real-world%20planning%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13682v1&entry.124074799=Read"},
{"title": "Thinking in Space: How Multimodal Large Language Models See, Remember,\n  and Recall Spaces", "author": "Jihan Yang and Shusheng Yang and Anjali W. Gupta and Rilyn Han and Li Fei-Fei and Saining Xie", "abstract": "  Humans possess the visual-spatial intelligence to remember spaces from\nsequential visual observations. However, can Multimodal Large Language Models\n(MLLMs) trained on million-scale video datasets also ``think in space'' from\nvideos? We present a novel video-based visual-spatial intelligence benchmark\n(VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit\ncompetitive - though subhuman - visual-spatial intelligence. We probe models to\nexpress how they think in space both linguistically and visually and find that\nwhile spatial reasoning capabilities remain the primary bottleneck for MLLMs to\nreach higher benchmark performance, local world models and spatial awareness do\nemerge within these models. Notably, prevailing linguistic reasoning techniques\n(e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve\nperformance, whereas explicitly generating cognitive maps during\nquestion-answering enhances MLLMs' spatial distance ability.\n", "link": "http://arxiv.org/abs/2412.14171v1", "date": "2024-12-18", "relevancy": 2.366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6022}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20in%20Space%3A%20How%20Multimodal%20Large%20Language%20Models%20See%2C%20Remember%2C%0A%20%20and%20Recall%20Spaces&body=Title%3A%20Thinking%20in%20Space%3A%20How%20Multimodal%20Large%20Language%20Models%20See%2C%20Remember%2C%0A%20%20and%20Recall%20Spaces%0AAuthor%3A%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Anjali%20W.%20Gupta%20and%20Rilyn%20Han%20and%20Li%20Fei-Fei%20and%20Saining%20Xie%0AAbstract%3A%20%20%20Humans%20possess%20the%20visual-spatial%20intelligence%20to%20remember%20spaces%20from%0Asequential%20visual%20observations.%20However%2C%20can%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20trained%20on%20million-scale%20video%20datasets%20also%20%60%60think%20in%20space%27%27%20from%0Avideos%3F%20We%20present%20a%20novel%20video-based%20visual-spatial%20intelligence%20benchmark%0A%28VSI-Bench%29%20of%20over%205%2C000%20question-answer%20pairs%2C%20and%20find%20that%20MLLMs%20exhibit%0Acompetitive%20-%20though%20subhuman%20-%20visual-spatial%20intelligence.%20We%20probe%20models%20to%0Aexpress%20how%20they%20think%20in%20space%20both%20linguistically%20and%20visually%20and%20find%20that%0Awhile%20spatial%20reasoning%20capabilities%20remain%20the%20primary%20bottleneck%20for%20MLLMs%20to%0Areach%20higher%20benchmark%20performance%2C%20local%20world%20models%20and%20spatial%20awareness%20do%0Aemerge%20within%20these%20models.%20Notably%2C%20prevailing%20linguistic%20reasoning%20techniques%0A%28e.g.%2C%20chain-of-thought%2C%20self-consistency%2C%20tree-of-thoughts%29%20fail%20to%20improve%0Aperformance%2C%20whereas%20explicitly%20generating%20cognitive%20maps%20during%0Aquestion-answering%20enhances%20MLLMs%27%20spatial%20distance%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520in%2520Space%253A%2520How%2520Multimodal%2520Large%2520Language%2520Models%2520See%252C%2520Remember%252C%250A%2520%2520and%2520Recall%2520Spaces%26entry.906535625%3DJihan%2520Yang%2520and%2520Shusheng%2520Yang%2520and%2520Anjali%2520W.%2520Gupta%2520and%2520Rilyn%2520Han%2520and%2520Li%2520Fei-Fei%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%2520Humans%2520possess%2520the%2520visual-spatial%2520intelligence%2520to%2520remember%2520spaces%2520from%250Asequential%2520visual%2520observations.%2520However%252C%2520can%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520trained%2520on%2520million-scale%2520video%2520datasets%2520also%2520%2560%2560think%2520in%2520space%2527%2527%2520from%250Avideos%253F%2520We%2520present%2520a%2520novel%2520video-based%2520visual-spatial%2520intelligence%2520benchmark%250A%2528VSI-Bench%2529%2520of%2520over%25205%252C000%2520question-answer%2520pairs%252C%2520and%2520find%2520that%2520MLLMs%2520exhibit%250Acompetitive%2520-%2520though%2520subhuman%2520-%2520visual-spatial%2520intelligence.%2520We%2520probe%2520models%2520to%250Aexpress%2520how%2520they%2520think%2520in%2520space%2520both%2520linguistically%2520and%2520visually%2520and%2520find%2520that%250Awhile%2520spatial%2520reasoning%2520capabilities%2520remain%2520the%2520primary%2520bottleneck%2520for%2520MLLMs%2520to%250Areach%2520higher%2520benchmark%2520performance%252C%2520local%2520world%2520models%2520and%2520spatial%2520awareness%2520do%250Aemerge%2520within%2520these%2520models.%2520Notably%252C%2520prevailing%2520linguistic%2520reasoning%2520techniques%250A%2528e.g.%252C%2520chain-of-thought%252C%2520self-consistency%252C%2520tree-of-thoughts%2529%2520fail%2520to%2520improve%250Aperformance%252C%2520whereas%2520explicitly%2520generating%2520cognitive%2520maps%2520during%250Aquestion-answering%2520enhances%2520MLLMs%2527%2520spatial%2520distance%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20in%20Space%3A%20How%20Multimodal%20Large%20Language%20Models%20See%2C%20Remember%2C%0A%20%20and%20Recall%20Spaces&entry.906535625=Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Anjali%20W.%20Gupta%20and%20Rilyn%20Han%20and%20Li%20Fei-Fei%20and%20Saining%20Xie&entry.1292438233=%20%20Humans%20possess%20the%20visual-spatial%20intelligence%20to%20remember%20spaces%20from%0Asequential%20visual%20observations.%20However%2C%20can%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20trained%20on%20million-scale%20video%20datasets%20also%20%60%60think%20in%20space%27%27%20from%0Avideos%3F%20We%20present%20a%20novel%20video-based%20visual-spatial%20intelligence%20benchmark%0A%28VSI-Bench%29%20of%20over%205%2C000%20question-answer%20pairs%2C%20and%20find%20that%20MLLMs%20exhibit%0Acompetitive%20-%20though%20subhuman%20-%20visual-spatial%20intelligence.%20We%20probe%20models%20to%0Aexpress%20how%20they%20think%20in%20space%20both%20linguistically%20and%20visually%20and%20find%20that%0Awhile%20spatial%20reasoning%20capabilities%20remain%20the%20primary%20bottleneck%20for%20MLLMs%20to%0Areach%20higher%20benchmark%20performance%2C%20local%20world%20models%20and%20spatial%20awareness%20do%0Aemerge%20within%20these%20models.%20Notably%2C%20prevailing%20linguistic%20reasoning%20techniques%0A%28e.g.%2C%20chain-of-thought%2C%20self-consistency%2C%20tree-of-thoughts%29%20fail%20to%20improve%0Aperformance%2C%20whereas%20explicitly%20generating%20cognitive%20maps%20during%0Aquestion-answering%20enhances%20MLLMs%27%20spatial%20distance%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14171v1&entry.124074799=Read"},
{"title": "Joint Perception and Prediction for Autonomous Driving: A Survey", "author": "Lucas Dal'Col and Miguel Oliveira and V\u00edtor Santos", "abstract": "  Perception and prediction modules are critical components of autonomous\ndriving systems, enabling vehicles to navigate safely through complex\nenvironments. The perception module is responsible for perceiving the\nenvironment, including static and dynamic objects, while the prediction module\nis responsible for predicting the future behavior of these objects. These\nmodules are typically divided into three tasks: object detection, object\ntracking, and motion prediction. Traditionally, these tasks are developed and\noptimized independently, with outputs passed sequentially from one to the next.\nHowever, this approach has significant limitations: computational resources are\nnot shared across tasks, the lack of joint optimization can amplify errors as\nthey propagate throughout the pipeline, and uncertainty is rarely propagated\nbetween modules, resulting in significant information loss. To address these\nchallenges, the joint perception and prediction paradigm has emerged,\nintegrating perception and prediction into a unified model through multi-task\nlearning. This strategy not only overcomes the limitations of previous methods,\nbut also enables the three tasks to have direct access to raw sensor data,\nallowing richer and more nuanced environmental interpretations. This paper\npresents the first comprehensive survey of joint perception and prediction for\nautonomous driving. We propose a taxonomy that categorizes approaches based on\ninput representation, scene context modeling, and output representation,\nhighlighting their contributions and limitations. Additionally, we present a\nqualitative analysis and quantitative comparison of existing methods. Finally,\nwe discuss future research directions based on identified gaps in the\nstate-of-the-art.\n", "link": "http://arxiv.org/abs/2412.14088v1", "date": "2024-12-18", "relevancy": 2.3633, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5926}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Perception%20and%20Prediction%20for%20Autonomous%20Driving%3A%20A%20Survey&body=Title%3A%20Joint%20Perception%20and%20Prediction%20for%20Autonomous%20Driving%3A%20A%20Survey%0AAuthor%3A%20Lucas%20Dal%27Col%20and%20Miguel%20Oliveira%20and%20V%C3%ADtor%20Santos%0AAbstract%3A%20%20%20Perception%20and%20prediction%20modules%20are%20critical%20components%20of%20autonomous%0Adriving%20systems%2C%20enabling%20vehicles%20to%20navigate%20safely%20through%20complex%0Aenvironments.%20The%20perception%20module%20is%20responsible%20for%20perceiving%20the%0Aenvironment%2C%20including%20static%20and%20dynamic%20objects%2C%20while%20the%20prediction%20module%0Ais%20responsible%20for%20predicting%20the%20future%20behavior%20of%20these%20objects.%20These%0Amodules%20are%20typically%20divided%20into%20three%20tasks%3A%20object%20detection%2C%20object%0Atracking%2C%20and%20motion%20prediction.%20Traditionally%2C%20these%20tasks%20are%20developed%20and%0Aoptimized%20independently%2C%20with%20outputs%20passed%20sequentially%20from%20one%20to%20the%20next.%0AHowever%2C%20this%20approach%20has%20significant%20limitations%3A%20computational%20resources%20are%0Anot%20shared%20across%20tasks%2C%20the%20lack%20of%20joint%20optimization%20can%20amplify%20errors%20as%0Athey%20propagate%20throughout%20the%20pipeline%2C%20and%20uncertainty%20is%20rarely%20propagated%0Abetween%20modules%2C%20resulting%20in%20significant%20information%20loss.%20To%20address%20these%0Achallenges%2C%20the%20joint%20perception%20and%20prediction%20paradigm%20has%20emerged%2C%0Aintegrating%20perception%20and%20prediction%20into%20a%20unified%20model%20through%20multi-task%0Alearning.%20This%20strategy%20not%20only%20overcomes%20the%20limitations%20of%20previous%20methods%2C%0Abut%20also%20enables%20the%20three%20tasks%20to%20have%20direct%20access%20to%20raw%20sensor%20data%2C%0Aallowing%20richer%20and%20more%20nuanced%20environmental%20interpretations.%20This%20paper%0Apresents%20the%20first%20comprehensive%20survey%20of%20joint%20perception%20and%20prediction%20for%0Aautonomous%20driving.%20We%20propose%20a%20taxonomy%20that%20categorizes%20approaches%20based%20on%0Ainput%20representation%2C%20scene%20context%20modeling%2C%20and%20output%20representation%2C%0Ahighlighting%20their%20contributions%20and%20limitations.%20Additionally%2C%20we%20present%20a%0Aqualitative%20analysis%20and%20quantitative%20comparison%20of%20existing%20methods.%20Finally%2C%0Awe%20discuss%20future%20research%20directions%20based%20on%20identified%20gaps%20in%20the%0Astate-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Perception%2520and%2520Prediction%2520for%2520Autonomous%2520Driving%253A%2520A%2520Survey%26entry.906535625%3DLucas%2520Dal%2527Col%2520and%2520Miguel%2520Oliveira%2520and%2520V%25C3%25ADtor%2520Santos%26entry.1292438233%3D%2520%2520Perception%2520and%2520prediction%2520modules%2520are%2520critical%2520components%2520of%2520autonomous%250Adriving%2520systems%252C%2520enabling%2520vehicles%2520to%2520navigate%2520safely%2520through%2520complex%250Aenvironments.%2520The%2520perception%2520module%2520is%2520responsible%2520for%2520perceiving%2520the%250Aenvironment%252C%2520including%2520static%2520and%2520dynamic%2520objects%252C%2520while%2520the%2520prediction%2520module%250Ais%2520responsible%2520for%2520predicting%2520the%2520future%2520behavior%2520of%2520these%2520objects.%2520These%250Amodules%2520are%2520typically%2520divided%2520into%2520three%2520tasks%253A%2520object%2520detection%252C%2520object%250Atracking%252C%2520and%2520motion%2520prediction.%2520Traditionally%252C%2520these%2520tasks%2520are%2520developed%2520and%250Aoptimized%2520independently%252C%2520with%2520outputs%2520passed%2520sequentially%2520from%2520one%2520to%2520the%2520next.%250AHowever%252C%2520this%2520approach%2520has%2520significant%2520limitations%253A%2520computational%2520resources%2520are%250Anot%2520shared%2520across%2520tasks%252C%2520the%2520lack%2520of%2520joint%2520optimization%2520can%2520amplify%2520errors%2520as%250Athey%2520propagate%2520throughout%2520the%2520pipeline%252C%2520and%2520uncertainty%2520is%2520rarely%2520propagated%250Abetween%2520modules%252C%2520resulting%2520in%2520significant%2520information%2520loss.%2520To%2520address%2520these%250Achallenges%252C%2520the%2520joint%2520perception%2520and%2520prediction%2520paradigm%2520has%2520emerged%252C%250Aintegrating%2520perception%2520and%2520prediction%2520into%2520a%2520unified%2520model%2520through%2520multi-task%250Alearning.%2520This%2520strategy%2520not%2520only%2520overcomes%2520the%2520limitations%2520of%2520previous%2520methods%252C%250Abut%2520also%2520enables%2520the%2520three%2520tasks%2520to%2520have%2520direct%2520access%2520to%2520raw%2520sensor%2520data%252C%250Aallowing%2520richer%2520and%2520more%2520nuanced%2520environmental%2520interpretations.%2520This%2520paper%250Apresents%2520the%2520first%2520comprehensive%2520survey%2520of%2520joint%2520perception%2520and%2520prediction%2520for%250Aautonomous%2520driving.%2520We%2520propose%2520a%2520taxonomy%2520that%2520categorizes%2520approaches%2520based%2520on%250Ainput%2520representation%252C%2520scene%2520context%2520modeling%252C%2520and%2520output%2520representation%252C%250Ahighlighting%2520their%2520contributions%2520and%2520limitations.%2520Additionally%252C%2520we%2520present%2520a%250Aqualitative%2520analysis%2520and%2520quantitative%2520comparison%2520of%2520existing%2520methods.%2520Finally%252C%250Awe%2520discuss%2520future%2520research%2520directions%2520based%2520on%2520identified%2520gaps%2520in%2520the%250Astate-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Perception%20and%20Prediction%20for%20Autonomous%20Driving%3A%20A%20Survey&entry.906535625=Lucas%20Dal%27Col%20and%20Miguel%20Oliveira%20and%20V%C3%ADtor%20Santos&entry.1292438233=%20%20Perception%20and%20prediction%20modules%20are%20critical%20components%20of%20autonomous%0Adriving%20systems%2C%20enabling%20vehicles%20to%20navigate%20safely%20through%20complex%0Aenvironments.%20The%20perception%20module%20is%20responsible%20for%20perceiving%20the%0Aenvironment%2C%20including%20static%20and%20dynamic%20objects%2C%20while%20the%20prediction%20module%0Ais%20responsible%20for%20predicting%20the%20future%20behavior%20of%20these%20objects.%20These%0Amodules%20are%20typically%20divided%20into%20three%20tasks%3A%20object%20detection%2C%20object%0Atracking%2C%20and%20motion%20prediction.%20Traditionally%2C%20these%20tasks%20are%20developed%20and%0Aoptimized%20independently%2C%20with%20outputs%20passed%20sequentially%20from%20one%20to%20the%20next.%0AHowever%2C%20this%20approach%20has%20significant%20limitations%3A%20computational%20resources%20are%0Anot%20shared%20across%20tasks%2C%20the%20lack%20of%20joint%20optimization%20can%20amplify%20errors%20as%0Athey%20propagate%20throughout%20the%20pipeline%2C%20and%20uncertainty%20is%20rarely%20propagated%0Abetween%20modules%2C%20resulting%20in%20significant%20information%20loss.%20To%20address%20these%0Achallenges%2C%20the%20joint%20perception%20and%20prediction%20paradigm%20has%20emerged%2C%0Aintegrating%20perception%20and%20prediction%20into%20a%20unified%20model%20through%20multi-task%0Alearning.%20This%20strategy%20not%20only%20overcomes%20the%20limitations%20of%20previous%20methods%2C%0Abut%20also%20enables%20the%20three%20tasks%20to%20have%20direct%20access%20to%20raw%20sensor%20data%2C%0Aallowing%20richer%20and%20more%20nuanced%20environmental%20interpretations.%20This%20paper%0Apresents%20the%20first%20comprehensive%20survey%20of%20joint%20perception%20and%20prediction%20for%0Aautonomous%20driving.%20We%20propose%20a%20taxonomy%20that%20categorizes%20approaches%20based%20on%0Ainput%20representation%2C%20scene%20context%20modeling%2C%20and%20output%20representation%2C%0Ahighlighting%20their%20contributions%20and%20limitations.%20Additionally%2C%20we%20present%20a%0Aqualitative%20analysis%20and%20quantitative%20comparison%20of%20existing%20methods.%20Finally%2C%0Awe%20discuss%20future%20research%20directions%20based%20on%20identified%20gaps%20in%20the%0Astate-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14088v1&entry.124074799=Read"},
{"title": "MMO-IG: Multi-Class and Multi-Scale Object Image Generation for Remote\n  Sensing", "author": "Chuang Yang and Bingxuan Zhao and Qing Zhou and Qi Wang", "abstract": "  The rapid advancement of deep generative models (DGMs) has significantly\nadvanced research in computer vision, providing a cost-effective alternative to\nacquiring vast quantities of expensive imagery. However, existing methods\npredominantly focus on synthesizing remote sensing (RS) images aligned with\nreal images in a global layout view, which limits their applicability in RS\nimage object detection (RSIOD) research. To address these challenges, we\npropose a multi-class and multi-scale object image generator based on DGMs,\ntermed MMO-IG, designed to generate RS images with supervised object labels\nfrom global and local aspects simultaneously. Specifically, from the local\nview, MMO-IG encodes various RS instances using an iso-spacing instance map\n(ISIM). During the generation process, it decodes each instance region with\niso-spacing value in ISIM-corresponding to both background and foreground\ninstances-to produce RS images through the denoising process of diffusion\nmodels. Considering the complex interdependencies among MMOs, we construct a\nspatial-cross dependency knowledge graph (SCDKG). This ensures a realistic and\nreliable multidirectional distribution among MMOs for region embedding, thereby\nreducing the discrepancy between source and target domains. Besides, we propose\na structured object distribution instruction (SODI) to guide the generation of\nsynthesized RS image content from a global aspect with SCDKG-based ISIM\ntogether. Extensive experimental results demonstrate that our MMO-IG exhibits\nsuperior generation capabilities for RS images with dense MMO-supervised\nlabels, and RS detectors pre-trained with MMO-IG show excellent performance on\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2412.13684v1", "date": "2024-12-18", "relevancy": 2.3478, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6159}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMO-IG%3A%20Multi-Class%20and%20Multi-Scale%20Object%20Image%20Generation%20for%20Remote%0A%20%20Sensing&body=Title%3A%20MMO-IG%3A%20Multi-Class%20and%20Multi-Scale%20Object%20Image%20Generation%20for%20Remote%0A%20%20Sensing%0AAuthor%3A%20Chuang%20Yang%20and%20Bingxuan%20Zhao%20and%20Qing%20Zhou%20and%20Qi%20Wang%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20deep%20generative%20models%20%28DGMs%29%20has%20significantly%0Aadvanced%20research%20in%20computer%20vision%2C%20providing%20a%20cost-effective%20alternative%20to%0Aacquiring%20vast%20quantities%20of%20expensive%20imagery.%20However%2C%20existing%20methods%0Apredominantly%20focus%20on%20synthesizing%20remote%20sensing%20%28RS%29%20images%20aligned%20with%0Areal%20images%20in%20a%20global%20layout%20view%2C%20which%20limits%20their%20applicability%20in%20RS%0Aimage%20object%20detection%20%28RSIOD%29%20research.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20multi-class%20and%20multi-scale%20object%20image%20generator%20based%20on%20DGMs%2C%0Atermed%20MMO-IG%2C%20designed%20to%20generate%20RS%20images%20with%20supervised%20object%20labels%0Afrom%20global%20and%20local%20aspects%20simultaneously.%20Specifically%2C%20from%20the%20local%0Aview%2C%20MMO-IG%20encodes%20various%20RS%20instances%20using%20an%20iso-spacing%20instance%20map%0A%28ISIM%29.%20During%20the%20generation%20process%2C%20it%20decodes%20each%20instance%20region%20with%0Aiso-spacing%20value%20in%20ISIM-corresponding%20to%20both%20background%20and%20foreground%0Ainstances-to%20produce%20RS%20images%20through%20the%20denoising%20process%20of%20diffusion%0Amodels.%20Considering%20the%20complex%20interdependencies%20among%20MMOs%2C%20we%20construct%20a%0Aspatial-cross%20dependency%20knowledge%20graph%20%28SCDKG%29.%20This%20ensures%20a%20realistic%20and%0Areliable%20multidirectional%20distribution%20among%20MMOs%20for%20region%20embedding%2C%20thereby%0Areducing%20the%20discrepancy%20between%20source%20and%20target%20domains.%20Besides%2C%20we%20propose%0Aa%20structured%20object%20distribution%20instruction%20%28SODI%29%20to%20guide%20the%20generation%20of%0Asynthesized%20RS%20image%20content%20from%20a%20global%20aspect%20with%20SCDKG-based%20ISIM%0Atogether.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20MMO-IG%20exhibits%0Asuperior%20generation%20capabilities%20for%20RS%20images%20with%20dense%20MMO-supervised%0Alabels%2C%20and%20RS%20detectors%20pre-trained%20with%20MMO-IG%20show%20excellent%20performance%20on%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMO-IG%253A%2520Multi-Class%2520and%2520Multi-Scale%2520Object%2520Image%2520Generation%2520for%2520Remote%250A%2520%2520Sensing%26entry.906535625%3DChuang%2520Yang%2520and%2520Bingxuan%2520Zhao%2520and%2520Qing%2520Zhou%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520deep%2520generative%2520models%2520%2528DGMs%2529%2520has%2520significantly%250Aadvanced%2520research%2520in%2520computer%2520vision%252C%2520providing%2520a%2520cost-effective%2520alternative%2520to%250Aacquiring%2520vast%2520quantities%2520of%2520expensive%2520imagery.%2520However%252C%2520existing%2520methods%250Apredominantly%2520focus%2520on%2520synthesizing%2520remote%2520sensing%2520%2528RS%2529%2520images%2520aligned%2520with%250Areal%2520images%2520in%2520a%2520global%2520layout%2520view%252C%2520which%2520limits%2520their%2520applicability%2520in%2520RS%250Aimage%2520object%2520detection%2520%2528RSIOD%2529%2520research.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520multi-class%2520and%2520multi-scale%2520object%2520image%2520generator%2520based%2520on%2520DGMs%252C%250Atermed%2520MMO-IG%252C%2520designed%2520to%2520generate%2520RS%2520images%2520with%2520supervised%2520object%2520labels%250Afrom%2520global%2520and%2520local%2520aspects%2520simultaneously.%2520Specifically%252C%2520from%2520the%2520local%250Aview%252C%2520MMO-IG%2520encodes%2520various%2520RS%2520instances%2520using%2520an%2520iso-spacing%2520instance%2520map%250A%2528ISIM%2529.%2520During%2520the%2520generation%2520process%252C%2520it%2520decodes%2520each%2520instance%2520region%2520with%250Aiso-spacing%2520value%2520in%2520ISIM-corresponding%2520to%2520both%2520background%2520and%2520foreground%250Ainstances-to%2520produce%2520RS%2520images%2520through%2520the%2520denoising%2520process%2520of%2520diffusion%250Amodels.%2520Considering%2520the%2520complex%2520interdependencies%2520among%2520MMOs%252C%2520we%2520construct%2520a%250Aspatial-cross%2520dependency%2520knowledge%2520graph%2520%2528SCDKG%2529.%2520This%2520ensures%2520a%2520realistic%2520and%250Areliable%2520multidirectional%2520distribution%2520among%2520MMOs%2520for%2520region%2520embedding%252C%2520thereby%250Areducing%2520the%2520discrepancy%2520between%2520source%2520and%2520target%2520domains.%2520Besides%252C%2520we%2520propose%250Aa%2520structured%2520object%2520distribution%2520instruction%2520%2528SODI%2529%2520to%2520guide%2520the%2520generation%2520of%250Asynthesized%2520RS%2520image%2520content%2520from%2520a%2520global%2520aspect%2520with%2520SCDKG-based%2520ISIM%250Atogether.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520MMO-IG%2520exhibits%250Asuperior%2520generation%2520capabilities%2520for%2520RS%2520images%2520with%2520dense%2520MMO-supervised%250Alabels%252C%2520and%2520RS%2520detectors%2520pre-trained%2520with%2520MMO-IG%2520show%2520excellent%2520performance%2520on%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMO-IG%3A%20Multi-Class%20and%20Multi-Scale%20Object%20Image%20Generation%20for%20Remote%0A%20%20Sensing&entry.906535625=Chuang%20Yang%20and%20Bingxuan%20Zhao%20and%20Qing%20Zhou%20and%20Qi%20Wang&entry.1292438233=%20%20The%20rapid%20advancement%20of%20deep%20generative%20models%20%28DGMs%29%20has%20significantly%0Aadvanced%20research%20in%20computer%20vision%2C%20providing%20a%20cost-effective%20alternative%20to%0Aacquiring%20vast%20quantities%20of%20expensive%20imagery.%20However%2C%20existing%20methods%0Apredominantly%20focus%20on%20synthesizing%20remote%20sensing%20%28RS%29%20images%20aligned%20with%0Areal%20images%20in%20a%20global%20layout%20view%2C%20which%20limits%20their%20applicability%20in%20RS%0Aimage%20object%20detection%20%28RSIOD%29%20research.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20multi-class%20and%20multi-scale%20object%20image%20generator%20based%20on%20DGMs%2C%0Atermed%20MMO-IG%2C%20designed%20to%20generate%20RS%20images%20with%20supervised%20object%20labels%0Afrom%20global%20and%20local%20aspects%20simultaneously.%20Specifically%2C%20from%20the%20local%0Aview%2C%20MMO-IG%20encodes%20various%20RS%20instances%20using%20an%20iso-spacing%20instance%20map%0A%28ISIM%29.%20During%20the%20generation%20process%2C%20it%20decodes%20each%20instance%20region%20with%0Aiso-spacing%20value%20in%20ISIM-corresponding%20to%20both%20background%20and%20foreground%0Ainstances-to%20produce%20RS%20images%20through%20the%20denoising%20process%20of%20diffusion%0Amodels.%20Considering%20the%20complex%20interdependencies%20among%20MMOs%2C%20we%20construct%20a%0Aspatial-cross%20dependency%20knowledge%20graph%20%28SCDKG%29.%20This%20ensures%20a%20realistic%20and%0Areliable%20multidirectional%20distribution%20among%20MMOs%20for%20region%20embedding%2C%20thereby%0Areducing%20the%20discrepancy%20between%20source%20and%20target%20domains.%20Besides%2C%20we%20propose%0Aa%20structured%20object%20distribution%20instruction%20%28SODI%29%20to%20guide%20the%20generation%20of%0Asynthesized%20RS%20image%20content%20from%20a%20global%20aspect%20with%20SCDKG-based%20ISIM%0Atogether.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20MMO-IG%20exhibits%0Asuperior%20generation%20capabilities%20for%20RS%20images%20with%20dense%20MMO-supervised%0Alabels%2C%20and%20RS%20detectors%20pre-trained%20with%20MMO-IG%20show%20excellent%20performance%20on%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13684v1&entry.124074799=Read"},
{"title": "VQTalker: Towards Multilingual Talking Avatars through Facial Motion\n  Tokenization", "author": "Tao Liu and Ziyang Ma and Qi Chen and Feilong Chen and Shuai Fan and Xie Chen and Kai Yu", "abstract": "  We present VQTalker, a Vector Quantization-based framework for multilingual\ntalking head generation that addresses the challenges of lip synchronization\nand natural motion across diverse languages. Our approach is grounded in the\nphonetic principle that human speech comprises a finite set of distinct sound\nunits (phonemes) and corresponding visual articulations (visemes), which often\nshare commonalities across languages. We introduce a facial motion tokenizer\nbased on Group Residual Finite Scalar Quantization (GRFSQ), which creates a\ndiscretized representation of facial features. This method enables\ncomprehensive capture of facial movements while improving generalization to\nmultiple languages, even with limited training data. Building on this quantized\nrepresentation, we implement a coarse-to-fine motion generation process that\nprogressively refines facial animations. Extensive experiments demonstrate that\nVQTalker achieves state-of-the-art performance in both video-driven and\nspeech-driven scenarios, particularly in multilingual settings. Notably, our\nmethod achieves high-quality results at a resolution of 512*512 pixels while\nmaintaining a lower bitrate of approximately 11 kbps. Our work opens new\npossibilities for cross-lingual talking face generation. Synthetic results can\nbe viewed at https://x-lance.github.io/VQTalker.\n", "link": "http://arxiv.org/abs/2412.09892v2", "date": "2024-12-18", "relevancy": 2.3431, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5887}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5887}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VQTalker%3A%20Towards%20Multilingual%20Talking%20Avatars%20through%20Facial%20Motion%0A%20%20Tokenization&body=Title%3A%20VQTalker%3A%20Towards%20Multilingual%20Talking%20Avatars%20through%20Facial%20Motion%0A%20%20Tokenization%0AAuthor%3A%20Tao%20Liu%20and%20Ziyang%20Ma%20and%20Qi%20Chen%20and%20Feilong%20Chen%20and%20Shuai%20Fan%20and%20Xie%20Chen%20and%20Kai%20Yu%0AAbstract%3A%20%20%20We%20present%20VQTalker%2C%20a%20Vector%20Quantization-based%20framework%20for%20multilingual%0Atalking%20head%20generation%20that%20addresses%20the%20challenges%20of%20lip%20synchronization%0Aand%20natural%20motion%20across%20diverse%20languages.%20Our%20approach%20is%20grounded%20in%20the%0Aphonetic%20principle%20that%20human%20speech%20comprises%20a%20finite%20set%20of%20distinct%20sound%0Aunits%20%28phonemes%29%20and%20corresponding%20visual%20articulations%20%28visemes%29%2C%20which%20often%0Ashare%20commonalities%20across%20languages.%20We%20introduce%20a%20facial%20motion%20tokenizer%0Abased%20on%20Group%20Residual%20Finite%20Scalar%20Quantization%20%28GRFSQ%29%2C%20which%20creates%20a%0Adiscretized%20representation%20of%20facial%20features.%20This%20method%20enables%0Acomprehensive%20capture%20of%20facial%20movements%20while%20improving%20generalization%20to%0Amultiple%20languages%2C%20even%20with%20limited%20training%20data.%20Building%20on%20this%20quantized%0Arepresentation%2C%20we%20implement%20a%20coarse-to-fine%20motion%20generation%20process%20that%0Aprogressively%20refines%20facial%20animations.%20Extensive%20experiments%20demonstrate%20that%0AVQTalker%20achieves%20state-of-the-art%20performance%20in%20both%20video-driven%20and%0Aspeech-driven%20scenarios%2C%20particularly%20in%20multilingual%20settings.%20Notably%2C%20our%0Amethod%20achieves%20high-quality%20results%20at%20a%20resolution%20of%20512%2A512%20pixels%20while%0Amaintaining%20a%20lower%20bitrate%20of%20approximately%2011%20kbps.%20Our%20work%20opens%20new%0Apossibilities%20for%20cross-lingual%20talking%20face%20generation.%20Synthetic%20results%20can%0Abe%20viewed%20at%20https%3A//x-lance.github.io/VQTalker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09892v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVQTalker%253A%2520Towards%2520Multilingual%2520Talking%2520Avatars%2520through%2520Facial%2520Motion%250A%2520%2520Tokenization%26entry.906535625%3DTao%2520Liu%2520and%2520Ziyang%2520Ma%2520and%2520Qi%2520Chen%2520and%2520Feilong%2520Chen%2520and%2520Shuai%2520Fan%2520and%2520Xie%2520Chen%2520and%2520Kai%2520Yu%26entry.1292438233%3D%2520%2520We%2520present%2520VQTalker%252C%2520a%2520Vector%2520Quantization-based%2520framework%2520for%2520multilingual%250Atalking%2520head%2520generation%2520that%2520addresses%2520the%2520challenges%2520of%2520lip%2520synchronization%250Aand%2520natural%2520motion%2520across%2520diverse%2520languages.%2520Our%2520approach%2520is%2520grounded%2520in%2520the%250Aphonetic%2520principle%2520that%2520human%2520speech%2520comprises%2520a%2520finite%2520set%2520of%2520distinct%2520sound%250Aunits%2520%2528phonemes%2529%2520and%2520corresponding%2520visual%2520articulations%2520%2528visemes%2529%252C%2520which%2520often%250Ashare%2520commonalities%2520across%2520languages.%2520We%2520introduce%2520a%2520facial%2520motion%2520tokenizer%250Abased%2520on%2520Group%2520Residual%2520Finite%2520Scalar%2520Quantization%2520%2528GRFSQ%2529%252C%2520which%2520creates%2520a%250Adiscretized%2520representation%2520of%2520facial%2520features.%2520This%2520method%2520enables%250Acomprehensive%2520capture%2520of%2520facial%2520movements%2520while%2520improving%2520generalization%2520to%250Amultiple%2520languages%252C%2520even%2520with%2520limited%2520training%2520data.%2520Building%2520on%2520this%2520quantized%250Arepresentation%252C%2520we%2520implement%2520a%2520coarse-to-fine%2520motion%2520generation%2520process%2520that%250Aprogressively%2520refines%2520facial%2520animations.%2520Extensive%2520experiments%2520demonstrate%2520that%250AVQTalker%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520video-driven%2520and%250Aspeech-driven%2520scenarios%252C%2520particularly%2520in%2520multilingual%2520settings.%2520Notably%252C%2520our%250Amethod%2520achieves%2520high-quality%2520results%2520at%2520a%2520resolution%2520of%2520512%252A512%2520pixels%2520while%250Amaintaining%2520a%2520lower%2520bitrate%2520of%2520approximately%252011%2520kbps.%2520Our%2520work%2520opens%2520new%250Apossibilities%2520for%2520cross-lingual%2520talking%2520face%2520generation.%2520Synthetic%2520results%2520can%250Abe%2520viewed%2520at%2520https%253A//x-lance.github.io/VQTalker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09892v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VQTalker%3A%20Towards%20Multilingual%20Talking%20Avatars%20through%20Facial%20Motion%0A%20%20Tokenization&entry.906535625=Tao%20Liu%20and%20Ziyang%20Ma%20and%20Qi%20Chen%20and%20Feilong%20Chen%20and%20Shuai%20Fan%20and%20Xie%20Chen%20and%20Kai%20Yu&entry.1292438233=%20%20We%20present%20VQTalker%2C%20a%20Vector%20Quantization-based%20framework%20for%20multilingual%0Atalking%20head%20generation%20that%20addresses%20the%20challenges%20of%20lip%20synchronization%0Aand%20natural%20motion%20across%20diverse%20languages.%20Our%20approach%20is%20grounded%20in%20the%0Aphonetic%20principle%20that%20human%20speech%20comprises%20a%20finite%20set%20of%20distinct%20sound%0Aunits%20%28phonemes%29%20and%20corresponding%20visual%20articulations%20%28visemes%29%2C%20which%20often%0Ashare%20commonalities%20across%20languages.%20We%20introduce%20a%20facial%20motion%20tokenizer%0Abased%20on%20Group%20Residual%20Finite%20Scalar%20Quantization%20%28GRFSQ%29%2C%20which%20creates%20a%0Adiscretized%20representation%20of%20facial%20features.%20This%20method%20enables%0Acomprehensive%20capture%20of%20facial%20movements%20while%20improving%20generalization%20to%0Amultiple%20languages%2C%20even%20with%20limited%20training%20data.%20Building%20on%20this%20quantized%0Arepresentation%2C%20we%20implement%20a%20coarse-to-fine%20motion%20generation%20process%20that%0Aprogressively%20refines%20facial%20animations.%20Extensive%20experiments%20demonstrate%20that%0AVQTalker%20achieves%20state-of-the-art%20performance%20in%20both%20video-driven%20and%0Aspeech-driven%20scenarios%2C%20particularly%20in%20multilingual%20settings.%20Notably%2C%20our%0Amethod%20achieves%20high-quality%20results%20at%20a%20resolution%20of%20512%2A512%20pixels%20while%0Amaintaining%20a%20lower%20bitrate%20of%20approximately%2011%20kbps.%20Our%20work%20opens%20new%0Apossibilities%20for%20cross-lingual%20talking%20face%20generation.%20Synthetic%20results%20can%0Abe%20viewed%20at%20https%3A//x-lance.github.io/VQTalker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09892v2&entry.124074799=Read"},
{"title": "An Efficient Occupancy World Model via Decoupled Dynamic Flow and\n  Image-assisted Training", "author": "Haiming Zhang and Ying Xue and Xu Yan and Jiacheng Zhang and Weichao Qiu and Dongfeng Bai and Bingbing Liu and Shuguang Cui and Zhen Li", "abstract": "  The field of autonomous driving is experiencing a surge of interest in world\nmodels, which aim to predict potential future scenarios based on historical\nobservations. In this paper, we introduce DFIT-OccWorld, an efficient 3D\noccupancy world model that leverages decoupled dynamic flow and image-assisted\ntraining strategy, substantially improving 4D scene forecasting performance. To\nsimplify the training process, we discard the previous two-stage training\nstrategy and innovatively reformulate the occupancy forecasting problem as a\ndecoupled voxels warping process. Our model forecasts future dynamic voxels by\nwarping existing observations using voxel flow, whereas static voxels are\neasily obtained through pose transformation. Moreover, our method incorporates\nan image-assisted training paradigm to enhance prediction reliability.\nSpecifically, differentiable volume rendering is adopted to generate rendered\ndepth maps through predicted future volumes, which are adopted in render-based\nphotometric consistency. Experiments demonstrate the effectiveness of our\napproach, showcasing its state-of-the-art performance on the nuScenes and\nOpenScene benchmarks for 4D occupancy forecasting, end-to-end motion planning\nand point cloud forecasting. Concretely, it achieves state-of-the-art\nperformances compared to existing 3D world models while incurring substantially\nlower computational costs.\n", "link": "http://arxiv.org/abs/2412.13772v1", "date": "2024-12-18", "relevancy": 2.3391, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5867}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5867}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Occupancy%20World%20Model%20via%20Decoupled%20Dynamic%20Flow%20and%0A%20%20Image-assisted%20Training&body=Title%3A%20An%20Efficient%20Occupancy%20World%20Model%20via%20Decoupled%20Dynamic%20Flow%20and%0A%20%20Image-assisted%20Training%0AAuthor%3A%20Haiming%20Zhang%20and%20Ying%20Xue%20and%20Xu%20Yan%20and%20Jiacheng%20Zhang%20and%20Weichao%20Qiu%20and%20Dongfeng%20Bai%20and%20Bingbing%20Liu%20and%20Shuguang%20Cui%20and%20Zhen%20Li%0AAbstract%3A%20%20%20The%20field%20of%20autonomous%20driving%20is%20experiencing%20a%20surge%20of%20interest%20in%20world%0Amodels%2C%20which%20aim%20to%20predict%20potential%20future%20scenarios%20based%20on%20historical%0Aobservations.%20In%20this%20paper%2C%20we%20introduce%20DFIT-OccWorld%2C%20an%20efficient%203D%0Aoccupancy%20world%20model%20that%20leverages%20decoupled%20dynamic%20flow%20and%20image-assisted%0Atraining%20strategy%2C%20substantially%20improving%204D%20scene%20forecasting%20performance.%20To%0Asimplify%20the%20training%20process%2C%20we%20discard%20the%20previous%20two-stage%20training%0Astrategy%20and%20innovatively%20reformulate%20the%20occupancy%20forecasting%20problem%20as%20a%0Adecoupled%20voxels%20warping%20process.%20Our%20model%20forecasts%20future%20dynamic%20voxels%20by%0Awarping%20existing%20observations%20using%20voxel%20flow%2C%20whereas%20static%20voxels%20are%0Aeasily%20obtained%20through%20pose%20transformation.%20Moreover%2C%20our%20method%20incorporates%0Aan%20image-assisted%20training%20paradigm%20to%20enhance%20prediction%20reliability.%0ASpecifically%2C%20differentiable%20volume%20rendering%20is%20adopted%20to%20generate%20rendered%0Adepth%20maps%20through%20predicted%20future%20volumes%2C%20which%20are%20adopted%20in%20render-based%0Aphotometric%20consistency.%20Experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20showcasing%20its%20state-of-the-art%20performance%20on%20the%20nuScenes%20and%0AOpenScene%20benchmarks%20for%204D%20occupancy%20forecasting%2C%20end-to-end%20motion%20planning%0Aand%20point%20cloud%20forecasting.%20Concretely%2C%20it%20achieves%20state-of-the-art%0Aperformances%20compared%20to%20existing%203D%20world%20models%20while%20incurring%20substantially%0Alower%20computational%20costs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Occupancy%2520World%2520Model%2520via%2520Decoupled%2520Dynamic%2520Flow%2520and%250A%2520%2520Image-assisted%2520Training%26entry.906535625%3DHaiming%2520Zhang%2520and%2520Ying%2520Xue%2520and%2520Xu%2520Yan%2520and%2520Jiacheng%2520Zhang%2520and%2520Weichao%2520Qiu%2520and%2520Dongfeng%2520Bai%2520and%2520Bingbing%2520Liu%2520and%2520Shuguang%2520Cui%2520and%2520Zhen%2520Li%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520autonomous%2520driving%2520is%2520experiencing%2520a%2520surge%2520of%2520interest%2520in%2520world%250Amodels%252C%2520which%2520aim%2520to%2520predict%2520potential%2520future%2520scenarios%2520based%2520on%2520historical%250Aobservations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DFIT-OccWorld%252C%2520an%2520efficient%25203D%250Aoccupancy%2520world%2520model%2520that%2520leverages%2520decoupled%2520dynamic%2520flow%2520and%2520image-assisted%250Atraining%2520strategy%252C%2520substantially%2520improving%25204D%2520scene%2520forecasting%2520performance.%2520To%250Asimplify%2520the%2520training%2520process%252C%2520we%2520discard%2520the%2520previous%2520two-stage%2520training%250Astrategy%2520and%2520innovatively%2520reformulate%2520the%2520occupancy%2520forecasting%2520problem%2520as%2520a%250Adecoupled%2520voxels%2520warping%2520process.%2520Our%2520model%2520forecasts%2520future%2520dynamic%2520voxels%2520by%250Awarping%2520existing%2520observations%2520using%2520voxel%2520flow%252C%2520whereas%2520static%2520voxels%2520are%250Aeasily%2520obtained%2520through%2520pose%2520transformation.%2520Moreover%252C%2520our%2520method%2520incorporates%250Aan%2520image-assisted%2520training%2520paradigm%2520to%2520enhance%2520prediction%2520reliability.%250ASpecifically%252C%2520differentiable%2520volume%2520rendering%2520is%2520adopted%2520to%2520generate%2520rendered%250Adepth%2520maps%2520through%2520predicted%2520future%2520volumes%252C%2520which%2520are%2520adopted%2520in%2520render-based%250Aphotometric%2520consistency.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%252C%2520showcasing%2520its%2520state-of-the-art%2520performance%2520on%2520the%2520nuScenes%2520and%250AOpenScene%2520benchmarks%2520for%25204D%2520occupancy%2520forecasting%252C%2520end-to-end%2520motion%2520planning%250Aand%2520point%2520cloud%2520forecasting.%2520Concretely%252C%2520it%2520achieves%2520state-of-the-art%250Aperformances%2520compared%2520to%2520existing%25203D%2520world%2520models%2520while%2520incurring%2520substantially%250Alower%2520computational%2520costs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Occupancy%20World%20Model%20via%20Decoupled%20Dynamic%20Flow%20and%0A%20%20Image-assisted%20Training&entry.906535625=Haiming%20Zhang%20and%20Ying%20Xue%20and%20Xu%20Yan%20and%20Jiacheng%20Zhang%20and%20Weichao%20Qiu%20and%20Dongfeng%20Bai%20and%20Bingbing%20Liu%20and%20Shuguang%20Cui%20and%20Zhen%20Li&entry.1292438233=%20%20The%20field%20of%20autonomous%20driving%20is%20experiencing%20a%20surge%20of%20interest%20in%20world%0Amodels%2C%20which%20aim%20to%20predict%20potential%20future%20scenarios%20based%20on%20historical%0Aobservations.%20In%20this%20paper%2C%20we%20introduce%20DFIT-OccWorld%2C%20an%20efficient%203D%0Aoccupancy%20world%20model%20that%20leverages%20decoupled%20dynamic%20flow%20and%20image-assisted%0Atraining%20strategy%2C%20substantially%20improving%204D%20scene%20forecasting%20performance.%20To%0Asimplify%20the%20training%20process%2C%20we%20discard%20the%20previous%20two-stage%20training%0Astrategy%20and%20innovatively%20reformulate%20the%20occupancy%20forecasting%20problem%20as%20a%0Adecoupled%20voxels%20warping%20process.%20Our%20model%20forecasts%20future%20dynamic%20voxels%20by%0Awarping%20existing%20observations%20using%20voxel%20flow%2C%20whereas%20static%20voxels%20are%0Aeasily%20obtained%20through%20pose%20transformation.%20Moreover%2C%20our%20method%20incorporates%0Aan%20image-assisted%20training%20paradigm%20to%20enhance%20prediction%20reliability.%0ASpecifically%2C%20differentiable%20volume%20rendering%20is%20adopted%20to%20generate%20rendered%0Adepth%20maps%20through%20predicted%20future%20volumes%2C%20which%20are%20adopted%20in%20render-based%0Aphotometric%20consistency.%20Experiments%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%2C%20showcasing%20its%20state-of-the-art%20performance%20on%20the%20nuScenes%20and%0AOpenScene%20benchmarks%20for%204D%20occupancy%20forecasting%2C%20end-to-end%20motion%20planning%0Aand%20point%20cloud%20forecasting.%20Concretely%2C%20it%20achieves%20state-of-the-art%0Aperformances%20compared%20to%20existing%203D%20world%20models%20while%20incurring%20substantially%0Alower%20computational%20costs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13772v1&entry.124074799=Read"},
{"title": "MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data", "author": "Hanwen Jiang and Zexiang Xu and Desai Xie and Ziwen Chen and Haian Jin and Fujun Luan and Zhixin Shu and Kai Zhang and Sai Bi and Xin Sun and Jiuxiang Gu and Qixing Huang and Georgios Pavlakos and Hao Tan", "abstract": "  We propose scaling up 3D scene reconstruction by training with synthesized\ndata. At the core of our work is MegaSynth, a procedurally generated 3D dataset\ncomprising 700K scenes - over 50 times larger than the prior real dataset DL3DV\n- dramatically scaling the training data. To enable scalable data generation,\nour key idea is eliminating semantic information, removing the need to model\ncomplex semantic priors such as object affordances and scene composition.\nInstead, we model scenes with basic spatial structures and geometry primitives,\noffering scalability. Besides, we control data complexity to facilitate\ntraining while loosely aligning it with real-world data distribution to benefit\nreal-world generalization. We explore training LRMs with both MegaSynth and\navailable real data. Experiment results show that joint training or\npre-training with MegaSynth improves reconstruction quality by 1.2 to 1.8 dB\nPSNR across diverse image domains. Moreover, models trained solely on MegaSynth\nperform comparably to those trained on real data, underscoring the low-level\nnature of 3D reconstruction. Additionally, we provide an in-depth analysis of\nMegaSynth's properties for enhancing model capability, training stability, and\ngeneralization.\n", "link": "http://arxiv.org/abs/2412.14166v1", "date": "2024-12-18", "relevancy": 2.3357, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6062}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5875}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaSynth%3A%20Scaling%20Up%203D%20Scene%20Reconstruction%20with%20Synthesized%20Data&body=Title%3A%20MegaSynth%3A%20Scaling%20Up%203D%20Scene%20Reconstruction%20with%20Synthesized%20Data%0AAuthor%3A%20Hanwen%20Jiang%20and%20Zexiang%20Xu%20and%20Desai%20Xie%20and%20Ziwen%20Chen%20and%20Haian%20Jin%20and%20Fujun%20Luan%20and%20Zhixin%20Shu%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Xin%20Sun%20and%20Jiuxiang%20Gu%20and%20Qixing%20Huang%20and%20Georgios%20Pavlakos%20and%20Hao%20Tan%0AAbstract%3A%20%20%20We%20propose%20scaling%20up%203D%20scene%20reconstruction%20by%20training%20with%20synthesized%0Adata.%20At%20the%20core%20of%20our%20work%20is%20MegaSynth%2C%20a%20procedurally%20generated%203D%20dataset%0Acomprising%20700K%20scenes%20-%20over%2050%20times%20larger%20than%20the%20prior%20real%20dataset%20DL3DV%0A-%20dramatically%20scaling%20the%20training%20data.%20To%20enable%20scalable%20data%20generation%2C%0Aour%20key%20idea%20is%20eliminating%20semantic%20information%2C%20removing%20the%20need%20to%20model%0Acomplex%20semantic%20priors%20such%20as%20object%20affordances%20and%20scene%20composition.%0AInstead%2C%20we%20model%20scenes%20with%20basic%20spatial%20structures%20and%20geometry%20primitives%2C%0Aoffering%20scalability.%20Besides%2C%20we%20control%20data%20complexity%20to%20facilitate%0Atraining%20while%20loosely%20aligning%20it%20with%20real-world%20data%20distribution%20to%20benefit%0Areal-world%20generalization.%20We%20explore%20training%20LRMs%20with%20both%20MegaSynth%20and%0Aavailable%20real%20data.%20Experiment%20results%20show%20that%20joint%20training%20or%0Apre-training%20with%20MegaSynth%20improves%20reconstruction%20quality%20by%201.2%20to%201.8%20dB%0APSNR%20across%20diverse%20image%20domains.%20Moreover%2C%20models%20trained%20solely%20on%20MegaSynth%0Aperform%20comparably%20to%20those%20trained%20on%20real%20data%2C%20underscoring%20the%20low-level%0Anature%20of%203D%20reconstruction.%20Additionally%2C%20we%20provide%20an%20in-depth%20analysis%20of%0AMegaSynth%27s%20properties%20for%20enhancing%20model%20capability%2C%20training%20stability%2C%20and%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14166v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaSynth%253A%2520Scaling%2520Up%25203D%2520Scene%2520Reconstruction%2520with%2520Synthesized%2520Data%26entry.906535625%3DHanwen%2520Jiang%2520and%2520Zexiang%2520Xu%2520and%2520Desai%2520Xie%2520and%2520Ziwen%2520Chen%2520and%2520Haian%2520Jin%2520and%2520Fujun%2520Luan%2520and%2520Zhixin%2520Shu%2520and%2520Kai%2520Zhang%2520and%2520Sai%2520Bi%2520and%2520Xin%2520Sun%2520and%2520Jiuxiang%2520Gu%2520and%2520Qixing%2520Huang%2520and%2520Georgios%2520Pavlakos%2520and%2520Hao%2520Tan%26entry.1292438233%3D%2520%2520We%2520propose%2520scaling%2520up%25203D%2520scene%2520reconstruction%2520by%2520training%2520with%2520synthesized%250Adata.%2520At%2520the%2520core%2520of%2520our%2520work%2520is%2520MegaSynth%252C%2520a%2520procedurally%2520generated%25203D%2520dataset%250Acomprising%2520700K%2520scenes%2520-%2520over%252050%2520times%2520larger%2520than%2520the%2520prior%2520real%2520dataset%2520DL3DV%250A-%2520dramatically%2520scaling%2520the%2520training%2520data.%2520To%2520enable%2520scalable%2520data%2520generation%252C%250Aour%2520key%2520idea%2520is%2520eliminating%2520semantic%2520information%252C%2520removing%2520the%2520need%2520to%2520model%250Acomplex%2520semantic%2520priors%2520such%2520as%2520object%2520affordances%2520and%2520scene%2520composition.%250AInstead%252C%2520we%2520model%2520scenes%2520with%2520basic%2520spatial%2520structures%2520and%2520geometry%2520primitives%252C%250Aoffering%2520scalability.%2520Besides%252C%2520we%2520control%2520data%2520complexity%2520to%2520facilitate%250Atraining%2520while%2520loosely%2520aligning%2520it%2520with%2520real-world%2520data%2520distribution%2520to%2520benefit%250Areal-world%2520generalization.%2520We%2520explore%2520training%2520LRMs%2520with%2520both%2520MegaSynth%2520and%250Aavailable%2520real%2520data.%2520Experiment%2520results%2520show%2520that%2520joint%2520training%2520or%250Apre-training%2520with%2520MegaSynth%2520improves%2520reconstruction%2520quality%2520by%25201.2%2520to%25201.8%2520dB%250APSNR%2520across%2520diverse%2520image%2520domains.%2520Moreover%252C%2520models%2520trained%2520solely%2520on%2520MegaSynth%250Aperform%2520comparably%2520to%2520those%2520trained%2520on%2520real%2520data%252C%2520underscoring%2520the%2520low-level%250Anature%2520of%25203D%2520reconstruction.%2520Additionally%252C%2520we%2520provide%2520an%2520in-depth%2520analysis%2520of%250AMegaSynth%2527s%2520properties%2520for%2520enhancing%2520model%2520capability%252C%2520training%2520stability%252C%2520and%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14166v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaSynth%3A%20Scaling%20Up%203D%20Scene%20Reconstruction%20with%20Synthesized%20Data&entry.906535625=Hanwen%20Jiang%20and%20Zexiang%20Xu%20and%20Desai%20Xie%20and%20Ziwen%20Chen%20and%20Haian%20Jin%20and%20Fujun%20Luan%20and%20Zhixin%20Shu%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Xin%20Sun%20and%20Jiuxiang%20Gu%20and%20Qixing%20Huang%20and%20Georgios%20Pavlakos%20and%20Hao%20Tan&entry.1292438233=%20%20We%20propose%20scaling%20up%203D%20scene%20reconstruction%20by%20training%20with%20synthesized%0Adata.%20At%20the%20core%20of%20our%20work%20is%20MegaSynth%2C%20a%20procedurally%20generated%203D%20dataset%0Acomprising%20700K%20scenes%20-%20over%2050%20times%20larger%20than%20the%20prior%20real%20dataset%20DL3DV%0A-%20dramatically%20scaling%20the%20training%20data.%20To%20enable%20scalable%20data%20generation%2C%0Aour%20key%20idea%20is%20eliminating%20semantic%20information%2C%20removing%20the%20need%20to%20model%0Acomplex%20semantic%20priors%20such%20as%20object%20affordances%20and%20scene%20composition.%0AInstead%2C%20we%20model%20scenes%20with%20basic%20spatial%20structures%20and%20geometry%20primitives%2C%0Aoffering%20scalability.%20Besides%2C%20we%20control%20data%20complexity%20to%20facilitate%0Atraining%20while%20loosely%20aligning%20it%20with%20real-world%20data%20distribution%20to%20benefit%0Areal-world%20generalization.%20We%20explore%20training%20LRMs%20with%20both%20MegaSynth%20and%0Aavailable%20real%20data.%20Experiment%20results%20show%20that%20joint%20training%20or%0Apre-training%20with%20MegaSynth%20improves%20reconstruction%20quality%20by%201.2%20to%201.8%20dB%0APSNR%20across%20diverse%20image%20domains.%20Moreover%2C%20models%20trained%20solely%20on%20MegaSynth%0Aperform%20comparably%20to%20those%20trained%20on%20real%20data%2C%20underscoring%20the%20low-level%0Anature%20of%203D%20reconstruction.%20Additionally%2C%20we%20provide%20an%20in-depth%20analysis%20of%0AMegaSynth%27s%20properties%20for%20enhancing%20model%20capability%2C%20training%20stability%2C%20and%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14166v1&entry.124074799=Read"},
{"title": "MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling", "author": "Weihao Yuan and Weichao Shen and Yisheng He and Yuan Dong and Xiaodong Gu and Zilong Dong and Liefeng Bo and Qixing Huang", "abstract": "  Motion generation from discrete quantization offers many advantages over\ncontinuous regression, but at the cost of inevitable approximation errors.\nPrevious methods usually quantize the entire body pose into one code, which not\nonly faces the difficulty in encoding all joints within one vector but also\nloses the spatial relationship between different joints. Differently, in this\nwork we quantize each individual joint into one vector, which i) simplifies the\nquantization process as the complexity associated with a single joint is\nmarkedly lower than that of the entire pose; ii) maintains a spatial-temporal\nstructure that preserves both the spatial relationships among joints and the\ntemporal movement patterns; iii) yields a 2D token map, which enables the\napplication of various 2D operations widely used in 2D images. Grounded in the\n2D motion quantization, we build a spatial-temporal modeling framework, where\n2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D\nattention are proposed to take advantage of spatial-temporal signals among the\n2D tokens. Extensive experiments demonstrate that our method significantly\noutperforms previous methods across different datasets, with a 26.6% decrease\nof FID on HumanML3D and a 29.9% decrease on KIT-ML. Project page:\nhttps://aigc3d.github.io/mogents.\n", "link": "http://arxiv.org/abs/2409.17686v2", "date": "2024-12-18", "relevancy": 2.3353, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5936}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5809}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGenTS%3A%20Motion%20Generation%20based%20on%20Spatial-Temporal%20Joint%20Modeling&body=Title%3A%20MoGenTS%3A%20Motion%20Generation%20based%20on%20Spatial-Temporal%20Joint%20Modeling%0AAuthor%3A%20Weihao%20Yuan%20and%20Weichao%20Shen%20and%20Yisheng%20He%20and%20Yuan%20Dong%20and%20Xiaodong%20Gu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Qixing%20Huang%0AAbstract%3A%20%20%20Motion%20generation%20from%20discrete%20quantization%20offers%20many%20advantages%20over%0Acontinuous%20regression%2C%20but%20at%20the%20cost%20of%20inevitable%20approximation%20errors.%0APrevious%20methods%20usually%20quantize%20the%20entire%20body%20pose%20into%20one%20code%2C%20which%20not%0Aonly%20faces%20the%20difficulty%20in%20encoding%20all%20joints%20within%20one%20vector%20but%20also%0Aloses%20the%20spatial%20relationship%20between%20different%20joints.%20Differently%2C%20in%20this%0Awork%20we%20quantize%20each%20individual%20joint%20into%20one%20vector%2C%20which%20i%29%20simplifies%20the%0Aquantization%20process%20as%20the%20complexity%20associated%20with%20a%20single%20joint%20is%0Amarkedly%20lower%20than%20that%20of%20the%20entire%20pose%3B%20ii%29%20maintains%20a%20spatial-temporal%0Astructure%20that%20preserves%20both%20the%20spatial%20relationships%20among%20joints%20and%20the%0Atemporal%20movement%20patterns%3B%20iii%29%20yields%20a%202D%20token%20map%2C%20which%20enables%20the%0Aapplication%20of%20various%202D%20operations%20widely%20used%20in%202D%20images.%20Grounded%20in%20the%0A2D%20motion%20quantization%2C%20we%20build%20a%20spatial-temporal%20modeling%20framework%2C%20where%0A2D%20joint%20VQVAE%2C%20temporal-spatial%202D%20masking%20technique%2C%20and%20spatial-temporal%202D%0Aattention%20are%20proposed%20to%20take%20advantage%20of%20spatial-temporal%20signals%20among%20the%0A2D%20tokens.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20previous%20methods%20across%20different%20datasets%2C%20with%20a%2026.6%25%20decrease%0Aof%20FID%20on%20HumanML3D%20and%20a%2029.9%25%20decrease%20on%20KIT-ML.%20Project%20page%3A%0Ahttps%3A//aigc3d.github.io/mogents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17686v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGenTS%253A%2520Motion%2520Generation%2520based%2520on%2520Spatial-Temporal%2520Joint%2520Modeling%26entry.906535625%3DWeihao%2520Yuan%2520and%2520Weichao%2520Shen%2520and%2520Yisheng%2520He%2520and%2520Yuan%2520Dong%2520and%2520Xiaodong%2520Gu%2520and%2520Zilong%2520Dong%2520and%2520Liefeng%2520Bo%2520and%2520Qixing%2520Huang%26entry.1292438233%3D%2520%2520Motion%2520generation%2520from%2520discrete%2520quantization%2520offers%2520many%2520advantages%2520over%250Acontinuous%2520regression%252C%2520but%2520at%2520the%2520cost%2520of%2520inevitable%2520approximation%2520errors.%250APrevious%2520methods%2520usually%2520quantize%2520the%2520entire%2520body%2520pose%2520into%2520one%2520code%252C%2520which%2520not%250Aonly%2520faces%2520the%2520difficulty%2520in%2520encoding%2520all%2520joints%2520within%2520one%2520vector%2520but%2520also%250Aloses%2520the%2520spatial%2520relationship%2520between%2520different%2520joints.%2520Differently%252C%2520in%2520this%250Awork%2520we%2520quantize%2520each%2520individual%2520joint%2520into%2520one%2520vector%252C%2520which%2520i%2529%2520simplifies%2520the%250Aquantization%2520process%2520as%2520the%2520complexity%2520associated%2520with%2520a%2520single%2520joint%2520is%250Amarkedly%2520lower%2520than%2520that%2520of%2520the%2520entire%2520pose%253B%2520ii%2529%2520maintains%2520a%2520spatial-temporal%250Astructure%2520that%2520preserves%2520both%2520the%2520spatial%2520relationships%2520among%2520joints%2520and%2520the%250Atemporal%2520movement%2520patterns%253B%2520iii%2529%2520yields%2520a%25202D%2520token%2520map%252C%2520which%2520enables%2520the%250Aapplication%2520of%2520various%25202D%2520operations%2520widely%2520used%2520in%25202D%2520images.%2520Grounded%2520in%2520the%250A2D%2520motion%2520quantization%252C%2520we%2520build%2520a%2520spatial-temporal%2520modeling%2520framework%252C%2520where%250A2D%2520joint%2520VQVAE%252C%2520temporal-spatial%25202D%2520masking%2520technique%252C%2520and%2520spatial-temporal%25202D%250Aattention%2520are%2520proposed%2520to%2520take%2520advantage%2520of%2520spatial-temporal%2520signals%2520among%2520the%250A2D%2520tokens.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520significantly%250Aoutperforms%2520previous%2520methods%2520across%2520different%2520datasets%252C%2520with%2520a%252026.6%2525%2520decrease%250Aof%2520FID%2520on%2520HumanML3D%2520and%2520a%252029.9%2525%2520decrease%2520on%2520KIT-ML.%2520Project%2520page%253A%250Ahttps%253A//aigc3d.github.io/mogents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17686v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGenTS%3A%20Motion%20Generation%20based%20on%20Spatial-Temporal%20Joint%20Modeling&entry.906535625=Weihao%20Yuan%20and%20Weichao%20Shen%20and%20Yisheng%20He%20and%20Yuan%20Dong%20and%20Xiaodong%20Gu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Qixing%20Huang&entry.1292438233=%20%20Motion%20generation%20from%20discrete%20quantization%20offers%20many%20advantages%20over%0Acontinuous%20regression%2C%20but%20at%20the%20cost%20of%20inevitable%20approximation%20errors.%0APrevious%20methods%20usually%20quantize%20the%20entire%20body%20pose%20into%20one%20code%2C%20which%20not%0Aonly%20faces%20the%20difficulty%20in%20encoding%20all%20joints%20within%20one%20vector%20but%20also%0Aloses%20the%20spatial%20relationship%20between%20different%20joints.%20Differently%2C%20in%20this%0Awork%20we%20quantize%20each%20individual%20joint%20into%20one%20vector%2C%20which%20i%29%20simplifies%20the%0Aquantization%20process%20as%20the%20complexity%20associated%20with%20a%20single%20joint%20is%0Amarkedly%20lower%20than%20that%20of%20the%20entire%20pose%3B%20ii%29%20maintains%20a%20spatial-temporal%0Astructure%20that%20preserves%20both%20the%20spatial%20relationships%20among%20joints%20and%20the%0Atemporal%20movement%20patterns%3B%20iii%29%20yields%20a%202D%20token%20map%2C%20which%20enables%20the%0Aapplication%20of%20various%202D%20operations%20widely%20used%20in%202D%20images.%20Grounded%20in%20the%0A2D%20motion%20quantization%2C%20we%20build%20a%20spatial-temporal%20modeling%20framework%2C%20where%0A2D%20joint%20VQVAE%2C%20temporal-spatial%202D%20masking%20technique%2C%20and%20spatial-temporal%202D%0Aattention%20are%20proposed%20to%20take%20advantage%20of%20spatial-temporal%20signals%20among%20the%0A2D%20tokens.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20significantly%0Aoutperforms%20previous%20methods%20across%20different%20datasets%2C%20with%20a%2026.6%25%20decrease%0Aof%20FID%20on%20HumanML3D%20and%20a%2029.9%25%20decrease%20on%20KIT-ML.%20Project%20page%3A%0Ahttps%3A//aigc3d.github.io/mogents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17686v2&entry.124074799=Read"},
{"title": "Maybe you are looking for CroQS: Cross-modal Query Suggestion for\n  Text-to-Image Retrieval", "author": "Giacomo Pacini and Fabio Carrara and Nicola Messina and Nicola Tonellotto and Giuseppe Amato and Fabrizio Falchi", "abstract": "  Query suggestion, a technique widely adopted in information retrieval,\nenhances system interactivity and the browsing experience of document\ncollections. In cross-modal retrieval, many works have focused on retrieving\nrelevant items from natural language queries, while few have explored query\nsuggestion solutions. In this work, we address query suggestion in cross-modal\nretrieval, introducing a novel task that focuses on suggesting minimal textual\nmodifications needed to explore visually consistent subsets of the collection,\nfollowing the premise of ''Maybe you are looking for''. To facilitate the\nevaluation and development of methods, we present a tailored benchmark named\nCroQS. This dataset comprises initial queries, grouped result sets, and\nhuman-defined suggested queries for each group. We establish dedicated metrics\nto rigorously evaluate the performance of various methods on this task,\nmeasuring representativeness, cluster specificity, and similarity of the\nsuggested queries to the original ones. Baseline methods from related fields,\nsuch as image captioning and content summarization, are adapted for this task\nto provide reference performance scores. Although relatively far from human\nperformance, our experiments reveal that both LLM-based and captioning-based\nmethods achieve competitive results on CroQS, improving the recall on cluster\nspecificity by more than 115% and representativeness mAP by more than 52% with\nrespect to the initial query. The dataset, the implementation of the baseline\nmethods and the notebooks containing our experiments are available here:\nhttps://paciosoft.com/CroQS-benchmark/\n", "link": "http://arxiv.org/abs/2412.13834v1", "date": "2024-12-18", "relevancy": 2.3312, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Maybe%20you%20are%20looking%20for%20CroQS%3A%20Cross-modal%20Query%20Suggestion%20for%0A%20%20Text-to-Image%20Retrieval&body=Title%3A%20Maybe%20you%20are%20looking%20for%20CroQS%3A%20Cross-modal%20Query%20Suggestion%20for%0A%20%20Text-to-Image%20Retrieval%0AAuthor%3A%20Giacomo%20Pacini%20and%20Fabio%20Carrara%20and%20Nicola%20Messina%20and%20Nicola%20Tonellotto%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi%0AAbstract%3A%20%20%20Query%20suggestion%2C%20a%20technique%20widely%20adopted%20in%20information%20retrieval%2C%0Aenhances%20system%20interactivity%20and%20the%20browsing%20experience%20of%20document%0Acollections.%20In%20cross-modal%20retrieval%2C%20many%20works%20have%20focused%20on%20retrieving%0Arelevant%20items%20from%20natural%20language%20queries%2C%20while%20few%20have%20explored%20query%0Asuggestion%20solutions.%20In%20this%20work%2C%20we%20address%20query%20suggestion%20in%20cross-modal%0Aretrieval%2C%20introducing%20a%20novel%20task%20that%20focuses%20on%20suggesting%20minimal%20textual%0Amodifications%20needed%20to%20explore%20visually%20consistent%20subsets%20of%20the%20collection%2C%0Afollowing%20the%20premise%20of%20%27%27Maybe%20you%20are%20looking%20for%27%27.%20To%20facilitate%20the%0Aevaluation%20and%20development%20of%20methods%2C%20we%20present%20a%20tailored%20benchmark%20named%0ACroQS.%20This%20dataset%20comprises%20initial%20queries%2C%20grouped%20result%20sets%2C%20and%0Ahuman-defined%20suggested%20queries%20for%20each%20group.%20We%20establish%20dedicated%20metrics%0Ato%20rigorously%20evaluate%20the%20performance%20of%20various%20methods%20on%20this%20task%2C%0Ameasuring%20representativeness%2C%20cluster%20specificity%2C%20and%20similarity%20of%20the%0Asuggested%20queries%20to%20the%20original%20ones.%20Baseline%20methods%20from%20related%20fields%2C%0Asuch%20as%20image%20captioning%20and%20content%20summarization%2C%20are%20adapted%20for%20this%20task%0Ato%20provide%20reference%20performance%20scores.%20Although%20relatively%20far%20from%20human%0Aperformance%2C%20our%20experiments%20reveal%20that%20both%20LLM-based%20and%20captioning-based%0Amethods%20achieve%20competitive%20results%20on%20CroQS%2C%20improving%20the%20recall%20on%20cluster%0Aspecificity%20by%20more%20than%20115%25%20and%20representativeness%20mAP%20by%20more%20than%2052%25%20with%0Arespect%20to%20the%20initial%20query.%20The%20dataset%2C%20the%20implementation%20of%20the%20baseline%0Amethods%20and%20the%20notebooks%20containing%20our%20experiments%20are%20available%20here%3A%0Ahttps%3A//paciosoft.com/CroQS-benchmark/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaybe%2520you%2520are%2520looking%2520for%2520CroQS%253A%2520Cross-modal%2520Query%2520Suggestion%2520for%250A%2520%2520Text-to-Image%2520Retrieval%26entry.906535625%3DGiacomo%2520Pacini%2520and%2520Fabio%2520Carrara%2520and%2520Nicola%2520Messina%2520and%2520Nicola%2520Tonellotto%2520and%2520Giuseppe%2520Amato%2520and%2520Fabrizio%2520Falchi%26entry.1292438233%3D%2520%2520Query%2520suggestion%252C%2520a%2520technique%2520widely%2520adopted%2520in%2520information%2520retrieval%252C%250Aenhances%2520system%2520interactivity%2520and%2520the%2520browsing%2520experience%2520of%2520document%250Acollections.%2520In%2520cross-modal%2520retrieval%252C%2520many%2520works%2520have%2520focused%2520on%2520retrieving%250Arelevant%2520items%2520from%2520natural%2520language%2520queries%252C%2520while%2520few%2520have%2520explored%2520query%250Asuggestion%2520solutions.%2520In%2520this%2520work%252C%2520we%2520address%2520query%2520suggestion%2520in%2520cross-modal%250Aretrieval%252C%2520introducing%2520a%2520novel%2520task%2520that%2520focuses%2520on%2520suggesting%2520minimal%2520textual%250Amodifications%2520needed%2520to%2520explore%2520visually%2520consistent%2520subsets%2520of%2520the%2520collection%252C%250Afollowing%2520the%2520premise%2520of%2520%2527%2527Maybe%2520you%2520are%2520looking%2520for%2527%2527.%2520To%2520facilitate%2520the%250Aevaluation%2520and%2520development%2520of%2520methods%252C%2520we%2520present%2520a%2520tailored%2520benchmark%2520named%250ACroQS.%2520This%2520dataset%2520comprises%2520initial%2520queries%252C%2520grouped%2520result%2520sets%252C%2520and%250Ahuman-defined%2520suggested%2520queries%2520for%2520each%2520group.%2520We%2520establish%2520dedicated%2520metrics%250Ato%2520rigorously%2520evaluate%2520the%2520performance%2520of%2520various%2520methods%2520on%2520this%2520task%252C%250Ameasuring%2520representativeness%252C%2520cluster%2520specificity%252C%2520and%2520similarity%2520of%2520the%250Asuggested%2520queries%2520to%2520the%2520original%2520ones.%2520Baseline%2520methods%2520from%2520related%2520fields%252C%250Asuch%2520as%2520image%2520captioning%2520and%2520content%2520summarization%252C%2520are%2520adapted%2520for%2520this%2520task%250Ato%2520provide%2520reference%2520performance%2520scores.%2520Although%2520relatively%2520far%2520from%2520human%250Aperformance%252C%2520our%2520experiments%2520reveal%2520that%2520both%2520LLM-based%2520and%2520captioning-based%250Amethods%2520achieve%2520competitive%2520results%2520on%2520CroQS%252C%2520improving%2520the%2520recall%2520on%2520cluster%250Aspecificity%2520by%2520more%2520than%2520115%2525%2520and%2520representativeness%2520mAP%2520by%2520more%2520than%252052%2525%2520with%250Arespect%2520to%2520the%2520initial%2520query.%2520The%2520dataset%252C%2520the%2520implementation%2520of%2520the%2520baseline%250Amethods%2520and%2520the%2520notebooks%2520containing%2520our%2520experiments%2520are%2520available%2520here%253A%250Ahttps%253A//paciosoft.com/CroQS-benchmark/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Maybe%20you%20are%20looking%20for%20CroQS%3A%20Cross-modal%20Query%20Suggestion%20for%0A%20%20Text-to-Image%20Retrieval&entry.906535625=Giacomo%20Pacini%20and%20Fabio%20Carrara%20and%20Nicola%20Messina%20and%20Nicola%20Tonellotto%20and%20Giuseppe%20Amato%20and%20Fabrizio%20Falchi&entry.1292438233=%20%20Query%20suggestion%2C%20a%20technique%20widely%20adopted%20in%20information%20retrieval%2C%0Aenhances%20system%20interactivity%20and%20the%20browsing%20experience%20of%20document%0Acollections.%20In%20cross-modal%20retrieval%2C%20many%20works%20have%20focused%20on%20retrieving%0Arelevant%20items%20from%20natural%20language%20queries%2C%20while%20few%20have%20explored%20query%0Asuggestion%20solutions.%20In%20this%20work%2C%20we%20address%20query%20suggestion%20in%20cross-modal%0Aretrieval%2C%20introducing%20a%20novel%20task%20that%20focuses%20on%20suggesting%20minimal%20textual%0Amodifications%20needed%20to%20explore%20visually%20consistent%20subsets%20of%20the%20collection%2C%0Afollowing%20the%20premise%20of%20%27%27Maybe%20you%20are%20looking%20for%27%27.%20To%20facilitate%20the%0Aevaluation%20and%20development%20of%20methods%2C%20we%20present%20a%20tailored%20benchmark%20named%0ACroQS.%20This%20dataset%20comprises%20initial%20queries%2C%20grouped%20result%20sets%2C%20and%0Ahuman-defined%20suggested%20queries%20for%20each%20group.%20We%20establish%20dedicated%20metrics%0Ato%20rigorously%20evaluate%20the%20performance%20of%20various%20methods%20on%20this%20task%2C%0Ameasuring%20representativeness%2C%20cluster%20specificity%2C%20and%20similarity%20of%20the%0Asuggested%20queries%20to%20the%20original%20ones.%20Baseline%20methods%20from%20related%20fields%2C%0Asuch%20as%20image%20captioning%20and%20content%20summarization%2C%20are%20adapted%20for%20this%20task%0Ato%20provide%20reference%20performance%20scores.%20Although%20relatively%20far%20from%20human%0Aperformance%2C%20our%20experiments%20reveal%20that%20both%20LLM-based%20and%20captioning-based%0Amethods%20achieve%20competitive%20results%20on%20CroQS%2C%20improving%20the%20recall%20on%20cluster%0Aspecificity%20by%20more%20than%20115%25%20and%20representativeness%20mAP%20by%20more%20than%2052%25%20with%0Arespect%20to%20the%20initial%20query.%20The%20dataset%2C%20the%20implementation%20of%20the%20baseline%0Amethods%20and%20the%20notebooks%20containing%20our%20experiments%20are%20available%20here%3A%0Ahttps%3A//paciosoft.com/CroQS-benchmark/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13834v1&entry.124074799=Read"},
{"title": "Adversarial Robustness of Link Sign Prediction in Signed Graphs", "author": "Jialong Zhou and Xing Ai and Yuni Lai and Tomasz Michalak and Gaolei Li and Jianhua Li and Kai Zhou", "abstract": "  Signed graphs serve as fundamental data structures for representing positive\nand negative relationships in social networks, with signed graph neural\nnetworks (SGNNs) emerging as the primary tool for their analysis. Our\ninvestigation reveals that balance theory, while essential for modeling signed\nrelationships in SGNNs, inadvertently introduces exploitable vulnerabilities to\nblack-box attacks. To demonstrate this vulnerability, we propose\nbalance-attack, a novel adversarial strategy specifically designed to\ncompromise graph balance degree, and develop an efficient heuristic algorithm\nto solve the associated NP-hard optimization problem. While existing approaches\nattempt to restore attacked graphs through balance learning techniques, they\nface a critical challenge we term \"Irreversibility of Balance-related\nInformation,\" where restored edges fail to align with original attack targets.\nTo address this limitation, we introduce Balance Augmented-Signed Graph\nContrastive Learning (BA-SGCL), an innovative framework that combines\ncontrastive learning with balance augmentation techniques to achieve robust\ngraph representations. By maintaining high balance degree in the latent space,\nBA-SGCL effectively circumvents the irreversibility challenge and enhances\nmodel resilience. Extensive experiments across multiple SGNN architectures and\nreal-world datasets demonstrate both the effectiveness of our proposed\nbalance-attack and the superior robustness of BA-SGCL, advancing the security\nand reliability of signed graph analysis in social networks. Datasets and codes\nof the proposed framework are at the github repository\nhttps://anonymous.4open.science/r/BA-SGCL-submit-DF41/.\n", "link": "http://arxiv.org/abs/2401.10590v2", "date": "2024-12-18", "relevancy": 2.3239, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4861}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4596}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Robustness%20of%20Link%20Sign%20Prediction%20in%20Signed%20Graphs&body=Title%3A%20Adversarial%20Robustness%20of%20Link%20Sign%20Prediction%20in%20Signed%20Graphs%0AAuthor%3A%20Jialong%20Zhou%20and%20Xing%20Ai%20and%20Yuni%20Lai%20and%20Tomasz%20Michalak%20and%20Gaolei%20Li%20and%20Jianhua%20Li%20and%20Kai%20Zhou%0AAbstract%3A%20%20%20Signed%20graphs%20serve%20as%20fundamental%20data%20structures%20for%20representing%20positive%0Aand%20negative%20relationships%20in%20social%20networks%2C%20with%20signed%20graph%20neural%0Anetworks%20%28SGNNs%29%20emerging%20as%20the%20primary%20tool%20for%20their%20analysis.%20Our%0Ainvestigation%20reveals%20that%20balance%20theory%2C%20while%20essential%20for%20modeling%20signed%0Arelationships%20in%20SGNNs%2C%20inadvertently%20introduces%20exploitable%20vulnerabilities%20to%0Ablack-box%20attacks.%20To%20demonstrate%20this%20vulnerability%2C%20we%20propose%0Abalance-attack%2C%20a%20novel%20adversarial%20strategy%20specifically%20designed%20to%0Acompromise%20graph%20balance%20degree%2C%20and%20develop%20an%20efficient%20heuristic%20algorithm%0Ato%20solve%20the%20associated%20NP-hard%20optimization%20problem.%20While%20existing%20approaches%0Aattempt%20to%20restore%20attacked%20graphs%20through%20balance%20learning%20techniques%2C%20they%0Aface%20a%20critical%20challenge%20we%20term%20%22Irreversibility%20of%20Balance-related%0AInformation%2C%22%20where%20restored%20edges%20fail%20to%20align%20with%20original%20attack%20targets.%0ATo%20address%20this%20limitation%2C%20we%20introduce%20Balance%20Augmented-Signed%20Graph%0AContrastive%20Learning%20%28BA-SGCL%29%2C%20an%20innovative%20framework%20that%20combines%0Acontrastive%20learning%20with%20balance%20augmentation%20techniques%20to%20achieve%20robust%0Agraph%20representations.%20By%20maintaining%20high%20balance%20degree%20in%20the%20latent%20space%2C%0ABA-SGCL%20effectively%20circumvents%20the%20irreversibility%20challenge%20and%20enhances%0Amodel%20resilience.%20Extensive%20experiments%20across%20multiple%20SGNN%20architectures%20and%0Areal-world%20datasets%20demonstrate%20both%20the%20effectiveness%20of%20our%20proposed%0Abalance-attack%20and%20the%20superior%20robustness%20of%20BA-SGCL%2C%20advancing%20the%20security%0Aand%20reliability%20of%20signed%20graph%20analysis%20in%20social%20networks.%20Datasets%20and%20codes%0Aof%20the%20proposed%20framework%20are%20at%20the%20github%20repository%0Ahttps%3A//anonymous.4open.science/r/BA-SGCL-submit-DF41/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.10590v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Robustness%2520of%2520Link%2520Sign%2520Prediction%2520in%2520Signed%2520Graphs%26entry.906535625%3DJialong%2520Zhou%2520and%2520Xing%2520Ai%2520and%2520Yuni%2520Lai%2520and%2520Tomasz%2520Michalak%2520and%2520Gaolei%2520Li%2520and%2520Jianhua%2520Li%2520and%2520Kai%2520Zhou%26entry.1292438233%3D%2520%2520Signed%2520graphs%2520serve%2520as%2520fundamental%2520data%2520structures%2520for%2520representing%2520positive%250Aand%2520negative%2520relationships%2520in%2520social%2520networks%252C%2520with%2520signed%2520graph%2520neural%250Anetworks%2520%2528SGNNs%2529%2520emerging%2520as%2520the%2520primary%2520tool%2520for%2520their%2520analysis.%2520Our%250Ainvestigation%2520reveals%2520that%2520balance%2520theory%252C%2520while%2520essential%2520for%2520modeling%2520signed%250Arelationships%2520in%2520SGNNs%252C%2520inadvertently%2520introduces%2520exploitable%2520vulnerabilities%2520to%250Ablack-box%2520attacks.%2520To%2520demonstrate%2520this%2520vulnerability%252C%2520we%2520propose%250Abalance-attack%252C%2520a%2520novel%2520adversarial%2520strategy%2520specifically%2520designed%2520to%250Acompromise%2520graph%2520balance%2520degree%252C%2520and%2520develop%2520an%2520efficient%2520heuristic%2520algorithm%250Ato%2520solve%2520the%2520associated%2520NP-hard%2520optimization%2520problem.%2520While%2520existing%2520approaches%250Aattempt%2520to%2520restore%2520attacked%2520graphs%2520through%2520balance%2520learning%2520techniques%252C%2520they%250Aface%2520a%2520critical%2520challenge%2520we%2520term%2520%2522Irreversibility%2520of%2520Balance-related%250AInformation%252C%2522%2520where%2520restored%2520edges%2520fail%2520to%2520align%2520with%2520original%2520attack%2520targets.%250ATo%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520Balance%2520Augmented-Signed%2520Graph%250AContrastive%2520Learning%2520%2528BA-SGCL%2529%252C%2520an%2520innovative%2520framework%2520that%2520combines%250Acontrastive%2520learning%2520with%2520balance%2520augmentation%2520techniques%2520to%2520achieve%2520robust%250Agraph%2520representations.%2520By%2520maintaining%2520high%2520balance%2520degree%2520in%2520the%2520latent%2520space%252C%250ABA-SGCL%2520effectively%2520circumvents%2520the%2520irreversibility%2520challenge%2520and%2520enhances%250Amodel%2520resilience.%2520Extensive%2520experiments%2520across%2520multiple%2520SGNN%2520architectures%2520and%250Areal-world%2520datasets%2520demonstrate%2520both%2520the%2520effectiveness%2520of%2520our%2520proposed%250Abalance-attack%2520and%2520the%2520superior%2520robustness%2520of%2520BA-SGCL%252C%2520advancing%2520the%2520security%250Aand%2520reliability%2520of%2520signed%2520graph%2520analysis%2520in%2520social%2520networks.%2520Datasets%2520and%2520codes%250Aof%2520the%2520proposed%2520framework%2520are%2520at%2520the%2520github%2520repository%250Ahttps%253A//anonymous.4open.science/r/BA-SGCL-submit-DF41/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.10590v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Robustness%20of%20Link%20Sign%20Prediction%20in%20Signed%20Graphs&entry.906535625=Jialong%20Zhou%20and%20Xing%20Ai%20and%20Yuni%20Lai%20and%20Tomasz%20Michalak%20and%20Gaolei%20Li%20and%20Jianhua%20Li%20and%20Kai%20Zhou&entry.1292438233=%20%20Signed%20graphs%20serve%20as%20fundamental%20data%20structures%20for%20representing%20positive%0Aand%20negative%20relationships%20in%20social%20networks%2C%20with%20signed%20graph%20neural%0Anetworks%20%28SGNNs%29%20emerging%20as%20the%20primary%20tool%20for%20their%20analysis.%20Our%0Ainvestigation%20reveals%20that%20balance%20theory%2C%20while%20essential%20for%20modeling%20signed%0Arelationships%20in%20SGNNs%2C%20inadvertently%20introduces%20exploitable%20vulnerabilities%20to%0Ablack-box%20attacks.%20To%20demonstrate%20this%20vulnerability%2C%20we%20propose%0Abalance-attack%2C%20a%20novel%20adversarial%20strategy%20specifically%20designed%20to%0Acompromise%20graph%20balance%20degree%2C%20and%20develop%20an%20efficient%20heuristic%20algorithm%0Ato%20solve%20the%20associated%20NP-hard%20optimization%20problem.%20While%20existing%20approaches%0Aattempt%20to%20restore%20attacked%20graphs%20through%20balance%20learning%20techniques%2C%20they%0Aface%20a%20critical%20challenge%20we%20term%20%22Irreversibility%20of%20Balance-related%0AInformation%2C%22%20where%20restored%20edges%20fail%20to%20align%20with%20original%20attack%20targets.%0ATo%20address%20this%20limitation%2C%20we%20introduce%20Balance%20Augmented-Signed%20Graph%0AContrastive%20Learning%20%28BA-SGCL%29%2C%20an%20innovative%20framework%20that%20combines%0Acontrastive%20learning%20with%20balance%20augmentation%20techniques%20to%20achieve%20robust%0Agraph%20representations.%20By%20maintaining%20high%20balance%20degree%20in%20the%20latent%20space%2C%0ABA-SGCL%20effectively%20circumvents%20the%20irreversibility%20challenge%20and%20enhances%0Amodel%20resilience.%20Extensive%20experiments%20across%20multiple%20SGNN%20architectures%20and%0Areal-world%20datasets%20demonstrate%20both%20the%20effectiveness%20of%20our%20proposed%0Abalance-attack%20and%20the%20superior%20robustness%20of%20BA-SGCL%2C%20advancing%20the%20security%0Aand%20reliability%20of%20signed%20graph%20analysis%20in%20social%20networks.%20Datasets%20and%20codes%0Aof%20the%20proposed%20framework%20are%20at%20the%20github%20repository%0Ahttps%3A//anonymous.4open.science/r/BA-SGCL-submit-DF41/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.10590v2&entry.124074799=Read"},
{"title": "JoVALE: Detecting Human Actions in Video Using Audiovisual and Language\n  Contexts", "author": "Taein Son and Soo Won Seo and Jisong Kim and Seok Hwan Lee and Jun Won Choi", "abstract": "  Video Action Detection (VAD) involves localizing and categorizing action\ninstances in videos. Videos inherently contain various information sources,\nincluding audio, visual cues, and surrounding scene contexts. Effectively\nleveraging this multi-modal information for VAD is challenging, as the model\nmust accurately focus on action-relevant cues. In this study, we introduce a\nnovel multi-modal VAD architecture called the Joint Actor-centric Visual,\nAudio, Language Encoder (JoVALE). JoVALE is the first VAD method to integrate\naudio and visual features with scene descriptive context derived from large\nimage captioning models. The core principle of JoVALE is the actor-centric\naggregation of audio, visual, and scene descriptive contexts, where\naction-related cues from each modality are identified and adaptively combined.\nWe propose a specialized module called the Actor-centric Multi-modal Fusion\nNetwork, designed to capture the joint interactions among actors and\nmulti-modal contexts through Transformer architecture. Our evaluation conducted\non three popular VAD benchmarks, AVA, UCF101-24, and JHMDB51-21, demonstrates\nthat incorporating multi-modal information leads to significant performance\ngains. JoVALE achieves state-of-the-art performances. The code will be\navailable at \\texttt{https://github.com/taeiin/AAAI2025-JoVALE}.\n", "link": "http://arxiv.org/abs/2412.13708v1", "date": "2024-12-18", "relevancy": 2.3234, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5853}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JoVALE%3A%20Detecting%20Human%20Actions%20in%20Video%20Using%20Audiovisual%20and%20Language%0A%20%20Contexts&body=Title%3A%20JoVALE%3A%20Detecting%20Human%20Actions%20in%20Video%20Using%20Audiovisual%20and%20Language%0A%20%20Contexts%0AAuthor%3A%20Taein%20Son%20and%20Soo%20Won%20Seo%20and%20Jisong%20Kim%20and%20Seok%20Hwan%20Lee%20and%20Jun%20Won%20Choi%0AAbstract%3A%20%20%20Video%20Action%20Detection%20%28VAD%29%20involves%20localizing%20and%20categorizing%20action%0Ainstances%20in%20videos.%20Videos%20inherently%20contain%20various%20information%20sources%2C%0Aincluding%20audio%2C%20visual%20cues%2C%20and%20surrounding%20scene%20contexts.%20Effectively%0Aleveraging%20this%20multi-modal%20information%20for%20VAD%20is%20challenging%2C%20as%20the%20model%0Amust%20accurately%20focus%20on%20action-relevant%20cues.%20In%20this%20study%2C%20we%20introduce%20a%0Anovel%20multi-modal%20VAD%20architecture%20called%20the%20Joint%20Actor-centric%20Visual%2C%0AAudio%2C%20Language%20Encoder%20%28JoVALE%29.%20JoVALE%20is%20the%20first%20VAD%20method%20to%20integrate%0Aaudio%20and%20visual%20features%20with%20scene%20descriptive%20context%20derived%20from%20large%0Aimage%20captioning%20models.%20The%20core%20principle%20of%20JoVALE%20is%20the%20actor-centric%0Aaggregation%20of%20audio%2C%20visual%2C%20and%20scene%20descriptive%20contexts%2C%20where%0Aaction-related%20cues%20from%20each%20modality%20are%20identified%20and%20adaptively%20combined.%0AWe%20propose%20a%20specialized%20module%20called%20the%20Actor-centric%20Multi-modal%20Fusion%0ANetwork%2C%20designed%20to%20capture%20the%20joint%20interactions%20among%20actors%20and%0Amulti-modal%20contexts%20through%20Transformer%20architecture.%20Our%20evaluation%20conducted%0Aon%20three%20popular%20VAD%20benchmarks%2C%20AVA%2C%20UCF101-24%2C%20and%20JHMDB51-21%2C%20demonstrates%0Athat%20incorporating%20multi-modal%20information%20leads%20to%20significant%20performance%0Agains.%20JoVALE%20achieves%20state-of-the-art%20performances.%20The%20code%20will%20be%0Aavailable%20at%20%5Ctexttt%7Bhttps%3A//github.com/taeiin/AAAI2025-JoVALE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoVALE%253A%2520Detecting%2520Human%2520Actions%2520in%2520Video%2520Using%2520Audiovisual%2520and%2520Language%250A%2520%2520Contexts%26entry.906535625%3DTaein%2520Son%2520and%2520Soo%2520Won%2520Seo%2520and%2520Jisong%2520Kim%2520and%2520Seok%2520Hwan%2520Lee%2520and%2520Jun%2520Won%2520Choi%26entry.1292438233%3D%2520%2520Video%2520Action%2520Detection%2520%2528VAD%2529%2520involves%2520localizing%2520and%2520categorizing%2520action%250Ainstances%2520in%2520videos.%2520Videos%2520inherently%2520contain%2520various%2520information%2520sources%252C%250Aincluding%2520audio%252C%2520visual%2520cues%252C%2520and%2520surrounding%2520scene%2520contexts.%2520Effectively%250Aleveraging%2520this%2520multi-modal%2520information%2520for%2520VAD%2520is%2520challenging%252C%2520as%2520the%2520model%250Amust%2520accurately%2520focus%2520on%2520action-relevant%2520cues.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%250Anovel%2520multi-modal%2520VAD%2520architecture%2520called%2520the%2520Joint%2520Actor-centric%2520Visual%252C%250AAudio%252C%2520Language%2520Encoder%2520%2528JoVALE%2529.%2520JoVALE%2520is%2520the%2520first%2520VAD%2520method%2520to%2520integrate%250Aaudio%2520and%2520visual%2520features%2520with%2520scene%2520descriptive%2520context%2520derived%2520from%2520large%250Aimage%2520captioning%2520models.%2520The%2520core%2520principle%2520of%2520JoVALE%2520is%2520the%2520actor-centric%250Aaggregation%2520of%2520audio%252C%2520visual%252C%2520and%2520scene%2520descriptive%2520contexts%252C%2520where%250Aaction-related%2520cues%2520from%2520each%2520modality%2520are%2520identified%2520and%2520adaptively%2520combined.%250AWe%2520propose%2520a%2520specialized%2520module%2520called%2520the%2520Actor-centric%2520Multi-modal%2520Fusion%250ANetwork%252C%2520designed%2520to%2520capture%2520the%2520joint%2520interactions%2520among%2520actors%2520and%250Amulti-modal%2520contexts%2520through%2520Transformer%2520architecture.%2520Our%2520evaluation%2520conducted%250Aon%2520three%2520popular%2520VAD%2520benchmarks%252C%2520AVA%252C%2520UCF101-24%252C%2520and%2520JHMDB51-21%252C%2520demonstrates%250Athat%2520incorporating%2520multi-modal%2520information%2520leads%2520to%2520significant%2520performance%250Agains.%2520JoVALE%2520achieves%2520state-of-the-art%2520performances.%2520The%2520code%2520will%2520be%250Aavailable%2520at%2520%255Ctexttt%257Bhttps%253A//github.com/taeiin/AAAI2025-JoVALE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JoVALE%3A%20Detecting%20Human%20Actions%20in%20Video%20Using%20Audiovisual%20and%20Language%0A%20%20Contexts&entry.906535625=Taein%20Son%20and%20Soo%20Won%20Seo%20and%20Jisong%20Kim%20and%20Seok%20Hwan%20Lee%20and%20Jun%20Won%20Choi&entry.1292438233=%20%20Video%20Action%20Detection%20%28VAD%29%20involves%20localizing%20and%20categorizing%20action%0Ainstances%20in%20videos.%20Videos%20inherently%20contain%20various%20information%20sources%2C%0Aincluding%20audio%2C%20visual%20cues%2C%20and%20surrounding%20scene%20contexts.%20Effectively%0Aleveraging%20this%20multi-modal%20information%20for%20VAD%20is%20challenging%2C%20as%20the%20model%0Amust%20accurately%20focus%20on%20action-relevant%20cues.%20In%20this%20study%2C%20we%20introduce%20a%0Anovel%20multi-modal%20VAD%20architecture%20called%20the%20Joint%20Actor-centric%20Visual%2C%0AAudio%2C%20Language%20Encoder%20%28JoVALE%29.%20JoVALE%20is%20the%20first%20VAD%20method%20to%20integrate%0Aaudio%20and%20visual%20features%20with%20scene%20descriptive%20context%20derived%20from%20large%0Aimage%20captioning%20models.%20The%20core%20principle%20of%20JoVALE%20is%20the%20actor-centric%0Aaggregation%20of%20audio%2C%20visual%2C%20and%20scene%20descriptive%20contexts%2C%20where%0Aaction-related%20cues%20from%20each%20modality%20are%20identified%20and%20adaptively%20combined.%0AWe%20propose%20a%20specialized%20module%20called%20the%20Actor-centric%20Multi-modal%20Fusion%0ANetwork%2C%20designed%20to%20capture%20the%20joint%20interactions%20among%20actors%20and%0Amulti-modal%20contexts%20through%20Transformer%20architecture.%20Our%20evaluation%20conducted%0Aon%20three%20popular%20VAD%20benchmarks%2C%20AVA%2C%20UCF101-24%2C%20and%20JHMDB51-21%2C%20demonstrates%0Athat%20incorporating%20multi-modal%20information%20leads%20to%20significant%20performance%0Agains.%20JoVALE%20achieves%20state-of-the-art%20performances.%20The%20code%20will%20be%0Aavailable%20at%20%5Ctexttt%7Bhttps%3A//github.com/taeiin/AAAI2025-JoVALE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13708v1&entry.124074799=Read"},
{"title": "Unleashing the Power of Continual Learning on Non-Centralized Devices: A\n  Survey", "author": "Yichen Li and Haozhao Wang and Wenchao Xu and Tianzhe Xiao and Hong Liu and Minzhu Tu and Yuying Wang and Xin Yang and Rui Zhang and Shui Yu and Song Guo and Ruixuan Li", "abstract": "  Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL.\n", "link": "http://arxiv.org/abs/2412.13840v1", "date": "2024-12-18", "relevancy": 2.3217, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4374}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Continual%20Learning%20on%20Non-Centralized%20Devices%3A%20A%0A%20%20Survey&body=Title%3A%20Unleashing%20the%20Power%20of%20Continual%20Learning%20on%20Non-Centralized%20Devices%3A%20A%0A%20%20Survey%0AAuthor%3A%20Yichen%20Li%20and%20Haozhao%20Wang%20and%20Wenchao%20Xu%20and%20Tianzhe%20Xiao%20and%20Hong%20Liu%20and%20Minzhu%20Tu%20and%20Yuying%20Wang%20and%20Xin%20Yang%20and%20Rui%20Zhang%20and%20Shui%20Yu%20and%20Song%20Guo%20and%20Ruixuan%20Li%0AAbstract%3A%20%20%20Non-Centralized%20Continual%20Learning%20%28NCCL%29%20has%20become%20an%20emerging%20paradigm%20for%0Aenabling%20distributed%20devices%20such%20as%20vehicles%20and%20servers%20to%20handle%20streaming%0Adata%20from%20a%20joint%20non-stationary%20environment.%20To%20achieve%20high%20reliability%20and%0Ascalability%20in%20deploying%20this%20paradigm%20in%20distributed%20systems%2C%20it%20is%20essential%0Ato%20conquer%20challenges%20stemming%20from%20both%20spatial%20and%20temporal%20dimensions%2C%0Amanifesting%20as%20distribution%20shifts%2C%20catastrophic%20forgetting%2C%20heterogeneity%2C%20and%0Aprivacy%20issues.%20This%20survey%20focuses%20on%20a%20comprehensive%20examination%20of%20the%0Adevelopment%20of%20the%20non-centralized%20continual%20learning%20algorithms%20and%20the%0Areal-world%20deployment%20across%20distributed%20devices.%20We%20begin%20with%20an%20introduction%0Ato%20the%20background%20and%20fundamentals%20of%20non-centralized%20learning%20and%20continual%0Alearning.%20Then%2C%20we%20review%20existing%20solutions%20from%20three%20levels%20to%20represent%20how%0Aexisting%20techniques%20alleviate%20the%20catastrophic%20forgetting%20and%20distribution%0Ashift.%20Additionally%2C%20we%20delve%20into%20the%20various%20types%20of%20heterogeneity%20issues%2C%0Asecurity%2C%20and%20privacy%20attributes%2C%20as%20well%20as%20real-world%20applications%20across%0Athree%20prevalent%20scenarios.%20Furthermore%2C%20we%20establish%20a%20large-scale%20benchmark%20to%0Arevisit%20this%20problem%20and%20analyze%20the%20performance%20of%20the%20state-of-the-art%20NCCL%0Aapproaches.%20Finally%2C%20we%20discuss%20the%20important%20challenges%20and%20future%20research%0Adirections%20in%20NCCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Continual%2520Learning%2520on%2520Non-Centralized%2520Devices%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DYichen%2520Li%2520and%2520Haozhao%2520Wang%2520and%2520Wenchao%2520Xu%2520and%2520Tianzhe%2520Xiao%2520and%2520Hong%2520Liu%2520and%2520Minzhu%2520Tu%2520and%2520Yuying%2520Wang%2520and%2520Xin%2520Yang%2520and%2520Rui%2520Zhang%2520and%2520Shui%2520Yu%2520and%2520Song%2520Guo%2520and%2520Ruixuan%2520Li%26entry.1292438233%3D%2520%2520Non-Centralized%2520Continual%2520Learning%2520%2528NCCL%2529%2520has%2520become%2520an%2520emerging%2520paradigm%2520for%250Aenabling%2520distributed%2520devices%2520such%2520as%2520vehicles%2520and%2520servers%2520to%2520handle%2520streaming%250Adata%2520from%2520a%2520joint%2520non-stationary%2520environment.%2520To%2520achieve%2520high%2520reliability%2520and%250Ascalability%2520in%2520deploying%2520this%2520paradigm%2520in%2520distributed%2520systems%252C%2520it%2520is%2520essential%250Ato%2520conquer%2520challenges%2520stemming%2520from%2520both%2520spatial%2520and%2520temporal%2520dimensions%252C%250Amanifesting%2520as%2520distribution%2520shifts%252C%2520catastrophic%2520forgetting%252C%2520heterogeneity%252C%2520and%250Aprivacy%2520issues.%2520This%2520survey%2520focuses%2520on%2520a%2520comprehensive%2520examination%2520of%2520the%250Adevelopment%2520of%2520the%2520non-centralized%2520continual%2520learning%2520algorithms%2520and%2520the%250Areal-world%2520deployment%2520across%2520distributed%2520devices.%2520We%2520begin%2520with%2520an%2520introduction%250Ato%2520the%2520background%2520and%2520fundamentals%2520of%2520non-centralized%2520learning%2520and%2520continual%250Alearning.%2520Then%252C%2520we%2520review%2520existing%2520solutions%2520from%2520three%2520levels%2520to%2520represent%2520how%250Aexisting%2520techniques%2520alleviate%2520the%2520catastrophic%2520forgetting%2520and%2520distribution%250Ashift.%2520Additionally%252C%2520we%2520delve%2520into%2520the%2520various%2520types%2520of%2520heterogeneity%2520issues%252C%250Asecurity%252C%2520and%2520privacy%2520attributes%252C%2520as%2520well%2520as%2520real-world%2520applications%2520across%250Athree%2520prevalent%2520scenarios.%2520Furthermore%252C%2520we%2520establish%2520a%2520large-scale%2520benchmark%2520to%250Arevisit%2520this%2520problem%2520and%2520analyze%2520the%2520performance%2520of%2520the%2520state-of-the-art%2520NCCL%250Aapproaches.%2520Finally%252C%2520we%2520discuss%2520the%2520important%2520challenges%2520and%2520future%2520research%250Adirections%2520in%2520NCCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Continual%20Learning%20on%20Non-Centralized%20Devices%3A%20A%0A%20%20Survey&entry.906535625=Yichen%20Li%20and%20Haozhao%20Wang%20and%20Wenchao%20Xu%20and%20Tianzhe%20Xiao%20and%20Hong%20Liu%20and%20Minzhu%20Tu%20and%20Yuying%20Wang%20and%20Xin%20Yang%20and%20Rui%20Zhang%20and%20Shui%20Yu%20and%20Song%20Guo%20and%20Ruixuan%20Li&entry.1292438233=%20%20Non-Centralized%20Continual%20Learning%20%28NCCL%29%20has%20become%20an%20emerging%20paradigm%20for%0Aenabling%20distributed%20devices%20such%20as%20vehicles%20and%20servers%20to%20handle%20streaming%0Adata%20from%20a%20joint%20non-stationary%20environment.%20To%20achieve%20high%20reliability%20and%0Ascalability%20in%20deploying%20this%20paradigm%20in%20distributed%20systems%2C%20it%20is%20essential%0Ato%20conquer%20challenges%20stemming%20from%20both%20spatial%20and%20temporal%20dimensions%2C%0Amanifesting%20as%20distribution%20shifts%2C%20catastrophic%20forgetting%2C%20heterogeneity%2C%20and%0Aprivacy%20issues.%20This%20survey%20focuses%20on%20a%20comprehensive%20examination%20of%20the%0Adevelopment%20of%20the%20non-centralized%20continual%20learning%20algorithms%20and%20the%0Areal-world%20deployment%20across%20distributed%20devices.%20We%20begin%20with%20an%20introduction%0Ato%20the%20background%20and%20fundamentals%20of%20non-centralized%20learning%20and%20continual%0Alearning.%20Then%2C%20we%20review%20existing%20solutions%20from%20three%20levels%20to%20represent%20how%0Aexisting%20techniques%20alleviate%20the%20catastrophic%20forgetting%20and%20distribution%0Ashift.%20Additionally%2C%20we%20delve%20into%20the%20various%20types%20of%20heterogeneity%20issues%2C%0Asecurity%2C%20and%20privacy%20attributes%2C%20as%20well%20as%20real-world%20applications%20across%0Athree%20prevalent%20scenarios.%20Furthermore%2C%20we%20establish%20a%20large-scale%20benchmark%20to%0Arevisit%20this%20problem%20and%20analyze%20the%20performance%20of%20the%20state-of-the-art%20NCCL%0Aapproaches.%20Finally%2C%20we%20discuss%20the%20important%20challenges%20and%20future%20research%0Adirections%20in%20NCCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13840v1&entry.124074799=Read"},
{"title": "Learning from Massive Human Videos for Universal Humanoid Pose Control", "author": "Jiageng Mao and Siheng Zhao and Siqi Song and Tianheng Shi and Junjie Ye and Mingtong Zhang and Haoran Geng and Jitendra Malik and Vitor Guizilini and Yue Wang", "abstract": "  Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.\n", "link": "http://arxiv.org/abs/2412.14172v1", "date": "2024-12-18", "relevancy": 2.3105, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5922}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.569}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Massive%20Human%20Videos%20for%20Universal%20Humanoid%20Pose%20Control&body=Title%3A%20Learning%20from%20Massive%20Human%20Videos%20for%20Universal%20Humanoid%20Pose%20Control%0AAuthor%3A%20Jiageng%20Mao%20and%20Siheng%20Zhao%20and%20Siqi%20Song%20and%20Tianheng%20Shi%20and%20Junjie%20Ye%20and%20Mingtong%20Zhang%20and%20Haoran%20Geng%20and%20Jitendra%20Malik%20and%20Vitor%20Guizilini%20and%20Yue%20Wang%0AAbstract%3A%20%20%20Scalable%20learning%20of%20humanoid%20robots%20is%20crucial%20for%20their%20deployment%20in%0Areal-world%20applications.%20While%20traditional%20approaches%20primarily%20rely%20on%0Areinforcement%20learning%20or%20teleoperation%20to%20achieve%20whole-body%20control%2C%20they%20are%0Aoften%20limited%20by%20the%20diversity%20of%20simulated%20environments%20and%20the%20high%20costs%20of%0Ademonstration%20collection.%20In%20contrast%2C%20human%20videos%20are%20ubiquitous%20and%20present%0Aan%20untapped%20source%20of%20semantic%20and%20motion%20information%20that%20could%20significantly%0Aenhance%20the%20generalization%20capabilities%20of%20humanoid%20robots.%20This%20paper%0Aintroduces%20Humanoid-X%2C%20a%20large-scale%20dataset%20of%20over%2020%20million%20humanoid%20robot%0Aposes%20with%20corresponding%20text-based%20motion%20descriptions%2C%20designed%20to%20leverage%0Athis%20abundant%20data.%20Humanoid-X%20is%20curated%20through%20a%20comprehensive%20pipeline%3A%0Adata%20mining%20from%20the%20Internet%2C%20video%20caption%20generation%2C%20motion%20retargeting%20of%0Ahumans%20to%20humanoid%20robots%2C%20and%20policy%20learning%20for%20real-world%20deployment.%20With%0AHumanoid-X%2C%20we%20further%20train%20a%20large%20humanoid%20model%2C%20UH-1%2C%20which%20takes%20text%0Ainstructions%20as%20input%20and%20outputs%20corresponding%20actions%20to%20control%20a%20humanoid%0Arobot.%20Extensive%20simulated%20and%20real-world%20experiments%20validate%20that%20our%0Ascalable%20training%20approach%20leads%20to%20superior%20generalization%20in%20text-based%0Ahumanoid%20control%2C%20marking%20a%20significant%20step%20toward%20adaptable%2C%20real-world-ready%0Ahumanoid%20robots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Massive%2520Human%2520Videos%2520for%2520Universal%2520Humanoid%2520Pose%2520Control%26entry.906535625%3DJiageng%2520Mao%2520and%2520Siheng%2520Zhao%2520and%2520Siqi%2520Song%2520and%2520Tianheng%2520Shi%2520and%2520Junjie%2520Ye%2520and%2520Mingtong%2520Zhang%2520and%2520Haoran%2520Geng%2520and%2520Jitendra%2520Malik%2520and%2520Vitor%2520Guizilini%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520Scalable%2520learning%2520of%2520humanoid%2520robots%2520is%2520crucial%2520for%2520their%2520deployment%2520in%250Areal-world%2520applications.%2520While%2520traditional%2520approaches%2520primarily%2520rely%2520on%250Areinforcement%2520learning%2520or%2520teleoperation%2520to%2520achieve%2520whole-body%2520control%252C%2520they%2520are%250Aoften%2520limited%2520by%2520the%2520diversity%2520of%2520simulated%2520environments%2520and%2520the%2520high%2520costs%2520of%250Ademonstration%2520collection.%2520In%2520contrast%252C%2520human%2520videos%2520are%2520ubiquitous%2520and%2520present%250Aan%2520untapped%2520source%2520of%2520semantic%2520and%2520motion%2520information%2520that%2520could%2520significantly%250Aenhance%2520the%2520generalization%2520capabilities%2520of%2520humanoid%2520robots.%2520This%2520paper%250Aintroduces%2520Humanoid-X%252C%2520a%2520large-scale%2520dataset%2520of%2520over%252020%2520million%2520humanoid%2520robot%250Aposes%2520with%2520corresponding%2520text-based%2520motion%2520descriptions%252C%2520designed%2520to%2520leverage%250Athis%2520abundant%2520data.%2520Humanoid-X%2520is%2520curated%2520through%2520a%2520comprehensive%2520pipeline%253A%250Adata%2520mining%2520from%2520the%2520Internet%252C%2520video%2520caption%2520generation%252C%2520motion%2520retargeting%2520of%250Ahumans%2520to%2520humanoid%2520robots%252C%2520and%2520policy%2520learning%2520for%2520real-world%2520deployment.%2520With%250AHumanoid-X%252C%2520we%2520further%2520train%2520a%2520large%2520humanoid%2520model%252C%2520UH-1%252C%2520which%2520takes%2520text%250Ainstructions%2520as%2520input%2520and%2520outputs%2520corresponding%2520actions%2520to%2520control%2520a%2520humanoid%250Arobot.%2520Extensive%2520simulated%2520and%2520real-world%2520experiments%2520validate%2520that%2520our%250Ascalable%2520training%2520approach%2520leads%2520to%2520superior%2520generalization%2520in%2520text-based%250Ahumanoid%2520control%252C%2520marking%2520a%2520significant%2520step%2520toward%2520adaptable%252C%2520real-world-ready%250Ahumanoid%2520robots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Massive%20Human%20Videos%20for%20Universal%20Humanoid%20Pose%20Control&entry.906535625=Jiageng%20Mao%20and%20Siheng%20Zhao%20and%20Siqi%20Song%20and%20Tianheng%20Shi%20and%20Junjie%20Ye%20and%20Mingtong%20Zhang%20and%20Haoran%20Geng%20and%20Jitendra%20Malik%20and%20Vitor%20Guizilini%20and%20Yue%20Wang&entry.1292438233=%20%20Scalable%20learning%20of%20humanoid%20robots%20is%20crucial%20for%20their%20deployment%20in%0Areal-world%20applications.%20While%20traditional%20approaches%20primarily%20rely%20on%0Areinforcement%20learning%20or%20teleoperation%20to%20achieve%20whole-body%20control%2C%20they%20are%0Aoften%20limited%20by%20the%20diversity%20of%20simulated%20environments%20and%20the%20high%20costs%20of%0Ademonstration%20collection.%20In%20contrast%2C%20human%20videos%20are%20ubiquitous%20and%20present%0Aan%20untapped%20source%20of%20semantic%20and%20motion%20information%20that%20could%20significantly%0Aenhance%20the%20generalization%20capabilities%20of%20humanoid%20robots.%20This%20paper%0Aintroduces%20Humanoid-X%2C%20a%20large-scale%20dataset%20of%20over%2020%20million%20humanoid%20robot%0Aposes%20with%20corresponding%20text-based%20motion%20descriptions%2C%20designed%20to%20leverage%0Athis%20abundant%20data.%20Humanoid-X%20is%20curated%20through%20a%20comprehensive%20pipeline%3A%0Adata%20mining%20from%20the%20Internet%2C%20video%20caption%20generation%2C%20motion%20retargeting%20of%0Ahumans%20to%20humanoid%20robots%2C%20and%20policy%20learning%20for%20real-world%20deployment.%20With%0AHumanoid-X%2C%20we%20further%20train%20a%20large%20humanoid%20model%2C%20UH-1%2C%20which%20takes%20text%0Ainstructions%20as%20input%20and%20outputs%20corresponding%20actions%20to%20control%20a%20humanoid%0Arobot.%20Extensive%20simulated%20and%20real-world%20experiments%20validate%20that%20our%0Ascalable%20training%20approach%20leads%20to%20superior%20generalization%20in%20text-based%0Ahumanoid%20control%2C%20marking%20a%20significant%20step%20toward%20adaptable%2C%20real-world-ready%0Ahumanoid%20robots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14172v1&entry.124074799=Read"},
{"title": "DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential\n  Recommendation", "author": "Haoyi Zhang and Guohao Sun and Jinhu Lu and Guanfeng Liu and Xiu Susie Fang", "abstract": "  Sequential recommendation (SR) tasks aim to predict users' next interaction\nby learning their behavior sequence and capturing the connection between users'\npast interactions and their changing preferences. Conventional SR models often\nfocus solely on capturing sequential patterns within the training data,\nneglecting the broader context and semantic information embedded in item titles\nfrom external sources. This limits their predictive power and adaptability.\nLarge language models (LLMs) have recently shown promise in SR tasks due to\ntheir advanced understanding capabilities and strong generalization abilities.\nResearchers have attempted to enhance LLMs-based recommendation performance by\nincorporating information from conventional SR models. However, previous\napproaches have encountered problems such as 1) limited textual information\nleading to poor recommendation performance, 2) incomplete understanding and\nutilization of conventional SR model information by LLMs, and 3) excessive\ncomplexity and low interpretability of LLMs-based methods. To improve the\nperformance of LLMs-based SR, we propose a novel framework, Distilling\nSequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),\nwhich aims to extract knowledge from conventional SR models and enable LLMs to\neasily comprehend and utilize the extracted knowledge for more effective SRs.\nDELRec consists of two main stages: 1) Distill Pattern from Conventional SR\nModels, focusing on extracting behavioral patterns exhibited by conventional SR\nmodels using soft prompts through two well-designed strategies; 2) LLMs-based\nSequential Recommendation, aiming to fine-tune LLMs to effectively use the\ndistilled auxiliary information to perform SR tasks. Extensive experimental\nresults conducted on four real datasets validate the effectiveness of the\nDELRec framework.\n", "link": "http://arxiv.org/abs/2406.11156v4", "date": "2024-12-18", "relevancy": 2.3033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4641}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELRec%3A%20Distilling%20Sequential%20Pattern%20to%20Enhance%20LLMs-based%20Sequential%0A%20%20Recommendation&body=Title%3A%20DELRec%3A%20Distilling%20Sequential%20Pattern%20to%20Enhance%20LLMs-based%20Sequential%0A%20%20Recommendation%0AAuthor%3A%20Haoyi%20Zhang%20and%20Guohao%20Sun%20and%20Jinhu%20Lu%20and%20Guanfeng%20Liu%20and%20Xiu%20Susie%20Fang%0AAbstract%3A%20%20%20Sequential%20recommendation%20%28SR%29%20tasks%20aim%20to%20predict%20users%27%20next%20interaction%0Aby%20learning%20their%20behavior%20sequence%20and%20capturing%20the%20connection%20between%20users%27%0Apast%20interactions%20and%20their%20changing%20preferences.%20Conventional%20SR%20models%20often%0Afocus%20solely%20on%20capturing%20sequential%20patterns%20within%20the%20training%20data%2C%0Aneglecting%20the%20broader%20context%20and%20semantic%20information%20embedded%20in%20item%20titles%0Afrom%20external%20sources.%20This%20limits%20their%20predictive%20power%20and%20adaptability.%0ALarge%20language%20models%20%28LLMs%29%20have%20recently%20shown%20promise%20in%20SR%20tasks%20due%20to%0Atheir%20advanced%20understanding%20capabilities%20and%20strong%20generalization%20abilities.%0AResearchers%20have%20attempted%20to%20enhance%20LLMs-based%20recommendation%20performance%20by%0Aincorporating%20information%20from%20conventional%20SR%20models.%20However%2C%20previous%0Aapproaches%20have%20encountered%20problems%20such%20as%201%29%20limited%20textual%20information%0Aleading%20to%20poor%20recommendation%20performance%2C%202%29%20incomplete%20understanding%20and%0Autilization%20of%20conventional%20SR%20model%20information%20by%20LLMs%2C%20and%203%29%20excessive%0Acomplexity%20and%20low%20interpretability%20of%20LLMs-based%20methods.%20To%20improve%20the%0Aperformance%20of%20LLMs-based%20SR%2C%20we%20propose%20a%20novel%20framework%2C%20Distilling%0ASequential%20Pattern%20to%20Enhance%20LLMs-based%20Sequential%20Recommendation%20%28DELRec%29%2C%0Awhich%20aims%20to%20extract%20knowledge%20from%20conventional%20SR%20models%20and%20enable%20LLMs%20to%0Aeasily%20comprehend%20and%20utilize%20the%20extracted%20knowledge%20for%20more%20effective%20SRs.%0ADELRec%20consists%20of%20two%20main%20stages%3A%201%29%20Distill%20Pattern%20from%20Conventional%20SR%0AModels%2C%20focusing%20on%20extracting%20behavioral%20patterns%20exhibited%20by%20conventional%20SR%0Amodels%20using%20soft%20prompts%20through%20two%20well-designed%20strategies%3B%202%29%20LLMs-based%0ASequential%20Recommendation%2C%20aiming%20to%20fine-tune%20LLMs%20to%20effectively%20use%20the%0Adistilled%20auxiliary%20information%20to%20perform%20SR%20tasks.%20Extensive%20experimental%0Aresults%20conducted%20on%20four%20real%20datasets%20validate%20the%20effectiveness%20of%20the%0ADELRec%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11156v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELRec%253A%2520Distilling%2520Sequential%2520Pattern%2520to%2520Enhance%2520LLMs-based%2520Sequential%250A%2520%2520Recommendation%26entry.906535625%3DHaoyi%2520Zhang%2520and%2520Guohao%2520Sun%2520and%2520Jinhu%2520Lu%2520and%2520Guanfeng%2520Liu%2520and%2520Xiu%2520Susie%2520Fang%26entry.1292438233%3D%2520%2520Sequential%2520recommendation%2520%2528SR%2529%2520tasks%2520aim%2520to%2520predict%2520users%2527%2520next%2520interaction%250Aby%2520learning%2520their%2520behavior%2520sequence%2520and%2520capturing%2520the%2520connection%2520between%2520users%2527%250Apast%2520interactions%2520and%2520their%2520changing%2520preferences.%2520Conventional%2520SR%2520models%2520often%250Afocus%2520solely%2520on%2520capturing%2520sequential%2520patterns%2520within%2520the%2520training%2520data%252C%250Aneglecting%2520the%2520broader%2520context%2520and%2520semantic%2520information%2520embedded%2520in%2520item%2520titles%250Afrom%2520external%2520sources.%2520This%2520limits%2520their%2520predictive%2520power%2520and%2520adaptability.%250ALarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520shown%2520promise%2520in%2520SR%2520tasks%2520due%2520to%250Atheir%2520advanced%2520understanding%2520capabilities%2520and%2520strong%2520generalization%2520abilities.%250AResearchers%2520have%2520attempted%2520to%2520enhance%2520LLMs-based%2520recommendation%2520performance%2520by%250Aincorporating%2520information%2520from%2520conventional%2520SR%2520models.%2520However%252C%2520previous%250Aapproaches%2520have%2520encountered%2520problems%2520such%2520as%25201%2529%2520limited%2520textual%2520information%250Aleading%2520to%2520poor%2520recommendation%2520performance%252C%25202%2529%2520incomplete%2520understanding%2520and%250Autilization%2520of%2520conventional%2520SR%2520model%2520information%2520by%2520LLMs%252C%2520and%25203%2529%2520excessive%250Acomplexity%2520and%2520low%2520interpretability%2520of%2520LLMs-based%2520methods.%2520To%2520improve%2520the%250Aperformance%2520of%2520LLMs-based%2520SR%252C%2520we%2520propose%2520a%2520novel%2520framework%252C%2520Distilling%250ASequential%2520Pattern%2520to%2520Enhance%2520LLMs-based%2520Sequential%2520Recommendation%2520%2528DELRec%2529%252C%250Awhich%2520aims%2520to%2520extract%2520knowledge%2520from%2520conventional%2520SR%2520models%2520and%2520enable%2520LLMs%2520to%250Aeasily%2520comprehend%2520and%2520utilize%2520the%2520extracted%2520knowledge%2520for%2520more%2520effective%2520SRs.%250ADELRec%2520consists%2520of%2520two%2520main%2520stages%253A%25201%2529%2520Distill%2520Pattern%2520from%2520Conventional%2520SR%250AModels%252C%2520focusing%2520on%2520extracting%2520behavioral%2520patterns%2520exhibited%2520by%2520conventional%2520SR%250Amodels%2520using%2520soft%2520prompts%2520through%2520two%2520well-designed%2520strategies%253B%25202%2529%2520LLMs-based%250ASequential%2520Recommendation%252C%2520aiming%2520to%2520fine-tune%2520LLMs%2520to%2520effectively%2520use%2520the%250Adistilled%2520auxiliary%2520information%2520to%2520perform%2520SR%2520tasks.%2520Extensive%2520experimental%250Aresults%2520conducted%2520on%2520four%2520real%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520the%250ADELRec%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11156v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELRec%3A%20Distilling%20Sequential%20Pattern%20to%20Enhance%20LLMs-based%20Sequential%0A%20%20Recommendation&entry.906535625=Haoyi%20Zhang%20and%20Guohao%20Sun%20and%20Jinhu%20Lu%20and%20Guanfeng%20Liu%20and%20Xiu%20Susie%20Fang&entry.1292438233=%20%20Sequential%20recommendation%20%28SR%29%20tasks%20aim%20to%20predict%20users%27%20next%20interaction%0Aby%20learning%20their%20behavior%20sequence%20and%20capturing%20the%20connection%20between%20users%27%0Apast%20interactions%20and%20their%20changing%20preferences.%20Conventional%20SR%20models%20often%0Afocus%20solely%20on%20capturing%20sequential%20patterns%20within%20the%20training%20data%2C%0Aneglecting%20the%20broader%20context%20and%20semantic%20information%20embedded%20in%20item%20titles%0Afrom%20external%20sources.%20This%20limits%20their%20predictive%20power%20and%20adaptability.%0ALarge%20language%20models%20%28LLMs%29%20have%20recently%20shown%20promise%20in%20SR%20tasks%20due%20to%0Atheir%20advanced%20understanding%20capabilities%20and%20strong%20generalization%20abilities.%0AResearchers%20have%20attempted%20to%20enhance%20LLMs-based%20recommendation%20performance%20by%0Aincorporating%20information%20from%20conventional%20SR%20models.%20However%2C%20previous%0Aapproaches%20have%20encountered%20problems%20such%20as%201%29%20limited%20textual%20information%0Aleading%20to%20poor%20recommendation%20performance%2C%202%29%20incomplete%20understanding%20and%0Autilization%20of%20conventional%20SR%20model%20information%20by%20LLMs%2C%20and%203%29%20excessive%0Acomplexity%20and%20low%20interpretability%20of%20LLMs-based%20methods.%20To%20improve%20the%0Aperformance%20of%20LLMs-based%20SR%2C%20we%20propose%20a%20novel%20framework%2C%20Distilling%0ASequential%20Pattern%20to%20Enhance%20LLMs-based%20Sequential%20Recommendation%20%28DELRec%29%2C%0Awhich%20aims%20to%20extract%20knowledge%20from%20conventional%20SR%20models%20and%20enable%20LLMs%20to%0Aeasily%20comprehend%20and%20utilize%20the%20extracted%20knowledge%20for%20more%20effective%20SRs.%0ADELRec%20consists%20of%20two%20main%20stages%3A%201%29%20Distill%20Pattern%20from%20Conventional%20SR%0AModels%2C%20focusing%20on%20extracting%20behavioral%20patterns%20exhibited%20by%20conventional%20SR%0Amodels%20using%20soft%20prompts%20through%20two%20well-designed%20strategies%3B%202%29%20LLMs-based%0ASequential%20Recommendation%2C%20aiming%20to%20fine-tune%20LLMs%20to%20effectively%20use%20the%0Adistilled%20auxiliary%20information%20to%20perform%20SR%20tasks.%20Extensive%20experimental%0Aresults%20conducted%20on%20four%20real%20datasets%20validate%20the%20effectiveness%20of%20the%0ADELRec%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11156v4&entry.124074799=Read"},
{"title": "GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images", "author": "Ziyang Xu and Huangxuan Zhao and Wenyu Liu and Xinggang Wang", "abstract": "  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n", "link": "http://arxiv.org/abs/2412.14118v1", "date": "2024-12-18", "relevancy": 2.2926, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5952}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5603}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaraMoSt%3A%20Parallel%20Multi-Granularity%20Motion%20and%20Structural%20Modeling%20for%0A%20%20Efficient%20Multi-Frame%20Interpolation%20in%20DSA%20Images&body=Title%3A%20GaraMoSt%3A%20Parallel%20Multi-Granularity%20Motion%20and%20Structural%20Modeling%20for%0A%20%20Efficient%20Multi-Frame%20Interpolation%20in%20DSA%20Images%0AAuthor%3A%20Ziyang%20Xu%20and%20Huangxuan%20Zhao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20The%20rapid%20and%20accurate%20direct%20multi-frame%20interpolation%20method%20for%20Digital%0ASubtraction%20Angiography%20%28DSA%29%20images%20is%20crucial%20for%20reducing%20radiation%20and%0Aproviding%20real-time%20assistance%20to%20physicians%20for%20precise%20diagnostics%20and%0Atreatment.%20DSA%20images%20contain%20complex%20vascular%20structures%20and%20various%20motions.%0AApplying%20natural%20scene%20Video%20Frame%20Interpolation%20%28VFI%29%20methods%20results%20in%0Amotion%20artifacts%2C%20structural%20dissipation%2C%20and%20blurriness.%20Recently%2C%20MoSt-DSA%0Ahas%20specifically%20addressed%20these%20issues%20for%20the%20first%20time%20and%20achieved%20SOTA%0Aresults.%20However%2C%20MoSt-DSA%27s%20focus%20on%20real-time%20performance%20leads%20to%0Ainsufficient%20suppression%20of%20high-frequency%20noise%20and%20incomplete%20filtering%20of%0Alow-frequency%20noise%20in%20the%20generated%20images.%20To%20address%20these%20issues%20within%20the%0Asame%20computational%20time%20scale%2C%20we%20propose%20GaraMoSt.%20Specifically%2C%20we%20optimize%0Athe%20network%20pipeline%20with%20a%20parallel%20design%20and%20propose%20a%20module%20named%20MG-MSFE.%0AMG-MSFE%20extracts%20frame-relative%20motion%20and%20structural%20features%20at%20various%0Agranularities%20in%20a%20fully%20convolutional%20parallel%20manner%20and%20supports%0Aindependent%2C%20flexible%20adjustment%20of%20context-aware%20granularity%20at%20different%0Ascales%2C%20thus%20enhancing%20computational%20efficiency%20and%20accuracy.%20Extensive%0Aexperiments%20demonstrate%20that%20GaraMoSt%20achieves%20the%20SOTA%20performance%20in%0Aaccuracy%2C%20robustness%2C%20visual%20effects%2C%20and%20noise%20suppression%2C%20comprehensively%0Asurpassing%20MoSt-DSA%20and%20other%20natural%20scene%20VFI%20methods.%20The%20code%20and%20models%0Aare%20available%20at%20https%3A//github.com/ZyoungXu/GaraMoSt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaraMoSt%253A%2520Parallel%2520Multi-Granularity%2520Motion%2520and%2520Structural%2520Modeling%2520for%250A%2520%2520Efficient%2520Multi-Frame%2520Interpolation%2520in%2520DSA%2520Images%26entry.906535625%3DZiyang%2520Xu%2520and%2520Huangxuan%2520Zhao%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520The%2520rapid%2520and%2520accurate%2520direct%2520multi-frame%2520interpolation%2520method%2520for%2520Digital%250ASubtraction%2520Angiography%2520%2528DSA%2529%2520images%2520is%2520crucial%2520for%2520reducing%2520radiation%2520and%250Aproviding%2520real-time%2520assistance%2520to%2520physicians%2520for%2520precise%2520diagnostics%2520and%250Atreatment.%2520DSA%2520images%2520contain%2520complex%2520vascular%2520structures%2520and%2520various%2520motions.%250AApplying%2520natural%2520scene%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520methods%2520results%2520in%250Amotion%2520artifacts%252C%2520structural%2520dissipation%252C%2520and%2520blurriness.%2520Recently%252C%2520MoSt-DSA%250Ahas%2520specifically%2520addressed%2520these%2520issues%2520for%2520the%2520first%2520time%2520and%2520achieved%2520SOTA%250Aresults.%2520However%252C%2520MoSt-DSA%2527s%2520focus%2520on%2520real-time%2520performance%2520leads%2520to%250Ainsufficient%2520suppression%2520of%2520high-frequency%2520noise%2520and%2520incomplete%2520filtering%2520of%250Alow-frequency%2520noise%2520in%2520the%2520generated%2520images.%2520To%2520address%2520these%2520issues%2520within%2520the%250Asame%2520computational%2520time%2520scale%252C%2520we%2520propose%2520GaraMoSt.%2520Specifically%252C%2520we%2520optimize%250Athe%2520network%2520pipeline%2520with%2520a%2520parallel%2520design%2520and%2520propose%2520a%2520module%2520named%2520MG-MSFE.%250AMG-MSFE%2520extracts%2520frame-relative%2520motion%2520and%2520structural%2520features%2520at%2520various%250Agranularities%2520in%2520a%2520fully%2520convolutional%2520parallel%2520manner%2520and%2520supports%250Aindependent%252C%2520flexible%2520adjustment%2520of%2520context-aware%2520granularity%2520at%2520different%250Ascales%252C%2520thus%2520enhancing%2520computational%2520efficiency%2520and%2520accuracy.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520GaraMoSt%2520achieves%2520the%2520SOTA%2520performance%2520in%250Aaccuracy%252C%2520robustness%252C%2520visual%2520effects%252C%2520and%2520noise%2520suppression%252C%2520comprehensively%250Asurpassing%2520MoSt-DSA%2520and%2520other%2520natural%2520scene%2520VFI%2520methods.%2520The%2520code%2520and%2520models%250Aare%2520available%2520at%2520https%253A//github.com/ZyoungXu/GaraMoSt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaraMoSt%3A%20Parallel%20Multi-Granularity%20Motion%20and%20Structural%20Modeling%20for%0A%20%20Efficient%20Multi-Frame%20Interpolation%20in%20DSA%20Images&entry.906535625=Ziyang%20Xu%20and%20Huangxuan%20Zhao%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20The%20rapid%20and%20accurate%20direct%20multi-frame%20interpolation%20method%20for%20Digital%0ASubtraction%20Angiography%20%28DSA%29%20images%20is%20crucial%20for%20reducing%20radiation%20and%0Aproviding%20real-time%20assistance%20to%20physicians%20for%20precise%20diagnostics%20and%0Atreatment.%20DSA%20images%20contain%20complex%20vascular%20structures%20and%20various%20motions.%0AApplying%20natural%20scene%20Video%20Frame%20Interpolation%20%28VFI%29%20methods%20results%20in%0Amotion%20artifacts%2C%20structural%20dissipation%2C%20and%20blurriness.%20Recently%2C%20MoSt-DSA%0Ahas%20specifically%20addressed%20these%20issues%20for%20the%20first%20time%20and%20achieved%20SOTA%0Aresults.%20However%2C%20MoSt-DSA%27s%20focus%20on%20real-time%20performance%20leads%20to%0Ainsufficient%20suppression%20of%20high-frequency%20noise%20and%20incomplete%20filtering%20of%0Alow-frequency%20noise%20in%20the%20generated%20images.%20To%20address%20these%20issues%20within%20the%0Asame%20computational%20time%20scale%2C%20we%20propose%20GaraMoSt.%20Specifically%2C%20we%20optimize%0Athe%20network%20pipeline%20with%20a%20parallel%20design%20and%20propose%20a%20module%20named%20MG-MSFE.%0AMG-MSFE%20extracts%20frame-relative%20motion%20and%20structural%20features%20at%20various%0Agranularities%20in%20a%20fully%20convolutional%20parallel%20manner%20and%20supports%0Aindependent%2C%20flexible%20adjustment%20of%20context-aware%20granularity%20at%20different%0Ascales%2C%20thus%20enhancing%20computational%20efficiency%20and%20accuracy.%20Extensive%0Aexperiments%20demonstrate%20that%20GaraMoSt%20achieves%20the%20SOTA%20performance%20in%0Aaccuracy%2C%20robustness%2C%20visual%20effects%2C%20and%20noise%20suppression%2C%20comprehensively%0Asurpassing%20MoSt-DSA%20and%20other%20natural%20scene%20VFI%20methods.%20The%20code%20and%20models%0Aare%20available%20at%20https%3A//github.com/ZyoungXu/GaraMoSt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14118v1&entry.124074799=Read"},
{"title": "Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models", "author": "Xinghang Li and Peiyan Li and Minghuan Liu and Dong Wang and Jirong Liu and Bingyi Kang and Xiao Ma and Tao Kong and Hanbo Zhang and Huaping Liu", "abstract": "  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n", "link": "http://arxiv.org/abs/2412.14058v1", "date": "2024-12-18", "relevancy": 2.2762, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5907}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalist%20Robot%20Policies%3A%20What%20Matters%20in%20Building%0A%20%20Vision-Language-Action%20Models&body=Title%3A%20Towards%20Generalist%20Robot%20Policies%3A%20What%20Matters%20in%20Building%0A%20%20Vision-Language-Action%20Models%0AAuthor%3A%20Xinghang%20Li%20and%20Peiyan%20Li%20and%20Minghuan%20Liu%20and%20Dong%20Wang%20and%20Jirong%20Liu%20and%20Bingyi%20Kang%20and%20Xiao%20Ma%20and%20Tao%20Kong%20and%20Hanbo%20Zhang%20and%20Huaping%20Liu%0AAbstract%3A%20%20%20Foundation%20Vision%20Language%20Models%20%28VLMs%29%20exhibit%20strong%20capabilities%20in%0Amulti-modal%20representation%20learning%2C%20comprehension%2C%20and%20reasoning.%20By%20injecting%0Aaction%20components%20into%20the%20VLMs%2C%20Vision-Language-Action%20Models%20%28VLAs%29%20can%20be%0Anaturally%20formed%20and%20also%20show%20promising%20performance.%20Existing%20work%20has%0Ademonstrated%20the%20effectiveness%20and%20generalization%20of%20VLAs%20in%20multiple%20scenarios%0Aand%20tasks.%20Nevertheless%2C%20the%20transfer%20from%20VLMs%20to%20VLAs%20is%20not%20trivial%20since%0Aexisting%20VLAs%20differ%20in%20their%20backbones%2C%20action-prediction%20formulations%2C%20data%0Adistributions%2C%20and%20training%20recipes.%20This%20leads%20to%20a%20missing%20piece%20for%20a%0Asystematic%20understanding%20of%20the%20design%20choices%20of%20VLAs.%20In%20this%20work%2C%20we%0Adisclose%20the%20key%20factors%20that%20significantly%20influence%20the%20performance%20of%20VLA%0Aand%20focus%20on%20answering%20three%20essential%20design%20choices%3A%20which%20backbone%20to%0Aselect%2C%20how%20to%20formulate%20the%20VLA%20architectures%2C%20and%20when%20to%20add%0Across-embodiment%20data.%20The%20obtained%20results%20convince%20us%20firmly%20to%20explain%20why%0Awe%20need%20VLA%20and%20develop%20a%20new%20family%20of%20VLAs%2C%20RoboVLMs%2C%20which%20require%20very%20few%0Amanual%20designs%20and%20achieve%20a%20new%20state-of-the-art%20performance%20in%20three%0Asimulation%20tasks%20and%20real-world%20experiments.%20Through%20our%20extensive%20experiments%2C%0Awhich%20include%20over%208%20VLM%20backbones%2C%204%20policy%20architectures%2C%20and%20over%20600%0Adistinct%20designed%20experiments%2C%20we%20provide%20a%20detailed%20guidebook%20for%20the%20future%0Adesign%20of%20VLAs.%20In%20addition%20to%20the%20study%2C%20the%20highly%20flexible%20RoboVLMs%0Aframework%2C%20which%20supports%20easy%20integrations%20of%20new%20VLMs%20and%20free%20combinations%0Aof%20various%20design%20choices%2C%20is%20made%20public%20to%20facilitate%20future%20research.%20We%0Aopen-source%20all%20details%2C%20including%20codes%2C%20models%2C%20datasets%2C%20and%20toolkits%2C%20along%0Awith%20detailed%20training%20and%20evaluation%20recipes%20at%3A%20robovlms.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14058v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Generalist%2520Robot%2520Policies%253A%2520What%2520Matters%2520in%2520Building%250A%2520%2520Vision-Language-Action%2520Models%26entry.906535625%3DXinghang%2520Li%2520and%2520Peiyan%2520Li%2520and%2520Minghuan%2520Liu%2520and%2520Dong%2520Wang%2520and%2520Jirong%2520Liu%2520and%2520Bingyi%2520Kang%2520and%2520Xiao%2520Ma%2520and%2520Tao%2520Kong%2520and%2520Hanbo%2520Zhang%2520and%2520Huaping%2520Liu%26entry.1292438233%3D%2520%2520Foundation%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520strong%2520capabilities%2520in%250Amulti-modal%2520representation%2520learning%252C%2520comprehension%252C%2520and%2520reasoning.%2520By%2520injecting%250Aaction%2520components%2520into%2520the%2520VLMs%252C%2520Vision-Language-Action%2520Models%2520%2528VLAs%2529%2520can%2520be%250Anaturally%2520formed%2520and%2520also%2520show%2520promising%2520performance.%2520Existing%2520work%2520has%250Ademonstrated%2520the%2520effectiveness%2520and%2520generalization%2520of%2520VLAs%2520in%2520multiple%2520scenarios%250Aand%2520tasks.%2520Nevertheless%252C%2520the%2520transfer%2520from%2520VLMs%2520to%2520VLAs%2520is%2520not%2520trivial%2520since%250Aexisting%2520VLAs%2520differ%2520in%2520their%2520backbones%252C%2520action-prediction%2520formulations%252C%2520data%250Adistributions%252C%2520and%2520training%2520recipes.%2520This%2520leads%2520to%2520a%2520missing%2520piece%2520for%2520a%250Asystematic%2520understanding%2520of%2520the%2520design%2520choices%2520of%2520VLAs.%2520In%2520this%2520work%252C%2520we%250Adisclose%2520the%2520key%2520factors%2520that%2520significantly%2520influence%2520the%2520performance%2520of%2520VLA%250Aand%2520focus%2520on%2520answering%2520three%2520essential%2520design%2520choices%253A%2520which%2520backbone%2520to%250Aselect%252C%2520how%2520to%2520formulate%2520the%2520VLA%2520architectures%252C%2520and%2520when%2520to%2520add%250Across-embodiment%2520data.%2520The%2520obtained%2520results%2520convince%2520us%2520firmly%2520to%2520explain%2520why%250Awe%2520need%2520VLA%2520and%2520develop%2520a%2520new%2520family%2520of%2520VLAs%252C%2520RoboVLMs%252C%2520which%2520require%2520very%2520few%250Amanual%2520designs%2520and%2520achieve%2520a%2520new%2520state-of-the-art%2520performance%2520in%2520three%250Asimulation%2520tasks%2520and%2520real-world%2520experiments.%2520Through%2520our%2520extensive%2520experiments%252C%250Awhich%2520include%2520over%25208%2520VLM%2520backbones%252C%25204%2520policy%2520architectures%252C%2520and%2520over%2520600%250Adistinct%2520designed%2520experiments%252C%2520we%2520provide%2520a%2520detailed%2520guidebook%2520for%2520the%2520future%250Adesign%2520of%2520VLAs.%2520In%2520addition%2520to%2520the%2520study%252C%2520the%2520highly%2520flexible%2520RoboVLMs%250Aframework%252C%2520which%2520supports%2520easy%2520integrations%2520of%2520new%2520VLMs%2520and%2520free%2520combinations%250Aof%2520various%2520design%2520choices%252C%2520is%2520made%2520public%2520to%2520facilitate%2520future%2520research.%2520We%250Aopen-source%2520all%2520details%252C%2520including%2520codes%252C%2520models%252C%2520datasets%252C%2520and%2520toolkits%252C%2520along%250Awith%2520detailed%2520training%2520and%2520evaluation%2520recipes%2520at%253A%2520robovlms.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14058v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalist%20Robot%20Policies%3A%20What%20Matters%20in%20Building%0A%20%20Vision-Language-Action%20Models&entry.906535625=Xinghang%20Li%20and%20Peiyan%20Li%20and%20Minghuan%20Liu%20and%20Dong%20Wang%20and%20Jirong%20Liu%20and%20Bingyi%20Kang%20and%20Xiao%20Ma%20and%20Tao%20Kong%20and%20Hanbo%20Zhang%20and%20Huaping%20Liu&entry.1292438233=%20%20Foundation%20Vision%20Language%20Models%20%28VLMs%29%20exhibit%20strong%20capabilities%20in%0Amulti-modal%20representation%20learning%2C%20comprehension%2C%20and%20reasoning.%20By%20injecting%0Aaction%20components%20into%20the%20VLMs%2C%20Vision-Language-Action%20Models%20%28VLAs%29%20can%20be%0Anaturally%20formed%20and%20also%20show%20promising%20performance.%20Existing%20work%20has%0Ademonstrated%20the%20effectiveness%20and%20generalization%20of%20VLAs%20in%20multiple%20scenarios%0Aand%20tasks.%20Nevertheless%2C%20the%20transfer%20from%20VLMs%20to%20VLAs%20is%20not%20trivial%20since%0Aexisting%20VLAs%20differ%20in%20their%20backbones%2C%20action-prediction%20formulations%2C%20data%0Adistributions%2C%20and%20training%20recipes.%20This%20leads%20to%20a%20missing%20piece%20for%20a%0Asystematic%20understanding%20of%20the%20design%20choices%20of%20VLAs.%20In%20this%20work%2C%20we%0Adisclose%20the%20key%20factors%20that%20significantly%20influence%20the%20performance%20of%20VLA%0Aand%20focus%20on%20answering%20three%20essential%20design%20choices%3A%20which%20backbone%20to%0Aselect%2C%20how%20to%20formulate%20the%20VLA%20architectures%2C%20and%20when%20to%20add%0Across-embodiment%20data.%20The%20obtained%20results%20convince%20us%20firmly%20to%20explain%20why%0Awe%20need%20VLA%20and%20develop%20a%20new%20family%20of%20VLAs%2C%20RoboVLMs%2C%20which%20require%20very%20few%0Amanual%20designs%20and%20achieve%20a%20new%20state-of-the-art%20performance%20in%20three%0Asimulation%20tasks%20and%20real-world%20experiments.%20Through%20our%20extensive%20experiments%2C%0Awhich%20include%20over%208%20VLM%20backbones%2C%204%20policy%20architectures%2C%20and%20over%20600%0Adistinct%20designed%20experiments%2C%20we%20provide%20a%20detailed%20guidebook%20for%20the%20future%0Adesign%20of%20VLAs.%20In%20addition%20to%20the%20study%2C%20the%20highly%20flexible%20RoboVLMs%0Aframework%2C%20which%20supports%20easy%20integrations%20of%20new%20VLMs%20and%20free%20combinations%0Aof%20various%20design%20choices%2C%20is%20made%20public%20to%20facilitate%20future%20research.%20We%0Aopen-source%20all%20details%2C%20including%20codes%2C%20models%2C%20datasets%2C%20and%20toolkits%2C%20along%0Awith%20detailed%20training%20and%20evaluation%20recipes%20at%3A%20robovlms.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14058v1&entry.124074799=Read"},
{"title": "AKiRa: Augmentation Kit on Rays for optical video generation", "author": "Xi Wang and Robin Courant and Marc Christie and Vicky Kalogeiton", "abstract": "  Recent advances in text-conditioned video diffusion have greatly improved\nvideo quality. However, these methods offer limited or sometimes no control to\nusers on camera aspects, including dynamic camera motion, zoom, distorted lens\nand focus shifts. These motion and optical aspects are crucial for adding\ncontrollability and cinematic elements to generation frameworks, ultimately\nresulting in visual content that draws focus, enhances mood, and guides\nemotions according to filmmakers' controls. In this paper, we aim to close the\ngap between controllable video generation and camera optics. To achieve this,\nwe propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework\nthat builds and trains a camera adapter with a complex camera model over an\nexisting video generation backbone. It enables fine-tuned control over camera\nmotion as well as complex optical parameters (focal length, distortion,\naperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.\nExtensive experiments demonstrate AKiRa's effectiveness in combining and\ncomposing camera optics while outperforming all state-of-the-art methods. This\nwork sets a new landmark in controlled and optically enhanced video generation,\npaving the way for future optical video generation methods.\n", "link": "http://arxiv.org/abs/2412.14158v1", "date": "2024-12-18", "relevancy": 2.2728, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5773}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5689}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AKiRa%3A%20Augmentation%20Kit%20on%20Rays%20for%20optical%20video%20generation&body=Title%3A%20AKiRa%3A%20Augmentation%20Kit%20on%20Rays%20for%20optical%20video%20generation%0AAuthor%3A%20Xi%20Wang%20and%20Robin%20Courant%20and%20Marc%20Christie%20and%20Vicky%20Kalogeiton%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-conditioned%20video%20diffusion%20have%20greatly%20improved%0Avideo%20quality.%20However%2C%20these%20methods%20offer%20limited%20or%20sometimes%20no%20control%20to%0Ausers%20on%20camera%20aspects%2C%20including%20dynamic%20camera%20motion%2C%20zoom%2C%20distorted%20lens%0Aand%20focus%20shifts.%20These%20motion%20and%20optical%20aspects%20are%20crucial%20for%20adding%0Acontrollability%20and%20cinematic%20elements%20to%20generation%20frameworks%2C%20ultimately%0Aresulting%20in%20visual%20content%20that%20draws%20focus%2C%20enhances%20mood%2C%20and%20guides%0Aemotions%20according%20to%20filmmakers%27%20controls.%20In%20this%20paper%2C%20we%20aim%20to%20close%20the%0Agap%20between%20controllable%20video%20generation%20and%20camera%20optics.%20To%20achieve%20this%2C%0Awe%20propose%20AKiRa%20%28Augmentation%20Kit%20on%20Rays%29%2C%20a%20novel%20augmentation%20framework%0Athat%20builds%20and%20trains%20a%20camera%20adapter%20with%20a%20complex%20camera%20model%20over%20an%0Aexisting%20video%20generation%20backbone.%20It%20enables%20fine-tuned%20control%20over%20camera%0Amotion%20as%20well%20as%20complex%20optical%20parameters%20%28focal%20length%2C%20distortion%2C%0Aaperture%29%20to%20achieve%20cinematic%20effects%20such%20as%20zoom%2C%20fisheye%20effect%2C%20and%20bokeh.%0AExtensive%20experiments%20demonstrate%20AKiRa%27s%20effectiveness%20in%20combining%20and%0Acomposing%20camera%20optics%20while%20outperforming%20all%20state-of-the-art%20methods.%20This%0Awork%20sets%20a%20new%20landmark%20in%20controlled%20and%20optically%20enhanced%20video%20generation%2C%0Apaving%20the%20way%20for%20future%20optical%20video%20generation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAKiRa%253A%2520Augmentation%2520Kit%2520on%2520Rays%2520for%2520optical%2520video%2520generation%26entry.906535625%3DXi%2520Wang%2520and%2520Robin%2520Courant%2520and%2520Marc%2520Christie%2520and%2520Vicky%2520Kalogeiton%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-conditioned%2520video%2520diffusion%2520have%2520greatly%2520improved%250Avideo%2520quality.%2520However%252C%2520these%2520methods%2520offer%2520limited%2520or%2520sometimes%2520no%2520control%2520to%250Ausers%2520on%2520camera%2520aspects%252C%2520including%2520dynamic%2520camera%2520motion%252C%2520zoom%252C%2520distorted%2520lens%250Aand%2520focus%2520shifts.%2520These%2520motion%2520and%2520optical%2520aspects%2520are%2520crucial%2520for%2520adding%250Acontrollability%2520and%2520cinematic%2520elements%2520to%2520generation%2520frameworks%252C%2520ultimately%250Aresulting%2520in%2520visual%2520content%2520that%2520draws%2520focus%252C%2520enhances%2520mood%252C%2520and%2520guides%250Aemotions%2520according%2520to%2520filmmakers%2527%2520controls.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520close%2520the%250Agap%2520between%2520controllable%2520video%2520generation%2520and%2520camera%2520optics.%2520To%2520achieve%2520this%252C%250Awe%2520propose%2520AKiRa%2520%2528Augmentation%2520Kit%2520on%2520Rays%2529%252C%2520a%2520novel%2520augmentation%2520framework%250Athat%2520builds%2520and%2520trains%2520a%2520camera%2520adapter%2520with%2520a%2520complex%2520camera%2520model%2520over%2520an%250Aexisting%2520video%2520generation%2520backbone.%2520It%2520enables%2520fine-tuned%2520control%2520over%2520camera%250Amotion%2520as%2520well%2520as%2520complex%2520optical%2520parameters%2520%2528focal%2520length%252C%2520distortion%252C%250Aaperture%2529%2520to%2520achieve%2520cinematic%2520effects%2520such%2520as%2520zoom%252C%2520fisheye%2520effect%252C%2520and%2520bokeh.%250AExtensive%2520experiments%2520demonstrate%2520AKiRa%2527s%2520effectiveness%2520in%2520combining%2520and%250Acomposing%2520camera%2520optics%2520while%2520outperforming%2520all%2520state-of-the-art%2520methods.%2520This%250Awork%2520sets%2520a%2520new%2520landmark%2520in%2520controlled%2520and%2520optically%2520enhanced%2520video%2520generation%252C%250Apaving%2520the%2520way%2520for%2520future%2520optical%2520video%2520generation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AKiRa%3A%20Augmentation%20Kit%20on%20Rays%20for%20optical%20video%20generation&entry.906535625=Xi%20Wang%20and%20Robin%20Courant%20and%20Marc%20Christie%20and%20Vicky%20Kalogeiton&entry.1292438233=%20%20Recent%20advances%20in%20text-conditioned%20video%20diffusion%20have%20greatly%20improved%0Avideo%20quality.%20However%2C%20these%20methods%20offer%20limited%20or%20sometimes%20no%20control%20to%0Ausers%20on%20camera%20aspects%2C%20including%20dynamic%20camera%20motion%2C%20zoom%2C%20distorted%20lens%0Aand%20focus%20shifts.%20These%20motion%20and%20optical%20aspects%20are%20crucial%20for%20adding%0Acontrollability%20and%20cinematic%20elements%20to%20generation%20frameworks%2C%20ultimately%0Aresulting%20in%20visual%20content%20that%20draws%20focus%2C%20enhances%20mood%2C%20and%20guides%0Aemotions%20according%20to%20filmmakers%27%20controls.%20In%20this%20paper%2C%20we%20aim%20to%20close%20the%0Agap%20between%20controllable%20video%20generation%20and%20camera%20optics.%20To%20achieve%20this%2C%0Awe%20propose%20AKiRa%20%28Augmentation%20Kit%20on%20Rays%29%2C%20a%20novel%20augmentation%20framework%0Athat%20builds%20and%20trains%20a%20camera%20adapter%20with%20a%20complex%20camera%20model%20over%20an%0Aexisting%20video%20generation%20backbone.%20It%20enables%20fine-tuned%20control%20over%20camera%0Amotion%20as%20well%20as%20complex%20optical%20parameters%20%28focal%20length%2C%20distortion%2C%0Aaperture%29%20to%20achieve%20cinematic%20effects%20such%20as%20zoom%2C%20fisheye%20effect%2C%20and%20bokeh.%0AExtensive%20experiments%20demonstrate%20AKiRa%27s%20effectiveness%20in%20combining%20and%0Acomposing%20camera%20optics%20while%20outperforming%20all%20state-of-the-art%20methods.%20This%0Awork%20sets%20a%20new%20landmark%20in%20controlled%20and%20optically%20enhanced%20video%20generation%2C%0Apaving%20the%20way%20for%20future%20optical%20video%20generation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14158v1&entry.124074799=Read"},
{"title": "Hansel: Output Length Controlling Framework for Large Language Models", "author": "Seoha Song and Junhyun Lee and Hyeonmok Ko", "abstract": "  Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training.\n", "link": "http://arxiv.org/abs/2412.14033v1", "date": "2024-12-18", "relevancy": 2.2437, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.466}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4401}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4401}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hansel%3A%20Output%20Length%20Controlling%20Framework%20for%20Large%20Language%20Models&body=Title%3A%20Hansel%3A%20Output%20Length%20Controlling%20Framework%20for%20Large%20Language%20Models%0AAuthor%3A%20Seoha%20Song%20and%20Junhyun%20Lee%20and%20Hyeonmok%20Ko%0AAbstract%3A%20%20%20Despite%20the%20great%20success%20of%20large%20language%20models%20%28LLMs%29%2C%20efficiently%0Acontrolling%20the%20length%20of%20the%20output%20sequence%20still%20remains%20a%20challenge.%20In%0Athis%20paper%2C%20we%20propose%20Hansel%2C%20an%20efficient%20framework%20for%20length%20control%20in%0ALLMs%20without%20affecting%20its%20generation%20ability.%20Hansel%20utilizes%20periodically%0Aoutputted%20hidden%20special%20tokens%20to%20keep%20track%20of%20the%20remaining%20target%20length%20of%0Athe%20output%20sequence.%20Together%20with%20techniques%20to%20avoid%20abrupt%20termination%20of%0Athe%20output%2C%20this%20seemingly%20simple%20method%20proved%20to%20be%20efficient%20and%20versatile%2C%0Awhile%20not%20harming%20the%20coherency%20and%20fluency%20of%20the%20generated%20text.%20The%0Aframework%20can%20be%20applied%20to%20any%20pre-trained%20LLMs%20during%20the%20finetuning%20stage%20of%0Athe%20model%2C%20regardless%20of%20its%20original%20positional%20encoding%20method.%20We%0Ademonstrate%20this%20by%20finetuning%20four%20different%20LLMs%20with%20Hansel%20and%20show%20that%0Athe%20mean%20absolute%20error%20of%20the%20output%20sequence%20decreases%20significantly%20in%20every%0Amodel%20and%20dataset%20compared%20to%20the%20prompt-based%20length%20control%20finetuning.%0AMoreover%2C%20the%20framework%20showed%20a%20substantially%20improved%20ability%20to%20extrapolate%0Ato%20target%20lengths%20unseen%20during%20finetuning%2C%20such%20as%20long%20dialog%20responses%20or%0Aextremely%20short%20summaries.%20This%20indicates%20that%20the%20model%20learns%20the%20general%0Ameans%20of%20length%20control%2C%20rather%20than%20learning%20to%20match%20output%20lengths%20to%20those%0Aseen%20during%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHansel%253A%2520Output%2520Length%2520Controlling%2520Framework%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DSeoha%2520Song%2520and%2520Junhyun%2520Lee%2520and%2520Hyeonmok%2520Ko%26entry.1292438233%3D%2520%2520Despite%2520the%2520great%2520success%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520efficiently%250Acontrolling%2520the%2520length%2520of%2520the%2520output%2520sequence%2520still%2520remains%2520a%2520challenge.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Hansel%252C%2520an%2520efficient%2520framework%2520for%2520length%2520control%2520in%250ALLMs%2520without%2520affecting%2520its%2520generation%2520ability.%2520Hansel%2520utilizes%2520periodically%250Aoutputted%2520hidden%2520special%2520tokens%2520to%2520keep%2520track%2520of%2520the%2520remaining%2520target%2520length%2520of%250Athe%2520output%2520sequence.%2520Together%2520with%2520techniques%2520to%2520avoid%2520abrupt%2520termination%2520of%250Athe%2520output%252C%2520this%2520seemingly%2520simple%2520method%2520proved%2520to%2520be%2520efficient%2520and%2520versatile%252C%250Awhile%2520not%2520harming%2520the%2520coherency%2520and%2520fluency%2520of%2520the%2520generated%2520text.%2520The%250Aframework%2520can%2520be%2520applied%2520to%2520any%2520pre-trained%2520LLMs%2520during%2520the%2520finetuning%2520stage%2520of%250Athe%2520model%252C%2520regardless%2520of%2520its%2520original%2520positional%2520encoding%2520method.%2520We%250Ademonstrate%2520this%2520by%2520finetuning%2520four%2520different%2520LLMs%2520with%2520Hansel%2520and%2520show%2520that%250Athe%2520mean%2520absolute%2520error%2520of%2520the%2520output%2520sequence%2520decreases%2520significantly%2520in%2520every%250Amodel%2520and%2520dataset%2520compared%2520to%2520the%2520prompt-based%2520length%2520control%2520finetuning.%250AMoreover%252C%2520the%2520framework%2520showed%2520a%2520substantially%2520improved%2520ability%2520to%2520extrapolate%250Ato%2520target%2520lengths%2520unseen%2520during%2520finetuning%252C%2520such%2520as%2520long%2520dialog%2520responses%2520or%250Aextremely%2520short%2520summaries.%2520This%2520indicates%2520that%2520the%2520model%2520learns%2520the%2520general%250Ameans%2520of%2520length%2520control%252C%2520rather%2520than%2520learning%2520to%2520match%2520output%2520lengths%2520to%2520those%250Aseen%2520during%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hansel%3A%20Output%20Length%20Controlling%20Framework%20for%20Large%20Language%20Models&entry.906535625=Seoha%20Song%20and%20Junhyun%20Lee%20and%20Hyeonmok%20Ko&entry.1292438233=%20%20Despite%20the%20great%20success%20of%20large%20language%20models%20%28LLMs%29%2C%20efficiently%0Acontrolling%20the%20length%20of%20the%20output%20sequence%20still%20remains%20a%20challenge.%20In%0Athis%20paper%2C%20we%20propose%20Hansel%2C%20an%20efficient%20framework%20for%20length%20control%20in%0ALLMs%20without%20affecting%20its%20generation%20ability.%20Hansel%20utilizes%20periodically%0Aoutputted%20hidden%20special%20tokens%20to%20keep%20track%20of%20the%20remaining%20target%20length%20of%0Athe%20output%20sequence.%20Together%20with%20techniques%20to%20avoid%20abrupt%20termination%20of%0Athe%20output%2C%20this%20seemingly%20simple%20method%20proved%20to%20be%20efficient%20and%20versatile%2C%0Awhile%20not%20harming%20the%20coherency%20and%20fluency%20of%20the%20generated%20text.%20The%0Aframework%20can%20be%20applied%20to%20any%20pre-trained%20LLMs%20during%20the%20finetuning%20stage%20of%0Athe%20model%2C%20regardless%20of%20its%20original%20positional%20encoding%20method.%20We%0Ademonstrate%20this%20by%20finetuning%20four%20different%20LLMs%20with%20Hansel%20and%20show%20that%0Athe%20mean%20absolute%20error%20of%20the%20output%20sequence%20decreases%20significantly%20in%20every%0Amodel%20and%20dataset%20compared%20to%20the%20prompt-based%20length%20control%20finetuning.%0AMoreover%2C%20the%20framework%20showed%20a%20substantially%20improved%20ability%20to%20extrapolate%0Ato%20target%20lengths%20unseen%20during%20finetuning%2C%20such%20as%20long%20dialog%20responses%20or%0Aextremely%20short%20summaries.%20This%20indicates%20that%20the%20model%20learns%20the%20general%0Ameans%20of%20length%20control%2C%20rather%20than%20learning%20to%20match%20output%20lengths%20to%20those%0Aseen%20during%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14033v1&entry.124074799=Read"},
{"title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction", "author": "Chen Bao and Jiarui Xu and Xiaolong Wang and Abhinav Gupta and Homanga Bharadhwaj", "abstract": "  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results https://www.chenbao.tech/handsonvlm/\n", "link": "http://arxiv.org/abs/2412.13187v2", "date": "2024-12-18", "relevancy": 2.2222, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5869}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HandsOnVLM%3A%20Vision-Language%20Models%20for%20Hand-Object%20Interaction%0A%20%20Prediction&body=Title%3A%20HandsOnVLM%3A%20Vision-Language%20Models%20for%20Hand-Object%20Interaction%0A%20%20Prediction%0AAuthor%3A%20Chen%20Bao%20and%20Jiarui%20Xu%20and%20Xiaolong%20Wang%20and%20Abhinav%20Gupta%20and%20Homanga%20Bharadhwaj%0AAbstract%3A%20%20%20How%20can%20we%20predict%20future%20interaction%20trajectories%20of%20human%20hands%20in%20a%20scene%0Agiven%20high-level%20colloquial%20task%20specifications%20in%20the%20form%20of%20natural%0Alanguage%3F%20In%20this%20paper%2C%20we%20extend%20the%20classic%20hand%20trajectory%20prediction%20task%0Ato%20two%20tasks%20involving%20explicit%20or%20implicit%20language%20queries.%20Our%20proposed%0Atasks%20require%20extensive%20understanding%20of%20human%20daily%20activities%20and%20reasoning%0Aabilities%20about%20what%20should%20be%20happening%20next%20given%20cues%20from%20the%20current%0Ascene.%20We%20also%20develop%20new%20benchmarks%20to%20evaluate%20the%20proposed%20two%20tasks%2C%0AVanilla%20Hand%20Prediction%20%28VHP%29%20and%20Reasoning-Based%20Hand%20Prediction%20%28RBHP%29.%20We%0Aenable%20solving%20these%20tasks%20by%20integrating%20high-level%20world%20knowledge%20and%0Areasoning%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29%20with%20the%0Aauto-regressive%20nature%20of%20low-level%20ego-centric%20hand%20trajectories.%20Our%20model%2C%0AHandsOnVLM%20is%20a%20novel%20VLM%20that%20can%20generate%20textual%20responses%20and%20produce%0Afuture%20hand%20trajectories%20through%20natural-language%20conversations.%20Our%0Aexperiments%20show%20that%20HandsOnVLM%20outperforms%20existing%20task-specific%20methods%20and%0Aother%20VLM%20baselines%20on%20proposed%20tasks%2C%20and%20demonstrates%20its%20ability%20to%0Aeffectively%20utilize%20world%20knowledge%20for%20reasoning%20about%20low-level%20human%20hand%0Atrajectories%20based%20on%20the%20provided%20context.%20Our%20website%20contains%20code%20and%0Adetailed%20video%20results%20https%3A//www.chenbao.tech/handsonvlm/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13187v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandsOnVLM%253A%2520Vision-Language%2520Models%2520for%2520Hand-Object%2520Interaction%250A%2520%2520Prediction%26entry.906535625%3DChen%2520Bao%2520and%2520Jiarui%2520Xu%2520and%2520Xiaolong%2520Wang%2520and%2520Abhinav%2520Gupta%2520and%2520Homanga%2520Bharadhwaj%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520predict%2520future%2520interaction%2520trajectories%2520of%2520human%2520hands%2520in%2520a%2520scene%250Agiven%2520high-level%2520colloquial%2520task%2520specifications%2520in%2520the%2520form%2520of%2520natural%250Alanguage%253F%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%2520classic%2520hand%2520trajectory%2520prediction%2520task%250Ato%2520two%2520tasks%2520involving%2520explicit%2520or%2520implicit%2520language%2520queries.%2520Our%2520proposed%250Atasks%2520require%2520extensive%2520understanding%2520of%2520human%2520daily%2520activities%2520and%2520reasoning%250Aabilities%2520about%2520what%2520should%2520be%2520happening%2520next%2520given%2520cues%2520from%2520the%2520current%250Ascene.%2520We%2520also%2520develop%2520new%2520benchmarks%2520to%2520evaluate%2520the%2520proposed%2520two%2520tasks%252C%250AVanilla%2520Hand%2520Prediction%2520%2528VHP%2529%2520and%2520Reasoning-Based%2520Hand%2520Prediction%2520%2528RBHP%2529.%2520We%250Aenable%2520solving%2520these%2520tasks%2520by%2520integrating%2520high-level%2520world%2520knowledge%2520and%250Areasoning%2520capabilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520with%2520the%250Aauto-regressive%2520nature%2520of%2520low-level%2520ego-centric%2520hand%2520trajectories.%2520Our%2520model%252C%250AHandsOnVLM%2520is%2520a%2520novel%2520VLM%2520that%2520can%2520generate%2520textual%2520responses%2520and%2520produce%250Afuture%2520hand%2520trajectories%2520through%2520natural-language%2520conversations.%2520Our%250Aexperiments%2520show%2520that%2520HandsOnVLM%2520outperforms%2520existing%2520task-specific%2520methods%2520and%250Aother%2520VLM%2520baselines%2520on%2520proposed%2520tasks%252C%2520and%2520demonstrates%2520its%2520ability%2520to%250Aeffectively%2520utilize%2520world%2520knowledge%2520for%2520reasoning%2520about%2520low-level%2520human%2520hand%250Atrajectories%2520based%2520on%2520the%2520provided%2520context.%2520Our%2520website%2520contains%2520code%2520and%250Adetailed%2520video%2520results%2520https%253A//www.chenbao.tech/handsonvlm/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13187v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandsOnVLM%3A%20Vision-Language%20Models%20for%20Hand-Object%20Interaction%0A%20%20Prediction&entry.906535625=Chen%20Bao%20and%20Jiarui%20Xu%20and%20Xiaolong%20Wang%20and%20Abhinav%20Gupta%20and%20Homanga%20Bharadhwaj&entry.1292438233=%20%20How%20can%20we%20predict%20future%20interaction%20trajectories%20of%20human%20hands%20in%20a%20scene%0Agiven%20high-level%20colloquial%20task%20specifications%20in%20the%20form%20of%20natural%0Alanguage%3F%20In%20this%20paper%2C%20we%20extend%20the%20classic%20hand%20trajectory%20prediction%20task%0Ato%20two%20tasks%20involving%20explicit%20or%20implicit%20language%20queries.%20Our%20proposed%0Atasks%20require%20extensive%20understanding%20of%20human%20daily%20activities%20and%20reasoning%0Aabilities%20about%20what%20should%20be%20happening%20next%20given%20cues%20from%20the%20current%0Ascene.%20We%20also%20develop%20new%20benchmarks%20to%20evaluate%20the%20proposed%20two%20tasks%2C%0AVanilla%20Hand%20Prediction%20%28VHP%29%20and%20Reasoning-Based%20Hand%20Prediction%20%28RBHP%29.%20We%0Aenable%20solving%20these%20tasks%20by%20integrating%20high-level%20world%20knowledge%20and%0Areasoning%20capabilities%20of%20Vision-Language%20Models%20%28VLMs%29%20with%20the%0Aauto-regressive%20nature%20of%20low-level%20ego-centric%20hand%20trajectories.%20Our%20model%2C%0AHandsOnVLM%20is%20a%20novel%20VLM%20that%20can%20generate%20textual%20responses%20and%20produce%0Afuture%20hand%20trajectories%20through%20natural-language%20conversations.%20Our%0Aexperiments%20show%20that%20HandsOnVLM%20outperforms%20existing%20task-specific%20methods%20and%0Aother%20VLM%20baselines%20on%20proposed%20tasks%2C%20and%20demonstrates%20its%20ability%20to%0Aeffectively%20utilize%20world%20knowledge%20for%20reasoning%20about%20low-level%20human%20hand%0Atrajectories%20based%20on%20the%20provided%20context.%20Our%20website%20contains%20code%20and%0Adetailed%20video%20results%20https%3A//www.chenbao.tech/handsonvlm/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13187v2&entry.124074799=Read"},
{"title": "Extreme Multi-label Completion for Semantic Document Labelling with\n  Taxonomy-Aware Parallel Learning", "author": "Julien Audiffren and Christophe Broillet and Ljiljana Dolamic and Philippe Cudr\u00e9-Mauroux", "abstract": "  In Extreme Multi Label Completion (XMLCo), the objective is to predict the\nmissing labels of a collection of documents. Together with XML Classification,\nXMLCo is arguably one of the most challenging document classification tasks, as\nthe very high number of labels (at least ten of thousands) is generally very\nlarge compared to the number of available labelled documents in the training\ndataset. Such a task is often accompanied by a taxonomy that encodes the labels\norganic relationships, and many methods have been proposed to leverage this\nhierarchy to improve the results of XMLCo algorithms. In this paper, we propose\na new approach to this problem, TAMLEC (Taxonomy-Aware Multi-task Learning for\nExtreme multi-label Completion). TAMLEC divides the problem into several\nTaxonomy-Aware Tasks, i.e. subsets of labels adapted to the hierarchical paths\nof the taxonomy, and trains on these tasks using a dynamic Parallel Feature\nsharing approach, where some parts of the model are shared between tasks while\nothers are task-specific. Then, at inference time, TAMLEC uses the labels\navailable in a document to infer the appropriate tasks and to predict missing\nlabels. To achieve this result, TAMLEC uses a modified transformer architecture\nthat predicts ordered sequences of labels on a Weak-Semilattice structure that\nis naturally induced by the tasks. This approach yields multiple advantages.\nFirst, our experiments on real-world datasets show that TAMLEC outperforms\nstate-of-the-art methods for various XMLCo problems. Second, TAMLEC is by\nconstruction particularly suited for few-shots XML tasks, where new tasks or\nlabels are introduced with only few examples, and extensive evaluations\nhighlight its strong performance compared to existing methods.\n", "link": "http://arxiv.org/abs/2412.13809v1", "date": "2024-12-18", "relevancy": 2.2045, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5969}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5184}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extreme%20Multi-label%20Completion%20for%20Semantic%20Document%20Labelling%20with%0A%20%20Taxonomy-Aware%20Parallel%20Learning&body=Title%3A%20Extreme%20Multi-label%20Completion%20for%20Semantic%20Document%20Labelling%20with%0A%20%20Taxonomy-Aware%20Parallel%20Learning%0AAuthor%3A%20Julien%20Audiffren%20and%20Christophe%20Broillet%20and%20Ljiljana%20Dolamic%20and%20Philippe%20Cudr%C3%A9-Mauroux%0AAbstract%3A%20%20%20In%20Extreme%20Multi%20Label%20Completion%20%28XMLCo%29%2C%20the%20objective%20is%20to%20predict%20the%0Amissing%20labels%20of%20a%20collection%20of%20documents.%20Together%20with%20XML%20Classification%2C%0AXMLCo%20is%20arguably%20one%20of%20the%20most%20challenging%20document%20classification%20tasks%2C%20as%0Athe%20very%20high%20number%20of%20labels%20%28at%20least%20ten%20of%20thousands%29%20is%20generally%20very%0Alarge%20compared%20to%20the%20number%20of%20available%20labelled%20documents%20in%20the%20training%0Adataset.%20Such%20a%20task%20is%20often%20accompanied%20by%20a%20taxonomy%20that%20encodes%20the%20labels%0Aorganic%20relationships%2C%20and%20many%20methods%20have%20been%20proposed%20to%20leverage%20this%0Ahierarchy%20to%20improve%20the%20results%20of%20XMLCo%20algorithms.%20In%20this%20paper%2C%20we%20propose%0Aa%20new%20approach%20to%20this%20problem%2C%20TAMLEC%20%28Taxonomy-Aware%20Multi-task%20Learning%20for%0AExtreme%20multi-label%20Completion%29.%20TAMLEC%20divides%20the%20problem%20into%20several%0ATaxonomy-Aware%20Tasks%2C%20i.e.%20subsets%20of%20labels%20adapted%20to%20the%20hierarchical%20paths%0Aof%20the%20taxonomy%2C%20and%20trains%20on%20these%20tasks%20using%20a%20dynamic%20Parallel%20Feature%0Asharing%20approach%2C%20where%20some%20parts%20of%20the%20model%20are%20shared%20between%20tasks%20while%0Aothers%20are%20task-specific.%20Then%2C%20at%20inference%20time%2C%20TAMLEC%20uses%20the%20labels%0Aavailable%20in%20a%20document%20to%20infer%20the%20appropriate%20tasks%20and%20to%20predict%20missing%0Alabels.%20To%20achieve%20this%20result%2C%20TAMLEC%20uses%20a%20modified%20transformer%20architecture%0Athat%20predicts%20ordered%20sequences%20of%20labels%20on%20a%20Weak-Semilattice%20structure%20that%0Ais%20naturally%20induced%20by%20the%20tasks.%20This%20approach%20yields%20multiple%20advantages.%0AFirst%2C%20our%20experiments%20on%20real-world%20datasets%20show%20that%20TAMLEC%20outperforms%0Astate-of-the-art%20methods%20for%20various%20XMLCo%20problems.%20Second%2C%20TAMLEC%20is%20by%0Aconstruction%20particularly%20suited%20for%20few-shots%20XML%20tasks%2C%20where%20new%20tasks%20or%0Alabels%20are%20introduced%20with%20only%20few%20examples%2C%20and%20extensive%20evaluations%0Ahighlight%20its%20strong%20performance%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtreme%2520Multi-label%2520Completion%2520for%2520Semantic%2520Document%2520Labelling%2520with%250A%2520%2520Taxonomy-Aware%2520Parallel%2520Learning%26entry.906535625%3DJulien%2520Audiffren%2520and%2520Christophe%2520Broillet%2520and%2520Ljiljana%2520Dolamic%2520and%2520Philippe%2520Cudr%25C3%25A9-Mauroux%26entry.1292438233%3D%2520%2520In%2520Extreme%2520Multi%2520Label%2520Completion%2520%2528XMLCo%2529%252C%2520the%2520objective%2520is%2520to%2520predict%2520the%250Amissing%2520labels%2520of%2520a%2520collection%2520of%2520documents.%2520Together%2520with%2520XML%2520Classification%252C%250AXMLCo%2520is%2520arguably%2520one%2520of%2520the%2520most%2520challenging%2520document%2520classification%2520tasks%252C%2520as%250Athe%2520very%2520high%2520number%2520of%2520labels%2520%2528at%2520least%2520ten%2520of%2520thousands%2529%2520is%2520generally%2520very%250Alarge%2520compared%2520to%2520the%2520number%2520of%2520available%2520labelled%2520documents%2520in%2520the%2520training%250Adataset.%2520Such%2520a%2520task%2520is%2520often%2520accompanied%2520by%2520a%2520taxonomy%2520that%2520encodes%2520the%2520labels%250Aorganic%2520relationships%252C%2520and%2520many%2520methods%2520have%2520been%2520proposed%2520to%2520leverage%2520this%250Ahierarchy%2520to%2520improve%2520the%2520results%2520of%2520XMLCo%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520new%2520approach%2520to%2520this%2520problem%252C%2520TAMLEC%2520%2528Taxonomy-Aware%2520Multi-task%2520Learning%2520for%250AExtreme%2520multi-label%2520Completion%2529.%2520TAMLEC%2520divides%2520the%2520problem%2520into%2520several%250ATaxonomy-Aware%2520Tasks%252C%2520i.e.%2520subsets%2520of%2520labels%2520adapted%2520to%2520the%2520hierarchical%2520paths%250Aof%2520the%2520taxonomy%252C%2520and%2520trains%2520on%2520these%2520tasks%2520using%2520a%2520dynamic%2520Parallel%2520Feature%250Asharing%2520approach%252C%2520where%2520some%2520parts%2520of%2520the%2520model%2520are%2520shared%2520between%2520tasks%2520while%250Aothers%2520are%2520task-specific.%2520Then%252C%2520at%2520inference%2520time%252C%2520TAMLEC%2520uses%2520the%2520labels%250Aavailable%2520in%2520a%2520document%2520to%2520infer%2520the%2520appropriate%2520tasks%2520and%2520to%2520predict%2520missing%250Alabels.%2520To%2520achieve%2520this%2520result%252C%2520TAMLEC%2520uses%2520a%2520modified%2520transformer%2520architecture%250Athat%2520predicts%2520ordered%2520sequences%2520of%2520labels%2520on%2520a%2520Weak-Semilattice%2520structure%2520that%250Ais%2520naturally%2520induced%2520by%2520the%2520tasks.%2520This%2520approach%2520yields%2520multiple%2520advantages.%250AFirst%252C%2520our%2520experiments%2520on%2520real-world%2520datasets%2520show%2520that%2520TAMLEC%2520outperforms%250Astate-of-the-art%2520methods%2520for%2520various%2520XMLCo%2520problems.%2520Second%252C%2520TAMLEC%2520is%2520by%250Aconstruction%2520particularly%2520suited%2520for%2520few-shots%2520XML%2520tasks%252C%2520where%2520new%2520tasks%2520or%250Alabels%2520are%2520introduced%2520with%2520only%2520few%2520examples%252C%2520and%2520extensive%2520evaluations%250Ahighlight%2520its%2520strong%2520performance%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extreme%20Multi-label%20Completion%20for%20Semantic%20Document%20Labelling%20with%0A%20%20Taxonomy-Aware%20Parallel%20Learning&entry.906535625=Julien%20Audiffren%20and%20Christophe%20Broillet%20and%20Ljiljana%20Dolamic%20and%20Philippe%20Cudr%C3%A9-Mauroux&entry.1292438233=%20%20In%20Extreme%20Multi%20Label%20Completion%20%28XMLCo%29%2C%20the%20objective%20is%20to%20predict%20the%0Amissing%20labels%20of%20a%20collection%20of%20documents.%20Together%20with%20XML%20Classification%2C%0AXMLCo%20is%20arguably%20one%20of%20the%20most%20challenging%20document%20classification%20tasks%2C%20as%0Athe%20very%20high%20number%20of%20labels%20%28at%20least%20ten%20of%20thousands%29%20is%20generally%20very%0Alarge%20compared%20to%20the%20number%20of%20available%20labelled%20documents%20in%20the%20training%0Adataset.%20Such%20a%20task%20is%20often%20accompanied%20by%20a%20taxonomy%20that%20encodes%20the%20labels%0Aorganic%20relationships%2C%20and%20many%20methods%20have%20been%20proposed%20to%20leverage%20this%0Ahierarchy%20to%20improve%20the%20results%20of%20XMLCo%20algorithms.%20In%20this%20paper%2C%20we%20propose%0Aa%20new%20approach%20to%20this%20problem%2C%20TAMLEC%20%28Taxonomy-Aware%20Multi-task%20Learning%20for%0AExtreme%20multi-label%20Completion%29.%20TAMLEC%20divides%20the%20problem%20into%20several%0ATaxonomy-Aware%20Tasks%2C%20i.e.%20subsets%20of%20labels%20adapted%20to%20the%20hierarchical%20paths%0Aof%20the%20taxonomy%2C%20and%20trains%20on%20these%20tasks%20using%20a%20dynamic%20Parallel%20Feature%0Asharing%20approach%2C%20where%20some%20parts%20of%20the%20model%20are%20shared%20between%20tasks%20while%0Aothers%20are%20task-specific.%20Then%2C%20at%20inference%20time%2C%20TAMLEC%20uses%20the%20labels%0Aavailable%20in%20a%20document%20to%20infer%20the%20appropriate%20tasks%20and%20to%20predict%20missing%0Alabels.%20To%20achieve%20this%20result%2C%20TAMLEC%20uses%20a%20modified%20transformer%20architecture%0Athat%20predicts%20ordered%20sequences%20of%20labels%20on%20a%20Weak-Semilattice%20structure%20that%0Ais%20naturally%20induced%20by%20the%20tasks.%20This%20approach%20yields%20multiple%20advantages.%0AFirst%2C%20our%20experiments%20on%20real-world%20datasets%20show%20that%20TAMLEC%20outperforms%0Astate-of-the-art%20methods%20for%20various%20XMLCo%20problems.%20Second%2C%20TAMLEC%20is%20by%0Aconstruction%20particularly%20suited%20for%20few-shots%20XML%20tasks%2C%20where%20new%20tasks%20or%0Alabels%20are%20introduced%20with%20only%20few%20examples%2C%20and%20extensive%20evaluations%0Ahighlight%20its%20strong%20performance%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13809v1&entry.124074799=Read"},
{"title": "CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer\n  Learning", "author": "Eloy Geenjaar and Lie Lu", "abstract": "  Transfer learning for bio-signals has recently become an important technique\nto improve prediction performance on downstream tasks with small bio-signal\ndatasets. Recent works have shown that pre-training a neural network model on a\nlarge dataset (e.g. EEG) with a self-supervised task, replacing the\nself-supervised head with a linear classification head, and fine-tuning the\nmodel on different downstream bio-signal datasets (e.g., EMG or ECG) can\ndramatically improve the performance on those datasets. In this paper, we\npropose a new convolution-transformer hybrid model architecture with masked\nauto-encoding for low-data bio-signal transfer learning, introduce a\nfrequency-based masked auto-encoding task, employ a more comprehensive\nevaluation framework, and evaluate how much and when (multimodal) pre-training\nimproves fine-tuning performance. We also introduce a dramatically more\nperformant method of aligning a downstream dataset with a different temporal\nlength and sampling rate to the original pre-training dataset. Our findings\nindicate that the convolution-only part of our hybrid model can achieve\nstate-of-the-art performance on some low-data downstream tasks. The performance\nis often improved even further with our full model. In the case of\ntransformer-based models we find that pre-training especially improves\nperformance on downstream datasets, multimodal pre-training often increases\nthose gains further, and our frequency-based pre-training performs the best on\naverage for the lowest and highest data regimes.\n", "link": "http://arxiv.org/abs/2412.11695v2", "date": "2024-12-18", "relevancy": 2.2037, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5476}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CiTrus%3A%20Squeezing%20Extra%20Performance%20out%20of%20Low-data%20Bio-signal%20Transfer%0A%20%20Learning&body=Title%3A%20CiTrus%3A%20Squeezing%20Extra%20Performance%20out%20of%20Low-data%20Bio-signal%20Transfer%0A%20%20Learning%0AAuthor%3A%20Eloy%20Geenjaar%20and%20Lie%20Lu%0AAbstract%3A%20%20%20Transfer%20learning%20for%20bio-signals%20has%20recently%20become%20an%20important%20technique%0Ato%20improve%20prediction%20performance%20on%20downstream%20tasks%20with%20small%20bio-signal%0Adatasets.%20Recent%20works%20have%20shown%20that%20pre-training%20a%20neural%20network%20model%20on%20a%0Alarge%20dataset%20%28e.g.%20EEG%29%20with%20a%20self-supervised%20task%2C%20replacing%20the%0Aself-supervised%20head%20with%20a%20linear%20classification%20head%2C%20and%20fine-tuning%20the%0Amodel%20on%20different%20downstream%20bio-signal%20datasets%20%28e.g.%2C%20EMG%20or%20ECG%29%20can%0Adramatically%20improve%20the%20performance%20on%20those%20datasets.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20convolution-transformer%20hybrid%20model%20architecture%20with%20masked%0Aauto-encoding%20for%20low-data%20bio-signal%20transfer%20learning%2C%20introduce%20a%0Afrequency-based%20masked%20auto-encoding%20task%2C%20employ%20a%20more%20comprehensive%0Aevaluation%20framework%2C%20and%20evaluate%20how%20much%20and%20when%20%28multimodal%29%20pre-training%0Aimproves%20fine-tuning%20performance.%20We%20also%20introduce%20a%20dramatically%20more%0Aperformant%20method%20of%20aligning%20a%20downstream%20dataset%20with%20a%20different%20temporal%0Alength%20and%20sampling%20rate%20to%20the%20original%20pre-training%20dataset.%20Our%20findings%0Aindicate%20that%20the%20convolution-only%20part%20of%20our%20hybrid%20model%20can%20achieve%0Astate-of-the-art%20performance%20on%20some%20low-data%20downstream%20tasks.%20The%20performance%0Ais%20often%20improved%20even%20further%20with%20our%20full%20model.%20In%20the%20case%20of%0Atransformer-based%20models%20we%20find%20that%20pre-training%20especially%20improves%0Aperformance%20on%20downstream%20datasets%2C%20multimodal%20pre-training%20often%20increases%0Athose%20gains%20further%2C%20and%20our%20frequency-based%20pre-training%20performs%20the%20best%20on%0Aaverage%20for%20the%20lowest%20and%20highest%20data%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCiTrus%253A%2520Squeezing%2520Extra%2520Performance%2520out%2520of%2520Low-data%2520Bio-signal%2520Transfer%250A%2520%2520Learning%26entry.906535625%3DEloy%2520Geenjaar%2520and%2520Lie%2520Lu%26entry.1292438233%3D%2520%2520Transfer%2520learning%2520for%2520bio-signals%2520has%2520recently%2520become%2520an%2520important%2520technique%250Ato%2520improve%2520prediction%2520performance%2520on%2520downstream%2520tasks%2520with%2520small%2520bio-signal%250Adatasets.%2520Recent%2520works%2520have%2520shown%2520that%2520pre-training%2520a%2520neural%2520network%2520model%2520on%2520a%250Alarge%2520dataset%2520%2528e.g.%2520EEG%2529%2520with%2520a%2520self-supervised%2520task%252C%2520replacing%2520the%250Aself-supervised%2520head%2520with%2520a%2520linear%2520classification%2520head%252C%2520and%2520fine-tuning%2520the%250Amodel%2520on%2520different%2520downstream%2520bio-signal%2520datasets%2520%2528e.g.%252C%2520EMG%2520or%2520ECG%2529%2520can%250Adramatically%2520improve%2520the%2520performance%2520on%2520those%2520datasets.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520new%2520convolution-transformer%2520hybrid%2520model%2520architecture%2520with%2520masked%250Aauto-encoding%2520for%2520low-data%2520bio-signal%2520transfer%2520learning%252C%2520introduce%2520a%250Afrequency-based%2520masked%2520auto-encoding%2520task%252C%2520employ%2520a%2520more%2520comprehensive%250Aevaluation%2520framework%252C%2520and%2520evaluate%2520how%2520much%2520and%2520when%2520%2528multimodal%2529%2520pre-training%250Aimproves%2520fine-tuning%2520performance.%2520We%2520also%2520introduce%2520a%2520dramatically%2520more%250Aperformant%2520method%2520of%2520aligning%2520a%2520downstream%2520dataset%2520with%2520a%2520different%2520temporal%250Alength%2520and%2520sampling%2520rate%2520to%2520the%2520original%2520pre-training%2520dataset.%2520Our%2520findings%250Aindicate%2520that%2520the%2520convolution-only%2520part%2520of%2520our%2520hybrid%2520model%2520can%2520achieve%250Astate-of-the-art%2520performance%2520on%2520some%2520low-data%2520downstream%2520tasks.%2520The%2520performance%250Ais%2520often%2520improved%2520even%2520further%2520with%2520our%2520full%2520model.%2520In%2520the%2520case%2520of%250Atransformer-based%2520models%2520we%2520find%2520that%2520pre-training%2520especially%2520improves%250Aperformance%2520on%2520downstream%2520datasets%252C%2520multimodal%2520pre-training%2520often%2520increases%250Athose%2520gains%2520further%252C%2520and%2520our%2520frequency-based%2520pre-training%2520performs%2520the%2520best%2520on%250Aaverage%2520for%2520the%2520lowest%2520and%2520highest%2520data%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CiTrus%3A%20Squeezing%20Extra%20Performance%20out%20of%20Low-data%20Bio-signal%20Transfer%0A%20%20Learning&entry.906535625=Eloy%20Geenjaar%20and%20Lie%20Lu&entry.1292438233=%20%20Transfer%20learning%20for%20bio-signals%20has%20recently%20become%20an%20important%20technique%0Ato%20improve%20prediction%20performance%20on%20downstream%20tasks%20with%20small%20bio-signal%0Adatasets.%20Recent%20works%20have%20shown%20that%20pre-training%20a%20neural%20network%20model%20on%20a%0Alarge%20dataset%20%28e.g.%20EEG%29%20with%20a%20self-supervised%20task%2C%20replacing%20the%0Aself-supervised%20head%20with%20a%20linear%20classification%20head%2C%20and%20fine-tuning%20the%0Amodel%20on%20different%20downstream%20bio-signal%20datasets%20%28e.g.%2C%20EMG%20or%20ECG%29%20can%0Adramatically%20improve%20the%20performance%20on%20those%20datasets.%20In%20this%20paper%2C%20we%0Apropose%20a%20new%20convolution-transformer%20hybrid%20model%20architecture%20with%20masked%0Aauto-encoding%20for%20low-data%20bio-signal%20transfer%20learning%2C%20introduce%20a%0Afrequency-based%20masked%20auto-encoding%20task%2C%20employ%20a%20more%20comprehensive%0Aevaluation%20framework%2C%20and%20evaluate%20how%20much%20and%20when%20%28multimodal%29%20pre-training%0Aimproves%20fine-tuning%20performance.%20We%20also%20introduce%20a%20dramatically%20more%0Aperformant%20method%20of%20aligning%20a%20downstream%20dataset%20with%20a%20different%20temporal%0Alength%20and%20sampling%20rate%20to%20the%20original%20pre-training%20dataset.%20Our%20findings%0Aindicate%20that%20the%20convolution-only%20part%20of%20our%20hybrid%20model%20can%20achieve%0Astate-of-the-art%20performance%20on%20some%20low-data%20downstream%20tasks.%20The%20performance%0Ais%20often%20improved%20even%20further%20with%20our%20full%20model.%20In%20the%20case%20of%0Atransformer-based%20models%20we%20find%20that%20pre-training%20especially%20improves%0Aperformance%20on%20downstream%20datasets%2C%20multimodal%20pre-training%20often%20increases%0Athose%20gains%20further%2C%20and%20our%20frequency-based%20pre-training%20performs%20the%20best%20on%0Aaverage%20for%20the%20lowest%20and%20highest%20data%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11695v2&entry.124074799=Read"},
{"title": "PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction", "author": "Yujing Xue and Jiaxiang Liu and Jiawei Du and Joey Tianyi Zhou", "abstract": "  Recently, polar coordinate-based representations have shown promise for 3D\nperceptual tasks. Compared to Cartesian methods, polar grids provide a viable\nalternative, offering better detail preservation in nearby spaces while\ncovering larger areas. However, they face feature distortion due to non-uniform\ndivision. To address these issues, we introduce the Polar Voxel Occupancy\nPredictor (PVP), a novel 3D multi-modal predictor that operates in polar\ncoordinates. PVP features two key design elements to overcome distortion: a\nGlobal Represent Propagation (GRP) module that integrates global spatial data\ninto 3D volumes, and a Plane Decomposed Convolution (PD-Conv) that simplifies\n3D distortions into 2D convolutions. These innovations enable PVP to outperform\nexisting methods, achieving significant improvements in mIoU and IoU metrics on\nthe OpenOccupancy dataset.\n", "link": "http://arxiv.org/abs/2412.07616v2", "date": "2024-12-18", "relevancy": 2.1969, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.568}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PVP%3A%20Polar%20Representation%20Boost%20for%203D%20Semantic%20Occupancy%20Prediction&body=Title%3A%20PVP%3A%20Polar%20Representation%20Boost%20for%203D%20Semantic%20Occupancy%20Prediction%0AAuthor%3A%20Yujing%20Xue%20and%20Jiaxiang%20Liu%20and%20Jiawei%20Du%20and%20Joey%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20polar%20coordinate-based%20representations%20have%20shown%20promise%20for%203D%0Aperceptual%20tasks.%20Compared%20to%20Cartesian%20methods%2C%20polar%20grids%20provide%20a%20viable%0Aalternative%2C%20offering%20better%20detail%20preservation%20in%20nearby%20spaces%20while%0Acovering%20larger%20areas.%20However%2C%20they%20face%20feature%20distortion%20due%20to%20non-uniform%0Adivision.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20Polar%20Voxel%20Occupancy%0APredictor%20%28PVP%29%2C%20a%20novel%203D%20multi-modal%20predictor%20that%20operates%20in%20polar%0Acoordinates.%20PVP%20features%20two%20key%20design%20elements%20to%20overcome%20distortion%3A%20a%0AGlobal%20Represent%20Propagation%20%28GRP%29%20module%20that%20integrates%20global%20spatial%20data%0Ainto%203D%20volumes%2C%20and%20a%20Plane%20Decomposed%20Convolution%20%28PD-Conv%29%20that%20simplifies%0A3D%20distortions%20into%202D%20convolutions.%20These%20innovations%20enable%20PVP%20to%20outperform%0Aexisting%20methods%2C%20achieving%20significant%20improvements%20in%20mIoU%20and%20IoU%20metrics%20on%0Athe%20OpenOccupancy%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07616v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPVP%253A%2520Polar%2520Representation%2520Boost%2520for%25203D%2520Semantic%2520Occupancy%2520Prediction%26entry.906535625%3DYujing%2520Xue%2520and%2520Jiaxiang%2520Liu%2520and%2520Jiawei%2520Du%2520and%2520Joey%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520polar%2520coordinate-based%2520representations%2520have%2520shown%2520promise%2520for%25203D%250Aperceptual%2520tasks.%2520Compared%2520to%2520Cartesian%2520methods%252C%2520polar%2520grids%2520provide%2520a%2520viable%250Aalternative%252C%2520offering%2520better%2520detail%2520preservation%2520in%2520nearby%2520spaces%2520while%250Acovering%2520larger%2520areas.%2520However%252C%2520they%2520face%2520feature%2520distortion%2520due%2520to%2520non-uniform%250Adivision.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520the%2520Polar%2520Voxel%2520Occupancy%250APredictor%2520%2528PVP%2529%252C%2520a%2520novel%25203D%2520multi-modal%2520predictor%2520that%2520operates%2520in%2520polar%250Acoordinates.%2520PVP%2520features%2520two%2520key%2520design%2520elements%2520to%2520overcome%2520distortion%253A%2520a%250AGlobal%2520Represent%2520Propagation%2520%2528GRP%2529%2520module%2520that%2520integrates%2520global%2520spatial%2520data%250Ainto%25203D%2520volumes%252C%2520and%2520a%2520Plane%2520Decomposed%2520Convolution%2520%2528PD-Conv%2529%2520that%2520simplifies%250A3D%2520distortions%2520into%25202D%2520convolutions.%2520These%2520innovations%2520enable%2520PVP%2520to%2520outperform%250Aexisting%2520methods%252C%2520achieving%2520significant%2520improvements%2520in%2520mIoU%2520and%2520IoU%2520metrics%2520on%250Athe%2520OpenOccupancy%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07616v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PVP%3A%20Polar%20Representation%20Boost%20for%203D%20Semantic%20Occupancy%20Prediction&entry.906535625=Yujing%20Xue%20and%20Jiaxiang%20Liu%20and%20Jiawei%20Du%20and%20Joey%20Tianyi%20Zhou&entry.1292438233=%20%20Recently%2C%20polar%20coordinate-based%20representations%20have%20shown%20promise%20for%203D%0Aperceptual%20tasks.%20Compared%20to%20Cartesian%20methods%2C%20polar%20grids%20provide%20a%20viable%0Aalternative%2C%20offering%20better%20detail%20preservation%20in%20nearby%20spaces%20while%0Acovering%20larger%20areas.%20However%2C%20they%20face%20feature%20distortion%20due%20to%20non-uniform%0Adivision.%20To%20address%20these%20issues%2C%20we%20introduce%20the%20Polar%20Voxel%20Occupancy%0APredictor%20%28PVP%29%2C%20a%20novel%203D%20multi-modal%20predictor%20that%20operates%20in%20polar%0Acoordinates.%20PVP%20features%20two%20key%20design%20elements%20to%20overcome%20distortion%3A%20a%0AGlobal%20Represent%20Propagation%20%28GRP%29%20module%20that%20integrates%20global%20spatial%20data%0Ainto%203D%20volumes%2C%20and%20a%20Plane%20Decomposed%20Convolution%20%28PD-Conv%29%20that%20simplifies%0A3D%20distortions%20into%202D%20convolutions.%20These%20innovations%20enable%20PVP%20to%20outperform%0Aexisting%20methods%2C%20achieving%20significant%20improvements%20in%20mIoU%20and%20IoU%20metrics%20on%0Athe%20OpenOccupancy%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07616v2&entry.124074799=Read"},
{"title": "Immersive Human-in-the-Loop Control: Real-Time 3D Surface Meshing and\n  Physics Simulation", "author": "Sait Akturk and Justin Valentine and Junaid Ahmad and Martin Jagersand", "abstract": "  This paper introduces the TactiMesh Teleoperator Interface (TTI), a novel\npredictive visual and haptic system designed explicitly for human-in-the-loop\nrobot control using a head-mounted display (HMD). By employing simultaneous\nlocalization and mapping (SLAM)in tandem with a space carving method (CARV),\nTTI creates a real time 3D surface mesh of remote environments from an RGB\ncamera mounted on a Barrett WAM arm. The generated mesh is integrated into a\nphysics simulator, featuring a digital twin of the WAM robot arm to create a\nvirtual environment. In this virtual environment, TTI provides haptic feedback\ndirectly in response to the operator's movements, eliminating the problem with\ndelayed response from the haptic follower robot. Furthermore, texturing the 3D\nmesh with keyframes from SLAM allows the operator to control the viewpoint of\ntheir Head Mounted Display (HMD) independently of the arm-mounted robot camera,\ngiving a better visual immersion and improving manipulation speed.\nIncorporating predictive visual and haptic feedback significantly improves\nteleoperation in applications such as search and rescue, inspection, and remote\nmaintenance.\n", "link": "http://arxiv.org/abs/2412.13752v1", "date": "2024-12-18", "relevancy": 2.1768, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5495}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Immersive%20Human-in-the-Loop%20Control%3A%20Real-Time%203D%20Surface%20Meshing%20and%0A%20%20Physics%20Simulation&body=Title%3A%20Immersive%20Human-in-the-Loop%20Control%3A%20Real-Time%203D%20Surface%20Meshing%20and%0A%20%20Physics%20Simulation%0AAuthor%3A%20Sait%20Akturk%20and%20Justin%20Valentine%20and%20Junaid%20Ahmad%20and%20Martin%20Jagersand%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20TactiMesh%20Teleoperator%20Interface%20%28TTI%29%2C%20a%20novel%0Apredictive%20visual%20and%20haptic%20system%20designed%20explicitly%20for%20human-in-the-loop%0Arobot%20control%20using%20a%20head-mounted%20display%20%28HMD%29.%20By%20employing%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29in%20tandem%20with%20a%20space%20carving%20method%20%28CARV%29%2C%0ATTI%20creates%20a%20real%20time%203D%20surface%20mesh%20of%20remote%20environments%20from%20an%20RGB%0Acamera%20mounted%20on%20a%20Barrett%20WAM%20arm.%20The%20generated%20mesh%20is%20integrated%20into%20a%0Aphysics%20simulator%2C%20featuring%20a%20digital%20twin%20of%20the%20WAM%20robot%20arm%20to%20create%20a%0Avirtual%20environment.%20In%20this%20virtual%20environment%2C%20TTI%20provides%20haptic%20feedback%0Adirectly%20in%20response%20to%20the%20operator%27s%20movements%2C%20eliminating%20the%20problem%20with%0Adelayed%20response%20from%20the%20haptic%20follower%20robot.%20Furthermore%2C%20texturing%20the%203D%0Amesh%20with%20keyframes%20from%20SLAM%20allows%20the%20operator%20to%20control%20the%20viewpoint%20of%0Atheir%20Head%20Mounted%20Display%20%28HMD%29%20independently%20of%20the%20arm-mounted%20robot%20camera%2C%0Agiving%20a%20better%20visual%20immersion%20and%20improving%20manipulation%20speed.%0AIncorporating%20predictive%20visual%20and%20haptic%20feedback%20significantly%20improves%0Ateleoperation%20in%20applications%20such%20as%20search%20and%20rescue%2C%20inspection%2C%20and%20remote%0Amaintenance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImmersive%2520Human-in-the-Loop%2520Control%253A%2520Real-Time%25203D%2520Surface%2520Meshing%2520and%250A%2520%2520Physics%2520Simulation%26entry.906535625%3DSait%2520Akturk%2520and%2520Justin%2520Valentine%2520and%2520Junaid%2520Ahmad%2520and%2520Martin%2520Jagersand%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520TactiMesh%2520Teleoperator%2520Interface%2520%2528TTI%2529%252C%2520a%2520novel%250Apredictive%2520visual%2520and%2520haptic%2520system%2520designed%2520explicitly%2520for%2520human-in-the-loop%250Arobot%2520control%2520using%2520a%2520head-mounted%2520display%2520%2528HMD%2529.%2520By%2520employing%2520simultaneous%250Alocalization%2520and%2520mapping%2520%2528SLAM%2529in%2520tandem%2520with%2520a%2520space%2520carving%2520method%2520%2528CARV%2529%252C%250ATTI%2520creates%2520a%2520real%2520time%25203D%2520surface%2520mesh%2520of%2520remote%2520environments%2520from%2520an%2520RGB%250Acamera%2520mounted%2520on%2520a%2520Barrett%2520WAM%2520arm.%2520The%2520generated%2520mesh%2520is%2520integrated%2520into%2520a%250Aphysics%2520simulator%252C%2520featuring%2520a%2520digital%2520twin%2520of%2520the%2520WAM%2520robot%2520arm%2520to%2520create%2520a%250Avirtual%2520environment.%2520In%2520this%2520virtual%2520environment%252C%2520TTI%2520provides%2520haptic%2520feedback%250Adirectly%2520in%2520response%2520to%2520the%2520operator%2527s%2520movements%252C%2520eliminating%2520the%2520problem%2520with%250Adelayed%2520response%2520from%2520the%2520haptic%2520follower%2520robot.%2520Furthermore%252C%2520texturing%2520the%25203D%250Amesh%2520with%2520keyframes%2520from%2520SLAM%2520allows%2520the%2520operator%2520to%2520control%2520the%2520viewpoint%2520of%250Atheir%2520Head%2520Mounted%2520Display%2520%2528HMD%2529%2520independently%2520of%2520the%2520arm-mounted%2520robot%2520camera%252C%250Agiving%2520a%2520better%2520visual%2520immersion%2520and%2520improving%2520manipulation%2520speed.%250AIncorporating%2520predictive%2520visual%2520and%2520haptic%2520feedback%2520significantly%2520improves%250Ateleoperation%2520in%2520applications%2520such%2520as%2520search%2520and%2520rescue%252C%2520inspection%252C%2520and%2520remote%250Amaintenance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Immersive%20Human-in-the-Loop%20Control%3A%20Real-Time%203D%20Surface%20Meshing%20and%0A%20%20Physics%20Simulation&entry.906535625=Sait%20Akturk%20and%20Justin%20Valentine%20and%20Junaid%20Ahmad%20and%20Martin%20Jagersand&entry.1292438233=%20%20This%20paper%20introduces%20the%20TactiMesh%20Teleoperator%20Interface%20%28TTI%29%2C%20a%20novel%0Apredictive%20visual%20and%20haptic%20system%20designed%20explicitly%20for%20human-in-the-loop%0Arobot%20control%20using%20a%20head-mounted%20display%20%28HMD%29.%20By%20employing%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29in%20tandem%20with%20a%20space%20carving%20method%20%28CARV%29%2C%0ATTI%20creates%20a%20real%20time%203D%20surface%20mesh%20of%20remote%20environments%20from%20an%20RGB%0Acamera%20mounted%20on%20a%20Barrett%20WAM%20arm.%20The%20generated%20mesh%20is%20integrated%20into%20a%0Aphysics%20simulator%2C%20featuring%20a%20digital%20twin%20of%20the%20WAM%20robot%20arm%20to%20create%20a%0Avirtual%20environment.%20In%20this%20virtual%20environment%2C%20TTI%20provides%20haptic%20feedback%0Adirectly%20in%20response%20to%20the%20operator%27s%20movements%2C%20eliminating%20the%20problem%20with%0Adelayed%20response%20from%20the%20haptic%20follower%20robot.%20Furthermore%2C%20texturing%20the%203D%0Amesh%20with%20keyframes%20from%20SLAM%20allows%20the%20operator%20to%20control%20the%20viewpoint%20of%0Atheir%20Head%20Mounted%20Display%20%28HMD%29%20independently%20of%20the%20arm-mounted%20robot%20camera%2C%0Agiving%20a%20better%20visual%20immersion%20and%20improving%20manipulation%20speed.%0AIncorporating%20predictive%20visual%20and%20haptic%20feedback%20significantly%20improves%0Ateleoperation%20in%20applications%20such%20as%20search%20and%20rescue%2C%20inspection%2C%20and%20remote%0Amaintenance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13752v1&entry.124074799=Read"},
{"title": "AniDoc: Animation Creation Made Easier", "author": "Yihao Meng and Hao Ouyang and Hanlin Wang and Qiuyu Wang and Wen Wang and Ka Leong Cheng and Zhiheng Liu and Yujun Shen and Huamin Qu", "abstract": "  The production of 2D animation follows an industry-standard workflow,\nencompassing four essential stages: character design, keyframe animation,\nin-betweening, and coloring. Our research focuses on reducing the labor costs\nin the above process by harnessing the potential of increasingly powerful\ngenerative AI. Using video diffusion models as the foundation, AniDoc emerges\nas a video line art colorization tool, which automatically converts sketch\nsequences into colored animations following the reference character\nspecification. Our model exploits correspondence matching as an explicit\nguidance, yielding strong robustness to the variations (e.g., posture) between\nthe reference character and each line art frame. In addition, our model could\neven automate the in-betweening process, such that users can easily create a\ntemporally consistent animation by simply providing a character image as well\nas the start and end sketches. Our code is available at:\nhttps://yihao-meng.github.io/AniDoc_demo.\n", "link": "http://arxiv.org/abs/2412.14173v1", "date": "2024-12-18", "relevancy": 2.1688, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5759}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5214}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AniDoc%3A%20Animation%20Creation%20Made%20Easier&body=Title%3A%20AniDoc%3A%20Animation%20Creation%20Made%20Easier%0AAuthor%3A%20Yihao%20Meng%20and%20Hao%20Ouyang%20and%20Hanlin%20Wang%20and%20Qiuyu%20Wang%20and%20Wen%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Zhiheng%20Liu%20and%20Yujun%20Shen%20and%20Huamin%20Qu%0AAbstract%3A%20%20%20The%20production%20of%202D%20animation%20follows%20an%20industry-standard%20workflow%2C%0Aencompassing%20four%20essential%20stages%3A%20character%20design%2C%20keyframe%20animation%2C%0Ain-betweening%2C%20and%20coloring.%20Our%20research%20focuses%20on%20reducing%20the%20labor%20costs%0Ain%20the%20above%20process%20by%20harnessing%20the%20potential%20of%20increasingly%20powerful%0Agenerative%20AI.%20Using%20video%20diffusion%20models%20as%20the%20foundation%2C%20AniDoc%20emerges%0Aas%20a%20video%20line%20art%20colorization%20tool%2C%20which%20automatically%20converts%20sketch%0Asequences%20into%20colored%20animations%20following%20the%20reference%20character%0Aspecification.%20Our%20model%20exploits%20correspondence%20matching%20as%20an%20explicit%0Aguidance%2C%20yielding%20strong%20robustness%20to%20the%20variations%20%28e.g.%2C%20posture%29%20between%0Athe%20reference%20character%20and%20each%20line%20art%20frame.%20In%20addition%2C%20our%20model%20could%0Aeven%20automate%20the%20in-betweening%20process%2C%20such%20that%20users%20can%20easily%20create%20a%0Atemporally%20consistent%20animation%20by%20simply%20providing%20a%20character%20image%20as%20well%0Aas%20the%20start%20and%20end%20sketches.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//yihao-meng.github.io/AniDoc_demo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14173v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAniDoc%253A%2520Animation%2520Creation%2520Made%2520Easier%26entry.906535625%3DYihao%2520Meng%2520and%2520Hao%2520Ouyang%2520and%2520Hanlin%2520Wang%2520and%2520Qiuyu%2520Wang%2520and%2520Wen%2520Wang%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Zhiheng%2520Liu%2520and%2520Yujun%2520Shen%2520and%2520Huamin%2520Qu%26entry.1292438233%3D%2520%2520The%2520production%2520of%25202D%2520animation%2520follows%2520an%2520industry-standard%2520workflow%252C%250Aencompassing%2520four%2520essential%2520stages%253A%2520character%2520design%252C%2520keyframe%2520animation%252C%250Ain-betweening%252C%2520and%2520coloring.%2520Our%2520research%2520focuses%2520on%2520reducing%2520the%2520labor%2520costs%250Ain%2520the%2520above%2520process%2520by%2520harnessing%2520the%2520potential%2520of%2520increasingly%2520powerful%250Agenerative%2520AI.%2520Using%2520video%2520diffusion%2520models%2520as%2520the%2520foundation%252C%2520AniDoc%2520emerges%250Aas%2520a%2520video%2520line%2520art%2520colorization%2520tool%252C%2520which%2520automatically%2520converts%2520sketch%250Asequences%2520into%2520colored%2520animations%2520following%2520the%2520reference%2520character%250Aspecification.%2520Our%2520model%2520exploits%2520correspondence%2520matching%2520as%2520an%2520explicit%250Aguidance%252C%2520yielding%2520strong%2520robustness%2520to%2520the%2520variations%2520%2528e.g.%252C%2520posture%2529%2520between%250Athe%2520reference%2520character%2520and%2520each%2520line%2520art%2520frame.%2520In%2520addition%252C%2520our%2520model%2520could%250Aeven%2520automate%2520the%2520in-betweening%2520process%252C%2520such%2520that%2520users%2520can%2520easily%2520create%2520a%250Atemporally%2520consistent%2520animation%2520by%2520simply%2520providing%2520a%2520character%2520image%2520as%2520well%250Aas%2520the%2520start%2520and%2520end%2520sketches.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//yihao-meng.github.io/AniDoc_demo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14173v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniDoc%3A%20Animation%20Creation%20Made%20Easier&entry.906535625=Yihao%20Meng%20and%20Hao%20Ouyang%20and%20Hanlin%20Wang%20and%20Qiuyu%20Wang%20and%20Wen%20Wang%20and%20Ka%20Leong%20Cheng%20and%20Zhiheng%20Liu%20and%20Yujun%20Shen%20and%20Huamin%20Qu&entry.1292438233=%20%20The%20production%20of%202D%20animation%20follows%20an%20industry-standard%20workflow%2C%0Aencompassing%20four%20essential%20stages%3A%20character%20design%2C%20keyframe%20animation%2C%0Ain-betweening%2C%20and%20coloring.%20Our%20research%20focuses%20on%20reducing%20the%20labor%20costs%0Ain%20the%20above%20process%20by%20harnessing%20the%20potential%20of%20increasingly%20powerful%0Agenerative%20AI.%20Using%20video%20diffusion%20models%20as%20the%20foundation%2C%20AniDoc%20emerges%0Aas%20a%20video%20line%20art%20colorization%20tool%2C%20which%20automatically%20converts%20sketch%0Asequences%20into%20colored%20animations%20following%20the%20reference%20character%0Aspecification.%20Our%20model%20exploits%20correspondence%20matching%20as%20an%20explicit%0Aguidance%2C%20yielding%20strong%20robustness%20to%20the%20variations%20%28e.g.%2C%20posture%29%20between%0Athe%20reference%20character%20and%20each%20line%20art%20frame.%20In%20addition%2C%20our%20model%20could%0Aeven%20automate%20the%20in-betweening%20process%2C%20such%20that%20users%20can%20easily%20create%20a%0Atemporally%20consistent%20animation%20by%20simply%20providing%20a%20character%20image%20as%20well%0Aas%20the%20start%20and%20end%20sketches.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//yihao-meng.github.io/AniDoc_demo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14173v1&entry.124074799=Read"},
{"title": "Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe", "author": "Purba Mukherjee and Anjan A. Sen", "abstract": "  Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results.\n", "link": "http://arxiv.org/abs/2412.13973v1", "date": "2024-12-18", "relevancy": 2.1688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4354}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Agnostic%20Cosmological%20Inference%20with%20SDSS-IV%20eBOSS%3A%20Simultaneous%0A%20%20Probing%20for%20Background%20and%20Perturbed%20Universe&body=Title%3A%20Model-Agnostic%20Cosmological%20Inference%20with%20SDSS-IV%20eBOSS%3A%20Simultaneous%0A%20%20Probing%20for%20Background%20and%20Perturbed%20Universe%0AAuthor%3A%20Purba%20Mukherjee%20and%20Anjan%20A.%20Sen%0AAbstract%3A%20%20%20Here%20we%20explore%20certain%20subtle%20features%20imprinted%20in%20data%20from%20the%20completed%0ASloan%20Digital%20Sky%20Survey%20IV%20%28SDSS-IV%29%20extended%20Baryon%20Oscillation%20Spectroscopic%0ASurvey%20%28eBOSS%29%20as%20a%20combined%20probe%20for%20the%20background%20and%20perturbed%20Universe.%0AWe%20reconstruct%20the%20baryon%20Acoustic%20Oscillation%20%28BAO%29%20and%20Redshift%20Space%0ADistortion%20%28RSD%29%20observables%20as%20functions%20of%20redshift%2C%20using%20measurements%20from%0ASDSS%20alone.%20We%20apply%20the%20Multi-Task%20Gaussian%20Process%20%28MTGP%29%20framework%20to%20model%0Athe%20interdependencies%20of%20cosmological%20observables%20%24D_M%28z%29/r_d%24%2C%20%24D_H%28z%29/r_d%24%2C%0Aand%20%24f%5Csigma_8%28z%29%24%2C%20and%20track%20their%20evolution%20across%20different%20redshifts.%0ASubsequently%2C%20we%20obtain%20constrained%20three-dimensional%20phase%20space%20containing%0A%24D_M%28z%29/r_d%24%2C%20%24D_H%28z%29/r_d%24%2C%20and%20%24f%5Csigma_8%28z%29%24%20at%20different%20redshifts%20probed%20by%0Athe%20SDSS-IV%20eBOSS%20survey.%20Furthermore%2C%20assuming%20the%20%24%5CLambda%24CDM%20model%2C%20we%0Aobtain%20constraints%20on%20model%20parameters%20%24%5COmega_%7Bm%7D%24%2C%20%24H_%7B0%7Dr_%7Bd%7D%24%2C%20%24%5Csigma_%7B8%7D%24%0Aand%20%24S_%7B8%7D%24%20at%20each%20redshift%20probed%20by%20SDSS-IV%20eBOSS.%20This%20indicates%0Aredshift-dependent%20trends%20in%20%24H_0%24%2C%20%24%5COmega_m%24%2C%20%24%5Csigma_8%24%20and%20%24S_8%24%20in%20the%0A%24%5CLambda%24CDM%20model%2C%20suggesting%20a%20possible%20inconsistency%20in%20the%20%24%5CLambda%24CDM%0Amodel.%20Ours%20is%20a%20template%20for%20model-independent%20extraction%20of%20information%20for%0Aboth%20background%20and%20perturbed%20Universe%20using%20a%20single%20galaxy%20survey%20taking%20into%0Aaccount%20all%20the%20existing%20correlations%20between%20background%20and%20perturbed%0Aobservables%20and%20this%20can%20be%20easily%20extended%20to%20future%20DESI-3YR%20as%20well%20as%0AEuclid%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Agnostic%2520Cosmological%2520Inference%2520with%2520SDSS-IV%2520eBOSS%253A%2520Simultaneous%250A%2520%2520Probing%2520for%2520Background%2520and%2520Perturbed%2520Universe%26entry.906535625%3DPurba%2520Mukherjee%2520and%2520Anjan%2520A.%2520Sen%26entry.1292438233%3D%2520%2520Here%2520we%2520explore%2520certain%2520subtle%2520features%2520imprinted%2520in%2520data%2520from%2520the%2520completed%250ASloan%2520Digital%2520Sky%2520Survey%2520IV%2520%2528SDSS-IV%2529%2520extended%2520Baryon%2520Oscillation%2520Spectroscopic%250ASurvey%2520%2528eBOSS%2529%2520as%2520a%2520combined%2520probe%2520for%2520the%2520background%2520and%2520perturbed%2520Universe.%250AWe%2520reconstruct%2520the%2520baryon%2520Acoustic%2520Oscillation%2520%2528BAO%2529%2520and%2520Redshift%2520Space%250ADistortion%2520%2528RSD%2529%2520observables%2520as%2520functions%2520of%2520redshift%252C%2520using%2520measurements%2520from%250ASDSS%2520alone.%2520We%2520apply%2520the%2520Multi-Task%2520Gaussian%2520Process%2520%2528MTGP%2529%2520framework%2520to%2520model%250Athe%2520interdependencies%2520of%2520cosmological%2520observables%2520%2524D_M%2528z%2529/r_d%2524%252C%2520%2524D_H%2528z%2529/r_d%2524%252C%250Aand%2520%2524f%255Csigma_8%2528z%2529%2524%252C%2520and%2520track%2520their%2520evolution%2520across%2520different%2520redshifts.%250ASubsequently%252C%2520we%2520obtain%2520constrained%2520three-dimensional%2520phase%2520space%2520containing%250A%2524D_M%2528z%2529/r_d%2524%252C%2520%2524D_H%2528z%2529/r_d%2524%252C%2520and%2520%2524f%255Csigma_8%2528z%2529%2524%2520at%2520different%2520redshifts%2520probed%2520by%250Athe%2520SDSS-IV%2520eBOSS%2520survey.%2520Furthermore%252C%2520assuming%2520the%2520%2524%255CLambda%2524CDM%2520model%252C%2520we%250Aobtain%2520constraints%2520on%2520model%2520parameters%2520%2524%255COmega_%257Bm%257D%2524%252C%2520%2524H_%257B0%257Dr_%257Bd%257D%2524%252C%2520%2524%255Csigma_%257B8%257D%2524%250Aand%2520%2524S_%257B8%257D%2524%2520at%2520each%2520redshift%2520probed%2520by%2520SDSS-IV%2520eBOSS.%2520This%2520indicates%250Aredshift-dependent%2520trends%2520in%2520%2524H_0%2524%252C%2520%2524%255COmega_m%2524%252C%2520%2524%255Csigma_8%2524%2520and%2520%2524S_8%2524%2520in%2520the%250A%2524%255CLambda%2524CDM%2520model%252C%2520suggesting%2520a%2520possible%2520inconsistency%2520in%2520the%2520%2524%255CLambda%2524CDM%250Amodel.%2520Ours%2520is%2520a%2520template%2520for%2520model-independent%2520extraction%2520of%2520information%2520for%250Aboth%2520background%2520and%2520perturbed%2520Universe%2520using%2520a%2520single%2520galaxy%2520survey%2520taking%2520into%250Aaccount%2520all%2520the%2520existing%2520correlations%2520between%2520background%2520and%2520perturbed%250Aobservables%2520and%2520this%2520can%2520be%2520easily%2520extended%2520to%2520future%2520DESI-3YR%2520as%2520well%2520as%250AEuclid%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Agnostic%20Cosmological%20Inference%20with%20SDSS-IV%20eBOSS%3A%20Simultaneous%0A%20%20Probing%20for%20Background%20and%20Perturbed%20Universe&entry.906535625=Purba%20Mukherjee%20and%20Anjan%20A.%20Sen&entry.1292438233=%20%20Here%20we%20explore%20certain%20subtle%20features%20imprinted%20in%20data%20from%20the%20completed%0ASloan%20Digital%20Sky%20Survey%20IV%20%28SDSS-IV%29%20extended%20Baryon%20Oscillation%20Spectroscopic%0ASurvey%20%28eBOSS%29%20as%20a%20combined%20probe%20for%20the%20background%20and%20perturbed%20Universe.%0AWe%20reconstruct%20the%20baryon%20Acoustic%20Oscillation%20%28BAO%29%20and%20Redshift%20Space%0ADistortion%20%28RSD%29%20observables%20as%20functions%20of%20redshift%2C%20using%20measurements%20from%0ASDSS%20alone.%20We%20apply%20the%20Multi-Task%20Gaussian%20Process%20%28MTGP%29%20framework%20to%20model%0Athe%20interdependencies%20of%20cosmological%20observables%20%24D_M%28z%29/r_d%24%2C%20%24D_H%28z%29/r_d%24%2C%0Aand%20%24f%5Csigma_8%28z%29%24%2C%20and%20track%20their%20evolution%20across%20different%20redshifts.%0ASubsequently%2C%20we%20obtain%20constrained%20three-dimensional%20phase%20space%20containing%0A%24D_M%28z%29/r_d%24%2C%20%24D_H%28z%29/r_d%24%2C%20and%20%24f%5Csigma_8%28z%29%24%20at%20different%20redshifts%20probed%20by%0Athe%20SDSS-IV%20eBOSS%20survey.%20Furthermore%2C%20assuming%20the%20%24%5CLambda%24CDM%20model%2C%20we%0Aobtain%20constraints%20on%20model%20parameters%20%24%5COmega_%7Bm%7D%24%2C%20%24H_%7B0%7Dr_%7Bd%7D%24%2C%20%24%5Csigma_%7B8%7D%24%0Aand%20%24S_%7B8%7D%24%20at%20each%20redshift%20probed%20by%20SDSS-IV%20eBOSS.%20This%20indicates%0Aredshift-dependent%20trends%20in%20%24H_0%24%2C%20%24%5COmega_m%24%2C%20%24%5Csigma_8%24%20and%20%24S_8%24%20in%20the%0A%24%5CLambda%24CDM%20model%2C%20suggesting%20a%20possible%20inconsistency%20in%20the%20%24%5CLambda%24CDM%0Amodel.%20Ours%20is%20a%20template%20for%20model-independent%20extraction%20of%20information%20for%0Aboth%20background%20and%20perturbed%20Universe%20using%20a%20single%20galaxy%20survey%20taking%20into%0Aaccount%20all%20the%20existing%20correlations%20between%20background%20and%20perturbed%0Aobservables%20and%20this%20can%20be%20easily%20extended%20to%20future%20DESI-3YR%20as%20well%20as%0AEuclid%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13973v1&entry.124074799=Read"},
{"title": "Sharing Key Semantics in Transformer Makes Efficient Image Restoration", "author": "Bin Ren and Yawei Li and Jingyun Liang and Rakesh Ranjan and Mengyuan Liu and Rita Cucchiara and Luc Van Gool and Ming-Hsuan Yang and Nicu Sebe", "abstract": "  Image Restoration (IR), a classic low-level vision task, has witnessed\nsignificant advancements through deep models that effectively model global\ninformation. Notably, the emergence of Vision Transformers (ViTs) has further\npropelled these advancements. When computing, the self-attention mechanism, a\ncornerstone of ViTs, tends to encompass all global cues, even those from\nsemantically unrelated objects or regions. This inclusivity introduces\ncomputational inefficiencies, particularly noticeable with high input\nresolution, as it requires processing irrelevant information, thereby impeding\nefficiency. Additionally, for IR, it is commonly noted that small segments of a\ndegraded image, particularly those closely aligned semantically, provide\nparticularly relevant information to aid in the restoration process, as they\ncontribute essential contextual cues crucial for accurate reconstruction. To\naddress these challenges, we propose boosting IR's performance by sharing the\nkey semantics via Transformer for IR (\\ie, SemanIR) in this paper.\nSpecifically, SemanIR initially constructs a sparse yet comprehensive\nkey-semantic dictionary within each transformer stage by establishing essential\nsemantic connections for every degraded patch. Subsequently, this dictionary is\nshared across all subsequent transformer blocks within the same stage. This\nstrategy optimizes attention calculation within each block by focusing\nexclusively on semantically related components stored in the key-semantic\ndictionary. As a result, attention calculation achieves linear computational\ncomplexity within each window. Extensive experiments across 6 IR tasks confirm\nthe proposed SemanIR's state-of-the-art performance, quantitatively and\nqualitatively showcasing advancements. The visual results, code, and trained\nmodels are available at https://github.com/Amazingren/SemanIR.\n", "link": "http://arxiv.org/abs/2405.20008v2", "date": "2024-12-18", "relevancy": 2.1441, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5847}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5283}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharing%20Key%20Semantics%20in%20Transformer%20Makes%20Efficient%20Image%20Restoration&body=Title%3A%20Sharing%20Key%20Semantics%20in%20Transformer%20Makes%20Efficient%20Image%20Restoration%0AAuthor%3A%20Bin%20Ren%20and%20Yawei%20Li%20and%20Jingyun%20Liang%20and%20Rakesh%20Ranjan%20and%20Mengyuan%20Liu%20and%20Rita%20Cucchiara%20and%20Luc%20Van%20Gool%20and%20Ming-Hsuan%20Yang%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Image%20Restoration%20%28IR%29%2C%20a%20classic%20low-level%20vision%20task%2C%20has%20witnessed%0Asignificant%20advancements%20through%20deep%20models%20that%20effectively%20model%20global%0Ainformation.%20Notably%2C%20the%20emergence%20of%20Vision%20Transformers%20%28ViTs%29%20has%20further%0Apropelled%20these%20advancements.%20When%20computing%2C%20the%20self-attention%20mechanism%2C%20a%0Acornerstone%20of%20ViTs%2C%20tends%20to%20encompass%20all%20global%20cues%2C%20even%20those%20from%0Asemantically%20unrelated%20objects%20or%20regions.%20This%20inclusivity%20introduces%0Acomputational%20inefficiencies%2C%20particularly%20noticeable%20with%20high%20input%0Aresolution%2C%20as%20it%20requires%20processing%20irrelevant%20information%2C%20thereby%20impeding%0Aefficiency.%20Additionally%2C%20for%20IR%2C%20it%20is%20commonly%20noted%20that%20small%20segments%20of%20a%0Adegraded%20image%2C%20particularly%20those%20closely%20aligned%20semantically%2C%20provide%0Aparticularly%20relevant%20information%20to%20aid%20in%20the%20restoration%20process%2C%20as%20they%0Acontribute%20essential%20contextual%20cues%20crucial%20for%20accurate%20reconstruction.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20boosting%20IR%27s%20performance%20by%20sharing%20the%0Akey%20semantics%20via%20Transformer%20for%20IR%20%28%5Cie%2C%20SemanIR%29%20in%20this%20paper.%0ASpecifically%2C%20SemanIR%20initially%20constructs%20a%20sparse%20yet%20comprehensive%0Akey-semantic%20dictionary%20within%20each%20transformer%20stage%20by%20establishing%20essential%0Asemantic%20connections%20for%20every%20degraded%20patch.%20Subsequently%2C%20this%20dictionary%20is%0Ashared%20across%20all%20subsequent%20transformer%20blocks%20within%20the%20same%20stage.%20This%0Astrategy%20optimizes%20attention%20calculation%20within%20each%20block%20by%20focusing%0Aexclusively%20on%20semantically%20related%20components%20stored%20in%20the%20key-semantic%0Adictionary.%20As%20a%20result%2C%20attention%20calculation%20achieves%20linear%20computational%0Acomplexity%20within%20each%20window.%20Extensive%20experiments%20across%206%20IR%20tasks%20confirm%0Athe%20proposed%20SemanIR%27s%20state-of-the-art%20performance%2C%20quantitatively%20and%0Aqualitatively%20showcasing%20advancements.%20The%20visual%20results%2C%20code%2C%20and%20trained%0Amodels%20are%20available%20at%20https%3A//github.com/Amazingren/SemanIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharing%2520Key%2520Semantics%2520in%2520Transformer%2520Makes%2520Efficient%2520Image%2520Restoration%26entry.906535625%3DBin%2520Ren%2520and%2520Yawei%2520Li%2520and%2520Jingyun%2520Liang%2520and%2520Rakesh%2520Ranjan%2520and%2520Mengyuan%2520Liu%2520and%2520Rita%2520Cucchiara%2520and%2520Luc%2520Van%2520Gool%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Image%2520Restoration%2520%2528IR%2529%252C%2520a%2520classic%2520low-level%2520vision%2520task%252C%2520has%2520witnessed%250Asignificant%2520advancements%2520through%2520deep%2520models%2520that%2520effectively%2520model%2520global%250Ainformation.%2520Notably%252C%2520the%2520emergence%2520of%2520Vision%2520Transformers%2520%2528ViTs%2529%2520has%2520further%250Apropelled%2520these%2520advancements.%2520When%2520computing%252C%2520the%2520self-attention%2520mechanism%252C%2520a%250Acornerstone%2520of%2520ViTs%252C%2520tends%2520to%2520encompass%2520all%2520global%2520cues%252C%2520even%2520those%2520from%250Asemantically%2520unrelated%2520objects%2520or%2520regions.%2520This%2520inclusivity%2520introduces%250Acomputational%2520inefficiencies%252C%2520particularly%2520noticeable%2520with%2520high%2520input%250Aresolution%252C%2520as%2520it%2520requires%2520processing%2520irrelevant%2520information%252C%2520thereby%2520impeding%250Aefficiency.%2520Additionally%252C%2520for%2520IR%252C%2520it%2520is%2520commonly%2520noted%2520that%2520small%2520segments%2520of%2520a%250Adegraded%2520image%252C%2520particularly%2520those%2520closely%2520aligned%2520semantically%252C%2520provide%250Aparticularly%2520relevant%2520information%2520to%2520aid%2520in%2520the%2520restoration%2520process%252C%2520as%2520they%250Acontribute%2520essential%2520contextual%2520cues%2520crucial%2520for%2520accurate%2520reconstruction.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520boosting%2520IR%2527s%2520performance%2520by%2520sharing%2520the%250Akey%2520semantics%2520via%2520Transformer%2520for%2520IR%2520%2528%255Cie%252C%2520SemanIR%2529%2520in%2520this%2520paper.%250ASpecifically%252C%2520SemanIR%2520initially%2520constructs%2520a%2520sparse%2520yet%2520comprehensive%250Akey-semantic%2520dictionary%2520within%2520each%2520transformer%2520stage%2520by%2520establishing%2520essential%250Asemantic%2520connections%2520for%2520every%2520degraded%2520patch.%2520Subsequently%252C%2520this%2520dictionary%2520is%250Ashared%2520across%2520all%2520subsequent%2520transformer%2520blocks%2520within%2520the%2520same%2520stage.%2520This%250Astrategy%2520optimizes%2520attention%2520calculation%2520within%2520each%2520block%2520by%2520focusing%250Aexclusively%2520on%2520semantically%2520related%2520components%2520stored%2520in%2520the%2520key-semantic%250Adictionary.%2520As%2520a%2520result%252C%2520attention%2520calculation%2520achieves%2520linear%2520computational%250Acomplexity%2520within%2520each%2520window.%2520Extensive%2520experiments%2520across%25206%2520IR%2520tasks%2520confirm%250Athe%2520proposed%2520SemanIR%2527s%2520state-of-the-art%2520performance%252C%2520quantitatively%2520and%250Aqualitatively%2520showcasing%2520advancements.%2520The%2520visual%2520results%252C%2520code%252C%2520and%2520trained%250Amodels%2520are%2520available%2520at%2520https%253A//github.com/Amazingren/SemanIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharing%20Key%20Semantics%20in%20Transformer%20Makes%20Efficient%20Image%20Restoration&entry.906535625=Bin%20Ren%20and%20Yawei%20Li%20and%20Jingyun%20Liang%20and%20Rakesh%20Ranjan%20and%20Mengyuan%20Liu%20and%20Rita%20Cucchiara%20and%20Luc%20Van%20Gool%20and%20Ming-Hsuan%20Yang%20and%20Nicu%20Sebe&entry.1292438233=%20%20Image%20Restoration%20%28IR%29%2C%20a%20classic%20low-level%20vision%20task%2C%20has%20witnessed%0Asignificant%20advancements%20through%20deep%20models%20that%20effectively%20model%20global%0Ainformation.%20Notably%2C%20the%20emergence%20of%20Vision%20Transformers%20%28ViTs%29%20has%20further%0Apropelled%20these%20advancements.%20When%20computing%2C%20the%20self-attention%20mechanism%2C%20a%0Acornerstone%20of%20ViTs%2C%20tends%20to%20encompass%20all%20global%20cues%2C%20even%20those%20from%0Asemantically%20unrelated%20objects%20or%20regions.%20This%20inclusivity%20introduces%0Acomputational%20inefficiencies%2C%20particularly%20noticeable%20with%20high%20input%0Aresolution%2C%20as%20it%20requires%20processing%20irrelevant%20information%2C%20thereby%20impeding%0Aefficiency.%20Additionally%2C%20for%20IR%2C%20it%20is%20commonly%20noted%20that%20small%20segments%20of%20a%0Adegraded%20image%2C%20particularly%20those%20closely%20aligned%20semantically%2C%20provide%0Aparticularly%20relevant%20information%20to%20aid%20in%20the%20restoration%20process%2C%20as%20they%0Acontribute%20essential%20contextual%20cues%20crucial%20for%20accurate%20reconstruction.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20boosting%20IR%27s%20performance%20by%20sharing%20the%0Akey%20semantics%20via%20Transformer%20for%20IR%20%28%5Cie%2C%20SemanIR%29%20in%20this%20paper.%0ASpecifically%2C%20SemanIR%20initially%20constructs%20a%20sparse%20yet%20comprehensive%0Akey-semantic%20dictionary%20within%20each%20transformer%20stage%20by%20establishing%20essential%0Asemantic%20connections%20for%20every%20degraded%20patch.%20Subsequently%2C%20this%20dictionary%20is%0Ashared%20across%20all%20subsequent%20transformer%20blocks%20within%20the%20same%20stage.%20This%0Astrategy%20optimizes%20attention%20calculation%20within%20each%20block%20by%20focusing%0Aexclusively%20on%20semantically%20related%20components%20stored%20in%20the%20key-semantic%0Adictionary.%20As%20a%20result%2C%20attention%20calculation%20achieves%20linear%20computational%0Acomplexity%20within%20each%20window.%20Extensive%20experiments%20across%206%20IR%20tasks%20confirm%0Athe%20proposed%20SemanIR%27s%20state-of-the-art%20performance%2C%20quantitatively%20and%0Aqualitatively%20showcasing%20advancements.%20The%20visual%20results%2C%20code%2C%20and%20trained%0Amodels%20are%20available%20at%20https%3A//github.com/Amazingren/SemanIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20008v2&entry.124074799=Read"},
{"title": "To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation", "author": "Abdul Hameed Azeemi and Ihsan Ayyub Qazi and Agha Ali Raza", "abstract": "  Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose Hybrid Uncertainty and Diversity Sampling (HUDS), an AL strategy for\ndomain adaptation in NMT that combines uncertainty and diversity for sentence\nselection. HUDS computes uncertainty scores for unlabeled sentences and\nsubsequently stratifies them. It then clusters sentence embeddings within each\nstratum and computes diversity scores by distance to the centroid. A weighted\nhybrid score that combines uncertainty and diversity is then used to select the\ntop instances for annotation in each AL iteration. Experiments on multi-domain\nGerman-English and French-English datasets demonstrate the better performance\nof HUDS over other strong AL baselines. We analyze the sentence selection with\nHUDS and show that it prioritizes diverse instances having high model\nuncertainty for annotation in early AL iterations.\n", "link": "http://arxiv.org/abs/2403.09259v2", "date": "2024-12-18", "relevancy": 2.1412, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5256}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation&body=Title%3A%20To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation%0AAuthor%3A%20Abdul%20Hameed%20Azeemi%20and%20Ihsan%20Ayyub%20Qazi%20and%20Agha%20Ali%20Raza%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20techniques%20reduce%20labeling%20costs%20for%20training%20neural%0Amachine%20translation%20%28NMT%29%20models%20by%20selecting%20smaller%20representative%20subsets%0Afrom%20unlabeled%20data%20for%20annotation.%20Diversity%20sampling%20techniques%20select%0Aheterogeneous%20instances%2C%20while%20uncertainty%20sampling%20methods%20select%20instances%0Awith%20the%20highest%20model%20uncertainty.%20Both%20approaches%20have%20limitations%20-%0Adiversity%20methods%20may%20extract%20varied%20but%20trivial%20examples%2C%20while%20uncertainty%0Asampling%20can%20yield%20repetitive%2C%20uninformative%20instances.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20Hybrid%20Uncertainty%20and%20Diversity%20Sampling%20%28HUDS%29%2C%20an%20AL%20strategy%20for%0Adomain%20adaptation%20in%20NMT%20that%20combines%20uncertainty%20and%20diversity%20for%20sentence%0Aselection.%20HUDS%20computes%20uncertainty%20scores%20for%20unlabeled%20sentences%20and%0Asubsequently%20stratifies%20them.%20It%20then%20clusters%20sentence%20embeddings%20within%20each%0Astratum%20and%20computes%20diversity%20scores%20by%20distance%20to%20the%20centroid.%20A%20weighted%0Ahybrid%20score%20that%20combines%20uncertainty%20and%20diversity%20is%20then%20used%20to%20select%20the%0Atop%20instances%20for%20annotation%20in%20each%20AL%20iteration.%20Experiments%20on%20multi-domain%0AGerman-English%20and%20French-English%20datasets%20demonstrate%20the%20better%20performance%0Aof%20HUDS%20over%20other%20strong%20AL%20baselines.%20We%20analyze%20the%20sentence%20selection%20with%0AHUDS%20and%20show%20that%20it%20prioritizes%20diverse%20instances%20having%20high%20model%0Auncertainty%20for%20annotation%20in%20early%20AL%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09259v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTo%2520Label%2520or%2520Not%2520to%2520Label%253A%2520Hybrid%2520Active%2520Learning%2520for%2520Neural%2520Machine%250A%2520%2520Translation%26entry.906535625%3DAbdul%2520Hameed%2520Azeemi%2520and%2520Ihsan%2520Ayyub%2520Qazi%2520and%2520Agha%2520Ali%2520Raza%26entry.1292438233%3D%2520%2520Active%2520learning%2520%2528AL%2529%2520techniques%2520reduce%2520labeling%2520costs%2520for%2520training%2520neural%250Amachine%2520translation%2520%2528NMT%2529%2520models%2520by%2520selecting%2520smaller%2520representative%2520subsets%250Afrom%2520unlabeled%2520data%2520for%2520annotation.%2520Diversity%2520sampling%2520techniques%2520select%250Aheterogeneous%2520instances%252C%2520while%2520uncertainty%2520sampling%2520methods%2520select%2520instances%250Awith%2520the%2520highest%2520model%2520uncertainty.%2520Both%2520approaches%2520have%2520limitations%2520-%250Adiversity%2520methods%2520may%2520extract%2520varied%2520but%2520trivial%2520examples%252C%2520while%2520uncertainty%250Asampling%2520can%2520yield%2520repetitive%252C%2520uninformative%2520instances.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Apropose%2520Hybrid%2520Uncertainty%2520and%2520Diversity%2520Sampling%2520%2528HUDS%2529%252C%2520an%2520AL%2520strategy%2520for%250Adomain%2520adaptation%2520in%2520NMT%2520that%2520combines%2520uncertainty%2520and%2520diversity%2520for%2520sentence%250Aselection.%2520HUDS%2520computes%2520uncertainty%2520scores%2520for%2520unlabeled%2520sentences%2520and%250Asubsequently%2520stratifies%2520them.%2520It%2520then%2520clusters%2520sentence%2520embeddings%2520within%2520each%250Astratum%2520and%2520computes%2520diversity%2520scores%2520by%2520distance%2520to%2520the%2520centroid.%2520A%2520weighted%250Ahybrid%2520score%2520that%2520combines%2520uncertainty%2520and%2520diversity%2520is%2520then%2520used%2520to%2520select%2520the%250Atop%2520instances%2520for%2520annotation%2520in%2520each%2520AL%2520iteration.%2520Experiments%2520on%2520multi-domain%250AGerman-English%2520and%2520French-English%2520datasets%2520demonstrate%2520the%2520better%2520performance%250Aof%2520HUDS%2520over%2520other%2520strong%2520AL%2520baselines.%2520We%2520analyze%2520the%2520sentence%2520selection%2520with%250AHUDS%2520and%2520show%2520that%2520it%2520prioritizes%2520diverse%2520instances%2520having%2520high%2520model%250Auncertainty%2520for%2520annotation%2520in%2520early%2520AL%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09259v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=To%20Label%20or%20Not%20to%20Label%3A%20Hybrid%20Active%20Learning%20for%20Neural%20Machine%0A%20%20Translation&entry.906535625=Abdul%20Hameed%20Azeemi%20and%20Ihsan%20Ayyub%20Qazi%20and%20Agha%20Ali%20Raza&entry.1292438233=%20%20Active%20learning%20%28AL%29%20techniques%20reduce%20labeling%20costs%20for%20training%20neural%0Amachine%20translation%20%28NMT%29%20models%20by%20selecting%20smaller%20representative%20subsets%0Afrom%20unlabeled%20data%20for%20annotation.%20Diversity%20sampling%20techniques%20select%0Aheterogeneous%20instances%2C%20while%20uncertainty%20sampling%20methods%20select%20instances%0Awith%20the%20highest%20model%20uncertainty.%20Both%20approaches%20have%20limitations%20-%0Adiversity%20methods%20may%20extract%20varied%20but%20trivial%20examples%2C%20while%20uncertainty%0Asampling%20can%20yield%20repetitive%2C%20uninformative%20instances.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20Hybrid%20Uncertainty%20and%20Diversity%20Sampling%20%28HUDS%29%2C%20an%20AL%20strategy%20for%0Adomain%20adaptation%20in%20NMT%20that%20combines%20uncertainty%20and%20diversity%20for%20sentence%0Aselection.%20HUDS%20computes%20uncertainty%20scores%20for%20unlabeled%20sentences%20and%0Asubsequently%20stratifies%20them.%20It%20then%20clusters%20sentence%20embeddings%20within%20each%0Astratum%20and%20computes%20diversity%20scores%20by%20distance%20to%20the%20centroid.%20A%20weighted%0Ahybrid%20score%20that%20combines%20uncertainty%20and%20diversity%20is%20then%20used%20to%20select%20the%0Atop%20instances%20for%20annotation%20in%20each%20AL%20iteration.%20Experiments%20on%20multi-domain%0AGerman-English%20and%20French-English%20datasets%20demonstrate%20the%20better%20performance%0Aof%20HUDS%20over%20other%20strong%20AL%20baselines.%20We%20analyze%20the%20sentence%20selection%20with%0AHUDS%20and%20show%20that%20it%20prioritizes%20diverse%20instances%20having%20high%20model%0Auncertainty%20for%20annotation%20in%20early%20AL%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09259v2&entry.124074799=Read"},
{"title": "On the Compression of Language Models for Code: An Empirical Study on\n  CodeBERT", "author": "Giordano d'Aloisio and Luca Traini and Federica Sarro and Antinisca Di Marco", "abstract": "  Language models have proven successful across a wide range of software\nengineering tasks, but their significant computational costs often hinder their\npractical adoption. To address this challenge, researchers have begun applying\nvarious compression strategies to improve the efficiency of language models for\ncode. These strategies aim to optimize inference latency and memory usage,\nthough often at the cost of reduced model effectiveness. However, there is\nstill a significant gap in understanding how these strategies influence the\nefficiency and effectiveness of language models for code. Here, we empirically\ninvestigate the impact of three well-known compression strategies -- knowledge\ndistillation, quantization, and pruning -- across three different classes of\nsoftware engineering tasks: vulnerability detection, code summarization, and\ncode search. Our findings reveal that the impact of these strategies varies\ngreatly depending on the task and the specific compression method employed.\nPractitioners and researchers can use these insights to make informed decisions\nwhen selecting the most appropriate compression strategy, balancing both\nefficiency and effectiveness based on their specific needs.\n", "link": "http://arxiv.org/abs/2412.13737v1", "date": "2024-12-18", "relevancy": 2.1361, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Compression%20of%20Language%20Models%20for%20Code%3A%20An%20Empirical%20Study%20on%0A%20%20CodeBERT&body=Title%3A%20On%20the%20Compression%20of%20Language%20Models%20for%20Code%3A%20An%20Empirical%20Study%20on%0A%20%20CodeBERT%0AAuthor%3A%20Giordano%20d%27Aloisio%20and%20Luca%20Traini%20and%20Federica%20Sarro%20and%20Antinisca%20Di%20Marco%0AAbstract%3A%20%20%20Language%20models%20have%20proven%20successful%20across%20a%20wide%20range%20of%20software%0Aengineering%20tasks%2C%20but%20their%20significant%20computational%20costs%20often%20hinder%20their%0Apractical%20adoption.%20To%20address%20this%20challenge%2C%20researchers%20have%20begun%20applying%0Avarious%20compression%20strategies%20to%20improve%20the%20efficiency%20of%20language%20models%20for%0Acode.%20These%20strategies%20aim%20to%20optimize%20inference%20latency%20and%20memory%20usage%2C%0Athough%20often%20at%20the%20cost%20of%20reduced%20model%20effectiveness.%20However%2C%20there%20is%0Astill%20a%20significant%20gap%20in%20understanding%20how%20these%20strategies%20influence%20the%0Aefficiency%20and%20effectiveness%20of%20language%20models%20for%20code.%20Here%2C%20we%20empirically%0Ainvestigate%20the%20impact%20of%20three%20well-known%20compression%20strategies%20--%20knowledge%0Adistillation%2C%20quantization%2C%20and%20pruning%20--%20across%20three%20different%20classes%20of%0Asoftware%20engineering%20tasks%3A%20vulnerability%20detection%2C%20code%20summarization%2C%20and%0Acode%20search.%20Our%20findings%20reveal%20that%20the%20impact%20of%20these%20strategies%20varies%0Agreatly%20depending%20on%20the%20task%20and%20the%20specific%20compression%20method%20employed.%0APractitioners%20and%20researchers%20can%20use%20these%20insights%20to%20make%20informed%20decisions%0Awhen%20selecting%20the%20most%20appropriate%20compression%20strategy%2C%20balancing%20both%0Aefficiency%20and%20effectiveness%20based%20on%20their%20specific%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Compression%2520of%2520Language%2520Models%2520for%2520Code%253A%2520An%2520Empirical%2520Study%2520on%250A%2520%2520CodeBERT%26entry.906535625%3DGiordano%2520d%2527Aloisio%2520and%2520Luca%2520Traini%2520and%2520Federica%2520Sarro%2520and%2520Antinisca%2520Di%2520Marco%26entry.1292438233%3D%2520%2520Language%2520models%2520have%2520proven%2520successful%2520across%2520a%2520wide%2520range%2520of%2520software%250Aengineering%2520tasks%252C%2520but%2520their%2520significant%2520computational%2520costs%2520often%2520hinder%2520their%250Apractical%2520adoption.%2520To%2520address%2520this%2520challenge%252C%2520researchers%2520have%2520begun%2520applying%250Avarious%2520compression%2520strategies%2520to%2520improve%2520the%2520efficiency%2520of%2520language%2520models%2520for%250Acode.%2520These%2520strategies%2520aim%2520to%2520optimize%2520inference%2520latency%2520and%2520memory%2520usage%252C%250Athough%2520often%2520at%2520the%2520cost%2520of%2520reduced%2520model%2520effectiveness.%2520However%252C%2520there%2520is%250Astill%2520a%2520significant%2520gap%2520in%2520understanding%2520how%2520these%2520strategies%2520influence%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520language%2520models%2520for%2520code.%2520Here%252C%2520we%2520empirically%250Ainvestigate%2520the%2520impact%2520of%2520three%2520well-known%2520compression%2520strategies%2520--%2520knowledge%250Adistillation%252C%2520quantization%252C%2520and%2520pruning%2520--%2520across%2520three%2520different%2520classes%2520of%250Asoftware%2520engineering%2520tasks%253A%2520vulnerability%2520detection%252C%2520code%2520summarization%252C%2520and%250Acode%2520search.%2520Our%2520findings%2520reveal%2520that%2520the%2520impact%2520of%2520these%2520strategies%2520varies%250Agreatly%2520depending%2520on%2520the%2520task%2520and%2520the%2520specific%2520compression%2520method%2520employed.%250APractitioners%2520and%2520researchers%2520can%2520use%2520these%2520insights%2520to%2520make%2520informed%2520decisions%250Awhen%2520selecting%2520the%2520most%2520appropriate%2520compression%2520strategy%252C%2520balancing%2520both%250Aefficiency%2520and%2520effectiveness%2520based%2520on%2520their%2520specific%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Compression%20of%20Language%20Models%20for%20Code%3A%20An%20Empirical%20Study%20on%0A%20%20CodeBERT&entry.906535625=Giordano%20d%27Aloisio%20and%20Luca%20Traini%20and%20Federica%20Sarro%20and%20Antinisca%20Di%20Marco&entry.1292438233=%20%20Language%20models%20have%20proven%20successful%20across%20a%20wide%20range%20of%20software%0Aengineering%20tasks%2C%20but%20their%20significant%20computational%20costs%20often%20hinder%20their%0Apractical%20adoption.%20To%20address%20this%20challenge%2C%20researchers%20have%20begun%20applying%0Avarious%20compression%20strategies%20to%20improve%20the%20efficiency%20of%20language%20models%20for%0Acode.%20These%20strategies%20aim%20to%20optimize%20inference%20latency%20and%20memory%20usage%2C%0Athough%20often%20at%20the%20cost%20of%20reduced%20model%20effectiveness.%20However%2C%20there%20is%0Astill%20a%20significant%20gap%20in%20understanding%20how%20these%20strategies%20influence%20the%0Aefficiency%20and%20effectiveness%20of%20language%20models%20for%20code.%20Here%2C%20we%20empirically%0Ainvestigate%20the%20impact%20of%20three%20well-known%20compression%20strategies%20--%20knowledge%0Adistillation%2C%20quantization%2C%20and%20pruning%20--%20across%20three%20different%20classes%20of%0Asoftware%20engineering%20tasks%3A%20vulnerability%20detection%2C%20code%20summarization%2C%20and%0Acode%20search.%20Our%20findings%20reveal%20that%20the%20impact%20of%20these%20strategies%20varies%0Agreatly%20depending%20on%20the%20task%20and%20the%20specific%20compression%20method%20employed.%0APractitioners%20and%20researchers%20can%20use%20these%20insights%20to%20make%20informed%20decisions%0Awhen%20selecting%20the%20most%20appropriate%20compression%20strategy%2C%20balancing%20both%0Aefficiency%20and%20effectiveness%20based%20on%20their%20specific%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13737v1&entry.124074799=Read"},
{"title": "Retrieval Augmented Image Harmonization", "author": "Haolin Wang and Ming Liu and Zifei Yan and Chao Zhou and Longan Xiao and Wangmeng Zuo", "abstract": "  When embedding objects (foreground) into images (background), considering the\ninfluence of photography conditions like illumination, it is usually necessary\nto perform image harmonization to make the foreground object coordinate with\nthe background image in terms of brightness, color, and etc. Although existing\nimage harmonization methods have made continuous efforts toward visually\npleasing results, they are still plagued by two main issues. Firstly, the image\nharmonization becomes highly ill-posed when there are no contents similar to\nthe foreground object in the background, making the harmonization results\nunreliable. Secondly, even when similar contents are available, the\nharmonization process is often interfered with by irrelevant areas, mainly\nattributed to an insufficient understanding of image contents and inaccurate\nattention. As a remedy, we present a retrieval-augmented image harmonization\n(Raiha) framework, which seeks proper reference images to reduce the\nill-posedness and restricts the attention to better utilize the useful\ninformation. Specifically, an efficient retrieval method is designed to find\nreference images that contain similar objects as the foreground while the\nillumination is consistent with the background. For training the Raiha\nframework to effectively utilize the reference information, a data augmentation\nstrategy is delicately designed by leveraging existing non-reference image\nharmonization datasets. Besides, the image content priors are introduced to\nensure reasonable attention. With the presented Raiha framework, the image\nharmonization performance is greatly boosted under both non-reference and\nretrieval-augmented settings. The source code and pre-trained models will be\npublicly available.\n", "link": "http://arxiv.org/abs/2412.13916v1", "date": "2024-12-18", "relevancy": 2.13, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5407}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5337}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Augmented%20Image%20Harmonization&body=Title%3A%20Retrieval%20Augmented%20Image%20Harmonization%0AAuthor%3A%20Haolin%20Wang%20and%20Ming%20Liu%20and%20Zifei%20Yan%20and%20Chao%20Zhou%20and%20Longan%20Xiao%20and%20Wangmeng%20Zuo%0AAbstract%3A%20%20%20When%20embedding%20objects%20%28foreground%29%20into%20images%20%28background%29%2C%20considering%20the%0Ainfluence%20of%20photography%20conditions%20like%20illumination%2C%20it%20is%20usually%20necessary%0Ato%20perform%20image%20harmonization%20to%20make%20the%20foreground%20object%20coordinate%20with%0Athe%20background%20image%20in%20terms%20of%20brightness%2C%20color%2C%20and%20etc.%20Although%20existing%0Aimage%20harmonization%20methods%20have%20made%20continuous%20efforts%20toward%20visually%0Apleasing%20results%2C%20they%20are%20still%20plagued%20by%20two%20main%20issues.%20Firstly%2C%20the%20image%0Aharmonization%20becomes%20highly%20ill-posed%20when%20there%20are%20no%20contents%20similar%20to%0Athe%20foreground%20object%20in%20the%20background%2C%20making%20the%20harmonization%20results%0Aunreliable.%20Secondly%2C%20even%20when%20similar%20contents%20are%20available%2C%20the%0Aharmonization%20process%20is%20often%20interfered%20with%20by%20irrelevant%20areas%2C%20mainly%0Aattributed%20to%20an%20insufficient%20understanding%20of%20image%20contents%20and%20inaccurate%0Aattention.%20As%20a%20remedy%2C%20we%20present%20a%20retrieval-augmented%20image%20harmonization%0A%28Raiha%29%20framework%2C%20which%20seeks%20proper%20reference%20images%20to%20reduce%20the%0Aill-posedness%20and%20restricts%20the%20attention%20to%20better%20utilize%20the%20useful%0Ainformation.%20Specifically%2C%20an%20efficient%20retrieval%20method%20is%20designed%20to%20find%0Areference%20images%20that%20contain%20similar%20objects%20as%20the%20foreground%20while%20the%0Aillumination%20is%20consistent%20with%20the%20background.%20For%20training%20the%20Raiha%0Aframework%20to%20effectively%20utilize%20the%20reference%20information%2C%20a%20data%20augmentation%0Astrategy%20is%20delicately%20designed%20by%20leveraging%20existing%20non-reference%20image%0Aharmonization%20datasets.%20Besides%2C%20the%20image%20content%20priors%20are%20introduced%20to%0Aensure%20reasonable%20attention.%20With%20the%20presented%20Raiha%20framework%2C%20the%20image%0Aharmonization%20performance%20is%20greatly%20boosted%20under%20both%20non-reference%20and%0Aretrieval-augmented%20settings.%20The%20source%20code%20and%20pre-trained%20models%20will%20be%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13916v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Augmented%2520Image%2520Harmonization%26entry.906535625%3DHaolin%2520Wang%2520and%2520Ming%2520Liu%2520and%2520Zifei%2520Yan%2520and%2520Chao%2520Zhou%2520and%2520Longan%2520Xiao%2520and%2520Wangmeng%2520Zuo%26entry.1292438233%3D%2520%2520When%2520embedding%2520objects%2520%2528foreground%2529%2520into%2520images%2520%2528background%2529%252C%2520considering%2520the%250Ainfluence%2520of%2520photography%2520conditions%2520like%2520illumination%252C%2520it%2520is%2520usually%2520necessary%250Ato%2520perform%2520image%2520harmonization%2520to%2520make%2520the%2520foreground%2520object%2520coordinate%2520with%250Athe%2520background%2520image%2520in%2520terms%2520of%2520brightness%252C%2520color%252C%2520and%2520etc.%2520Although%2520existing%250Aimage%2520harmonization%2520methods%2520have%2520made%2520continuous%2520efforts%2520toward%2520visually%250Apleasing%2520results%252C%2520they%2520are%2520still%2520plagued%2520by%2520two%2520main%2520issues.%2520Firstly%252C%2520the%2520image%250Aharmonization%2520becomes%2520highly%2520ill-posed%2520when%2520there%2520are%2520no%2520contents%2520similar%2520to%250Athe%2520foreground%2520object%2520in%2520the%2520background%252C%2520making%2520the%2520harmonization%2520results%250Aunreliable.%2520Secondly%252C%2520even%2520when%2520similar%2520contents%2520are%2520available%252C%2520the%250Aharmonization%2520process%2520is%2520often%2520interfered%2520with%2520by%2520irrelevant%2520areas%252C%2520mainly%250Aattributed%2520to%2520an%2520insufficient%2520understanding%2520of%2520image%2520contents%2520and%2520inaccurate%250Aattention.%2520As%2520a%2520remedy%252C%2520we%2520present%2520a%2520retrieval-augmented%2520image%2520harmonization%250A%2528Raiha%2529%2520framework%252C%2520which%2520seeks%2520proper%2520reference%2520images%2520to%2520reduce%2520the%250Aill-posedness%2520and%2520restricts%2520the%2520attention%2520to%2520better%2520utilize%2520the%2520useful%250Ainformation.%2520Specifically%252C%2520an%2520efficient%2520retrieval%2520method%2520is%2520designed%2520to%2520find%250Areference%2520images%2520that%2520contain%2520similar%2520objects%2520as%2520the%2520foreground%2520while%2520the%250Aillumination%2520is%2520consistent%2520with%2520the%2520background.%2520For%2520training%2520the%2520Raiha%250Aframework%2520to%2520effectively%2520utilize%2520the%2520reference%2520information%252C%2520a%2520data%2520augmentation%250Astrategy%2520is%2520delicately%2520designed%2520by%2520leveraging%2520existing%2520non-reference%2520image%250Aharmonization%2520datasets.%2520Besides%252C%2520the%2520image%2520content%2520priors%2520are%2520introduced%2520to%250Aensure%2520reasonable%2520attention.%2520With%2520the%2520presented%2520Raiha%2520framework%252C%2520the%2520image%250Aharmonization%2520performance%2520is%2520greatly%2520boosted%2520under%2520both%2520non-reference%2520and%250Aretrieval-augmented%2520settings.%2520The%2520source%2520code%2520and%2520pre-trained%2520models%2520will%2520be%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13916v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Augmented%20Image%20Harmonization&entry.906535625=Haolin%20Wang%20and%20Ming%20Liu%20and%20Zifei%20Yan%20and%20Chao%20Zhou%20and%20Longan%20Xiao%20and%20Wangmeng%20Zuo&entry.1292438233=%20%20When%20embedding%20objects%20%28foreground%29%20into%20images%20%28background%29%2C%20considering%20the%0Ainfluence%20of%20photography%20conditions%20like%20illumination%2C%20it%20is%20usually%20necessary%0Ato%20perform%20image%20harmonization%20to%20make%20the%20foreground%20object%20coordinate%20with%0Athe%20background%20image%20in%20terms%20of%20brightness%2C%20color%2C%20and%20etc.%20Although%20existing%0Aimage%20harmonization%20methods%20have%20made%20continuous%20efforts%20toward%20visually%0Apleasing%20results%2C%20they%20are%20still%20plagued%20by%20two%20main%20issues.%20Firstly%2C%20the%20image%0Aharmonization%20becomes%20highly%20ill-posed%20when%20there%20are%20no%20contents%20similar%20to%0Athe%20foreground%20object%20in%20the%20background%2C%20making%20the%20harmonization%20results%0Aunreliable.%20Secondly%2C%20even%20when%20similar%20contents%20are%20available%2C%20the%0Aharmonization%20process%20is%20often%20interfered%20with%20by%20irrelevant%20areas%2C%20mainly%0Aattributed%20to%20an%20insufficient%20understanding%20of%20image%20contents%20and%20inaccurate%0Aattention.%20As%20a%20remedy%2C%20we%20present%20a%20retrieval-augmented%20image%20harmonization%0A%28Raiha%29%20framework%2C%20which%20seeks%20proper%20reference%20images%20to%20reduce%20the%0Aill-posedness%20and%20restricts%20the%20attention%20to%20better%20utilize%20the%20useful%0Ainformation.%20Specifically%2C%20an%20efficient%20retrieval%20method%20is%20designed%20to%20find%0Areference%20images%20that%20contain%20similar%20objects%20as%20the%20foreground%20while%20the%0Aillumination%20is%20consistent%20with%20the%20background.%20For%20training%20the%20Raiha%0Aframework%20to%20effectively%20utilize%20the%20reference%20information%2C%20a%20data%20augmentation%0Astrategy%20is%20delicately%20designed%20by%20leveraging%20existing%20non-reference%20image%0Aharmonization%20datasets.%20Besides%2C%20the%20image%20content%20priors%20are%20introduced%20to%0Aensure%20reasonable%20attention.%20With%20the%20presented%20Raiha%20framework%2C%20the%20image%0Aharmonization%20performance%20is%20greatly%20boosted%20under%20both%20non-reference%20and%0Aretrieval-augmented%20settings.%20The%20source%20code%20and%20pre-trained%20models%20will%20be%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13916v1&entry.124074799=Read"},
{"title": "Learnable Prompting SAM-induced Knowledge Distillation for\n  Semi-supervised Medical Image Segmentation", "author": "Kaiwen Huang and Tao Zhou and Huazhu Fu and Yizhe Zhang and Yi Zhou and Chen Gong and Dong Liang", "abstract": "  The limited availability of labeled data has driven advancements in\nsemi-supervised learning for medical image segmentation. Modern large-scale\nmodels tailored for general segmentation, such as the Segment Anything Model\n(SAM), have revealed robust generalization capabilities. However, applying\nthese models directly to medical image segmentation still exposes performance\ndegradation. In this paper, we propose a learnable prompting SAM-induced\nKnowledge distillation framework (KnowSAM) for semi-supervised medical image\nsegmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that\nemploys two distinct sub-networks to employ a co-teaching paradigm, resulting\nin more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS)\nto dynamically produce dense prompts and integrate an adapter to fine-tune SAM\nspecifically for medical image segmentation tasks. Moreover, we propose\nSAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM\nto two sub-networks, enabling them to learn from SAM's predictions and\nalleviate the effects of incorrect pseudo-labels during training. Notably, the\npredictions generated by our subnets are used to produce mask prompts for SAM,\nfacilitating effective inter-module information exchange. Extensive\nexperimental results on various medical segmentation tasks demonstrate that our\nmodel outperforms the state-of-the-art semi-supervised segmentation approaches.\nCrucially, our SAM distillation framework can be seamlessly integrated into\nother semi-supervised segmentation methods to enhance performance. The code\nwill be released upon acceptance of this manuscript at:\nhttps://github.com/taozh2017/KnowSAM\n", "link": "http://arxiv.org/abs/2412.13742v1", "date": "2024-12-18", "relevancy": 2.1279, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Prompting%20SAM-induced%20Knowledge%20Distillation%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation&body=Title%3A%20Learnable%20Prompting%20SAM-induced%20Knowledge%20Distillation%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation%0AAuthor%3A%20Kaiwen%20Huang%20and%20Tao%20Zhou%20and%20Huazhu%20Fu%20and%20Yizhe%20Zhang%20and%20Yi%20Zhou%20and%20Chen%20Gong%20and%20Dong%20Liang%0AAbstract%3A%20%20%20The%20limited%20availability%20of%20labeled%20data%20has%20driven%20advancements%20in%0Asemi-supervised%20learning%20for%20medical%20image%20segmentation.%20Modern%20large-scale%0Amodels%20tailored%20for%20general%20segmentation%2C%20such%20as%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20have%20revealed%20robust%20generalization%20capabilities.%20However%2C%20applying%0Athese%20models%20directly%20to%20medical%20image%20segmentation%20still%20exposes%20performance%0Adegradation.%20In%20this%20paper%2C%20we%20propose%20a%20learnable%20prompting%20SAM-induced%0AKnowledge%20distillation%20framework%20%28KnowSAM%29%20for%20semi-supervised%20medical%20image%0Asegmentation.%20Firstly%2C%20we%20propose%20a%20Multi-view%20Co-training%20%28MC%29%20strategy%20that%0Aemploys%20two%20distinct%20sub-networks%20to%20employ%20a%20co-teaching%20paradigm%2C%20resulting%0Ain%20more%20robust%20outcomes.%20Secondly%2C%20we%20present%20a%20Learnable%20Prompt%20Strategy%20%28LPS%29%0Ato%20dynamically%20produce%20dense%20prompts%20and%20integrate%20an%20adapter%20to%20fine-tune%20SAM%0Aspecifically%20for%20medical%20image%20segmentation%20tasks.%20Moreover%2C%20we%20propose%0ASAM-induced%20Knowledge%20Distillation%20%28SKD%29%20to%20transfer%20useful%20knowledge%20from%20SAM%0Ato%20two%20sub-networks%2C%20enabling%20them%20to%20learn%20from%20SAM%27s%20predictions%20and%0Aalleviate%20the%20effects%20of%20incorrect%20pseudo-labels%20during%20training.%20Notably%2C%20the%0Apredictions%20generated%20by%20our%20subnets%20are%20used%20to%20produce%20mask%20prompts%20for%20SAM%2C%0Afacilitating%20effective%20inter-module%20information%20exchange.%20Extensive%0Aexperimental%20results%20on%20various%20medical%20segmentation%20tasks%20demonstrate%20that%20our%0Amodel%20outperforms%20the%20state-of-the-art%20semi-supervised%20segmentation%20approaches.%0ACrucially%2C%20our%20SAM%20distillation%20framework%20can%20be%20seamlessly%20integrated%20into%0Aother%20semi-supervised%20segmentation%20methods%20to%20enhance%20performance.%20The%20code%0Awill%20be%20released%20upon%20acceptance%20of%20this%20manuscript%20at%3A%0Ahttps%3A//github.com/taozh2017/KnowSAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Prompting%2520SAM-induced%2520Knowledge%2520Distillation%2520for%250A%2520%2520Semi-supervised%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DKaiwen%2520Huang%2520and%2520Tao%2520Zhou%2520and%2520Huazhu%2520Fu%2520and%2520Yizhe%2520Zhang%2520and%2520Yi%2520Zhou%2520and%2520Chen%2520Gong%2520and%2520Dong%2520Liang%26entry.1292438233%3D%2520%2520The%2520limited%2520availability%2520of%2520labeled%2520data%2520has%2520driven%2520advancements%2520in%250Asemi-supervised%2520learning%2520for%2520medical%2520image%2520segmentation.%2520Modern%2520large-scale%250Amodels%2520tailored%2520for%2520general%2520segmentation%252C%2520such%2520as%2520the%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%252C%2520have%2520revealed%2520robust%2520generalization%2520capabilities.%2520However%252C%2520applying%250Athese%2520models%2520directly%2520to%2520medical%2520image%2520segmentation%2520still%2520exposes%2520performance%250Adegradation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520learnable%2520prompting%2520SAM-induced%250AKnowledge%2520distillation%2520framework%2520%2528KnowSAM%2529%2520for%2520semi-supervised%2520medical%2520image%250Asegmentation.%2520Firstly%252C%2520we%2520propose%2520a%2520Multi-view%2520Co-training%2520%2528MC%2529%2520strategy%2520that%250Aemploys%2520two%2520distinct%2520sub-networks%2520to%2520employ%2520a%2520co-teaching%2520paradigm%252C%2520resulting%250Ain%2520more%2520robust%2520outcomes.%2520Secondly%252C%2520we%2520present%2520a%2520Learnable%2520Prompt%2520Strategy%2520%2528LPS%2529%250Ato%2520dynamically%2520produce%2520dense%2520prompts%2520and%2520integrate%2520an%2520adapter%2520to%2520fine-tune%2520SAM%250Aspecifically%2520for%2520medical%2520image%2520segmentation%2520tasks.%2520Moreover%252C%2520we%2520propose%250ASAM-induced%2520Knowledge%2520Distillation%2520%2528SKD%2529%2520to%2520transfer%2520useful%2520knowledge%2520from%2520SAM%250Ato%2520two%2520sub-networks%252C%2520enabling%2520them%2520to%2520learn%2520from%2520SAM%2527s%2520predictions%2520and%250Aalleviate%2520the%2520effects%2520of%2520incorrect%2520pseudo-labels%2520during%2520training.%2520Notably%252C%2520the%250Apredictions%2520generated%2520by%2520our%2520subnets%2520are%2520used%2520to%2520produce%2520mask%2520prompts%2520for%2520SAM%252C%250Afacilitating%2520effective%2520inter-module%2520information%2520exchange.%2520Extensive%250Aexperimental%2520results%2520on%2520various%2520medical%2520segmentation%2520tasks%2520demonstrate%2520that%2520our%250Amodel%2520outperforms%2520the%2520state-of-the-art%2520semi-supervised%2520segmentation%2520approaches.%250ACrucially%252C%2520our%2520SAM%2520distillation%2520framework%2520can%2520be%2520seamlessly%2520integrated%2520into%250Aother%2520semi-supervised%2520segmentation%2520methods%2520to%2520enhance%2520performance.%2520The%2520code%250Awill%2520be%2520released%2520upon%2520acceptance%2520of%2520this%2520manuscript%2520at%253A%250Ahttps%253A//github.com/taozh2017/KnowSAM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Prompting%20SAM-induced%20Knowledge%20Distillation%20for%0A%20%20Semi-supervised%20Medical%20Image%20Segmentation&entry.906535625=Kaiwen%20Huang%20and%20Tao%20Zhou%20and%20Huazhu%20Fu%20and%20Yizhe%20Zhang%20and%20Yi%20Zhou%20and%20Chen%20Gong%20and%20Dong%20Liang&entry.1292438233=%20%20The%20limited%20availability%20of%20labeled%20data%20has%20driven%20advancements%20in%0Asemi-supervised%20learning%20for%20medical%20image%20segmentation.%20Modern%20large-scale%0Amodels%20tailored%20for%20general%20segmentation%2C%20such%20as%20the%20Segment%20Anything%20Model%0A%28SAM%29%2C%20have%20revealed%20robust%20generalization%20capabilities.%20However%2C%20applying%0Athese%20models%20directly%20to%20medical%20image%20segmentation%20still%20exposes%20performance%0Adegradation.%20In%20this%20paper%2C%20we%20propose%20a%20learnable%20prompting%20SAM-induced%0AKnowledge%20distillation%20framework%20%28KnowSAM%29%20for%20semi-supervised%20medical%20image%0Asegmentation.%20Firstly%2C%20we%20propose%20a%20Multi-view%20Co-training%20%28MC%29%20strategy%20that%0Aemploys%20two%20distinct%20sub-networks%20to%20employ%20a%20co-teaching%20paradigm%2C%20resulting%0Ain%20more%20robust%20outcomes.%20Secondly%2C%20we%20present%20a%20Learnable%20Prompt%20Strategy%20%28LPS%29%0Ato%20dynamically%20produce%20dense%20prompts%20and%20integrate%20an%20adapter%20to%20fine-tune%20SAM%0Aspecifically%20for%20medical%20image%20segmentation%20tasks.%20Moreover%2C%20we%20propose%0ASAM-induced%20Knowledge%20Distillation%20%28SKD%29%20to%20transfer%20useful%20knowledge%20from%20SAM%0Ato%20two%20sub-networks%2C%20enabling%20them%20to%20learn%20from%20SAM%27s%20predictions%20and%0Aalleviate%20the%20effects%20of%20incorrect%20pseudo-labels%20during%20training.%20Notably%2C%20the%0Apredictions%20generated%20by%20our%20subnets%20are%20used%20to%20produce%20mask%20prompts%20for%20SAM%2C%0Afacilitating%20effective%20inter-module%20information%20exchange.%20Extensive%0Aexperimental%20results%20on%20various%20medical%20segmentation%20tasks%20demonstrate%20that%20our%0Amodel%20outperforms%20the%20state-of-the-art%20semi-supervised%20segmentation%20approaches.%0ACrucially%2C%20our%20SAM%20distillation%20framework%20can%20be%20seamlessly%20integrated%20into%0Aother%20semi-supervised%20segmentation%20methods%20to%20enhance%20performance.%20The%20code%0Awill%20be%20released%20upon%20acceptance%20of%20this%20manuscript%20at%3A%0Ahttps%3A//github.com/taozh2017/KnowSAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13742v1&entry.124074799=Read"},
{"title": "Threshold Neuron: A Brain-inspired Artificial Neuron for Efficient\n  On-device Inference", "author": "Zihao Zheng and Yuanchun Li and Jiayu Chen and Peng Zhou and Xiang Chen and Yunxin Liu", "abstract": "  Enhancing the computational efficiency of on-device Deep Neural Networks\n(DNNs) remains a significant challengein mobile and edge computing. As we aim\nto execute increasingly complex tasks with constrained computational resources,\nmuch of the research has focused on compressing neural network structures and\noptimizing systems. Although many studies have focused on compressing neural\nnetwork structures and parameters or optimizing underlying systems, there has\nbeen limited attention on optimizing the fundamental building blocks of neural\nnetworks: the neurons. In this study, we deliberate on a simple but important\nresearch question: Can we design artificial neurons that offer greater\nefficiency than the traditional neuron paradigm? Inspired by the threshold\nmechanisms and the excitation-inhibition balance observed in biological\nneurons, we propose a novel artificial neuron model, Threshold Neurons. Using\nThreshold Neurons, we can construct neural networks similar to those with\ntraditional artificial neurons, while significantly reducing hardware\nimplementation complexity. Our extensive experiments validate the effectiveness\nof neural networks utilizing Threshold Neurons, achieving substantial power\nsavings of 7.51x to 8.19x and area savings of 3.89x to 4.33x at the kernel\nlevel, with minimal loss in precision. Furthermore, FPGA-based implementations\nof these networks demonstrate 2.52x power savings and 1.75x speed enhancements\nat the system level. The source code will be made available upon publication.\n", "link": "http://arxiv.org/abs/2412.13902v1", "date": "2024-12-18", "relevancy": 2.1246, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5322}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Threshold%20Neuron%3A%20A%20Brain-inspired%20Artificial%20Neuron%20for%20Efficient%0A%20%20On-device%20Inference&body=Title%3A%20Threshold%20Neuron%3A%20A%20Brain-inspired%20Artificial%20Neuron%20for%20Efficient%0A%20%20On-device%20Inference%0AAuthor%3A%20Zihao%20Zheng%20and%20Yuanchun%20Li%20and%20Jiayu%20Chen%20and%20Peng%20Zhou%20and%20Xiang%20Chen%20and%20Yunxin%20Liu%0AAbstract%3A%20%20%20Enhancing%20the%20computational%20efficiency%20of%20on-device%20Deep%20Neural%20Networks%0A%28DNNs%29%20remains%20a%20significant%20challengein%20mobile%20and%20edge%20computing.%20As%20we%20aim%0Ato%20execute%20increasingly%20complex%20tasks%20with%20constrained%20computational%20resources%2C%0Amuch%20of%20the%20research%20has%20focused%20on%20compressing%20neural%20network%20structures%20and%0Aoptimizing%20systems.%20Although%20many%20studies%20have%20focused%20on%20compressing%20neural%0Anetwork%20structures%20and%20parameters%20or%20optimizing%20underlying%20systems%2C%20there%20has%0Abeen%20limited%20attention%20on%20optimizing%20the%20fundamental%20building%20blocks%20of%20neural%0Anetworks%3A%20the%20neurons.%20In%20this%20study%2C%20we%20deliberate%20on%20a%20simple%20but%20important%0Aresearch%20question%3A%20Can%20we%20design%20artificial%20neurons%20that%20offer%20greater%0Aefficiency%20than%20the%20traditional%20neuron%20paradigm%3F%20Inspired%20by%20the%20threshold%0Amechanisms%20and%20the%20excitation-inhibition%20balance%20observed%20in%20biological%0Aneurons%2C%20we%20propose%20a%20novel%20artificial%20neuron%20model%2C%20Threshold%20Neurons.%20Using%0AThreshold%20Neurons%2C%20we%20can%20construct%20neural%20networks%20similar%20to%20those%20with%0Atraditional%20artificial%20neurons%2C%20while%20significantly%20reducing%20hardware%0Aimplementation%20complexity.%20Our%20extensive%20experiments%20validate%20the%20effectiveness%0Aof%20neural%20networks%20utilizing%20Threshold%20Neurons%2C%20achieving%20substantial%20power%0Asavings%20of%207.51x%20to%208.19x%20and%20area%20savings%20of%203.89x%20to%204.33x%20at%20the%20kernel%0Alevel%2C%20with%20minimal%20loss%20in%20precision.%20Furthermore%2C%20FPGA-based%20implementations%0Aof%20these%20networks%20demonstrate%202.52x%20power%20savings%20and%201.75x%20speed%20enhancements%0Aat%20the%20system%20level.%20The%20source%20code%20will%20be%20made%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThreshold%2520Neuron%253A%2520A%2520Brain-inspired%2520Artificial%2520Neuron%2520for%2520Efficient%250A%2520%2520On-device%2520Inference%26entry.906535625%3DZihao%2520Zheng%2520and%2520Yuanchun%2520Li%2520and%2520Jiayu%2520Chen%2520and%2520Peng%2520Zhou%2520and%2520Xiang%2520Chen%2520and%2520Yunxin%2520Liu%26entry.1292438233%3D%2520%2520Enhancing%2520the%2520computational%2520efficiency%2520of%2520on-device%2520Deep%2520Neural%2520Networks%250A%2528DNNs%2529%2520remains%2520a%2520significant%2520challengein%2520mobile%2520and%2520edge%2520computing.%2520As%2520we%2520aim%250Ato%2520execute%2520increasingly%2520complex%2520tasks%2520with%2520constrained%2520computational%2520resources%252C%250Amuch%2520of%2520the%2520research%2520has%2520focused%2520on%2520compressing%2520neural%2520network%2520structures%2520and%250Aoptimizing%2520systems.%2520Although%2520many%2520studies%2520have%2520focused%2520on%2520compressing%2520neural%250Anetwork%2520structures%2520and%2520parameters%2520or%2520optimizing%2520underlying%2520systems%252C%2520there%2520has%250Abeen%2520limited%2520attention%2520on%2520optimizing%2520the%2520fundamental%2520building%2520blocks%2520of%2520neural%250Anetworks%253A%2520the%2520neurons.%2520In%2520this%2520study%252C%2520we%2520deliberate%2520on%2520a%2520simple%2520but%2520important%250Aresearch%2520question%253A%2520Can%2520we%2520design%2520artificial%2520neurons%2520that%2520offer%2520greater%250Aefficiency%2520than%2520the%2520traditional%2520neuron%2520paradigm%253F%2520Inspired%2520by%2520the%2520threshold%250Amechanisms%2520and%2520the%2520excitation-inhibition%2520balance%2520observed%2520in%2520biological%250Aneurons%252C%2520we%2520propose%2520a%2520novel%2520artificial%2520neuron%2520model%252C%2520Threshold%2520Neurons.%2520Using%250AThreshold%2520Neurons%252C%2520we%2520can%2520construct%2520neural%2520networks%2520similar%2520to%2520those%2520with%250Atraditional%2520artificial%2520neurons%252C%2520while%2520significantly%2520reducing%2520hardware%250Aimplementation%2520complexity.%2520Our%2520extensive%2520experiments%2520validate%2520the%2520effectiveness%250Aof%2520neural%2520networks%2520utilizing%2520Threshold%2520Neurons%252C%2520achieving%2520substantial%2520power%250Asavings%2520of%25207.51x%2520to%25208.19x%2520and%2520area%2520savings%2520of%25203.89x%2520to%25204.33x%2520at%2520the%2520kernel%250Alevel%252C%2520with%2520minimal%2520loss%2520in%2520precision.%2520Furthermore%252C%2520FPGA-based%2520implementations%250Aof%2520these%2520networks%2520demonstrate%25202.52x%2520power%2520savings%2520and%25201.75x%2520speed%2520enhancements%250Aat%2520the%2520system%2520level.%2520The%2520source%2520code%2520will%2520be%2520made%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Threshold%20Neuron%3A%20A%20Brain-inspired%20Artificial%20Neuron%20for%20Efficient%0A%20%20On-device%20Inference&entry.906535625=Zihao%20Zheng%20and%20Yuanchun%20Li%20and%20Jiayu%20Chen%20and%20Peng%20Zhou%20and%20Xiang%20Chen%20and%20Yunxin%20Liu&entry.1292438233=%20%20Enhancing%20the%20computational%20efficiency%20of%20on-device%20Deep%20Neural%20Networks%0A%28DNNs%29%20remains%20a%20significant%20challengein%20mobile%20and%20edge%20computing.%20As%20we%20aim%0Ato%20execute%20increasingly%20complex%20tasks%20with%20constrained%20computational%20resources%2C%0Amuch%20of%20the%20research%20has%20focused%20on%20compressing%20neural%20network%20structures%20and%0Aoptimizing%20systems.%20Although%20many%20studies%20have%20focused%20on%20compressing%20neural%0Anetwork%20structures%20and%20parameters%20or%20optimizing%20underlying%20systems%2C%20there%20has%0Abeen%20limited%20attention%20on%20optimizing%20the%20fundamental%20building%20blocks%20of%20neural%0Anetworks%3A%20the%20neurons.%20In%20this%20study%2C%20we%20deliberate%20on%20a%20simple%20but%20important%0Aresearch%20question%3A%20Can%20we%20design%20artificial%20neurons%20that%20offer%20greater%0Aefficiency%20than%20the%20traditional%20neuron%20paradigm%3F%20Inspired%20by%20the%20threshold%0Amechanisms%20and%20the%20excitation-inhibition%20balance%20observed%20in%20biological%0Aneurons%2C%20we%20propose%20a%20novel%20artificial%20neuron%20model%2C%20Threshold%20Neurons.%20Using%0AThreshold%20Neurons%2C%20we%20can%20construct%20neural%20networks%20similar%20to%20those%20with%0Atraditional%20artificial%20neurons%2C%20while%20significantly%20reducing%20hardware%0Aimplementation%20complexity.%20Our%20extensive%20experiments%20validate%20the%20effectiveness%0Aof%20neural%20networks%20utilizing%20Threshold%20Neurons%2C%20achieving%20substantial%20power%0Asavings%20of%207.51x%20to%208.19x%20and%20area%20savings%20of%203.89x%20to%204.33x%20at%20the%20kernel%0Alevel%2C%20with%20minimal%20loss%20in%20precision.%20Furthermore%2C%20FPGA-based%20implementations%0Aof%20these%20networks%20demonstrate%202.52x%20power%20savings%20and%201.75x%20speed%20enhancements%0Aat%20the%20system%20level.%20The%20source%20code%20will%20be%20made%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13902v1&entry.124074799=Read"},
{"title": "Event-based Photometric Bundle Adjustment", "author": "Shuang Guo and Guillermo Gallego", "abstract": "  We tackle the problem of bundle adjustment (i.e., simultaneous refinement of\ncamera poses and scene map) for a purely rotating event camera. Starting from\nfirst principles, we formulate the problem as a classical non-linear least\nsquares optimization. The photometric error is defined using the event\ngeneration model directly in the camera rotations and the semi-dense scene\nbrightness that triggers the events. We leverage the sparsity of event data to\ndesign a tractable Levenberg-Marquardt solver that handles the very large\nnumber of variables involved. To the best of our knowledge, our method, which\nwe call Event-based Photometric Bundle Adjustment (EPBA), is the first\nevent-only photometric bundle adjustment method that works on the brightness\nmap directly and exploits the space-time characteristics of event data, without\nhaving to convert events into image-like representations. Comprehensive\nexperiments on both synthetic and real-world datasets demonstrate EPBA's\neffectiveness in decreasing the photometric error (by up to 90%), yielding\nresults of unparalleled quality. The refined maps reveal details that were\nhidden using prior state-of-the-art rotation-only estimation methods. The\nexperiments on modern high-resolution event cameras show the applicability of\nEPBA to panoramic imaging in various scenarios (without map initialization, at\nmultiple resolutions, and in combination with other methods, such as IMU dead\nreckoning or previous event-based rotation estimation methods). We make the\nsource code publicly available. https://github.com/tub-rip/epba\n", "link": "http://arxiv.org/abs/2412.14111v1", "date": "2024-12-18", "relevancy": 2.1234, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5501}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Event-based%20Photometric%20Bundle%20Adjustment&body=Title%3A%20Event-based%20Photometric%20Bundle%20Adjustment%0AAuthor%3A%20Shuang%20Guo%20and%20Guillermo%20Gallego%0AAbstract%3A%20%20%20We%20tackle%20the%20problem%20of%20bundle%20adjustment%20%28i.e.%2C%20simultaneous%20refinement%20of%0Acamera%20poses%20and%20scene%20map%29%20for%20a%20purely%20rotating%20event%20camera.%20Starting%20from%0Afirst%20principles%2C%20we%20formulate%20the%20problem%20as%20a%20classical%20non-linear%20least%0Asquares%20optimization.%20The%20photometric%20error%20is%20defined%20using%20the%20event%0Ageneration%20model%20directly%20in%20the%20camera%20rotations%20and%20the%20semi-dense%20scene%0Abrightness%20that%20triggers%20the%20events.%20We%20leverage%20the%20sparsity%20of%20event%20data%20to%0Adesign%20a%20tractable%20Levenberg-Marquardt%20solver%20that%20handles%20the%20very%20large%0Anumber%20of%20variables%20involved.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20method%2C%20which%0Awe%20call%20Event-based%20Photometric%20Bundle%20Adjustment%20%28EPBA%29%2C%20is%20the%20first%0Aevent-only%20photometric%20bundle%20adjustment%20method%20that%20works%20on%20the%20brightness%0Amap%20directly%20and%20exploits%20the%20space-time%20characteristics%20of%20event%20data%2C%20without%0Ahaving%20to%20convert%20events%20into%20image-like%20representations.%20Comprehensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20EPBA%27s%0Aeffectiveness%20in%20decreasing%20the%20photometric%20error%20%28by%20up%20to%2090%25%29%2C%20yielding%0Aresults%20of%20unparalleled%20quality.%20The%20refined%20maps%20reveal%20details%20that%20were%0Ahidden%20using%20prior%20state-of-the-art%20rotation-only%20estimation%20methods.%20The%0Aexperiments%20on%20modern%20high-resolution%20event%20cameras%20show%20the%20applicability%20of%0AEPBA%20to%20panoramic%20imaging%20in%20various%20scenarios%20%28without%20map%20initialization%2C%20at%0Amultiple%20resolutions%2C%20and%20in%20combination%20with%20other%20methods%2C%20such%20as%20IMU%20dead%0Areckoning%20or%20previous%20event-based%20rotation%20estimation%20methods%29.%20We%20make%20the%0Asource%20code%20publicly%20available.%20https%3A//github.com/tub-rip/epba%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvent-based%2520Photometric%2520Bundle%2520Adjustment%26entry.906535625%3DShuang%2520Guo%2520and%2520Guillermo%2520Gallego%26entry.1292438233%3D%2520%2520We%2520tackle%2520the%2520problem%2520of%2520bundle%2520adjustment%2520%2528i.e.%252C%2520simultaneous%2520refinement%2520of%250Acamera%2520poses%2520and%2520scene%2520map%2529%2520for%2520a%2520purely%2520rotating%2520event%2520camera.%2520Starting%2520from%250Afirst%2520principles%252C%2520we%2520formulate%2520the%2520problem%2520as%2520a%2520classical%2520non-linear%2520least%250Asquares%2520optimization.%2520The%2520photometric%2520error%2520is%2520defined%2520using%2520the%2520event%250Ageneration%2520model%2520directly%2520in%2520the%2520camera%2520rotations%2520and%2520the%2520semi-dense%2520scene%250Abrightness%2520that%2520triggers%2520the%2520events.%2520We%2520leverage%2520the%2520sparsity%2520of%2520event%2520data%2520to%250Adesign%2520a%2520tractable%2520Levenberg-Marquardt%2520solver%2520that%2520handles%2520the%2520very%2520large%250Anumber%2520of%2520variables%2520involved.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520method%252C%2520which%250Awe%2520call%2520Event-based%2520Photometric%2520Bundle%2520Adjustment%2520%2528EPBA%2529%252C%2520is%2520the%2520first%250Aevent-only%2520photometric%2520bundle%2520adjustment%2520method%2520that%2520works%2520on%2520the%2520brightness%250Amap%2520directly%2520and%2520exploits%2520the%2520space-time%2520characteristics%2520of%2520event%2520data%252C%2520without%250Ahaving%2520to%2520convert%2520events%2520into%2520image-like%2520representations.%2520Comprehensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520EPBA%2527s%250Aeffectiveness%2520in%2520decreasing%2520the%2520photometric%2520error%2520%2528by%2520up%2520to%252090%2525%2529%252C%2520yielding%250Aresults%2520of%2520unparalleled%2520quality.%2520The%2520refined%2520maps%2520reveal%2520details%2520that%2520were%250Ahidden%2520using%2520prior%2520state-of-the-art%2520rotation-only%2520estimation%2520methods.%2520The%250Aexperiments%2520on%2520modern%2520high-resolution%2520event%2520cameras%2520show%2520the%2520applicability%2520of%250AEPBA%2520to%2520panoramic%2520imaging%2520in%2520various%2520scenarios%2520%2528without%2520map%2520initialization%252C%2520at%250Amultiple%2520resolutions%252C%2520and%2520in%2520combination%2520with%2520other%2520methods%252C%2520such%2520as%2520IMU%2520dead%250Areckoning%2520or%2520previous%2520event-based%2520rotation%2520estimation%2520methods%2529.%2520We%2520make%2520the%250Asource%2520code%2520publicly%2520available.%2520https%253A//github.com/tub-rip/epba%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Event-based%20Photometric%20Bundle%20Adjustment&entry.906535625=Shuang%20Guo%20and%20Guillermo%20Gallego&entry.1292438233=%20%20We%20tackle%20the%20problem%20of%20bundle%20adjustment%20%28i.e.%2C%20simultaneous%20refinement%20of%0Acamera%20poses%20and%20scene%20map%29%20for%20a%20purely%20rotating%20event%20camera.%20Starting%20from%0Afirst%20principles%2C%20we%20formulate%20the%20problem%20as%20a%20classical%20non-linear%20least%0Asquares%20optimization.%20The%20photometric%20error%20is%20defined%20using%20the%20event%0Ageneration%20model%20directly%20in%20the%20camera%20rotations%20and%20the%20semi-dense%20scene%0Abrightness%20that%20triggers%20the%20events.%20We%20leverage%20the%20sparsity%20of%20event%20data%20to%0Adesign%20a%20tractable%20Levenberg-Marquardt%20solver%20that%20handles%20the%20very%20large%0Anumber%20of%20variables%20involved.%20To%20the%20best%20of%20our%20knowledge%2C%20our%20method%2C%20which%0Awe%20call%20Event-based%20Photometric%20Bundle%20Adjustment%20%28EPBA%29%2C%20is%20the%20first%0Aevent-only%20photometric%20bundle%20adjustment%20method%20that%20works%20on%20the%20brightness%0Amap%20directly%20and%20exploits%20the%20space-time%20characteristics%20of%20event%20data%2C%20without%0Ahaving%20to%20convert%20events%20into%20image-like%20representations.%20Comprehensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20EPBA%27s%0Aeffectiveness%20in%20decreasing%20the%20photometric%20error%20%28by%20up%20to%2090%25%29%2C%20yielding%0Aresults%20of%20unparalleled%20quality.%20The%20refined%20maps%20reveal%20details%20that%20were%0Ahidden%20using%20prior%20state-of-the-art%20rotation-only%20estimation%20methods.%20The%0Aexperiments%20on%20modern%20high-resolution%20event%20cameras%20show%20the%20applicability%20of%0AEPBA%20to%20panoramic%20imaging%20in%20various%20scenarios%20%28without%20map%20initialization%2C%20at%0Amultiple%20resolutions%2C%20and%20in%20combination%20with%20other%20methods%2C%20such%20as%20IMU%20dead%0Areckoning%20or%20previous%20event-based%20rotation%20estimation%20methods%29.%20We%20make%20the%0Asource%20code%20publicly%20available.%20https%3A//github.com/tub-rip/epba%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14111v1&entry.124074799=Read"},
{"title": "Evidential Deep Learning for Probabilistic Modelling of Extreme Storm\n  Events", "author": "Ayush Khot and Xihaier Luo and Ai Kagawa and Shinjae Yoo", "abstract": "  Uncertainty quantification (UQ) methods play an important role in reducing\nerrors in weather forecasting. Conventional approaches in UQ for weather\nforecasting rely on generating an ensemble of forecasts from physics-based\nsimulations to estimate the uncertainty. However, it is computationally\nexpensive to generate many forecasts to predict real-time extreme weather\nevents. Evidential Deep Learning (EDL) is an uncertainty-aware deep learning\napproach designed to provide confidence about its predictions using only one\nforecast. It treats learning as an evidence acquisition process where more\nevidence is interpreted as increased predictive confidence. We apply EDL to\nstorm forecasting using real-world weather datasets and compare its performance\nwith traditional methods. Our findings indicate that EDL not only reduces\ncomputational overhead but also enhances predictive uncertainty. This method\nopens up novel opportunities in research areas such as climate risk assessment,\nwhere quantifying the uncertainty about future climate is crucial.\n", "link": "http://arxiv.org/abs/2412.14048v1", "date": "2024-12-18", "relevancy": 2.106, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6389}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5128}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidential%20Deep%20Learning%20for%20Probabilistic%20Modelling%20of%20Extreme%20Storm%0A%20%20Events&body=Title%3A%20Evidential%20Deep%20Learning%20for%20Probabilistic%20Modelling%20of%20Extreme%20Storm%0A%20%20Events%0AAuthor%3A%20Ayush%20Khot%20and%20Xihaier%20Luo%20and%20Ai%20Kagawa%20and%20Shinjae%20Yoo%0AAbstract%3A%20%20%20Uncertainty%20quantification%20%28UQ%29%20methods%20play%20an%20important%20role%20in%20reducing%0Aerrors%20in%20weather%20forecasting.%20Conventional%20approaches%20in%20UQ%20for%20weather%0Aforecasting%20rely%20on%20generating%20an%20ensemble%20of%20forecasts%20from%20physics-based%0Asimulations%20to%20estimate%20the%20uncertainty.%20However%2C%20it%20is%20computationally%0Aexpensive%20to%20generate%20many%20forecasts%20to%20predict%20real-time%20extreme%20weather%0Aevents.%20Evidential%20Deep%20Learning%20%28EDL%29%20is%20an%20uncertainty-aware%20deep%20learning%0Aapproach%20designed%20to%20provide%20confidence%20about%20its%20predictions%20using%20only%20one%0Aforecast.%20It%20treats%20learning%20as%20an%20evidence%20acquisition%20process%20where%20more%0Aevidence%20is%20interpreted%20as%20increased%20predictive%20confidence.%20We%20apply%20EDL%20to%0Astorm%20forecasting%20using%20real-world%20weather%20datasets%20and%20compare%20its%20performance%0Awith%20traditional%20methods.%20Our%20findings%20indicate%20that%20EDL%20not%20only%20reduces%0Acomputational%20overhead%20but%20also%20enhances%20predictive%20uncertainty.%20This%20method%0Aopens%20up%20novel%20opportunities%20in%20research%20areas%20such%20as%20climate%20risk%20assessment%2C%0Awhere%20quantifying%20the%20uncertainty%20about%20future%20climate%20is%20crucial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidential%2520Deep%2520Learning%2520for%2520Probabilistic%2520Modelling%2520of%2520Extreme%2520Storm%250A%2520%2520Events%26entry.906535625%3DAyush%2520Khot%2520and%2520Xihaier%2520Luo%2520and%2520Ai%2520Kagawa%2520and%2520Shinjae%2520Yoo%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520%2528UQ%2529%2520methods%2520play%2520an%2520important%2520role%2520in%2520reducing%250Aerrors%2520in%2520weather%2520forecasting.%2520Conventional%2520approaches%2520in%2520UQ%2520for%2520weather%250Aforecasting%2520rely%2520on%2520generating%2520an%2520ensemble%2520of%2520forecasts%2520from%2520physics-based%250Asimulations%2520to%2520estimate%2520the%2520uncertainty.%2520However%252C%2520it%2520is%2520computationally%250Aexpensive%2520to%2520generate%2520many%2520forecasts%2520to%2520predict%2520real-time%2520extreme%2520weather%250Aevents.%2520Evidential%2520Deep%2520Learning%2520%2528EDL%2529%2520is%2520an%2520uncertainty-aware%2520deep%2520learning%250Aapproach%2520designed%2520to%2520provide%2520confidence%2520about%2520its%2520predictions%2520using%2520only%2520one%250Aforecast.%2520It%2520treats%2520learning%2520as%2520an%2520evidence%2520acquisition%2520process%2520where%2520more%250Aevidence%2520is%2520interpreted%2520as%2520increased%2520predictive%2520confidence.%2520We%2520apply%2520EDL%2520to%250Astorm%2520forecasting%2520using%2520real-world%2520weather%2520datasets%2520and%2520compare%2520its%2520performance%250Awith%2520traditional%2520methods.%2520Our%2520findings%2520indicate%2520that%2520EDL%2520not%2520only%2520reduces%250Acomputational%2520overhead%2520but%2520also%2520enhances%2520predictive%2520uncertainty.%2520This%2520method%250Aopens%2520up%2520novel%2520opportunities%2520in%2520research%2520areas%2520such%2520as%2520climate%2520risk%2520assessment%252C%250Awhere%2520quantifying%2520the%2520uncertainty%2520about%2520future%2520climate%2520is%2520crucial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidential%20Deep%20Learning%20for%20Probabilistic%20Modelling%20of%20Extreme%20Storm%0A%20%20Events&entry.906535625=Ayush%20Khot%20and%20Xihaier%20Luo%20and%20Ai%20Kagawa%20and%20Shinjae%20Yoo&entry.1292438233=%20%20Uncertainty%20quantification%20%28UQ%29%20methods%20play%20an%20important%20role%20in%20reducing%0Aerrors%20in%20weather%20forecasting.%20Conventional%20approaches%20in%20UQ%20for%20weather%0Aforecasting%20rely%20on%20generating%20an%20ensemble%20of%20forecasts%20from%20physics-based%0Asimulations%20to%20estimate%20the%20uncertainty.%20However%2C%20it%20is%20computationally%0Aexpensive%20to%20generate%20many%20forecasts%20to%20predict%20real-time%20extreme%20weather%0Aevents.%20Evidential%20Deep%20Learning%20%28EDL%29%20is%20an%20uncertainty-aware%20deep%20learning%0Aapproach%20designed%20to%20provide%20confidence%20about%20its%20predictions%20using%20only%20one%0Aforecast.%20It%20treats%20learning%20as%20an%20evidence%20acquisition%20process%20where%20more%0Aevidence%20is%20interpreted%20as%20increased%20predictive%20confidence.%20We%20apply%20EDL%20to%0Astorm%20forecasting%20using%20real-world%20weather%20datasets%20and%20compare%20its%20performance%0Awith%20traditional%20methods.%20Our%20findings%20indicate%20that%20EDL%20not%20only%20reduces%0Acomputational%20overhead%20but%20also%20enhances%20predictive%20uncertainty.%20This%20method%0Aopens%20up%20novel%20opportunities%20in%20research%20areas%20such%20as%20climate%20risk%20assessment%2C%0Awhere%20quantifying%20the%20uncertainty%20about%20future%20climate%20is%20crucial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14048v1&entry.124074799=Read"},
{"title": "Wonderful Matrices: Combining for a More Efficient and Effective\n  Foundation Model Architecture", "author": "Jingze Shi and Bingheng Wu", "abstract": "  In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.\n", "link": "http://arxiv.org/abs/2412.11834v2", "date": "2024-12-18", "relevancy": 2.1057, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5283}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wonderful%20Matrices%3A%20Combining%20for%20a%20More%20Efficient%20and%20Effective%0A%20%20Foundation%20Model%20Architecture&body=Title%3A%20Wonderful%20Matrices%3A%20Combining%20for%20a%20More%20Efficient%20and%20Effective%0A%20%20Foundation%20Model%20Architecture%0AAuthor%3A%20Jingze%20Shi%20and%20Bingheng%20Wu%0AAbstract%3A%20%20%20In%20order%20to%20make%20the%20foundation%20model%20more%20efficient%20and%20effective%2C%20our%20idea%0Ais%20combining%20sequence%20transformation%20and%20state%20transformation.%20First%2C%20we%20prove%0Athe%20availability%20of%20rotary%20position%20embedding%20in%20the%20state%20space%20duality%0Aalgorithm%2C%20which%20reduces%20the%20perplexity%20of%20the%20hybrid%20quadratic%20causal%0Aself-attention%20and%20state%20space%20duality%20by%20more%20than%204%25%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20unifies%20position%20encoding.%20Second%2C%20we%20propose%0Adynamic%20mask%20attention%2C%20which%20maintains%20100%25%20accuracy%20in%20the%20more%20challenging%0Amulti-query%20associative%20recall%20task%2C%20improving%20by%20more%20than%20150%25%20compared%20to%0Aquadratic%20causal%20self-attention%20and%20state%20space%20duality%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20selectively%20filters%20relevant%20information.%0AThird%2C%20we%20design%20cross%20domain%20mixture%20of%20experts%2C%20which%20makes%20the%20computational%0Aspeed%20of%20expert%20retrieval%20with%20more%20than%201024%20experts%208%20to%2010%20times%20faster%20than%0Athe%20mixture%20of%20experts%2C%20to%20ensure%20that%20the%20combining%20state%20transformation%0Aquickly%20retrieval%20mixture.%20Finally%2C%20we%20summarize%20these%20matrix%20algorithms%20that%0Acan%20form%20the%20foundation%20model%3A%20Wonderful%20Matrices%2C%20which%20can%20be%20a%20competitor%20to%0Apopular%20model%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWonderful%2520Matrices%253A%2520Combining%2520for%2520a%2520More%2520Efficient%2520and%2520Effective%250A%2520%2520Foundation%2520Model%2520Architecture%26entry.906535625%3DJingze%2520Shi%2520and%2520Bingheng%2520Wu%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520make%2520the%2520foundation%2520model%2520more%2520efficient%2520and%2520effective%252C%2520our%2520idea%250Ais%2520combining%2520sequence%2520transformation%2520and%2520state%2520transformation.%2520First%252C%2520we%2520prove%250Athe%2520availability%2520of%2520rotary%2520position%2520embedding%2520in%2520the%2520state%2520space%2520duality%250Aalgorithm%252C%2520which%2520reduces%2520the%2520perplexity%2520of%2520the%2520hybrid%2520quadratic%2520causal%250Aself-attention%2520and%2520state%2520space%2520duality%2520by%2520more%2520than%25204%2525%252C%2520to%2520ensure%2520that%2520the%250Acombining%2520sequence%2520transformation%2520unifies%2520position%2520encoding.%2520Second%252C%2520we%2520propose%250Adynamic%2520mask%2520attention%252C%2520which%2520maintains%2520100%2525%2520accuracy%2520in%2520the%2520more%2520challenging%250Amulti-query%2520associative%2520recall%2520task%252C%2520improving%2520by%2520more%2520than%2520150%2525%2520compared%2520to%250Aquadratic%2520causal%2520self-attention%2520and%2520state%2520space%2520duality%252C%2520to%2520ensure%2520that%2520the%250Acombining%2520sequence%2520transformation%2520selectively%2520filters%2520relevant%2520information.%250AThird%252C%2520we%2520design%2520cross%2520domain%2520mixture%2520of%2520experts%252C%2520which%2520makes%2520the%2520computational%250Aspeed%2520of%2520expert%2520retrieval%2520with%2520more%2520than%25201024%2520experts%25208%2520to%252010%2520times%2520faster%2520than%250Athe%2520mixture%2520of%2520experts%252C%2520to%2520ensure%2520that%2520the%2520combining%2520state%2520transformation%250Aquickly%2520retrieval%2520mixture.%2520Finally%252C%2520we%2520summarize%2520these%2520matrix%2520algorithms%2520that%250Acan%2520form%2520the%2520foundation%2520model%253A%2520Wonderful%2520Matrices%252C%2520which%2520can%2520be%2520a%2520competitor%2520to%250Apopular%2520model%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wonderful%20Matrices%3A%20Combining%20for%20a%20More%20Efficient%20and%20Effective%0A%20%20Foundation%20Model%20Architecture&entry.906535625=Jingze%20Shi%20and%20Bingheng%20Wu&entry.1292438233=%20%20In%20order%20to%20make%20the%20foundation%20model%20more%20efficient%20and%20effective%2C%20our%20idea%0Ais%20combining%20sequence%20transformation%20and%20state%20transformation.%20First%2C%20we%20prove%0Athe%20availability%20of%20rotary%20position%20embedding%20in%20the%20state%20space%20duality%0Aalgorithm%2C%20which%20reduces%20the%20perplexity%20of%20the%20hybrid%20quadratic%20causal%0Aself-attention%20and%20state%20space%20duality%20by%20more%20than%204%25%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20unifies%20position%20encoding.%20Second%2C%20we%20propose%0Adynamic%20mask%20attention%2C%20which%20maintains%20100%25%20accuracy%20in%20the%20more%20challenging%0Amulti-query%20associative%20recall%20task%2C%20improving%20by%20more%20than%20150%25%20compared%20to%0Aquadratic%20causal%20self-attention%20and%20state%20space%20duality%2C%20to%20ensure%20that%20the%0Acombining%20sequence%20transformation%20selectively%20filters%20relevant%20information.%0AThird%2C%20we%20design%20cross%20domain%20mixture%20of%20experts%2C%20which%20makes%20the%20computational%0Aspeed%20of%20expert%20retrieval%20with%20more%20than%201024%20experts%208%20to%2010%20times%20faster%20than%0Athe%20mixture%20of%20experts%2C%20to%20ensure%20that%20the%20combining%20state%20transformation%0Aquickly%20retrieval%20mixture.%20Finally%2C%20we%20summarize%20these%20matrix%20algorithms%20that%0Acan%20form%20the%20foundation%20model%3A%20Wonderful%20Matrices%2C%20which%20can%20be%20a%20competitor%20to%0Apopular%20model%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11834v2&entry.124074799=Read"},
{"title": "On Enhancing Root Cause Analysis with SQL Summaries for Failures in\n  Database Workload Replays at SAP HANA", "author": "Neetha Jambigi and Joshua Hammesfahr and Moritz Mueller and Thomas Bach and Michael Felderer", "abstract": "  Capturing the workload of a database and replaying this workload for a new\nversion of the database can be an effective approach for regression testing.\nHowever, false positive errors caused by many factors such as data privacy\nlimitations, time dependency or non-determinism in multi-threaded environment\ncan negatively impact the effectiveness. Therefore, we employ a machine\nlearning based framework to automate the root cause analysis of failures found\nduring replays. However, handling unseen novel issues not found in the training\ndata is one general challenge of machine learning approaches with respect to\ngeneralizability of the learned model. We describe how we continue to address\nthis challenge for more robust long-term solutions. From our experience,\nretraining with new failures is inadequate due to features overlapping across\ndistinct root causes. Hence, we leverage a large language model (LLM) to\nanalyze failed SQL statements and extract concise failure summaries as an\nadditional feature to enhance the classification process. Our experiments show\nthe F1-Macro score improved by 4.77% for our data. We consider our approach\nbeneficial for providing end users with additional information to gain more\ninsights into the found issues and to improve the assessment of the replay\nresults.\n", "link": "http://arxiv.org/abs/2412.13679v1", "date": "2024-12-18", "relevancy": 2.1043, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4222}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4222}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Enhancing%20Root%20Cause%20Analysis%20with%20SQL%20Summaries%20for%20Failures%20in%0A%20%20Database%20Workload%20Replays%20at%20SAP%20HANA&body=Title%3A%20On%20Enhancing%20Root%20Cause%20Analysis%20with%20SQL%20Summaries%20for%20Failures%20in%0A%20%20Database%20Workload%20Replays%20at%20SAP%20HANA%0AAuthor%3A%20Neetha%20Jambigi%20and%20Joshua%20Hammesfahr%20and%20Moritz%20Mueller%20and%20Thomas%20Bach%20and%20Michael%20Felderer%0AAbstract%3A%20%20%20Capturing%20the%20workload%20of%20a%20database%20and%20replaying%20this%20workload%20for%20a%20new%0Aversion%20of%20the%20database%20can%20be%20an%20effective%20approach%20for%20regression%20testing.%0AHowever%2C%20false%20positive%20errors%20caused%20by%20many%20factors%20such%20as%20data%20privacy%0Alimitations%2C%20time%20dependency%20or%20non-determinism%20in%20multi-threaded%20environment%0Acan%20negatively%20impact%20the%20effectiveness.%20Therefore%2C%20we%20employ%20a%20machine%0Alearning%20based%20framework%20to%20automate%20the%20root%20cause%20analysis%20of%20failures%20found%0Aduring%20replays.%20However%2C%20handling%20unseen%20novel%20issues%20not%20found%20in%20the%20training%0Adata%20is%20one%20general%20challenge%20of%20machine%20learning%20approaches%20with%20respect%20to%0Ageneralizability%20of%20the%20learned%20model.%20We%20describe%20how%20we%20continue%20to%20address%0Athis%20challenge%20for%20more%20robust%20long-term%20solutions.%20From%20our%20experience%2C%0Aretraining%20with%20new%20failures%20is%20inadequate%20due%20to%20features%20overlapping%20across%0Adistinct%20root%20causes.%20Hence%2C%20we%20leverage%20a%20large%20language%20model%20%28LLM%29%20to%0Aanalyze%20failed%20SQL%20statements%20and%20extract%20concise%20failure%20summaries%20as%20an%0Aadditional%20feature%20to%20enhance%20the%20classification%20process.%20Our%20experiments%20show%0Athe%20F1-Macro%20score%20improved%20by%204.77%25%20for%20our%20data.%20We%20consider%20our%20approach%0Abeneficial%20for%20providing%20end%20users%20with%20additional%20information%20to%20gain%20more%0Ainsights%20into%20the%20found%20issues%20and%20to%20improve%20the%20assessment%20of%20the%20replay%0Aresults.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Enhancing%2520Root%2520Cause%2520Analysis%2520with%2520SQL%2520Summaries%2520for%2520Failures%2520in%250A%2520%2520Database%2520Workload%2520Replays%2520at%2520SAP%2520HANA%26entry.906535625%3DNeetha%2520Jambigi%2520and%2520Joshua%2520Hammesfahr%2520and%2520Moritz%2520Mueller%2520and%2520Thomas%2520Bach%2520and%2520Michael%2520Felderer%26entry.1292438233%3D%2520%2520Capturing%2520the%2520workload%2520of%2520a%2520database%2520and%2520replaying%2520this%2520workload%2520for%2520a%2520new%250Aversion%2520of%2520the%2520database%2520can%2520be%2520an%2520effective%2520approach%2520for%2520regression%2520testing.%250AHowever%252C%2520false%2520positive%2520errors%2520caused%2520by%2520many%2520factors%2520such%2520as%2520data%2520privacy%250Alimitations%252C%2520time%2520dependency%2520or%2520non-determinism%2520in%2520multi-threaded%2520environment%250Acan%2520negatively%2520impact%2520the%2520effectiveness.%2520Therefore%252C%2520we%2520employ%2520a%2520machine%250Alearning%2520based%2520framework%2520to%2520automate%2520the%2520root%2520cause%2520analysis%2520of%2520failures%2520found%250Aduring%2520replays.%2520However%252C%2520handling%2520unseen%2520novel%2520issues%2520not%2520found%2520in%2520the%2520training%250Adata%2520is%2520one%2520general%2520challenge%2520of%2520machine%2520learning%2520approaches%2520with%2520respect%2520to%250Ageneralizability%2520of%2520the%2520learned%2520model.%2520We%2520describe%2520how%2520we%2520continue%2520to%2520address%250Athis%2520challenge%2520for%2520more%2520robust%2520long-term%2520solutions.%2520From%2520our%2520experience%252C%250Aretraining%2520with%2520new%2520failures%2520is%2520inadequate%2520due%2520to%2520features%2520overlapping%2520across%250Adistinct%2520root%2520causes.%2520Hence%252C%2520we%2520leverage%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%250Aanalyze%2520failed%2520SQL%2520statements%2520and%2520extract%2520concise%2520failure%2520summaries%2520as%2520an%250Aadditional%2520feature%2520to%2520enhance%2520the%2520classification%2520process.%2520Our%2520experiments%2520show%250Athe%2520F1-Macro%2520score%2520improved%2520by%25204.77%2525%2520for%2520our%2520data.%2520We%2520consider%2520our%2520approach%250Abeneficial%2520for%2520providing%2520end%2520users%2520with%2520additional%2520information%2520to%2520gain%2520more%250Ainsights%2520into%2520the%2520found%2520issues%2520and%2520to%2520improve%2520the%2520assessment%2520of%2520the%2520replay%250Aresults.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Enhancing%20Root%20Cause%20Analysis%20with%20SQL%20Summaries%20for%20Failures%20in%0A%20%20Database%20Workload%20Replays%20at%20SAP%20HANA&entry.906535625=Neetha%20Jambigi%20and%20Joshua%20Hammesfahr%20and%20Moritz%20Mueller%20and%20Thomas%20Bach%20and%20Michael%20Felderer&entry.1292438233=%20%20Capturing%20the%20workload%20of%20a%20database%20and%20replaying%20this%20workload%20for%20a%20new%0Aversion%20of%20the%20database%20can%20be%20an%20effective%20approach%20for%20regression%20testing.%0AHowever%2C%20false%20positive%20errors%20caused%20by%20many%20factors%20such%20as%20data%20privacy%0Alimitations%2C%20time%20dependency%20or%20non-determinism%20in%20multi-threaded%20environment%0Acan%20negatively%20impact%20the%20effectiveness.%20Therefore%2C%20we%20employ%20a%20machine%0Alearning%20based%20framework%20to%20automate%20the%20root%20cause%20analysis%20of%20failures%20found%0Aduring%20replays.%20However%2C%20handling%20unseen%20novel%20issues%20not%20found%20in%20the%20training%0Adata%20is%20one%20general%20challenge%20of%20machine%20learning%20approaches%20with%20respect%20to%0Ageneralizability%20of%20the%20learned%20model.%20We%20describe%20how%20we%20continue%20to%20address%0Athis%20challenge%20for%20more%20robust%20long-term%20solutions.%20From%20our%20experience%2C%0Aretraining%20with%20new%20failures%20is%20inadequate%20due%20to%20features%20overlapping%20across%0Adistinct%20root%20causes.%20Hence%2C%20we%20leverage%20a%20large%20language%20model%20%28LLM%29%20to%0Aanalyze%20failed%20SQL%20statements%20and%20extract%20concise%20failure%20summaries%20as%20an%0Aadditional%20feature%20to%20enhance%20the%20classification%20process.%20Our%20experiments%20show%0Athe%20F1-Macro%20score%20improved%20by%204.77%25%20for%20our%20data.%20We%20consider%20our%20approach%0Abeneficial%20for%20providing%20end%20users%20with%20additional%20information%20to%20gain%20more%0Ainsights%20into%20the%20found%20issues%20and%20to%20improve%20the%20assessment%20of%20the%20replay%0Aresults.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13679v1&entry.124074799=Read"},
{"title": "Denoising Diffusion Probabilistic Models for Magnetic Resonance\n  Fingerprinting", "author": "Perla Mayo and Carolin M. Pirkl and Alin Achim and Bjoern H. Menze and Mohammad Golbabaee", "abstract": "  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI, enabling the mapping of multiple tissue properties from a\nsingle, accelerated scan. However, achieving accurate reconstructions remains\nchallenging, particularly in highly accelerated and undersampled acquisitions,\nwhich are crucial for reducing scan times. While deep learning techniques have\nadvanced image reconstruction, the recent introduction of diffusion models\noffers new possibilities for imaging tasks, though their application in the\nmedical field is still emerging. Notably, diffusion models have not yet been\nexplored for the MRF problem. In this work, we propose for the first time a\nconditional diffusion probabilistic model for MRF image reconstruction.\nQualitative and quantitative comparisons on in-vivo brain scan data demonstrate\nthat the proposed approach can outperform established deep learning and\ncompressed sensing algorithms for MRF reconstruction. Extensive ablation\nstudies also explore strategies to improve computational efficiency of our\napproach.\n", "link": "http://arxiv.org/abs/2410.23318v2", "date": "2024-12-18", "relevancy": 1.668, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5986}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5455}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Magnetic%20Resonance%0A%20%20Fingerprinting&body=Title%3A%20Denoising%20Diffusion%20Probabilistic%20Models%20for%20Magnetic%20Resonance%0A%20%20Fingerprinting%0AAuthor%3A%20Perla%20Mayo%20and%20Carolin%20M.%20Pirkl%20and%20Alin%20Achim%20and%20Bjoern%20H.%20Menze%20and%20Mohammad%20Golbabaee%0AAbstract%3A%20%20%20Magnetic%20Resonance%20Fingerprinting%20%28MRF%29%20is%20a%20time-efficient%20approach%20to%0Aquantitative%20MRI%2C%20enabling%20the%20mapping%20of%20multiple%20tissue%20properties%20from%20a%0Asingle%2C%20accelerated%20scan.%20However%2C%20achieving%20accurate%20reconstructions%20remains%0Achallenging%2C%20particularly%20in%20highly%20accelerated%20and%20undersampled%20acquisitions%2C%0Awhich%20are%20crucial%20for%20reducing%20scan%20times.%20While%20deep%20learning%20techniques%20have%0Aadvanced%20image%20reconstruction%2C%20the%20recent%20introduction%20of%20diffusion%20models%0Aoffers%20new%20possibilities%20for%20imaging%20tasks%2C%20though%20their%20application%20in%20the%0Amedical%20field%20is%20still%20emerging.%20Notably%2C%20diffusion%20models%20have%20not%20yet%20been%0Aexplored%20for%20the%20MRF%20problem.%20In%20this%20work%2C%20we%20propose%20for%20the%20first%20time%20a%0Aconditional%20diffusion%20probabilistic%20model%20for%20MRF%20image%20reconstruction.%0AQualitative%20and%20quantitative%20comparisons%20on%20in-vivo%20brain%20scan%20data%20demonstrate%0Athat%20the%20proposed%20approach%20can%20outperform%20established%20deep%20learning%20and%0Acompressed%20sensing%20algorithms%20for%20MRF%20reconstruction.%20Extensive%20ablation%0Astudies%20also%20explore%20strategies%20to%20improve%20computational%20efficiency%20of%20our%0Aapproach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising%2520Diffusion%2520Probabilistic%2520Models%2520for%2520Magnetic%2520Resonance%250A%2520%2520Fingerprinting%26entry.906535625%3DPerla%2520Mayo%2520and%2520Carolin%2520M.%2520Pirkl%2520and%2520Alin%2520Achim%2520and%2520Bjoern%2520H.%2520Menze%2520and%2520Mohammad%2520Golbabaee%26entry.1292438233%3D%2520%2520Magnetic%2520Resonance%2520Fingerprinting%2520%2528MRF%2529%2520is%2520a%2520time-efficient%2520approach%2520to%250Aquantitative%2520MRI%252C%2520enabling%2520the%2520mapping%2520of%2520multiple%2520tissue%2520properties%2520from%2520a%250Asingle%252C%2520accelerated%2520scan.%2520However%252C%2520achieving%2520accurate%2520reconstructions%2520remains%250Achallenging%252C%2520particularly%2520in%2520highly%2520accelerated%2520and%2520undersampled%2520acquisitions%252C%250Awhich%2520are%2520crucial%2520for%2520reducing%2520scan%2520times.%2520While%2520deep%2520learning%2520techniques%2520have%250Aadvanced%2520image%2520reconstruction%252C%2520the%2520recent%2520introduction%2520of%2520diffusion%2520models%250Aoffers%2520new%2520possibilities%2520for%2520imaging%2520tasks%252C%2520though%2520their%2520application%2520in%2520the%250Amedical%2520field%2520is%2520still%2520emerging.%2520Notably%252C%2520diffusion%2520models%2520have%2520not%2520yet%2520been%250Aexplored%2520for%2520the%2520MRF%2520problem.%2520In%2520this%2520work%252C%2520we%2520propose%2520for%2520the%2520first%2520time%2520a%250Aconditional%2520diffusion%2520probabilistic%2520model%2520for%2520MRF%2520image%2520reconstruction.%250AQualitative%2520and%2520quantitative%2520comparisons%2520on%2520in-vivo%2520brain%2520scan%2520data%2520demonstrate%250Athat%2520the%2520proposed%2520approach%2520can%2520outperform%2520established%2520deep%2520learning%2520and%250Acompressed%2520sensing%2520algorithms%2520for%2520MRF%2520reconstruction.%2520Extensive%2520ablation%250Astudies%2520also%2520explore%2520strategies%2520to%2520improve%2520computational%2520efficiency%2520of%2520our%250Aapproach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising%20Diffusion%20Probabilistic%20Models%20for%20Magnetic%20Resonance%0A%20%20Fingerprinting&entry.906535625=Perla%20Mayo%20and%20Carolin%20M.%20Pirkl%20and%20Alin%20Achim%20and%20Bjoern%20H.%20Menze%20and%20Mohammad%20Golbabaee&entry.1292438233=%20%20Magnetic%20Resonance%20Fingerprinting%20%28MRF%29%20is%20a%20time-efficient%20approach%20to%0Aquantitative%20MRI%2C%20enabling%20the%20mapping%20of%20multiple%20tissue%20properties%20from%20a%0Asingle%2C%20accelerated%20scan.%20However%2C%20achieving%20accurate%20reconstructions%20remains%0Achallenging%2C%20particularly%20in%20highly%20accelerated%20and%20undersampled%20acquisitions%2C%0Awhich%20are%20crucial%20for%20reducing%20scan%20times.%20While%20deep%20learning%20techniques%20have%0Aadvanced%20image%20reconstruction%2C%20the%20recent%20introduction%20of%20diffusion%20models%0Aoffers%20new%20possibilities%20for%20imaging%20tasks%2C%20though%20their%20application%20in%20the%0Amedical%20field%20is%20still%20emerging.%20Notably%2C%20diffusion%20models%20have%20not%20yet%20been%0Aexplored%20for%20the%20MRF%20problem.%20In%20this%20work%2C%20we%20propose%20for%20the%20first%20time%20a%0Aconditional%20diffusion%20probabilistic%20model%20for%20MRF%20image%20reconstruction.%0AQualitative%20and%20quantitative%20comparisons%20on%20in-vivo%20brain%20scan%20data%20demonstrate%0Athat%20the%20proposed%20approach%20can%20outperform%20established%20deep%20learning%20and%0Acompressed%20sensing%20algorithms%20for%20MRF%20reconstruction.%20Extensive%20ablation%0Astudies%20also%20explore%20strategies%20to%20improve%20computational%20efficiency%20of%20our%0Aapproach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23318v2&entry.124074799=Read"},
{"title": "Physics-Based Adversarial Attack on Near-Infrared Human Detector for\n  Nighttime Surveillance Camera Systems", "author": "Muyao Niu and Zhuoxiao Li and Yifan Zhan and Huy H. Nguyen and Isao Echizen and Yinqiang Zheng", "abstract": "  Many surveillance cameras switch between daytime and nighttime modes based on\nilluminance levels. During the day, the camera records ordinary RGB images\nthrough an enabled IR-cut filter. At night, the filter is disabled to capture\nnear-infrared (NIR) light emitted from NIR LEDs typically mounted around the\nlens. While RGB-based AI algorithm vulnerabilities have been widely reported,\nthe vulnerabilities of NIR-based AI have rarely been investigated. In this\npaper, we identify fundamental vulnerabilities in NIR-based image understanding\ncaused by color and texture loss due to the intrinsic characteristics of\nclothes' reflectance and cameras' spectral sensitivity in the NIR range. We\nfurther show that the nearly co-located configuration of illuminants and\ncameras in existing surveillance systems facilitates concealing and fully\npassive attacks in the physical world. Specifically, we demonstrate how\nretro-reflective and insulation plastic tapes can manipulate the intensity\ndistribution of NIR images. We showcase an attack on the YOLO-based human\ndetector using binary patterns designed in the digital space (via black-box\nquery and searching) and then physically realized using tapes pasted onto\nclothes. Our attack highlights significant reliability concerns for nighttime\nsurveillance systems, which are intended to enhance security. Codes Available:\nhttps://github.com/MyNiuuu/AdvNIR\n", "link": "http://arxiv.org/abs/2412.13709v1", "date": "2024-12-18", "relevancy": 1.404, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4703}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4685}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Based%20Adversarial%20Attack%20on%20Near-Infrared%20Human%20Detector%20for%0A%20%20Nighttime%20Surveillance%20Camera%20Systems&body=Title%3A%20Physics-Based%20Adversarial%20Attack%20on%20Near-Infrared%20Human%20Detector%20for%0A%20%20Nighttime%20Surveillance%20Camera%20Systems%0AAuthor%3A%20Muyao%20Niu%20and%20Zhuoxiao%20Li%20and%20Yifan%20Zhan%20and%20Huy%20H.%20Nguyen%20and%20Isao%20Echizen%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Many%20surveillance%20cameras%20switch%20between%20daytime%20and%20nighttime%20modes%20based%20on%0Ailluminance%20levels.%20During%20the%20day%2C%20the%20camera%20records%20ordinary%20RGB%20images%0Athrough%20an%20enabled%20IR-cut%20filter.%20At%20night%2C%20the%20filter%20is%20disabled%20to%20capture%0Anear-infrared%20%28NIR%29%20light%20emitted%20from%20NIR%20LEDs%20typically%20mounted%20around%20the%0Alens.%20While%20RGB-based%20AI%20algorithm%20vulnerabilities%20have%20been%20widely%20reported%2C%0Athe%20vulnerabilities%20of%20NIR-based%20AI%20have%20rarely%20been%20investigated.%20In%20this%0Apaper%2C%20we%20identify%20fundamental%20vulnerabilities%20in%20NIR-based%20image%20understanding%0Acaused%20by%20color%20and%20texture%20loss%20due%20to%20the%20intrinsic%20characteristics%20of%0Aclothes%27%20reflectance%20and%20cameras%27%20spectral%20sensitivity%20in%20the%20NIR%20range.%20We%0Afurther%20show%20that%20the%20nearly%20co-located%20configuration%20of%20illuminants%20and%0Acameras%20in%20existing%20surveillance%20systems%20facilitates%20concealing%20and%20fully%0Apassive%20attacks%20in%20the%20physical%20world.%20Specifically%2C%20we%20demonstrate%20how%0Aretro-reflective%20and%20insulation%20plastic%20tapes%20can%20manipulate%20the%20intensity%0Adistribution%20of%20NIR%20images.%20We%20showcase%20an%20attack%20on%20the%20YOLO-based%20human%0Adetector%20using%20binary%20patterns%20designed%20in%20the%20digital%20space%20%28via%20black-box%0Aquery%20and%20searching%29%20and%20then%20physically%20realized%20using%20tapes%20pasted%20onto%0Aclothes.%20Our%20attack%20highlights%20significant%20reliability%20concerns%20for%20nighttime%0Asurveillance%20systems%2C%20which%20are%20intended%20to%20enhance%20security.%20Codes%20Available%3A%0Ahttps%3A//github.com/MyNiuuu/AdvNIR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Based%2520Adversarial%2520Attack%2520on%2520Near-Infrared%2520Human%2520Detector%2520for%250A%2520%2520Nighttime%2520Surveillance%2520Camera%2520Systems%26entry.906535625%3DMuyao%2520Niu%2520and%2520Zhuoxiao%2520Li%2520and%2520Yifan%2520Zhan%2520and%2520Huy%2520H.%2520Nguyen%2520and%2520Isao%2520Echizen%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Many%2520surveillance%2520cameras%2520switch%2520between%2520daytime%2520and%2520nighttime%2520modes%2520based%2520on%250Ailluminance%2520levels.%2520During%2520the%2520day%252C%2520the%2520camera%2520records%2520ordinary%2520RGB%2520images%250Athrough%2520an%2520enabled%2520IR-cut%2520filter.%2520At%2520night%252C%2520the%2520filter%2520is%2520disabled%2520to%2520capture%250Anear-infrared%2520%2528NIR%2529%2520light%2520emitted%2520from%2520NIR%2520LEDs%2520typically%2520mounted%2520around%2520the%250Alens.%2520While%2520RGB-based%2520AI%2520algorithm%2520vulnerabilities%2520have%2520been%2520widely%2520reported%252C%250Athe%2520vulnerabilities%2520of%2520NIR-based%2520AI%2520have%2520rarely%2520been%2520investigated.%2520In%2520this%250Apaper%252C%2520we%2520identify%2520fundamental%2520vulnerabilities%2520in%2520NIR-based%2520image%2520understanding%250Acaused%2520by%2520color%2520and%2520texture%2520loss%2520due%2520to%2520the%2520intrinsic%2520characteristics%2520of%250Aclothes%2527%2520reflectance%2520and%2520cameras%2527%2520spectral%2520sensitivity%2520in%2520the%2520NIR%2520range.%2520We%250Afurther%2520show%2520that%2520the%2520nearly%2520co-located%2520configuration%2520of%2520illuminants%2520and%250Acameras%2520in%2520existing%2520surveillance%2520systems%2520facilitates%2520concealing%2520and%2520fully%250Apassive%2520attacks%2520in%2520the%2520physical%2520world.%2520Specifically%252C%2520we%2520demonstrate%2520how%250Aretro-reflective%2520and%2520insulation%2520plastic%2520tapes%2520can%2520manipulate%2520the%2520intensity%250Adistribution%2520of%2520NIR%2520images.%2520We%2520showcase%2520an%2520attack%2520on%2520the%2520YOLO-based%2520human%250Adetector%2520using%2520binary%2520patterns%2520designed%2520in%2520the%2520digital%2520space%2520%2528via%2520black-box%250Aquery%2520and%2520searching%2529%2520and%2520then%2520physically%2520realized%2520using%2520tapes%2520pasted%2520onto%250Aclothes.%2520Our%2520attack%2520highlights%2520significant%2520reliability%2520concerns%2520for%2520nighttime%250Asurveillance%2520systems%252C%2520which%2520are%2520intended%2520to%2520enhance%2520security.%2520Codes%2520Available%253A%250Ahttps%253A//github.com/MyNiuuu/AdvNIR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Based%20Adversarial%20Attack%20on%20Near-Infrared%20Human%20Detector%20for%0A%20%20Nighttime%20Surveillance%20Camera%20Systems&entry.906535625=Muyao%20Niu%20and%20Zhuoxiao%20Li%20and%20Yifan%20Zhan%20and%20Huy%20H.%20Nguyen%20and%20Isao%20Echizen%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Many%20surveillance%20cameras%20switch%20between%20daytime%20and%20nighttime%20modes%20based%20on%0Ailluminance%20levels.%20During%20the%20day%2C%20the%20camera%20records%20ordinary%20RGB%20images%0Athrough%20an%20enabled%20IR-cut%20filter.%20At%20night%2C%20the%20filter%20is%20disabled%20to%20capture%0Anear-infrared%20%28NIR%29%20light%20emitted%20from%20NIR%20LEDs%20typically%20mounted%20around%20the%0Alens.%20While%20RGB-based%20AI%20algorithm%20vulnerabilities%20have%20been%20widely%20reported%2C%0Athe%20vulnerabilities%20of%20NIR-based%20AI%20have%20rarely%20been%20investigated.%20In%20this%0Apaper%2C%20we%20identify%20fundamental%20vulnerabilities%20in%20NIR-based%20image%20understanding%0Acaused%20by%20color%20and%20texture%20loss%20due%20to%20the%20intrinsic%20characteristics%20of%0Aclothes%27%20reflectance%20and%20cameras%27%20spectral%20sensitivity%20in%20the%20NIR%20range.%20We%0Afurther%20show%20that%20the%20nearly%20co-located%20configuration%20of%20illuminants%20and%0Acameras%20in%20existing%20surveillance%20systems%20facilitates%20concealing%20and%20fully%0Apassive%20attacks%20in%20the%20physical%20world.%20Specifically%2C%20we%20demonstrate%20how%0Aretro-reflective%20and%20insulation%20plastic%20tapes%20can%20manipulate%20the%20intensity%0Adistribution%20of%20NIR%20images.%20We%20showcase%20an%20attack%20on%20the%20YOLO-based%20human%0Adetector%20using%20binary%20patterns%20designed%20in%20the%20digital%20space%20%28via%20black-box%0Aquery%20and%20searching%29%20and%20then%20physically%20realized%20using%20tapes%20pasted%20onto%0Aclothes.%20Our%20attack%20highlights%20significant%20reliability%20concerns%20for%20nighttime%0Asurveillance%20systems%2C%20which%20are%20intended%20to%20enhance%20security.%20Codes%20Available%3A%0Ahttps%3A//github.com/MyNiuuu/AdvNIR%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13709v1&entry.124074799=Read"},
{"title": "A Computationally Grounded Framework for Cognitive Attitudes (extended\n  version)", "author": "Tiago de Lima and Emiliano Lorini and Elise Perrotin and Fran\u00e7ois Schwarzentruber", "abstract": "  We introduce a novel language for reasoning about agents' cognitive attitudes\nof both epistemic and motivational type. We interpret it by means of a\ncomputationally grounded semantics using belief bases. Our language includes\nfive types of modal operators for implicit belief, complete attraction,\ncomplete repulsion, realistic attraction and realistic repulsion. We give an\naxiomatization and show that our operators are not mutually expressible and\nthat they can be combined to represent a large variety of psychological\nconcepts including ambivalence, indifference, being motivated, being\ndemotivated and preference. We present a dynamic extension of the language that\nsupports reasoning about the effects of belief change operations. Finally, we\nprovide a succinct formulation of model checking for our languages and a PSPACE\nmodel checking algorithm relying on a reduction into TQBF. We present some\nexperimental results for the implemented algorithm on computation time in a\nconcrete example.\n", "link": "http://arxiv.org/abs/2412.14073v1", "date": "2024-12-18", "relevancy": 0.9215, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5232}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4366}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Computationally%20Grounded%20Framework%20for%20Cognitive%20Attitudes%20%28extended%0A%20%20version%29&body=Title%3A%20A%20Computationally%20Grounded%20Framework%20for%20Cognitive%20Attitudes%20%28extended%0A%20%20version%29%0AAuthor%3A%20Tiago%20de%20Lima%20and%20Emiliano%20Lorini%20and%20Elise%20Perrotin%20and%20Fran%C3%A7ois%20Schwarzentruber%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20language%20for%20reasoning%20about%20agents%27%20cognitive%20attitudes%0Aof%20both%20epistemic%20and%20motivational%20type.%20We%20interpret%20it%20by%20means%20of%20a%0Acomputationally%20grounded%20semantics%20using%20belief%20bases.%20Our%20language%20includes%0Afive%20types%20of%20modal%20operators%20for%20implicit%20belief%2C%20complete%20attraction%2C%0Acomplete%20repulsion%2C%20realistic%20attraction%20and%20realistic%20repulsion.%20We%20give%20an%0Aaxiomatization%20and%20show%20that%20our%20operators%20are%20not%20mutually%20expressible%20and%0Athat%20they%20can%20be%20combined%20to%20represent%20a%20large%20variety%20of%20psychological%0Aconcepts%20including%20ambivalence%2C%20indifference%2C%20being%20motivated%2C%20being%0Ademotivated%20and%20preference.%20We%20present%20a%20dynamic%20extension%20of%20the%20language%20that%0Asupports%20reasoning%20about%20the%20effects%20of%20belief%20change%20operations.%20Finally%2C%20we%0Aprovide%20a%20succinct%20formulation%20of%20model%20checking%20for%20our%20languages%20and%20a%20PSPACE%0Amodel%20checking%20algorithm%20relying%20on%20a%20reduction%20into%20TQBF.%20We%20present%20some%0Aexperimental%20results%20for%20the%20implemented%20algorithm%20on%20computation%20time%20in%20a%0Aconcrete%20example.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Computationally%2520Grounded%2520Framework%2520for%2520Cognitive%2520Attitudes%2520%2528extended%250A%2520%2520version%2529%26entry.906535625%3DTiago%2520de%2520Lima%2520and%2520Emiliano%2520Lorini%2520and%2520Elise%2520Perrotin%2520and%2520Fran%25C3%25A7ois%2520Schwarzentruber%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520language%2520for%2520reasoning%2520about%2520agents%2527%2520cognitive%2520attitudes%250Aof%2520both%2520epistemic%2520and%2520motivational%2520type.%2520We%2520interpret%2520it%2520by%2520means%2520of%2520a%250Acomputationally%2520grounded%2520semantics%2520using%2520belief%2520bases.%2520Our%2520language%2520includes%250Afive%2520types%2520of%2520modal%2520operators%2520for%2520implicit%2520belief%252C%2520complete%2520attraction%252C%250Acomplete%2520repulsion%252C%2520realistic%2520attraction%2520and%2520realistic%2520repulsion.%2520We%2520give%2520an%250Aaxiomatization%2520and%2520show%2520that%2520our%2520operators%2520are%2520not%2520mutually%2520expressible%2520and%250Athat%2520they%2520can%2520be%2520combined%2520to%2520represent%2520a%2520large%2520variety%2520of%2520psychological%250Aconcepts%2520including%2520ambivalence%252C%2520indifference%252C%2520being%2520motivated%252C%2520being%250Ademotivated%2520and%2520preference.%2520We%2520present%2520a%2520dynamic%2520extension%2520of%2520the%2520language%2520that%250Asupports%2520reasoning%2520about%2520the%2520effects%2520of%2520belief%2520change%2520operations.%2520Finally%252C%2520we%250Aprovide%2520a%2520succinct%2520formulation%2520of%2520model%2520checking%2520for%2520our%2520languages%2520and%2520a%2520PSPACE%250Amodel%2520checking%2520algorithm%2520relying%2520on%2520a%2520reduction%2520into%2520TQBF.%2520We%2520present%2520some%250Aexperimental%2520results%2520for%2520the%2520implemented%2520algorithm%2520on%2520computation%2520time%2520in%2520a%250Aconcrete%2520example.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Computationally%20Grounded%20Framework%20for%20Cognitive%20Attitudes%20%28extended%0A%20%20version%29&entry.906535625=Tiago%20de%20Lima%20and%20Emiliano%20Lorini%20and%20Elise%20Perrotin%20and%20Fran%C3%A7ois%20Schwarzentruber&entry.1292438233=%20%20We%20introduce%20a%20novel%20language%20for%20reasoning%20about%20agents%27%20cognitive%20attitudes%0Aof%20both%20epistemic%20and%20motivational%20type.%20We%20interpret%20it%20by%20means%20of%20a%0Acomputationally%20grounded%20semantics%20using%20belief%20bases.%20Our%20language%20includes%0Afive%20types%20of%20modal%20operators%20for%20implicit%20belief%2C%20complete%20attraction%2C%0Acomplete%20repulsion%2C%20realistic%20attraction%20and%20realistic%20repulsion.%20We%20give%20an%0Aaxiomatization%20and%20show%20that%20our%20operators%20are%20not%20mutually%20expressible%20and%0Athat%20they%20can%20be%20combined%20to%20represent%20a%20large%20variety%20of%20psychological%0Aconcepts%20including%20ambivalence%2C%20indifference%2C%20being%20motivated%2C%20being%0Ademotivated%20and%20preference.%20We%20present%20a%20dynamic%20extension%20of%20the%20language%20that%0Asupports%20reasoning%20about%20the%20effects%20of%20belief%20change%20operations.%20Finally%2C%20we%0Aprovide%20a%20succinct%20formulation%20of%20model%20checking%20for%20our%20languages%20and%20a%20PSPACE%0Amodel%20checking%20algorithm%20relying%20on%20a%20reduction%20into%20TQBF.%20We%20present%20some%0Aexperimental%20results%20for%20the%20implemented%20algorithm%20on%20computation%20time%20in%20a%0Aconcrete%20example.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14073v1&entry.124074799=Read"},
{"title": "Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement\n  Learning Approach", "author": "Shuang Qiu and Boxiang Lyu and Qinglin Meng and Zhaoran Wang and Zhuoran Yang and Michael I. Jordan", "abstract": "  Dynamic mechanism design studies how mechanism designers should allocate\nresources among agents in a time-varying environment. We consider the problem\nwhere the agents interact with the mechanism designer according to an unknown\nMarkov Decision Process (MDP), where agent rewards and the mechanism designer's\nstate evolve according to an episodic MDP with unknown reward functions and\ntransition kernels. We focus on the online setting with linear function\napproximation and propose novel learning algorithms to recover the dynamic\nVickrey-Clarke-Grove (VCG) mechanism over multiple rounds of interaction. A key\ncontribution of our approach is incorporating reward-free online Reinforcement\nLearning (RL) to aid exploration over a rich policy space to estimate prices in\nthe dynamic VCG mechanism. We show that the regret of our proposed method is\nupper bounded by $\\tilde{\\mathcal{O}}(T^{2/3})$ and further devise a lower\nbound to show that our algorithm is efficient, incurring the same $\\Omega(T^{2\n/ 3})$ regret as the lower bound, where $T$ is the total number of rounds. Our\nwork establishes the regret guarantee for online RL in solving dynamic\nmechanism design problems without prior knowledge of the underlying model.\n", "link": "http://arxiv.org/abs/2202.12797v3", "date": "2024-12-18", "relevancy": 1.5014, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5169}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Dynamic%20Mechanisms%20in%20Unknown%20Environments%3A%20A%20Reinforcement%0A%20%20Learning%20Approach&body=Title%3A%20Learning%20Dynamic%20Mechanisms%20in%20Unknown%20Environments%3A%20A%20Reinforcement%0A%20%20Learning%20Approach%0AAuthor%3A%20Shuang%20Qiu%20and%20Boxiang%20Lyu%20and%20Qinglin%20Meng%20and%20Zhaoran%20Wang%20and%20Zhuoran%20Yang%20and%20Michael%20I.%20Jordan%0AAbstract%3A%20%20%20Dynamic%20mechanism%20design%20studies%20how%20mechanism%20designers%20should%20allocate%0Aresources%20among%20agents%20in%20a%20time-varying%20environment.%20We%20consider%20the%20problem%0Awhere%20the%20agents%20interact%20with%20the%20mechanism%20designer%20according%20to%20an%20unknown%0AMarkov%20Decision%20Process%20%28MDP%29%2C%20where%20agent%20rewards%20and%20the%20mechanism%20designer%27s%0Astate%20evolve%20according%20to%20an%20episodic%20MDP%20with%20unknown%20reward%20functions%20and%0Atransition%20kernels.%20We%20focus%20on%20the%20online%20setting%20with%20linear%20function%0Aapproximation%20and%20propose%20novel%20learning%20algorithms%20to%20recover%20the%20dynamic%0AVickrey-Clarke-Grove%20%28VCG%29%20mechanism%20over%20multiple%20rounds%20of%20interaction.%20A%20key%0Acontribution%20of%20our%20approach%20is%20incorporating%20reward-free%20online%20Reinforcement%0ALearning%20%28RL%29%20to%20aid%20exploration%20over%20a%20rich%20policy%20space%20to%20estimate%20prices%20in%0Athe%20dynamic%20VCG%20mechanism.%20We%20show%20that%20the%20regret%20of%20our%20proposed%20method%20is%0Aupper%20bounded%20by%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B2/3%7D%29%24%20and%20further%20devise%20a%20lower%0Abound%20to%20show%20that%20our%20algorithm%20is%20efficient%2C%20incurring%20the%20same%20%24%5COmega%28T%5E%7B2%0A/%203%7D%29%24%20regret%20as%20the%20lower%20bound%2C%20where%20%24T%24%20is%20the%20total%20number%20of%20rounds.%20Our%0Awork%20establishes%20the%20regret%20guarantee%20for%20online%20RL%20in%20solving%20dynamic%0Amechanism%20design%20problems%20without%20prior%20knowledge%20of%20the%20underlying%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.12797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Dynamic%2520Mechanisms%2520in%2520Unknown%2520Environments%253A%2520A%2520Reinforcement%250A%2520%2520Learning%2520Approach%26entry.906535625%3DShuang%2520Qiu%2520and%2520Boxiang%2520Lyu%2520and%2520Qinglin%2520Meng%2520and%2520Zhaoran%2520Wang%2520and%2520Zhuoran%2520Yang%2520and%2520Michael%2520I.%2520Jordan%26entry.1292438233%3D%2520%2520Dynamic%2520mechanism%2520design%2520studies%2520how%2520mechanism%2520designers%2520should%2520allocate%250Aresources%2520among%2520agents%2520in%2520a%2520time-varying%2520environment.%2520We%2520consider%2520the%2520problem%250Awhere%2520the%2520agents%2520interact%2520with%2520the%2520mechanism%2520designer%2520according%2520to%2520an%2520unknown%250AMarkov%2520Decision%2520Process%2520%2528MDP%2529%252C%2520where%2520agent%2520rewards%2520and%2520the%2520mechanism%2520designer%2527s%250Astate%2520evolve%2520according%2520to%2520an%2520episodic%2520MDP%2520with%2520unknown%2520reward%2520functions%2520and%250Atransition%2520kernels.%2520We%2520focus%2520on%2520the%2520online%2520setting%2520with%2520linear%2520function%250Aapproximation%2520and%2520propose%2520novel%2520learning%2520algorithms%2520to%2520recover%2520the%2520dynamic%250AVickrey-Clarke-Grove%2520%2528VCG%2529%2520mechanism%2520over%2520multiple%2520rounds%2520of%2520interaction.%2520A%2520key%250Acontribution%2520of%2520our%2520approach%2520is%2520incorporating%2520reward-free%2520online%2520Reinforcement%250ALearning%2520%2528RL%2529%2520to%2520aid%2520exploration%2520over%2520a%2520rich%2520policy%2520space%2520to%2520estimate%2520prices%2520in%250Athe%2520dynamic%2520VCG%2520mechanism.%2520We%2520show%2520that%2520the%2520regret%2520of%2520our%2520proposed%2520method%2520is%250Aupper%2520bounded%2520by%2520%2524%255Ctilde%257B%255Cmathcal%257BO%257D%257D%2528T%255E%257B2/3%257D%2529%2524%2520and%2520further%2520devise%2520a%2520lower%250Abound%2520to%2520show%2520that%2520our%2520algorithm%2520is%2520efficient%252C%2520incurring%2520the%2520same%2520%2524%255COmega%2528T%255E%257B2%250A/%25203%257D%2529%2524%2520regret%2520as%2520the%2520lower%2520bound%252C%2520where%2520%2524T%2524%2520is%2520the%2520total%2520number%2520of%2520rounds.%2520Our%250Awork%2520establishes%2520the%2520regret%2520guarantee%2520for%2520online%2520RL%2520in%2520solving%2520dynamic%250Amechanism%2520design%2520problems%2520without%2520prior%2520knowledge%2520of%2520the%2520underlying%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.12797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Dynamic%20Mechanisms%20in%20Unknown%20Environments%3A%20A%20Reinforcement%0A%20%20Learning%20Approach&entry.906535625=Shuang%20Qiu%20and%20Boxiang%20Lyu%20and%20Qinglin%20Meng%20and%20Zhaoran%20Wang%20and%20Zhuoran%20Yang%20and%20Michael%20I.%20Jordan&entry.1292438233=%20%20Dynamic%20mechanism%20design%20studies%20how%20mechanism%20designers%20should%20allocate%0Aresources%20among%20agents%20in%20a%20time-varying%20environment.%20We%20consider%20the%20problem%0Awhere%20the%20agents%20interact%20with%20the%20mechanism%20designer%20according%20to%20an%20unknown%0AMarkov%20Decision%20Process%20%28MDP%29%2C%20where%20agent%20rewards%20and%20the%20mechanism%20designer%27s%0Astate%20evolve%20according%20to%20an%20episodic%20MDP%20with%20unknown%20reward%20functions%20and%0Atransition%20kernels.%20We%20focus%20on%20the%20online%20setting%20with%20linear%20function%0Aapproximation%20and%20propose%20novel%20learning%20algorithms%20to%20recover%20the%20dynamic%0AVickrey-Clarke-Grove%20%28VCG%29%20mechanism%20over%20multiple%20rounds%20of%20interaction.%20A%20key%0Acontribution%20of%20our%20approach%20is%20incorporating%20reward-free%20online%20Reinforcement%0ALearning%20%28RL%29%20to%20aid%20exploration%20over%20a%20rich%20policy%20space%20to%20estimate%20prices%20in%0Athe%20dynamic%20VCG%20mechanism.%20We%20show%20that%20the%20regret%20of%20our%20proposed%20method%20is%0Aupper%20bounded%20by%20%24%5Ctilde%7B%5Cmathcal%7BO%7D%7D%28T%5E%7B2/3%7D%29%24%20and%20further%20devise%20a%20lower%0Abound%20to%20show%20that%20our%20algorithm%20is%20efficient%2C%20incurring%20the%20same%20%24%5COmega%28T%5E%7B2%0A/%203%7D%29%24%20regret%20as%20the%20lower%20bound%2C%20where%20%24T%24%20is%20the%20total%20number%20of%20rounds.%20Our%0Awork%20establishes%20the%20regret%20guarantee%20for%20online%20RL%20in%20solving%20dynamic%0Amechanism%20design%20problems%20without%20prior%20knowledge%20of%20the%20underlying%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.12797v3&entry.124074799=Read"},
{"title": "Context Matters: Leveraging Contextual Features for Time Series\n  Forecasting", "author": "Sameep Chattopadhyay and Pulkit Paliwal and Sai Shankar Narasimhan and Shubhankar Agarwal and Sandeep P. Chinchali", "abstract": "  Time series forecasts are often influenced by exogenous contextual features\nin addition to their corresponding history. For example, in financial settings,\nit is hard to accurately predict a stock price without considering public\nsentiments and policy decisions in the form of news articles, tweets, etc.\nThough this is common knowledge, the current state-of-the-art (SOTA)\nforecasting models fail to incorporate such contextual information, owing to\nits heterogeneity and multimodal nature. To address this, we introduce\nContextFormer, a novel plug-and-play method to surgically integrate multimodal\ncontextual information into existing pre-trained forecasting models.\nContextFormer effectively distills forecast-specific information from rich\nmultimodal contexts, including categorical, continuous, time-varying, and even\ntextual information, to significantly enhance the performance of existing base\nforecasters. ContextFormer outperforms SOTA forecasting models by up to 30% on\na range of real-world datasets spanning energy, traffic, environmental, and\nfinancial domains.\n", "link": "http://arxiv.org/abs/2410.12672v4", "date": "2024-12-18", "relevancy": 1.9554, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5071}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20Matters%3A%20Leveraging%20Contextual%20Features%20for%20Time%20Series%0A%20%20Forecasting&body=Title%3A%20Context%20Matters%3A%20Leveraging%20Contextual%20Features%20for%20Time%20Series%0A%20%20Forecasting%0AAuthor%3A%20Sameep%20Chattopadhyay%20and%20Pulkit%20Paliwal%20and%20Sai%20Shankar%20Narasimhan%20and%20Shubhankar%20Agarwal%20and%20Sandeep%20P.%20Chinchali%0AAbstract%3A%20%20%20Time%20series%20forecasts%20are%20often%20influenced%20by%20exogenous%20contextual%20features%0Ain%20addition%20to%20their%20corresponding%20history.%20For%20example%2C%20in%20financial%20settings%2C%0Ait%20is%20hard%20to%20accurately%20predict%20a%20stock%20price%20without%20considering%20public%0Asentiments%20and%20policy%20decisions%20in%20the%20form%20of%20news%20articles%2C%20tweets%2C%20etc.%0AThough%20this%20is%20common%20knowledge%2C%20the%20current%20state-of-the-art%20%28SOTA%29%0Aforecasting%20models%20fail%20to%20incorporate%20such%20contextual%20information%2C%20owing%20to%0Aits%20heterogeneity%20and%20multimodal%20nature.%20To%20address%20this%2C%20we%20introduce%0AContextFormer%2C%20a%20novel%20plug-and-play%20method%20to%20surgically%20integrate%20multimodal%0Acontextual%20information%20into%20existing%20pre-trained%20forecasting%20models.%0AContextFormer%20effectively%20distills%20forecast-specific%20information%20from%20rich%0Amultimodal%20contexts%2C%20including%20categorical%2C%20continuous%2C%20time-varying%2C%20and%20even%0Atextual%20information%2C%20to%20significantly%20enhance%20the%20performance%20of%20existing%20base%0Aforecasters.%20ContextFormer%20outperforms%20SOTA%20forecasting%20models%20by%20up%20to%2030%25%20on%0Aa%20range%20of%20real-world%20datasets%20spanning%20energy%2C%20traffic%2C%20environmental%2C%20and%0Afinancial%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12672v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520Matters%253A%2520Leveraging%2520Contextual%2520Features%2520for%2520Time%2520Series%250A%2520%2520Forecasting%26entry.906535625%3DSameep%2520Chattopadhyay%2520and%2520Pulkit%2520Paliwal%2520and%2520Sai%2520Shankar%2520Narasimhan%2520and%2520Shubhankar%2520Agarwal%2520and%2520Sandeep%2520P.%2520Chinchali%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasts%2520are%2520often%2520influenced%2520by%2520exogenous%2520contextual%2520features%250Ain%2520addition%2520to%2520their%2520corresponding%2520history.%2520For%2520example%252C%2520in%2520financial%2520settings%252C%250Ait%2520is%2520hard%2520to%2520accurately%2520predict%2520a%2520stock%2520price%2520without%2520considering%2520public%250Asentiments%2520and%2520policy%2520decisions%2520in%2520the%2520form%2520of%2520news%2520articles%252C%2520tweets%252C%2520etc.%250AThough%2520this%2520is%2520common%2520knowledge%252C%2520the%2520current%2520state-of-the-art%2520%2528SOTA%2529%250Aforecasting%2520models%2520fail%2520to%2520incorporate%2520such%2520contextual%2520information%252C%2520owing%2520to%250Aits%2520heterogeneity%2520and%2520multimodal%2520nature.%2520To%2520address%2520this%252C%2520we%2520introduce%250AContextFormer%252C%2520a%2520novel%2520plug-and-play%2520method%2520to%2520surgically%2520integrate%2520multimodal%250Acontextual%2520information%2520into%2520existing%2520pre-trained%2520forecasting%2520models.%250AContextFormer%2520effectively%2520distills%2520forecast-specific%2520information%2520from%2520rich%250Amultimodal%2520contexts%252C%2520including%2520categorical%252C%2520continuous%252C%2520time-varying%252C%2520and%2520even%250Atextual%2520information%252C%2520to%2520significantly%2520enhance%2520the%2520performance%2520of%2520existing%2520base%250Aforecasters.%2520ContextFormer%2520outperforms%2520SOTA%2520forecasting%2520models%2520by%2520up%2520to%252030%2525%2520on%250Aa%2520range%2520of%2520real-world%2520datasets%2520spanning%2520energy%252C%2520traffic%252C%2520environmental%252C%2520and%250Afinancial%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12672v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Matters%3A%20Leveraging%20Contextual%20Features%20for%20Time%20Series%0A%20%20Forecasting&entry.906535625=Sameep%20Chattopadhyay%20and%20Pulkit%20Paliwal%20and%20Sai%20Shankar%20Narasimhan%20and%20Shubhankar%20Agarwal%20and%20Sandeep%20P.%20Chinchali&entry.1292438233=%20%20Time%20series%20forecasts%20are%20often%20influenced%20by%20exogenous%20contextual%20features%0Ain%20addition%20to%20their%20corresponding%20history.%20For%20example%2C%20in%20financial%20settings%2C%0Ait%20is%20hard%20to%20accurately%20predict%20a%20stock%20price%20without%20considering%20public%0Asentiments%20and%20policy%20decisions%20in%20the%20form%20of%20news%20articles%2C%20tweets%2C%20etc.%0AThough%20this%20is%20common%20knowledge%2C%20the%20current%20state-of-the-art%20%28SOTA%29%0Aforecasting%20models%20fail%20to%20incorporate%20such%20contextual%20information%2C%20owing%20to%0Aits%20heterogeneity%20and%20multimodal%20nature.%20To%20address%20this%2C%20we%20introduce%0AContextFormer%2C%20a%20novel%20plug-and-play%20method%20to%20surgically%20integrate%20multimodal%0Acontextual%20information%20into%20existing%20pre-trained%20forecasting%20models.%0AContextFormer%20effectively%20distills%20forecast-specific%20information%20from%20rich%0Amultimodal%20contexts%2C%20including%20categorical%2C%20continuous%2C%20time-varying%2C%20and%20even%0Atextual%20information%2C%20to%20significantly%20enhance%20the%20performance%20of%20existing%20base%0Aforecasters.%20ContextFormer%20outperforms%20SOTA%20forecasting%20models%20by%20up%20to%2030%25%20on%0Aa%20range%20of%20real-world%20datasets%20spanning%20energy%2C%20traffic%2C%20environmental%2C%20and%0Afinancial%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12672v4&entry.124074799=Read"},
{"title": "A2RNet: Adversarial Attack Resilient Network for Robust Infrared and\n  Visible Image Fusion", "author": "Jiawei Li and Hongwei Yu and Jiansheng Chen and Xinlong Ding and Jinlong Wang and Jinyuan Liu and Bochao Zou and Huimin Ma", "abstract": "  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n", "link": "http://arxiv.org/abs/2412.09954v2", "date": "2024-12-18", "relevancy": 1.9596, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5146}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4726}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A2RNet%3A%20Adversarial%20Attack%20Resilient%20Network%20for%20Robust%20Infrared%20and%0A%20%20Visible%20Image%20Fusion&body=Title%3A%20A2RNet%3A%20Adversarial%20Attack%20Resilient%20Network%20for%20Robust%20Infrared%20and%0A%20%20Visible%20Image%20Fusion%0AAuthor%3A%20Jiawei%20Li%20and%20Hongwei%20Yu%20and%20Jiansheng%20Chen%20and%20Xinlong%20Ding%20and%20Jinlong%20Wang%20and%20Jinyuan%20Liu%20and%20Bochao%20Zou%20and%20Huimin%20Ma%0AAbstract%3A%20%20%20Infrared%20and%20visible%20image%20fusion%20%28IVIF%29%20is%20a%20crucial%20technique%20for%20enhancing%0Avisual%20performance%20by%20integrating%20unique%20information%20from%20different%20modalities%0Ainto%20one%20fused%20image.%20Exiting%20methods%20pay%20more%20attention%20to%20conducting%20fusion%0Awith%20undisturbed%20data%2C%20while%20overlooking%20the%20impact%20of%20deliberate%20interference%0Aon%20the%20effectiveness%20of%20fusion%20results.%20To%20investigate%20the%20robustness%20of%20fusion%0Amodels%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20adversarial%20attack%20resilient%20network%2C%0Acalled%20%24%5Ctextrm%7BA%7D%5E%7B%5Ctextrm%7B2%7D%7D%24RNet.%20Specifically%2C%20we%20develop%20an%20adversarial%0Aparadigm%20with%20an%20anti-attack%20loss%20function%20to%20implement%20adversarial%20attacks%20and%0Atraining.%20It%20is%20constructed%20based%20on%20the%20intrinsic%20nature%20of%20IVIF%20and%20provide%20a%0Arobust%20foundation%20for%20future%20research%20advancements.%20We%20adopt%20a%20Unet%20as%20the%0Apipeline%20with%20a%20transformer-based%20defensive%20refinement%20module%20%28DRM%29%20under%20this%0Aparadigm%2C%20which%20guarantees%20fused%20image%20quality%20in%20a%20robust%20coarse-to-fine%0Amanner.%20Compared%20to%20previous%20works%2C%20our%20method%20mitigates%20the%20adverse%20effects%20of%0Aadversarial%20perturbations%2C%20consistently%20maintaining%20high-fidelity%20fusion%0Aresults.%20Furthermore%2C%20the%20performance%20of%20downstream%20tasks%20can%20also%20be%20well%0Amaintained%20under%20adversarial%20attacks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lok-18/A2RNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.09954v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA2RNet%253A%2520Adversarial%2520Attack%2520Resilient%2520Network%2520for%2520Robust%2520Infrared%2520and%250A%2520%2520Visible%2520Image%2520Fusion%26entry.906535625%3DJiawei%2520Li%2520and%2520Hongwei%2520Yu%2520and%2520Jiansheng%2520Chen%2520and%2520Xinlong%2520Ding%2520and%2520Jinlong%2520Wang%2520and%2520Jinyuan%2520Liu%2520and%2520Bochao%2520Zou%2520and%2520Huimin%2520Ma%26entry.1292438233%3D%2520%2520Infrared%2520and%2520visible%2520image%2520fusion%2520%2528IVIF%2529%2520is%2520a%2520crucial%2520technique%2520for%2520enhancing%250Avisual%2520performance%2520by%2520integrating%2520unique%2520information%2520from%2520different%2520modalities%250Ainto%2520one%2520fused%2520image.%2520Exiting%2520methods%2520pay%2520more%2520attention%2520to%2520conducting%2520fusion%250Awith%2520undisturbed%2520data%252C%2520while%2520overlooking%2520the%2520impact%2520of%2520deliberate%2520interference%250Aon%2520the%2520effectiveness%2520of%2520fusion%2520results.%2520To%2520investigate%2520the%2520robustness%2520of%2520fusion%250Amodels%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520adversarial%2520attack%2520resilient%2520network%252C%250Acalled%2520%2524%255Ctextrm%257BA%257D%255E%257B%255Ctextrm%257B2%257D%257D%2524RNet.%2520Specifically%252C%2520we%2520develop%2520an%2520adversarial%250Aparadigm%2520with%2520an%2520anti-attack%2520loss%2520function%2520to%2520implement%2520adversarial%2520attacks%2520and%250Atraining.%2520It%2520is%2520constructed%2520based%2520on%2520the%2520intrinsic%2520nature%2520of%2520IVIF%2520and%2520provide%2520a%250Arobust%2520foundation%2520for%2520future%2520research%2520advancements.%2520We%2520adopt%2520a%2520Unet%2520as%2520the%250Apipeline%2520with%2520a%2520transformer-based%2520defensive%2520refinement%2520module%2520%2528DRM%2529%2520under%2520this%250Aparadigm%252C%2520which%2520guarantees%2520fused%2520image%2520quality%2520in%2520a%2520robust%2520coarse-to-fine%250Amanner.%2520Compared%2520to%2520previous%2520works%252C%2520our%2520method%2520mitigates%2520the%2520adverse%2520effects%2520of%250Aadversarial%2520perturbations%252C%2520consistently%2520maintaining%2520high-fidelity%2520fusion%250Aresults.%2520Furthermore%252C%2520the%2520performance%2520of%2520downstream%2520tasks%2520can%2520also%2520be%2520well%250Amaintained%2520under%2520adversarial%2520attacks.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/lok-18/A2RNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.09954v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2RNet%3A%20Adversarial%20Attack%20Resilient%20Network%20for%20Robust%20Infrared%20and%0A%20%20Visible%20Image%20Fusion&entry.906535625=Jiawei%20Li%20and%20Hongwei%20Yu%20and%20Jiansheng%20Chen%20and%20Xinlong%20Ding%20and%20Jinlong%20Wang%20and%20Jinyuan%20Liu%20and%20Bochao%20Zou%20and%20Huimin%20Ma&entry.1292438233=%20%20Infrared%20and%20visible%20image%20fusion%20%28IVIF%29%20is%20a%20crucial%20technique%20for%20enhancing%0Avisual%20performance%20by%20integrating%20unique%20information%20from%20different%20modalities%0Ainto%20one%20fused%20image.%20Exiting%20methods%20pay%20more%20attention%20to%20conducting%20fusion%0Awith%20undisturbed%20data%2C%20while%20overlooking%20the%20impact%20of%20deliberate%20interference%0Aon%20the%20effectiveness%20of%20fusion%20results.%20To%20investigate%20the%20robustness%20of%20fusion%0Amodels%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20adversarial%20attack%20resilient%20network%2C%0Acalled%20%24%5Ctextrm%7BA%7D%5E%7B%5Ctextrm%7B2%7D%7D%24RNet.%20Specifically%2C%20we%20develop%20an%20adversarial%0Aparadigm%20with%20an%20anti-attack%20loss%20function%20to%20implement%20adversarial%20attacks%20and%0Atraining.%20It%20is%20constructed%20based%20on%20the%20intrinsic%20nature%20of%20IVIF%20and%20provide%20a%0Arobust%20foundation%20for%20future%20research%20advancements.%20We%20adopt%20a%20Unet%20as%20the%0Apipeline%20with%20a%20transformer-based%20defensive%20refinement%20module%20%28DRM%29%20under%20this%0Aparadigm%2C%20which%20guarantees%20fused%20image%20quality%20in%20a%20robust%20coarse-to-fine%0Amanner.%20Compared%20to%20previous%20works%2C%20our%20method%20mitigates%20the%20adverse%20effects%20of%0Aadversarial%20perturbations%2C%20consistently%20maintaining%20high-fidelity%20fusion%0Aresults.%20Furthermore%2C%20the%20performance%20of%20downstream%20tasks%20can%20also%20be%20well%0Amaintained%20under%20adversarial%20attacks.%20Code%20is%20available%20at%0Ahttps%3A//github.com/lok-18/A2RNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.09954v2&entry.124074799=Read"},
{"title": "Comparative Analysis of Machine Learning-Based Imputation Techniques for\n  Air Quality Datasets with High Missing Data Rates", "author": "Sen Yan and David J. O'Connor and Xiaojun Wang and Noel E. O'Connor and Alan. F. Smeaton and Mingming Liu", "abstract": "  Urban pollution poses serious health risks, particularly in relation to\ntraffic-related air pollution, which remains a major concern in many cities.\nVehicle emissions contribute to respiratory and cardiovascular issues,\nespecially for vulnerable and exposed road users like pedestrians and cyclists.\nTherefore, accurate air quality monitoring with high spatial resolution is\nvital for good urban environmental management. This study aims to provide\ninsights for processing spatiotemporal datasets with high missing data rates.\nIn this study, the challenge of high missing data rates is a result of the\nlimited data available and the fine granularity required for precise\nclassification of PM2.5 levels. The data used for analysis and imputation were\ncollected from both mobile sensors and fixed stations by Dynamic Parcel\nDistribution, the Environmental Protection Agency, and Google in Dublin,\nIreland, where the missing data rate was approximately 82.42%, making accurate\nParticulate Matter 2.5 level predictions particularly difficult. Various\nimputation and prediction approaches were evaluated and compared, including\nensemble methods, deep learning models, and diffusion models. External features\nsuch as traffic flow, weather conditions, and data from the nearest stations\nwere incorporated to enhance model performance. The results indicate that\ndiffusion methods with external features achieved the highest F1 score,\nreaching 0.9486 (Accuracy: 94.26%, Precision: 94.42%, Recall: 94.82%), with\nensemble models achieving the highest accuracy of 94.82%, illustrating that\ngood performance can be obtained despite a high missing data rate.\n", "link": "http://arxiv.org/abs/2412.13966v1", "date": "2024-12-18", "relevancy": 0.9562, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4833}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.477}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Analysis%20of%20Machine%20Learning-Based%20Imputation%20Techniques%20for%0A%20%20Air%20Quality%20Datasets%20with%20High%20Missing%20Data%20Rates&body=Title%3A%20Comparative%20Analysis%20of%20Machine%20Learning-Based%20Imputation%20Techniques%20for%0A%20%20Air%20Quality%20Datasets%20with%20High%20Missing%20Data%20Rates%0AAuthor%3A%20Sen%20Yan%20and%20David%20J.%20O%27Connor%20and%20Xiaojun%20Wang%20and%20Noel%20E.%20O%27Connor%20and%20Alan.%20F.%20Smeaton%20and%20Mingming%20Liu%0AAbstract%3A%20%20%20Urban%20pollution%20poses%20serious%20health%20risks%2C%20particularly%20in%20relation%20to%0Atraffic-related%20air%20pollution%2C%20which%20remains%20a%20major%20concern%20in%20many%20cities.%0AVehicle%20emissions%20contribute%20to%20respiratory%20and%20cardiovascular%20issues%2C%0Aespecially%20for%20vulnerable%20and%20exposed%20road%20users%20like%20pedestrians%20and%20cyclists.%0ATherefore%2C%20accurate%20air%20quality%20monitoring%20with%20high%20spatial%20resolution%20is%0Avital%20for%20good%20urban%20environmental%20management.%20This%20study%20aims%20to%20provide%0Ainsights%20for%20processing%20spatiotemporal%20datasets%20with%20high%20missing%20data%20rates.%0AIn%20this%20study%2C%20the%20challenge%20of%20high%20missing%20data%20rates%20is%20a%20result%20of%20the%0Alimited%20data%20available%20and%20the%20fine%20granularity%20required%20for%20precise%0Aclassification%20of%20PM2.5%20levels.%20The%20data%20used%20for%20analysis%20and%20imputation%20were%0Acollected%20from%20both%20mobile%20sensors%20and%20fixed%20stations%20by%20Dynamic%20Parcel%0ADistribution%2C%20the%20Environmental%20Protection%20Agency%2C%20and%20Google%20in%20Dublin%2C%0AIreland%2C%20where%20the%20missing%20data%20rate%20was%20approximately%2082.42%25%2C%20making%20accurate%0AParticulate%20Matter%202.5%20level%20predictions%20particularly%20difficult.%20Various%0Aimputation%20and%20prediction%20approaches%20were%20evaluated%20and%20compared%2C%20including%0Aensemble%20methods%2C%20deep%20learning%20models%2C%20and%20diffusion%20models.%20External%20features%0Asuch%20as%20traffic%20flow%2C%20weather%20conditions%2C%20and%20data%20from%20the%20nearest%20stations%0Awere%20incorporated%20to%20enhance%20model%20performance.%20The%20results%20indicate%20that%0Adiffusion%20methods%20with%20external%20features%20achieved%20the%20highest%20F1%20score%2C%0Areaching%200.9486%20%28Accuracy%3A%2094.26%25%2C%20Precision%3A%2094.42%25%2C%20Recall%3A%2094.82%25%29%2C%20with%0Aensemble%20models%20achieving%20the%20highest%20accuracy%20of%2094.82%25%2C%20illustrating%20that%0Agood%20performance%20can%20be%20obtained%20despite%20a%20high%20missing%20data%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Analysis%2520of%2520Machine%2520Learning-Based%2520Imputation%2520Techniques%2520for%250A%2520%2520Air%2520Quality%2520Datasets%2520with%2520High%2520Missing%2520Data%2520Rates%26entry.906535625%3DSen%2520Yan%2520and%2520David%2520J.%2520O%2527Connor%2520and%2520Xiaojun%2520Wang%2520and%2520Noel%2520E.%2520O%2527Connor%2520and%2520Alan.%2520F.%2520Smeaton%2520and%2520Mingming%2520Liu%26entry.1292438233%3D%2520%2520Urban%2520pollution%2520poses%2520serious%2520health%2520risks%252C%2520particularly%2520in%2520relation%2520to%250Atraffic-related%2520air%2520pollution%252C%2520which%2520remains%2520a%2520major%2520concern%2520in%2520many%2520cities.%250AVehicle%2520emissions%2520contribute%2520to%2520respiratory%2520and%2520cardiovascular%2520issues%252C%250Aespecially%2520for%2520vulnerable%2520and%2520exposed%2520road%2520users%2520like%2520pedestrians%2520and%2520cyclists.%250ATherefore%252C%2520accurate%2520air%2520quality%2520monitoring%2520with%2520high%2520spatial%2520resolution%2520is%250Avital%2520for%2520good%2520urban%2520environmental%2520management.%2520This%2520study%2520aims%2520to%2520provide%250Ainsights%2520for%2520processing%2520spatiotemporal%2520datasets%2520with%2520high%2520missing%2520data%2520rates.%250AIn%2520this%2520study%252C%2520the%2520challenge%2520of%2520high%2520missing%2520data%2520rates%2520is%2520a%2520result%2520of%2520the%250Alimited%2520data%2520available%2520and%2520the%2520fine%2520granularity%2520required%2520for%2520precise%250Aclassification%2520of%2520PM2.5%2520levels.%2520The%2520data%2520used%2520for%2520analysis%2520and%2520imputation%2520were%250Acollected%2520from%2520both%2520mobile%2520sensors%2520and%2520fixed%2520stations%2520by%2520Dynamic%2520Parcel%250ADistribution%252C%2520the%2520Environmental%2520Protection%2520Agency%252C%2520and%2520Google%2520in%2520Dublin%252C%250AIreland%252C%2520where%2520the%2520missing%2520data%2520rate%2520was%2520approximately%252082.42%2525%252C%2520making%2520accurate%250AParticulate%2520Matter%25202.5%2520level%2520predictions%2520particularly%2520difficult.%2520Various%250Aimputation%2520and%2520prediction%2520approaches%2520were%2520evaluated%2520and%2520compared%252C%2520including%250Aensemble%2520methods%252C%2520deep%2520learning%2520models%252C%2520and%2520diffusion%2520models.%2520External%2520features%250Asuch%2520as%2520traffic%2520flow%252C%2520weather%2520conditions%252C%2520and%2520data%2520from%2520the%2520nearest%2520stations%250Awere%2520incorporated%2520to%2520enhance%2520model%2520performance.%2520The%2520results%2520indicate%2520that%250Adiffusion%2520methods%2520with%2520external%2520features%2520achieved%2520the%2520highest%2520F1%2520score%252C%250Areaching%25200.9486%2520%2528Accuracy%253A%252094.26%2525%252C%2520Precision%253A%252094.42%2525%252C%2520Recall%253A%252094.82%2525%2529%252C%2520with%250Aensemble%2520models%2520achieving%2520the%2520highest%2520accuracy%2520of%252094.82%2525%252C%2520illustrating%2520that%250Agood%2520performance%2520can%2520be%2520obtained%2520despite%2520a%2520high%2520missing%2520data%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Analysis%20of%20Machine%20Learning-Based%20Imputation%20Techniques%20for%0A%20%20Air%20Quality%20Datasets%20with%20High%20Missing%20Data%20Rates&entry.906535625=Sen%20Yan%20and%20David%20J.%20O%27Connor%20and%20Xiaojun%20Wang%20and%20Noel%20E.%20O%27Connor%20and%20Alan.%20F.%20Smeaton%20and%20Mingming%20Liu&entry.1292438233=%20%20Urban%20pollution%20poses%20serious%20health%20risks%2C%20particularly%20in%20relation%20to%0Atraffic-related%20air%20pollution%2C%20which%20remains%20a%20major%20concern%20in%20many%20cities.%0AVehicle%20emissions%20contribute%20to%20respiratory%20and%20cardiovascular%20issues%2C%0Aespecially%20for%20vulnerable%20and%20exposed%20road%20users%20like%20pedestrians%20and%20cyclists.%0ATherefore%2C%20accurate%20air%20quality%20monitoring%20with%20high%20spatial%20resolution%20is%0Avital%20for%20good%20urban%20environmental%20management.%20This%20study%20aims%20to%20provide%0Ainsights%20for%20processing%20spatiotemporal%20datasets%20with%20high%20missing%20data%20rates.%0AIn%20this%20study%2C%20the%20challenge%20of%20high%20missing%20data%20rates%20is%20a%20result%20of%20the%0Alimited%20data%20available%20and%20the%20fine%20granularity%20required%20for%20precise%0Aclassification%20of%20PM2.5%20levels.%20The%20data%20used%20for%20analysis%20and%20imputation%20were%0Acollected%20from%20both%20mobile%20sensors%20and%20fixed%20stations%20by%20Dynamic%20Parcel%0ADistribution%2C%20the%20Environmental%20Protection%20Agency%2C%20and%20Google%20in%20Dublin%2C%0AIreland%2C%20where%20the%20missing%20data%20rate%20was%20approximately%2082.42%25%2C%20making%20accurate%0AParticulate%20Matter%202.5%20level%20predictions%20particularly%20difficult.%20Various%0Aimputation%20and%20prediction%20approaches%20were%20evaluated%20and%20compared%2C%20including%0Aensemble%20methods%2C%20deep%20learning%20models%2C%20and%20diffusion%20models.%20External%20features%0Asuch%20as%20traffic%20flow%2C%20weather%20conditions%2C%20and%20data%20from%20the%20nearest%20stations%0Awere%20incorporated%20to%20enhance%20model%20performance.%20The%20results%20indicate%20that%0Adiffusion%20methods%20with%20external%20features%20achieved%20the%20highest%20F1%20score%2C%0Areaching%200.9486%20%28Accuracy%3A%2094.26%25%2C%20Precision%3A%2094.42%25%2C%20Recall%3A%2094.82%25%29%2C%20with%0Aensemble%20models%20achieving%20the%20highest%20accuracy%20of%2094.82%25%2C%20illustrating%20that%0Agood%20performance%20can%20be%20obtained%20despite%20a%20high%20missing%20data%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13966v1&entry.124074799=Read"},
{"title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection", "author": "Le Yang and Ziwei Zheng and Boxu Chen and Zhengyu Zhao and Chenhao Lin and Chao Shen", "abstract": "  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.\n", "link": "http://arxiv.org/abs/2412.13817v1", "date": "2024-12-18", "relevancy": 2.0536, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nullu%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20via%20HalluSpace%20Projection&body=Title%3A%20Nullu%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20via%20HalluSpace%20Projection%0AAuthor%3A%20Le%20Yang%20and%20Ziwei%20Zheng%20and%20Boxu%20Chen%20and%20Zhengyu%20Zhao%20and%20Chenhao%20Lin%20and%20Chao%20Shen%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20large%20vision-language%20models%20%28LVLMs%29%20often%0Asuffer%20from%20the%20issue%20of%20object%20hallucinations%20%28OH%29.%20To%20mitigate%20this%20issue%2C%20we%0Aintroduce%20an%20efficient%20method%20that%20edits%20the%20model%20weights%20based%20on%20an%20unsafe%0Asubspace%2C%20which%20we%20call%20HalluSpace%20in%20this%20paper.%20With%20truthful%20and%0Ahallucinated%20text%20prompts%20accompanying%20the%20visual%20content%20as%20inputs%2C%20the%0AHalluSpace%20can%20be%20identified%20by%20extracting%20the%20hallucinated%20embedding%20features%0Aand%20removing%20the%20truthful%20representations%20in%20LVLMs.%20By%20orthogonalizing%20the%0Amodel%20weights%2C%20input%20features%20will%20be%20projected%20into%20the%20Null%20space%20of%20the%0AHalluSpace%20to%20reduce%20OH%2C%20based%20on%20which%20we%20name%20our%20method%20Nullu.%20We%20reveal%0Athat%20HalluSpaces%20generally%20contain%20statistical%20bias%20and%20unimodal%20priors%20of%20the%0Alarge%20language%20models%20%28LLMs%29%20applied%20to%20build%20LVLMs%2C%20which%20have%20been%20shown%20as%0Aessential%20causes%20of%20OH%20in%20previous%20studies.%20Therefore%2C%20null%20space%20projection%0Asuppresses%20the%20LLMs%27%20priors%20to%20filter%20out%20the%20hallucinated%20features%2C%20resulting%0Ain%20contextually%20accurate%20outputs.%20Experiments%20show%20that%20our%20method%20can%0Aeffectively%20mitigate%20OH%20across%20different%20LVLM%20families%20without%20extra%20inference%0Acosts%20and%20also%20show%20strong%20performance%20in%20general%20LVLM%20benchmarks.%20Code%20is%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/Ziwei-Zheng/Nullu%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13817v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNullu%253A%2520Mitigating%2520Object%2520Hallucinations%2520in%2520Large%2520Vision-Language%2520Models%250A%2520%2520via%2520HalluSpace%2520Projection%26entry.906535625%3DLe%2520Yang%2520and%2520Ziwei%2520Zheng%2520and%2520Boxu%2520Chen%2520and%2520Zhengyu%2520Zhao%2520and%2520Chenhao%2520Lin%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520often%250Asuffer%2520from%2520the%2520issue%2520of%2520object%2520hallucinations%2520%2528OH%2529.%2520To%2520mitigate%2520this%2520issue%252C%2520we%250Aintroduce%2520an%2520efficient%2520method%2520that%2520edits%2520the%2520model%2520weights%2520based%2520on%2520an%2520unsafe%250Asubspace%252C%2520which%2520we%2520call%2520HalluSpace%2520in%2520this%2520paper.%2520With%2520truthful%2520and%250Ahallucinated%2520text%2520prompts%2520accompanying%2520the%2520visual%2520content%2520as%2520inputs%252C%2520the%250AHalluSpace%2520can%2520be%2520identified%2520by%2520extracting%2520the%2520hallucinated%2520embedding%2520features%250Aand%2520removing%2520the%2520truthful%2520representations%2520in%2520LVLMs.%2520By%2520orthogonalizing%2520the%250Amodel%2520weights%252C%2520input%2520features%2520will%2520be%2520projected%2520into%2520the%2520Null%2520space%2520of%2520the%250AHalluSpace%2520to%2520reduce%2520OH%252C%2520based%2520on%2520which%2520we%2520name%2520our%2520method%2520Nullu.%2520We%2520reveal%250Athat%2520HalluSpaces%2520generally%2520contain%2520statistical%2520bias%2520and%2520unimodal%2520priors%2520of%2520the%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520applied%2520to%2520build%2520LVLMs%252C%2520which%2520have%2520been%2520shown%2520as%250Aessential%2520causes%2520of%2520OH%2520in%2520previous%2520studies.%2520Therefore%252C%2520null%2520space%2520projection%250Asuppresses%2520the%2520LLMs%2527%2520priors%2520to%2520filter%2520out%2520the%2520hallucinated%2520features%252C%2520resulting%250Ain%2520contextually%2520accurate%2520outputs.%2520Experiments%2520show%2520that%2520our%2520method%2520can%250Aeffectively%2520mitigate%2520OH%2520across%2520different%2520LVLM%2520families%2520without%2520extra%2520inference%250Acosts%2520and%2520also%2520show%2520strong%2520performance%2520in%2520general%2520LVLM%2520benchmarks.%2520Code%2520is%250Areleased%2520at%2520%255Curl%257Bhttps%253A//github.com/Ziwei-Zheng/Nullu%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13817v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nullu%3A%20Mitigating%20Object%20Hallucinations%20in%20Large%20Vision-Language%20Models%0A%20%20via%20HalluSpace%20Projection&entry.906535625=Le%20Yang%20and%20Ziwei%20Zheng%20and%20Boxu%20Chen%20and%20Zhengyu%20Zhao%20and%20Chenhao%20Lin%20and%20Chao%20Shen&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20large%20vision-language%20models%20%28LVLMs%29%20often%0Asuffer%20from%20the%20issue%20of%20object%20hallucinations%20%28OH%29.%20To%20mitigate%20this%20issue%2C%20we%0Aintroduce%20an%20efficient%20method%20that%20edits%20the%20model%20weights%20based%20on%20an%20unsafe%0Asubspace%2C%20which%20we%20call%20HalluSpace%20in%20this%20paper.%20With%20truthful%20and%0Ahallucinated%20text%20prompts%20accompanying%20the%20visual%20content%20as%20inputs%2C%20the%0AHalluSpace%20can%20be%20identified%20by%20extracting%20the%20hallucinated%20embedding%20features%0Aand%20removing%20the%20truthful%20representations%20in%20LVLMs.%20By%20orthogonalizing%20the%0Amodel%20weights%2C%20input%20features%20will%20be%20projected%20into%20the%20Null%20space%20of%20the%0AHalluSpace%20to%20reduce%20OH%2C%20based%20on%20which%20we%20name%20our%20method%20Nullu.%20We%20reveal%0Athat%20HalluSpaces%20generally%20contain%20statistical%20bias%20and%20unimodal%20priors%20of%20the%0Alarge%20language%20models%20%28LLMs%29%20applied%20to%20build%20LVLMs%2C%20which%20have%20been%20shown%20as%0Aessential%20causes%20of%20OH%20in%20previous%20studies.%20Therefore%2C%20null%20space%20projection%0Asuppresses%20the%20LLMs%27%20priors%20to%20filter%20out%20the%20hallucinated%20features%2C%20resulting%0Ain%20contextually%20accurate%20outputs.%20Experiments%20show%20that%20our%20method%20can%0Aeffectively%20mitigate%20OH%20across%20different%20LVLM%20families%20without%20extra%20inference%0Acosts%20and%20also%20show%20strong%20performance%20in%20general%20LVLM%20benchmarks.%20Code%20is%0Areleased%20at%20%5Curl%7Bhttps%3A//github.com/Ziwei-Zheng/Nullu%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13817v1&entry.124074799=Read"},
{"title": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation", "author": "Minkyoung Kim and Yunha Kim and Hyeram Seo and Heejung Choi and Jiye Han and Gaeun Kee and Soyoung Ko and HyoJe Jung and Byeolhee Kim and Young-Hak Kim and Sanghyun Park and Tae Joon Jun", "abstract": "  Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.\n", "link": "http://arxiv.org/abs/2412.13705v1", "date": "2024-12-18", "relevancy": 1.8845, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4909}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4695}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Adversarial%20Attacks%20in%20LLMs%20through%20Defensive%20Suffix%0A%20%20Generation&body=Title%3A%20Mitigating%20Adversarial%20Attacks%20in%20LLMs%20through%20Defensive%20Suffix%0A%20%20Generation%0AAuthor%3A%20Minkyoung%20Kim%20and%20Yunha%20Kim%20and%20Hyeram%20Seo%20and%20Heejung%20Choi%20and%20Jiye%20Han%20and%20Gaeun%20Kee%20and%20Soyoung%20Ko%20and%20HyoJe%20Jung%20and%20Byeolhee%20Kim%20and%20Young-Hak%20Kim%20and%20Sanghyun%20Park%20and%20Tae%20Joon%20Jun%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20outstanding%20performance%20in%0Anatural%20language%20processing%20tasks.%20However%2C%20these%20models%20remain%20susceptible%20to%0Aadversarial%20attacks%20in%20which%20slight%20input%20perturbations%20can%20lead%20to%20harmful%20or%0Amisleading%20outputs.%20A%20gradient-based%20defensive%20suffix%20generation%20algorithm%20is%0Adesigned%20to%20bolster%20the%20robustness%20of%20LLMs.%20By%20appending%20carefully%20optimized%0Adefensive%20suffixes%20to%20input%20prompts%2C%20the%20algorithm%20mitigates%20adversarial%0Ainfluences%20while%20preserving%20the%20models%27%20utility.%20To%20enhance%20adversarial%0Aunderstanding%2C%20a%20novel%20total%20loss%20function%20%28%24L_%7B%5Ctext%7Btotal%7D%7D%24%29%20combining%0Adefensive%20loss%20%28%24L_%7B%5Ctext%7Bdef%7D%7D%24%29%20and%20adversarial%20loss%20%28%24L_%7B%5Ctext%7Badv%7D%7D%24%29%0Agenerates%20defensive%20suffixes%20more%20effectively.%20Experimental%20evaluations%0Aconducted%20on%20open-source%20LLMs%20such%20as%20Gemma-7B%2C%20mistral-7B%2C%20Llama2-7B%2C%20and%0ALlama2-13B%20show%20that%20the%20proposed%20method%20reduces%20attack%20success%20rates%20%28ASR%29%20by%0Aan%20average%20of%2011%5C%25%20compared%20to%20models%20without%20defensive%20suffixes.%20Additionally%2C%0Athe%20perplexity%20score%20of%20Gemma-7B%20decreased%20from%206.57%20to%203.93%20when%20applying%20the%0Adefensive%20suffix%20generated%20by%20openELM-270M.%20Furthermore%2C%20TruthfulQA%20evaluations%0Ademonstrate%20consistent%20improvements%20with%20Truthfulness%20scores%20increasing%20by%20up%0Ato%2010%5C%25%20across%20tested%20configurations.%20This%20approach%20significantly%20enhances%20the%0Asecurity%20of%20LLMs%20in%20critical%20applications%20without%20requiring%20extensive%0Aretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Adversarial%2520Attacks%2520in%2520LLMs%2520through%2520Defensive%2520Suffix%250A%2520%2520Generation%26entry.906535625%3DMinkyoung%2520Kim%2520and%2520Yunha%2520Kim%2520and%2520Hyeram%2520Seo%2520and%2520Heejung%2520Choi%2520and%2520Jiye%2520Han%2520and%2520Gaeun%2520Kee%2520and%2520Soyoung%2520Ko%2520and%2520HyoJe%2520Jung%2520and%2520Byeolhee%2520Kim%2520and%2520Young-Hak%2520Kim%2520and%2520Sanghyun%2520Park%2520and%2520Tae%2520Joon%2520Jun%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520exhibited%2520outstanding%2520performance%2520in%250Anatural%2520language%2520processing%2520tasks.%2520However%252C%2520these%2520models%2520remain%2520susceptible%2520to%250Aadversarial%2520attacks%2520in%2520which%2520slight%2520input%2520perturbations%2520can%2520lead%2520to%2520harmful%2520or%250Amisleading%2520outputs.%2520A%2520gradient-based%2520defensive%2520suffix%2520generation%2520algorithm%2520is%250Adesigned%2520to%2520bolster%2520the%2520robustness%2520of%2520LLMs.%2520By%2520appending%2520carefully%2520optimized%250Adefensive%2520suffixes%2520to%2520input%2520prompts%252C%2520the%2520algorithm%2520mitigates%2520adversarial%250Ainfluences%2520while%2520preserving%2520the%2520models%2527%2520utility.%2520To%2520enhance%2520adversarial%250Aunderstanding%252C%2520a%2520novel%2520total%2520loss%2520function%2520%2528%2524L_%257B%255Ctext%257Btotal%257D%257D%2524%2529%2520combining%250Adefensive%2520loss%2520%2528%2524L_%257B%255Ctext%257Bdef%257D%257D%2524%2529%2520and%2520adversarial%2520loss%2520%2528%2524L_%257B%255Ctext%257Badv%257D%257D%2524%2529%250Agenerates%2520defensive%2520suffixes%2520more%2520effectively.%2520Experimental%2520evaluations%250Aconducted%2520on%2520open-source%2520LLMs%2520such%2520as%2520Gemma-7B%252C%2520mistral-7B%252C%2520Llama2-7B%252C%2520and%250ALlama2-13B%2520show%2520that%2520the%2520proposed%2520method%2520reduces%2520attack%2520success%2520rates%2520%2528ASR%2529%2520by%250Aan%2520average%2520of%252011%255C%2525%2520compared%2520to%2520models%2520without%2520defensive%2520suffixes.%2520Additionally%252C%250Athe%2520perplexity%2520score%2520of%2520Gemma-7B%2520decreased%2520from%25206.57%2520to%25203.93%2520when%2520applying%2520the%250Adefensive%2520suffix%2520generated%2520by%2520openELM-270M.%2520Furthermore%252C%2520TruthfulQA%2520evaluations%250Ademonstrate%2520consistent%2520improvements%2520with%2520Truthfulness%2520scores%2520increasing%2520by%2520up%250Ato%252010%255C%2525%2520across%2520tested%2520configurations.%2520This%2520approach%2520significantly%2520enhances%2520the%250Asecurity%2520of%2520LLMs%2520in%2520critical%2520applications%2520without%2520requiring%2520extensive%250Aretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Adversarial%20Attacks%20in%20LLMs%20through%20Defensive%20Suffix%0A%20%20Generation&entry.906535625=Minkyoung%20Kim%20and%20Yunha%20Kim%20and%20Hyeram%20Seo%20and%20Heejung%20Choi%20and%20Jiye%20Han%20and%20Gaeun%20Kee%20and%20Soyoung%20Ko%20and%20HyoJe%20Jung%20and%20Byeolhee%20Kim%20and%20Young-Hak%20Kim%20and%20Sanghyun%20Park%20and%20Tae%20Joon%20Jun&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20outstanding%20performance%20in%0Anatural%20language%20processing%20tasks.%20However%2C%20these%20models%20remain%20susceptible%20to%0Aadversarial%20attacks%20in%20which%20slight%20input%20perturbations%20can%20lead%20to%20harmful%20or%0Amisleading%20outputs.%20A%20gradient-based%20defensive%20suffix%20generation%20algorithm%20is%0Adesigned%20to%20bolster%20the%20robustness%20of%20LLMs.%20By%20appending%20carefully%20optimized%0Adefensive%20suffixes%20to%20input%20prompts%2C%20the%20algorithm%20mitigates%20adversarial%0Ainfluences%20while%20preserving%20the%20models%27%20utility.%20To%20enhance%20adversarial%0Aunderstanding%2C%20a%20novel%20total%20loss%20function%20%28%24L_%7B%5Ctext%7Btotal%7D%7D%24%29%20combining%0Adefensive%20loss%20%28%24L_%7B%5Ctext%7Bdef%7D%7D%24%29%20and%20adversarial%20loss%20%28%24L_%7B%5Ctext%7Badv%7D%7D%24%29%0Agenerates%20defensive%20suffixes%20more%20effectively.%20Experimental%20evaluations%0Aconducted%20on%20open-source%20LLMs%20such%20as%20Gemma-7B%2C%20mistral-7B%2C%20Llama2-7B%2C%20and%0ALlama2-13B%20show%20that%20the%20proposed%20method%20reduces%20attack%20success%20rates%20%28ASR%29%20by%0Aan%20average%20of%2011%5C%25%20compared%20to%20models%20without%20defensive%20suffixes.%20Additionally%2C%0Athe%20perplexity%20score%20of%20Gemma-7B%20decreased%20from%206.57%20to%203.93%20when%20applying%20the%0Adefensive%20suffix%20generated%20by%20openELM-270M.%20Furthermore%2C%20TruthfulQA%20evaluations%0Ademonstrate%20consistent%20improvements%20with%20Truthfulness%20scores%20increasing%20by%20up%0Ato%2010%5C%25%20across%20tested%20configurations.%20This%20approach%20significantly%20enhances%20the%0Asecurity%20of%20LLMs%20in%20critical%20applications%20without%20requiring%20extensive%0Aretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13705v1&entry.124074799=Read"},
{"title": "Discovering maximally consistent distribution of causal tournaments with\n  Large Language Models", "author": "Federico Baldo and Simon Ferreira and Charles K. Assaad", "abstract": "  Causal discovery is essential for understanding complex systems, yet\ntraditional methods often depend on strong, untestable assumptions, making the\nprocess challenging. Large Language Models (LLMs) present a promising\nalternative for extracting causal insights from text-based metadata, which\nconsolidates domain expertise. However, LLMs are prone to unreliability and\nhallucinations, necessitating strategies that account for their limitations.\nOne such strategy involves leveraging a consistency measure to evaluate\nreliability. Additionally, most text metadata does not clearly distinguish\ndirect causal relationships from indirect ones, further complicating the\ninference of causal graphs. As a result, focusing on causal orderings, rather\nthan causal graphs, emerges as a more practical and robust approach. We propose\na novel method to derive a distribution of acyclic tournaments (representing\nplausible causal orders) that maximizes a consistency score. Our approach\nbegins by computing pairwise consistency scores between variables, yielding a\ncyclic tournament that aggregates these scores. From this structure, we\nidentify optimal acyclic tournaments compatible with the original tournament,\nprioritizing those that maximize consistency across all configurations. We\ntested our method on both classical and well-established bechmarks, as well as\nreal-world datasets from epidemiology and public health. Our results\ndemonstrate the effectiveness of our approach in recovering distributions\ncausal orders with minimal error.\n", "link": "http://arxiv.org/abs/2412.14019v1", "date": "2024-12-18", "relevancy": 1.2533, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4393}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4137}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20maximally%20consistent%20distribution%20of%20causal%20tournaments%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20Discovering%20maximally%20consistent%20distribution%20of%20causal%20tournaments%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Federico%20Baldo%20and%20Simon%20Ferreira%20and%20Charles%20K.%20Assaad%0AAbstract%3A%20%20%20Causal%20discovery%20is%20essential%20for%20understanding%20complex%20systems%2C%20yet%0Atraditional%20methods%20often%20depend%20on%20strong%2C%20untestable%20assumptions%2C%20making%20the%0Aprocess%20challenging.%20Large%20Language%20Models%20%28LLMs%29%20present%20a%20promising%0Aalternative%20for%20extracting%20causal%20insights%20from%20text-based%20metadata%2C%20which%0Aconsolidates%20domain%20expertise.%20However%2C%20LLMs%20are%20prone%20to%20unreliability%20and%0Ahallucinations%2C%20necessitating%20strategies%20that%20account%20for%20their%20limitations.%0AOne%20such%20strategy%20involves%20leveraging%20a%20consistency%20measure%20to%20evaluate%0Areliability.%20Additionally%2C%20most%20text%20metadata%20does%20not%20clearly%20distinguish%0Adirect%20causal%20relationships%20from%20indirect%20ones%2C%20further%20complicating%20the%0Ainference%20of%20causal%20graphs.%20As%20a%20result%2C%20focusing%20on%20causal%20orderings%2C%20rather%0Athan%20causal%20graphs%2C%20emerges%20as%20a%20more%20practical%20and%20robust%20approach.%20We%20propose%0Aa%20novel%20method%20to%20derive%20a%20distribution%20of%20acyclic%20tournaments%20%28representing%0Aplausible%20causal%20orders%29%20that%20maximizes%20a%20consistency%20score.%20Our%20approach%0Abegins%20by%20computing%20pairwise%20consistency%20scores%20between%20variables%2C%20yielding%20a%0Acyclic%20tournament%20that%20aggregates%20these%20scores.%20From%20this%20structure%2C%20we%0Aidentify%20optimal%20acyclic%20tournaments%20compatible%20with%20the%20original%20tournament%2C%0Aprioritizing%20those%20that%20maximize%20consistency%20across%20all%20configurations.%20We%0Atested%20our%20method%20on%20both%20classical%20and%20well-established%20bechmarks%2C%20as%20well%20as%0Areal-world%20datasets%20from%20epidemiology%20and%20public%20health.%20Our%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20recovering%20distributions%0Acausal%20orders%20with%20minimal%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520maximally%2520consistent%2520distribution%2520of%2520causal%2520tournaments%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DFederico%2520Baldo%2520and%2520Simon%2520Ferreira%2520and%2520Charles%2520K.%2520Assaad%26entry.1292438233%3D%2520%2520Causal%2520discovery%2520is%2520essential%2520for%2520understanding%2520complex%2520systems%252C%2520yet%250Atraditional%2520methods%2520often%2520depend%2520on%2520strong%252C%2520untestable%2520assumptions%252C%2520making%2520the%250Aprocess%2520challenging.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520present%2520a%2520promising%250Aalternative%2520for%2520extracting%2520causal%2520insights%2520from%2520text-based%2520metadata%252C%2520which%250Aconsolidates%2520domain%2520expertise.%2520However%252C%2520LLMs%2520are%2520prone%2520to%2520unreliability%2520and%250Ahallucinations%252C%2520necessitating%2520strategies%2520that%2520account%2520for%2520their%2520limitations.%250AOne%2520such%2520strategy%2520involves%2520leveraging%2520a%2520consistency%2520measure%2520to%2520evaluate%250Areliability.%2520Additionally%252C%2520most%2520text%2520metadata%2520does%2520not%2520clearly%2520distinguish%250Adirect%2520causal%2520relationships%2520from%2520indirect%2520ones%252C%2520further%2520complicating%2520the%250Ainference%2520of%2520causal%2520graphs.%2520As%2520a%2520result%252C%2520focusing%2520on%2520causal%2520orderings%252C%2520rather%250Athan%2520causal%2520graphs%252C%2520emerges%2520as%2520a%2520more%2520practical%2520and%2520robust%2520approach.%2520We%2520propose%250Aa%2520novel%2520method%2520to%2520derive%2520a%2520distribution%2520of%2520acyclic%2520tournaments%2520%2528representing%250Aplausible%2520causal%2520orders%2529%2520that%2520maximizes%2520a%2520consistency%2520score.%2520Our%2520approach%250Abegins%2520by%2520computing%2520pairwise%2520consistency%2520scores%2520between%2520variables%252C%2520yielding%2520a%250Acyclic%2520tournament%2520that%2520aggregates%2520these%2520scores.%2520From%2520this%2520structure%252C%2520we%250Aidentify%2520optimal%2520acyclic%2520tournaments%2520compatible%2520with%2520the%2520original%2520tournament%252C%250Aprioritizing%2520those%2520that%2520maximize%2520consistency%2520across%2520all%2520configurations.%2520We%250Atested%2520our%2520method%2520on%2520both%2520classical%2520and%2520well-established%2520bechmarks%252C%2520as%2520well%2520as%250Areal-world%2520datasets%2520from%2520epidemiology%2520and%2520public%2520health.%2520Our%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520recovering%2520distributions%250Acausal%2520orders%2520with%2520minimal%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20maximally%20consistent%20distribution%20of%20causal%20tournaments%20with%0A%20%20Large%20Language%20Models&entry.906535625=Federico%20Baldo%20and%20Simon%20Ferreira%20and%20Charles%20K.%20Assaad&entry.1292438233=%20%20Causal%20discovery%20is%20essential%20for%20understanding%20complex%20systems%2C%20yet%0Atraditional%20methods%20often%20depend%20on%20strong%2C%20untestable%20assumptions%2C%20making%20the%0Aprocess%20challenging.%20Large%20Language%20Models%20%28LLMs%29%20present%20a%20promising%0Aalternative%20for%20extracting%20causal%20insights%20from%20text-based%20metadata%2C%20which%0Aconsolidates%20domain%20expertise.%20However%2C%20LLMs%20are%20prone%20to%20unreliability%20and%0Ahallucinations%2C%20necessitating%20strategies%20that%20account%20for%20their%20limitations.%0AOne%20such%20strategy%20involves%20leveraging%20a%20consistency%20measure%20to%20evaluate%0Areliability.%20Additionally%2C%20most%20text%20metadata%20does%20not%20clearly%20distinguish%0Adirect%20causal%20relationships%20from%20indirect%20ones%2C%20further%20complicating%20the%0Ainference%20of%20causal%20graphs.%20As%20a%20result%2C%20focusing%20on%20causal%20orderings%2C%20rather%0Athan%20causal%20graphs%2C%20emerges%20as%20a%20more%20practical%20and%20robust%20approach.%20We%20propose%0Aa%20novel%20method%20to%20derive%20a%20distribution%20of%20acyclic%20tournaments%20%28representing%0Aplausible%20causal%20orders%29%20that%20maximizes%20a%20consistency%20score.%20Our%20approach%0Abegins%20by%20computing%20pairwise%20consistency%20scores%20between%20variables%2C%20yielding%20a%0Acyclic%20tournament%20that%20aggregates%20these%20scores.%20From%20this%20structure%2C%20we%0Aidentify%20optimal%20acyclic%20tournaments%20compatible%20with%20the%20original%20tournament%2C%0Aprioritizing%20those%20that%20maximize%20consistency%20across%20all%20configurations.%20We%0Atested%20our%20method%20on%20both%20classical%20and%20well-established%20bechmarks%2C%20as%20well%20as%0Areal-world%20datasets%20from%20epidemiology%20and%20public%20health.%20Our%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20in%20recovering%20distributions%0Acausal%20orders%20with%20minimal%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14019v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


