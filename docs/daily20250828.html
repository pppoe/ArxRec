<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250827.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction\n  with Large Gaussian Reconstruction Transformers", "author": "Yue Wu and Yufan Wu and Wen Li and Yuxi Lu and Kairui Feng and Xuanhong Chen", "abstract": "  Despite significant progress in 3D avatar reconstruction, it still faces\nchallenges such as high time complexity, sensitivity to data quality, and low\ndata utilization. We propose FastAvatar, a feedforward 3D avatar framework\ncapable of flexibly leveraging diverse daily recordings (e.g., a single image,\nmulti-view observations, or monocular video) to reconstruct a high-quality 3D\nGaussian Splatting (3DGS) model within seconds, using only a single unified\nmodel. FastAvatar's core is a Large Gaussian Reconstruction Transformer\nfeaturing three key designs: First, a variant VGGT-style transformer\narchitecture aggregating multi-frame cues while injecting initial 3D prompt to\npredict an aggregatable canonical 3DGS representation; Second, multi-granular\nguidance encoding (camera pose, FLAME expression, head pose) mitigating\nanimation-induced misalignment for variable-length inputs; Third, incremental\nGaussian aggregation via landmark tracking and sliced fusion losses.\nIntegrating these features, FastAvatar enables incremental reconstruction,\ni.e., improving quality with more observations, unlike prior work wasting input\ndata. This yields a quality-speed-tunable paradigm for highly usable avatar\nmodeling. Extensive experiments show that FastAvatar has higher quality and\nhighly competitive speed compared to existing methods.\n", "link": "http://arxiv.org/abs/2508.19754v1", "date": "2025-08-27", "relevancy": 3.4783, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.706}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.706}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastAvatar%3A%20Towards%20Unified%20Fast%20High-Fidelity%203D%20Avatar%20Reconstruction%0A%20%20with%20Large%20Gaussian%20Reconstruction%20Transformers&body=Title%3A%20FastAvatar%3A%20Towards%20Unified%20Fast%20High-Fidelity%203D%20Avatar%20Reconstruction%0A%20%20with%20Large%20Gaussian%20Reconstruction%20Transformers%0AAuthor%3A%20Yue%20Wu%20and%20Yufan%20Wu%20and%20Wen%20Li%20and%20Yuxi%20Lu%20and%20Kairui%20Feng%20and%20Xuanhong%20Chen%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%203D%20avatar%20reconstruction%2C%20it%20still%20faces%0Achallenges%20such%20as%20high%20time%20complexity%2C%20sensitivity%20to%20data%20quality%2C%20and%20low%0Adata%20utilization.%20We%20propose%20FastAvatar%2C%20a%20feedforward%203D%20avatar%20framework%0Acapable%20of%20flexibly%20leveraging%20diverse%20daily%20recordings%20%28e.g.%2C%20a%20single%20image%2C%0Amulti-view%20observations%2C%20or%20monocular%20video%29%20to%20reconstruct%20a%20high-quality%203D%0AGaussian%20Splatting%20%283DGS%29%20model%20within%20seconds%2C%20using%20only%20a%20single%20unified%0Amodel.%20FastAvatar%27s%20core%20is%20a%20Large%20Gaussian%20Reconstruction%20Transformer%0Afeaturing%20three%20key%20designs%3A%20First%2C%20a%20variant%20VGGT-style%20transformer%0Aarchitecture%20aggregating%20multi-frame%20cues%20while%20injecting%20initial%203D%20prompt%20to%0Apredict%20an%20aggregatable%20canonical%203DGS%20representation%3B%20Second%2C%20multi-granular%0Aguidance%20encoding%20%28camera%20pose%2C%20FLAME%20expression%2C%20head%20pose%29%20mitigating%0Aanimation-induced%20misalignment%20for%20variable-length%20inputs%3B%20Third%2C%20incremental%0AGaussian%20aggregation%20via%20landmark%20tracking%20and%20sliced%20fusion%20losses.%0AIntegrating%20these%20features%2C%20FastAvatar%20enables%20incremental%20reconstruction%2C%0Ai.e.%2C%20improving%20quality%20with%20more%20observations%2C%20unlike%20prior%20work%20wasting%20input%0Adata.%20This%20yields%20a%20quality-speed-tunable%20paradigm%20for%20highly%20usable%20avatar%0Amodeling.%20Extensive%20experiments%20show%20that%20FastAvatar%20has%20higher%20quality%20and%0Ahighly%20competitive%20speed%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastAvatar%253A%2520Towards%2520Unified%2520Fast%2520High-Fidelity%25203D%2520Avatar%2520Reconstruction%250A%2520%2520with%2520Large%2520Gaussian%2520Reconstruction%2520Transformers%26entry.906535625%3DYue%2520Wu%2520and%2520Yufan%2520Wu%2520and%2520Wen%2520Li%2520and%2520Yuxi%2520Lu%2520and%2520Kairui%2520Feng%2520and%2520Xuanhong%2520Chen%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%25203D%2520avatar%2520reconstruction%252C%2520it%2520still%2520faces%250Achallenges%2520such%2520as%2520high%2520time%2520complexity%252C%2520sensitivity%2520to%2520data%2520quality%252C%2520and%2520low%250Adata%2520utilization.%2520We%2520propose%2520FastAvatar%252C%2520a%2520feedforward%25203D%2520avatar%2520framework%250Acapable%2520of%2520flexibly%2520leveraging%2520diverse%2520daily%2520recordings%2520%2528e.g.%252C%2520a%2520single%2520image%252C%250Amulti-view%2520observations%252C%2520or%2520monocular%2520video%2529%2520to%2520reconstruct%2520a%2520high-quality%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520model%2520within%2520seconds%252C%2520using%2520only%2520a%2520single%2520unified%250Amodel.%2520FastAvatar%2527s%2520core%2520is%2520a%2520Large%2520Gaussian%2520Reconstruction%2520Transformer%250Afeaturing%2520three%2520key%2520designs%253A%2520First%252C%2520a%2520variant%2520VGGT-style%2520transformer%250Aarchitecture%2520aggregating%2520multi-frame%2520cues%2520while%2520injecting%2520initial%25203D%2520prompt%2520to%250Apredict%2520an%2520aggregatable%2520canonical%25203DGS%2520representation%253B%2520Second%252C%2520multi-granular%250Aguidance%2520encoding%2520%2528camera%2520pose%252C%2520FLAME%2520expression%252C%2520head%2520pose%2529%2520mitigating%250Aanimation-induced%2520misalignment%2520for%2520variable-length%2520inputs%253B%2520Third%252C%2520incremental%250AGaussian%2520aggregation%2520via%2520landmark%2520tracking%2520and%2520sliced%2520fusion%2520losses.%250AIntegrating%2520these%2520features%252C%2520FastAvatar%2520enables%2520incremental%2520reconstruction%252C%250Ai.e.%252C%2520improving%2520quality%2520with%2520more%2520observations%252C%2520unlike%2520prior%2520work%2520wasting%2520input%250Adata.%2520This%2520yields%2520a%2520quality-speed-tunable%2520paradigm%2520for%2520highly%2520usable%2520avatar%250Amodeling.%2520Extensive%2520experiments%2520show%2520that%2520FastAvatar%2520has%2520higher%2520quality%2520and%250Ahighly%2520competitive%2520speed%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastAvatar%3A%20Towards%20Unified%20Fast%20High-Fidelity%203D%20Avatar%20Reconstruction%0A%20%20with%20Large%20Gaussian%20Reconstruction%20Transformers&entry.906535625=Yue%20Wu%20and%20Yufan%20Wu%20and%20Wen%20Li%20and%20Yuxi%20Lu%20and%20Kairui%20Feng%20and%20Xuanhong%20Chen&entry.1292438233=%20%20Despite%20significant%20progress%20in%203D%20avatar%20reconstruction%2C%20it%20still%20faces%0Achallenges%20such%20as%20high%20time%20complexity%2C%20sensitivity%20to%20data%20quality%2C%20and%20low%0Adata%20utilization.%20We%20propose%20FastAvatar%2C%20a%20feedforward%203D%20avatar%20framework%0Acapable%20of%20flexibly%20leveraging%20diverse%20daily%20recordings%20%28e.g.%2C%20a%20single%20image%2C%0Amulti-view%20observations%2C%20or%20monocular%20video%29%20to%20reconstruct%20a%20high-quality%203D%0AGaussian%20Splatting%20%283DGS%29%20model%20within%20seconds%2C%20using%20only%20a%20single%20unified%0Amodel.%20FastAvatar%27s%20core%20is%20a%20Large%20Gaussian%20Reconstruction%20Transformer%0Afeaturing%20three%20key%20designs%3A%20First%2C%20a%20variant%20VGGT-style%20transformer%0Aarchitecture%20aggregating%20multi-frame%20cues%20while%20injecting%20initial%203D%20prompt%20to%0Apredict%20an%20aggregatable%20canonical%203DGS%20representation%3B%20Second%2C%20multi-granular%0Aguidance%20encoding%20%28camera%20pose%2C%20FLAME%20expression%2C%20head%20pose%29%20mitigating%0Aanimation-induced%20misalignment%20for%20variable-length%20inputs%3B%20Third%2C%20incremental%0AGaussian%20aggregation%20via%20landmark%20tracking%20and%20sliced%20fusion%20losses.%0AIntegrating%20these%20features%2C%20FastAvatar%20enables%20incremental%20reconstruction%2C%0Ai.e.%2C%20improving%20quality%20with%20more%20observations%2C%20unlike%20prior%20work%20wasting%20input%0Adata.%20This%20yields%20a%20quality-speed-tunable%20paradigm%20for%20highly%20usable%20avatar%0Amodeling.%20Extensive%20experiments%20show%20that%20FastAvatar%20has%20higher%20quality%20and%0Ahighly%20competitive%20speed%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19754v1&entry.124074799=Read"},
{"title": "MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for\n  High-Fidelity Dynamic Scene Reconstruction", "author": "Han Jiao and Jiakai Sun and Yexing Xu and Lei Zhao and Wei Xing and Huaizhong Lin", "abstract": "  3D Gaussian Splatting, known for enabling high-quality static scene\nreconstruction with fast rendering, is increasingly being applied to dynamic\nscene reconstruction. A common strategy involves learning a deformation field\nto model the temporal changes of a canonical set of 3D Gaussians. However,\nthese deformation-based methods often produce blurred renderings and lose fine\nmotion details in highly dynamic regions due to the inherent limitations of a\nsingle, unified model in representing diverse motion patterns. To address these\nchallenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian\nSplatting (MAPo), a novel framework for high-fidelity dynamic scene\nreconstruction. Its core is a dynamic score-based partitioning strategy that\ndistinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D\nGaussians, we recursively partition them temporally and duplicate their\ndeformation networks for each new temporal segment, enabling specialized\nmodeling to capture intricate motion details. Concurrently, low-dynamic 3DGs\nare treated as static to reduce computational costs. However, this temporal\npartitioning strategy for high-dynamic 3DGs can introduce visual\ndiscontinuities across frames at the partition boundaries. To address this, we\nintroduce a cross-frame consistency loss, which not only ensures visual\ncontinuity but also further enhances rendering quality. Extensive experiments\ndemonstrate that MAPo achieves superior rendering quality compared to baselines\nwhile maintaining comparable computational costs, particularly in regions with\ncomplex or rapid motions.\n", "link": "http://arxiv.org/abs/2508.19786v1", "date": "2025-08-27", "relevancy": 3.4033, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7051}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6926}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPo%20%3A%20Motion-Aware%20Partitioning%20of%20Deformable%203D%20Gaussian%20Splatting%20for%0A%20%20High-Fidelity%20Dynamic%20Scene%20Reconstruction&body=Title%3A%20MAPo%20%3A%20Motion-Aware%20Partitioning%20of%20Deformable%203D%20Gaussian%20Splatting%20for%0A%20%20High-Fidelity%20Dynamic%20Scene%20Reconstruction%0AAuthor%3A%20Han%20Jiao%20and%20Jiakai%20Sun%20and%20Yexing%20Xu%20and%20Lei%20Zhao%20and%20Wei%20Xing%20and%20Huaizhong%20Lin%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%2C%20known%20for%20enabling%20high-quality%20static%20scene%0Areconstruction%20with%20fast%20rendering%2C%20is%20increasingly%20being%20applied%20to%20dynamic%0Ascene%20reconstruction.%20A%20common%20strategy%20involves%20learning%20a%20deformation%20field%0Ato%20model%20the%20temporal%20changes%20of%20a%20canonical%20set%20of%203D%20Gaussians.%20However%2C%0Athese%20deformation-based%20methods%20often%20produce%20blurred%20renderings%20and%20lose%20fine%0Amotion%20details%20in%20highly%20dynamic%20regions%20due%20to%20the%20inherent%20limitations%20of%20a%0Asingle%2C%20unified%20model%20in%20representing%20diverse%20motion%20patterns.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Motion-Aware%20Partitioning%20of%20Deformable%203D%20Gaussian%0ASplatting%20%28MAPo%29%2C%20a%20novel%20framework%20for%20high-fidelity%20dynamic%20scene%0Areconstruction.%20Its%20core%20is%20a%20dynamic%20score-based%20partitioning%20strategy%20that%0Adistinguishes%20between%20high-%20and%20low-dynamic%203D%20Gaussians.%20For%20high-dynamic%203D%0AGaussians%2C%20we%20recursively%20partition%20them%20temporally%20and%20duplicate%20their%0Adeformation%20networks%20for%20each%20new%20temporal%20segment%2C%20enabling%20specialized%0Amodeling%20to%20capture%20intricate%20motion%20details.%20Concurrently%2C%20low-dynamic%203DGs%0Aare%20treated%20as%20static%20to%20reduce%20computational%20costs.%20However%2C%20this%20temporal%0Apartitioning%20strategy%20for%20high-dynamic%203DGs%20can%20introduce%20visual%0Adiscontinuities%20across%20frames%20at%20the%20partition%20boundaries.%20To%20address%20this%2C%20we%0Aintroduce%20a%20cross-frame%20consistency%20loss%2C%20which%20not%20only%20ensures%20visual%0Acontinuity%20but%20also%20further%20enhances%20rendering%20quality.%20Extensive%20experiments%0Ademonstrate%20that%20MAPo%20achieves%20superior%20rendering%20quality%20compared%20to%20baselines%0Awhile%20maintaining%20comparable%20computational%20costs%2C%20particularly%20in%20regions%20with%0Acomplex%20or%20rapid%20motions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19786v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPo%2520%253A%2520Motion-Aware%2520Partitioning%2520of%2520Deformable%25203D%2520Gaussian%2520Splatting%2520for%250A%2520%2520High-Fidelity%2520Dynamic%2520Scene%2520Reconstruction%26entry.906535625%3DHan%2520Jiao%2520and%2520Jiakai%2520Sun%2520and%2520Yexing%2520Xu%2520and%2520Lei%2520Zhao%2520and%2520Wei%2520Xing%2520and%2520Huaizhong%2520Lin%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%252C%2520known%2520for%2520enabling%2520high-quality%2520static%2520scene%250Areconstruction%2520with%2520fast%2520rendering%252C%2520is%2520increasingly%2520being%2520applied%2520to%2520dynamic%250Ascene%2520reconstruction.%2520A%2520common%2520strategy%2520involves%2520learning%2520a%2520deformation%2520field%250Ato%2520model%2520the%2520temporal%2520changes%2520of%2520a%2520canonical%2520set%2520of%25203D%2520Gaussians.%2520However%252C%250Athese%2520deformation-based%2520methods%2520often%2520produce%2520blurred%2520renderings%2520and%2520lose%2520fine%250Amotion%2520details%2520in%2520highly%2520dynamic%2520regions%2520due%2520to%2520the%2520inherent%2520limitations%2520of%2520a%250Asingle%252C%2520unified%2520model%2520in%2520representing%2520diverse%2520motion%2520patterns.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520Motion-Aware%2520Partitioning%2520of%2520Deformable%25203D%2520Gaussian%250ASplatting%2520%2528MAPo%2529%252C%2520a%2520novel%2520framework%2520for%2520high-fidelity%2520dynamic%2520scene%250Areconstruction.%2520Its%2520core%2520is%2520a%2520dynamic%2520score-based%2520partitioning%2520strategy%2520that%250Adistinguishes%2520between%2520high-%2520and%2520low-dynamic%25203D%2520Gaussians.%2520For%2520high-dynamic%25203D%250AGaussians%252C%2520we%2520recursively%2520partition%2520them%2520temporally%2520and%2520duplicate%2520their%250Adeformation%2520networks%2520for%2520each%2520new%2520temporal%2520segment%252C%2520enabling%2520specialized%250Amodeling%2520to%2520capture%2520intricate%2520motion%2520details.%2520Concurrently%252C%2520low-dynamic%25203DGs%250Aare%2520treated%2520as%2520static%2520to%2520reduce%2520computational%2520costs.%2520However%252C%2520this%2520temporal%250Apartitioning%2520strategy%2520for%2520high-dynamic%25203DGs%2520can%2520introduce%2520visual%250Adiscontinuities%2520across%2520frames%2520at%2520the%2520partition%2520boundaries.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520cross-frame%2520consistency%2520loss%252C%2520which%2520not%2520only%2520ensures%2520visual%250Acontinuity%2520but%2520also%2520further%2520enhances%2520rendering%2520quality.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520MAPo%2520achieves%2520superior%2520rendering%2520quality%2520compared%2520to%2520baselines%250Awhile%2520maintaining%2520comparable%2520computational%2520costs%252C%2520particularly%2520in%2520regions%2520with%250Acomplex%2520or%2520rapid%2520motions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19786v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPo%20%3A%20Motion-Aware%20Partitioning%20of%20Deformable%203D%20Gaussian%20Splatting%20for%0A%20%20High-Fidelity%20Dynamic%20Scene%20Reconstruction&entry.906535625=Han%20Jiao%20and%20Jiakai%20Sun%20and%20Yexing%20Xu%20and%20Lei%20Zhao%20and%20Wei%20Xing%20and%20Huaizhong%20Lin&entry.1292438233=%20%203D%20Gaussian%20Splatting%2C%20known%20for%20enabling%20high-quality%20static%20scene%0Areconstruction%20with%20fast%20rendering%2C%20is%20increasingly%20being%20applied%20to%20dynamic%0Ascene%20reconstruction.%20A%20common%20strategy%20involves%20learning%20a%20deformation%20field%0Ato%20model%20the%20temporal%20changes%20of%20a%20canonical%20set%20of%203D%20Gaussians.%20However%2C%0Athese%20deformation-based%20methods%20often%20produce%20blurred%20renderings%20and%20lose%20fine%0Amotion%20details%20in%20highly%20dynamic%20regions%20due%20to%20the%20inherent%20limitations%20of%20a%0Asingle%2C%20unified%20model%20in%20representing%20diverse%20motion%20patterns.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Motion-Aware%20Partitioning%20of%20Deformable%203D%20Gaussian%0ASplatting%20%28MAPo%29%2C%20a%20novel%20framework%20for%20high-fidelity%20dynamic%20scene%0Areconstruction.%20Its%20core%20is%20a%20dynamic%20score-based%20partitioning%20strategy%20that%0Adistinguishes%20between%20high-%20and%20low-dynamic%203D%20Gaussians.%20For%20high-dynamic%203D%0AGaussians%2C%20we%20recursively%20partition%20them%20temporally%20and%20duplicate%20their%0Adeformation%20networks%20for%20each%20new%20temporal%20segment%2C%20enabling%20specialized%0Amodeling%20to%20capture%20intricate%20motion%20details.%20Concurrently%2C%20low-dynamic%203DGs%0Aare%20treated%20as%20static%20to%20reduce%20computational%20costs.%20However%2C%20this%20temporal%0Apartitioning%20strategy%20for%20high-dynamic%203DGs%20can%20introduce%20visual%0Adiscontinuities%20across%20frames%20at%20the%20partition%20boundaries.%20To%20address%20this%2C%20we%0Aintroduce%20a%20cross-frame%20consistency%20loss%2C%20which%20not%20only%20ensures%20visual%0Acontinuity%20but%20also%20further%20enhances%20rendering%20quality.%20Extensive%20experiments%0Ademonstrate%20that%20MAPo%20achieves%20superior%20rendering%20quality%20compared%20to%20baselines%0Awhile%20maintaining%20comparable%20computational%20costs%2C%20particularly%20in%20regions%20with%0Acomplex%20or%20rapid%20motions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19786v1&entry.124074799=Read"},
{"title": "Seam360GS: Seamless 360\u00b0 Gaussian Splatting from Real-World\n  Omnidirectional Images", "author": "Changha Shin and Woong Oh Cho and Seon Joo Kim", "abstract": "  360-degree visual content is widely shared on platforms such as YouTube and\nplays a central role in virtual reality, robotics, and autonomous navigation.\nHowever, consumer-grade dual-fisheye systems consistently yield imperfect\npanoramas due to inherent lens separation and angular distortions. In this\nwork, we introduce a novel calibration framework that incorporates a\ndual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach\nnot only simulates the realistic visual artifacts produced by dual-fisheye\ncameras but also enables the synthesis of seamlessly rendered 360-degree\nimages. By jointly optimizing 3D Gaussian parameters alongside calibration\nvariables that emulate lens gaps and angular distortions, our framework\ntransforms imperfect omnidirectional inputs into flawless novel view synthesis.\nExtensive evaluations on real-world datasets confirm that our method produces\nseamless renderings-even from imperfect images-and outperforms existing\n360-degree rendering models.\n", "link": "http://arxiv.org/abs/2508.20080v1", "date": "2025-08-27", "relevancy": 3.2535, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6679}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6539}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seam360GS%3A%20Seamless%20360%C2%B0%20Gaussian%20Splatting%20from%20Real-World%0A%20%20Omnidirectional%20Images&body=Title%3A%20Seam360GS%3A%20Seamless%20360%C2%B0%20Gaussian%20Splatting%20from%20Real-World%0A%20%20Omnidirectional%20Images%0AAuthor%3A%20Changha%20Shin%20and%20Woong%20Oh%20Cho%20and%20Seon%20Joo%20Kim%0AAbstract%3A%20%20%20360-degree%20visual%20content%20is%20widely%20shared%20on%20platforms%20such%20as%20YouTube%20and%0Aplays%20a%20central%20role%20in%20virtual%20reality%2C%20robotics%2C%20and%20autonomous%20navigation.%0AHowever%2C%20consumer-grade%20dual-fisheye%20systems%20consistently%20yield%20imperfect%0Apanoramas%20due%20to%20inherent%20lens%20separation%20and%20angular%20distortions.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20calibration%20framework%20that%20incorporates%20a%0Adual-fisheye%20camera%20model%20into%20the%203D%20Gaussian%20splatting%20pipeline.%20Our%20approach%0Anot%20only%20simulates%20the%20realistic%20visual%20artifacts%20produced%20by%20dual-fisheye%0Acameras%20but%20also%20enables%20the%20synthesis%20of%20seamlessly%20rendered%20360-degree%0Aimages.%20By%20jointly%20optimizing%203D%20Gaussian%20parameters%20alongside%20calibration%0Avariables%20that%20emulate%20lens%20gaps%20and%20angular%20distortions%2C%20our%20framework%0Atransforms%20imperfect%20omnidirectional%20inputs%20into%20flawless%20novel%20view%20synthesis.%0AExtensive%20evaluations%20on%20real-world%20datasets%20confirm%20that%20our%20method%20produces%0Aseamless%20renderings-even%20from%20imperfect%20images-and%20outperforms%20existing%0A360-degree%20rendering%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeam360GS%253A%2520Seamless%2520360%25C2%25B0%2520Gaussian%2520Splatting%2520from%2520Real-World%250A%2520%2520Omnidirectional%2520Images%26entry.906535625%3DChangha%2520Shin%2520and%2520Woong%2520Oh%2520Cho%2520and%2520Seon%2520Joo%2520Kim%26entry.1292438233%3D%2520%2520360-degree%2520visual%2520content%2520is%2520widely%2520shared%2520on%2520platforms%2520such%2520as%2520YouTube%2520and%250Aplays%2520a%2520central%2520role%2520in%2520virtual%2520reality%252C%2520robotics%252C%2520and%2520autonomous%2520navigation.%250AHowever%252C%2520consumer-grade%2520dual-fisheye%2520systems%2520consistently%2520yield%2520imperfect%250Apanoramas%2520due%2520to%2520inherent%2520lens%2520separation%2520and%2520angular%2520distortions.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520a%2520novel%2520calibration%2520framework%2520that%2520incorporates%2520a%250Adual-fisheye%2520camera%2520model%2520into%2520the%25203D%2520Gaussian%2520splatting%2520pipeline.%2520Our%2520approach%250Anot%2520only%2520simulates%2520the%2520realistic%2520visual%2520artifacts%2520produced%2520by%2520dual-fisheye%250Acameras%2520but%2520also%2520enables%2520the%2520synthesis%2520of%2520seamlessly%2520rendered%2520360-degree%250Aimages.%2520By%2520jointly%2520optimizing%25203D%2520Gaussian%2520parameters%2520alongside%2520calibration%250Avariables%2520that%2520emulate%2520lens%2520gaps%2520and%2520angular%2520distortions%252C%2520our%2520framework%250Atransforms%2520imperfect%2520omnidirectional%2520inputs%2520into%2520flawless%2520novel%2520view%2520synthesis.%250AExtensive%2520evaluations%2520on%2520real-world%2520datasets%2520confirm%2520that%2520our%2520method%2520produces%250Aseamless%2520renderings-even%2520from%2520imperfect%2520images-and%2520outperforms%2520existing%250A360-degree%2520rendering%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seam360GS%3A%20Seamless%20360%C2%B0%20Gaussian%20Splatting%20from%20Real-World%0A%20%20Omnidirectional%20Images&entry.906535625=Changha%20Shin%20and%20Woong%20Oh%20Cho%20and%20Seon%20Joo%20Kim&entry.1292438233=%20%20360-degree%20visual%20content%20is%20widely%20shared%20on%20platforms%20such%20as%20YouTube%20and%0Aplays%20a%20central%20role%20in%20virtual%20reality%2C%20robotics%2C%20and%20autonomous%20navigation.%0AHowever%2C%20consumer-grade%20dual-fisheye%20systems%20consistently%20yield%20imperfect%0Apanoramas%20due%20to%20inherent%20lens%20separation%20and%20angular%20distortions.%20In%20this%0Awork%2C%20we%20introduce%20a%20novel%20calibration%20framework%20that%20incorporates%20a%0Adual-fisheye%20camera%20model%20into%20the%203D%20Gaussian%20splatting%20pipeline.%20Our%20approach%0Anot%20only%20simulates%20the%20realistic%20visual%20artifacts%20produced%20by%20dual-fisheye%0Acameras%20but%20also%20enables%20the%20synthesis%20of%20seamlessly%20rendered%20360-degree%0Aimages.%20By%20jointly%20optimizing%203D%20Gaussian%20parameters%20alongside%20calibration%0Avariables%20that%20emulate%20lens%20gaps%20and%20angular%20distortions%2C%20our%20framework%0Atransforms%20imperfect%20omnidirectional%20inputs%20into%20flawless%20novel%20view%20synthesis.%0AExtensive%20evaluations%20on%20real-world%20datasets%20confirm%20that%20our%20method%20produces%0Aseamless%20renderings-even%20from%20imperfect%20images-and%20outperforms%20existing%0A360-degree%20rendering%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20080v1&entry.124074799=Read"},
{"title": "Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven\n  Framework for Resilient 3D Place Recognition", "author": "Xiaohui Jiang and Haijiang Zhu and Chade Li and Fulin Tang and Ning An", "abstract": "  LiDAR-based place recognition serves as a crucial enabler for long-term\nautonomy in robotics and autonomous driving systems. Yet, prevailing\nmethodologies relying on handcrafted feature extraction face dual challenges:\n(1) Inconsistent point cloud density, induced by ego-motion dynamics and\nenvironmental disturbances during repeated traversals, leads to descriptor\ninstability, and (2) Representation fragility stems from reliance on\nsingle-level geometric abstractions that lack discriminative power in\nstructurally complex scenarios. To address these limitations, we propose a\nnovel framework that redefines 3D place recognition through density-agnostic\ngeometric reasoning. Specifically, we introduce an implicit 3D representation\nbased on elastic points, which is immune to the interference of original scene\npoint cloud density and achieves the characteristic of uniform distribution.\nSubsequently, we derive the occupancy grid and normal vector information of the\nscene from this implicit representation. Finally, with the aid of these two\ntypes of information, we obtain descriptors that fuse geometric information\nfrom both bird's-eye view (capturing macro-level spatial layouts) and 3D\nsegment (encoding micro-scale surface geometries) perspectives. We conducted\nextensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT)\nacross diverse environments. The experimental results demonstrate that our\nmethod achieves state-of-the-art performance. Moreover, our approach strikes an\noptimal balance between accuracy, runtime, and memory optimization for\nhistorical maps, showcasing excellent Resilient and scalability. Our code will\nbe open-sourced in the future.\n", "link": "http://arxiv.org/abs/2506.14243v3", "date": "2025-08-27", "relevancy": 3.1328, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6375}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Modal%20Geometric%20Hierarchy%20Fusion%3A%20An%20Implicit-Submap%20Driven%0A%20%20Framework%20for%20Resilient%203D%20Place%20Recognition&body=Title%3A%20Cross-Modal%20Geometric%20Hierarchy%20Fusion%3A%20An%20Implicit-Submap%20Driven%0A%20%20Framework%20for%20Resilient%203D%20Place%20Recognition%0AAuthor%3A%20Xiaohui%20Jiang%20and%20Haijiang%20Zhu%20and%20Chade%20Li%20and%20Fulin%20Tang%20and%20Ning%20An%0AAbstract%3A%20%20%20LiDAR-based%20place%20recognition%20serves%20as%20a%20crucial%20enabler%20for%20long-term%0Aautonomy%20in%20robotics%20and%20autonomous%20driving%20systems.%20Yet%2C%20prevailing%0Amethodologies%20relying%20on%20handcrafted%20feature%20extraction%20face%20dual%20challenges%3A%0A%281%29%20Inconsistent%20point%20cloud%20density%2C%20induced%20by%20ego-motion%20dynamics%20and%0Aenvironmental%20disturbances%20during%20repeated%20traversals%2C%20leads%20to%20descriptor%0Ainstability%2C%20and%20%282%29%20Representation%20fragility%20stems%20from%20reliance%20on%0Asingle-level%20geometric%20abstractions%20that%20lack%20discriminative%20power%20in%0Astructurally%20complex%20scenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anovel%20framework%20that%20redefines%203D%20place%20recognition%20through%20density-agnostic%0Ageometric%20reasoning.%20Specifically%2C%20we%20introduce%20an%20implicit%203D%20representation%0Abased%20on%20elastic%20points%2C%20which%20is%20immune%20to%20the%20interference%20of%20original%20scene%0Apoint%20cloud%20density%20and%20achieves%20the%20characteristic%20of%20uniform%20distribution.%0ASubsequently%2C%20we%20derive%20the%20occupancy%20grid%20and%20normal%20vector%20information%20of%20the%0Ascene%20from%20this%20implicit%20representation.%20Finally%2C%20with%20the%20aid%20of%20these%20two%0Atypes%20of%20information%2C%20we%20obtain%20descriptors%20that%20fuse%20geometric%20information%0Afrom%20both%20bird%27s-eye%20view%20%28capturing%20macro-level%20spatial%20layouts%29%20and%203D%0Asegment%20%28encoding%20micro-scale%20surface%20geometries%29%20perspectives.%20We%20conducted%0Aextensive%20experiments%20on%20numerous%20datasets%20%28KITTI%2C%20KITTI-360%2C%20MulRan%2C%20NCLT%29%0Aacross%20diverse%20environments.%20The%20experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance.%20Moreover%2C%20our%20approach%20strikes%20an%0Aoptimal%20balance%20between%20accuracy%2C%20runtime%2C%20and%20memory%20optimization%20for%0Ahistorical%20maps%2C%20showcasing%20excellent%20Resilient%20and%20scalability.%20Our%20code%20will%0Abe%20open-sourced%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.14243v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Modal%2520Geometric%2520Hierarchy%2520Fusion%253A%2520An%2520Implicit-Submap%2520Driven%250A%2520%2520Framework%2520for%2520Resilient%25203D%2520Place%2520Recognition%26entry.906535625%3DXiaohui%2520Jiang%2520and%2520Haijiang%2520Zhu%2520and%2520Chade%2520Li%2520and%2520Fulin%2520Tang%2520and%2520Ning%2520An%26entry.1292438233%3D%2520%2520LiDAR-based%2520place%2520recognition%2520serves%2520as%2520a%2520crucial%2520enabler%2520for%2520long-term%250Aautonomy%2520in%2520robotics%2520and%2520autonomous%2520driving%2520systems.%2520Yet%252C%2520prevailing%250Amethodologies%2520relying%2520on%2520handcrafted%2520feature%2520extraction%2520face%2520dual%2520challenges%253A%250A%25281%2529%2520Inconsistent%2520point%2520cloud%2520density%252C%2520induced%2520by%2520ego-motion%2520dynamics%2520and%250Aenvironmental%2520disturbances%2520during%2520repeated%2520traversals%252C%2520leads%2520to%2520descriptor%250Ainstability%252C%2520and%2520%25282%2529%2520Representation%2520fragility%2520stems%2520from%2520reliance%2520on%250Asingle-level%2520geometric%2520abstractions%2520that%2520lack%2520discriminative%2520power%2520in%250Astructurally%2520complex%2520scenarios.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anovel%2520framework%2520that%2520redefines%25203D%2520place%2520recognition%2520through%2520density-agnostic%250Ageometric%2520reasoning.%2520Specifically%252C%2520we%2520introduce%2520an%2520implicit%25203D%2520representation%250Abased%2520on%2520elastic%2520points%252C%2520which%2520is%2520immune%2520to%2520the%2520interference%2520of%2520original%2520scene%250Apoint%2520cloud%2520density%2520and%2520achieves%2520the%2520characteristic%2520of%2520uniform%2520distribution.%250ASubsequently%252C%2520we%2520derive%2520the%2520occupancy%2520grid%2520and%2520normal%2520vector%2520information%2520of%2520the%250Ascene%2520from%2520this%2520implicit%2520representation.%2520Finally%252C%2520with%2520the%2520aid%2520of%2520these%2520two%250Atypes%2520of%2520information%252C%2520we%2520obtain%2520descriptors%2520that%2520fuse%2520geometric%2520information%250Afrom%2520both%2520bird%2527s-eye%2520view%2520%2528capturing%2520macro-level%2520spatial%2520layouts%2529%2520and%25203D%250Asegment%2520%2528encoding%2520micro-scale%2520surface%2520geometries%2529%2520perspectives.%2520We%2520conducted%250Aextensive%2520experiments%2520on%2520numerous%2520datasets%2520%2528KITTI%252C%2520KITTI-360%252C%2520MulRan%252C%2520NCLT%2529%250Aacross%2520diverse%2520environments.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520state-of-the-art%2520performance.%2520Moreover%252C%2520our%2520approach%2520strikes%2520an%250Aoptimal%2520balance%2520between%2520accuracy%252C%2520runtime%252C%2520and%2520memory%2520optimization%2520for%250Ahistorical%2520maps%252C%2520showcasing%2520excellent%2520Resilient%2520and%2520scalability.%2520Our%2520code%2520will%250Abe%2520open-sourced%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.14243v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Modal%20Geometric%20Hierarchy%20Fusion%3A%20An%20Implicit-Submap%20Driven%0A%20%20Framework%20for%20Resilient%203D%20Place%20Recognition&entry.906535625=Xiaohui%20Jiang%20and%20Haijiang%20Zhu%20and%20Chade%20Li%20and%20Fulin%20Tang%20and%20Ning%20An&entry.1292438233=%20%20LiDAR-based%20place%20recognition%20serves%20as%20a%20crucial%20enabler%20for%20long-term%0Aautonomy%20in%20robotics%20and%20autonomous%20driving%20systems.%20Yet%2C%20prevailing%0Amethodologies%20relying%20on%20handcrafted%20feature%20extraction%20face%20dual%20challenges%3A%0A%281%29%20Inconsistent%20point%20cloud%20density%2C%20induced%20by%20ego-motion%20dynamics%20and%0Aenvironmental%20disturbances%20during%20repeated%20traversals%2C%20leads%20to%20descriptor%0Ainstability%2C%20and%20%282%29%20Representation%20fragility%20stems%20from%20reliance%20on%0Asingle-level%20geometric%20abstractions%20that%20lack%20discriminative%20power%20in%0Astructurally%20complex%20scenarios.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anovel%20framework%20that%20redefines%203D%20place%20recognition%20through%20density-agnostic%0Ageometric%20reasoning.%20Specifically%2C%20we%20introduce%20an%20implicit%203D%20representation%0Abased%20on%20elastic%20points%2C%20which%20is%20immune%20to%20the%20interference%20of%20original%20scene%0Apoint%20cloud%20density%20and%20achieves%20the%20characteristic%20of%20uniform%20distribution.%0ASubsequently%2C%20we%20derive%20the%20occupancy%20grid%20and%20normal%20vector%20information%20of%20the%0Ascene%20from%20this%20implicit%20representation.%20Finally%2C%20with%20the%20aid%20of%20these%20two%0Atypes%20of%20information%2C%20we%20obtain%20descriptors%20that%20fuse%20geometric%20information%0Afrom%20both%20bird%27s-eye%20view%20%28capturing%20macro-level%20spatial%20layouts%29%20and%203D%0Asegment%20%28encoding%20micro-scale%20surface%20geometries%29%20perspectives.%20We%20conducted%0Aextensive%20experiments%20on%20numerous%20datasets%20%28KITTI%2C%20KITTI-360%2C%20MulRan%2C%20NCLT%29%0Aacross%20diverse%20environments.%20The%20experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20state-of-the-art%20performance.%20Moreover%2C%20our%20approach%20strikes%20an%0Aoptimal%20balance%20between%20accuracy%2C%20runtime%2C%20and%20memory%20optimization%20for%0Ahistorical%20maps%2C%20showcasing%20excellent%20Resilient%20and%20scalability.%20Our%20code%20will%0Abe%20open-sourced%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.14243v3&entry.124074799=Read"},
{"title": "Fast 3D Diffusion for Scalable Granular Media Synthesis", "author": "Muhammad Moeeze Hassan and R\u00e9gis Cottereau and Filippo Gatti and Patryk Dec", "abstract": "  Simulating granular media, using Discrete Element Method is a computationally\nintensive task. This is especially true during initialization phase, which\ndominates total simulation time because of large displacements involved and\nassociated kinetic energy. We overcome this bottleneck with a novel generative\npipeline based on 3D diffusion models that directly synthesizes arbitrarily\nlarge granular assemblies in their final and physically realistic\nconfigurations. The approach frames the problem as a 3D generative modeling\ntask, consisting of a two-stage pipeline. First a diffusion model is trained to\ngenerate independent 3D voxel grids representing granular media. Second, a 3D\ninpainting model, adapted from 2D inpainting techniques using masked inputs,\nstitches these grids together seamlessly, enabling synthesis of large samples\nwith physically realistic structure. The inpainting model explores several\nmasking strategies for the inputs to the underlying UNets by training the\nnetwork to infer missing portions of voxel grids from a concatenation of noised\ntensors, masks, and masked tensors as input channels. The model also adapts a\n2D repainting technique of re-injecting noise scheduler output with ground\ntruth to provide a strong guidance to the 3D model. This along with weighted\nlosses ensures long-term coherence over generation of masked regions. Both\nmodels are trained on the same binarized 3D occupancy grids extracted from\nsmall-scale DEM simulations, achieving linear scaling of computational time\nwith respect to sample size. Quantitatively, a 1.2 m long ballasted rail track\nsynthesis equivalent to a 3-hour DEM simulation, was completed under 20\nseconds. The generated voxel grids can also be post-processed to extract grain\ngeometries for DEM-compatibility as well, enabling physically coherent,\nreal-time, scalable granular media synthesis for industrial applications.\n", "link": "http://arxiv.org/abs/2508.19752v1", "date": "2025-08-27", "relevancy": 3.0379, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6091}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6068}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%203D%20Diffusion%20for%20Scalable%20Granular%20Media%20Synthesis&body=Title%3A%20Fast%203D%20Diffusion%20for%20Scalable%20Granular%20Media%20Synthesis%0AAuthor%3A%20Muhammad%20Moeeze%20Hassan%20and%20R%C3%A9gis%20Cottereau%20and%20Filippo%20Gatti%20and%20Patryk%20Dec%0AAbstract%3A%20%20%20Simulating%20granular%20media%2C%20using%20Discrete%20Element%20Method%20is%20a%20computationally%0Aintensive%20task.%20This%20is%20especially%20true%20during%20initialization%20phase%2C%20which%0Adominates%20total%20simulation%20time%20because%20of%20large%20displacements%20involved%20and%0Aassociated%20kinetic%20energy.%20We%20overcome%20this%20bottleneck%20with%20a%20novel%20generative%0Apipeline%20based%20on%203D%20diffusion%20models%20that%20directly%20synthesizes%20arbitrarily%0Alarge%20granular%20assemblies%20in%20their%20final%20and%20physically%20realistic%0Aconfigurations.%20The%20approach%20frames%20the%20problem%20as%20a%203D%20generative%20modeling%0Atask%2C%20consisting%20of%20a%20two-stage%20pipeline.%20First%20a%20diffusion%20model%20is%20trained%20to%0Agenerate%20independent%203D%20voxel%20grids%20representing%20granular%20media.%20Second%2C%20a%203D%0Ainpainting%20model%2C%20adapted%20from%202D%20inpainting%20techniques%20using%20masked%20inputs%2C%0Astitches%20these%20grids%20together%20seamlessly%2C%20enabling%20synthesis%20of%20large%20samples%0Awith%20physically%20realistic%20structure.%20The%20inpainting%20model%20explores%20several%0Amasking%20strategies%20for%20the%20inputs%20to%20the%20underlying%20UNets%20by%20training%20the%0Anetwork%20to%20infer%20missing%20portions%20of%20voxel%20grids%20from%20a%20concatenation%20of%20noised%0Atensors%2C%20masks%2C%20and%20masked%20tensors%20as%20input%20channels.%20The%20model%20also%20adapts%20a%0A2D%20repainting%20technique%20of%20re-injecting%20noise%20scheduler%20output%20with%20ground%0Atruth%20to%20provide%20a%20strong%20guidance%20to%20the%203D%20model.%20This%20along%20with%20weighted%0Alosses%20ensures%20long-term%20coherence%20over%20generation%20of%20masked%20regions.%20Both%0Amodels%20are%20trained%20on%20the%20same%20binarized%203D%20occupancy%20grids%20extracted%20from%0Asmall-scale%20DEM%20simulations%2C%20achieving%20linear%20scaling%20of%20computational%20time%0Awith%20respect%20to%20sample%20size.%20Quantitatively%2C%20a%201.2%20m%20long%20ballasted%20rail%20track%0Asynthesis%20equivalent%20to%20a%203-hour%20DEM%20simulation%2C%20was%20completed%20under%2020%0Aseconds.%20The%20generated%20voxel%20grids%20can%20also%20be%20post-processed%20to%20extract%20grain%0Ageometries%20for%20DEM-compatibility%20as%20well%2C%20enabling%20physically%20coherent%2C%0Areal-time%2C%20scalable%20granular%20media%20synthesis%20for%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%25203D%2520Diffusion%2520for%2520Scalable%2520Granular%2520Media%2520Synthesis%26entry.906535625%3DMuhammad%2520Moeeze%2520Hassan%2520and%2520R%25C3%25A9gis%2520Cottereau%2520and%2520Filippo%2520Gatti%2520and%2520Patryk%2520Dec%26entry.1292438233%3D%2520%2520Simulating%2520granular%2520media%252C%2520using%2520Discrete%2520Element%2520Method%2520is%2520a%2520computationally%250Aintensive%2520task.%2520This%2520is%2520especially%2520true%2520during%2520initialization%2520phase%252C%2520which%250Adominates%2520total%2520simulation%2520time%2520because%2520of%2520large%2520displacements%2520involved%2520and%250Aassociated%2520kinetic%2520energy.%2520We%2520overcome%2520this%2520bottleneck%2520with%2520a%2520novel%2520generative%250Apipeline%2520based%2520on%25203D%2520diffusion%2520models%2520that%2520directly%2520synthesizes%2520arbitrarily%250Alarge%2520granular%2520assemblies%2520in%2520their%2520final%2520and%2520physically%2520realistic%250Aconfigurations.%2520The%2520approach%2520frames%2520the%2520problem%2520as%2520a%25203D%2520generative%2520modeling%250Atask%252C%2520consisting%2520of%2520a%2520two-stage%2520pipeline.%2520First%2520a%2520diffusion%2520model%2520is%2520trained%2520to%250Agenerate%2520independent%25203D%2520voxel%2520grids%2520representing%2520granular%2520media.%2520Second%252C%2520a%25203D%250Ainpainting%2520model%252C%2520adapted%2520from%25202D%2520inpainting%2520techniques%2520using%2520masked%2520inputs%252C%250Astitches%2520these%2520grids%2520together%2520seamlessly%252C%2520enabling%2520synthesis%2520of%2520large%2520samples%250Awith%2520physically%2520realistic%2520structure.%2520The%2520inpainting%2520model%2520explores%2520several%250Amasking%2520strategies%2520for%2520the%2520inputs%2520to%2520the%2520underlying%2520UNets%2520by%2520training%2520the%250Anetwork%2520to%2520infer%2520missing%2520portions%2520of%2520voxel%2520grids%2520from%2520a%2520concatenation%2520of%2520noised%250Atensors%252C%2520masks%252C%2520and%2520masked%2520tensors%2520as%2520input%2520channels.%2520The%2520model%2520also%2520adapts%2520a%250A2D%2520repainting%2520technique%2520of%2520re-injecting%2520noise%2520scheduler%2520output%2520with%2520ground%250Atruth%2520to%2520provide%2520a%2520strong%2520guidance%2520to%2520the%25203D%2520model.%2520This%2520along%2520with%2520weighted%250Alosses%2520ensures%2520long-term%2520coherence%2520over%2520generation%2520of%2520masked%2520regions.%2520Both%250Amodels%2520are%2520trained%2520on%2520the%2520same%2520binarized%25203D%2520occupancy%2520grids%2520extracted%2520from%250Asmall-scale%2520DEM%2520simulations%252C%2520achieving%2520linear%2520scaling%2520of%2520computational%2520time%250Awith%2520respect%2520to%2520sample%2520size.%2520Quantitatively%252C%2520a%25201.2%2520m%2520long%2520ballasted%2520rail%2520track%250Asynthesis%2520equivalent%2520to%2520a%25203-hour%2520DEM%2520simulation%252C%2520was%2520completed%2520under%252020%250Aseconds.%2520The%2520generated%2520voxel%2520grids%2520can%2520also%2520be%2520post-processed%2520to%2520extract%2520grain%250Ageometries%2520for%2520DEM-compatibility%2520as%2520well%252C%2520enabling%2520physically%2520coherent%252C%250Areal-time%252C%2520scalable%2520granular%2520media%2520synthesis%2520for%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%203D%20Diffusion%20for%20Scalable%20Granular%20Media%20Synthesis&entry.906535625=Muhammad%20Moeeze%20Hassan%20and%20R%C3%A9gis%20Cottereau%20and%20Filippo%20Gatti%20and%20Patryk%20Dec&entry.1292438233=%20%20Simulating%20granular%20media%2C%20using%20Discrete%20Element%20Method%20is%20a%20computationally%0Aintensive%20task.%20This%20is%20especially%20true%20during%20initialization%20phase%2C%20which%0Adominates%20total%20simulation%20time%20because%20of%20large%20displacements%20involved%20and%0Aassociated%20kinetic%20energy.%20We%20overcome%20this%20bottleneck%20with%20a%20novel%20generative%0Apipeline%20based%20on%203D%20diffusion%20models%20that%20directly%20synthesizes%20arbitrarily%0Alarge%20granular%20assemblies%20in%20their%20final%20and%20physically%20realistic%0Aconfigurations.%20The%20approach%20frames%20the%20problem%20as%20a%203D%20generative%20modeling%0Atask%2C%20consisting%20of%20a%20two-stage%20pipeline.%20First%20a%20diffusion%20model%20is%20trained%20to%0Agenerate%20independent%203D%20voxel%20grids%20representing%20granular%20media.%20Second%2C%20a%203D%0Ainpainting%20model%2C%20adapted%20from%202D%20inpainting%20techniques%20using%20masked%20inputs%2C%0Astitches%20these%20grids%20together%20seamlessly%2C%20enabling%20synthesis%20of%20large%20samples%0Awith%20physically%20realistic%20structure.%20The%20inpainting%20model%20explores%20several%0Amasking%20strategies%20for%20the%20inputs%20to%20the%20underlying%20UNets%20by%20training%20the%0Anetwork%20to%20infer%20missing%20portions%20of%20voxel%20grids%20from%20a%20concatenation%20of%20noised%0Atensors%2C%20masks%2C%20and%20masked%20tensors%20as%20input%20channels.%20The%20model%20also%20adapts%20a%0A2D%20repainting%20technique%20of%20re-injecting%20noise%20scheduler%20output%20with%20ground%0Atruth%20to%20provide%20a%20strong%20guidance%20to%20the%203D%20model.%20This%20along%20with%20weighted%0Alosses%20ensures%20long-term%20coherence%20over%20generation%20of%20masked%20regions.%20Both%0Amodels%20are%20trained%20on%20the%20same%20binarized%203D%20occupancy%20grids%20extracted%20from%0Asmall-scale%20DEM%20simulations%2C%20achieving%20linear%20scaling%20of%20computational%20time%0Awith%20respect%20to%20sample%20size.%20Quantitatively%2C%20a%201.2%20m%20long%20ballasted%20rail%20track%0Asynthesis%20equivalent%20to%20a%203-hour%20DEM%20simulation%2C%20was%20completed%20under%2020%0Aseconds.%20The%20generated%20voxel%20grids%20can%20also%20be%20post-processed%20to%20extract%20grain%0Ageometries%20for%20DEM-compatibility%20as%20well%2C%20enabling%20physically%20coherent%2C%0Areal-time%2C%20scalable%20granular%20media%20synthesis%20for%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19752v1&entry.124074799=Read"},
{"title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency", "author": "Weiyun Wang and Zhangwei Gao and Lixin Gu and Hengjun Pu and Long Cui and Xingguang Wei and Zhaoyang Liu and Linglin Jing and Shenglong Ye and Jie Shao and Zhaokai Wang and Zhe Chen and Hongjie Zhang and Ganlin Yang and Haomin Wang and Qi Wei and Jinhui Yin and Wenhao Li and Erfei Cui and Guanzhou Chen and Zichen Ding and Changyao Tian and Zhenyu Wu and Jingjing Xie and Zehao Li and Bowen Yang and Yuchen Duan and Xuehui Wang and Zhi Hou and Haoran Hao and Tianyi Zhang and Songze Li and Xiangyu Zhao and Haodong Duan and Nianchen Deng and Bin Fu and Yinan He and Yi Wang and Conghui He and Botian Shi and Junjun He and Yingtong Xiong and Han Lv and Lijun Wu and Wenqi Shao and Kaipeng Zhang and Huipeng Deng and Biqing Qi and Jiaye Ge and Qipeng Guo and Wenwei Zhang and Songyang Zhang and Maosong Cao and Junyao Lin and Kexian Tang and Jianfei Gao and Haian Huang and Yuzhe Gu and Chengqi Lyu and Huanze Tang and Rui Wang and Haijun Lv and Wanli Ouyang and Limin Wang and Min Dou and Xizhou Zhu and Tong Lu and Dahua Lin and Jifeng Dai and Weijie Su and Bowen Zhou and Kai Chen and Yu Qiao and Wenhai Wang and Gen Luo", "abstract": "  We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05$\\times$ inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.\n", "link": "http://arxiv.org/abs/2508.18265v2", "date": "2025-08-27", "relevancy": 2.9864, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6129}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVL3.5%3A%20Advancing%20Open-Source%20Multimodal%20Models%20in%20Versatility%2C%0A%20%20Reasoning%2C%20and%20Efficiency&body=Title%3A%20InternVL3.5%3A%20Advancing%20Open-Source%20Multimodal%20Models%20in%20Versatility%2C%0A%20%20Reasoning%2C%20and%20Efficiency%0AAuthor%3A%20Weiyun%20Wang%20and%20Zhangwei%20Gao%20and%20Lixin%20Gu%20and%20Hengjun%20Pu%20and%20Long%20Cui%20and%20Xingguang%20Wei%20and%20Zhaoyang%20Liu%20and%20Linglin%20Jing%20and%20Shenglong%20Ye%20and%20Jie%20Shao%20and%20Zhaokai%20Wang%20and%20Zhe%20Chen%20and%20Hongjie%20Zhang%20and%20Ganlin%20Yang%20and%20Haomin%20Wang%20and%20Qi%20Wei%20and%20Jinhui%20Yin%20and%20Wenhao%20Li%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Zichen%20Ding%20and%20Changyao%20Tian%20and%20Zhenyu%20Wu%20and%20Jingjing%20Xie%20and%20Zehao%20Li%20and%20Bowen%20Yang%20and%20Yuchen%20Duan%20and%20Xuehui%20Wang%20and%20Zhi%20Hou%20and%20Haoran%20Hao%20and%20Tianyi%20Zhang%20and%20Songze%20Li%20and%20Xiangyu%20Zhao%20and%20Haodong%20Duan%20and%20Nianchen%20Deng%20and%20Bin%20Fu%20and%20Yinan%20He%20and%20Yi%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Junjun%20He%20and%20Yingtong%20Xiong%20and%20Han%20Lv%20and%20Lijun%20Wu%20and%20Wenqi%20Shao%20and%20Kaipeng%20Zhang%20and%20Huipeng%20Deng%20and%20Biqing%20Qi%20and%20Jiaye%20Ge%20and%20Qipeng%20Guo%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Maosong%20Cao%20and%20Junyao%20Lin%20and%20Kexian%20Tang%20and%20Jianfei%20Gao%20and%20Haian%20Huang%20and%20Yuzhe%20Gu%20and%20Chengqi%20Lyu%20and%20Huanze%20Tang%20and%20Rui%20Wang%20and%20Haijun%20Lv%20and%20Wanli%20Ouyang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Jifeng%20Dai%20and%20Weijie%20Su%20and%20Bowen%20Zhou%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo%0AAbstract%3A%20%20%20We%20introduce%20InternVL%203.5%2C%20a%20new%20family%20of%20open-source%20multimodal%20models%20that%0Asignificantly%20advances%20versatility%2C%20reasoning%20capability%2C%20and%20inference%0Aefficiency%20along%20the%20InternVL%20series.%20A%20key%20innovation%20is%20the%20Cascade%0AReinforcement%20Learning%20%28Cascade%20RL%29%20framework%2C%20which%20enhances%20reasoning%20through%0Aa%20two-stage%20process%3A%20offline%20RL%20for%20stable%20convergence%20and%20online%20RL%20for%0Arefined%20alignment.%20This%20coarse-to-fine%20training%20strategy%20leads%20to%20substantial%0Aimprovements%20on%20downstream%20reasoning%20tasks%2C%20e.g.%2C%20MMMU%20and%20MathVista.%20To%0Aoptimize%20efficiency%2C%20we%20propose%20a%20Visual%20Resolution%20Router%20%28ViR%29%20that%0Adynamically%20adjusts%20the%20resolution%20of%20visual%20tokens%20without%20compromising%0Aperformance.%20Coupled%20with%20ViR%2C%20our%20Decoupled%20Vision-Language%20Deployment%20%28DvD%29%0Astrategy%20separates%20the%20vision%20encoder%20and%20language%20model%20across%20different%20GPUs%2C%0Aeffectively%20balancing%20computational%20load.%20These%20contributions%20collectively%0Aenable%20InternVL3.5%20to%20achieve%20up%20to%20a%20%2B16.0%5C%25%20gain%20in%20overall%20reasoning%0Aperformance%20and%20a%204.05%24%5Ctimes%24%20inference%20speedup%20compared%20to%20its%20predecessor%2C%0Ai.e.%2C%20InternVL3.%20In%20addition%2C%20InternVL3.5%20supports%20novel%20capabilities%20such%20as%0AGUI%20interaction%20and%20embodied%20agency.%20Notably%2C%20our%20largest%20model%2C%20i.e.%2C%0AInternVL3.5-241B-A28B%2C%20attains%20state-of-the-art%20results%20among%20open-source%20MLLMs%0Aacross%20general%20multimodal%2C%20reasoning%2C%20text%2C%20and%20agentic%20tasks%20--%20narrowing%20the%0Aperformance%20gap%20with%20leading%20commercial%20models%20like%20GPT-5.%20All%20models%20and%20code%0Aare%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVL3.5%253A%2520Advancing%2520Open-Source%2520Multimodal%2520Models%2520in%2520Versatility%252C%250A%2520%2520Reasoning%252C%2520and%2520Efficiency%26entry.906535625%3DWeiyun%2520Wang%2520and%2520Zhangwei%2520Gao%2520and%2520Lixin%2520Gu%2520and%2520Hengjun%2520Pu%2520and%2520Long%2520Cui%2520and%2520Xingguang%2520Wei%2520and%2520Zhaoyang%2520Liu%2520and%2520Linglin%2520Jing%2520and%2520Shenglong%2520Ye%2520and%2520Jie%2520Shao%2520and%2520Zhaokai%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Hongjie%2520Zhang%2520and%2520Ganlin%2520Yang%2520and%2520Haomin%2520Wang%2520and%2520Qi%2520Wei%2520and%2520Jinhui%2520Yin%2520and%2520Wenhao%2520Li%2520and%2520Erfei%2520Cui%2520and%2520Guanzhou%2520Chen%2520and%2520Zichen%2520Ding%2520and%2520Changyao%2520Tian%2520and%2520Zhenyu%2520Wu%2520and%2520Jingjing%2520Xie%2520and%2520Zehao%2520Li%2520and%2520Bowen%2520Yang%2520and%2520Yuchen%2520Duan%2520and%2520Xuehui%2520Wang%2520and%2520Zhi%2520Hou%2520and%2520Haoran%2520Hao%2520and%2520Tianyi%2520Zhang%2520and%2520Songze%2520Li%2520and%2520Xiangyu%2520Zhao%2520and%2520Haodong%2520Duan%2520and%2520Nianchen%2520Deng%2520and%2520Bin%2520Fu%2520and%2520Yinan%2520He%2520and%2520Yi%2520Wang%2520and%2520Conghui%2520He%2520and%2520Botian%2520Shi%2520and%2520Junjun%2520He%2520and%2520Yingtong%2520Xiong%2520and%2520Han%2520Lv%2520and%2520Lijun%2520Wu%2520and%2520Wenqi%2520Shao%2520and%2520Kaipeng%2520Zhang%2520and%2520Huipeng%2520Deng%2520and%2520Biqing%2520Qi%2520and%2520Jiaye%2520Ge%2520and%2520Qipeng%2520Guo%2520and%2520Wenwei%2520Zhang%2520and%2520Songyang%2520Zhang%2520and%2520Maosong%2520Cao%2520and%2520Junyao%2520Lin%2520and%2520Kexian%2520Tang%2520and%2520Jianfei%2520Gao%2520and%2520Haian%2520Huang%2520and%2520Yuzhe%2520Gu%2520and%2520Chengqi%2520Lyu%2520and%2520Huanze%2520Tang%2520and%2520Rui%2520Wang%2520and%2520Haijun%2520Lv%2520and%2520Wanli%2520Ouyang%2520and%2520Limin%2520Wang%2520and%2520Min%2520Dou%2520and%2520Xizhou%2520Zhu%2520and%2520Tong%2520Lu%2520and%2520Dahua%2520Lin%2520and%2520Jifeng%2520Dai%2520and%2520Weijie%2520Su%2520and%2520Bowen%2520Zhou%2520and%2520Kai%2520Chen%2520and%2520Yu%2520Qiao%2520and%2520Wenhai%2520Wang%2520and%2520Gen%2520Luo%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVL%25203.5%252C%2520a%2520new%2520family%2520of%2520open-source%2520multimodal%2520models%2520that%250Asignificantly%2520advances%2520versatility%252C%2520reasoning%2520capability%252C%2520and%2520inference%250Aefficiency%2520along%2520the%2520InternVL%2520series.%2520A%2520key%2520innovation%2520is%2520the%2520Cascade%250AReinforcement%2520Learning%2520%2528Cascade%2520RL%2529%2520framework%252C%2520which%2520enhances%2520reasoning%2520through%250Aa%2520two-stage%2520process%253A%2520offline%2520RL%2520for%2520stable%2520convergence%2520and%2520online%2520RL%2520for%250Arefined%2520alignment.%2520This%2520coarse-to-fine%2520training%2520strategy%2520leads%2520to%2520substantial%250Aimprovements%2520on%2520downstream%2520reasoning%2520tasks%252C%2520e.g.%252C%2520MMMU%2520and%2520MathVista.%2520To%250Aoptimize%2520efficiency%252C%2520we%2520propose%2520a%2520Visual%2520Resolution%2520Router%2520%2528ViR%2529%2520that%250Adynamically%2520adjusts%2520the%2520resolution%2520of%2520visual%2520tokens%2520without%2520compromising%250Aperformance.%2520Coupled%2520with%2520ViR%252C%2520our%2520Decoupled%2520Vision-Language%2520Deployment%2520%2528DvD%2529%250Astrategy%2520separates%2520the%2520vision%2520encoder%2520and%2520language%2520model%2520across%2520different%2520GPUs%252C%250Aeffectively%2520balancing%2520computational%2520load.%2520These%2520contributions%2520collectively%250Aenable%2520InternVL3.5%2520to%2520achieve%2520up%2520to%2520a%2520%252B16.0%255C%2525%2520gain%2520in%2520overall%2520reasoning%250Aperformance%2520and%2520a%25204.05%2524%255Ctimes%2524%2520inference%2520speedup%2520compared%2520to%2520its%2520predecessor%252C%250Ai.e.%252C%2520InternVL3.%2520In%2520addition%252C%2520InternVL3.5%2520supports%2520novel%2520capabilities%2520such%2520as%250AGUI%2520interaction%2520and%2520embodied%2520agency.%2520Notably%252C%2520our%2520largest%2520model%252C%2520i.e.%252C%250AInternVL3.5-241B-A28B%252C%2520attains%2520state-of-the-art%2520results%2520among%2520open-source%2520MLLMs%250Aacross%2520general%2520multimodal%252C%2520reasoning%252C%2520text%252C%2520and%2520agentic%2520tasks%2520--%2520narrowing%2520the%250Aperformance%2520gap%2520with%2520leading%2520commercial%2520models%2520like%2520GPT-5.%2520All%2520models%2520and%2520code%250Aare%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVL3.5%3A%20Advancing%20Open-Source%20Multimodal%20Models%20in%20Versatility%2C%0A%20%20Reasoning%2C%20and%20Efficiency&entry.906535625=Weiyun%20Wang%20and%20Zhangwei%20Gao%20and%20Lixin%20Gu%20and%20Hengjun%20Pu%20and%20Long%20Cui%20and%20Xingguang%20Wei%20and%20Zhaoyang%20Liu%20and%20Linglin%20Jing%20and%20Shenglong%20Ye%20and%20Jie%20Shao%20and%20Zhaokai%20Wang%20and%20Zhe%20Chen%20and%20Hongjie%20Zhang%20and%20Ganlin%20Yang%20and%20Haomin%20Wang%20and%20Qi%20Wei%20and%20Jinhui%20Yin%20and%20Wenhao%20Li%20and%20Erfei%20Cui%20and%20Guanzhou%20Chen%20and%20Zichen%20Ding%20and%20Changyao%20Tian%20and%20Zhenyu%20Wu%20and%20Jingjing%20Xie%20and%20Zehao%20Li%20and%20Bowen%20Yang%20and%20Yuchen%20Duan%20and%20Xuehui%20Wang%20and%20Zhi%20Hou%20and%20Haoran%20Hao%20and%20Tianyi%20Zhang%20and%20Songze%20Li%20and%20Xiangyu%20Zhao%20and%20Haodong%20Duan%20and%20Nianchen%20Deng%20and%20Bin%20Fu%20and%20Yinan%20He%20and%20Yi%20Wang%20and%20Conghui%20He%20and%20Botian%20Shi%20and%20Junjun%20He%20and%20Yingtong%20Xiong%20and%20Han%20Lv%20and%20Lijun%20Wu%20and%20Wenqi%20Shao%20and%20Kaipeng%20Zhang%20and%20Huipeng%20Deng%20and%20Biqing%20Qi%20and%20Jiaye%20Ge%20and%20Qipeng%20Guo%20and%20Wenwei%20Zhang%20and%20Songyang%20Zhang%20and%20Maosong%20Cao%20and%20Junyao%20Lin%20and%20Kexian%20Tang%20and%20Jianfei%20Gao%20and%20Haian%20Huang%20and%20Yuzhe%20Gu%20and%20Chengqi%20Lyu%20and%20Huanze%20Tang%20and%20Rui%20Wang%20and%20Haijun%20Lv%20and%20Wanli%20Ouyang%20and%20Limin%20Wang%20and%20Min%20Dou%20and%20Xizhou%20Zhu%20and%20Tong%20Lu%20and%20Dahua%20Lin%20and%20Jifeng%20Dai%20and%20Weijie%20Su%20and%20Bowen%20Zhou%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Wenhai%20Wang%20and%20Gen%20Luo&entry.1292438233=%20%20We%20introduce%20InternVL%203.5%2C%20a%20new%20family%20of%20open-source%20multimodal%20models%20that%0Asignificantly%20advances%20versatility%2C%20reasoning%20capability%2C%20and%20inference%0Aefficiency%20along%20the%20InternVL%20series.%20A%20key%20innovation%20is%20the%20Cascade%0AReinforcement%20Learning%20%28Cascade%20RL%29%20framework%2C%20which%20enhances%20reasoning%20through%0Aa%20two-stage%20process%3A%20offline%20RL%20for%20stable%20convergence%20and%20online%20RL%20for%0Arefined%20alignment.%20This%20coarse-to-fine%20training%20strategy%20leads%20to%20substantial%0Aimprovements%20on%20downstream%20reasoning%20tasks%2C%20e.g.%2C%20MMMU%20and%20MathVista.%20To%0Aoptimize%20efficiency%2C%20we%20propose%20a%20Visual%20Resolution%20Router%20%28ViR%29%20that%0Adynamically%20adjusts%20the%20resolution%20of%20visual%20tokens%20without%20compromising%0Aperformance.%20Coupled%20with%20ViR%2C%20our%20Decoupled%20Vision-Language%20Deployment%20%28DvD%29%0Astrategy%20separates%20the%20vision%20encoder%20and%20language%20model%20across%20different%20GPUs%2C%0Aeffectively%20balancing%20computational%20load.%20These%20contributions%20collectively%0Aenable%20InternVL3.5%20to%20achieve%20up%20to%20a%20%2B16.0%5C%25%20gain%20in%20overall%20reasoning%0Aperformance%20and%20a%204.05%24%5Ctimes%24%20inference%20speedup%20compared%20to%20its%20predecessor%2C%0Ai.e.%2C%20InternVL3.%20In%20addition%2C%20InternVL3.5%20supports%20novel%20capabilities%20such%20as%0AGUI%20interaction%20and%20embodied%20agency.%20Notably%2C%20our%20largest%20model%2C%20i.e.%2C%0AInternVL3.5-241B-A28B%2C%20attains%20state-of-the-art%20results%20among%20open-source%20MLLMs%0Aacross%20general%20multimodal%2C%20reasoning%2C%20text%2C%20and%20agentic%20tasks%20--%20narrowing%20the%0Aperformance%20gap%20with%20leading%20commercial%20models%20like%20GPT-5.%20All%20models%20and%20code%0Aare%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18265v2&entry.124074799=Read"},
{"title": "Self-supervised structured object representation learning", "author": "Oussama Hadjerci and Antoine Letienne and Mohamed Abbas Hedjazi and Adel Hafiane", "abstract": "  Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning visual representations. While recent SSL approaches achieve strong\nresults in global image understanding, they are limited in capturing the\nstructured representation in scenes. In this work, we propose a self-supervised\napproach that progressively builds structured visual representations by\ncombining semantic grouping, instance level separation, and hierarchical\nstructuring. Our approach, based on a novel ProtoScale module, captures visual\nelements across multiple spatial scales. Unlike common strategies like DINO\nthat rely on random cropping and global embeddings, we preserve full scene\ncontext across augmented views to improve performance in dense prediction\ntasks. We validate our method on downstream object detection tasks using a\ncombined subset of multiple datasets (COCO and UA-DETRAC). Experimental results\nshow that our method learns object centric representations that enhance\nsupervised object detection and outperform the state-of-the-art methods, even\nwhen trained with limited annotated data and fewer fine-tuning epochs.\n", "link": "http://arxiv.org/abs/2508.19864v1", "date": "2025-08-27", "relevancy": 2.9745, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6821}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20structured%20object%20representation%20learning&body=Title%3A%20Self-supervised%20structured%20object%20representation%20learning%0AAuthor%3A%20Oussama%20Hadjerci%20and%20Antoine%20Letienne%20and%20Mohamed%20Abbas%20Hedjazi%20and%20Adel%20Hafiane%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20technique%20for%0Alearning%20visual%20representations.%20While%20recent%20SSL%20approaches%20achieve%20strong%0Aresults%20in%20global%20image%20understanding%2C%20they%20are%20limited%20in%20capturing%20the%0Astructured%20representation%20in%20scenes.%20In%20this%20work%2C%20we%20propose%20a%20self-supervised%0Aapproach%20that%20progressively%20builds%20structured%20visual%20representations%20by%0Acombining%20semantic%20grouping%2C%20instance%20level%20separation%2C%20and%20hierarchical%0Astructuring.%20Our%20approach%2C%20based%20on%20a%20novel%20ProtoScale%20module%2C%20captures%20visual%0Aelements%20across%20multiple%20spatial%20scales.%20Unlike%20common%20strategies%20like%20DINO%0Athat%20rely%20on%20random%20cropping%20and%20global%20embeddings%2C%20we%20preserve%20full%20scene%0Acontext%20across%20augmented%20views%20to%20improve%20performance%20in%20dense%20prediction%0Atasks.%20We%20validate%20our%20method%20on%20downstream%20object%20detection%20tasks%20using%20a%0Acombined%20subset%20of%20multiple%20datasets%20%28COCO%20and%20UA-DETRAC%29.%20Experimental%20results%0Ashow%20that%20our%20method%20learns%20object%20centric%20representations%20that%20enhance%0Asupervised%20object%20detection%20and%20outperform%20the%20state-of-the-art%20methods%2C%20even%0Awhen%20trained%20with%20limited%20annotated%20data%20and%20fewer%20fine-tuning%20epochs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520structured%2520object%2520representation%2520learning%26entry.906535625%3DOussama%2520Hadjerci%2520and%2520Antoine%2520Letienne%2520and%2520Mohamed%2520Abbas%2520Hedjazi%2520and%2520Adel%2520Hafiane%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520for%250Alearning%2520visual%2520representations.%2520While%2520recent%2520SSL%2520approaches%2520achieve%2520strong%250Aresults%2520in%2520global%2520image%2520understanding%252C%2520they%2520are%2520limited%2520in%2520capturing%2520the%250Astructured%2520representation%2520in%2520scenes.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520self-supervised%250Aapproach%2520that%2520progressively%2520builds%2520structured%2520visual%2520representations%2520by%250Acombining%2520semantic%2520grouping%252C%2520instance%2520level%2520separation%252C%2520and%2520hierarchical%250Astructuring.%2520Our%2520approach%252C%2520based%2520on%2520a%2520novel%2520ProtoScale%2520module%252C%2520captures%2520visual%250Aelements%2520across%2520multiple%2520spatial%2520scales.%2520Unlike%2520common%2520strategies%2520like%2520DINO%250Athat%2520rely%2520on%2520random%2520cropping%2520and%2520global%2520embeddings%252C%2520we%2520preserve%2520full%2520scene%250Acontext%2520across%2520augmented%2520views%2520to%2520improve%2520performance%2520in%2520dense%2520prediction%250Atasks.%2520We%2520validate%2520our%2520method%2520on%2520downstream%2520object%2520detection%2520tasks%2520using%2520a%250Acombined%2520subset%2520of%2520multiple%2520datasets%2520%2528COCO%2520and%2520UA-DETRAC%2529.%2520Experimental%2520results%250Ashow%2520that%2520our%2520method%2520learns%2520object%2520centric%2520representations%2520that%2520enhance%250Asupervised%2520object%2520detection%2520and%2520outperform%2520the%2520state-of-the-art%2520methods%252C%2520even%250Awhen%2520trained%2520with%2520limited%2520annotated%2520data%2520and%2520fewer%2520fine-tuning%2520epochs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20structured%20object%20representation%20learning&entry.906535625=Oussama%20Hadjerci%20and%20Antoine%20Letienne%20and%20Mohamed%20Abbas%20Hedjazi%20and%20Adel%20Hafiane&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20technique%20for%0Alearning%20visual%20representations.%20While%20recent%20SSL%20approaches%20achieve%20strong%0Aresults%20in%20global%20image%20understanding%2C%20they%20are%20limited%20in%20capturing%20the%0Astructured%20representation%20in%20scenes.%20In%20this%20work%2C%20we%20propose%20a%20self-supervised%0Aapproach%20that%20progressively%20builds%20structured%20visual%20representations%20by%0Acombining%20semantic%20grouping%2C%20instance%20level%20separation%2C%20and%20hierarchical%0Astructuring.%20Our%20approach%2C%20based%20on%20a%20novel%20ProtoScale%20module%2C%20captures%20visual%0Aelements%20across%20multiple%20spatial%20scales.%20Unlike%20common%20strategies%20like%20DINO%0Athat%20rely%20on%20random%20cropping%20and%20global%20embeddings%2C%20we%20preserve%20full%20scene%0Acontext%20across%20augmented%20views%20to%20improve%20performance%20in%20dense%20prediction%0Atasks.%20We%20validate%20our%20method%20on%20downstream%20object%20detection%20tasks%20using%20a%0Acombined%20subset%20of%20multiple%20datasets%20%28COCO%20and%20UA-DETRAC%29.%20Experimental%20results%0Ashow%20that%20our%20method%20learns%20object%20centric%20representations%20that%20enhance%0Asupervised%20object%20detection%20and%20outperform%20the%20state-of-the-art%20methods%2C%20even%0Awhen%20trained%20with%20limited%20annotated%20data%20and%20fewer%20fine-tuning%20epochs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19864v1&entry.124074799=Read"},
{"title": "GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation", "author": "Ken Deng and Yunhan Yang and Jingxiang Sun and Xihui Liu and Yebin Liu and Ding Liang and Yan-Pei Cao", "abstract": "  We introduce GeoSAM2, a prompt-controllable framework for 3D part\nsegmentation that casts the task as multi-view 2D mask prediction. Given a\ntextureless object, we render normal and point maps from predefined viewpoints\nand accept simple 2D prompts - clicks or boxes - to guide part selection. These\nprompts are processed by a shared SAM2 backbone augmented with LoRA and\nresidual geometry fusion, enabling view-specific reasoning while preserving\npretrained priors. The predicted masks are back-projected to the object and\naggregated across views. Our method enables fine-grained, part-specific control\nwithout requiring text prompts, per-shape optimization, or full 3D labels. In\ncontrast to global clustering or scale-based methods, prompts are explicit,\nspatially grounded, and interpretable. We achieve state-of-the-art\nclass-agnostic performance on PartObjaverse-Tiny and PartNetE, outperforming\nboth slow optimization-based pipelines and fast but coarse feedforward\napproaches. Our results highlight a new paradigm: aligning the paradigm of 3D\nsegmentation with SAM2, leveraging interactive 2D inputs to unlock\ncontrollability and precision in object-level part understanding.\n", "link": "http://arxiv.org/abs/2508.14036v2", "date": "2025-08-27", "relevancy": 2.9373, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6065}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5848}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSAM2%3A%20Unleashing%20the%20Power%20of%20SAM2%20for%203D%20Part%20Segmentation&body=Title%3A%20GeoSAM2%3A%20Unleashing%20the%20Power%20of%20SAM2%20for%203D%20Part%20Segmentation%0AAuthor%3A%20Ken%20Deng%20and%20Yunhan%20Yang%20and%20Jingxiang%20Sun%20and%20Xihui%20Liu%20and%20Yebin%20Liu%20and%20Ding%20Liang%20and%20Yan-Pei%20Cao%0AAbstract%3A%20%20%20We%20introduce%20GeoSAM2%2C%20a%20prompt-controllable%20framework%20for%203D%20part%0Asegmentation%20that%20casts%20the%20task%20as%20multi-view%202D%20mask%20prediction.%20Given%20a%0Atextureless%20object%2C%20we%20render%20normal%20and%20point%20maps%20from%20predefined%20viewpoints%0Aand%20accept%20simple%202D%20prompts%20-%20clicks%20or%20boxes%20-%20to%20guide%20part%20selection.%20These%0Aprompts%20are%20processed%20by%20a%20shared%20SAM2%20backbone%20augmented%20with%20LoRA%20and%0Aresidual%20geometry%20fusion%2C%20enabling%20view-specific%20reasoning%20while%20preserving%0Apretrained%20priors.%20The%20predicted%20masks%20are%20back-projected%20to%20the%20object%20and%0Aaggregated%20across%20views.%20Our%20method%20enables%20fine-grained%2C%20part-specific%20control%0Awithout%20requiring%20text%20prompts%2C%20per-shape%20optimization%2C%20or%20full%203D%20labels.%20In%0Acontrast%20to%20global%20clustering%20or%20scale-based%20methods%2C%20prompts%20are%20explicit%2C%0Aspatially%20grounded%2C%20and%20interpretable.%20We%20achieve%20state-of-the-art%0Aclass-agnostic%20performance%20on%20PartObjaverse-Tiny%20and%20PartNetE%2C%20outperforming%0Aboth%20slow%20optimization-based%20pipelines%20and%20fast%20but%20coarse%20feedforward%0Aapproaches.%20Our%20results%20highlight%20a%20new%20paradigm%3A%20aligning%20the%20paradigm%20of%203D%0Asegmentation%20with%20SAM2%2C%20leveraging%20interactive%202D%20inputs%20to%20unlock%0Acontrollability%20and%20precision%20in%20object-level%20part%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.14036v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSAM2%253A%2520Unleashing%2520the%2520Power%2520of%2520SAM2%2520for%25203D%2520Part%2520Segmentation%26entry.906535625%3DKen%2520Deng%2520and%2520Yunhan%2520Yang%2520and%2520Jingxiang%2520Sun%2520and%2520Xihui%2520Liu%2520and%2520Yebin%2520Liu%2520and%2520Ding%2520Liang%2520and%2520Yan-Pei%2520Cao%26entry.1292438233%3D%2520%2520We%2520introduce%2520GeoSAM2%252C%2520a%2520prompt-controllable%2520framework%2520for%25203D%2520part%250Asegmentation%2520that%2520casts%2520the%2520task%2520as%2520multi-view%25202D%2520mask%2520prediction.%2520Given%2520a%250Atextureless%2520object%252C%2520we%2520render%2520normal%2520and%2520point%2520maps%2520from%2520predefined%2520viewpoints%250Aand%2520accept%2520simple%25202D%2520prompts%2520-%2520clicks%2520or%2520boxes%2520-%2520to%2520guide%2520part%2520selection.%2520These%250Aprompts%2520are%2520processed%2520by%2520a%2520shared%2520SAM2%2520backbone%2520augmented%2520with%2520LoRA%2520and%250Aresidual%2520geometry%2520fusion%252C%2520enabling%2520view-specific%2520reasoning%2520while%2520preserving%250Apretrained%2520priors.%2520The%2520predicted%2520masks%2520are%2520back-projected%2520to%2520the%2520object%2520and%250Aaggregated%2520across%2520views.%2520Our%2520method%2520enables%2520fine-grained%252C%2520part-specific%2520control%250Awithout%2520requiring%2520text%2520prompts%252C%2520per-shape%2520optimization%252C%2520or%2520full%25203D%2520labels.%2520In%250Acontrast%2520to%2520global%2520clustering%2520or%2520scale-based%2520methods%252C%2520prompts%2520are%2520explicit%252C%250Aspatially%2520grounded%252C%2520and%2520interpretable.%2520We%2520achieve%2520state-of-the-art%250Aclass-agnostic%2520performance%2520on%2520PartObjaverse-Tiny%2520and%2520PartNetE%252C%2520outperforming%250Aboth%2520slow%2520optimization-based%2520pipelines%2520and%2520fast%2520but%2520coarse%2520feedforward%250Aapproaches.%2520Our%2520results%2520highlight%2520a%2520new%2520paradigm%253A%2520aligning%2520the%2520paradigm%2520of%25203D%250Asegmentation%2520with%2520SAM2%252C%2520leveraging%2520interactive%25202D%2520inputs%2520to%2520unlock%250Acontrollability%2520and%2520precision%2520in%2520object-level%2520part%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14036v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSAM2%3A%20Unleashing%20the%20Power%20of%20SAM2%20for%203D%20Part%20Segmentation&entry.906535625=Ken%20Deng%20and%20Yunhan%20Yang%20and%20Jingxiang%20Sun%20and%20Xihui%20Liu%20and%20Yebin%20Liu%20and%20Ding%20Liang%20and%20Yan-Pei%20Cao&entry.1292438233=%20%20We%20introduce%20GeoSAM2%2C%20a%20prompt-controllable%20framework%20for%203D%20part%0Asegmentation%20that%20casts%20the%20task%20as%20multi-view%202D%20mask%20prediction.%20Given%20a%0Atextureless%20object%2C%20we%20render%20normal%20and%20point%20maps%20from%20predefined%20viewpoints%0Aand%20accept%20simple%202D%20prompts%20-%20clicks%20or%20boxes%20-%20to%20guide%20part%20selection.%20These%0Aprompts%20are%20processed%20by%20a%20shared%20SAM2%20backbone%20augmented%20with%20LoRA%20and%0Aresidual%20geometry%20fusion%2C%20enabling%20view-specific%20reasoning%20while%20preserving%0Apretrained%20priors.%20The%20predicted%20masks%20are%20back-projected%20to%20the%20object%20and%0Aaggregated%20across%20views.%20Our%20method%20enables%20fine-grained%2C%20part-specific%20control%0Awithout%20requiring%20text%20prompts%2C%20per-shape%20optimization%2C%20or%20full%203D%20labels.%20In%0Acontrast%20to%20global%20clustering%20or%20scale-based%20methods%2C%20prompts%20are%20explicit%2C%0Aspatially%20grounded%2C%20and%20interpretable.%20We%20achieve%20state-of-the-art%0Aclass-agnostic%20performance%20on%20PartObjaverse-Tiny%20and%20PartNetE%2C%20outperforming%0Aboth%20slow%20optimization-based%20pipelines%20and%20fast%20but%20coarse%20feedforward%0Aapproaches.%20Our%20results%20highlight%20a%20new%20paradigm%3A%20aligning%20the%20paradigm%20of%203D%0Asegmentation%20with%20SAM2%2C%20leveraging%20interactive%202D%20inputs%20to%20unlock%0Acontrollability%20and%20precision%20in%20object-level%20part%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.14036v2&entry.124074799=Read"},
{"title": "TAGS: 3D Tumor-Adaptive Guidance for SAM", "author": "Sirui Li and Linkai Peng and Zheyuan Zhang and Gorkem Durak and Ulas Bagci", "abstract": "  Foundation models (FMs) such as CLIP and SAM have recently shown great\npromise in image segmentation tasks, yet their adaptation to 3D medical\nimaging-particularly for pathology detection and segmentation-remains\nunderexplored. A critical challenge arises from the domain gap between natural\nimages and medical volumes: existing FMs, pre-trained on 2D data, struggle to\ncapture 3D anatomical context, limiting their utility in clinical applications\nlike tumor segmentation. To address this, we propose an adaptation framework\ncalled TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D\nmedical tasks through multi-prompt fusion. By preserving most of the\npre-trained weights, our approach enhances SAM's spatial feature extraction\nusing CLIP's semantic insights and anatomy-specific prompts. Extensive\nexperiments on three open-source tumor segmentation datasets prove that our\nmodel surpasses the state-of-the-art medical image segmentation models (+46.88%\nover nnUNet), interactive segmentation frameworks, and other established\nmedical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and\nSAM-B (at least +13% over them). This highlights the robustness and\nadaptability of our proposed framework across diverse medical segmentation\ntasks.\n", "link": "http://arxiv.org/abs/2505.17096v2", "date": "2025-08-27", "relevancy": 2.8918, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6567}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5413}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAGS%3A%203D%20Tumor-Adaptive%20Guidance%20for%20SAM&body=Title%3A%20TAGS%3A%203D%20Tumor-Adaptive%20Guidance%20for%20SAM%0AAuthor%3A%20Sirui%20Li%20and%20Linkai%20Peng%20and%20Zheyuan%20Zhang%20and%20Gorkem%20Durak%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20such%20as%20CLIP%20and%20SAM%20have%20recently%20shown%20great%0Apromise%20in%20image%20segmentation%20tasks%2C%20yet%20their%20adaptation%20to%203D%20medical%0Aimaging-particularly%20for%20pathology%20detection%20and%20segmentation-remains%0Aunderexplored.%20A%20critical%20challenge%20arises%20from%20the%20domain%20gap%20between%20natural%0Aimages%20and%20medical%20volumes%3A%20existing%20FMs%2C%20pre-trained%20on%202D%20data%2C%20struggle%20to%0Acapture%203D%20anatomical%20context%2C%20limiting%20their%20utility%20in%20clinical%20applications%0Alike%20tumor%20segmentation.%20To%20address%20this%2C%20we%20propose%20an%20adaptation%20framework%0Acalled%20TAGS%3A%20Tumor%20Adaptive%20Guidance%20for%20SAM%2C%20which%20unlocks%202D%20FMs%20for%203D%0Amedical%20tasks%20through%20multi-prompt%20fusion.%20By%20preserving%20most%20of%20the%0Apre-trained%20weights%2C%20our%20approach%20enhances%20SAM%27s%20spatial%20feature%20extraction%0Ausing%20CLIP%27s%20semantic%20insights%20and%20anatomy-specific%20prompts.%20Extensive%0Aexperiments%20on%20three%20open-source%20tumor%20segmentation%20datasets%20prove%20that%20our%0Amodel%20surpasses%20the%20state-of-the-art%20medical%20image%20segmentation%20models%20%28%2B46.88%25%0Aover%20nnUNet%29%2C%20interactive%20segmentation%20frameworks%2C%20and%20other%20established%0Amedical%20FMs%2C%20including%20SAM-Med2D%2C%20SAM-Med3D%2C%20SegVol%2C%20Universal%2C%203D-Adapter%2C%20and%0ASAM-B%20%28at%20least%20%2B13%25%20over%20them%29.%20This%20highlights%20the%20robustness%20and%0Aadaptability%20of%20our%20proposed%20framework%20across%20diverse%20medical%20segmentation%0Atasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17096v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAGS%253A%25203D%2520Tumor-Adaptive%2520Guidance%2520for%2520SAM%26entry.906535625%3DSirui%2520Li%2520and%2520Linkai%2520Peng%2520and%2520Zheyuan%2520Zhang%2520and%2520Gorkem%2520Durak%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520such%2520as%2520CLIP%2520and%2520SAM%2520have%2520recently%2520shown%2520great%250Apromise%2520in%2520image%2520segmentation%2520tasks%252C%2520yet%2520their%2520adaptation%2520to%25203D%2520medical%250Aimaging-particularly%2520for%2520pathology%2520detection%2520and%2520segmentation-remains%250Aunderexplored.%2520A%2520critical%2520challenge%2520arises%2520from%2520the%2520domain%2520gap%2520between%2520natural%250Aimages%2520and%2520medical%2520volumes%253A%2520existing%2520FMs%252C%2520pre-trained%2520on%25202D%2520data%252C%2520struggle%2520to%250Acapture%25203D%2520anatomical%2520context%252C%2520limiting%2520their%2520utility%2520in%2520clinical%2520applications%250Alike%2520tumor%2520segmentation.%2520To%2520address%2520this%252C%2520we%2520propose%2520an%2520adaptation%2520framework%250Acalled%2520TAGS%253A%2520Tumor%2520Adaptive%2520Guidance%2520for%2520SAM%252C%2520which%2520unlocks%25202D%2520FMs%2520for%25203D%250Amedical%2520tasks%2520through%2520multi-prompt%2520fusion.%2520By%2520preserving%2520most%2520of%2520the%250Apre-trained%2520weights%252C%2520our%2520approach%2520enhances%2520SAM%2527s%2520spatial%2520feature%2520extraction%250Ausing%2520CLIP%2527s%2520semantic%2520insights%2520and%2520anatomy-specific%2520prompts.%2520Extensive%250Aexperiments%2520on%2520three%2520open-source%2520tumor%2520segmentation%2520datasets%2520prove%2520that%2520our%250Amodel%2520surpasses%2520the%2520state-of-the-art%2520medical%2520image%2520segmentation%2520models%2520%2528%252B46.88%2525%250Aover%2520nnUNet%2529%252C%2520interactive%2520segmentation%2520frameworks%252C%2520and%2520other%2520established%250Amedical%2520FMs%252C%2520including%2520SAM-Med2D%252C%2520SAM-Med3D%252C%2520SegVol%252C%2520Universal%252C%25203D-Adapter%252C%2520and%250ASAM-B%2520%2528at%2520least%2520%252B13%2525%2520over%2520them%2529.%2520This%2520highlights%2520the%2520robustness%2520and%250Aadaptability%2520of%2520our%2520proposed%2520framework%2520across%2520diverse%2520medical%2520segmentation%250Atasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17096v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAGS%3A%203D%20Tumor-Adaptive%20Guidance%20for%20SAM&entry.906535625=Sirui%20Li%20and%20Linkai%20Peng%20and%20Zheyuan%20Zhang%20and%20Gorkem%20Durak%20and%20Ulas%20Bagci&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20such%20as%20CLIP%20and%20SAM%20have%20recently%20shown%20great%0Apromise%20in%20image%20segmentation%20tasks%2C%20yet%20their%20adaptation%20to%203D%20medical%0Aimaging-particularly%20for%20pathology%20detection%20and%20segmentation-remains%0Aunderexplored.%20A%20critical%20challenge%20arises%20from%20the%20domain%20gap%20between%20natural%0Aimages%20and%20medical%20volumes%3A%20existing%20FMs%2C%20pre-trained%20on%202D%20data%2C%20struggle%20to%0Acapture%203D%20anatomical%20context%2C%20limiting%20their%20utility%20in%20clinical%20applications%0Alike%20tumor%20segmentation.%20To%20address%20this%2C%20we%20propose%20an%20adaptation%20framework%0Acalled%20TAGS%3A%20Tumor%20Adaptive%20Guidance%20for%20SAM%2C%20which%20unlocks%202D%20FMs%20for%203D%0Amedical%20tasks%20through%20multi-prompt%20fusion.%20By%20preserving%20most%20of%20the%0Apre-trained%20weights%2C%20our%20approach%20enhances%20SAM%27s%20spatial%20feature%20extraction%0Ausing%20CLIP%27s%20semantic%20insights%20and%20anatomy-specific%20prompts.%20Extensive%0Aexperiments%20on%20three%20open-source%20tumor%20segmentation%20datasets%20prove%20that%20our%0Amodel%20surpasses%20the%20state-of-the-art%20medical%20image%20segmentation%20models%20%28%2B46.88%25%0Aover%20nnUNet%29%2C%20interactive%20segmentation%20frameworks%2C%20and%20other%20established%0Amedical%20FMs%2C%20including%20SAM-Med2D%2C%20SAM-Med3D%2C%20SegVol%2C%20Universal%2C%203D-Adapter%2C%20and%0ASAM-B%20%28at%20least%20%2B13%25%20over%20them%29.%20This%20highlights%20the%20robustness%20and%0Aadaptability%20of%20our%20proposed%20framework%20across%20diverse%20medical%20segmentation%0Atasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17096v2&entry.124074799=Read"},
{"title": "Do Vision Encoders Truly Explain Object Hallucination?: Mitigating\n  Object Hallucination via Simple Fine-Grained CLIPScore", "author": "Hongseok Oh and Wonseok Hwang", "abstract": "  Recently, Large Vision-Language Models (LVLMs) show remarkable performance\nacross various domains. However, these models suffer from object hallucination.\nThis study revisits the previous claim that the cause of such hallucinations\nlies in the limited representational capacity of the vision encoder. Our\nanalysis implies that the capacity of the vision encoder is not necessarily a\nmajor limiting factor in detecting object hallucination. Based on this insight,\nwe propose Fine-grained CLIPScore (F-CLIPScore), a simple yet effective\nevaluation metric that enhances object-level granularity by incorporating text\nembeddings at the noun level. Evaluations on the OHD-Caps benchmark show that\nF-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a\nlarge margin of \\textbf{39.6\\%} without additional training. We further\ndemonstrate that F-CLIPScore-based data filtering reduces object hallucination\nin LVLM (4.9\\% in POPE).\n", "link": "http://arxiv.org/abs/2502.20034v3", "date": "2025-08-27", "relevancy": 2.8313, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.596}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Vision%20Encoders%20Truly%20Explain%20Object%20Hallucination%3F%3A%20Mitigating%0A%20%20Object%20Hallucination%20via%20Simple%20Fine-Grained%20CLIPScore&body=Title%3A%20Do%20Vision%20Encoders%20Truly%20Explain%20Object%20Hallucination%3F%3A%20Mitigating%0A%20%20Object%20Hallucination%20via%20Simple%20Fine-Grained%20CLIPScore%0AAuthor%3A%20Hongseok%20Oh%20and%20Wonseok%20Hwang%0AAbstract%3A%20%20%20Recently%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20show%20remarkable%20performance%0Aacross%20various%20domains.%20However%2C%20these%20models%20suffer%20from%20object%20hallucination.%0AThis%20study%20revisits%20the%20previous%20claim%20that%20the%20cause%20of%20such%20hallucinations%0Alies%20in%20the%20limited%20representational%20capacity%20of%20the%20vision%20encoder.%20Our%0Aanalysis%20implies%20that%20the%20capacity%20of%20the%20vision%20encoder%20is%20not%20necessarily%20a%0Amajor%20limiting%20factor%20in%20detecting%20object%20hallucination.%20Based%20on%20this%20insight%2C%0Awe%20propose%20Fine-grained%20CLIPScore%20%28F-CLIPScore%29%2C%20a%20simple%20yet%20effective%0Aevaluation%20metric%20that%20enhances%20object-level%20granularity%20by%20incorporating%20text%0Aembeddings%20at%20the%20noun%20level.%20Evaluations%20on%20the%20OHD-Caps%20benchmark%20show%20that%0AF-CLIPScore%20significantly%20outperforms%20conventional%20CLIPScore%20in%20accuracy%20by%20a%0Alarge%20margin%20of%20%5Ctextbf%7B39.6%5C%25%7D%20without%20additional%20training.%20We%20further%0Ademonstrate%20that%20F-CLIPScore-based%20data%20filtering%20reduces%20object%20hallucination%0Ain%20LVLM%20%284.9%5C%25%20in%20POPE%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20034v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Vision%2520Encoders%2520Truly%2520Explain%2520Object%2520Hallucination%253F%253A%2520Mitigating%250A%2520%2520Object%2520Hallucination%2520via%2520Simple%2520Fine-Grained%2520CLIPScore%26entry.906535625%3DHongseok%2520Oh%2520and%2520Wonseok%2520Hwang%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520show%2520remarkable%2520performance%250Aacross%2520various%2520domains.%2520However%252C%2520these%2520models%2520suffer%2520from%2520object%2520hallucination.%250AThis%2520study%2520revisits%2520the%2520previous%2520claim%2520that%2520the%2520cause%2520of%2520such%2520hallucinations%250Alies%2520in%2520the%2520limited%2520representational%2520capacity%2520of%2520the%2520vision%2520encoder.%2520Our%250Aanalysis%2520implies%2520that%2520the%2520capacity%2520of%2520the%2520vision%2520encoder%2520is%2520not%2520necessarily%2520a%250Amajor%2520limiting%2520factor%2520in%2520detecting%2520object%2520hallucination.%2520Based%2520on%2520this%2520insight%252C%250Awe%2520propose%2520Fine-grained%2520CLIPScore%2520%2528F-CLIPScore%2529%252C%2520a%2520simple%2520yet%2520effective%250Aevaluation%2520metric%2520that%2520enhances%2520object-level%2520granularity%2520by%2520incorporating%2520text%250Aembeddings%2520at%2520the%2520noun%2520level.%2520Evaluations%2520on%2520the%2520OHD-Caps%2520benchmark%2520show%2520that%250AF-CLIPScore%2520significantly%2520outperforms%2520conventional%2520CLIPScore%2520in%2520accuracy%2520by%2520a%250Alarge%2520margin%2520of%2520%255Ctextbf%257B39.6%255C%2525%257D%2520without%2520additional%2520training.%2520We%2520further%250Ademonstrate%2520that%2520F-CLIPScore-based%2520data%2520filtering%2520reduces%2520object%2520hallucination%250Ain%2520LVLM%2520%25284.9%255C%2525%2520in%2520POPE%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20034v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision%20Encoders%20Truly%20Explain%20Object%20Hallucination%3F%3A%20Mitigating%0A%20%20Object%20Hallucination%20via%20Simple%20Fine-Grained%20CLIPScore&entry.906535625=Hongseok%20Oh%20and%20Wonseok%20Hwang&entry.1292438233=%20%20Recently%2C%20Large%20Vision-Language%20Models%20%28LVLMs%29%20show%20remarkable%20performance%0Aacross%20various%20domains.%20However%2C%20these%20models%20suffer%20from%20object%20hallucination.%0AThis%20study%20revisits%20the%20previous%20claim%20that%20the%20cause%20of%20such%20hallucinations%0Alies%20in%20the%20limited%20representational%20capacity%20of%20the%20vision%20encoder.%20Our%0Aanalysis%20implies%20that%20the%20capacity%20of%20the%20vision%20encoder%20is%20not%20necessarily%20a%0Amajor%20limiting%20factor%20in%20detecting%20object%20hallucination.%20Based%20on%20this%20insight%2C%0Awe%20propose%20Fine-grained%20CLIPScore%20%28F-CLIPScore%29%2C%20a%20simple%20yet%20effective%0Aevaluation%20metric%20that%20enhances%20object-level%20granularity%20by%20incorporating%20text%0Aembeddings%20at%20the%20noun%20level.%20Evaluations%20on%20the%20OHD-Caps%20benchmark%20show%20that%0AF-CLIPScore%20significantly%20outperforms%20conventional%20CLIPScore%20in%20accuracy%20by%20a%0Alarge%20margin%20of%20%5Ctextbf%7B39.6%5C%25%7D%20without%20additional%20training.%20We%20further%0Ademonstrate%20that%20F-CLIPScore-based%20data%20filtering%20reduces%20object%20hallucination%0Ain%20LVLM%20%284.9%5C%25%20in%20POPE%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20034v3&entry.124074799=Read"},
{"title": "Decoding Dense Embeddings: Sparse Autoencoders for Interpreting and\n  Discretizing Dense Retrieval", "author": "Seongwan Park and Taeklim Kim and Youngjoong Ko", "abstract": "  Despite their strong performance, Dense Passage Retrieval (DPR) models suffer\nfrom a lack of interpretability. In this work, we propose a novel\ninterpretability framework that leverages Sparse Autoencoders (SAEs) to\ndecompose previously uninterpretable dense embeddings from DPR models into\ndistinct, interpretable latent concepts. We generate natural language\ndescriptions for each latent concept, enabling human interpretations of both\nthe dense embeddings and the query-document similarity scores of DPR models. We\nfurther introduce Concept-Level Sparse Retrieval (CL-SR), a retrieval framework\nthat directly utilizes the extracted latent concepts as indexing units. CL-SR\neffectively combines the semantic expressiveness of dense embeddings with the\ntransparency and efficiency of sparse representations. We show that CL-SR\nachieves high index-space and computational efficiency while maintaining robust\nperformance across vocabulary and semantic mismatches.\n", "link": "http://arxiv.org/abs/2506.00041v2", "date": "2025-08-27", "relevancy": 2.7884, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Dense%20Embeddings%3A%20Sparse%20Autoencoders%20for%20Interpreting%20and%0A%20%20Discretizing%20Dense%20Retrieval&body=Title%3A%20Decoding%20Dense%20Embeddings%3A%20Sparse%20Autoencoders%20for%20Interpreting%20and%0A%20%20Discretizing%20Dense%20Retrieval%0AAuthor%3A%20Seongwan%20Park%20and%20Taeklim%20Kim%20and%20Youngjoong%20Ko%0AAbstract%3A%20%20%20Despite%20their%20strong%20performance%2C%20Dense%20Passage%20Retrieval%20%28DPR%29%20models%20suffer%0Afrom%20a%20lack%20of%20interpretability.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Ainterpretability%20framework%20that%20leverages%20Sparse%20Autoencoders%20%28SAEs%29%20to%0Adecompose%20previously%20uninterpretable%20dense%20embeddings%20from%20DPR%20models%20into%0Adistinct%2C%20interpretable%20latent%20concepts.%20We%20generate%20natural%20language%0Adescriptions%20for%20each%20latent%20concept%2C%20enabling%20human%20interpretations%20of%20both%0Athe%20dense%20embeddings%20and%20the%20query-document%20similarity%20scores%20of%20DPR%20models.%20We%0Afurther%20introduce%20Concept-Level%20Sparse%20Retrieval%20%28CL-SR%29%2C%20a%20retrieval%20framework%0Athat%20directly%20utilizes%20the%20extracted%20latent%20concepts%20as%20indexing%20units.%20CL-SR%0Aeffectively%20combines%20the%20semantic%20expressiveness%20of%20dense%20embeddings%20with%20the%0Atransparency%20and%20efficiency%20of%20sparse%20representations.%20We%20show%20that%20CL-SR%0Aachieves%20high%20index-space%20and%20computational%20efficiency%20while%20maintaining%20robust%0Aperformance%20across%20vocabulary%20and%20semantic%20mismatches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Dense%2520Embeddings%253A%2520Sparse%2520Autoencoders%2520for%2520Interpreting%2520and%250A%2520%2520Discretizing%2520Dense%2520Retrieval%26entry.906535625%3DSeongwan%2520Park%2520and%2520Taeklim%2520Kim%2520and%2520Youngjoong%2520Ko%26entry.1292438233%3D%2520%2520Despite%2520their%2520strong%2520performance%252C%2520Dense%2520Passage%2520Retrieval%2520%2528DPR%2529%2520models%2520suffer%250Afrom%2520a%2520lack%2520of%2520interpretability.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Ainterpretability%2520framework%2520that%2520leverages%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520to%250Adecompose%2520previously%2520uninterpretable%2520dense%2520embeddings%2520from%2520DPR%2520models%2520into%250Adistinct%252C%2520interpretable%2520latent%2520concepts.%2520We%2520generate%2520natural%2520language%250Adescriptions%2520for%2520each%2520latent%2520concept%252C%2520enabling%2520human%2520interpretations%2520of%2520both%250Athe%2520dense%2520embeddings%2520and%2520the%2520query-document%2520similarity%2520scores%2520of%2520DPR%2520models.%2520We%250Afurther%2520introduce%2520Concept-Level%2520Sparse%2520Retrieval%2520%2528CL-SR%2529%252C%2520a%2520retrieval%2520framework%250Athat%2520directly%2520utilizes%2520the%2520extracted%2520latent%2520concepts%2520as%2520indexing%2520units.%2520CL-SR%250Aeffectively%2520combines%2520the%2520semantic%2520expressiveness%2520of%2520dense%2520embeddings%2520with%2520the%250Atransparency%2520and%2520efficiency%2520of%2520sparse%2520representations.%2520We%2520show%2520that%2520CL-SR%250Aachieves%2520high%2520index-space%2520and%2520computational%2520efficiency%2520while%2520maintaining%2520robust%250Aperformance%2520across%2520vocabulary%2520and%2520semantic%2520mismatches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Dense%20Embeddings%3A%20Sparse%20Autoencoders%20for%20Interpreting%20and%0A%20%20Discretizing%20Dense%20Retrieval&entry.906535625=Seongwan%20Park%20and%20Taeklim%20Kim%20and%20Youngjoong%20Ko&entry.1292438233=%20%20Despite%20their%20strong%20performance%2C%20Dense%20Passage%20Retrieval%20%28DPR%29%20models%20suffer%0Afrom%20a%20lack%20of%20interpretability.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Ainterpretability%20framework%20that%20leverages%20Sparse%20Autoencoders%20%28SAEs%29%20to%0Adecompose%20previously%20uninterpretable%20dense%20embeddings%20from%20DPR%20models%20into%0Adistinct%2C%20interpretable%20latent%20concepts.%20We%20generate%20natural%20language%0Adescriptions%20for%20each%20latent%20concept%2C%20enabling%20human%20interpretations%20of%20both%0Athe%20dense%20embeddings%20and%20the%20query-document%20similarity%20scores%20of%20DPR%20models.%20We%0Afurther%20introduce%20Concept-Level%20Sparse%20Retrieval%20%28CL-SR%29%2C%20a%20retrieval%20framework%0Athat%20directly%20utilizes%20the%20extracted%20latent%20concepts%20as%20indexing%20units.%20CL-SR%0Aeffectively%20combines%20the%20semantic%20expressiveness%20of%20dense%20embeddings%20with%20the%0Atransparency%20and%20efficiency%20of%20sparse%20representations.%20We%20show%20that%20CL-SR%0Aachieves%20high%20index-space%20and%20computational%20efficiency%20while%20maintaining%20robust%0Aperformance%20across%20vocabulary%20and%20semantic%20mismatches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00041v2&entry.124074799=Read"},
{"title": "NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More\n  Accurate and Interpretable CNNs", "author": "Davorin Mili\u010devi\u0107 and Ratko Grbi\u0107", "abstract": "  Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.\n", "link": "http://arxiv.org/abs/2508.19896v1", "date": "2025-08-27", "relevancy": 2.7712, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5913}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NM-Hebb%3A%20Coupling%20Local%20Hebbian%20Plasticity%20with%20Metric%20Learning%20for%20More%0A%20%20Accurate%20and%20Interpretable%20CNNs&body=Title%3A%20NM-Hebb%3A%20Coupling%20Local%20Hebbian%20Plasticity%20with%20Metric%20Learning%20for%20More%0A%20%20Accurate%20and%20Interpretable%20CNNs%0AAuthor%3A%20Davorin%20Mili%C4%8Devi%C4%87%20and%20Ratko%20Grbi%C4%87%0AAbstract%3A%20%20%20Deep%20Convolutional%20Neural%20Networks%20%28CNNs%29%20achieve%20high%20accuracy%20but%20often%0Arely%20on%20purely%20global%2C%20gradient-based%20optimisation%2C%20which%20can%20lead%20to%0Aoverfitting%2C%20redundant%20filters%2C%20and%20reduced%20interpretability.%20To%20address%20these%0Alimitations%2C%20we%20propose%20NM-Hebb%2C%20a%20two-phase%20training%20framework%20that%20integrates%0Aneuro-inspired%20local%20plasticity%20with%20distance-aware%20supervision.%20Phase%201%0Aextends%20standard%20supervised%20training%20by%20jointly%20optimising%20a%20cross-entropy%0Aobjective%20with%20two%20biologically%20inspired%20mechanisms%3A%20%28i%29%20a%20Hebbian%20regulariser%0Athat%20aligns%20the%20spatial%20mean%20of%20activations%20with%20the%20mean%20of%20the%20corresponding%0Aconvolutional%20filter%20weights%2C%20encouraging%20structured%2C%20reusable%20primitives%3B%20and%0A%28ii%29%20a%20learnable%20neuromodulator%20that%20gates%20an%20elastic-weight-style%0Aconsolidation%20loss%2C%20preserving%20beneficial%20parameters%20without%20freezing%20the%0Anetwork.%20Phase%202%20fine-tunes%20the%20backbone%20with%20a%20pairwise%20metric-learning%20loss%2C%0Aexplicitly%20compressing%20intra-class%20distances%20and%20enlarging%20inter-class%20margins%0Ain%20the%20embedding%20space.%20Evaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20TinyImageNet%0Aacross%20five%20backbones%20%28ResNet-18%2C%20VGG-11%2C%20MobileNet-v2%2C%20EfficientNet-V2%2C%0ADenseNet-121%29%2C%20NM-Hebb%20achieves%20consistent%20gains%20over%20baseline%20and%20other%0Amethods%3A%20Top-1%20accuracy%20improves%20by%20%2B2.0-10.0%20pp%20%28CIFAR-10%29%2C%20%2B2.0-9.0%20pp%0A%28CIFAR-100%29%2C%20and%20up%20to%20%2B4.3-8.9%20pp%20%28TinyImageNet%29%2C%20with%20Normalised%20Mutual%0AInformation%20%28NMI%29%20increased%20by%20up%20to%20%2B0.15.%20Qualitative%20visualisations%20and%0Afilter-level%20analyses%20further%20confirm%20that%20NM-Hebb%20produces%20more%20structured%20and%0Aselective%20features%2C%20yielding%20tighter%20and%20more%20interpretable%20class%20clusters.%0AOverall%2C%20coupling%20local%20Hebbian%20plasticity%20with%20metric-based%20fine-tuning%20yields%0ACNNs%20that%20are%20not%20only%20more%20accurate%20but%20also%20more%20interpretable%2C%20offering%0Apractical%20benefits%20for%20resource-constrained%20and%20safety-critical%20AI%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNM-Hebb%253A%2520Coupling%2520Local%2520Hebbian%2520Plasticity%2520with%2520Metric%2520Learning%2520for%2520More%250A%2520%2520Accurate%2520and%2520Interpretable%2520CNNs%26entry.906535625%3DDavorin%2520Mili%25C4%258Devi%25C4%2587%2520and%2520Ratko%2520Grbi%25C4%2587%26entry.1292438233%3D%2520%2520Deep%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520achieve%2520high%2520accuracy%2520but%2520often%250Arely%2520on%2520purely%2520global%252C%2520gradient-based%2520optimisation%252C%2520which%2520can%2520lead%2520to%250Aoverfitting%252C%2520redundant%2520filters%252C%2520and%2520reduced%2520interpretability.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520NM-Hebb%252C%2520a%2520two-phase%2520training%2520framework%2520that%2520integrates%250Aneuro-inspired%2520local%2520plasticity%2520with%2520distance-aware%2520supervision.%2520Phase%25201%250Aextends%2520standard%2520supervised%2520training%2520by%2520jointly%2520optimising%2520a%2520cross-entropy%250Aobjective%2520with%2520two%2520biologically%2520inspired%2520mechanisms%253A%2520%2528i%2529%2520a%2520Hebbian%2520regulariser%250Athat%2520aligns%2520the%2520spatial%2520mean%2520of%2520activations%2520with%2520the%2520mean%2520of%2520the%2520corresponding%250Aconvolutional%2520filter%2520weights%252C%2520encouraging%2520structured%252C%2520reusable%2520primitives%253B%2520and%250A%2528ii%2529%2520a%2520learnable%2520neuromodulator%2520that%2520gates%2520an%2520elastic-weight-style%250Aconsolidation%2520loss%252C%2520preserving%2520beneficial%2520parameters%2520without%2520freezing%2520the%250Anetwork.%2520Phase%25202%2520fine-tunes%2520the%2520backbone%2520with%2520a%2520pairwise%2520metric-learning%2520loss%252C%250Aexplicitly%2520compressing%2520intra-class%2520distances%2520and%2520enlarging%2520inter-class%2520margins%250Ain%2520the%2520embedding%2520space.%2520Evaluated%2520on%2520CIFAR-10%252C%2520CIFAR-100%252C%2520and%2520TinyImageNet%250Aacross%2520five%2520backbones%2520%2528ResNet-18%252C%2520VGG-11%252C%2520MobileNet-v2%252C%2520EfficientNet-V2%252C%250ADenseNet-121%2529%252C%2520NM-Hebb%2520achieves%2520consistent%2520gains%2520over%2520baseline%2520and%2520other%250Amethods%253A%2520Top-1%2520accuracy%2520improves%2520by%2520%252B2.0-10.0%2520pp%2520%2528CIFAR-10%2529%252C%2520%252B2.0-9.0%2520pp%250A%2528CIFAR-100%2529%252C%2520and%2520up%2520to%2520%252B4.3-8.9%2520pp%2520%2528TinyImageNet%2529%252C%2520with%2520Normalised%2520Mutual%250AInformation%2520%2528NMI%2529%2520increased%2520by%2520up%2520to%2520%252B0.15.%2520Qualitative%2520visualisations%2520and%250Afilter-level%2520analyses%2520further%2520confirm%2520that%2520NM-Hebb%2520produces%2520more%2520structured%2520and%250Aselective%2520features%252C%2520yielding%2520tighter%2520and%2520more%2520interpretable%2520class%2520clusters.%250AOverall%252C%2520coupling%2520local%2520Hebbian%2520plasticity%2520with%2520metric-based%2520fine-tuning%2520yields%250ACNNs%2520that%2520are%2520not%2520only%2520more%2520accurate%2520but%2520also%2520more%2520interpretable%252C%2520offering%250Apractical%2520benefits%2520for%2520resource-constrained%2520and%2520safety-critical%2520AI%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NM-Hebb%3A%20Coupling%20Local%20Hebbian%20Plasticity%20with%20Metric%20Learning%20for%20More%0A%20%20Accurate%20and%20Interpretable%20CNNs&entry.906535625=Davorin%20Mili%C4%8Devi%C4%87%20and%20Ratko%20Grbi%C4%87&entry.1292438233=%20%20Deep%20Convolutional%20Neural%20Networks%20%28CNNs%29%20achieve%20high%20accuracy%20but%20often%0Arely%20on%20purely%20global%2C%20gradient-based%20optimisation%2C%20which%20can%20lead%20to%0Aoverfitting%2C%20redundant%20filters%2C%20and%20reduced%20interpretability.%20To%20address%20these%0Alimitations%2C%20we%20propose%20NM-Hebb%2C%20a%20two-phase%20training%20framework%20that%20integrates%0Aneuro-inspired%20local%20plasticity%20with%20distance-aware%20supervision.%20Phase%201%0Aextends%20standard%20supervised%20training%20by%20jointly%20optimising%20a%20cross-entropy%0Aobjective%20with%20two%20biologically%20inspired%20mechanisms%3A%20%28i%29%20a%20Hebbian%20regulariser%0Athat%20aligns%20the%20spatial%20mean%20of%20activations%20with%20the%20mean%20of%20the%20corresponding%0Aconvolutional%20filter%20weights%2C%20encouraging%20structured%2C%20reusable%20primitives%3B%20and%0A%28ii%29%20a%20learnable%20neuromodulator%20that%20gates%20an%20elastic-weight-style%0Aconsolidation%20loss%2C%20preserving%20beneficial%20parameters%20without%20freezing%20the%0Anetwork.%20Phase%202%20fine-tunes%20the%20backbone%20with%20a%20pairwise%20metric-learning%20loss%2C%0Aexplicitly%20compressing%20intra-class%20distances%20and%20enlarging%20inter-class%20margins%0Ain%20the%20embedding%20space.%20Evaluated%20on%20CIFAR-10%2C%20CIFAR-100%2C%20and%20TinyImageNet%0Aacross%20five%20backbones%20%28ResNet-18%2C%20VGG-11%2C%20MobileNet-v2%2C%20EfficientNet-V2%2C%0ADenseNet-121%29%2C%20NM-Hebb%20achieves%20consistent%20gains%20over%20baseline%20and%20other%0Amethods%3A%20Top-1%20accuracy%20improves%20by%20%2B2.0-10.0%20pp%20%28CIFAR-10%29%2C%20%2B2.0-9.0%20pp%0A%28CIFAR-100%29%2C%20and%20up%20to%20%2B4.3-8.9%20pp%20%28TinyImageNet%29%2C%20with%20Normalised%20Mutual%0AInformation%20%28NMI%29%20increased%20by%20up%20to%20%2B0.15.%20Qualitative%20visualisations%20and%0Afilter-level%20analyses%20further%20confirm%20that%20NM-Hebb%20produces%20more%20structured%20and%0Aselective%20features%2C%20yielding%20tighter%20and%20more%20interpretable%20class%20clusters.%0AOverall%2C%20coupling%20local%20Hebbian%20plasticity%20with%20metric-based%20fine-tuning%20yields%0ACNNs%20that%20are%20not%20only%20more%20accurate%20but%20also%20more%20interpretable%2C%20offering%0Apractical%20benefits%20for%20resource-constrained%20and%20safety-critical%20AI%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19896v1&entry.124074799=Read"},
{"title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth\n  Prediction", "author": "Long Chen and Ashiv Patel and Mengyun Qiao and Mohammad Yousuf Salmasi and Salah A. Hammouche and Vasilis Stavrinides and Jasleen Nagi and Soodeh Kalaie and Xiao Yun Xu and Wenjia Bai and Declan P. O'Regan", "abstract": "  Personalized, accurate prediction of aortic aneurysm progression is essential\nfor timely intervention but remains challenging due to the need to model both\nsubtle local deformations and global anatomical changes within complex 3D\ngeometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh\ngenerative adversarial network for 3D aneurysm growth prediction. MCMeshGAN\nintroduces a dual-branch architecture combining a novel local KNN-based\nconvolutional network (KCN) to preserve fine-grained geometric details and a\nglobal graph convolutional network (GCN) to capture long-range structural\ncontext, overcoming the over-smoothing limitations of deep GCNs. A dedicated\ncondition branch encodes clinical attributes (age, sex) and the target time\ninterval to generate anatomically plausible, temporally controlled predictions,\nenabling retrospective and prospective modeling. We curated TAAMesh, a new\nlongitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal\nrecords (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive\nexperiments demonstrate that MCMeshGAN consistently outperforms\nstate-of-the-art baselines in both geometric accuracy and clinically important\ndiameter estimation. This framework offers a robust step toward clinically\ndeployable, personalized 3D disease trajectory modeling. The source code for\nMCMeshGAN and the baseline methods is publicly available at\nhttps://github.com/ImperialCollegeLondon/MCMeshGAN.\n", "link": "http://arxiv.org/abs/2508.19862v1", "date": "2025-08-27", "relevancy": 2.7157, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Conditional%20MeshGAN%20for%20Personalized%20Aneurysm%20Growth%0A%20%20Prediction&body=Title%3A%20Multimodal%20Conditional%20MeshGAN%20for%20Personalized%20Aneurysm%20Growth%0A%20%20Prediction%0AAuthor%3A%20Long%20Chen%20and%20Ashiv%20Patel%20and%20Mengyun%20Qiao%20and%20Mohammad%20Yousuf%20Salmasi%20and%20Salah%20A.%20Hammouche%20and%20Vasilis%20Stavrinides%20and%20Jasleen%20Nagi%20and%20Soodeh%20Kalaie%20and%20Xiao%20Yun%20Xu%20and%20Wenjia%20Bai%20and%20Declan%20P.%20O%27Regan%0AAbstract%3A%20%20%20Personalized%2C%20accurate%20prediction%20of%20aortic%20aneurysm%20progression%20is%20essential%0Afor%20timely%20intervention%20but%20remains%20challenging%20due%20to%20the%20need%20to%20model%20both%0Asubtle%20local%20deformations%20and%20global%20anatomical%20changes%20within%20complex%203D%0Ageometries.%20We%20propose%20MCMeshGAN%2C%20the%20first%20multimodal%20conditional%20mesh-to-mesh%0Agenerative%20adversarial%20network%20for%203D%20aneurysm%20growth%20prediction.%20MCMeshGAN%0Aintroduces%20a%20dual-branch%20architecture%20combining%20a%20novel%20local%20KNN-based%0Aconvolutional%20network%20%28KCN%29%20to%20preserve%20fine-grained%20geometric%20details%20and%20a%0Aglobal%20graph%20convolutional%20network%20%28GCN%29%20to%20capture%20long-range%20structural%0Acontext%2C%20overcoming%20the%20over-smoothing%20limitations%20of%20deep%20GCNs.%20A%20dedicated%0Acondition%20branch%20encodes%20clinical%20attributes%20%28age%2C%20sex%29%20and%20the%20target%20time%0Ainterval%20to%20generate%20anatomically%20plausible%2C%20temporally%20controlled%20predictions%2C%0Aenabling%20retrospective%20and%20prospective%20modeling.%20We%20curated%20TAAMesh%2C%20a%20new%0Alongitudinal%20thoracic%20aortic%20aneurysm%20mesh%20dataset%20consisting%20of%20590%20multimodal%0Arecords%20%28CT%20scans%2C%203D%20meshes%2C%20and%20clinical%20data%29%20from%20208%20patients.%20Extensive%0Aexperiments%20demonstrate%20that%20MCMeshGAN%20consistently%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20geometric%20accuracy%20and%20clinically%20important%0Adiameter%20estimation.%20This%20framework%20offers%20a%20robust%20step%20toward%20clinically%0Adeployable%2C%20personalized%203D%20disease%20trajectory%20modeling.%20The%20source%20code%20for%0AMCMeshGAN%20and%20the%20baseline%20methods%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ImperialCollegeLondon/MCMeshGAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Conditional%2520MeshGAN%2520for%2520Personalized%2520Aneurysm%2520Growth%250A%2520%2520Prediction%26entry.906535625%3DLong%2520Chen%2520and%2520Ashiv%2520Patel%2520and%2520Mengyun%2520Qiao%2520and%2520Mohammad%2520Yousuf%2520Salmasi%2520and%2520Salah%2520A.%2520Hammouche%2520and%2520Vasilis%2520Stavrinides%2520and%2520Jasleen%2520Nagi%2520and%2520Soodeh%2520Kalaie%2520and%2520Xiao%2520Yun%2520Xu%2520and%2520Wenjia%2520Bai%2520and%2520Declan%2520P.%2520O%2527Regan%26entry.1292438233%3D%2520%2520Personalized%252C%2520accurate%2520prediction%2520of%2520aortic%2520aneurysm%2520progression%2520is%2520essential%250Afor%2520timely%2520intervention%2520but%2520remains%2520challenging%2520due%2520to%2520the%2520need%2520to%2520model%2520both%250Asubtle%2520local%2520deformations%2520and%2520global%2520anatomical%2520changes%2520within%2520complex%25203D%250Ageometries.%2520We%2520propose%2520MCMeshGAN%252C%2520the%2520first%2520multimodal%2520conditional%2520mesh-to-mesh%250Agenerative%2520adversarial%2520network%2520for%25203D%2520aneurysm%2520growth%2520prediction.%2520MCMeshGAN%250Aintroduces%2520a%2520dual-branch%2520architecture%2520combining%2520a%2520novel%2520local%2520KNN-based%250Aconvolutional%2520network%2520%2528KCN%2529%2520to%2520preserve%2520fine-grained%2520geometric%2520details%2520and%2520a%250Aglobal%2520graph%2520convolutional%2520network%2520%2528GCN%2529%2520to%2520capture%2520long-range%2520structural%250Acontext%252C%2520overcoming%2520the%2520over-smoothing%2520limitations%2520of%2520deep%2520GCNs.%2520A%2520dedicated%250Acondition%2520branch%2520encodes%2520clinical%2520attributes%2520%2528age%252C%2520sex%2529%2520and%2520the%2520target%2520time%250Ainterval%2520to%2520generate%2520anatomically%2520plausible%252C%2520temporally%2520controlled%2520predictions%252C%250Aenabling%2520retrospective%2520and%2520prospective%2520modeling.%2520We%2520curated%2520TAAMesh%252C%2520a%2520new%250Alongitudinal%2520thoracic%2520aortic%2520aneurysm%2520mesh%2520dataset%2520consisting%2520of%2520590%2520multimodal%250Arecords%2520%2528CT%2520scans%252C%25203D%2520meshes%252C%2520and%2520clinical%2520data%2529%2520from%2520208%2520patients.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520MCMeshGAN%2520consistently%2520outperforms%250Astate-of-the-art%2520baselines%2520in%2520both%2520geometric%2520accuracy%2520and%2520clinically%2520important%250Adiameter%2520estimation.%2520This%2520framework%2520offers%2520a%2520robust%2520step%2520toward%2520clinically%250Adeployable%252C%2520personalized%25203D%2520disease%2520trajectory%2520modeling.%2520The%2520source%2520code%2520for%250AMCMeshGAN%2520and%2520the%2520baseline%2520methods%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/ImperialCollegeLondon/MCMeshGAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Conditional%20MeshGAN%20for%20Personalized%20Aneurysm%20Growth%0A%20%20Prediction&entry.906535625=Long%20Chen%20and%20Ashiv%20Patel%20and%20Mengyun%20Qiao%20and%20Mohammad%20Yousuf%20Salmasi%20and%20Salah%20A.%20Hammouche%20and%20Vasilis%20Stavrinides%20and%20Jasleen%20Nagi%20and%20Soodeh%20Kalaie%20and%20Xiao%20Yun%20Xu%20and%20Wenjia%20Bai%20and%20Declan%20P.%20O%27Regan&entry.1292438233=%20%20Personalized%2C%20accurate%20prediction%20of%20aortic%20aneurysm%20progression%20is%20essential%0Afor%20timely%20intervention%20but%20remains%20challenging%20due%20to%20the%20need%20to%20model%20both%0Asubtle%20local%20deformations%20and%20global%20anatomical%20changes%20within%20complex%203D%0Ageometries.%20We%20propose%20MCMeshGAN%2C%20the%20first%20multimodal%20conditional%20mesh-to-mesh%0Agenerative%20adversarial%20network%20for%203D%20aneurysm%20growth%20prediction.%20MCMeshGAN%0Aintroduces%20a%20dual-branch%20architecture%20combining%20a%20novel%20local%20KNN-based%0Aconvolutional%20network%20%28KCN%29%20to%20preserve%20fine-grained%20geometric%20details%20and%20a%0Aglobal%20graph%20convolutional%20network%20%28GCN%29%20to%20capture%20long-range%20structural%0Acontext%2C%20overcoming%20the%20over-smoothing%20limitations%20of%20deep%20GCNs.%20A%20dedicated%0Acondition%20branch%20encodes%20clinical%20attributes%20%28age%2C%20sex%29%20and%20the%20target%20time%0Ainterval%20to%20generate%20anatomically%20plausible%2C%20temporally%20controlled%20predictions%2C%0Aenabling%20retrospective%20and%20prospective%20modeling.%20We%20curated%20TAAMesh%2C%20a%20new%0Alongitudinal%20thoracic%20aortic%20aneurysm%20mesh%20dataset%20consisting%20of%20590%20multimodal%0Arecords%20%28CT%20scans%2C%203D%20meshes%2C%20and%20clinical%20data%29%20from%20208%20patients.%20Extensive%0Aexperiments%20demonstrate%20that%20MCMeshGAN%20consistently%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20geometric%20accuracy%20and%20clinically%20important%0Adiameter%20estimation.%20This%20framework%20offers%20a%20robust%20step%20toward%20clinically%0Adeployable%2C%20personalized%203D%20disease%20trajectory%20modeling.%20The%20source%20code%20for%0AMCMeshGAN%20and%20the%20baseline%20methods%20is%20publicly%20available%20at%0Ahttps%3A//github.com/ImperialCollegeLondon/MCMeshGAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19862v1&entry.124074799=Read"},
{"title": "HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology\n  Datasets with Foundational Embedding Models", "author": "Aakash Tripathi and Asim Waqas and Matthew B. Schabath and Yasin Yilmaz and Ghulam Rasool", "abstract": "  HONeYBEE (Harmonized ONcologY Biomedical Embedding Encoder) is an open-source\nframework that integrates multimodal biomedical data for oncology applications.\nIt processes clinical data (structured and unstructured), whole-slide images,\nradiology scans, and molecular profiles to generate unified patient-level\nembeddings using domain-specific foundation models and fusion strategies. These\nembeddings enable survival prediction, cancer-type classification, patient\nsimilarity retrieval, and cohort clustering. Evaluated on 11,400+ patients\nacross 33 cancer types from The Cancer Genome Atlas (TCGA), clinical embeddings\nshowed the strongest single-modality performance with 98.5% classification\naccuracy and 96.4% precision@10 in patient retrieval. They also achieved the\nhighest survival prediction concordance indices across most cancer types.\nMultimodal fusion provided complementary benefits for specific cancers,\nimproving overall survival prediction beyond clinical features alone.\nComparative evaluation of four large language models revealed that\ngeneral-purpose models like Qwen3 outperformed specialized medical models for\nclinical text representation, though task-specific fine-tuning improved\nperformance on heterogeneous data such as pathology reports.\n", "link": "http://arxiv.org/abs/2405.07460v5", "date": "2025-08-27", "relevancy": 2.7111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoneyBee%3A%20A%20Scalable%20Modular%20Framework%20for%20Creating%20Multimodal%20Oncology%0A%20%20Datasets%20with%20Foundational%20Embedding%20Models&body=Title%3A%20HoneyBee%3A%20A%20Scalable%20Modular%20Framework%20for%20Creating%20Multimodal%20Oncology%0A%20%20Datasets%20with%20Foundational%20Embedding%20Models%0AAuthor%3A%20Aakash%20Tripathi%20and%20Asim%20Waqas%20and%20Matthew%20B.%20Schabath%20and%20Yasin%20Yilmaz%20and%20Ghulam%20Rasool%0AAbstract%3A%20%20%20HONeYBEE%20%28Harmonized%20ONcologY%20Biomedical%20Embedding%20Encoder%29%20is%20an%20open-source%0Aframework%20that%20integrates%20multimodal%20biomedical%20data%20for%20oncology%20applications.%0AIt%20processes%20clinical%20data%20%28structured%20and%20unstructured%29%2C%20whole-slide%20images%2C%0Aradiology%20scans%2C%20and%20molecular%20profiles%20to%20generate%20unified%20patient-level%0Aembeddings%20using%20domain-specific%20foundation%20models%20and%20fusion%20strategies.%20These%0Aembeddings%20enable%20survival%20prediction%2C%20cancer-type%20classification%2C%20patient%0Asimilarity%20retrieval%2C%20and%20cohort%20clustering.%20Evaluated%20on%2011%2C400%2B%20patients%0Aacross%2033%20cancer%20types%20from%20The%20Cancer%20Genome%20Atlas%20%28TCGA%29%2C%20clinical%20embeddings%0Ashowed%20the%20strongest%20single-modality%20performance%20with%2098.5%25%20classification%0Aaccuracy%20and%2096.4%25%20precision%4010%20in%20patient%20retrieval.%20They%20also%20achieved%20the%0Ahighest%20survival%20prediction%20concordance%20indices%20across%20most%20cancer%20types.%0AMultimodal%20fusion%20provided%20complementary%20benefits%20for%20specific%20cancers%2C%0Aimproving%20overall%20survival%20prediction%20beyond%20clinical%20features%20alone.%0AComparative%20evaluation%20of%20four%20large%20language%20models%20revealed%20that%0Ageneral-purpose%20models%20like%20Qwen3%20outperformed%20specialized%20medical%20models%20for%0Aclinical%20text%20representation%2C%20though%20task-specific%20fine-tuning%20improved%0Aperformance%20on%20heterogeneous%20data%20such%20as%20pathology%20reports.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07460v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoneyBee%253A%2520A%2520Scalable%2520Modular%2520Framework%2520for%2520Creating%2520Multimodal%2520Oncology%250A%2520%2520Datasets%2520with%2520Foundational%2520Embedding%2520Models%26entry.906535625%3DAakash%2520Tripathi%2520and%2520Asim%2520Waqas%2520and%2520Matthew%2520B.%2520Schabath%2520and%2520Yasin%2520Yilmaz%2520and%2520Ghulam%2520Rasool%26entry.1292438233%3D%2520%2520HONeYBEE%2520%2528Harmonized%2520ONcologY%2520Biomedical%2520Embedding%2520Encoder%2529%2520is%2520an%2520open-source%250Aframework%2520that%2520integrates%2520multimodal%2520biomedical%2520data%2520for%2520oncology%2520applications.%250AIt%2520processes%2520clinical%2520data%2520%2528structured%2520and%2520unstructured%2529%252C%2520whole-slide%2520images%252C%250Aradiology%2520scans%252C%2520and%2520molecular%2520profiles%2520to%2520generate%2520unified%2520patient-level%250Aembeddings%2520using%2520domain-specific%2520foundation%2520models%2520and%2520fusion%2520strategies.%2520These%250Aembeddings%2520enable%2520survival%2520prediction%252C%2520cancer-type%2520classification%252C%2520patient%250Asimilarity%2520retrieval%252C%2520and%2520cohort%2520clustering.%2520Evaluated%2520on%252011%252C400%252B%2520patients%250Aacross%252033%2520cancer%2520types%2520from%2520The%2520Cancer%2520Genome%2520Atlas%2520%2528TCGA%2529%252C%2520clinical%2520embeddings%250Ashowed%2520the%2520strongest%2520single-modality%2520performance%2520with%252098.5%2525%2520classification%250Aaccuracy%2520and%252096.4%2525%2520precision%254010%2520in%2520patient%2520retrieval.%2520They%2520also%2520achieved%2520the%250Ahighest%2520survival%2520prediction%2520concordance%2520indices%2520across%2520most%2520cancer%2520types.%250AMultimodal%2520fusion%2520provided%2520complementary%2520benefits%2520for%2520specific%2520cancers%252C%250Aimproving%2520overall%2520survival%2520prediction%2520beyond%2520clinical%2520features%2520alone.%250AComparative%2520evaluation%2520of%2520four%2520large%2520language%2520models%2520revealed%2520that%250Ageneral-purpose%2520models%2520like%2520Qwen3%2520outperformed%2520specialized%2520medical%2520models%2520for%250Aclinical%2520text%2520representation%252C%2520though%2520task-specific%2520fine-tuning%2520improved%250Aperformance%2520on%2520heterogeneous%2520data%2520such%2520as%2520pathology%2520reports.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07460v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoneyBee%3A%20A%20Scalable%20Modular%20Framework%20for%20Creating%20Multimodal%20Oncology%0A%20%20Datasets%20with%20Foundational%20Embedding%20Models&entry.906535625=Aakash%20Tripathi%20and%20Asim%20Waqas%20and%20Matthew%20B.%20Schabath%20and%20Yasin%20Yilmaz%20and%20Ghulam%20Rasool&entry.1292438233=%20%20HONeYBEE%20%28Harmonized%20ONcologY%20Biomedical%20Embedding%20Encoder%29%20is%20an%20open-source%0Aframework%20that%20integrates%20multimodal%20biomedical%20data%20for%20oncology%20applications.%0AIt%20processes%20clinical%20data%20%28structured%20and%20unstructured%29%2C%20whole-slide%20images%2C%0Aradiology%20scans%2C%20and%20molecular%20profiles%20to%20generate%20unified%20patient-level%0Aembeddings%20using%20domain-specific%20foundation%20models%20and%20fusion%20strategies.%20These%0Aembeddings%20enable%20survival%20prediction%2C%20cancer-type%20classification%2C%20patient%0Asimilarity%20retrieval%2C%20and%20cohort%20clustering.%20Evaluated%20on%2011%2C400%2B%20patients%0Aacross%2033%20cancer%20types%20from%20The%20Cancer%20Genome%20Atlas%20%28TCGA%29%2C%20clinical%20embeddings%0Ashowed%20the%20strongest%20single-modality%20performance%20with%2098.5%25%20classification%0Aaccuracy%20and%2096.4%25%20precision%4010%20in%20patient%20retrieval.%20They%20also%20achieved%20the%0Ahighest%20survival%20prediction%20concordance%20indices%20across%20most%20cancer%20types.%0AMultimodal%20fusion%20provided%20complementary%20benefits%20for%20specific%20cancers%2C%0Aimproving%20overall%20survival%20prediction%20beyond%20clinical%20features%20alone.%0AComparative%20evaluation%20of%20four%20large%20language%20models%20revealed%20that%0Ageneral-purpose%20models%20like%20Qwen3%20outperformed%20specialized%20medical%20models%20for%0Aclinical%20text%20representation%2C%20though%20task-specific%20fine-tuning%20improved%0Aperformance%20on%20heterogeneous%20data%20such%20as%20pathology%20reports.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07460v5&entry.124074799=Read"},
{"title": "On Domain-Adaptive Post-Training for Multimodal Large Language Models", "author": "Daixuan Cheng and Shaohan Huang and Ziyu Zhu and Xintong Zhang and Wayne Xin Zhao and Zhongzhi Luan and Bo Dai and Zhenliang Zhang", "abstract": "  Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs via post-training, focusing on data synthesis,\ntraining pipeline, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models in enhancing domain-specific performance. (2) Training\nPipeline: Unlike general MLLMs that typically adopt a two-stage training\nparadigm, we find that a single-stage approach is more effective for domain\nadaptation. (3) Task Evaluation: We conduct extensive experiments in\nhigh-impact domains such as biomedicine, food, and remote sensing, by\npost-training a variety of MLLMs and then evaluating MLLM performance on\nvarious domain-specific tasks. Finally, we fully open-source our models, code,\nand data to encourage future research in this area.\n", "link": "http://arxiv.org/abs/2411.19930v4", "date": "2025-08-27", "relevancy": 2.6983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Domain-Adaptive%20Post-Training%20for%20Multimodal%20Large%20Language%20Models&body=Title%3A%20On%20Domain-Adaptive%20Post-Training%20for%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Daixuan%20Cheng%20and%20Shaohan%20Huang%20and%20Ziyu%20Zhu%20and%20Xintong%20Zhang%20and%20Wayne%20Xin%20Zhao%20and%20Zhongzhi%20Luan%20and%20Bo%20Dai%20and%20Zhenliang%20Zhang%0AAbstract%3A%20%20%20Adapting%20general%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20specific%0Adomains%2C%20such%20as%20scientific%20and%20industrial%20fields%2C%20is%20highly%20significant%20in%0Apromoting%20their%20practical%20applications.%20This%20paper%20systematically%20investigates%0Adomain%20adaptation%20of%20MLLMs%20via%20post-training%2C%20focusing%20on%20data%20synthesis%2C%0Atraining%20pipeline%2C%20and%20task%20evaluation.%20%281%29%20Data%20Synthesis%3A%20Using%20only%0Aopen-source%20models%2C%20we%20develop%20a%20generate-then-filter%20pipeline%20that%20curates%0Adiverse%20visual%20instruction%20tasks%20based%20on%20domain-specific%20image-caption%20pairs.%0AThe%20resulting%20data%20surpass%20the%20data%20synthesized%20by%20manual%20rules%20or%20strong%0Aclosed-source%20models%20in%20enhancing%20domain-specific%20performance.%20%282%29%20Training%0APipeline%3A%20Unlike%20general%20MLLMs%20that%20typically%20adopt%20a%20two-stage%20training%0Aparadigm%2C%20we%20find%20that%20a%20single-stage%20approach%20is%20more%20effective%20for%20domain%0Aadaptation.%20%283%29%20Task%20Evaluation%3A%20We%20conduct%20extensive%20experiments%20in%0Ahigh-impact%20domains%20such%20as%20biomedicine%2C%20food%2C%20and%20remote%20sensing%2C%20by%0Apost-training%20a%20variety%20of%20MLLMs%20and%20then%20evaluating%20MLLM%20performance%20on%0Avarious%20domain-specific%20tasks.%20Finally%2C%20we%20fully%20open-source%20our%20models%2C%20code%2C%0Aand%20data%20to%20encourage%20future%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19930v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Domain-Adaptive%2520Post-Training%2520for%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DDaixuan%2520Cheng%2520and%2520Shaohan%2520Huang%2520and%2520Ziyu%2520Zhu%2520and%2520Xintong%2520Zhang%2520and%2520Wayne%2520Xin%2520Zhao%2520and%2520Zhongzhi%2520Luan%2520and%2520Bo%2520Dai%2520and%2520Zhenliang%2520Zhang%26entry.1292438233%3D%2520%2520Adapting%2520general%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520specific%250Adomains%252C%2520such%2520as%2520scientific%2520and%2520industrial%2520fields%252C%2520is%2520highly%2520significant%2520in%250Apromoting%2520their%2520practical%2520applications.%2520This%2520paper%2520systematically%2520investigates%250Adomain%2520adaptation%2520of%2520MLLMs%2520via%2520post-training%252C%2520focusing%2520on%2520data%2520synthesis%252C%250Atraining%2520pipeline%252C%2520and%2520task%2520evaluation.%2520%25281%2529%2520Data%2520Synthesis%253A%2520Using%2520only%250Aopen-source%2520models%252C%2520we%2520develop%2520a%2520generate-then-filter%2520pipeline%2520that%2520curates%250Adiverse%2520visual%2520instruction%2520tasks%2520based%2520on%2520domain-specific%2520image-caption%2520pairs.%250AThe%2520resulting%2520data%2520surpass%2520the%2520data%2520synthesized%2520by%2520manual%2520rules%2520or%2520strong%250Aclosed-source%2520models%2520in%2520enhancing%2520domain-specific%2520performance.%2520%25282%2529%2520Training%250APipeline%253A%2520Unlike%2520general%2520MLLMs%2520that%2520typically%2520adopt%2520a%2520two-stage%2520training%250Aparadigm%252C%2520we%2520find%2520that%2520a%2520single-stage%2520approach%2520is%2520more%2520effective%2520for%2520domain%250Aadaptation.%2520%25283%2529%2520Task%2520Evaluation%253A%2520We%2520conduct%2520extensive%2520experiments%2520in%250Ahigh-impact%2520domains%2520such%2520as%2520biomedicine%252C%2520food%252C%2520and%2520remote%2520sensing%252C%2520by%250Apost-training%2520a%2520variety%2520of%2520MLLMs%2520and%2520then%2520evaluating%2520MLLM%2520performance%2520on%250Avarious%2520domain-specific%2520tasks.%2520Finally%252C%2520we%2520fully%2520open-source%2520our%2520models%252C%2520code%252C%250Aand%2520data%2520to%2520encourage%2520future%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19930v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Domain-Adaptive%20Post-Training%20for%20Multimodal%20Large%20Language%20Models&entry.906535625=Daixuan%20Cheng%20and%20Shaohan%20Huang%20and%20Ziyu%20Zhu%20and%20Xintong%20Zhang%20and%20Wayne%20Xin%20Zhao%20and%20Zhongzhi%20Luan%20and%20Bo%20Dai%20and%20Zhenliang%20Zhang&entry.1292438233=%20%20Adapting%20general%20multimodal%20large%20language%20models%20%28MLLMs%29%20to%20specific%0Adomains%2C%20such%20as%20scientific%20and%20industrial%20fields%2C%20is%20highly%20significant%20in%0Apromoting%20their%20practical%20applications.%20This%20paper%20systematically%20investigates%0Adomain%20adaptation%20of%20MLLMs%20via%20post-training%2C%20focusing%20on%20data%20synthesis%2C%0Atraining%20pipeline%2C%20and%20task%20evaluation.%20%281%29%20Data%20Synthesis%3A%20Using%20only%0Aopen-source%20models%2C%20we%20develop%20a%20generate-then-filter%20pipeline%20that%20curates%0Adiverse%20visual%20instruction%20tasks%20based%20on%20domain-specific%20image-caption%20pairs.%0AThe%20resulting%20data%20surpass%20the%20data%20synthesized%20by%20manual%20rules%20or%20strong%0Aclosed-source%20models%20in%20enhancing%20domain-specific%20performance.%20%282%29%20Training%0APipeline%3A%20Unlike%20general%20MLLMs%20that%20typically%20adopt%20a%20two-stage%20training%0Aparadigm%2C%20we%20find%20that%20a%20single-stage%20approach%20is%20more%20effective%20for%20domain%0Aadaptation.%20%283%29%20Task%20Evaluation%3A%20We%20conduct%20extensive%20experiments%20in%0Ahigh-impact%20domains%20such%20as%20biomedicine%2C%20food%2C%20and%20remote%20sensing%2C%20by%0Apost-training%20a%20variety%20of%20MLLMs%20and%20then%20evaluating%20MLLM%20performance%20on%0Avarious%20domain-specific%20tasks.%20Finally%2C%20we%20fully%20open-source%20our%20models%2C%20code%2C%0Aand%20data%20to%20encourage%20future%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19930v4&entry.124074799=Read"},
{"title": "Assessing the Geolocation Capabilities, Limitations and Societal Risks\n  of Generative Vision-Language Models", "author": "Oliver Grainge and Sania Waheed and Jack Stilgoe and Michael Milford and Shoaib Ehsan", "abstract": "  Geo-localization is the task of identifying the location of an image using\nvisual cues alone. It has beneficial applications, such as improving disaster\nresponse, enhancing navigation, and geography education. Recently,\nVision-Language Models (VLMs) are increasingly demonstrating capabilities as\naccurate image geo-locators. This brings significant privacy risks, including\nthose related to stalking and surveillance, considering the widespread uses of\nAI models and sharing of photos on social media. The precision of these models\nis likely to improve in the future. Despite these risks, there is little work\non systematically evaluating the geolocation precision of Generative VLMs,\ntheir limits and potential for unintended inferences. To bridge this gap, we\nconduct a comprehensive assessment of the geolocation capabilities of 25\nstate-of-the-art VLMs on four benchmark image datasets captured in diverse\nenvironments. Our results offer insight into the internal reasoning of VLMs and\nhighlight their strengths, limitations, and potential societal risks. Our\nfindings indicate that current VLMs perform poorly on generic street-level\nimages yet achieve notably high accuracy (61\\%) on images resembling social\nmedia content, raising significant and urgent privacy concerns.\n", "link": "http://arxiv.org/abs/2508.19967v1", "date": "2025-08-27", "relevancy": 2.6703, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20the%20Geolocation%20Capabilities%2C%20Limitations%20and%20Societal%20Risks%0A%20%20of%20Generative%20Vision-Language%20Models&body=Title%3A%20Assessing%20the%20Geolocation%20Capabilities%2C%20Limitations%20and%20Societal%20Risks%0A%20%20of%20Generative%20Vision-Language%20Models%0AAuthor%3A%20Oliver%20Grainge%20and%20Sania%20Waheed%20and%20Jack%20Stilgoe%20and%20Michael%20Milford%20and%20Shoaib%20Ehsan%0AAbstract%3A%20%20%20Geo-localization%20is%20the%20task%20of%20identifying%20the%20location%20of%20an%20image%20using%0Avisual%20cues%20alone.%20It%20has%20beneficial%20applications%2C%20such%20as%20improving%20disaster%0Aresponse%2C%20enhancing%20navigation%2C%20and%20geography%20education.%20Recently%2C%0AVision-Language%20Models%20%28VLMs%29%20are%20increasingly%20demonstrating%20capabilities%20as%0Aaccurate%20image%20geo-locators.%20This%20brings%20significant%20privacy%20risks%2C%20including%0Athose%20related%20to%20stalking%20and%20surveillance%2C%20considering%20the%20widespread%20uses%20of%0AAI%20models%20and%20sharing%20of%20photos%20on%20social%20media.%20The%20precision%20of%20these%20models%0Ais%20likely%20to%20improve%20in%20the%20future.%20Despite%20these%20risks%2C%20there%20is%20little%20work%0Aon%20systematically%20evaluating%20the%20geolocation%20precision%20of%20Generative%20VLMs%2C%0Atheir%20limits%20and%20potential%20for%20unintended%20inferences.%20To%20bridge%20this%20gap%2C%20we%0Aconduct%20a%20comprehensive%20assessment%20of%20the%20geolocation%20capabilities%20of%2025%0Astate-of-the-art%20VLMs%20on%20four%20benchmark%20image%20datasets%20captured%20in%20diverse%0Aenvironments.%20Our%20results%20offer%20insight%20into%20the%20internal%20reasoning%20of%20VLMs%20and%0Ahighlight%20their%20strengths%2C%20limitations%2C%20and%20potential%20societal%20risks.%20Our%0Afindings%20indicate%20that%20current%20VLMs%20perform%20poorly%20on%20generic%20street-level%0Aimages%20yet%20achieve%20notably%20high%20accuracy%20%2861%5C%25%29%20on%20images%20resembling%20social%0Amedia%20content%2C%20raising%20significant%20and%20urgent%20privacy%20concerns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520the%2520Geolocation%2520Capabilities%252C%2520Limitations%2520and%2520Societal%2520Risks%250A%2520%2520of%2520Generative%2520Vision-Language%2520Models%26entry.906535625%3DOliver%2520Grainge%2520and%2520Sania%2520Waheed%2520and%2520Jack%2520Stilgoe%2520and%2520Michael%2520Milford%2520and%2520Shoaib%2520Ehsan%26entry.1292438233%3D%2520%2520Geo-localization%2520is%2520the%2520task%2520of%2520identifying%2520the%2520location%2520of%2520an%2520image%2520using%250Avisual%2520cues%2520alone.%2520It%2520has%2520beneficial%2520applications%252C%2520such%2520as%2520improving%2520disaster%250Aresponse%252C%2520enhancing%2520navigation%252C%2520and%2520geography%2520education.%2520Recently%252C%250AVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520increasingly%2520demonstrating%2520capabilities%2520as%250Aaccurate%2520image%2520geo-locators.%2520This%2520brings%2520significant%2520privacy%2520risks%252C%2520including%250Athose%2520related%2520to%2520stalking%2520and%2520surveillance%252C%2520considering%2520the%2520widespread%2520uses%2520of%250AAI%2520models%2520and%2520sharing%2520of%2520photos%2520on%2520social%2520media.%2520The%2520precision%2520of%2520these%2520models%250Ais%2520likely%2520to%2520improve%2520in%2520the%2520future.%2520Despite%2520these%2520risks%252C%2520there%2520is%2520little%2520work%250Aon%2520systematically%2520evaluating%2520the%2520geolocation%2520precision%2520of%2520Generative%2520VLMs%252C%250Atheir%2520limits%2520and%2520potential%2520for%2520unintended%2520inferences.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aconduct%2520a%2520comprehensive%2520assessment%2520of%2520the%2520geolocation%2520capabilities%2520of%252025%250Astate-of-the-art%2520VLMs%2520on%2520four%2520benchmark%2520image%2520datasets%2520captured%2520in%2520diverse%250Aenvironments.%2520Our%2520results%2520offer%2520insight%2520into%2520the%2520internal%2520reasoning%2520of%2520VLMs%2520and%250Ahighlight%2520their%2520strengths%252C%2520limitations%252C%2520and%2520potential%2520societal%2520risks.%2520Our%250Afindings%2520indicate%2520that%2520current%2520VLMs%2520perform%2520poorly%2520on%2520generic%2520street-level%250Aimages%2520yet%2520achieve%2520notably%2520high%2520accuracy%2520%252861%255C%2525%2529%2520on%2520images%2520resembling%2520social%250Amedia%2520content%252C%2520raising%2520significant%2520and%2520urgent%2520privacy%2520concerns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20the%20Geolocation%20Capabilities%2C%20Limitations%20and%20Societal%20Risks%0A%20%20of%20Generative%20Vision-Language%20Models&entry.906535625=Oliver%20Grainge%20and%20Sania%20Waheed%20and%20Jack%20Stilgoe%20and%20Michael%20Milford%20and%20Shoaib%20Ehsan&entry.1292438233=%20%20Geo-localization%20is%20the%20task%20of%20identifying%20the%20location%20of%20an%20image%20using%0Avisual%20cues%20alone.%20It%20has%20beneficial%20applications%2C%20such%20as%20improving%20disaster%0Aresponse%2C%20enhancing%20navigation%2C%20and%20geography%20education.%20Recently%2C%0AVision-Language%20Models%20%28VLMs%29%20are%20increasingly%20demonstrating%20capabilities%20as%0Aaccurate%20image%20geo-locators.%20This%20brings%20significant%20privacy%20risks%2C%20including%0Athose%20related%20to%20stalking%20and%20surveillance%2C%20considering%20the%20widespread%20uses%20of%0AAI%20models%20and%20sharing%20of%20photos%20on%20social%20media.%20The%20precision%20of%20these%20models%0Ais%20likely%20to%20improve%20in%20the%20future.%20Despite%20these%20risks%2C%20there%20is%20little%20work%0Aon%20systematically%20evaluating%20the%20geolocation%20precision%20of%20Generative%20VLMs%2C%0Atheir%20limits%20and%20potential%20for%20unintended%20inferences.%20To%20bridge%20this%20gap%2C%20we%0Aconduct%20a%20comprehensive%20assessment%20of%20the%20geolocation%20capabilities%20of%2025%0Astate-of-the-art%20VLMs%20on%20four%20benchmark%20image%20datasets%20captured%20in%20diverse%0Aenvironments.%20Our%20results%20offer%20insight%20into%20the%20internal%20reasoning%20of%20VLMs%20and%0Ahighlight%20their%20strengths%2C%20limitations%2C%20and%20potential%20societal%20risks.%20Our%0Afindings%20indicate%20that%20current%20VLMs%20perform%20poorly%20on%20generic%20street-level%0Aimages%20yet%20achieve%20notably%20high%20accuracy%20%2861%5C%25%29%20on%20images%20resembling%20social%0Amedia%20content%2C%20raising%20significant%20and%20urgent%20privacy%20concerns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19967v1&entry.124074799=Read"},
{"title": "The Next Layer: Augmenting Foundation Models with Structure-Preserving\n  and Attention-Guided Learning for Local Patches to Global Context Awareness\n  in Computational Pathology", "author": "Muhammad Waqas and Rukhmini Bandyopadhyay and Eman Showkatian and Amgad Muneer and Anas Zafar and Frank Rojas Alvarez and Maricel Corredor Marin and Wentao Li and David Jaffray and Cara Haymaker and John Heymach and Natalie I Vokes and Luisa Maren Solis Soto and Jianjun Zhang and Jia Wu", "abstract": "  Foundation models have recently emerged as powerful feature extractors in\ncomputational pathology, yet they typically omit mechanisms for leveraging the\nglobal spatial structure of tissues and the local contextual relationships\namong diagnostically relevant regions - key elements for understanding the\ntumor microenvironment. Multiple instance learning (MIL) remains an essential\nnext step following foundation model, designing a framework to aggregate\npatch-level features into slide-level predictions. We present EAGLE-Net, a\nstructure-preserving, attention-guided MIL architecture designed to augment\nprediction and interpretability. EAGLE-Net integrates multi-scale absolute\nspatial encoding to capture global tissue architecture, a top-K\nneighborhood-aware loss to focus attention on local microenvironments, and\nbackground suppression loss to minimize false positives. We benchmarked\nEAGLE-Net on large pan-cancer datasets, including three cancer types for\nclassification (10,260 slides) and seven cancer types for survival prediction\n(4,172 slides), using three distinct histology foundation backbones (REMEDIES,\nUni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher\nclassification accuracy and the top concordance indices in 6 of 7 cancer types,\nproducing smooth, biologically coherent attention maps that aligned with expert\nannotations and highlighted invasive fronts, necrosis, and immune infiltration.\nThese results position EAGLE-Net as a generalizable, interpretable framework\nthat complements foundation models, enabling improved biomarker discovery,\nprognostic modeling, and clinical decision support\n", "link": "http://arxiv.org/abs/2508.19914v1", "date": "2025-08-27", "relevancy": 2.6607, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5324}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Next%20Layer%3A%20Augmenting%20Foundation%20Models%20with%20Structure-Preserving%0A%20%20and%20Attention-Guided%20Learning%20for%20Local%20Patches%20to%20Global%20Context%20Awareness%0A%20%20in%20Computational%20Pathology&body=Title%3A%20The%20Next%20Layer%3A%20Augmenting%20Foundation%20Models%20with%20Structure-Preserving%0A%20%20and%20Attention-Guided%20Learning%20for%20Local%20Patches%20to%20Global%20Context%20Awareness%0A%20%20in%20Computational%20Pathology%0AAuthor%3A%20Muhammad%20Waqas%20and%20Rukhmini%20Bandyopadhyay%20and%20Eman%20Showkatian%20and%20Amgad%20Muneer%20and%20Anas%20Zafar%20and%20Frank%20Rojas%20Alvarez%20and%20Maricel%20Corredor%20Marin%20and%20Wentao%20Li%20and%20David%20Jaffray%20and%20Cara%20Haymaker%20and%20John%20Heymach%20and%20Natalie%20I%20Vokes%20and%20Luisa%20Maren%20Solis%20Soto%20and%20Jianjun%20Zhang%20and%20Jia%20Wu%0AAbstract%3A%20%20%20Foundation%20models%20have%20recently%20emerged%20as%20powerful%20feature%20extractors%20in%0Acomputational%20pathology%2C%20yet%20they%20typically%20omit%20mechanisms%20for%20leveraging%20the%0Aglobal%20spatial%20structure%20of%20tissues%20and%20the%20local%20contextual%20relationships%0Aamong%20diagnostically%20relevant%20regions%20-%20key%20elements%20for%20understanding%20the%0Atumor%20microenvironment.%20Multiple%20instance%20learning%20%28MIL%29%20remains%20an%20essential%0Anext%20step%20following%20foundation%20model%2C%20designing%20a%20framework%20to%20aggregate%0Apatch-level%20features%20into%20slide-level%20predictions.%20We%20present%20EAGLE-Net%2C%20a%0Astructure-preserving%2C%20attention-guided%20MIL%20architecture%20designed%20to%20augment%0Aprediction%20and%20interpretability.%20EAGLE-Net%20integrates%20multi-scale%20absolute%0Aspatial%20encoding%20to%20capture%20global%20tissue%20architecture%2C%20a%20top-K%0Aneighborhood-aware%20loss%20to%20focus%20attention%20on%20local%20microenvironments%2C%20and%0Abackground%20suppression%20loss%20to%20minimize%20false%20positives.%20We%20benchmarked%0AEAGLE-Net%20on%20large%20pan-cancer%20datasets%2C%20including%20three%20cancer%20types%20for%0Aclassification%20%2810%2C260%20slides%29%20and%20seven%20cancer%20types%20for%20survival%20prediction%0A%284%2C172%20slides%29%2C%20using%20three%20distinct%20histology%20foundation%20backbones%20%28REMEDIES%2C%0AUni-V1%2C%20Uni2-h%29.%20Across%20tasks%2C%20EAGLE-Net%20achieved%20up%20to%203%25%20higher%0Aclassification%20accuracy%20and%20the%20top%20concordance%20indices%20in%206%20of%207%20cancer%20types%2C%0Aproducing%20smooth%2C%20biologically%20coherent%20attention%20maps%20that%20aligned%20with%20expert%0Aannotations%20and%20highlighted%20invasive%20fronts%2C%20necrosis%2C%20and%20immune%20infiltration.%0AThese%20results%20position%20EAGLE-Net%20as%20a%20generalizable%2C%20interpretable%20framework%0Athat%20complements%20foundation%20models%2C%20enabling%20improved%20biomarker%20discovery%2C%0Aprognostic%20modeling%2C%20and%20clinical%20decision%20support%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Next%2520Layer%253A%2520Augmenting%2520Foundation%2520Models%2520with%2520Structure-Preserving%250A%2520%2520and%2520Attention-Guided%2520Learning%2520for%2520Local%2520Patches%2520to%2520Global%2520Context%2520Awareness%250A%2520%2520in%2520Computational%2520Pathology%26entry.906535625%3DMuhammad%2520Waqas%2520and%2520Rukhmini%2520Bandyopadhyay%2520and%2520Eman%2520Showkatian%2520and%2520Amgad%2520Muneer%2520and%2520Anas%2520Zafar%2520and%2520Frank%2520Rojas%2520Alvarez%2520and%2520Maricel%2520Corredor%2520Marin%2520and%2520Wentao%2520Li%2520and%2520David%2520Jaffray%2520and%2520Cara%2520Haymaker%2520and%2520John%2520Heymach%2520and%2520Natalie%2520I%2520Vokes%2520and%2520Luisa%2520Maren%2520Solis%2520Soto%2520and%2520Jianjun%2520Zhang%2520and%2520Jia%2520Wu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520have%2520recently%2520emerged%2520as%2520powerful%2520feature%2520extractors%2520in%250Acomputational%2520pathology%252C%2520yet%2520they%2520typically%2520omit%2520mechanisms%2520for%2520leveraging%2520the%250Aglobal%2520spatial%2520structure%2520of%2520tissues%2520and%2520the%2520local%2520contextual%2520relationships%250Aamong%2520diagnostically%2520relevant%2520regions%2520-%2520key%2520elements%2520for%2520understanding%2520the%250Atumor%2520microenvironment.%2520Multiple%2520instance%2520learning%2520%2528MIL%2529%2520remains%2520an%2520essential%250Anext%2520step%2520following%2520foundation%2520model%252C%2520designing%2520a%2520framework%2520to%2520aggregate%250Apatch-level%2520features%2520into%2520slide-level%2520predictions.%2520We%2520present%2520EAGLE-Net%252C%2520a%250Astructure-preserving%252C%2520attention-guided%2520MIL%2520architecture%2520designed%2520to%2520augment%250Aprediction%2520and%2520interpretability.%2520EAGLE-Net%2520integrates%2520multi-scale%2520absolute%250Aspatial%2520encoding%2520to%2520capture%2520global%2520tissue%2520architecture%252C%2520a%2520top-K%250Aneighborhood-aware%2520loss%2520to%2520focus%2520attention%2520on%2520local%2520microenvironments%252C%2520and%250Abackground%2520suppression%2520loss%2520to%2520minimize%2520false%2520positives.%2520We%2520benchmarked%250AEAGLE-Net%2520on%2520large%2520pan-cancer%2520datasets%252C%2520including%2520three%2520cancer%2520types%2520for%250Aclassification%2520%252810%252C260%2520slides%2529%2520and%2520seven%2520cancer%2520types%2520for%2520survival%2520prediction%250A%25284%252C172%2520slides%2529%252C%2520using%2520three%2520distinct%2520histology%2520foundation%2520backbones%2520%2528REMEDIES%252C%250AUni-V1%252C%2520Uni2-h%2529.%2520Across%2520tasks%252C%2520EAGLE-Net%2520achieved%2520up%2520to%25203%2525%2520higher%250Aclassification%2520accuracy%2520and%2520the%2520top%2520concordance%2520indices%2520in%25206%2520of%25207%2520cancer%2520types%252C%250Aproducing%2520smooth%252C%2520biologically%2520coherent%2520attention%2520maps%2520that%2520aligned%2520with%2520expert%250Aannotations%2520and%2520highlighted%2520invasive%2520fronts%252C%2520necrosis%252C%2520and%2520immune%2520infiltration.%250AThese%2520results%2520position%2520EAGLE-Net%2520as%2520a%2520generalizable%252C%2520interpretable%2520framework%250Athat%2520complements%2520foundation%2520models%252C%2520enabling%2520improved%2520biomarker%2520discovery%252C%250Aprognostic%2520modeling%252C%2520and%2520clinical%2520decision%2520support%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Next%20Layer%3A%20Augmenting%20Foundation%20Models%20with%20Structure-Preserving%0A%20%20and%20Attention-Guided%20Learning%20for%20Local%20Patches%20to%20Global%20Context%20Awareness%0A%20%20in%20Computational%20Pathology&entry.906535625=Muhammad%20Waqas%20and%20Rukhmini%20Bandyopadhyay%20and%20Eman%20Showkatian%20and%20Amgad%20Muneer%20and%20Anas%20Zafar%20and%20Frank%20Rojas%20Alvarez%20and%20Maricel%20Corredor%20Marin%20and%20Wentao%20Li%20and%20David%20Jaffray%20and%20Cara%20Haymaker%20and%20John%20Heymach%20and%20Natalie%20I%20Vokes%20and%20Luisa%20Maren%20Solis%20Soto%20and%20Jianjun%20Zhang%20and%20Jia%20Wu&entry.1292438233=%20%20Foundation%20models%20have%20recently%20emerged%20as%20powerful%20feature%20extractors%20in%0Acomputational%20pathology%2C%20yet%20they%20typically%20omit%20mechanisms%20for%20leveraging%20the%0Aglobal%20spatial%20structure%20of%20tissues%20and%20the%20local%20contextual%20relationships%0Aamong%20diagnostically%20relevant%20regions%20-%20key%20elements%20for%20understanding%20the%0Atumor%20microenvironment.%20Multiple%20instance%20learning%20%28MIL%29%20remains%20an%20essential%0Anext%20step%20following%20foundation%20model%2C%20designing%20a%20framework%20to%20aggregate%0Apatch-level%20features%20into%20slide-level%20predictions.%20We%20present%20EAGLE-Net%2C%20a%0Astructure-preserving%2C%20attention-guided%20MIL%20architecture%20designed%20to%20augment%0Aprediction%20and%20interpretability.%20EAGLE-Net%20integrates%20multi-scale%20absolute%0Aspatial%20encoding%20to%20capture%20global%20tissue%20architecture%2C%20a%20top-K%0Aneighborhood-aware%20loss%20to%20focus%20attention%20on%20local%20microenvironments%2C%20and%0Abackground%20suppression%20loss%20to%20minimize%20false%20positives.%20We%20benchmarked%0AEAGLE-Net%20on%20large%20pan-cancer%20datasets%2C%20including%20three%20cancer%20types%20for%0Aclassification%20%2810%2C260%20slides%29%20and%20seven%20cancer%20types%20for%20survival%20prediction%0A%284%2C172%20slides%29%2C%20using%20three%20distinct%20histology%20foundation%20backbones%20%28REMEDIES%2C%0AUni-V1%2C%20Uni2-h%29.%20Across%20tasks%2C%20EAGLE-Net%20achieved%20up%20to%203%25%20higher%0Aclassification%20accuracy%20and%20the%20top%20concordance%20indices%20in%206%20of%207%20cancer%20types%2C%0Aproducing%20smooth%2C%20biologically%20coherent%20attention%20maps%20that%20aligned%20with%20expert%0Aannotations%20and%20highlighted%20invasive%20fronts%2C%20necrosis%2C%20and%20immune%20infiltration.%0AThese%20results%20position%20EAGLE-Net%20as%20a%20generalizable%2C%20interpretable%20framework%0Athat%20complements%20foundation%20models%2C%20enabling%20improved%20biomarker%20discovery%2C%0Aprognostic%20modeling%2C%20and%20clinical%20decision%20support%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19914v1&entry.124074799=Read"},
{"title": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models", "author": "Yuxin Guo and Teng Wang and Yuying Ge and Shijie Ma and Yixiao Ge and Wei Zou and Ying Shan", "abstract": "  Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory\n", "link": "http://arxiv.org/abs/2508.20088v1", "date": "2025-08-27", "relevancy": 2.6152, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioStory%3A%20Generating%20Long-Form%20Narrative%20Audio%20with%20Large%20Language%0A%20%20Models&body=Title%3A%20AudioStory%3A%20Generating%20Long-Form%20Narrative%20Audio%20with%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yuxin%20Guo%20and%20Teng%20Wang%20and%20Yuying%20Ge%20and%20Shijie%20Ma%20and%20Yixiao%20Ge%20and%20Wei%20Zou%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-audio%20%28TTA%29%20generation%20excel%20at%20synthesizing%20short%0Aaudio%20clips%20but%20struggle%20with%20long-form%20narrative%20audio%2C%20which%20requires%0Atemporal%20coherence%20and%20compositional%20reasoning.%20To%20address%20this%20gap%2C%20we%20propose%0AAudioStory%2C%20a%20unified%20framework%20that%20integrates%20large%20language%20models%20%28LLMs%29%0Awith%20TTA%20systems%20to%20generate%20structured%2C%20long-form%20audio%20narratives.%20AudioStory%0Apossesses%20strong%20instruction-following%20reasoning%20generation%20capabilities.%20It%0Aemploys%20LLMs%20to%20decompose%20complex%20narrative%20queries%20into%20temporally%20ordered%0Asub-tasks%20with%20contextual%20cues%2C%20enabling%20coherent%20scene%20transitions%20and%0Aemotional%20tone%20consistency.%20AudioStory%20has%20two%20appealing%20features%3A%20%281%29%0ADecoupled%20bridging%20mechanism%3A%20AudioStory%20disentangles%20LLM-diffuser%0Acollaboration%20into%20two%20specialized%20components%2C%20i.e.%2C%20a%20bridging%20query%20for%0Aintra-event%20semantic%20alignment%20and%20a%20residual%20query%20for%20cross-event%20coherence%0Apreservation.%20%282%29%20End-to-end%20training%3A%20By%20unifying%20instruction%20comprehension%0Aand%20audio%20generation%20within%20a%20single%20end-to-end%20framework%2C%20AudioStory%0Aeliminates%20the%20need%20for%20modular%20training%20pipelines%20while%20enhancing%20synergy%0Abetween%20components.%20Furthermore%2C%20we%20establish%20a%20benchmark%20AudioStory-10K%2C%0Aencompassing%20diverse%20domains%20such%20as%20animated%20soundscapes%20and%20natural%20sound%0Anarratives.%20Extensive%20experiments%20show%20the%20superiority%20of%20AudioStory%20on%20both%0Asingle-audio%20generation%20and%20narrative%20audio%20generation%2C%20surpassing%20prior%20TTA%0Abaselines%20in%20both%20instruction-following%20ability%20and%20audio%20fidelity.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/TencentARC/AudioStory%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioStory%253A%2520Generating%2520Long-Form%2520Narrative%2520Audio%2520with%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYuxin%2520Guo%2520and%2520Teng%2520Wang%2520and%2520Yuying%2520Ge%2520and%2520Shijie%2520Ma%2520and%2520Yixiao%2520Ge%2520and%2520Wei%2520Zou%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-audio%2520%2528TTA%2529%2520generation%2520excel%2520at%2520synthesizing%2520short%250Aaudio%2520clips%2520but%2520struggle%2520with%2520long-form%2520narrative%2520audio%252C%2520which%2520requires%250Atemporal%2520coherence%2520and%2520compositional%2520reasoning.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%250AAudioStory%252C%2520a%2520unified%2520framework%2520that%2520integrates%2520large%2520language%2520models%2520%2528LLMs%2529%250Awith%2520TTA%2520systems%2520to%2520generate%2520structured%252C%2520long-form%2520audio%2520narratives.%2520AudioStory%250Apossesses%2520strong%2520instruction-following%2520reasoning%2520generation%2520capabilities.%2520It%250Aemploys%2520LLMs%2520to%2520decompose%2520complex%2520narrative%2520queries%2520into%2520temporally%2520ordered%250Asub-tasks%2520with%2520contextual%2520cues%252C%2520enabling%2520coherent%2520scene%2520transitions%2520and%250Aemotional%2520tone%2520consistency.%2520AudioStory%2520has%2520two%2520appealing%2520features%253A%2520%25281%2529%250ADecoupled%2520bridging%2520mechanism%253A%2520AudioStory%2520disentangles%2520LLM-diffuser%250Acollaboration%2520into%2520two%2520specialized%2520components%252C%2520i.e.%252C%2520a%2520bridging%2520query%2520for%250Aintra-event%2520semantic%2520alignment%2520and%2520a%2520residual%2520query%2520for%2520cross-event%2520coherence%250Apreservation.%2520%25282%2529%2520End-to-end%2520training%253A%2520By%2520unifying%2520instruction%2520comprehension%250Aand%2520audio%2520generation%2520within%2520a%2520single%2520end-to-end%2520framework%252C%2520AudioStory%250Aeliminates%2520the%2520need%2520for%2520modular%2520training%2520pipelines%2520while%2520enhancing%2520synergy%250Abetween%2520components.%2520Furthermore%252C%2520we%2520establish%2520a%2520benchmark%2520AudioStory-10K%252C%250Aencompassing%2520diverse%2520domains%2520such%2520as%2520animated%2520soundscapes%2520and%2520natural%2520sound%250Anarratives.%2520Extensive%2520experiments%2520show%2520the%2520superiority%2520of%2520AudioStory%2520on%2520both%250Asingle-audio%2520generation%2520and%2520narrative%2520audio%2520generation%252C%2520surpassing%2520prior%2520TTA%250Abaselines%2520in%2520both%2520instruction-following%2520ability%2520and%2520audio%2520fidelity.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/TencentARC/AudioStory%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioStory%3A%20Generating%20Long-Form%20Narrative%20Audio%20with%20Large%20Language%0A%20%20Models&entry.906535625=Yuxin%20Guo%20and%20Teng%20Wang%20and%20Yuying%20Ge%20and%20Shijie%20Ma%20and%20Yixiao%20Ge%20and%20Wei%20Zou%20and%20Ying%20Shan&entry.1292438233=%20%20Recent%20advances%20in%20text-to-audio%20%28TTA%29%20generation%20excel%20at%20synthesizing%20short%0Aaudio%20clips%20but%20struggle%20with%20long-form%20narrative%20audio%2C%20which%20requires%0Atemporal%20coherence%20and%20compositional%20reasoning.%20To%20address%20this%20gap%2C%20we%20propose%0AAudioStory%2C%20a%20unified%20framework%20that%20integrates%20large%20language%20models%20%28LLMs%29%0Awith%20TTA%20systems%20to%20generate%20structured%2C%20long-form%20audio%20narratives.%20AudioStory%0Apossesses%20strong%20instruction-following%20reasoning%20generation%20capabilities.%20It%0Aemploys%20LLMs%20to%20decompose%20complex%20narrative%20queries%20into%20temporally%20ordered%0Asub-tasks%20with%20contextual%20cues%2C%20enabling%20coherent%20scene%20transitions%20and%0Aemotional%20tone%20consistency.%20AudioStory%20has%20two%20appealing%20features%3A%20%281%29%0ADecoupled%20bridging%20mechanism%3A%20AudioStory%20disentangles%20LLM-diffuser%0Acollaboration%20into%20two%20specialized%20components%2C%20i.e.%2C%20a%20bridging%20query%20for%0Aintra-event%20semantic%20alignment%20and%20a%20residual%20query%20for%20cross-event%20coherence%0Apreservation.%20%282%29%20End-to-end%20training%3A%20By%20unifying%20instruction%20comprehension%0Aand%20audio%20generation%20within%20a%20single%20end-to-end%20framework%2C%20AudioStory%0Aeliminates%20the%20need%20for%20modular%20training%20pipelines%20while%20enhancing%20synergy%0Abetween%20components.%20Furthermore%2C%20we%20establish%20a%20benchmark%20AudioStory-10K%2C%0Aencompassing%20diverse%20domains%20such%20as%20animated%20soundscapes%20and%20natural%20sound%0Anarratives.%20Extensive%20experiments%20show%20the%20superiority%20of%20AudioStory%20on%20both%0Asingle-audio%20generation%20and%20narrative%20audio%20generation%2C%20surpassing%20prior%20TTA%0Abaselines%20in%20both%20instruction-following%20ability%20and%20audio%20fidelity.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/TencentARC/AudioStory%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20088v1&entry.124074799=Read"},
{"title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local\n  Similarity", "author": "Seongheon Park and Yixuan Li", "abstract": "  Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.\n", "link": "http://arxiv.org/abs/2508.19972v1", "date": "2025-08-27", "relevancy": 2.5827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5218}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLSim%3A%20Detecting%20Object%20Hallucinations%20in%20LVLMs%20via%20Global-Local%0A%20%20Similarity&body=Title%3A%20GLSim%3A%20Detecting%20Object%20Hallucinations%20in%20LVLMs%20via%20Global-Local%0A%20%20Similarity%0AAuthor%3A%20Seongheon%20Park%20and%20Yixuan%20Li%0AAbstract%3A%20%20%20Object%20hallucination%20in%20large%20vision-language%20models%20presents%20a%20significant%0Achallenge%20to%20their%20safe%20deployment%20in%20real-world%20applications.%20Recent%20works%0Ahave%20proposed%20object-level%20hallucination%20scores%20to%20estimate%20the%20likelihood%20of%0Aobject%20hallucination%3B%20however%2C%20these%20methods%20typically%20adopt%20either%20a%20global%20or%0Alocal%20perspective%20in%20isolation%2C%20which%20may%20limit%20detection%20reliability.%20In%20this%0Apaper%2C%20we%20introduce%20GLSim%2C%20a%20novel%20training-free%20object%20hallucination%20detection%0Aframework%20that%20leverages%20complementary%20global%20and%20local%20embedding%20similarity%0Asignals%20between%20image%20and%20text%20modalities%2C%20enabling%20more%20accurate%20and%20reliable%0Ahallucination%20detection%20in%20diverse%20scenarios.%20We%20comprehensively%20benchmark%0Aexisting%20object%20hallucination%20detection%20methods%20and%20demonstrate%20that%20GLSim%0Aachieves%20superior%20detection%20performance%2C%20outperforming%20competitive%20baselines%20by%0Aa%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLSim%253A%2520Detecting%2520Object%2520Hallucinations%2520in%2520LVLMs%2520via%2520Global-Local%250A%2520%2520Similarity%26entry.906535625%3DSeongheon%2520Park%2520and%2520Yixuan%2520Li%26entry.1292438233%3D%2520%2520Object%2520hallucination%2520in%2520large%2520vision-language%2520models%2520presents%2520a%2520significant%250Achallenge%2520to%2520their%2520safe%2520deployment%2520in%2520real-world%2520applications.%2520Recent%2520works%250Ahave%2520proposed%2520object-level%2520hallucination%2520scores%2520to%2520estimate%2520the%2520likelihood%2520of%250Aobject%2520hallucination%253B%2520however%252C%2520these%2520methods%2520typically%2520adopt%2520either%2520a%2520global%2520or%250Alocal%2520perspective%2520in%2520isolation%252C%2520which%2520may%2520limit%2520detection%2520reliability.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520GLSim%252C%2520a%2520novel%2520training-free%2520object%2520hallucination%2520detection%250Aframework%2520that%2520leverages%2520complementary%2520global%2520and%2520local%2520embedding%2520similarity%250Asignals%2520between%2520image%2520and%2520text%2520modalities%252C%2520enabling%2520more%2520accurate%2520and%2520reliable%250Ahallucination%2520detection%2520in%2520diverse%2520scenarios.%2520We%2520comprehensively%2520benchmark%250Aexisting%2520object%2520hallucination%2520detection%2520methods%2520and%2520demonstrate%2520that%2520GLSim%250Aachieves%2520superior%2520detection%2520performance%252C%2520outperforming%2520competitive%2520baselines%2520by%250Aa%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLSim%3A%20Detecting%20Object%20Hallucinations%20in%20LVLMs%20via%20Global-Local%0A%20%20Similarity&entry.906535625=Seongheon%20Park%20and%20Yixuan%20Li&entry.1292438233=%20%20Object%20hallucination%20in%20large%20vision-language%20models%20presents%20a%20significant%0Achallenge%20to%20their%20safe%20deployment%20in%20real-world%20applications.%20Recent%20works%0Ahave%20proposed%20object-level%20hallucination%20scores%20to%20estimate%20the%20likelihood%20of%0Aobject%20hallucination%3B%20however%2C%20these%20methods%20typically%20adopt%20either%20a%20global%20or%0Alocal%20perspective%20in%20isolation%2C%20which%20may%20limit%20detection%20reliability.%20In%20this%0Apaper%2C%20we%20introduce%20GLSim%2C%20a%20novel%20training-free%20object%20hallucination%20detection%0Aframework%20that%20leverages%20complementary%20global%20and%20local%20embedding%20similarity%0Asignals%20between%20image%20and%20text%20modalities%2C%20enabling%20more%20accurate%20and%20reliable%0Ahallucination%20detection%20in%20diverse%20scenarios.%20We%20comprehensively%20benchmark%0Aexisting%20object%20hallucination%20detection%20methods%20and%20demonstrate%20that%20GLSim%0Aachieves%20superior%20detection%20performance%2C%20outperforming%20competitive%20baselines%20by%0Aa%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19972v1&entry.124074799=Read"},
{"title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA\n  Attuned to Diverse Visual Contexts", "author": "Taebaek Hwang and Minseo Kim and Gisang Lee and Seonuk Kim and Hyunjun Eun", "abstract": "  Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.\n", "link": "http://arxiv.org/abs/2508.19944v1", "date": "2025-08-27", "relevancy": 2.5806, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5298}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KRETA%3A%20A%20Benchmark%20for%20Korean%20Reading%20and%20Reasoning%20in%20Text-Rich%20VQA%0A%20%20Attuned%20to%20Diverse%20Visual%20Contexts&body=Title%3A%20KRETA%3A%20A%20Benchmark%20for%20Korean%20Reading%20and%20Reasoning%20in%20Text-Rich%20VQA%0A%20%20Attuned%20to%20Diverse%20Visual%20Contexts%0AAuthor%3A%20Taebaek%20Hwang%20and%20Minseo%20Kim%20and%20Gisang%20Lee%20and%20Seonuk%20Kim%20and%20Hyunjun%20Eun%0AAbstract%3A%20%20%20Understanding%20and%20reasoning%20over%20text%20within%20visual%20contexts%20poses%20a%0Asignificant%20challenge%20for%20Vision-Language%20Models%20%28VLMs%29%2C%20given%20the%20complexity%0Aand%20diversity%20of%20real-world%20scenarios.%20To%20address%20this%20challenge%2C%20text-rich%0AVisual%20Question%20Answering%20%28VQA%29%20datasets%20and%20benchmarks%20have%20emerged%20for%0Ahigh-resource%20languages%20like%20English.%20However%2C%20a%20critical%20gap%20persists%20for%0Alow-resource%20languages%20such%20as%20Korean%2C%20where%20the%20lack%20of%20comprehensive%0Abenchmarks%20hinders%20robust%20model%20evaluation%20and%20comparison.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20KRETA%2C%20a%20benchmark%20for%20Korean%20Reading%20and%20rEasoning%20in%20Text-rich%0AVQA%20Attuned%20to%20diverse%20visual%20contexts.%20KRETA%20facilitates%20an%20in-depth%0Aevaluation%20of%20both%20visual%20text%20understanding%20and%20reasoning%20capabilities%2C%20while%0Aalso%20supporting%20a%20multifaceted%20assessment%20across%2015%20domains%20and%2026%20image%20types.%0AAdditionally%2C%20we%20introduce%20a%20semi-automated%20VQA%20generation%20pipeline%0Aspecifically%20optimized%20for%20text-rich%20settings%2C%20leveraging%20refined%20stepwise%0Aimage%20decomposition%20and%20a%20rigorous%20seven-metric%20evaluation%20protocol%20to%20ensure%0Adata%20quality.%20While%20KRETA%20is%20tailored%20for%20Korean%2C%20we%20hope%20our%20adaptable%20and%0Aextensible%20pipeline%20will%20facilitate%20the%20development%20of%20similar%20benchmarks%20in%0Aother%20languages%2C%20thereby%20accelerating%20multilingual%20VLM%20research.%20The%20code%20and%0Adataset%20for%20KRETA%20are%20available%20at%20https%3A//github.com/tabtoyou/KRETA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKRETA%253A%2520A%2520Benchmark%2520for%2520Korean%2520Reading%2520and%2520Reasoning%2520in%2520Text-Rich%2520VQA%250A%2520%2520Attuned%2520to%2520Diverse%2520Visual%2520Contexts%26entry.906535625%3DTaebaek%2520Hwang%2520and%2520Minseo%2520Kim%2520and%2520Gisang%2520Lee%2520and%2520Seonuk%2520Kim%2520and%2520Hyunjun%2520Eun%26entry.1292438233%3D%2520%2520Understanding%2520and%2520reasoning%2520over%2520text%2520within%2520visual%2520contexts%2520poses%2520a%250Asignificant%2520challenge%2520for%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520given%2520the%2520complexity%250Aand%2520diversity%2520of%2520real-world%2520scenarios.%2520To%2520address%2520this%2520challenge%252C%2520text-rich%250AVisual%2520Question%2520Answering%2520%2528VQA%2529%2520datasets%2520and%2520benchmarks%2520have%2520emerged%2520for%250Ahigh-resource%2520languages%2520like%2520English.%2520However%252C%2520a%2520critical%2520gap%2520persists%2520for%250Alow-resource%2520languages%2520such%2520as%2520Korean%252C%2520where%2520the%2520lack%2520of%2520comprehensive%250Abenchmarks%2520hinders%2520robust%2520model%2520evaluation%2520and%2520comparison.%2520To%2520bridge%2520this%2520gap%252C%250Awe%2520introduce%2520KRETA%252C%2520a%2520benchmark%2520for%2520Korean%2520Reading%2520and%2520rEasoning%2520in%2520Text-rich%250AVQA%2520Attuned%2520to%2520diverse%2520visual%2520contexts.%2520KRETA%2520facilitates%2520an%2520in-depth%250Aevaluation%2520of%2520both%2520visual%2520text%2520understanding%2520and%2520reasoning%2520capabilities%252C%2520while%250Aalso%2520supporting%2520a%2520multifaceted%2520assessment%2520across%252015%2520domains%2520and%252026%2520image%2520types.%250AAdditionally%252C%2520we%2520introduce%2520a%2520semi-automated%2520VQA%2520generation%2520pipeline%250Aspecifically%2520optimized%2520for%2520text-rich%2520settings%252C%2520leveraging%2520refined%2520stepwise%250Aimage%2520decomposition%2520and%2520a%2520rigorous%2520seven-metric%2520evaluation%2520protocol%2520to%2520ensure%250Adata%2520quality.%2520While%2520KRETA%2520is%2520tailored%2520for%2520Korean%252C%2520we%2520hope%2520our%2520adaptable%2520and%250Aextensible%2520pipeline%2520will%2520facilitate%2520the%2520development%2520of%2520similar%2520benchmarks%2520in%250Aother%2520languages%252C%2520thereby%2520accelerating%2520multilingual%2520VLM%2520research.%2520The%2520code%2520and%250Adataset%2520for%2520KRETA%2520are%2520available%2520at%2520https%253A//github.com/tabtoyou/KRETA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KRETA%3A%20A%20Benchmark%20for%20Korean%20Reading%20and%20Reasoning%20in%20Text-Rich%20VQA%0A%20%20Attuned%20to%20Diverse%20Visual%20Contexts&entry.906535625=Taebaek%20Hwang%20and%20Minseo%20Kim%20and%20Gisang%20Lee%20and%20Seonuk%20Kim%20and%20Hyunjun%20Eun&entry.1292438233=%20%20Understanding%20and%20reasoning%20over%20text%20within%20visual%20contexts%20poses%20a%0Asignificant%20challenge%20for%20Vision-Language%20Models%20%28VLMs%29%2C%20given%20the%20complexity%0Aand%20diversity%20of%20real-world%20scenarios.%20To%20address%20this%20challenge%2C%20text-rich%0AVisual%20Question%20Answering%20%28VQA%29%20datasets%20and%20benchmarks%20have%20emerged%20for%0Ahigh-resource%20languages%20like%20English.%20However%2C%20a%20critical%20gap%20persists%20for%0Alow-resource%20languages%20such%20as%20Korean%2C%20where%20the%20lack%20of%20comprehensive%0Abenchmarks%20hinders%20robust%20model%20evaluation%20and%20comparison.%20To%20bridge%20this%20gap%2C%0Awe%20introduce%20KRETA%2C%20a%20benchmark%20for%20Korean%20Reading%20and%20rEasoning%20in%20Text-rich%0AVQA%20Attuned%20to%20diverse%20visual%20contexts.%20KRETA%20facilitates%20an%20in-depth%0Aevaluation%20of%20both%20visual%20text%20understanding%20and%20reasoning%20capabilities%2C%20while%0Aalso%20supporting%20a%20multifaceted%20assessment%20across%2015%20domains%20and%2026%20image%20types.%0AAdditionally%2C%20we%20introduce%20a%20semi-automated%20VQA%20generation%20pipeline%0Aspecifically%20optimized%20for%20text-rich%20settings%2C%20leveraging%20refined%20stepwise%0Aimage%20decomposition%20and%20a%20rigorous%20seven-metric%20evaluation%20protocol%20to%20ensure%0Adata%20quality.%20While%20KRETA%20is%20tailored%20for%20Korean%2C%20we%20hope%20our%20adaptable%20and%0Aextensible%20pipeline%20will%20facilitate%20the%20development%20of%20similar%20benchmarks%20in%0Aother%20languages%2C%20thereby%20accelerating%20multilingual%20VLM%20research.%20The%20code%20and%0Adataset%20for%20KRETA%20are%20available%20at%20https%3A//github.com/tabtoyou/KRETA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19944v1&entry.124074799=Read"},
{"title": "PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos", "author": "Ziyun Qian and Runyu Xiao and Shuyuan Tu and Wei Xue and Dingkang Yang and Mingcheng Li and Dongliang Kou and Minghao Han and Zizhi Chen and Lihua Zhang", "abstract": "  Recent advances in motion generation show remarkable progress. However,\nseveral limitations remain: (1) Existing pose-guided character motion transfer\nmethods merely replicate motion without learning its style characteristics,\nresulting in inexpressive characters. (2) Motion style transfer methods rely\nheavily on motion capture data, which is difficult to obtain. (3) Generated\nmotions sometimes violate physical laws. To address these challenges, this\npaper pioneers a new task: Video-to-Video Motion Personalization. We propose a\nnovel framework, PersonaAnimator, which learns personalized motion patterns\ndirectly from unconstrained videos. This enables personalized motion transfer.\nTo support this task, we introduce PersonaVid, the first video-based\npersonalized motion dataset. It contains 20 motion content categories and 120\nmotion style categories. We further propose a Physics-aware Motion Style\nRegularization mechanism to enforce physical plausibility in the generated\nmotions. Extensive experiments show that PersonaAnimator outperforms\nstate-of-the-art motion transfer methods and sets a new benchmark for the\nVideo-to-Video Motion Personalization task.\n", "link": "http://arxiv.org/abs/2508.19895v1", "date": "2025-08-27", "relevancy": 2.5731, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6648}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6416}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PersonaAnimator%3A%20Personalized%20Motion%20Transfer%20from%20Unconstrained%20Videos&body=Title%3A%20PersonaAnimator%3A%20Personalized%20Motion%20Transfer%20from%20Unconstrained%20Videos%0AAuthor%3A%20Ziyun%20Qian%20and%20Runyu%20Xiao%20and%20Shuyuan%20Tu%20and%20Wei%20Xue%20and%20Dingkang%20Yang%20and%20Mingcheng%20Li%20and%20Dongliang%20Kou%20and%20Minghao%20Han%20and%20Zizhi%20Chen%20and%20Lihua%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20motion%20generation%20show%20remarkable%20progress.%20However%2C%0Aseveral%20limitations%20remain%3A%20%281%29%20Existing%20pose-guided%20character%20motion%20transfer%0Amethods%20merely%20replicate%20motion%20without%20learning%20its%20style%20characteristics%2C%0Aresulting%20in%20inexpressive%20characters.%20%282%29%20Motion%20style%20transfer%20methods%20rely%0Aheavily%20on%20motion%20capture%20data%2C%20which%20is%20difficult%20to%20obtain.%20%283%29%20Generated%0Amotions%20sometimes%20violate%20physical%20laws.%20To%20address%20these%20challenges%2C%20this%0Apaper%20pioneers%20a%20new%20task%3A%20Video-to-Video%20Motion%20Personalization.%20We%20propose%20a%0Anovel%20framework%2C%20PersonaAnimator%2C%20which%20learns%20personalized%20motion%20patterns%0Adirectly%20from%20unconstrained%20videos.%20This%20enables%20personalized%20motion%20transfer.%0ATo%20support%20this%20task%2C%20we%20introduce%20PersonaVid%2C%20the%20first%20video-based%0Apersonalized%20motion%20dataset.%20It%20contains%2020%20motion%20content%20categories%20and%20120%0Amotion%20style%20categories.%20We%20further%20propose%20a%20Physics-aware%20Motion%20Style%0ARegularization%20mechanism%20to%20enforce%20physical%20plausibility%20in%20the%20generated%0Amotions.%20Extensive%20experiments%20show%20that%20PersonaAnimator%20outperforms%0Astate-of-the-art%20motion%20transfer%20methods%20and%20sets%20a%20new%20benchmark%20for%20the%0AVideo-to-Video%20Motion%20Personalization%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonaAnimator%253A%2520Personalized%2520Motion%2520Transfer%2520from%2520Unconstrained%2520Videos%26entry.906535625%3DZiyun%2520Qian%2520and%2520Runyu%2520Xiao%2520and%2520Shuyuan%2520Tu%2520and%2520Wei%2520Xue%2520and%2520Dingkang%2520Yang%2520and%2520Mingcheng%2520Li%2520and%2520Dongliang%2520Kou%2520and%2520Minghao%2520Han%2520and%2520Zizhi%2520Chen%2520and%2520Lihua%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520motion%2520generation%2520show%2520remarkable%2520progress.%2520However%252C%250Aseveral%2520limitations%2520remain%253A%2520%25281%2529%2520Existing%2520pose-guided%2520character%2520motion%2520transfer%250Amethods%2520merely%2520replicate%2520motion%2520without%2520learning%2520its%2520style%2520characteristics%252C%250Aresulting%2520in%2520inexpressive%2520characters.%2520%25282%2529%2520Motion%2520style%2520transfer%2520methods%2520rely%250Aheavily%2520on%2520motion%2520capture%2520data%252C%2520which%2520is%2520difficult%2520to%2520obtain.%2520%25283%2529%2520Generated%250Amotions%2520sometimes%2520violate%2520physical%2520laws.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520pioneers%2520a%2520new%2520task%253A%2520Video-to-Video%2520Motion%2520Personalization.%2520We%2520propose%2520a%250Anovel%2520framework%252C%2520PersonaAnimator%252C%2520which%2520learns%2520personalized%2520motion%2520patterns%250Adirectly%2520from%2520unconstrained%2520videos.%2520This%2520enables%2520personalized%2520motion%2520transfer.%250ATo%2520support%2520this%2520task%252C%2520we%2520introduce%2520PersonaVid%252C%2520the%2520first%2520video-based%250Apersonalized%2520motion%2520dataset.%2520It%2520contains%252020%2520motion%2520content%2520categories%2520and%2520120%250Amotion%2520style%2520categories.%2520We%2520further%2520propose%2520a%2520Physics-aware%2520Motion%2520Style%250ARegularization%2520mechanism%2520to%2520enforce%2520physical%2520plausibility%2520in%2520the%2520generated%250Amotions.%2520Extensive%2520experiments%2520show%2520that%2520PersonaAnimator%2520outperforms%250Astate-of-the-art%2520motion%2520transfer%2520methods%2520and%2520sets%2520a%2520new%2520benchmark%2520for%2520the%250AVideo-to-Video%2520Motion%2520Personalization%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PersonaAnimator%3A%20Personalized%20Motion%20Transfer%20from%20Unconstrained%20Videos&entry.906535625=Ziyun%20Qian%20and%20Runyu%20Xiao%20and%20Shuyuan%20Tu%20and%20Wei%20Xue%20and%20Dingkang%20Yang%20and%20Mingcheng%20Li%20and%20Dongliang%20Kou%20and%20Minghao%20Han%20and%20Zizhi%20Chen%20and%20Lihua%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20motion%20generation%20show%20remarkable%20progress.%20However%2C%0Aseveral%20limitations%20remain%3A%20%281%29%20Existing%20pose-guided%20character%20motion%20transfer%0Amethods%20merely%20replicate%20motion%20without%20learning%20its%20style%20characteristics%2C%0Aresulting%20in%20inexpressive%20characters.%20%282%29%20Motion%20style%20transfer%20methods%20rely%0Aheavily%20on%20motion%20capture%20data%2C%20which%20is%20difficult%20to%20obtain.%20%283%29%20Generated%0Amotions%20sometimes%20violate%20physical%20laws.%20To%20address%20these%20challenges%2C%20this%0Apaper%20pioneers%20a%20new%20task%3A%20Video-to-Video%20Motion%20Personalization.%20We%20propose%20a%0Anovel%20framework%2C%20PersonaAnimator%2C%20which%20learns%20personalized%20motion%20patterns%0Adirectly%20from%20unconstrained%20videos.%20This%20enables%20personalized%20motion%20transfer.%0ATo%20support%20this%20task%2C%20we%20introduce%20PersonaVid%2C%20the%20first%20video-based%0Apersonalized%20motion%20dataset.%20It%20contains%2020%20motion%20content%20categories%20and%20120%0Amotion%20style%20categories.%20We%20further%20propose%20a%20Physics-aware%20Motion%20Style%0ARegularization%20mechanism%20to%20enforce%20physical%20plausibility%20in%20the%20generated%0Amotions.%20Extensive%20experiments%20show%20that%20PersonaAnimator%20outperforms%0Astate-of-the-art%20motion%20transfer%20methods%20and%20sets%20a%20new%20benchmark%20for%20the%0AVideo-to-Video%20Motion%20Personalization%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19895v1&entry.124074799=Read"},
{"title": "InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural\n  Networks with Negative Corrections", "author": "Meng Qin and Weihua Li and Jinqiang Cui and Sen Pei", "abstract": "  Graph partitioning (GP), a.k.a. community detection, is a classic problem\nthat divides nodes of a graph into densely-connected blocks. From a perspective\nof graph signal processing, we find that graph Laplacian with a negative\ncorrection can derive graph frequencies beyond the conventional range $[0, 2]$.\nTo explore whether the low-frequency information beyond this range can encode\nmore informative properties about community structures, we propose InfraredGP.\nIt (\\romannumeral1) adopts a spectral GNN as its backbone combined with\nlow-pass filters and a negative correction mechanism, (\\romannumeral2) only\nfeeds random inputs to this backbone, (\\romannumeral3) derives graph embeddings\nvia one feed-forward propagation (FFP) without any training, and\n(\\romannumeral4) obtains feasible GP results by feeding the derived embeddings\nto BIRCH. Surprisingly, our experiments demonstrate that based solely on the\nnegative correction mechanism that amplifies low-frequency information beyond\n$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard\nclustering modules (e.g., BIRCH) and obtain high-quality results for GP without\nany training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate\nInfraredGP for both static and streaming GP, where InfraredGP can achieve much\nbetter efficiency (e.g., 16x-23x faster) and competitive quality over various\nbaselines. We have made our code public at\nhttps://github.com/KuroginQin/InfraredGP\n", "link": "http://arxiv.org/abs/2508.19737v1", "date": "2025-08-27", "relevancy": 2.5492, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5282}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5128}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfraredGP%3A%20Efficient%20Graph%20Partitioning%20via%20Spectral%20Graph%20Neural%0A%20%20Networks%20with%20Negative%20Corrections&body=Title%3A%20InfraredGP%3A%20Efficient%20Graph%20Partitioning%20via%20Spectral%20Graph%20Neural%0A%20%20Networks%20with%20Negative%20Corrections%0AAuthor%3A%20Meng%20Qin%20and%20Weihua%20Li%20and%20Jinqiang%20Cui%20and%20Sen%20Pei%0AAbstract%3A%20%20%20Graph%20partitioning%20%28GP%29%2C%20a.k.a.%20community%20detection%2C%20is%20a%20classic%20problem%0Athat%20divides%20nodes%20of%20a%20graph%20into%20densely-connected%20blocks.%20From%20a%20perspective%0Aof%20graph%20signal%20processing%2C%20we%20find%20that%20graph%20Laplacian%20with%20a%20negative%0Acorrection%20can%20derive%20graph%20frequencies%20beyond%20the%20conventional%20range%20%24%5B0%2C%202%5D%24.%0ATo%20explore%20whether%20the%20low-frequency%20information%20beyond%20this%20range%20can%20encode%0Amore%20informative%20properties%20about%20community%20structures%2C%20we%20propose%20InfraredGP.%0AIt%20%28%5Cromannumeral1%29%20adopts%20a%20spectral%20GNN%20as%20its%20backbone%20combined%20with%0Alow-pass%20filters%20and%20a%20negative%20correction%20mechanism%2C%20%28%5Cromannumeral2%29%20only%0Afeeds%20random%20inputs%20to%20this%20backbone%2C%20%28%5Cromannumeral3%29%20derives%20graph%20embeddings%0Avia%20one%20feed-forward%20propagation%20%28FFP%29%20without%20any%20training%2C%20and%0A%28%5Cromannumeral4%29%20obtains%20feasible%20GP%20results%20by%20feeding%20the%20derived%20embeddings%0Ato%20BIRCH.%20Surprisingly%2C%20our%20experiments%20demonstrate%20that%20based%20solely%20on%20the%0Anegative%20correction%20mechanism%20that%20amplifies%20low-frequency%20information%20beyond%0A%24%5B0%2C%202%5D%24%2C%20InfraredGP%20can%20derive%20distinguishable%20embeddings%20for%20some%20standard%0Aclustering%20modules%20%28e.g.%2C%20BIRCH%29%20and%20obtain%20high-quality%20results%20for%20GP%20without%0Aany%20training.%20Following%20the%20IEEE%20HPEC%20Graph%20Challenge%20benchmark%2C%20we%20evaluate%0AInfraredGP%20for%20both%20static%20and%20streaming%20GP%2C%20where%20InfraredGP%20can%20achieve%20much%0Abetter%20efficiency%20%28e.g.%2C%2016x-23x%20faster%29%20and%20competitive%20quality%20over%20various%0Abaselines.%20We%20have%20made%20our%20code%20public%20at%0Ahttps%3A//github.com/KuroginQin/InfraredGP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfraredGP%253A%2520Efficient%2520Graph%2520Partitioning%2520via%2520Spectral%2520Graph%2520Neural%250A%2520%2520Networks%2520with%2520Negative%2520Corrections%26entry.906535625%3DMeng%2520Qin%2520and%2520Weihua%2520Li%2520and%2520Jinqiang%2520Cui%2520and%2520Sen%2520Pei%26entry.1292438233%3D%2520%2520Graph%2520partitioning%2520%2528GP%2529%252C%2520a.k.a.%2520community%2520detection%252C%2520is%2520a%2520classic%2520problem%250Athat%2520divides%2520nodes%2520of%2520a%2520graph%2520into%2520densely-connected%2520blocks.%2520From%2520a%2520perspective%250Aof%2520graph%2520signal%2520processing%252C%2520we%2520find%2520that%2520graph%2520Laplacian%2520with%2520a%2520negative%250Acorrection%2520can%2520derive%2520graph%2520frequencies%2520beyond%2520the%2520conventional%2520range%2520%2524%255B0%252C%25202%255D%2524.%250ATo%2520explore%2520whether%2520the%2520low-frequency%2520information%2520beyond%2520this%2520range%2520can%2520encode%250Amore%2520informative%2520properties%2520about%2520community%2520structures%252C%2520we%2520propose%2520InfraredGP.%250AIt%2520%2528%255Cromannumeral1%2529%2520adopts%2520a%2520spectral%2520GNN%2520as%2520its%2520backbone%2520combined%2520with%250Alow-pass%2520filters%2520and%2520a%2520negative%2520correction%2520mechanism%252C%2520%2528%255Cromannumeral2%2529%2520only%250Afeeds%2520random%2520inputs%2520to%2520this%2520backbone%252C%2520%2528%255Cromannumeral3%2529%2520derives%2520graph%2520embeddings%250Avia%2520one%2520feed-forward%2520propagation%2520%2528FFP%2529%2520without%2520any%2520training%252C%2520and%250A%2528%255Cromannumeral4%2529%2520obtains%2520feasible%2520GP%2520results%2520by%2520feeding%2520the%2520derived%2520embeddings%250Ato%2520BIRCH.%2520Surprisingly%252C%2520our%2520experiments%2520demonstrate%2520that%2520based%2520solely%2520on%2520the%250Anegative%2520correction%2520mechanism%2520that%2520amplifies%2520low-frequency%2520information%2520beyond%250A%2524%255B0%252C%25202%255D%2524%252C%2520InfraredGP%2520can%2520derive%2520distinguishable%2520embeddings%2520for%2520some%2520standard%250Aclustering%2520modules%2520%2528e.g.%252C%2520BIRCH%2529%2520and%2520obtain%2520high-quality%2520results%2520for%2520GP%2520without%250Aany%2520training.%2520Following%2520the%2520IEEE%2520HPEC%2520Graph%2520Challenge%2520benchmark%252C%2520we%2520evaluate%250AInfraredGP%2520for%2520both%2520static%2520and%2520streaming%2520GP%252C%2520where%2520InfraredGP%2520can%2520achieve%2520much%250Abetter%2520efficiency%2520%2528e.g.%252C%252016x-23x%2520faster%2529%2520and%2520competitive%2520quality%2520over%2520various%250Abaselines.%2520We%2520have%2520made%2520our%2520code%2520public%2520at%250Ahttps%253A//github.com/KuroginQin/InfraredGP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfraredGP%3A%20Efficient%20Graph%20Partitioning%20via%20Spectral%20Graph%20Neural%0A%20%20Networks%20with%20Negative%20Corrections&entry.906535625=Meng%20Qin%20and%20Weihua%20Li%20and%20Jinqiang%20Cui%20and%20Sen%20Pei&entry.1292438233=%20%20Graph%20partitioning%20%28GP%29%2C%20a.k.a.%20community%20detection%2C%20is%20a%20classic%20problem%0Athat%20divides%20nodes%20of%20a%20graph%20into%20densely-connected%20blocks.%20From%20a%20perspective%0Aof%20graph%20signal%20processing%2C%20we%20find%20that%20graph%20Laplacian%20with%20a%20negative%0Acorrection%20can%20derive%20graph%20frequencies%20beyond%20the%20conventional%20range%20%24%5B0%2C%202%5D%24.%0ATo%20explore%20whether%20the%20low-frequency%20information%20beyond%20this%20range%20can%20encode%0Amore%20informative%20properties%20about%20community%20structures%2C%20we%20propose%20InfraredGP.%0AIt%20%28%5Cromannumeral1%29%20adopts%20a%20spectral%20GNN%20as%20its%20backbone%20combined%20with%0Alow-pass%20filters%20and%20a%20negative%20correction%20mechanism%2C%20%28%5Cromannumeral2%29%20only%0Afeeds%20random%20inputs%20to%20this%20backbone%2C%20%28%5Cromannumeral3%29%20derives%20graph%20embeddings%0Avia%20one%20feed-forward%20propagation%20%28FFP%29%20without%20any%20training%2C%20and%0A%28%5Cromannumeral4%29%20obtains%20feasible%20GP%20results%20by%20feeding%20the%20derived%20embeddings%0Ato%20BIRCH.%20Surprisingly%2C%20our%20experiments%20demonstrate%20that%20based%20solely%20on%20the%0Anegative%20correction%20mechanism%20that%20amplifies%20low-frequency%20information%20beyond%0A%24%5B0%2C%202%5D%24%2C%20InfraredGP%20can%20derive%20distinguishable%20embeddings%20for%20some%20standard%0Aclustering%20modules%20%28e.g.%2C%20BIRCH%29%20and%20obtain%20high-quality%20results%20for%20GP%20without%0Aany%20training.%20Following%20the%20IEEE%20HPEC%20Graph%20Challenge%20benchmark%2C%20we%20evaluate%0AInfraredGP%20for%20both%20static%20and%20streaming%20GP%2C%20where%20InfraredGP%20can%20achieve%20much%0Abetter%20efficiency%20%28e.g.%2C%2016x-23x%20faster%29%20and%20competitive%20quality%20over%20various%0Abaselines.%20We%20have%20made%20our%20code%20public%20at%0Ahttps%3A//github.com/KuroginQin/InfraredGP%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19737v1&entry.124074799=Read"},
{"title": "Training with Explanations Alone: A New Paradigm to Prevent Shortcut\n  Learning", "author": "Pedro R. A. S. Bassi and Haydr A. H. Ali and Andrea Cavalli and Sergio Decherchi", "abstract": "  Application of Artificial Intelligence (AI) in critical domains, like the\nmedical one, is often hampered by shortcut learning, which hinders AI\ngeneralization to diverse hospitals and patients. Shortcut learning can be\ncaused, for example, by background biases -- features in image backgrounds that\nare spuriously correlated to classification labels (e.g., words in X-rays). To\nmitigate the influence of image background and foreground bias on AI, we\nintroduce a new training paradigm, dubbed Training with Explanations Alone\n(TEA). TEA trains a classifier (TEA student) only by making its explanation\nheatmaps match target heatmaps from a larger teacher model. By learning from\nits explanation heatmaps, the TEA student pays attention to the same image\nfeatures as the teacher. For example, a teacher uses a large segmenter to\nremove image backgrounds before classification, thus ignoring background bias.\nBy learning from the teacher's explanation heatmaps, the TEA student learns to\nalso ignore backgrounds -- but it does not need a segmenter. With different\nteachers, the TEA student can also resist bias in the image foreground.\nSurprisingly, by training with heatmaps alone the student output naturally\nmatches the teacher output -- with no loss function applied to the student\noutput. We compared the TEA student against 14 state-of-the-art methods in 5\ndatasets with strong background or foreground bias, including Waterbirds and an\nX-Ray dataset for COVID-19 and pneumonia classification. The TEA student had\nbetter resistance to bias, strongly surpassing state-of-the-art methods, and\ngeneralizing better to hospitals not seen in training.\n", "link": "http://arxiv.org/abs/2407.09788v2", "date": "2025-08-27", "relevancy": 2.5444, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5428}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4955}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20with%20Explanations%20Alone%3A%20A%20New%20Paradigm%20to%20Prevent%20Shortcut%0A%20%20Learning&body=Title%3A%20Training%20with%20Explanations%20Alone%3A%20A%20New%20Paradigm%20to%20Prevent%20Shortcut%0A%20%20Learning%0AAuthor%3A%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Haydr%20A.%20H.%20Ali%20and%20Andrea%20Cavalli%20and%20Sergio%20Decherchi%0AAbstract%3A%20%20%20Application%20of%20Artificial%20Intelligence%20%28AI%29%20in%20critical%20domains%2C%20like%20the%0Amedical%20one%2C%20is%20often%20hampered%20by%20shortcut%20learning%2C%20which%20hinders%20AI%0Ageneralization%20to%20diverse%20hospitals%20and%20patients.%20Shortcut%20learning%20can%20be%0Acaused%2C%20for%20example%2C%20by%20background%20biases%20--%20features%20in%20image%20backgrounds%20that%0Aare%20spuriously%20correlated%20to%20classification%20labels%20%28e.g.%2C%20words%20in%20X-rays%29.%20To%0Amitigate%20the%20influence%20of%20image%20background%20and%20foreground%20bias%20on%20AI%2C%20we%0Aintroduce%20a%20new%20training%20paradigm%2C%20dubbed%20Training%20with%20Explanations%20Alone%0A%28TEA%29.%20TEA%20trains%20a%20classifier%20%28TEA%20student%29%20only%20by%20making%20its%20explanation%0Aheatmaps%20match%20target%20heatmaps%20from%20a%20larger%20teacher%20model.%20By%20learning%20from%0Aits%20explanation%20heatmaps%2C%20the%20TEA%20student%20pays%20attention%20to%20the%20same%20image%0Afeatures%20as%20the%20teacher.%20For%20example%2C%20a%20teacher%20uses%20a%20large%20segmenter%20to%0Aremove%20image%20backgrounds%20before%20classification%2C%20thus%20ignoring%20background%20bias.%0ABy%20learning%20from%20the%20teacher%27s%20explanation%20heatmaps%2C%20the%20TEA%20student%20learns%20to%0Aalso%20ignore%20backgrounds%20--%20but%20it%20does%20not%20need%20a%20segmenter.%20With%20different%0Ateachers%2C%20the%20TEA%20student%20can%20also%20resist%20bias%20in%20the%20image%20foreground.%0ASurprisingly%2C%20by%20training%20with%20heatmaps%20alone%20the%20student%20output%20naturally%0Amatches%20the%20teacher%20output%20--%20with%20no%20loss%20function%20applied%20to%20the%20student%0Aoutput.%20We%20compared%20the%20TEA%20student%20against%2014%20state-of-the-art%20methods%20in%205%0Adatasets%20with%20strong%20background%20or%20foreground%20bias%2C%20including%20Waterbirds%20and%20an%0AX-Ray%20dataset%20for%20COVID-19%20and%20pneumonia%20classification.%20The%20TEA%20student%20had%0Abetter%20resistance%20to%20bias%2C%20strongly%20surpassing%20state-of-the-art%20methods%2C%20and%0Ageneralizing%20better%20to%20hospitals%20not%20seen%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520with%2520Explanations%2520Alone%253A%2520A%2520New%2520Paradigm%2520to%2520Prevent%2520Shortcut%250A%2520%2520Learning%26entry.906535625%3DPedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Haydr%2520A.%2520H.%2520Ali%2520and%2520Andrea%2520Cavalli%2520and%2520Sergio%2520Decherchi%26entry.1292438233%3D%2520%2520Application%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520in%2520critical%2520domains%252C%2520like%2520the%250Amedical%2520one%252C%2520is%2520often%2520hampered%2520by%2520shortcut%2520learning%252C%2520which%2520hinders%2520AI%250Ageneralization%2520to%2520diverse%2520hospitals%2520and%2520patients.%2520Shortcut%2520learning%2520can%2520be%250Acaused%252C%2520for%2520example%252C%2520by%2520background%2520biases%2520--%2520features%2520in%2520image%2520backgrounds%2520that%250Aare%2520spuriously%2520correlated%2520to%2520classification%2520labels%2520%2528e.g.%252C%2520words%2520in%2520X-rays%2529.%2520To%250Amitigate%2520the%2520influence%2520of%2520image%2520background%2520and%2520foreground%2520bias%2520on%2520AI%252C%2520we%250Aintroduce%2520a%2520new%2520training%2520paradigm%252C%2520dubbed%2520Training%2520with%2520Explanations%2520Alone%250A%2528TEA%2529.%2520TEA%2520trains%2520a%2520classifier%2520%2528TEA%2520student%2529%2520only%2520by%2520making%2520its%2520explanation%250Aheatmaps%2520match%2520target%2520heatmaps%2520from%2520a%2520larger%2520teacher%2520model.%2520By%2520learning%2520from%250Aits%2520explanation%2520heatmaps%252C%2520the%2520TEA%2520student%2520pays%2520attention%2520to%2520the%2520same%2520image%250Afeatures%2520as%2520the%2520teacher.%2520For%2520example%252C%2520a%2520teacher%2520uses%2520a%2520large%2520segmenter%2520to%250Aremove%2520image%2520backgrounds%2520before%2520classification%252C%2520thus%2520ignoring%2520background%2520bias.%250ABy%2520learning%2520from%2520the%2520teacher%2527s%2520explanation%2520heatmaps%252C%2520the%2520TEA%2520student%2520learns%2520to%250Aalso%2520ignore%2520backgrounds%2520--%2520but%2520it%2520does%2520not%2520need%2520a%2520segmenter.%2520With%2520different%250Ateachers%252C%2520the%2520TEA%2520student%2520can%2520also%2520resist%2520bias%2520in%2520the%2520image%2520foreground.%250ASurprisingly%252C%2520by%2520training%2520with%2520heatmaps%2520alone%2520the%2520student%2520output%2520naturally%250Amatches%2520the%2520teacher%2520output%2520--%2520with%2520no%2520loss%2520function%2520applied%2520to%2520the%2520student%250Aoutput.%2520We%2520compared%2520the%2520TEA%2520student%2520against%252014%2520state-of-the-art%2520methods%2520in%25205%250Adatasets%2520with%2520strong%2520background%2520or%2520foreground%2520bias%252C%2520including%2520Waterbirds%2520and%2520an%250AX-Ray%2520dataset%2520for%2520COVID-19%2520and%2520pneumonia%2520classification.%2520The%2520TEA%2520student%2520had%250Abetter%2520resistance%2520to%2520bias%252C%2520strongly%2520surpassing%2520state-of-the-art%2520methods%252C%2520and%250Ageneralizing%2520better%2520to%2520hospitals%2520not%2520seen%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20with%20Explanations%20Alone%3A%20A%20New%20Paradigm%20to%20Prevent%20Shortcut%0A%20%20Learning&entry.906535625=Pedro%20R.%20A.%20S.%20Bassi%20and%20Haydr%20A.%20H.%20Ali%20and%20Andrea%20Cavalli%20and%20Sergio%20Decherchi&entry.1292438233=%20%20Application%20of%20Artificial%20Intelligence%20%28AI%29%20in%20critical%20domains%2C%20like%20the%0Amedical%20one%2C%20is%20often%20hampered%20by%20shortcut%20learning%2C%20which%20hinders%20AI%0Ageneralization%20to%20diverse%20hospitals%20and%20patients.%20Shortcut%20learning%20can%20be%0Acaused%2C%20for%20example%2C%20by%20background%20biases%20--%20features%20in%20image%20backgrounds%20that%0Aare%20spuriously%20correlated%20to%20classification%20labels%20%28e.g.%2C%20words%20in%20X-rays%29.%20To%0Amitigate%20the%20influence%20of%20image%20background%20and%20foreground%20bias%20on%20AI%2C%20we%0Aintroduce%20a%20new%20training%20paradigm%2C%20dubbed%20Training%20with%20Explanations%20Alone%0A%28TEA%29.%20TEA%20trains%20a%20classifier%20%28TEA%20student%29%20only%20by%20making%20its%20explanation%0Aheatmaps%20match%20target%20heatmaps%20from%20a%20larger%20teacher%20model.%20By%20learning%20from%0Aits%20explanation%20heatmaps%2C%20the%20TEA%20student%20pays%20attention%20to%20the%20same%20image%0Afeatures%20as%20the%20teacher.%20For%20example%2C%20a%20teacher%20uses%20a%20large%20segmenter%20to%0Aremove%20image%20backgrounds%20before%20classification%2C%20thus%20ignoring%20background%20bias.%0ABy%20learning%20from%20the%20teacher%27s%20explanation%20heatmaps%2C%20the%20TEA%20student%20learns%20to%0Aalso%20ignore%20backgrounds%20--%20but%20it%20does%20not%20need%20a%20segmenter.%20With%20different%0Ateachers%2C%20the%20TEA%20student%20can%20also%20resist%20bias%20in%20the%20image%20foreground.%0ASurprisingly%2C%20by%20training%20with%20heatmaps%20alone%20the%20student%20output%20naturally%0Amatches%20the%20teacher%20output%20--%20with%20no%20loss%20function%20applied%20to%20the%20student%0Aoutput.%20We%20compared%20the%20TEA%20student%20against%2014%20state-of-the-art%20methods%20in%205%0Adatasets%20with%20strong%20background%20or%20foreground%20bias%2C%20including%20Waterbirds%20and%20an%0AX-Ray%20dataset%20for%20COVID-19%20and%20pneumonia%20classification.%20The%20TEA%20student%20had%0Abetter%20resistance%20to%20bias%2C%20strongly%20surpassing%20state-of-the-art%20methods%2C%20and%0Ageneralizing%20better%20to%20hospitals%20not%20seen%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09788v2&entry.124074799=Read"},
{"title": "UltraRay: Introducing Full-Path Ray Tracing in Physics-Based Ultrasound\n  Simulation", "author": "Felix Duelmer and Mohammad Farid Azampour and Magdalena Wysocki and Nassir Navab", "abstract": "  Traditional ultrasound simulators solve the wave equation to model pressure\ndistribution fields, achieving high accuracy but requiring significant\ncomputational time and resources. To address this, ray tracing approaches have\nbeen introduced, modeling wave propagation as rays interacting with boundaries\nand scatterers. However, existing models simplify ray propagation, generating\nechoes at interaction points without considering return paths to the sensor.\nThis can result in unrealistic artifacts and necessitates careful scene tuning\nfor plausible results. We propose a novel ultrasound simulation pipeline that\nutilizes a ray tracing algorithm to generate echo data, tracing each ray from\nthe transducer through the scene and back to the sensor. To replicate advanced\nultrasound imaging, we introduce a ray emission scheme optimized for plane wave\nimaging, incorporating delay and steering capabilities. Furthermore, we\nintegrate a standard signal processing pipeline to simulate end-to-end\nultrasound image formation. We showcase the efficacy of the proposed pipeline\nby modeling synthetic scenes featuring highly reflective objects, such as\nbones. In doing so, our proposed approach, UltraRay, not only enhances the\noverall visual quality but also improves the realism of the simulated images by\naccurately capturing secondary reflections and reducing unnatural artifacts. By\nbuilding on top of a differentiable framework, the proposed pipeline lays the\ngroundwork for a fast and differentiable ultrasound simulation tool necessary\nfor gradient-based optimization, enabling advanced ultrasound beamforming\nstrategies, neural network integration, and accurate inverse scene\nreconstruction.\n", "link": "http://arxiv.org/abs/2501.05828v3", "date": "2025-08-27", "relevancy": 2.5386, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5094}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5094}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraRay%3A%20Introducing%20Full-Path%20Ray%20Tracing%20in%20Physics-Based%20Ultrasound%0A%20%20Simulation&body=Title%3A%20UltraRay%3A%20Introducing%20Full-Path%20Ray%20Tracing%20in%20Physics-Based%20Ultrasound%0A%20%20Simulation%0AAuthor%3A%20Felix%20Duelmer%20and%20Mohammad%20Farid%20Azampour%20and%20Magdalena%20Wysocki%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20Traditional%20ultrasound%20simulators%20solve%20the%20wave%20equation%20to%20model%20pressure%0Adistribution%20fields%2C%20achieving%20high%20accuracy%20but%20requiring%20significant%0Acomputational%20time%20and%20resources.%20To%20address%20this%2C%20ray%20tracing%20approaches%20have%0Abeen%20introduced%2C%20modeling%20wave%20propagation%20as%20rays%20interacting%20with%20boundaries%0Aand%20scatterers.%20However%2C%20existing%20models%20simplify%20ray%20propagation%2C%20generating%0Aechoes%20at%20interaction%20points%20without%20considering%20return%20paths%20to%20the%20sensor.%0AThis%20can%20result%20in%20unrealistic%20artifacts%20and%20necessitates%20careful%20scene%20tuning%0Afor%20plausible%20results.%20We%20propose%20a%20novel%20ultrasound%20simulation%20pipeline%20that%0Autilizes%20a%20ray%20tracing%20algorithm%20to%20generate%20echo%20data%2C%20tracing%20each%20ray%20from%0Athe%20transducer%20through%20the%20scene%20and%20back%20to%20the%20sensor.%20To%20replicate%20advanced%0Aultrasound%20imaging%2C%20we%20introduce%20a%20ray%20emission%20scheme%20optimized%20for%20plane%20wave%0Aimaging%2C%20incorporating%20delay%20and%20steering%20capabilities.%20Furthermore%2C%20we%0Aintegrate%20a%20standard%20signal%20processing%20pipeline%20to%20simulate%20end-to-end%0Aultrasound%20image%20formation.%20We%20showcase%20the%20efficacy%20of%20the%20proposed%20pipeline%0Aby%20modeling%20synthetic%20scenes%20featuring%20highly%20reflective%20objects%2C%20such%20as%0Abones.%20In%20doing%20so%2C%20our%20proposed%20approach%2C%20UltraRay%2C%20not%20only%20enhances%20the%0Aoverall%20visual%20quality%20but%20also%20improves%20the%20realism%20of%20the%20simulated%20images%20by%0Aaccurately%20capturing%20secondary%20reflections%20and%20reducing%20unnatural%20artifacts.%20By%0Abuilding%20on%20top%20of%20a%20differentiable%20framework%2C%20the%20proposed%20pipeline%20lays%20the%0Agroundwork%20for%20a%20fast%20and%20differentiable%20ultrasound%20simulation%20tool%20necessary%0Afor%20gradient-based%20optimization%2C%20enabling%20advanced%20ultrasound%20beamforming%0Astrategies%2C%20neural%20network%20integration%2C%20and%20accurate%20inverse%20scene%0Areconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraRay%253A%2520Introducing%2520Full-Path%2520Ray%2520Tracing%2520in%2520Physics-Based%2520Ultrasound%250A%2520%2520Simulation%26entry.906535625%3DFelix%2520Duelmer%2520and%2520Mohammad%2520Farid%2520Azampour%2520and%2520Magdalena%2520Wysocki%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520Traditional%2520ultrasound%2520simulators%2520solve%2520the%2520wave%2520equation%2520to%2520model%2520pressure%250Adistribution%2520fields%252C%2520achieving%2520high%2520accuracy%2520but%2520requiring%2520significant%250Acomputational%2520time%2520and%2520resources.%2520To%2520address%2520this%252C%2520ray%2520tracing%2520approaches%2520have%250Abeen%2520introduced%252C%2520modeling%2520wave%2520propagation%2520as%2520rays%2520interacting%2520with%2520boundaries%250Aand%2520scatterers.%2520However%252C%2520existing%2520models%2520simplify%2520ray%2520propagation%252C%2520generating%250Aechoes%2520at%2520interaction%2520points%2520without%2520considering%2520return%2520paths%2520to%2520the%2520sensor.%250AThis%2520can%2520result%2520in%2520unrealistic%2520artifacts%2520and%2520necessitates%2520careful%2520scene%2520tuning%250Afor%2520plausible%2520results.%2520We%2520propose%2520a%2520novel%2520ultrasound%2520simulation%2520pipeline%2520that%250Autilizes%2520a%2520ray%2520tracing%2520algorithm%2520to%2520generate%2520echo%2520data%252C%2520tracing%2520each%2520ray%2520from%250Athe%2520transducer%2520through%2520the%2520scene%2520and%2520back%2520to%2520the%2520sensor.%2520To%2520replicate%2520advanced%250Aultrasound%2520imaging%252C%2520we%2520introduce%2520a%2520ray%2520emission%2520scheme%2520optimized%2520for%2520plane%2520wave%250Aimaging%252C%2520incorporating%2520delay%2520and%2520steering%2520capabilities.%2520Furthermore%252C%2520we%250Aintegrate%2520a%2520standard%2520signal%2520processing%2520pipeline%2520to%2520simulate%2520end-to-end%250Aultrasound%2520image%2520formation.%2520We%2520showcase%2520the%2520efficacy%2520of%2520the%2520proposed%2520pipeline%250Aby%2520modeling%2520synthetic%2520scenes%2520featuring%2520highly%2520reflective%2520objects%252C%2520such%2520as%250Abones.%2520In%2520doing%2520so%252C%2520our%2520proposed%2520approach%252C%2520UltraRay%252C%2520not%2520only%2520enhances%2520the%250Aoverall%2520visual%2520quality%2520but%2520also%2520improves%2520the%2520realism%2520of%2520the%2520simulated%2520images%2520by%250Aaccurately%2520capturing%2520secondary%2520reflections%2520and%2520reducing%2520unnatural%2520artifacts.%2520By%250Abuilding%2520on%2520top%2520of%2520a%2520differentiable%2520framework%252C%2520the%2520proposed%2520pipeline%2520lays%2520the%250Agroundwork%2520for%2520a%2520fast%2520and%2520differentiable%2520ultrasound%2520simulation%2520tool%2520necessary%250Afor%2520gradient-based%2520optimization%252C%2520enabling%2520advanced%2520ultrasound%2520beamforming%250Astrategies%252C%2520neural%2520network%2520integration%252C%2520and%2520accurate%2520inverse%2520scene%250Areconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraRay%3A%20Introducing%20Full-Path%20Ray%20Tracing%20in%20Physics-Based%20Ultrasound%0A%20%20Simulation&entry.906535625=Felix%20Duelmer%20and%20Mohammad%20Farid%20Azampour%20and%20Magdalena%20Wysocki%20and%20Nassir%20Navab&entry.1292438233=%20%20Traditional%20ultrasound%20simulators%20solve%20the%20wave%20equation%20to%20model%20pressure%0Adistribution%20fields%2C%20achieving%20high%20accuracy%20but%20requiring%20significant%0Acomputational%20time%20and%20resources.%20To%20address%20this%2C%20ray%20tracing%20approaches%20have%0Abeen%20introduced%2C%20modeling%20wave%20propagation%20as%20rays%20interacting%20with%20boundaries%0Aand%20scatterers.%20However%2C%20existing%20models%20simplify%20ray%20propagation%2C%20generating%0Aechoes%20at%20interaction%20points%20without%20considering%20return%20paths%20to%20the%20sensor.%0AThis%20can%20result%20in%20unrealistic%20artifacts%20and%20necessitates%20careful%20scene%20tuning%0Afor%20plausible%20results.%20We%20propose%20a%20novel%20ultrasound%20simulation%20pipeline%20that%0Autilizes%20a%20ray%20tracing%20algorithm%20to%20generate%20echo%20data%2C%20tracing%20each%20ray%20from%0Athe%20transducer%20through%20the%20scene%20and%20back%20to%20the%20sensor.%20To%20replicate%20advanced%0Aultrasound%20imaging%2C%20we%20introduce%20a%20ray%20emission%20scheme%20optimized%20for%20plane%20wave%0Aimaging%2C%20incorporating%20delay%20and%20steering%20capabilities.%20Furthermore%2C%20we%0Aintegrate%20a%20standard%20signal%20processing%20pipeline%20to%20simulate%20end-to-end%0Aultrasound%20image%20formation.%20We%20showcase%20the%20efficacy%20of%20the%20proposed%20pipeline%0Aby%20modeling%20synthetic%20scenes%20featuring%20highly%20reflective%20objects%2C%20such%20as%0Abones.%20In%20doing%20so%2C%20our%20proposed%20approach%2C%20UltraRay%2C%20not%20only%20enhances%20the%0Aoverall%20visual%20quality%20but%20also%20improves%20the%20realism%20of%20the%20simulated%20images%20by%0Aaccurately%20capturing%20secondary%20reflections%20and%20reducing%20unnatural%20artifacts.%20By%0Abuilding%20on%20top%20of%20a%20differentiable%20framework%2C%20the%20proposed%20pipeline%20lays%20the%0Agroundwork%20for%20a%20fast%20and%20differentiable%20ultrasound%20simulation%20tool%20necessary%0Afor%20gradient-based%20optimization%2C%20enabling%20advanced%20ultrasound%20beamforming%0Astrategies%2C%20neural%20network%20integration%2C%20and%20accurate%20inverse%20scene%0Areconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05828v3&entry.124074799=Read"},
{"title": "Parameter-Free Structural-Diversity Message Passing for Graph Neural\n  Networks", "author": "Mingyue Kong and Yinglong Zhang and Chengda Xu and Xuewen Xia and Xing Xu", "abstract": "  Graph Neural Networks (GNNs) have shown remarkable performance in structured\ndata modeling tasks such as node classification. However, mainstream approaches\ngenerally rely on a large number of trainable parameters and fixed aggregation\nrules, making it difficult to adapt to graph data with strong structural\nheterogeneity and complex feature distributions. This often leads to\nover-smoothing of node representations and semantic degradation. To address\nthese issues, this paper proposes a parameter-free graph neural network\nframework based on structural diversity, namely SDGNN (Structural-Diversity\nGraph Neural Network). The framework is inspired by structural diversity theory\nand designs a unified structural-diversity message passing mechanism that\nsimultaneously captures the heterogeneity of neighborhood structures and the\nstability of feature semantics, without introducing additional trainable\nparameters. Unlike traditional parameterized methods, SDGNN does not rely on\ncomplex model training, but instead leverages complementary modeling from both\nstructure-driven and feature-driven perspectives, thereby effectively improving\nadaptability across datasets and scenarios. Experimental results show that on\neight public benchmark datasets and an interdisciplinary PubMed citation\nnetwork, SDGNN consistently outperforms mainstream GNNs under challenging\nconditions such as low supervision, class imbalance, and cross-domain transfer.\nThis work provides a new theoretical perspective and general approach for the\ndesign of parameter-free graph neural networks, and further validates the\nimportance of structural diversity as a core signal in graph representation\nlearning. To facilitate reproducibility and further research, the full\nimplementation of SDGNN has been released at:\nhttps://github.com/mingyue15694/SGDNN/tree/main\n", "link": "http://arxiv.org/abs/2508.19884v1", "date": "2025-08-27", "relevancy": 2.5342, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5156}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5028}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-Free%20Structural-Diversity%20Message%20Passing%20for%20Graph%20Neural%0A%20%20Networks&body=Title%3A%20Parameter-Free%20Structural-Diversity%20Message%20Passing%20for%20Graph%20Neural%0A%20%20Networks%0AAuthor%3A%20Mingyue%20Kong%20and%20Yinglong%20Zhang%20and%20Chengda%20Xu%20and%20Xuewen%20Xia%20and%20Xing%20Xu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20performance%20in%20structured%0Adata%20modeling%20tasks%20such%20as%20node%20classification.%20However%2C%20mainstream%20approaches%0Agenerally%20rely%20on%20a%20large%20number%20of%20trainable%20parameters%20and%20fixed%20aggregation%0Arules%2C%20making%20it%20difficult%20to%20adapt%20to%20graph%20data%20with%20strong%20structural%0Aheterogeneity%20and%20complex%20feature%20distributions.%20This%20often%20leads%20to%0Aover-smoothing%20of%20node%20representations%20and%20semantic%20degradation.%20To%20address%0Athese%20issues%2C%20this%20paper%20proposes%20a%20parameter-free%20graph%20neural%20network%0Aframework%20based%20on%20structural%20diversity%2C%20namely%20SDGNN%20%28Structural-Diversity%0AGraph%20Neural%20Network%29.%20The%20framework%20is%20inspired%20by%20structural%20diversity%20theory%0Aand%20designs%20a%20unified%20structural-diversity%20message%20passing%20mechanism%20that%0Asimultaneously%20captures%20the%20heterogeneity%20of%20neighborhood%20structures%20and%20the%0Astability%20of%20feature%20semantics%2C%20without%20introducing%20additional%20trainable%0Aparameters.%20Unlike%20traditional%20parameterized%20methods%2C%20SDGNN%20does%20not%20rely%20on%0Acomplex%20model%20training%2C%20but%20instead%20leverages%20complementary%20modeling%20from%20both%0Astructure-driven%20and%20feature-driven%20perspectives%2C%20thereby%20effectively%20improving%0Aadaptability%20across%20datasets%20and%20scenarios.%20Experimental%20results%20show%20that%20on%0Aeight%20public%20benchmark%20datasets%20and%20an%20interdisciplinary%20PubMed%20citation%0Anetwork%2C%20SDGNN%20consistently%20outperforms%20mainstream%20GNNs%20under%20challenging%0Aconditions%20such%20as%20low%20supervision%2C%20class%20imbalance%2C%20and%20cross-domain%20transfer.%0AThis%20work%20provides%20a%20new%20theoretical%20perspective%20and%20general%20approach%20for%20the%0Adesign%20of%20parameter-free%20graph%20neural%20networks%2C%20and%20further%20validates%20the%0Aimportance%20of%20structural%20diversity%20as%20a%20core%20signal%20in%20graph%20representation%0Alearning.%20To%20facilitate%20reproducibility%20and%20further%20research%2C%20the%20full%0Aimplementation%20of%20SDGNN%20has%20been%20released%20at%3A%0Ahttps%3A//github.com/mingyue15694/SGDNN/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-Free%2520Structural-Diversity%2520Message%2520Passing%2520for%2520Graph%2520Neural%250A%2520%2520Networks%26entry.906535625%3DMingyue%2520Kong%2520and%2520Yinglong%2520Zhang%2520and%2520Chengda%2520Xu%2520and%2520Xuewen%2520Xia%2520and%2520Xing%2520Xu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520remarkable%2520performance%2520in%2520structured%250Adata%2520modeling%2520tasks%2520such%2520as%2520node%2520classification.%2520However%252C%2520mainstream%2520approaches%250Agenerally%2520rely%2520on%2520a%2520large%2520number%2520of%2520trainable%2520parameters%2520and%2520fixed%2520aggregation%250Arules%252C%2520making%2520it%2520difficult%2520to%2520adapt%2520to%2520graph%2520data%2520with%2520strong%2520structural%250Aheterogeneity%2520and%2520complex%2520feature%2520distributions.%2520This%2520often%2520leads%2520to%250Aover-smoothing%2520of%2520node%2520representations%2520and%2520semantic%2520degradation.%2520To%2520address%250Athese%2520issues%252C%2520this%2520paper%2520proposes%2520a%2520parameter-free%2520graph%2520neural%2520network%250Aframework%2520based%2520on%2520structural%2520diversity%252C%2520namely%2520SDGNN%2520%2528Structural-Diversity%250AGraph%2520Neural%2520Network%2529.%2520The%2520framework%2520is%2520inspired%2520by%2520structural%2520diversity%2520theory%250Aand%2520designs%2520a%2520unified%2520structural-diversity%2520message%2520passing%2520mechanism%2520that%250Asimultaneously%2520captures%2520the%2520heterogeneity%2520of%2520neighborhood%2520structures%2520and%2520the%250Astability%2520of%2520feature%2520semantics%252C%2520without%2520introducing%2520additional%2520trainable%250Aparameters.%2520Unlike%2520traditional%2520parameterized%2520methods%252C%2520SDGNN%2520does%2520not%2520rely%2520on%250Acomplex%2520model%2520training%252C%2520but%2520instead%2520leverages%2520complementary%2520modeling%2520from%2520both%250Astructure-driven%2520and%2520feature-driven%2520perspectives%252C%2520thereby%2520effectively%2520improving%250Aadaptability%2520across%2520datasets%2520and%2520scenarios.%2520Experimental%2520results%2520show%2520that%2520on%250Aeight%2520public%2520benchmark%2520datasets%2520and%2520an%2520interdisciplinary%2520PubMed%2520citation%250Anetwork%252C%2520SDGNN%2520consistently%2520outperforms%2520mainstream%2520GNNs%2520under%2520challenging%250Aconditions%2520such%2520as%2520low%2520supervision%252C%2520class%2520imbalance%252C%2520and%2520cross-domain%2520transfer.%250AThis%2520work%2520provides%2520a%2520new%2520theoretical%2520perspective%2520and%2520general%2520approach%2520for%2520the%250Adesign%2520of%2520parameter-free%2520graph%2520neural%2520networks%252C%2520and%2520further%2520validates%2520the%250Aimportance%2520of%2520structural%2520diversity%2520as%2520a%2520core%2520signal%2520in%2520graph%2520representation%250Alearning.%2520To%2520facilitate%2520reproducibility%2520and%2520further%2520research%252C%2520the%2520full%250Aimplementation%2520of%2520SDGNN%2520has%2520been%2520released%2520at%253A%250Ahttps%253A//github.com/mingyue15694/SGDNN/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-Free%20Structural-Diversity%20Message%20Passing%20for%20Graph%20Neural%0A%20%20Networks&entry.906535625=Mingyue%20Kong%20and%20Yinglong%20Zhang%20and%20Chengda%20Xu%20and%20Xuewen%20Xia%20and%20Xing%20Xu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20remarkable%20performance%20in%20structured%0Adata%20modeling%20tasks%20such%20as%20node%20classification.%20However%2C%20mainstream%20approaches%0Agenerally%20rely%20on%20a%20large%20number%20of%20trainable%20parameters%20and%20fixed%20aggregation%0Arules%2C%20making%20it%20difficult%20to%20adapt%20to%20graph%20data%20with%20strong%20structural%0Aheterogeneity%20and%20complex%20feature%20distributions.%20This%20often%20leads%20to%0Aover-smoothing%20of%20node%20representations%20and%20semantic%20degradation.%20To%20address%0Athese%20issues%2C%20this%20paper%20proposes%20a%20parameter-free%20graph%20neural%20network%0Aframework%20based%20on%20structural%20diversity%2C%20namely%20SDGNN%20%28Structural-Diversity%0AGraph%20Neural%20Network%29.%20The%20framework%20is%20inspired%20by%20structural%20diversity%20theory%0Aand%20designs%20a%20unified%20structural-diversity%20message%20passing%20mechanism%20that%0Asimultaneously%20captures%20the%20heterogeneity%20of%20neighborhood%20structures%20and%20the%0Astability%20of%20feature%20semantics%2C%20without%20introducing%20additional%20trainable%0Aparameters.%20Unlike%20traditional%20parameterized%20methods%2C%20SDGNN%20does%20not%20rely%20on%0Acomplex%20model%20training%2C%20but%20instead%20leverages%20complementary%20modeling%20from%20both%0Astructure-driven%20and%20feature-driven%20perspectives%2C%20thereby%20effectively%20improving%0Aadaptability%20across%20datasets%20and%20scenarios.%20Experimental%20results%20show%20that%20on%0Aeight%20public%20benchmark%20datasets%20and%20an%20interdisciplinary%20PubMed%20citation%0Anetwork%2C%20SDGNN%20consistently%20outperforms%20mainstream%20GNNs%20under%20challenging%0Aconditions%20such%20as%20low%20supervision%2C%20class%20imbalance%2C%20and%20cross-domain%20transfer.%0AThis%20work%20provides%20a%20new%20theoretical%20perspective%20and%20general%20approach%20for%20the%0Adesign%20of%20parameter-free%20graph%20neural%20networks%2C%20and%20further%20validates%20the%0Aimportance%20of%20structural%20diversity%20as%20a%20core%20signal%20in%20graph%20representation%0Alearning.%20To%20facilitate%20reproducibility%20and%20further%20research%2C%20the%20full%0Aimplementation%20of%20SDGNN%20has%20been%20released%20at%3A%0Ahttps%3A//github.com/mingyue15694/SGDNN/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19884v1&entry.124074799=Read"},
{"title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust\n  Cross-View Geo-Localization under Noisy Correspondence", "author": "Zheng Li and Yanming Guo and WenZhe Liu and Xueyi Zhang and Zhaoyun Ding and Long Xu and Mingrui Lao", "abstract": "  Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.\n", "link": "http://arxiv.org/abs/2508.20066v1", "date": "2025-08-27", "relevancy": 2.5305, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.7082}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6033}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAUL%3A%20Uncertainty-Guided%20Partition%20and%20Augmentation%20for%20Robust%0A%20%20Cross-View%20Geo-Localization%20under%20Noisy%20Correspondence&body=Title%3A%20PAUL%3A%20Uncertainty-Guided%20Partition%20and%20Augmentation%20for%20Robust%0A%20%20Cross-View%20Geo-Localization%20under%20Noisy%20Correspondence%0AAuthor%3A%20Zheng%20Li%20and%20Yanming%20Guo%20and%20WenZhe%20Liu%20and%20Xueyi%20Zhang%20and%20Zhaoyun%20Ding%20and%20Long%20Xu%20and%20Mingrui%20Lao%0AAbstract%3A%20%20%20Cross-view%20geo-localization%20is%20a%20critical%20task%20for%20UAV%20navigation%2C%20event%0Adetection%2C%20and%20aerial%20surveying%2C%20as%20it%20enables%20matching%20between%20drone-captured%0Aand%20satellite%20imagery.%20Most%20existing%20approaches%20embed%20multi-modal%20data%20into%20a%0Ajoint%20feature%20space%20to%20maximize%20the%20similarity%20of%20paired%20images.%20However%2C%20these%0Amethods%20typically%20assume%20perfect%20alignment%20of%20image%20pairs%20during%20training%2C%0Awhich%20rarely%20holds%20true%20in%20real-world%20scenarios.%20In%20practice%2C%20factors%20such%20as%0Aurban%20canyon%20effects%2C%20electromagnetic%20interference%2C%20and%20adverse%20weather%0Afrequently%20induce%20GPS%20drift%2C%20resulting%20in%20systematic%20alignment%20shifts%20where%0Aonly%20partial%20correspondences%20exist%20between%20pairs.%20Despite%20its%20prevalence%2C%20this%0Asource%20of%20noisy%20correspondence%20has%20received%20limited%20attention%20in%20current%0Aresearch.%20In%20this%20paper%2C%20we%20formally%20introduce%20and%20address%20the%20Noisy%0ACorrespondence%20on%20Cross-View%20Geo-Localization%20%28NC-CVGL%29%20problem%2C%20aiming%20to%0Abridge%20the%20gap%20between%20idealized%20benchmarks%20and%20practical%20applications.%20To%20this%0Aend%2C%20we%20propose%20PAUL%20%28Partition%20and%20Augmentation%20by%20Uncertainty%20Learning%29%2C%20a%0Anovel%20framework%20that%20partitions%20and%20augments%20training%20data%20based%20on%20estimated%0Adata%20uncertainty%20through%20uncertainty-aware%20co-augmentation%20and%20evidential%0Aco-training.%20Specifically%2C%20PAUL%20selectively%20augments%20regions%20with%20high%0Acorrespondence%20confidence%20and%20utilizes%20uncertainty%20estimation%20to%20refine%20feature%0Alearning%2C%20effectively%20suppressing%20noise%20from%20misaligned%20pairs.%20Distinct%20from%0Atraditional%20filtering%20or%20label%20correction%2C%20PAUL%20leverages%20both%20data%20uncertainty%0Aand%20loss%20discrepancy%20for%20targeted%20partitioning%20and%20augmentation%2C%20thus%20providing%0Arobust%20supervision%20for%20noisy%20samples.%20Comprehensive%20experiments%20validate%20the%0Aeffectiveness%20of%20individual%20components%20in%20PAUL%2Cwhich%20consistently%20achieves%0Asuperior%20performance%20over%20other%20competitive%20noisy-correspondence-driven%20methods%0Ain%20various%20noise%20ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAUL%253A%2520Uncertainty-Guided%2520Partition%2520and%2520Augmentation%2520for%2520Robust%250A%2520%2520Cross-View%2520Geo-Localization%2520under%2520Noisy%2520Correspondence%26entry.906535625%3DZheng%2520Li%2520and%2520Yanming%2520Guo%2520and%2520WenZhe%2520Liu%2520and%2520Xueyi%2520Zhang%2520and%2520Zhaoyun%2520Ding%2520and%2520Long%2520Xu%2520and%2520Mingrui%2520Lao%26entry.1292438233%3D%2520%2520Cross-view%2520geo-localization%2520is%2520a%2520critical%2520task%2520for%2520UAV%2520navigation%252C%2520event%250Adetection%252C%2520and%2520aerial%2520surveying%252C%2520as%2520it%2520enables%2520matching%2520between%2520drone-captured%250Aand%2520satellite%2520imagery.%2520Most%2520existing%2520approaches%2520embed%2520multi-modal%2520data%2520into%2520a%250Ajoint%2520feature%2520space%2520to%2520maximize%2520the%2520similarity%2520of%2520paired%2520images.%2520However%252C%2520these%250Amethods%2520typically%2520assume%2520perfect%2520alignment%2520of%2520image%2520pairs%2520during%2520training%252C%250Awhich%2520rarely%2520holds%2520true%2520in%2520real-world%2520scenarios.%2520In%2520practice%252C%2520factors%2520such%2520as%250Aurban%2520canyon%2520effects%252C%2520electromagnetic%2520interference%252C%2520and%2520adverse%2520weather%250Afrequently%2520induce%2520GPS%2520drift%252C%2520resulting%2520in%2520systematic%2520alignment%2520shifts%2520where%250Aonly%2520partial%2520correspondences%2520exist%2520between%2520pairs.%2520Despite%2520its%2520prevalence%252C%2520this%250Asource%2520of%2520noisy%2520correspondence%2520has%2520received%2520limited%2520attention%2520in%2520current%250Aresearch.%2520In%2520this%2520paper%252C%2520we%2520formally%2520introduce%2520and%2520address%2520the%2520Noisy%250ACorrespondence%2520on%2520Cross-View%2520Geo-Localization%2520%2528NC-CVGL%2529%2520problem%252C%2520aiming%2520to%250Abridge%2520the%2520gap%2520between%2520idealized%2520benchmarks%2520and%2520practical%2520applications.%2520To%2520this%250Aend%252C%2520we%2520propose%2520PAUL%2520%2528Partition%2520and%2520Augmentation%2520by%2520Uncertainty%2520Learning%2529%252C%2520a%250Anovel%2520framework%2520that%2520partitions%2520and%2520augments%2520training%2520data%2520based%2520on%2520estimated%250Adata%2520uncertainty%2520through%2520uncertainty-aware%2520co-augmentation%2520and%2520evidential%250Aco-training.%2520Specifically%252C%2520PAUL%2520selectively%2520augments%2520regions%2520with%2520high%250Acorrespondence%2520confidence%2520and%2520utilizes%2520uncertainty%2520estimation%2520to%2520refine%2520feature%250Alearning%252C%2520effectively%2520suppressing%2520noise%2520from%2520misaligned%2520pairs.%2520Distinct%2520from%250Atraditional%2520filtering%2520or%2520label%2520correction%252C%2520PAUL%2520leverages%2520both%2520data%2520uncertainty%250Aand%2520loss%2520discrepancy%2520for%2520targeted%2520partitioning%2520and%2520augmentation%252C%2520thus%2520providing%250Arobust%2520supervision%2520for%2520noisy%2520samples.%2520Comprehensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520individual%2520components%2520in%2520PAUL%252Cwhich%2520consistently%2520achieves%250Asuperior%2520performance%2520over%2520other%2520competitive%2520noisy-correspondence-driven%2520methods%250Ain%2520various%2520noise%2520ratios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAUL%3A%20Uncertainty-Guided%20Partition%20and%20Augmentation%20for%20Robust%0A%20%20Cross-View%20Geo-Localization%20under%20Noisy%20Correspondence&entry.906535625=Zheng%20Li%20and%20Yanming%20Guo%20and%20WenZhe%20Liu%20and%20Xueyi%20Zhang%20and%20Zhaoyun%20Ding%20and%20Long%20Xu%20and%20Mingrui%20Lao&entry.1292438233=%20%20Cross-view%20geo-localization%20is%20a%20critical%20task%20for%20UAV%20navigation%2C%20event%0Adetection%2C%20and%20aerial%20surveying%2C%20as%20it%20enables%20matching%20between%20drone-captured%0Aand%20satellite%20imagery.%20Most%20existing%20approaches%20embed%20multi-modal%20data%20into%20a%0Ajoint%20feature%20space%20to%20maximize%20the%20similarity%20of%20paired%20images.%20However%2C%20these%0Amethods%20typically%20assume%20perfect%20alignment%20of%20image%20pairs%20during%20training%2C%0Awhich%20rarely%20holds%20true%20in%20real-world%20scenarios.%20In%20practice%2C%20factors%20such%20as%0Aurban%20canyon%20effects%2C%20electromagnetic%20interference%2C%20and%20adverse%20weather%0Afrequently%20induce%20GPS%20drift%2C%20resulting%20in%20systematic%20alignment%20shifts%20where%0Aonly%20partial%20correspondences%20exist%20between%20pairs.%20Despite%20its%20prevalence%2C%20this%0Asource%20of%20noisy%20correspondence%20has%20received%20limited%20attention%20in%20current%0Aresearch.%20In%20this%20paper%2C%20we%20formally%20introduce%20and%20address%20the%20Noisy%0ACorrespondence%20on%20Cross-View%20Geo-Localization%20%28NC-CVGL%29%20problem%2C%20aiming%20to%0Abridge%20the%20gap%20between%20idealized%20benchmarks%20and%20practical%20applications.%20To%20this%0Aend%2C%20we%20propose%20PAUL%20%28Partition%20and%20Augmentation%20by%20Uncertainty%20Learning%29%2C%20a%0Anovel%20framework%20that%20partitions%20and%20augments%20training%20data%20based%20on%20estimated%0Adata%20uncertainty%20through%20uncertainty-aware%20co-augmentation%20and%20evidential%0Aco-training.%20Specifically%2C%20PAUL%20selectively%20augments%20regions%20with%20high%0Acorrespondence%20confidence%20and%20utilizes%20uncertainty%20estimation%20to%20refine%20feature%0Alearning%2C%20effectively%20suppressing%20noise%20from%20misaligned%20pairs.%20Distinct%20from%0Atraditional%20filtering%20or%20label%20correction%2C%20PAUL%20leverages%20both%20data%20uncertainty%0Aand%20loss%20discrepancy%20for%20targeted%20partitioning%20and%20augmentation%2C%20thus%20providing%0Arobust%20supervision%20for%20noisy%20samples.%20Comprehensive%20experiments%20validate%20the%0Aeffectiveness%20of%20individual%20components%20in%20PAUL%2Cwhich%20consistently%20achieves%0Asuperior%20performance%20over%20other%20competitive%20noisy-correspondence-driven%20methods%0Ain%20various%20noise%20ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20066v1&entry.124074799=Read"},
{"title": "Evaluating Language Model Reasoning about Confidential Information", "author": "Dylan Sam and Alexander Robey and Andy Zou and Matt Fredrikson and J. Zico Kolter", "abstract": "  As language models are increasingly deployed as autonomous agents in\nhigh-stakes settings, ensuring that they reliably follow user-defined rules has\nbecome a critical safety concern. To this end, we study whether language models\nexhibit contextual robustness, or the capability to adhere to context-dependent\nsafety specifications. For this analysis, we develop a benchmark (PasswordEval)\nthat measures whether language models can correctly determine when a user\nrequest is authorized (i.e., with a correct password). We find that current\nopen- and closed-source models struggle with this seemingly simple task, and\nthat, perhaps surprisingly, reasoning capabilities do not generally improve\nperformance. In fact, we find that reasoning traces frequently leak\nconfidential information, which calls into question whether reasoning traces\nshould be exposed to users in such applications. We also scale the difficulty\nof our evaluation along multiple axes: (i) by adding adversarial user pressure\nthrough various jailbreaking strategies, and (ii) through longer multi-turn\nconversations where password verification is more challenging. Overall, our\nresults suggest that current frontier models are not well-suited to handling\nconfidential information, and that reasoning capabilities may need to be\ntrained in a different manner to make them safer for release in high-stakes\nsettings.\n", "link": "http://arxiv.org/abs/2508.19980v1", "date": "2025-08-27", "relevancy": 2.5151, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5179}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Language%20Model%20Reasoning%20about%20Confidential%20Information&body=Title%3A%20Evaluating%20Language%20Model%20Reasoning%20about%20Confidential%20Information%0AAuthor%3A%20Dylan%20Sam%20and%20Alexander%20Robey%20and%20Andy%20Zou%20and%20Matt%20Fredrikson%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20As%20language%20models%20are%20increasingly%20deployed%20as%20autonomous%20agents%20in%0Ahigh-stakes%20settings%2C%20ensuring%20that%20they%20reliably%20follow%20user-defined%20rules%20has%0Abecome%20a%20critical%20safety%20concern.%20To%20this%20end%2C%20we%20study%20whether%20language%20models%0Aexhibit%20contextual%20robustness%2C%20or%20the%20capability%20to%20adhere%20to%20context-dependent%0Asafety%20specifications.%20For%20this%20analysis%2C%20we%20develop%20a%20benchmark%20%28PasswordEval%29%0Athat%20measures%20whether%20language%20models%20can%20correctly%20determine%20when%20a%20user%0Arequest%20is%20authorized%20%28i.e.%2C%20with%20a%20correct%20password%29.%20We%20find%20that%20current%0Aopen-%20and%20closed-source%20models%20struggle%20with%20this%20seemingly%20simple%20task%2C%20and%0Athat%2C%20perhaps%20surprisingly%2C%20reasoning%20capabilities%20do%20not%20generally%20improve%0Aperformance.%20In%20fact%2C%20we%20find%20that%20reasoning%20traces%20frequently%20leak%0Aconfidential%20information%2C%20which%20calls%20into%20question%20whether%20reasoning%20traces%0Ashould%20be%20exposed%20to%20users%20in%20such%20applications.%20We%20also%20scale%20the%20difficulty%0Aof%20our%20evaluation%20along%20multiple%20axes%3A%20%28i%29%20by%20adding%20adversarial%20user%20pressure%0Athrough%20various%20jailbreaking%20strategies%2C%20and%20%28ii%29%20through%20longer%20multi-turn%0Aconversations%20where%20password%20verification%20is%20more%20challenging.%20Overall%2C%20our%0Aresults%20suggest%20that%20current%20frontier%20models%20are%20not%20well-suited%20to%20handling%0Aconfidential%20information%2C%20and%20that%20reasoning%20capabilities%20may%20need%20to%20be%0Atrained%20in%20a%20different%20manner%20to%20make%20them%20safer%20for%20release%20in%20high-stakes%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Language%2520Model%2520Reasoning%2520about%2520Confidential%2520Information%26entry.906535625%3DDylan%2520Sam%2520and%2520Alexander%2520Robey%2520and%2520Andy%2520Zou%2520and%2520Matt%2520Fredrikson%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520As%2520language%2520models%2520are%2520increasingly%2520deployed%2520as%2520autonomous%2520agents%2520in%250Ahigh-stakes%2520settings%252C%2520ensuring%2520that%2520they%2520reliably%2520follow%2520user-defined%2520rules%2520has%250Abecome%2520a%2520critical%2520safety%2520concern.%2520To%2520this%2520end%252C%2520we%2520study%2520whether%2520language%2520models%250Aexhibit%2520contextual%2520robustness%252C%2520or%2520the%2520capability%2520to%2520adhere%2520to%2520context-dependent%250Asafety%2520specifications.%2520For%2520this%2520analysis%252C%2520we%2520develop%2520a%2520benchmark%2520%2528PasswordEval%2529%250Athat%2520measures%2520whether%2520language%2520models%2520can%2520correctly%2520determine%2520when%2520a%2520user%250Arequest%2520is%2520authorized%2520%2528i.e.%252C%2520with%2520a%2520correct%2520password%2529.%2520We%2520find%2520that%2520current%250Aopen-%2520and%2520closed-source%2520models%2520struggle%2520with%2520this%2520seemingly%2520simple%2520task%252C%2520and%250Athat%252C%2520perhaps%2520surprisingly%252C%2520reasoning%2520capabilities%2520do%2520not%2520generally%2520improve%250Aperformance.%2520In%2520fact%252C%2520we%2520find%2520that%2520reasoning%2520traces%2520frequently%2520leak%250Aconfidential%2520information%252C%2520which%2520calls%2520into%2520question%2520whether%2520reasoning%2520traces%250Ashould%2520be%2520exposed%2520to%2520users%2520in%2520such%2520applications.%2520We%2520also%2520scale%2520the%2520difficulty%250Aof%2520our%2520evaluation%2520along%2520multiple%2520axes%253A%2520%2528i%2529%2520by%2520adding%2520adversarial%2520user%2520pressure%250Athrough%2520various%2520jailbreaking%2520strategies%252C%2520and%2520%2528ii%2529%2520through%2520longer%2520multi-turn%250Aconversations%2520where%2520password%2520verification%2520is%2520more%2520challenging.%2520Overall%252C%2520our%250Aresults%2520suggest%2520that%2520current%2520frontier%2520models%2520are%2520not%2520well-suited%2520to%2520handling%250Aconfidential%2520information%252C%2520and%2520that%2520reasoning%2520capabilities%2520may%2520need%2520to%2520be%250Atrained%2520in%2520a%2520different%2520manner%2520to%2520make%2520them%2520safer%2520for%2520release%2520in%2520high-stakes%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Language%20Model%20Reasoning%20about%20Confidential%20Information&entry.906535625=Dylan%20Sam%20and%20Alexander%20Robey%20and%20Andy%20Zou%20and%20Matt%20Fredrikson%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20As%20language%20models%20are%20increasingly%20deployed%20as%20autonomous%20agents%20in%0Ahigh-stakes%20settings%2C%20ensuring%20that%20they%20reliably%20follow%20user-defined%20rules%20has%0Abecome%20a%20critical%20safety%20concern.%20To%20this%20end%2C%20we%20study%20whether%20language%20models%0Aexhibit%20contextual%20robustness%2C%20or%20the%20capability%20to%20adhere%20to%20context-dependent%0Asafety%20specifications.%20For%20this%20analysis%2C%20we%20develop%20a%20benchmark%20%28PasswordEval%29%0Athat%20measures%20whether%20language%20models%20can%20correctly%20determine%20when%20a%20user%0Arequest%20is%20authorized%20%28i.e.%2C%20with%20a%20correct%20password%29.%20We%20find%20that%20current%0Aopen-%20and%20closed-source%20models%20struggle%20with%20this%20seemingly%20simple%20task%2C%20and%0Athat%2C%20perhaps%20surprisingly%2C%20reasoning%20capabilities%20do%20not%20generally%20improve%0Aperformance.%20In%20fact%2C%20we%20find%20that%20reasoning%20traces%20frequently%20leak%0Aconfidential%20information%2C%20which%20calls%20into%20question%20whether%20reasoning%20traces%0Ashould%20be%20exposed%20to%20users%20in%20such%20applications.%20We%20also%20scale%20the%20difficulty%0Aof%20our%20evaluation%20along%20multiple%20axes%3A%20%28i%29%20by%20adding%20adversarial%20user%20pressure%0Athrough%20various%20jailbreaking%20strategies%2C%20and%20%28ii%29%20through%20longer%20multi-turn%0Aconversations%20where%20password%20verification%20is%20more%20challenging.%20Overall%2C%20our%0Aresults%20suggest%20that%20current%20frontier%20models%20are%20not%20well-suited%20to%20handling%0Aconfidential%20information%2C%20and%20that%20reasoning%20capabilities%20may%20need%20to%20be%0Atrained%20in%20a%20different%20manner%20to%20make%20them%20safer%20for%20release%20in%20high-stakes%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19980v1&entry.124074799=Read"},
{"title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "author": "Shuo Shao and Yiming Li and Yu He and Hongwei Yao and Wenyuan Yang and Dacheng Tao and Zhan Qin", "abstract": "  The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench.\n", "link": "http://arxiv.org/abs/2508.19843v1", "date": "2025-08-27", "relevancy": 2.5119, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5052}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20Large%20Language%20Model%20Copyright%20Auditing%20via%20Fingerprinting&body=Title%3A%20SoK%3A%20Large%20Language%20Model%20Copyright%20Auditing%20via%20Fingerprinting%0AAuthor%3A%20Shuo%20Shao%20and%20Yiming%20Li%20and%20Yu%20He%20and%20Hongwei%20Yao%20and%20Wenyuan%20Yang%20and%20Dacheng%20Tao%20and%20Zhan%20Qin%0AAbstract%3A%20%20%20The%20broad%20capabilities%20and%20substantial%20resources%20required%20to%20train%20Large%0ALanguage%20Models%20%28LLMs%29%20make%20them%20valuable%20intellectual%20property%2C%20yet%20they%0Aremain%20vulnerable%20to%20copyright%20infringement%2C%20such%20as%20unauthorized%20use%20and%20model%0Atheft.%20LLM%20fingerprinting%2C%20a%20non-intrusive%20technique%20that%20extracts%20and%20compares%0Athe%20distinctive%20features%20from%20LLMs%20to%20identify%20infringements%2C%20offers%20a%0Apromising%20solution%20to%20copyright%20auditing.%20However%2C%20its%20reliability%20remains%0Auncertain%20due%20to%20the%20prevalence%20of%20diverse%20model%20modifications%20and%20the%20lack%20of%0Astandardized%20evaluation.%20In%20this%20SoK%2C%20we%20present%20the%20first%20comprehensive%20study%0Aof%20LLM%20fingerprinting.%20We%20introduce%20a%20unified%20framework%20and%20formal%20taxonomy%0Athat%20categorizes%20existing%20methods%20into%20white-box%20and%20black-box%20approaches%2C%0Aproviding%20a%20structured%20overview%20of%20the%20state%20of%20the%20art.%20We%20further%20propose%0ALeaFBench%2C%20the%20first%20systematic%20benchmark%20for%20evaluating%20LLM%20fingerprinting%0Aunder%20realistic%20deployment%20scenarios.%20Built%20upon%20mainstream%20foundation%20models%0Aand%20comprising%20149%20distinct%20model%20instances%2C%20LeaFBench%20integrates%2013%0Arepresentative%20post-development%20techniques%2C%20spanning%20both%20parameter-altering%0Amethods%20%28e.g.%2C%20fine-tuning%2C%20quantization%29%20and%20parameter-independent%20mechanisms%0A%28e.g.%2C%20system%20prompts%2C%20RAG%29.%20Extensive%20experiments%20on%20LeaFBench%20reveal%20the%0Astrengths%20and%20weaknesses%20of%20existing%20methods%2C%20thereby%20outlining%20future%20research%0Adirections%20and%20critical%20open%20problems%20in%20this%20emerging%20field.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/shaoshuo-ss/LeaFBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520Large%2520Language%2520Model%2520Copyright%2520Auditing%2520via%2520Fingerprinting%26entry.906535625%3DShuo%2520Shao%2520and%2520Yiming%2520Li%2520and%2520Yu%2520He%2520and%2520Hongwei%2520Yao%2520and%2520Wenyuan%2520Yang%2520and%2520Dacheng%2520Tao%2520and%2520Zhan%2520Qin%26entry.1292438233%3D%2520%2520The%2520broad%2520capabilities%2520and%2520substantial%2520resources%2520required%2520to%2520train%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520make%2520them%2520valuable%2520intellectual%2520property%252C%2520yet%2520they%250Aremain%2520vulnerable%2520to%2520copyright%2520infringement%252C%2520such%2520as%2520unauthorized%2520use%2520and%2520model%250Atheft.%2520LLM%2520fingerprinting%252C%2520a%2520non-intrusive%2520technique%2520that%2520extracts%2520and%2520compares%250Athe%2520distinctive%2520features%2520from%2520LLMs%2520to%2520identify%2520infringements%252C%2520offers%2520a%250Apromising%2520solution%2520to%2520copyright%2520auditing.%2520However%252C%2520its%2520reliability%2520remains%250Auncertain%2520due%2520to%2520the%2520prevalence%2520of%2520diverse%2520model%2520modifications%2520and%2520the%2520lack%2520of%250Astandardized%2520evaluation.%2520In%2520this%2520SoK%252C%2520we%2520present%2520the%2520first%2520comprehensive%2520study%250Aof%2520LLM%2520fingerprinting.%2520We%2520introduce%2520a%2520unified%2520framework%2520and%2520formal%2520taxonomy%250Athat%2520categorizes%2520existing%2520methods%2520into%2520white-box%2520and%2520black-box%2520approaches%252C%250Aproviding%2520a%2520structured%2520overview%2520of%2520the%2520state%2520of%2520the%2520art.%2520We%2520further%2520propose%250ALeaFBench%252C%2520the%2520first%2520systematic%2520benchmark%2520for%2520evaluating%2520LLM%2520fingerprinting%250Aunder%2520realistic%2520deployment%2520scenarios.%2520Built%2520upon%2520mainstream%2520foundation%2520models%250Aand%2520comprising%2520149%2520distinct%2520model%2520instances%252C%2520LeaFBench%2520integrates%252013%250Arepresentative%2520post-development%2520techniques%252C%2520spanning%2520both%2520parameter-altering%250Amethods%2520%2528e.g.%252C%2520fine-tuning%252C%2520quantization%2529%2520and%2520parameter-independent%2520mechanisms%250A%2528e.g.%252C%2520system%2520prompts%252C%2520RAG%2529.%2520Extensive%2520experiments%2520on%2520LeaFBench%2520reveal%2520the%250Astrengths%2520and%2520weaknesses%2520of%2520existing%2520methods%252C%2520thereby%2520outlining%2520future%2520research%250Adirections%2520and%2520critical%2520open%2520problems%2520in%2520this%2520emerging%2520field.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/shaoshuo-ss/LeaFBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20Large%20Language%20Model%20Copyright%20Auditing%20via%20Fingerprinting&entry.906535625=Shuo%20Shao%20and%20Yiming%20Li%20and%20Yu%20He%20and%20Hongwei%20Yao%20and%20Wenyuan%20Yang%20and%20Dacheng%20Tao%20and%20Zhan%20Qin&entry.1292438233=%20%20The%20broad%20capabilities%20and%20substantial%20resources%20required%20to%20train%20Large%0ALanguage%20Models%20%28LLMs%29%20make%20them%20valuable%20intellectual%20property%2C%20yet%20they%0Aremain%20vulnerable%20to%20copyright%20infringement%2C%20such%20as%20unauthorized%20use%20and%20model%0Atheft.%20LLM%20fingerprinting%2C%20a%20non-intrusive%20technique%20that%20extracts%20and%20compares%0Athe%20distinctive%20features%20from%20LLMs%20to%20identify%20infringements%2C%20offers%20a%0Apromising%20solution%20to%20copyright%20auditing.%20However%2C%20its%20reliability%20remains%0Auncertain%20due%20to%20the%20prevalence%20of%20diverse%20model%20modifications%20and%20the%20lack%20of%0Astandardized%20evaluation.%20In%20this%20SoK%2C%20we%20present%20the%20first%20comprehensive%20study%0Aof%20LLM%20fingerprinting.%20We%20introduce%20a%20unified%20framework%20and%20formal%20taxonomy%0Athat%20categorizes%20existing%20methods%20into%20white-box%20and%20black-box%20approaches%2C%0Aproviding%20a%20structured%20overview%20of%20the%20state%20of%20the%20art.%20We%20further%20propose%0ALeaFBench%2C%20the%20first%20systematic%20benchmark%20for%20evaluating%20LLM%20fingerprinting%0Aunder%20realistic%20deployment%20scenarios.%20Built%20upon%20mainstream%20foundation%20models%0Aand%20comprising%20149%20distinct%20model%20instances%2C%20LeaFBench%20integrates%2013%0Arepresentative%20post-development%20techniques%2C%20spanning%20both%20parameter-altering%0Amethods%20%28e.g.%2C%20fine-tuning%2C%20quantization%29%20and%20parameter-independent%20mechanisms%0A%28e.g.%2C%20system%20prompts%2C%20RAG%29.%20Extensive%20experiments%20on%20LeaFBench%20reveal%20the%0Astrengths%20and%20weaknesses%20of%20existing%20methods%2C%20thereby%20outlining%20future%20research%0Adirections%20and%20critical%20open%20problems%20in%20this%20emerging%20field.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/shaoshuo-ss/LeaFBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19843v1&entry.124074799=Read"},
{"title": "POEv2: a flexible and robust framework for generic line segment\n  detection and wireframe line segment detection", "author": "Chenguang Liu and Chisheng Wang and Yuhua Cai and Chuanhua Zhu and Qingquan Li", "abstract": "  Line segment detection in images has been studied for several decades.\nExisting line segment detectors can be roughly divided into two categories:\ngeneric line segment detectors and wireframe line segment detectors. Generic\nline segment detectors aim to detect all meaningful line segments in images and\ntraditional approaches usually fall into this category. Recent deep learning\nbased approaches are mostly wireframe line segment detectors. They detect only\nline segments that are geometrically meaningful and have large spatial support.\nDue to the difference in the aim of design, the performance of generic line\nsegment detectors for the task of wireframe line segment detection won't be\nsatisfactory, and vice versa. In this work, we propose a robust framework that\ncan be used for both generic line segment detection and wireframe line segment\ndetection. The proposed method is an improved version of the Pixel Orientation\nEstimation (POE) method. It is thus named as POEv2. POEv2 detects line segments\nfrom edge strength maps, and can be combined with any edge detector. We show in\nour experiments that by combining the proposed POEv2 with an efficient edge\ndetector, it achieves state-of-the-art performance on three publicly available\ndatasets.\n", "link": "http://arxiv.org/abs/2508.19742v1", "date": "2025-08-27", "relevancy": 2.4753, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5032}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POEv2%3A%20a%20flexible%20and%20robust%20framework%20for%20generic%20line%20segment%0A%20%20detection%20and%20wireframe%20line%20segment%20detection&body=Title%3A%20POEv2%3A%20a%20flexible%20and%20robust%20framework%20for%20generic%20line%20segment%0A%20%20detection%20and%20wireframe%20line%20segment%20detection%0AAuthor%3A%20Chenguang%20Liu%20and%20Chisheng%20Wang%20and%20Yuhua%20Cai%20and%20Chuanhua%20Zhu%20and%20Qingquan%20Li%0AAbstract%3A%20%20%20Line%20segment%20detection%20in%20images%20has%20been%20studied%20for%20several%20decades.%0AExisting%20line%20segment%20detectors%20can%20be%20roughly%20divided%20into%20two%20categories%3A%0Ageneric%20line%20segment%20detectors%20and%20wireframe%20line%20segment%20detectors.%20Generic%0Aline%20segment%20detectors%20aim%20to%20detect%20all%20meaningful%20line%20segments%20in%20images%20and%0Atraditional%20approaches%20usually%20fall%20into%20this%20category.%20Recent%20deep%20learning%0Abased%20approaches%20are%20mostly%20wireframe%20line%20segment%20detectors.%20They%20detect%20only%0Aline%20segments%20that%20are%20geometrically%20meaningful%20and%20have%20large%20spatial%20support.%0ADue%20to%20the%20difference%20in%20the%20aim%20of%20design%2C%20the%20performance%20of%20generic%20line%0Asegment%20detectors%20for%20the%20task%20of%20wireframe%20line%20segment%20detection%20won%27t%20be%0Asatisfactory%2C%20and%20vice%20versa.%20In%20this%20work%2C%20we%20propose%20a%20robust%20framework%20that%0Acan%20be%20used%20for%20both%20generic%20line%20segment%20detection%20and%20wireframe%20line%20segment%0Adetection.%20The%20proposed%20method%20is%20an%20improved%20version%20of%20the%20Pixel%20Orientation%0AEstimation%20%28POE%29%20method.%20It%20is%20thus%20named%20as%20POEv2.%20POEv2%20detects%20line%20segments%0Afrom%20edge%20strength%20maps%2C%20and%20can%20be%20combined%20with%20any%20edge%20detector.%20We%20show%20in%0Aour%20experiments%20that%20by%20combining%20the%20proposed%20POEv2%20with%20an%20efficient%20edge%0Adetector%2C%20it%20achieves%20state-of-the-art%20performance%20on%20three%20publicly%20available%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOEv2%253A%2520a%2520flexible%2520and%2520robust%2520framework%2520for%2520generic%2520line%2520segment%250A%2520%2520detection%2520and%2520wireframe%2520line%2520segment%2520detection%26entry.906535625%3DChenguang%2520Liu%2520and%2520Chisheng%2520Wang%2520and%2520Yuhua%2520Cai%2520and%2520Chuanhua%2520Zhu%2520and%2520Qingquan%2520Li%26entry.1292438233%3D%2520%2520Line%2520segment%2520detection%2520in%2520images%2520has%2520been%2520studied%2520for%2520several%2520decades.%250AExisting%2520line%2520segment%2520detectors%2520can%2520be%2520roughly%2520divided%2520into%2520two%2520categories%253A%250Ageneric%2520line%2520segment%2520detectors%2520and%2520wireframe%2520line%2520segment%2520detectors.%2520Generic%250Aline%2520segment%2520detectors%2520aim%2520to%2520detect%2520all%2520meaningful%2520line%2520segments%2520in%2520images%2520and%250Atraditional%2520approaches%2520usually%2520fall%2520into%2520this%2520category.%2520Recent%2520deep%2520learning%250Abased%2520approaches%2520are%2520mostly%2520wireframe%2520line%2520segment%2520detectors.%2520They%2520detect%2520only%250Aline%2520segments%2520that%2520are%2520geometrically%2520meaningful%2520and%2520have%2520large%2520spatial%2520support.%250ADue%2520to%2520the%2520difference%2520in%2520the%2520aim%2520of%2520design%252C%2520the%2520performance%2520of%2520generic%2520line%250Asegment%2520detectors%2520for%2520the%2520task%2520of%2520wireframe%2520line%2520segment%2520detection%2520won%2527t%2520be%250Asatisfactory%252C%2520and%2520vice%2520versa.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520robust%2520framework%2520that%250Acan%2520be%2520used%2520for%2520both%2520generic%2520line%2520segment%2520detection%2520and%2520wireframe%2520line%2520segment%250Adetection.%2520The%2520proposed%2520method%2520is%2520an%2520improved%2520version%2520of%2520the%2520Pixel%2520Orientation%250AEstimation%2520%2528POE%2529%2520method.%2520It%2520is%2520thus%2520named%2520as%2520POEv2.%2520POEv2%2520detects%2520line%2520segments%250Afrom%2520edge%2520strength%2520maps%252C%2520and%2520can%2520be%2520combined%2520with%2520any%2520edge%2520detector.%2520We%2520show%2520in%250Aour%2520experiments%2520that%2520by%2520combining%2520the%2520proposed%2520POEv2%2520with%2520an%2520efficient%2520edge%250Adetector%252C%2520it%2520achieves%2520state-of-the-art%2520performance%2520on%2520three%2520publicly%2520available%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POEv2%3A%20a%20flexible%20and%20robust%20framework%20for%20generic%20line%20segment%0A%20%20detection%20and%20wireframe%20line%20segment%20detection&entry.906535625=Chenguang%20Liu%20and%20Chisheng%20Wang%20and%20Yuhua%20Cai%20and%20Chuanhua%20Zhu%20and%20Qingquan%20Li&entry.1292438233=%20%20Line%20segment%20detection%20in%20images%20has%20been%20studied%20for%20several%20decades.%0AExisting%20line%20segment%20detectors%20can%20be%20roughly%20divided%20into%20two%20categories%3A%0Ageneric%20line%20segment%20detectors%20and%20wireframe%20line%20segment%20detectors.%20Generic%0Aline%20segment%20detectors%20aim%20to%20detect%20all%20meaningful%20line%20segments%20in%20images%20and%0Atraditional%20approaches%20usually%20fall%20into%20this%20category.%20Recent%20deep%20learning%0Abased%20approaches%20are%20mostly%20wireframe%20line%20segment%20detectors.%20They%20detect%20only%0Aline%20segments%20that%20are%20geometrically%20meaningful%20and%20have%20large%20spatial%20support.%0ADue%20to%20the%20difference%20in%20the%20aim%20of%20design%2C%20the%20performance%20of%20generic%20line%0Asegment%20detectors%20for%20the%20task%20of%20wireframe%20line%20segment%20detection%20won%27t%20be%0Asatisfactory%2C%20and%20vice%20versa.%20In%20this%20work%2C%20we%20propose%20a%20robust%20framework%20that%0Acan%20be%20used%20for%20both%20generic%20line%20segment%20detection%20and%20wireframe%20line%20segment%0Adetection.%20The%20proposed%20method%20is%20an%20improved%20version%20of%20the%20Pixel%20Orientation%0AEstimation%20%28POE%29%20method.%20It%20is%20thus%20named%20as%20POEv2.%20POEv2%20detects%20line%20segments%0Afrom%20edge%20strength%20maps%2C%20and%20can%20be%20combined%20with%20any%20edge%20detector.%20We%20show%20in%0Aour%20experiments%20that%20by%20combining%20the%20proposed%20POEv2%20with%20an%20efficient%20edge%0Adetector%2C%20it%20achieves%20state-of-the-art%20performance%20on%20three%20publicly%20available%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19742v1&entry.124074799=Read"},
{"title": "Personalized MR-Informed Diffusion Models for 3D PET Image\n  Reconstruction", "author": "George Webber and Alexander Hammers and Andrew P. King and Andrew J. Reader", "abstract": "  Recent work has shown improved lesion detectability and flexibility to\nreconstruction hyperparameters (e.g. scanner geometry or dose level) when PET\nimages are reconstructed by leveraging pre-trained diffusion models. Such\nmethods train a diffusion model (without sinogram data) on high-quality, but\nstill noisy, PET images. In this work, we propose a simple method for\ngenerating subject-specific PET images from a dataset of multi-subject PET-MR\nscans, synthesizing \"pseudo-PET\" images by transforming between different\npatients' anatomy using image registration. The images we synthesize retain\ninformation from the subject's MR scan, leading to higher resolution and the\nretention of anatomical features compared to the original set of PET images.\nWith simulated and real [$^{18}$F]FDG datasets, we show that pre-training a\npersonalized diffusion model with subject-specific \"pseudo-PET\" images improves\nreconstruction accuracy with low-count data. In particular, the method shows\npromise in combining information from a guidance MR scan without overly\nimposing anatomical features, demonstrating an improved trade-off between\nreconstructing PET-unique image features versus features present in both PET\nand MR. We believe this approach for generating and utilizing synthetic data\nhas further applications to medical imaging tasks, particularly because\npatient-specific PET images can be generated without resorting to generative\ndeep learning or large training datasets.\n", "link": "http://arxiv.org/abs/2506.03804v2", "date": "2025-08-27", "relevancy": 2.4613, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6219}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6219}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20MR-Informed%20Diffusion%20Models%20for%203D%20PET%20Image%0A%20%20Reconstruction&body=Title%3A%20Personalized%20MR-Informed%20Diffusion%20Models%20for%203D%20PET%20Image%0A%20%20Reconstruction%0AAuthor%3A%20George%20Webber%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20improved%20lesion%20detectability%20and%20flexibility%20to%0Areconstruction%20hyperparameters%20%28e.g.%20scanner%20geometry%20or%20dose%20level%29%20when%20PET%0Aimages%20are%20reconstructed%20by%20leveraging%20pre-trained%20diffusion%20models.%20Such%0Amethods%20train%20a%20diffusion%20model%20%28without%20sinogram%20data%29%20on%20high-quality%2C%20but%0Astill%20noisy%2C%20PET%20images.%20In%20this%20work%2C%20we%20propose%20a%20simple%20method%20for%0Agenerating%20subject-specific%20PET%20images%20from%20a%20dataset%20of%20multi-subject%20PET-MR%0Ascans%2C%20synthesizing%20%22pseudo-PET%22%20images%20by%20transforming%20between%20different%0Apatients%27%20anatomy%20using%20image%20registration.%20The%20images%20we%20synthesize%20retain%0Ainformation%20from%20the%20subject%27s%20MR%20scan%2C%20leading%20to%20higher%20resolution%20and%20the%0Aretention%20of%20anatomical%20features%20compared%20to%20the%20original%20set%20of%20PET%20images.%0AWith%20simulated%20and%20real%20%5B%24%5E%7B18%7D%24F%5DFDG%20datasets%2C%20we%20show%20that%20pre-training%20a%0Apersonalized%20diffusion%20model%20with%20subject-specific%20%22pseudo-PET%22%20images%20improves%0Areconstruction%20accuracy%20with%20low-count%20data.%20In%20particular%2C%20the%20method%20shows%0Apromise%20in%20combining%20information%20from%20a%20guidance%20MR%20scan%20without%20overly%0Aimposing%20anatomical%20features%2C%20demonstrating%20an%20improved%20trade-off%20between%0Areconstructing%20PET-unique%20image%20features%20versus%20features%20present%20in%20both%20PET%0Aand%20MR.%20We%20believe%20this%20approach%20for%20generating%20and%20utilizing%20synthetic%20data%0Ahas%20further%20applications%20to%20medical%20imaging%20tasks%2C%20particularly%20because%0Apatient-specific%20PET%20images%20can%20be%20generated%20without%20resorting%20to%20generative%0Adeep%20learning%20or%20large%20training%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03804v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520MR-Informed%2520Diffusion%2520Models%2520for%25203D%2520PET%2520Image%250A%2520%2520Reconstruction%26entry.906535625%3DGeorge%2520Webber%2520and%2520Alexander%2520Hammers%2520and%2520Andrew%2520P.%2520King%2520and%2520Andrew%2520J.%2520Reader%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520improved%2520lesion%2520detectability%2520and%2520flexibility%2520to%250Areconstruction%2520hyperparameters%2520%2528e.g.%2520scanner%2520geometry%2520or%2520dose%2520level%2529%2520when%2520PET%250Aimages%2520are%2520reconstructed%2520by%2520leveraging%2520pre-trained%2520diffusion%2520models.%2520Such%250Amethods%2520train%2520a%2520diffusion%2520model%2520%2528without%2520sinogram%2520data%2529%2520on%2520high-quality%252C%2520but%250Astill%2520noisy%252C%2520PET%2520images.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520simple%2520method%2520for%250Agenerating%2520subject-specific%2520PET%2520images%2520from%2520a%2520dataset%2520of%2520multi-subject%2520PET-MR%250Ascans%252C%2520synthesizing%2520%2522pseudo-PET%2522%2520images%2520by%2520transforming%2520between%2520different%250Apatients%2527%2520anatomy%2520using%2520image%2520registration.%2520The%2520images%2520we%2520synthesize%2520retain%250Ainformation%2520from%2520the%2520subject%2527s%2520MR%2520scan%252C%2520leading%2520to%2520higher%2520resolution%2520and%2520the%250Aretention%2520of%2520anatomical%2520features%2520compared%2520to%2520the%2520original%2520set%2520of%2520PET%2520images.%250AWith%2520simulated%2520and%2520real%2520%255B%2524%255E%257B18%257D%2524F%255DFDG%2520datasets%252C%2520we%2520show%2520that%2520pre-training%2520a%250Apersonalized%2520diffusion%2520model%2520with%2520subject-specific%2520%2522pseudo-PET%2522%2520images%2520improves%250Areconstruction%2520accuracy%2520with%2520low-count%2520data.%2520In%2520particular%252C%2520the%2520method%2520shows%250Apromise%2520in%2520combining%2520information%2520from%2520a%2520guidance%2520MR%2520scan%2520without%2520overly%250Aimposing%2520anatomical%2520features%252C%2520demonstrating%2520an%2520improved%2520trade-off%2520between%250Areconstructing%2520PET-unique%2520image%2520features%2520versus%2520features%2520present%2520in%2520both%2520PET%250Aand%2520MR.%2520We%2520believe%2520this%2520approach%2520for%2520generating%2520and%2520utilizing%2520synthetic%2520data%250Ahas%2520further%2520applications%2520to%2520medical%2520imaging%2520tasks%252C%2520particularly%2520because%250Apatient-specific%2520PET%2520images%2520can%2520be%2520generated%2520without%2520resorting%2520to%2520generative%250Adeep%2520learning%2520or%2520large%2520training%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03804v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20MR-Informed%20Diffusion%20Models%20for%203D%20PET%20Image%0A%20%20Reconstruction&entry.906535625=George%20Webber%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader&entry.1292438233=%20%20Recent%20work%20has%20shown%20improved%20lesion%20detectability%20and%20flexibility%20to%0Areconstruction%20hyperparameters%20%28e.g.%20scanner%20geometry%20or%20dose%20level%29%20when%20PET%0Aimages%20are%20reconstructed%20by%20leveraging%20pre-trained%20diffusion%20models.%20Such%0Amethods%20train%20a%20diffusion%20model%20%28without%20sinogram%20data%29%20on%20high-quality%2C%20but%0Astill%20noisy%2C%20PET%20images.%20In%20this%20work%2C%20we%20propose%20a%20simple%20method%20for%0Agenerating%20subject-specific%20PET%20images%20from%20a%20dataset%20of%20multi-subject%20PET-MR%0Ascans%2C%20synthesizing%20%22pseudo-PET%22%20images%20by%20transforming%20between%20different%0Apatients%27%20anatomy%20using%20image%20registration.%20The%20images%20we%20synthesize%20retain%0Ainformation%20from%20the%20subject%27s%20MR%20scan%2C%20leading%20to%20higher%20resolution%20and%20the%0Aretention%20of%20anatomical%20features%20compared%20to%20the%20original%20set%20of%20PET%20images.%0AWith%20simulated%20and%20real%20%5B%24%5E%7B18%7D%24F%5DFDG%20datasets%2C%20we%20show%20that%20pre-training%20a%0Apersonalized%20diffusion%20model%20with%20subject-specific%20%22pseudo-PET%22%20images%20improves%0Areconstruction%20accuracy%20with%20low-count%20data.%20In%20particular%2C%20the%20method%20shows%0Apromise%20in%20combining%20information%20from%20a%20guidance%20MR%20scan%20without%20overly%0Aimposing%20anatomical%20features%2C%20demonstrating%20an%20improved%20trade-off%20between%0Areconstructing%20PET-unique%20image%20features%20versus%20features%20present%20in%20both%20PET%0Aand%20MR.%20We%20believe%20this%20approach%20for%20generating%20and%20utilizing%20synthetic%20data%0Ahas%20further%20applications%20to%20medical%20imaging%20tasks%2C%20particularly%20because%0Apatient-specific%20PET%20images%20can%20be%20generated%20without%20resorting%20to%20generative%0Adeep%20learning%20or%20large%20training%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03804v2&entry.124074799=Read"},
{"title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without\n  Human Annotations", "author": "Peng-Hao Hsu and Ke Zhang and Fu-En Wang and Tao Tu and Ming-Feng Li and Yu-Lun Liu and Albert Y. C. Chen and Min Sun and Cheng-Hao Kuo", "abstract": "  Open-vocabulary (OV) 3D object detection is an emerging field, yet its\nexploration through image-based methods remains limited compared to 3D point\ncloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view\nindoor 3D object detector trained without human annotations. In particular,\nOpenM3D is a single-stage detector adapting the 2D-induced voxel features from\nthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic\n3D localization loss requiring high-quality 3D pseudo boxes and a\nvoxel-semantic alignment loss requiring diverse pre-trained CLIP features. We\nfollow the training setting of OV-3DET where posed RGB-D images are given but\nno human annotations of 3D boxes or classes are available. We propose a 3D\nPseudo Box Generation method using a graph embedding technique that combines 2D\nsegments into coherent 3D structures. Our pseudo-boxes achieve higher precision\nand recall than other methods, including the method proposed in OV-3DET. We\nfurther sample diverse CLIP features from 2D segments associated with each\ncoherent 3D structure to align with the corresponding voxel feature. The key to\ntraining a highly accurate single-stage detector requires both losses to be\nlearned toward high-quality targets. At inference, OpenM3D, a highly efficient\ndetector, requires only multi-view images for input and demonstrates superior\naccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor\nbenchmarks compared to existing methods. We outperform a strong two-stage\nmethod that leverages our class-agnostic detector with a ViT CLIP-based OV\nclassifier and a baseline incorporating multi-view depth estimator on both\naccuracy and speed.\n", "link": "http://arxiv.org/abs/2508.20063v1", "date": "2025-08-27", "relevancy": 2.4536, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6308}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6126}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenM3D%3A%20Open%20Vocabulary%20Multi-view%20Indoor%203D%20Object%20Detection%20without%0A%20%20Human%20Annotations&body=Title%3A%20OpenM3D%3A%20Open%20Vocabulary%20Multi-view%20Indoor%203D%20Object%20Detection%20without%0A%20%20Human%20Annotations%0AAuthor%3A%20Peng-Hao%20Hsu%20and%20Ke%20Zhang%20and%20Fu-En%20Wang%20and%20Tao%20Tu%20and%20Ming-Feng%20Li%20and%20Yu-Lun%20Liu%20and%20Albert%20Y.%20C.%20Chen%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo%0AAbstract%3A%20%20%20Open-vocabulary%20%28OV%29%203D%20object%20detection%20is%20an%20emerging%20field%2C%20yet%20its%0Aexploration%20through%20image-based%20methods%20remains%20limited%20compared%20to%203D%20point%0Acloud-based%20methods.%20We%20introduce%20OpenM3D%2C%20a%20novel%20open-vocabulary%20multi-view%0Aindoor%203D%20object%20detector%20trained%20without%20human%20annotations.%20In%20particular%2C%0AOpenM3D%20is%20a%20single-stage%20detector%20adapting%20the%202D-induced%20voxel%20features%20from%0Athe%20ImGeoNet%20model.%20To%20support%20OV%2C%20it%20is%20jointly%20trained%20with%20a%20class-agnostic%0A3D%20localization%20loss%20requiring%20high-quality%203D%20pseudo%20boxes%20and%20a%0Avoxel-semantic%20alignment%20loss%20requiring%20diverse%20pre-trained%20CLIP%20features.%20We%0Afollow%20the%20training%20setting%20of%20OV-3DET%20where%20posed%20RGB-D%20images%20are%20given%20but%0Ano%20human%20annotations%20of%203D%20boxes%20or%20classes%20are%20available.%20We%20propose%20a%203D%0APseudo%20Box%20Generation%20method%20using%20a%20graph%20embedding%20technique%20that%20combines%202D%0Asegments%20into%20coherent%203D%20structures.%20Our%20pseudo-boxes%20achieve%20higher%20precision%0Aand%20recall%20than%20other%20methods%2C%20including%20the%20method%20proposed%20in%20OV-3DET.%20We%0Afurther%20sample%20diverse%20CLIP%20features%20from%202D%20segments%20associated%20with%20each%0Acoherent%203D%20structure%20to%20align%20with%20the%20corresponding%20voxel%20feature.%20The%20key%20to%0Atraining%20a%20highly%20accurate%20single-stage%20detector%20requires%20both%20losses%20to%20be%0Alearned%20toward%20high-quality%20targets.%20At%20inference%2C%20OpenM3D%2C%20a%20highly%20efficient%0Adetector%2C%20requires%20only%20multi-view%20images%20for%20input%20and%20demonstrates%20superior%0Aaccuracy%20and%20speed%20%280.3%20sec.%20per%20scene%29%20on%20ScanNet200%20and%20ARKitScenes%20indoor%0Abenchmarks%20compared%20to%20existing%20methods.%20We%20outperform%20a%20strong%20two-stage%0Amethod%20that%20leverages%20our%20class-agnostic%20detector%20with%20a%20ViT%20CLIP-based%20OV%0Aclassifier%20and%20a%20baseline%20incorporating%20multi-view%20depth%20estimator%20on%20both%0Aaccuracy%20and%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenM3D%253A%2520Open%2520Vocabulary%2520Multi-view%2520Indoor%25203D%2520Object%2520Detection%2520without%250A%2520%2520Human%2520Annotations%26entry.906535625%3DPeng-Hao%2520Hsu%2520and%2520Ke%2520Zhang%2520and%2520Fu-En%2520Wang%2520and%2520Tao%2520Tu%2520and%2520Ming-Feng%2520Li%2520and%2520Yu-Lun%2520Liu%2520and%2520Albert%2520Y.%2520C.%2520Chen%2520and%2520Min%2520Sun%2520and%2520Cheng-Hao%2520Kuo%26entry.1292438233%3D%2520%2520Open-vocabulary%2520%2528OV%2529%25203D%2520object%2520detection%2520is%2520an%2520emerging%2520field%252C%2520yet%2520its%250Aexploration%2520through%2520image-based%2520methods%2520remains%2520limited%2520compared%2520to%25203D%2520point%250Acloud-based%2520methods.%2520We%2520introduce%2520OpenM3D%252C%2520a%2520novel%2520open-vocabulary%2520multi-view%250Aindoor%25203D%2520object%2520detector%2520trained%2520without%2520human%2520annotations.%2520In%2520particular%252C%250AOpenM3D%2520is%2520a%2520single-stage%2520detector%2520adapting%2520the%25202D-induced%2520voxel%2520features%2520from%250Athe%2520ImGeoNet%2520model.%2520To%2520support%2520OV%252C%2520it%2520is%2520jointly%2520trained%2520with%2520a%2520class-agnostic%250A3D%2520localization%2520loss%2520requiring%2520high-quality%25203D%2520pseudo%2520boxes%2520and%2520a%250Avoxel-semantic%2520alignment%2520loss%2520requiring%2520diverse%2520pre-trained%2520CLIP%2520features.%2520We%250Afollow%2520the%2520training%2520setting%2520of%2520OV-3DET%2520where%2520posed%2520RGB-D%2520images%2520are%2520given%2520but%250Ano%2520human%2520annotations%2520of%25203D%2520boxes%2520or%2520classes%2520are%2520available.%2520We%2520propose%2520a%25203D%250APseudo%2520Box%2520Generation%2520method%2520using%2520a%2520graph%2520embedding%2520technique%2520that%2520combines%25202D%250Asegments%2520into%2520coherent%25203D%2520structures.%2520Our%2520pseudo-boxes%2520achieve%2520higher%2520precision%250Aand%2520recall%2520than%2520other%2520methods%252C%2520including%2520the%2520method%2520proposed%2520in%2520OV-3DET.%2520We%250Afurther%2520sample%2520diverse%2520CLIP%2520features%2520from%25202D%2520segments%2520associated%2520with%2520each%250Acoherent%25203D%2520structure%2520to%2520align%2520with%2520the%2520corresponding%2520voxel%2520feature.%2520The%2520key%2520to%250Atraining%2520a%2520highly%2520accurate%2520single-stage%2520detector%2520requires%2520both%2520losses%2520to%2520be%250Alearned%2520toward%2520high-quality%2520targets.%2520At%2520inference%252C%2520OpenM3D%252C%2520a%2520highly%2520efficient%250Adetector%252C%2520requires%2520only%2520multi-view%2520images%2520for%2520input%2520and%2520demonstrates%2520superior%250Aaccuracy%2520and%2520speed%2520%25280.3%2520sec.%2520per%2520scene%2529%2520on%2520ScanNet200%2520and%2520ARKitScenes%2520indoor%250Abenchmarks%2520compared%2520to%2520existing%2520methods.%2520We%2520outperform%2520a%2520strong%2520two-stage%250Amethod%2520that%2520leverages%2520our%2520class-agnostic%2520detector%2520with%2520a%2520ViT%2520CLIP-based%2520OV%250Aclassifier%2520and%2520a%2520baseline%2520incorporating%2520multi-view%2520depth%2520estimator%2520on%2520both%250Aaccuracy%2520and%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenM3D%3A%20Open%20Vocabulary%20Multi-view%20Indoor%203D%20Object%20Detection%20without%0A%20%20Human%20Annotations&entry.906535625=Peng-Hao%20Hsu%20and%20Ke%20Zhang%20and%20Fu-En%20Wang%20and%20Tao%20Tu%20and%20Ming-Feng%20Li%20and%20Yu-Lun%20Liu%20and%20Albert%20Y.%20C.%20Chen%20and%20Min%20Sun%20and%20Cheng-Hao%20Kuo&entry.1292438233=%20%20Open-vocabulary%20%28OV%29%203D%20object%20detection%20is%20an%20emerging%20field%2C%20yet%20its%0Aexploration%20through%20image-based%20methods%20remains%20limited%20compared%20to%203D%20point%0Acloud-based%20methods.%20We%20introduce%20OpenM3D%2C%20a%20novel%20open-vocabulary%20multi-view%0Aindoor%203D%20object%20detector%20trained%20without%20human%20annotations.%20In%20particular%2C%0AOpenM3D%20is%20a%20single-stage%20detector%20adapting%20the%202D-induced%20voxel%20features%20from%0Athe%20ImGeoNet%20model.%20To%20support%20OV%2C%20it%20is%20jointly%20trained%20with%20a%20class-agnostic%0A3D%20localization%20loss%20requiring%20high-quality%203D%20pseudo%20boxes%20and%20a%0Avoxel-semantic%20alignment%20loss%20requiring%20diverse%20pre-trained%20CLIP%20features.%20We%0Afollow%20the%20training%20setting%20of%20OV-3DET%20where%20posed%20RGB-D%20images%20are%20given%20but%0Ano%20human%20annotations%20of%203D%20boxes%20or%20classes%20are%20available.%20We%20propose%20a%203D%0APseudo%20Box%20Generation%20method%20using%20a%20graph%20embedding%20technique%20that%20combines%202D%0Asegments%20into%20coherent%203D%20structures.%20Our%20pseudo-boxes%20achieve%20higher%20precision%0Aand%20recall%20than%20other%20methods%2C%20including%20the%20method%20proposed%20in%20OV-3DET.%20We%0Afurther%20sample%20diverse%20CLIP%20features%20from%202D%20segments%20associated%20with%20each%0Acoherent%203D%20structure%20to%20align%20with%20the%20corresponding%20voxel%20feature.%20The%20key%20to%0Atraining%20a%20highly%20accurate%20single-stage%20detector%20requires%20both%20losses%20to%20be%0Alearned%20toward%20high-quality%20targets.%20At%20inference%2C%20OpenM3D%2C%20a%20highly%20efficient%0Adetector%2C%20requires%20only%20multi-view%20images%20for%20input%20and%20demonstrates%20superior%0Aaccuracy%20and%20speed%20%280.3%20sec.%20per%20scene%29%20on%20ScanNet200%20and%20ARKitScenes%20indoor%0Abenchmarks%20compared%20to%20existing%20methods.%20We%20outperform%20a%20strong%20two-stage%0Amethod%20that%20leverages%20our%20class-agnostic%20detector%20with%20a%20ViT%20CLIP-based%20OV%0Aclassifier%20and%20a%20baseline%20incorporating%20multi-view%20depth%20estimator%20on%20both%0Aaccuracy%20and%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20063v1&entry.124074799=Read"},
{"title": "Multispectral LiDAR data for extracting tree points in urban and\n  suburban areas", "author": "Narges Takhtkeshha and Gabriele Mazzacca and Fabio Remondino and Juha Hyypp\u00e4 and Gottfried Mandlburger", "abstract": "  Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.\n", "link": "http://arxiv.org/abs/2508.19881v1", "date": "2025-08-27", "relevancy": 2.4432, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multispectral%20LiDAR%20data%20for%20extracting%20tree%20points%20in%20urban%20and%0A%20%20suburban%20areas&body=Title%3A%20Multispectral%20LiDAR%20data%20for%20extracting%20tree%20points%20in%20urban%20and%0A%20%20suburban%20areas%0AAuthor%3A%20Narges%20Takhtkeshha%20and%20Gabriele%20Mazzacca%20and%20Fabio%20Remondino%20and%20Juha%20Hyypp%C3%A4%20and%20Gottfried%20Mandlburger%0AAbstract%3A%20%20%20Monitoring%20urban%20tree%20dynamics%20is%20vital%20for%20supporting%20greening%20policies%20and%0Areducing%20risks%20to%20electrical%20infrastructure.%20Airborne%20laser%20scanning%20has%0Aadvanced%20large-scale%20tree%20management%2C%20but%20challenges%20remain%20due%20to%20complex%0Aurban%20environments%20and%20tree%20variability.%20Multispectral%20%28MS%29%20light%20detection%20and%0Aranging%20%28LiDAR%29%20improves%20this%20by%20capturing%20both%203D%20spatial%20and%20spectral%20data%2C%0Aenabling%20detailed%20mapping.%20This%20study%20explores%20tree%20point%20extraction%20using%0AMS-LiDAR%20and%20deep%20learning%20%28DL%29%20models.%20Three%20state-of-the-art%20models%20are%0Aevaluated%3A%20Superpoint%20Transformer%20%28SPT%29%2C%20Point%20Transformer%20V3%20%28PTv3%29%2C%20and%20Point%0ATransformer%20V1%20%28PTv1%29.%20Results%20show%20the%20notable%20time%20efficiency%20and%20accuracy%20of%0ASPT%2C%20with%20a%20mean%20intersection%20over%20union%20%28mIoU%29%20of%2085.28%25.%20The%20highest%0Adetection%20accuracy%20is%20achieved%20by%20incorporating%20pseudo%20normalized%20difference%0Avegetation%20index%20%28pNDVI%29%20with%20spatial%20data%2C%20reducing%20error%20rate%20by%2010.61%0Apercentage%20points%20%28pp%29%20compared%20to%20using%20spatial%20information%20alone.%20These%0Afindings%20highlight%20the%20potential%20of%20MS-LiDAR%20and%20DL%20to%20improve%20tree%20extraction%0Aand%20further%20tree%20inventories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultispectral%2520LiDAR%2520data%2520for%2520extracting%2520tree%2520points%2520in%2520urban%2520and%250A%2520%2520suburban%2520areas%26entry.906535625%3DNarges%2520Takhtkeshha%2520and%2520Gabriele%2520Mazzacca%2520and%2520Fabio%2520Remondino%2520and%2520Juha%2520Hyypp%25C3%25A4%2520and%2520Gottfried%2520Mandlburger%26entry.1292438233%3D%2520%2520Monitoring%2520urban%2520tree%2520dynamics%2520is%2520vital%2520for%2520supporting%2520greening%2520policies%2520and%250Areducing%2520risks%2520to%2520electrical%2520infrastructure.%2520Airborne%2520laser%2520scanning%2520has%250Aadvanced%2520large-scale%2520tree%2520management%252C%2520but%2520challenges%2520remain%2520due%2520to%2520complex%250Aurban%2520environments%2520and%2520tree%2520variability.%2520Multispectral%2520%2528MS%2529%2520light%2520detection%2520and%250Aranging%2520%2528LiDAR%2529%2520improves%2520this%2520by%2520capturing%2520both%25203D%2520spatial%2520and%2520spectral%2520data%252C%250Aenabling%2520detailed%2520mapping.%2520This%2520study%2520explores%2520tree%2520point%2520extraction%2520using%250AMS-LiDAR%2520and%2520deep%2520learning%2520%2528DL%2529%2520models.%2520Three%2520state-of-the-art%2520models%2520are%250Aevaluated%253A%2520Superpoint%2520Transformer%2520%2528SPT%2529%252C%2520Point%2520Transformer%2520V3%2520%2528PTv3%2529%252C%2520and%2520Point%250ATransformer%2520V1%2520%2528PTv1%2529.%2520Results%2520show%2520the%2520notable%2520time%2520efficiency%2520and%2520accuracy%2520of%250ASPT%252C%2520with%2520a%2520mean%2520intersection%2520over%2520union%2520%2528mIoU%2529%2520of%252085.28%2525.%2520The%2520highest%250Adetection%2520accuracy%2520is%2520achieved%2520by%2520incorporating%2520pseudo%2520normalized%2520difference%250Avegetation%2520index%2520%2528pNDVI%2529%2520with%2520spatial%2520data%252C%2520reducing%2520error%2520rate%2520by%252010.61%250Apercentage%2520points%2520%2528pp%2529%2520compared%2520to%2520using%2520spatial%2520information%2520alone.%2520These%250Afindings%2520highlight%2520the%2520potential%2520of%2520MS-LiDAR%2520and%2520DL%2520to%2520improve%2520tree%2520extraction%250Aand%2520further%2520tree%2520inventories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multispectral%20LiDAR%20data%20for%20extracting%20tree%20points%20in%20urban%20and%0A%20%20suburban%20areas&entry.906535625=Narges%20Takhtkeshha%20and%20Gabriele%20Mazzacca%20and%20Fabio%20Remondino%20and%20Juha%20Hyypp%C3%A4%20and%20Gottfried%20Mandlburger&entry.1292438233=%20%20Monitoring%20urban%20tree%20dynamics%20is%20vital%20for%20supporting%20greening%20policies%20and%0Areducing%20risks%20to%20electrical%20infrastructure.%20Airborne%20laser%20scanning%20has%0Aadvanced%20large-scale%20tree%20management%2C%20but%20challenges%20remain%20due%20to%20complex%0Aurban%20environments%20and%20tree%20variability.%20Multispectral%20%28MS%29%20light%20detection%20and%0Aranging%20%28LiDAR%29%20improves%20this%20by%20capturing%20both%203D%20spatial%20and%20spectral%20data%2C%0Aenabling%20detailed%20mapping.%20This%20study%20explores%20tree%20point%20extraction%20using%0AMS-LiDAR%20and%20deep%20learning%20%28DL%29%20models.%20Three%20state-of-the-art%20models%20are%0Aevaluated%3A%20Superpoint%20Transformer%20%28SPT%29%2C%20Point%20Transformer%20V3%20%28PTv3%29%2C%20and%20Point%0ATransformer%20V1%20%28PTv1%29.%20Results%20show%20the%20notable%20time%20efficiency%20and%20accuracy%20of%0ASPT%2C%20with%20a%20mean%20intersection%20over%20union%20%28mIoU%29%20of%2085.28%25.%20The%20highest%0Adetection%20accuracy%20is%20achieved%20by%20incorporating%20pseudo%20normalized%20difference%0Avegetation%20index%20%28pNDVI%29%20with%20spatial%20data%2C%20reducing%20error%20rate%20by%2010.61%0Apercentage%20points%20%28pp%29%20compared%20to%20using%20spatial%20information%20alone.%20These%0Afindings%20highlight%20the%20potential%20of%20MS-LiDAR%20and%20DL%20to%20improve%20tree%20extraction%0Aand%20further%20tree%20inventories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19881v1&entry.124074799=Read"},
{"title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text\n  Tasks", "author": "Luel Hagos Beyene and Vivek Verma and Min Ma and Jesujoba O. Alabi and Fabian David Schmidt and Joyce Nakatumba-Nabende and David Ifeoluwa Adelani", "abstract": "  Large Language models (LLMs) have demonstrated impressive performance on a\nwide range of tasks, including in multimodal settings such as speech. However,\ntheir evaluation is often limited to English and a few high-resource languages.\nFor low-resource languages, there is no standardized evaluation benchmark. In\nthis paper, we address this gap by introducing mSTEB, a new benchmark to\nevaluate the performance of LLMs on a wide range of tasks covering language\nidentification, text classification, question answering, and translation tasks\non both speech and text modalities. We evaluated the performance of leading\nLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open\nmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in\nperformance between high-resource and low-resource languages, especially for\nlanguages spoken in Africa and Americas/Oceania. Our findings show that more\ninvestment is needed to address their under-representation in LLMs coverage.\n", "link": "http://arxiv.org/abs/2506.08400v3", "date": "2025-08-27", "relevancy": 2.4182, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mSTEB%3A%20Massively%20Multilingual%20Evaluation%20of%20LLMs%20on%20Speech%20and%20Text%0A%20%20Tasks&body=Title%3A%20mSTEB%3A%20Massively%20Multilingual%20Evaluation%20of%20LLMs%20on%20Speech%20and%20Text%0A%20%20Tasks%0AAuthor%3A%20Luel%20Hagos%20Beyene%20and%20Vivek%20Verma%20and%20Min%20Ma%20and%20Jesujoba%20O.%20Alabi%20and%20Fabian%20David%20Schmidt%20and%20Joyce%20Nakatumba-Nabende%20and%20David%20Ifeoluwa%20Adelani%0AAbstract%3A%20%20%20Large%20Language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%20on%20a%0Awide%20range%20of%20tasks%2C%20including%20in%20multimodal%20settings%20such%20as%20speech.%20However%2C%0Atheir%20evaluation%20is%20often%20limited%20to%20English%20and%20a%20few%20high-resource%20languages.%0AFor%20low-resource%20languages%2C%20there%20is%20no%20standardized%20evaluation%20benchmark.%20In%0Athis%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20mSTEB%2C%20a%20new%20benchmark%20to%0Aevaluate%20the%20performance%20of%20LLMs%20on%20a%20wide%20range%20of%20tasks%20covering%20language%0Aidentification%2C%20text%20classification%2C%20question%20answering%2C%20and%20translation%20tasks%0Aon%20both%20speech%20and%20text%20modalities.%20We%20evaluated%20the%20performance%20of%20leading%0ALLMs%20such%20as%20Gemini%202.0%20Flash%20and%20GPT-4o%20%28Audio%29%20and%20state-of-the-art%20open%0Amodels%20such%20as%20Qwen%202%20Audio%20and%20Gemma%203%2027B.%20Our%20evaluation%20shows%20a%20wide%20gap%20in%0Aperformance%20between%20high-resource%20and%20low-resource%20languages%2C%20especially%20for%0Alanguages%20spoken%20in%20Africa%20and%20Americas/Oceania.%20Our%20findings%20show%20that%20more%0Ainvestment%20is%20needed%20to%20address%20their%20under-representation%20in%20LLMs%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08400v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmSTEB%253A%2520Massively%2520Multilingual%2520Evaluation%2520of%2520LLMs%2520on%2520Speech%2520and%2520Text%250A%2520%2520Tasks%26entry.906535625%3DLuel%2520Hagos%2520Beyene%2520and%2520Vivek%2520Verma%2520and%2520Min%2520Ma%2520and%2520Jesujoba%2520O.%2520Alabi%2520and%2520Fabian%2520David%2520Schmidt%2520and%2520Joyce%2520Nakatumba-Nabende%2520and%2520David%2520Ifeoluwa%2520Adelani%26entry.1292438233%3D%2520%2520Large%2520Language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520on%2520a%250Awide%2520range%2520of%2520tasks%252C%2520including%2520in%2520multimodal%2520settings%2520such%2520as%2520speech.%2520However%252C%250Atheir%2520evaluation%2520is%2520often%2520limited%2520to%2520English%2520and%2520a%2520few%2520high-resource%2520languages.%250AFor%2520low-resource%2520languages%252C%2520there%2520is%2520no%2520standardized%2520evaluation%2520benchmark.%2520In%250Athis%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520mSTEB%252C%2520a%2520new%2520benchmark%2520to%250Aevaluate%2520the%2520performance%2520of%2520LLMs%2520on%2520a%2520wide%2520range%2520of%2520tasks%2520covering%2520language%250Aidentification%252C%2520text%2520classification%252C%2520question%2520answering%252C%2520and%2520translation%2520tasks%250Aon%2520both%2520speech%2520and%2520text%2520modalities.%2520We%2520evaluated%2520the%2520performance%2520of%2520leading%250ALLMs%2520such%2520as%2520Gemini%25202.0%2520Flash%2520and%2520GPT-4o%2520%2528Audio%2529%2520and%2520state-of-the-art%2520open%250Amodels%2520such%2520as%2520Qwen%25202%2520Audio%2520and%2520Gemma%25203%252027B.%2520Our%2520evaluation%2520shows%2520a%2520wide%2520gap%2520in%250Aperformance%2520between%2520high-resource%2520and%2520low-resource%2520languages%252C%2520especially%2520for%250Alanguages%2520spoken%2520in%2520Africa%2520and%2520Americas/Oceania.%2520Our%2520findings%2520show%2520that%2520more%250Ainvestment%2520is%2520needed%2520to%2520address%2520their%2520under-representation%2520in%2520LLMs%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08400v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mSTEB%3A%20Massively%20Multilingual%20Evaluation%20of%20LLMs%20on%20Speech%20and%20Text%0A%20%20Tasks&entry.906535625=Luel%20Hagos%20Beyene%20and%20Vivek%20Verma%20and%20Min%20Ma%20and%20Jesujoba%20O.%20Alabi%20and%20Fabian%20David%20Schmidt%20and%20Joyce%20Nakatumba-Nabende%20and%20David%20Ifeoluwa%20Adelani&entry.1292438233=%20%20Large%20Language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20performance%20on%20a%0Awide%20range%20of%20tasks%2C%20including%20in%20multimodal%20settings%20such%20as%20speech.%20However%2C%0Atheir%20evaluation%20is%20often%20limited%20to%20English%20and%20a%20few%20high-resource%20languages.%0AFor%20low-resource%20languages%2C%20there%20is%20no%20standardized%20evaluation%20benchmark.%20In%0Athis%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20mSTEB%2C%20a%20new%20benchmark%20to%0Aevaluate%20the%20performance%20of%20LLMs%20on%20a%20wide%20range%20of%20tasks%20covering%20language%0Aidentification%2C%20text%20classification%2C%20question%20answering%2C%20and%20translation%20tasks%0Aon%20both%20speech%20and%20text%20modalities.%20We%20evaluated%20the%20performance%20of%20leading%0ALLMs%20such%20as%20Gemini%202.0%20Flash%20and%20GPT-4o%20%28Audio%29%20and%20state-of-the-art%20open%0Amodels%20such%20as%20Qwen%202%20Audio%20and%20Gemma%203%2027B.%20Our%20evaluation%20shows%20a%20wide%20gap%20in%0Aperformance%20between%20high-resource%20and%20low-resource%20languages%2C%20especially%20for%0Alanguages%20spoken%20in%20Africa%20and%20Americas/Oceania.%20Our%20findings%20show%20that%20more%0Ainvestment%20is%20needed%20to%20address%20their%20under-representation%20in%20LLMs%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08400v3&entry.124074799=Read"},
{"title": "Integrating SAM Supervision for 3D Weakly Supervised Point Cloud\n  Segmentation", "author": "Lechun You and Zhonghua Wu and Weide Liu and Xulei Yang and Jun Cheng and Wei Zhou and Bharadwaj Veeravalli and Guosheng Lin", "abstract": "  Current methods for 3D semantic segmentation propose training models with\nlimited annotations to address the difficulty of annotating large, irregular,\nand unordered 3D point cloud data. They usually focus on the 3D domain only,\nwithout leveraging the complementary nature of 2D and 3D data. Besides, some\nmethods extend original labels or generate pseudo labels to guide the training,\nbut they often fail to fully use these labels or address the noise within them.\nMeanwhile, the emergence of comprehensive and adaptable foundation models has\noffered effective solutions for segmenting 2D data. Leveraging this\nadvancement, we present a novel approach that maximizes the utility of sparsely\navailable 3D annotations by incorporating segmentation masks generated by 2D\nfoundation models. We further propagate the 2D segmentation masks into the 3D\nspace by establishing geometric correspondences between 3D scenes and 2D views.\nWe extend the highly sparse annotations to encompass the areas delineated by 3D\nmasks, thereby substantially augmenting the pool of available labels.\nFurthermore, we apply confidence- and uncertainty-based consistency\nregularization on augmentations of the 3D point cloud and select the reliable\npseudo labels, which are further spread on the 3D masks to generate more\nlabels. This innovative strategy bridges the gap between limited 3D annotations\nand the powerful capabilities of 2D foundation models, ultimately improving the\nperformance of 3D weakly supervised segmentation.\n", "link": "http://arxiv.org/abs/2508.19909v1", "date": "2025-08-27", "relevancy": 2.3969, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5993}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20SAM%20Supervision%20for%203D%20Weakly%20Supervised%20Point%20Cloud%0A%20%20Segmentation&body=Title%3A%20Integrating%20SAM%20Supervision%20for%203D%20Weakly%20Supervised%20Point%20Cloud%0A%20%20Segmentation%0AAuthor%3A%20Lechun%20You%20and%20Zhonghua%20Wu%20and%20Weide%20Liu%20and%20Xulei%20Yang%20and%20Jun%20Cheng%20and%20Wei%20Zhou%20and%20Bharadwaj%20Veeravalli%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Current%20methods%20for%203D%20semantic%20segmentation%20propose%20training%20models%20with%0Alimited%20annotations%20to%20address%20the%20difficulty%20of%20annotating%20large%2C%20irregular%2C%0Aand%20unordered%203D%20point%20cloud%20data.%20They%20usually%20focus%20on%20the%203D%20domain%20only%2C%0Awithout%20leveraging%20the%20complementary%20nature%20of%202D%20and%203D%20data.%20Besides%2C%20some%0Amethods%20extend%20original%20labels%20or%20generate%20pseudo%20labels%20to%20guide%20the%20training%2C%0Abut%20they%20often%20fail%20to%20fully%20use%20these%20labels%20or%20address%20the%20noise%20within%20them.%0AMeanwhile%2C%20the%20emergence%20of%20comprehensive%20and%20adaptable%20foundation%20models%20has%0Aoffered%20effective%20solutions%20for%20segmenting%202D%20data.%20Leveraging%20this%0Aadvancement%2C%20we%20present%20a%20novel%20approach%20that%20maximizes%20the%20utility%20of%20sparsely%0Aavailable%203D%20annotations%20by%20incorporating%20segmentation%20masks%20generated%20by%202D%0Afoundation%20models.%20We%20further%20propagate%20the%202D%20segmentation%20masks%20into%20the%203D%0Aspace%20by%20establishing%20geometric%20correspondences%20between%203D%20scenes%20and%202D%20views.%0AWe%20extend%20the%20highly%20sparse%20annotations%20to%20encompass%20the%20areas%20delineated%20by%203D%0Amasks%2C%20thereby%20substantially%20augmenting%20the%20pool%20of%20available%20labels.%0AFurthermore%2C%20we%20apply%20confidence-%20and%20uncertainty-based%20consistency%0Aregularization%20on%20augmentations%20of%20the%203D%20point%20cloud%20and%20select%20the%20reliable%0Apseudo%20labels%2C%20which%20are%20further%20spread%20on%20the%203D%20masks%20to%20generate%20more%0Alabels.%20This%20innovative%20strategy%20bridges%20the%20gap%20between%20limited%203D%20annotations%0Aand%20the%20powerful%20capabilities%20of%202D%20foundation%20models%2C%20ultimately%20improving%20the%0Aperformance%20of%203D%20weakly%20supervised%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520SAM%2520Supervision%2520for%25203D%2520Weakly%2520Supervised%2520Point%2520Cloud%250A%2520%2520Segmentation%26entry.906535625%3DLechun%2520You%2520and%2520Zhonghua%2520Wu%2520and%2520Weide%2520Liu%2520and%2520Xulei%2520Yang%2520and%2520Jun%2520Cheng%2520and%2520Wei%2520Zhou%2520and%2520Bharadwaj%2520Veeravalli%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Current%2520methods%2520for%25203D%2520semantic%2520segmentation%2520propose%2520training%2520models%2520with%250Alimited%2520annotations%2520to%2520address%2520the%2520difficulty%2520of%2520annotating%2520large%252C%2520irregular%252C%250Aand%2520unordered%25203D%2520point%2520cloud%2520data.%2520They%2520usually%2520focus%2520on%2520the%25203D%2520domain%2520only%252C%250Awithout%2520leveraging%2520the%2520complementary%2520nature%2520of%25202D%2520and%25203D%2520data.%2520Besides%252C%2520some%250Amethods%2520extend%2520original%2520labels%2520or%2520generate%2520pseudo%2520labels%2520to%2520guide%2520the%2520training%252C%250Abut%2520they%2520often%2520fail%2520to%2520fully%2520use%2520these%2520labels%2520or%2520address%2520the%2520noise%2520within%2520them.%250AMeanwhile%252C%2520the%2520emergence%2520of%2520comprehensive%2520and%2520adaptable%2520foundation%2520models%2520has%250Aoffered%2520effective%2520solutions%2520for%2520segmenting%25202D%2520data.%2520Leveraging%2520this%250Aadvancement%252C%2520we%2520present%2520a%2520novel%2520approach%2520that%2520maximizes%2520the%2520utility%2520of%2520sparsely%250Aavailable%25203D%2520annotations%2520by%2520incorporating%2520segmentation%2520masks%2520generated%2520by%25202D%250Afoundation%2520models.%2520We%2520further%2520propagate%2520the%25202D%2520segmentation%2520masks%2520into%2520the%25203D%250Aspace%2520by%2520establishing%2520geometric%2520correspondences%2520between%25203D%2520scenes%2520and%25202D%2520views.%250AWe%2520extend%2520the%2520highly%2520sparse%2520annotations%2520to%2520encompass%2520the%2520areas%2520delineated%2520by%25203D%250Amasks%252C%2520thereby%2520substantially%2520augmenting%2520the%2520pool%2520of%2520available%2520labels.%250AFurthermore%252C%2520we%2520apply%2520confidence-%2520and%2520uncertainty-based%2520consistency%250Aregularization%2520on%2520augmentations%2520of%2520the%25203D%2520point%2520cloud%2520and%2520select%2520the%2520reliable%250Apseudo%2520labels%252C%2520which%2520are%2520further%2520spread%2520on%2520the%25203D%2520masks%2520to%2520generate%2520more%250Alabels.%2520This%2520innovative%2520strategy%2520bridges%2520the%2520gap%2520between%2520limited%25203D%2520annotations%250Aand%2520the%2520powerful%2520capabilities%2520of%25202D%2520foundation%2520models%252C%2520ultimately%2520improving%2520the%250Aperformance%2520of%25203D%2520weakly%2520supervised%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20SAM%20Supervision%20for%203D%20Weakly%20Supervised%20Point%20Cloud%0A%20%20Segmentation&entry.906535625=Lechun%20You%20and%20Zhonghua%20Wu%20and%20Weide%20Liu%20and%20Xulei%20Yang%20and%20Jun%20Cheng%20and%20Wei%20Zhou%20and%20Bharadwaj%20Veeravalli%20and%20Guosheng%20Lin&entry.1292438233=%20%20Current%20methods%20for%203D%20semantic%20segmentation%20propose%20training%20models%20with%0Alimited%20annotations%20to%20address%20the%20difficulty%20of%20annotating%20large%2C%20irregular%2C%0Aand%20unordered%203D%20point%20cloud%20data.%20They%20usually%20focus%20on%20the%203D%20domain%20only%2C%0Awithout%20leveraging%20the%20complementary%20nature%20of%202D%20and%203D%20data.%20Besides%2C%20some%0Amethods%20extend%20original%20labels%20or%20generate%20pseudo%20labels%20to%20guide%20the%20training%2C%0Abut%20they%20often%20fail%20to%20fully%20use%20these%20labels%20or%20address%20the%20noise%20within%20them.%0AMeanwhile%2C%20the%20emergence%20of%20comprehensive%20and%20adaptable%20foundation%20models%20has%0Aoffered%20effective%20solutions%20for%20segmenting%202D%20data.%20Leveraging%20this%0Aadvancement%2C%20we%20present%20a%20novel%20approach%20that%20maximizes%20the%20utility%20of%20sparsely%0Aavailable%203D%20annotations%20by%20incorporating%20segmentation%20masks%20generated%20by%202D%0Afoundation%20models.%20We%20further%20propagate%20the%202D%20segmentation%20masks%20into%20the%203D%0Aspace%20by%20establishing%20geometric%20correspondences%20between%203D%20scenes%20and%202D%20views.%0AWe%20extend%20the%20highly%20sparse%20annotations%20to%20encompass%20the%20areas%20delineated%20by%203D%0Amasks%2C%20thereby%20substantially%20augmenting%20the%20pool%20of%20available%20labels.%0AFurthermore%2C%20we%20apply%20confidence-%20and%20uncertainty-based%20consistency%0Aregularization%20on%20augmentations%20of%20the%203D%20point%20cloud%20and%20select%20the%20reliable%0Apseudo%20labels%2C%20which%20are%20further%20spread%20on%20the%203D%20masks%20to%20generate%20more%0Alabels.%20This%20innovative%20strategy%20bridges%20the%20gap%20between%20limited%203D%20annotations%0Aand%20the%20powerful%20capabilities%20of%202D%20foundation%20models%2C%20ultimately%20improving%20the%0Aperformance%20of%203D%20weakly%20supervised%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19909v1&entry.124074799=Read"},
{"title": "Online Writer Retrieval with Chinese Handwritten Phrases: A Synergistic\n  Temporal-Frequency Representation Learning Approach", "author": "Peirong Zhang and Lianwen Jin", "abstract": "  Currently, the prevalence of online handwriting has spurred a critical need\nfor effective retrieval systems to accurately search relevant handwriting\ninstances from specific writers, known as online writer retrieval. Despite the\ngrowing demand, this field suffers from a scarcity of well-established\nmethodologies and public large-scale datasets. This paper tackles these\nchallenges with a focus on Chinese handwritten phrases. First, we propose\nDOLPHIN, a novel retrieval model designed to enhance handwriting\nrepresentations through synergistic temporal-frequency analysis. For frequency\nfeature learning, we propose the HFGA block, which performs gated\ncross-attention between the vanilla temporal handwriting sequence and its\nhigh-frequency sub-bands to amplify salient writing details. For temporal\nfeature learning, we propose the CAIR block, tailored to promote channel\ninteraction and reduce channel redundancy. Second, to address data deficit, we\nintroduce OLIWER, a large-scale online writer retrieval dataset encompassing\nover 670,000 Chinese handwritten phrases from 1,731 individuals. Through\nextensive evaluations, we demonstrate the superior performance of DOLPHIN over\nexisting methods. In addition, we explore cross-domain writer retrieval and\nreveal the pivotal role of increasing feature alignment in bridging the\ndistributional gap between different handwriting data. Our findings emphasize\nthe significance of point sampling frequency and pressure features in improving\nhandwriting representation quality and retrieval performance. Code and dataset\nare available at https://github.com/SCUT-DLVCLab/DOLPHIN.\n", "link": "http://arxiv.org/abs/2412.11668v2", "date": "2025-08-27", "relevancy": 2.3964, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4961}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4756}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Writer%20Retrieval%20with%20Chinese%20Handwritten%20Phrases%3A%20A%20Synergistic%0A%20%20Temporal-Frequency%20Representation%20Learning%20Approach&body=Title%3A%20Online%20Writer%20Retrieval%20with%20Chinese%20Handwritten%20Phrases%3A%20A%20Synergistic%0A%20%20Temporal-Frequency%20Representation%20Learning%20Approach%0AAuthor%3A%20Peirong%20Zhang%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Currently%2C%20the%20prevalence%20of%20online%20handwriting%20has%20spurred%20a%20critical%20need%0Afor%20effective%20retrieval%20systems%20to%20accurately%20search%20relevant%20handwriting%0Ainstances%20from%20specific%20writers%2C%20known%20as%20online%20writer%20retrieval.%20Despite%20the%0Agrowing%20demand%2C%20this%20field%20suffers%20from%20a%20scarcity%20of%20well-established%0Amethodologies%20and%20public%20large-scale%20datasets.%20This%20paper%20tackles%20these%0Achallenges%20with%20a%20focus%20on%20Chinese%20handwritten%20phrases.%20First%2C%20we%20propose%0ADOLPHIN%2C%20a%20novel%20retrieval%20model%20designed%20to%20enhance%20handwriting%0Arepresentations%20through%20synergistic%20temporal-frequency%20analysis.%20For%20frequency%0Afeature%20learning%2C%20we%20propose%20the%20HFGA%20block%2C%20which%20performs%20gated%0Across-attention%20between%20the%20vanilla%20temporal%20handwriting%20sequence%20and%20its%0Ahigh-frequency%20sub-bands%20to%20amplify%20salient%20writing%20details.%20For%20temporal%0Afeature%20learning%2C%20we%20propose%20the%20CAIR%20block%2C%20tailored%20to%20promote%20channel%0Ainteraction%20and%20reduce%20channel%20redundancy.%20Second%2C%20to%20address%20data%20deficit%2C%20we%0Aintroduce%20OLIWER%2C%20a%20large-scale%20online%20writer%20retrieval%20dataset%20encompassing%0Aover%20670%2C000%20Chinese%20handwritten%20phrases%20from%201%2C731%20individuals.%20Through%0Aextensive%20evaluations%2C%20we%20demonstrate%20the%20superior%20performance%20of%20DOLPHIN%20over%0Aexisting%20methods.%20In%20addition%2C%20we%20explore%20cross-domain%20writer%20retrieval%20and%0Areveal%20the%20pivotal%20role%20of%20increasing%20feature%20alignment%20in%20bridging%20the%0Adistributional%20gap%20between%20different%20handwriting%20data.%20Our%20findings%20emphasize%0Athe%20significance%20of%20point%20sampling%20frequency%20and%20pressure%20features%20in%20improving%0Ahandwriting%20representation%20quality%20and%20retrieval%20performance.%20Code%20and%20dataset%0Aare%20available%20at%20https%3A//github.com/SCUT-DLVCLab/DOLPHIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11668v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Writer%2520Retrieval%2520with%2520Chinese%2520Handwritten%2520Phrases%253A%2520A%2520Synergistic%250A%2520%2520Temporal-Frequency%2520Representation%2520Learning%2520Approach%26entry.906535625%3DPeirong%2520Zhang%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Currently%252C%2520the%2520prevalence%2520of%2520online%2520handwriting%2520has%2520spurred%2520a%2520critical%2520need%250Afor%2520effective%2520retrieval%2520systems%2520to%2520accurately%2520search%2520relevant%2520handwriting%250Ainstances%2520from%2520specific%2520writers%252C%2520known%2520as%2520online%2520writer%2520retrieval.%2520Despite%2520the%250Agrowing%2520demand%252C%2520this%2520field%2520suffers%2520from%2520a%2520scarcity%2520of%2520well-established%250Amethodologies%2520and%2520public%2520large-scale%2520datasets.%2520This%2520paper%2520tackles%2520these%250Achallenges%2520with%2520a%2520focus%2520on%2520Chinese%2520handwritten%2520phrases.%2520First%252C%2520we%2520propose%250ADOLPHIN%252C%2520a%2520novel%2520retrieval%2520model%2520designed%2520to%2520enhance%2520handwriting%250Arepresentations%2520through%2520synergistic%2520temporal-frequency%2520analysis.%2520For%2520frequency%250Afeature%2520learning%252C%2520we%2520propose%2520the%2520HFGA%2520block%252C%2520which%2520performs%2520gated%250Across-attention%2520between%2520the%2520vanilla%2520temporal%2520handwriting%2520sequence%2520and%2520its%250Ahigh-frequency%2520sub-bands%2520to%2520amplify%2520salient%2520writing%2520details.%2520For%2520temporal%250Afeature%2520learning%252C%2520we%2520propose%2520the%2520CAIR%2520block%252C%2520tailored%2520to%2520promote%2520channel%250Ainteraction%2520and%2520reduce%2520channel%2520redundancy.%2520Second%252C%2520to%2520address%2520data%2520deficit%252C%2520we%250Aintroduce%2520OLIWER%252C%2520a%2520large-scale%2520online%2520writer%2520retrieval%2520dataset%2520encompassing%250Aover%2520670%252C000%2520Chinese%2520handwritten%2520phrases%2520from%25201%252C731%2520individuals.%2520Through%250Aextensive%2520evaluations%252C%2520we%2520demonstrate%2520the%2520superior%2520performance%2520of%2520DOLPHIN%2520over%250Aexisting%2520methods.%2520In%2520addition%252C%2520we%2520explore%2520cross-domain%2520writer%2520retrieval%2520and%250Areveal%2520the%2520pivotal%2520role%2520of%2520increasing%2520feature%2520alignment%2520in%2520bridging%2520the%250Adistributional%2520gap%2520between%2520different%2520handwriting%2520data.%2520Our%2520findings%2520emphasize%250Athe%2520significance%2520of%2520point%2520sampling%2520frequency%2520and%2520pressure%2520features%2520in%2520improving%250Ahandwriting%2520representation%2520quality%2520and%2520retrieval%2520performance.%2520Code%2520and%2520dataset%250Aare%2520available%2520at%2520https%253A//github.com/SCUT-DLVCLab/DOLPHIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11668v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Writer%20Retrieval%20with%20Chinese%20Handwritten%20Phrases%3A%20A%20Synergistic%0A%20%20Temporal-Frequency%20Representation%20Learning%20Approach&entry.906535625=Peirong%20Zhang%20and%20Lianwen%20Jin&entry.1292438233=%20%20Currently%2C%20the%20prevalence%20of%20online%20handwriting%20has%20spurred%20a%20critical%20need%0Afor%20effective%20retrieval%20systems%20to%20accurately%20search%20relevant%20handwriting%0Ainstances%20from%20specific%20writers%2C%20known%20as%20online%20writer%20retrieval.%20Despite%20the%0Agrowing%20demand%2C%20this%20field%20suffers%20from%20a%20scarcity%20of%20well-established%0Amethodologies%20and%20public%20large-scale%20datasets.%20This%20paper%20tackles%20these%0Achallenges%20with%20a%20focus%20on%20Chinese%20handwritten%20phrases.%20First%2C%20we%20propose%0ADOLPHIN%2C%20a%20novel%20retrieval%20model%20designed%20to%20enhance%20handwriting%0Arepresentations%20through%20synergistic%20temporal-frequency%20analysis.%20For%20frequency%0Afeature%20learning%2C%20we%20propose%20the%20HFGA%20block%2C%20which%20performs%20gated%0Across-attention%20between%20the%20vanilla%20temporal%20handwriting%20sequence%20and%20its%0Ahigh-frequency%20sub-bands%20to%20amplify%20salient%20writing%20details.%20For%20temporal%0Afeature%20learning%2C%20we%20propose%20the%20CAIR%20block%2C%20tailored%20to%20promote%20channel%0Ainteraction%20and%20reduce%20channel%20redundancy.%20Second%2C%20to%20address%20data%20deficit%2C%20we%0Aintroduce%20OLIWER%2C%20a%20large-scale%20online%20writer%20retrieval%20dataset%20encompassing%0Aover%20670%2C000%20Chinese%20handwritten%20phrases%20from%201%2C731%20individuals.%20Through%0Aextensive%20evaluations%2C%20we%20demonstrate%20the%20superior%20performance%20of%20DOLPHIN%20over%0Aexisting%20methods.%20In%20addition%2C%20we%20explore%20cross-domain%20writer%20retrieval%20and%0Areveal%20the%20pivotal%20role%20of%20increasing%20feature%20alignment%20in%20bridging%20the%0Adistributional%20gap%20between%20different%20handwriting%20data.%20Our%20findings%20emphasize%0Athe%20significance%20of%20point%20sampling%20frequency%20and%20pressure%20features%20in%20improving%0Ahandwriting%20representation%20quality%20and%20retrieval%20performance.%20Code%20and%20dataset%0Aare%20available%20at%20https%3A//github.com/SCUT-DLVCLab/DOLPHIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11668v2&entry.124074799=Read"},
{"title": "FaceEditTalker: Controllable Talking Head Generation with Facial\n  Attribute Editing", "author": "Guanwen Feng and Zhiyuan Ma and Yunan Li and Jiahao Yang and Junwei Jing and Qiguang Miao", "abstract": "  Recent advances in audio-driven talking head generation have achieved\nimpressive results in lip synchronization and emotional expression. However,\nthey largely overlook the crucial task of facial attribute editing. This\ncapability is indispensable for achieving deep personalization and expanding\nthe range of practical applications, including user-tailored digital avatars,\nengaging online education content, and brand-specific digital customer service.\nIn these key domains, flexible adjustment of visual attributes, such as\nhairstyle, accessories, and subtle facial features, is essential for aligning\nwith user preferences, reflecting diverse brand identities and adapting to\nvarying contextual demands. In this paper, we present FaceEditTalker, a unified\nframework that enables controllable facial attribute manipulation while\ngenerating high-quality, audio-synchronized talking head videos. Our method\nconsists of two key components: an image feature space editing module, which\nextracts semantic and detail features and allows flexible control over\nattributes like expression, hairstyle, and accessories; and an audio-driven\nvideo generation module, which fuses these edited features with audio-guided\nfacial landmarks to drive a diffusion-based generator. This design ensures\ntemporal coherence, visual fidelity, and identity preservation across frames.\nExtensive experiments on public datasets demonstrate that our method achieves\ncomparable or superior performance to representative baseline methods in\nlip-sync accuracy, video quality, and attribute controllability. Project page:\nhttps://peterfanfan.github.io/FaceEditTalker/\n", "link": "http://arxiv.org/abs/2505.22141v2", "date": "2025-08-27", "relevancy": 2.3947, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6067}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6055}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceEditTalker%3A%20Controllable%20Talking%20Head%20Generation%20with%20Facial%0A%20%20Attribute%20Editing&body=Title%3A%20FaceEditTalker%3A%20Controllable%20Talking%20Head%20Generation%20with%20Facial%0A%20%20Attribute%20Editing%0AAuthor%3A%20Guanwen%20Feng%20and%20Zhiyuan%20Ma%20and%20Yunan%20Li%20and%20Jiahao%20Yang%20and%20Junwei%20Jing%20and%20Qiguang%20Miao%0AAbstract%3A%20%20%20Recent%20advances%20in%20audio-driven%20talking%20head%20generation%20have%20achieved%0Aimpressive%20results%20in%20lip%20synchronization%20and%20emotional%20expression.%20However%2C%0Athey%20largely%20overlook%20the%20crucial%20task%20of%20facial%20attribute%20editing.%20This%0Acapability%20is%20indispensable%20for%20achieving%20deep%20personalization%20and%20expanding%0Athe%20range%20of%20practical%20applications%2C%20including%20user-tailored%20digital%20avatars%2C%0Aengaging%20online%20education%20content%2C%20and%20brand-specific%20digital%20customer%20service.%0AIn%20these%20key%20domains%2C%20flexible%20adjustment%20of%20visual%20attributes%2C%20such%20as%0Ahairstyle%2C%20accessories%2C%20and%20subtle%20facial%20features%2C%20is%20essential%20for%20aligning%0Awith%20user%20preferences%2C%20reflecting%20diverse%20brand%20identities%20and%20adapting%20to%0Avarying%20contextual%20demands.%20In%20this%20paper%2C%20we%20present%20FaceEditTalker%2C%20a%20unified%0Aframework%20that%20enables%20controllable%20facial%20attribute%20manipulation%20while%0Agenerating%20high-quality%2C%20audio-synchronized%20talking%20head%20videos.%20Our%20method%0Aconsists%20of%20two%20key%20components%3A%20an%20image%20feature%20space%20editing%20module%2C%20which%0Aextracts%20semantic%20and%20detail%20features%20and%20allows%20flexible%20control%20over%0Aattributes%20like%20expression%2C%20hairstyle%2C%20and%20accessories%3B%20and%20an%20audio-driven%0Avideo%20generation%20module%2C%20which%20fuses%20these%20edited%20features%20with%20audio-guided%0Afacial%20landmarks%20to%20drive%20a%20diffusion-based%20generator.%20This%20design%20ensures%0Atemporal%20coherence%2C%20visual%20fidelity%2C%20and%20identity%20preservation%20across%20frames.%0AExtensive%20experiments%20on%20public%20datasets%20demonstrate%20that%20our%20method%20achieves%0Acomparable%20or%20superior%20performance%20to%20representative%20baseline%20methods%20in%0Alip-sync%20accuracy%2C%20video%20quality%2C%20and%20attribute%20controllability.%20Project%20page%3A%0Ahttps%3A//peterfanfan.github.io/FaceEditTalker/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22141v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceEditTalker%253A%2520Controllable%2520Talking%2520Head%2520Generation%2520with%2520Facial%250A%2520%2520Attribute%2520Editing%26entry.906535625%3DGuanwen%2520Feng%2520and%2520Zhiyuan%2520Ma%2520and%2520Yunan%2520Li%2520and%2520Jiahao%2520Yang%2520and%2520Junwei%2520Jing%2520and%2520Qiguang%2520Miao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520audio-driven%2520talking%2520head%2520generation%2520have%2520achieved%250Aimpressive%2520results%2520in%2520lip%2520synchronization%2520and%2520emotional%2520expression.%2520However%252C%250Athey%2520largely%2520overlook%2520the%2520crucial%2520task%2520of%2520facial%2520attribute%2520editing.%2520This%250Acapability%2520is%2520indispensable%2520for%2520achieving%2520deep%2520personalization%2520and%2520expanding%250Athe%2520range%2520of%2520practical%2520applications%252C%2520including%2520user-tailored%2520digital%2520avatars%252C%250Aengaging%2520online%2520education%2520content%252C%2520and%2520brand-specific%2520digital%2520customer%2520service.%250AIn%2520these%2520key%2520domains%252C%2520flexible%2520adjustment%2520of%2520visual%2520attributes%252C%2520such%2520as%250Ahairstyle%252C%2520accessories%252C%2520and%2520subtle%2520facial%2520features%252C%2520is%2520essential%2520for%2520aligning%250Awith%2520user%2520preferences%252C%2520reflecting%2520diverse%2520brand%2520identities%2520and%2520adapting%2520to%250Avarying%2520contextual%2520demands.%2520In%2520this%2520paper%252C%2520we%2520present%2520FaceEditTalker%252C%2520a%2520unified%250Aframework%2520that%2520enables%2520controllable%2520facial%2520attribute%2520manipulation%2520while%250Agenerating%2520high-quality%252C%2520audio-synchronized%2520talking%2520head%2520videos.%2520Our%2520method%250Aconsists%2520of%2520two%2520key%2520components%253A%2520an%2520image%2520feature%2520space%2520editing%2520module%252C%2520which%250Aextracts%2520semantic%2520and%2520detail%2520features%2520and%2520allows%2520flexible%2520control%2520over%250Aattributes%2520like%2520expression%252C%2520hairstyle%252C%2520and%2520accessories%253B%2520and%2520an%2520audio-driven%250Avideo%2520generation%2520module%252C%2520which%2520fuses%2520these%2520edited%2520features%2520with%2520audio-guided%250Afacial%2520landmarks%2520to%2520drive%2520a%2520diffusion-based%2520generator.%2520This%2520design%2520ensures%250Atemporal%2520coherence%252C%2520visual%2520fidelity%252C%2520and%2520identity%2520preservation%2520across%2520frames.%250AExtensive%2520experiments%2520on%2520public%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%250Acomparable%2520or%2520superior%2520performance%2520to%2520representative%2520baseline%2520methods%2520in%250Alip-sync%2520accuracy%252C%2520video%2520quality%252C%2520and%2520attribute%2520controllability.%2520Project%2520page%253A%250Ahttps%253A//peterfanfan.github.io/FaceEditTalker/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22141v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceEditTalker%3A%20Controllable%20Talking%20Head%20Generation%20with%20Facial%0A%20%20Attribute%20Editing&entry.906535625=Guanwen%20Feng%20and%20Zhiyuan%20Ma%20and%20Yunan%20Li%20and%20Jiahao%20Yang%20and%20Junwei%20Jing%20and%20Qiguang%20Miao&entry.1292438233=%20%20Recent%20advances%20in%20audio-driven%20talking%20head%20generation%20have%20achieved%0Aimpressive%20results%20in%20lip%20synchronization%20and%20emotional%20expression.%20However%2C%0Athey%20largely%20overlook%20the%20crucial%20task%20of%20facial%20attribute%20editing.%20This%0Acapability%20is%20indispensable%20for%20achieving%20deep%20personalization%20and%20expanding%0Athe%20range%20of%20practical%20applications%2C%20including%20user-tailored%20digital%20avatars%2C%0Aengaging%20online%20education%20content%2C%20and%20brand-specific%20digital%20customer%20service.%0AIn%20these%20key%20domains%2C%20flexible%20adjustment%20of%20visual%20attributes%2C%20such%20as%0Ahairstyle%2C%20accessories%2C%20and%20subtle%20facial%20features%2C%20is%20essential%20for%20aligning%0Awith%20user%20preferences%2C%20reflecting%20diverse%20brand%20identities%20and%20adapting%20to%0Avarying%20contextual%20demands.%20In%20this%20paper%2C%20we%20present%20FaceEditTalker%2C%20a%20unified%0Aframework%20that%20enables%20controllable%20facial%20attribute%20manipulation%20while%0Agenerating%20high-quality%2C%20audio-synchronized%20talking%20head%20videos.%20Our%20method%0Aconsists%20of%20two%20key%20components%3A%20an%20image%20feature%20space%20editing%20module%2C%20which%0Aextracts%20semantic%20and%20detail%20features%20and%20allows%20flexible%20control%20over%0Aattributes%20like%20expression%2C%20hairstyle%2C%20and%20accessories%3B%20and%20an%20audio-driven%0Avideo%20generation%20module%2C%20which%20fuses%20these%20edited%20features%20with%20audio-guided%0Afacial%20landmarks%20to%20drive%20a%20diffusion-based%20generator.%20This%20design%20ensures%0Atemporal%20coherence%2C%20visual%20fidelity%2C%20and%20identity%20preservation%20across%20frames.%0AExtensive%20experiments%20on%20public%20datasets%20demonstrate%20that%20our%20method%20achieves%0Acomparable%20or%20superior%20performance%20to%20representative%20baseline%20methods%20in%0Alip-sync%20accuracy%2C%20video%20quality%2C%20and%20attribute%20controllability.%20Project%20page%3A%0Ahttps%3A//peterfanfan.github.io/FaceEditTalker/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22141v2&entry.124074799=Read"},
{"title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient\n  Estimation", "author": "Ziniu Zhang and Zhenshuo Zhang and Dongyue Li and Lu Wang and Jennifer Dy and Hongyang R. Zhang", "abstract": "  This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.\n", "link": "http://arxiv.org/abs/2508.19999v1", "date": "2025-08-27", "relevancy": 2.379, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4868}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear-Time%20Demonstration%20Selection%20for%20In-Context%20Learning%20via%20Gradient%0A%20%20Estimation&body=Title%3A%20Linear-Time%20Demonstration%20Selection%20for%20In-Context%20Learning%20via%20Gradient%0A%20%20Estimation%0AAuthor%3A%20Ziniu%20Zhang%20and%20Zhenshuo%20Zhang%20and%20Dongyue%20Li%20and%20Lu%20Wang%20and%20Jennifer%20Dy%20and%20Hongyang%20R.%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20algorithm%20to%20select%20demonstration%20examples%20for%0Ain-context%20learning%20of%20a%20query%20set.%20Given%20a%20set%20of%20%24n%24%20examples%2C%20how%20can%20we%0Aquickly%20select%20%24k%24%20out%20of%20%24n%24%20to%20best%20serve%20as%20the%20conditioning%20for%20downstream%0Ainference%3F%20This%20problem%20has%20broad%20applications%20in%20prompt%20tuning%20and%0Achain-of-thought%20reasoning.%20Since%20model%20weights%20remain%20fixed%20during%20in-context%0Alearning%2C%20previous%20work%20has%20sought%20to%20design%20methods%20based%20on%20the%20similarity%20of%0Atoken%20embeddings.%20This%20work%20proposes%20a%20new%20approach%20based%20on%20gradients%20of%20the%0Aoutput%20taken%20in%20the%20input%20embedding%20space.%20Our%20approach%20estimates%20model%20outputs%0Athrough%20a%20first-order%20approximation%20using%20the%20gradients.%20Then%2C%20we%20apply%20this%0Aestimation%20to%20multiple%20randomly%20sampled%20subsets.%20Finally%2C%20we%20aggregate%20the%0Asampled%20subset%20outcomes%20to%20form%20an%20influence%20score%20for%20each%20demonstration%2C%20and%0Aselect%20%24k%24%20most%20relevant%20examples.%20This%20procedure%20only%20requires%20pre-computing%0Amodel%20outputs%20and%20gradients%20once%2C%20resulting%20in%20a%20linear-time%20algorithm%20relative%0Ato%20model%20and%20training%20set%20sizes.%20Extensive%20experiments%20across%20various%20models%0Aand%20datasets%20validate%20the%20efficiency%20of%20our%20approach.%20We%20show%20that%20the%20gradient%0Aestimation%20procedure%20yields%20approximations%20of%20full%20inference%20with%20less%20than%0A%24%5Cmathbf%7B1%7D%5C%25%24%20error%20across%20six%20datasets.%20This%20allows%20us%20to%20scale%20up%20subset%0Aselection%20that%20would%20otherwise%20run%20full%20inference%20by%20up%20to%0A%24%5Cmathbf%7B37.7%7D%5Ctimes%24%20on%20models%20with%20up%20to%20%2434%24%20billion%20parameters%2C%20and%0Aoutperform%20existing%20selection%20methods%20based%20on%20input%20embeddings%20by%0A%24%5Cmathbf%7B11%7D%5C%25%24%20on%20average.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear-Time%2520Demonstration%2520Selection%2520for%2520In-Context%2520Learning%2520via%2520Gradient%250A%2520%2520Estimation%26entry.906535625%3DZiniu%2520Zhang%2520and%2520Zhenshuo%2520Zhang%2520and%2520Dongyue%2520Li%2520and%2520Lu%2520Wang%2520and%2520Jennifer%2520Dy%2520and%2520Hongyang%2520R.%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520algorithm%2520to%2520select%2520demonstration%2520examples%2520for%250Ain-context%2520learning%2520of%2520a%2520query%2520set.%2520Given%2520a%2520set%2520of%2520%2524n%2524%2520examples%252C%2520how%2520can%2520we%250Aquickly%2520select%2520%2524k%2524%2520out%2520of%2520%2524n%2524%2520to%2520best%2520serve%2520as%2520the%2520conditioning%2520for%2520downstream%250Ainference%253F%2520This%2520problem%2520has%2520broad%2520applications%2520in%2520prompt%2520tuning%2520and%250Achain-of-thought%2520reasoning.%2520Since%2520model%2520weights%2520remain%2520fixed%2520during%2520in-context%250Alearning%252C%2520previous%2520work%2520has%2520sought%2520to%2520design%2520methods%2520based%2520on%2520the%2520similarity%2520of%250Atoken%2520embeddings.%2520This%2520work%2520proposes%2520a%2520new%2520approach%2520based%2520on%2520gradients%2520of%2520the%250Aoutput%2520taken%2520in%2520the%2520input%2520embedding%2520space.%2520Our%2520approach%2520estimates%2520model%2520outputs%250Athrough%2520a%2520first-order%2520approximation%2520using%2520the%2520gradients.%2520Then%252C%2520we%2520apply%2520this%250Aestimation%2520to%2520multiple%2520randomly%2520sampled%2520subsets.%2520Finally%252C%2520we%2520aggregate%2520the%250Asampled%2520subset%2520outcomes%2520to%2520form%2520an%2520influence%2520score%2520for%2520each%2520demonstration%252C%2520and%250Aselect%2520%2524k%2524%2520most%2520relevant%2520examples.%2520This%2520procedure%2520only%2520requires%2520pre-computing%250Amodel%2520outputs%2520and%2520gradients%2520once%252C%2520resulting%2520in%2520a%2520linear-time%2520algorithm%2520relative%250Ato%2520model%2520and%2520training%2520set%2520sizes.%2520Extensive%2520experiments%2520across%2520various%2520models%250Aand%2520datasets%2520validate%2520the%2520efficiency%2520of%2520our%2520approach.%2520We%2520show%2520that%2520the%2520gradient%250Aestimation%2520procedure%2520yields%2520approximations%2520of%2520full%2520inference%2520with%2520less%2520than%250A%2524%255Cmathbf%257B1%257D%255C%2525%2524%2520error%2520across%2520six%2520datasets.%2520This%2520allows%2520us%2520to%2520scale%2520up%2520subset%250Aselection%2520that%2520would%2520otherwise%2520run%2520full%2520inference%2520by%2520up%2520to%250A%2524%255Cmathbf%257B37.7%257D%255Ctimes%2524%2520on%2520models%2520with%2520up%2520to%2520%252434%2524%2520billion%2520parameters%252C%2520and%250Aoutperform%2520existing%2520selection%2520methods%2520based%2520on%2520input%2520embeddings%2520by%250A%2524%255Cmathbf%257B11%257D%255C%2525%2524%2520on%2520average.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear-Time%20Demonstration%20Selection%20for%20In-Context%20Learning%20via%20Gradient%0A%20%20Estimation&entry.906535625=Ziniu%20Zhang%20and%20Zhenshuo%20Zhang%20and%20Dongyue%20Li%20and%20Lu%20Wang%20and%20Jennifer%20Dy%20and%20Hongyang%20R.%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20an%20algorithm%20to%20select%20demonstration%20examples%20for%0Ain-context%20learning%20of%20a%20query%20set.%20Given%20a%20set%20of%20%24n%24%20examples%2C%20how%20can%20we%0Aquickly%20select%20%24k%24%20out%20of%20%24n%24%20to%20best%20serve%20as%20the%20conditioning%20for%20downstream%0Ainference%3F%20This%20problem%20has%20broad%20applications%20in%20prompt%20tuning%20and%0Achain-of-thought%20reasoning.%20Since%20model%20weights%20remain%20fixed%20during%20in-context%0Alearning%2C%20previous%20work%20has%20sought%20to%20design%20methods%20based%20on%20the%20similarity%20of%0Atoken%20embeddings.%20This%20work%20proposes%20a%20new%20approach%20based%20on%20gradients%20of%20the%0Aoutput%20taken%20in%20the%20input%20embedding%20space.%20Our%20approach%20estimates%20model%20outputs%0Athrough%20a%20first-order%20approximation%20using%20the%20gradients.%20Then%2C%20we%20apply%20this%0Aestimation%20to%20multiple%20randomly%20sampled%20subsets.%20Finally%2C%20we%20aggregate%20the%0Asampled%20subset%20outcomes%20to%20form%20an%20influence%20score%20for%20each%20demonstration%2C%20and%0Aselect%20%24k%24%20most%20relevant%20examples.%20This%20procedure%20only%20requires%20pre-computing%0Amodel%20outputs%20and%20gradients%20once%2C%20resulting%20in%20a%20linear-time%20algorithm%20relative%0Ato%20model%20and%20training%20set%20sizes.%20Extensive%20experiments%20across%20various%20models%0Aand%20datasets%20validate%20the%20efficiency%20of%20our%20approach.%20We%20show%20that%20the%20gradient%0Aestimation%20procedure%20yields%20approximations%20of%20full%20inference%20with%20less%20than%0A%24%5Cmathbf%7B1%7D%5C%25%24%20error%20across%20six%20datasets.%20This%20allows%20us%20to%20scale%20up%20subset%0Aselection%20that%20would%20otherwise%20run%20full%20inference%20by%20up%20to%0A%24%5Cmathbf%7B37.7%7D%5Ctimes%24%20on%20models%20with%20up%20to%20%2434%24%20billion%20parameters%2C%20and%0Aoutperform%20existing%20selection%20methods%20based%20on%20input%20embeddings%20by%0A%24%5Cmathbf%7B11%7D%5C%25%24%20on%20average.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19999v1&entry.124074799=Read"},
{"title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling", "author": "Ramya Keerthy Thatikonda and Wray Buntine and Ehsan Shareghi", "abstract": "  Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.\n", "link": "http://arxiv.org/abs/2508.19903v1", "date": "2025-08-27", "relevancy": 2.3715, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.488}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logical%20Reasoning%20with%20Outcome%20Reward%20Models%20for%20Test-Time%20Scaling&body=Title%3A%20Logical%20Reasoning%20with%20Outcome%20Reward%20Models%20for%20Test-Time%20Scaling%0AAuthor%3A%20Ramya%20Keerthy%20Thatikonda%20and%20Wray%20Buntine%20and%20Ehsan%20Shareghi%0AAbstract%3A%20%20%20Logical%20reasoning%20is%20a%20critical%20benchmark%20for%20evaluating%20the%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20as%20it%20reflects%20their%20ability%20to%20derive%20valid%0Aconclusions%20from%20given%20premises.%20While%20the%20combination%20of%20test-time%20scaling%0Awith%20dedicated%20outcome%20or%20process%20reward%20models%20has%20opened%20up%20new%20avenues%20to%0Aenhance%20LLMs%20performance%20in%20complex%20reasoning%20tasks%2C%20this%20space%20is%0Aunder-explored%20in%20deductive%20logical%20reasoning.%20We%20present%20a%20set%20of%20Outcome%0AReward%20Models%20%28ORMs%29%20for%20deductive%20reasoning.%20To%20train%20the%20ORMs%20we%20mainly%0Agenerate%20data%20using%20Chain-of-Thought%20%28CoT%29%20with%20single%20and%20multiple%20samples.%0AAdditionally%2C%20we%20propose%20a%20novel%20tactic%20to%20further%20expand%20the%20type%20of%20errors%0Acovered%20in%20the%20training%20dataset%20of%20the%20ORM.%20In%20particular%2C%20we%20propose%20an%20echo%0Ageneration%20technique%20that%20leverages%20LLMs%27%20tendency%20to%20reflect%20incorrect%0Aassumptions%20made%20in%20prompts%20to%20extract%20additional%20training%20data%2C%20covering%0Apreviously%20unexplored%20error%20types.%20While%20a%20standard%20CoT%20chain%20may%20contain%0Aerrors%20likely%20to%20be%20made%20by%20the%20reasoner%2C%20the%20echo%20strategy%20deliberately%20steers%0Athe%20model%20toward%20incorrect%20reasoning.%20We%20show%20that%20ORMs%20trained%20on%20CoT%20and%0Aecho-augmented%20data%20demonstrate%20improved%20performance%20on%20the%20FOLIO%2C%20JustLogic%2C%0Aand%20ProverQA%20datasets%20across%20four%20different%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogical%2520Reasoning%2520with%2520Outcome%2520Reward%2520Models%2520for%2520Test-Time%2520Scaling%26entry.906535625%3DRamya%2520Keerthy%2520Thatikonda%2520and%2520Wray%2520Buntine%2520and%2520Ehsan%2520Shareghi%26entry.1292438233%3D%2520%2520Logical%2520reasoning%2520is%2520a%2520critical%2520benchmark%2520for%2520evaluating%2520the%2520capabilities%2520of%250Alarge%2520language%2520models%2520%2528LLMs%2529%252C%2520as%2520it%2520reflects%2520their%2520ability%2520to%2520derive%2520valid%250Aconclusions%2520from%2520given%2520premises.%2520While%2520the%2520combination%2520of%2520test-time%2520scaling%250Awith%2520dedicated%2520outcome%2520or%2520process%2520reward%2520models%2520has%2520opened%2520up%2520new%2520avenues%2520to%250Aenhance%2520LLMs%2520performance%2520in%2520complex%2520reasoning%2520tasks%252C%2520this%2520space%2520is%250Aunder-explored%2520in%2520deductive%2520logical%2520reasoning.%2520We%2520present%2520a%2520set%2520of%2520Outcome%250AReward%2520Models%2520%2528ORMs%2529%2520for%2520deductive%2520reasoning.%2520To%2520train%2520the%2520ORMs%2520we%2520mainly%250Agenerate%2520data%2520using%2520Chain-of-Thought%2520%2528CoT%2529%2520with%2520single%2520and%2520multiple%2520samples.%250AAdditionally%252C%2520we%2520propose%2520a%2520novel%2520tactic%2520to%2520further%2520expand%2520the%2520type%2520of%2520errors%250Acovered%2520in%2520the%2520training%2520dataset%2520of%2520the%2520ORM.%2520In%2520particular%252C%2520we%2520propose%2520an%2520echo%250Ageneration%2520technique%2520that%2520leverages%2520LLMs%2527%2520tendency%2520to%2520reflect%2520incorrect%250Aassumptions%2520made%2520in%2520prompts%2520to%2520extract%2520additional%2520training%2520data%252C%2520covering%250Apreviously%2520unexplored%2520error%2520types.%2520While%2520a%2520standard%2520CoT%2520chain%2520may%2520contain%250Aerrors%2520likely%2520to%2520be%2520made%2520by%2520the%2520reasoner%252C%2520the%2520echo%2520strategy%2520deliberately%2520steers%250Athe%2520model%2520toward%2520incorrect%2520reasoning.%2520We%2520show%2520that%2520ORMs%2520trained%2520on%2520CoT%2520and%250Aecho-augmented%2520data%2520demonstrate%2520improved%2520performance%2520on%2520the%2520FOLIO%252C%2520JustLogic%252C%250Aand%2520ProverQA%2520datasets%2520across%2520four%2520different%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logical%20Reasoning%20with%20Outcome%20Reward%20Models%20for%20Test-Time%20Scaling&entry.906535625=Ramya%20Keerthy%20Thatikonda%20and%20Wray%20Buntine%20and%20Ehsan%20Shareghi&entry.1292438233=%20%20Logical%20reasoning%20is%20a%20critical%20benchmark%20for%20evaluating%20the%20capabilities%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20as%20it%20reflects%20their%20ability%20to%20derive%20valid%0Aconclusions%20from%20given%20premises.%20While%20the%20combination%20of%20test-time%20scaling%0Awith%20dedicated%20outcome%20or%20process%20reward%20models%20has%20opened%20up%20new%20avenues%20to%0Aenhance%20LLMs%20performance%20in%20complex%20reasoning%20tasks%2C%20this%20space%20is%0Aunder-explored%20in%20deductive%20logical%20reasoning.%20We%20present%20a%20set%20of%20Outcome%0AReward%20Models%20%28ORMs%29%20for%20deductive%20reasoning.%20To%20train%20the%20ORMs%20we%20mainly%0Agenerate%20data%20using%20Chain-of-Thought%20%28CoT%29%20with%20single%20and%20multiple%20samples.%0AAdditionally%2C%20we%20propose%20a%20novel%20tactic%20to%20further%20expand%20the%20type%20of%20errors%0Acovered%20in%20the%20training%20dataset%20of%20the%20ORM.%20In%20particular%2C%20we%20propose%20an%20echo%0Ageneration%20technique%20that%20leverages%20LLMs%27%20tendency%20to%20reflect%20incorrect%0Aassumptions%20made%20in%20prompts%20to%20extract%20additional%20training%20data%2C%20covering%0Apreviously%20unexplored%20error%20types.%20While%20a%20standard%20CoT%20chain%20may%20contain%0Aerrors%20likely%20to%20be%20made%20by%20the%20reasoner%2C%20the%20echo%20strategy%20deliberately%20steers%0Athe%20model%20toward%20incorrect%20reasoning.%20We%20show%20that%20ORMs%20trained%20on%20CoT%20and%0Aecho-augmented%20data%20demonstrate%20improved%20performance%20on%20the%20FOLIO%2C%20JustLogic%2C%0Aand%20ProverQA%20datasets%20across%20four%20different%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19903v1&entry.124074799=Read"},
{"title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional\n  Network and LSTM-CNN", "author": "Seyed Amir Latifi and Hassan Ghassemian and Maryam Imani", "abstract": "  Cardiovascular diseases represent a leading cause of mortality worldwide,\nnecessitating accurate and early diagnosis for improved patient outcomes.\nCurrent diagnostic approaches for cardiac abnormalities often present\nchallenges in clinical settings due to their complexity, cost, or limited\naccessibility. This study develops and evaluates novel deep learning\narchitectures that offer fast, accurate, and cost-effective methods for\nautomatic diagnosis of cardiac diseases, focusing specifically on addressing\nthe critical challenge of limited labeled datasets in medical contexts. We\npropose two innovative methodologies: first, a Multi-Branch Deep Convolutional\nNeural Network (MBDCN) that emulates human auditory processing by utilizing\ndiverse convolutional filter sizes and power spectrum input for enhanced\nfeature extraction; second, a Long Short-Term Memory-Convolutional Neural\n(LSCN) model that integrates LSTM blocks with MBDCN to improve time-domain\nfeature extraction. The synergistic integration of multiple parallel\nconvolutional branches with LSTM units enables superior performance in heart\nsound analysis. Experimental validation demonstrates that LSCN achieves\nmulticlass classification accuracy of 89.65% and binary classification accuracy\nof 93.93%, significantly outperforming state-of-the-art techniques and\ntraditional feature extraction methods such as Mel Frequency Cepstral\nCoefficients (MFCC) and wavelet transforms. A comprehensive 5-fold\ncross-validation confirms the robustness of our approach across varying data\npartitions. These findings establish the efficacy of our proposed architectures\nfor automated heart sound analysis, offering clinically viable and\ncomputationally efficient solutions for early detection of cardiovascular\ndiseases in diverse healthcare environments.\n", "link": "http://arxiv.org/abs/2407.10689v9", "date": "2025-08-27", "relevancy": 2.3694, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4784}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4739}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN&body=Title%3A%20Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN%0AAuthor%3A%20Seyed%20Amir%20Latifi%20and%20Hassan%20Ghassemian%20and%20Maryam%20Imani%0AAbstract%3A%20%20%20Cardiovascular%20diseases%20represent%20a%20leading%20cause%20of%20mortality%20worldwide%2C%0Anecessitating%20accurate%20and%20early%20diagnosis%20for%20improved%20patient%20outcomes.%0ACurrent%20diagnostic%20approaches%20for%20cardiac%20abnormalities%20often%20present%0Achallenges%20in%20clinical%20settings%20due%20to%20their%20complexity%2C%20cost%2C%20or%20limited%0Aaccessibility.%20This%20study%20develops%20and%20evaluates%20novel%20deep%20learning%0Aarchitectures%20that%20offer%20fast%2C%20accurate%2C%20and%20cost-effective%20methods%20for%0Aautomatic%20diagnosis%20of%20cardiac%20diseases%2C%20focusing%20specifically%20on%20addressing%0Athe%20critical%20challenge%20of%20limited%20labeled%20datasets%20in%20medical%20contexts.%20We%0Apropose%20two%20innovative%20methodologies%3A%20first%2C%20a%20Multi-Branch%20Deep%20Convolutional%0ANeural%20Network%20%28MBDCN%29%20that%20emulates%20human%20auditory%20processing%20by%20utilizing%0Adiverse%20convolutional%20filter%20sizes%20and%20power%20spectrum%20input%20for%20enhanced%0Afeature%20extraction%3B%20second%2C%20a%20Long%20Short-Term%20Memory-Convolutional%20Neural%0A%28LSCN%29%20model%20that%20integrates%20LSTM%20blocks%20with%20MBDCN%20to%20improve%20time-domain%0Afeature%20extraction.%20The%20synergistic%20integration%20of%20multiple%20parallel%0Aconvolutional%20branches%20with%20LSTM%20units%20enables%20superior%20performance%20in%20heart%0Asound%20analysis.%20Experimental%20validation%20demonstrates%20that%20LSCN%20achieves%0Amulticlass%20classification%20accuracy%20of%2089.65%25%20and%20binary%20classification%20accuracy%0Aof%2093.93%25%2C%20significantly%20outperforming%20state-of-the-art%20techniques%20and%0Atraditional%20feature%20extraction%20methods%20such%20as%20Mel%20Frequency%20Cepstral%0ACoefficients%20%28MFCC%29%20and%20wavelet%20transforms.%20A%20comprehensive%205-fold%0Across-validation%20confirms%20the%20robustness%20of%20our%20approach%20across%20varying%20data%0Apartitions.%20These%20findings%20establish%20the%20efficacy%20of%20our%20proposed%20architectures%0Afor%20automated%20heart%20sound%20analysis%2C%20offering%20clinically%20viable%20and%0Acomputationally%20efficient%20solutions%20for%20early%20detection%20of%20cardiovascular%0Adiseases%20in%20diverse%20healthcare%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10689v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520of%2520Heart%2520Sounds%2520Using%2520Multi-Branch%2520Deep%2520Convolutional%250A%2520%2520Network%2520and%2520LSTM-CNN%26entry.906535625%3DSeyed%2520Amir%2520Latifi%2520and%2520Hassan%2520Ghassemian%2520and%2520Maryam%2520Imani%26entry.1292438233%3D%2520%2520Cardiovascular%2520diseases%2520represent%2520a%2520leading%2520cause%2520of%2520mortality%2520worldwide%252C%250Anecessitating%2520accurate%2520and%2520early%2520diagnosis%2520for%2520improved%2520patient%2520outcomes.%250ACurrent%2520diagnostic%2520approaches%2520for%2520cardiac%2520abnormalities%2520often%2520present%250Achallenges%2520in%2520clinical%2520settings%2520due%2520to%2520their%2520complexity%252C%2520cost%252C%2520or%2520limited%250Aaccessibility.%2520This%2520study%2520develops%2520and%2520evaluates%2520novel%2520deep%2520learning%250Aarchitectures%2520that%2520offer%2520fast%252C%2520accurate%252C%2520and%2520cost-effective%2520methods%2520for%250Aautomatic%2520diagnosis%2520of%2520cardiac%2520diseases%252C%2520focusing%2520specifically%2520on%2520addressing%250Athe%2520critical%2520challenge%2520of%2520limited%2520labeled%2520datasets%2520in%2520medical%2520contexts.%2520We%250Apropose%2520two%2520innovative%2520methodologies%253A%2520first%252C%2520a%2520Multi-Branch%2520Deep%2520Convolutional%250ANeural%2520Network%2520%2528MBDCN%2529%2520that%2520emulates%2520human%2520auditory%2520processing%2520by%2520utilizing%250Adiverse%2520convolutional%2520filter%2520sizes%2520and%2520power%2520spectrum%2520input%2520for%2520enhanced%250Afeature%2520extraction%253B%2520second%252C%2520a%2520Long%2520Short-Term%2520Memory-Convolutional%2520Neural%250A%2528LSCN%2529%2520model%2520that%2520integrates%2520LSTM%2520blocks%2520with%2520MBDCN%2520to%2520improve%2520time-domain%250Afeature%2520extraction.%2520The%2520synergistic%2520integration%2520of%2520multiple%2520parallel%250Aconvolutional%2520branches%2520with%2520LSTM%2520units%2520enables%2520superior%2520performance%2520in%2520heart%250Asound%2520analysis.%2520Experimental%2520validation%2520demonstrates%2520that%2520LSCN%2520achieves%250Amulticlass%2520classification%2520accuracy%2520of%252089.65%2525%2520and%2520binary%2520classification%2520accuracy%250Aof%252093.93%2525%252C%2520significantly%2520outperforming%2520state-of-the-art%2520techniques%2520and%250Atraditional%2520feature%2520extraction%2520methods%2520such%2520as%2520Mel%2520Frequency%2520Cepstral%250ACoefficients%2520%2528MFCC%2529%2520and%2520wavelet%2520transforms.%2520A%2520comprehensive%25205-fold%250Across-validation%2520confirms%2520the%2520robustness%2520of%2520our%2520approach%2520across%2520varying%2520data%250Apartitions.%2520These%2520findings%2520establish%2520the%2520efficacy%2520of%2520our%2520proposed%2520architectures%250Afor%2520automated%2520heart%2520sound%2520analysis%252C%2520offering%2520clinically%2520viable%2520and%250Acomputationally%2520efficient%2520solutions%2520for%2520early%2520detection%2520of%2520cardiovascular%250Adiseases%2520in%2520diverse%2520healthcare%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10689v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20of%20Heart%20Sounds%20Using%20Multi-Branch%20Deep%20Convolutional%0A%20%20Network%20and%20LSTM-CNN&entry.906535625=Seyed%20Amir%20Latifi%20and%20Hassan%20Ghassemian%20and%20Maryam%20Imani&entry.1292438233=%20%20Cardiovascular%20diseases%20represent%20a%20leading%20cause%20of%20mortality%20worldwide%2C%0Anecessitating%20accurate%20and%20early%20diagnosis%20for%20improved%20patient%20outcomes.%0ACurrent%20diagnostic%20approaches%20for%20cardiac%20abnormalities%20often%20present%0Achallenges%20in%20clinical%20settings%20due%20to%20their%20complexity%2C%20cost%2C%20or%20limited%0Aaccessibility.%20This%20study%20develops%20and%20evaluates%20novel%20deep%20learning%0Aarchitectures%20that%20offer%20fast%2C%20accurate%2C%20and%20cost-effective%20methods%20for%0Aautomatic%20diagnosis%20of%20cardiac%20diseases%2C%20focusing%20specifically%20on%20addressing%0Athe%20critical%20challenge%20of%20limited%20labeled%20datasets%20in%20medical%20contexts.%20We%0Apropose%20two%20innovative%20methodologies%3A%20first%2C%20a%20Multi-Branch%20Deep%20Convolutional%0ANeural%20Network%20%28MBDCN%29%20that%20emulates%20human%20auditory%20processing%20by%20utilizing%0Adiverse%20convolutional%20filter%20sizes%20and%20power%20spectrum%20input%20for%20enhanced%0Afeature%20extraction%3B%20second%2C%20a%20Long%20Short-Term%20Memory-Convolutional%20Neural%0A%28LSCN%29%20model%20that%20integrates%20LSTM%20blocks%20with%20MBDCN%20to%20improve%20time-domain%0Afeature%20extraction.%20The%20synergistic%20integration%20of%20multiple%20parallel%0Aconvolutional%20branches%20with%20LSTM%20units%20enables%20superior%20performance%20in%20heart%0Asound%20analysis.%20Experimental%20validation%20demonstrates%20that%20LSCN%20achieves%0Amulticlass%20classification%20accuracy%20of%2089.65%25%20and%20binary%20classification%20accuracy%0Aof%2093.93%25%2C%20significantly%20outperforming%20state-of-the-art%20techniques%20and%0Atraditional%20feature%20extraction%20methods%20such%20as%20Mel%20Frequency%20Cepstral%0ACoefficients%20%28MFCC%29%20and%20wavelet%20transforms.%20A%20comprehensive%205-fold%0Across-validation%20confirms%20the%20robustness%20of%20our%20approach%20across%20varying%20data%0Apartitions.%20These%20findings%20establish%20the%20efficacy%20of%20our%20proposed%20architectures%0Afor%20automated%20heart%20sound%20analysis%2C%20offering%20clinically%20viable%20and%0Acomputationally%20efficient%20solutions%20for%20early%20detection%20of%20cardiovascular%0Adiseases%20in%20diverse%20healthcare%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10689v9&entry.124074799=Read"},
{"title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning", "author": "Wei Xiong and Wenting Zhao and Weizhe Yuan and Olga Golovneva and Tong Zhang and Jason Weston and Sainbayar Sukhbaatar", "abstract": "  As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.\n", "link": "http://arxiv.org/abs/2508.19229v2", "date": "2025-08-27", "relevancy": 2.3618, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4907}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4646}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning&body=Title%3A%20StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning%0AAuthor%3A%20Wei%20Xiong%20and%20Wenting%20Zhao%20and%20Weizhe%20Yuan%20and%20Olga%20Golovneva%20and%20Tong%20Zhang%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar%0AAbstract%3A%20%20%20As%20models%20increasingly%20leverage%20multi-step%20reasoning%20strategies%20to%20solve%0Acomplex%20problems%2C%20supervising%20the%20logical%20validity%20of%20these%20intermediate%20steps%0Ahas%20become%20a%20critical%20research%20challenge.%20Process%20reward%20models%20address%20this%20by%0Aproviding%20step-by-step%20feedback%2C%20but%20current%20approaches%20have%20two%20major%0Adrawbacks%3A%20they%20typically%20function%20as%20classifiers%20without%20providing%0Aexplanations%2C%20and%20their%20reliance%20on%20supervised%20fine-tuning%20with%20static%20datasets%0Alimits%20generalization.%20Inspired%20by%20recent%20advances%2C%20we%20reframe%20stepwise%20reward%0Amodeling%20from%20a%20classification%20task%20to%20a%20reasoning%20task%20itself.%20We%20thus%20propose%0Aa%20generative%20judge%20that%20reasons%20about%20the%20policy%20model%27s%20reasoning%20steps%20%28i.e.%2C%0Ameta-reasons%29%2C%20outputting%20thinking%20tokens%20before%20delivering%20a%20final%20verdict.%0AOur%20model%2C%20StepWiser%2C%20is%20trained%20by%20reinforcement%20learning%20using%20relative%0Aoutcomes%20of%20rollouts.%20We%20show%20it%20provides%20%28i%29%20better%20judgment%20accuracy%20on%0Aintermediate%20steps%20than%20existing%20methods%3B%20%28ii%29%20can%20be%20used%20to%20improve%20the%0Apolicy%20model%20at%20training%20time%3B%20and%20%28iii%29%20improves%20inference-time%20search.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepWiser%253A%2520Stepwise%2520Generative%2520Judges%2520for%2520Wiser%2520Reasoning%26entry.906535625%3DWei%2520Xiong%2520and%2520Wenting%2520Zhao%2520and%2520Weizhe%2520Yuan%2520and%2520Olga%2520Golovneva%2520and%2520Tong%2520Zhang%2520and%2520Jason%2520Weston%2520and%2520Sainbayar%2520Sukhbaatar%26entry.1292438233%3D%2520%2520As%2520models%2520increasingly%2520leverage%2520multi-step%2520reasoning%2520strategies%2520to%2520solve%250Acomplex%2520problems%252C%2520supervising%2520the%2520logical%2520validity%2520of%2520these%2520intermediate%2520steps%250Ahas%2520become%2520a%2520critical%2520research%2520challenge.%2520Process%2520reward%2520models%2520address%2520this%2520by%250Aproviding%2520step-by-step%2520feedback%252C%2520but%2520current%2520approaches%2520have%2520two%2520major%250Adrawbacks%253A%2520they%2520typically%2520function%2520as%2520classifiers%2520without%2520providing%250Aexplanations%252C%2520and%2520their%2520reliance%2520on%2520supervised%2520fine-tuning%2520with%2520static%2520datasets%250Alimits%2520generalization.%2520Inspired%2520by%2520recent%2520advances%252C%2520we%2520reframe%2520stepwise%2520reward%250Amodeling%2520from%2520a%2520classification%2520task%2520to%2520a%2520reasoning%2520task%2520itself.%2520We%2520thus%2520propose%250Aa%2520generative%2520judge%2520that%2520reasons%2520about%2520the%2520policy%2520model%2527s%2520reasoning%2520steps%2520%2528i.e.%252C%250Ameta-reasons%2529%252C%2520outputting%2520thinking%2520tokens%2520before%2520delivering%2520a%2520final%2520verdict.%250AOur%2520model%252C%2520StepWiser%252C%2520is%2520trained%2520by%2520reinforcement%2520learning%2520using%2520relative%250Aoutcomes%2520of%2520rollouts.%2520We%2520show%2520it%2520provides%2520%2528i%2529%2520better%2520judgment%2520accuracy%2520on%250Aintermediate%2520steps%2520than%2520existing%2520methods%253B%2520%2528ii%2529%2520can%2520be%2520used%2520to%2520improve%2520the%250Apolicy%2520model%2520at%2520training%2520time%253B%2520and%2520%2528iii%2529%2520improves%2520inference-time%2520search.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning&entry.906535625=Wei%20Xiong%20and%20Wenting%20Zhao%20and%20Weizhe%20Yuan%20and%20Olga%20Golovneva%20and%20Tong%20Zhang%20and%20Jason%20Weston%20and%20Sainbayar%20Sukhbaatar&entry.1292438233=%20%20As%20models%20increasingly%20leverage%20multi-step%20reasoning%20strategies%20to%20solve%0Acomplex%20problems%2C%20supervising%20the%20logical%20validity%20of%20these%20intermediate%20steps%0Ahas%20become%20a%20critical%20research%20challenge.%20Process%20reward%20models%20address%20this%20by%0Aproviding%20step-by-step%20feedback%2C%20but%20current%20approaches%20have%20two%20major%0Adrawbacks%3A%20they%20typically%20function%20as%20classifiers%20without%20providing%0Aexplanations%2C%20and%20their%20reliance%20on%20supervised%20fine-tuning%20with%20static%20datasets%0Alimits%20generalization.%20Inspired%20by%20recent%20advances%2C%20we%20reframe%20stepwise%20reward%0Amodeling%20from%20a%20classification%20task%20to%20a%20reasoning%20task%20itself.%20We%20thus%20propose%0Aa%20generative%20judge%20that%20reasons%20about%20the%20policy%20model%27s%20reasoning%20steps%20%28i.e.%2C%0Ameta-reasons%29%2C%20outputting%20thinking%20tokens%20before%20delivering%20a%20final%20verdict.%0AOur%20model%2C%20StepWiser%2C%20is%20trained%20by%20reinforcement%20learning%20using%20relative%0Aoutcomes%20of%20rollouts.%20We%20show%20it%20provides%20%28i%29%20better%20judgment%20accuracy%20on%0Aintermediate%20steps%20than%20existing%20methods%3B%20%28ii%29%20can%20be%20used%20to%20improve%20the%0Apolicy%20model%20at%20training%20time%3B%20and%20%28iii%29%20improves%20inference-time%20search.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19229v2&entry.124074799=Read"},
{"title": "Ego-centric Predictive Model Conditioned on Hand Trajectories", "author": "Binjie Zhang and Mike Zheng Shou", "abstract": "  In egocentric scenarios, anticipating both the next action and its visual\noutcome is essential for understanding human-object interactions and for\nenabling robotic planning. However, existing paradigms fall short of jointly\nmodeling these aspects. Vision-Language-Action (VLA) models focus on action\nprediction but lack explicit modeling of how actions influence the visual\nscene, while video prediction models generate future frames without\nconditioning on specific actions, often resulting in implausible or\ncontextually inconsistent outcomes. To bridge this gap, we propose a unified\ntwo-stage predictive framework that jointly models action and visual future in\negocentric scenarios, conditioned on hand trajectories. In the first stage, we\nperform consecutive state modeling to process heterogeneous inputs (visual\nobservations, language, and action history) and explicitly predict future hand\ntrajectories. In the second stage, we introduce causal cross-attention to fuse\nmulti-modal cues, leveraging inferred action signals to guide an image-based\nLatent Diffusion Model (LDM) for frame-by-frame future video generation. Our\napproach is the first unified model designed to handle both egocentric human\nactivity understanding and robotic manipulation tasks, providing explicit\npredictions of both upcoming actions and their visual consequences. Extensive\nexperiments on Ego4D, BridgeData, and RLBench demonstrate that our method\noutperforms state-of-the-art baselines in both action prediction and future\nvideo synthesis.\n", "link": "http://arxiv.org/abs/2508.19852v1", "date": "2025-08-27", "relevancy": 2.3589, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5981}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5959}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ego-centric%20Predictive%20Model%20Conditioned%20on%20Hand%20Trajectories&body=Title%3A%20Ego-centric%20Predictive%20Model%20Conditioned%20on%20Hand%20Trajectories%0AAuthor%3A%20Binjie%20Zhang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20In%20egocentric%20scenarios%2C%20anticipating%20both%20the%20next%20action%20and%20its%20visual%0Aoutcome%20is%20essential%20for%20understanding%20human-object%20interactions%20and%20for%0Aenabling%20robotic%20planning.%20However%2C%20existing%20paradigms%20fall%20short%20of%20jointly%0Amodeling%20these%20aspects.%20Vision-Language-Action%20%28VLA%29%20models%20focus%20on%20action%0Aprediction%20but%20lack%20explicit%20modeling%20of%20how%20actions%20influence%20the%20visual%0Ascene%2C%20while%20video%20prediction%20models%20generate%20future%20frames%20without%0Aconditioning%20on%20specific%20actions%2C%20often%20resulting%20in%20implausible%20or%0Acontextually%20inconsistent%20outcomes.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20unified%0Atwo-stage%20predictive%20framework%20that%20jointly%20models%20action%20and%20visual%20future%20in%0Aegocentric%20scenarios%2C%20conditioned%20on%20hand%20trajectories.%20In%20the%20first%20stage%2C%20we%0Aperform%20consecutive%20state%20modeling%20to%20process%20heterogeneous%20inputs%20%28visual%0Aobservations%2C%20language%2C%20and%20action%20history%29%20and%20explicitly%20predict%20future%20hand%0Atrajectories.%20In%20the%20second%20stage%2C%20we%20introduce%20causal%20cross-attention%20to%20fuse%0Amulti-modal%20cues%2C%20leveraging%20inferred%20action%20signals%20to%20guide%20an%20image-based%0ALatent%20Diffusion%20Model%20%28LDM%29%20for%20frame-by-frame%20future%20video%20generation.%20Our%0Aapproach%20is%20the%20first%20unified%20model%20designed%20to%20handle%20both%20egocentric%20human%0Aactivity%20understanding%20and%20robotic%20manipulation%20tasks%2C%20providing%20explicit%0Apredictions%20of%20both%20upcoming%20actions%20and%20their%20visual%20consequences.%20Extensive%0Aexperiments%20on%20Ego4D%2C%20BridgeData%2C%20and%20RLBench%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20baselines%20in%20both%20action%20prediction%20and%20future%0Avideo%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgo-centric%2520Predictive%2520Model%2520Conditioned%2520on%2520Hand%2520Trajectories%26entry.906535625%3DBinjie%2520Zhang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520In%2520egocentric%2520scenarios%252C%2520anticipating%2520both%2520the%2520next%2520action%2520and%2520its%2520visual%250Aoutcome%2520is%2520essential%2520for%2520understanding%2520human-object%2520interactions%2520and%2520for%250Aenabling%2520robotic%2520planning.%2520However%252C%2520existing%2520paradigms%2520fall%2520short%2520of%2520jointly%250Amodeling%2520these%2520aspects.%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520focus%2520on%2520action%250Aprediction%2520but%2520lack%2520explicit%2520modeling%2520of%2520how%2520actions%2520influence%2520the%2520visual%250Ascene%252C%2520while%2520video%2520prediction%2520models%2520generate%2520future%2520frames%2520without%250Aconditioning%2520on%2520specific%2520actions%252C%2520often%2520resulting%2520in%2520implausible%2520or%250Acontextually%2520inconsistent%2520outcomes.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520unified%250Atwo-stage%2520predictive%2520framework%2520that%2520jointly%2520models%2520action%2520and%2520visual%2520future%2520in%250Aegocentric%2520scenarios%252C%2520conditioned%2520on%2520hand%2520trajectories.%2520In%2520the%2520first%2520stage%252C%2520we%250Aperform%2520consecutive%2520state%2520modeling%2520to%2520process%2520heterogeneous%2520inputs%2520%2528visual%250Aobservations%252C%2520language%252C%2520and%2520action%2520history%2529%2520and%2520explicitly%2520predict%2520future%2520hand%250Atrajectories.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520causal%2520cross-attention%2520to%2520fuse%250Amulti-modal%2520cues%252C%2520leveraging%2520inferred%2520action%2520signals%2520to%2520guide%2520an%2520image-based%250ALatent%2520Diffusion%2520Model%2520%2528LDM%2529%2520for%2520frame-by-frame%2520future%2520video%2520generation.%2520Our%250Aapproach%2520is%2520the%2520first%2520unified%2520model%2520designed%2520to%2520handle%2520both%2520egocentric%2520human%250Aactivity%2520understanding%2520and%2520robotic%2520manipulation%2520tasks%252C%2520providing%2520explicit%250Apredictions%2520of%2520both%2520upcoming%2520actions%2520and%2520their%2520visual%2520consequences.%2520Extensive%250Aexperiments%2520on%2520Ego4D%252C%2520BridgeData%252C%2520and%2520RLBench%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520state-of-the-art%2520baselines%2520in%2520both%2520action%2520prediction%2520and%2520future%250Avideo%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ego-centric%20Predictive%20Model%20Conditioned%20on%20Hand%20Trajectories&entry.906535625=Binjie%20Zhang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20In%20egocentric%20scenarios%2C%20anticipating%20both%20the%20next%20action%20and%20its%20visual%0Aoutcome%20is%20essential%20for%20understanding%20human-object%20interactions%20and%20for%0Aenabling%20robotic%20planning.%20However%2C%20existing%20paradigms%20fall%20short%20of%20jointly%0Amodeling%20these%20aspects.%20Vision-Language-Action%20%28VLA%29%20models%20focus%20on%20action%0Aprediction%20but%20lack%20explicit%20modeling%20of%20how%20actions%20influence%20the%20visual%0Ascene%2C%20while%20video%20prediction%20models%20generate%20future%20frames%20without%0Aconditioning%20on%20specific%20actions%2C%20often%20resulting%20in%20implausible%20or%0Acontextually%20inconsistent%20outcomes.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20unified%0Atwo-stage%20predictive%20framework%20that%20jointly%20models%20action%20and%20visual%20future%20in%0Aegocentric%20scenarios%2C%20conditioned%20on%20hand%20trajectories.%20In%20the%20first%20stage%2C%20we%0Aperform%20consecutive%20state%20modeling%20to%20process%20heterogeneous%20inputs%20%28visual%0Aobservations%2C%20language%2C%20and%20action%20history%29%20and%20explicitly%20predict%20future%20hand%0Atrajectories.%20In%20the%20second%20stage%2C%20we%20introduce%20causal%20cross-attention%20to%20fuse%0Amulti-modal%20cues%2C%20leveraging%20inferred%20action%20signals%20to%20guide%20an%20image-based%0ALatent%20Diffusion%20Model%20%28LDM%29%20for%20frame-by-frame%20future%20video%20generation.%20Our%0Aapproach%20is%20the%20first%20unified%20model%20designed%20to%20handle%20both%20egocentric%20human%0Aactivity%20understanding%20and%20robotic%20manipulation%20tasks%2C%20providing%20explicit%0Apredictions%20of%20both%20upcoming%20actions%20and%20their%20visual%20consequences.%20Extensive%0Aexperiments%20on%20Ego4D%2C%20BridgeData%2C%20and%20RLBench%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20baselines%20in%20both%20action%20prediction%20and%20future%0Avideo%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19852v1&entry.124074799=Read"},
{"title": "Physics-Informed DeepONet Coupled with FEM for Convective Transport in\n  Porous Media with Sharp Gaussian Sources", "author": "Erdi Kara and Panos Stinis", "abstract": "  We present a hybrid framework that couples finite element methods (FEM) with\nphysics-informed DeepONet to model fluid transport in porous media from sharp,\nlocalized Gaussian sources. The governing system consists of a steady-state\nDarcy flow equation and a time-dependent convection-diffusion equation. Our\napproach solves the Darcy system using FEM and transfers the resulting velocity\nfield to a physics-informed DeepONet, which learns the mapping from source\nfunctions to solute concentration profiles. This modular strategy preserves\nFEM-level accuracy in the flow field while enabling fast inference for\ntransport dynamics. To handle steep gradients induced by sharp sources, we\nintroduce an adaptive sampling strategy for trunk collocation points. Numerical\nexperiments demonstrate that our method is in good agreement with the reference\nsolutions while offering orders of magnitude speedups over traditional solvers,\nmaking it suitable for practical applications in relevant scenarios.\nImplementation of our proposed method is available at\nhttps://github.com/erkara/fem-pi-deeponet.\n", "link": "http://arxiv.org/abs/2508.19847v1", "date": "2025-08-27", "relevancy": 2.3452, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4963}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4579}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20DeepONet%20Coupled%20with%20FEM%20for%20Convective%20Transport%20in%0A%20%20Porous%20Media%20with%20Sharp%20Gaussian%20Sources&body=Title%3A%20Physics-Informed%20DeepONet%20Coupled%20with%20FEM%20for%20Convective%20Transport%20in%0A%20%20Porous%20Media%20with%20Sharp%20Gaussian%20Sources%0AAuthor%3A%20Erdi%20Kara%20and%20Panos%20Stinis%0AAbstract%3A%20%20%20We%20present%20a%20hybrid%20framework%20that%20couples%20finite%20element%20methods%20%28FEM%29%20with%0Aphysics-informed%20DeepONet%20to%20model%20fluid%20transport%20in%20porous%20media%20from%20sharp%2C%0Alocalized%20Gaussian%20sources.%20The%20governing%20system%20consists%20of%20a%20steady-state%0ADarcy%20flow%20equation%20and%20a%20time-dependent%20convection-diffusion%20equation.%20Our%0Aapproach%20solves%20the%20Darcy%20system%20using%20FEM%20and%20transfers%20the%20resulting%20velocity%0Afield%20to%20a%20physics-informed%20DeepONet%2C%20which%20learns%20the%20mapping%20from%20source%0Afunctions%20to%20solute%20concentration%20profiles.%20This%20modular%20strategy%20preserves%0AFEM-level%20accuracy%20in%20the%20flow%20field%20while%20enabling%20fast%20inference%20for%0Atransport%20dynamics.%20To%20handle%20steep%20gradients%20induced%20by%20sharp%20sources%2C%20we%0Aintroduce%20an%20adaptive%20sampling%20strategy%20for%20trunk%20collocation%20points.%20Numerical%0Aexperiments%20demonstrate%20that%20our%20method%20is%20in%20good%20agreement%20with%20the%20reference%0Asolutions%20while%20offering%20orders%20of%20magnitude%20speedups%20over%20traditional%20solvers%2C%0Amaking%20it%20suitable%20for%20practical%20applications%20in%20relevant%20scenarios.%0AImplementation%20of%20our%20proposed%20method%20is%20available%20at%0Ahttps%3A//github.com/erkara/fem-pi-deeponet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520DeepONet%2520Coupled%2520with%2520FEM%2520for%2520Convective%2520Transport%2520in%250A%2520%2520Porous%2520Media%2520with%2520Sharp%2520Gaussian%2520Sources%26entry.906535625%3DErdi%2520Kara%2520and%2520Panos%2520Stinis%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520hybrid%2520framework%2520that%2520couples%2520finite%2520element%2520methods%2520%2528FEM%2529%2520with%250Aphysics-informed%2520DeepONet%2520to%2520model%2520fluid%2520transport%2520in%2520porous%2520media%2520from%2520sharp%252C%250Alocalized%2520Gaussian%2520sources.%2520The%2520governing%2520system%2520consists%2520of%2520a%2520steady-state%250ADarcy%2520flow%2520equation%2520and%2520a%2520time-dependent%2520convection-diffusion%2520equation.%2520Our%250Aapproach%2520solves%2520the%2520Darcy%2520system%2520using%2520FEM%2520and%2520transfers%2520the%2520resulting%2520velocity%250Afield%2520to%2520a%2520physics-informed%2520DeepONet%252C%2520which%2520learns%2520the%2520mapping%2520from%2520source%250Afunctions%2520to%2520solute%2520concentration%2520profiles.%2520This%2520modular%2520strategy%2520preserves%250AFEM-level%2520accuracy%2520in%2520the%2520flow%2520field%2520while%2520enabling%2520fast%2520inference%2520for%250Atransport%2520dynamics.%2520To%2520handle%2520steep%2520gradients%2520induced%2520by%2520sharp%2520sources%252C%2520we%250Aintroduce%2520an%2520adaptive%2520sampling%2520strategy%2520for%2520trunk%2520collocation%2520points.%2520Numerical%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520is%2520in%2520good%2520agreement%2520with%2520the%2520reference%250Asolutions%2520while%2520offering%2520orders%2520of%2520magnitude%2520speedups%2520over%2520traditional%2520solvers%252C%250Amaking%2520it%2520suitable%2520for%2520practical%2520applications%2520in%2520relevant%2520scenarios.%250AImplementation%2520of%2520our%2520proposed%2520method%2520is%2520available%2520at%250Ahttps%253A//github.com/erkara/fem-pi-deeponet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20DeepONet%20Coupled%20with%20FEM%20for%20Convective%20Transport%20in%0A%20%20Porous%20Media%20with%20Sharp%20Gaussian%20Sources&entry.906535625=Erdi%20Kara%20and%20Panos%20Stinis&entry.1292438233=%20%20We%20present%20a%20hybrid%20framework%20that%20couples%20finite%20element%20methods%20%28FEM%29%20with%0Aphysics-informed%20DeepONet%20to%20model%20fluid%20transport%20in%20porous%20media%20from%20sharp%2C%0Alocalized%20Gaussian%20sources.%20The%20governing%20system%20consists%20of%20a%20steady-state%0ADarcy%20flow%20equation%20and%20a%20time-dependent%20convection-diffusion%20equation.%20Our%0Aapproach%20solves%20the%20Darcy%20system%20using%20FEM%20and%20transfers%20the%20resulting%20velocity%0Afield%20to%20a%20physics-informed%20DeepONet%2C%20which%20learns%20the%20mapping%20from%20source%0Afunctions%20to%20solute%20concentration%20profiles.%20This%20modular%20strategy%20preserves%0AFEM-level%20accuracy%20in%20the%20flow%20field%20while%20enabling%20fast%20inference%20for%0Atransport%20dynamics.%20To%20handle%20steep%20gradients%20induced%20by%20sharp%20sources%2C%20we%0Aintroduce%20an%20adaptive%20sampling%20strategy%20for%20trunk%20collocation%20points.%20Numerical%0Aexperiments%20demonstrate%20that%20our%20method%20is%20in%20good%20agreement%20with%20the%20reference%0Asolutions%20while%20offering%20orders%20of%20magnitude%20speedups%20over%20traditional%20solvers%2C%0Amaking%20it%20suitable%20for%20practical%20applications%20in%20relevant%20scenarios.%0AImplementation%20of%20our%20proposed%20method%20is%20available%20at%0Ahttps%3A//github.com/erkara/fem-pi-deeponet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19847v1&entry.124074799=Read"},
{"title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs", "author": "Zheng Qin and Ruobing Zheng and Yabing Wang and Tianqi Li and Yi Yuan and Jingdong Chen and Le Wang", "abstract": "  While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\n\\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/\n", "link": "http://arxiv.org/abs/2508.10576v2", "date": "2025-08-27", "relevancy": 2.3326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5835}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5835}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%0A%20%20Responses%20through%20Reasoning%20MLLMs&body=Title%3A%20HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%0A%20%20Responses%20through%20Reasoning%20MLLMs%0AAuthor%3A%20Zheng%20Qin%20and%20Ruobing%20Zheng%20and%20Yabing%20Wang%20and%20Tianqi%20Li%20and%20Yi%20Yuan%20and%20Jingdong%20Chen%20and%20Le%20Wang%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20immense%20promise%20for%0Aachieving%20truly%20human-like%20interactions%2C%20progress%20is%20hindered%20by%20the%20lack%20of%0Afine-grained%20evaluation%20frameworks%20for%20human-centered%20scenarios%2C%20encompassing%0Aboth%20the%20understanding%20of%20complex%20human%20intentions%20and%20the%20provision%20of%0Aempathetic%2C%20context-aware%20responses.%20Here%20we%20introduce%20HumanSense%2C%20a%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20the%20human-centered%20perception%20and%0Ainteraction%20capabilities%20of%20MLLMs%2C%20with%20a%20particular%20focus%20on%20deep%0Aunderstanding%20of%20extended%20multimodal%20contexts%20and%20the%20formulation%20of%20rational%0Afeedback.%20Our%20evaluation%20reveals%20that%20leading%20MLLMs%20still%20have%20considerable%0Aroom%20for%20improvement%2C%20particularly%20for%20advanced%20interaction-oriented%20tasks.%0ASupplementing%20visual%20input%20with%20audio%20and%20text%20information%20yields%20substantial%0Aimprovements%2C%20and%20Omni-modal%20models%20show%20advantages%20on%20these%20tasks.%0AFurthermore%2C%20we%20argue%20that%20appropriate%20feedback%20stems%20from%20a%20contextual%0Aanalysis%20of%20the%20interlocutor%27s%20needs%20and%20emotions%2C%20with%20reasoning%20ability%0Aserving%20as%20the%20key%20to%20unlocking%20it.%20Accordingly%2C%20we%20employ%20a%20multi-stage%2C%0Amodality-progressive%20reinforcement%20learning%20to%20enhance%20the%20reasoning%20abilities%0Aof%20an%20Omni%20model%2C%20achieving%20substantial%20gains%20on%20evaluation%20results.%0AAdditionally%2C%20we%20observe%20that%20successful%20reasoning%20processes%20exhibit%20highly%0Aconsistent%20thought%20patterns.%20By%20designing%20corresponding%20prompts%2C%20we%20also%0Aenhance%20the%20performance%20of%20non-reasoning%20models%20in%20a%20training-free%20manner.%0AProject%20page%3A%0A%5Ctextcolor%7Bbrightpink%7Dhttps%3A//digital-avatar.github.io/ai/HumanSense/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.10576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanSense%253A%2520From%2520Multimodal%2520Perception%2520to%2520Empathetic%2520Context-Aware%250A%2520%2520Responses%2520through%2520Reasoning%2520MLLMs%26entry.906535625%3DZheng%2520Qin%2520and%2520Ruobing%2520Zheng%2520and%2520Yabing%2520Wang%2520and%2520Tianqi%2520Li%2520and%2520Yi%2520Yuan%2520and%2520Jingdong%2520Chen%2520and%2520Le%2520Wang%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520show%2520immense%2520promise%2520for%250Aachieving%2520truly%2520human-like%2520interactions%252C%2520progress%2520is%2520hindered%2520by%2520the%2520lack%2520of%250Afine-grained%2520evaluation%2520frameworks%2520for%2520human-centered%2520scenarios%252C%2520encompassing%250Aboth%2520the%2520understanding%2520of%2520complex%2520human%2520intentions%2520and%2520the%2520provision%2520of%250Aempathetic%252C%2520context-aware%2520responses.%2520Here%2520we%2520introduce%2520HumanSense%252C%2520a%250Acomprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520human-centered%2520perception%2520and%250Ainteraction%2520capabilities%2520of%2520MLLMs%252C%2520with%2520a%2520particular%2520focus%2520on%2520deep%250Aunderstanding%2520of%2520extended%2520multimodal%2520contexts%2520and%2520the%2520formulation%2520of%2520rational%250Afeedback.%2520Our%2520evaluation%2520reveals%2520that%2520leading%2520MLLMs%2520still%2520have%2520considerable%250Aroom%2520for%2520improvement%252C%2520particularly%2520for%2520advanced%2520interaction-oriented%2520tasks.%250ASupplementing%2520visual%2520input%2520with%2520audio%2520and%2520text%2520information%2520yields%2520substantial%250Aimprovements%252C%2520and%2520Omni-modal%2520models%2520show%2520advantages%2520on%2520these%2520tasks.%250AFurthermore%252C%2520we%2520argue%2520that%2520appropriate%2520feedback%2520stems%2520from%2520a%2520contextual%250Aanalysis%2520of%2520the%2520interlocutor%2527s%2520needs%2520and%2520emotions%252C%2520with%2520reasoning%2520ability%250Aserving%2520as%2520the%2520key%2520to%2520unlocking%2520it.%2520Accordingly%252C%2520we%2520employ%2520a%2520multi-stage%252C%250Amodality-progressive%2520reinforcement%2520learning%2520to%2520enhance%2520the%2520reasoning%2520abilities%250Aof%2520an%2520Omni%2520model%252C%2520achieving%2520substantial%2520gains%2520on%2520evaluation%2520results.%250AAdditionally%252C%2520we%2520observe%2520that%2520successful%2520reasoning%2520processes%2520exhibit%2520highly%250Aconsistent%2520thought%2520patterns.%2520By%2520designing%2520corresponding%2520prompts%252C%2520we%2520also%250Aenhance%2520the%2520performance%2520of%2520non-reasoning%2520models%2520in%2520a%2520training-free%2520manner.%250AProject%2520page%253A%250A%255Ctextcolor%257Bbrightpink%257Dhttps%253A//digital-avatar.github.io/ai/HumanSense/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanSense%3A%20From%20Multimodal%20Perception%20to%20Empathetic%20Context-Aware%0A%20%20Responses%20through%20Reasoning%20MLLMs&entry.906535625=Zheng%20Qin%20and%20Ruobing%20Zheng%20and%20Yabing%20Wang%20and%20Tianqi%20Li%20and%20Yi%20Yuan%20and%20Jingdong%20Chen%20and%20Le%20Wang&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20show%20immense%20promise%20for%0Aachieving%20truly%20human-like%20interactions%2C%20progress%20is%20hindered%20by%20the%20lack%20of%0Afine-grained%20evaluation%20frameworks%20for%20human-centered%20scenarios%2C%20encompassing%0Aboth%20the%20understanding%20of%20complex%20human%20intentions%20and%20the%20provision%20of%0Aempathetic%2C%20context-aware%20responses.%20Here%20we%20introduce%20HumanSense%2C%20a%0Acomprehensive%20benchmark%20designed%20to%20evaluate%20the%20human-centered%20perception%20and%0Ainteraction%20capabilities%20of%20MLLMs%2C%20with%20a%20particular%20focus%20on%20deep%0Aunderstanding%20of%20extended%20multimodal%20contexts%20and%20the%20formulation%20of%20rational%0Afeedback.%20Our%20evaluation%20reveals%20that%20leading%20MLLMs%20still%20have%20considerable%0Aroom%20for%20improvement%2C%20particularly%20for%20advanced%20interaction-oriented%20tasks.%0ASupplementing%20visual%20input%20with%20audio%20and%20text%20information%20yields%20substantial%0Aimprovements%2C%20and%20Omni-modal%20models%20show%20advantages%20on%20these%20tasks.%0AFurthermore%2C%20we%20argue%20that%20appropriate%20feedback%20stems%20from%20a%20contextual%0Aanalysis%20of%20the%20interlocutor%27s%20needs%20and%20emotions%2C%20with%20reasoning%20ability%0Aserving%20as%20the%20key%20to%20unlocking%20it.%20Accordingly%2C%20we%20employ%20a%20multi-stage%2C%0Amodality-progressive%20reinforcement%20learning%20to%20enhance%20the%20reasoning%20abilities%0Aof%20an%20Omni%20model%2C%20achieving%20substantial%20gains%20on%20evaluation%20results.%0AAdditionally%2C%20we%20observe%20that%20successful%20reasoning%20processes%20exhibit%20highly%0Aconsistent%20thought%20patterns.%20By%20designing%20corresponding%20prompts%2C%20we%20also%0Aenhance%20the%20performance%20of%20non-reasoning%20models%20in%20a%20training-free%20manner.%0AProject%20page%3A%0A%5Ctextcolor%7Bbrightpink%7Dhttps%3A//digital-avatar.github.io/ai/HumanSense/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.10576v2&entry.124074799=Read"},
{"title": "Tune My Adam, Please!", "author": "Theodoros Athanasiadis and Steven Adriaensen and Samuel M\u00fcller and Frank Hutter", "abstract": "  The Adam optimizer remains one of the most widely used optimizers in deep\nlearning, and effectively tuning its hyperparameters is key to optimizing\nperformance. However, tuning can be tedious and costly. Freeze-thaw Bayesian\nOptimization (BO) is a recent promising approach for low-budget hyperparameter\ntuning, but is limited by generic surrogates without prior knowledge of how\nhyperparameters affect learning. We propose Adam-PFN, a new surrogate model for\nFreeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from\nTaskSet, together with a new learning curve augmentation method, CDF-augment,\nwhich artificially increases the number of available training examples. Our\napproach improves both learning curve extrapolation and accelerates\nhyperparameter optimization on TaskSet evaluation tasks, with strong\nperformance on out-of-distribution (OOD) tasks.\n", "link": "http://arxiv.org/abs/2508.19733v1", "date": "2025-08-27", "relevancy": 2.3304, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4683}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4655}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tune%20My%20Adam%2C%20Please%21&body=Title%3A%20Tune%20My%20Adam%2C%20Please%21%0AAuthor%3A%20Theodoros%20Athanasiadis%20and%20Steven%20Adriaensen%20and%20Samuel%20M%C3%BCller%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20The%20Adam%20optimizer%20remains%20one%20of%20the%20most%20widely%20used%20optimizers%20in%20deep%0Alearning%2C%20and%20effectively%20tuning%20its%20hyperparameters%20is%20key%20to%20optimizing%0Aperformance.%20However%2C%20tuning%20can%20be%20tedious%20and%20costly.%20Freeze-thaw%20Bayesian%0AOptimization%20%28BO%29%20is%20a%20recent%20promising%20approach%20for%20low-budget%20hyperparameter%0Atuning%2C%20but%20is%20limited%20by%20generic%20surrogates%20without%20prior%20knowledge%20of%20how%0Ahyperparameters%20affect%20learning.%20We%20propose%20Adam-PFN%2C%20a%20new%20surrogate%20model%20for%0AFreeze-thaw%20BO%20of%20Adam%27s%20hyperparameters%2C%20pre-trained%20on%20learning%20curves%20from%0ATaskSet%2C%20together%20with%20a%20new%20learning%20curve%20augmentation%20method%2C%20CDF-augment%2C%0Awhich%20artificially%20increases%20the%20number%20of%20available%20training%20examples.%20Our%0Aapproach%20improves%20both%20learning%20curve%20extrapolation%20and%20accelerates%0Ahyperparameter%20optimization%20on%20TaskSet%20evaluation%20tasks%2C%20with%20strong%0Aperformance%20on%20out-of-distribution%20%28OOD%29%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTune%2520My%2520Adam%252C%2520Please%2521%26entry.906535625%3DTheodoros%2520Athanasiadis%2520and%2520Steven%2520Adriaensen%2520and%2520Samuel%2520M%25C3%25BCller%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520The%2520Adam%2520optimizer%2520remains%2520one%2520of%2520the%2520most%2520widely%2520used%2520optimizers%2520in%2520deep%250Alearning%252C%2520and%2520effectively%2520tuning%2520its%2520hyperparameters%2520is%2520key%2520to%2520optimizing%250Aperformance.%2520However%252C%2520tuning%2520can%2520be%2520tedious%2520and%2520costly.%2520Freeze-thaw%2520Bayesian%250AOptimization%2520%2528BO%2529%2520is%2520a%2520recent%2520promising%2520approach%2520for%2520low-budget%2520hyperparameter%250Atuning%252C%2520but%2520is%2520limited%2520by%2520generic%2520surrogates%2520without%2520prior%2520knowledge%2520of%2520how%250Ahyperparameters%2520affect%2520learning.%2520We%2520propose%2520Adam-PFN%252C%2520a%2520new%2520surrogate%2520model%2520for%250AFreeze-thaw%2520BO%2520of%2520Adam%2527s%2520hyperparameters%252C%2520pre-trained%2520on%2520learning%2520curves%2520from%250ATaskSet%252C%2520together%2520with%2520a%2520new%2520learning%2520curve%2520augmentation%2520method%252C%2520CDF-augment%252C%250Awhich%2520artificially%2520increases%2520the%2520number%2520of%2520available%2520training%2520examples.%2520Our%250Aapproach%2520improves%2520both%2520learning%2520curve%2520extrapolation%2520and%2520accelerates%250Ahyperparameter%2520optimization%2520on%2520TaskSet%2520evaluation%2520tasks%252C%2520with%2520strong%250Aperformance%2520on%2520out-of-distribution%2520%2528OOD%2529%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tune%20My%20Adam%2C%20Please%21&entry.906535625=Theodoros%20Athanasiadis%20and%20Steven%20Adriaensen%20and%20Samuel%20M%C3%BCller%20and%20Frank%20Hutter&entry.1292438233=%20%20The%20Adam%20optimizer%20remains%20one%20of%20the%20most%20widely%20used%20optimizers%20in%20deep%0Alearning%2C%20and%20effectively%20tuning%20its%20hyperparameters%20is%20key%20to%20optimizing%0Aperformance.%20However%2C%20tuning%20can%20be%20tedious%20and%20costly.%20Freeze-thaw%20Bayesian%0AOptimization%20%28BO%29%20is%20a%20recent%20promising%20approach%20for%20low-budget%20hyperparameter%0Atuning%2C%20but%20is%20limited%20by%20generic%20surrogates%20without%20prior%20knowledge%20of%20how%0Ahyperparameters%20affect%20learning.%20We%20propose%20Adam-PFN%2C%20a%20new%20surrogate%20model%20for%0AFreeze-thaw%20BO%20of%20Adam%27s%20hyperparameters%2C%20pre-trained%20on%20learning%20curves%20from%0ATaskSet%2C%20together%20with%20a%20new%20learning%20curve%20augmentation%20method%2C%20CDF-augment%2C%0Awhich%20artificially%20increases%20the%20number%20of%20available%20training%20examples.%20Our%0Aapproach%20improves%20both%20learning%20curve%20extrapolation%20and%20accelerates%0Ahyperparameter%20optimization%20on%20TaskSet%20evaluation%20tasks%2C%20with%20strong%0Aperformance%20on%20out-of-distribution%20%28OOD%29%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19733v1&entry.124074799=Read"},
{"title": "Pruning Strategies for Backdoor Defense in LLMs", "author": "Santosh Chapagain and Shah Muhammad Hamdi and Soukaina Filali Boubrahimi", "abstract": "  Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks.\n", "link": "http://arxiv.org/abs/2508.20032v1", "date": "2025-08-27", "relevancy": 2.3264, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20Strategies%20for%20Backdoor%20Defense%20in%20LLMs&body=Title%3A%20Pruning%20Strategies%20for%20Backdoor%20Defense%20in%20LLMs%0AAuthor%3A%20Santosh%20Chapagain%20and%20Shah%20Muhammad%20Hamdi%20and%20Soukaina%20Filali%20Boubrahimi%0AAbstract%3A%20%20%20Backdoor%20attacks%20are%20a%20significant%20threat%20to%20the%20performance%20and%20integrity%20of%0Apre-trained%20language%20models.%20Although%20such%20models%20are%20routinely%20fine-tuned%20for%0Adownstream%20NLP%20tasks%2C%20recent%20work%20shows%20they%20remain%20vulnerable%20to%20backdoor%0Aattacks%20that%20survive%20vanilla%20fine-tuning.%20These%20attacks%20are%20difficult%20to%20defend%0Abecause%20end%20users%20typically%20lack%20knowledge%20of%20the%20attack%20triggers.%20Such%20attacks%0Aconsist%20of%20stealthy%20malicious%20triggers%20introduced%20through%20subtle%20syntactic%20or%0Astylistic%20manipulations%2C%20which%20can%20bypass%20traditional%20detection%20and%20remain%20in%0Athe%20model%2C%20making%20post-hoc%20purification%20essential.%20In%20this%20study%2C%20we%20explore%0Awhether%20attention-head%20pruning%20can%20mitigate%20these%20threats%20without%20any%20knowledge%0Aof%20the%20trigger%20or%20access%20to%20a%20clean%20reference%20model.%20To%20this%20end%2C%20we%20design%20and%0Aimplement%20six%20pruning-based%20strategies%3A%20%28i%29%20gradient-based%20pruning%2C%20%28ii%29%0Alayer-wise%20variance%20pruning%2C%20%28iii%29%20gradient-based%20pruning%20with%20structured%20L1/L2%0Asparsification%2C%20%28iv%29%20randomized%20ensemble%20pruning%2C%20%28v%29%0Areinforcement-learning-guided%20pruning%2C%20and%20%28vi%29%20Bayesian%20uncertainty%20pruning.%0AEach%20method%20iteratively%20removes%20the%20least%20informative%20heads%20while%20monitoring%0Avalidation%20accuracy%20to%20avoid%20over-pruning.%20Experimental%20evaluation%20shows%20that%0Agradient-based%20pruning%20performs%20best%20while%20defending%20the%20syntactic%20triggers%2C%0Awhereas%20reinforcement%20learning%20and%20Bayesian%20pruning%20better%20withstand%20stylistic%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520Strategies%2520for%2520Backdoor%2520Defense%2520in%2520LLMs%26entry.906535625%3DSantosh%2520Chapagain%2520and%2520Shah%2520Muhammad%2520Hamdi%2520and%2520Soukaina%2520Filali%2520Boubrahimi%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520are%2520a%2520significant%2520threat%2520to%2520the%2520performance%2520and%2520integrity%2520of%250Apre-trained%2520language%2520models.%2520Although%2520such%2520models%2520are%2520routinely%2520fine-tuned%2520for%250Adownstream%2520NLP%2520tasks%252C%2520recent%2520work%2520shows%2520they%2520remain%2520vulnerable%2520to%2520backdoor%250Aattacks%2520that%2520survive%2520vanilla%2520fine-tuning.%2520These%2520attacks%2520are%2520difficult%2520to%2520defend%250Abecause%2520end%2520users%2520typically%2520lack%2520knowledge%2520of%2520the%2520attack%2520triggers.%2520Such%2520attacks%250Aconsist%2520of%2520stealthy%2520malicious%2520triggers%2520introduced%2520through%2520subtle%2520syntactic%2520or%250Astylistic%2520manipulations%252C%2520which%2520can%2520bypass%2520traditional%2520detection%2520and%2520remain%2520in%250Athe%2520model%252C%2520making%2520post-hoc%2520purification%2520essential.%2520In%2520this%2520study%252C%2520we%2520explore%250Awhether%2520attention-head%2520pruning%2520can%2520mitigate%2520these%2520threats%2520without%2520any%2520knowledge%250Aof%2520the%2520trigger%2520or%2520access%2520to%2520a%2520clean%2520reference%2520model.%2520To%2520this%2520end%252C%2520we%2520design%2520and%250Aimplement%2520six%2520pruning-based%2520strategies%253A%2520%2528i%2529%2520gradient-based%2520pruning%252C%2520%2528ii%2529%250Alayer-wise%2520variance%2520pruning%252C%2520%2528iii%2529%2520gradient-based%2520pruning%2520with%2520structured%2520L1/L2%250Asparsification%252C%2520%2528iv%2529%2520randomized%2520ensemble%2520pruning%252C%2520%2528v%2529%250Areinforcement-learning-guided%2520pruning%252C%2520and%2520%2528vi%2529%2520Bayesian%2520uncertainty%2520pruning.%250AEach%2520method%2520iteratively%2520removes%2520the%2520least%2520informative%2520heads%2520while%2520monitoring%250Avalidation%2520accuracy%2520to%2520avoid%2520over-pruning.%2520Experimental%2520evaluation%2520shows%2520that%250Agradient-based%2520pruning%2520performs%2520best%2520while%2520defending%2520the%2520syntactic%2520triggers%252C%250Awhereas%2520reinforcement%2520learning%2520and%2520Bayesian%2520pruning%2520better%2520withstand%2520stylistic%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20Strategies%20for%20Backdoor%20Defense%20in%20LLMs&entry.906535625=Santosh%20Chapagain%20and%20Shah%20Muhammad%20Hamdi%20and%20Soukaina%20Filali%20Boubrahimi&entry.1292438233=%20%20Backdoor%20attacks%20are%20a%20significant%20threat%20to%20the%20performance%20and%20integrity%20of%0Apre-trained%20language%20models.%20Although%20such%20models%20are%20routinely%20fine-tuned%20for%0Adownstream%20NLP%20tasks%2C%20recent%20work%20shows%20they%20remain%20vulnerable%20to%20backdoor%0Aattacks%20that%20survive%20vanilla%20fine-tuning.%20These%20attacks%20are%20difficult%20to%20defend%0Abecause%20end%20users%20typically%20lack%20knowledge%20of%20the%20attack%20triggers.%20Such%20attacks%0Aconsist%20of%20stealthy%20malicious%20triggers%20introduced%20through%20subtle%20syntactic%20or%0Astylistic%20manipulations%2C%20which%20can%20bypass%20traditional%20detection%20and%20remain%20in%0Athe%20model%2C%20making%20post-hoc%20purification%20essential.%20In%20this%20study%2C%20we%20explore%0Awhether%20attention-head%20pruning%20can%20mitigate%20these%20threats%20without%20any%20knowledge%0Aof%20the%20trigger%20or%20access%20to%20a%20clean%20reference%20model.%20To%20this%20end%2C%20we%20design%20and%0Aimplement%20six%20pruning-based%20strategies%3A%20%28i%29%20gradient-based%20pruning%2C%20%28ii%29%0Alayer-wise%20variance%20pruning%2C%20%28iii%29%20gradient-based%20pruning%20with%20structured%20L1/L2%0Asparsification%2C%20%28iv%29%20randomized%20ensemble%20pruning%2C%20%28v%29%0Areinforcement-learning-guided%20pruning%2C%20and%20%28vi%29%20Bayesian%20uncertainty%20pruning.%0AEach%20method%20iteratively%20removes%20the%20least%20informative%20heads%20while%20monitoring%0Avalidation%20accuracy%20to%20avoid%20over-pruning.%20Experimental%20evaluation%20shows%20that%0Agradient-based%20pruning%20performs%20best%20while%20defending%20the%20syntactic%20triggers%2C%0Awhereas%20reinforcement%20learning%20and%20Bayesian%20pruning%20better%20withstand%20stylistic%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20032v1&entry.124074799=Read"},
{"title": "Towards Diagnostic Quality Flat-Panel Detector CT Imaging Using\n  Diffusion Models", "author": "H\u00e9l\u00e8ne Corbaz and Anh Nguyen and Victor Schulze-Zachau and Paul Friedrich and Alicia Durrer and Florentin Bieder and Philippe C. Cattin and Marios N Psychogios", "abstract": "  Patients undergoing a mechanical thrombectomy procedure usually have a\nmulti-detector CT (MDCT) scan before and after the intervention. The image\nquality of the flat panel detector CT (FDCT) present in the intervention room\nis generally much lower than that of a MDCT due to significant artifacts.\nHowever, using only FDCT images could improve patient management as the patient\nwould not need to be moved to the MDCT room. Several studies have evaluated the\npotential use of FDCT imaging alone and the time that could be saved by\nacquiring the images before and/or after the intervention only with the FDCT.\nThis study proposes using a denoising diffusion probabilistic model (DDPM) to\nimprove the image quality of FDCT scans, making them comparable to MDCT scans.\nClinicans evaluated FDCT, MDCT, and our model's predictions for diagnostic\npurposes using a questionnaire. The DDPM eliminated most artifacts and improved\nanatomical visibility without reducing bleeding detection, provided that the\ninput FDCT image quality is not too low. Our code can be found on github.\n", "link": "http://arxiv.org/abs/2508.16252v2", "date": "2025-08-27", "relevancy": 2.3263, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5823}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5823}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Diagnostic%20Quality%20Flat-Panel%20Detector%20CT%20Imaging%20Using%0A%20%20Diffusion%20Models&body=Title%3A%20Towards%20Diagnostic%20Quality%20Flat-Panel%20Detector%20CT%20Imaging%20Using%0A%20%20Diffusion%20Models%0AAuthor%3A%20H%C3%A9l%C3%A8ne%20Corbaz%20and%20Anh%20Nguyen%20and%20Victor%20Schulze-Zachau%20and%20Paul%20Friedrich%20and%20Alicia%20Durrer%20and%20Florentin%20Bieder%20and%20Philippe%20C.%20Cattin%20and%20Marios%20N%20Psychogios%0AAbstract%3A%20%20%20Patients%20undergoing%20a%20mechanical%20thrombectomy%20procedure%20usually%20have%20a%0Amulti-detector%20CT%20%28MDCT%29%20scan%20before%20and%20after%20the%20intervention.%20The%20image%0Aquality%20of%20the%20flat%20panel%20detector%20CT%20%28FDCT%29%20present%20in%20the%20intervention%20room%0Ais%20generally%20much%20lower%20than%20that%20of%20a%20MDCT%20due%20to%20significant%20artifacts.%0AHowever%2C%20using%20only%20FDCT%20images%20could%20improve%20patient%20management%20as%20the%20patient%0Awould%20not%20need%20to%20be%20moved%20to%20the%20MDCT%20room.%20Several%20studies%20have%20evaluated%20the%0Apotential%20use%20of%20FDCT%20imaging%20alone%20and%20the%20time%20that%20could%20be%20saved%20by%0Aacquiring%20the%20images%20before%20and/or%20after%20the%20intervention%20only%20with%20the%20FDCT.%0AThis%20study%20proposes%20using%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%20to%0Aimprove%20the%20image%20quality%20of%20FDCT%20scans%2C%20making%20them%20comparable%20to%20MDCT%20scans.%0AClinicans%20evaluated%20FDCT%2C%20MDCT%2C%20and%20our%20model%27s%20predictions%20for%20diagnostic%0Apurposes%20using%20a%20questionnaire.%20The%20DDPM%20eliminated%20most%20artifacts%20and%20improved%0Aanatomical%20visibility%20without%20reducing%20bleeding%20detection%2C%20provided%20that%20the%0Ainput%20FDCT%20image%20quality%20is%20not%20too%20low.%20Our%20code%20can%20be%20found%20on%20github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Diagnostic%2520Quality%2520Flat-Panel%2520Detector%2520CT%2520Imaging%2520Using%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DH%25C3%25A9l%25C3%25A8ne%2520Corbaz%2520and%2520Anh%2520Nguyen%2520and%2520Victor%2520Schulze-Zachau%2520and%2520Paul%2520Friedrich%2520and%2520Alicia%2520Durrer%2520and%2520Florentin%2520Bieder%2520and%2520Philippe%2520C.%2520Cattin%2520and%2520Marios%2520N%2520Psychogios%26entry.1292438233%3D%2520%2520Patients%2520undergoing%2520a%2520mechanical%2520thrombectomy%2520procedure%2520usually%2520have%2520a%250Amulti-detector%2520CT%2520%2528MDCT%2529%2520scan%2520before%2520and%2520after%2520the%2520intervention.%2520The%2520image%250Aquality%2520of%2520the%2520flat%2520panel%2520detector%2520CT%2520%2528FDCT%2529%2520present%2520in%2520the%2520intervention%2520room%250Ais%2520generally%2520much%2520lower%2520than%2520that%2520of%2520a%2520MDCT%2520due%2520to%2520significant%2520artifacts.%250AHowever%252C%2520using%2520only%2520FDCT%2520images%2520could%2520improve%2520patient%2520management%2520as%2520the%2520patient%250Awould%2520not%2520need%2520to%2520be%2520moved%2520to%2520the%2520MDCT%2520room.%2520Several%2520studies%2520have%2520evaluated%2520the%250Apotential%2520use%2520of%2520FDCT%2520imaging%2520alone%2520and%2520the%2520time%2520that%2520could%2520be%2520saved%2520by%250Aacquiring%2520the%2520images%2520before%2520and/or%2520after%2520the%2520intervention%2520only%2520with%2520the%2520FDCT.%250AThis%2520study%2520proposes%2520using%2520a%2520denoising%2520diffusion%2520probabilistic%2520model%2520%2528DDPM%2529%2520to%250Aimprove%2520the%2520image%2520quality%2520of%2520FDCT%2520scans%252C%2520making%2520them%2520comparable%2520to%2520MDCT%2520scans.%250AClinicans%2520evaluated%2520FDCT%252C%2520MDCT%252C%2520and%2520our%2520model%2527s%2520predictions%2520for%2520diagnostic%250Apurposes%2520using%2520a%2520questionnaire.%2520The%2520DDPM%2520eliminated%2520most%2520artifacts%2520and%2520improved%250Aanatomical%2520visibility%2520without%2520reducing%2520bleeding%2520detection%252C%2520provided%2520that%2520the%250Ainput%2520FDCT%2520image%2520quality%2520is%2520not%2520too%2520low.%2520Our%2520code%2520can%2520be%2520found%2520on%2520github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Diagnostic%20Quality%20Flat-Panel%20Detector%20CT%20Imaging%20Using%0A%20%20Diffusion%20Models&entry.906535625=H%C3%A9l%C3%A8ne%20Corbaz%20and%20Anh%20Nguyen%20and%20Victor%20Schulze-Zachau%20and%20Paul%20Friedrich%20and%20Alicia%20Durrer%20and%20Florentin%20Bieder%20and%20Philippe%20C.%20Cattin%20and%20Marios%20N%20Psychogios&entry.1292438233=%20%20Patients%20undergoing%20a%20mechanical%20thrombectomy%20procedure%20usually%20have%20a%0Amulti-detector%20CT%20%28MDCT%29%20scan%20before%20and%20after%20the%20intervention.%20The%20image%0Aquality%20of%20the%20flat%20panel%20detector%20CT%20%28FDCT%29%20present%20in%20the%20intervention%20room%0Ais%20generally%20much%20lower%20than%20that%20of%20a%20MDCT%20due%20to%20significant%20artifacts.%0AHowever%2C%20using%20only%20FDCT%20images%20could%20improve%20patient%20management%20as%20the%20patient%0Awould%20not%20need%20to%20be%20moved%20to%20the%20MDCT%20room.%20Several%20studies%20have%20evaluated%20the%0Apotential%20use%20of%20FDCT%20imaging%20alone%20and%20the%20time%20that%20could%20be%20saved%20by%0Aacquiring%20the%20images%20before%20and/or%20after%20the%20intervention%20only%20with%20the%20FDCT.%0AThis%20study%20proposes%20using%20a%20denoising%20diffusion%20probabilistic%20model%20%28DDPM%29%20to%0Aimprove%20the%20image%20quality%20of%20FDCT%20scans%2C%20making%20them%20comparable%20to%20MDCT%20scans.%0AClinicans%20evaluated%20FDCT%2C%20MDCT%2C%20and%20our%20model%27s%20predictions%20for%20diagnostic%0Apurposes%20using%20a%20questionnaire.%20The%20DDPM%20eliminated%20most%20artifacts%20and%20improved%0Aanatomical%20visibility%20without%20reducing%20bleeding%20detection%2C%20provided%20that%20the%0Ainput%20FDCT%20image%20quality%20is%20not%20too%20low.%20Our%20code%20can%20be%20found%20on%20github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16252v2&entry.124074799=Read"},
{"title": "11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with\n  Cognitive-Inspired Analysis", "author": "Chengzu Li and Wenshan Wu and Huanyu Zhang and Qingtao Li and Zeyu Gao and Yan Xia and Jos\u00e9 Hern\u00e1ndez-Orallo and Ivan Vuli\u0107 and Furu Wei", "abstract": "  For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.\n", "link": "http://arxiv.org/abs/2508.20068v1", "date": "2025-08-27", "relevancy": 2.3255, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5828}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%2011Plus-Bench%3A%20Demystifying%20Multimodal%20LLM%20Spatial%20Reasoning%20with%0A%20%20Cognitive-Inspired%20Analysis&body=Title%3A%2011Plus-Bench%3A%20Demystifying%20Multimodal%20LLM%20Spatial%20Reasoning%20with%0A%20%20Cognitive-Inspired%20Analysis%0AAuthor%3A%20Chengzu%20Li%20and%20Wenshan%20Wu%20and%20Huanyu%20Zhang%20and%20Qingtao%20Li%20and%20Zeyu%20Gao%20and%20Yan%20Xia%20and%20Jos%C3%A9%20Hern%C3%A1ndez-Orallo%20and%20Ivan%20Vuli%C4%87%20and%20Furu%20Wei%0AAbstract%3A%20%20%20For%20human%20cognitive%20process%2C%20spatial%20reasoning%20and%20perception%20are%20closely%0Aentangled%2C%20yet%20the%20nature%20of%20this%20interplay%20remains%20underexplored%20in%20the%0Aevaluation%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%20recent%20MLLM%0Aadvancements%20show%20impressive%20performance%20on%20reasoning%2C%20their%20capacity%20for%0Ahuman-like%20spatial%20cognition%20remains%20an%20open%20question.%20In%20this%20work%2C%20we%0Aintroduce%20a%20systematic%20evaluation%20framework%20to%20assess%20the%20spatial%20reasoning%0Aabilities%20of%20state-of-the-art%20MLLMs%20relative%20to%20human%20performance.%20Central%20to%0Aour%20work%20is%2011Plus-Bench%2C%20a%20high-quality%20benchmark%20derived%20from%20realistic%0Astandardized%20spatial%20aptitude%20tests.%2011Plus-Bench%20also%20features%20fine-grained%0Aexpert%20annotations%20of%20both%20perceptual%20complexity%20and%20reasoning%20process%2C%0Aenabling%20detailed%20instance-level%20analysis%20of%20model%20behavior.%20Through%20extensive%0Aexperiments%20across%2014%20MLLMs%20and%20human%20evaluation%2C%20we%20find%20that%20current%20MLLMs%0Aexhibit%20early%20signs%20of%20spatial%20cognition.%20Despite%20a%20large%20performance%20gap%0Acompared%20to%20humans%2C%20MLLMs%27%20cognitive%20profiles%20resemble%20those%20of%20humans%20in%20that%0Acognitive%20effort%20correlates%20strongly%20with%20reasoning-related%20complexity.%0AHowever%2C%20instance-level%20performance%20in%20MLLMs%20remains%20largely%20random%2C%20whereas%0Ahuman%20correctness%20is%20highly%20predictable%20and%20shaped%20by%20abstract%20pattern%0Acomplexity.%20These%20findings%20highlight%20both%20emerging%20capabilities%20and%20limitations%0Ain%20current%20MLLMs%27%20spatial%20reasoning%20capabilities%20and%20provide%20actionable%0Ainsights%20for%20advancing%20model%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D11Plus-Bench%253A%2520Demystifying%2520Multimodal%2520LLM%2520Spatial%2520Reasoning%2520with%250A%2520%2520Cognitive-Inspired%2520Analysis%26entry.906535625%3DChengzu%2520Li%2520and%2520Wenshan%2520Wu%2520and%2520Huanyu%2520Zhang%2520and%2520Qingtao%2520Li%2520and%2520Zeyu%2520Gao%2520and%2520Yan%2520Xia%2520and%2520Jos%25C3%25A9%2520Hern%25C3%25A1ndez-Orallo%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520For%2520human%2520cognitive%2520process%252C%2520spatial%2520reasoning%2520and%2520perception%2520are%2520closely%250Aentangled%252C%2520yet%2520the%2520nature%2520of%2520this%2520interplay%2520remains%2520underexplored%2520in%2520the%250Aevaluation%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520While%2520recent%2520MLLM%250Aadvancements%2520show%2520impressive%2520performance%2520on%2520reasoning%252C%2520their%2520capacity%2520for%250Ahuman-like%2520spatial%2520cognition%2520remains%2520an%2520open%2520question.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520systematic%2520evaluation%2520framework%2520to%2520assess%2520the%2520spatial%2520reasoning%250Aabilities%2520of%2520state-of-the-art%2520MLLMs%2520relative%2520to%2520human%2520performance.%2520Central%2520to%250Aour%2520work%2520is%252011Plus-Bench%252C%2520a%2520high-quality%2520benchmark%2520derived%2520from%2520realistic%250Astandardized%2520spatial%2520aptitude%2520tests.%252011Plus-Bench%2520also%2520features%2520fine-grained%250Aexpert%2520annotations%2520of%2520both%2520perceptual%2520complexity%2520and%2520reasoning%2520process%252C%250Aenabling%2520detailed%2520instance-level%2520analysis%2520of%2520model%2520behavior.%2520Through%2520extensive%250Aexperiments%2520across%252014%2520MLLMs%2520and%2520human%2520evaluation%252C%2520we%2520find%2520that%2520current%2520MLLMs%250Aexhibit%2520early%2520signs%2520of%2520spatial%2520cognition.%2520Despite%2520a%2520large%2520performance%2520gap%250Acompared%2520to%2520humans%252C%2520MLLMs%2527%2520cognitive%2520profiles%2520resemble%2520those%2520of%2520humans%2520in%2520that%250Acognitive%2520effort%2520correlates%2520strongly%2520with%2520reasoning-related%2520complexity.%250AHowever%252C%2520instance-level%2520performance%2520in%2520MLLMs%2520remains%2520largely%2520random%252C%2520whereas%250Ahuman%2520correctness%2520is%2520highly%2520predictable%2520and%2520shaped%2520by%2520abstract%2520pattern%250Acomplexity.%2520These%2520findings%2520highlight%2520both%2520emerging%2520capabilities%2520and%2520limitations%250Ain%2520current%2520MLLMs%2527%2520spatial%2520reasoning%2520capabilities%2520and%2520provide%2520actionable%250Ainsights%2520for%2520advancing%2520model%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=11Plus-Bench%3A%20Demystifying%20Multimodal%20LLM%20Spatial%20Reasoning%20with%0A%20%20Cognitive-Inspired%20Analysis&entry.906535625=Chengzu%20Li%20and%20Wenshan%20Wu%20and%20Huanyu%20Zhang%20and%20Qingtao%20Li%20and%20Zeyu%20Gao%20and%20Yan%20Xia%20and%20Jos%C3%A9%20Hern%C3%A1ndez-Orallo%20and%20Ivan%20Vuli%C4%87%20and%20Furu%20Wei&entry.1292438233=%20%20For%20human%20cognitive%20process%2C%20spatial%20reasoning%20and%20perception%20are%20closely%0Aentangled%2C%20yet%20the%20nature%20of%20this%20interplay%20remains%20underexplored%20in%20the%0Aevaluation%20of%20multimodal%20large%20language%20models%20%28MLLMs%29.%20While%20recent%20MLLM%0Aadvancements%20show%20impressive%20performance%20on%20reasoning%2C%20their%20capacity%20for%0Ahuman-like%20spatial%20cognition%20remains%20an%20open%20question.%20In%20this%20work%2C%20we%0Aintroduce%20a%20systematic%20evaluation%20framework%20to%20assess%20the%20spatial%20reasoning%0Aabilities%20of%20state-of-the-art%20MLLMs%20relative%20to%20human%20performance.%20Central%20to%0Aour%20work%20is%2011Plus-Bench%2C%20a%20high-quality%20benchmark%20derived%20from%20realistic%0Astandardized%20spatial%20aptitude%20tests.%2011Plus-Bench%20also%20features%20fine-grained%0Aexpert%20annotations%20of%20both%20perceptual%20complexity%20and%20reasoning%20process%2C%0Aenabling%20detailed%20instance-level%20analysis%20of%20model%20behavior.%20Through%20extensive%0Aexperiments%20across%2014%20MLLMs%20and%20human%20evaluation%2C%20we%20find%20that%20current%20MLLMs%0Aexhibit%20early%20signs%20of%20spatial%20cognition.%20Despite%20a%20large%20performance%20gap%0Acompared%20to%20humans%2C%20MLLMs%27%20cognitive%20profiles%20resemble%20those%20of%20humans%20in%20that%0Acognitive%20effort%20correlates%20strongly%20with%20reasoning-related%20complexity.%0AHowever%2C%20instance-level%20performance%20in%20MLLMs%20remains%20largely%20random%2C%20whereas%0Ahuman%20correctness%20is%20highly%20predictable%20and%20shaped%20by%20abstract%20pattern%0Acomplexity.%20These%20findings%20highlight%20both%20emerging%20capabilities%20and%20limitations%0Ain%20current%20MLLMs%27%20spatial%20reasoning%20capabilities%20and%20provide%20actionable%0Ainsights%20for%20advancing%20model%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20068v1&entry.124074799=Read"},
{"title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor\n  Fusion Navigation and Mapping", "author": "Hailiang Tang and Tisheng Zhang and Liqiang Wang and Xin Ding and Man Yuan and Zhiyu Xiang and Jujin Chen and Yuhan Bian and Shuangyan Liu and Yuqing Wang and Guan Wang and Xiaoji Niu", "abstract": "  Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset includes ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.\n", "link": "http://arxiv.org/abs/2508.11485v2", "date": "2025-08-27", "relevancy": 2.3166, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.607}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5624}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%0A%20%20Fusion%20Navigation%20and%20Mapping&body=Title%3A%20i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%0A%20%20Fusion%20Navigation%20and%20Mapping%0AAuthor%3A%20Hailiang%20Tang%20and%20Tisheng%20Zhang%20and%20Liqiang%20Wang%20and%20Xin%20Ding%20and%20Man%20Yuan%20and%20Zhiyu%20Xiang%20and%20Jujin%20Chen%20and%20Yuhan%20Bian%20and%20Shuangyan%20Liu%20and%20Yuqing%20Wang%20and%20Guan%20Wang%20and%20Xiaoji%20Niu%0AAbstract%3A%20%20%20Accurate%20and%20reliable%20navigation%20is%20crucial%20for%20autonomous%20unmanned%20ground%0Avehicle%20%28UGV%29.%20However%2C%20current%20UGV%20datasets%20fall%20short%20in%20meeting%20the%20demands%0Afor%20advancing%20navigation%20and%20mapping%20techniques%20due%20to%20limitations%20in%20sensor%0Aconfiguration%2C%20time%20synchronization%2C%20ground%20truth%2C%20and%20scenario%20diversity.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20i2Nav-Robot%2C%20a%20large-scale%20dataset%0Adesigned%20for%20multi-sensor%20fusion%20navigation%20and%20mapping%20in%20indoor-outdoor%0Aenvironments.%20We%20integrate%20multi-modal%20sensors%2C%20including%20the%20newest%20front-view%0Aand%20360-degree%20solid-state%20LiDARs%2C%204-dimensional%20%284D%29%20radar%2C%20stereo%20cameras%2C%0Aodometer%2C%20global%20navigation%20satellite%20system%20%28GNSS%29%20receiver%2C%20and%20inertial%0Ameasurement%20units%20%28IMU%29%20on%20an%20omnidirectional%20wheeled%20robot.%20Accurate%0Atimestamps%20are%20obtained%20through%20both%20online%20hardware%20synchronization%20and%0Aoffline%20calibration%20for%20all%20sensors.%20The%20dataset%20includes%20ten%20larger-scale%0Asequences%20covering%20diverse%20UGV%20operating%20scenarios%2C%20such%20as%20outdoor%20streets%2C%0Aand%20indoor%20parking%20lots%2C%20with%20a%20total%20length%20of%20about%2017060%20meters.%0AHigh-frequency%20ground%20truth%2C%20with%20centimeter-level%20accuracy%20for%20position%2C%20is%0Aderived%20from%20post-processing%20integrated%20navigation%20methods%20using%20a%0Anavigation-grade%20IMU.%20The%20proposed%20i2Nav-Robot%20dataset%20is%20evaluated%20by%20more%0Athan%20ten%20open-sourced%20multi-sensor%20fusion%20systems%2C%20and%20it%20has%20proven%20to%20have%0Asuperior%20data%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.11485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Di2Nav-Robot%253A%2520A%2520Large-Scale%2520Indoor-Outdoor%2520Robot%2520Dataset%2520for%2520Multi-Sensor%250A%2520%2520Fusion%2520Navigation%2520and%2520Mapping%26entry.906535625%3DHailiang%2520Tang%2520and%2520Tisheng%2520Zhang%2520and%2520Liqiang%2520Wang%2520and%2520Xin%2520Ding%2520and%2520Man%2520Yuan%2520and%2520Zhiyu%2520Xiang%2520and%2520Jujin%2520Chen%2520and%2520Yuhan%2520Bian%2520and%2520Shuangyan%2520Liu%2520and%2520Yuqing%2520Wang%2520and%2520Guan%2520Wang%2520and%2520Xiaoji%2520Niu%26entry.1292438233%3D%2520%2520Accurate%2520and%2520reliable%2520navigation%2520is%2520crucial%2520for%2520autonomous%2520unmanned%2520ground%250Avehicle%2520%2528UGV%2529.%2520However%252C%2520current%2520UGV%2520datasets%2520fall%2520short%2520in%2520meeting%2520the%2520demands%250Afor%2520advancing%2520navigation%2520and%2520mapping%2520techniques%2520due%2520to%2520limitations%2520in%2520sensor%250Aconfiguration%252C%2520time%2520synchronization%252C%2520ground%2520truth%252C%2520and%2520scenario%2520diversity.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520present%2520i2Nav-Robot%252C%2520a%2520large-scale%2520dataset%250Adesigned%2520for%2520multi-sensor%2520fusion%2520navigation%2520and%2520mapping%2520in%2520indoor-outdoor%250Aenvironments.%2520We%2520integrate%2520multi-modal%2520sensors%252C%2520including%2520the%2520newest%2520front-view%250Aand%2520360-degree%2520solid-state%2520LiDARs%252C%25204-dimensional%2520%25284D%2529%2520radar%252C%2520stereo%2520cameras%252C%250Aodometer%252C%2520global%2520navigation%2520satellite%2520system%2520%2528GNSS%2529%2520receiver%252C%2520and%2520inertial%250Ameasurement%2520units%2520%2528IMU%2529%2520on%2520an%2520omnidirectional%2520wheeled%2520robot.%2520Accurate%250Atimestamps%2520are%2520obtained%2520through%2520both%2520online%2520hardware%2520synchronization%2520and%250Aoffline%2520calibration%2520for%2520all%2520sensors.%2520The%2520dataset%2520includes%2520ten%2520larger-scale%250Asequences%2520covering%2520diverse%2520UGV%2520operating%2520scenarios%252C%2520such%2520as%2520outdoor%2520streets%252C%250Aand%2520indoor%2520parking%2520lots%252C%2520with%2520a%2520total%2520length%2520of%2520about%252017060%2520meters.%250AHigh-frequency%2520ground%2520truth%252C%2520with%2520centimeter-level%2520accuracy%2520for%2520position%252C%2520is%250Aderived%2520from%2520post-processing%2520integrated%2520navigation%2520methods%2520using%2520a%250Anavigation-grade%2520IMU.%2520The%2520proposed%2520i2Nav-Robot%2520dataset%2520is%2520evaluated%2520by%2520more%250Athan%2520ten%2520open-sourced%2520multi-sensor%2520fusion%2520systems%252C%2520and%2520it%2520has%2520proven%2520to%2520have%250Asuperior%2520data%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=i2Nav-Robot%3A%20A%20Large-Scale%20Indoor-Outdoor%20Robot%20Dataset%20for%20Multi-Sensor%0A%20%20Fusion%20Navigation%20and%20Mapping&entry.906535625=Hailiang%20Tang%20and%20Tisheng%20Zhang%20and%20Liqiang%20Wang%20and%20Xin%20Ding%20and%20Man%20Yuan%20and%20Zhiyu%20Xiang%20and%20Jujin%20Chen%20and%20Yuhan%20Bian%20and%20Shuangyan%20Liu%20and%20Yuqing%20Wang%20and%20Guan%20Wang%20and%20Xiaoji%20Niu&entry.1292438233=%20%20Accurate%20and%20reliable%20navigation%20is%20crucial%20for%20autonomous%20unmanned%20ground%0Avehicle%20%28UGV%29.%20However%2C%20current%20UGV%20datasets%20fall%20short%20in%20meeting%20the%20demands%0Afor%20advancing%20navigation%20and%20mapping%20techniques%20due%20to%20limitations%20in%20sensor%0Aconfiguration%2C%20time%20synchronization%2C%20ground%20truth%2C%20and%20scenario%20diversity.%20To%0Aaddress%20these%20challenges%2C%20we%20present%20i2Nav-Robot%2C%20a%20large-scale%20dataset%0Adesigned%20for%20multi-sensor%20fusion%20navigation%20and%20mapping%20in%20indoor-outdoor%0Aenvironments.%20We%20integrate%20multi-modal%20sensors%2C%20including%20the%20newest%20front-view%0Aand%20360-degree%20solid-state%20LiDARs%2C%204-dimensional%20%284D%29%20radar%2C%20stereo%20cameras%2C%0Aodometer%2C%20global%20navigation%20satellite%20system%20%28GNSS%29%20receiver%2C%20and%20inertial%0Ameasurement%20units%20%28IMU%29%20on%20an%20omnidirectional%20wheeled%20robot.%20Accurate%0Atimestamps%20are%20obtained%20through%20both%20online%20hardware%20synchronization%20and%0Aoffline%20calibration%20for%20all%20sensors.%20The%20dataset%20includes%20ten%20larger-scale%0Asequences%20covering%20diverse%20UGV%20operating%20scenarios%2C%20such%20as%20outdoor%20streets%2C%0Aand%20indoor%20parking%20lots%2C%20with%20a%20total%20length%20of%20about%2017060%20meters.%0AHigh-frequency%20ground%20truth%2C%20with%20centimeter-level%20accuracy%20for%20position%2C%20is%0Aderived%20from%20post-processing%20integrated%20navigation%20methods%20using%20a%0Anavigation-grade%20IMU.%20The%20proposed%20i2Nav-Robot%20dataset%20is%20evaluated%20by%20more%0Athan%20ten%20open-sourced%20multi-sensor%20fusion%20systems%2C%20and%20it%20has%20proven%20to%20have%0Asuperior%20data%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.11485v2&entry.124074799=Read"},
{"title": "FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for\n  Physics-Based High-Dynamic Humanoid Control", "author": "Tan Jing and Shiting Chen and Yangfan Li and Weisheng Xu and Renjing Xu", "abstract": "  Unified physics-based humanoid controllers are pivotal for robotics and\ncharacter animation, yet models that excel on gentle, everyday motions still\nstumble on explosive actions, hampering real-world deployment. We bridge this\ngap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),\nan end-to-end framework composed of frame-accelerated augmentation, a robust\nbase controller, and a residual mixture-of-experts (MoE). Frame-accelerated\naugmentation exposes the model to high-velocity pose changes by widening\ninter-frame gaps. The base controller reliably tracks everyday low-dynamic\nmotions, while the residual MoE adaptively allocates additional network\ncapacity to handle challenging high-dynamic actions, significantly enhancing\ntracking accuracy. In the absence of a public benchmark, we curate the\nHigh-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically\nplausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and\nlowers global mean per-joint position error by 14.6\\% relative to the baseline,\nwhile preserving near-perfect accuracy on low-dynamic motions. These results\nestablish FARM as a new baseline for high-dynamic humanoid control and\nintroduce the first open benchmark dedicated to this challenge. The code and\ndataset will be released at https://github.com/Colin-Jing/FARM.\n", "link": "http://arxiv.org/abs/2508.19926v1", "date": "2025-08-27", "relevancy": 2.3007, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6081}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5542}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FARM%3A%20Frame-Accelerated%20Augmentation%20and%20Residual%20Mixture-of-Experts%20for%0A%20%20Physics-Based%20High-Dynamic%20Humanoid%20Control&body=Title%3A%20FARM%3A%20Frame-Accelerated%20Augmentation%20and%20Residual%20Mixture-of-Experts%20for%0A%20%20Physics-Based%20High-Dynamic%20Humanoid%20Control%0AAuthor%3A%20Tan%20Jing%20and%20Shiting%20Chen%20and%20Yangfan%20Li%20and%20Weisheng%20Xu%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Unified%20physics-based%20humanoid%20controllers%20are%20pivotal%20for%20robotics%20and%0Acharacter%20animation%2C%20yet%20models%20that%20excel%20on%20gentle%2C%20everyday%20motions%20still%0Astumble%20on%20explosive%20actions%2C%20hampering%20real-world%20deployment.%20We%20bridge%20this%0Agap%20with%20FARM%20%28Frame-Accelerated%20Augmentation%20and%20Residual%20Mixture-of-Experts%29%2C%0Aan%20end-to-end%20framework%20composed%20of%20frame-accelerated%20augmentation%2C%20a%20robust%0Abase%20controller%2C%20and%20a%20residual%20mixture-of-experts%20%28MoE%29.%20Frame-accelerated%0Aaugmentation%20exposes%20the%20model%20to%20high-velocity%20pose%20changes%20by%20widening%0Ainter-frame%20gaps.%20The%20base%20controller%20reliably%20tracks%20everyday%20low-dynamic%0Amotions%2C%20while%20the%20residual%20MoE%20adaptively%20allocates%20additional%20network%0Acapacity%20to%20handle%20challenging%20high-dynamic%20actions%2C%20significantly%20enhancing%0Atracking%20accuracy.%20In%20the%20absence%20of%20a%20public%20benchmark%2C%20we%20curate%20the%0AHigh-Dynamic%20Humanoid%20Motion%20%28HDHM%29%20dataset%2C%20comprising%203593%20physically%0Aplausible%20clips.%20On%20HDHM%2C%20FARM%20reduces%20the%20tracking%20failure%20rate%20by%2042.8%5C%25%20and%0Alowers%20global%20mean%20per-joint%20position%20error%20by%2014.6%5C%25%20relative%20to%20the%20baseline%2C%0Awhile%20preserving%20near-perfect%20accuracy%20on%20low-dynamic%20motions.%20These%20results%0Aestablish%20FARM%20as%20a%20new%20baseline%20for%20high-dynamic%20humanoid%20control%20and%0Aintroduce%20the%20first%20open%20benchmark%20dedicated%20to%20this%20challenge.%20The%20code%20and%0Adataset%20will%20be%20released%20at%20https%3A//github.com/Colin-Jing/FARM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFARM%253A%2520Frame-Accelerated%2520Augmentation%2520and%2520Residual%2520Mixture-of-Experts%2520for%250A%2520%2520Physics-Based%2520High-Dynamic%2520Humanoid%2520Control%26entry.906535625%3DTan%2520Jing%2520and%2520Shiting%2520Chen%2520and%2520Yangfan%2520Li%2520and%2520Weisheng%2520Xu%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Unified%2520physics-based%2520humanoid%2520controllers%2520are%2520pivotal%2520for%2520robotics%2520and%250Acharacter%2520animation%252C%2520yet%2520models%2520that%2520excel%2520on%2520gentle%252C%2520everyday%2520motions%2520still%250Astumble%2520on%2520explosive%2520actions%252C%2520hampering%2520real-world%2520deployment.%2520We%2520bridge%2520this%250Agap%2520with%2520FARM%2520%2528Frame-Accelerated%2520Augmentation%2520and%2520Residual%2520Mixture-of-Experts%2529%252C%250Aan%2520end-to-end%2520framework%2520composed%2520of%2520frame-accelerated%2520augmentation%252C%2520a%2520robust%250Abase%2520controller%252C%2520and%2520a%2520residual%2520mixture-of-experts%2520%2528MoE%2529.%2520Frame-accelerated%250Aaugmentation%2520exposes%2520the%2520model%2520to%2520high-velocity%2520pose%2520changes%2520by%2520widening%250Ainter-frame%2520gaps.%2520The%2520base%2520controller%2520reliably%2520tracks%2520everyday%2520low-dynamic%250Amotions%252C%2520while%2520the%2520residual%2520MoE%2520adaptively%2520allocates%2520additional%2520network%250Acapacity%2520to%2520handle%2520challenging%2520high-dynamic%2520actions%252C%2520significantly%2520enhancing%250Atracking%2520accuracy.%2520In%2520the%2520absence%2520of%2520a%2520public%2520benchmark%252C%2520we%2520curate%2520the%250AHigh-Dynamic%2520Humanoid%2520Motion%2520%2528HDHM%2529%2520dataset%252C%2520comprising%25203593%2520physically%250Aplausible%2520clips.%2520On%2520HDHM%252C%2520FARM%2520reduces%2520the%2520tracking%2520failure%2520rate%2520by%252042.8%255C%2525%2520and%250Alowers%2520global%2520mean%2520per-joint%2520position%2520error%2520by%252014.6%255C%2525%2520relative%2520to%2520the%2520baseline%252C%250Awhile%2520preserving%2520near-perfect%2520accuracy%2520on%2520low-dynamic%2520motions.%2520These%2520results%250Aestablish%2520FARM%2520as%2520a%2520new%2520baseline%2520for%2520high-dynamic%2520humanoid%2520control%2520and%250Aintroduce%2520the%2520first%2520open%2520benchmark%2520dedicated%2520to%2520this%2520challenge.%2520The%2520code%2520and%250Adataset%2520will%2520be%2520released%2520at%2520https%253A//github.com/Colin-Jing/FARM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FARM%3A%20Frame-Accelerated%20Augmentation%20and%20Residual%20Mixture-of-Experts%20for%0A%20%20Physics-Based%20High-Dynamic%20Humanoid%20Control&entry.906535625=Tan%20Jing%20and%20Shiting%20Chen%20and%20Yangfan%20Li%20and%20Weisheng%20Xu%20and%20Renjing%20Xu&entry.1292438233=%20%20Unified%20physics-based%20humanoid%20controllers%20are%20pivotal%20for%20robotics%20and%0Acharacter%20animation%2C%20yet%20models%20that%20excel%20on%20gentle%2C%20everyday%20motions%20still%0Astumble%20on%20explosive%20actions%2C%20hampering%20real-world%20deployment.%20We%20bridge%20this%0Agap%20with%20FARM%20%28Frame-Accelerated%20Augmentation%20and%20Residual%20Mixture-of-Experts%29%2C%0Aan%20end-to-end%20framework%20composed%20of%20frame-accelerated%20augmentation%2C%20a%20robust%0Abase%20controller%2C%20and%20a%20residual%20mixture-of-experts%20%28MoE%29.%20Frame-accelerated%0Aaugmentation%20exposes%20the%20model%20to%20high-velocity%20pose%20changes%20by%20widening%0Ainter-frame%20gaps.%20The%20base%20controller%20reliably%20tracks%20everyday%20low-dynamic%0Amotions%2C%20while%20the%20residual%20MoE%20adaptively%20allocates%20additional%20network%0Acapacity%20to%20handle%20challenging%20high-dynamic%20actions%2C%20significantly%20enhancing%0Atracking%20accuracy.%20In%20the%20absence%20of%20a%20public%20benchmark%2C%20we%20curate%20the%0AHigh-Dynamic%20Humanoid%20Motion%20%28HDHM%29%20dataset%2C%20comprising%203593%20physically%0Aplausible%20clips.%20On%20HDHM%2C%20FARM%20reduces%20the%20tracking%20failure%20rate%20by%2042.8%5C%25%20and%0Alowers%20global%20mean%20per-joint%20position%20error%20by%2014.6%5C%25%20relative%20to%20the%20baseline%2C%0Awhile%20preserving%20near-perfect%20accuracy%20on%20low-dynamic%20motions.%20These%20results%0Aestablish%20FARM%20as%20a%20new%20baseline%20for%20high-dynamic%20humanoid%20control%20and%0Aintroduce%20the%20first%20open%20benchmark%20dedicated%20to%20this%20challenge.%20The%20code%20and%0Adataset%20will%20be%20released%20at%20https%3A//github.com/Colin-Jing/FARM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19926v1&entry.124074799=Read"},
{"title": "Latent space configuration for improved generalization in supervised\n  autoencoder neural networks", "author": "Nikita Gabdullin", "abstract": "  Autoencoders (AE) are simple yet powerful class of neural networks that\ncompress data by projecting input into low-dimensional latent space (LS).\nWhereas LS is formed according to the loss function minimization during\ntraining, its properties and topology are not controlled directly. In this\npaper we focus on AE LS properties and propose two methods for obtaining LS\nwith desired topology, called LS configuration. The proposed methods include\nloss configuration using a geometric loss term that acts directly in LS, and\nencoder configuration. We show that the former allows to reliably obtain LS\nwith desired configuration by defining the positions and shapes of LS clusters\nfor supervised AE (SAE). Knowing LS configuration allows to define similarity\nmeasure in LS to predict labels or estimate similarity for multiple inputs\nwithout using decoders or classifiers. We also show that this leads to more\nstable and interpretable training. We show that SAE trained for clothes texture\nclassification using the proposed method generalizes well to unseen data from\nLIP, Market1501, and WildTrack datasets without fine-tuning, and even allows to\nevaluate similarity for unseen classes. We further illustrate the advantages of\npre-configured LS similarity estimation with cross-dataset searches and\ntext-based search using a text query without language models.\n", "link": "http://arxiv.org/abs/2402.08441v3", "date": "2025-08-27", "relevancy": 2.2801, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6071}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5692}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20space%20configuration%20for%20improved%20generalization%20in%20supervised%0A%20%20autoencoder%20neural%20networks&body=Title%3A%20Latent%20space%20configuration%20for%20improved%20generalization%20in%20supervised%0A%20%20autoencoder%20neural%20networks%0AAuthor%3A%20Nikita%20Gabdullin%0AAbstract%3A%20%20%20Autoencoders%20%28AE%29%20are%20simple%20yet%20powerful%20class%20of%20neural%20networks%20that%0Acompress%20data%20by%20projecting%20input%20into%20low-dimensional%20latent%20space%20%28LS%29.%0AWhereas%20LS%20is%20formed%20according%20to%20the%20loss%20function%20minimization%20during%0Atraining%2C%20its%20properties%20and%20topology%20are%20not%20controlled%20directly.%20In%20this%0Apaper%20we%20focus%20on%20AE%20LS%20properties%20and%20propose%20two%20methods%20for%20obtaining%20LS%0Awith%20desired%20topology%2C%20called%20LS%20configuration.%20The%20proposed%20methods%20include%0Aloss%20configuration%20using%20a%20geometric%20loss%20term%20that%20acts%20directly%20in%20LS%2C%20and%0Aencoder%20configuration.%20We%20show%20that%20the%20former%20allows%20to%20reliably%20obtain%20LS%0Awith%20desired%20configuration%20by%20defining%20the%20positions%20and%20shapes%20of%20LS%20clusters%0Afor%20supervised%20AE%20%28SAE%29.%20Knowing%20LS%20configuration%20allows%20to%20define%20similarity%0Ameasure%20in%20LS%20to%20predict%20labels%20or%20estimate%20similarity%20for%20multiple%20inputs%0Awithout%20using%20decoders%20or%20classifiers.%20We%20also%20show%20that%20this%20leads%20to%20more%0Astable%20and%20interpretable%20training.%20We%20show%20that%20SAE%20trained%20for%20clothes%20texture%0Aclassification%20using%20the%20proposed%20method%20generalizes%20well%20to%20unseen%20data%20from%0ALIP%2C%20Market1501%2C%20and%20WildTrack%20datasets%20without%20fine-tuning%2C%20and%20even%20allows%20to%0Aevaluate%20similarity%20for%20unseen%20classes.%20We%20further%20illustrate%20the%20advantages%20of%0Apre-configured%20LS%20similarity%20estimation%20with%20cross-dataset%20searches%20and%0Atext-based%20search%20using%20a%20text%20query%20without%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520space%2520configuration%2520for%2520improved%2520generalization%2520in%2520supervised%250A%2520%2520autoencoder%2520neural%2520networks%26entry.906535625%3DNikita%2520Gabdullin%26entry.1292438233%3D%2520%2520Autoencoders%2520%2528AE%2529%2520are%2520simple%2520yet%2520powerful%2520class%2520of%2520neural%2520networks%2520that%250Acompress%2520data%2520by%2520projecting%2520input%2520into%2520low-dimensional%2520latent%2520space%2520%2528LS%2529.%250AWhereas%2520LS%2520is%2520formed%2520according%2520to%2520the%2520loss%2520function%2520minimization%2520during%250Atraining%252C%2520its%2520properties%2520and%2520topology%2520are%2520not%2520controlled%2520directly.%2520In%2520this%250Apaper%2520we%2520focus%2520on%2520AE%2520LS%2520properties%2520and%2520propose%2520two%2520methods%2520for%2520obtaining%2520LS%250Awith%2520desired%2520topology%252C%2520called%2520LS%2520configuration.%2520The%2520proposed%2520methods%2520include%250Aloss%2520configuration%2520using%2520a%2520geometric%2520loss%2520term%2520that%2520acts%2520directly%2520in%2520LS%252C%2520and%250Aencoder%2520configuration.%2520We%2520show%2520that%2520the%2520former%2520allows%2520to%2520reliably%2520obtain%2520LS%250Awith%2520desired%2520configuration%2520by%2520defining%2520the%2520positions%2520and%2520shapes%2520of%2520LS%2520clusters%250Afor%2520supervised%2520AE%2520%2528SAE%2529.%2520Knowing%2520LS%2520configuration%2520allows%2520to%2520define%2520similarity%250Ameasure%2520in%2520LS%2520to%2520predict%2520labels%2520or%2520estimate%2520similarity%2520for%2520multiple%2520inputs%250Awithout%2520using%2520decoders%2520or%2520classifiers.%2520We%2520also%2520show%2520that%2520this%2520leads%2520to%2520more%250Astable%2520and%2520interpretable%2520training.%2520We%2520show%2520that%2520SAE%2520trained%2520for%2520clothes%2520texture%250Aclassification%2520using%2520the%2520proposed%2520method%2520generalizes%2520well%2520to%2520unseen%2520data%2520from%250ALIP%252C%2520Market1501%252C%2520and%2520WildTrack%2520datasets%2520without%2520fine-tuning%252C%2520and%2520even%2520allows%2520to%250Aevaluate%2520similarity%2520for%2520unseen%2520classes.%2520We%2520further%2520illustrate%2520the%2520advantages%2520of%250Apre-configured%2520LS%2520similarity%2520estimation%2520with%2520cross-dataset%2520searches%2520and%250Atext-based%2520search%2520using%2520a%2520text%2520query%2520without%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20space%20configuration%20for%20improved%20generalization%20in%20supervised%0A%20%20autoencoder%20neural%20networks&entry.906535625=Nikita%20Gabdullin&entry.1292438233=%20%20Autoencoders%20%28AE%29%20are%20simple%20yet%20powerful%20class%20of%20neural%20networks%20that%0Acompress%20data%20by%20projecting%20input%20into%20low-dimensional%20latent%20space%20%28LS%29.%0AWhereas%20LS%20is%20formed%20according%20to%20the%20loss%20function%20minimization%20during%0Atraining%2C%20its%20properties%20and%20topology%20are%20not%20controlled%20directly.%20In%20this%0Apaper%20we%20focus%20on%20AE%20LS%20properties%20and%20propose%20two%20methods%20for%20obtaining%20LS%0Awith%20desired%20topology%2C%20called%20LS%20configuration.%20The%20proposed%20methods%20include%0Aloss%20configuration%20using%20a%20geometric%20loss%20term%20that%20acts%20directly%20in%20LS%2C%20and%0Aencoder%20configuration.%20We%20show%20that%20the%20former%20allows%20to%20reliably%20obtain%20LS%0Awith%20desired%20configuration%20by%20defining%20the%20positions%20and%20shapes%20of%20LS%20clusters%0Afor%20supervised%20AE%20%28SAE%29.%20Knowing%20LS%20configuration%20allows%20to%20define%20similarity%0Ameasure%20in%20LS%20to%20predict%20labels%20or%20estimate%20similarity%20for%20multiple%20inputs%0Awithout%20using%20decoders%20or%20classifiers.%20We%20also%20show%20that%20this%20leads%20to%20more%0Astable%20and%20interpretable%20training.%20We%20show%20that%20SAE%20trained%20for%20clothes%20texture%0Aclassification%20using%20the%20proposed%20method%20generalizes%20well%20to%20unseen%20data%20from%0ALIP%2C%20Market1501%2C%20and%20WildTrack%20datasets%20without%20fine-tuning%2C%20and%20even%20allows%20to%0Aevaluate%20similarity%20for%20unseen%20classes.%20We%20further%20illustrate%20the%20advantages%20of%0Apre-configured%20LS%20similarity%20estimation%20with%20cross-dataset%20searches%20and%0Atext-based%20search%20using%20a%20text%20query%20without%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08441v3&entry.124074799=Read"},
{"title": "AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via\n  Automatic Quality Assessment", "author": "Kaixuan Lu and Mehmet Onurcan Kaya and Dim P. Papadopoulos", "abstract": "  Video Instance Segmentation (VIS) faces significant annotation challenges due\nto its dual requirements of pixel-level masks and temporal consistency labels.\nWhile recent unsupervised methods like VideoCutLER eliminate optical flow\ndependencies through synthetic data, they remain constrained by the\nsynthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised\nframework that bridges this gap through quality-guided self-training. Our\napproach establishes a closed-loop system between pseudo-label generation and\nautomatic quality assessment, enabling progressive adaptation from synthetic to\nreal videos. Experiments demonstrate state-of-the-art performance with 52.6\n$\\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous\nstate-of-the-art VideoCutLER by 4.4$\\%$, while requiring no human annotations.\nThis demonstrates the viability of quality-aware self-training for unsupervised\nVIS. The source code of our method is available at\nhttps://github.com/wcbup/AutoQ-VIS.\n", "link": "http://arxiv.org/abs/2508.19808v1", "date": "2025-08-27", "relevancy": 2.2646, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5759}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5607}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoQ-VIS%3A%20Improving%20Unsupervised%20Video%20Instance%20Segmentation%20via%0A%20%20Automatic%20Quality%20Assessment&body=Title%3A%20AutoQ-VIS%3A%20Improving%20Unsupervised%20Video%20Instance%20Segmentation%20via%0A%20%20Automatic%20Quality%20Assessment%0AAuthor%3A%20Kaixuan%20Lu%20and%20Mehmet%20Onurcan%20Kaya%20and%20Dim%20P.%20Papadopoulos%0AAbstract%3A%20%20%20Video%20Instance%20Segmentation%20%28VIS%29%20faces%20significant%20annotation%20challenges%20due%0Ato%20its%20dual%20requirements%20of%20pixel-level%20masks%20and%20temporal%20consistency%20labels.%0AWhile%20recent%20unsupervised%20methods%20like%20VideoCutLER%20eliminate%20optical%20flow%0Adependencies%20through%20synthetic%20data%2C%20they%20remain%20constrained%20by%20the%0Asynthetic-to-real%20domain%20gap.%20We%20present%20AutoQ-VIS%2C%20a%20novel%20unsupervised%0Aframework%20that%20bridges%20this%20gap%20through%20quality-guided%20self-training.%20Our%0Aapproach%20establishes%20a%20closed-loop%20system%20between%20pseudo-label%20generation%20and%0Aautomatic%20quality%20assessment%2C%20enabling%20progressive%20adaptation%20from%20synthetic%20to%0Areal%20videos.%20Experiments%20demonstrate%20state-of-the-art%20performance%20with%2052.6%0A%24%5Ctext%7BAP%7D_%7B50%7D%24%20on%20YouTubeVIS-2019%20val%20set%2C%20surpassing%20the%20previous%0Astate-of-the-art%20VideoCutLER%20by%204.4%24%5C%25%24%2C%20while%20requiring%20no%20human%20annotations.%0AThis%20demonstrates%20the%20viability%20of%20quality-aware%20self-training%20for%20unsupervised%0AVIS.%20The%20source%20code%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/wcbup/AutoQ-VIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoQ-VIS%253A%2520Improving%2520Unsupervised%2520Video%2520Instance%2520Segmentation%2520via%250A%2520%2520Automatic%2520Quality%2520Assessment%26entry.906535625%3DKaixuan%2520Lu%2520and%2520Mehmet%2520Onurcan%2520Kaya%2520and%2520Dim%2520P.%2520Papadopoulos%26entry.1292438233%3D%2520%2520Video%2520Instance%2520Segmentation%2520%2528VIS%2529%2520faces%2520significant%2520annotation%2520challenges%2520due%250Ato%2520its%2520dual%2520requirements%2520of%2520pixel-level%2520masks%2520and%2520temporal%2520consistency%2520labels.%250AWhile%2520recent%2520unsupervised%2520methods%2520like%2520VideoCutLER%2520eliminate%2520optical%2520flow%250Adependencies%2520through%2520synthetic%2520data%252C%2520they%2520remain%2520constrained%2520by%2520the%250Asynthetic-to-real%2520domain%2520gap.%2520We%2520present%2520AutoQ-VIS%252C%2520a%2520novel%2520unsupervised%250Aframework%2520that%2520bridges%2520this%2520gap%2520through%2520quality-guided%2520self-training.%2520Our%250Aapproach%2520establishes%2520a%2520closed-loop%2520system%2520between%2520pseudo-label%2520generation%2520and%250Aautomatic%2520quality%2520assessment%252C%2520enabling%2520progressive%2520adaptation%2520from%2520synthetic%2520to%250Areal%2520videos.%2520Experiments%2520demonstrate%2520state-of-the-art%2520performance%2520with%252052.6%250A%2524%255Ctext%257BAP%257D_%257B50%257D%2524%2520on%2520YouTubeVIS-2019%2520val%2520set%252C%2520surpassing%2520the%2520previous%250Astate-of-the-art%2520VideoCutLER%2520by%25204.4%2524%255C%2525%2524%252C%2520while%2520requiring%2520no%2520human%2520annotations.%250AThis%2520demonstrates%2520the%2520viability%2520of%2520quality-aware%2520self-training%2520for%2520unsupervised%250AVIS.%2520The%2520source%2520code%2520of%2520our%2520method%2520is%2520available%2520at%250Ahttps%253A//github.com/wcbup/AutoQ-VIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoQ-VIS%3A%20Improving%20Unsupervised%20Video%20Instance%20Segmentation%20via%0A%20%20Automatic%20Quality%20Assessment&entry.906535625=Kaixuan%20Lu%20and%20Mehmet%20Onurcan%20Kaya%20and%20Dim%20P.%20Papadopoulos&entry.1292438233=%20%20Video%20Instance%20Segmentation%20%28VIS%29%20faces%20significant%20annotation%20challenges%20due%0Ato%20its%20dual%20requirements%20of%20pixel-level%20masks%20and%20temporal%20consistency%20labels.%0AWhile%20recent%20unsupervised%20methods%20like%20VideoCutLER%20eliminate%20optical%20flow%0Adependencies%20through%20synthetic%20data%2C%20they%20remain%20constrained%20by%20the%0Asynthetic-to-real%20domain%20gap.%20We%20present%20AutoQ-VIS%2C%20a%20novel%20unsupervised%0Aframework%20that%20bridges%20this%20gap%20through%20quality-guided%20self-training.%20Our%0Aapproach%20establishes%20a%20closed-loop%20system%20between%20pseudo-label%20generation%20and%0Aautomatic%20quality%20assessment%2C%20enabling%20progressive%20adaptation%20from%20synthetic%20to%0Areal%20videos.%20Experiments%20demonstrate%20state-of-the-art%20performance%20with%2052.6%0A%24%5Ctext%7BAP%7D_%7B50%7D%24%20on%20YouTubeVIS-2019%20val%20set%2C%20surpassing%20the%20previous%0Astate-of-the-art%20VideoCutLER%20by%204.4%24%5C%25%24%2C%20while%20requiring%20no%20human%20annotations.%0AThis%20demonstrates%20the%20viability%20of%20quality-aware%20self-training%20for%20unsupervised%0AVIS.%20The%20source%20code%20of%20our%20method%20is%20available%20at%0Ahttps%3A//github.com/wcbup/AutoQ-VIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19808v1&entry.124074799=Read"},
{"title": "X-Prompt: Towards Universal In-Context Image Generation in\n  Auto-Regressive Vision Language Foundation Models", "author": "Zeyi Sun and Ziyang Chu and Pan Zhang and Tong Wu and Xiaoyi Dong and Yuhang Zang and Yuanjun Xiong and Dahua Lin and Jiaqi Wang", "abstract": "  In-context generation is a key component of large language models' (LLMs)\nopen-task generalization capability. By leveraging a few examples as context,\nLLMs can perform both in-domain and out-of-domain tasks. Recent advancements in\nauto-regressive vision-language models (VLMs) built upon LLMs have showcased\nimpressive performance in text-to-image generation. However, the potential of\nin-context learning for general image generation tasks remains largely\nunexplored. To address this, we introduce X-Prompt, a purely auto-regressive\nlarge-vision language model designed to deliver competitive performance across\na wide range of both seen and unseen image generation tasks, all within a\nunified in-context learning framework. X-Prompt incorporates a specialized\ndesign that efficiently compresses valuable features from in-context examples,\nsupporting longer in-context token sequences and improving its ability to\ngeneralize to unseen tasks. A unified training task for both text and image\nprediction enables X-Prompt to handle general image generation with enhanced\ntask awareness from in-context examples. Extensive experiments validate the\nmodel's performance across diverse seen image generation tasks and its capacity\nto generalize to previously unseen tasks.\n", "link": "http://arxiv.org/abs/2412.01824v2", "date": "2025-08-27", "relevancy": 2.2549, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Prompt%3A%20Towards%20Universal%20In-Context%20Image%20Generation%20in%0A%20%20Auto-Regressive%20Vision%20Language%20Foundation%20Models&body=Title%3A%20X-Prompt%3A%20Towards%20Universal%20In-Context%20Image%20Generation%20in%0A%20%20Auto-Regressive%20Vision%20Language%20Foundation%20Models%0AAuthor%3A%20Zeyi%20Sun%20and%20Ziyang%20Chu%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuanjun%20Xiong%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20In-context%20generation%20is%20a%20key%20component%20of%20large%20language%20models%27%20%28LLMs%29%0Aopen-task%20generalization%20capability.%20By%20leveraging%20a%20few%20examples%20as%20context%2C%0ALLMs%20can%20perform%20both%20in-domain%20and%20out-of-domain%20tasks.%20Recent%20advancements%20in%0Aauto-regressive%20vision-language%20models%20%28VLMs%29%20built%20upon%20LLMs%20have%20showcased%0Aimpressive%20performance%20in%20text-to-image%20generation.%20However%2C%20the%20potential%20of%0Ain-context%20learning%20for%20general%20image%20generation%20tasks%20remains%20largely%0Aunexplored.%20To%20address%20this%2C%20we%20introduce%20X-Prompt%2C%20a%20purely%20auto-regressive%0Alarge-vision%20language%20model%20designed%20to%20deliver%20competitive%20performance%20across%0Aa%20wide%20range%20of%20both%20seen%20and%20unseen%20image%20generation%20tasks%2C%20all%20within%20a%0Aunified%20in-context%20learning%20framework.%20X-Prompt%20incorporates%20a%20specialized%0Adesign%20that%20efficiently%20compresses%20valuable%20features%20from%20in-context%20examples%2C%0Asupporting%20longer%20in-context%20token%20sequences%20and%20improving%20its%20ability%20to%0Ageneralize%20to%20unseen%20tasks.%20A%20unified%20training%20task%20for%20both%20text%20and%20image%0Aprediction%20enables%20X-Prompt%20to%20handle%20general%20image%20generation%20with%20enhanced%0Atask%20awareness%20from%20in-context%20examples.%20Extensive%20experiments%20validate%20the%0Amodel%27s%20performance%20across%20diverse%20seen%20image%20generation%20tasks%20and%20its%20capacity%0Ato%20generalize%20to%20previously%20unseen%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01824v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Prompt%253A%2520Towards%2520Universal%2520In-Context%2520Image%2520Generation%2520in%250A%2520%2520Auto-Regressive%2520Vision%2520Language%2520Foundation%2520Models%26entry.906535625%3DZeyi%2520Sun%2520and%2520Ziyang%2520Chu%2520and%2520Pan%2520Zhang%2520and%2520Tong%2520Wu%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuanjun%2520Xiong%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520In-context%2520generation%2520is%2520a%2520key%2520component%2520of%2520large%2520language%2520models%2527%2520%2528LLMs%2529%250Aopen-task%2520generalization%2520capability.%2520By%2520leveraging%2520a%2520few%2520examples%2520as%2520context%252C%250ALLMs%2520can%2520perform%2520both%2520in-domain%2520and%2520out-of-domain%2520tasks.%2520Recent%2520advancements%2520in%250Aauto-regressive%2520vision-language%2520models%2520%2528VLMs%2529%2520built%2520upon%2520LLMs%2520have%2520showcased%250Aimpressive%2520performance%2520in%2520text-to-image%2520generation.%2520However%252C%2520the%2520potential%2520of%250Ain-context%2520learning%2520for%2520general%2520image%2520generation%2520tasks%2520remains%2520largely%250Aunexplored.%2520To%2520address%2520this%252C%2520we%2520introduce%2520X-Prompt%252C%2520a%2520purely%2520auto-regressive%250Alarge-vision%2520language%2520model%2520designed%2520to%2520deliver%2520competitive%2520performance%2520across%250Aa%2520wide%2520range%2520of%2520both%2520seen%2520and%2520unseen%2520image%2520generation%2520tasks%252C%2520all%2520within%2520a%250Aunified%2520in-context%2520learning%2520framework.%2520X-Prompt%2520incorporates%2520a%2520specialized%250Adesign%2520that%2520efficiently%2520compresses%2520valuable%2520features%2520from%2520in-context%2520examples%252C%250Asupporting%2520longer%2520in-context%2520token%2520sequences%2520and%2520improving%2520its%2520ability%2520to%250Ageneralize%2520to%2520unseen%2520tasks.%2520A%2520unified%2520training%2520task%2520for%2520both%2520text%2520and%2520image%250Aprediction%2520enables%2520X-Prompt%2520to%2520handle%2520general%2520image%2520generation%2520with%2520enhanced%250Atask%2520awareness%2520from%2520in-context%2520examples.%2520Extensive%2520experiments%2520validate%2520the%250Amodel%2527s%2520performance%2520across%2520diverse%2520seen%2520image%2520generation%2520tasks%2520and%2520its%2520capacity%250Ato%2520generalize%2520to%2520previously%2520unseen%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01824v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Prompt%3A%20Towards%20Universal%20In-Context%20Image%20Generation%20in%0A%20%20Auto-Regressive%20Vision%20Language%20Foundation%20Models&entry.906535625=Zeyi%20Sun%20and%20Ziyang%20Chu%20and%20Pan%20Zhang%20and%20Tong%20Wu%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuanjun%20Xiong%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20In-context%20generation%20is%20a%20key%20component%20of%20large%20language%20models%27%20%28LLMs%29%0Aopen-task%20generalization%20capability.%20By%20leveraging%20a%20few%20examples%20as%20context%2C%0ALLMs%20can%20perform%20both%20in-domain%20and%20out-of-domain%20tasks.%20Recent%20advancements%20in%0Aauto-regressive%20vision-language%20models%20%28VLMs%29%20built%20upon%20LLMs%20have%20showcased%0Aimpressive%20performance%20in%20text-to-image%20generation.%20However%2C%20the%20potential%20of%0Ain-context%20learning%20for%20general%20image%20generation%20tasks%20remains%20largely%0Aunexplored.%20To%20address%20this%2C%20we%20introduce%20X-Prompt%2C%20a%20purely%20auto-regressive%0Alarge-vision%20language%20model%20designed%20to%20deliver%20competitive%20performance%20across%0Aa%20wide%20range%20of%20both%20seen%20and%20unseen%20image%20generation%20tasks%2C%20all%20within%20a%0Aunified%20in-context%20learning%20framework.%20X-Prompt%20incorporates%20a%20specialized%0Adesign%20that%20efficiently%20compresses%20valuable%20features%20from%20in-context%20examples%2C%0Asupporting%20longer%20in-context%20token%20sequences%20and%20improving%20its%20ability%20to%0Ageneralize%20to%20unseen%20tasks.%20A%20unified%20training%20task%20for%20both%20text%20and%20image%0Aprediction%20enables%20X-Prompt%20to%20handle%20general%20image%20generation%20with%20enhanced%0Atask%20awareness%20from%20in-context%20examples.%20Extensive%20experiments%20validate%20the%0Amodel%27s%20performance%20across%20diverse%20seen%20image%20generation%20tasks%20and%20its%20capacity%0Ato%20generalize%20to%20previously%20unseen%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01824v2&entry.124074799=Read"},
{"title": "Evaluating the Fitness of Ontologies for the Task of Question Generation", "author": "Samah Alkhuzaey and Floriana Grasso and Terry R. Payne and Valentina Tamma", "abstract": "  Ontology-based question generation is an important application of\nsemantic-aware systems that enables the creation of large question banks for\ndiverse learning environments. The effectiveness of these systems, both in\nterms of the calibre and cognitive difficulty of the resulting questions,\ndepends heavily on the quality and modelling approach of the underlying\nontologies, making it crucial to assess their fitness for this task. To date,\nthere has been no comprehensive investigation into the specific ontology\naspects or characteristics that affect the question generation process.\nTherefore, this paper proposes a set of requirements and task-specific metrics\nfor evaluating the fitness of ontologies for question generation tasks in\npedagogical settings. Using the ROMEO methodology (a structured framework used\nfor identifying task-specific metrics), a set of evaluation metrics have been\nderived from an expert assessment of questions generated by a question\ngeneration model. To validate the proposed metrics, we apply them to a set of\nontologies previously used in question generation to illustrate how the metric\nscores align with and complement findings reported in earlier studies. The\nanalysis confirms that ontology characteristics significantly impact the\neffectiveness of question generation, with different ontologies exhibiting\nvarying performance levels. This highlights the importance of assessing\nontology quality with respect to Automatic Question Generation (AQG) tasks.\n", "link": "http://arxiv.org/abs/2504.07994v2", "date": "2025-08-27", "relevancy": 2.2253, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4422}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Fitness%20of%20Ontologies%20for%20the%20Task%20of%20Question%20Generation&body=Title%3A%20Evaluating%20the%20Fitness%20of%20Ontologies%20for%20the%20Task%20of%20Question%20Generation%0AAuthor%3A%20Samah%20Alkhuzaey%20and%20Floriana%20Grasso%20and%20Terry%20R.%20Payne%20and%20Valentina%20Tamma%0AAbstract%3A%20%20%20Ontology-based%20question%20generation%20is%20an%20important%20application%20of%0Asemantic-aware%20systems%20that%20enables%20the%20creation%20of%20large%20question%20banks%20for%0Adiverse%20learning%20environments.%20The%20effectiveness%20of%20these%20systems%2C%20both%20in%0Aterms%20of%20the%20calibre%20and%20cognitive%20difficulty%20of%20the%20resulting%20questions%2C%0Adepends%20heavily%20on%20the%20quality%20and%20modelling%20approach%20of%20the%20underlying%0Aontologies%2C%20making%20it%20crucial%20to%20assess%20their%20fitness%20for%20this%20task.%20To%20date%2C%0Athere%20has%20been%20no%20comprehensive%20investigation%20into%20the%20specific%20ontology%0Aaspects%20or%20characteristics%20that%20affect%20the%20question%20generation%20process.%0ATherefore%2C%20this%20paper%20proposes%20a%20set%20of%20requirements%20and%20task-specific%20metrics%0Afor%20evaluating%20the%20fitness%20of%20ontologies%20for%20question%20generation%20tasks%20in%0Apedagogical%20settings.%20Using%20the%20ROMEO%20methodology%20%28a%20structured%20framework%20used%0Afor%20identifying%20task-specific%20metrics%29%2C%20a%20set%20of%20evaluation%20metrics%20have%20been%0Aderived%20from%20an%20expert%20assessment%20of%20questions%20generated%20by%20a%20question%0Ageneration%20model.%20To%20validate%20the%20proposed%20metrics%2C%20we%20apply%20them%20to%20a%20set%20of%0Aontologies%20previously%20used%20in%20question%20generation%20to%20illustrate%20how%20the%20metric%0Ascores%20align%20with%20and%20complement%20findings%20reported%20in%20earlier%20studies.%20The%0Aanalysis%20confirms%20that%20ontology%20characteristics%20significantly%20impact%20the%0Aeffectiveness%20of%20question%20generation%2C%20with%20different%20ontologies%20exhibiting%0Avarying%20performance%20levels.%20This%20highlights%20the%20importance%20of%20assessing%0Aontology%20quality%20with%20respect%20to%20Automatic%20Question%20Generation%20%28AQG%29%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Fitness%2520of%2520Ontologies%2520for%2520the%2520Task%2520of%2520Question%2520Generation%26entry.906535625%3DSamah%2520Alkhuzaey%2520and%2520Floriana%2520Grasso%2520and%2520Terry%2520R.%2520Payne%2520and%2520Valentina%2520Tamma%26entry.1292438233%3D%2520%2520Ontology-based%2520question%2520generation%2520is%2520an%2520important%2520application%2520of%250Asemantic-aware%2520systems%2520that%2520enables%2520the%2520creation%2520of%2520large%2520question%2520banks%2520for%250Adiverse%2520learning%2520environments.%2520The%2520effectiveness%2520of%2520these%2520systems%252C%2520both%2520in%250Aterms%2520of%2520the%2520calibre%2520and%2520cognitive%2520difficulty%2520of%2520the%2520resulting%2520questions%252C%250Adepends%2520heavily%2520on%2520the%2520quality%2520and%2520modelling%2520approach%2520of%2520the%2520underlying%250Aontologies%252C%2520making%2520it%2520crucial%2520to%2520assess%2520their%2520fitness%2520for%2520this%2520task.%2520To%2520date%252C%250Athere%2520has%2520been%2520no%2520comprehensive%2520investigation%2520into%2520the%2520specific%2520ontology%250Aaspects%2520or%2520characteristics%2520that%2520affect%2520the%2520question%2520generation%2520process.%250ATherefore%252C%2520this%2520paper%2520proposes%2520a%2520set%2520of%2520requirements%2520and%2520task-specific%2520metrics%250Afor%2520evaluating%2520the%2520fitness%2520of%2520ontologies%2520for%2520question%2520generation%2520tasks%2520in%250Apedagogical%2520settings.%2520Using%2520the%2520ROMEO%2520methodology%2520%2528a%2520structured%2520framework%2520used%250Afor%2520identifying%2520task-specific%2520metrics%2529%252C%2520a%2520set%2520of%2520evaluation%2520metrics%2520have%2520been%250Aderived%2520from%2520an%2520expert%2520assessment%2520of%2520questions%2520generated%2520by%2520a%2520question%250Ageneration%2520model.%2520To%2520validate%2520the%2520proposed%2520metrics%252C%2520we%2520apply%2520them%2520to%2520a%2520set%2520of%250Aontologies%2520previously%2520used%2520in%2520question%2520generation%2520to%2520illustrate%2520how%2520the%2520metric%250Ascores%2520align%2520with%2520and%2520complement%2520findings%2520reported%2520in%2520earlier%2520studies.%2520The%250Aanalysis%2520confirms%2520that%2520ontology%2520characteristics%2520significantly%2520impact%2520the%250Aeffectiveness%2520of%2520question%2520generation%252C%2520with%2520different%2520ontologies%2520exhibiting%250Avarying%2520performance%2520levels.%2520This%2520highlights%2520the%2520importance%2520of%2520assessing%250Aontology%2520quality%2520with%2520respect%2520to%2520Automatic%2520Question%2520Generation%2520%2528AQG%2529%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Fitness%20of%20Ontologies%20for%20the%20Task%20of%20Question%20Generation&entry.906535625=Samah%20Alkhuzaey%20and%20Floriana%20Grasso%20and%20Terry%20R.%20Payne%20and%20Valentina%20Tamma&entry.1292438233=%20%20Ontology-based%20question%20generation%20is%20an%20important%20application%20of%0Asemantic-aware%20systems%20that%20enables%20the%20creation%20of%20large%20question%20banks%20for%0Adiverse%20learning%20environments.%20The%20effectiveness%20of%20these%20systems%2C%20both%20in%0Aterms%20of%20the%20calibre%20and%20cognitive%20difficulty%20of%20the%20resulting%20questions%2C%0Adepends%20heavily%20on%20the%20quality%20and%20modelling%20approach%20of%20the%20underlying%0Aontologies%2C%20making%20it%20crucial%20to%20assess%20their%20fitness%20for%20this%20task.%20To%20date%2C%0Athere%20has%20been%20no%20comprehensive%20investigation%20into%20the%20specific%20ontology%0Aaspects%20or%20characteristics%20that%20affect%20the%20question%20generation%20process.%0ATherefore%2C%20this%20paper%20proposes%20a%20set%20of%20requirements%20and%20task-specific%20metrics%0Afor%20evaluating%20the%20fitness%20of%20ontologies%20for%20question%20generation%20tasks%20in%0Apedagogical%20settings.%20Using%20the%20ROMEO%20methodology%20%28a%20structured%20framework%20used%0Afor%20identifying%20task-specific%20metrics%29%2C%20a%20set%20of%20evaluation%20metrics%20have%20been%0Aderived%20from%20an%20expert%20assessment%20of%20questions%20generated%20by%20a%20question%0Ageneration%20model.%20To%20validate%20the%20proposed%20metrics%2C%20we%20apply%20them%20to%20a%20set%20of%0Aontologies%20previously%20used%20in%20question%20generation%20to%20illustrate%20how%20the%20metric%0Ascores%20align%20with%20and%20complement%20findings%20reported%20in%20earlier%20studies.%20The%0Aanalysis%20confirms%20that%20ontology%20characteristics%20significantly%20impact%20the%0Aeffectiveness%20of%20question%20generation%2C%20with%20different%20ontologies%20exhibiting%0Avarying%20performance%20levels.%20This%20highlights%20the%20importance%20of%20assessing%0Aontology%20quality%20with%20respect%20to%20Automatic%20Question%20Generation%20%28AQG%29%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07994v2&entry.124074799=Read"},
{"title": "Pseudo-Simulation for Autonomous Driving", "author": "Wei Cao and Marcel Hallgarten and Tianyu Li and Daniel Dauner and Xunjiang Gu and Caojun Wang and Yakov Miron and Marco Aiello and Hongyang Li and Igor Gilitschenski and Boris Ivanovic and Marco Pavone and Andreas Geiger and Kashyap Chitta", "abstract": "  Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations ($R^2=0.8$) than the best existing open-loop\napproach ($R^2=0.7$). We also establish a public leaderboard for the community\nto benchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n", "link": "http://arxiv.org/abs/2506.04218v2", "date": "2025-08-27", "relevancy": 2.2141, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5691}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Simulation%20for%20Autonomous%20Driving&body=Title%3A%20Pseudo-Simulation%20for%20Autonomous%20Driving%0AAuthor%3A%20Wei%20Cao%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Daniel%20Dauner%20and%20Xunjiang%20Gu%20and%20Caojun%20Wang%20and%20Yakov%20Miron%20and%20Marco%20Aiello%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20%20%20Existing%20evaluation%20paradigms%20for%20Autonomous%20Vehicles%20%28AVs%29%20face%20critical%0Alimitations.%20Real-world%20evaluation%20is%20often%20challenging%20due%20to%20safety%20concerns%0Aand%20a%20lack%20of%20reproducibility%2C%20whereas%20closed-loop%20simulation%20can%20face%0Ainsufficient%20realism%20or%20high%20computational%20costs.%20Open-loop%20evaluation%2C%20while%0Abeing%20efficient%20and%20data-driven%2C%20relies%20on%20metrics%20that%20generally%20overlook%0Acompounding%20errors.%20In%20this%20paper%2C%20we%20propose%20pseudo-simulation%2C%20a%20novel%0Aparadigm%20that%20addresses%20these%20limitations.%20Pseudo-simulation%20operates%20on%20real%0Adatasets%2C%20similar%20to%20open-loop%20evaluation%2C%20but%20augments%20them%20with%20synthetic%0Aobservations%20generated%20prior%20to%20evaluation%20using%203D%20Gaussian%20Splatting.%20Our%20key%0Aidea%20is%20to%20approximate%20potential%20future%20states%20the%20AV%20might%20encounter%20by%0Agenerating%20a%20diverse%20set%20of%20observations%20that%20vary%20in%20position%2C%20heading%2C%20and%0Aspeed.%20Our%20method%20then%20assigns%20a%20higher%20importance%20to%20synthetic%20observations%0Athat%20best%20match%20the%20AV%27s%20likely%20behavior%20using%20a%20novel%20proximity-based%0Aweighting%20scheme.%20This%20enables%20evaluating%20error%20recovery%20and%20the%20mitigation%20of%0Acausal%20confusion%2C%20as%20in%20closed-loop%20benchmarks%2C%20without%20requiring%20sequential%0Ainteractive%20simulation.%20We%20show%20that%20pseudo-simulation%20is%20better%20correlated%0Awith%20closed-loop%20simulations%20%28%24R%5E2%3D0.8%24%29%20than%20the%20best%20existing%20open-loop%0Aapproach%20%28%24R%5E2%3D0.7%24%29.%20We%20also%20establish%20a%20public%20leaderboard%20for%20the%20community%0Ato%20benchmark%20new%20methodologies%20with%20pseudo-simulation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.04218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Simulation%2520for%2520Autonomous%2520Driving%26entry.906535625%3DWei%2520Cao%2520and%2520Marcel%2520Hallgarten%2520and%2520Tianyu%2520Li%2520and%2520Daniel%2520Dauner%2520and%2520Xunjiang%2520Gu%2520and%2520Caojun%2520Wang%2520and%2520Yakov%2520Miron%2520and%2520Marco%2520Aiello%2520and%2520Hongyang%2520Li%2520and%2520Igor%2520Gilitschenski%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3D%2520%2520Existing%2520evaluation%2520paradigms%2520for%2520Autonomous%2520Vehicles%2520%2528AVs%2529%2520face%2520critical%250Alimitations.%2520Real-world%2520evaluation%2520is%2520often%2520challenging%2520due%2520to%2520safety%2520concerns%250Aand%2520a%2520lack%2520of%2520reproducibility%252C%2520whereas%2520closed-loop%2520simulation%2520can%2520face%250Ainsufficient%2520realism%2520or%2520high%2520computational%2520costs.%2520Open-loop%2520evaluation%252C%2520while%250Abeing%2520efficient%2520and%2520data-driven%252C%2520relies%2520on%2520metrics%2520that%2520generally%2520overlook%250Acompounding%2520errors.%2520In%2520this%2520paper%252C%2520we%2520propose%2520pseudo-simulation%252C%2520a%2520novel%250Aparadigm%2520that%2520addresses%2520these%2520limitations.%2520Pseudo-simulation%2520operates%2520on%2520real%250Adatasets%252C%2520similar%2520to%2520open-loop%2520evaluation%252C%2520but%2520augments%2520them%2520with%2520synthetic%250Aobservations%2520generated%2520prior%2520to%2520evaluation%2520using%25203D%2520Gaussian%2520Splatting.%2520Our%2520key%250Aidea%2520is%2520to%2520approximate%2520potential%2520future%2520states%2520the%2520AV%2520might%2520encounter%2520by%250Agenerating%2520a%2520diverse%2520set%2520of%2520observations%2520that%2520vary%2520in%2520position%252C%2520heading%252C%2520and%250Aspeed.%2520Our%2520method%2520then%2520assigns%2520a%2520higher%2520importance%2520to%2520synthetic%2520observations%250Athat%2520best%2520match%2520the%2520AV%2527s%2520likely%2520behavior%2520using%2520a%2520novel%2520proximity-based%250Aweighting%2520scheme.%2520This%2520enables%2520evaluating%2520error%2520recovery%2520and%2520the%2520mitigation%2520of%250Acausal%2520confusion%252C%2520as%2520in%2520closed-loop%2520benchmarks%252C%2520without%2520requiring%2520sequential%250Ainteractive%2520simulation.%2520We%2520show%2520that%2520pseudo-simulation%2520is%2520better%2520correlated%250Awith%2520closed-loop%2520simulations%2520%2528%2524R%255E2%253D0.8%2524%2529%2520than%2520the%2520best%2520existing%2520open-loop%250Aapproach%2520%2528%2524R%255E2%253D0.7%2524%2529.%2520We%2520also%2520establish%2520a%2520public%2520leaderboard%2520for%2520the%2520community%250Ato%2520benchmark%2520new%2520methodologies%2520with%2520pseudo-simulation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/autonomousvision/navsim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Simulation%20for%20Autonomous%20Driving&entry.906535625=Wei%20Cao%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Daniel%20Dauner%20and%20Xunjiang%20Gu%20and%20Caojun%20Wang%20and%20Yakov%20Miron%20and%20Marco%20Aiello%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=%20%20Existing%20evaluation%20paradigms%20for%20Autonomous%20Vehicles%20%28AVs%29%20face%20critical%0Alimitations.%20Real-world%20evaluation%20is%20often%20challenging%20due%20to%20safety%20concerns%0Aand%20a%20lack%20of%20reproducibility%2C%20whereas%20closed-loop%20simulation%20can%20face%0Ainsufficient%20realism%20or%20high%20computational%20costs.%20Open-loop%20evaluation%2C%20while%0Abeing%20efficient%20and%20data-driven%2C%20relies%20on%20metrics%20that%20generally%20overlook%0Acompounding%20errors.%20In%20this%20paper%2C%20we%20propose%20pseudo-simulation%2C%20a%20novel%0Aparadigm%20that%20addresses%20these%20limitations.%20Pseudo-simulation%20operates%20on%20real%0Adatasets%2C%20similar%20to%20open-loop%20evaluation%2C%20but%20augments%20them%20with%20synthetic%0Aobservations%20generated%20prior%20to%20evaluation%20using%203D%20Gaussian%20Splatting.%20Our%20key%0Aidea%20is%20to%20approximate%20potential%20future%20states%20the%20AV%20might%20encounter%20by%0Agenerating%20a%20diverse%20set%20of%20observations%20that%20vary%20in%20position%2C%20heading%2C%20and%0Aspeed.%20Our%20method%20then%20assigns%20a%20higher%20importance%20to%20synthetic%20observations%0Athat%20best%20match%20the%20AV%27s%20likely%20behavior%20using%20a%20novel%20proximity-based%0Aweighting%20scheme.%20This%20enables%20evaluating%20error%20recovery%20and%20the%20mitigation%20of%0Acausal%20confusion%2C%20as%20in%20closed-loop%20benchmarks%2C%20without%20requiring%20sequential%0Ainteractive%20simulation.%20We%20show%20that%20pseudo-simulation%20is%20better%20correlated%0Awith%20closed-loop%20simulations%20%28%24R%5E2%3D0.8%24%29%20than%20the%20best%20existing%20open-loop%0Aapproach%20%28%24R%5E2%3D0.7%24%29.%20We%20also%20establish%20a%20public%20leaderboard%20for%20the%20community%0Ato%20benchmark%20new%20methodologies%20with%20pseudo-simulation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.04218v2&entry.124074799=Read"},
{"title": "Learning Deployable Locomotion Control via Differentiable Simulation", "author": "Clemens Schwarke and Victor Klemm and Joshua Bagajo and Jean-Pierre Sleiman and Ignat Georgiev and Jesus Tordesillas and Marco Hutter", "abstract": "  Differentiable simulators promise to improve sample efficiency in robot\nlearning by providing analytic gradients of the system dynamics. Yet, their\napplication to contact-rich tasks like locomotion is complicated by the\ninherently non-smooth nature of contact, impeding effective gradient-based\noptimization. Existing works thus often rely on soft contact models that\nprovide smooth gradients but lack physical accuracy, constraining results to\nsimulation. To address this limitation, we propose a differentiable contact\nmodel designed to provide informative gradients while maintaining high physical\nfidelity. We demonstrate the efficacy of our approach by training a quadrupedal\nlocomotion policy within our differentiable simulator leveraging analytic\ngradients and successfully transferring the learned policy zero-shot to the\nreal world. To the best of our knowledge, this represents the first successful\nsim-to-real transfer of a legged locomotion policy learned entirely within a\ndifferentiable simulator, establishing the feasibility of using differentiable\nsimulation for real-world locomotion control.\n", "link": "http://arxiv.org/abs/2404.02887v2", "date": "2025-08-27", "relevancy": 2.1927, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5911}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5544}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Deployable%20Locomotion%20Control%20via%20Differentiable%20Simulation&body=Title%3A%20Learning%20Deployable%20Locomotion%20Control%20via%20Differentiable%20Simulation%0AAuthor%3A%20Clemens%20Schwarke%20and%20Victor%20Klemm%20and%20Joshua%20Bagajo%20and%20Jean-Pierre%20Sleiman%20and%20Ignat%20Georgiev%20and%20Jesus%20Tordesillas%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Differentiable%20simulators%20promise%20to%20improve%20sample%20efficiency%20in%20robot%0Alearning%20by%20providing%20analytic%20gradients%20of%20the%20system%20dynamics.%20Yet%2C%20their%0Aapplication%20to%20contact-rich%20tasks%20like%20locomotion%20is%20complicated%20by%20the%0Ainherently%20non-smooth%20nature%20of%20contact%2C%20impeding%20effective%20gradient-based%0Aoptimization.%20Existing%20works%20thus%20often%20rely%20on%20soft%20contact%20models%20that%0Aprovide%20smooth%20gradients%20but%20lack%20physical%20accuracy%2C%20constraining%20results%20to%0Asimulation.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20differentiable%20contact%0Amodel%20designed%20to%20provide%20informative%20gradients%20while%20maintaining%20high%20physical%0Afidelity.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20by%20training%20a%20quadrupedal%0Alocomotion%20policy%20within%20our%20differentiable%20simulator%20leveraging%20analytic%0Agradients%20and%20successfully%20transferring%20the%20learned%20policy%20zero-shot%20to%20the%0Areal%20world.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20represents%20the%20first%20successful%0Asim-to-real%20transfer%20of%20a%20legged%20locomotion%20policy%20learned%20entirely%20within%20a%0Adifferentiable%20simulator%2C%20establishing%20the%20feasibility%20of%20using%20differentiable%0Asimulation%20for%20real-world%20locomotion%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Deployable%2520Locomotion%2520Control%2520via%2520Differentiable%2520Simulation%26entry.906535625%3DClemens%2520Schwarke%2520and%2520Victor%2520Klemm%2520and%2520Joshua%2520Bagajo%2520and%2520Jean-Pierre%2520Sleiman%2520and%2520Ignat%2520Georgiev%2520and%2520Jesus%2520Tordesillas%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Differentiable%2520simulators%2520promise%2520to%2520improve%2520sample%2520efficiency%2520in%2520robot%250Alearning%2520by%2520providing%2520analytic%2520gradients%2520of%2520the%2520system%2520dynamics.%2520Yet%252C%2520their%250Aapplication%2520to%2520contact-rich%2520tasks%2520like%2520locomotion%2520is%2520complicated%2520by%2520the%250Ainherently%2520non-smooth%2520nature%2520of%2520contact%252C%2520impeding%2520effective%2520gradient-based%250Aoptimization.%2520Existing%2520works%2520thus%2520often%2520rely%2520on%2520soft%2520contact%2520models%2520that%250Aprovide%2520smooth%2520gradients%2520but%2520lack%2520physical%2520accuracy%252C%2520constraining%2520results%2520to%250Asimulation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520a%2520differentiable%2520contact%250Amodel%2520designed%2520to%2520provide%2520informative%2520gradients%2520while%2520maintaining%2520high%2520physical%250Afidelity.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520by%2520training%2520a%2520quadrupedal%250Alocomotion%2520policy%2520within%2520our%2520differentiable%2520simulator%2520leveraging%2520analytic%250Agradients%2520and%2520successfully%2520transferring%2520the%2520learned%2520policy%2520zero-shot%2520to%2520the%250Areal%2520world.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520represents%2520the%2520first%2520successful%250Asim-to-real%2520transfer%2520of%2520a%2520legged%2520locomotion%2520policy%2520learned%2520entirely%2520within%2520a%250Adifferentiable%2520simulator%252C%2520establishing%2520the%2520feasibility%2520of%2520using%2520differentiable%250Asimulation%2520for%2520real-world%2520locomotion%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Deployable%20Locomotion%20Control%20via%20Differentiable%20Simulation&entry.906535625=Clemens%20Schwarke%20and%20Victor%20Klemm%20and%20Joshua%20Bagajo%20and%20Jean-Pierre%20Sleiman%20and%20Ignat%20Georgiev%20and%20Jesus%20Tordesillas%20and%20Marco%20Hutter&entry.1292438233=%20%20Differentiable%20simulators%20promise%20to%20improve%20sample%20efficiency%20in%20robot%0Alearning%20by%20providing%20analytic%20gradients%20of%20the%20system%20dynamics.%20Yet%2C%20their%0Aapplication%20to%20contact-rich%20tasks%20like%20locomotion%20is%20complicated%20by%20the%0Ainherently%20non-smooth%20nature%20of%20contact%2C%20impeding%20effective%20gradient-based%0Aoptimization.%20Existing%20works%20thus%20often%20rely%20on%20soft%20contact%20models%20that%0Aprovide%20smooth%20gradients%20but%20lack%20physical%20accuracy%2C%20constraining%20results%20to%0Asimulation.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20differentiable%20contact%0Amodel%20designed%20to%20provide%20informative%20gradients%20while%20maintaining%20high%20physical%0Afidelity.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20by%20training%20a%20quadrupedal%0Alocomotion%20policy%20within%20our%20differentiable%20simulator%20leveraging%20analytic%0Agradients%20and%20successfully%20transferring%20the%20learned%20policy%20zero-shot%20to%20the%0Areal%20world.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20represents%20the%20first%20successful%0Asim-to-real%20transfer%20of%20a%20legged%20locomotion%20policy%20learned%20entirely%20within%20a%0Adifferentiable%20simulator%2C%20establishing%20the%20feasibility%20of%20using%20differentiable%0Asimulation%20for%20real-world%20locomotion%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02887v2&entry.124074799=Read"},
{"title": "TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect\n  Classification", "author": "Darya Taratynova and Alya Almsouti and Beknur Kalmakhanbet and Numan Saeed and Mohammad Yaqub", "abstract": "  Congenital heart defect (CHD) detection in ultrasound videos is hindered by\nimage noise and probe positioning variability. While automated methods can\nreduce operator dependence, current machine learning approaches often neglect\ntemporal information, limit themselves to binary classification, and do not\naccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),\na method leveraging foundation image-text model and prompt-aware contrastive\nlearning to classify fetal CHD on cardiac ultrasound videos. TPA extracts\nfeatures from each frame of video subclips using an image encoder, aggregates\nthem with a trainable temporal extractor to capture heart motion, and aligns\nthe video representation with class-specific text prompts via a margin-hinge\ncontrastive loss. To enhance calibration for clinical reliability, we introduce\na Conditional Variational Autoencoder Style Modulation (CVAESM) module, which\nlearns a latent style vector to modulate embeddings and quantifies\nclassification uncertainty. Evaluated on a private dataset for CHD detection\nand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA\nachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while\nalso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On\nEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to\n58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital\nheart defect (CHD) classification in ultrasound videos that integrates temporal\nmodeling, prompt-aware contrastive learning, and uncertainty quantification.\n", "link": "http://arxiv.org/abs/2508.15298v3", "date": "2025-08-27", "relevancy": 2.1891, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.544}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPA%3A%20Temporal%20Prompt%20Alignment%20for%20Fetal%20Congenital%20Heart%20Defect%0A%20%20Classification&body=Title%3A%20TPA%3A%20Temporal%20Prompt%20Alignment%20for%20Fetal%20Congenital%20Heart%20Defect%0A%20%20Classification%0AAuthor%3A%20Darya%20Taratynova%20and%20Alya%20Almsouti%20and%20Beknur%20Kalmakhanbet%20and%20Numan%20Saeed%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Congenital%20heart%20defect%20%28CHD%29%20detection%20in%20ultrasound%20videos%20is%20hindered%20by%0Aimage%20noise%20and%20probe%20positioning%20variability.%20While%20automated%20methods%20can%0Areduce%20operator%20dependence%2C%20current%20machine%20learning%20approaches%20often%20neglect%0Atemporal%20information%2C%20limit%20themselves%20to%20binary%20classification%2C%20and%20do%20not%0Aaccount%20for%20prediction%20calibration.%20We%20propose%20Temporal%20Prompt%20Alignment%20%28TPA%29%2C%0Aa%20method%20leveraging%20foundation%20image-text%20model%20and%20prompt-aware%20contrastive%0Alearning%20to%20classify%20fetal%20CHD%20on%20cardiac%20ultrasound%20videos.%20TPA%20extracts%0Afeatures%20from%20each%20frame%20of%20video%20subclips%20using%20an%20image%20encoder%2C%20aggregates%0Athem%20with%20a%20trainable%20temporal%20extractor%20to%20capture%20heart%20motion%2C%20and%20aligns%0Athe%20video%20representation%20with%20class-specific%20text%20prompts%20via%20a%20margin-hinge%0Acontrastive%20loss.%20To%20enhance%20calibration%20for%20clinical%20reliability%2C%20we%20introduce%0Aa%20Conditional%20Variational%20Autoencoder%20Style%20Modulation%20%28CVAESM%29%20module%2C%20which%0Alearns%20a%20latent%20style%20vector%20to%20modulate%20embeddings%20and%20quantifies%0Aclassification%20uncertainty.%20Evaluated%20on%20a%20private%20dataset%20for%20CHD%20detection%0Aand%20on%20a%20large%20public%20dataset%2C%20EchoNet-Dynamic%2C%20for%20systolic%20dysfunction%2C%20TPA%0Aachieves%20state-of-the-art%20macro%20F1%20scores%20of%2085.40%25%20for%20CHD%20diagnosis%2C%20while%0Aalso%20reducing%20expected%20calibration%20error%20by%205.38%25%20and%20adaptive%20ECE%20by%206.8%25.%20On%0AEchoNet-Dynamic%27s%20three-class%20task%2C%20it%20boosts%20macro%20F1%20by%204.73%25%20%28from%2053.89%25%20to%0A58.62%25%29.%20Temporal%20Prompt%20Alignment%20%28TPA%29%20is%20a%20framework%20for%20fetal%20congenital%0Aheart%20defect%20%28CHD%29%20classification%20in%20ultrasound%20videos%20that%20integrates%20temporal%0Amodeling%2C%20prompt-aware%20contrastive%20learning%2C%20and%20uncertainty%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.15298v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPA%253A%2520Temporal%2520Prompt%2520Alignment%2520for%2520Fetal%2520Congenital%2520Heart%2520Defect%250A%2520%2520Classification%26entry.906535625%3DDarya%2520Taratynova%2520and%2520Alya%2520Almsouti%2520and%2520Beknur%2520Kalmakhanbet%2520and%2520Numan%2520Saeed%2520and%2520Mohammad%2520Yaqub%26entry.1292438233%3D%2520%2520Congenital%2520heart%2520defect%2520%2528CHD%2529%2520detection%2520in%2520ultrasound%2520videos%2520is%2520hindered%2520by%250Aimage%2520noise%2520and%2520probe%2520positioning%2520variability.%2520While%2520automated%2520methods%2520can%250Areduce%2520operator%2520dependence%252C%2520current%2520machine%2520learning%2520approaches%2520often%2520neglect%250Atemporal%2520information%252C%2520limit%2520themselves%2520to%2520binary%2520classification%252C%2520and%2520do%2520not%250Aaccount%2520for%2520prediction%2520calibration.%2520We%2520propose%2520Temporal%2520Prompt%2520Alignment%2520%2528TPA%2529%252C%250Aa%2520method%2520leveraging%2520foundation%2520image-text%2520model%2520and%2520prompt-aware%2520contrastive%250Alearning%2520to%2520classify%2520fetal%2520CHD%2520on%2520cardiac%2520ultrasound%2520videos.%2520TPA%2520extracts%250Afeatures%2520from%2520each%2520frame%2520of%2520video%2520subclips%2520using%2520an%2520image%2520encoder%252C%2520aggregates%250Athem%2520with%2520a%2520trainable%2520temporal%2520extractor%2520to%2520capture%2520heart%2520motion%252C%2520and%2520aligns%250Athe%2520video%2520representation%2520with%2520class-specific%2520text%2520prompts%2520via%2520a%2520margin-hinge%250Acontrastive%2520loss.%2520To%2520enhance%2520calibration%2520for%2520clinical%2520reliability%252C%2520we%2520introduce%250Aa%2520Conditional%2520Variational%2520Autoencoder%2520Style%2520Modulation%2520%2528CVAESM%2529%2520module%252C%2520which%250Alearns%2520a%2520latent%2520style%2520vector%2520to%2520modulate%2520embeddings%2520and%2520quantifies%250Aclassification%2520uncertainty.%2520Evaluated%2520on%2520a%2520private%2520dataset%2520for%2520CHD%2520detection%250Aand%2520on%2520a%2520large%2520public%2520dataset%252C%2520EchoNet-Dynamic%252C%2520for%2520systolic%2520dysfunction%252C%2520TPA%250Aachieves%2520state-of-the-art%2520macro%2520F1%2520scores%2520of%252085.40%2525%2520for%2520CHD%2520diagnosis%252C%2520while%250Aalso%2520reducing%2520expected%2520calibration%2520error%2520by%25205.38%2525%2520and%2520adaptive%2520ECE%2520by%25206.8%2525.%2520On%250AEchoNet-Dynamic%2527s%2520three-class%2520task%252C%2520it%2520boosts%2520macro%2520F1%2520by%25204.73%2525%2520%2528from%252053.89%2525%2520to%250A58.62%2525%2529.%2520Temporal%2520Prompt%2520Alignment%2520%2528TPA%2529%2520is%2520a%2520framework%2520for%2520fetal%2520congenital%250Aheart%2520defect%2520%2528CHD%2529%2520classification%2520in%2520ultrasound%2520videos%2520that%2520integrates%2520temporal%250Amodeling%252C%2520prompt-aware%2520contrastive%2520learning%252C%2520and%2520uncertainty%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15298v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPA%3A%20Temporal%20Prompt%20Alignment%20for%20Fetal%20Congenital%20Heart%20Defect%0A%20%20Classification&entry.906535625=Darya%20Taratynova%20and%20Alya%20Almsouti%20and%20Beknur%20Kalmakhanbet%20and%20Numan%20Saeed%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Congenital%20heart%20defect%20%28CHD%29%20detection%20in%20ultrasound%20videos%20is%20hindered%20by%0Aimage%20noise%20and%20probe%20positioning%20variability.%20While%20automated%20methods%20can%0Areduce%20operator%20dependence%2C%20current%20machine%20learning%20approaches%20often%20neglect%0Atemporal%20information%2C%20limit%20themselves%20to%20binary%20classification%2C%20and%20do%20not%0Aaccount%20for%20prediction%20calibration.%20We%20propose%20Temporal%20Prompt%20Alignment%20%28TPA%29%2C%0Aa%20method%20leveraging%20foundation%20image-text%20model%20and%20prompt-aware%20contrastive%0Alearning%20to%20classify%20fetal%20CHD%20on%20cardiac%20ultrasound%20videos.%20TPA%20extracts%0Afeatures%20from%20each%20frame%20of%20video%20subclips%20using%20an%20image%20encoder%2C%20aggregates%0Athem%20with%20a%20trainable%20temporal%20extractor%20to%20capture%20heart%20motion%2C%20and%20aligns%0Athe%20video%20representation%20with%20class-specific%20text%20prompts%20via%20a%20margin-hinge%0Acontrastive%20loss.%20To%20enhance%20calibration%20for%20clinical%20reliability%2C%20we%20introduce%0Aa%20Conditional%20Variational%20Autoencoder%20Style%20Modulation%20%28CVAESM%29%20module%2C%20which%0Alearns%20a%20latent%20style%20vector%20to%20modulate%20embeddings%20and%20quantifies%0Aclassification%20uncertainty.%20Evaluated%20on%20a%20private%20dataset%20for%20CHD%20detection%0Aand%20on%20a%20large%20public%20dataset%2C%20EchoNet-Dynamic%2C%20for%20systolic%20dysfunction%2C%20TPA%0Aachieves%20state-of-the-art%20macro%20F1%20scores%20of%2085.40%25%20for%20CHD%20diagnosis%2C%20while%0Aalso%20reducing%20expected%20calibration%20error%20by%205.38%25%20and%20adaptive%20ECE%20by%206.8%25.%20On%0AEchoNet-Dynamic%27s%20three-class%20task%2C%20it%20boosts%20macro%20F1%20by%204.73%25%20%28from%2053.89%25%20to%0A58.62%25%29.%20Temporal%20Prompt%20Alignment%20%28TPA%29%20is%20a%20framework%20for%20fetal%20congenital%0Aheart%20defect%20%28CHD%29%20classification%20in%20ultrasound%20videos%20that%20integrates%20temporal%0Amodeling%2C%20prompt-aware%20contrastive%20learning%2C%20and%20uncertainty%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.15298v3&entry.124074799=Read"},
{"title": "General agents contain world models", "author": "Jonathan Richens and David Abel and Alexis Bellot and Tom Everitt", "abstract": "  Are world models a necessary ingredient for flexible, goal-directed\nbehaviour, or is model-free learning sufficient? We provide a formal answer to\nthis question, showing that any agent capable of generalizing to multi-step\ngoal-directed tasks must have learned a predictive model of its environment. We\nshow that this model can be extracted from the agent's policy, and that\nincreasing the agents performance or the complexity of the goals it can achieve\nrequires learning increasingly accurate world models. This has a number of\nconsequences: from developing safe and general agents, to bounding agent\ncapabilities in complex environments, and providing new algorithms for\neliciting world models from agents.\n", "link": "http://arxiv.org/abs/2506.01622v3", "date": "2025-08-27", "relevancy": 2.1839, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6012}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5098}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20agents%20contain%20world%20models&body=Title%3A%20General%20agents%20contain%20world%20models%0AAuthor%3A%20Jonathan%20Richens%20and%20David%20Abel%20and%20Alexis%20Bellot%20and%20Tom%20Everitt%0AAbstract%3A%20%20%20Are%20world%20models%20a%20necessary%20ingredient%20for%20flexible%2C%20goal-directed%0Abehaviour%2C%20or%20is%20model-free%20learning%20sufficient%3F%20We%20provide%20a%20formal%20answer%20to%0Athis%20question%2C%20showing%20that%20any%20agent%20capable%20of%20generalizing%20to%20multi-step%0Agoal-directed%20tasks%20must%20have%20learned%20a%20predictive%20model%20of%20its%20environment.%20We%0Ashow%20that%20this%20model%20can%20be%20extracted%20from%20the%20agent%27s%20policy%2C%20and%20that%0Aincreasing%20the%20agents%20performance%20or%20the%20complexity%20of%20the%20goals%20it%20can%20achieve%0Arequires%20learning%20increasingly%20accurate%20world%20models.%20This%20has%20a%20number%20of%0Aconsequences%3A%20from%20developing%20safe%20and%20general%20agents%2C%20to%20bounding%20agent%0Acapabilities%20in%20complex%20environments%2C%20and%20providing%20new%20algorithms%20for%0Aeliciting%20world%20models%20from%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01622v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520agents%2520contain%2520world%2520models%26entry.906535625%3DJonathan%2520Richens%2520and%2520David%2520Abel%2520and%2520Alexis%2520Bellot%2520and%2520Tom%2520Everitt%26entry.1292438233%3D%2520%2520Are%2520world%2520models%2520a%2520necessary%2520ingredient%2520for%2520flexible%252C%2520goal-directed%250Abehaviour%252C%2520or%2520is%2520model-free%2520learning%2520sufficient%253F%2520We%2520provide%2520a%2520formal%2520answer%2520to%250Athis%2520question%252C%2520showing%2520that%2520any%2520agent%2520capable%2520of%2520generalizing%2520to%2520multi-step%250Agoal-directed%2520tasks%2520must%2520have%2520learned%2520a%2520predictive%2520model%2520of%2520its%2520environment.%2520We%250Ashow%2520that%2520this%2520model%2520can%2520be%2520extracted%2520from%2520the%2520agent%2527s%2520policy%252C%2520and%2520that%250Aincreasing%2520the%2520agents%2520performance%2520or%2520the%2520complexity%2520of%2520the%2520goals%2520it%2520can%2520achieve%250Arequires%2520learning%2520increasingly%2520accurate%2520world%2520models.%2520This%2520has%2520a%2520number%2520of%250Aconsequences%253A%2520from%2520developing%2520safe%2520and%2520general%2520agents%252C%2520to%2520bounding%2520agent%250Acapabilities%2520in%2520complex%2520environments%252C%2520and%2520providing%2520new%2520algorithms%2520for%250Aeliciting%2520world%2520models%2520from%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01622v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20agents%20contain%20world%20models&entry.906535625=Jonathan%20Richens%20and%20David%20Abel%20and%20Alexis%20Bellot%20and%20Tom%20Everitt&entry.1292438233=%20%20Are%20world%20models%20a%20necessary%20ingredient%20for%20flexible%2C%20goal-directed%0Abehaviour%2C%20or%20is%20model-free%20learning%20sufficient%3F%20We%20provide%20a%20formal%20answer%20to%0Athis%20question%2C%20showing%20that%20any%20agent%20capable%20of%20generalizing%20to%20multi-step%0Agoal-directed%20tasks%20must%20have%20learned%20a%20predictive%20model%20of%20its%20environment.%20We%0Ashow%20that%20this%20model%20can%20be%20extracted%20from%20the%20agent%27s%20policy%2C%20and%20that%0Aincreasing%20the%20agents%20performance%20or%20the%20complexity%20of%20the%20goals%20it%20can%20achieve%0Arequires%20learning%20increasingly%20accurate%20world%20models.%20This%20has%20a%20number%20of%0Aconsequences%3A%20from%20developing%20safe%20and%20general%20agents%2C%20to%20bounding%20agent%0Acapabilities%20in%20complex%20environments%2C%20and%20providing%20new%20algorithms%20for%0Aeliciting%20world%20models%20from%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01622v3&entry.124074799=Read"},
{"title": "ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic\n  Segmentation", "author": "Jingyun Wang and Guoliang Kang", "abstract": "  Recent works utilize CLIP to perform the challenging unsupervised semantic\nsegmentation task where only images without annotations are available. However,\nwe observe that when adopting CLIP to such a pixel-level understanding task,\nunexpected bias (including class-preference bias and space-preference bias)\noccurs. Previous works don't explicitly model the bias, which largely\nconstrains the segmentation performance. In this paper, we propose to\nexplicitly model and rectify the bias existing in CLIP to facilitate the\nunsupervised semantic segmentation task. Specifically, we design a learnable\n\"Reference\" prompt to encode class-preference bias and a projection of the\npositional embedding in the vision transformer to encode space-preference bias\nrespectively. To avoid interference, two kinds of biases are firstly\nindependently encoded into different features, i.e., the Reference feature and\nthe positional feature. Via a matrix multiplication between the Reference\nfeature and the positional feature, a bias logit map is generated to explicitly\nrepresent two kinds of biases. Then we rectify the logits of CLIP via a simple\nelement-wise subtraction. To make the rectified results smoother and more\ncontextual, we design a mask decoder which takes the feature of CLIP and the\nrectified logits as input and outputs a rectified segmentation mask with the\nhelp of Gumbel-Softmax operation. A contrastive loss based on the masked visual\nfeatures and the text features of different classes is imposed, which makes the\nbias modeling and rectification process meaningful and effective. Extensive\nexperiments on various benchmarks including PASCAL VOC, PASCAL Context, ADE20K,\nCityscapes, and COCO Stuff demonstrate that our method performs favorably\nagainst previous state-of-the-arts. The implementation is available at:\nhttps://github.com/dogehhh/ReCLIP.\n", "link": "http://arxiv.org/abs/2408.06747v3", "date": "2025-08-27", "relevancy": 2.1798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5745}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5268}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCLIP%2B%2B%3A%20Learn%20to%20Rectify%20the%20Bias%20of%20CLIP%20for%20Unsupervised%20Semantic%0A%20%20Segmentation&body=Title%3A%20ReCLIP%2B%2B%3A%20Learn%20to%20Rectify%20the%20Bias%20of%20CLIP%20for%20Unsupervised%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Jingyun%20Wang%20and%20Guoliang%20Kang%0AAbstract%3A%20%20%20Recent%20works%20utilize%20CLIP%20to%20perform%20the%20challenging%20unsupervised%20semantic%0Asegmentation%20task%20where%20only%20images%20without%20annotations%20are%20available.%20However%2C%0Awe%20observe%20that%20when%20adopting%20CLIP%20to%20such%20a%20pixel-level%20understanding%20task%2C%0Aunexpected%20bias%20%28including%20class-preference%20bias%20and%20space-preference%20bias%29%0Aoccurs.%20Previous%20works%20don%27t%20explicitly%20model%20the%20bias%2C%20which%20largely%0Aconstrains%20the%20segmentation%20performance.%20In%20this%20paper%2C%20we%20propose%20to%0Aexplicitly%20model%20and%20rectify%20the%20bias%20existing%20in%20CLIP%20to%20facilitate%20the%0Aunsupervised%20semantic%20segmentation%20task.%20Specifically%2C%20we%20design%20a%20learnable%0A%22Reference%22%20prompt%20to%20encode%20class-preference%20bias%20and%20a%20projection%20of%20the%0Apositional%20embedding%20in%20the%20vision%20transformer%20to%20encode%20space-preference%20bias%0Arespectively.%20To%20avoid%20interference%2C%20two%20kinds%20of%20biases%20are%20firstly%0Aindependently%20encoded%20into%20different%20features%2C%20i.e.%2C%20the%20Reference%20feature%20and%0Athe%20positional%20feature.%20Via%20a%20matrix%20multiplication%20between%20the%20Reference%0Afeature%20and%20the%20positional%20feature%2C%20a%20bias%20logit%20map%20is%20generated%20to%20explicitly%0Arepresent%20two%20kinds%20of%20biases.%20Then%20we%20rectify%20the%20logits%20of%20CLIP%20via%20a%20simple%0Aelement-wise%20subtraction.%20To%20make%20the%20rectified%20results%20smoother%20and%20more%0Acontextual%2C%20we%20design%20a%20mask%20decoder%20which%20takes%20the%20feature%20of%20CLIP%20and%20the%0Arectified%20logits%20as%20input%20and%20outputs%20a%20rectified%20segmentation%20mask%20with%20the%0Ahelp%20of%20Gumbel-Softmax%20operation.%20A%20contrastive%20loss%20based%20on%20the%20masked%20visual%0Afeatures%20and%20the%20text%20features%20of%20different%20classes%20is%20imposed%2C%20which%20makes%20the%0Abias%20modeling%20and%20rectification%20process%20meaningful%20and%20effective.%20Extensive%0Aexperiments%20on%20various%20benchmarks%20including%20PASCAL%20VOC%2C%20PASCAL%20Context%2C%20ADE20K%2C%0ACityscapes%2C%20and%20COCO%20Stuff%20demonstrate%20that%20our%20method%20performs%20favorably%0Aagainst%20previous%20state-of-the-arts.%20The%20implementation%20is%20available%20at%3A%0Ahttps%3A//github.com/dogehhh/ReCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06747v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCLIP%252B%252B%253A%2520Learn%2520to%2520Rectify%2520the%2520Bias%2520of%2520CLIP%2520for%2520Unsupervised%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DJingyun%2520Wang%2520and%2520Guoliang%2520Kang%26entry.1292438233%3D%2520%2520Recent%2520works%2520utilize%2520CLIP%2520to%2520perform%2520the%2520challenging%2520unsupervised%2520semantic%250Asegmentation%2520task%2520where%2520only%2520images%2520without%2520annotations%2520are%2520available.%2520However%252C%250Awe%2520observe%2520that%2520when%2520adopting%2520CLIP%2520to%2520such%2520a%2520pixel-level%2520understanding%2520task%252C%250Aunexpected%2520bias%2520%2528including%2520class-preference%2520bias%2520and%2520space-preference%2520bias%2529%250Aoccurs.%2520Previous%2520works%2520don%2527t%2520explicitly%2520model%2520the%2520bias%252C%2520which%2520largely%250Aconstrains%2520the%2520segmentation%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%250Aexplicitly%2520model%2520and%2520rectify%2520the%2520bias%2520existing%2520in%2520CLIP%2520to%2520facilitate%2520the%250Aunsupervised%2520semantic%2520segmentation%2520task.%2520Specifically%252C%2520we%2520design%2520a%2520learnable%250A%2522Reference%2522%2520prompt%2520to%2520encode%2520class-preference%2520bias%2520and%2520a%2520projection%2520of%2520the%250Apositional%2520embedding%2520in%2520the%2520vision%2520transformer%2520to%2520encode%2520space-preference%2520bias%250Arespectively.%2520To%2520avoid%2520interference%252C%2520two%2520kinds%2520of%2520biases%2520are%2520firstly%250Aindependently%2520encoded%2520into%2520different%2520features%252C%2520i.e.%252C%2520the%2520Reference%2520feature%2520and%250Athe%2520positional%2520feature.%2520Via%2520a%2520matrix%2520multiplication%2520between%2520the%2520Reference%250Afeature%2520and%2520the%2520positional%2520feature%252C%2520a%2520bias%2520logit%2520map%2520is%2520generated%2520to%2520explicitly%250Arepresent%2520two%2520kinds%2520of%2520biases.%2520Then%2520we%2520rectify%2520the%2520logits%2520of%2520CLIP%2520via%2520a%2520simple%250Aelement-wise%2520subtraction.%2520To%2520make%2520the%2520rectified%2520results%2520smoother%2520and%2520more%250Acontextual%252C%2520we%2520design%2520a%2520mask%2520decoder%2520which%2520takes%2520the%2520feature%2520of%2520CLIP%2520and%2520the%250Arectified%2520logits%2520as%2520input%2520and%2520outputs%2520a%2520rectified%2520segmentation%2520mask%2520with%2520the%250Ahelp%2520of%2520Gumbel-Softmax%2520operation.%2520A%2520contrastive%2520loss%2520based%2520on%2520the%2520masked%2520visual%250Afeatures%2520and%2520the%2520text%2520features%2520of%2520different%2520classes%2520is%2520imposed%252C%2520which%2520makes%2520the%250Abias%2520modeling%2520and%2520rectification%2520process%2520meaningful%2520and%2520effective.%2520Extensive%250Aexperiments%2520on%2520various%2520benchmarks%2520including%2520PASCAL%2520VOC%252C%2520PASCAL%2520Context%252C%2520ADE20K%252C%250ACityscapes%252C%2520and%2520COCO%2520Stuff%2520demonstrate%2520that%2520our%2520method%2520performs%2520favorably%250Aagainst%2520previous%2520state-of-the-arts.%2520The%2520implementation%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/dogehhh/ReCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06747v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCLIP%2B%2B%3A%20Learn%20to%20Rectify%20the%20Bias%20of%20CLIP%20for%20Unsupervised%20Semantic%0A%20%20Segmentation&entry.906535625=Jingyun%20Wang%20and%20Guoliang%20Kang&entry.1292438233=%20%20Recent%20works%20utilize%20CLIP%20to%20perform%20the%20challenging%20unsupervised%20semantic%0Asegmentation%20task%20where%20only%20images%20without%20annotations%20are%20available.%20However%2C%0Awe%20observe%20that%20when%20adopting%20CLIP%20to%20such%20a%20pixel-level%20understanding%20task%2C%0Aunexpected%20bias%20%28including%20class-preference%20bias%20and%20space-preference%20bias%29%0Aoccurs.%20Previous%20works%20don%27t%20explicitly%20model%20the%20bias%2C%20which%20largely%0Aconstrains%20the%20segmentation%20performance.%20In%20this%20paper%2C%20we%20propose%20to%0Aexplicitly%20model%20and%20rectify%20the%20bias%20existing%20in%20CLIP%20to%20facilitate%20the%0Aunsupervised%20semantic%20segmentation%20task.%20Specifically%2C%20we%20design%20a%20learnable%0A%22Reference%22%20prompt%20to%20encode%20class-preference%20bias%20and%20a%20projection%20of%20the%0Apositional%20embedding%20in%20the%20vision%20transformer%20to%20encode%20space-preference%20bias%0Arespectively.%20To%20avoid%20interference%2C%20two%20kinds%20of%20biases%20are%20firstly%0Aindependently%20encoded%20into%20different%20features%2C%20i.e.%2C%20the%20Reference%20feature%20and%0Athe%20positional%20feature.%20Via%20a%20matrix%20multiplication%20between%20the%20Reference%0Afeature%20and%20the%20positional%20feature%2C%20a%20bias%20logit%20map%20is%20generated%20to%20explicitly%0Arepresent%20two%20kinds%20of%20biases.%20Then%20we%20rectify%20the%20logits%20of%20CLIP%20via%20a%20simple%0Aelement-wise%20subtraction.%20To%20make%20the%20rectified%20results%20smoother%20and%20more%0Acontextual%2C%20we%20design%20a%20mask%20decoder%20which%20takes%20the%20feature%20of%20CLIP%20and%20the%0Arectified%20logits%20as%20input%20and%20outputs%20a%20rectified%20segmentation%20mask%20with%20the%0Ahelp%20of%20Gumbel-Softmax%20operation.%20A%20contrastive%20loss%20based%20on%20the%20masked%20visual%0Afeatures%20and%20the%20text%20features%20of%20different%20classes%20is%20imposed%2C%20which%20makes%20the%0Abias%20modeling%20and%20rectification%20process%20meaningful%20and%20effective.%20Extensive%0Aexperiments%20on%20various%20benchmarks%20including%20PASCAL%20VOC%2C%20PASCAL%20Context%2C%20ADE20K%2C%0ACityscapes%2C%20and%20COCO%20Stuff%20demonstrate%20that%20our%20method%20performs%20favorably%0Aagainst%20previous%20state-of-the-arts.%20The%20implementation%20is%20available%20at%3A%0Ahttps%3A//github.com/dogehhh/ReCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06747v3&entry.124074799=Read"},
{"title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient\n  Object Detection", "author": "Qiyao Xu and Qiming Wu and Xiaowei Li", "abstract": "  Segment Anything Model (SAM) has demonstrated remarkable capabilities in\nsolving light field salient object detection (LF SOD). However, most existing\nmodels tend to neglect the extraction of prompt information under this task.\nMeanwhile, traditional models ignore the analysis of frequency-domain\ninformation, which leads to small objects being overwhelmed by noise. In this\npaper, we put forward a novel model called self-prompting light field segment\nanything model (SPLF-SAM), equipped with unified multi-scale feature embedding\nblock (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is\ncapable of identifying multiple objects of varying sizes, while MAFA, by\nlearning frequency features, effectively prevents small objects from being\noverwhelmed by noise. Extensive experiments have demonstrated the superiority\nof our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be\navailable at https://github.com/XucherCH/splfsam.\n", "link": "http://arxiv.org/abs/2508.19746v1", "date": "2025-08-27", "relevancy": 2.1722, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5456}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPLF-SAM%3A%20Self-Prompting%20Segment%20Anything%20Model%20for%20Light%20Field%20Salient%0A%20%20Object%20Detection&body=Title%3A%20SPLF-SAM%3A%20Self-Prompting%20Segment%20Anything%20Model%20for%20Light%20Field%20Salient%0A%20%20Object%20Detection%0AAuthor%3A%20Qiyao%20Xu%20and%20Qiming%20Wu%20and%20Xiaowei%20Li%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20remarkable%20capabilities%20in%0Asolving%20light%20field%20salient%20object%20detection%20%28LF%20SOD%29.%20However%2C%20most%20existing%0Amodels%20tend%20to%20neglect%20the%20extraction%20of%20prompt%20information%20under%20this%20task.%0AMeanwhile%2C%20traditional%20models%20ignore%20the%20analysis%20of%20frequency-domain%0Ainformation%2C%20which%20leads%20to%20small%20objects%20being%20overwhelmed%20by%20noise.%20In%20this%0Apaper%2C%20we%20put%20forward%20a%20novel%20model%20called%20self-prompting%20light%20field%20segment%0Aanything%20model%20%28SPLF-SAM%29%2C%20equipped%20with%20unified%20multi-scale%20feature%20embedding%0Ablock%20%28UMFEB%29%20and%20a%20multi-scale%20adaptive%20filtering%20adapter%20%28MAFA%29.%20UMFEB%20is%0Acapable%20of%20identifying%20multiple%20objects%20of%20varying%20sizes%2C%20while%20MAFA%2C%20by%0Alearning%20frequency%20features%2C%20effectively%20prevents%20small%20objects%20from%20being%0Aoverwhelmed%20by%20noise.%20Extensive%20experiments%20have%20demonstrated%20the%20superiority%0Aof%20our%20method%20over%20ten%20state-of-the-art%20%28SOTA%29%20LF%20SOD%20methods.%20Our%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/XucherCH/splfsam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPLF-SAM%253A%2520Self-Prompting%2520Segment%2520Anything%2520Model%2520for%2520Light%2520Field%2520Salient%250A%2520%2520Object%2520Detection%26entry.906535625%3DQiyao%2520Xu%2520and%2520Qiming%2520Wu%2520and%2520Xiaowei%2520Li%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520demonstrated%2520remarkable%2520capabilities%2520in%250Asolving%2520light%2520field%2520salient%2520object%2520detection%2520%2528LF%2520SOD%2529.%2520However%252C%2520most%2520existing%250Amodels%2520tend%2520to%2520neglect%2520the%2520extraction%2520of%2520prompt%2520information%2520under%2520this%2520task.%250AMeanwhile%252C%2520traditional%2520models%2520ignore%2520the%2520analysis%2520of%2520frequency-domain%250Ainformation%252C%2520which%2520leads%2520to%2520small%2520objects%2520being%2520overwhelmed%2520by%2520noise.%2520In%2520this%250Apaper%252C%2520we%2520put%2520forward%2520a%2520novel%2520model%2520called%2520self-prompting%2520light%2520field%2520segment%250Aanything%2520model%2520%2528SPLF-SAM%2529%252C%2520equipped%2520with%2520unified%2520multi-scale%2520feature%2520embedding%250Ablock%2520%2528UMFEB%2529%2520and%2520a%2520multi-scale%2520adaptive%2520filtering%2520adapter%2520%2528MAFA%2529.%2520UMFEB%2520is%250Acapable%2520of%2520identifying%2520multiple%2520objects%2520of%2520varying%2520sizes%252C%2520while%2520MAFA%252C%2520by%250Alearning%2520frequency%2520features%252C%2520effectively%2520prevents%2520small%2520objects%2520from%2520being%250Aoverwhelmed%2520by%2520noise.%2520Extensive%2520experiments%2520have%2520demonstrated%2520the%2520superiority%250Aof%2520our%2520method%2520over%2520ten%2520state-of-the-art%2520%2528SOTA%2529%2520LF%2520SOD%2520methods.%2520Our%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/XucherCH/splfsam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPLF-SAM%3A%20Self-Prompting%20Segment%20Anything%20Model%20for%20Light%20Field%20Salient%0A%20%20Object%20Detection&entry.906535625=Qiyao%20Xu%20and%20Qiming%20Wu%20and%20Xiaowei%20Li&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20demonstrated%20remarkable%20capabilities%20in%0Asolving%20light%20field%20salient%20object%20detection%20%28LF%20SOD%29.%20However%2C%20most%20existing%0Amodels%20tend%20to%20neglect%20the%20extraction%20of%20prompt%20information%20under%20this%20task.%0AMeanwhile%2C%20traditional%20models%20ignore%20the%20analysis%20of%20frequency-domain%0Ainformation%2C%20which%20leads%20to%20small%20objects%20being%20overwhelmed%20by%20noise.%20In%20this%0Apaper%2C%20we%20put%20forward%20a%20novel%20model%20called%20self-prompting%20light%20field%20segment%0Aanything%20model%20%28SPLF-SAM%29%2C%20equipped%20with%20unified%20multi-scale%20feature%20embedding%0Ablock%20%28UMFEB%29%20and%20a%20multi-scale%20adaptive%20filtering%20adapter%20%28MAFA%29.%20UMFEB%20is%0Acapable%20of%20identifying%20multiple%20objects%20of%20varying%20sizes%2C%20while%20MAFA%2C%20by%0Alearning%20frequency%20features%2C%20effectively%20prevents%20small%20objects%20from%20being%0Aoverwhelmed%20by%20noise.%20Extensive%20experiments%20have%20demonstrated%20the%20superiority%0Aof%20our%20method%20over%20ten%20state-of-the-art%20%28SOTA%29%20LF%20SOD%20methods.%20Our%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/XucherCH/splfsam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19746v1&entry.124074799=Read"},
{"title": "DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With\n  Aerial Imaging", "author": "Mohamed Youssef and Jian Peng and Oliver Bimber", "abstract": "  Access to below-canopy volumetric vegetation data is crucial for\nunderstanding ecosystem dynamics. We address the long-standing limitation of\nremote sensing to penetrate deep into dense canopy layers. LiDAR and radar are\ncurrently considered the primary options for measuring 3D vegetation\nstructures, while cameras can only extract the reflectance and depth of top\nlayers. Using conventional, high-resolution aerial images, our approach allows\nsensing deep into self-occluding vegetation volumes, such as forests. It is\nsimilar in spirit to the imaging process of wide-field microscopy, but can\nhandle much larger scales and strong occlusion. We scan focal stacks by\nsynthetic-aperture imaging with drones and reduce out-of-focus signal\ncontributions using pre-trained 3D convolutional neural networks with mean\nsquared error (MSE) as the loss function. The resulting volumetric reflectance\nstacks contain low-frequency representations of the vegetation volume.\nCombining multiple reflectance stacks from various spectral channels provides\ninsights into plant health, growth, and environmental conditions throughout the\nentire vegetation volume. Compared with simulated ground truth, our correction\nleads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of\n220 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of\n0.05 when comparing with the top-vegetation layer that was measured with\nclassical multispectral aerial imaging.\n", "link": "http://arxiv.org/abs/2502.02171v4", "date": "2025-08-27", "relevancy": 2.1697, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepForest%3A%20Sensing%20Into%20Self-Occluding%20Volumes%20of%20Vegetation%20With%0A%20%20Aerial%20Imaging&body=Title%3A%20DeepForest%3A%20Sensing%20Into%20Self-Occluding%20Volumes%20of%20Vegetation%20With%0A%20%20Aerial%20Imaging%0AAuthor%3A%20Mohamed%20Youssef%20and%20Jian%20Peng%20and%20Oliver%20Bimber%0AAbstract%3A%20%20%20Access%20to%20below-canopy%20volumetric%20vegetation%20data%20is%20crucial%20for%0Aunderstanding%20ecosystem%20dynamics.%20We%20address%20the%20long-standing%20limitation%20of%0Aremote%20sensing%20to%20penetrate%20deep%20into%20dense%20canopy%20layers.%20LiDAR%20and%20radar%20are%0Acurrently%20considered%20the%20primary%20options%20for%20measuring%203D%20vegetation%0Astructures%2C%20while%20cameras%20can%20only%20extract%20the%20reflectance%20and%20depth%20of%20top%0Alayers.%20Using%20conventional%2C%20high-resolution%20aerial%20images%2C%20our%20approach%20allows%0Asensing%20deep%20into%20self-occluding%20vegetation%20volumes%2C%20such%20as%20forests.%20It%20is%0Asimilar%20in%20spirit%20to%20the%20imaging%20process%20of%20wide-field%20microscopy%2C%20but%20can%0Ahandle%20much%20larger%20scales%20and%20strong%20occlusion.%20We%20scan%20focal%20stacks%20by%0Asynthetic-aperture%20imaging%20with%20drones%20and%20reduce%20out-of-focus%20signal%0Acontributions%20using%20pre-trained%203D%20convolutional%20neural%20networks%20with%20mean%0Asquared%20error%20%28MSE%29%20as%20the%20loss%20function.%20The%20resulting%20volumetric%20reflectance%0Astacks%20contain%20low-frequency%20representations%20of%20the%20vegetation%20volume.%0ACombining%20multiple%20reflectance%20stacks%20from%20various%20spectral%20channels%20provides%0Ainsights%20into%20plant%20health%2C%20growth%2C%20and%20environmental%20conditions%20throughout%20the%0Aentire%20vegetation%20volume.%20Compared%20with%20simulated%20ground%20truth%2C%20our%20correction%0Aleads%20to%20~x7%20average%20improvements%20%28min%3A%20~x2%2C%20max%3A%20~x12%29%20for%20forest%20densities%20of%0A220%20trees/ha%20-%201680%20trees/ha.%20In%20our%20field%20experiment%2C%20we%20achieved%20an%20MSE%20of%0A0.05%20when%20comparing%20with%20the%20top-vegetation%20layer%20that%20was%20measured%20with%0Aclassical%20multispectral%20aerial%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02171v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepForest%253A%2520Sensing%2520Into%2520Self-Occluding%2520Volumes%2520of%2520Vegetation%2520With%250A%2520%2520Aerial%2520Imaging%26entry.906535625%3DMohamed%2520Youssef%2520and%2520Jian%2520Peng%2520and%2520Oliver%2520Bimber%26entry.1292438233%3D%2520%2520Access%2520to%2520below-canopy%2520volumetric%2520vegetation%2520data%2520is%2520crucial%2520for%250Aunderstanding%2520ecosystem%2520dynamics.%2520We%2520address%2520the%2520long-standing%2520limitation%2520of%250Aremote%2520sensing%2520to%2520penetrate%2520deep%2520into%2520dense%2520canopy%2520layers.%2520LiDAR%2520and%2520radar%2520are%250Acurrently%2520considered%2520the%2520primary%2520options%2520for%2520measuring%25203D%2520vegetation%250Astructures%252C%2520while%2520cameras%2520can%2520only%2520extract%2520the%2520reflectance%2520and%2520depth%2520of%2520top%250Alayers.%2520Using%2520conventional%252C%2520high-resolution%2520aerial%2520images%252C%2520our%2520approach%2520allows%250Asensing%2520deep%2520into%2520self-occluding%2520vegetation%2520volumes%252C%2520such%2520as%2520forests.%2520It%2520is%250Asimilar%2520in%2520spirit%2520to%2520the%2520imaging%2520process%2520of%2520wide-field%2520microscopy%252C%2520but%2520can%250Ahandle%2520much%2520larger%2520scales%2520and%2520strong%2520occlusion.%2520We%2520scan%2520focal%2520stacks%2520by%250Asynthetic-aperture%2520imaging%2520with%2520drones%2520and%2520reduce%2520out-of-focus%2520signal%250Acontributions%2520using%2520pre-trained%25203D%2520convolutional%2520neural%2520networks%2520with%2520mean%250Asquared%2520error%2520%2528MSE%2529%2520as%2520the%2520loss%2520function.%2520The%2520resulting%2520volumetric%2520reflectance%250Astacks%2520contain%2520low-frequency%2520representations%2520of%2520the%2520vegetation%2520volume.%250ACombining%2520multiple%2520reflectance%2520stacks%2520from%2520various%2520spectral%2520channels%2520provides%250Ainsights%2520into%2520plant%2520health%252C%2520growth%252C%2520and%2520environmental%2520conditions%2520throughout%2520the%250Aentire%2520vegetation%2520volume.%2520Compared%2520with%2520simulated%2520ground%2520truth%252C%2520our%2520correction%250Aleads%2520to%2520~x7%2520average%2520improvements%2520%2528min%253A%2520~x2%252C%2520max%253A%2520~x12%2529%2520for%2520forest%2520densities%2520of%250A220%2520trees/ha%2520-%25201680%2520trees/ha.%2520In%2520our%2520field%2520experiment%252C%2520we%2520achieved%2520an%2520MSE%2520of%250A0.05%2520when%2520comparing%2520with%2520the%2520top-vegetation%2520layer%2520that%2520was%2520measured%2520with%250Aclassical%2520multispectral%2520aerial%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02171v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepForest%3A%20Sensing%20Into%20Self-Occluding%20Volumes%20of%20Vegetation%20With%0A%20%20Aerial%20Imaging&entry.906535625=Mohamed%20Youssef%20and%20Jian%20Peng%20and%20Oliver%20Bimber&entry.1292438233=%20%20Access%20to%20below-canopy%20volumetric%20vegetation%20data%20is%20crucial%20for%0Aunderstanding%20ecosystem%20dynamics.%20We%20address%20the%20long-standing%20limitation%20of%0Aremote%20sensing%20to%20penetrate%20deep%20into%20dense%20canopy%20layers.%20LiDAR%20and%20radar%20are%0Acurrently%20considered%20the%20primary%20options%20for%20measuring%203D%20vegetation%0Astructures%2C%20while%20cameras%20can%20only%20extract%20the%20reflectance%20and%20depth%20of%20top%0Alayers.%20Using%20conventional%2C%20high-resolution%20aerial%20images%2C%20our%20approach%20allows%0Asensing%20deep%20into%20self-occluding%20vegetation%20volumes%2C%20such%20as%20forests.%20It%20is%0Asimilar%20in%20spirit%20to%20the%20imaging%20process%20of%20wide-field%20microscopy%2C%20but%20can%0Ahandle%20much%20larger%20scales%20and%20strong%20occlusion.%20We%20scan%20focal%20stacks%20by%0Asynthetic-aperture%20imaging%20with%20drones%20and%20reduce%20out-of-focus%20signal%0Acontributions%20using%20pre-trained%203D%20convolutional%20neural%20networks%20with%20mean%0Asquared%20error%20%28MSE%29%20as%20the%20loss%20function.%20The%20resulting%20volumetric%20reflectance%0Astacks%20contain%20low-frequency%20representations%20of%20the%20vegetation%20volume.%0ACombining%20multiple%20reflectance%20stacks%20from%20various%20spectral%20channels%20provides%0Ainsights%20into%20plant%20health%2C%20growth%2C%20and%20environmental%20conditions%20throughout%20the%0Aentire%20vegetation%20volume.%20Compared%20with%20simulated%20ground%20truth%2C%20our%20correction%0Aleads%20to%20~x7%20average%20improvements%20%28min%3A%20~x2%2C%20max%3A%20~x12%29%20for%20forest%20densities%20of%0A220%20trees/ha%20-%201680%20trees/ha.%20In%20our%20field%20experiment%2C%20we%20achieved%20an%20MSE%20of%0A0.05%20when%20comparing%20with%20the%20top-vegetation%20layer%20that%20was%20measured%20with%0Aclassical%20multispectral%20aerial%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02171v4&entry.124074799=Read"},
{"title": "Diffusion Language Models Know the Answer Before Decoding", "author": "Pengxiang Li and Yefan Zhou and Dilxat Muhtar and Lu Yin and Shilin Yan and Li Shen and Yi Liang and Soroush Vosoughi and Shiwei Liu", "abstract": "  Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.\n", "link": "http://arxiv.org/abs/2508.19982v1", "date": "2025-08-27", "relevancy": 2.1585, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Language%20Models%20Know%20the%20Answer%20Before%20Decoding&body=Title%3A%20Diffusion%20Language%20Models%20Know%20the%20Answer%20Before%20Decoding%0AAuthor%3A%20Pengxiang%20Li%20and%20Yefan%20Zhou%20and%20Dilxat%20Muhtar%20and%20Lu%20Yin%20and%20Shilin%20Yan%20and%20Li%20Shen%20and%20Yi%20Liang%20and%20Soroush%20Vosoughi%20and%20Shiwei%20Liu%0AAbstract%3A%20%20%20Diffusion%20language%20models%20%28DLMs%29%20have%20recently%20emerged%20as%20an%20alternative%20to%0Aautoregressive%20approaches%2C%20offering%20parallel%20sequence%20generation%20and%20flexible%0Atoken%20orders.%20However%2C%20their%20inference%20remains%20slower%20than%20that%20of%0Aautoregressive%20models%2C%20primarily%20due%20to%20the%20cost%20of%20bidirectional%20attention%20and%0Athe%20large%20number%20of%20refinement%20steps%20required%20for%20high%20quality%20outputs.%20In%20this%0Awork%2C%20we%20highlight%20and%20leverage%20an%20overlooked%20property%20of%20DLMs%20early%20answer%0Aconvergence%3A%20in%20many%20cases%2C%20the%20correct%20answer%20can%20be%20internally%20identified%20by%0Ahalf%20steps%20before%20the%20final%20decoding%20step%2C%20both%20under%20semi-autoregressive%20and%0Arandom%20remasking%20schedules.%20For%20example%2C%20on%20GSM8K%20and%20MMLU%2C%20up%20to%2097%25%20and%2099%25%0Aof%20instances%2C%20respectively%2C%20can%20be%20decoded%20correctly%20using%20only%20half%20of%20the%0Arefinement%20steps.%20Building%20on%20this%20observation%2C%20we%20introduce%20Prophet%2C%20a%0Atraining-free%20fast%20decoding%20paradigm%20that%20enables%20early%20commit%20decoding.%0ASpecifically%2C%20Prophet%20dynamically%20decides%20whether%20to%20continue%20refinement%20or%20to%0Ago%20%22all-in%22%20%28i.e.%2C%20decode%20all%20remaining%20tokens%20in%20one%20step%29%2C%20using%20the%0Aconfidence%20gap%20between%20the%20top-2%20prediction%20candidates%20as%20the%20criterion.%20It%0Aintegrates%20seamlessly%20into%20existing%20DLM%20implementations%2C%20incurs%20negligible%0Aoverhead%2C%20and%20requires%20no%20additional%20training.%20Empirical%20evaluations%20of%0ALLaDA-8B%20and%20Dream-7B%20across%20multiple%20tasks%20show%20that%20Prophet%20reduces%20the%0Anumber%20of%20decoding%20steps%20by%20up%20to%203.4x%20while%20preserving%20high%20generation%0Aquality.%20These%20results%20recast%20DLM%20decoding%20as%20a%20problem%20of%20when%20to%20stop%0Asampling%2C%20and%20demonstrate%20that%20early%20decode%20convergence%20provides%20a%20simple%20yet%0Apowerful%20mechanism%20for%20accelerating%20DLM%20inference%2C%20complementary%20to%20existing%0Aspeedup%20techniques.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/pixeli99/Prophet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Language%2520Models%2520Know%2520the%2520Answer%2520Before%2520Decoding%26entry.906535625%3DPengxiang%2520Li%2520and%2520Yefan%2520Zhou%2520and%2520Dilxat%2520Muhtar%2520and%2520Lu%2520Yin%2520and%2520Shilin%2520Yan%2520and%2520Li%2520Shen%2520and%2520Yi%2520Liang%2520and%2520Soroush%2520Vosoughi%2520and%2520Shiwei%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520language%2520models%2520%2528DLMs%2529%2520have%2520recently%2520emerged%2520as%2520an%2520alternative%2520to%250Aautoregressive%2520approaches%252C%2520offering%2520parallel%2520sequence%2520generation%2520and%2520flexible%250Atoken%2520orders.%2520However%252C%2520their%2520inference%2520remains%2520slower%2520than%2520that%2520of%250Aautoregressive%2520models%252C%2520primarily%2520due%2520to%2520the%2520cost%2520of%2520bidirectional%2520attention%2520and%250Athe%2520large%2520number%2520of%2520refinement%2520steps%2520required%2520for%2520high%2520quality%2520outputs.%2520In%2520this%250Awork%252C%2520we%2520highlight%2520and%2520leverage%2520an%2520overlooked%2520property%2520of%2520DLMs%2520early%2520answer%250Aconvergence%253A%2520in%2520many%2520cases%252C%2520the%2520correct%2520answer%2520can%2520be%2520internally%2520identified%2520by%250Ahalf%2520steps%2520before%2520the%2520final%2520decoding%2520step%252C%2520both%2520under%2520semi-autoregressive%2520and%250Arandom%2520remasking%2520schedules.%2520For%2520example%252C%2520on%2520GSM8K%2520and%2520MMLU%252C%2520up%2520to%252097%2525%2520and%252099%2525%250Aof%2520instances%252C%2520respectively%252C%2520can%2520be%2520decoded%2520correctly%2520using%2520only%2520half%2520of%2520the%250Arefinement%2520steps.%2520Building%2520on%2520this%2520observation%252C%2520we%2520introduce%2520Prophet%252C%2520a%250Atraining-free%2520fast%2520decoding%2520paradigm%2520that%2520enables%2520early%2520commit%2520decoding.%250ASpecifically%252C%2520Prophet%2520dynamically%2520decides%2520whether%2520to%2520continue%2520refinement%2520or%2520to%250Ago%2520%2522all-in%2522%2520%2528i.e.%252C%2520decode%2520all%2520remaining%2520tokens%2520in%2520one%2520step%2529%252C%2520using%2520the%250Aconfidence%2520gap%2520between%2520the%2520top-2%2520prediction%2520candidates%2520as%2520the%2520criterion.%2520It%250Aintegrates%2520seamlessly%2520into%2520existing%2520DLM%2520implementations%252C%2520incurs%2520negligible%250Aoverhead%252C%2520and%2520requires%2520no%2520additional%2520training.%2520Empirical%2520evaluations%2520of%250ALLaDA-8B%2520and%2520Dream-7B%2520across%2520multiple%2520tasks%2520show%2520that%2520Prophet%2520reduces%2520the%250Anumber%2520of%2520decoding%2520steps%2520by%2520up%2520to%25203.4x%2520while%2520preserving%2520high%2520generation%250Aquality.%2520These%2520results%2520recast%2520DLM%2520decoding%2520as%2520a%2520problem%2520of%2520when%2520to%2520stop%250Asampling%252C%2520and%2520demonstrate%2520that%2520early%2520decode%2520convergence%2520provides%2520a%2520simple%2520yet%250Apowerful%2520mechanism%2520for%2520accelerating%2520DLM%2520inference%252C%2520complementary%2520to%2520existing%250Aspeedup%2520techniques.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/pixeli99/Prophet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Language%20Models%20Know%20the%20Answer%20Before%20Decoding&entry.906535625=Pengxiang%20Li%20and%20Yefan%20Zhou%20and%20Dilxat%20Muhtar%20and%20Lu%20Yin%20and%20Shilin%20Yan%20and%20Li%20Shen%20and%20Yi%20Liang%20and%20Soroush%20Vosoughi%20and%20Shiwei%20Liu&entry.1292438233=%20%20Diffusion%20language%20models%20%28DLMs%29%20have%20recently%20emerged%20as%20an%20alternative%20to%0Aautoregressive%20approaches%2C%20offering%20parallel%20sequence%20generation%20and%20flexible%0Atoken%20orders.%20However%2C%20their%20inference%20remains%20slower%20than%20that%20of%0Aautoregressive%20models%2C%20primarily%20due%20to%20the%20cost%20of%20bidirectional%20attention%20and%0Athe%20large%20number%20of%20refinement%20steps%20required%20for%20high%20quality%20outputs.%20In%20this%0Awork%2C%20we%20highlight%20and%20leverage%20an%20overlooked%20property%20of%20DLMs%20early%20answer%0Aconvergence%3A%20in%20many%20cases%2C%20the%20correct%20answer%20can%20be%20internally%20identified%20by%0Ahalf%20steps%20before%20the%20final%20decoding%20step%2C%20both%20under%20semi-autoregressive%20and%0Arandom%20remasking%20schedules.%20For%20example%2C%20on%20GSM8K%20and%20MMLU%2C%20up%20to%2097%25%20and%2099%25%0Aof%20instances%2C%20respectively%2C%20can%20be%20decoded%20correctly%20using%20only%20half%20of%20the%0Arefinement%20steps.%20Building%20on%20this%20observation%2C%20we%20introduce%20Prophet%2C%20a%0Atraining-free%20fast%20decoding%20paradigm%20that%20enables%20early%20commit%20decoding.%0ASpecifically%2C%20Prophet%20dynamically%20decides%20whether%20to%20continue%20refinement%20or%20to%0Ago%20%22all-in%22%20%28i.e.%2C%20decode%20all%20remaining%20tokens%20in%20one%20step%29%2C%20using%20the%0Aconfidence%20gap%20between%20the%20top-2%20prediction%20candidates%20as%20the%20criterion.%20It%0Aintegrates%20seamlessly%20into%20existing%20DLM%20implementations%2C%20incurs%20negligible%0Aoverhead%2C%20and%20requires%20no%20additional%20training.%20Empirical%20evaluations%20of%0ALLaDA-8B%20and%20Dream-7B%20across%20multiple%20tasks%20show%20that%20Prophet%20reduces%20the%0Anumber%20of%20decoding%20steps%20by%20up%20to%203.4x%20while%20preserving%20high%20generation%0Aquality.%20These%20results%20recast%20DLM%20decoding%20as%20a%20problem%20of%20when%20to%20stop%0Asampling%2C%20and%20demonstrate%20that%20early%20decode%20convergence%20provides%20a%20simple%20yet%0Apowerful%20mechanism%20for%20accelerating%20DLM%20inference%2C%20complementary%20to%20existing%0Aspeedup%20techniques.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/pixeli99/Prophet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19982v1&entry.124074799=Read"},
{"title": "Gradient Rectification for Robust Calibration under Distribution Shift", "author": "Yilin Zhang and Cai Xu and You Wu and Ziyu Guan and Wei Zhao", "abstract": "  Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.\n", "link": "http://arxiv.org/abs/2508.19830v1", "date": "2025-08-27", "relevancy": 2.1458, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5582}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5387}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Rectification%20for%20Robust%20Calibration%20under%20Distribution%20Shift&body=Title%3A%20Gradient%20Rectification%20for%20Robust%20Calibration%20under%20Distribution%20Shift%0AAuthor%3A%20Yilin%20Zhang%20and%20Cai%20Xu%20and%20You%20Wu%20and%20Ziyu%20Guan%20and%20Wei%20Zhao%0AAbstract%3A%20%20%20Deep%20neural%20networks%20often%20produce%20overconfident%20predictions%2C%20undermining%0Atheir%20reliability%20in%20safety-critical%20applications.%20This%20miscalibration%20is%0Afurther%20exacerbated%20under%20distribution%20shift%2C%20where%20test%20data%20deviates%20from%20the%0Atraining%20distribution%20due%20to%20environmental%20or%20acquisition%20changes.%20While%0Aexisting%20approaches%20improve%20calibration%20through%20training-time%20regularization%20or%0Apost-hoc%20adjustment%2C%20their%20reliance%20on%20access%20to%20or%20simulation%20of%20target%0Adomains%20limits%20their%20practicality%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20calibration%20framework%20that%20operates%20without%20access%20to%20target%0Adomain%20information.%20From%20a%20frequency-domain%20perspective%2C%20we%20identify%20that%0Adistribution%20shifts%20often%20distort%20high-frequency%20visual%20cues%20exploited%20by%20deep%0Amodels%2C%20and%20introduce%20a%20low-frequency%20filtering%20strategy%20to%20encourage%20reliance%0Aon%20domain-invariant%20features.%20However%2C%20such%20information%20loss%20may%20degrade%0AIn-Distribution%20%28ID%29%20calibration%20performance.%20Therefore%2C%20we%20further%20propose%20a%0Agradient-based%20rectification%20mechanism%20that%20enforces%20ID%20calibration%20as%20a%20hard%0Aconstraint%20during%20optimization.%20Experiments%20on%20synthetic%20and%20real-world%20shifted%0Adatasets%2C%20including%20CIFAR-10/100-C%20and%20WILDS%2C%20demonstrate%20that%20our%20method%0Asignificantly%20improves%20calibration%20under%20distribution%20shift%20while%20maintaining%0Astrong%20in-distribution%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Rectification%2520for%2520Robust%2520Calibration%2520under%2520Distribution%2520Shift%26entry.906535625%3DYilin%2520Zhang%2520and%2520Cai%2520Xu%2520and%2520You%2520Wu%2520and%2520Ziyu%2520Guan%2520and%2520Wei%2520Zhao%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520often%2520produce%2520overconfident%2520predictions%252C%2520undermining%250Atheir%2520reliability%2520in%2520safety-critical%2520applications.%2520This%2520miscalibration%2520is%250Afurther%2520exacerbated%2520under%2520distribution%2520shift%252C%2520where%2520test%2520data%2520deviates%2520from%2520the%250Atraining%2520distribution%2520due%2520to%2520environmental%2520or%2520acquisition%2520changes.%2520While%250Aexisting%2520approaches%2520improve%2520calibration%2520through%2520training-time%2520regularization%2520or%250Apost-hoc%2520adjustment%252C%2520their%2520reliance%2520on%2520access%2520to%2520or%2520simulation%2520of%2520target%250Adomains%2520limits%2520their%2520practicality%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520calibration%2520framework%2520that%2520operates%2520without%2520access%2520to%2520target%250Adomain%2520information.%2520From%2520a%2520frequency-domain%2520perspective%252C%2520we%2520identify%2520that%250Adistribution%2520shifts%2520often%2520distort%2520high-frequency%2520visual%2520cues%2520exploited%2520by%2520deep%250Amodels%252C%2520and%2520introduce%2520a%2520low-frequency%2520filtering%2520strategy%2520to%2520encourage%2520reliance%250Aon%2520domain-invariant%2520features.%2520However%252C%2520such%2520information%2520loss%2520may%2520degrade%250AIn-Distribution%2520%2528ID%2529%2520calibration%2520performance.%2520Therefore%252C%2520we%2520further%2520propose%2520a%250Agradient-based%2520rectification%2520mechanism%2520that%2520enforces%2520ID%2520calibration%2520as%2520a%2520hard%250Aconstraint%2520during%2520optimization.%2520Experiments%2520on%2520synthetic%2520and%2520real-world%2520shifted%250Adatasets%252C%2520including%2520CIFAR-10/100-C%2520and%2520WILDS%252C%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520improves%2520calibration%2520under%2520distribution%2520shift%2520while%2520maintaining%250Astrong%2520in-distribution%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Rectification%20for%20Robust%20Calibration%20under%20Distribution%20Shift&entry.906535625=Yilin%20Zhang%20and%20Cai%20Xu%20and%20You%20Wu%20and%20Ziyu%20Guan%20and%20Wei%20Zhao&entry.1292438233=%20%20Deep%20neural%20networks%20often%20produce%20overconfident%20predictions%2C%20undermining%0Atheir%20reliability%20in%20safety-critical%20applications.%20This%20miscalibration%20is%0Afurther%20exacerbated%20under%20distribution%20shift%2C%20where%20test%20data%20deviates%20from%20the%0Atraining%20distribution%20due%20to%20environmental%20or%20acquisition%20changes.%20While%0Aexisting%20approaches%20improve%20calibration%20through%20training-time%20regularization%20or%0Apost-hoc%20adjustment%2C%20their%20reliance%20on%20access%20to%20or%20simulation%20of%20target%0Adomains%20limits%20their%20practicality%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20calibration%20framework%20that%20operates%20without%20access%20to%20target%0Adomain%20information.%20From%20a%20frequency-domain%20perspective%2C%20we%20identify%20that%0Adistribution%20shifts%20often%20distort%20high-frequency%20visual%20cues%20exploited%20by%20deep%0Amodels%2C%20and%20introduce%20a%20low-frequency%20filtering%20strategy%20to%20encourage%20reliance%0Aon%20domain-invariant%20features.%20However%2C%20such%20information%20loss%20may%20degrade%0AIn-Distribution%20%28ID%29%20calibration%20performance.%20Therefore%2C%20we%20further%20propose%20a%0Agradient-based%20rectification%20mechanism%20that%20enforces%20ID%20calibration%20as%20a%20hard%0Aconstraint%20during%20optimization.%20Experiments%20on%20synthetic%20and%20real-world%20shifted%0Adatasets%2C%20including%20CIFAR-10/100-C%20and%20WILDS%2C%20demonstrate%20that%20our%20method%0Asignificantly%20improves%20calibration%20under%20distribution%20shift%20while%20maintaining%0Astrong%20in-distribution%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19830v1&entry.124074799=Read"},
{"title": "Streamlining the Development of Active Learning Methods in Real-World\n  Object Detection", "author": "Moussa Kassem Sbeyti and Nadja Klein and Michelle Karg and Christian Wirth and Sahin Albayrak", "abstract": "  Active learning (AL) for real-world object detection faces computational and\nreliability challenges that limit practical deployment. Developing new AL\nmethods requires training multiple detectors across iterations to compare\nagainst existing approaches. This creates high costs for autonomous driving\ndatasets where the training of one detector requires up to 282 GPU hours.\nAdditionally, AL method rankings vary substantially across validation sets,\ncompromising reliability in safety-critical transportation systems. We\nintroduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses\nthese challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without\nrequiring detector training by measuring similarity between training sets and\ntarget domains using object-level features. This enables the elimination of\nineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables\nthe selection of representative validation sets for robust evaluation. We\nvalidate our similarity-based approach on three autonomous driving datasets\n(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with\ntwo detector architectures (EfficientDet, YOLOv3). This work is the first to\nunify AL training and evaluation strategies in object detection based on object\nsimilarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object\ncrops, and integrates with existing AL pipelines. This provides a practical\nframework for deploying AL in real-world applications where computational\nefficiency and evaluation reliability are critical. Code is available at\nhttps://mos-ks.github.io/publications/.\n", "link": "http://arxiv.org/abs/2508.19906v1", "date": "2025-08-27", "relevancy": 2.1383, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.538}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5242}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streamlining%20the%20Development%20of%20Active%20Learning%20Methods%20in%20Real-World%0A%20%20Object%20Detection&body=Title%3A%20Streamlining%20the%20Development%20of%20Active%20Learning%20Methods%20in%20Real-World%0A%20%20Object%20Detection%0AAuthor%3A%20Moussa%20Kassem%20Sbeyti%20and%20Nadja%20Klein%20and%20Michelle%20Karg%20and%20Christian%20Wirth%20and%20Sahin%20Albayrak%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20for%20real-world%20object%20detection%20faces%20computational%20and%0Areliability%20challenges%20that%20limit%20practical%20deployment.%20Developing%20new%20AL%0Amethods%20requires%20training%20multiple%20detectors%20across%20iterations%20to%20compare%0Aagainst%20existing%20approaches.%20This%20creates%20high%20costs%20for%20autonomous%20driving%0Adatasets%20where%20the%20training%20of%20one%20detector%20requires%20up%20to%20282%20GPU%20hours.%0AAdditionally%2C%20AL%20method%20rankings%20vary%20substantially%20across%20validation%20sets%2C%0Acompromising%20reliability%20in%20safety-critical%20transportation%20systems.%20We%0Aintroduce%20object-based%20set%20similarity%20%28%24%5Cmathrm%7BOSS%7D%24%29%2C%20a%20metric%20that%20addresses%0Athese%20challenges.%20%24%5Cmathrm%7BOSS%7D%24%20%281%29%20quantifies%20AL%20method%20effectiveness%20without%0Arequiring%20detector%20training%20by%20measuring%20similarity%20between%20training%20sets%20and%0Atarget%20domains%20using%20object-level%20features.%20This%20enables%20the%20elimination%20of%0Aineffective%20AL%20methods%20before%20training.%20Furthermore%2C%20%24%5Cmathrm%7BOSS%7D%24%20%282%29%20enables%0Athe%20selection%20of%20representative%20validation%20sets%20for%20robust%20evaluation.%20We%0Avalidate%20our%20similarity-based%20approach%20on%20three%20autonomous%20driving%20datasets%0A%28KITTI%2C%20BDD100K%2C%20CODA%29%20using%20uncertainty-based%20AL%20methods%20as%20a%20case%20study%20with%0Atwo%20detector%20architectures%20%28EfficientDet%2C%20YOLOv3%29.%20This%20work%20is%20the%20first%20to%0Aunify%20AL%20training%20and%20evaluation%20strategies%20in%20object%20detection%20based%20on%20object%0Asimilarity.%20%24%5Cmathrm%7BOSS%7D%24%20is%20detector-agnostic%2C%20requires%20only%20labeled%20object%0Acrops%2C%20and%20integrates%20with%20existing%20AL%20pipelines.%20This%20provides%20a%20practical%0Aframework%20for%20deploying%20AL%20in%20real-world%20applications%20where%20computational%0Aefficiency%20and%20evaluation%20reliability%20are%20critical.%20Code%20is%20available%20at%0Ahttps%3A//mos-ks.github.io/publications/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamlining%2520the%2520Development%2520of%2520Active%2520Learning%2520Methods%2520in%2520Real-World%250A%2520%2520Object%2520Detection%26entry.906535625%3DMoussa%2520Kassem%2520Sbeyti%2520and%2520Nadja%2520Klein%2520and%2520Michelle%2520Karg%2520and%2520Christian%2520Wirth%2520and%2520Sahin%2520Albayrak%26entry.1292438233%3D%2520%2520Active%2520learning%2520%2528AL%2529%2520for%2520real-world%2520object%2520detection%2520faces%2520computational%2520and%250Areliability%2520challenges%2520that%2520limit%2520practical%2520deployment.%2520Developing%2520new%2520AL%250Amethods%2520requires%2520training%2520multiple%2520detectors%2520across%2520iterations%2520to%2520compare%250Aagainst%2520existing%2520approaches.%2520This%2520creates%2520high%2520costs%2520for%2520autonomous%2520driving%250Adatasets%2520where%2520the%2520training%2520of%2520one%2520detector%2520requires%2520up%2520to%2520282%2520GPU%2520hours.%250AAdditionally%252C%2520AL%2520method%2520rankings%2520vary%2520substantially%2520across%2520validation%2520sets%252C%250Acompromising%2520reliability%2520in%2520safety-critical%2520transportation%2520systems.%2520We%250Aintroduce%2520object-based%2520set%2520similarity%2520%2528%2524%255Cmathrm%257BOSS%257D%2524%2529%252C%2520a%2520metric%2520that%2520addresses%250Athese%2520challenges.%2520%2524%255Cmathrm%257BOSS%257D%2524%2520%25281%2529%2520quantifies%2520AL%2520method%2520effectiveness%2520without%250Arequiring%2520detector%2520training%2520by%2520measuring%2520similarity%2520between%2520training%2520sets%2520and%250Atarget%2520domains%2520using%2520object-level%2520features.%2520This%2520enables%2520the%2520elimination%2520of%250Aineffective%2520AL%2520methods%2520before%2520training.%2520Furthermore%252C%2520%2524%255Cmathrm%257BOSS%257D%2524%2520%25282%2529%2520enables%250Athe%2520selection%2520of%2520representative%2520validation%2520sets%2520for%2520robust%2520evaluation.%2520We%250Avalidate%2520our%2520similarity-based%2520approach%2520on%2520three%2520autonomous%2520driving%2520datasets%250A%2528KITTI%252C%2520BDD100K%252C%2520CODA%2529%2520using%2520uncertainty-based%2520AL%2520methods%2520as%2520a%2520case%2520study%2520with%250Atwo%2520detector%2520architectures%2520%2528EfficientDet%252C%2520YOLOv3%2529.%2520This%2520work%2520is%2520the%2520first%2520to%250Aunify%2520AL%2520training%2520and%2520evaluation%2520strategies%2520in%2520object%2520detection%2520based%2520on%2520object%250Asimilarity.%2520%2524%255Cmathrm%257BOSS%257D%2524%2520is%2520detector-agnostic%252C%2520requires%2520only%2520labeled%2520object%250Acrops%252C%2520and%2520integrates%2520with%2520existing%2520AL%2520pipelines.%2520This%2520provides%2520a%2520practical%250Aframework%2520for%2520deploying%2520AL%2520in%2520real-world%2520applications%2520where%2520computational%250Aefficiency%2520and%2520evaluation%2520reliability%2520are%2520critical.%2520Code%2520is%2520available%2520at%250Ahttps%253A//mos-ks.github.io/publications/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streamlining%20the%20Development%20of%20Active%20Learning%20Methods%20in%20Real-World%0A%20%20Object%20Detection&entry.906535625=Moussa%20Kassem%20Sbeyti%20and%20Nadja%20Klein%20and%20Michelle%20Karg%20and%20Christian%20Wirth%20and%20Sahin%20Albayrak&entry.1292438233=%20%20Active%20learning%20%28AL%29%20for%20real-world%20object%20detection%20faces%20computational%20and%0Areliability%20challenges%20that%20limit%20practical%20deployment.%20Developing%20new%20AL%0Amethods%20requires%20training%20multiple%20detectors%20across%20iterations%20to%20compare%0Aagainst%20existing%20approaches.%20This%20creates%20high%20costs%20for%20autonomous%20driving%0Adatasets%20where%20the%20training%20of%20one%20detector%20requires%20up%20to%20282%20GPU%20hours.%0AAdditionally%2C%20AL%20method%20rankings%20vary%20substantially%20across%20validation%20sets%2C%0Acompromising%20reliability%20in%20safety-critical%20transportation%20systems.%20We%0Aintroduce%20object-based%20set%20similarity%20%28%24%5Cmathrm%7BOSS%7D%24%29%2C%20a%20metric%20that%20addresses%0Athese%20challenges.%20%24%5Cmathrm%7BOSS%7D%24%20%281%29%20quantifies%20AL%20method%20effectiveness%20without%0Arequiring%20detector%20training%20by%20measuring%20similarity%20between%20training%20sets%20and%0Atarget%20domains%20using%20object-level%20features.%20This%20enables%20the%20elimination%20of%0Aineffective%20AL%20methods%20before%20training.%20Furthermore%2C%20%24%5Cmathrm%7BOSS%7D%24%20%282%29%20enables%0Athe%20selection%20of%20representative%20validation%20sets%20for%20robust%20evaluation.%20We%0Avalidate%20our%20similarity-based%20approach%20on%20three%20autonomous%20driving%20datasets%0A%28KITTI%2C%20BDD100K%2C%20CODA%29%20using%20uncertainty-based%20AL%20methods%20as%20a%20case%20study%20with%0Atwo%20detector%20architectures%20%28EfficientDet%2C%20YOLOv3%29.%20This%20work%20is%20the%20first%20to%0Aunify%20AL%20training%20and%20evaluation%20strategies%20in%20object%20detection%20based%20on%20object%0Asimilarity.%20%24%5Cmathrm%7BOSS%7D%24%20is%20detector-agnostic%2C%20requires%20only%20labeled%20object%0Acrops%2C%20and%20integrates%20with%20existing%20AL%20pipelines.%20This%20provides%20a%20practical%0Aframework%20for%20deploying%20AL%20in%20real-world%20applications%20where%20computational%0Aefficiency%20and%20evaluation%20reliability%20are%20critical.%20Code%20is%20available%20at%0Ahttps%3A//mos-ks.github.io/publications/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19906v1&entry.124074799=Read"},
{"title": "Synthesizing High-Quality Programming Tasks with LLM-based Expert and\n  Student Agents", "author": "Manh Hung Nguyen and Victor-Alexandru P\u0103durean and Alkis Gotovos and Sebastian Tschiatschek and Adish Singla", "abstract": "  Generative AI is transforming computing education by enabling the automatic\ngeneration of personalized content and feedback. We investigate its\ncapabilities in providing high-quality programming tasks to students. Despite\npromising advancements in task generation, a quality gap remains between\nAI-generated and expert-created tasks. The AI-generated tasks may not align\nwith target programming concepts, could be incomprehensible to students, or may\ncontain critical issues such as incorrect tests. Existing works often require\ninterventions from human teachers for validation. We address these challenges\nby introducing PyTaskSyn, a novel synthesis technique that first generates a\nprogramming task and then decides whether it meets certain quality criteria to\nbe given to students. The key idea is to break this process into multiple\nstages performed by expert and student agents simulated using both strong and\nweaker generative models. Through extensive evaluation, we show that PyTaskSyn\nsignificantly improves task quality compared to baseline techniques and\nshowcases the importance of each specialized agent type in our validation\npipeline. Additionally, we conducted user studies using our publicly available\nweb application and show that PyTaskSyn can deliver high-quality programming\ntasks comparable to expert-designed ones while reducing workload and costs, and\nbeing more engaging than programming tasks that are available in online\nresources.\n", "link": "http://arxiv.org/abs/2504.07655v2", "date": "2025-08-27", "relevancy": 2.1368, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5361}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5342}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20High-Quality%20Programming%20Tasks%20with%20LLM-based%20Expert%20and%0A%20%20Student%20Agents&body=Title%3A%20Synthesizing%20High-Quality%20Programming%20Tasks%20with%20LLM-based%20Expert%20and%0A%20%20Student%20Agents%0AAuthor%3A%20Manh%20Hung%20Nguyen%20and%20Victor-Alexandru%20P%C4%83durean%20and%20Alkis%20Gotovos%20and%20Sebastian%20Tschiatschek%20and%20Adish%20Singla%0AAbstract%3A%20%20%20Generative%20AI%20is%20transforming%20computing%20education%20by%20enabling%20the%20automatic%0Ageneration%20of%20personalized%20content%20and%20feedback.%20We%20investigate%20its%0Acapabilities%20in%20providing%20high-quality%20programming%20tasks%20to%20students.%20Despite%0Apromising%20advancements%20in%20task%20generation%2C%20a%20quality%20gap%20remains%20between%0AAI-generated%20and%20expert-created%20tasks.%20The%20AI-generated%20tasks%20may%20not%20align%0Awith%20target%20programming%20concepts%2C%20could%20be%20incomprehensible%20to%20students%2C%20or%20may%0Acontain%20critical%20issues%20such%20as%20incorrect%20tests.%20Existing%20works%20often%20require%0Ainterventions%20from%20human%20teachers%20for%20validation.%20We%20address%20these%20challenges%0Aby%20introducing%20PyTaskSyn%2C%20a%20novel%20synthesis%20technique%20that%20first%20generates%20a%0Aprogramming%20task%20and%20then%20decides%20whether%20it%20meets%20certain%20quality%20criteria%20to%0Abe%20given%20to%20students.%20The%20key%20idea%20is%20to%20break%20this%20process%20into%20multiple%0Astages%20performed%20by%20expert%20and%20student%20agents%20simulated%20using%20both%20strong%20and%0Aweaker%20generative%20models.%20Through%20extensive%20evaluation%2C%20we%20show%20that%20PyTaskSyn%0Asignificantly%20improves%20task%20quality%20compared%20to%20baseline%20techniques%20and%0Ashowcases%20the%20importance%20of%20each%20specialized%20agent%20type%20in%20our%20validation%0Apipeline.%20Additionally%2C%20we%20conducted%20user%20studies%20using%20our%20publicly%20available%0Aweb%20application%20and%20show%20that%20PyTaskSyn%20can%20deliver%20high-quality%20programming%0Atasks%20comparable%20to%20expert-designed%20ones%20while%20reducing%20workload%20and%20costs%2C%20and%0Abeing%20more%20engaging%20than%20programming%20tasks%20that%20are%20available%20in%20online%0Aresources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizing%2520High-Quality%2520Programming%2520Tasks%2520with%2520LLM-based%2520Expert%2520and%250A%2520%2520Student%2520Agents%26entry.906535625%3DManh%2520Hung%2520Nguyen%2520and%2520Victor-Alexandru%2520P%25C4%2583durean%2520and%2520Alkis%2520Gotovos%2520and%2520Sebastian%2520Tschiatschek%2520and%2520Adish%2520Singla%26entry.1292438233%3D%2520%2520Generative%2520AI%2520is%2520transforming%2520computing%2520education%2520by%2520enabling%2520the%2520automatic%250Ageneration%2520of%2520personalized%2520content%2520and%2520feedback.%2520We%2520investigate%2520its%250Acapabilities%2520in%2520providing%2520high-quality%2520programming%2520tasks%2520to%2520students.%2520Despite%250Apromising%2520advancements%2520in%2520task%2520generation%252C%2520a%2520quality%2520gap%2520remains%2520between%250AAI-generated%2520and%2520expert-created%2520tasks.%2520The%2520AI-generated%2520tasks%2520may%2520not%2520align%250Awith%2520target%2520programming%2520concepts%252C%2520could%2520be%2520incomprehensible%2520to%2520students%252C%2520or%2520may%250Acontain%2520critical%2520issues%2520such%2520as%2520incorrect%2520tests.%2520Existing%2520works%2520often%2520require%250Ainterventions%2520from%2520human%2520teachers%2520for%2520validation.%2520We%2520address%2520these%2520challenges%250Aby%2520introducing%2520PyTaskSyn%252C%2520a%2520novel%2520synthesis%2520technique%2520that%2520first%2520generates%2520a%250Aprogramming%2520task%2520and%2520then%2520decides%2520whether%2520it%2520meets%2520certain%2520quality%2520criteria%2520to%250Abe%2520given%2520to%2520students.%2520The%2520key%2520idea%2520is%2520to%2520break%2520this%2520process%2520into%2520multiple%250Astages%2520performed%2520by%2520expert%2520and%2520student%2520agents%2520simulated%2520using%2520both%2520strong%2520and%250Aweaker%2520generative%2520models.%2520Through%2520extensive%2520evaluation%252C%2520we%2520show%2520that%2520PyTaskSyn%250Asignificantly%2520improves%2520task%2520quality%2520compared%2520to%2520baseline%2520techniques%2520and%250Ashowcases%2520the%2520importance%2520of%2520each%2520specialized%2520agent%2520type%2520in%2520our%2520validation%250Apipeline.%2520Additionally%252C%2520we%2520conducted%2520user%2520studies%2520using%2520our%2520publicly%2520available%250Aweb%2520application%2520and%2520show%2520that%2520PyTaskSyn%2520can%2520deliver%2520high-quality%2520programming%250Atasks%2520comparable%2520to%2520expert-designed%2520ones%2520while%2520reducing%2520workload%2520and%2520costs%252C%2520and%250Abeing%2520more%2520engaging%2520than%2520programming%2520tasks%2520that%2520are%2520available%2520in%2520online%250Aresources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20High-Quality%20Programming%20Tasks%20with%20LLM-based%20Expert%20and%0A%20%20Student%20Agents&entry.906535625=Manh%20Hung%20Nguyen%20and%20Victor-Alexandru%20P%C4%83durean%20and%20Alkis%20Gotovos%20and%20Sebastian%20Tschiatschek%20and%20Adish%20Singla&entry.1292438233=%20%20Generative%20AI%20is%20transforming%20computing%20education%20by%20enabling%20the%20automatic%0Ageneration%20of%20personalized%20content%20and%20feedback.%20We%20investigate%20its%0Acapabilities%20in%20providing%20high-quality%20programming%20tasks%20to%20students.%20Despite%0Apromising%20advancements%20in%20task%20generation%2C%20a%20quality%20gap%20remains%20between%0AAI-generated%20and%20expert-created%20tasks.%20The%20AI-generated%20tasks%20may%20not%20align%0Awith%20target%20programming%20concepts%2C%20could%20be%20incomprehensible%20to%20students%2C%20or%20may%0Acontain%20critical%20issues%20such%20as%20incorrect%20tests.%20Existing%20works%20often%20require%0Ainterventions%20from%20human%20teachers%20for%20validation.%20We%20address%20these%20challenges%0Aby%20introducing%20PyTaskSyn%2C%20a%20novel%20synthesis%20technique%20that%20first%20generates%20a%0Aprogramming%20task%20and%20then%20decides%20whether%20it%20meets%20certain%20quality%20criteria%20to%0Abe%20given%20to%20students.%20The%20key%20idea%20is%20to%20break%20this%20process%20into%20multiple%0Astages%20performed%20by%20expert%20and%20student%20agents%20simulated%20using%20both%20strong%20and%0Aweaker%20generative%20models.%20Through%20extensive%20evaluation%2C%20we%20show%20that%20PyTaskSyn%0Asignificantly%20improves%20task%20quality%20compared%20to%20baseline%20techniques%20and%0Ashowcases%20the%20importance%20of%20each%20specialized%20agent%20type%20in%20our%20validation%0Apipeline.%20Additionally%2C%20we%20conducted%20user%20studies%20using%20our%20publicly%20available%0Aweb%20application%20and%20show%20that%20PyTaskSyn%20can%20deliver%20high-quality%20programming%0Atasks%20comparable%20to%20expert-designed%20ones%20while%20reducing%20workload%20and%20costs%2C%20and%0Abeing%20more%20engaging%20than%20programming%20tasks%20that%20are%20available%20in%20online%0Aresources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07655v2&entry.124074799=Read"},
{"title": "AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning", "author": "Shu Shen and C. L. Philip Chen and Tong Zhang", "abstract": "  Multimodal learning has significantly enhanced machine learning performance\nbut still faces numerous challenges and limitations. Imbalanced multimodal\nlearning is one of the problems extensively studied in recent works and is\ntypically mitigated by modulating the learning of each modality. However, we\nfind that these methods typically hinder the dominant modality's learning to\npromote weaker modalities, which affects overall multimodal performance. We\nanalyze the cause of this issue and highlight a commonly overlooked problem:\noptimization bias within networks. To address this, we propose Adaptive\nIntra-Network Modulation (AIM) to improve balanced modality learning. AIM\naccounts for differences in optimization state across parameters and depths\nwithin the network during modulation, achieving balanced multimodal learning\nwithout hindering either dominant or weak modalities for the first time.\nSpecifically, AIM decouples the dominant modality's under-optimized parameters\ninto Auxiliary Blocks and encourages reliance on these performance-degraded\nblocks for joint training with weaker modalities. This approach effectively\nprevents suppression of weaker modalities while enabling targeted optimization\nof under-optimized parameters to improve the dominant modality. Additionally,\nAIM assesses modality imbalance level across network depths and adaptively\nadjusts modulation strength at each depth. Experimental results demonstrate\nthat AIM outperforms state-of-the-art imbalanced modality learning methods\nacross multiple benchmarks and exhibits strong generalizability across\ndifferent backbones, fusion strategies, and optimizers.\n", "link": "http://arxiv.org/abs/2508.19769v1", "date": "2025-08-27", "relevancy": 2.1308, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5321}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIM%3A%20Adaptive%20Intra-Network%20Modulation%20for%20Balanced%20Multimodal%20Learning&body=Title%3A%20AIM%3A%20Adaptive%20Intra-Network%20Modulation%20for%20Balanced%20Multimodal%20Learning%0AAuthor%3A%20Shu%20Shen%20and%20C.%20L.%20Philip%20Chen%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Multimodal%20learning%20has%20significantly%20enhanced%20machine%20learning%20performance%0Abut%20still%20faces%20numerous%20challenges%20and%20limitations.%20Imbalanced%20multimodal%0Alearning%20is%20one%20of%20the%20problems%20extensively%20studied%20in%20recent%20works%20and%20is%0Atypically%20mitigated%20by%20modulating%20the%20learning%20of%20each%20modality.%20However%2C%20we%0Afind%20that%20these%20methods%20typically%20hinder%20the%20dominant%20modality%27s%20learning%20to%0Apromote%20weaker%20modalities%2C%20which%20affects%20overall%20multimodal%20performance.%20We%0Aanalyze%20the%20cause%20of%20this%20issue%20and%20highlight%20a%20commonly%20overlooked%20problem%3A%0Aoptimization%20bias%20within%20networks.%20To%20address%20this%2C%20we%20propose%20Adaptive%0AIntra-Network%20Modulation%20%28AIM%29%20to%20improve%20balanced%20modality%20learning.%20AIM%0Aaccounts%20for%20differences%20in%20optimization%20state%20across%20parameters%20and%20depths%0Awithin%20the%20network%20during%20modulation%2C%20achieving%20balanced%20multimodal%20learning%0Awithout%20hindering%20either%20dominant%20or%20weak%20modalities%20for%20the%20first%20time.%0ASpecifically%2C%20AIM%20decouples%20the%20dominant%20modality%27s%20under-optimized%20parameters%0Ainto%20Auxiliary%20Blocks%20and%20encourages%20reliance%20on%20these%20performance-degraded%0Ablocks%20for%20joint%20training%20with%20weaker%20modalities.%20This%20approach%20effectively%0Aprevents%20suppression%20of%20weaker%20modalities%20while%20enabling%20targeted%20optimization%0Aof%20under-optimized%20parameters%20to%20improve%20the%20dominant%20modality.%20Additionally%2C%0AAIM%20assesses%20modality%20imbalance%20level%20across%20network%20depths%20and%20adaptively%0Aadjusts%20modulation%20strength%20at%20each%20depth.%20Experimental%20results%20demonstrate%0Athat%20AIM%20outperforms%20state-of-the-art%20imbalanced%20modality%20learning%20methods%0Aacross%20multiple%20benchmarks%20and%20exhibits%20strong%20generalizability%20across%0Adifferent%20backbones%2C%20fusion%20strategies%2C%20and%20optimizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIM%253A%2520Adaptive%2520Intra-Network%2520Modulation%2520for%2520Balanced%2520Multimodal%2520Learning%26entry.906535625%3DShu%2520Shen%2520and%2520C.%2520L.%2520Philip%2520Chen%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520has%2520significantly%2520enhanced%2520machine%2520learning%2520performance%250Abut%2520still%2520faces%2520numerous%2520challenges%2520and%2520limitations.%2520Imbalanced%2520multimodal%250Alearning%2520is%2520one%2520of%2520the%2520problems%2520extensively%2520studied%2520in%2520recent%2520works%2520and%2520is%250Atypically%2520mitigated%2520by%2520modulating%2520the%2520learning%2520of%2520each%2520modality.%2520However%252C%2520we%250Afind%2520that%2520these%2520methods%2520typically%2520hinder%2520the%2520dominant%2520modality%2527s%2520learning%2520to%250Apromote%2520weaker%2520modalities%252C%2520which%2520affects%2520overall%2520multimodal%2520performance.%2520We%250Aanalyze%2520the%2520cause%2520of%2520this%2520issue%2520and%2520highlight%2520a%2520commonly%2520overlooked%2520problem%253A%250Aoptimization%2520bias%2520within%2520networks.%2520To%2520address%2520this%252C%2520we%2520propose%2520Adaptive%250AIntra-Network%2520Modulation%2520%2528AIM%2529%2520to%2520improve%2520balanced%2520modality%2520learning.%2520AIM%250Aaccounts%2520for%2520differences%2520in%2520optimization%2520state%2520across%2520parameters%2520and%2520depths%250Awithin%2520the%2520network%2520during%2520modulation%252C%2520achieving%2520balanced%2520multimodal%2520learning%250Awithout%2520hindering%2520either%2520dominant%2520or%2520weak%2520modalities%2520for%2520the%2520first%2520time.%250ASpecifically%252C%2520AIM%2520decouples%2520the%2520dominant%2520modality%2527s%2520under-optimized%2520parameters%250Ainto%2520Auxiliary%2520Blocks%2520and%2520encourages%2520reliance%2520on%2520these%2520performance-degraded%250Ablocks%2520for%2520joint%2520training%2520with%2520weaker%2520modalities.%2520This%2520approach%2520effectively%250Aprevents%2520suppression%2520of%2520weaker%2520modalities%2520while%2520enabling%2520targeted%2520optimization%250Aof%2520under-optimized%2520parameters%2520to%2520improve%2520the%2520dominant%2520modality.%2520Additionally%252C%250AAIM%2520assesses%2520modality%2520imbalance%2520level%2520across%2520network%2520depths%2520and%2520adaptively%250Aadjusts%2520modulation%2520strength%2520at%2520each%2520depth.%2520Experimental%2520results%2520demonstrate%250Athat%2520AIM%2520outperforms%2520state-of-the-art%2520imbalanced%2520modality%2520learning%2520methods%250Aacross%2520multiple%2520benchmarks%2520and%2520exhibits%2520strong%2520generalizability%2520across%250Adifferent%2520backbones%252C%2520fusion%2520strategies%252C%2520and%2520optimizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIM%3A%20Adaptive%20Intra-Network%20Modulation%20for%20Balanced%20Multimodal%20Learning&entry.906535625=Shu%20Shen%20and%20C.%20L.%20Philip%20Chen%20and%20Tong%20Zhang&entry.1292438233=%20%20Multimodal%20learning%20has%20significantly%20enhanced%20machine%20learning%20performance%0Abut%20still%20faces%20numerous%20challenges%20and%20limitations.%20Imbalanced%20multimodal%0Alearning%20is%20one%20of%20the%20problems%20extensively%20studied%20in%20recent%20works%20and%20is%0Atypically%20mitigated%20by%20modulating%20the%20learning%20of%20each%20modality.%20However%2C%20we%0Afind%20that%20these%20methods%20typically%20hinder%20the%20dominant%20modality%27s%20learning%20to%0Apromote%20weaker%20modalities%2C%20which%20affects%20overall%20multimodal%20performance.%20We%0Aanalyze%20the%20cause%20of%20this%20issue%20and%20highlight%20a%20commonly%20overlooked%20problem%3A%0Aoptimization%20bias%20within%20networks.%20To%20address%20this%2C%20we%20propose%20Adaptive%0AIntra-Network%20Modulation%20%28AIM%29%20to%20improve%20balanced%20modality%20learning.%20AIM%0Aaccounts%20for%20differences%20in%20optimization%20state%20across%20parameters%20and%20depths%0Awithin%20the%20network%20during%20modulation%2C%20achieving%20balanced%20multimodal%20learning%0Awithout%20hindering%20either%20dominant%20or%20weak%20modalities%20for%20the%20first%20time.%0ASpecifically%2C%20AIM%20decouples%20the%20dominant%20modality%27s%20under-optimized%20parameters%0Ainto%20Auxiliary%20Blocks%20and%20encourages%20reliance%20on%20these%20performance-degraded%0Ablocks%20for%20joint%20training%20with%20weaker%20modalities.%20This%20approach%20effectively%0Aprevents%20suppression%20of%20weaker%20modalities%20while%20enabling%20targeted%20optimization%0Aof%20under-optimized%20parameters%20to%20improve%20the%20dominant%20modality.%20Additionally%2C%0AAIM%20assesses%20modality%20imbalance%20level%20across%20network%20depths%20and%20adaptively%0Aadjusts%20modulation%20strength%20at%20each%20depth.%20Experimental%20results%20demonstrate%0Athat%20AIM%20outperforms%20state-of-the-art%20imbalanced%20modality%20learning%20methods%0Aacross%20multiple%20benchmarks%20and%20exhibits%20strong%20generalizability%20across%0Adifferent%20backbones%2C%20fusion%20strategies%2C%20and%20optimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19769v1&entry.124074799=Read"},
{"title": "Staircase Recognition and Location Based on Polarization Vision", "author": "Weifeng Kong and Zhiying Tan", "abstract": "  Staircase is one of the most common structures in artificial scenes. However,\nit is difficult for humanoid robots and people with lower limb disabilities or\nvisual impairment to cross the scene without the help of sensors and\nintelligent algorithms. Staircase scene perception technology is a prerequisite\nfor recognition and localization. This technology is of great significance for\nthe mode switching of the robot and the calculation of the footprint position\nto adapt to the discontinuous terrain. However, there are still many problems\nthat constrain the application of this technology, such as low recognition\naccuracy, high initial noise from sensors, unstable output signals and high\ncomputational requirements. In terms of scene reconstruction, the binocular and\ntime of flight (TOF) reconstruction of the scene can be easily affected by\nenvironmental light and the surface material of the target object. In contrast,\ndue to the special structure of the polarizer, the polarization can selectively\ntransmit polarized light in a specific direction and this reconstruction method\nrelies on the polarization information of the object surface. So the advantages\nof polarization reconstruction are reflected, which are less affected by\nenvironmental light and not dependent on the texture information of the object\nsurface. In this paper, in order to achieve the detection of staircase, this\npaper proposes a contrast enhancement algorithm that integrates polarization\nand light intensity information, and integrates point cloud segmentation based\non YOLOv11. To realize the high-quality reconstruction, we proposed a method of\nfusing polarized binocular and TOF depth information to realize the\nthree-dimensional (3D) reconstruction of the staircase. Besides, it also\nproposes a joint calibration algorithm of monocular camera and TOF camera based\non ICP registration and improved gray wolf optimization algorithm.\n", "link": "http://arxiv.org/abs/2505.19026v2", "date": "2025-08-27", "relevancy": 2.1259, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5419}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5391}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Staircase%20Recognition%20and%20Location%20Based%20on%20Polarization%20Vision&body=Title%3A%20Staircase%20Recognition%20and%20Location%20Based%20on%20Polarization%20Vision%0AAuthor%3A%20Weifeng%20Kong%20and%20Zhiying%20Tan%0AAbstract%3A%20%20%20Staircase%20is%20one%20of%20the%20most%20common%20structures%20in%20artificial%20scenes.%20However%2C%0Ait%20is%20difficult%20for%20humanoid%20robots%20and%20people%20with%20lower%20limb%20disabilities%20or%0Avisual%20impairment%20to%20cross%20the%20scene%20without%20the%20help%20of%20sensors%20and%0Aintelligent%20algorithms.%20Staircase%20scene%20perception%20technology%20is%20a%20prerequisite%0Afor%20recognition%20and%20localization.%20This%20technology%20is%20of%20great%20significance%20for%0Athe%20mode%20switching%20of%20the%20robot%20and%20the%20calculation%20of%20the%20footprint%20position%0Ato%20adapt%20to%20the%20discontinuous%20terrain.%20However%2C%20there%20are%20still%20many%20problems%0Athat%20constrain%20the%20application%20of%20this%20technology%2C%20such%20as%20low%20recognition%0Aaccuracy%2C%20high%20initial%20noise%20from%20sensors%2C%20unstable%20output%20signals%20and%20high%0Acomputational%20requirements.%20In%20terms%20of%20scene%20reconstruction%2C%20the%20binocular%20and%0Atime%20of%20flight%20%28TOF%29%20reconstruction%20of%20the%20scene%20can%20be%20easily%20affected%20by%0Aenvironmental%20light%20and%20the%20surface%20material%20of%20the%20target%20object.%20In%20contrast%2C%0Adue%20to%20the%20special%20structure%20of%20the%20polarizer%2C%20the%20polarization%20can%20selectively%0Atransmit%20polarized%20light%20in%20a%20specific%20direction%20and%20this%20reconstruction%20method%0Arelies%20on%20the%20polarization%20information%20of%20the%20object%20surface.%20So%20the%20advantages%0Aof%20polarization%20reconstruction%20are%20reflected%2C%20which%20are%20less%20affected%20by%0Aenvironmental%20light%20and%20not%20dependent%20on%20the%20texture%20information%20of%20the%20object%0Asurface.%20In%20this%20paper%2C%20in%20order%20to%20achieve%20the%20detection%20of%20staircase%2C%20this%0Apaper%20proposes%20a%20contrast%20enhancement%20algorithm%20that%20integrates%20polarization%0Aand%20light%20intensity%20information%2C%20and%20integrates%20point%20cloud%20segmentation%20based%0Aon%20YOLOv11.%20To%20realize%20the%20high-quality%20reconstruction%2C%20we%20proposed%20a%20method%20of%0Afusing%20polarized%20binocular%20and%20TOF%20depth%20information%20to%20realize%20the%0Athree-dimensional%20%283D%29%20reconstruction%20of%20the%20staircase.%20Besides%2C%20it%20also%0Aproposes%20a%20joint%20calibration%20algorithm%20of%20monocular%20camera%20and%20TOF%20camera%20based%0Aon%20ICP%20registration%20and%20improved%20gray%20wolf%20optimization%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19026v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStaircase%2520Recognition%2520and%2520Location%2520Based%2520on%2520Polarization%2520Vision%26entry.906535625%3DWeifeng%2520Kong%2520and%2520Zhiying%2520Tan%26entry.1292438233%3D%2520%2520Staircase%2520is%2520one%2520of%2520the%2520most%2520common%2520structures%2520in%2520artificial%2520scenes.%2520However%252C%250Ait%2520is%2520difficult%2520for%2520humanoid%2520robots%2520and%2520people%2520with%2520lower%2520limb%2520disabilities%2520or%250Avisual%2520impairment%2520to%2520cross%2520the%2520scene%2520without%2520the%2520help%2520of%2520sensors%2520and%250Aintelligent%2520algorithms.%2520Staircase%2520scene%2520perception%2520technology%2520is%2520a%2520prerequisite%250Afor%2520recognition%2520and%2520localization.%2520This%2520technology%2520is%2520of%2520great%2520significance%2520for%250Athe%2520mode%2520switching%2520of%2520the%2520robot%2520and%2520the%2520calculation%2520of%2520the%2520footprint%2520position%250Ato%2520adapt%2520to%2520the%2520discontinuous%2520terrain.%2520However%252C%2520there%2520are%2520still%2520many%2520problems%250Athat%2520constrain%2520the%2520application%2520of%2520this%2520technology%252C%2520such%2520as%2520low%2520recognition%250Aaccuracy%252C%2520high%2520initial%2520noise%2520from%2520sensors%252C%2520unstable%2520output%2520signals%2520and%2520high%250Acomputational%2520requirements.%2520In%2520terms%2520of%2520scene%2520reconstruction%252C%2520the%2520binocular%2520and%250Atime%2520of%2520flight%2520%2528TOF%2529%2520reconstruction%2520of%2520the%2520scene%2520can%2520be%2520easily%2520affected%2520by%250Aenvironmental%2520light%2520and%2520the%2520surface%2520material%2520of%2520the%2520target%2520object.%2520In%2520contrast%252C%250Adue%2520to%2520the%2520special%2520structure%2520of%2520the%2520polarizer%252C%2520the%2520polarization%2520can%2520selectively%250Atransmit%2520polarized%2520light%2520in%2520a%2520specific%2520direction%2520and%2520this%2520reconstruction%2520method%250Arelies%2520on%2520the%2520polarization%2520information%2520of%2520the%2520object%2520surface.%2520So%2520the%2520advantages%250Aof%2520polarization%2520reconstruction%2520are%2520reflected%252C%2520which%2520are%2520less%2520affected%2520by%250Aenvironmental%2520light%2520and%2520not%2520dependent%2520on%2520the%2520texture%2520information%2520of%2520the%2520object%250Asurface.%2520In%2520this%2520paper%252C%2520in%2520order%2520to%2520achieve%2520the%2520detection%2520of%2520staircase%252C%2520this%250Apaper%2520proposes%2520a%2520contrast%2520enhancement%2520algorithm%2520that%2520integrates%2520polarization%250Aand%2520light%2520intensity%2520information%252C%2520and%2520integrates%2520point%2520cloud%2520segmentation%2520based%250Aon%2520YOLOv11.%2520To%2520realize%2520the%2520high-quality%2520reconstruction%252C%2520we%2520proposed%2520a%2520method%2520of%250Afusing%2520polarized%2520binocular%2520and%2520TOF%2520depth%2520information%2520to%2520realize%2520the%250Athree-dimensional%2520%25283D%2529%2520reconstruction%2520of%2520the%2520staircase.%2520Besides%252C%2520it%2520also%250Aproposes%2520a%2520joint%2520calibration%2520algorithm%2520of%2520monocular%2520camera%2520and%2520TOF%2520camera%2520based%250Aon%2520ICP%2520registration%2520and%2520improved%2520gray%2520wolf%2520optimization%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19026v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Staircase%20Recognition%20and%20Location%20Based%20on%20Polarization%20Vision&entry.906535625=Weifeng%20Kong%20and%20Zhiying%20Tan&entry.1292438233=%20%20Staircase%20is%20one%20of%20the%20most%20common%20structures%20in%20artificial%20scenes.%20However%2C%0Ait%20is%20difficult%20for%20humanoid%20robots%20and%20people%20with%20lower%20limb%20disabilities%20or%0Avisual%20impairment%20to%20cross%20the%20scene%20without%20the%20help%20of%20sensors%20and%0Aintelligent%20algorithms.%20Staircase%20scene%20perception%20technology%20is%20a%20prerequisite%0Afor%20recognition%20and%20localization.%20This%20technology%20is%20of%20great%20significance%20for%0Athe%20mode%20switching%20of%20the%20robot%20and%20the%20calculation%20of%20the%20footprint%20position%0Ato%20adapt%20to%20the%20discontinuous%20terrain.%20However%2C%20there%20are%20still%20many%20problems%0Athat%20constrain%20the%20application%20of%20this%20technology%2C%20such%20as%20low%20recognition%0Aaccuracy%2C%20high%20initial%20noise%20from%20sensors%2C%20unstable%20output%20signals%20and%20high%0Acomputational%20requirements.%20In%20terms%20of%20scene%20reconstruction%2C%20the%20binocular%20and%0Atime%20of%20flight%20%28TOF%29%20reconstruction%20of%20the%20scene%20can%20be%20easily%20affected%20by%0Aenvironmental%20light%20and%20the%20surface%20material%20of%20the%20target%20object.%20In%20contrast%2C%0Adue%20to%20the%20special%20structure%20of%20the%20polarizer%2C%20the%20polarization%20can%20selectively%0Atransmit%20polarized%20light%20in%20a%20specific%20direction%20and%20this%20reconstruction%20method%0Arelies%20on%20the%20polarization%20information%20of%20the%20object%20surface.%20So%20the%20advantages%0Aof%20polarization%20reconstruction%20are%20reflected%2C%20which%20are%20less%20affected%20by%0Aenvironmental%20light%20and%20not%20dependent%20on%20the%20texture%20information%20of%20the%20object%0Asurface.%20In%20this%20paper%2C%20in%20order%20to%20achieve%20the%20detection%20of%20staircase%2C%20this%0Apaper%20proposes%20a%20contrast%20enhancement%20algorithm%20that%20integrates%20polarization%0Aand%20light%20intensity%20information%2C%20and%20integrates%20point%20cloud%20segmentation%20based%0Aon%20YOLOv11.%20To%20realize%20the%20high-quality%20reconstruction%2C%20we%20proposed%20a%20method%20of%0Afusing%20polarized%20binocular%20and%20TOF%20depth%20information%20to%20realize%20the%0Athree-dimensional%20%283D%29%20reconstruction%20of%20the%20staircase.%20Besides%2C%20it%20also%0Aproposes%20a%20joint%20calibration%20algorithm%20of%20monocular%20camera%20and%20TOF%20camera%20based%0Aon%20ICP%20registration%20and%20improved%20gray%20wolf%20optimization%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19026v2&entry.124074799=Read"},
{"title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature\n  Fusion for Foggy Conditions", "author": "Ronghui Zhang and Yuhang Ma and Tengfei Li and Ziyu Lin and Yueying Wu and Junzhou Chen and Lin Zhang and Jia Hu and Tony Z. Qiu and Konghui Guo", "abstract": "  Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments.\n", "link": "http://arxiv.org/abs/2504.06121v9", "date": "2025-08-27", "relevancy": 2.1231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5516}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5179}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Robust%20Real-Time%20Lane%20Detection%20Method%20with%20Fog-Enhanced%20Feature%0A%20%20Fusion%20for%20Foggy%20Conditions&body=Title%3A%20A%20Robust%20Real-Time%20Lane%20Detection%20Method%20with%20Fog-Enhanced%20Feature%0A%20%20Fusion%20for%20Foggy%20Conditions%0AAuthor%3A%20Ronghui%20Zhang%20and%20Yuhang%20Ma%20and%20Tengfei%20Li%20and%20Ziyu%20Lin%20and%20Yueying%20Wu%20and%20Junzhou%20Chen%20and%20Lin%20Zhang%20and%20Jia%20Hu%20and%20Tony%20Z.%20Qiu%20and%20Konghui%20Guo%0AAbstract%3A%20%20%20Lane%20detection%20is%20a%20critical%20component%20of%20Advanced%20Driver%20Assistance%20Systems%0A%28ADAS%29.%20Existing%20lane%20detection%20algorithms%20generally%20perform%20well%20under%0Afavorable%20weather%20conditions.%20However%2C%20their%20performance%20degrades%20significantly%0Ain%20adverse%20conditions%2C%20such%20as%20fog%2C%20which%20increases%20the%20risk%20of%20traffic%0Aaccidents.%20This%20challenge%20is%20compounded%20by%20the%20lack%20of%20specialized%20datasets%20and%0Amethods%20designed%20for%20foggy%20environments.%20To%20address%20this%2C%20we%20introduce%20the%0AFoggyLane%20dataset%2C%20captured%20in%20real-world%20foggy%20scenarios%2C%20and%20synthesize%20two%0Aadditional%20datasets%2C%20FoggyCULane%20and%20FoggyTusimple%2C%20from%20existing%20popular%20lane%0Adetection%20datasets.%20Furthermore%2C%20we%20propose%20a%20robust%20Fog-Enhanced%20Network%20for%0Alane%20detection%2C%20incorporating%20a%20Global%20Feature%20Fusion%20Module%20%28GFFM%29%20to%20capture%0Aglobal%20relationships%20in%20foggy%20images%2C%20a%20Kernel%20Feature%20Fusion%20Module%20%28KFFM%29%20to%0Amodel%20the%20structural%20and%20positional%20relationships%20of%20lane%20instances%2C%20and%20a%0ALow-level%20Edge%20Enhanced%20Module%20%28LEEM%29%20to%20address%20missing%20edge%20details%20in%20foggy%0Aconditions.%20Comprehensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%2C%20with%20F1-scores%20of%2095.04%20on%20FoggyLane%2C%2079.85%20on%0AFoggyCULane%2C%20and%2096.95%20on%20FoggyTusimple.%20Additionally%2C%20with%20TensorRT%0Aacceleration%2C%20the%20method%20reaches%20a%20processing%20speed%20of%2038.4%20FPS%20on%20the%20NVIDIA%0AJetson%20AGX%20Orin%2C%20confirming%20its%20real-time%20capabilities%20and%20robustness%20in%20foggy%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06121v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Robust%2520Real-Time%2520Lane%2520Detection%2520Method%2520with%2520Fog-Enhanced%2520Feature%250A%2520%2520Fusion%2520for%2520Foggy%2520Conditions%26entry.906535625%3DRonghui%2520Zhang%2520and%2520Yuhang%2520Ma%2520and%2520Tengfei%2520Li%2520and%2520Ziyu%2520Lin%2520and%2520Yueying%2520Wu%2520and%2520Junzhou%2520Chen%2520and%2520Lin%2520Zhang%2520and%2520Jia%2520Hu%2520and%2520Tony%2520Z.%2520Qiu%2520and%2520Konghui%2520Guo%26entry.1292438233%3D%2520%2520Lane%2520detection%2520is%2520a%2520critical%2520component%2520of%2520Advanced%2520Driver%2520Assistance%2520Systems%250A%2528ADAS%2529.%2520Existing%2520lane%2520detection%2520algorithms%2520generally%2520perform%2520well%2520under%250Afavorable%2520weather%2520conditions.%2520However%252C%2520their%2520performance%2520degrades%2520significantly%250Ain%2520adverse%2520conditions%252C%2520such%2520as%2520fog%252C%2520which%2520increases%2520the%2520risk%2520of%2520traffic%250Aaccidents.%2520This%2520challenge%2520is%2520compounded%2520by%2520the%2520lack%2520of%2520specialized%2520datasets%2520and%250Amethods%2520designed%2520for%2520foggy%2520environments.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%250AFoggyLane%2520dataset%252C%2520captured%2520in%2520real-world%2520foggy%2520scenarios%252C%2520and%2520synthesize%2520two%250Aadditional%2520datasets%252C%2520FoggyCULane%2520and%2520FoggyTusimple%252C%2520from%2520existing%2520popular%2520lane%250Adetection%2520datasets.%2520Furthermore%252C%2520we%2520propose%2520a%2520robust%2520Fog-Enhanced%2520Network%2520for%250Alane%2520detection%252C%2520incorporating%2520a%2520Global%2520Feature%2520Fusion%2520Module%2520%2528GFFM%2529%2520to%2520capture%250Aglobal%2520relationships%2520in%2520foggy%2520images%252C%2520a%2520Kernel%2520Feature%2520Fusion%2520Module%2520%2528KFFM%2529%2520to%250Amodel%2520the%2520structural%2520and%2520positional%2520relationships%2520of%2520lane%2520instances%252C%2520and%2520a%250ALow-level%2520Edge%2520Enhanced%2520Module%2520%2528LEEM%2529%2520to%2520address%2520missing%2520edge%2520details%2520in%2520foggy%250Aconditions.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520performance%252C%2520with%2520F1-scores%2520of%252095.04%2520on%2520FoggyLane%252C%252079.85%2520on%250AFoggyCULane%252C%2520and%252096.95%2520on%2520FoggyTusimple.%2520Additionally%252C%2520with%2520TensorRT%250Aacceleration%252C%2520the%2520method%2520reaches%2520a%2520processing%2520speed%2520of%252038.4%2520FPS%2520on%2520the%2520NVIDIA%250AJetson%2520AGX%2520Orin%252C%2520confirming%2520its%2520real-time%2520capabilities%2520and%2520robustness%2520in%2520foggy%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06121v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Robust%20Real-Time%20Lane%20Detection%20Method%20with%20Fog-Enhanced%20Feature%0A%20%20Fusion%20for%20Foggy%20Conditions&entry.906535625=Ronghui%20Zhang%20and%20Yuhang%20Ma%20and%20Tengfei%20Li%20and%20Ziyu%20Lin%20and%20Yueying%20Wu%20and%20Junzhou%20Chen%20and%20Lin%20Zhang%20and%20Jia%20Hu%20and%20Tony%20Z.%20Qiu%20and%20Konghui%20Guo&entry.1292438233=%20%20Lane%20detection%20is%20a%20critical%20component%20of%20Advanced%20Driver%20Assistance%20Systems%0A%28ADAS%29.%20Existing%20lane%20detection%20algorithms%20generally%20perform%20well%20under%0Afavorable%20weather%20conditions.%20However%2C%20their%20performance%20degrades%20significantly%0Ain%20adverse%20conditions%2C%20such%20as%20fog%2C%20which%20increases%20the%20risk%20of%20traffic%0Aaccidents.%20This%20challenge%20is%20compounded%20by%20the%20lack%20of%20specialized%20datasets%20and%0Amethods%20designed%20for%20foggy%20environments.%20To%20address%20this%2C%20we%20introduce%20the%0AFoggyLane%20dataset%2C%20captured%20in%20real-world%20foggy%20scenarios%2C%20and%20synthesize%20two%0Aadditional%20datasets%2C%20FoggyCULane%20and%20FoggyTusimple%2C%20from%20existing%20popular%20lane%0Adetection%20datasets.%20Furthermore%2C%20we%20propose%20a%20robust%20Fog-Enhanced%20Network%20for%0Alane%20detection%2C%20incorporating%20a%20Global%20Feature%20Fusion%20Module%20%28GFFM%29%20to%20capture%0Aglobal%20relationships%20in%20foggy%20images%2C%20a%20Kernel%20Feature%20Fusion%20Module%20%28KFFM%29%20to%0Amodel%20the%20structural%20and%20positional%20relationships%20of%20lane%20instances%2C%20and%20a%0ALow-level%20Edge%20Enhanced%20Module%20%28LEEM%29%20to%20address%20missing%20edge%20details%20in%20foggy%0Aconditions.%20Comprehensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20performance%2C%20with%20F1-scores%20of%2095.04%20on%20FoggyLane%2C%2079.85%20on%0AFoggyCULane%2C%20and%2096.95%20on%20FoggyTusimple.%20Additionally%2C%20with%20TensorRT%0Aacceleration%2C%20the%20method%20reaches%20a%20processing%20speed%20of%2038.4%20FPS%20on%20the%20NVIDIA%0AJetson%20AGX%20Orin%2C%20confirming%20its%20real-time%20capabilities%20and%20robustness%20in%20foggy%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06121v9&entry.124074799=Read"},
{"title": "From Imitation to Optimization: A Comparative Study of Offline Learning\n  for Autonomous Driving", "author": "Antonio Guillen-Perez", "abstract": "  Learning robust driving policies from large-scale, real-world datasets is a\ncentral challenge in autonomous driving, as online data collection is often\nunsafe and impractical. While Behavioral Cloning (BC) offers a straightforward\napproach to imitation learning, policies trained with BC are notoriously\nbrittle and suffer from compounding errors in closed-loop execution. This work\npresents a comprehensive pipeline and a comparative study to address this\nlimitation. We first develop a series of increasingly sophisticated BC\nbaselines, culminating in a Transformer-based model that operates on a\nstructured, entity-centric state representation. While this model achieves low\nimitation loss, we show that it still fails in long-horizon simulations. We\nthen demonstrate that by applying a state-of-the-art Offline Reinforcement\nLearning algorithm, Conservative Q-Learning (CQL), to the same data and\narchitecture, we can learn a significantly more robust policy. Using a\ncarefully engineered reward function, the CQL agent learns a conservative value\nfunction that enables it to recover from minor errors and avoid\nout-of-distribution states. In a large-scale evaluation on 1,000 unseen\nscenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a\n3.2x higher success rate and a 7.4x lower collision rate than the strongest BC\nbaseline, proving that an offline RL approach is critical for learning robust,\nlong-horizon driving policies from static expert data.\n", "link": "http://arxiv.org/abs/2508.07029v2", "date": "2025-08-27", "relevancy": 2.1183, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5519}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5422}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Imitation%20to%20Optimization%3A%20A%20Comparative%20Study%20of%20Offline%20Learning%0A%20%20for%20Autonomous%20Driving&body=Title%3A%20From%20Imitation%20to%20Optimization%3A%20A%20Comparative%20Study%20of%20Offline%20Learning%0A%20%20for%20Autonomous%20Driving%0AAuthor%3A%20Antonio%20Guillen-Perez%0AAbstract%3A%20%20%20Learning%20robust%20driving%20policies%20from%20large-scale%2C%20real-world%20datasets%20is%20a%0Acentral%20challenge%20in%20autonomous%20driving%2C%20as%20online%20data%20collection%20is%20often%0Aunsafe%20and%20impractical.%20While%20Behavioral%20Cloning%20%28BC%29%20offers%20a%20straightforward%0Aapproach%20to%20imitation%20learning%2C%20policies%20trained%20with%20BC%20are%20notoriously%0Abrittle%20and%20suffer%20from%20compounding%20errors%20in%20closed-loop%20execution.%20This%20work%0Apresents%20a%20comprehensive%20pipeline%20and%20a%20comparative%20study%20to%20address%20this%0Alimitation.%20We%20first%20develop%20a%20series%20of%20increasingly%20sophisticated%20BC%0Abaselines%2C%20culminating%20in%20a%20Transformer-based%20model%20that%20operates%20on%20a%0Astructured%2C%20entity-centric%20state%20representation.%20While%20this%20model%20achieves%20low%0Aimitation%20loss%2C%20we%20show%20that%20it%20still%20fails%20in%20long-horizon%20simulations.%20We%0Athen%20demonstrate%20that%20by%20applying%20a%20state-of-the-art%20Offline%20Reinforcement%0ALearning%20algorithm%2C%20Conservative%20Q-Learning%20%28CQL%29%2C%20to%20the%20same%20data%20and%0Aarchitecture%2C%20we%20can%20learn%20a%20significantly%20more%20robust%20policy.%20Using%20a%0Acarefully%20engineered%20reward%20function%2C%20the%20CQL%20agent%20learns%20a%20conservative%20value%0Afunction%20that%20enables%20it%20to%20recover%20from%20minor%20errors%20and%20avoid%0Aout-of-distribution%20states.%20In%20a%20large-scale%20evaluation%20on%201%2C000%20unseen%0Ascenarios%20from%20the%20Waymo%20Open%20Motion%20Dataset%2C%20our%20final%20CQL%20agent%20achieves%20a%0A3.2x%20higher%20success%20rate%20and%20a%207.4x%20lower%20collision%20rate%20than%20the%20strongest%20BC%0Abaseline%2C%20proving%20that%20an%20offline%20RL%20approach%20is%20critical%20for%20learning%20robust%2C%0Along-horizon%20driving%20policies%20from%20static%20expert%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.07029v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Imitation%2520to%2520Optimization%253A%2520A%2520Comparative%2520Study%2520of%2520Offline%2520Learning%250A%2520%2520for%2520Autonomous%2520Driving%26entry.906535625%3DAntonio%2520Guillen-Perez%26entry.1292438233%3D%2520%2520Learning%2520robust%2520driving%2520policies%2520from%2520large-scale%252C%2520real-world%2520datasets%2520is%2520a%250Acentral%2520challenge%2520in%2520autonomous%2520driving%252C%2520as%2520online%2520data%2520collection%2520is%2520often%250Aunsafe%2520and%2520impractical.%2520While%2520Behavioral%2520Cloning%2520%2528BC%2529%2520offers%2520a%2520straightforward%250Aapproach%2520to%2520imitation%2520learning%252C%2520policies%2520trained%2520with%2520BC%2520are%2520notoriously%250Abrittle%2520and%2520suffer%2520from%2520compounding%2520errors%2520in%2520closed-loop%2520execution.%2520This%2520work%250Apresents%2520a%2520comprehensive%2520pipeline%2520and%2520a%2520comparative%2520study%2520to%2520address%2520this%250Alimitation.%2520We%2520first%2520develop%2520a%2520series%2520of%2520increasingly%2520sophisticated%2520BC%250Abaselines%252C%2520culminating%2520in%2520a%2520Transformer-based%2520model%2520that%2520operates%2520on%2520a%250Astructured%252C%2520entity-centric%2520state%2520representation.%2520While%2520this%2520model%2520achieves%2520low%250Aimitation%2520loss%252C%2520we%2520show%2520that%2520it%2520still%2520fails%2520in%2520long-horizon%2520simulations.%2520We%250Athen%2520demonstrate%2520that%2520by%2520applying%2520a%2520state-of-the-art%2520Offline%2520Reinforcement%250ALearning%2520algorithm%252C%2520Conservative%2520Q-Learning%2520%2528CQL%2529%252C%2520to%2520the%2520same%2520data%2520and%250Aarchitecture%252C%2520we%2520can%2520learn%2520a%2520significantly%2520more%2520robust%2520policy.%2520Using%2520a%250Acarefully%2520engineered%2520reward%2520function%252C%2520the%2520CQL%2520agent%2520learns%2520a%2520conservative%2520value%250Afunction%2520that%2520enables%2520it%2520to%2520recover%2520from%2520minor%2520errors%2520and%2520avoid%250Aout-of-distribution%2520states.%2520In%2520a%2520large-scale%2520evaluation%2520on%25201%252C000%2520unseen%250Ascenarios%2520from%2520the%2520Waymo%2520Open%2520Motion%2520Dataset%252C%2520our%2520final%2520CQL%2520agent%2520achieves%2520a%250A3.2x%2520higher%2520success%2520rate%2520and%2520a%25207.4x%2520lower%2520collision%2520rate%2520than%2520the%2520strongest%2520BC%250Abaseline%252C%2520proving%2520that%2520an%2520offline%2520RL%2520approach%2520is%2520critical%2520for%2520learning%2520robust%252C%250Along-horizon%2520driving%2520policies%2520from%2520static%2520expert%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07029v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Imitation%20to%20Optimization%3A%20A%20Comparative%20Study%20of%20Offline%20Learning%0A%20%20for%20Autonomous%20Driving&entry.906535625=Antonio%20Guillen-Perez&entry.1292438233=%20%20Learning%20robust%20driving%20policies%20from%20large-scale%2C%20real-world%20datasets%20is%20a%0Acentral%20challenge%20in%20autonomous%20driving%2C%20as%20online%20data%20collection%20is%20often%0Aunsafe%20and%20impractical.%20While%20Behavioral%20Cloning%20%28BC%29%20offers%20a%20straightforward%0Aapproach%20to%20imitation%20learning%2C%20policies%20trained%20with%20BC%20are%20notoriously%0Abrittle%20and%20suffer%20from%20compounding%20errors%20in%20closed-loop%20execution.%20This%20work%0Apresents%20a%20comprehensive%20pipeline%20and%20a%20comparative%20study%20to%20address%20this%0Alimitation.%20We%20first%20develop%20a%20series%20of%20increasingly%20sophisticated%20BC%0Abaselines%2C%20culminating%20in%20a%20Transformer-based%20model%20that%20operates%20on%20a%0Astructured%2C%20entity-centric%20state%20representation.%20While%20this%20model%20achieves%20low%0Aimitation%20loss%2C%20we%20show%20that%20it%20still%20fails%20in%20long-horizon%20simulations.%20We%0Athen%20demonstrate%20that%20by%20applying%20a%20state-of-the-art%20Offline%20Reinforcement%0ALearning%20algorithm%2C%20Conservative%20Q-Learning%20%28CQL%29%2C%20to%20the%20same%20data%20and%0Aarchitecture%2C%20we%20can%20learn%20a%20significantly%20more%20robust%20policy.%20Using%20a%0Acarefully%20engineered%20reward%20function%2C%20the%20CQL%20agent%20learns%20a%20conservative%20value%0Afunction%20that%20enables%20it%20to%20recover%20from%20minor%20errors%20and%20avoid%0Aout-of-distribution%20states.%20In%20a%20large-scale%20evaluation%20on%201%2C000%20unseen%0Ascenarios%20from%20the%20Waymo%20Open%20Motion%20Dataset%2C%20our%20final%20CQL%20agent%20achieves%20a%0A3.2x%20higher%20success%20rate%20and%20a%207.4x%20lower%20collision%20rate%20than%20the%20strongest%20BC%0Abaseline%2C%20proving%20that%20an%20offline%20RL%20approach%20is%20critical%20for%20learning%20robust%2C%0Along-horizon%20driving%20policies%20from%20static%20expert%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.07029v2&entry.124074799=Read"},
{"title": "Unfolding AlphaFold's Bayesian Roots in Probability Kinematics", "author": "Thomas Hamelryck and Kanti V. Mardia", "abstract": "  We present a novel theoretical interpretation of AlphaFold1 that reveals the\npotential of generalized Bayesian updating for probabilistic deep learning. The\nseminal breakthrough of AlphaFold1 in protein structure prediction by deep\nlearning relied on a learned potential energy function, in contrast to the\nlater end-to-end architectures of AlphaFold2 and AlphaFold3. While this\npotential was originally justified by referring to physical potentials of mean\nforce (PMFs), we reinterpret AlphaFold1's potential as an instance of {\\em\nprobability kinematics} -- also known as {\\em Jeffrey conditioning} -- a\nprincipled but under-recognised generalization of conventional Bayesian\nupdating. Probability kinematics accommodates uncertain or {\\em soft} evidence\nin the form of updated probabilities over a partition. This perspective reveals\nAlphaFold1's potential as a form of generalized Bayesian updating, rather than\na thermodynamic potential. To confirm our probabilistic framework's scope and\nprecision, we analyze a synthetic 2D model in which an angular random walk\nprior is updated with evidence on distances via probability kinematics,\nmirroring AlphaFold1's approach. This theoretical contribution connects\nAlphaFold1 to a broader class of well-justified Bayesian methods, allowing\nprecise quantification, surpassing merely qualitative heuristics based on PMFs.\nOur contribution is theoretical: we replace AlphaFold1's heuristic analogy with\na principled probabilistic framework, tested in a controlled synthetic setting\nwhere correctness can be assessed. More broadly, our results point to the\nconsiderable promise of probability kinematics for probabilistic deep learning,\nby allowing the formulation of complex models from a few simpler components.\n", "link": "http://arxiv.org/abs/2505.19763v2", "date": "2025-08-27", "relevancy": 2.116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6203}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5126}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unfolding%20AlphaFold%27s%20Bayesian%20Roots%20in%20Probability%20Kinematics&body=Title%3A%20Unfolding%20AlphaFold%27s%20Bayesian%20Roots%20in%20Probability%20Kinematics%0AAuthor%3A%20Thomas%20Hamelryck%20and%20Kanti%20V.%20Mardia%0AAbstract%3A%20%20%20We%20present%20a%20novel%20theoretical%20interpretation%20of%20AlphaFold1%20that%20reveals%20the%0Apotential%20of%20generalized%20Bayesian%20updating%20for%20probabilistic%20deep%20learning.%20The%0Aseminal%20breakthrough%20of%20AlphaFold1%20in%20protein%20structure%20prediction%20by%20deep%0Alearning%20relied%20on%20a%20learned%20potential%20energy%20function%2C%20in%20contrast%20to%20the%0Alater%20end-to-end%20architectures%20of%20AlphaFold2%20and%20AlphaFold3.%20While%20this%0Apotential%20was%20originally%20justified%20by%20referring%20to%20physical%20potentials%20of%20mean%0Aforce%20%28PMFs%29%2C%20we%20reinterpret%20AlphaFold1%27s%20potential%20as%20an%20instance%20of%20%7B%5Cem%0Aprobability%20kinematics%7D%20--%20also%20known%20as%20%7B%5Cem%20Jeffrey%20conditioning%7D%20--%20a%0Aprincipled%20but%20under-recognised%20generalization%20of%20conventional%20Bayesian%0Aupdating.%20Probability%20kinematics%20accommodates%20uncertain%20or%20%7B%5Cem%20soft%7D%20evidence%0Ain%20the%20form%20of%20updated%20probabilities%20over%20a%20partition.%20This%20perspective%20reveals%0AAlphaFold1%27s%20potential%20as%20a%20form%20of%20generalized%20Bayesian%20updating%2C%20rather%20than%0Aa%20thermodynamic%20potential.%20To%20confirm%20our%20probabilistic%20framework%27s%20scope%20and%0Aprecision%2C%20we%20analyze%20a%20synthetic%202D%20model%20in%20which%20an%20angular%20random%20walk%0Aprior%20is%20updated%20with%20evidence%20on%20distances%20via%20probability%20kinematics%2C%0Amirroring%20AlphaFold1%27s%20approach.%20This%20theoretical%20contribution%20connects%0AAlphaFold1%20to%20a%20broader%20class%20of%20well-justified%20Bayesian%20methods%2C%20allowing%0Aprecise%20quantification%2C%20surpassing%20merely%20qualitative%20heuristics%20based%20on%20PMFs.%0AOur%20contribution%20is%20theoretical%3A%20we%20replace%20AlphaFold1%27s%20heuristic%20analogy%20with%0Aa%20principled%20probabilistic%20framework%2C%20tested%20in%20a%20controlled%20synthetic%20setting%0Awhere%20correctness%20can%20be%20assessed.%20More%20broadly%2C%20our%20results%20point%20to%20the%0Aconsiderable%20promise%20of%20probability%20kinematics%20for%20probabilistic%20deep%20learning%2C%0Aby%20allowing%20the%20formulation%20of%20complex%20models%20from%20a%20few%20simpler%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnfolding%2520AlphaFold%2527s%2520Bayesian%2520Roots%2520in%2520Probability%2520Kinematics%26entry.906535625%3DThomas%2520Hamelryck%2520and%2520Kanti%2520V.%2520Mardia%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520theoretical%2520interpretation%2520of%2520AlphaFold1%2520that%2520reveals%2520the%250Apotential%2520of%2520generalized%2520Bayesian%2520updating%2520for%2520probabilistic%2520deep%2520learning.%2520The%250Aseminal%2520breakthrough%2520of%2520AlphaFold1%2520in%2520protein%2520structure%2520prediction%2520by%2520deep%250Alearning%2520relied%2520on%2520a%2520learned%2520potential%2520energy%2520function%252C%2520in%2520contrast%2520to%2520the%250Alater%2520end-to-end%2520architectures%2520of%2520AlphaFold2%2520and%2520AlphaFold3.%2520While%2520this%250Apotential%2520was%2520originally%2520justified%2520by%2520referring%2520to%2520physical%2520potentials%2520of%2520mean%250Aforce%2520%2528PMFs%2529%252C%2520we%2520reinterpret%2520AlphaFold1%2527s%2520potential%2520as%2520an%2520instance%2520of%2520%257B%255Cem%250Aprobability%2520kinematics%257D%2520--%2520also%2520known%2520as%2520%257B%255Cem%2520Jeffrey%2520conditioning%257D%2520--%2520a%250Aprincipled%2520but%2520under-recognised%2520generalization%2520of%2520conventional%2520Bayesian%250Aupdating.%2520Probability%2520kinematics%2520accommodates%2520uncertain%2520or%2520%257B%255Cem%2520soft%257D%2520evidence%250Ain%2520the%2520form%2520of%2520updated%2520probabilities%2520over%2520a%2520partition.%2520This%2520perspective%2520reveals%250AAlphaFold1%2527s%2520potential%2520as%2520a%2520form%2520of%2520generalized%2520Bayesian%2520updating%252C%2520rather%2520than%250Aa%2520thermodynamic%2520potential.%2520To%2520confirm%2520our%2520probabilistic%2520framework%2527s%2520scope%2520and%250Aprecision%252C%2520we%2520analyze%2520a%2520synthetic%25202D%2520model%2520in%2520which%2520an%2520angular%2520random%2520walk%250Aprior%2520is%2520updated%2520with%2520evidence%2520on%2520distances%2520via%2520probability%2520kinematics%252C%250Amirroring%2520AlphaFold1%2527s%2520approach.%2520This%2520theoretical%2520contribution%2520connects%250AAlphaFold1%2520to%2520a%2520broader%2520class%2520of%2520well-justified%2520Bayesian%2520methods%252C%2520allowing%250Aprecise%2520quantification%252C%2520surpassing%2520merely%2520qualitative%2520heuristics%2520based%2520on%2520PMFs.%250AOur%2520contribution%2520is%2520theoretical%253A%2520we%2520replace%2520AlphaFold1%2527s%2520heuristic%2520analogy%2520with%250Aa%2520principled%2520probabilistic%2520framework%252C%2520tested%2520in%2520a%2520controlled%2520synthetic%2520setting%250Awhere%2520correctness%2520can%2520be%2520assessed.%2520More%2520broadly%252C%2520our%2520results%2520point%2520to%2520the%250Aconsiderable%2520promise%2520of%2520probability%2520kinematics%2520for%2520probabilistic%2520deep%2520learning%252C%250Aby%2520allowing%2520the%2520formulation%2520of%2520complex%2520models%2520from%2520a%2520few%2520simpler%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unfolding%20AlphaFold%27s%20Bayesian%20Roots%20in%20Probability%20Kinematics&entry.906535625=Thomas%20Hamelryck%20and%20Kanti%20V.%20Mardia&entry.1292438233=%20%20We%20present%20a%20novel%20theoretical%20interpretation%20of%20AlphaFold1%20that%20reveals%20the%0Apotential%20of%20generalized%20Bayesian%20updating%20for%20probabilistic%20deep%20learning.%20The%0Aseminal%20breakthrough%20of%20AlphaFold1%20in%20protein%20structure%20prediction%20by%20deep%0Alearning%20relied%20on%20a%20learned%20potential%20energy%20function%2C%20in%20contrast%20to%20the%0Alater%20end-to-end%20architectures%20of%20AlphaFold2%20and%20AlphaFold3.%20While%20this%0Apotential%20was%20originally%20justified%20by%20referring%20to%20physical%20potentials%20of%20mean%0Aforce%20%28PMFs%29%2C%20we%20reinterpret%20AlphaFold1%27s%20potential%20as%20an%20instance%20of%20%7B%5Cem%0Aprobability%20kinematics%7D%20--%20also%20known%20as%20%7B%5Cem%20Jeffrey%20conditioning%7D%20--%20a%0Aprincipled%20but%20under-recognised%20generalization%20of%20conventional%20Bayesian%0Aupdating.%20Probability%20kinematics%20accommodates%20uncertain%20or%20%7B%5Cem%20soft%7D%20evidence%0Ain%20the%20form%20of%20updated%20probabilities%20over%20a%20partition.%20This%20perspective%20reveals%0AAlphaFold1%27s%20potential%20as%20a%20form%20of%20generalized%20Bayesian%20updating%2C%20rather%20than%0Aa%20thermodynamic%20potential.%20To%20confirm%20our%20probabilistic%20framework%27s%20scope%20and%0Aprecision%2C%20we%20analyze%20a%20synthetic%202D%20model%20in%20which%20an%20angular%20random%20walk%0Aprior%20is%20updated%20with%20evidence%20on%20distances%20via%20probability%20kinematics%2C%0Amirroring%20AlphaFold1%27s%20approach.%20This%20theoretical%20contribution%20connects%0AAlphaFold1%20to%20a%20broader%20class%20of%20well-justified%20Bayesian%20methods%2C%20allowing%0Aprecise%20quantification%2C%20surpassing%20merely%20qualitative%20heuristics%20based%20on%20PMFs.%0AOur%20contribution%20is%20theoretical%3A%20we%20replace%20AlphaFold1%27s%20heuristic%20analogy%20with%0Aa%20principled%20probabilistic%20framework%2C%20tested%20in%20a%20controlled%20synthetic%20setting%0Awhere%20correctness%20can%20be%20assessed.%20More%20broadly%2C%20our%20results%20point%20to%20the%0Aconsiderable%20promise%20of%20probability%20kinematics%20for%20probabilistic%20deep%20learning%2C%0Aby%20allowing%20the%20formulation%20of%20complex%20models%20from%20a%20few%20simpler%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19763v2&entry.124074799=Read"},
{"title": "Tracking World States with Language Models: State-Based Evaluation Using\n  Chess", "author": "Romain Harang and Jason Naradowsky and Yaswitha Gujju and Yusuke Miyao", "abstract": "  Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.\n", "link": "http://arxiv.org/abs/2508.19851v1", "date": "2025-08-27", "relevancy": 2.1027, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tracking%20World%20States%20with%20Language%20Models%3A%20State-Based%20Evaluation%20Using%0A%20%20Chess&body=Title%3A%20Tracking%20World%20States%20with%20Language%20Models%3A%20State-Based%20Evaluation%20Using%0A%20%20Chess%0AAuthor%3A%20Romain%20Harang%20and%20Jason%20Naradowsky%20and%20Yaswitha%20Gujju%20and%20Yusuke%20Miyao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20emergent%20capabilities%20in%20structured%0Adomains%2C%20suggesting%20they%20may%20implicitly%20internalize%20high-fidelity%0Arepresentations%20of%20world%20models.%20While%20probing%20techniques%20have%20shown%20promising%0Asigns%20of%20this%20in%20scientific%20and%20game-based%20settings%2C%20they%20rely%20on%0Amodel-specific%20internal%20activations%2C%20which%20limit%20interpretability%20and%0Ageneralizability.%20In%20this%20work%2C%20we%20propose%20a%20model-agnostic%2C%20state-based%0Aevaluation%20framework%20using%20chess%20as%20a%20benchmark%20to%20assess%20whether%20LLMs%20preserve%0Athe%20semantics%20of%20structured%20environments.%20Our%20method%20analyzes%20the%20downstream%0Alegal%20move%20distributions%20%28state%20affordances%29%20to%20estimate%20semantic%20fidelity%0Abetween%20predicted%20and%20actual%20game%20states.%20This%20approach%20offers%20a%20more%0Ameaningful%20evaluation%20than%20conventional%20string-based%20metrics%20by%20aligning%20more%0Aclosely%20with%20the%20strategic%20and%20rule-governed%20nature%20of%20chess.%20Experimental%0Aresults%20demonstrate%20that%20our%20metrics%20capture%20deficiencies%20in%20state-tracking%2C%0Ahighlighting%20limitations%20of%20LLMs%20in%20maintaining%20coherent%20internal%20models%20over%0Along%20sequences.%20Our%20framework%20provides%20a%20robust%20tool%20for%20evaluating%20structured%0Areasoning%20in%20LLMs%20without%20requiring%20internal%20model%20access%2C%20and%20generalizes%20to%20a%0Awide%20class%20of%20symbolic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTracking%2520World%2520States%2520with%2520Language%2520Models%253A%2520State-Based%2520Evaluation%2520Using%250A%2520%2520Chess%26entry.906535625%3DRomain%2520Harang%2520and%2520Jason%2520Naradowsky%2520and%2520Yaswitha%2520Gujju%2520and%2520Yusuke%2520Miyao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520emergent%2520capabilities%2520in%2520structured%250Adomains%252C%2520suggesting%2520they%2520may%2520implicitly%2520internalize%2520high-fidelity%250Arepresentations%2520of%2520world%2520models.%2520While%2520probing%2520techniques%2520have%2520shown%2520promising%250Asigns%2520of%2520this%2520in%2520scientific%2520and%2520game-based%2520settings%252C%2520they%2520rely%2520on%250Amodel-specific%2520internal%2520activations%252C%2520which%2520limit%2520interpretability%2520and%250Ageneralizability.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520model-agnostic%252C%2520state-based%250Aevaluation%2520framework%2520using%2520chess%2520as%2520a%2520benchmark%2520to%2520assess%2520whether%2520LLMs%2520preserve%250Athe%2520semantics%2520of%2520structured%2520environments.%2520Our%2520method%2520analyzes%2520the%2520downstream%250Alegal%2520move%2520distributions%2520%2528state%2520affordances%2529%2520to%2520estimate%2520semantic%2520fidelity%250Abetween%2520predicted%2520and%2520actual%2520game%2520states.%2520This%2520approach%2520offers%2520a%2520more%250Ameaningful%2520evaluation%2520than%2520conventional%2520string-based%2520metrics%2520by%2520aligning%2520more%250Aclosely%2520with%2520the%2520strategic%2520and%2520rule-governed%2520nature%2520of%2520chess.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520metrics%2520capture%2520deficiencies%2520in%2520state-tracking%252C%250Ahighlighting%2520limitations%2520of%2520LLMs%2520in%2520maintaining%2520coherent%2520internal%2520models%2520over%250Along%2520sequences.%2520Our%2520framework%2520provides%2520a%2520robust%2520tool%2520for%2520evaluating%2520structured%250Areasoning%2520in%2520LLMs%2520without%2520requiring%2520internal%2520model%2520access%252C%2520and%2520generalizes%2520to%2520a%250Awide%2520class%2520of%2520symbolic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tracking%20World%20States%20with%20Language%20Models%3A%20State-Based%20Evaluation%20Using%0A%20%20Chess&entry.906535625=Romain%20Harang%20and%20Jason%20Naradowsky%20and%20Yaswitha%20Gujju%20and%20Yusuke%20Miyao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20emergent%20capabilities%20in%20structured%0Adomains%2C%20suggesting%20they%20may%20implicitly%20internalize%20high-fidelity%0Arepresentations%20of%20world%20models.%20While%20probing%20techniques%20have%20shown%20promising%0Asigns%20of%20this%20in%20scientific%20and%20game-based%20settings%2C%20they%20rely%20on%0Amodel-specific%20internal%20activations%2C%20which%20limit%20interpretability%20and%0Ageneralizability.%20In%20this%20work%2C%20we%20propose%20a%20model-agnostic%2C%20state-based%0Aevaluation%20framework%20using%20chess%20as%20a%20benchmark%20to%20assess%20whether%20LLMs%20preserve%0Athe%20semantics%20of%20structured%20environments.%20Our%20method%20analyzes%20the%20downstream%0Alegal%20move%20distributions%20%28state%20affordances%29%20to%20estimate%20semantic%20fidelity%0Abetween%20predicted%20and%20actual%20game%20states.%20This%20approach%20offers%20a%20more%0Ameaningful%20evaluation%20than%20conventional%20string-based%20metrics%20by%20aligning%20more%0Aclosely%20with%20the%20strategic%20and%20rule-governed%20nature%20of%20chess.%20Experimental%0Aresults%20demonstrate%20that%20our%20metrics%20capture%20deficiencies%20in%20state-tracking%2C%0Ahighlighting%20limitations%20of%20LLMs%20in%20maintaining%20coherent%20internal%20models%20over%0Along%20sequences.%20Our%20framework%20provides%20a%20robust%20tool%20for%20evaluating%20structured%0Areasoning%20in%20LLMs%20without%20requiring%20internal%20model%20access%2C%20and%20generalizes%20to%20a%0Awide%20class%20of%20symbolic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19851v1&entry.124074799=Read"},
{"title": "Input-Time Scaling", "author": "Rapheal Huang and Weilong Guo", "abstract": "  Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.\n", "link": "http://arxiv.org/abs/2508.13654v3", "date": "2025-08-27", "relevancy": 2.1018, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Input-Time%20Scaling&body=Title%3A%20Input-Time%20Scaling%0AAuthor%3A%20Rapheal%20Huang%20and%20Weilong%20Guo%0AAbstract%3A%20%20%20Current%20Large%20Language%20Models%20%28LLMs%29%20are%20usually%20post-trained%20on%20large-scale%0Acarefully%20curated%20datasets%20%28data%20%26%20training%20scaling%29%20and%20doing%20reasoning%20in%0Atest%20time%20%28inference%20time%20scaling%29.%20In%20this%20work%2C%20we%20present%20a%20new%20scaling%0Aparadigm%2C%20Input-Time%20Scaling%2C%20to%20complement%20previous%20scaling%20methods%20by%20putting%0Aresources%20on%20queries%20%28input%20time%29.%20During%20training%20and%20testing%2C%20we%20utilize%0Ameta-knowledge%20from%20LLMs%20to%20refine%20inputs%20with%20different%20strategies.%20We%20also%0Adiscover%20a%20new%20phenomenon%2C%20train-test%20co-design.%20It%20requires%20us%20to%20apply%20query%0Astrategies%20during%20training%20and%20testing%20as%20a%20whole.%20Only%20applying%20strategies%20on%0Atraining%20or%20testing%20would%20seriously%20degrade%20the%20performance%20gained.%20We%20are%20also%0Asurprised%20to%20find%20that%20seemingly%20low%20data%20quality%20datasets%20can%20perform%20better.%0AWe%20can%20get%20the%20best%20performance%20even%20by%20adding%20irrelevant%20information%20to%20the%0Aqueries%2C%20with%20randomly%20selected%201k%20examples%20from%20a%20minimally%20filtered%20dataset.%0AThese%20findings%20contradict%20the%20widely%20held%20inductive%20bias%2C%20%22garbage%20in%2C%20garbage%0Aout%22.%20Curating%20datasets%20with%20seemingly%20high-quality%20data%20can%20even%20potentially%0Alimit%20the%20performance%20ceiling.%20In%20addition%2C%20models%20trained%20on%20more%20data%20with%0Asimilar%20quality%20%2815k%20VS%201k%29%20perform%20worse%2C%20the%20intuition%20of%20simply%20scaling%20the%0Asize%20should%20also%20be%20carefully%20inspected.%20The%20good%20news%20is%20that%20our%20findings%20are%0Acompatible%20with%20the%20Less%20is%20More%20phenomenon.%201K%20examples%20are%20enough%20to%20invoke%0Ahigh-level%20reasoning%20ability.%20With%20experiments%20on%20Qwen2.5-32B-Instruct%2C%20we%20are%0Aable%20to%20reach%20SOTA%20performance%20among%2032B%20models%20on%20AIME24%2876.7%25%29%20and%0AAIME25%2876.7%25%29%20pass%401.%20We%20can%20further%20achieve%20AIME24%2876.7%25%29%20and%20AIME25%2880%25%29%20with%0Aa%20majority%20vote%20of%20three%20models.%20Starting%20from%20DeepSeek-R1-Distill-Qwen-32B%2C%0Athe%20result%20would%20be%2086.7%25%20on%20AIME24%20and%2076.7%25%20on%20AIME25.%20To%20facilitate%0Areproducibility%20and%20further%20research%2C%20we%20are%20working%20on%20open-source%20our%0Adatasets%2C%20data%20pipelines%2C%20evaluation%20results%2C%20and%20checkpoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13654v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInput-Time%2520Scaling%26entry.906535625%3DRapheal%2520Huang%2520and%2520Weilong%2520Guo%26entry.1292438233%3D%2520%2520Current%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520usually%2520post-trained%2520on%2520large-scale%250Acarefully%2520curated%2520datasets%2520%2528data%2520%2526%2520training%2520scaling%2529%2520and%2520doing%2520reasoning%2520in%250Atest%2520time%2520%2528inference%2520time%2520scaling%2529.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520scaling%250Aparadigm%252C%2520Input-Time%2520Scaling%252C%2520to%2520complement%2520previous%2520scaling%2520methods%2520by%2520putting%250Aresources%2520on%2520queries%2520%2528input%2520time%2529.%2520During%2520training%2520and%2520testing%252C%2520we%2520utilize%250Ameta-knowledge%2520from%2520LLMs%2520to%2520refine%2520inputs%2520with%2520different%2520strategies.%2520We%2520also%250Adiscover%2520a%2520new%2520phenomenon%252C%2520train-test%2520co-design.%2520It%2520requires%2520us%2520to%2520apply%2520query%250Astrategies%2520during%2520training%2520and%2520testing%2520as%2520a%2520whole.%2520Only%2520applying%2520strategies%2520on%250Atraining%2520or%2520testing%2520would%2520seriously%2520degrade%2520the%2520performance%2520gained.%2520We%2520are%2520also%250Asurprised%2520to%2520find%2520that%2520seemingly%2520low%2520data%2520quality%2520datasets%2520can%2520perform%2520better.%250AWe%2520can%2520get%2520the%2520best%2520performance%2520even%2520by%2520adding%2520irrelevant%2520information%2520to%2520the%250Aqueries%252C%2520with%2520randomly%2520selected%25201k%2520examples%2520from%2520a%2520minimally%2520filtered%2520dataset.%250AThese%2520findings%2520contradict%2520the%2520widely%2520held%2520inductive%2520bias%252C%2520%2522garbage%2520in%252C%2520garbage%250Aout%2522.%2520Curating%2520datasets%2520with%2520seemingly%2520high-quality%2520data%2520can%2520even%2520potentially%250Alimit%2520the%2520performance%2520ceiling.%2520In%2520addition%252C%2520models%2520trained%2520on%2520more%2520data%2520with%250Asimilar%2520quality%2520%252815k%2520VS%25201k%2529%2520perform%2520worse%252C%2520the%2520intuition%2520of%2520simply%2520scaling%2520the%250Asize%2520should%2520also%2520be%2520carefully%2520inspected.%2520The%2520good%2520news%2520is%2520that%2520our%2520findings%2520are%250Acompatible%2520with%2520the%2520Less%2520is%2520More%2520phenomenon.%25201K%2520examples%2520are%2520enough%2520to%2520invoke%250Ahigh-level%2520reasoning%2520ability.%2520With%2520experiments%2520on%2520Qwen2.5-32B-Instruct%252C%2520we%2520are%250Aable%2520to%2520reach%2520SOTA%2520performance%2520among%252032B%2520models%2520on%2520AIME24%252876.7%2525%2529%2520and%250AAIME25%252876.7%2525%2529%2520pass%25401.%2520We%2520can%2520further%2520achieve%2520AIME24%252876.7%2525%2529%2520and%2520AIME25%252880%2525%2529%2520with%250Aa%2520majority%2520vote%2520of%2520three%2520models.%2520Starting%2520from%2520DeepSeek-R1-Distill-Qwen-32B%252C%250Athe%2520result%2520would%2520be%252086.7%2525%2520on%2520AIME24%2520and%252076.7%2525%2520on%2520AIME25.%2520To%2520facilitate%250Areproducibility%2520and%2520further%2520research%252C%2520we%2520are%2520working%2520on%2520open-source%2520our%250Adatasets%252C%2520data%2520pipelines%252C%2520evaluation%2520results%252C%2520and%2520checkpoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13654v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Input-Time%20Scaling&entry.906535625=Rapheal%20Huang%20and%20Weilong%20Guo&entry.1292438233=%20%20Current%20Large%20Language%20Models%20%28LLMs%29%20are%20usually%20post-trained%20on%20large-scale%0Acarefully%20curated%20datasets%20%28data%20%26%20training%20scaling%29%20and%20doing%20reasoning%20in%0Atest%20time%20%28inference%20time%20scaling%29.%20In%20this%20work%2C%20we%20present%20a%20new%20scaling%0Aparadigm%2C%20Input-Time%20Scaling%2C%20to%20complement%20previous%20scaling%20methods%20by%20putting%0Aresources%20on%20queries%20%28input%20time%29.%20During%20training%20and%20testing%2C%20we%20utilize%0Ameta-knowledge%20from%20LLMs%20to%20refine%20inputs%20with%20different%20strategies.%20We%20also%0Adiscover%20a%20new%20phenomenon%2C%20train-test%20co-design.%20It%20requires%20us%20to%20apply%20query%0Astrategies%20during%20training%20and%20testing%20as%20a%20whole.%20Only%20applying%20strategies%20on%0Atraining%20or%20testing%20would%20seriously%20degrade%20the%20performance%20gained.%20We%20are%20also%0Asurprised%20to%20find%20that%20seemingly%20low%20data%20quality%20datasets%20can%20perform%20better.%0AWe%20can%20get%20the%20best%20performance%20even%20by%20adding%20irrelevant%20information%20to%20the%0Aqueries%2C%20with%20randomly%20selected%201k%20examples%20from%20a%20minimally%20filtered%20dataset.%0AThese%20findings%20contradict%20the%20widely%20held%20inductive%20bias%2C%20%22garbage%20in%2C%20garbage%0Aout%22.%20Curating%20datasets%20with%20seemingly%20high-quality%20data%20can%20even%20potentially%0Alimit%20the%20performance%20ceiling.%20In%20addition%2C%20models%20trained%20on%20more%20data%20with%0Asimilar%20quality%20%2815k%20VS%201k%29%20perform%20worse%2C%20the%20intuition%20of%20simply%20scaling%20the%0Asize%20should%20also%20be%20carefully%20inspected.%20The%20good%20news%20is%20that%20our%20findings%20are%0Acompatible%20with%20the%20Less%20is%20More%20phenomenon.%201K%20examples%20are%20enough%20to%20invoke%0Ahigh-level%20reasoning%20ability.%20With%20experiments%20on%20Qwen2.5-32B-Instruct%2C%20we%20are%0Aable%20to%20reach%20SOTA%20performance%20among%2032B%20models%20on%20AIME24%2876.7%25%29%20and%0AAIME25%2876.7%25%29%20pass%401.%20We%20can%20further%20achieve%20AIME24%2876.7%25%29%20and%20AIME25%2880%25%29%20with%0Aa%20majority%20vote%20of%20three%20models.%20Starting%20from%20DeepSeek-R1-Distill-Qwen-32B%2C%0Athe%20result%20would%20be%2086.7%25%20on%20AIME24%20and%2076.7%25%20on%20AIME25.%20To%20facilitate%0Areproducibility%20and%20further%20research%2C%20we%20are%20working%20on%20open-source%20our%0Adatasets%2C%20data%20pipelines%2C%20evaluation%20results%2C%20and%20checkpoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13654v3&entry.124074799=Read"},
{"title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A\n  Multimodal Hierarchical Classification Approach", "author": "Lotte Gross and Rebecca Walter and Nicole Zoppi and Adrien Justus and Alessandro Gambetti and Qiwei Han and Maximilian Kaiser", "abstract": "  This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.\n", "link": "http://arxiv.org/abs/2508.20013v1", "date": "2025-08-27", "relevancy": 2.0941, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5259}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5227}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Platform%20E-Commerce%20Product%20Categorization%20and%20Recategorization%3A%20A%0A%20%20Multimodal%20Hierarchical%20Classification%20Approach&body=Title%3A%20Cross-Platform%20E-Commerce%20Product%20Categorization%20and%20Recategorization%3A%20A%0A%20%20Multimodal%20Hierarchical%20Classification%20Approach%0AAuthor%3A%20Lotte%20Gross%20and%20Rebecca%20Walter%20and%20Nicole%20Zoppi%20and%20Adrien%20Justus%20and%20Alessandro%20Gambetti%20and%20Qiwei%20Han%20and%20Maximilian%20Kaiser%0AAbstract%3A%20%20%20This%20study%20addresses%20critical%20industrial%20challenges%20in%20e-commerce%20product%0Acategorization%2C%20namely%20platform%20heterogeneity%20and%20the%20structural%20limitations%20of%0Aexisting%20taxonomies%2C%20by%20developing%20and%20deploying%20a%20multimodal%20hierarchical%0Aclassification%20framework.%20Using%20a%20dataset%20of%20271%2C700%20products%20from%2040%0Ainternational%20fashion%20e-commerce%20platforms%2C%20we%20integrate%20textual%20features%0A%28RoBERTa%29%2C%20visual%20features%20%28ViT%29%2C%20and%20joint%20vision--language%20representations%0A%28CLIP%29.%20We%20investigate%20fusion%20strategies%2C%20including%20early%2C%20late%2C%20and%0Aattention-based%20fusion%20within%20a%20hierarchical%20architecture%20enhanced%20by%20dynamic%0Amasking%20to%20ensure%20taxonomic%20consistency.%20Results%20show%20that%20CLIP%20embeddings%0Acombined%20via%20an%20MLP-based%20late-fusion%20strategy%20achieve%20the%20highest%20hierarchical%0AF1%20%2898.59%5C%25%29%2C%20outperforming%20unimodal%20baselines.%20To%20address%20shallow%20or%0Ainconsistent%20categories%2C%20we%20further%20introduce%20a%20self-supervised%20%60%60product%0Arecategorization%27%27%20pipeline%20using%20SimCLR%2C%20UMAP%2C%20and%20cascade%20clustering%2C%20which%0Adiscovered%20new%2C%20fine-grained%20categories%20%28e.g.%2C%20subtypes%20of%20%60%60Shoes%27%27%29%20with%0Acluster%20purities%20above%2086%5C%25.%20Cross-platform%20experiments%20reveal%20a%0Adeployment-relevant%20trade-off%3A%20complex%20late-fusion%20methods%20maximize%20accuracy%0Awith%20diverse%20training%20data%2C%20while%20simpler%20early-fusion%20methods%20generalize%20more%0Aeffectively%20to%20unseen%20platforms.%20Finally%2C%20we%20demonstrate%20the%20framework%27s%0Aindustrial%20scalability%20through%20deployment%20in%20EURWEB%27s%20commercial%20transaction%0Aintelligence%20platform%20via%20a%20two-stage%20inference%20pipeline%2C%20combining%20a%0Alightweight%20RoBERTa%20stage%20with%20a%20GPU--accelerated%20multimodal%20stage%20to%20balance%0Acost%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Platform%2520E-Commerce%2520Product%2520Categorization%2520and%2520Recategorization%253A%2520A%250A%2520%2520Multimodal%2520Hierarchical%2520Classification%2520Approach%26entry.906535625%3DLotte%2520Gross%2520and%2520Rebecca%2520Walter%2520and%2520Nicole%2520Zoppi%2520and%2520Adrien%2520Justus%2520and%2520Alessandro%2520Gambetti%2520and%2520Qiwei%2520Han%2520and%2520Maximilian%2520Kaiser%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520critical%2520industrial%2520challenges%2520in%2520e-commerce%2520product%250Acategorization%252C%2520namely%2520platform%2520heterogeneity%2520and%2520the%2520structural%2520limitations%2520of%250Aexisting%2520taxonomies%252C%2520by%2520developing%2520and%2520deploying%2520a%2520multimodal%2520hierarchical%250Aclassification%2520framework.%2520Using%2520a%2520dataset%2520of%2520271%252C700%2520products%2520from%252040%250Ainternational%2520fashion%2520e-commerce%2520platforms%252C%2520we%2520integrate%2520textual%2520features%250A%2528RoBERTa%2529%252C%2520visual%2520features%2520%2528ViT%2529%252C%2520and%2520joint%2520vision--language%2520representations%250A%2528CLIP%2529.%2520We%2520investigate%2520fusion%2520strategies%252C%2520including%2520early%252C%2520late%252C%2520and%250Aattention-based%2520fusion%2520within%2520a%2520hierarchical%2520architecture%2520enhanced%2520by%2520dynamic%250Amasking%2520to%2520ensure%2520taxonomic%2520consistency.%2520Results%2520show%2520that%2520CLIP%2520embeddings%250Acombined%2520via%2520an%2520MLP-based%2520late-fusion%2520strategy%2520achieve%2520the%2520highest%2520hierarchical%250AF1%2520%252898.59%255C%2525%2529%252C%2520outperforming%2520unimodal%2520baselines.%2520To%2520address%2520shallow%2520or%250Ainconsistent%2520categories%252C%2520we%2520further%2520introduce%2520a%2520self-supervised%2520%2560%2560product%250Arecategorization%2527%2527%2520pipeline%2520using%2520SimCLR%252C%2520UMAP%252C%2520and%2520cascade%2520clustering%252C%2520which%250Adiscovered%2520new%252C%2520fine-grained%2520categories%2520%2528e.g.%252C%2520subtypes%2520of%2520%2560%2560Shoes%2527%2527%2529%2520with%250Acluster%2520purities%2520above%252086%255C%2525.%2520Cross-platform%2520experiments%2520reveal%2520a%250Adeployment-relevant%2520trade-off%253A%2520complex%2520late-fusion%2520methods%2520maximize%2520accuracy%250Awith%2520diverse%2520training%2520data%252C%2520while%2520simpler%2520early-fusion%2520methods%2520generalize%2520more%250Aeffectively%2520to%2520unseen%2520platforms.%2520Finally%252C%2520we%2520demonstrate%2520the%2520framework%2527s%250Aindustrial%2520scalability%2520through%2520deployment%2520in%2520EURWEB%2527s%2520commercial%2520transaction%250Aintelligence%2520platform%2520via%2520a%2520two-stage%2520inference%2520pipeline%252C%2520combining%2520a%250Alightweight%2520RoBERTa%2520stage%2520with%2520a%2520GPU--accelerated%2520multimodal%2520stage%2520to%2520balance%250Acost%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Platform%20E-Commerce%20Product%20Categorization%20and%20Recategorization%3A%20A%0A%20%20Multimodal%20Hierarchical%20Classification%20Approach&entry.906535625=Lotte%20Gross%20and%20Rebecca%20Walter%20and%20Nicole%20Zoppi%20and%20Adrien%20Justus%20and%20Alessandro%20Gambetti%20and%20Qiwei%20Han%20and%20Maximilian%20Kaiser&entry.1292438233=%20%20This%20study%20addresses%20critical%20industrial%20challenges%20in%20e-commerce%20product%0Acategorization%2C%20namely%20platform%20heterogeneity%20and%20the%20structural%20limitations%20of%0Aexisting%20taxonomies%2C%20by%20developing%20and%20deploying%20a%20multimodal%20hierarchical%0Aclassification%20framework.%20Using%20a%20dataset%20of%20271%2C700%20products%20from%2040%0Ainternational%20fashion%20e-commerce%20platforms%2C%20we%20integrate%20textual%20features%0A%28RoBERTa%29%2C%20visual%20features%20%28ViT%29%2C%20and%20joint%20vision--language%20representations%0A%28CLIP%29.%20We%20investigate%20fusion%20strategies%2C%20including%20early%2C%20late%2C%20and%0Aattention-based%20fusion%20within%20a%20hierarchical%20architecture%20enhanced%20by%20dynamic%0Amasking%20to%20ensure%20taxonomic%20consistency.%20Results%20show%20that%20CLIP%20embeddings%0Acombined%20via%20an%20MLP-based%20late-fusion%20strategy%20achieve%20the%20highest%20hierarchical%0AF1%20%2898.59%5C%25%29%2C%20outperforming%20unimodal%20baselines.%20To%20address%20shallow%20or%0Ainconsistent%20categories%2C%20we%20further%20introduce%20a%20self-supervised%20%60%60product%0Arecategorization%27%27%20pipeline%20using%20SimCLR%2C%20UMAP%2C%20and%20cascade%20clustering%2C%20which%0Adiscovered%20new%2C%20fine-grained%20categories%20%28e.g.%2C%20subtypes%20of%20%60%60Shoes%27%27%29%20with%0Acluster%20purities%20above%2086%5C%25.%20Cross-platform%20experiments%20reveal%20a%0Adeployment-relevant%20trade-off%3A%20complex%20late-fusion%20methods%20maximize%20accuracy%0Awith%20diverse%20training%20data%2C%20while%20simpler%20early-fusion%20methods%20generalize%20more%0Aeffectively%20to%20unseen%20platforms.%20Finally%2C%20we%20demonstrate%20the%20framework%27s%0Aindustrial%20scalability%20through%20deployment%20in%20EURWEB%27s%20commercial%20transaction%0Aintelligence%20platform%20via%20a%20two-stage%20inference%20pipeline%2C%20combining%20a%0Alightweight%20RoBERTa%20stage%20with%20a%20GPU--accelerated%20multimodal%20stage%20to%20balance%0Acost%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20013v1&entry.124074799=Read"},
{"title": "Segmentation Assisted Incremental Test Time Adaptation in an Open World", "author": "Manogna Sreenivas and Soma Biswas", "abstract": "  In dynamic environments, unfamiliar objects and distribution shifts are often\nencountered, which challenge the generalization abilities of the deployed\ntrained models. This work addresses Incremental Test Time Adaptation of Vision\nLanguage Models, tackling scenarios where unseen classes and unseen domains\ncontinuously appear during testing. Unlike traditional Test Time Adaptation\napproaches, where the test stream comes only from a predefined set of classes,\nour framework allows models to adapt simultaneously to both covariate and label\nshifts, actively incorporating new classes as they emerge. Towards this goal,\nwe establish a new benchmark for ITTA, integrating single image TTA methods for\nVLMs with active labeling techniques that query an oracle for samples\npotentially representing unseen classes during test time. We propose a\nsegmentation assisted active labeling module, termed SegAssist, which is\ntraining free and repurposes the segmentation capabilities of VLMs to refine\nactive sample selection, prioritizing samples likely to belong to unseen\nclasses. Extensive experiments on several benchmark datasets demonstrate the\npotential of SegAssist to enhance the performance of VLMs in real world\nscenarios, where continuous adaptation to emerging data is essential.\nProject-page:https://manogna-s.github.io/segassist/\n", "link": "http://arxiv.org/abs/2508.20029v1", "date": "2025-08-27", "relevancy": 2.0929, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5012}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation%20Assisted%20Incremental%20Test%20Time%20Adaptation%20in%20an%20Open%20World&body=Title%3A%20Segmentation%20Assisted%20Incremental%20Test%20Time%20Adaptation%20in%20an%20Open%20World%0AAuthor%3A%20Manogna%20Sreenivas%20and%20Soma%20Biswas%0AAbstract%3A%20%20%20In%20dynamic%20environments%2C%20unfamiliar%20objects%20and%20distribution%20shifts%20are%20often%0Aencountered%2C%20which%20challenge%20the%20generalization%20abilities%20of%20the%20deployed%0Atrained%20models.%20This%20work%20addresses%20Incremental%20Test%20Time%20Adaptation%20of%20Vision%0ALanguage%20Models%2C%20tackling%20scenarios%20where%20unseen%20classes%20and%20unseen%20domains%0Acontinuously%20appear%20during%20testing.%20Unlike%20traditional%20Test%20Time%20Adaptation%0Aapproaches%2C%20where%20the%20test%20stream%20comes%20only%20from%20a%20predefined%20set%20of%20classes%2C%0Aour%20framework%20allows%20models%20to%20adapt%20simultaneously%20to%20both%20covariate%20and%20label%0Ashifts%2C%20actively%20incorporating%20new%20classes%20as%20they%20emerge.%20Towards%20this%20goal%2C%0Awe%20establish%20a%20new%20benchmark%20for%20ITTA%2C%20integrating%20single%20image%20TTA%20methods%20for%0AVLMs%20with%20active%20labeling%20techniques%20that%20query%20an%20oracle%20for%20samples%0Apotentially%20representing%20unseen%20classes%20during%20test%20time.%20We%20propose%20a%0Asegmentation%20assisted%20active%20labeling%20module%2C%20termed%20SegAssist%2C%20which%20is%0Atraining%20free%20and%20repurposes%20the%20segmentation%20capabilities%20of%20VLMs%20to%20refine%0Aactive%20sample%20selection%2C%20prioritizing%20samples%20likely%20to%20belong%20to%20unseen%0Aclasses.%20Extensive%20experiments%20on%20several%20benchmark%20datasets%20demonstrate%20the%0Apotential%20of%20SegAssist%20to%20enhance%20the%20performance%20of%20VLMs%20in%20real%20world%0Ascenarios%2C%20where%20continuous%20adaptation%20to%20emerging%20data%20is%20essential.%0AProject-page%3Ahttps%3A//manogna-s.github.io/segassist/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation%2520Assisted%2520Incremental%2520Test%2520Time%2520Adaptation%2520in%2520an%2520Open%2520World%26entry.906535625%3DManogna%2520Sreenivas%2520and%2520Soma%2520Biswas%26entry.1292438233%3D%2520%2520In%2520dynamic%2520environments%252C%2520unfamiliar%2520objects%2520and%2520distribution%2520shifts%2520are%2520often%250Aencountered%252C%2520which%2520challenge%2520the%2520generalization%2520abilities%2520of%2520the%2520deployed%250Atrained%2520models.%2520This%2520work%2520addresses%2520Incremental%2520Test%2520Time%2520Adaptation%2520of%2520Vision%250ALanguage%2520Models%252C%2520tackling%2520scenarios%2520where%2520unseen%2520classes%2520and%2520unseen%2520domains%250Acontinuously%2520appear%2520during%2520testing.%2520Unlike%2520traditional%2520Test%2520Time%2520Adaptation%250Aapproaches%252C%2520where%2520the%2520test%2520stream%2520comes%2520only%2520from%2520a%2520predefined%2520set%2520of%2520classes%252C%250Aour%2520framework%2520allows%2520models%2520to%2520adapt%2520simultaneously%2520to%2520both%2520covariate%2520and%2520label%250Ashifts%252C%2520actively%2520incorporating%2520new%2520classes%2520as%2520they%2520emerge.%2520Towards%2520this%2520goal%252C%250Awe%2520establish%2520a%2520new%2520benchmark%2520for%2520ITTA%252C%2520integrating%2520single%2520image%2520TTA%2520methods%2520for%250AVLMs%2520with%2520active%2520labeling%2520techniques%2520that%2520query%2520an%2520oracle%2520for%2520samples%250Apotentially%2520representing%2520unseen%2520classes%2520during%2520test%2520time.%2520We%2520propose%2520a%250Asegmentation%2520assisted%2520active%2520labeling%2520module%252C%2520termed%2520SegAssist%252C%2520which%2520is%250Atraining%2520free%2520and%2520repurposes%2520the%2520segmentation%2520capabilities%2520of%2520VLMs%2520to%2520refine%250Aactive%2520sample%2520selection%252C%2520prioritizing%2520samples%2520likely%2520to%2520belong%2520to%2520unseen%250Aclasses.%2520Extensive%2520experiments%2520on%2520several%2520benchmark%2520datasets%2520demonstrate%2520the%250Apotential%2520of%2520SegAssist%2520to%2520enhance%2520the%2520performance%2520of%2520VLMs%2520in%2520real%2520world%250Ascenarios%252C%2520where%2520continuous%2520adaptation%2520to%2520emerging%2520data%2520is%2520essential.%250AProject-page%253Ahttps%253A//manogna-s.github.io/segassist/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation%20Assisted%20Incremental%20Test%20Time%20Adaptation%20in%20an%20Open%20World&entry.906535625=Manogna%20Sreenivas%20and%20Soma%20Biswas&entry.1292438233=%20%20In%20dynamic%20environments%2C%20unfamiliar%20objects%20and%20distribution%20shifts%20are%20often%0Aencountered%2C%20which%20challenge%20the%20generalization%20abilities%20of%20the%20deployed%0Atrained%20models.%20This%20work%20addresses%20Incremental%20Test%20Time%20Adaptation%20of%20Vision%0ALanguage%20Models%2C%20tackling%20scenarios%20where%20unseen%20classes%20and%20unseen%20domains%0Acontinuously%20appear%20during%20testing.%20Unlike%20traditional%20Test%20Time%20Adaptation%0Aapproaches%2C%20where%20the%20test%20stream%20comes%20only%20from%20a%20predefined%20set%20of%20classes%2C%0Aour%20framework%20allows%20models%20to%20adapt%20simultaneously%20to%20both%20covariate%20and%20label%0Ashifts%2C%20actively%20incorporating%20new%20classes%20as%20they%20emerge.%20Towards%20this%20goal%2C%0Awe%20establish%20a%20new%20benchmark%20for%20ITTA%2C%20integrating%20single%20image%20TTA%20methods%20for%0AVLMs%20with%20active%20labeling%20techniques%20that%20query%20an%20oracle%20for%20samples%0Apotentially%20representing%20unseen%20classes%20during%20test%20time.%20We%20propose%20a%0Asegmentation%20assisted%20active%20labeling%20module%2C%20termed%20SegAssist%2C%20which%20is%0Atraining%20free%20and%20repurposes%20the%20segmentation%20capabilities%20of%20VLMs%20to%20refine%0Aactive%20sample%20selection%2C%20prioritizing%20samples%20likely%20to%20belong%20to%20unseen%0Aclasses.%20Extensive%20experiments%20on%20several%20benchmark%20datasets%20demonstrate%20the%0Apotential%20of%20SegAssist%20to%20enhance%20the%20performance%20of%20VLMs%20in%20real%20world%0Ascenarios%2C%20where%20continuous%20adaptation%20to%20emerging%20data%20is%20essential.%0AProject-page%3Ahttps%3A//manogna-s.github.io/segassist/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20029v1&entry.124074799=Read"},
{"title": "Self-Supervised Pre-Training with Equilibrium Constraints", "author": "Xiaodong Cui and A F M Saif and Brian Kingsbury and Tianyi Chen", "abstract": "  Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.\n", "link": "http://arxiv.org/abs/2508.19990v1", "date": "2025-08-27", "relevancy": 2.0668, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5373}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5212}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Pre-Training%20with%20Equilibrium%20Constraints&body=Title%3A%20Self-Supervised%20Pre-Training%20with%20Equilibrium%20Constraints%0AAuthor%3A%20Xiaodong%20Cui%20and%20A%20F%20M%20Saif%20and%20Brian%20Kingsbury%20and%20Tianyi%20Chen%0AAbstract%3A%20%20%20Self-supervised%20pre-training%20using%20unlabeled%20data%20is%20widely%20used%20in%20machine%0Alearning.%20In%20this%20paper%2C%20we%20propose%20a%20new%20self-supervised%20pre-training%20approach%0Ato%20dealing%20with%20heterogeneous%20data.%20Instead%20of%20mixing%20all%20the%20data%20and%0Aminimizing%20the%20averaged%20global%20loss%20in%20the%20conventional%20way%2C%20we%20impose%0Aadditional%20equilibrium%20constraints%20to%20ensure%20that%20the%20models%20optimizes%20each%0Asource%20of%20heterogeneous%20data%20to%20its%20local%20optima%20after%20%24K%24-step%20gradient%0Adescent%20initialized%20from%20the%20model.%20We%20formulate%20this%20as%20a%20bilevel%20optimization%0Aproblem%2C%20and%20use%20the%20first-order%20approximation%20method%20to%20solve%20the%20problem.%20We%0Adiscuss%20its%20connection%20to%20model-agnostic%20meta%20learning%20%28MAML%29.%20Experiments%20are%0Acarried%20out%20on%20self-supervised%20pre-training%20using%20multi-domain%20and%20multilingual%0Adatasets%2C%20demonstrating%20that%20the%20proposed%20approach%20can%20significantly%20improve%0Athe%20adaptivity%20of%20the%20self-supervised%20pre-trained%20model%20for%20the%20downstream%0Asupervised%20fine-tuning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Pre-Training%2520with%2520Equilibrium%2520Constraints%26entry.906535625%3DXiaodong%2520Cui%2520and%2520A%2520F%2520M%2520Saif%2520and%2520Brian%2520Kingsbury%2520and%2520Tianyi%2520Chen%26entry.1292438233%3D%2520%2520Self-supervised%2520pre-training%2520using%2520unlabeled%2520data%2520is%2520widely%2520used%2520in%2520machine%250Alearning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520self-supervised%2520pre-training%2520approach%250Ato%2520dealing%2520with%2520heterogeneous%2520data.%2520Instead%2520of%2520mixing%2520all%2520the%2520data%2520and%250Aminimizing%2520the%2520averaged%2520global%2520loss%2520in%2520the%2520conventional%2520way%252C%2520we%2520impose%250Aadditional%2520equilibrium%2520constraints%2520to%2520ensure%2520that%2520the%2520models%2520optimizes%2520each%250Asource%2520of%2520heterogeneous%2520data%2520to%2520its%2520local%2520optima%2520after%2520%2524K%2524-step%2520gradient%250Adescent%2520initialized%2520from%2520the%2520model.%2520We%2520formulate%2520this%2520as%2520a%2520bilevel%2520optimization%250Aproblem%252C%2520and%2520use%2520the%2520first-order%2520approximation%2520method%2520to%2520solve%2520the%2520problem.%2520We%250Adiscuss%2520its%2520connection%2520to%2520model-agnostic%2520meta%2520learning%2520%2528MAML%2529.%2520Experiments%2520are%250Acarried%2520out%2520on%2520self-supervised%2520pre-training%2520using%2520multi-domain%2520and%2520multilingual%250Adatasets%252C%2520demonstrating%2520that%2520the%2520proposed%2520approach%2520can%2520significantly%2520improve%250Athe%2520adaptivity%2520of%2520the%2520self-supervised%2520pre-trained%2520model%2520for%2520the%2520downstream%250Asupervised%2520fine-tuning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Pre-Training%20with%20Equilibrium%20Constraints&entry.906535625=Xiaodong%20Cui%20and%20A%20F%20M%20Saif%20and%20Brian%20Kingsbury%20and%20Tianyi%20Chen&entry.1292438233=%20%20Self-supervised%20pre-training%20using%20unlabeled%20data%20is%20widely%20used%20in%20machine%0Alearning.%20In%20this%20paper%2C%20we%20propose%20a%20new%20self-supervised%20pre-training%20approach%0Ato%20dealing%20with%20heterogeneous%20data.%20Instead%20of%20mixing%20all%20the%20data%20and%0Aminimizing%20the%20averaged%20global%20loss%20in%20the%20conventional%20way%2C%20we%20impose%0Aadditional%20equilibrium%20constraints%20to%20ensure%20that%20the%20models%20optimizes%20each%0Asource%20of%20heterogeneous%20data%20to%20its%20local%20optima%20after%20%24K%24-step%20gradient%0Adescent%20initialized%20from%20the%20model.%20We%20formulate%20this%20as%20a%20bilevel%20optimization%0Aproblem%2C%20and%20use%20the%20first-order%20approximation%20method%20to%20solve%20the%20problem.%20We%0Adiscuss%20its%20connection%20to%20model-agnostic%20meta%20learning%20%28MAML%29.%20Experiments%20are%0Acarried%20out%20on%20self-supervised%20pre-training%20using%20multi-domain%20and%20multilingual%0Adatasets%2C%20demonstrating%20that%20the%20proposed%20approach%20can%20significantly%20improve%0Athe%20adaptivity%20of%20the%20self-supervised%20pre-trained%20model%20for%20the%20downstream%0Asupervised%20fine-tuning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19990v1&entry.124074799=Read"},
{"title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in\n  Mobile GUI Control", "author": "Quanfeng Lu and Zhantao Ma and Shuai Zhong and Jin Wang and Dahai Yu and Michael K. Ng and Ping Luo", "abstract": "  The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.\n", "link": "http://arxiv.org/abs/2508.20018v1", "date": "2025-08-27", "relevancy": 2.0543, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.533}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWIRL%3A%20A%20Staged%20Workflow%20for%20Interleaved%20Reinforcement%20Learning%20in%0A%20%20Mobile%20GUI%20Control&body=Title%3A%20SWIRL%3A%20A%20Staged%20Workflow%20for%20Interleaved%20Reinforcement%20Learning%20in%0A%20%20Mobile%20GUI%20Control%0AAuthor%3A%20Quanfeng%20Lu%20and%20Zhantao%20Ma%20and%20Shuai%20Zhong%20and%20Jin%20Wang%20and%20Dahai%20Yu%20and%20Michael%20K.%20Ng%20and%20Ping%20Luo%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20large%20vision%20language%20models%20%28LVLMs%29%20and%20agent%0Asystems%20has%20heightened%20interest%20in%20mobile%20GUI%20agents%20that%20can%20reliably%0Atranslate%20natural%20language%20into%20interface%20operations.%20Existing%20single-agent%0Aapproaches%2C%20however%2C%20remain%20limited%20by%20structural%20constraints.%20Although%0Amulti-agent%20systems%20naturally%20decouple%20different%20competencies%2C%20recent%20progress%0Ain%20multi-agent%20reinforcement%20learning%20%28MARL%29%20has%20often%20been%20hindered%20by%0Ainefficiency%20and%20remains%20incompatible%20with%20current%20LVLM%20architectures.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20SWIRL%2C%20a%20staged%20workflow%20for%20interleaved%0Areinforcement%20learning%20designed%20for%20multi-agent%20systems.%20SWIRL%20reformulates%0AMARL%20into%20a%20sequence%20of%20single-agent%20reinforcement%20learning%20tasks%2C%20updating%20one%0Aagent%20at%20a%20time%20while%20keeping%20the%20others%20fixed.%20This%20formulation%20enables%20stable%0Atraining%20and%20promotes%20efficient%20coordination%20across%20agents.%20Theoretically%2C%20we%0Aprovide%20a%20stepwise%20safety%20bound%2C%20a%20cross-round%20monotonic%20improvement%20theorem%2C%0Aand%20convergence%20guarantees%20on%20return%2C%20ensuring%20robust%20and%20principled%0Aoptimization.%20In%20application%20to%20mobile%20GUI%20control%2C%20SWIRL%20instantiates%20a%0ANavigator%20that%20converts%20language%20and%20screen%20context%20into%20structured%20plans%2C%20and%0Aan%20Interactor%20that%20grounds%20these%20plans%20into%20executable%20atomic%20actions.%0AExtensive%20experiments%20demonstrate%20superior%20performance%20on%20both%20high-level%20and%0Alow-level%20GUI%20benchmarks.%20Beyond%20GUI%20tasks%2C%20SWIRL%20also%20demonstrates%20strong%0Acapability%20in%20multi-agent%20mathematical%20reasoning%2C%20underscoring%20its%20potential%20as%0Aa%20general%20framework%20for%20developing%20efficient%20and%20robust%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWIRL%253A%2520A%2520Staged%2520Workflow%2520for%2520Interleaved%2520Reinforcement%2520Learning%2520in%250A%2520%2520Mobile%2520GUI%2520Control%26entry.906535625%3DQuanfeng%2520Lu%2520and%2520Zhantao%2520Ma%2520and%2520Shuai%2520Zhong%2520and%2520Jin%2520Wang%2520and%2520Dahai%2520Yu%2520and%2520Michael%2520K.%2520Ng%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520and%2520agent%250Asystems%2520has%2520heightened%2520interest%2520in%2520mobile%2520GUI%2520agents%2520that%2520can%2520reliably%250Atranslate%2520natural%2520language%2520into%2520interface%2520operations.%2520Existing%2520single-agent%250Aapproaches%252C%2520however%252C%2520remain%2520limited%2520by%2520structural%2520constraints.%2520Although%250Amulti-agent%2520systems%2520naturally%2520decouple%2520different%2520competencies%252C%2520recent%2520progress%250Ain%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520has%2520often%2520been%2520hindered%2520by%250Ainefficiency%2520and%2520remains%2520incompatible%2520with%2520current%2520LVLM%2520architectures.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520SWIRL%252C%2520a%2520staged%2520workflow%2520for%2520interleaved%250Areinforcement%2520learning%2520designed%2520for%2520multi-agent%2520systems.%2520SWIRL%2520reformulates%250AMARL%2520into%2520a%2520sequence%2520of%2520single-agent%2520reinforcement%2520learning%2520tasks%252C%2520updating%2520one%250Aagent%2520at%2520a%2520time%2520while%2520keeping%2520the%2520others%2520fixed.%2520This%2520formulation%2520enables%2520stable%250Atraining%2520and%2520promotes%2520efficient%2520coordination%2520across%2520agents.%2520Theoretically%252C%2520we%250Aprovide%2520a%2520stepwise%2520safety%2520bound%252C%2520a%2520cross-round%2520monotonic%2520improvement%2520theorem%252C%250Aand%2520convergence%2520guarantees%2520on%2520return%252C%2520ensuring%2520robust%2520and%2520principled%250Aoptimization.%2520In%2520application%2520to%2520mobile%2520GUI%2520control%252C%2520SWIRL%2520instantiates%2520a%250ANavigator%2520that%2520converts%2520language%2520and%2520screen%2520context%2520into%2520structured%2520plans%252C%2520and%250Aan%2520Interactor%2520that%2520grounds%2520these%2520plans%2520into%2520executable%2520atomic%2520actions.%250AExtensive%2520experiments%2520demonstrate%2520superior%2520performance%2520on%2520both%2520high-level%2520and%250Alow-level%2520GUI%2520benchmarks.%2520Beyond%2520GUI%2520tasks%252C%2520SWIRL%2520also%2520demonstrates%2520strong%250Acapability%2520in%2520multi-agent%2520mathematical%2520reasoning%252C%2520underscoring%2520its%2520potential%2520as%250Aa%2520general%2520framework%2520for%2520developing%2520efficient%2520and%2520robust%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWIRL%3A%20A%20Staged%20Workflow%20for%20Interleaved%20Reinforcement%20Learning%20in%0A%20%20Mobile%20GUI%20Control&entry.906535625=Quanfeng%20Lu%20and%20Zhantao%20Ma%20and%20Shuai%20Zhong%20and%20Jin%20Wang%20and%20Dahai%20Yu%20and%20Michael%20K.%20Ng%20and%20Ping%20Luo&entry.1292438233=%20%20The%20rapid%20advancement%20of%20large%20vision%20language%20models%20%28LVLMs%29%20and%20agent%0Asystems%20has%20heightened%20interest%20in%20mobile%20GUI%20agents%20that%20can%20reliably%0Atranslate%20natural%20language%20into%20interface%20operations.%20Existing%20single-agent%0Aapproaches%2C%20however%2C%20remain%20limited%20by%20structural%20constraints.%20Although%0Amulti-agent%20systems%20naturally%20decouple%20different%20competencies%2C%20recent%20progress%0Ain%20multi-agent%20reinforcement%20learning%20%28MARL%29%20has%20often%20been%20hindered%20by%0Ainefficiency%20and%20remains%20incompatible%20with%20current%20LVLM%20architectures.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20SWIRL%2C%20a%20staged%20workflow%20for%20interleaved%0Areinforcement%20learning%20designed%20for%20multi-agent%20systems.%20SWIRL%20reformulates%0AMARL%20into%20a%20sequence%20of%20single-agent%20reinforcement%20learning%20tasks%2C%20updating%20one%0Aagent%20at%20a%20time%20while%20keeping%20the%20others%20fixed.%20This%20formulation%20enables%20stable%0Atraining%20and%20promotes%20efficient%20coordination%20across%20agents.%20Theoretically%2C%20we%0Aprovide%20a%20stepwise%20safety%20bound%2C%20a%20cross-round%20monotonic%20improvement%20theorem%2C%0Aand%20convergence%20guarantees%20on%20return%2C%20ensuring%20robust%20and%20principled%0Aoptimization.%20In%20application%20to%20mobile%20GUI%20control%2C%20SWIRL%20instantiates%20a%0ANavigator%20that%20converts%20language%20and%20screen%20context%20into%20structured%20plans%2C%20and%0Aan%20Interactor%20that%20grounds%20these%20plans%20into%20executable%20atomic%20actions.%0AExtensive%20experiments%20demonstrate%20superior%20performance%20on%20both%20high-level%20and%0Alow-level%20GUI%20benchmarks.%20Beyond%20GUI%20tasks%2C%20SWIRL%20also%20demonstrates%20strong%0Acapability%20in%20multi-agent%20mathematical%20reasoning%2C%20underscoring%20its%20potential%20as%0Aa%20general%20framework%20for%20developing%20efficient%20and%20robust%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20018v1&entry.124074799=Read"},
{"title": "Generative AI for Testing of Autonomous Driving Systems: A Survey", "author": "Qunying Song and He Ye and Mark Harman and Federica Sarro", "abstract": "  Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field.\n", "link": "http://arxiv.org/abs/2508.19882v1", "date": "2025-08-27", "relevancy": 2.0489, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5438}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4947}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Testing%20of%20Autonomous%20Driving%20Systems%3A%20A%20Survey&body=Title%3A%20Generative%20AI%20for%20Testing%20of%20Autonomous%20Driving%20Systems%3A%20A%20Survey%0AAuthor%3A%20Qunying%20Song%20and%20He%20Ye%20and%20Mark%20Harman%20and%20Federica%20Sarro%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20%28ADS%29%20have%20been%20an%20active%20area%20of%20research%2C%20with%0Athe%20potential%20to%20deliver%20significant%20benefits%20to%20society.%20However%2C%20before%0Alarge-scale%20deployment%20on%20public%20roads%2C%20extensive%20testing%20is%20necessary%20to%0Avalidate%20their%20functionality%20and%20safety%20under%20diverse%20driving%20conditions.%0ATherefore%2C%20different%20testing%20approaches%20are%20required%2C%20and%20achieving%20effective%0Aand%20efficient%20testing%20of%20ADS%20remains%20an%20open%20challenge.%20Recently%2C%20generative%20AI%0Ahas%20emerged%20as%20a%20powerful%20tool%20across%20many%20domains%2C%20and%20it%20is%20increasingly%0Abeing%20applied%20to%20ADS%20testing%20due%20to%20its%20ability%20to%20interpret%20context%2C%20reason%0Aabout%20complex%20tasks%2C%20and%20generate%20diverse%20outputs.%20To%20gain%20a%20deeper%0Aunderstanding%20of%20its%20role%20in%20ADS%20testing%2C%20we%20systematically%20analyzed%2091%0Arelevant%20studies%20and%20synthesized%20their%20findings%20into%20six%20major%20application%0Acategories%2C%20primarily%20centered%20on%20scenario-based%20testing%20of%20ADS.%20We%20also%0Areviewed%20their%20effectiveness%20and%20compiled%20a%20wide%20range%20of%20datasets%2C%20simulators%2C%0AADS%2C%20metrics%2C%20and%20benchmarks%20used%20for%20evaluation%2C%20while%20identifying%2027%0Alimitations.%20This%20survey%20provides%20an%20overview%20and%20practical%20insights%20into%20the%0Ause%20of%20generative%20AI%20for%20testing%20ADS%2C%20highlights%20existing%20challenges%2C%20and%0Aoutlines%20directions%20for%20future%20research%20in%20this%20rapidly%20evolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Testing%2520of%2520Autonomous%2520Driving%2520Systems%253A%2520A%2520Survey%26entry.906535625%3DQunying%2520Song%2520and%2520He%2520Ye%2520and%2520Mark%2520Harman%2520and%2520Federica%2520Sarro%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520%2528ADS%2529%2520have%2520been%2520an%2520active%2520area%2520of%2520research%252C%2520with%250Athe%2520potential%2520to%2520deliver%2520significant%2520benefits%2520to%2520society.%2520However%252C%2520before%250Alarge-scale%2520deployment%2520on%2520public%2520roads%252C%2520extensive%2520testing%2520is%2520necessary%2520to%250Avalidate%2520their%2520functionality%2520and%2520safety%2520under%2520diverse%2520driving%2520conditions.%250ATherefore%252C%2520different%2520testing%2520approaches%2520are%2520required%252C%2520and%2520achieving%2520effective%250Aand%2520efficient%2520testing%2520of%2520ADS%2520remains%2520an%2520open%2520challenge.%2520Recently%252C%2520generative%2520AI%250Ahas%2520emerged%2520as%2520a%2520powerful%2520tool%2520across%2520many%2520domains%252C%2520and%2520it%2520is%2520increasingly%250Abeing%2520applied%2520to%2520ADS%2520testing%2520due%2520to%2520its%2520ability%2520to%2520interpret%2520context%252C%2520reason%250Aabout%2520complex%2520tasks%252C%2520and%2520generate%2520diverse%2520outputs.%2520To%2520gain%2520a%2520deeper%250Aunderstanding%2520of%2520its%2520role%2520in%2520ADS%2520testing%252C%2520we%2520systematically%2520analyzed%252091%250Arelevant%2520studies%2520and%2520synthesized%2520their%2520findings%2520into%2520six%2520major%2520application%250Acategories%252C%2520primarily%2520centered%2520on%2520scenario-based%2520testing%2520of%2520ADS.%2520We%2520also%250Areviewed%2520their%2520effectiveness%2520and%2520compiled%2520a%2520wide%2520range%2520of%2520datasets%252C%2520simulators%252C%250AADS%252C%2520metrics%252C%2520and%2520benchmarks%2520used%2520for%2520evaluation%252C%2520while%2520identifying%252027%250Alimitations.%2520This%2520survey%2520provides%2520an%2520overview%2520and%2520practical%2520insights%2520into%2520the%250Ause%2520of%2520generative%2520AI%2520for%2520testing%2520ADS%252C%2520highlights%2520existing%2520challenges%252C%2520and%250Aoutlines%2520directions%2520for%2520future%2520research%2520in%2520this%2520rapidly%2520evolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Testing%20of%20Autonomous%20Driving%20Systems%3A%20A%20Survey&entry.906535625=Qunying%20Song%20and%20He%20Ye%20and%20Mark%20Harman%20and%20Federica%20Sarro&entry.1292438233=%20%20Autonomous%20driving%20systems%20%28ADS%29%20have%20been%20an%20active%20area%20of%20research%2C%20with%0Athe%20potential%20to%20deliver%20significant%20benefits%20to%20society.%20However%2C%20before%0Alarge-scale%20deployment%20on%20public%20roads%2C%20extensive%20testing%20is%20necessary%20to%0Avalidate%20their%20functionality%20and%20safety%20under%20diverse%20driving%20conditions.%0ATherefore%2C%20different%20testing%20approaches%20are%20required%2C%20and%20achieving%20effective%0Aand%20efficient%20testing%20of%20ADS%20remains%20an%20open%20challenge.%20Recently%2C%20generative%20AI%0Ahas%20emerged%20as%20a%20powerful%20tool%20across%20many%20domains%2C%20and%20it%20is%20increasingly%0Abeing%20applied%20to%20ADS%20testing%20due%20to%20its%20ability%20to%20interpret%20context%2C%20reason%0Aabout%20complex%20tasks%2C%20and%20generate%20diverse%20outputs.%20To%20gain%20a%20deeper%0Aunderstanding%20of%20its%20role%20in%20ADS%20testing%2C%20we%20systematically%20analyzed%2091%0Arelevant%20studies%20and%20synthesized%20their%20findings%20into%20six%20major%20application%0Acategories%2C%20primarily%20centered%20on%20scenario-based%20testing%20of%20ADS.%20We%20also%0Areviewed%20their%20effectiveness%20and%20compiled%20a%20wide%20range%20of%20datasets%2C%20simulators%2C%0AADS%2C%20metrics%2C%20and%20benchmarks%20used%20for%20evaluation%2C%20while%20identifying%2027%0Alimitations.%20This%20survey%20provides%20an%20overview%20and%20practical%20insights%20into%20the%0Ause%20of%20generative%20AI%20for%20testing%20ADS%2C%20highlights%20existing%20challenges%2C%20and%0Aoutlines%20directions%20for%20future%20research%20in%20this%20rapidly%20evolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19882v1&entry.124074799=Read"},
{"title": "Image Quality Assessment for Machines: Paradigm, Large-scale Database,\n  and Models", "author": "Xiaoqi Wang and Yun Zhang and Weisi Lin", "abstract": "  Machine vision systems (MVS) are intrinsically vulnerable to performance\ndegradation under adverse visual conditions. To address this, we propose a\nmachine-centric image quality assessment (MIQA) framework that quantifies the\nimpact of image degradations on MVS performance. We establish an MIQA paradigm\nencompassing the end-to-end assessment workflow. To support this, we construct\na machine-centric image quality database (MIQD-2.5M), comprising 2.5 million\nsamples that capture distinctive degradation responses in both consistency and\naccuracy metrics, spanning 75 vision models, 250 degradation types, and three\nrepresentative vision tasks. We further propose a region-aware MIQA (RA-MIQA)\nmodel to evaluate MVS visual quality through fine-grained spatial degradation\nanalysis. Extensive experiments benchmark the proposed RA-MIQA against seven\nhuman visual system (HVS)-based IQA metrics and five retrained classical\nbackbones. Results demonstrate RA-MIQA's superior performance in multiple\ndimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on\naccuracy for image classification, while also revealing task-specific\ndegradation sensitivities. Critically, HVS-based metrics prove inadequate for\nMVS quality prediction, while even specialized MIQA models struggle with\nbackground degradations, accuracy-oriented estimation, and subtle distortions.\nThis study can advance MVS reliability and establish foundations for\nmachine-centric image processing and optimization. The model and code are\navailable at: https://github.com/XiaoqiWang/MIQA.\n", "link": "http://arxiv.org/abs/2508.19850v1", "date": "2025-08-27", "relevancy": 2.0476, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5158}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5092}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Quality%20Assessment%20for%20Machines%3A%20Paradigm%2C%20Large-scale%20Database%2C%0A%20%20and%20Models&body=Title%3A%20Image%20Quality%20Assessment%20for%20Machines%3A%20Paradigm%2C%20Large-scale%20Database%2C%0A%20%20and%20Models%0AAuthor%3A%20Xiaoqi%20Wang%20and%20Yun%20Zhang%20and%20Weisi%20Lin%0AAbstract%3A%20%20%20Machine%20vision%20systems%20%28MVS%29%20are%20intrinsically%20vulnerable%20to%20performance%0Adegradation%20under%20adverse%20visual%20conditions.%20To%20address%20this%2C%20we%20propose%20a%0Amachine-centric%20image%20quality%20assessment%20%28MIQA%29%20framework%20that%20quantifies%20the%0Aimpact%20of%20image%20degradations%20on%20MVS%20performance.%20We%20establish%20an%20MIQA%20paradigm%0Aencompassing%20the%20end-to-end%20assessment%20workflow.%20To%20support%20this%2C%20we%20construct%0Aa%20machine-centric%20image%20quality%20database%20%28MIQD-2.5M%29%2C%20comprising%202.5%20million%0Asamples%20that%20capture%20distinctive%20degradation%20responses%20in%20both%20consistency%20and%0Aaccuracy%20metrics%2C%20spanning%2075%20vision%20models%2C%20250%20degradation%20types%2C%20and%20three%0Arepresentative%20vision%20tasks.%20We%20further%20propose%20a%20region-aware%20MIQA%20%28RA-MIQA%29%0Amodel%20to%20evaluate%20MVS%20visual%20quality%20through%20fine-grained%20spatial%20degradation%0Aanalysis.%20Extensive%20experiments%20benchmark%20the%20proposed%20RA-MIQA%20against%20seven%0Ahuman%20visual%20system%20%28HVS%29-based%20IQA%20metrics%20and%20five%20retrained%20classical%0Abackbones.%20Results%20demonstrate%20RA-MIQA%27s%20superior%20performance%20in%20multiple%0Adimensions%2C%20e.g.%2C%20achieving%20SRCC%20gains%20of%2013.56%25%20on%20consistency%20and%2013.37%25%20on%0Aaccuracy%20for%20image%20classification%2C%20while%20also%20revealing%20task-specific%0Adegradation%20sensitivities.%20Critically%2C%20HVS-based%20metrics%20prove%20inadequate%20for%0AMVS%20quality%20prediction%2C%20while%20even%20specialized%20MIQA%20models%20struggle%20with%0Abackground%20degradations%2C%20accuracy-oriented%20estimation%2C%20and%20subtle%20distortions.%0AThis%20study%20can%20advance%20MVS%20reliability%20and%20establish%20foundations%20for%0Amachine-centric%20image%20processing%20and%20optimization.%20The%20model%20and%20code%20are%0Aavailable%20at%3A%20https%3A//github.com/XiaoqiWang/MIQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Quality%2520Assessment%2520for%2520Machines%253A%2520Paradigm%252C%2520Large-scale%2520Database%252C%250A%2520%2520and%2520Models%26entry.906535625%3DXiaoqi%2520Wang%2520and%2520Yun%2520Zhang%2520and%2520Weisi%2520Lin%26entry.1292438233%3D%2520%2520Machine%2520vision%2520systems%2520%2528MVS%2529%2520are%2520intrinsically%2520vulnerable%2520to%2520performance%250Adegradation%2520under%2520adverse%2520visual%2520conditions.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Amachine-centric%2520image%2520quality%2520assessment%2520%2528MIQA%2529%2520framework%2520that%2520quantifies%2520the%250Aimpact%2520of%2520image%2520degradations%2520on%2520MVS%2520performance.%2520We%2520establish%2520an%2520MIQA%2520paradigm%250Aencompassing%2520the%2520end-to-end%2520assessment%2520workflow.%2520To%2520support%2520this%252C%2520we%2520construct%250Aa%2520machine-centric%2520image%2520quality%2520database%2520%2528MIQD-2.5M%2529%252C%2520comprising%25202.5%2520million%250Asamples%2520that%2520capture%2520distinctive%2520degradation%2520responses%2520in%2520both%2520consistency%2520and%250Aaccuracy%2520metrics%252C%2520spanning%252075%2520vision%2520models%252C%2520250%2520degradation%2520types%252C%2520and%2520three%250Arepresentative%2520vision%2520tasks.%2520We%2520further%2520propose%2520a%2520region-aware%2520MIQA%2520%2528RA-MIQA%2529%250Amodel%2520to%2520evaluate%2520MVS%2520visual%2520quality%2520through%2520fine-grained%2520spatial%2520degradation%250Aanalysis.%2520Extensive%2520experiments%2520benchmark%2520the%2520proposed%2520RA-MIQA%2520against%2520seven%250Ahuman%2520visual%2520system%2520%2528HVS%2529-based%2520IQA%2520metrics%2520and%2520five%2520retrained%2520classical%250Abackbones.%2520Results%2520demonstrate%2520RA-MIQA%2527s%2520superior%2520performance%2520in%2520multiple%250Adimensions%252C%2520e.g.%252C%2520achieving%2520SRCC%2520gains%2520of%252013.56%2525%2520on%2520consistency%2520and%252013.37%2525%2520on%250Aaccuracy%2520for%2520image%2520classification%252C%2520while%2520also%2520revealing%2520task-specific%250Adegradation%2520sensitivities.%2520Critically%252C%2520HVS-based%2520metrics%2520prove%2520inadequate%2520for%250AMVS%2520quality%2520prediction%252C%2520while%2520even%2520specialized%2520MIQA%2520models%2520struggle%2520with%250Abackground%2520degradations%252C%2520accuracy-oriented%2520estimation%252C%2520and%2520subtle%2520distortions.%250AThis%2520study%2520can%2520advance%2520MVS%2520reliability%2520and%2520establish%2520foundations%2520for%250Amachine-centric%2520image%2520processing%2520and%2520optimization.%2520The%2520model%2520and%2520code%2520are%250Aavailable%2520at%253A%2520https%253A//github.com/XiaoqiWang/MIQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Quality%20Assessment%20for%20Machines%3A%20Paradigm%2C%20Large-scale%20Database%2C%0A%20%20and%20Models&entry.906535625=Xiaoqi%20Wang%20and%20Yun%20Zhang%20and%20Weisi%20Lin&entry.1292438233=%20%20Machine%20vision%20systems%20%28MVS%29%20are%20intrinsically%20vulnerable%20to%20performance%0Adegradation%20under%20adverse%20visual%20conditions.%20To%20address%20this%2C%20we%20propose%20a%0Amachine-centric%20image%20quality%20assessment%20%28MIQA%29%20framework%20that%20quantifies%20the%0Aimpact%20of%20image%20degradations%20on%20MVS%20performance.%20We%20establish%20an%20MIQA%20paradigm%0Aencompassing%20the%20end-to-end%20assessment%20workflow.%20To%20support%20this%2C%20we%20construct%0Aa%20machine-centric%20image%20quality%20database%20%28MIQD-2.5M%29%2C%20comprising%202.5%20million%0Asamples%20that%20capture%20distinctive%20degradation%20responses%20in%20both%20consistency%20and%0Aaccuracy%20metrics%2C%20spanning%2075%20vision%20models%2C%20250%20degradation%20types%2C%20and%20three%0Arepresentative%20vision%20tasks.%20We%20further%20propose%20a%20region-aware%20MIQA%20%28RA-MIQA%29%0Amodel%20to%20evaluate%20MVS%20visual%20quality%20through%20fine-grained%20spatial%20degradation%0Aanalysis.%20Extensive%20experiments%20benchmark%20the%20proposed%20RA-MIQA%20against%20seven%0Ahuman%20visual%20system%20%28HVS%29-based%20IQA%20metrics%20and%20five%20retrained%20classical%0Abackbones.%20Results%20demonstrate%20RA-MIQA%27s%20superior%20performance%20in%20multiple%0Adimensions%2C%20e.g.%2C%20achieving%20SRCC%20gains%20of%2013.56%25%20on%20consistency%20and%2013.37%25%20on%0Aaccuracy%20for%20image%20classification%2C%20while%20also%20revealing%20task-specific%0Adegradation%20sensitivities.%20Critically%2C%20HVS-based%20metrics%20prove%20inadequate%20for%0AMVS%20quality%20prediction%2C%20while%20even%20specialized%20MIQA%20models%20struggle%20with%0Abackground%20degradations%2C%20accuracy-oriented%20estimation%2C%20and%20subtle%20distortions.%0AThis%20study%20can%20advance%20MVS%20reliability%20and%20establish%20foundations%20for%0Amachine-centric%20image%20processing%20and%20optimization.%20The%20model%20and%20code%20are%0Aavailable%20at%3A%20https%3A//github.com/XiaoqiWang/MIQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19850v1&entry.124074799=Read"},
{"title": "Model Science: getting serious about verification, explanation and\n  control of AI systems", "author": "Przemyslaw Biecek and Wojciech Samek", "abstract": "  The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.\n", "link": "http://arxiv.org/abs/2508.20040v1", "date": "2025-08-27", "relevancy": 2.0469, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5132}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Science%3A%20getting%20serious%20about%20verification%2C%20explanation%20and%0A%20%20control%20of%20AI%20systems&body=Title%3A%20Model%20Science%3A%20getting%20serious%20about%20verification%2C%20explanation%20and%0A%20%20control%20of%20AI%20systems%0AAuthor%3A%20Przemyslaw%20Biecek%20and%20Wojciech%20Samek%0AAbstract%3A%20%20%20The%20growing%20adoption%20of%20foundation%20models%20calls%20for%20a%20paradigm%20shift%20from%0AData%20Science%20to%20Model%20Science.%20Unlike%20data-centric%20approaches%2C%20Model%20Science%0Aplaces%20the%20trained%20model%20at%20the%20core%20of%20analysis%2C%20aiming%20to%20interact%2C%20verify%2C%0Aexplain%2C%20and%20control%20its%20behavior%20across%20diverse%20operational%20contexts.%20This%0Apaper%20introduces%20a%20conceptual%20framework%20for%20a%20new%20discipline%20called%20Model%0AScience%2C%20along%20with%20the%20proposal%20for%20its%20four%20key%20pillars%3A%20Verification%2C%20which%0Arequires%20strict%2C%20context-aware%20evaluation%20protocols%3B%20Explanation%2C%20which%20is%0Aunderstood%20as%20various%20approaches%20to%20explore%20of%20internal%20model%20operations%3B%0AControl%2C%20which%20integrates%20alignment%20techniques%20to%20steer%20model%20behavior%3B%20and%0AInterface%2C%20which%20develops%20interactive%20and%20visual%20explanation%20tools%20to%20improve%0Ahuman%20calibration%20and%20decision-making.%20The%20proposed%20framework%20aims%20to%20guide%20the%0Adevelopment%20of%20credible%2C%20safe%2C%20and%20human-aligned%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Science%253A%2520getting%2520serious%2520about%2520verification%252C%2520explanation%2520and%250A%2520%2520control%2520of%2520AI%2520systems%26entry.906535625%3DPrzemyslaw%2520Biecek%2520and%2520Wojciech%2520Samek%26entry.1292438233%3D%2520%2520The%2520growing%2520adoption%2520of%2520foundation%2520models%2520calls%2520for%2520a%2520paradigm%2520shift%2520from%250AData%2520Science%2520to%2520Model%2520Science.%2520Unlike%2520data-centric%2520approaches%252C%2520Model%2520Science%250Aplaces%2520the%2520trained%2520model%2520at%2520the%2520core%2520of%2520analysis%252C%2520aiming%2520to%2520interact%252C%2520verify%252C%250Aexplain%252C%2520and%2520control%2520its%2520behavior%2520across%2520diverse%2520operational%2520contexts.%2520This%250Apaper%2520introduces%2520a%2520conceptual%2520framework%2520for%2520a%2520new%2520discipline%2520called%2520Model%250AScience%252C%2520along%2520with%2520the%2520proposal%2520for%2520its%2520four%2520key%2520pillars%253A%2520Verification%252C%2520which%250Arequires%2520strict%252C%2520context-aware%2520evaluation%2520protocols%253B%2520Explanation%252C%2520which%2520is%250Aunderstood%2520as%2520various%2520approaches%2520to%2520explore%2520of%2520internal%2520model%2520operations%253B%250AControl%252C%2520which%2520integrates%2520alignment%2520techniques%2520to%2520steer%2520model%2520behavior%253B%2520and%250AInterface%252C%2520which%2520develops%2520interactive%2520and%2520visual%2520explanation%2520tools%2520to%2520improve%250Ahuman%2520calibration%2520and%2520decision-making.%2520The%2520proposed%2520framework%2520aims%2520to%2520guide%2520the%250Adevelopment%2520of%2520credible%252C%2520safe%252C%2520and%2520human-aligned%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Science%3A%20getting%20serious%20about%20verification%2C%20explanation%20and%0A%20%20control%20of%20AI%20systems&entry.906535625=Przemyslaw%20Biecek%20and%20Wojciech%20Samek&entry.1292438233=%20%20The%20growing%20adoption%20of%20foundation%20models%20calls%20for%20a%20paradigm%20shift%20from%0AData%20Science%20to%20Model%20Science.%20Unlike%20data-centric%20approaches%2C%20Model%20Science%0Aplaces%20the%20trained%20model%20at%20the%20core%20of%20analysis%2C%20aiming%20to%20interact%2C%20verify%2C%0Aexplain%2C%20and%20control%20its%20behavior%20across%20diverse%20operational%20contexts.%20This%0Apaper%20introduces%20a%20conceptual%20framework%20for%20a%20new%20discipline%20called%20Model%0AScience%2C%20along%20with%20the%20proposal%20for%20its%20four%20key%20pillars%3A%20Verification%2C%20which%0Arequires%20strict%2C%20context-aware%20evaluation%20protocols%3B%20Explanation%2C%20which%20is%0Aunderstood%20as%20various%20approaches%20to%20explore%20of%20internal%20model%20operations%3B%0AControl%2C%20which%20integrates%20alignment%20techniques%20to%20steer%20model%20behavior%3B%20and%0AInterface%2C%20which%20develops%20interactive%20and%20visual%20explanation%20tools%20to%20improve%0Ahuman%20calibration%20and%20decision-making.%20The%20proposed%20framework%20aims%20to%20guide%20the%0Adevelopment%20of%20credible%2C%20safe%2C%20and%20human-aligned%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20040v1&entry.124074799=Read"},
{"title": "The Return of Structural Handwritten Mathematical Expression Recognition", "author": "Jakob Seitz and Tobias Lengfeld and Radu Timofte", "abstract": "  Handwritten Mathematical Expression Recognition is foundational for\neducational technologies, enabling applications like digital note-taking and\nautomated grading. While modern encoder-decoder architectures with large\nlanguage models excel at LaTeX generation, they lack explicit symbol-to-trace\nalignment, a critical limitation for error analysis, interpretability, and\nspatially aware interactive applications requiring selective content updates.\nThis paper introduces a structural recognition approach with two innovations: 1\nan automatic annotation system that uses a neural network to map LaTeX\nequations to raw traces, automatically generating annotations for symbol\nsegmentation, classification, and spatial relations, and 2 a modular structural\nrecognition system that independently optimizes segmentation, classification,\nand relation prediction. By leveraging a dataset enriched with structural\nannotations from our auto-labeling system, the proposed recognition system\ncombines graph-based trace sorting, a hybrid convolutional-recurrent network,\nand transformer-based correction to achieve competitive performance on the\nCROHME-2023 benchmark. Crucially, our structural recognition system generates a\ncomplete graph structure that directly links handwritten traces to predicted\nsymbols, enabling transparent error analysis and interpretable outputs.\n", "link": "http://arxiv.org/abs/2508.19773v1", "date": "2025-08-27", "relevancy": 2.0374, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Return%20of%20Structural%20Handwritten%20Mathematical%20Expression%20Recognition&body=Title%3A%20The%20Return%20of%20Structural%20Handwritten%20Mathematical%20Expression%20Recognition%0AAuthor%3A%20Jakob%20Seitz%20and%20Tobias%20Lengfeld%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20Handwritten%20Mathematical%20Expression%20Recognition%20is%20foundational%20for%0Aeducational%20technologies%2C%20enabling%20applications%20like%20digital%20note-taking%20and%0Aautomated%20grading.%20While%20modern%20encoder-decoder%20architectures%20with%20large%0Alanguage%20models%20excel%20at%20LaTeX%20generation%2C%20they%20lack%20explicit%20symbol-to-trace%0Aalignment%2C%20a%20critical%20limitation%20for%20error%20analysis%2C%20interpretability%2C%20and%0Aspatially%20aware%20interactive%20applications%20requiring%20selective%20content%20updates.%0AThis%20paper%20introduces%20a%20structural%20recognition%20approach%20with%20two%20innovations%3A%201%0Aan%20automatic%20annotation%20system%20that%20uses%20a%20neural%20network%20to%20map%20LaTeX%0Aequations%20to%20raw%20traces%2C%20automatically%20generating%20annotations%20for%20symbol%0Asegmentation%2C%20classification%2C%20and%20spatial%20relations%2C%20and%202%20a%20modular%20structural%0Arecognition%20system%20that%20independently%20optimizes%20segmentation%2C%20classification%2C%0Aand%20relation%20prediction.%20By%20leveraging%20a%20dataset%20enriched%20with%20structural%0Aannotations%20from%20our%20auto-labeling%20system%2C%20the%20proposed%20recognition%20system%0Acombines%20graph-based%20trace%20sorting%2C%20a%20hybrid%20convolutional-recurrent%20network%2C%0Aand%20transformer-based%20correction%20to%20achieve%20competitive%20performance%20on%20the%0ACROHME-2023%20benchmark.%20Crucially%2C%20our%20structural%20recognition%20system%20generates%20a%0Acomplete%20graph%20structure%20that%20directly%20links%20handwritten%20traces%20to%20predicted%0Asymbols%2C%20enabling%20transparent%20error%20analysis%20and%20interpretable%20outputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Return%2520of%2520Structural%2520Handwritten%2520Mathematical%2520Expression%2520Recognition%26entry.906535625%3DJakob%2520Seitz%2520and%2520Tobias%2520Lengfeld%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520Handwritten%2520Mathematical%2520Expression%2520Recognition%2520is%2520foundational%2520for%250Aeducational%2520technologies%252C%2520enabling%2520applications%2520like%2520digital%2520note-taking%2520and%250Aautomated%2520grading.%2520While%2520modern%2520encoder-decoder%2520architectures%2520with%2520large%250Alanguage%2520models%2520excel%2520at%2520LaTeX%2520generation%252C%2520they%2520lack%2520explicit%2520symbol-to-trace%250Aalignment%252C%2520a%2520critical%2520limitation%2520for%2520error%2520analysis%252C%2520interpretability%252C%2520and%250Aspatially%2520aware%2520interactive%2520applications%2520requiring%2520selective%2520content%2520updates.%250AThis%2520paper%2520introduces%2520a%2520structural%2520recognition%2520approach%2520with%2520two%2520innovations%253A%25201%250Aan%2520automatic%2520annotation%2520system%2520that%2520uses%2520a%2520neural%2520network%2520to%2520map%2520LaTeX%250Aequations%2520to%2520raw%2520traces%252C%2520automatically%2520generating%2520annotations%2520for%2520symbol%250Asegmentation%252C%2520classification%252C%2520and%2520spatial%2520relations%252C%2520and%25202%2520a%2520modular%2520structural%250Arecognition%2520system%2520that%2520independently%2520optimizes%2520segmentation%252C%2520classification%252C%250Aand%2520relation%2520prediction.%2520By%2520leveraging%2520a%2520dataset%2520enriched%2520with%2520structural%250Aannotations%2520from%2520our%2520auto-labeling%2520system%252C%2520the%2520proposed%2520recognition%2520system%250Acombines%2520graph-based%2520trace%2520sorting%252C%2520a%2520hybrid%2520convolutional-recurrent%2520network%252C%250Aand%2520transformer-based%2520correction%2520to%2520achieve%2520competitive%2520performance%2520on%2520the%250ACROHME-2023%2520benchmark.%2520Crucially%252C%2520our%2520structural%2520recognition%2520system%2520generates%2520a%250Acomplete%2520graph%2520structure%2520that%2520directly%2520links%2520handwritten%2520traces%2520to%2520predicted%250Asymbols%252C%2520enabling%2520transparent%2520error%2520analysis%2520and%2520interpretable%2520outputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Return%20of%20Structural%20Handwritten%20Mathematical%20Expression%20Recognition&entry.906535625=Jakob%20Seitz%20and%20Tobias%20Lengfeld%20and%20Radu%20Timofte&entry.1292438233=%20%20Handwritten%20Mathematical%20Expression%20Recognition%20is%20foundational%20for%0Aeducational%20technologies%2C%20enabling%20applications%20like%20digital%20note-taking%20and%0Aautomated%20grading.%20While%20modern%20encoder-decoder%20architectures%20with%20large%0Alanguage%20models%20excel%20at%20LaTeX%20generation%2C%20they%20lack%20explicit%20symbol-to-trace%0Aalignment%2C%20a%20critical%20limitation%20for%20error%20analysis%2C%20interpretability%2C%20and%0Aspatially%20aware%20interactive%20applications%20requiring%20selective%20content%20updates.%0AThis%20paper%20introduces%20a%20structural%20recognition%20approach%20with%20two%20innovations%3A%201%0Aan%20automatic%20annotation%20system%20that%20uses%20a%20neural%20network%20to%20map%20LaTeX%0Aequations%20to%20raw%20traces%2C%20automatically%20generating%20annotations%20for%20symbol%0Asegmentation%2C%20classification%2C%20and%20spatial%20relations%2C%20and%202%20a%20modular%20structural%0Arecognition%20system%20that%20independently%20optimizes%20segmentation%2C%20classification%2C%0Aand%20relation%20prediction.%20By%20leveraging%20a%20dataset%20enriched%20with%20structural%0Aannotations%20from%20our%20auto-labeling%20system%2C%20the%20proposed%20recognition%20system%0Acombines%20graph-based%20trace%20sorting%2C%20a%20hybrid%20convolutional-recurrent%20network%2C%0Aand%20transformer-based%20correction%20to%20achieve%20competitive%20performance%20on%20the%0ACROHME-2023%20benchmark.%20Crucially%2C%20our%20structural%20recognition%20system%20generates%20a%0Acomplete%20graph%20structure%20that%20directly%20links%20handwritten%20traces%20to%20predicted%0Asymbols%2C%20enabling%20transparent%20error%20analysis%20and%20interpretable%20outputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19773v1&entry.124074799=Read"},
{"title": "FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding\n  and Comprehensive Modality Optimization", "author": "Muhammad Ali and Omar Ali AlSuwaidi", "abstract": "  In the realm of waste management, automating the sorting process for\nnon-biodegradable materials presents considerable challenges due to the\ncomplexity and variability of waste streams. To address these challenges, we\nintroduce an enhanced neural architecture that builds upon an existing\nEncoder-Decoder structure to improve the accuracy and efficiency of waste\nsorting systems. Our model integrates several key innovations: a Comprehensive\nAttention Block within the decoder, which refines feature representations by\ncombining convolutional and upsampling operations. In parallel, we utilize\nattention through the Mamba architecture, providing an additional performance\nboost. We also introduce a Data Fusion Block that fuses images with more than\nthree channels. To achieve this, we apply PCA transformation to reduce the\ndimensionality while retaining the maximum variance and essential information\nacross three dimensions, which are then used for further processing. We\nevaluated the model on RGB, hyperspectral, multispectral, and a combination of\nRGB and hyperspectral data. The results demonstrate that our approach\noutperforms existing methods by a significant margin.\n", "link": "http://arxiv.org/abs/2508.19798v1", "date": "2025-08-27", "relevancy": 2.0251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5074}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FusionSort%3A%20Enhanced%20Cluttered%20Waste%20Segmentation%20with%20Advanced%20Decoding%0A%20%20and%20Comprehensive%20Modality%20Optimization&body=Title%3A%20FusionSort%3A%20Enhanced%20Cluttered%20Waste%20Segmentation%20with%20Advanced%20Decoding%0A%20%20and%20Comprehensive%20Modality%20Optimization%0AAuthor%3A%20Muhammad%20Ali%20and%20Omar%20Ali%20AlSuwaidi%0AAbstract%3A%20%20%20In%20the%20realm%20of%20waste%20management%2C%20automating%20the%20sorting%20process%20for%0Anon-biodegradable%20materials%20presents%20considerable%20challenges%20due%20to%20the%0Acomplexity%20and%20variability%20of%20waste%20streams.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20an%20enhanced%20neural%20architecture%20that%20builds%20upon%20an%20existing%0AEncoder-Decoder%20structure%20to%20improve%20the%20accuracy%20and%20efficiency%20of%20waste%0Asorting%20systems.%20Our%20model%20integrates%20several%20key%20innovations%3A%20a%20Comprehensive%0AAttention%20Block%20within%20the%20decoder%2C%20which%20refines%20feature%20representations%20by%0Acombining%20convolutional%20and%20upsampling%20operations.%20In%20parallel%2C%20we%20utilize%0Aattention%20through%20the%20Mamba%20architecture%2C%20providing%20an%20additional%20performance%0Aboost.%20We%20also%20introduce%20a%20Data%20Fusion%20Block%20that%20fuses%20images%20with%20more%20than%0Athree%20channels.%20To%20achieve%20this%2C%20we%20apply%20PCA%20transformation%20to%20reduce%20the%0Adimensionality%20while%20retaining%20the%20maximum%20variance%20and%20essential%20information%0Aacross%20three%20dimensions%2C%20which%20are%20then%20used%20for%20further%20processing.%20We%0Aevaluated%20the%20model%20on%20RGB%2C%20hyperspectral%2C%20multispectral%2C%20and%20a%20combination%20of%0ARGB%20and%20hyperspectral%20data.%20The%20results%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20methods%20by%20a%20significant%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusionSort%253A%2520Enhanced%2520Cluttered%2520Waste%2520Segmentation%2520with%2520Advanced%2520Decoding%250A%2520%2520and%2520Comprehensive%2520Modality%2520Optimization%26entry.906535625%3DMuhammad%2520Ali%2520and%2520Omar%2520Ali%2520AlSuwaidi%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520waste%2520management%252C%2520automating%2520the%2520sorting%2520process%2520for%250Anon-biodegradable%2520materials%2520presents%2520considerable%2520challenges%2520due%2520to%2520the%250Acomplexity%2520and%2520variability%2520of%2520waste%2520streams.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520an%2520enhanced%2520neural%2520architecture%2520that%2520builds%2520upon%2520an%2520existing%250AEncoder-Decoder%2520structure%2520to%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%2520waste%250Asorting%2520systems.%2520Our%2520model%2520integrates%2520several%2520key%2520innovations%253A%2520a%2520Comprehensive%250AAttention%2520Block%2520within%2520the%2520decoder%252C%2520which%2520refines%2520feature%2520representations%2520by%250Acombining%2520convolutional%2520and%2520upsampling%2520operations.%2520In%2520parallel%252C%2520we%2520utilize%250Aattention%2520through%2520the%2520Mamba%2520architecture%252C%2520providing%2520an%2520additional%2520performance%250Aboost.%2520We%2520also%2520introduce%2520a%2520Data%2520Fusion%2520Block%2520that%2520fuses%2520images%2520with%2520more%2520than%250Athree%2520channels.%2520To%2520achieve%2520this%252C%2520we%2520apply%2520PCA%2520transformation%2520to%2520reduce%2520the%250Adimensionality%2520while%2520retaining%2520the%2520maximum%2520variance%2520and%2520essential%2520information%250Aacross%2520three%2520dimensions%252C%2520which%2520are%2520then%2520used%2520for%2520further%2520processing.%2520We%250Aevaluated%2520the%2520model%2520on%2520RGB%252C%2520hyperspectral%252C%2520multispectral%252C%2520and%2520a%2520combination%2520of%250ARGB%2520and%2520hyperspectral%2520data.%2520The%2520results%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520existing%2520methods%2520by%2520a%2520significant%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionSort%3A%20Enhanced%20Cluttered%20Waste%20Segmentation%20with%20Advanced%20Decoding%0A%20%20and%20Comprehensive%20Modality%20Optimization&entry.906535625=Muhammad%20Ali%20and%20Omar%20Ali%20AlSuwaidi&entry.1292438233=%20%20In%20the%20realm%20of%20waste%20management%2C%20automating%20the%20sorting%20process%20for%0Anon-biodegradable%20materials%20presents%20considerable%20challenges%20due%20to%20the%0Acomplexity%20and%20variability%20of%20waste%20streams.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20an%20enhanced%20neural%20architecture%20that%20builds%20upon%20an%20existing%0AEncoder-Decoder%20structure%20to%20improve%20the%20accuracy%20and%20efficiency%20of%20waste%0Asorting%20systems.%20Our%20model%20integrates%20several%20key%20innovations%3A%20a%20Comprehensive%0AAttention%20Block%20within%20the%20decoder%2C%20which%20refines%20feature%20representations%20by%0Acombining%20convolutional%20and%20upsampling%20operations.%20In%20parallel%2C%20we%20utilize%0Aattention%20through%20the%20Mamba%20architecture%2C%20providing%20an%20additional%20performance%0Aboost.%20We%20also%20introduce%20a%20Data%20Fusion%20Block%20that%20fuses%20images%20with%20more%20than%0Athree%20channels.%20To%20achieve%20this%2C%20we%20apply%20PCA%20transformation%20to%20reduce%20the%0Adimensionality%20while%20retaining%20the%20maximum%20variance%20and%20essential%20information%0Aacross%20three%20dimensions%2C%20which%20are%20then%20used%20for%20further%20processing.%20We%0Aevaluated%20the%20model%20on%20RGB%2C%20hyperspectral%2C%20multispectral%2C%20and%20a%20combination%20of%0ARGB%20and%20hyperspectral%20data.%20The%20results%20demonstrate%20that%20our%20approach%0Aoutperforms%20existing%20methods%20by%20a%20significant%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19798v1&entry.124074799=Read"},
{"title": "Bridging Domain Gaps for Fine-Grained Moth Classification Through\n  Expert-Informed Adaptation and Foundation Model Priors", "author": "Ross J Gardiner and Guillaume Mougeot and Sareh Rowlands and Benno I Simmons and Flemming Helsing and Toke Thomas H\u00f8ye", "abstract": "  Labelling images of Lepidoptera (moths) from automated camera systems is\nvital for understanding insect declines. However, accurate species\nidentification is challenging due to domain shifts between curated images and\nnoisy field imagery. We propose a lightweight classification approach,\ncombining limited expert-labelled field data with knowledge distillation from\nthe high-performance BioCLIP2 foundation model into a ConvNeXt-tiny\narchitecture. Experiments on 101 Danish moth species from AMI camera systems\ndemonstrate that BioCLIP2 substantially outperforms other methods and that our\ndistilled lightweight model achieves comparable accuracy with significantly\nreduced computational cost. These insights offer practical guidelines for the\ndevelopment of efficient insect monitoring systems and bridging domain gaps for\nfine-grained classification.\n", "link": "http://arxiv.org/abs/2508.20089v1", "date": "2025-08-27", "relevancy": 2.0024, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5172}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4981}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Domain%20Gaps%20for%20Fine-Grained%20Moth%20Classification%20Through%0A%20%20Expert-Informed%20Adaptation%20and%20Foundation%20Model%20Priors&body=Title%3A%20Bridging%20Domain%20Gaps%20for%20Fine-Grained%20Moth%20Classification%20Through%0A%20%20Expert-Informed%20Adaptation%20and%20Foundation%20Model%20Priors%0AAuthor%3A%20Ross%20J%20Gardiner%20and%20Guillaume%20Mougeot%20and%20Sareh%20Rowlands%20and%20Benno%20I%20Simmons%20and%20Flemming%20Helsing%20and%20Toke%20Thomas%20H%C3%B8ye%0AAbstract%3A%20%20%20Labelling%20images%20of%20Lepidoptera%20%28moths%29%20from%20automated%20camera%20systems%20is%0Avital%20for%20understanding%20insect%20declines.%20However%2C%20accurate%20species%0Aidentification%20is%20challenging%20due%20to%20domain%20shifts%20between%20curated%20images%20and%0Anoisy%20field%20imagery.%20We%20propose%20a%20lightweight%20classification%20approach%2C%0Acombining%20limited%20expert-labelled%20field%20data%20with%20knowledge%20distillation%20from%0Athe%20high-performance%20BioCLIP2%20foundation%20model%20into%20a%20ConvNeXt-tiny%0Aarchitecture.%20Experiments%20on%20101%20Danish%20moth%20species%20from%20AMI%20camera%20systems%0Ademonstrate%20that%20BioCLIP2%20substantially%20outperforms%20other%20methods%20and%20that%20our%0Adistilled%20lightweight%20model%20achieves%20comparable%20accuracy%20with%20significantly%0Areduced%20computational%20cost.%20These%20insights%20offer%20practical%20guidelines%20for%20the%0Adevelopment%20of%20efficient%20insect%20monitoring%20systems%20and%20bridging%20domain%20gaps%20for%0Afine-grained%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Domain%2520Gaps%2520for%2520Fine-Grained%2520Moth%2520Classification%2520Through%250A%2520%2520Expert-Informed%2520Adaptation%2520and%2520Foundation%2520Model%2520Priors%26entry.906535625%3DRoss%2520J%2520Gardiner%2520and%2520Guillaume%2520Mougeot%2520and%2520Sareh%2520Rowlands%2520and%2520Benno%2520I%2520Simmons%2520and%2520Flemming%2520Helsing%2520and%2520Toke%2520Thomas%2520H%25C3%25B8ye%26entry.1292438233%3D%2520%2520Labelling%2520images%2520of%2520Lepidoptera%2520%2528moths%2529%2520from%2520automated%2520camera%2520systems%2520is%250Avital%2520for%2520understanding%2520insect%2520declines.%2520However%252C%2520accurate%2520species%250Aidentification%2520is%2520challenging%2520due%2520to%2520domain%2520shifts%2520between%2520curated%2520images%2520and%250Anoisy%2520field%2520imagery.%2520We%2520propose%2520a%2520lightweight%2520classification%2520approach%252C%250Acombining%2520limited%2520expert-labelled%2520field%2520data%2520with%2520knowledge%2520distillation%2520from%250Athe%2520high-performance%2520BioCLIP2%2520foundation%2520model%2520into%2520a%2520ConvNeXt-tiny%250Aarchitecture.%2520Experiments%2520on%2520101%2520Danish%2520moth%2520species%2520from%2520AMI%2520camera%2520systems%250Ademonstrate%2520that%2520BioCLIP2%2520substantially%2520outperforms%2520other%2520methods%2520and%2520that%2520our%250Adistilled%2520lightweight%2520model%2520achieves%2520comparable%2520accuracy%2520with%2520significantly%250Areduced%2520computational%2520cost.%2520These%2520insights%2520offer%2520practical%2520guidelines%2520for%2520the%250Adevelopment%2520of%2520efficient%2520insect%2520monitoring%2520systems%2520and%2520bridging%2520domain%2520gaps%2520for%250Afine-grained%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Domain%20Gaps%20for%20Fine-Grained%20Moth%20Classification%20Through%0A%20%20Expert-Informed%20Adaptation%20and%20Foundation%20Model%20Priors&entry.906535625=Ross%20J%20Gardiner%20and%20Guillaume%20Mougeot%20and%20Sareh%20Rowlands%20and%20Benno%20I%20Simmons%20and%20Flemming%20Helsing%20and%20Toke%20Thomas%20H%C3%B8ye&entry.1292438233=%20%20Labelling%20images%20of%20Lepidoptera%20%28moths%29%20from%20automated%20camera%20systems%20is%0Avital%20for%20understanding%20insect%20declines.%20However%2C%20accurate%20species%0Aidentification%20is%20challenging%20due%20to%20domain%20shifts%20between%20curated%20images%20and%0Anoisy%20field%20imagery.%20We%20propose%20a%20lightweight%20classification%20approach%2C%0Acombining%20limited%20expert-labelled%20field%20data%20with%20knowledge%20distillation%20from%0Athe%20high-performance%20BioCLIP2%20foundation%20model%20into%20a%20ConvNeXt-tiny%0Aarchitecture.%20Experiments%20on%20101%20Danish%20moth%20species%20from%20AMI%20camera%20systems%0Ademonstrate%20that%20BioCLIP2%20substantially%20outperforms%20other%20methods%20and%20that%20our%0Adistilled%20lightweight%20model%20achieves%20comparable%20accuracy%20with%20significantly%0Areduced%20computational%20cost.%20These%20insights%20offer%20practical%20guidelines%20for%20the%0Adevelopment%20of%20efficient%20insect%20monitoring%20systems%20and%20bridging%20domain%20gaps%20for%0Afine-grained%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20089v1&entry.124074799=Read"},
{"title": "Towards Interpretable Concept Learning over Time Series via Temporal\n  Logic Semantics", "author": "Irene Ferfoglia and Simone Silvetti and Gaia Saveri and Laura Nenzi and Luca Bortolussi", "abstract": "  Time series classification is a task of paramount importance, as this kind of\ndata often arises in safety-critical applications. However, it is typically\ntackled with black-box deep learning methods, making it hard for humans to\nunderstand the rationale behind their output. To take on this challenge, we\npropose a neuro-symbolic framework that unifies classification and explanation\nthrough direct embedding of trajectories into a space of Signal Temporal Logic\n(STL) concepts. By introducing a novel STL-inspired kernel that maps raw time\nseries to their alignment with predefined STL formulae, our model jointly\noptimises for accuracy and interpretability, as each prediction is accompanied\nby the most relevant logical concepts that characterise it. This enables\nclassification grounded in human-interpretable temporal patterns and produces\nboth local and global symbolic explanations. Early results show competitive\nperformance while offering high-quality logical justifications for model\ndecisions.\n", "link": "http://arxiv.org/abs/2508.03269v2", "date": "2025-08-27", "relevancy": 2.0003, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5254}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretable%20Concept%20Learning%20over%20Time%20Series%20via%20Temporal%0A%20%20Logic%20Semantics&body=Title%3A%20Towards%20Interpretable%20Concept%20Learning%20over%20Time%20Series%20via%20Temporal%0A%20%20Logic%20Semantics%0AAuthor%3A%20Irene%20Ferfoglia%20and%20Simone%20Silvetti%20and%20Gaia%20Saveri%20and%20Laura%20Nenzi%20and%20Luca%20Bortolussi%0AAbstract%3A%20%20%20Time%20series%20classification%20is%20a%20task%20of%20paramount%20importance%2C%20as%20this%20kind%20of%0Adata%20often%20arises%20in%20safety-critical%20applications.%20However%2C%20it%20is%20typically%0Atackled%20with%20black-box%20deep%20learning%20methods%2C%20making%20it%20hard%20for%20humans%20to%0Aunderstand%20the%20rationale%20behind%20their%20output.%20To%20take%20on%20this%20challenge%2C%20we%0Apropose%20a%20neuro-symbolic%20framework%20that%20unifies%20classification%20and%20explanation%0Athrough%20direct%20embedding%20of%20trajectories%20into%20a%20space%20of%20Signal%20Temporal%20Logic%0A%28STL%29%20concepts.%20By%20introducing%20a%20novel%20STL-inspired%20kernel%20that%20maps%20raw%20time%0Aseries%20to%20their%20alignment%20with%20predefined%20STL%20formulae%2C%20our%20model%20jointly%0Aoptimises%20for%20accuracy%20and%20interpretability%2C%20as%20each%20prediction%20is%20accompanied%0Aby%20the%20most%20relevant%20logical%20concepts%20that%20characterise%20it.%20This%20enables%0Aclassification%20grounded%20in%20human-interpretable%20temporal%20patterns%20and%20produces%0Aboth%20local%20and%20global%20symbolic%20explanations.%20Early%20results%20show%20competitive%0Aperformance%20while%20offering%20high-quality%20logical%20justifications%20for%20model%0Adecisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretable%2520Concept%2520Learning%2520over%2520Time%2520Series%2520via%2520Temporal%250A%2520%2520Logic%2520Semantics%26entry.906535625%3DIrene%2520Ferfoglia%2520and%2520Simone%2520Silvetti%2520and%2520Gaia%2520Saveri%2520and%2520Laura%2520Nenzi%2520and%2520Luca%2520Bortolussi%26entry.1292438233%3D%2520%2520Time%2520series%2520classification%2520is%2520a%2520task%2520of%2520paramount%2520importance%252C%2520as%2520this%2520kind%2520of%250Adata%2520often%2520arises%2520in%2520safety-critical%2520applications.%2520However%252C%2520it%2520is%2520typically%250Atackled%2520with%2520black-box%2520deep%2520learning%2520methods%252C%2520making%2520it%2520hard%2520for%2520humans%2520to%250Aunderstand%2520the%2520rationale%2520behind%2520their%2520output.%2520To%2520take%2520on%2520this%2520challenge%252C%2520we%250Apropose%2520a%2520neuro-symbolic%2520framework%2520that%2520unifies%2520classification%2520and%2520explanation%250Athrough%2520direct%2520embedding%2520of%2520trajectories%2520into%2520a%2520space%2520of%2520Signal%2520Temporal%2520Logic%250A%2528STL%2529%2520concepts.%2520By%2520introducing%2520a%2520novel%2520STL-inspired%2520kernel%2520that%2520maps%2520raw%2520time%250Aseries%2520to%2520their%2520alignment%2520with%2520predefined%2520STL%2520formulae%252C%2520our%2520model%2520jointly%250Aoptimises%2520for%2520accuracy%2520and%2520interpretability%252C%2520as%2520each%2520prediction%2520is%2520accompanied%250Aby%2520the%2520most%2520relevant%2520logical%2520concepts%2520that%2520characterise%2520it.%2520This%2520enables%250Aclassification%2520grounded%2520in%2520human-interpretable%2520temporal%2520patterns%2520and%2520produces%250Aboth%2520local%2520and%2520global%2520symbolic%2520explanations.%2520Early%2520results%2520show%2520competitive%250Aperformance%2520while%2520offering%2520high-quality%2520logical%2520justifications%2520for%2520model%250Adecisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretable%20Concept%20Learning%20over%20Time%20Series%20via%20Temporal%0A%20%20Logic%20Semantics&entry.906535625=Irene%20Ferfoglia%20and%20Simone%20Silvetti%20and%20Gaia%20Saveri%20and%20Laura%20Nenzi%20and%20Luca%20Bortolussi&entry.1292438233=%20%20Time%20series%20classification%20is%20a%20task%20of%20paramount%20importance%2C%20as%20this%20kind%20of%0Adata%20often%20arises%20in%20safety-critical%20applications.%20However%2C%20it%20is%20typically%0Atackled%20with%20black-box%20deep%20learning%20methods%2C%20making%20it%20hard%20for%20humans%20to%0Aunderstand%20the%20rationale%20behind%20their%20output.%20To%20take%20on%20this%20challenge%2C%20we%0Apropose%20a%20neuro-symbolic%20framework%20that%20unifies%20classification%20and%20explanation%0Athrough%20direct%20embedding%20of%20trajectories%20into%20a%20space%20of%20Signal%20Temporal%20Logic%0A%28STL%29%20concepts.%20By%20introducing%20a%20novel%20STL-inspired%20kernel%20that%20maps%20raw%20time%0Aseries%20to%20their%20alignment%20with%20predefined%20STL%20formulae%2C%20our%20model%20jointly%0Aoptimises%20for%20accuracy%20and%20interpretability%2C%20as%20each%20prediction%20is%20accompanied%0Aby%20the%20most%20relevant%20logical%20concepts%20that%20characterise%20it.%20This%20enables%0Aclassification%20grounded%20in%20human-interpretable%20temporal%20patterns%20and%20produces%0Aboth%20local%20and%20global%20symbolic%20explanations.%20Early%20results%20show%20competitive%0Aperformance%20while%20offering%20high-quality%20logical%20justifications%20for%20model%0Adecisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03269v2&entry.124074799=Read"},
{"title": "Graphical Transformation Models", "author": "Matthias Herp and Johannes Brachem and Michael Altenbuchinger and Thomas Kneib", "abstract": "  Graphical Transformation Models (GTMs) are introduced as a novel approach to\neffectively model multivariate data with intricate marginals and complex\ndependency structures semiparametrically, while maintaining interpretability\nthrough the identification of varying conditional independencies. GTMs extend\nmultivariate transformation models by replacing the Gaussian copula with a\ncustom-designed multivariate transformation, offering two major advantages.\nFirstly, GTMs can capture more complex interdependencies using penalized\nsplines, which also provide an efficient regularization scheme. Secondly, we\ndemonstrate how to approximately regularize GTMs towards pairwise conditional\nindependencies using a lasso penalty, akin to Gaussian graphical models. The\nmodel's robustness and effectiveness are validated through simulations,\nshowcasing its ability to accurately learn complex dependencies and identify\nconditional independencies. Additionally, the model is applied to a benchmark\nastrophysics dataset, where the GTM demonstrates favorable performance compared\nto non-parametric vine copulas in learning complex multivariate distributions.\n", "link": "http://arxiv.org/abs/2503.17845v4", "date": "2025-08-27", "relevancy": 1.9994, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5308}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4793}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graphical%20Transformation%20Models&body=Title%3A%20Graphical%20Transformation%20Models%0AAuthor%3A%20Matthias%20Herp%20and%20Johannes%20Brachem%20and%20Michael%20Altenbuchinger%20and%20Thomas%20Kneib%0AAbstract%3A%20%20%20Graphical%20Transformation%20Models%20%28GTMs%29%20are%20introduced%20as%20a%20novel%20approach%20to%0Aeffectively%20model%20multivariate%20data%20with%20intricate%20marginals%20and%20complex%0Adependency%20structures%20semiparametrically%2C%20while%20maintaining%20interpretability%0Athrough%20the%20identification%20of%20varying%20conditional%20independencies.%20GTMs%20extend%0Amultivariate%20transformation%20models%20by%20replacing%20the%20Gaussian%20copula%20with%20a%0Acustom-designed%20multivariate%20transformation%2C%20offering%20two%20major%20advantages.%0AFirstly%2C%20GTMs%20can%20capture%20more%20complex%20interdependencies%20using%20penalized%0Asplines%2C%20which%20also%20provide%20an%20efficient%20regularization%20scheme.%20Secondly%2C%20we%0Ademonstrate%20how%20to%20approximately%20regularize%20GTMs%20towards%20pairwise%20conditional%0Aindependencies%20using%20a%20lasso%20penalty%2C%20akin%20to%20Gaussian%20graphical%20models.%20The%0Amodel%27s%20robustness%20and%20effectiveness%20are%20validated%20through%20simulations%2C%0Ashowcasing%20its%20ability%20to%20accurately%20learn%20complex%20dependencies%20and%20identify%0Aconditional%20independencies.%20Additionally%2C%20the%20model%20is%20applied%20to%20a%20benchmark%0Aastrophysics%20dataset%2C%20where%20the%20GTM%20demonstrates%20favorable%20performance%20compared%0Ato%20non-parametric%20vine%20copulas%20in%20learning%20complex%20multivariate%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17845v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphical%2520Transformation%2520Models%26entry.906535625%3DMatthias%2520Herp%2520and%2520Johannes%2520Brachem%2520and%2520Michael%2520Altenbuchinger%2520and%2520Thomas%2520Kneib%26entry.1292438233%3D%2520%2520Graphical%2520Transformation%2520Models%2520%2528GTMs%2529%2520are%2520introduced%2520as%2520a%2520novel%2520approach%2520to%250Aeffectively%2520model%2520multivariate%2520data%2520with%2520intricate%2520marginals%2520and%2520complex%250Adependency%2520structures%2520semiparametrically%252C%2520while%2520maintaining%2520interpretability%250Athrough%2520the%2520identification%2520of%2520varying%2520conditional%2520independencies.%2520GTMs%2520extend%250Amultivariate%2520transformation%2520models%2520by%2520replacing%2520the%2520Gaussian%2520copula%2520with%2520a%250Acustom-designed%2520multivariate%2520transformation%252C%2520offering%2520two%2520major%2520advantages.%250AFirstly%252C%2520GTMs%2520can%2520capture%2520more%2520complex%2520interdependencies%2520using%2520penalized%250Asplines%252C%2520which%2520also%2520provide%2520an%2520efficient%2520regularization%2520scheme.%2520Secondly%252C%2520we%250Ademonstrate%2520how%2520to%2520approximately%2520regularize%2520GTMs%2520towards%2520pairwise%2520conditional%250Aindependencies%2520using%2520a%2520lasso%2520penalty%252C%2520akin%2520to%2520Gaussian%2520graphical%2520models.%2520The%250Amodel%2527s%2520robustness%2520and%2520effectiveness%2520are%2520validated%2520through%2520simulations%252C%250Ashowcasing%2520its%2520ability%2520to%2520accurately%2520learn%2520complex%2520dependencies%2520and%2520identify%250Aconditional%2520independencies.%2520Additionally%252C%2520the%2520model%2520is%2520applied%2520to%2520a%2520benchmark%250Aastrophysics%2520dataset%252C%2520where%2520the%2520GTM%2520demonstrates%2520favorable%2520performance%2520compared%250Ato%2520non-parametric%2520vine%2520copulas%2520in%2520learning%2520complex%2520multivariate%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17845v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graphical%20Transformation%20Models&entry.906535625=Matthias%20Herp%20and%20Johannes%20Brachem%20and%20Michael%20Altenbuchinger%20and%20Thomas%20Kneib&entry.1292438233=%20%20Graphical%20Transformation%20Models%20%28GTMs%29%20are%20introduced%20as%20a%20novel%20approach%20to%0Aeffectively%20model%20multivariate%20data%20with%20intricate%20marginals%20and%20complex%0Adependency%20structures%20semiparametrically%2C%20while%20maintaining%20interpretability%0Athrough%20the%20identification%20of%20varying%20conditional%20independencies.%20GTMs%20extend%0Amultivariate%20transformation%20models%20by%20replacing%20the%20Gaussian%20copula%20with%20a%0Acustom-designed%20multivariate%20transformation%2C%20offering%20two%20major%20advantages.%0AFirstly%2C%20GTMs%20can%20capture%20more%20complex%20interdependencies%20using%20penalized%0Asplines%2C%20which%20also%20provide%20an%20efficient%20regularization%20scheme.%20Secondly%2C%20we%0Ademonstrate%20how%20to%20approximately%20regularize%20GTMs%20towards%20pairwise%20conditional%0Aindependencies%20using%20a%20lasso%20penalty%2C%20akin%20to%20Gaussian%20graphical%20models.%20The%0Amodel%27s%20robustness%20and%20effectiveness%20are%20validated%20through%20simulations%2C%0Ashowcasing%20its%20ability%20to%20accurately%20learn%20complex%20dependencies%20and%20identify%0Aconditional%20independencies.%20Additionally%2C%20the%20model%20is%20applied%20to%20a%20benchmark%0Aastrophysics%20dataset%2C%20where%20the%20GTM%20demonstrates%20favorable%20performance%20compared%0Ato%20non-parametric%20vine%20copulas%20in%20learning%20complex%20multivariate%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17845v4&entry.124074799=Read"},
{"title": "A Comprehensive Review on Traffic Datasets and Simulators for Autonomous\n  Vehicles", "author": "Supriya Sarker and Brent Maples and Iftekharul Islam and Muyang Fan and Christos Papadopoulos and Weizi Li", "abstract": "  Autonomous driving has rapidly evolved through synergistic developments in\nhardware and artificial intelligence. This comprehensive review investigates\ntraffic datasets and simulators as dual pillars supporting autonomous vehicle\n(AV) development. Unlike prior surveys that examine these resources\nindependently, we present an integrated analysis spanning the entire AV\npipeline-perception, localization, prediction, planning, and control. We\nevaluate annotation practices and quality metrics while examining how\ngeographic diversity and environmental conditions affect system reliability.\nOur analysis includes detailed characterizations of datasets organized by\nfunctional domains and an in-depth examination of traffic simulators\ncategorized by their specialized contributions to research and development. The\npaper explores emerging trends, including novel architecture frameworks,\nmultimodal AI integration, and advanced data generation techniques that address\ncritical edge cases. By highlighting the interconnections between real-world\ndata collection and simulation environments, this review offers researchers a\nroadmap for developing more robust and resilient autonomous systems equipped to\nhandle the diverse challenges encountered in real-world driving environments.\n", "link": "http://arxiv.org/abs/2412.14207v3", "date": "2025-08-27", "relevancy": 1.998, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5102}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4927}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Review%20on%20Traffic%20Datasets%20and%20Simulators%20for%20Autonomous%0A%20%20Vehicles&body=Title%3A%20A%20Comprehensive%20Review%20on%20Traffic%20Datasets%20and%20Simulators%20for%20Autonomous%0A%20%20Vehicles%0AAuthor%3A%20Supriya%20Sarker%20and%20Brent%20Maples%20and%20Iftekharul%20Islam%20and%20Muyang%20Fan%20and%20Christos%20Papadopoulos%20and%20Weizi%20Li%0AAbstract%3A%20%20%20Autonomous%20driving%20has%20rapidly%20evolved%20through%20synergistic%20developments%20in%0Ahardware%20and%20artificial%20intelligence.%20This%20comprehensive%20review%20investigates%0Atraffic%20datasets%20and%20simulators%20as%20dual%20pillars%20supporting%20autonomous%20vehicle%0A%28AV%29%20development.%20Unlike%20prior%20surveys%20that%20examine%20these%20resources%0Aindependently%2C%20we%20present%20an%20integrated%20analysis%20spanning%20the%20entire%20AV%0Apipeline-perception%2C%20localization%2C%20prediction%2C%20planning%2C%20and%20control.%20We%0Aevaluate%20annotation%20practices%20and%20quality%20metrics%20while%20examining%20how%0Ageographic%20diversity%20and%20environmental%20conditions%20affect%20system%20reliability.%0AOur%20analysis%20includes%20detailed%20characterizations%20of%20datasets%20organized%20by%0Afunctional%20domains%20and%20an%20in-depth%20examination%20of%20traffic%20simulators%0Acategorized%20by%20their%20specialized%20contributions%20to%20research%20and%20development.%20The%0Apaper%20explores%20emerging%20trends%2C%20including%20novel%20architecture%20frameworks%2C%0Amultimodal%20AI%20integration%2C%20and%20advanced%20data%20generation%20techniques%20that%20address%0Acritical%20edge%20cases.%20By%20highlighting%20the%20interconnections%20between%20real-world%0Adata%20collection%20and%20simulation%20environments%2C%20this%20review%20offers%20researchers%20a%0Aroadmap%20for%20developing%20more%20robust%20and%20resilient%20autonomous%20systems%20equipped%20to%0Ahandle%20the%20diverse%20challenges%20encountered%20in%20real-world%20driving%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14207v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Review%2520on%2520Traffic%2520Datasets%2520and%2520Simulators%2520for%2520Autonomous%250A%2520%2520Vehicles%26entry.906535625%3DSupriya%2520Sarker%2520and%2520Brent%2520Maples%2520and%2520Iftekharul%2520Islam%2520and%2520Muyang%2520Fan%2520and%2520Christos%2520Papadopoulos%2520and%2520Weizi%2520Li%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520has%2520rapidly%2520evolved%2520through%2520synergistic%2520developments%2520in%250Ahardware%2520and%2520artificial%2520intelligence.%2520This%2520comprehensive%2520review%2520investigates%250Atraffic%2520datasets%2520and%2520simulators%2520as%2520dual%2520pillars%2520supporting%2520autonomous%2520vehicle%250A%2528AV%2529%2520development.%2520Unlike%2520prior%2520surveys%2520that%2520examine%2520these%2520resources%250Aindependently%252C%2520we%2520present%2520an%2520integrated%2520analysis%2520spanning%2520the%2520entire%2520AV%250Apipeline-perception%252C%2520localization%252C%2520prediction%252C%2520planning%252C%2520and%2520control.%2520We%250Aevaluate%2520annotation%2520practices%2520and%2520quality%2520metrics%2520while%2520examining%2520how%250Ageographic%2520diversity%2520and%2520environmental%2520conditions%2520affect%2520system%2520reliability.%250AOur%2520analysis%2520includes%2520detailed%2520characterizations%2520of%2520datasets%2520organized%2520by%250Afunctional%2520domains%2520and%2520an%2520in-depth%2520examination%2520of%2520traffic%2520simulators%250Acategorized%2520by%2520their%2520specialized%2520contributions%2520to%2520research%2520and%2520development.%2520The%250Apaper%2520explores%2520emerging%2520trends%252C%2520including%2520novel%2520architecture%2520frameworks%252C%250Amultimodal%2520AI%2520integration%252C%2520and%2520advanced%2520data%2520generation%2520techniques%2520that%2520address%250Acritical%2520edge%2520cases.%2520By%2520highlighting%2520the%2520interconnections%2520between%2520real-world%250Adata%2520collection%2520and%2520simulation%2520environments%252C%2520this%2520review%2520offers%2520researchers%2520a%250Aroadmap%2520for%2520developing%2520more%2520robust%2520and%2520resilient%2520autonomous%2520systems%2520equipped%2520to%250Ahandle%2520the%2520diverse%2520challenges%2520encountered%2520in%2520real-world%2520driving%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14207v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Review%20on%20Traffic%20Datasets%20and%20Simulators%20for%20Autonomous%0A%20%20Vehicles&entry.906535625=Supriya%20Sarker%20and%20Brent%20Maples%20and%20Iftekharul%20Islam%20and%20Muyang%20Fan%20and%20Christos%20Papadopoulos%20and%20Weizi%20Li&entry.1292438233=%20%20Autonomous%20driving%20has%20rapidly%20evolved%20through%20synergistic%20developments%20in%0Ahardware%20and%20artificial%20intelligence.%20This%20comprehensive%20review%20investigates%0Atraffic%20datasets%20and%20simulators%20as%20dual%20pillars%20supporting%20autonomous%20vehicle%0A%28AV%29%20development.%20Unlike%20prior%20surveys%20that%20examine%20these%20resources%0Aindependently%2C%20we%20present%20an%20integrated%20analysis%20spanning%20the%20entire%20AV%0Apipeline-perception%2C%20localization%2C%20prediction%2C%20planning%2C%20and%20control.%20We%0Aevaluate%20annotation%20practices%20and%20quality%20metrics%20while%20examining%20how%0Ageographic%20diversity%20and%20environmental%20conditions%20affect%20system%20reliability.%0AOur%20analysis%20includes%20detailed%20characterizations%20of%20datasets%20organized%20by%0Afunctional%20domains%20and%20an%20in-depth%20examination%20of%20traffic%20simulators%0Acategorized%20by%20their%20specialized%20contributions%20to%20research%20and%20development.%20The%0Apaper%20explores%20emerging%20trends%2C%20including%20novel%20architecture%20frameworks%2C%0Amultimodal%20AI%20integration%2C%20and%20advanced%20data%20generation%20techniques%20that%20address%0Acritical%20edge%20cases.%20By%20highlighting%20the%20interconnections%20between%20real-world%0Adata%20collection%20and%20simulation%20environments%2C%20this%20review%20offers%20researchers%20a%0Aroadmap%20for%20developing%20more%20robust%20and%20resilient%20autonomous%20systems%20equipped%20to%0Ahandle%20the%20diverse%20challenges%20encountered%20in%20real-world%20driving%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14207v3&entry.124074799=Read"},
{"title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an\n  Adversarial Perspective", "author": "Shuo Shao and Yiming Li and Mengren Zheng and Zhiyang Hu and Yukun Chen and Boheng Li and Yu He and Junfeng Guo and Dacheng Tao and Zhan Qin", "abstract": "  The widespread application of Deep Learning across diverse domains hinges\ncritically on the quality and composition of training datasets. However, the\ncommon lack of disclosure regarding their usage raises significant privacy and\ncopyright concerns. Dataset auditing techniques, which aim to determine if a\nspecific dataset was used to train a given suspicious model, provide promising\nsolutions to addressing these transparency gaps. While prior work has developed\nvarious auditing methods, their resilience against dedicated adversarial\nattacks remains largely unexplored. To bridge the gap, this paper initiates a\ncomprehensive study evaluating dataset auditing from an adversarial\nperspective. We start with introducing a novel taxonomy, classifying existing\nmethods based on their reliance on internal features (IF) (inherent to the\ndata) versus external features (EF) (artificially introduced for auditing).\nSubsequently, we formulate two primary attack types: evasion attacks, designed\nto conceal the use of a dataset, and forgery attacks, intending to falsely\nimplicate an unused dataset. Building on the understanding of existing methods\nand attack objectives, we further propose systematic attack strategies:\ndecoupling, removal, and detection for evasion; adversarial example-based\nmethods for forgery. These formulations and strategies lead to our new\nbenchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9\nrepresentative auditing methods. Extensive evaluations using DATABench reveal\nthat none of the evaluated auditing methods are sufficiently robust or\ndistinctive under adversarial settings. These findings underscore the urgent\nneed for developing a more secure and reliable dataset auditing method capable\nof withstanding sophisticated adversarial manipulation. Code is available at\nhttps://github.com/shaoshuo-ss/DATABench.\n", "link": "http://arxiv.org/abs/2507.05622v2", "date": "2025-08-27", "relevancy": 1.9975, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5311}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5243}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DATABench%3A%20Evaluating%20Dataset%20Auditing%20in%20Deep%20Learning%20from%20an%0A%20%20Adversarial%20Perspective&body=Title%3A%20DATABench%3A%20Evaluating%20Dataset%20Auditing%20in%20Deep%20Learning%20from%20an%0A%20%20Adversarial%20Perspective%0AAuthor%3A%20Shuo%20Shao%20and%20Yiming%20Li%20and%20Mengren%20Zheng%20and%20Zhiyang%20Hu%20and%20Yukun%20Chen%20and%20Boheng%20Li%20and%20Yu%20He%20and%20Junfeng%20Guo%20and%20Dacheng%20Tao%20and%20Zhan%20Qin%0AAbstract%3A%20%20%20The%20widespread%20application%20of%20Deep%20Learning%20across%20diverse%20domains%20hinges%0Acritically%20on%20the%20quality%20and%20composition%20of%20training%20datasets.%20However%2C%20the%0Acommon%20lack%20of%20disclosure%20regarding%20their%20usage%20raises%20significant%20privacy%20and%0Acopyright%20concerns.%20Dataset%20auditing%20techniques%2C%20which%20aim%20to%20determine%20if%20a%0Aspecific%20dataset%20was%20used%20to%20train%20a%20given%20suspicious%20model%2C%20provide%20promising%0Asolutions%20to%20addressing%20these%20transparency%20gaps.%20While%20prior%20work%20has%20developed%0Avarious%20auditing%20methods%2C%20their%20resilience%20against%20dedicated%20adversarial%0Aattacks%20remains%20largely%20unexplored.%20To%20bridge%20the%20gap%2C%20this%20paper%20initiates%20a%0Acomprehensive%20study%20evaluating%20dataset%20auditing%20from%20an%20adversarial%0Aperspective.%20We%20start%20with%20introducing%20a%20novel%20taxonomy%2C%20classifying%20existing%0Amethods%20based%20on%20their%20reliance%20on%20internal%20features%20%28IF%29%20%28inherent%20to%20the%0Adata%29%20versus%20external%20features%20%28EF%29%20%28artificially%20introduced%20for%20auditing%29.%0ASubsequently%2C%20we%20formulate%20two%20primary%20attack%20types%3A%20evasion%20attacks%2C%20designed%0Ato%20conceal%20the%20use%20of%20a%20dataset%2C%20and%20forgery%20attacks%2C%20intending%20to%20falsely%0Aimplicate%20an%20unused%20dataset.%20Building%20on%20the%20understanding%20of%20existing%20methods%0Aand%20attack%20objectives%2C%20we%20further%20propose%20systematic%20attack%20strategies%3A%0Adecoupling%2C%20removal%2C%20and%20detection%20for%20evasion%3B%20adversarial%20example-based%0Amethods%20for%20forgery.%20These%20formulations%20and%20strategies%20lead%20to%20our%20new%0Abenchmark%2C%20DATABench%2C%20comprising%2017%20evasion%20attacks%2C%205%20forgery%20attacks%2C%20and%209%0Arepresentative%20auditing%20methods.%20Extensive%20evaluations%20using%20DATABench%20reveal%0Athat%20none%20of%20the%20evaluated%20auditing%20methods%20are%20sufficiently%20robust%20or%0Adistinctive%20under%20adversarial%20settings.%20These%20findings%20underscore%20the%20urgent%0Aneed%20for%20developing%20a%20more%20secure%20and%20reliable%20dataset%20auditing%20method%20capable%0Aof%20withstanding%20sophisticated%20adversarial%20manipulation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/shaoshuo-ss/DATABench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05622v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDATABench%253A%2520Evaluating%2520Dataset%2520Auditing%2520in%2520Deep%2520Learning%2520from%2520an%250A%2520%2520Adversarial%2520Perspective%26entry.906535625%3DShuo%2520Shao%2520and%2520Yiming%2520Li%2520and%2520Mengren%2520Zheng%2520and%2520Zhiyang%2520Hu%2520and%2520Yukun%2520Chen%2520and%2520Boheng%2520Li%2520and%2520Yu%2520He%2520and%2520Junfeng%2520Guo%2520and%2520Dacheng%2520Tao%2520and%2520Zhan%2520Qin%26entry.1292438233%3D%2520%2520The%2520widespread%2520application%2520of%2520Deep%2520Learning%2520across%2520diverse%2520domains%2520hinges%250Acritically%2520on%2520the%2520quality%2520and%2520composition%2520of%2520training%2520datasets.%2520However%252C%2520the%250Acommon%2520lack%2520of%2520disclosure%2520regarding%2520their%2520usage%2520raises%2520significant%2520privacy%2520and%250Acopyright%2520concerns.%2520Dataset%2520auditing%2520techniques%252C%2520which%2520aim%2520to%2520determine%2520if%2520a%250Aspecific%2520dataset%2520was%2520used%2520to%2520train%2520a%2520given%2520suspicious%2520model%252C%2520provide%2520promising%250Asolutions%2520to%2520addressing%2520these%2520transparency%2520gaps.%2520While%2520prior%2520work%2520has%2520developed%250Avarious%2520auditing%2520methods%252C%2520their%2520resilience%2520against%2520dedicated%2520adversarial%250Aattacks%2520remains%2520largely%2520unexplored.%2520To%2520bridge%2520the%2520gap%252C%2520this%2520paper%2520initiates%2520a%250Acomprehensive%2520study%2520evaluating%2520dataset%2520auditing%2520from%2520an%2520adversarial%250Aperspective.%2520We%2520start%2520with%2520introducing%2520a%2520novel%2520taxonomy%252C%2520classifying%2520existing%250Amethods%2520based%2520on%2520their%2520reliance%2520on%2520internal%2520features%2520%2528IF%2529%2520%2528inherent%2520to%2520the%250Adata%2529%2520versus%2520external%2520features%2520%2528EF%2529%2520%2528artificially%2520introduced%2520for%2520auditing%2529.%250ASubsequently%252C%2520we%2520formulate%2520two%2520primary%2520attack%2520types%253A%2520evasion%2520attacks%252C%2520designed%250Ato%2520conceal%2520the%2520use%2520of%2520a%2520dataset%252C%2520and%2520forgery%2520attacks%252C%2520intending%2520to%2520falsely%250Aimplicate%2520an%2520unused%2520dataset.%2520Building%2520on%2520the%2520understanding%2520of%2520existing%2520methods%250Aand%2520attack%2520objectives%252C%2520we%2520further%2520propose%2520systematic%2520attack%2520strategies%253A%250Adecoupling%252C%2520removal%252C%2520and%2520detection%2520for%2520evasion%253B%2520adversarial%2520example-based%250Amethods%2520for%2520forgery.%2520These%2520formulations%2520and%2520strategies%2520lead%2520to%2520our%2520new%250Abenchmark%252C%2520DATABench%252C%2520comprising%252017%2520evasion%2520attacks%252C%25205%2520forgery%2520attacks%252C%2520and%25209%250Arepresentative%2520auditing%2520methods.%2520Extensive%2520evaluations%2520using%2520DATABench%2520reveal%250Athat%2520none%2520of%2520the%2520evaluated%2520auditing%2520methods%2520are%2520sufficiently%2520robust%2520or%250Adistinctive%2520under%2520adversarial%2520settings.%2520These%2520findings%2520underscore%2520the%2520urgent%250Aneed%2520for%2520developing%2520a%2520more%2520secure%2520and%2520reliable%2520dataset%2520auditing%2520method%2520capable%250Aof%2520withstanding%2520sophisticated%2520adversarial%2520manipulation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/shaoshuo-ss/DATABench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05622v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DATABench%3A%20Evaluating%20Dataset%20Auditing%20in%20Deep%20Learning%20from%20an%0A%20%20Adversarial%20Perspective&entry.906535625=Shuo%20Shao%20and%20Yiming%20Li%20and%20Mengren%20Zheng%20and%20Zhiyang%20Hu%20and%20Yukun%20Chen%20and%20Boheng%20Li%20and%20Yu%20He%20and%20Junfeng%20Guo%20and%20Dacheng%20Tao%20and%20Zhan%20Qin&entry.1292438233=%20%20The%20widespread%20application%20of%20Deep%20Learning%20across%20diverse%20domains%20hinges%0Acritically%20on%20the%20quality%20and%20composition%20of%20training%20datasets.%20However%2C%20the%0Acommon%20lack%20of%20disclosure%20regarding%20their%20usage%20raises%20significant%20privacy%20and%0Acopyright%20concerns.%20Dataset%20auditing%20techniques%2C%20which%20aim%20to%20determine%20if%20a%0Aspecific%20dataset%20was%20used%20to%20train%20a%20given%20suspicious%20model%2C%20provide%20promising%0Asolutions%20to%20addressing%20these%20transparency%20gaps.%20While%20prior%20work%20has%20developed%0Avarious%20auditing%20methods%2C%20their%20resilience%20against%20dedicated%20adversarial%0Aattacks%20remains%20largely%20unexplored.%20To%20bridge%20the%20gap%2C%20this%20paper%20initiates%20a%0Acomprehensive%20study%20evaluating%20dataset%20auditing%20from%20an%20adversarial%0Aperspective.%20We%20start%20with%20introducing%20a%20novel%20taxonomy%2C%20classifying%20existing%0Amethods%20based%20on%20their%20reliance%20on%20internal%20features%20%28IF%29%20%28inherent%20to%20the%0Adata%29%20versus%20external%20features%20%28EF%29%20%28artificially%20introduced%20for%20auditing%29.%0ASubsequently%2C%20we%20formulate%20two%20primary%20attack%20types%3A%20evasion%20attacks%2C%20designed%0Ato%20conceal%20the%20use%20of%20a%20dataset%2C%20and%20forgery%20attacks%2C%20intending%20to%20falsely%0Aimplicate%20an%20unused%20dataset.%20Building%20on%20the%20understanding%20of%20existing%20methods%0Aand%20attack%20objectives%2C%20we%20further%20propose%20systematic%20attack%20strategies%3A%0Adecoupling%2C%20removal%2C%20and%20detection%20for%20evasion%3B%20adversarial%20example-based%0Amethods%20for%20forgery.%20These%20formulations%20and%20strategies%20lead%20to%20our%20new%0Abenchmark%2C%20DATABench%2C%20comprising%2017%20evasion%20attacks%2C%205%20forgery%20attacks%2C%20and%209%0Arepresentative%20auditing%20methods.%20Extensive%20evaluations%20using%20DATABench%20reveal%0Athat%20none%20of%20the%20evaluated%20auditing%20methods%20are%20sufficiently%20robust%20or%0Adistinctive%20under%20adversarial%20settings.%20These%20findings%20underscore%20the%20urgent%0Aneed%20for%20developing%20a%20more%20secure%20and%20reliable%20dataset%20auditing%20method%20capable%0Aof%20withstanding%20sophisticated%20adversarial%20manipulation.%20Code%20is%20available%20at%0Ahttps%3A//github.com/shaoshuo-ss/DATABench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05622v2&entry.124074799=Read"},
{"title": "Local Learning Rules for Out-of-Equilibrium Physical Generative Models", "author": "Cyrill B\u00f6sch and Geoffrey Roeder and Marc Serra-Garcia and Ryan P. Adams", "abstract": "  We show that the out-of-equilibrium driving protocol of score-based\ngenerative models (SGMs) can be learned via local learning rules. The gradient\nwith respect to the parameters of the driving protocol is computed directly\nfrom force measurements or from observed system dynamics. As a demonstration,\nwe implement an SGM in a network of driven, nonlinear, overdamped oscillators\ncoupled to a thermal bath. We first apply it to the problem of sampling from a\nmixture of two Gaussians in 2D. Finally, we train a 12x12 oscillator network on\nthe MNIST dataset to generate images of handwritten digits 0 and 1.\n", "link": "http://arxiv.org/abs/2506.19136v3", "date": "2025-08-27", "relevancy": 1.9963, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5011}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4984}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Learning%20Rules%20for%20Out-of-Equilibrium%20Physical%20Generative%20Models&body=Title%3A%20Local%20Learning%20Rules%20for%20Out-of-Equilibrium%20Physical%20Generative%20Models%0AAuthor%3A%20Cyrill%20B%C3%B6sch%20and%20Geoffrey%20Roeder%20and%20Marc%20Serra-Garcia%20and%20Ryan%20P.%20Adams%0AAbstract%3A%20%20%20We%20show%20that%20the%20out-of-equilibrium%20driving%20protocol%20of%20score-based%0Agenerative%20models%20%28SGMs%29%20can%20be%20learned%20via%20local%20learning%20rules.%20The%20gradient%0Awith%20respect%20to%20the%20parameters%20of%20the%20driving%20protocol%20is%20computed%20directly%0Afrom%20force%20measurements%20or%20from%20observed%20system%20dynamics.%20As%20a%20demonstration%2C%0Awe%20implement%20an%20SGM%20in%20a%20network%20of%20driven%2C%20nonlinear%2C%20overdamped%20oscillators%0Acoupled%20to%20a%20thermal%20bath.%20We%20first%20apply%20it%20to%20the%20problem%20of%20sampling%20from%20a%0Amixture%20of%20two%20Gaussians%20in%202D.%20Finally%2C%20we%20train%20a%2012x12%20oscillator%20network%20on%0Athe%20MNIST%20dataset%20to%20generate%20images%20of%20handwritten%20digits%200%20and%201.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19136v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Learning%2520Rules%2520for%2520Out-of-Equilibrium%2520Physical%2520Generative%2520Models%26entry.906535625%3DCyrill%2520B%25C3%25B6sch%2520and%2520Geoffrey%2520Roeder%2520and%2520Marc%2520Serra-Garcia%2520and%2520Ryan%2520P.%2520Adams%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520the%2520out-of-equilibrium%2520driving%2520protocol%2520of%2520score-based%250Agenerative%2520models%2520%2528SGMs%2529%2520can%2520be%2520learned%2520via%2520local%2520learning%2520rules.%2520The%2520gradient%250Awith%2520respect%2520to%2520the%2520parameters%2520of%2520the%2520driving%2520protocol%2520is%2520computed%2520directly%250Afrom%2520force%2520measurements%2520or%2520from%2520observed%2520system%2520dynamics.%2520As%2520a%2520demonstration%252C%250Awe%2520implement%2520an%2520SGM%2520in%2520a%2520network%2520of%2520driven%252C%2520nonlinear%252C%2520overdamped%2520oscillators%250Acoupled%2520to%2520a%2520thermal%2520bath.%2520We%2520first%2520apply%2520it%2520to%2520the%2520problem%2520of%2520sampling%2520from%2520a%250Amixture%2520of%2520two%2520Gaussians%2520in%25202D.%2520Finally%252C%2520we%2520train%2520a%252012x12%2520oscillator%2520network%2520on%250Athe%2520MNIST%2520dataset%2520to%2520generate%2520images%2520of%2520handwritten%2520digits%25200%2520and%25201.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19136v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Learning%20Rules%20for%20Out-of-Equilibrium%20Physical%20Generative%20Models&entry.906535625=Cyrill%20B%C3%B6sch%20and%20Geoffrey%20Roeder%20and%20Marc%20Serra-Garcia%20and%20Ryan%20P.%20Adams&entry.1292438233=%20%20We%20show%20that%20the%20out-of-equilibrium%20driving%20protocol%20of%20score-based%0Agenerative%20models%20%28SGMs%29%20can%20be%20learned%20via%20local%20learning%20rules.%20The%20gradient%0Awith%20respect%20to%20the%20parameters%20of%20the%20driving%20protocol%20is%20computed%20directly%0Afrom%20force%20measurements%20or%20from%20observed%20system%20dynamics.%20As%20a%20demonstration%2C%0Awe%20implement%20an%20SGM%20in%20a%20network%20of%20driven%2C%20nonlinear%2C%20overdamped%20oscillators%0Acoupled%20to%20a%20thermal%20bath.%20We%20first%20apply%20it%20to%20the%20problem%20of%20sampling%20from%20a%0Amixture%20of%20two%20Gaussians%20in%202D.%20Finally%2C%20we%20train%20a%2012x12%20oscillator%20network%20on%0Athe%20MNIST%20dataset%20to%20generate%20images%20of%20handwritten%20digits%200%20and%201.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19136v3&entry.124074799=Read"},
{"title": "Tree-Based Grafting Approach for Bidirectional Motion Planning with\n  Local Subsets Optimization", "author": "Liding Zhang and Yao Ling and Zhenshan Bing and Fan Wu and Sami Haddadin and Alois Knoll", "abstract": "  Bidirectional motion planning often reduces planning time compared to its\nunidirectional counterparts. It requires connecting the forward and reverse\nsearch trees to form a continuous path. However, this process could fail and\nrestart the asymmetric bidirectional search due to the limitations of\nlazy-reverse search. To address this challenge, we propose Greedy GuILD\nGrafting Trees (G3T*), a novel path planner that grafts invalid edge\nconnections at both ends to re-establish tree-based connectivity, enabling\nrapid path convergence. G3T* employs a greedy approach using the minimum\nLebesgue measure of guided incremental local densification (GuILD) subsets to\noptimize paths efficiently. Furthermore, G3T* dynamically adjusts the sampling\ndistribution between the informed set and GuILD subsets based on historical and\ncurrent cost improvements, ensuring asymptotic optimality. These features\nenhance the forward search's growth towards the reverse tree, achieving faster\nconvergence and lower solution costs. Benchmark experiments across dimensions\nfrom R^2 to R^8 and real-world robotic evaluations demonstrate G3T*'s superior\nperformance compared to existing single-query sampling-based planners. A video\nshowcasing our experimental results is available at:\nhttps://youtu.be/3mfCRL5SQIU\n", "link": "http://arxiv.org/abs/2508.19776v1", "date": "2025-08-27", "relevancy": 1.9839, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4995}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4955}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree-Based%20Grafting%20Approach%20for%20Bidirectional%20Motion%20Planning%20with%0A%20%20Local%20Subsets%20Optimization&body=Title%3A%20Tree-Based%20Grafting%20Approach%20for%20Bidirectional%20Motion%20Planning%20with%0A%20%20Local%20Subsets%20Optimization%0AAuthor%3A%20Liding%20Zhang%20and%20Yao%20Ling%20and%20Zhenshan%20Bing%20and%20Fan%20Wu%20and%20Sami%20Haddadin%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Bidirectional%20motion%20planning%20often%20reduces%20planning%20time%20compared%20to%20its%0Aunidirectional%20counterparts.%20It%20requires%20connecting%20the%20forward%20and%20reverse%0Asearch%20trees%20to%20form%20a%20continuous%20path.%20However%2C%20this%20process%20could%20fail%20and%0Arestart%20the%20asymmetric%20bidirectional%20search%20due%20to%20the%20limitations%20of%0Alazy-reverse%20search.%20To%20address%20this%20challenge%2C%20we%20propose%20Greedy%20GuILD%0AGrafting%20Trees%20%28G3T%2A%29%2C%20a%20novel%20path%20planner%20that%20grafts%20invalid%20edge%0Aconnections%20at%20both%20ends%20to%20re-establish%20tree-based%20connectivity%2C%20enabling%0Arapid%20path%20convergence.%20G3T%2A%20employs%20a%20greedy%20approach%20using%20the%20minimum%0ALebesgue%20measure%20of%20guided%20incremental%20local%20densification%20%28GuILD%29%20subsets%20to%0Aoptimize%20paths%20efficiently.%20Furthermore%2C%20G3T%2A%20dynamically%20adjusts%20the%20sampling%0Adistribution%20between%20the%20informed%20set%20and%20GuILD%20subsets%20based%20on%20historical%20and%0Acurrent%20cost%20improvements%2C%20ensuring%20asymptotic%20optimality.%20These%20features%0Aenhance%20the%20forward%20search%27s%20growth%20towards%20the%20reverse%20tree%2C%20achieving%20faster%0Aconvergence%20and%20lower%20solution%20costs.%20Benchmark%20experiments%20across%20dimensions%0Afrom%20R%5E2%20to%20R%5E8%20and%20real-world%20robotic%20evaluations%20demonstrate%20G3T%2A%27s%20superior%0Aperformance%20compared%20to%20existing%20single-query%20sampling-based%20planners.%20A%20video%0Ashowcasing%20our%20experimental%20results%20is%20available%20at%3A%0Ahttps%3A//youtu.be/3mfCRL5SQIU%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree-Based%2520Grafting%2520Approach%2520for%2520Bidirectional%2520Motion%2520Planning%2520with%250A%2520%2520Local%2520Subsets%2520Optimization%26entry.906535625%3DLiding%2520Zhang%2520and%2520Yao%2520Ling%2520and%2520Zhenshan%2520Bing%2520and%2520Fan%2520Wu%2520and%2520Sami%2520Haddadin%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Bidirectional%2520motion%2520planning%2520often%2520reduces%2520planning%2520time%2520compared%2520to%2520its%250Aunidirectional%2520counterparts.%2520It%2520requires%2520connecting%2520the%2520forward%2520and%2520reverse%250Asearch%2520trees%2520to%2520form%2520a%2520continuous%2520path.%2520However%252C%2520this%2520process%2520could%2520fail%2520and%250Arestart%2520the%2520asymmetric%2520bidirectional%2520search%2520due%2520to%2520the%2520limitations%2520of%250Alazy-reverse%2520search.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Greedy%2520GuILD%250AGrafting%2520Trees%2520%2528G3T%252A%2529%252C%2520a%2520novel%2520path%2520planner%2520that%2520grafts%2520invalid%2520edge%250Aconnections%2520at%2520both%2520ends%2520to%2520re-establish%2520tree-based%2520connectivity%252C%2520enabling%250Arapid%2520path%2520convergence.%2520G3T%252A%2520employs%2520a%2520greedy%2520approach%2520using%2520the%2520minimum%250ALebesgue%2520measure%2520of%2520guided%2520incremental%2520local%2520densification%2520%2528GuILD%2529%2520subsets%2520to%250Aoptimize%2520paths%2520efficiently.%2520Furthermore%252C%2520G3T%252A%2520dynamically%2520adjusts%2520the%2520sampling%250Adistribution%2520between%2520the%2520informed%2520set%2520and%2520GuILD%2520subsets%2520based%2520on%2520historical%2520and%250Acurrent%2520cost%2520improvements%252C%2520ensuring%2520asymptotic%2520optimality.%2520These%2520features%250Aenhance%2520the%2520forward%2520search%2527s%2520growth%2520towards%2520the%2520reverse%2520tree%252C%2520achieving%2520faster%250Aconvergence%2520and%2520lower%2520solution%2520costs.%2520Benchmark%2520experiments%2520across%2520dimensions%250Afrom%2520R%255E2%2520to%2520R%255E8%2520and%2520real-world%2520robotic%2520evaluations%2520demonstrate%2520G3T%252A%2527s%2520superior%250Aperformance%2520compared%2520to%2520existing%2520single-query%2520sampling-based%2520planners.%2520A%2520video%250Ashowcasing%2520our%2520experimental%2520results%2520is%2520available%2520at%253A%250Ahttps%253A//youtu.be/3mfCRL5SQIU%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree-Based%20Grafting%20Approach%20for%20Bidirectional%20Motion%20Planning%20with%0A%20%20Local%20Subsets%20Optimization&entry.906535625=Liding%20Zhang%20and%20Yao%20Ling%20and%20Zhenshan%20Bing%20and%20Fan%20Wu%20and%20Sami%20Haddadin%20and%20Alois%20Knoll&entry.1292438233=%20%20Bidirectional%20motion%20planning%20often%20reduces%20planning%20time%20compared%20to%20its%0Aunidirectional%20counterparts.%20It%20requires%20connecting%20the%20forward%20and%20reverse%0Asearch%20trees%20to%20form%20a%20continuous%20path.%20However%2C%20this%20process%20could%20fail%20and%0Arestart%20the%20asymmetric%20bidirectional%20search%20due%20to%20the%20limitations%20of%0Alazy-reverse%20search.%20To%20address%20this%20challenge%2C%20we%20propose%20Greedy%20GuILD%0AGrafting%20Trees%20%28G3T%2A%29%2C%20a%20novel%20path%20planner%20that%20grafts%20invalid%20edge%0Aconnections%20at%20both%20ends%20to%20re-establish%20tree-based%20connectivity%2C%20enabling%0Arapid%20path%20convergence.%20G3T%2A%20employs%20a%20greedy%20approach%20using%20the%20minimum%0ALebesgue%20measure%20of%20guided%20incremental%20local%20densification%20%28GuILD%29%20subsets%20to%0Aoptimize%20paths%20efficiently.%20Furthermore%2C%20G3T%2A%20dynamically%20adjusts%20the%20sampling%0Adistribution%20between%20the%20informed%20set%20and%20GuILD%20subsets%20based%20on%20historical%20and%0Acurrent%20cost%20improvements%2C%20ensuring%20asymptotic%20optimality.%20These%20features%0Aenhance%20the%20forward%20search%27s%20growth%20towards%20the%20reverse%20tree%2C%20achieving%20faster%0Aconvergence%20and%20lower%20solution%20costs.%20Benchmark%20experiments%20across%20dimensions%0Afrom%20R%5E2%20to%20R%5E8%20and%20real-world%20robotic%20evaluations%20demonstrate%20G3T%2A%27s%20superior%0Aperformance%20compared%20to%20existing%20single-query%20sampling-based%20planners.%20A%20video%0Ashowcasing%20our%20experimental%20results%20is%20available%20at%3A%0Ahttps%3A//youtu.be/3mfCRL5SQIU%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19776v1&entry.124074799=Read"},
{"title": "ERSR: An Ellipse-constrained pseudo-label refinement and symmetric\n  regularization framework for semi-supervised fetal head segmentation in\n  ultrasound images", "author": "Linkuan Zhou and Zhexin Chen and Yufei Shen and Junlin Xu and Ping Xuan and Yixin Zhu and Yuqi Fang and Cong Cong and Leyi Wei and Ran Su and Jia Zhou and Qiangguo Jin", "abstract": "  Automated segmentation of the fetal head in ultrasound images is critical for\nprenatal monitoring. However, achieving robust segmentation remains challenging\ndue to the poor quality of ultrasound images and the lack of annotated data.\nSemi-supervised methods alleviate the lack of annotated data but struggle with\nthe unique characteristics of fetal head ultrasound images, making it\nchallenging to generate reliable pseudo-labels and enforce effective\nconsistency regularization constraints. To address this issue, we propose a\nnovel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.\nOur framework consists of the dual-scoring adaptive filtering strategy, the\nellipse-constrained pseudo-label refinement, and the symmetry-based multiple\nconsistency regularization. The dual-scoring adaptive filtering strategy uses\nboundary consistency and contour regularity criteria to evaluate and filter\nteacher outputs. The ellipse-constrained pseudo-label refinement refines these\nfiltered outputs by fitting least-squares ellipses, which strengthens pixels\nnear the center of the fitted ellipse and suppresses noise simultaneously. The\nsymmetry-based multiple consistency regularization enforces multi-level\nconsistency across perturbed images, symmetric regions, and between original\npredictions and pseudo-labels, enabling the model to capture robust and stable\nshape representations. Our method achieves state-of-the-art performance on two\nbenchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%\nwith 10% and 20% labeled data, respectively. On the PSFH dataset, the scores\nare 91.68% and 93.70% under the same settings.\n", "link": "http://arxiv.org/abs/2508.19815v1", "date": "2025-08-27", "relevancy": 1.9799, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.541}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4867}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ERSR%3A%20An%20Ellipse-constrained%20pseudo-label%20refinement%20and%20symmetric%0A%20%20regularization%20framework%20for%20semi-supervised%20fetal%20head%20segmentation%20in%0A%20%20ultrasound%20images&body=Title%3A%20ERSR%3A%20An%20Ellipse-constrained%20pseudo-label%20refinement%20and%20symmetric%0A%20%20regularization%20framework%20for%20semi-supervised%20fetal%20head%20segmentation%20in%0A%20%20ultrasound%20images%0AAuthor%3A%20Linkuan%20Zhou%20and%20Zhexin%20Chen%20and%20Yufei%20Shen%20and%20Junlin%20Xu%20and%20Ping%20Xuan%20and%20Yixin%20Zhu%20and%20Yuqi%20Fang%20and%20Cong%20Cong%20and%20Leyi%20Wei%20and%20Ran%20Su%20and%20Jia%20Zhou%20and%20Qiangguo%20Jin%0AAbstract%3A%20%20%20Automated%20segmentation%20of%20the%20fetal%20head%20in%20ultrasound%20images%20is%20critical%20for%0Aprenatal%20monitoring.%20However%2C%20achieving%20robust%20segmentation%20remains%20challenging%0Adue%20to%20the%20poor%20quality%20of%20ultrasound%20images%20and%20the%20lack%20of%20annotated%20data.%0ASemi-supervised%20methods%20alleviate%20the%20lack%20of%20annotated%20data%20but%20struggle%20with%0Athe%20unique%20characteristics%20of%20fetal%20head%20ultrasound%20images%2C%20making%20it%0Achallenging%20to%20generate%20reliable%20pseudo-labels%20and%20enforce%20effective%0Aconsistency%20regularization%20constraints.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20semi-supervised%20framework%2C%20ERSR%2C%20for%20fetal%20head%20ultrasound%20segmentation.%0AOur%20framework%20consists%20of%20the%20dual-scoring%20adaptive%20filtering%20strategy%2C%20the%0Aellipse-constrained%20pseudo-label%20refinement%2C%20and%20the%20symmetry-based%20multiple%0Aconsistency%20regularization.%20The%20dual-scoring%20adaptive%20filtering%20strategy%20uses%0Aboundary%20consistency%20and%20contour%20regularity%20criteria%20to%20evaluate%20and%20filter%0Ateacher%20outputs.%20The%20ellipse-constrained%20pseudo-label%20refinement%20refines%20these%0Afiltered%20outputs%20by%20fitting%20least-squares%20ellipses%2C%20which%20strengthens%20pixels%0Anear%20the%20center%20of%20the%20fitted%20ellipse%20and%20suppresses%20noise%20simultaneously.%20The%0Asymmetry-based%20multiple%20consistency%20regularization%20enforces%20multi-level%0Aconsistency%20across%20perturbed%20images%2C%20symmetric%20regions%2C%20and%20between%20original%0Apredictions%20and%20pseudo-labels%2C%20enabling%20the%20model%20to%20capture%20robust%20and%20stable%0Ashape%20representations.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20two%0Abenchmarks.%20On%20the%20HC18%20dataset%2C%20it%20reaches%20Dice%20scores%20of%2092.05%25%20and%2095.36%25%0Awith%2010%25%20and%2020%25%20labeled%20data%2C%20respectively.%20On%20the%20PSFH%20dataset%2C%20the%20scores%0Aare%2091.68%25%20and%2093.70%25%20under%20the%20same%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DERSR%253A%2520An%2520Ellipse-constrained%2520pseudo-label%2520refinement%2520and%2520symmetric%250A%2520%2520regularization%2520framework%2520for%2520semi-supervised%2520fetal%2520head%2520segmentation%2520in%250A%2520%2520ultrasound%2520images%26entry.906535625%3DLinkuan%2520Zhou%2520and%2520Zhexin%2520Chen%2520and%2520Yufei%2520Shen%2520and%2520Junlin%2520Xu%2520and%2520Ping%2520Xuan%2520and%2520Yixin%2520Zhu%2520and%2520Yuqi%2520Fang%2520and%2520Cong%2520Cong%2520and%2520Leyi%2520Wei%2520and%2520Ran%2520Su%2520and%2520Jia%2520Zhou%2520and%2520Qiangguo%2520Jin%26entry.1292438233%3D%2520%2520Automated%2520segmentation%2520of%2520the%2520fetal%2520head%2520in%2520ultrasound%2520images%2520is%2520critical%2520for%250Aprenatal%2520monitoring.%2520However%252C%2520achieving%2520robust%2520segmentation%2520remains%2520challenging%250Adue%2520to%2520the%2520poor%2520quality%2520of%2520ultrasound%2520images%2520and%2520the%2520lack%2520of%2520annotated%2520data.%250ASemi-supervised%2520methods%2520alleviate%2520the%2520lack%2520of%2520annotated%2520data%2520but%2520struggle%2520with%250Athe%2520unique%2520characteristics%2520of%2520fetal%2520head%2520ultrasound%2520images%252C%2520making%2520it%250Achallenging%2520to%2520generate%2520reliable%2520pseudo-labels%2520and%2520enforce%2520effective%250Aconsistency%2520regularization%2520constraints.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Anovel%2520semi-supervised%2520framework%252C%2520ERSR%252C%2520for%2520fetal%2520head%2520ultrasound%2520segmentation.%250AOur%2520framework%2520consists%2520of%2520the%2520dual-scoring%2520adaptive%2520filtering%2520strategy%252C%2520the%250Aellipse-constrained%2520pseudo-label%2520refinement%252C%2520and%2520the%2520symmetry-based%2520multiple%250Aconsistency%2520regularization.%2520The%2520dual-scoring%2520adaptive%2520filtering%2520strategy%2520uses%250Aboundary%2520consistency%2520and%2520contour%2520regularity%2520criteria%2520to%2520evaluate%2520and%2520filter%250Ateacher%2520outputs.%2520The%2520ellipse-constrained%2520pseudo-label%2520refinement%2520refines%2520these%250Afiltered%2520outputs%2520by%2520fitting%2520least-squares%2520ellipses%252C%2520which%2520strengthens%2520pixels%250Anear%2520the%2520center%2520of%2520the%2520fitted%2520ellipse%2520and%2520suppresses%2520noise%2520simultaneously.%2520The%250Asymmetry-based%2520multiple%2520consistency%2520regularization%2520enforces%2520multi-level%250Aconsistency%2520across%2520perturbed%2520images%252C%2520symmetric%2520regions%252C%2520and%2520between%2520original%250Apredictions%2520and%2520pseudo-labels%252C%2520enabling%2520the%2520model%2520to%2520capture%2520robust%2520and%2520stable%250Ashape%2520representations.%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520two%250Abenchmarks.%2520On%2520the%2520HC18%2520dataset%252C%2520it%2520reaches%2520Dice%2520scores%2520of%252092.05%2525%2520and%252095.36%2525%250Awith%252010%2525%2520and%252020%2525%2520labeled%2520data%252C%2520respectively.%2520On%2520the%2520PSFH%2520dataset%252C%2520the%2520scores%250Aare%252091.68%2525%2520and%252093.70%2525%2520under%2520the%2520same%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ERSR%3A%20An%20Ellipse-constrained%20pseudo-label%20refinement%20and%20symmetric%0A%20%20regularization%20framework%20for%20semi-supervised%20fetal%20head%20segmentation%20in%0A%20%20ultrasound%20images&entry.906535625=Linkuan%20Zhou%20and%20Zhexin%20Chen%20and%20Yufei%20Shen%20and%20Junlin%20Xu%20and%20Ping%20Xuan%20and%20Yixin%20Zhu%20and%20Yuqi%20Fang%20and%20Cong%20Cong%20and%20Leyi%20Wei%20and%20Ran%20Su%20and%20Jia%20Zhou%20and%20Qiangguo%20Jin&entry.1292438233=%20%20Automated%20segmentation%20of%20the%20fetal%20head%20in%20ultrasound%20images%20is%20critical%20for%0Aprenatal%20monitoring.%20However%2C%20achieving%20robust%20segmentation%20remains%20challenging%0Adue%20to%20the%20poor%20quality%20of%20ultrasound%20images%20and%20the%20lack%20of%20annotated%20data.%0ASemi-supervised%20methods%20alleviate%20the%20lack%20of%20annotated%20data%20but%20struggle%20with%0Athe%20unique%20characteristics%20of%20fetal%20head%20ultrasound%20images%2C%20making%20it%0Achallenging%20to%20generate%20reliable%20pseudo-labels%20and%20enforce%20effective%0Aconsistency%20regularization%20constraints.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20semi-supervised%20framework%2C%20ERSR%2C%20for%20fetal%20head%20ultrasound%20segmentation.%0AOur%20framework%20consists%20of%20the%20dual-scoring%20adaptive%20filtering%20strategy%2C%20the%0Aellipse-constrained%20pseudo-label%20refinement%2C%20and%20the%20symmetry-based%20multiple%0Aconsistency%20regularization.%20The%20dual-scoring%20adaptive%20filtering%20strategy%20uses%0Aboundary%20consistency%20and%20contour%20regularity%20criteria%20to%20evaluate%20and%20filter%0Ateacher%20outputs.%20The%20ellipse-constrained%20pseudo-label%20refinement%20refines%20these%0Afiltered%20outputs%20by%20fitting%20least-squares%20ellipses%2C%20which%20strengthens%20pixels%0Anear%20the%20center%20of%20the%20fitted%20ellipse%20and%20suppresses%20noise%20simultaneously.%20The%0Asymmetry-based%20multiple%20consistency%20regularization%20enforces%20multi-level%0Aconsistency%20across%20perturbed%20images%2C%20symmetric%20regions%2C%20and%20between%20original%0Apredictions%20and%20pseudo-labels%2C%20enabling%20the%20model%20to%20capture%20robust%20and%20stable%0Ashape%20representations.%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20two%0Abenchmarks.%20On%20the%20HC18%20dataset%2C%20it%20reaches%20Dice%20scores%20of%2092.05%25%20and%2095.36%25%0Awith%2010%25%20and%2020%25%20labeled%20data%2C%20respectively.%20On%20the%20PSFH%20dataset%2C%20the%20scores%0Aare%2091.68%25%20and%2093.70%25%20under%20the%20same%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19815v1&entry.124074799=Read"},
{"title": "Apple Intelligence Foundation Language Models: Tech Report 2025", "author": "Ethan Li and Anders Boesen Lindbo Larsen and Chen Zhang and Xiyou Zhou and Jun Qin and Dian Ang Yap and Narendran Raghavan and Xuankai Chang and Margit Bowler and Eray Yildiz and John Peebles and Hannah Gillis Coleman and Matteo Ronchi and Peter Gray and Keen You and Anthony Spalvieri-Kruse and Ruoming Pang and Reed Li and Yuli Yang and Emad Soroush and Zhiyun Lu and Crystal Xiao and Rong Situ and Jordan Huffaker and David Griffiths and Zaid Ahmed and Peng Zhang and Daniel Parilla and Asaf Liberman and Jennifer Mallalieu and Parsa Mazaheri and Qibin Chen and Manjot Bilkhu and Aonan Zhang and Eric Wang and Dave Nelson and Michael FitzMaurice and Thomas Voice and Jeremy Liu and Josh Shaffer and Shiwen Zhao and Prasanth Yadla and Farzin Rasteh and Pengsheng Guo and Arsalan Farooq and Jeremy Snow and Stephen Murphy and Tao Lei and Minsik Cho and George Horrell and Sam Dodge and Lindsay Hislop and Sumeet Singh and Alex Dombrowski and Aiswarya Raghavan and Sasha Sirovica and Mandana Saebi and Faye Lao and Max Lam and TJ Lu and Zhaoyang Xu and Karanjeet Singh and Marc Kirchner and David Mizrahi and Rajat Arora and Haotian Zhang and Henry Mason and Lawrence Zhou and Yi Hua and Ankur Jain and Felix Bai and Joseph Astrauskas and Floris Weers and Josh Gardner and Mira Chiang and Yi Zhang and Pulkit Agrawal and Tony Sun and Quentin Keunebroek and Matthew Hopkins and Bugu Wu and Tao Jia and Chen Chen and Xingyu Zhou and Nanzhu Wang and Peng Liu and Ruixuan Hou and Rene Rauch and Yuan Gao and Afshin Dehghan and Jonathan Janke and Zirui Wang and Cha Chen and Xiaoyi Ren and Feng Nan and Josh Elman and Dong Yin and Yusuf Goren and Jeff Lai and Yiran Fei and Syd Evans and Muyang Yu and Guoli Yin and Yi Qin and Erin Feldman and Isha Garg and Aparna Rajamani and Karla Vega and Walker Cheng and TJ Collins and Hans Han and Raul Rea Menacho and Simon Yeung and Sophy Lee and Phani Mutyala and Ying-Chang Cheng and Zhe Gan and Sprite Chu and Justin Lazarow and Alessandro Pappalardo and Federico Scozzafava and Jing Lu and Erik Daxberger and Laurent Duchesne and Jen Liu and David G\u00fcera and Stefano Ligas and Mary Beth Kery and Brent Ramerth and Ciro Sannino and Marcin Eichner and Haoshuo Huang and Rui Qian and Moritz Schwarzer-Becker and David Riazati and Mingfei Gao and Bailin Wang and Jack Cackler and Yang Lu and Ransen Niu and John Dennison and Guillaume Klein and Jeffrey Bigham and Deepak Gopinath and Navid Shiee and Darren Botten and Guillaume Tartavel and Alex Guillen Garcia and Sam Xu and Victoria M\u00f6nchJuan Haladjian and Zi-Yi Dou and Matthias Paulik and Adolfo Lopez Mendez and Zhen Li and Hong-You Chen and Chao Jia and Dhaval Doshi and Zhengdong Zhang and Raunak Manjani and Aaron Franklin and Zhile Ren and David Chen and Artsiom Peshko and Nandhitha Raghuram and Hans Hao and Jiulong Shan and Kavya Nerella and Ramsey Tantawi and Vivek Kumar and Saiwen Wang and Brycen Wershing and Bhuwan Dhingra and Dhruti Shah and Ob Adaranijo and Xin Zheng and Tait Madsen and Hadas Kotek and Chang Liu and Yin Xia and Hanli Li and Suma Jayaram and Yanchao Sun and Ahmed Fakhry and Vasileios Saveris and Dustin Withers and Yanghao Li and Alp Aygar and Andres Romero Mier Y Teran and Kaiwei Huang and Mark Lee and Xiujun Li and Yuhong Li and Tyler Johnson and Jay Tang and Joseph Yitan Cheng and Futang Peng and Andrew Walkingshaw and Lucas Guibert and Abhishek Sharma and Cheng Shen and Piotr Maj and Yasutaka Tanaka and You-Cyuan Jhang and Vivian Ma and Tommi Vehvilainen and Kelvin Zou and Jeff Nichols and Matthew Lei and David Qiu and Yihao Qian and Gokul Santhanam and Wentao Wu and Yena Han and Dominik Moritz and Haijing Fu and Mingze Xu and Vivek Rathod and Jian Liu and Louis D'hauwe and Qin Ba and Haitian Sun and Haoran Yan and Philipp Dufter and Anh Nguyen and Yihao Feng and Emma Wang and Keyu He and Rahul Nair and Sanskruti Shah and Jiarui Lu and Patrick Sonnenberg and Jeremy Warner and Yuanzhi Li and Bowen Pan and Ziyi Zhong and Joe Zhou and Sam Davarnia and Olli Saarikivi and Irina Belousova and Rachel Burger and Shang-Chen Wu and Di Feng and Bas Straathof and James Chou and Yuanyang Zhang and Marco Zuliani and Eduardo Jimenez and Abhishek Sundararajan and Xianzhi Du and Chang Lan and Nilesh Shahdadpuri and Peter Grasch and Sergiu Sima and Josh Newnham and Varsha Paidi and Jianyu Wang and Kaelen Haag and Alex Braunstein and Daniele Molinari and Richard Wei and Brenda Yang and Nicholas Lusskin and Joanna Arreaza-Taylor and Meng Cao and Nicholas Seidl and Simon Wang and Jiaming Hu and Yiping Ma and Mengyu Li and Kieran Liu and Hang Su and Sachin Ravi and Chong Wang and Xin Wang and Kevin Smith and Haoxuan You and Binazir Karimzadeh and Rui Li and Jinhao Lei and Wei Fang and Alec Doane and Sam Wiseman and Ismael Fernandez and Jane Li and Andrew Hansen and Javier Movellan and Christopher Neubauer and Hanzhi Zhou and Chris Chaney and Nazir Kamaldin and Valentin Wolf and Fernando Berm\u00fadez-Medina and Joris Pelemans and Peter Fu and Howard Xing and Xiang Kong and Wayne Shan and Gabriel Jacoby-Cooper and Dongcai Shen and Tom Gunter and Guillaume Seguin and Fangping Shi and Shiyu Li and Yang Xu and Areeba Kamal and Dan Masi and Saptarshi Guha and Qi Zhu and Jenna Thibodeau and Changyuan Zhang and Rebecca Callahan and Charles Maalouf and Wilson Tsao and Boyue Li and Qingqing Cao and Naomy Sabo and Cheng Leong and Yi Wang and Anupama Mann Anupama and Colorado Reed and Kenneth Jung and Zhifeng Chen and Mohana Prasad Sathya Moorthy and Yifei He and Erik Hornberger and Devi Krishna and Senyu Tong and  Michael and  Lee and David Haldimann and Yang Zhao and Bowen Zhang and Chang Gao and Chris Bartels and Sushma Rao and Nathalie Tran and Simon Lehnerer and Co Giang and Patrick Dong and Junting Pan and Biyao Wang and Dongxu Li and Mehrdad Farajtabar and Dongseong Hwang and Grace Duanmu and Eshan Verma and Sujeeth Reddy and Qi Shan and Hongbin Gao and Nan Du and Pragnya Sridhar and Forrest Huang and Yingbo Wang and Nikhil Bhendawade and Diane Zhu and Sai Aitharaju and Fred Hohman and Lauren Gardiner and Chung-Cheng Chiu and Yinfei Yang and Alper Kokmen and Frank Chu and Ke Ye and Kaan Elgin and Oron Levy and John Park and Donald Zhang and Eldon Schoop and Nina Wenzel and Michael Booker and Hyunjik Kim and Chinguun Erdenebileg and Nan Dun and Eric Liang Yang and Priyal Chhatrapati and Vishaal Mahtani and Haiming Gang and Kohen Chia and Deepa Seshadri and Donghan Yu and Yan Meng and Kelsey Peterson and Zhen Yang and Yongqiang Wang and Carina Peng and Doug Kang and Anuva Agarwal and Albert Antony and Juan Lao Tebar and Albin Madappally Jose and Regan Poston and Andy De Wang and Gerard Casamayor and Elmira Amirloo and Violet Yao and Wojciech Kryscinski and Kun Duan and Lezhi L", "abstract": "  We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.\n", "link": "http://arxiv.org/abs/2507.13575v3", "date": "2025-08-27", "relevancy": 1.9713, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Apple%20Intelligence%20Foundation%20Language%20Models%3A%20Tech%20Report%202025&body=Title%3A%20Apple%20Intelligence%20Foundation%20Language%20Models%3A%20Tech%20Report%202025%0AAuthor%3A%20Ethan%20Li%20and%20Anders%20Boesen%20Lindbo%20Larsen%20and%20Chen%20Zhang%20and%20Xiyou%20Zhou%20and%20Jun%20Qin%20and%20Dian%20Ang%20Yap%20and%20Narendran%20Raghavan%20and%20Xuankai%20Chang%20and%20Margit%20Bowler%20and%20Eray%20Yildiz%20and%20John%20Peebles%20and%20Hannah%20Gillis%20Coleman%20and%20Matteo%20Ronchi%20and%20Peter%20Gray%20and%20Keen%20You%20and%20Anthony%20Spalvieri-Kruse%20and%20Ruoming%20Pang%20and%20Reed%20Li%20and%20Yuli%20Yang%20and%20Emad%20Soroush%20and%20Zhiyun%20Lu%20and%20Crystal%20Xiao%20and%20Rong%20Situ%20and%20Jordan%20Huffaker%20and%20David%20Griffiths%20and%20Zaid%20Ahmed%20and%20Peng%20Zhang%20and%20Daniel%20Parilla%20and%20Asaf%20Liberman%20and%20Jennifer%20Mallalieu%20and%20Parsa%20Mazaheri%20and%20Qibin%20Chen%20and%20Manjot%20Bilkhu%20and%20Aonan%20Zhang%20and%20Eric%20Wang%20and%20Dave%20Nelson%20and%20Michael%20FitzMaurice%20and%20Thomas%20Voice%20and%20Jeremy%20Liu%20and%20Josh%20Shaffer%20and%20Shiwen%20Zhao%20and%20Prasanth%20Yadla%20and%20Farzin%20Rasteh%20and%20Pengsheng%20Guo%20and%20Arsalan%20Farooq%20and%20Jeremy%20Snow%20and%20Stephen%20Murphy%20and%20Tao%20Lei%20and%20Minsik%20Cho%20and%20George%20Horrell%20and%20Sam%20Dodge%20and%20Lindsay%20Hislop%20and%20Sumeet%20Singh%20and%20Alex%20Dombrowski%20and%20Aiswarya%20Raghavan%20and%20Sasha%20Sirovica%20and%20Mandana%20Saebi%20and%20Faye%20Lao%20and%20Max%20Lam%20and%20TJ%20Lu%20and%20Zhaoyang%20Xu%20and%20Karanjeet%20Singh%20and%20Marc%20Kirchner%20and%20David%20Mizrahi%20and%20Rajat%20Arora%20and%20Haotian%20Zhang%20and%20Henry%20Mason%20and%20Lawrence%20Zhou%20and%20Yi%20Hua%20and%20Ankur%20Jain%20and%20Felix%20Bai%20and%20Joseph%20Astrauskas%20and%20Floris%20Weers%20and%20Josh%20Gardner%20and%20Mira%20Chiang%20and%20Yi%20Zhang%20and%20Pulkit%20Agrawal%20and%20Tony%20Sun%20and%20Quentin%20Keunebroek%20and%20Matthew%20Hopkins%20and%20Bugu%20Wu%20and%20Tao%20Jia%20and%20Chen%20Chen%20and%20Xingyu%20Zhou%20and%20Nanzhu%20Wang%20and%20Peng%20Liu%20and%20Ruixuan%20Hou%20and%20Rene%20Rauch%20and%20Yuan%20Gao%20and%20Afshin%20Dehghan%20and%20Jonathan%20Janke%20and%20Zirui%20Wang%20and%20Cha%20Chen%20and%20Xiaoyi%20Ren%20and%20Feng%20Nan%20and%20Josh%20Elman%20and%20Dong%20Yin%20and%20Yusuf%20Goren%20and%20Jeff%20Lai%20and%20Yiran%20Fei%20and%20Syd%20Evans%20and%20Muyang%20Yu%20and%20Guoli%20Yin%20and%20Yi%20Qin%20and%20Erin%20Feldman%20and%20Isha%20Garg%20and%20Aparna%20Rajamani%20and%20Karla%20Vega%20and%20Walker%20Cheng%20and%20TJ%20Collins%20and%20Hans%20Han%20and%20Raul%20Rea%20Menacho%20and%20Simon%20Yeung%20and%20Sophy%20Lee%20and%20Phani%20Mutyala%20and%20Ying-Chang%20Cheng%20and%20Zhe%20Gan%20and%20Sprite%20Chu%20and%20Justin%20Lazarow%20and%20Alessandro%20Pappalardo%20and%20Federico%20Scozzafava%20and%20Jing%20Lu%20and%20Erik%20Daxberger%20and%20Laurent%20Duchesne%20and%20Jen%20Liu%20and%20David%20G%C3%BCera%20and%20Stefano%20Ligas%20and%20Mary%20Beth%20Kery%20and%20Brent%20Ramerth%20and%20Ciro%20Sannino%20and%20Marcin%20Eichner%20and%20Haoshuo%20Huang%20and%20Rui%20Qian%20and%20Moritz%20Schwarzer-Becker%20and%20David%20Riazati%20and%20Mingfei%20Gao%20and%20Bailin%20Wang%20and%20Jack%20Cackler%20and%20Yang%20Lu%20and%20Ransen%20Niu%20and%20John%20Dennison%20and%20Guillaume%20Klein%20and%20Jeffrey%20Bigham%20and%20Deepak%20Gopinath%20and%20Navid%20Shiee%20and%20Darren%20Botten%20and%20Guillaume%20Tartavel%20and%20Alex%20Guillen%20Garcia%20and%20Sam%20Xu%20and%20Victoria%20M%C3%B6nchJuan%20Haladjian%20and%20Zi-Yi%20Dou%20and%20Matthias%20Paulik%20and%20Adolfo%20Lopez%20Mendez%20and%20Zhen%20Li%20and%20Hong-You%20Chen%20and%20Chao%20Jia%20and%20Dhaval%20Doshi%20and%20Zhengdong%20Zhang%20and%20Raunak%20Manjani%20and%20Aaron%20Franklin%20and%20Zhile%20Ren%20and%20David%20Chen%20and%20Artsiom%20Peshko%20and%20Nandhitha%20Raghuram%20and%20Hans%20Hao%20and%20Jiulong%20Shan%20and%20Kavya%20Nerella%20and%20Ramsey%20Tantawi%20and%20Vivek%20Kumar%20and%20Saiwen%20Wang%20and%20Brycen%20Wershing%20and%20Bhuwan%20Dhingra%20and%20Dhruti%20Shah%20and%20Ob%20Adaranijo%20and%20Xin%20Zheng%20and%20Tait%20Madsen%20and%20Hadas%20Kotek%20and%20Chang%20Liu%20and%20Yin%20Xia%20and%20Hanli%20Li%20and%20Suma%20Jayaram%20and%20Yanchao%20Sun%20and%20Ahmed%20Fakhry%20and%20Vasileios%20Saveris%20and%20Dustin%20Withers%20and%20Yanghao%20Li%20and%20Alp%20Aygar%20and%20Andres%20Romero%20Mier%20Y%20Teran%20and%20Kaiwei%20Huang%20and%20Mark%20Lee%20and%20Xiujun%20Li%20and%20Yuhong%20Li%20and%20Tyler%20Johnson%20and%20Jay%20Tang%20and%20Joseph%20Yitan%20Cheng%20and%20Futang%20Peng%20and%20Andrew%20Walkingshaw%20and%20Lucas%20Guibert%20and%20Abhishek%20Sharma%20and%20Cheng%20Shen%20and%20Piotr%20Maj%20and%20Yasutaka%20Tanaka%20and%20You-Cyuan%20Jhang%20and%20Vivian%20Ma%20and%20Tommi%20Vehvilainen%20and%20Kelvin%20Zou%20and%20Jeff%20Nichols%20and%20Matthew%20Lei%20and%20David%20Qiu%20and%20Yihao%20Qian%20and%20Gokul%20Santhanam%20and%20Wentao%20Wu%20and%20Yena%20Han%20and%20Dominik%20Moritz%20and%20Haijing%20Fu%20and%20Mingze%20Xu%20and%20Vivek%20Rathod%20and%20Jian%20Liu%20and%20Louis%20D%27hauwe%20and%20Qin%20Ba%20and%20Haitian%20Sun%20and%20Haoran%20Yan%20and%20Philipp%20Dufter%20and%20Anh%20Nguyen%20and%20Yihao%20Feng%20and%20Emma%20Wang%20and%20Keyu%20He%20and%20Rahul%20Nair%20and%20Sanskruti%20Shah%20and%20Jiarui%20Lu%20and%20Patrick%20Sonnenberg%20and%20Jeremy%20Warner%20and%20Yuanzhi%20Li%20and%20Bowen%20Pan%20and%20Ziyi%20Zhong%20and%20Joe%20Zhou%20and%20Sam%20Davarnia%20and%20Olli%20Saarikivi%20and%20Irina%20Belousova%20and%20Rachel%20Burger%20and%20Shang-Chen%20Wu%20and%20Di%20Feng%20and%20Bas%20Straathof%20and%20James%20Chou%20and%20Yuanyang%20Zhang%20and%20Marco%20Zuliani%20and%20Eduardo%20Jimenez%20and%20Abhishek%20Sundararajan%20and%20Xianzhi%20Du%20and%20Chang%20Lan%20and%20Nilesh%20Shahdadpuri%20and%20Peter%20Grasch%20and%20Sergiu%20Sima%20and%20Josh%20Newnham%20and%20Varsha%20Paidi%20and%20Jianyu%20Wang%20and%20Kaelen%20Haag%20and%20Alex%20Braunstein%20and%20Daniele%20Molinari%20and%20Richard%20Wei%20and%20Brenda%20Yang%20and%20Nicholas%20Lusskin%20and%20Joanna%20Arreaza-Taylor%20and%20Meng%20Cao%20and%20Nicholas%20Seidl%20and%20Simon%20Wang%20and%20Jiaming%20Hu%20and%20Yiping%20Ma%20and%20Mengyu%20Li%20and%20Kieran%20Liu%20and%20Hang%20Su%20and%20Sachin%20Ravi%20and%20Chong%20Wang%20and%20Xin%20Wang%20and%20Kevin%20Smith%20and%20Haoxuan%20You%20and%20Binazir%20Karimzadeh%20and%20Rui%20Li%20and%20Jinhao%20Lei%20and%20Wei%20Fang%20and%20Alec%20Doane%20and%20Sam%20Wiseman%20and%20Ismael%20Fernandez%20and%20Jane%20Li%20and%20Andrew%20Hansen%20and%20Javier%20Movellan%20and%20Christopher%20Neubauer%20and%20Hanzhi%20Zhou%20and%20Chris%20Chaney%20and%20Nazir%20Kamaldin%20and%20Valentin%20Wolf%20and%20Fernando%20Berm%C3%BAdez-Medina%20and%20Joris%20Pelemans%20and%20Peter%20Fu%20and%20Howard%20Xing%20and%20Xiang%20Kong%20and%20Wayne%20Shan%20and%20Gabriel%20Jacoby-Cooper%20and%20Dongcai%20Shen%20and%20Tom%20Gunter%20and%20Guillaume%20Seguin%20and%20Fangping%20Shi%20and%20Shiyu%20Li%20and%20Yang%20Xu%20and%20Areeba%20Kamal%20and%20Dan%20Masi%20and%20Saptarshi%20Guha%20and%20Qi%20Zhu%20and%20Jenna%20Thibodeau%20and%20Changyuan%20Zhang%20and%20Rebecca%20Callahan%20and%20Charles%20Maalouf%20and%20Wilson%20Tsao%20and%20Boyue%20Li%20and%20Qingqing%20Cao%20and%20Naomy%20Sabo%20and%20Cheng%20Leong%20and%20Yi%20Wang%20and%20Anupama%20Mann%20Anupama%20and%20Colorado%20Reed%20and%20Kenneth%20Jung%20and%20Zhifeng%20Chen%20and%20Mohana%20Prasad%20Sathya%20Moorthy%20and%20Yifei%20He%20and%20Erik%20Hornberger%20and%20Devi%20Krishna%20and%20Senyu%20Tong%20and%20%20Michael%20and%20%20Lee%20and%20David%20Haldimann%20and%20Yang%20Zhao%20and%20Bowen%20Zhang%20and%20Chang%20Gao%20and%20Chris%20Bartels%20and%20Sushma%20Rao%20and%20Nathalie%20Tran%20and%20Simon%20Lehnerer%20and%20Co%20Giang%20and%20Patrick%20Dong%20and%20Junting%20Pan%20and%20Biyao%20Wang%20and%20Dongxu%20Li%20and%20Mehrdad%20Farajtabar%20and%20Dongseong%20Hwang%20and%20Grace%20Duanmu%20and%20Eshan%20Verma%20and%20Sujeeth%20Reddy%20and%20Qi%20Shan%20and%20Hongbin%20Gao%20and%20Nan%20Du%20and%20Pragnya%20Sridhar%20and%20Forrest%20Huang%20and%20Yingbo%20Wang%20and%20Nikhil%20Bhendawade%20and%20Diane%20Zhu%20and%20Sai%20Aitharaju%20and%20Fred%20Hohman%20and%20Lauren%20Gardiner%20and%20Chung-Cheng%20Chiu%20and%20Yinfei%20Yang%20and%20Alper%20Kokmen%20and%20Frank%20Chu%20and%20Ke%20Ye%20and%20Kaan%20Elgin%20and%20Oron%20Levy%20and%20John%20Park%20and%20Donald%20Zhang%20and%20Eldon%20Schoop%20and%20Nina%20Wenzel%20and%20Michael%20Booker%20and%20Hyunjik%20Kim%20and%20Chinguun%20Erdenebileg%20and%20Nan%20Dun%20and%20Eric%20Liang%20Yang%20and%20Priyal%20Chhatrapati%20and%20Vishaal%20Mahtani%20and%20Haiming%20Gang%20and%20Kohen%20Chia%20and%20Deepa%20Seshadri%20and%20Donghan%20Yu%20and%20Yan%20Meng%20and%20Kelsey%20Peterson%20and%20Zhen%20Yang%20and%20Yongqiang%20Wang%20and%20Carina%20Peng%20and%20Doug%20Kang%20and%20Anuva%20Agarwal%20and%20Albert%20Antony%20and%20Juan%20Lao%20Tebar%20and%20Albin%20Madappally%20Jose%20and%20Regan%20Poston%20and%20Andy%20De%20Wang%20and%20Gerard%20Casamayor%20and%20Elmira%20Amirloo%20and%20Violet%20Yao%20and%20Wojciech%20Kryscinski%20and%20Kun%20Duan%20and%20Lezhi%20L%0AAbstract%3A%20%20%20We%20introduce%20two%20multilingual%2C%20multimodal%20foundation%20language%20models%20that%0Apower%20Apple%20Intelligence%20features%20across%20Apple%20devices%20and%20services%3A%20i%20a%0A3B-parameter%20on-device%20model%20optimized%20for%20Apple%20silicon%20through%20architectural%0Ainnovations%20such%20as%20KV-cache%20sharing%20and%202-bit%20quantization-aware%20training%3B%20and%0Aii%20a%20scalable%20server%20model%20built%20on%20a%20novel%20Parallel-Track%20Mixture-of-Experts%0APT-MoE%20transformer%20that%20combines%20track%20parallelism%2C%20mixture-of-experts%20sparse%0Acomputation%2C%20and%20interleaved%20global-local%20attention%20to%20deliver%20high%20quality%0Awith%20competitive%20cost%20on%20Apple%27s%20Private%20Cloud%20Compute%20platform.%20Both%20models%0Aare%20trained%20on%20large-scale%20multilingual%20and%20multimodal%20datasets%20sourced%20via%0Aresponsible%20web%20crawling%2C%20licensed%20corpora%2C%20and%20high-quality%20synthetic%20data%2C%0Athen%20further%20refined%20with%20supervised%20fine-tuning%20and%20reinforcement%20learning%20on%0Aa%20new%20asynchronous%20platform.%20The%20resulting%20models%20support%20several%20additional%0Alanguages%20while%20understanding%20images%20and%20executing%20tool%20calls.%20In%20public%0Abenchmarks%20and%20human%20evaluations%2C%20both%20the%20server%20model%20and%20the%20on-device%20model%0Amatch%20or%20surpass%20comparably%20sized%20open%20baselines.%0A%20%20A%20new%20Swift-centric%20Foundation%20Models%20framework%20exposes%20guided%20generation%2C%0Aconstrained%20tool%20calling%2C%20and%20LoRA%20adapter%20fine-tuning%2C%20allowing%20developers%20to%0Aintegrate%20these%20capabilities%20with%20a%20few%20lines%20of%20code.%20The%20latest%20advancements%0Ain%20Apple%20Intelligence%20models%20are%20grounded%20in%20our%20Responsible%20AI%20approach%20with%0Asafeguards%20like%20content%20filtering%20and%20locale-specific%20evaluation%2C%20as%20well%20as%0Aour%20commitment%20to%20protecting%20our%20users%27%20privacy%20with%20innovations%20like%20Private%0ACloud%20Compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13575v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApple%2520Intelligence%2520Foundation%2520Language%2520Models%253A%2520Tech%2520Report%25202025%26entry.906535625%3DEthan%2520Li%2520and%2520Anders%2520Boesen%2520Lindbo%2520Larsen%2520and%2520Chen%2520Zhang%2520and%2520Xiyou%2520Zhou%2520and%2520Jun%2520Qin%2520and%2520Dian%2520Ang%2520Yap%2520and%2520Narendran%2520Raghavan%2520and%2520Xuankai%2520Chang%2520and%2520Margit%2520Bowler%2520and%2520Eray%2520Yildiz%2520and%2520John%2520Peebles%2520and%2520Hannah%2520Gillis%2520Coleman%2520and%2520Matteo%2520Ronchi%2520and%2520Peter%2520Gray%2520and%2520Keen%2520You%2520and%2520Anthony%2520Spalvieri-Kruse%2520and%2520Ruoming%2520Pang%2520and%2520Reed%2520Li%2520and%2520Yuli%2520Yang%2520and%2520Emad%2520Soroush%2520and%2520Zhiyun%2520Lu%2520and%2520Crystal%2520Xiao%2520and%2520Rong%2520Situ%2520and%2520Jordan%2520Huffaker%2520and%2520David%2520Griffiths%2520and%2520Zaid%2520Ahmed%2520and%2520Peng%2520Zhang%2520and%2520Daniel%2520Parilla%2520and%2520Asaf%2520Liberman%2520and%2520Jennifer%2520Mallalieu%2520and%2520Parsa%2520Mazaheri%2520and%2520Qibin%2520Chen%2520and%2520Manjot%2520Bilkhu%2520and%2520Aonan%2520Zhang%2520and%2520Eric%2520Wang%2520and%2520Dave%2520Nelson%2520and%2520Michael%2520FitzMaurice%2520and%2520Thomas%2520Voice%2520and%2520Jeremy%2520Liu%2520and%2520Josh%2520Shaffer%2520and%2520Shiwen%2520Zhao%2520and%2520Prasanth%2520Yadla%2520and%2520Farzin%2520Rasteh%2520and%2520Pengsheng%2520Guo%2520and%2520Arsalan%2520Farooq%2520and%2520Jeremy%2520Snow%2520and%2520Stephen%2520Murphy%2520and%2520Tao%2520Lei%2520and%2520Minsik%2520Cho%2520and%2520George%2520Horrell%2520and%2520Sam%2520Dodge%2520and%2520Lindsay%2520Hislop%2520and%2520Sumeet%2520Singh%2520and%2520Alex%2520Dombrowski%2520and%2520Aiswarya%2520Raghavan%2520and%2520Sasha%2520Sirovica%2520and%2520Mandana%2520Saebi%2520and%2520Faye%2520Lao%2520and%2520Max%2520Lam%2520and%2520TJ%2520Lu%2520and%2520Zhaoyang%2520Xu%2520and%2520Karanjeet%2520Singh%2520and%2520Marc%2520Kirchner%2520and%2520David%2520Mizrahi%2520and%2520Rajat%2520Arora%2520and%2520Haotian%2520Zhang%2520and%2520Henry%2520Mason%2520and%2520Lawrence%2520Zhou%2520and%2520Yi%2520Hua%2520and%2520Ankur%2520Jain%2520and%2520Felix%2520Bai%2520and%2520Joseph%2520Astrauskas%2520and%2520Floris%2520Weers%2520and%2520Josh%2520Gardner%2520and%2520Mira%2520Chiang%2520and%2520Yi%2520Zhang%2520and%2520Pulkit%2520Agrawal%2520and%2520Tony%2520Sun%2520and%2520Quentin%2520Keunebroek%2520and%2520Matthew%2520Hopkins%2520and%2520Bugu%2520Wu%2520and%2520Tao%2520Jia%2520and%2520Chen%2520Chen%2520and%2520Xingyu%2520Zhou%2520and%2520Nanzhu%2520Wang%2520and%2520Peng%2520Liu%2520and%2520Ruixuan%2520Hou%2520and%2520Rene%2520Rauch%2520and%2520Yuan%2520Gao%2520and%2520Afshin%2520Dehghan%2520and%2520Jonathan%2520Janke%2520and%2520Zirui%2520Wang%2520and%2520Cha%2520Chen%2520and%2520Xiaoyi%2520Ren%2520and%2520Feng%2520Nan%2520and%2520Josh%2520Elman%2520and%2520Dong%2520Yin%2520and%2520Yusuf%2520Goren%2520and%2520Jeff%2520Lai%2520and%2520Yiran%2520Fei%2520and%2520Syd%2520Evans%2520and%2520Muyang%2520Yu%2520and%2520Guoli%2520Yin%2520and%2520Yi%2520Qin%2520and%2520Erin%2520Feldman%2520and%2520Isha%2520Garg%2520and%2520Aparna%2520Rajamani%2520and%2520Karla%2520Vega%2520and%2520Walker%2520Cheng%2520and%2520TJ%2520Collins%2520and%2520Hans%2520Han%2520and%2520Raul%2520Rea%2520Menacho%2520and%2520Simon%2520Yeung%2520and%2520Sophy%2520Lee%2520and%2520Phani%2520Mutyala%2520and%2520Ying-Chang%2520Cheng%2520and%2520Zhe%2520Gan%2520and%2520Sprite%2520Chu%2520and%2520Justin%2520Lazarow%2520and%2520Alessandro%2520Pappalardo%2520and%2520Federico%2520Scozzafava%2520and%2520Jing%2520Lu%2520and%2520Erik%2520Daxberger%2520and%2520Laurent%2520Duchesne%2520and%2520Jen%2520Liu%2520and%2520David%2520G%25C3%25BCera%2520and%2520Stefano%2520Ligas%2520and%2520Mary%2520Beth%2520Kery%2520and%2520Brent%2520Ramerth%2520and%2520Ciro%2520Sannino%2520and%2520Marcin%2520Eichner%2520and%2520Haoshuo%2520Huang%2520and%2520Rui%2520Qian%2520and%2520Moritz%2520Schwarzer-Becker%2520and%2520David%2520Riazati%2520and%2520Mingfei%2520Gao%2520and%2520Bailin%2520Wang%2520and%2520Jack%2520Cackler%2520and%2520Yang%2520Lu%2520and%2520Ransen%2520Niu%2520and%2520John%2520Dennison%2520and%2520Guillaume%2520Klein%2520and%2520Jeffrey%2520Bigham%2520and%2520Deepak%2520Gopinath%2520and%2520Navid%2520Shiee%2520and%2520Darren%2520Botten%2520and%2520Guillaume%2520Tartavel%2520and%2520Alex%2520Guillen%2520Garcia%2520and%2520Sam%2520Xu%2520and%2520Victoria%2520M%25C3%25B6nchJuan%2520Haladjian%2520and%2520Zi-Yi%2520Dou%2520and%2520Matthias%2520Paulik%2520and%2520Adolfo%2520Lopez%2520Mendez%2520and%2520Zhen%2520Li%2520and%2520Hong-You%2520Chen%2520and%2520Chao%2520Jia%2520and%2520Dhaval%2520Doshi%2520and%2520Zhengdong%2520Zhang%2520and%2520Raunak%2520Manjani%2520and%2520Aaron%2520Franklin%2520and%2520Zhile%2520Ren%2520and%2520David%2520Chen%2520and%2520Artsiom%2520Peshko%2520and%2520Nandhitha%2520Raghuram%2520and%2520Hans%2520Hao%2520and%2520Jiulong%2520Shan%2520and%2520Kavya%2520Nerella%2520and%2520Ramsey%2520Tantawi%2520and%2520Vivek%2520Kumar%2520and%2520Saiwen%2520Wang%2520and%2520Brycen%2520Wershing%2520and%2520Bhuwan%2520Dhingra%2520and%2520Dhruti%2520Shah%2520and%2520Ob%2520Adaranijo%2520and%2520Xin%2520Zheng%2520and%2520Tait%2520Madsen%2520and%2520Hadas%2520Kotek%2520and%2520Chang%2520Liu%2520and%2520Yin%2520Xia%2520and%2520Hanli%2520Li%2520and%2520Suma%2520Jayaram%2520and%2520Yanchao%2520Sun%2520and%2520Ahmed%2520Fakhry%2520and%2520Vasileios%2520Saveris%2520and%2520Dustin%2520Withers%2520and%2520Yanghao%2520Li%2520and%2520Alp%2520Aygar%2520and%2520Andres%2520Romero%2520Mier%2520Y%2520Teran%2520and%2520Kaiwei%2520Huang%2520and%2520Mark%2520Lee%2520and%2520Xiujun%2520Li%2520and%2520Yuhong%2520Li%2520and%2520Tyler%2520Johnson%2520and%2520Jay%2520Tang%2520and%2520Joseph%2520Yitan%2520Cheng%2520and%2520Futang%2520Peng%2520and%2520Andrew%2520Walkingshaw%2520and%2520Lucas%2520Guibert%2520and%2520Abhishek%2520Sharma%2520and%2520Cheng%2520Shen%2520and%2520Piotr%2520Maj%2520and%2520Yasutaka%2520Tanaka%2520and%2520You-Cyuan%2520Jhang%2520and%2520Vivian%2520Ma%2520and%2520Tommi%2520Vehvilainen%2520and%2520Kelvin%2520Zou%2520and%2520Jeff%2520Nichols%2520and%2520Matthew%2520Lei%2520and%2520David%2520Qiu%2520and%2520Yihao%2520Qian%2520and%2520Gokul%2520Santhanam%2520and%2520Wentao%2520Wu%2520and%2520Yena%2520Han%2520and%2520Dominik%2520Moritz%2520and%2520Haijing%2520Fu%2520and%2520Mingze%2520Xu%2520and%2520Vivek%2520Rathod%2520and%2520Jian%2520Liu%2520and%2520Louis%2520D%2527hauwe%2520and%2520Qin%2520Ba%2520and%2520Haitian%2520Sun%2520and%2520Haoran%2520Yan%2520and%2520Philipp%2520Dufter%2520and%2520Anh%2520Nguyen%2520and%2520Yihao%2520Feng%2520and%2520Emma%2520Wang%2520and%2520Keyu%2520He%2520and%2520Rahul%2520Nair%2520and%2520Sanskruti%2520Shah%2520and%2520Jiarui%2520Lu%2520and%2520Patrick%2520Sonnenberg%2520and%2520Jeremy%2520Warner%2520and%2520Yuanzhi%2520Li%2520and%2520Bowen%2520Pan%2520and%2520Ziyi%2520Zhong%2520and%2520Joe%2520Zhou%2520and%2520Sam%2520Davarnia%2520and%2520Olli%2520Saarikivi%2520and%2520Irina%2520Belousova%2520and%2520Rachel%2520Burger%2520and%2520Shang-Chen%2520Wu%2520and%2520Di%2520Feng%2520and%2520Bas%2520Straathof%2520and%2520James%2520Chou%2520and%2520Yuanyang%2520Zhang%2520and%2520Marco%2520Zuliani%2520and%2520Eduardo%2520Jimenez%2520and%2520Abhishek%2520Sundararajan%2520and%2520Xianzhi%2520Du%2520and%2520Chang%2520Lan%2520and%2520Nilesh%2520Shahdadpuri%2520and%2520Peter%2520Grasch%2520and%2520Sergiu%2520Sima%2520and%2520Josh%2520Newnham%2520and%2520Varsha%2520Paidi%2520and%2520Jianyu%2520Wang%2520and%2520Kaelen%2520Haag%2520and%2520Alex%2520Braunstein%2520and%2520Daniele%2520Molinari%2520and%2520Richard%2520Wei%2520and%2520Brenda%2520Yang%2520and%2520Nicholas%2520Lusskin%2520and%2520Joanna%2520Arreaza-Taylor%2520and%2520Meng%2520Cao%2520and%2520Nicholas%2520Seidl%2520and%2520Simon%2520Wang%2520and%2520Jiaming%2520Hu%2520and%2520Yiping%2520Ma%2520and%2520Mengyu%2520Li%2520and%2520Kieran%2520Liu%2520and%2520Hang%2520Su%2520and%2520Sachin%2520Ravi%2520and%2520Chong%2520Wang%2520and%2520Xin%2520Wang%2520and%2520Kevin%2520Smith%2520and%2520Haoxuan%2520You%2520and%2520Binazir%2520Karimzadeh%2520and%2520Rui%2520Li%2520and%2520Jinhao%2520Lei%2520and%2520Wei%2520Fang%2520and%2520Alec%2520Doane%2520and%2520Sam%2520Wiseman%2520and%2520Ismael%2520Fernandez%2520and%2520Jane%2520Li%2520and%2520Andrew%2520Hansen%2520and%2520Javier%2520Movellan%2520and%2520Christopher%2520Neubauer%2520and%2520Hanzhi%2520Zhou%2520and%2520Chris%2520Chaney%2520and%2520Nazir%2520Kamaldin%2520and%2520Valentin%2520Wolf%2520and%2520Fernando%2520Berm%25C3%25BAdez-Medina%2520and%2520Joris%2520Pelemans%2520and%2520Peter%2520Fu%2520and%2520Howard%2520Xing%2520and%2520Xiang%2520Kong%2520and%2520Wayne%2520Shan%2520and%2520Gabriel%2520Jacoby-Cooper%2520and%2520Dongcai%2520Shen%2520and%2520Tom%2520Gunter%2520and%2520Guillaume%2520Seguin%2520and%2520Fangping%2520Shi%2520and%2520Shiyu%2520Li%2520and%2520Yang%2520Xu%2520and%2520Areeba%2520Kamal%2520and%2520Dan%2520Masi%2520and%2520Saptarshi%2520Guha%2520and%2520Qi%2520Zhu%2520and%2520Jenna%2520Thibodeau%2520and%2520Changyuan%2520Zhang%2520and%2520Rebecca%2520Callahan%2520and%2520Charles%2520Maalouf%2520and%2520Wilson%2520Tsao%2520and%2520Boyue%2520Li%2520and%2520Qingqing%2520Cao%2520and%2520Naomy%2520Sabo%2520and%2520Cheng%2520Leong%2520and%2520Yi%2520Wang%2520and%2520Anupama%2520Mann%2520Anupama%2520and%2520Colorado%2520Reed%2520and%2520Kenneth%2520Jung%2520and%2520Zhifeng%2520Chen%2520and%2520Mohana%2520Prasad%2520Sathya%2520Moorthy%2520and%2520Yifei%2520He%2520and%2520Erik%2520Hornberger%2520and%2520Devi%2520Krishna%2520and%2520Senyu%2520Tong%2520and%2520%2520Michael%2520and%2520%2520Lee%2520and%2520David%2520Haldimann%2520and%2520Yang%2520Zhao%2520and%2520Bowen%2520Zhang%2520and%2520Chang%2520Gao%2520and%2520Chris%2520Bartels%2520and%2520Sushma%2520Rao%2520and%2520Nathalie%2520Tran%2520and%2520Simon%2520Lehnerer%2520and%2520Co%2520Giang%2520and%2520Patrick%2520Dong%2520and%2520Junting%2520Pan%2520and%2520Biyao%2520Wang%2520and%2520Dongxu%2520Li%2520and%2520Mehrdad%2520Farajtabar%2520and%2520Dongseong%2520Hwang%2520and%2520Grace%2520Duanmu%2520and%2520Eshan%2520Verma%2520and%2520Sujeeth%2520Reddy%2520and%2520Qi%2520Shan%2520and%2520Hongbin%2520Gao%2520and%2520Nan%2520Du%2520and%2520Pragnya%2520Sridhar%2520and%2520Forrest%2520Huang%2520and%2520Yingbo%2520Wang%2520and%2520Nikhil%2520Bhendawade%2520and%2520Diane%2520Zhu%2520and%2520Sai%2520Aitharaju%2520and%2520Fred%2520Hohman%2520and%2520Lauren%2520Gardiner%2520and%2520Chung-Cheng%2520Chiu%2520and%2520Yinfei%2520Yang%2520and%2520Alper%2520Kokmen%2520and%2520Frank%2520Chu%2520and%2520Ke%2520Ye%2520and%2520Kaan%2520Elgin%2520and%2520Oron%2520Levy%2520and%2520John%2520Park%2520and%2520Donald%2520Zhang%2520and%2520Eldon%2520Schoop%2520and%2520Nina%2520Wenzel%2520and%2520Michael%2520Booker%2520and%2520Hyunjik%2520Kim%2520and%2520Chinguun%2520Erdenebileg%2520and%2520Nan%2520Dun%2520and%2520Eric%2520Liang%2520Yang%2520and%2520Priyal%2520Chhatrapati%2520and%2520Vishaal%2520Mahtani%2520and%2520Haiming%2520Gang%2520and%2520Kohen%2520Chia%2520and%2520Deepa%2520Seshadri%2520and%2520Donghan%2520Yu%2520and%2520Yan%2520Meng%2520and%2520Kelsey%2520Peterson%2520and%2520Zhen%2520Yang%2520and%2520Yongqiang%2520Wang%2520and%2520Carina%2520Peng%2520and%2520Doug%2520Kang%2520and%2520Anuva%2520Agarwal%2520and%2520Albert%2520Antony%2520and%2520Juan%2520Lao%2520Tebar%2520and%2520Albin%2520Madappally%2520Jose%2520and%2520Regan%2520Poston%2520and%2520Andy%2520De%2520Wang%2520and%2520Gerard%2520Casamayor%2520and%2520Elmira%2520Amirloo%2520and%2520Violet%2520Yao%2520and%2520Wojciech%2520Kryscinski%2520and%2520Kun%2520Duan%2520and%2520Lezhi%2520L%26entry.1292438233%3D%2520%2520We%2520introduce%2520two%2520multilingual%252C%2520multimodal%2520foundation%2520language%2520models%2520that%250Apower%2520Apple%2520Intelligence%2520features%2520across%2520Apple%2520devices%2520and%2520services%253A%2520i%2520a%250A3B-parameter%2520on-device%2520model%2520optimized%2520for%2520Apple%2520silicon%2520through%2520architectural%250Ainnovations%2520such%2520as%2520KV-cache%2520sharing%2520and%25202-bit%2520quantization-aware%2520training%253B%2520and%250Aii%2520a%2520scalable%2520server%2520model%2520built%2520on%2520a%2520novel%2520Parallel-Track%2520Mixture-of-Experts%250APT-MoE%2520transformer%2520that%2520combines%2520track%2520parallelism%252C%2520mixture-of-experts%2520sparse%250Acomputation%252C%2520and%2520interleaved%2520global-local%2520attention%2520to%2520deliver%2520high%2520quality%250Awith%2520competitive%2520cost%2520on%2520Apple%2527s%2520Private%2520Cloud%2520Compute%2520platform.%2520Both%2520models%250Aare%2520trained%2520on%2520large-scale%2520multilingual%2520and%2520multimodal%2520datasets%2520sourced%2520via%250Aresponsible%2520web%2520crawling%252C%2520licensed%2520corpora%252C%2520and%2520high-quality%2520synthetic%2520data%252C%250Athen%2520further%2520refined%2520with%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520on%250Aa%2520new%2520asynchronous%2520platform.%2520The%2520resulting%2520models%2520support%2520several%2520additional%250Alanguages%2520while%2520understanding%2520images%2520and%2520executing%2520tool%2520calls.%2520In%2520public%250Abenchmarks%2520and%2520human%2520evaluations%252C%2520both%2520the%2520server%2520model%2520and%2520the%2520on-device%2520model%250Amatch%2520or%2520surpass%2520comparably%2520sized%2520open%2520baselines.%250A%2520%2520A%2520new%2520Swift-centric%2520Foundation%2520Models%2520framework%2520exposes%2520guided%2520generation%252C%250Aconstrained%2520tool%2520calling%252C%2520and%2520LoRA%2520adapter%2520fine-tuning%252C%2520allowing%2520developers%2520to%250Aintegrate%2520these%2520capabilities%2520with%2520a%2520few%2520lines%2520of%2520code.%2520The%2520latest%2520advancements%250Ain%2520Apple%2520Intelligence%2520models%2520are%2520grounded%2520in%2520our%2520Responsible%2520AI%2520approach%2520with%250Asafeguards%2520like%2520content%2520filtering%2520and%2520locale-specific%2520evaluation%252C%2520as%2520well%2520as%250Aour%2520commitment%2520to%2520protecting%2520our%2520users%2527%2520privacy%2520with%2520innovations%2520like%2520Private%250ACloud%2520Compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13575v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Apple%20Intelligence%20Foundation%20Language%20Models%3A%20Tech%20Report%202025&entry.906535625=Ethan%20Li%20and%20Anders%20Boesen%20Lindbo%20Larsen%20and%20Chen%20Zhang%20and%20Xiyou%20Zhou%20and%20Jun%20Qin%20and%20Dian%20Ang%20Yap%20and%20Narendran%20Raghavan%20and%20Xuankai%20Chang%20and%20Margit%20Bowler%20and%20Eray%20Yildiz%20and%20John%20Peebles%20and%20Hannah%20Gillis%20Coleman%20and%20Matteo%20Ronchi%20and%20Peter%20Gray%20and%20Keen%20You%20and%20Anthony%20Spalvieri-Kruse%20and%20Ruoming%20Pang%20and%20Reed%20Li%20and%20Yuli%20Yang%20and%20Emad%20Soroush%20and%20Zhiyun%20Lu%20and%20Crystal%20Xiao%20and%20Rong%20Situ%20and%20Jordan%20Huffaker%20and%20David%20Griffiths%20and%20Zaid%20Ahmed%20and%20Peng%20Zhang%20and%20Daniel%20Parilla%20and%20Asaf%20Liberman%20and%20Jennifer%20Mallalieu%20and%20Parsa%20Mazaheri%20and%20Qibin%20Chen%20and%20Manjot%20Bilkhu%20and%20Aonan%20Zhang%20and%20Eric%20Wang%20and%20Dave%20Nelson%20and%20Michael%20FitzMaurice%20and%20Thomas%20Voice%20and%20Jeremy%20Liu%20and%20Josh%20Shaffer%20and%20Shiwen%20Zhao%20and%20Prasanth%20Yadla%20and%20Farzin%20Rasteh%20and%20Pengsheng%20Guo%20and%20Arsalan%20Farooq%20and%20Jeremy%20Snow%20and%20Stephen%20Murphy%20and%20Tao%20Lei%20and%20Minsik%20Cho%20and%20George%20Horrell%20and%20Sam%20Dodge%20and%20Lindsay%20Hislop%20and%20Sumeet%20Singh%20and%20Alex%20Dombrowski%20and%20Aiswarya%20Raghavan%20and%20Sasha%20Sirovica%20and%20Mandana%20Saebi%20and%20Faye%20Lao%20and%20Max%20Lam%20and%20TJ%20Lu%20and%20Zhaoyang%20Xu%20and%20Karanjeet%20Singh%20and%20Marc%20Kirchner%20and%20David%20Mizrahi%20and%20Rajat%20Arora%20and%20Haotian%20Zhang%20and%20Henry%20Mason%20and%20Lawrence%20Zhou%20and%20Yi%20Hua%20and%20Ankur%20Jain%20and%20Felix%20Bai%20and%20Joseph%20Astrauskas%20and%20Floris%20Weers%20and%20Josh%20Gardner%20and%20Mira%20Chiang%20and%20Yi%20Zhang%20and%20Pulkit%20Agrawal%20and%20Tony%20Sun%20and%20Quentin%20Keunebroek%20and%20Matthew%20Hopkins%20and%20Bugu%20Wu%20and%20Tao%20Jia%20and%20Chen%20Chen%20and%20Xingyu%20Zhou%20and%20Nanzhu%20Wang%20and%20Peng%20Liu%20and%20Ruixuan%20Hou%20and%20Rene%20Rauch%20and%20Yuan%20Gao%20and%20Afshin%20Dehghan%20and%20Jonathan%20Janke%20and%20Zirui%20Wang%20and%20Cha%20Chen%20and%20Xiaoyi%20Ren%20and%20Feng%20Nan%20and%20Josh%20Elman%20and%20Dong%20Yin%20and%20Yusuf%20Goren%20and%20Jeff%20Lai%20and%20Yiran%20Fei%20and%20Syd%20Evans%20and%20Muyang%20Yu%20and%20Guoli%20Yin%20and%20Yi%20Qin%20and%20Erin%20Feldman%20and%20Isha%20Garg%20and%20Aparna%20Rajamani%20and%20Karla%20Vega%20and%20Walker%20Cheng%20and%20TJ%20Collins%20and%20Hans%20Han%20and%20Raul%20Rea%20Menacho%20and%20Simon%20Yeung%20and%20Sophy%20Lee%20and%20Phani%20Mutyala%20and%20Ying-Chang%20Cheng%20and%20Zhe%20Gan%20and%20Sprite%20Chu%20and%20Justin%20Lazarow%20and%20Alessandro%20Pappalardo%20and%20Federico%20Scozzafava%20and%20Jing%20Lu%20and%20Erik%20Daxberger%20and%20Laurent%20Duchesne%20and%20Jen%20Liu%20and%20David%20G%C3%BCera%20and%20Stefano%20Ligas%20and%20Mary%20Beth%20Kery%20and%20Brent%20Ramerth%20and%20Ciro%20Sannino%20and%20Marcin%20Eichner%20and%20Haoshuo%20Huang%20and%20Rui%20Qian%20and%20Moritz%20Schwarzer-Becker%20and%20David%20Riazati%20and%20Mingfei%20Gao%20and%20Bailin%20Wang%20and%20Jack%20Cackler%20and%20Yang%20Lu%20and%20Ransen%20Niu%20and%20John%20Dennison%20and%20Guillaume%20Klein%20and%20Jeffrey%20Bigham%20and%20Deepak%20Gopinath%20and%20Navid%20Shiee%20and%20Darren%20Botten%20and%20Guillaume%20Tartavel%20and%20Alex%20Guillen%20Garcia%20and%20Sam%20Xu%20and%20Victoria%20M%C3%B6nchJuan%20Haladjian%20and%20Zi-Yi%20Dou%20and%20Matthias%20Paulik%20and%20Adolfo%20Lopez%20Mendez%20and%20Zhen%20Li%20and%20Hong-You%20Chen%20and%20Chao%20Jia%20and%20Dhaval%20Doshi%20and%20Zhengdong%20Zhang%20and%20Raunak%20Manjani%20and%20Aaron%20Franklin%20and%20Zhile%20Ren%20and%20David%20Chen%20and%20Artsiom%20Peshko%20and%20Nandhitha%20Raghuram%20and%20Hans%20Hao%20and%20Jiulong%20Shan%20and%20Kavya%20Nerella%20and%20Ramsey%20Tantawi%20and%20Vivek%20Kumar%20and%20Saiwen%20Wang%20and%20Brycen%20Wershing%20and%20Bhuwan%20Dhingra%20and%20Dhruti%20Shah%20and%20Ob%20Adaranijo%20and%20Xin%20Zheng%20and%20Tait%20Madsen%20and%20Hadas%20Kotek%20and%20Chang%20Liu%20and%20Yin%20Xia%20and%20Hanli%20Li%20and%20Suma%20Jayaram%20and%20Yanchao%20Sun%20and%20Ahmed%20Fakhry%20and%20Vasileios%20Saveris%20and%20Dustin%20Withers%20and%20Yanghao%20Li%20and%20Alp%20Aygar%20and%20Andres%20Romero%20Mier%20Y%20Teran%20and%20Kaiwei%20Huang%20and%20Mark%20Lee%20and%20Xiujun%20Li%20and%20Yuhong%20Li%20and%20Tyler%20Johnson%20and%20Jay%20Tang%20and%20Joseph%20Yitan%20Cheng%20and%20Futang%20Peng%20and%20Andrew%20Walkingshaw%20and%20Lucas%20Guibert%20and%20Abhishek%20Sharma%20and%20Cheng%20Shen%20and%20Piotr%20Maj%20and%20Yasutaka%20Tanaka%20and%20You-Cyuan%20Jhang%20and%20Vivian%20Ma%20and%20Tommi%20Vehvilainen%20and%20Kelvin%20Zou%20and%20Jeff%20Nichols%20and%20Matthew%20Lei%20and%20David%20Qiu%20and%20Yihao%20Qian%20and%20Gokul%20Santhanam%20and%20Wentao%20Wu%20and%20Yena%20Han%20and%20Dominik%20Moritz%20and%20Haijing%20Fu%20and%20Mingze%20Xu%20and%20Vivek%20Rathod%20and%20Jian%20Liu%20and%20Louis%20D%27hauwe%20and%20Qin%20Ba%20and%20Haitian%20Sun%20and%20Haoran%20Yan%20and%20Philipp%20Dufter%20and%20Anh%20Nguyen%20and%20Yihao%20Feng%20and%20Emma%20Wang%20and%20Keyu%20He%20and%20Rahul%20Nair%20and%20Sanskruti%20Shah%20and%20Jiarui%20Lu%20and%20Patrick%20Sonnenberg%20and%20Jeremy%20Warner%20and%20Yuanzhi%20Li%20and%20Bowen%20Pan%20and%20Ziyi%20Zhong%20and%20Joe%20Zhou%20and%20Sam%20Davarnia%20and%20Olli%20Saarikivi%20and%20Irina%20Belousova%20and%20Rachel%20Burger%20and%20Shang-Chen%20Wu%20and%20Di%20Feng%20and%20Bas%20Straathof%20and%20James%20Chou%20and%20Yuanyang%20Zhang%20and%20Marco%20Zuliani%20and%20Eduardo%20Jimenez%20and%20Abhishek%20Sundararajan%20and%20Xianzhi%20Du%20and%20Chang%20Lan%20and%20Nilesh%20Shahdadpuri%20and%20Peter%20Grasch%20and%20Sergiu%20Sima%20and%20Josh%20Newnham%20and%20Varsha%20Paidi%20and%20Jianyu%20Wang%20and%20Kaelen%20Haag%20and%20Alex%20Braunstein%20and%20Daniele%20Molinari%20and%20Richard%20Wei%20and%20Brenda%20Yang%20and%20Nicholas%20Lusskin%20and%20Joanna%20Arreaza-Taylor%20and%20Meng%20Cao%20and%20Nicholas%20Seidl%20and%20Simon%20Wang%20and%20Jiaming%20Hu%20and%20Yiping%20Ma%20and%20Mengyu%20Li%20and%20Kieran%20Liu%20and%20Hang%20Su%20and%20Sachin%20Ravi%20and%20Chong%20Wang%20and%20Xin%20Wang%20and%20Kevin%20Smith%20and%20Haoxuan%20You%20and%20Binazir%20Karimzadeh%20and%20Rui%20Li%20and%20Jinhao%20Lei%20and%20Wei%20Fang%20and%20Alec%20Doane%20and%20Sam%20Wiseman%20and%20Ismael%20Fernandez%20and%20Jane%20Li%20and%20Andrew%20Hansen%20and%20Javier%20Movellan%20and%20Christopher%20Neubauer%20and%20Hanzhi%20Zhou%20and%20Chris%20Chaney%20and%20Nazir%20Kamaldin%20and%20Valentin%20Wolf%20and%20Fernando%20Berm%C3%BAdez-Medina%20and%20Joris%20Pelemans%20and%20Peter%20Fu%20and%20Howard%20Xing%20and%20Xiang%20Kong%20and%20Wayne%20Shan%20and%20Gabriel%20Jacoby-Cooper%20and%20Dongcai%20Shen%20and%20Tom%20Gunter%20and%20Guillaume%20Seguin%20and%20Fangping%20Shi%20and%20Shiyu%20Li%20and%20Yang%20Xu%20and%20Areeba%20Kamal%20and%20Dan%20Masi%20and%20Saptarshi%20Guha%20and%20Qi%20Zhu%20and%20Jenna%20Thibodeau%20and%20Changyuan%20Zhang%20and%20Rebecca%20Callahan%20and%20Charles%20Maalouf%20and%20Wilson%20Tsao%20and%20Boyue%20Li%20and%20Qingqing%20Cao%20and%20Naomy%20Sabo%20and%20Cheng%20Leong%20and%20Yi%20Wang%20and%20Anupama%20Mann%20Anupama%20and%20Colorado%20Reed%20and%20Kenneth%20Jung%20and%20Zhifeng%20Chen%20and%20Mohana%20Prasad%20Sathya%20Moorthy%20and%20Yifei%20He%20and%20Erik%20Hornberger%20and%20Devi%20Krishna%20and%20Senyu%20Tong%20and%20%20Michael%20and%20%20Lee%20and%20David%20Haldimann%20and%20Yang%20Zhao%20and%20Bowen%20Zhang%20and%20Chang%20Gao%20and%20Chris%20Bartels%20and%20Sushma%20Rao%20and%20Nathalie%20Tran%20and%20Simon%20Lehnerer%20and%20Co%20Giang%20and%20Patrick%20Dong%20and%20Junting%20Pan%20and%20Biyao%20Wang%20and%20Dongxu%20Li%20and%20Mehrdad%20Farajtabar%20and%20Dongseong%20Hwang%20and%20Grace%20Duanmu%20and%20Eshan%20Verma%20and%20Sujeeth%20Reddy%20and%20Qi%20Shan%20and%20Hongbin%20Gao%20and%20Nan%20Du%20and%20Pragnya%20Sridhar%20and%20Forrest%20Huang%20and%20Yingbo%20Wang%20and%20Nikhil%20Bhendawade%20and%20Diane%20Zhu%20and%20Sai%20Aitharaju%20and%20Fred%20Hohman%20and%20Lauren%20Gardiner%20and%20Chung-Cheng%20Chiu%20and%20Yinfei%20Yang%20and%20Alper%20Kokmen%20and%20Frank%20Chu%20and%20Ke%20Ye%20and%20Kaan%20Elgin%20and%20Oron%20Levy%20and%20John%20Park%20and%20Donald%20Zhang%20and%20Eldon%20Schoop%20and%20Nina%20Wenzel%20and%20Michael%20Booker%20and%20Hyunjik%20Kim%20and%20Chinguun%20Erdenebileg%20and%20Nan%20Dun%20and%20Eric%20Liang%20Yang%20and%20Priyal%20Chhatrapati%20and%20Vishaal%20Mahtani%20and%20Haiming%20Gang%20and%20Kohen%20Chia%20and%20Deepa%20Seshadri%20and%20Donghan%20Yu%20and%20Yan%20Meng%20and%20Kelsey%20Peterson%20and%20Zhen%20Yang%20and%20Yongqiang%20Wang%20and%20Carina%20Peng%20and%20Doug%20Kang%20and%20Anuva%20Agarwal%20and%20Albert%20Antony%20and%20Juan%20Lao%20Tebar%20and%20Albin%20Madappally%20Jose%20and%20Regan%20Poston%20and%20Andy%20De%20Wang%20and%20Gerard%20Casamayor%20and%20Elmira%20Amirloo%20and%20Violet%20Yao%20and%20Wojciech%20Kryscinski%20and%20Kun%20Duan%20and%20Lezhi%20L&entry.1292438233=%20%20We%20introduce%20two%20multilingual%2C%20multimodal%20foundation%20language%20models%20that%0Apower%20Apple%20Intelligence%20features%20across%20Apple%20devices%20and%20services%3A%20i%20a%0A3B-parameter%20on-device%20model%20optimized%20for%20Apple%20silicon%20through%20architectural%0Ainnovations%20such%20as%20KV-cache%20sharing%20and%202-bit%20quantization-aware%20training%3B%20and%0Aii%20a%20scalable%20server%20model%20built%20on%20a%20novel%20Parallel-Track%20Mixture-of-Experts%0APT-MoE%20transformer%20that%20combines%20track%20parallelism%2C%20mixture-of-experts%20sparse%0Acomputation%2C%20and%20interleaved%20global-local%20attention%20to%20deliver%20high%20quality%0Awith%20competitive%20cost%20on%20Apple%27s%20Private%20Cloud%20Compute%20platform.%20Both%20models%0Aare%20trained%20on%20large-scale%20multilingual%20and%20multimodal%20datasets%20sourced%20via%0Aresponsible%20web%20crawling%2C%20licensed%20corpora%2C%20and%20high-quality%20synthetic%20data%2C%0Athen%20further%20refined%20with%20supervised%20fine-tuning%20and%20reinforcement%20learning%20on%0Aa%20new%20asynchronous%20platform.%20The%20resulting%20models%20support%20several%20additional%0Alanguages%20while%20understanding%20images%20and%20executing%20tool%20calls.%20In%20public%0Abenchmarks%20and%20human%20evaluations%2C%20both%20the%20server%20model%20and%20the%20on-device%20model%0Amatch%20or%20surpass%20comparably%20sized%20open%20baselines.%0A%20%20A%20new%20Swift-centric%20Foundation%20Models%20framework%20exposes%20guided%20generation%2C%0Aconstrained%20tool%20calling%2C%20and%20LoRA%20adapter%20fine-tuning%2C%20allowing%20developers%20to%0Aintegrate%20these%20capabilities%20with%20a%20few%20lines%20of%20code.%20The%20latest%20advancements%0Ain%20Apple%20Intelligence%20models%20are%20grounded%20in%20our%20Responsible%20AI%20approach%20with%0Asafeguards%20like%20content%20filtering%20and%20locale-specific%20evaluation%2C%20as%20well%20as%0Aour%20commitment%20to%20protecting%20our%20users%27%20privacy%20with%20innovations%20like%20Private%0ACloud%20Compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13575v3&entry.124074799=Read"},
{"title": "Emotions as Ambiguity-aware Ordinal Representations", "author": "Jingyao Wu and Matthew Barthet and David Melhart and Georgios N. Yannakakis", "abstract": "  Emotions are inherently ambiguous and dynamic phenomena, yet existing\ncontinuous emotion recognition approaches either ignore their ambiguity or\ntreat ambiguity as an independent and static variable over time. Motivated by\nthis gap in the literature, in this paper we introduce ambiguity-aware ordinal\nemotion representations, a novel framework that captures both the ambiguity\npresent in emotion annotation and the inherent temporal dynamics of emotional\ntraces. Specifically, we propose approaches that model emotion ambiguity\nthrough its rate of change. We evaluate our framework on two affective corpora\n-- RECOLA and GameVibe -- testing our proposed approaches on both bounded\n(arousal, valence) and unbounded (engagement) continuous traces. Our results\ndemonstrate that ordinal representations outperform conventional\nambiguity-aware models on unbounded labels, achieving the highest Concordance\nCorrelation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,\nhighlighting their effectiveness in modeling the traces' dynamics. For bounded\ntraces, ordinal representations excel in SDA, revealing their superior ability\nto capture relative changes of annotated emotion traces.\n", "link": "http://arxiv.org/abs/2508.19193v2", "date": "2025-08-27", "relevancy": 1.9667, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5119}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotions%20as%20Ambiguity-aware%20Ordinal%20Representations&body=Title%3A%20Emotions%20as%20Ambiguity-aware%20Ordinal%20Representations%0AAuthor%3A%20Jingyao%20Wu%20and%20Matthew%20Barthet%20and%20David%20Melhart%20and%20Georgios%20N.%20Yannakakis%0AAbstract%3A%20%20%20Emotions%20are%20inherently%20ambiguous%20and%20dynamic%20phenomena%2C%20yet%20existing%0Acontinuous%20emotion%20recognition%20approaches%20either%20ignore%20their%20ambiguity%20or%0Atreat%20ambiguity%20as%20an%20independent%20and%20static%20variable%20over%20time.%20Motivated%20by%0Athis%20gap%20in%20the%20literature%2C%20in%20this%20paper%20we%20introduce%20ambiguity-aware%20ordinal%0Aemotion%20representations%2C%20a%20novel%20framework%20that%20captures%20both%20the%20ambiguity%0Apresent%20in%20emotion%20annotation%20and%20the%20inherent%20temporal%20dynamics%20of%20emotional%0Atraces.%20Specifically%2C%20we%20propose%20approaches%20that%20model%20emotion%20ambiguity%0Athrough%20its%20rate%20of%20change.%20We%20evaluate%20our%20framework%20on%20two%20affective%20corpora%0A--%20RECOLA%20and%20GameVibe%20--%20testing%20our%20proposed%20approaches%20on%20both%20bounded%0A%28arousal%2C%20valence%29%20and%20unbounded%20%28engagement%29%20continuous%20traces.%20Our%20results%0Ademonstrate%20that%20ordinal%20representations%20outperform%20conventional%0Aambiguity-aware%20models%20on%20unbounded%20labels%2C%20achieving%20the%20highest%20Concordance%0ACorrelation%20Coefficient%20%28CCC%29%20and%20Signed%20Differential%20Agreement%20%28SDA%29%20scores%2C%0Ahighlighting%20their%20effectiveness%20in%20modeling%20the%20traces%27%20dynamics.%20For%20bounded%0Atraces%2C%20ordinal%20representations%20excel%20in%20SDA%2C%20revealing%20their%20superior%20ability%0Ato%20capture%20relative%20changes%20of%20annotated%20emotion%20traces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotions%2520as%2520Ambiguity-aware%2520Ordinal%2520Representations%26entry.906535625%3DJingyao%2520Wu%2520and%2520Matthew%2520Barthet%2520and%2520David%2520Melhart%2520and%2520Georgios%2520N.%2520Yannakakis%26entry.1292438233%3D%2520%2520Emotions%2520are%2520inherently%2520ambiguous%2520and%2520dynamic%2520phenomena%252C%2520yet%2520existing%250Acontinuous%2520emotion%2520recognition%2520approaches%2520either%2520ignore%2520their%2520ambiguity%2520or%250Atreat%2520ambiguity%2520as%2520an%2520independent%2520and%2520static%2520variable%2520over%2520time.%2520Motivated%2520by%250Athis%2520gap%2520in%2520the%2520literature%252C%2520in%2520this%2520paper%2520we%2520introduce%2520ambiguity-aware%2520ordinal%250Aemotion%2520representations%252C%2520a%2520novel%2520framework%2520that%2520captures%2520both%2520the%2520ambiguity%250Apresent%2520in%2520emotion%2520annotation%2520and%2520the%2520inherent%2520temporal%2520dynamics%2520of%2520emotional%250Atraces.%2520Specifically%252C%2520we%2520propose%2520approaches%2520that%2520model%2520emotion%2520ambiguity%250Athrough%2520its%2520rate%2520of%2520change.%2520We%2520evaluate%2520our%2520framework%2520on%2520two%2520affective%2520corpora%250A--%2520RECOLA%2520and%2520GameVibe%2520--%2520testing%2520our%2520proposed%2520approaches%2520on%2520both%2520bounded%250A%2528arousal%252C%2520valence%2529%2520and%2520unbounded%2520%2528engagement%2529%2520continuous%2520traces.%2520Our%2520results%250Ademonstrate%2520that%2520ordinal%2520representations%2520outperform%2520conventional%250Aambiguity-aware%2520models%2520on%2520unbounded%2520labels%252C%2520achieving%2520the%2520highest%2520Concordance%250ACorrelation%2520Coefficient%2520%2528CCC%2529%2520and%2520Signed%2520Differential%2520Agreement%2520%2528SDA%2529%2520scores%252C%250Ahighlighting%2520their%2520effectiveness%2520in%2520modeling%2520the%2520traces%2527%2520dynamics.%2520For%2520bounded%250Atraces%252C%2520ordinal%2520representations%2520excel%2520in%2520SDA%252C%2520revealing%2520their%2520superior%2520ability%250Ato%2520capture%2520relative%2520changes%2520of%2520annotated%2520emotion%2520traces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotions%20as%20Ambiguity-aware%20Ordinal%20Representations&entry.906535625=Jingyao%20Wu%20and%20Matthew%20Barthet%20and%20David%20Melhart%20and%20Georgios%20N.%20Yannakakis&entry.1292438233=%20%20Emotions%20are%20inherently%20ambiguous%20and%20dynamic%20phenomena%2C%20yet%20existing%0Acontinuous%20emotion%20recognition%20approaches%20either%20ignore%20their%20ambiguity%20or%0Atreat%20ambiguity%20as%20an%20independent%20and%20static%20variable%20over%20time.%20Motivated%20by%0Athis%20gap%20in%20the%20literature%2C%20in%20this%20paper%20we%20introduce%20ambiguity-aware%20ordinal%0Aemotion%20representations%2C%20a%20novel%20framework%20that%20captures%20both%20the%20ambiguity%0Apresent%20in%20emotion%20annotation%20and%20the%20inherent%20temporal%20dynamics%20of%20emotional%0Atraces.%20Specifically%2C%20we%20propose%20approaches%20that%20model%20emotion%20ambiguity%0Athrough%20its%20rate%20of%20change.%20We%20evaluate%20our%20framework%20on%20two%20affective%20corpora%0A--%20RECOLA%20and%20GameVibe%20--%20testing%20our%20proposed%20approaches%20on%20both%20bounded%0A%28arousal%2C%20valence%29%20and%20unbounded%20%28engagement%29%20continuous%20traces.%20Our%20results%0Ademonstrate%20that%20ordinal%20representations%20outperform%20conventional%0Aambiguity-aware%20models%20on%20unbounded%20labels%2C%20achieving%20the%20highest%20Concordance%0ACorrelation%20Coefficient%20%28CCC%29%20and%20Signed%20Differential%20Agreement%20%28SDA%29%20scores%2C%0Ahighlighting%20their%20effectiveness%20in%20modeling%20the%20traces%27%20dynamics.%20For%20bounded%0Atraces%2C%20ordinal%20representations%20excel%20in%20SDA%2C%20revealing%20their%20superior%20ability%0Ato%20capture%20relative%20changes%20of%20annotated%20emotion%20traces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19193v2&entry.124074799=Read"},
{"title": "StableIntrinsic: Detail-preserving One-step Diffusion Model for\n  Multi-view Material Estimation", "author": "Xiuchao Wu and Pengfei Zhu and Jiangjing Lyu and Xinguo Liu and Jie Guo and Yanwen Guo and Weiwei Xu and Chengfei Lyu", "abstract": "  Recovering material information from images has been extensively studied in\ncomputer graphics and vision. Recent works in material estimation leverage\ndiffusion model showing promising results. However, these diffusion-based\nmethods adopt a multi-step denoising strategy, which is time-consuming for each\nestimation. Such stochastic inference also conflicts with the deterministic\nmaterial estimation task, leading to a high variance estimated results. In this\npaper, we introduce StableIntrinsic, a one-step diffusion model for multi-view\nmaterial estimation that can produce high-quality material parameters with low\nvariance. To address the overly-smoothing problem in one-step diffusion,\nStableIntrinsic applies losses in pixel space, with each loss designed based on\nthe properties of the material. Additionally, StableIntrinsic introduces a\nDetail Injection Network (DIN) to eliminate the detail loss caused by VAE\nencoding, while further enhancing the sharpness of material prediction results.\nThe experimental results indicate that our method surpasses the current\nstate-of-the-art techniques by achieving a $9.9\\%$ improvement in the Peak\nSignal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error\n(MSE) for metallic and roughness by $44.4\\%$ and $60.0\\%$, respectively.\n", "link": "http://arxiv.org/abs/2508.19789v1", "date": "2025-08-27", "relevancy": 1.8223, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6115}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6072}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableIntrinsic%3A%20Detail-preserving%20One-step%20Diffusion%20Model%20for%0A%20%20Multi-view%20Material%20Estimation&body=Title%3A%20StableIntrinsic%3A%20Detail-preserving%20One-step%20Diffusion%20Model%20for%0A%20%20Multi-view%20Material%20Estimation%0AAuthor%3A%20Xiuchao%20Wu%20and%20Pengfei%20Zhu%20and%20Jiangjing%20Lyu%20and%20Xinguo%20Liu%20and%20Jie%20Guo%20and%20Yanwen%20Guo%20and%20Weiwei%20Xu%20and%20Chengfei%20Lyu%0AAbstract%3A%20%20%20Recovering%20material%20information%20from%20images%20has%20been%20extensively%20studied%20in%0Acomputer%20graphics%20and%20vision.%20Recent%20works%20in%20material%20estimation%20leverage%0Adiffusion%20model%20showing%20promising%20results.%20However%2C%20these%20diffusion-based%0Amethods%20adopt%20a%20multi-step%20denoising%20strategy%2C%20which%20is%20time-consuming%20for%20each%0Aestimation.%20Such%20stochastic%20inference%20also%20conflicts%20with%20the%20deterministic%0Amaterial%20estimation%20task%2C%20leading%20to%20a%20high%20variance%20estimated%20results.%20In%20this%0Apaper%2C%20we%20introduce%20StableIntrinsic%2C%20a%20one-step%20diffusion%20model%20for%20multi-view%0Amaterial%20estimation%20that%20can%20produce%20high-quality%20material%20parameters%20with%20low%0Avariance.%20To%20address%20the%20overly-smoothing%20problem%20in%20one-step%20diffusion%2C%0AStableIntrinsic%20applies%20losses%20in%20pixel%20space%2C%20with%20each%20loss%20designed%20based%20on%0Athe%20properties%20of%20the%20material.%20Additionally%2C%20StableIntrinsic%20introduces%20a%0ADetail%20Injection%20Network%20%28DIN%29%20to%20eliminate%20the%20detail%20loss%20caused%20by%20VAE%0Aencoding%2C%20while%20further%20enhancing%20the%20sharpness%20of%20material%20prediction%20results.%0AThe%20experimental%20results%20indicate%20that%20our%20method%20surpasses%20the%20current%0Astate-of-the-art%20techniques%20by%20achieving%20a%20%249.9%5C%25%24%20improvement%20in%20the%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%20of%20albedo%2C%20and%20by%20reducing%20the%20Mean%20Square%20Error%0A%28MSE%29%20for%20metallic%20and%20roughness%20by%20%2444.4%5C%25%24%20and%20%2460.0%5C%25%24%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableIntrinsic%253A%2520Detail-preserving%2520One-step%2520Diffusion%2520Model%2520for%250A%2520%2520Multi-view%2520Material%2520Estimation%26entry.906535625%3DXiuchao%2520Wu%2520and%2520Pengfei%2520Zhu%2520and%2520Jiangjing%2520Lyu%2520and%2520Xinguo%2520Liu%2520and%2520Jie%2520Guo%2520and%2520Yanwen%2520Guo%2520and%2520Weiwei%2520Xu%2520and%2520Chengfei%2520Lyu%26entry.1292438233%3D%2520%2520Recovering%2520material%2520information%2520from%2520images%2520has%2520been%2520extensively%2520studied%2520in%250Acomputer%2520graphics%2520and%2520vision.%2520Recent%2520works%2520in%2520material%2520estimation%2520leverage%250Adiffusion%2520model%2520showing%2520promising%2520results.%2520However%252C%2520these%2520diffusion-based%250Amethods%2520adopt%2520a%2520multi-step%2520denoising%2520strategy%252C%2520which%2520is%2520time-consuming%2520for%2520each%250Aestimation.%2520Such%2520stochastic%2520inference%2520also%2520conflicts%2520with%2520the%2520deterministic%250Amaterial%2520estimation%2520task%252C%2520leading%2520to%2520a%2520high%2520variance%2520estimated%2520results.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520StableIntrinsic%252C%2520a%2520one-step%2520diffusion%2520model%2520for%2520multi-view%250Amaterial%2520estimation%2520that%2520can%2520produce%2520high-quality%2520material%2520parameters%2520with%2520low%250Avariance.%2520To%2520address%2520the%2520overly-smoothing%2520problem%2520in%2520one-step%2520diffusion%252C%250AStableIntrinsic%2520applies%2520losses%2520in%2520pixel%2520space%252C%2520with%2520each%2520loss%2520designed%2520based%2520on%250Athe%2520properties%2520of%2520the%2520material.%2520Additionally%252C%2520StableIntrinsic%2520introduces%2520a%250ADetail%2520Injection%2520Network%2520%2528DIN%2529%2520to%2520eliminate%2520the%2520detail%2520loss%2520caused%2520by%2520VAE%250Aencoding%252C%2520while%2520further%2520enhancing%2520the%2520sharpness%2520of%2520material%2520prediction%2520results.%250AThe%2520experimental%2520results%2520indicate%2520that%2520our%2520method%2520surpasses%2520the%2520current%250Astate-of-the-art%2520techniques%2520by%2520achieving%2520a%2520%25249.9%255C%2525%2524%2520improvement%2520in%2520the%2520Peak%250ASignal-to-Noise%2520Ratio%2520%2528PSNR%2529%2520of%2520albedo%252C%2520and%2520by%2520reducing%2520the%2520Mean%2520Square%2520Error%250A%2528MSE%2529%2520for%2520metallic%2520and%2520roughness%2520by%2520%252444.4%255C%2525%2524%2520and%2520%252460.0%255C%2525%2524%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableIntrinsic%3A%20Detail-preserving%20One-step%20Diffusion%20Model%20for%0A%20%20Multi-view%20Material%20Estimation&entry.906535625=Xiuchao%20Wu%20and%20Pengfei%20Zhu%20and%20Jiangjing%20Lyu%20and%20Xinguo%20Liu%20and%20Jie%20Guo%20and%20Yanwen%20Guo%20and%20Weiwei%20Xu%20and%20Chengfei%20Lyu&entry.1292438233=%20%20Recovering%20material%20information%20from%20images%20has%20been%20extensively%20studied%20in%0Acomputer%20graphics%20and%20vision.%20Recent%20works%20in%20material%20estimation%20leverage%0Adiffusion%20model%20showing%20promising%20results.%20However%2C%20these%20diffusion-based%0Amethods%20adopt%20a%20multi-step%20denoising%20strategy%2C%20which%20is%20time-consuming%20for%20each%0Aestimation.%20Such%20stochastic%20inference%20also%20conflicts%20with%20the%20deterministic%0Amaterial%20estimation%20task%2C%20leading%20to%20a%20high%20variance%20estimated%20results.%20In%20this%0Apaper%2C%20we%20introduce%20StableIntrinsic%2C%20a%20one-step%20diffusion%20model%20for%20multi-view%0Amaterial%20estimation%20that%20can%20produce%20high-quality%20material%20parameters%20with%20low%0Avariance.%20To%20address%20the%20overly-smoothing%20problem%20in%20one-step%20diffusion%2C%0AStableIntrinsic%20applies%20losses%20in%20pixel%20space%2C%20with%20each%20loss%20designed%20based%20on%0Athe%20properties%20of%20the%20material.%20Additionally%2C%20StableIntrinsic%20introduces%20a%0ADetail%20Injection%20Network%20%28DIN%29%20to%20eliminate%20the%20detail%20loss%20caused%20by%20VAE%0Aencoding%2C%20while%20further%20enhancing%20the%20sharpness%20of%20material%20prediction%20results.%0AThe%20experimental%20results%20indicate%20that%20our%20method%20surpasses%20the%20current%0Astate-of-the-art%20techniques%20by%20achieving%20a%20%249.9%5C%25%24%20improvement%20in%20the%20Peak%0ASignal-to-Noise%20Ratio%20%28PSNR%29%20of%20albedo%2C%20and%20by%20reducing%20the%20Mean%20Square%20Error%0A%28MSE%29%20for%20metallic%20and%20roughness%20by%20%2444.4%5C%25%24%20and%20%2460.0%5C%25%24%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19789v1&entry.124074799=Read"},
{"title": "Principled Detection of Hallucinations in Large Language Models via\n  Multiple Testing", "author": "Jiawei Li and Akshayaa Magesh and Venugopal V. Veeravalli", "abstract": "  While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2508.18473v2", "date": "2025-08-27", "relevancy": 1.4167, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4852}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Principled%20Detection%20of%20Hallucinations%20in%20Large%20Language%20Models%20via%0A%20%20Multiple%20Testing&body=Title%3A%20Principled%20Detection%20of%20Hallucinations%20in%20Large%20Language%20Models%20via%0A%20%20Multiple%20Testing%0AAuthor%3A%20Jiawei%20Li%20and%20Akshayaa%20Magesh%20and%20Venugopal%20V.%20Veeravalli%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20foundational%0Amodels%20to%20solve%20a%20variety%20of%20tasks%2C%20they%20have%20also%20been%20shown%20to%20be%20prone%20to%0Ahallucinations%2C%20i.e.%2C%20generating%20responses%20that%20sound%20confident%20but%20are%0Aactually%20incorrect%20or%20even%20nonsensical.%20In%20this%20work%2C%20we%20formulate%20the%20problem%0Aof%20detecting%20hallucinations%20as%20a%20hypothesis%20testing%20problem%20and%20draw%20parallels%0Ato%20the%20problem%20of%20out-of-distribution%20detection%20in%20machine%20learning%20models.%20We%0Apropose%20a%20multiple-testing-inspired%20method%20to%20solve%20the%20hallucination%20detection%0Aproblem%2C%20and%20provide%20extensive%20experimental%20results%20to%20validate%20the%20robustness%0Aof%20our%20approach%20against%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.18473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrincipled%2520Detection%2520of%2520Hallucinations%2520in%2520Large%2520Language%2520Models%2520via%250A%2520%2520Multiple%2520Testing%26entry.906535625%3DJiawei%2520Li%2520and%2520Akshayaa%2520Magesh%2520and%2520Venugopal%2520V.%2520Veeravalli%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520foundational%250Amodels%2520to%2520solve%2520a%2520variety%2520of%2520tasks%252C%2520they%2520have%2520also%2520been%2520shown%2520to%2520be%2520prone%2520to%250Ahallucinations%252C%2520i.e.%252C%2520generating%2520responses%2520that%2520sound%2520confident%2520but%2520are%250Aactually%2520incorrect%2520or%2520even%2520nonsensical.%2520In%2520this%2520work%252C%2520we%2520formulate%2520the%2520problem%250Aof%2520detecting%2520hallucinations%2520as%2520a%2520hypothesis%2520testing%2520problem%2520and%2520draw%2520parallels%250Ato%2520the%2520problem%2520of%2520out-of-distribution%2520detection%2520in%2520machine%2520learning%2520models.%2520We%250Apropose%2520a%2520multiple-testing-inspired%2520method%2520to%2520solve%2520the%2520hallucination%2520detection%250Aproblem%252C%2520and%2520provide%2520extensive%2520experimental%2520results%2520to%2520validate%2520the%2520robustness%250Aof%2520our%2520approach%2520against%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Principled%20Detection%20of%20Hallucinations%20in%20Large%20Language%20Models%20via%0A%20%20Multiple%20Testing&entry.906535625=Jiawei%20Li%20and%20Akshayaa%20Magesh%20and%20Venugopal%20V.%20Veeravalli&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20foundational%0Amodels%20to%20solve%20a%20variety%20of%20tasks%2C%20they%20have%20also%20been%20shown%20to%20be%20prone%20to%0Ahallucinations%2C%20i.e.%2C%20generating%20responses%20that%20sound%20confident%20but%20are%0Aactually%20incorrect%20or%20even%20nonsensical.%20In%20this%20work%2C%20we%20formulate%20the%20problem%0Aof%20detecting%20hallucinations%20as%20a%20hypothesis%20testing%20problem%20and%20draw%20parallels%0Ato%20the%20problem%20of%20out-of-distribution%20detection%20in%20machine%20learning%20models.%20We%0Apropose%20a%20multiple-testing-inspired%20method%20to%20solve%20the%20hallucination%20detection%0Aproblem%2C%20and%20provide%20extensive%20experimental%20results%20to%20validate%20the%20robustness%0Aof%20our%20approach%20against%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.18473v2&entry.124074799=Read"},
{"title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for\n  Generative Research Synthesis", "author": "Liana Patel and Negar Arabzadeh and Harshit Gupta and Ankita Sundar and Ion Stoica and Matei Zaharia and Carlos Guestrin", "abstract": "  The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.\n", "link": "http://arxiv.org/abs/2508.20033v1", "date": "2025-08-27", "relevancy": 1.5095, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5122}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4985}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepScholar-Bench%3A%20A%20Live%20Benchmark%20and%20Automated%20Evaluation%20for%0A%20%20Generative%20Research%20Synthesis&body=Title%3A%20DeepScholar-Bench%3A%20A%20Live%20Benchmark%20and%20Automated%20Evaluation%20for%0A%20%20Generative%20Research%20Synthesis%0AAuthor%3A%20Liana%20Patel%20and%20Negar%20Arabzadeh%20and%20Harshit%20Gupta%20and%20Ankita%20Sundar%20and%20Ion%20Stoica%20and%20Matei%20Zaharia%20and%20Carlos%20Guestrin%0AAbstract%3A%20%20%20The%20ability%20to%20research%20and%20synthesize%20knowledge%20is%20central%20to%20human%0Aexpertise%20and%20progress.%20An%20emerging%20class%20of%20systems%20promises%20these%20exciting%0Acapabilities%20through%20generative%20research%20synthesis%2C%20performing%20retrieval%20over%0Athe%20live%20web%20and%20synthesizing%20discovered%20sources%20into%20long-form%2C%20cited%0Asummaries.%20However%2C%20evaluating%20such%20systems%20remains%20an%20open%20challenge%3A%20existing%0Aquestion-answering%20benchmarks%20focus%20on%20short-form%20factual%20responses%2C%20while%0Aexpert-curated%20datasets%20risk%20staleness%20and%20data%20contamination.%20Both%20fail%20to%0Acapture%20the%20complexity%20and%20evolving%20nature%20of%20real%20research%20synthesis%20tasks.%20In%0Athis%20work%2C%20we%20introduce%20DeepScholar-bench%2C%20a%20live%20benchmark%20and%20holistic%2C%0Aautomated%20evaluation%20framework%20designed%20to%20evaluate%20generative%20research%0Asynthesis.%20DeepScholar-bench%20draws%20queries%20from%20recent%2C%20high-quality%20ArXiv%0Apapers%20and%20focuses%20on%20a%20real%20research%20synthesis%20task%3A%20generating%20the%20related%0Awork%20sections%20of%20a%20paper%20by%20retrieving%2C%20synthesizing%2C%20and%20citing%20prior%0Aresearch.%20Our%20evaluation%20framework%20holistically%20assesses%20performance%20across%0Athree%20key%20dimensions%2C%20knowledge%20synthesis%2C%20retrieval%20quality%2C%20and%0Averifiability.%20We%20also%20develop%20DeepScholar-base%2C%20a%20reference%20pipeline%0Aimplemented%20efficiently%20using%20the%20LOTUS%20API.%20Using%20the%20DeepScholar-bench%0Aframework%2C%20we%20perform%20a%20systematic%20evaluation%20of%20prior%20open-source%20systems%2C%0Asearch%20AI%27s%2C%20OpenAI%27s%20DeepResearch%2C%20and%20DeepScholar-base.%20We%20find%20that%0ADeepScholar-base%20establishes%20a%20strong%20baseline%2C%20attaining%20competitive%20or%20higher%0Aperformance%20than%20each%20other%20method.%20We%20also%20find%20that%20DeepScholar-bench%20remains%0Afar%20from%20saturated%2C%20with%20no%20system%20exceeding%20a%20score%20of%20%2419%5C%25%24%20across%20all%0Ametrics.%20These%20results%20underscore%20the%20difficulty%20of%20DeepScholar-bench%2C%20as%20well%0Aas%20its%20importance%20for%20progress%20towards%20AI%20systems%20capable%20of%20generative%0Aresearch%20synthesis.%20We%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/guestrin-lab/deepscholar-bench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.20033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepScholar-Bench%253A%2520A%2520Live%2520Benchmark%2520and%2520Automated%2520Evaluation%2520for%250A%2520%2520Generative%2520Research%2520Synthesis%26entry.906535625%3DLiana%2520Patel%2520and%2520Negar%2520Arabzadeh%2520and%2520Harshit%2520Gupta%2520and%2520Ankita%2520Sundar%2520and%2520Ion%2520Stoica%2520and%2520Matei%2520Zaharia%2520and%2520Carlos%2520Guestrin%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520research%2520and%2520synthesize%2520knowledge%2520is%2520central%2520to%2520human%250Aexpertise%2520and%2520progress.%2520An%2520emerging%2520class%2520of%2520systems%2520promises%2520these%2520exciting%250Acapabilities%2520through%2520generative%2520research%2520synthesis%252C%2520performing%2520retrieval%2520over%250Athe%2520live%2520web%2520and%2520synthesizing%2520discovered%2520sources%2520into%2520long-form%252C%2520cited%250Asummaries.%2520However%252C%2520evaluating%2520such%2520systems%2520remains%2520an%2520open%2520challenge%253A%2520existing%250Aquestion-answering%2520benchmarks%2520focus%2520on%2520short-form%2520factual%2520responses%252C%2520while%250Aexpert-curated%2520datasets%2520risk%2520staleness%2520and%2520data%2520contamination.%2520Both%2520fail%2520to%250Acapture%2520the%2520complexity%2520and%2520evolving%2520nature%2520of%2520real%2520research%2520synthesis%2520tasks.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520DeepScholar-bench%252C%2520a%2520live%2520benchmark%2520and%2520holistic%252C%250Aautomated%2520evaluation%2520framework%2520designed%2520to%2520evaluate%2520generative%2520research%250Asynthesis.%2520DeepScholar-bench%2520draws%2520queries%2520from%2520recent%252C%2520high-quality%2520ArXiv%250Apapers%2520and%2520focuses%2520on%2520a%2520real%2520research%2520synthesis%2520task%253A%2520generating%2520the%2520related%250Awork%2520sections%2520of%2520a%2520paper%2520by%2520retrieving%252C%2520synthesizing%252C%2520and%2520citing%2520prior%250Aresearch.%2520Our%2520evaluation%2520framework%2520holistically%2520assesses%2520performance%2520across%250Athree%2520key%2520dimensions%252C%2520knowledge%2520synthesis%252C%2520retrieval%2520quality%252C%2520and%250Averifiability.%2520We%2520also%2520develop%2520DeepScholar-base%252C%2520a%2520reference%2520pipeline%250Aimplemented%2520efficiently%2520using%2520the%2520LOTUS%2520API.%2520Using%2520the%2520DeepScholar-bench%250Aframework%252C%2520we%2520perform%2520a%2520systematic%2520evaluation%2520of%2520prior%2520open-source%2520systems%252C%250Asearch%2520AI%2527s%252C%2520OpenAI%2527s%2520DeepResearch%252C%2520and%2520DeepScholar-base.%2520We%2520find%2520that%250ADeepScholar-base%2520establishes%2520a%2520strong%2520baseline%252C%2520attaining%2520competitive%2520or%2520higher%250Aperformance%2520than%2520each%2520other%2520method.%2520We%2520also%2520find%2520that%2520DeepScholar-bench%2520remains%250Afar%2520from%2520saturated%252C%2520with%2520no%2520system%2520exceeding%2520a%2520score%2520of%2520%252419%255C%2525%2524%2520across%2520all%250Ametrics.%2520These%2520results%2520underscore%2520the%2520difficulty%2520of%2520DeepScholar-bench%252C%2520as%2520well%250Aas%2520its%2520importance%2520for%2520progress%2520towards%2520AI%2520systems%2520capable%2520of%2520generative%250Aresearch%2520synthesis.%2520We%2520make%2520our%2520code%2520available%2520at%250Ahttps%253A//github.com/guestrin-lab/deepscholar-bench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepScholar-Bench%3A%20A%20Live%20Benchmark%20and%20Automated%20Evaluation%20for%0A%20%20Generative%20Research%20Synthesis&entry.906535625=Liana%20Patel%20and%20Negar%20Arabzadeh%20and%20Harshit%20Gupta%20and%20Ankita%20Sundar%20and%20Ion%20Stoica%20and%20Matei%20Zaharia%20and%20Carlos%20Guestrin&entry.1292438233=%20%20The%20ability%20to%20research%20and%20synthesize%20knowledge%20is%20central%20to%20human%0Aexpertise%20and%20progress.%20An%20emerging%20class%20of%20systems%20promises%20these%20exciting%0Acapabilities%20through%20generative%20research%20synthesis%2C%20performing%20retrieval%20over%0Athe%20live%20web%20and%20synthesizing%20discovered%20sources%20into%20long-form%2C%20cited%0Asummaries.%20However%2C%20evaluating%20such%20systems%20remains%20an%20open%20challenge%3A%20existing%0Aquestion-answering%20benchmarks%20focus%20on%20short-form%20factual%20responses%2C%20while%0Aexpert-curated%20datasets%20risk%20staleness%20and%20data%20contamination.%20Both%20fail%20to%0Acapture%20the%20complexity%20and%20evolving%20nature%20of%20real%20research%20synthesis%20tasks.%20In%0Athis%20work%2C%20we%20introduce%20DeepScholar-bench%2C%20a%20live%20benchmark%20and%20holistic%2C%0Aautomated%20evaluation%20framework%20designed%20to%20evaluate%20generative%20research%0Asynthesis.%20DeepScholar-bench%20draws%20queries%20from%20recent%2C%20high-quality%20ArXiv%0Apapers%20and%20focuses%20on%20a%20real%20research%20synthesis%20task%3A%20generating%20the%20related%0Awork%20sections%20of%20a%20paper%20by%20retrieving%2C%20synthesizing%2C%20and%20citing%20prior%0Aresearch.%20Our%20evaluation%20framework%20holistically%20assesses%20performance%20across%0Athree%20key%20dimensions%2C%20knowledge%20synthesis%2C%20retrieval%20quality%2C%20and%0Averifiability.%20We%20also%20develop%20DeepScholar-base%2C%20a%20reference%20pipeline%0Aimplemented%20efficiently%20using%20the%20LOTUS%20API.%20Using%20the%20DeepScholar-bench%0Aframework%2C%20we%20perform%20a%20systematic%20evaluation%20of%20prior%20open-source%20systems%2C%0Asearch%20AI%27s%2C%20OpenAI%27s%20DeepResearch%2C%20and%20DeepScholar-base.%20We%20find%20that%0ADeepScholar-base%20establishes%20a%20strong%20baseline%2C%20attaining%20competitive%20or%20higher%0Aperformance%20than%20each%20other%20method.%20We%20also%20find%20that%20DeepScholar-bench%20remains%0Afar%20from%20saturated%2C%20with%20no%20system%20exceeding%20a%20score%20of%20%2419%5C%25%24%20across%20all%0Ametrics.%20These%20results%20underscore%20the%20difficulty%20of%20DeepScholar-bench%2C%20as%20well%0Aas%20its%20importance%20for%20progress%20towards%20AI%20systems%20capable%20of%20generative%0Aresearch%20synthesis.%20We%20make%20our%20code%20available%20at%0Ahttps%3A//github.com/guestrin-lab/deepscholar-bench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.20033v1&entry.124074799=Read"},
{"title": "FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for\n  Traffic Classification", "author": "Liming Liu and Ruoyu Li and Qing Li and Meijia Hou and Yong Jiang and Mingwei Xu", "abstract": "  Network traffic classification using pre-training models has shown promising\nresults, but existing methods struggle to capture packet structural\ncharacteristics, flow-level behaviors, hierarchical protocol semantics, and\ninter-packet contextual relationships. To address these challenges, we propose\nFlowletFormer, a BERT-based pre-training model specifically designed for\nnetwork traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware\nTraffic Representation Model for segmenting traffic into semantically\nmeaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture\nmultilayer protocol semantics, and Field-Specific and Context-Aware Pretraining\nTasks to enhance both inter-packet and inter-flow learning. Experimental\nresults demonstrate that FlowletFormer significantly outperforms existing\nmethods in the effectiveness of traffic representation, classification\naccuracy, and few-shot learning capability. Moreover, by effectively\nintegrating domain-specific network knowledge, FlowletFormer shows better\ncomprehension of the principles of network transmission (e.g., stateful\nconnections of TCP), providing a more robust and trustworthy framework for\ntraffic analysis.\n", "link": "http://arxiv.org/abs/2508.19924v1", "date": "2025-08-27", "relevancy": 1.8877, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlowletFormer%3A%20Network%20Behavioral%20Semantic%20Aware%20Pre-training%20Model%20for%0A%20%20Traffic%20Classification&body=Title%3A%20FlowletFormer%3A%20Network%20Behavioral%20Semantic%20Aware%20Pre-training%20Model%20for%0A%20%20Traffic%20Classification%0AAuthor%3A%20Liming%20Liu%20and%20Ruoyu%20Li%20and%20Qing%20Li%20and%20Meijia%20Hou%20and%20Yong%20Jiang%20and%20Mingwei%20Xu%0AAbstract%3A%20%20%20Network%20traffic%20classification%20using%20pre-training%20models%20has%20shown%20promising%0Aresults%2C%20but%20existing%20methods%20struggle%20to%20capture%20packet%20structural%0Acharacteristics%2C%20flow-level%20behaviors%2C%20hierarchical%20protocol%20semantics%2C%20and%0Ainter-packet%20contextual%20relationships.%20To%20address%20these%20challenges%2C%20we%20propose%0AFlowletFormer%2C%20a%20BERT-based%20pre-training%20model%20specifically%20designed%20for%0Anetwork%20traffic%20analysis.%20FlowletFormer%20introduces%20a%20Coherent%20Behavior-Aware%0ATraffic%20Representation%20Model%20for%20segmenting%20traffic%20into%20semantically%0Ameaningful%20units%2C%20a%20Protocol%20Stack%20Alignment-Based%20Embedding%20Layer%20to%20capture%0Amultilayer%20protocol%20semantics%2C%20and%20Field-Specific%20and%20Context-Aware%20Pretraining%0ATasks%20to%20enhance%20both%20inter-packet%20and%20inter-flow%20learning.%20Experimental%0Aresults%20demonstrate%20that%20FlowletFormer%20significantly%20outperforms%20existing%0Amethods%20in%20the%20effectiveness%20of%20traffic%20representation%2C%20classification%0Aaccuracy%2C%20and%20few-shot%20learning%20capability.%20Moreover%2C%20by%20effectively%0Aintegrating%20domain-specific%20network%20knowledge%2C%20FlowletFormer%20shows%20better%0Acomprehension%20of%20the%20principles%20of%20network%20transmission%20%28e.g.%2C%20stateful%0Aconnections%20of%20TCP%29%2C%20providing%20a%20more%20robust%20and%20trustworthy%20framework%20for%0Atraffic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlowletFormer%253A%2520Network%2520Behavioral%2520Semantic%2520Aware%2520Pre-training%2520Model%2520for%250A%2520%2520Traffic%2520Classification%26entry.906535625%3DLiming%2520Liu%2520and%2520Ruoyu%2520Li%2520and%2520Qing%2520Li%2520and%2520Meijia%2520Hou%2520and%2520Yong%2520Jiang%2520and%2520Mingwei%2520Xu%26entry.1292438233%3D%2520%2520Network%2520traffic%2520classification%2520using%2520pre-training%2520models%2520has%2520shown%2520promising%250Aresults%252C%2520but%2520existing%2520methods%2520struggle%2520to%2520capture%2520packet%2520structural%250Acharacteristics%252C%2520flow-level%2520behaviors%252C%2520hierarchical%2520protocol%2520semantics%252C%2520and%250Ainter-packet%2520contextual%2520relationships.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AFlowletFormer%252C%2520a%2520BERT-based%2520pre-training%2520model%2520specifically%2520designed%2520for%250Anetwork%2520traffic%2520analysis.%2520FlowletFormer%2520introduces%2520a%2520Coherent%2520Behavior-Aware%250ATraffic%2520Representation%2520Model%2520for%2520segmenting%2520traffic%2520into%2520semantically%250Ameaningful%2520units%252C%2520a%2520Protocol%2520Stack%2520Alignment-Based%2520Embedding%2520Layer%2520to%2520capture%250Amultilayer%2520protocol%2520semantics%252C%2520and%2520Field-Specific%2520and%2520Context-Aware%2520Pretraining%250ATasks%2520to%2520enhance%2520both%2520inter-packet%2520and%2520inter-flow%2520learning.%2520Experimental%250Aresults%2520demonstrate%2520that%2520FlowletFormer%2520significantly%2520outperforms%2520existing%250Amethods%2520in%2520the%2520effectiveness%2520of%2520traffic%2520representation%252C%2520classification%250Aaccuracy%252C%2520and%2520few-shot%2520learning%2520capability.%2520Moreover%252C%2520by%2520effectively%250Aintegrating%2520domain-specific%2520network%2520knowledge%252C%2520FlowletFormer%2520shows%2520better%250Acomprehension%2520of%2520the%2520principles%2520of%2520network%2520transmission%2520%2528e.g.%252C%2520stateful%250Aconnections%2520of%2520TCP%2529%252C%2520providing%2520a%2520more%2520robust%2520and%2520trustworthy%2520framework%2520for%250Atraffic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlowletFormer%3A%20Network%20Behavioral%20Semantic%20Aware%20Pre-training%20Model%20for%0A%20%20Traffic%20Classification&entry.906535625=Liming%20Liu%20and%20Ruoyu%20Li%20and%20Qing%20Li%20and%20Meijia%20Hou%20and%20Yong%20Jiang%20and%20Mingwei%20Xu&entry.1292438233=%20%20Network%20traffic%20classification%20using%20pre-training%20models%20has%20shown%20promising%0Aresults%2C%20but%20existing%20methods%20struggle%20to%20capture%20packet%20structural%0Acharacteristics%2C%20flow-level%20behaviors%2C%20hierarchical%20protocol%20semantics%2C%20and%0Ainter-packet%20contextual%20relationships.%20To%20address%20these%20challenges%2C%20we%20propose%0AFlowletFormer%2C%20a%20BERT-based%20pre-training%20model%20specifically%20designed%20for%0Anetwork%20traffic%20analysis.%20FlowletFormer%20introduces%20a%20Coherent%20Behavior-Aware%0ATraffic%20Representation%20Model%20for%20segmenting%20traffic%20into%20semantically%0Ameaningful%20units%2C%20a%20Protocol%20Stack%20Alignment-Based%20Embedding%20Layer%20to%20capture%0Amultilayer%20protocol%20semantics%2C%20and%20Field-Specific%20and%20Context-Aware%20Pretraining%0ATasks%20to%20enhance%20both%20inter-packet%20and%20inter-flow%20learning.%20Experimental%0Aresults%20demonstrate%20that%20FlowletFormer%20significantly%20outperforms%20existing%0Amethods%20in%20the%20effectiveness%20of%20traffic%20representation%2C%20classification%0Aaccuracy%2C%20and%20few-shot%20learning%20capability.%20Moreover%2C%20by%20effectively%0Aintegrating%20domain-specific%20network%20knowledge%2C%20FlowletFormer%20shows%20better%0Acomprehension%20of%20the%20principles%20of%20network%20transmission%20%28e.g.%2C%20stateful%0Aconnections%20of%20TCP%29%2C%20providing%20a%20more%20robust%20and%20trustworthy%20framework%20for%0Atraffic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19924v1&entry.124074799=Read"},
{"title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset\n  with LLM-Assisted Translation Refinement", "author": "Mohammed Rakibul Hasan and Rafi Majid and Ahanaf Tahmid", "abstract": "  In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.\n", "link": "http://arxiv.org/abs/2508.19887v1", "date": "2025-08-27", "relevancy": 1.8307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4595}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bangla-Bayanno%3A%20A%2052K-Pair%20Bengali%20Visual%20Question%20Answering%20Dataset%0A%20%20with%20LLM-Assisted%20Translation%20Refinement&body=Title%3A%20Bangla-Bayanno%3A%20A%2052K-Pair%20Bengali%20Visual%20Question%20Answering%20Dataset%0A%20%20with%20LLM-Assisted%20Translation%20Refinement%0AAuthor%3A%20Mohammed%20Rakibul%20Hasan%20and%20Rafi%20Majid%20and%20Ahanaf%20Tahmid%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Bangla-Bayanno%2C%20an%20open-ended%20Visual%20Question%0AAnswering%20%28VQA%29%20Dataset%20in%20Bangla%2C%20a%20widely%20used%2C%20low-resource%20language%20in%0Amultimodal%20AI%20research.%20The%20majority%20of%20existing%20datasets%20are%20either%20manually%0Aannotated%20with%20an%20emphasis%20on%20a%20specific%20domain%2C%20query%20type%2C%20or%20answer%20type%20or%0Aare%20constrained%20by%20niche%20answer%20formats.%20In%20order%20to%20mitigate%20human-induced%0Aerrors%20and%20guarantee%20lucidity%2C%20we%20implemented%20a%20multilingual%20LLM-assisted%0Atranslation%20refinement%20pipeline.%20This%20dataset%20overcomes%20the%20issues%20of%0Alow-quality%20translations%20from%20multilingual%20sources.%20The%20dataset%20comprises%0A52%2C650%20question-answer%20pairs%20across%204750%2B%20images.%20Questions%20are%20classified%20into%0Athree%20distinct%20answer%20types%3A%20nominal%20%28short%20descriptive%29%2C%20quantitative%0A%28numeric%29%2C%20and%20polar%20%28yes/no%29.%20Bangla-Bayanno%20provides%20the%20most%20comprehensive%0Aopen-source%2C%20high-quality%20VQA%20benchmark%20in%20Bangla%2C%20aiming%20to%20advance%20research%0Ain%20low-resource%20multimodal%20learning%20and%20facilitate%20the%20development%20of%20more%0Ainclusive%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBangla-Bayanno%253A%2520A%252052K-Pair%2520Bengali%2520Visual%2520Question%2520Answering%2520Dataset%250A%2520%2520with%2520LLM-Assisted%2520Translation%2520Refinement%26entry.906535625%3DMohammed%2520Rakibul%2520Hasan%2520and%2520Rafi%2520Majid%2520and%2520Ahanaf%2520Tahmid%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Bangla-Bayanno%252C%2520an%2520open-ended%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520Dataset%2520in%2520Bangla%252C%2520a%2520widely%2520used%252C%2520low-resource%2520language%2520in%250Amultimodal%2520AI%2520research.%2520The%2520majority%2520of%2520existing%2520datasets%2520are%2520either%2520manually%250Aannotated%2520with%2520an%2520emphasis%2520on%2520a%2520specific%2520domain%252C%2520query%2520type%252C%2520or%2520answer%2520type%2520or%250Aare%2520constrained%2520by%2520niche%2520answer%2520formats.%2520In%2520order%2520to%2520mitigate%2520human-induced%250Aerrors%2520and%2520guarantee%2520lucidity%252C%2520we%2520implemented%2520a%2520multilingual%2520LLM-assisted%250Atranslation%2520refinement%2520pipeline.%2520This%2520dataset%2520overcomes%2520the%2520issues%2520of%250Alow-quality%2520translations%2520from%2520multilingual%2520sources.%2520The%2520dataset%2520comprises%250A52%252C650%2520question-answer%2520pairs%2520across%25204750%252B%2520images.%2520Questions%2520are%2520classified%2520into%250Athree%2520distinct%2520answer%2520types%253A%2520nominal%2520%2528short%2520descriptive%2529%252C%2520quantitative%250A%2528numeric%2529%252C%2520and%2520polar%2520%2528yes/no%2529.%2520Bangla-Bayanno%2520provides%2520the%2520most%2520comprehensive%250Aopen-source%252C%2520high-quality%2520VQA%2520benchmark%2520in%2520Bangla%252C%2520aiming%2520to%2520advance%2520research%250Ain%2520low-resource%2520multimodal%2520learning%2520and%2520facilitate%2520the%2520development%2520of%2520more%250Ainclusive%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bangla-Bayanno%3A%20A%2052K-Pair%20Bengali%20Visual%20Question%20Answering%20Dataset%0A%20%20with%20LLM-Assisted%20Translation%20Refinement&entry.906535625=Mohammed%20Rakibul%20Hasan%20and%20Rafi%20Majid%20and%20Ahanaf%20Tahmid&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Bangla-Bayanno%2C%20an%20open-ended%20Visual%20Question%0AAnswering%20%28VQA%29%20Dataset%20in%20Bangla%2C%20a%20widely%20used%2C%20low-resource%20language%20in%0Amultimodal%20AI%20research.%20The%20majority%20of%20existing%20datasets%20are%20either%20manually%0Aannotated%20with%20an%20emphasis%20on%20a%20specific%20domain%2C%20query%20type%2C%20or%20answer%20type%20or%0Aare%20constrained%20by%20niche%20answer%20formats.%20In%20order%20to%20mitigate%20human-induced%0Aerrors%20and%20guarantee%20lucidity%2C%20we%20implemented%20a%20multilingual%20LLM-assisted%0Atranslation%20refinement%20pipeline.%20This%20dataset%20overcomes%20the%20issues%20of%0Alow-quality%20translations%20from%20multilingual%20sources.%20The%20dataset%20comprises%0A52%2C650%20question-answer%20pairs%20across%204750%2B%20images.%20Questions%20are%20classified%20into%0Athree%20distinct%20answer%20types%3A%20nominal%20%28short%20descriptive%29%2C%20quantitative%0A%28numeric%29%2C%20and%20polar%20%28yes/no%29.%20Bangla-Bayanno%20provides%20the%20most%20comprehensive%0Aopen-source%2C%20high-quality%20VQA%20benchmark%20in%20Bangla%2C%20aiming%20to%20advance%20research%0Ain%20low-resource%20multimodal%20learning%20and%20facilitate%20the%20development%20of%20more%0Ainclusive%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19887v1&entry.124074799=Read"},
{"title": "SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for\n  Binary Classifiers", "author": "Tom Siegl and Kutalm\u0131\u015f Co\u015fkun and Bjarne C. Hiller and Amin Mirzaei and Florian Lemmerich and Martin Becker", "abstract": "  Machine learning (ML) is increasingly employed in real-world applications\nlike medicine or economics, thus, potentially affecting large populations.\nHowever, ML models often do not perform homogeneously, leading to\nunderperformance or, conversely, unusually high performance in certain\nsubgroups (e.g., sex=female AND marital_status=married). Identifying such\nsubgroups can support practical decisions on which subpopulation a model is\nsafe to deploy or where more training data is required. However, an efficient\nand coherent framework for effective search is missing. Consequently, we\nintroduce SubROC, an open-source, easy-to-use framework based on Exceptional\nModel Mining for reliably and efficiently finding strengths and weaknesses of\nclassification models in the form of interpretable population subgroups. SubROC\nincorporates common evaluation measures (ROC and PR AUC), efficient search\nspace pruning for fast exhaustive subgroup search, control for class imbalance,\nadjustment for redundant patterns, and significance testing. We illustrate the\npractical benefits of SubROC in case studies as well as in comparative analyses\nacross multiple datasets.\n", "link": "http://arxiv.org/abs/2505.11283v2", "date": "2025-08-27", "relevancy": 1.3939, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4897}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SubROC%3A%20AUC-Based%20Discovery%20of%20Exceptional%20Subgroup%20Performance%20for%0A%20%20Binary%20Classifiers&body=Title%3A%20SubROC%3A%20AUC-Based%20Discovery%20of%20Exceptional%20Subgroup%20Performance%20for%0A%20%20Binary%20Classifiers%0AAuthor%3A%20Tom%20Siegl%20and%20Kutalm%C4%B1%C5%9F%20Co%C5%9Fkun%20and%20Bjarne%20C.%20Hiller%20and%20Amin%20Mirzaei%20and%20Florian%20Lemmerich%20and%20Martin%20Becker%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20is%20increasingly%20employed%20in%20real-world%20applications%0Alike%20medicine%20or%20economics%2C%20thus%2C%20potentially%20affecting%20large%20populations.%0AHowever%2C%20ML%20models%20often%20do%20not%20perform%20homogeneously%2C%20leading%20to%0Aunderperformance%20or%2C%20conversely%2C%20unusually%20high%20performance%20in%20certain%0Asubgroups%20%28e.g.%2C%20sex%3Dfemale%20AND%20marital_status%3Dmarried%29.%20Identifying%20such%0Asubgroups%20can%20support%20practical%20decisions%20on%20which%20subpopulation%20a%20model%20is%0Asafe%20to%20deploy%20or%20where%20more%20training%20data%20is%20required.%20However%2C%20an%20efficient%0Aand%20coherent%20framework%20for%20effective%20search%20is%20missing.%20Consequently%2C%20we%0Aintroduce%20SubROC%2C%20an%20open-source%2C%20easy-to-use%20framework%20based%20on%20Exceptional%0AModel%20Mining%20for%20reliably%20and%20efficiently%20finding%20strengths%20and%20weaknesses%20of%0Aclassification%20models%20in%20the%20form%20of%20interpretable%20population%20subgroups.%20SubROC%0Aincorporates%20common%20evaluation%20measures%20%28ROC%20and%20PR%20AUC%29%2C%20efficient%20search%0Aspace%20pruning%20for%20fast%20exhaustive%20subgroup%20search%2C%20control%20for%20class%20imbalance%2C%0Aadjustment%20for%20redundant%20patterns%2C%20and%20significance%20testing.%20We%20illustrate%20the%0Apractical%20benefits%20of%20SubROC%20in%20case%20studies%20as%20well%20as%20in%20comparative%20analyses%0Aacross%20multiple%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubROC%253A%2520AUC-Based%2520Discovery%2520of%2520Exceptional%2520Subgroup%2520Performance%2520for%250A%2520%2520Binary%2520Classifiers%26entry.906535625%3DTom%2520Siegl%2520and%2520Kutalm%25C4%25B1%25C5%259F%2520Co%25C5%259Fkun%2520and%2520Bjarne%2520C.%2520Hiller%2520and%2520Amin%2520Mirzaei%2520and%2520Florian%2520Lemmerich%2520and%2520Martin%2520Becker%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520is%2520increasingly%2520employed%2520in%2520real-world%2520applications%250Alike%2520medicine%2520or%2520economics%252C%2520thus%252C%2520potentially%2520affecting%2520large%2520populations.%250AHowever%252C%2520ML%2520models%2520often%2520do%2520not%2520perform%2520homogeneously%252C%2520leading%2520to%250Aunderperformance%2520or%252C%2520conversely%252C%2520unusually%2520high%2520performance%2520in%2520certain%250Asubgroups%2520%2528e.g.%252C%2520sex%253Dfemale%2520AND%2520marital_status%253Dmarried%2529.%2520Identifying%2520such%250Asubgroups%2520can%2520support%2520practical%2520decisions%2520on%2520which%2520subpopulation%2520a%2520model%2520is%250Asafe%2520to%2520deploy%2520or%2520where%2520more%2520training%2520data%2520is%2520required.%2520However%252C%2520an%2520efficient%250Aand%2520coherent%2520framework%2520for%2520effective%2520search%2520is%2520missing.%2520Consequently%252C%2520we%250Aintroduce%2520SubROC%252C%2520an%2520open-source%252C%2520easy-to-use%2520framework%2520based%2520on%2520Exceptional%250AModel%2520Mining%2520for%2520reliably%2520and%2520efficiently%2520finding%2520strengths%2520and%2520weaknesses%2520of%250Aclassification%2520models%2520in%2520the%2520form%2520of%2520interpretable%2520population%2520subgroups.%2520SubROC%250Aincorporates%2520common%2520evaluation%2520measures%2520%2528ROC%2520and%2520PR%2520AUC%2529%252C%2520efficient%2520search%250Aspace%2520pruning%2520for%2520fast%2520exhaustive%2520subgroup%2520search%252C%2520control%2520for%2520class%2520imbalance%252C%250Aadjustment%2520for%2520redundant%2520patterns%252C%2520and%2520significance%2520testing.%2520We%2520illustrate%2520the%250Apractical%2520benefits%2520of%2520SubROC%2520in%2520case%2520studies%2520as%2520well%2520as%2520in%2520comparative%2520analyses%250Aacross%2520multiple%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SubROC%3A%20AUC-Based%20Discovery%20of%20Exceptional%20Subgroup%20Performance%20for%0A%20%20Binary%20Classifiers&entry.906535625=Tom%20Siegl%20and%20Kutalm%C4%B1%C5%9F%20Co%C5%9Fkun%20and%20Bjarne%20C.%20Hiller%20and%20Amin%20Mirzaei%20and%20Florian%20Lemmerich%20and%20Martin%20Becker&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20is%20increasingly%20employed%20in%20real-world%20applications%0Alike%20medicine%20or%20economics%2C%20thus%2C%20potentially%20affecting%20large%20populations.%0AHowever%2C%20ML%20models%20often%20do%20not%20perform%20homogeneously%2C%20leading%20to%0Aunderperformance%20or%2C%20conversely%2C%20unusually%20high%20performance%20in%20certain%0Asubgroups%20%28e.g.%2C%20sex%3Dfemale%20AND%20marital_status%3Dmarried%29.%20Identifying%20such%0Asubgroups%20can%20support%20practical%20decisions%20on%20which%20subpopulation%20a%20model%20is%0Asafe%20to%20deploy%20or%20where%20more%20training%20data%20is%20required.%20However%2C%20an%20efficient%0Aand%20coherent%20framework%20for%20effective%20search%20is%20missing.%20Consequently%2C%20we%0Aintroduce%20SubROC%2C%20an%20open-source%2C%20easy-to-use%20framework%20based%20on%20Exceptional%0AModel%20Mining%20for%20reliably%20and%20efficiently%20finding%20strengths%20and%20weaknesses%20of%0Aclassification%20models%20in%20the%20form%20of%20interpretable%20population%20subgroups.%20SubROC%0Aincorporates%20common%20evaluation%20measures%20%28ROC%20and%20PR%20AUC%29%2C%20efficient%20search%0Aspace%20pruning%20for%20fast%20exhaustive%20subgroup%20search%2C%20control%20for%20class%20imbalance%2C%0Aadjustment%20for%20redundant%20patterns%2C%20and%20significance%20testing.%20We%20illustrate%20the%0Apractical%20benefits%20of%20SubROC%20in%20case%20studies%20as%20well%20as%20in%20comparative%20analyses%0Aacross%20multiple%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11283v2&entry.124074799=Read"},
{"title": "Analysis and Synthesis Denoisers for Forward-Backward Plug-and-Play\n  Algorithms", "author": "Matthieu Kowalski and Beno\u00eet Mal\u00e9zieux and Thomas Moreau and Audrey Repetti", "abstract": "  In this work we study the behavior of the forward-backward (FB) algorithm\nwhen the proximity operator is replaced by a sub-iterative procedure to\napproximate a Gaussian denoiser, in a Plug-and-Play (PnP) fashion. In\nparticular, we consider both analysis and synthesis Gaussian denoisers within a\ndictionary framework, obtained by unrolling dual-FB iterations or FB\niterations, respectively. We analyze the associated minimization problems as\nwell as the asymptotic behavior of the resulting FB-PnP iterations. In\nparticular, we show that the synthesis Gaussian denoising problem can be viewed\nas a proximity operator. For each case, analysis and synthesis, we show that\nthe FB-PnP algorithms solve the same problem whether we use only one or an\ninfinite number of sub-iteration to solve the denoising problem at each\niteration. To this aim, we show that each \"one sub-iteration\" strategy within\nthe FB-PnP can be interpreted as a primal-dual algorithm when a warm-restart\nstrategy is used. We further present similar results when using a Moreau-Yosida\nsmoothing of the global problem, for an arbitrary number of sub-iterations.\nFinally, we provide numerical simulations to illustrate our theoretical\nresults. In particular we first consider a toy compressive sensing example, as\nwell as an image restoration problem in a deep dictionary framework.\n", "link": "http://arxiv.org/abs/2411.13276v3", "date": "2025-08-27", "relevancy": 1.5274, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5164}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5044}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20and%20Synthesis%20Denoisers%20for%20Forward-Backward%20Plug-and-Play%0A%20%20Algorithms&body=Title%3A%20Analysis%20and%20Synthesis%20Denoisers%20for%20Forward-Backward%20Plug-and-Play%0A%20%20Algorithms%0AAuthor%3A%20Matthieu%20Kowalski%20and%20Beno%C3%AEt%20Mal%C3%A9zieux%20and%20Thomas%20Moreau%20and%20Audrey%20Repetti%0AAbstract%3A%20%20%20In%20this%20work%20we%20study%20the%20behavior%20of%20the%20forward-backward%20%28FB%29%20algorithm%0Awhen%20the%20proximity%20operator%20is%20replaced%20by%20a%20sub-iterative%20procedure%20to%0Aapproximate%20a%20Gaussian%20denoiser%2C%20in%20a%20Plug-and-Play%20%28PnP%29%20fashion.%20In%0Aparticular%2C%20we%20consider%20both%20analysis%20and%20synthesis%20Gaussian%20denoisers%20within%20a%0Adictionary%20framework%2C%20obtained%20by%20unrolling%20dual-FB%20iterations%20or%20FB%0Aiterations%2C%20respectively.%20We%20analyze%20the%20associated%20minimization%20problems%20as%0Awell%20as%20the%20asymptotic%20behavior%20of%20the%20resulting%20FB-PnP%20iterations.%20In%0Aparticular%2C%20we%20show%20that%20the%20synthesis%20Gaussian%20denoising%20problem%20can%20be%20viewed%0Aas%20a%20proximity%20operator.%20For%20each%20case%2C%20analysis%20and%20synthesis%2C%20we%20show%20that%0Athe%20FB-PnP%20algorithms%20solve%20the%20same%20problem%20whether%20we%20use%20only%20one%20or%20an%0Ainfinite%20number%20of%20sub-iteration%20to%20solve%20the%20denoising%20problem%20at%20each%0Aiteration.%20To%20this%20aim%2C%20we%20show%20that%20each%20%22one%20sub-iteration%22%20strategy%20within%0Athe%20FB-PnP%20can%20be%20interpreted%20as%20a%20primal-dual%20algorithm%20when%20a%20warm-restart%0Astrategy%20is%20used.%20We%20further%20present%20similar%20results%20when%20using%20a%20Moreau-Yosida%0Asmoothing%20of%20the%20global%20problem%2C%20for%20an%20arbitrary%20number%20of%20sub-iterations.%0AFinally%2C%20we%20provide%20numerical%20simulations%20to%20illustrate%20our%20theoretical%0Aresults.%20In%20particular%20we%20first%20consider%20a%20toy%20compressive%20sensing%20example%2C%20as%0Awell%20as%20an%20image%20restoration%20problem%20in%20a%20deep%20dictionary%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.13276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520and%2520Synthesis%2520Denoisers%2520for%2520Forward-Backward%2520Plug-and-Play%250A%2520%2520Algorithms%26entry.906535625%3DMatthieu%2520Kowalski%2520and%2520Beno%25C3%25AEt%2520Mal%25C3%25A9zieux%2520and%2520Thomas%2520Moreau%2520and%2520Audrey%2520Repetti%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520study%2520the%2520behavior%2520of%2520the%2520forward-backward%2520%2528FB%2529%2520algorithm%250Awhen%2520the%2520proximity%2520operator%2520is%2520replaced%2520by%2520a%2520sub-iterative%2520procedure%2520to%250Aapproximate%2520a%2520Gaussian%2520denoiser%252C%2520in%2520a%2520Plug-and-Play%2520%2528PnP%2529%2520fashion.%2520In%250Aparticular%252C%2520we%2520consider%2520both%2520analysis%2520and%2520synthesis%2520Gaussian%2520denoisers%2520within%2520a%250Adictionary%2520framework%252C%2520obtained%2520by%2520unrolling%2520dual-FB%2520iterations%2520or%2520FB%250Aiterations%252C%2520respectively.%2520We%2520analyze%2520the%2520associated%2520minimization%2520problems%2520as%250Awell%2520as%2520the%2520asymptotic%2520behavior%2520of%2520the%2520resulting%2520FB-PnP%2520iterations.%2520In%250Aparticular%252C%2520we%2520show%2520that%2520the%2520synthesis%2520Gaussian%2520denoising%2520problem%2520can%2520be%2520viewed%250Aas%2520a%2520proximity%2520operator.%2520For%2520each%2520case%252C%2520analysis%2520and%2520synthesis%252C%2520we%2520show%2520that%250Athe%2520FB-PnP%2520algorithms%2520solve%2520the%2520same%2520problem%2520whether%2520we%2520use%2520only%2520one%2520or%2520an%250Ainfinite%2520number%2520of%2520sub-iteration%2520to%2520solve%2520the%2520denoising%2520problem%2520at%2520each%250Aiteration.%2520To%2520this%2520aim%252C%2520we%2520show%2520that%2520each%2520%2522one%2520sub-iteration%2522%2520strategy%2520within%250Athe%2520FB-PnP%2520can%2520be%2520interpreted%2520as%2520a%2520primal-dual%2520algorithm%2520when%2520a%2520warm-restart%250Astrategy%2520is%2520used.%2520We%2520further%2520present%2520similar%2520results%2520when%2520using%2520a%2520Moreau-Yosida%250Asmoothing%2520of%2520the%2520global%2520problem%252C%2520for%2520an%2520arbitrary%2520number%2520of%2520sub-iterations.%250AFinally%252C%2520we%2520provide%2520numerical%2520simulations%2520to%2520illustrate%2520our%2520theoretical%250Aresults.%2520In%2520particular%2520we%2520first%2520consider%2520a%2520toy%2520compressive%2520sensing%2520example%252C%2520as%250Awell%2520as%2520an%2520image%2520restoration%2520problem%2520in%2520a%2520deep%2520dictionary%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.13276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20and%20Synthesis%20Denoisers%20for%20Forward-Backward%20Plug-and-Play%0A%20%20Algorithms&entry.906535625=Matthieu%20Kowalski%20and%20Beno%C3%AEt%20Mal%C3%A9zieux%20and%20Thomas%20Moreau%20and%20Audrey%20Repetti&entry.1292438233=%20%20In%20this%20work%20we%20study%20the%20behavior%20of%20the%20forward-backward%20%28FB%29%20algorithm%0Awhen%20the%20proximity%20operator%20is%20replaced%20by%20a%20sub-iterative%20procedure%20to%0Aapproximate%20a%20Gaussian%20denoiser%2C%20in%20a%20Plug-and-Play%20%28PnP%29%20fashion.%20In%0Aparticular%2C%20we%20consider%20both%20analysis%20and%20synthesis%20Gaussian%20denoisers%20within%20a%0Adictionary%20framework%2C%20obtained%20by%20unrolling%20dual-FB%20iterations%20or%20FB%0Aiterations%2C%20respectively.%20We%20analyze%20the%20associated%20minimization%20problems%20as%0Awell%20as%20the%20asymptotic%20behavior%20of%20the%20resulting%20FB-PnP%20iterations.%20In%0Aparticular%2C%20we%20show%20that%20the%20synthesis%20Gaussian%20denoising%20problem%20can%20be%20viewed%0Aas%20a%20proximity%20operator.%20For%20each%20case%2C%20analysis%20and%20synthesis%2C%20we%20show%20that%0Athe%20FB-PnP%20algorithms%20solve%20the%20same%20problem%20whether%20we%20use%20only%20one%20or%20an%0Ainfinite%20number%20of%20sub-iteration%20to%20solve%20the%20denoising%20problem%20at%20each%0Aiteration.%20To%20this%20aim%2C%20we%20show%20that%20each%20%22one%20sub-iteration%22%20strategy%20within%0Athe%20FB-PnP%20can%20be%20interpreted%20as%20a%20primal-dual%20algorithm%20when%20a%20warm-restart%0Astrategy%20is%20used.%20We%20further%20present%20similar%20results%20when%20using%20a%20Moreau-Yosida%0Asmoothing%20of%20the%20global%20problem%2C%20for%20an%20arbitrary%20number%20of%20sub-iterations.%0AFinally%2C%20we%20provide%20numerical%20simulations%20to%20illustrate%20our%20theoretical%0Aresults.%20In%20particular%20we%20first%20consider%20a%20toy%20compressive%20sensing%20example%2C%20as%0Awell%20as%20an%20image%20restoration%20problem%20in%20a%20deep%20dictionary%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.13276v3&entry.124074799=Read"},
{"title": "CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based\n  Inference", "author": "Luben M. C. Cabezas and Vagner S. Santos and Thiago R. Ramos and Pedro L. C. Rodrigues and Rafael Izbicki", "abstract": "  Current experimental scientists have been increasingly relying on\nsimulation-based inference (SBI) to invert complex non-linear models with\nintractable likelihoods. However, posterior approximations obtained with SBI\nare often miscalibrated, causing credible regions to undercover true\nparameters. We develop $\\texttt{CP4SBI}$, a model-agnostic conformal\ncalibration framework that constructs credible sets with local Bayesian\ncoverage. Our two proposed variants, namely local calibration via regression\ntrees and CDF-based calibration, enable finite-sample local coverage guarantees\nfor any scoring function, including HPD, symmetric, and quantile-based regions.\nExperiments on widely used SBI benchmarks demonstrate that our approach\nimproves the quality of uncertainty quantification for neural posterior\nestimators using both normalizing flows and score-diffusion modeling.\n", "link": "http://arxiv.org/abs/2508.17077v2", "date": "2025-08-27", "relevancy": 1.5097, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5145}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CP4SBI%3A%20Local%20Conformal%20Calibration%20of%20Credible%20Sets%20in%20Simulation-Based%0A%20%20Inference&body=Title%3A%20CP4SBI%3A%20Local%20Conformal%20Calibration%20of%20Credible%20Sets%20in%20Simulation-Based%0A%20%20Inference%0AAuthor%3A%20Luben%20M.%20C.%20Cabezas%20and%20Vagner%20S.%20Santos%20and%20Thiago%20R.%20Ramos%20and%20Pedro%20L.%20C.%20Rodrigues%20and%20Rafael%20Izbicki%0AAbstract%3A%20%20%20Current%20experimental%20scientists%20have%20been%20increasingly%20relying%20on%0Asimulation-based%20inference%20%28SBI%29%20to%20invert%20complex%20non-linear%20models%20with%0Aintractable%20likelihoods.%20However%2C%20posterior%20approximations%20obtained%20with%20SBI%0Aare%20often%20miscalibrated%2C%20causing%20credible%20regions%20to%20undercover%20true%0Aparameters.%20We%20develop%20%24%5Ctexttt%7BCP4SBI%7D%24%2C%20a%20model-agnostic%20conformal%0Acalibration%20framework%20that%20constructs%20credible%20sets%20with%20local%20Bayesian%0Acoverage.%20Our%20two%20proposed%20variants%2C%20namely%20local%20calibration%20via%20regression%0Atrees%20and%20CDF-based%20calibration%2C%20enable%20finite-sample%20local%20coverage%20guarantees%0Afor%20any%20scoring%20function%2C%20including%20HPD%2C%20symmetric%2C%20and%20quantile-based%20regions.%0AExperiments%20on%20widely%20used%20SBI%20benchmarks%20demonstrate%20that%20our%20approach%0Aimproves%20the%20quality%20of%20uncertainty%20quantification%20for%20neural%20posterior%0Aestimators%20using%20both%20normalizing%20flows%20and%20score-diffusion%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.17077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCP4SBI%253A%2520Local%2520Conformal%2520Calibration%2520of%2520Credible%2520Sets%2520in%2520Simulation-Based%250A%2520%2520Inference%26entry.906535625%3DLuben%2520M.%2520C.%2520Cabezas%2520and%2520Vagner%2520S.%2520Santos%2520and%2520Thiago%2520R.%2520Ramos%2520and%2520Pedro%2520L.%2520C.%2520Rodrigues%2520and%2520Rafael%2520Izbicki%26entry.1292438233%3D%2520%2520Current%2520experimental%2520scientists%2520have%2520been%2520increasingly%2520relying%2520on%250Asimulation-based%2520inference%2520%2528SBI%2529%2520to%2520invert%2520complex%2520non-linear%2520models%2520with%250Aintractable%2520likelihoods.%2520However%252C%2520posterior%2520approximations%2520obtained%2520with%2520SBI%250Aare%2520often%2520miscalibrated%252C%2520causing%2520credible%2520regions%2520to%2520undercover%2520true%250Aparameters.%2520We%2520develop%2520%2524%255Ctexttt%257BCP4SBI%257D%2524%252C%2520a%2520model-agnostic%2520conformal%250Acalibration%2520framework%2520that%2520constructs%2520credible%2520sets%2520with%2520local%2520Bayesian%250Acoverage.%2520Our%2520two%2520proposed%2520variants%252C%2520namely%2520local%2520calibration%2520via%2520regression%250Atrees%2520and%2520CDF-based%2520calibration%252C%2520enable%2520finite-sample%2520local%2520coverage%2520guarantees%250Afor%2520any%2520scoring%2520function%252C%2520including%2520HPD%252C%2520symmetric%252C%2520and%2520quantile-based%2520regions.%250AExperiments%2520on%2520widely%2520used%2520SBI%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%250Aimproves%2520the%2520quality%2520of%2520uncertainty%2520quantification%2520for%2520neural%2520posterior%250Aestimators%2520using%2520both%2520normalizing%2520flows%2520and%2520score-diffusion%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.17077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CP4SBI%3A%20Local%20Conformal%20Calibration%20of%20Credible%20Sets%20in%20Simulation-Based%0A%20%20Inference&entry.906535625=Luben%20M.%20C.%20Cabezas%20and%20Vagner%20S.%20Santos%20and%20Thiago%20R.%20Ramos%20and%20Pedro%20L.%20C.%20Rodrigues%20and%20Rafael%20Izbicki&entry.1292438233=%20%20Current%20experimental%20scientists%20have%20been%20increasingly%20relying%20on%0Asimulation-based%20inference%20%28SBI%29%20to%20invert%20complex%20non-linear%20models%20with%0Aintractable%20likelihoods.%20However%2C%20posterior%20approximations%20obtained%20with%20SBI%0Aare%20often%20miscalibrated%2C%20causing%20credible%20regions%20to%20undercover%20true%0Aparameters.%20We%20develop%20%24%5Ctexttt%7BCP4SBI%7D%24%2C%20a%20model-agnostic%20conformal%0Acalibration%20framework%20that%20constructs%20credible%20sets%20with%20local%20Bayesian%0Acoverage.%20Our%20two%20proposed%20variants%2C%20namely%20local%20calibration%20via%20regression%0Atrees%20and%20CDF-based%20calibration%2C%20enable%20finite-sample%20local%20coverage%20guarantees%0Afor%20any%20scoring%20function%2C%20including%20HPD%2C%20symmetric%2C%20and%20quantile-based%20regions.%0AExperiments%20on%20widely%20used%20SBI%20benchmarks%20demonstrate%20that%20our%20approach%0Aimproves%20the%20quality%20of%20uncertainty%20quantification%20for%20neural%20posterior%0Aestimators%20using%20both%20normalizing%20flows%20and%20score-diffusion%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.17077v2&entry.124074799=Read"},
{"title": "Time-Aware One Step Diffusion Network for Real-World Image\n  Super-Resolution", "author": "Tainyi Zhang and Zheng-Peng Duan and Peng-Tao Jiang and Bo Li and Ming-Ming Cheng and Chun-Le Guo and Chongyi Li", "abstract": "  Diffusion-based real-world image super-resolution (Real-ISR) methods have\ndemonstrated impressive performance. To achieve efficient Real-ISR, many works\nemploy Variational Score Distillation (VSD) to distill pre-trained\nstable-diffusion (SD) model for one-step SR with a fixed timestep. However, due\nto the different noise injection timesteps, the SD will perform different\ngenerative priors. Therefore, a fixed timestep is difficult for these methods\nto fully leverage the generative priors in SD, leading to suboptimal\nperformance. To address this, we propose a Time-Aware one-step Diffusion\nNetwork for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder,\nwhich projects the same image into different latent features based on\ntimesteps. Through joint dynamic variation of timesteps and latent features,\nthe student model can better align with the input pattern distribution of the\npre-trained SD, thereby enabling more effective utilization of SD's generative\ncapabilities. To better activate the generative prior of SD at different\ntimesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the\nstudent model and those of the teacher model, thereby producing more consistent\ngenerative prior guidance conditioned on timesteps. Additionally, though\nutilizing the generative prior in SD at different timesteps, our method can\nnaturally achieve controllable trade-offs between fidelity and realism by\nchanging the timestep condition. Experimental results demonstrate that our\nmethod achieves both state-of-the-art performance and controllable SR results\nwith only a single step.\n", "link": "http://arxiv.org/abs/2508.16557v2", "date": "2025-08-27", "relevancy": 1.8685, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7027}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6105}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Aware%20One%20Step%20Diffusion%20Network%20for%20Real-World%20Image%0A%20%20Super-Resolution&body=Title%3A%20Time-Aware%20One%20Step%20Diffusion%20Network%20for%20Real-World%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Tainyi%20Zhang%20and%20Zheng-Peng%20Duan%20and%20Peng-Tao%20Jiang%20and%20Bo%20Li%20and%20Ming-Ming%20Cheng%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li%0AAbstract%3A%20%20%20Diffusion-based%20real-world%20image%20super-resolution%20%28Real-ISR%29%20methods%20have%0Ademonstrated%20impressive%20performance.%20To%20achieve%20efficient%20Real-ISR%2C%20many%20works%0Aemploy%20Variational%20Score%20Distillation%20%28VSD%29%20to%20distill%20pre-trained%0Astable-diffusion%20%28SD%29%20model%20for%20one-step%20SR%20with%20a%20fixed%20timestep.%20However%2C%20due%0Ato%20the%20different%20noise%20injection%20timesteps%2C%20the%20SD%20will%20perform%20different%0Agenerative%20priors.%20Therefore%2C%20a%20fixed%20timestep%20is%20difficult%20for%20these%20methods%0Ato%20fully%20leverage%20the%20generative%20priors%20in%20SD%2C%20leading%20to%20suboptimal%0Aperformance.%20To%20address%20this%2C%20we%20propose%20a%20Time-Aware%20one-step%20Diffusion%0ANetwork%20for%20Real-ISR%20%28TADSR%29.%20We%20first%20introduce%20a%20Time-Aware%20VAE%20Encoder%2C%0Awhich%20projects%20the%20same%20image%20into%20different%20latent%20features%20based%20on%0Atimesteps.%20Through%20joint%20dynamic%20variation%20of%20timesteps%20and%20latent%20features%2C%0Athe%20student%20model%20can%20better%20align%20with%20the%20input%20pattern%20distribution%20of%20the%0Apre-trained%20SD%2C%20thereby%20enabling%20more%20effective%20utilization%20of%20SD%27s%20generative%0Acapabilities.%20To%20better%20activate%20the%20generative%20prior%20of%20SD%20at%20different%0Atimesteps%2C%20we%20propose%20a%20Time-Aware%20VSD%20loss%20that%20bridges%20the%20timesteps%20of%20the%0Astudent%20model%20and%20those%20of%20the%20teacher%20model%2C%20thereby%20producing%20more%20consistent%0Agenerative%20prior%20guidance%20conditioned%20on%20timesteps.%20Additionally%2C%20though%0Autilizing%20the%20generative%20prior%20in%20SD%20at%20different%20timesteps%2C%20our%20method%20can%0Anaturally%20achieve%20controllable%20trade-offs%20between%20fidelity%20and%20realism%20by%0Achanging%20the%20timestep%20condition.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20both%20state-of-the-art%20performance%20and%20controllable%20SR%20results%0Awith%20only%20a%20single%20step.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.16557v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Aware%2520One%2520Step%2520Diffusion%2520Network%2520for%2520Real-World%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DTainyi%2520Zhang%2520and%2520Zheng-Peng%2520Duan%2520and%2520Peng-Tao%2520Jiang%2520and%2520Bo%2520Li%2520and%2520Ming-Ming%2520Cheng%2520and%2520Chun-Le%2520Guo%2520and%2520Chongyi%2520Li%26entry.1292438233%3D%2520%2520Diffusion-based%2520real-world%2520image%2520super-resolution%2520%2528Real-ISR%2529%2520methods%2520have%250Ademonstrated%2520impressive%2520performance.%2520To%2520achieve%2520efficient%2520Real-ISR%252C%2520many%2520works%250Aemploy%2520Variational%2520Score%2520Distillation%2520%2528VSD%2529%2520to%2520distill%2520pre-trained%250Astable-diffusion%2520%2528SD%2529%2520model%2520for%2520one-step%2520SR%2520with%2520a%2520fixed%2520timestep.%2520However%252C%2520due%250Ato%2520the%2520different%2520noise%2520injection%2520timesteps%252C%2520the%2520SD%2520will%2520perform%2520different%250Agenerative%2520priors.%2520Therefore%252C%2520a%2520fixed%2520timestep%2520is%2520difficult%2520for%2520these%2520methods%250Ato%2520fully%2520leverage%2520the%2520generative%2520priors%2520in%2520SD%252C%2520leading%2520to%2520suboptimal%250Aperformance.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520Time-Aware%2520one-step%2520Diffusion%250ANetwork%2520for%2520Real-ISR%2520%2528TADSR%2529.%2520We%2520first%2520introduce%2520a%2520Time-Aware%2520VAE%2520Encoder%252C%250Awhich%2520projects%2520the%2520same%2520image%2520into%2520different%2520latent%2520features%2520based%2520on%250Atimesteps.%2520Through%2520joint%2520dynamic%2520variation%2520of%2520timesteps%2520and%2520latent%2520features%252C%250Athe%2520student%2520model%2520can%2520better%2520align%2520with%2520the%2520input%2520pattern%2520distribution%2520of%2520the%250Apre-trained%2520SD%252C%2520thereby%2520enabling%2520more%2520effective%2520utilization%2520of%2520SD%2527s%2520generative%250Acapabilities.%2520To%2520better%2520activate%2520the%2520generative%2520prior%2520of%2520SD%2520at%2520different%250Atimesteps%252C%2520we%2520propose%2520a%2520Time-Aware%2520VSD%2520loss%2520that%2520bridges%2520the%2520timesteps%2520of%2520the%250Astudent%2520model%2520and%2520those%2520of%2520the%2520teacher%2520model%252C%2520thereby%2520producing%2520more%2520consistent%250Agenerative%2520prior%2520guidance%2520conditioned%2520on%2520timesteps.%2520Additionally%252C%2520though%250Autilizing%2520the%2520generative%2520prior%2520in%2520SD%2520at%2520different%2520timesteps%252C%2520our%2520method%2520can%250Anaturally%2520achieve%2520controllable%2520trade-offs%2520between%2520fidelity%2520and%2520realism%2520by%250Achanging%2520the%2520timestep%2520condition.%2520Experimental%2520results%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520both%2520state-of-the-art%2520performance%2520and%2520controllable%2520SR%2520results%250Awith%2520only%2520a%2520single%2520step.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16557v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Aware%20One%20Step%20Diffusion%20Network%20for%20Real-World%20Image%0A%20%20Super-Resolution&entry.906535625=Tainyi%20Zhang%20and%20Zheng-Peng%20Duan%20and%20Peng-Tao%20Jiang%20and%20Bo%20Li%20and%20Ming-Ming%20Cheng%20and%20Chun-Le%20Guo%20and%20Chongyi%20Li&entry.1292438233=%20%20Diffusion-based%20real-world%20image%20super-resolution%20%28Real-ISR%29%20methods%20have%0Ademonstrated%20impressive%20performance.%20To%20achieve%20efficient%20Real-ISR%2C%20many%20works%0Aemploy%20Variational%20Score%20Distillation%20%28VSD%29%20to%20distill%20pre-trained%0Astable-diffusion%20%28SD%29%20model%20for%20one-step%20SR%20with%20a%20fixed%20timestep.%20However%2C%20due%0Ato%20the%20different%20noise%20injection%20timesteps%2C%20the%20SD%20will%20perform%20different%0Agenerative%20priors.%20Therefore%2C%20a%20fixed%20timestep%20is%20difficult%20for%20these%20methods%0Ato%20fully%20leverage%20the%20generative%20priors%20in%20SD%2C%20leading%20to%20suboptimal%0Aperformance.%20To%20address%20this%2C%20we%20propose%20a%20Time-Aware%20one-step%20Diffusion%0ANetwork%20for%20Real-ISR%20%28TADSR%29.%20We%20first%20introduce%20a%20Time-Aware%20VAE%20Encoder%2C%0Awhich%20projects%20the%20same%20image%20into%20different%20latent%20features%20based%20on%0Atimesteps.%20Through%20joint%20dynamic%20variation%20of%20timesteps%20and%20latent%20features%2C%0Athe%20student%20model%20can%20better%20align%20with%20the%20input%20pattern%20distribution%20of%20the%0Apre-trained%20SD%2C%20thereby%20enabling%20more%20effective%20utilization%20of%20SD%27s%20generative%0Acapabilities.%20To%20better%20activate%20the%20generative%20prior%20of%20SD%20at%20different%0Atimesteps%2C%20we%20propose%20a%20Time-Aware%20VSD%20loss%20that%20bridges%20the%20timesteps%20of%20the%0Astudent%20model%20and%20those%20of%20the%20teacher%20model%2C%20thereby%20producing%20more%20consistent%0Agenerative%20prior%20guidance%20conditioned%20on%20timesteps.%20Additionally%2C%20though%0Autilizing%20the%20generative%20prior%20in%20SD%20at%20different%20timesteps%2C%20our%20method%20can%0Anaturally%20achieve%20controllable%20trade-offs%20between%20fidelity%20and%20realism%20by%0Achanging%20the%20timestep%20condition.%20Experimental%20results%20demonstrate%20that%20our%0Amethod%20achieves%20both%20state-of-the-art%20performance%20and%20controllable%20SR%20results%0Awith%20only%20a%20single%20step.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.16557v2&entry.124074799=Read"},
{"title": "GegenNet: Spectral Convolutional Neural Networks for Link Sign\n  Prediction in Signed Bipartite Graphs", "author": "Hewen Wang and Renchi Yang and Xiaokui Xiao", "abstract": "  Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,\nthe goal of link sign prediction is to predict the signs of potential links\nconnecting U and V based on known positive and negative edges in G. The\nmajority of existing solutions towards link sign prediction mainly focus on\nunipartite signed graphs, which are sub-optimal due to the neglect of node\nheterogeneity and unique bipartite characteristics of SBGs. To this end, recent\nstudies adapt graph neural networks to SBGs by introducing message-passing\nschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node\npairs. However, the fundamental spectral convolutional operators were\noriginally designed for positive links in unsigned graphs, and thus, are not\noptimal for inferring missing positive or negative links from known ones in\nSBGs.\n  Motivated by this, this paper proposes GegenNet, a novel and effective\nspectral convolutional neural network model for link sign prediction in SBGs.\nIn particular, GegenNet achieves enhanced model capacity and high predictive\naccuracy through three main technical contributions: (i) fast and theoretically\ngrounded spectral decomposition techniques for node feature initialization;\n(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and\n(iii) multi-layer sign-aware spectral convolutional networks alternating\nGegenbauer polynomial filters with positive and negative edges. Our extensive\nempirical studies reveal that GegenNet can achieve significantly superior\nperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign\nprediction compared to 11 strong competitors over 6 benchmark SBG datasets.\n", "link": "http://arxiv.org/abs/2508.19907v1", "date": "2025-08-27", "relevancy": 1.9532, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GegenNet%3A%20Spectral%20Convolutional%20Neural%20Networks%20for%20Link%20Sign%0A%20%20Prediction%20in%20Signed%20Bipartite%20Graphs&body=Title%3A%20GegenNet%3A%20Spectral%20Convolutional%20Neural%20Networks%20for%20Link%20Sign%0A%20%20Prediction%20in%20Signed%20Bipartite%20Graphs%0AAuthor%3A%20Hewen%20Wang%20and%20Renchi%20Yang%20and%20Xiaokui%20Xiao%0AAbstract%3A%20%20%20Given%20a%20signed%20bipartite%20graph%20%28SBG%29%20G%20with%20two%20disjoint%20node%20sets%20U%20and%20V%2C%0Athe%20goal%20of%20link%20sign%20prediction%20is%20to%20predict%20the%20signs%20of%20potential%20links%0Aconnecting%20U%20and%20V%20based%20on%20known%20positive%20and%20negative%20edges%20in%20G.%20The%0Amajority%20of%20existing%20solutions%20towards%20link%20sign%20prediction%20mainly%20focus%20on%0Aunipartite%20signed%20graphs%2C%20which%20are%20sub-optimal%20due%20to%20the%20neglect%20of%20node%0Aheterogeneity%20and%20unique%20bipartite%20characteristics%20of%20SBGs.%20To%20this%20end%2C%20recent%0Astudies%20adapt%20graph%20neural%20networks%20to%20SBGs%20by%20introducing%20message-passing%0Aschemes%20for%20both%20inter-partition%20%28UxV%29%20and%20intra-partition%20%28UxU%20or%20VxV%29%20node%0Apairs.%20However%2C%20the%20fundamental%20spectral%20convolutional%20operators%20were%0Aoriginally%20designed%20for%20positive%20links%20in%20unsigned%20graphs%2C%20and%20thus%2C%20are%20not%0Aoptimal%20for%20inferring%20missing%20positive%20or%20negative%20links%20from%20known%20ones%20in%0ASBGs.%0A%20%20Motivated%20by%20this%2C%20this%20paper%20proposes%20GegenNet%2C%20a%20novel%20and%20effective%0Aspectral%20convolutional%20neural%20network%20model%20for%20link%20sign%20prediction%20in%20SBGs.%0AIn%20particular%2C%20GegenNet%20achieves%20enhanced%20model%20capacity%20and%20high%20predictive%0Aaccuracy%20through%20three%20main%20technical%20contributions%3A%20%28i%29%20fast%20and%20theoretically%0Agrounded%20spectral%20decomposition%20techniques%20for%20node%20feature%20initialization%3B%0A%28ii%29%20a%20new%20spectral%20graph%20filter%20based%20on%20the%20Gegenbauer%20polynomial%20basis%3B%20and%0A%28iii%29%20multi-layer%20sign-aware%20spectral%20convolutional%20networks%20alternating%0AGegenbauer%20polynomial%20filters%20with%20positive%20and%20negative%20edges.%20Our%20extensive%0Aempirical%20studies%20reveal%20that%20GegenNet%20can%20achieve%20significantly%20superior%0Aperformance%20%28up%20to%20a%20gain%20of%204.28%25%20in%20AUC%20and%2011.69%25%20in%20F1%29%20in%20link%20sign%0Aprediction%20compared%20to%2011%20strong%20competitors%20over%206%20benchmark%20SBG%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.19907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGegenNet%253A%2520Spectral%2520Convolutional%2520Neural%2520Networks%2520for%2520Link%2520Sign%250A%2520%2520Prediction%2520in%2520Signed%2520Bipartite%2520Graphs%26entry.906535625%3DHewen%2520Wang%2520and%2520Renchi%2520Yang%2520and%2520Xiaokui%2520Xiao%26entry.1292438233%3D%2520%2520Given%2520a%2520signed%2520bipartite%2520graph%2520%2528SBG%2529%2520G%2520with%2520two%2520disjoint%2520node%2520sets%2520U%2520and%2520V%252C%250Athe%2520goal%2520of%2520link%2520sign%2520prediction%2520is%2520to%2520predict%2520the%2520signs%2520of%2520potential%2520links%250Aconnecting%2520U%2520and%2520V%2520based%2520on%2520known%2520positive%2520and%2520negative%2520edges%2520in%2520G.%2520The%250Amajority%2520of%2520existing%2520solutions%2520towards%2520link%2520sign%2520prediction%2520mainly%2520focus%2520on%250Aunipartite%2520signed%2520graphs%252C%2520which%2520are%2520sub-optimal%2520due%2520to%2520the%2520neglect%2520of%2520node%250Aheterogeneity%2520and%2520unique%2520bipartite%2520characteristics%2520of%2520SBGs.%2520To%2520this%2520end%252C%2520recent%250Astudies%2520adapt%2520graph%2520neural%2520networks%2520to%2520SBGs%2520by%2520introducing%2520message-passing%250Aschemes%2520for%2520both%2520inter-partition%2520%2528UxV%2529%2520and%2520intra-partition%2520%2528UxU%2520or%2520VxV%2529%2520node%250Apairs.%2520However%252C%2520the%2520fundamental%2520spectral%2520convolutional%2520operators%2520were%250Aoriginally%2520designed%2520for%2520positive%2520links%2520in%2520unsigned%2520graphs%252C%2520and%2520thus%252C%2520are%2520not%250Aoptimal%2520for%2520inferring%2520missing%2520positive%2520or%2520negative%2520links%2520from%2520known%2520ones%2520in%250ASBGs.%250A%2520%2520Motivated%2520by%2520this%252C%2520this%2520paper%2520proposes%2520GegenNet%252C%2520a%2520novel%2520and%2520effective%250Aspectral%2520convolutional%2520neural%2520network%2520model%2520for%2520link%2520sign%2520prediction%2520in%2520SBGs.%250AIn%2520particular%252C%2520GegenNet%2520achieves%2520enhanced%2520model%2520capacity%2520and%2520high%2520predictive%250Aaccuracy%2520through%2520three%2520main%2520technical%2520contributions%253A%2520%2528i%2529%2520fast%2520and%2520theoretically%250Agrounded%2520spectral%2520decomposition%2520techniques%2520for%2520node%2520feature%2520initialization%253B%250A%2528ii%2529%2520a%2520new%2520spectral%2520graph%2520filter%2520based%2520on%2520the%2520Gegenbauer%2520polynomial%2520basis%253B%2520and%250A%2528iii%2529%2520multi-layer%2520sign-aware%2520spectral%2520convolutional%2520networks%2520alternating%250AGegenbauer%2520polynomial%2520filters%2520with%2520positive%2520and%2520negative%2520edges.%2520Our%2520extensive%250Aempirical%2520studies%2520reveal%2520that%2520GegenNet%2520can%2520achieve%2520significantly%2520superior%250Aperformance%2520%2528up%2520to%2520a%2520gain%2520of%25204.28%2525%2520in%2520AUC%2520and%252011.69%2525%2520in%2520F1%2529%2520in%2520link%2520sign%250Aprediction%2520compared%2520to%252011%2520strong%2520competitors%2520over%25206%2520benchmark%2520SBG%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.19907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GegenNet%3A%20Spectral%20Convolutional%20Neural%20Networks%20for%20Link%20Sign%0A%20%20Prediction%20in%20Signed%20Bipartite%20Graphs&entry.906535625=Hewen%20Wang%20and%20Renchi%20Yang%20and%20Xiaokui%20Xiao&entry.1292438233=%20%20Given%20a%20signed%20bipartite%20graph%20%28SBG%29%20G%20with%20two%20disjoint%20node%20sets%20U%20and%20V%2C%0Athe%20goal%20of%20link%20sign%20prediction%20is%20to%20predict%20the%20signs%20of%20potential%20links%0Aconnecting%20U%20and%20V%20based%20on%20known%20positive%20and%20negative%20edges%20in%20G.%20The%0Amajority%20of%20existing%20solutions%20towards%20link%20sign%20prediction%20mainly%20focus%20on%0Aunipartite%20signed%20graphs%2C%20which%20are%20sub-optimal%20due%20to%20the%20neglect%20of%20node%0Aheterogeneity%20and%20unique%20bipartite%20characteristics%20of%20SBGs.%20To%20this%20end%2C%20recent%0Astudies%20adapt%20graph%20neural%20networks%20to%20SBGs%20by%20introducing%20message-passing%0Aschemes%20for%20both%20inter-partition%20%28UxV%29%20and%20intra-partition%20%28UxU%20or%20VxV%29%20node%0Apairs.%20However%2C%20the%20fundamental%20spectral%20convolutional%20operators%20were%0Aoriginally%20designed%20for%20positive%20links%20in%20unsigned%20graphs%2C%20and%20thus%2C%20are%20not%0Aoptimal%20for%20inferring%20missing%20positive%20or%20negative%20links%20from%20known%20ones%20in%0ASBGs.%0A%20%20Motivated%20by%20this%2C%20this%20paper%20proposes%20GegenNet%2C%20a%20novel%20and%20effective%0Aspectral%20convolutional%20neural%20network%20model%20for%20link%20sign%20prediction%20in%20SBGs.%0AIn%20particular%2C%20GegenNet%20achieves%20enhanced%20model%20capacity%20and%20high%20predictive%0Aaccuracy%20through%20three%20main%20technical%20contributions%3A%20%28i%29%20fast%20and%20theoretically%0Agrounded%20spectral%20decomposition%20techniques%20for%20node%20feature%20initialization%3B%0A%28ii%29%20a%20new%20spectral%20graph%20filter%20based%20on%20the%20Gegenbauer%20polynomial%20basis%3B%20and%0A%28iii%29%20multi-layer%20sign-aware%20spectral%20convolutional%20networks%20alternating%0AGegenbauer%20polynomial%20filters%20with%20positive%20and%20negative%20edges.%20Our%20extensive%0Aempirical%20studies%20reveal%20that%20GegenNet%20can%20achieve%20significantly%20superior%0Aperformance%20%28up%20to%20a%20gain%20of%204.28%25%20in%20AUC%20and%2011.69%25%20in%20F1%29%20in%20link%20sign%0Aprediction%20compared%20to%2011%20strong%20competitors%20over%206%20benchmark%20SBG%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.19907v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


