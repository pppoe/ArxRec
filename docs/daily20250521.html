<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250520.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Personalize Your Gaussian: Consistent 3D Scene Personalization from a\n  Single Image", "author": "Yuxuan Wang and Xuanyu Yi and Qingshan Xu and Yuan Zhou and Long Chen and Hanwang Zhang", "abstract": "  Personalizing 3D scenes from a single reference image enables intuitive\nuser-guided editing, which requires achieving both multi-view consistency\nacross perspectives and referential consistency with the input image. However,\nthese goals are particularly challenging due to the viewpoint bias caused by\nthe limited perspective provided in a single image. Lacking the mechanisms to\neffectively expand reference information beyond the original view, existing\nmethods of image-conditioned 3DGS personalization often suffer from this\nviewpoint bias and struggle to produce consistent results. Therefore, in this\npaper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),\na framework that progressively propagates the single-view reference appearance\nto novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D\ngeneration and iterative LoRA fine-tuning to extract and extend the reference\nappearance, and finally produces faithful multi-view guidance images and the\npersonalized 3DGS outputs through a view-consistent generation process guided\nby geometric cues. Extensive experiments on real-world scenes show that our\nCP-GS effectively mitigates the viewpoint bias, achieving high-quality\npersonalization that significantly outperforms existing methods. The code will\nbe released at https://github.com/Yuxuan-W/CP-GS.\n", "link": "http://arxiv.org/abs/2505.14537v1", "date": "2025-05-20", "relevancy": 3.3393, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6971}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6678}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalize%20Your%20Gaussian%3A%20Consistent%203D%20Scene%20Personalization%20from%20a%0A%20%20Single%20Image&body=Title%3A%20Personalize%20Your%20Gaussian%3A%20Consistent%203D%20Scene%20Personalization%20from%20a%0A%20%20Single%20Image%0AAuthor%3A%20Yuxuan%20Wang%20and%20Xuanyu%20Yi%20and%20Qingshan%20Xu%20and%20Yuan%20Zhou%20and%20Long%20Chen%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Personalizing%203D%20scenes%20from%20a%20single%20reference%20image%20enables%20intuitive%0Auser-guided%20editing%2C%20which%20requires%20achieving%20both%20multi-view%20consistency%0Aacross%20perspectives%20and%20referential%20consistency%20with%20the%20input%20image.%20However%2C%0Athese%20goals%20are%20particularly%20challenging%20due%20to%20the%20viewpoint%20bias%20caused%20by%0Athe%20limited%20perspective%20provided%20in%20a%20single%20image.%20Lacking%20the%20mechanisms%20to%0Aeffectively%20expand%20reference%20information%20beyond%20the%20original%20view%2C%20existing%0Amethods%20of%20image-conditioned%203DGS%20personalization%20often%20suffer%20from%20this%0Aviewpoint%20bias%20and%20struggle%20to%20produce%20consistent%20results.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20present%20Consistent%20Personalization%20for%203D%20Gaussian%20Splatting%20%28CP-GS%29%2C%0Aa%20framework%20that%20progressively%20propagates%20the%20single-view%20reference%20appearance%0Ato%20novel%20perspectives.%20In%20particular%2C%20CP-GS%20integrates%20pre-trained%20image-to-3D%0Ageneration%20and%20iterative%20LoRA%20fine-tuning%20to%20extract%20and%20extend%20the%20reference%0Aappearance%2C%20and%20finally%20produces%20faithful%20multi-view%20guidance%20images%20and%20the%0Apersonalized%203DGS%20outputs%20through%20a%20view-consistent%20generation%20process%20guided%0Aby%20geometric%20cues.%20Extensive%20experiments%20on%20real-world%20scenes%20show%20that%20our%0ACP-GS%20effectively%20mitigates%20the%20viewpoint%20bias%2C%20achieving%20high-quality%0Apersonalization%20that%20significantly%20outperforms%20existing%20methods.%20The%20code%20will%0Abe%20released%20at%20https%3A//github.com/Yuxuan-W/CP-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalize%2520Your%2520Gaussian%253A%2520Consistent%25203D%2520Scene%2520Personalization%2520from%2520a%250A%2520%2520Single%2520Image%26entry.906535625%3DYuxuan%2520Wang%2520and%2520Xuanyu%2520Yi%2520and%2520Qingshan%2520Xu%2520and%2520Yuan%2520Zhou%2520and%2520Long%2520Chen%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520Personalizing%25203D%2520scenes%2520from%2520a%2520single%2520reference%2520image%2520enables%2520intuitive%250Auser-guided%2520editing%252C%2520which%2520requires%2520achieving%2520both%2520multi-view%2520consistency%250Aacross%2520perspectives%2520and%2520referential%2520consistency%2520with%2520the%2520input%2520image.%2520However%252C%250Athese%2520goals%2520are%2520particularly%2520challenging%2520due%2520to%2520the%2520viewpoint%2520bias%2520caused%2520by%250Athe%2520limited%2520perspective%2520provided%2520in%2520a%2520single%2520image.%2520Lacking%2520the%2520mechanisms%2520to%250Aeffectively%2520expand%2520reference%2520information%2520beyond%2520the%2520original%2520view%252C%2520existing%250Amethods%2520of%2520image-conditioned%25203DGS%2520personalization%2520often%2520suffer%2520from%2520this%250Aviewpoint%2520bias%2520and%2520struggle%2520to%2520produce%2520consistent%2520results.%2520Therefore%252C%2520in%2520this%250Apaper%252C%2520we%2520present%2520Consistent%2520Personalization%2520for%25203D%2520Gaussian%2520Splatting%2520%2528CP-GS%2529%252C%250Aa%2520framework%2520that%2520progressively%2520propagates%2520the%2520single-view%2520reference%2520appearance%250Ato%2520novel%2520perspectives.%2520In%2520particular%252C%2520CP-GS%2520integrates%2520pre-trained%2520image-to-3D%250Ageneration%2520and%2520iterative%2520LoRA%2520fine-tuning%2520to%2520extract%2520and%2520extend%2520the%2520reference%250Aappearance%252C%2520and%2520finally%2520produces%2520faithful%2520multi-view%2520guidance%2520images%2520and%2520the%250Apersonalized%25203DGS%2520outputs%2520through%2520a%2520view-consistent%2520generation%2520process%2520guided%250Aby%2520geometric%2520cues.%2520Extensive%2520experiments%2520on%2520real-world%2520scenes%2520show%2520that%2520our%250ACP-GS%2520effectively%2520mitigates%2520the%2520viewpoint%2520bias%252C%2520achieving%2520high-quality%250Apersonalization%2520that%2520significantly%2520outperforms%2520existing%2520methods.%2520The%2520code%2520will%250Abe%2520released%2520at%2520https%253A//github.com/Yuxuan-W/CP-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalize%20Your%20Gaussian%3A%20Consistent%203D%20Scene%20Personalization%20from%20a%0A%20%20Single%20Image&entry.906535625=Yuxuan%20Wang%20and%20Xuanyu%20Yi%20and%20Qingshan%20Xu%20and%20Yuan%20Zhou%20and%20Long%20Chen%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Personalizing%203D%20scenes%20from%20a%20single%20reference%20image%20enables%20intuitive%0Auser-guided%20editing%2C%20which%20requires%20achieving%20both%20multi-view%20consistency%0Aacross%20perspectives%20and%20referential%20consistency%20with%20the%20input%20image.%20However%2C%0Athese%20goals%20are%20particularly%20challenging%20due%20to%20the%20viewpoint%20bias%20caused%20by%0Athe%20limited%20perspective%20provided%20in%20a%20single%20image.%20Lacking%20the%20mechanisms%20to%0Aeffectively%20expand%20reference%20information%20beyond%20the%20original%20view%2C%20existing%0Amethods%20of%20image-conditioned%203DGS%20personalization%20often%20suffer%20from%20this%0Aviewpoint%20bias%20and%20struggle%20to%20produce%20consistent%20results.%20Therefore%2C%20in%20this%0Apaper%2C%20we%20present%20Consistent%20Personalization%20for%203D%20Gaussian%20Splatting%20%28CP-GS%29%2C%0Aa%20framework%20that%20progressively%20propagates%20the%20single-view%20reference%20appearance%0Ato%20novel%20perspectives.%20In%20particular%2C%20CP-GS%20integrates%20pre-trained%20image-to-3D%0Ageneration%20and%20iterative%20LoRA%20fine-tuning%20to%20extract%20and%20extend%20the%20reference%0Aappearance%2C%20and%20finally%20produces%20faithful%20multi-view%20guidance%20images%20and%20the%0Apersonalized%203DGS%20outputs%20through%20a%20view-consistent%20generation%20process%20guided%0Aby%20geometric%20cues.%20Extensive%20experiments%20on%20real-world%20scenes%20show%20that%20our%0ACP-GS%20effectively%20mitigates%20the%20viewpoint%20bias%2C%20achieving%20high-quality%0Apersonalization%20that%20significantly%20outperforms%20existing%20methods.%20The%20code%20will%0Abe%20released%20at%20https%3A//github.com/Yuxuan-W/CP-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14537v1&entry.124074799=Read"},
{"title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and\n  Perspectives", "author": "Xingxing Weng and Chao Pang and Gui-Song Xia", "abstract": "  Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.\n", "link": "http://arxiv.org/abs/2505.14361v1", "date": "2025-05-20", "relevancy": 3.1091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6461}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Modeling%20Meets%20Remote%20Sensing%3A%20Models%2C%20Datasets%20and%0A%20%20Perspectives&body=Title%3A%20Vision-Language%20Modeling%20Meets%20Remote%20Sensing%3A%20Models%2C%20Datasets%20and%0A%20%20Perspectives%0AAuthor%3A%20Xingxing%20Weng%20and%20Chao%20Pang%20and%20Gui-Song%20Xia%0AAbstract%3A%20%20%20Vision-language%20modeling%20%28VLM%29%20aims%20to%20bridge%20the%20information%20gap%20between%0Aimages%20and%20natural%20language.%20Under%20the%20new%20paradigm%20of%20first%20pre-training%20on%0Amassive%20image-text%20pairs%20and%20then%20fine-tuning%20on%20task-specific%20data%2C%20VLM%20in%20the%0Aremote%20sensing%20domain%20has%20made%20significant%20progress.%20The%20resulting%20models%0Abenefit%20from%20the%20absorption%20of%20extensive%20general%20knowledge%20and%20demonstrate%0Astrong%20performance%20across%20a%20variety%20of%20remote%20sensing%20data%20analysis%20tasks.%0AMoreover%2C%20they%20are%20capable%20of%20interacting%20with%20users%20in%20a%20conversational%0Amanner.%20In%20this%20paper%2C%20we%20aim%20to%20provide%20the%20remote%20sensing%20community%20with%20a%0Atimely%20and%20comprehensive%20review%20of%20the%20developments%20in%20VLM%20using%20the%20two-stage%0Aparadigm.%20Specifically%2C%20we%20first%20cover%20a%20taxonomy%20of%20VLM%20in%20remote%20sensing%3A%0Acontrastive%20learning%2C%20visual%20instruction%20tuning%2C%20and%20text-conditioned%20image%0Ageneration.%20For%20each%20category%2C%20we%20detail%20the%20commonly%20used%20network%20architecture%0Aand%20pre-training%20objectives.%20Second%2C%20we%20conduct%20a%20thorough%20review%20of%20existing%0Aworks%2C%20examining%20foundation%20models%20and%20task-specific%20adaptation%20methods%20in%0Acontrastive-based%20VLM%2C%20architectural%20upgrades%2C%20training%20strategies%20and%20model%0Acapabilities%20in%20instruction-based%20VLM%2C%20as%20well%20as%20generative%20foundation%20models%0Awith%20their%20representative%20downstream%20applications.%20Third%2C%20we%20summarize%20datasets%0Aused%20for%20VLM%20pre-training%2C%20fine-tuning%2C%20and%20evaluation%2C%20with%20an%20analysis%20of%0Atheir%20construction%20methodologies%20%28including%20image%20sources%20and%20caption%0Ageneration%29%20and%20key%20properties%2C%20such%20as%20scale%20and%20task%20adaptability.%20Finally%2C%0Awe%20conclude%20this%20survey%20with%20insights%20and%20discussions%20on%20future%20research%0Adirections%3A%20cross-modal%20representation%20alignment%2C%20vague%20requirement%0Acomprehension%2C%20explanation-driven%20model%20reliability%2C%20continually%20scalable%20model%0Acapabilities%2C%20and%20large-scale%20datasets%20featuring%20richer%20modalities%20and%20greater%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14361v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Modeling%2520Meets%2520Remote%2520Sensing%253A%2520Models%252C%2520Datasets%2520and%250A%2520%2520Perspectives%26entry.906535625%3DXingxing%2520Weng%2520and%2520Chao%2520Pang%2520and%2520Gui-Song%2520Xia%26entry.1292438233%3D%2520%2520Vision-language%2520modeling%2520%2528VLM%2529%2520aims%2520to%2520bridge%2520the%2520information%2520gap%2520between%250Aimages%2520and%2520natural%2520language.%2520Under%2520the%2520new%2520paradigm%2520of%2520first%2520pre-training%2520on%250Amassive%2520image-text%2520pairs%2520and%2520then%2520fine-tuning%2520on%2520task-specific%2520data%252C%2520VLM%2520in%2520the%250Aremote%2520sensing%2520domain%2520has%2520made%2520significant%2520progress.%2520The%2520resulting%2520models%250Abenefit%2520from%2520the%2520absorption%2520of%2520extensive%2520general%2520knowledge%2520and%2520demonstrate%250Astrong%2520performance%2520across%2520a%2520variety%2520of%2520remote%2520sensing%2520data%2520analysis%2520tasks.%250AMoreover%252C%2520they%2520are%2520capable%2520of%2520interacting%2520with%2520users%2520in%2520a%2520conversational%250Amanner.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520provide%2520the%2520remote%2520sensing%2520community%2520with%2520a%250Atimely%2520and%2520comprehensive%2520review%2520of%2520the%2520developments%2520in%2520VLM%2520using%2520the%2520two-stage%250Aparadigm.%2520Specifically%252C%2520we%2520first%2520cover%2520a%2520taxonomy%2520of%2520VLM%2520in%2520remote%2520sensing%253A%250Acontrastive%2520learning%252C%2520visual%2520instruction%2520tuning%252C%2520and%2520text-conditioned%2520image%250Ageneration.%2520For%2520each%2520category%252C%2520we%2520detail%2520the%2520commonly%2520used%2520network%2520architecture%250Aand%2520pre-training%2520objectives.%2520Second%252C%2520we%2520conduct%2520a%2520thorough%2520review%2520of%2520existing%250Aworks%252C%2520examining%2520foundation%2520models%2520and%2520task-specific%2520adaptation%2520methods%2520in%250Acontrastive-based%2520VLM%252C%2520architectural%2520upgrades%252C%2520training%2520strategies%2520and%2520model%250Acapabilities%2520in%2520instruction-based%2520VLM%252C%2520as%2520well%2520as%2520generative%2520foundation%2520models%250Awith%2520their%2520representative%2520downstream%2520applications.%2520Third%252C%2520we%2520summarize%2520datasets%250Aused%2520for%2520VLM%2520pre-training%252C%2520fine-tuning%252C%2520and%2520evaluation%252C%2520with%2520an%2520analysis%2520of%250Atheir%2520construction%2520methodologies%2520%2528including%2520image%2520sources%2520and%2520caption%250Ageneration%2529%2520and%2520key%2520properties%252C%2520such%2520as%2520scale%2520and%2520task%2520adaptability.%2520Finally%252C%250Awe%2520conclude%2520this%2520survey%2520with%2520insights%2520and%2520discussions%2520on%2520future%2520research%250Adirections%253A%2520cross-modal%2520representation%2520alignment%252C%2520vague%2520requirement%250Acomprehension%252C%2520explanation-driven%2520model%2520reliability%252C%2520continually%2520scalable%2520model%250Acapabilities%252C%2520and%2520large-scale%2520datasets%2520featuring%2520richer%2520modalities%2520and%2520greater%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14361v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Modeling%20Meets%20Remote%20Sensing%3A%20Models%2C%20Datasets%20and%0A%20%20Perspectives&entry.906535625=Xingxing%20Weng%20and%20Chao%20Pang%20and%20Gui-Song%20Xia&entry.1292438233=%20%20Vision-language%20modeling%20%28VLM%29%20aims%20to%20bridge%20the%20information%20gap%20between%0Aimages%20and%20natural%20language.%20Under%20the%20new%20paradigm%20of%20first%20pre-training%20on%0Amassive%20image-text%20pairs%20and%20then%20fine-tuning%20on%20task-specific%20data%2C%20VLM%20in%20the%0Aremote%20sensing%20domain%20has%20made%20significant%20progress.%20The%20resulting%20models%0Abenefit%20from%20the%20absorption%20of%20extensive%20general%20knowledge%20and%20demonstrate%0Astrong%20performance%20across%20a%20variety%20of%20remote%20sensing%20data%20analysis%20tasks.%0AMoreover%2C%20they%20are%20capable%20of%20interacting%20with%20users%20in%20a%20conversational%0Amanner.%20In%20this%20paper%2C%20we%20aim%20to%20provide%20the%20remote%20sensing%20community%20with%20a%0Atimely%20and%20comprehensive%20review%20of%20the%20developments%20in%20VLM%20using%20the%20two-stage%0Aparadigm.%20Specifically%2C%20we%20first%20cover%20a%20taxonomy%20of%20VLM%20in%20remote%20sensing%3A%0Acontrastive%20learning%2C%20visual%20instruction%20tuning%2C%20and%20text-conditioned%20image%0Ageneration.%20For%20each%20category%2C%20we%20detail%20the%20commonly%20used%20network%20architecture%0Aand%20pre-training%20objectives.%20Second%2C%20we%20conduct%20a%20thorough%20review%20of%20existing%0Aworks%2C%20examining%20foundation%20models%20and%20task-specific%20adaptation%20methods%20in%0Acontrastive-based%20VLM%2C%20architectural%20upgrades%2C%20training%20strategies%20and%20model%0Acapabilities%20in%20instruction-based%20VLM%2C%20as%20well%20as%20generative%20foundation%20models%0Awith%20their%20representative%20downstream%20applications.%20Third%2C%20we%20summarize%20datasets%0Aused%20for%20VLM%20pre-training%2C%20fine-tuning%2C%20and%20evaluation%2C%20with%20an%20analysis%20of%0Atheir%20construction%20methodologies%20%28including%20image%20sources%20and%20caption%0Ageneration%29%20and%20key%20properties%2C%20such%20as%20scale%20and%20task%20adaptability.%20Finally%2C%0Awe%20conclude%20this%20survey%20with%20insights%20and%20discussions%20on%20future%20research%0Adirections%3A%20cross-modal%20representation%20alignment%2C%20vague%20requirement%0Acomprehension%2C%20explanation-driven%20model%20reliability%2C%20continually%20scalable%20model%0Acapabilities%2C%20and%20large-scale%20datasets%20featuring%20richer%20modalities%20and%20greater%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14361v1&entry.124074799=Read"},
{"title": "Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation\n  Benchmark", "author": "Haidong Xu and Meishan Zhang and Hao Ju and Zhedong Zheng and Erik Cambria and Min Zhang and Hao Fei", "abstract": "  Producing emotionally dynamic 3D facial avatars with text derived from spoken\nwords (Emo3D) has been a pivotal research topic in 3D avatar generation. While\nprogress has been made in general-purpose 3D avatar generation, the exploration\nof generating emotional 3D avatars remains scarce, primarily due to the\ncomplexities of identifying and rendering rich emotions from spoken words. This\npaper reexamines Emo3D generation and draws inspiration from human processes,\nbreaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping\n(T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in\ndetermining the quality of Emo3D generation and encompasses three key\nchallenges: Expression Diversity, Emotion-Content Consistency, and Expression\nFluidity. To address these challenges, we introduce a novel benchmark to\nadvance research in Emo3D generation. First, we present EmoAva, a large-scale,\nhigh-quality dataset for T3DEM, comprising 15,000 text-to-3D expression\nmappings that characterize the aforementioned three challenges in Emo3D\ngeneration. Furthermore, we develop various metrics to effectively evaluate\nmodels against these identified challenges. Next, to effectively model the\nconsistency, diversity, and fluidity of human expressions in the T3DEM step, we\npropose the Continuous Text-to-Expression Generator, which employs an\nautoregressive Conditional Variational Autoencoder for expression code\ngeneration, enhanced with Latent Temporal Attention and Expression-wise\nAttention mechanisms. Finally, to further enhance the 3DAR step on rendering\nhigher-quality subtle expressions, we present the Globally-informed Gaussian\nAvatar (GiGA) model. GiGA incorporates a global information mechanism into 3D\nGaussian representations, enabling the capture of subtle micro-expressions and\nseamless transitions between emotional states.\n", "link": "http://arxiv.org/abs/2412.02508v2", "date": "2025-05-20", "relevancy": 3.0753, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6362}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6362}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Rich%20Emotions%20in%203D%20Avatars%3A%20A%20Text-to-3D%20Avatar%20Generation%0A%20%20Benchmark&body=Title%3A%20Towards%20Rich%20Emotions%20in%203D%20Avatars%3A%20A%20Text-to-3D%20Avatar%20Generation%0A%20%20Benchmark%0AAuthor%3A%20Haidong%20Xu%20and%20Meishan%20Zhang%20and%20Hao%20Ju%20and%20Zhedong%20Zheng%20and%20Erik%20Cambria%20and%20Min%20Zhang%20and%20Hao%20Fei%0AAbstract%3A%20%20%20Producing%20emotionally%20dynamic%203D%20facial%20avatars%20with%20text%20derived%20from%20spoken%0Awords%20%28Emo3D%29%20has%20been%20a%20pivotal%20research%20topic%20in%203D%20avatar%20generation.%20While%0Aprogress%20has%20been%20made%20in%20general-purpose%203D%20avatar%20generation%2C%20the%20exploration%0Aof%20generating%20emotional%203D%20avatars%20remains%20scarce%2C%20primarily%20due%20to%20the%0Acomplexities%20of%20identifying%20and%20rendering%20rich%20emotions%20from%20spoken%20words.%20This%0Apaper%20reexamines%20Emo3D%20generation%20and%20draws%20inspiration%20from%20human%20processes%2C%0Abreaking%20down%20Emo3D%20into%20two%20cascading%20steps%3A%20Text-to-3D%20Expression%20Mapping%0A%28T3DEM%29%20and%203D%20Avatar%20Rendering%20%283DAR%29.%20T3DEM%20is%20the%20most%20crucial%20step%20in%0Adetermining%20the%20quality%20of%20Emo3D%20generation%20and%20encompasses%20three%20key%0Achallenges%3A%20Expression%20Diversity%2C%20Emotion-Content%20Consistency%2C%20and%20Expression%0AFluidity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20benchmark%20to%0Aadvance%20research%20in%20Emo3D%20generation.%20First%2C%20we%20present%20EmoAva%2C%20a%20large-scale%2C%0Ahigh-quality%20dataset%20for%20T3DEM%2C%20comprising%2015%2C000%20text-to-3D%20expression%0Amappings%20that%20characterize%20the%20aforementioned%20three%20challenges%20in%20Emo3D%0Ageneration.%20Furthermore%2C%20we%20develop%20various%20metrics%20to%20effectively%20evaluate%0Amodels%20against%20these%20identified%20challenges.%20Next%2C%20to%20effectively%20model%20the%0Aconsistency%2C%20diversity%2C%20and%20fluidity%20of%20human%20expressions%20in%20the%20T3DEM%20step%2C%20we%0Apropose%20the%20Continuous%20Text-to-Expression%20Generator%2C%20which%20employs%20an%0Aautoregressive%20Conditional%20Variational%20Autoencoder%20for%20expression%20code%0Ageneration%2C%20enhanced%20with%20Latent%20Temporal%20Attention%20and%20Expression-wise%0AAttention%20mechanisms.%20Finally%2C%20to%20further%20enhance%20the%203DAR%20step%20on%20rendering%0Ahigher-quality%20subtle%20expressions%2C%20we%20present%20the%20Globally-informed%20Gaussian%0AAvatar%20%28GiGA%29%20model.%20GiGA%20incorporates%20a%20global%20information%20mechanism%20into%203D%0AGaussian%20representations%2C%20enabling%20the%20capture%20of%20subtle%20micro-expressions%20and%0Aseamless%20transitions%20between%20emotional%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Rich%2520Emotions%2520in%25203D%2520Avatars%253A%2520A%2520Text-to-3D%2520Avatar%2520Generation%250A%2520%2520Benchmark%26entry.906535625%3DHaidong%2520Xu%2520and%2520Meishan%2520Zhang%2520and%2520Hao%2520Ju%2520and%2520Zhedong%2520Zheng%2520and%2520Erik%2520Cambria%2520and%2520Min%2520Zhang%2520and%2520Hao%2520Fei%26entry.1292438233%3D%2520%2520Producing%2520emotionally%2520dynamic%25203D%2520facial%2520avatars%2520with%2520text%2520derived%2520from%2520spoken%250Awords%2520%2528Emo3D%2529%2520has%2520been%2520a%2520pivotal%2520research%2520topic%2520in%25203D%2520avatar%2520generation.%2520While%250Aprogress%2520has%2520been%2520made%2520in%2520general-purpose%25203D%2520avatar%2520generation%252C%2520the%2520exploration%250Aof%2520generating%2520emotional%25203D%2520avatars%2520remains%2520scarce%252C%2520primarily%2520due%2520to%2520the%250Acomplexities%2520of%2520identifying%2520and%2520rendering%2520rich%2520emotions%2520from%2520spoken%2520words.%2520This%250Apaper%2520reexamines%2520Emo3D%2520generation%2520and%2520draws%2520inspiration%2520from%2520human%2520processes%252C%250Abreaking%2520down%2520Emo3D%2520into%2520two%2520cascading%2520steps%253A%2520Text-to-3D%2520Expression%2520Mapping%250A%2528T3DEM%2529%2520and%25203D%2520Avatar%2520Rendering%2520%25283DAR%2529.%2520T3DEM%2520is%2520the%2520most%2520crucial%2520step%2520in%250Adetermining%2520the%2520quality%2520of%2520Emo3D%2520generation%2520and%2520encompasses%2520three%2520key%250Achallenges%253A%2520Expression%2520Diversity%252C%2520Emotion-Content%2520Consistency%252C%2520and%2520Expression%250AFluidity.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520benchmark%2520to%250Aadvance%2520research%2520in%2520Emo3D%2520generation.%2520First%252C%2520we%2520present%2520EmoAva%252C%2520a%2520large-scale%252C%250Ahigh-quality%2520dataset%2520for%2520T3DEM%252C%2520comprising%252015%252C000%2520text-to-3D%2520expression%250Amappings%2520that%2520characterize%2520the%2520aforementioned%2520three%2520challenges%2520in%2520Emo3D%250Ageneration.%2520Furthermore%252C%2520we%2520develop%2520various%2520metrics%2520to%2520effectively%2520evaluate%250Amodels%2520against%2520these%2520identified%2520challenges.%2520Next%252C%2520to%2520effectively%2520model%2520the%250Aconsistency%252C%2520diversity%252C%2520and%2520fluidity%2520of%2520human%2520expressions%2520in%2520the%2520T3DEM%2520step%252C%2520we%250Apropose%2520the%2520Continuous%2520Text-to-Expression%2520Generator%252C%2520which%2520employs%2520an%250Aautoregressive%2520Conditional%2520Variational%2520Autoencoder%2520for%2520expression%2520code%250Ageneration%252C%2520enhanced%2520with%2520Latent%2520Temporal%2520Attention%2520and%2520Expression-wise%250AAttention%2520mechanisms.%2520Finally%252C%2520to%2520further%2520enhance%2520the%25203DAR%2520step%2520on%2520rendering%250Ahigher-quality%2520subtle%2520expressions%252C%2520we%2520present%2520the%2520Globally-informed%2520Gaussian%250AAvatar%2520%2528GiGA%2529%2520model.%2520GiGA%2520incorporates%2520a%2520global%2520information%2520mechanism%2520into%25203D%250AGaussian%2520representations%252C%2520enabling%2520the%2520capture%2520of%2520subtle%2520micro-expressions%2520and%250Aseamless%2520transitions%2520between%2520emotional%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Rich%20Emotions%20in%203D%20Avatars%3A%20A%20Text-to-3D%20Avatar%20Generation%0A%20%20Benchmark&entry.906535625=Haidong%20Xu%20and%20Meishan%20Zhang%20and%20Hao%20Ju%20and%20Zhedong%20Zheng%20and%20Erik%20Cambria%20and%20Min%20Zhang%20and%20Hao%20Fei&entry.1292438233=%20%20Producing%20emotionally%20dynamic%203D%20facial%20avatars%20with%20text%20derived%20from%20spoken%0Awords%20%28Emo3D%29%20has%20been%20a%20pivotal%20research%20topic%20in%203D%20avatar%20generation.%20While%0Aprogress%20has%20been%20made%20in%20general-purpose%203D%20avatar%20generation%2C%20the%20exploration%0Aof%20generating%20emotional%203D%20avatars%20remains%20scarce%2C%20primarily%20due%20to%20the%0Acomplexities%20of%20identifying%20and%20rendering%20rich%20emotions%20from%20spoken%20words.%20This%0Apaper%20reexamines%20Emo3D%20generation%20and%20draws%20inspiration%20from%20human%20processes%2C%0Abreaking%20down%20Emo3D%20into%20two%20cascading%20steps%3A%20Text-to-3D%20Expression%20Mapping%0A%28T3DEM%29%20and%203D%20Avatar%20Rendering%20%283DAR%29.%20T3DEM%20is%20the%20most%20crucial%20step%20in%0Adetermining%20the%20quality%20of%20Emo3D%20generation%20and%20encompasses%20three%20key%0Achallenges%3A%20Expression%20Diversity%2C%20Emotion-Content%20Consistency%2C%20and%20Expression%0AFluidity.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20benchmark%20to%0Aadvance%20research%20in%20Emo3D%20generation.%20First%2C%20we%20present%20EmoAva%2C%20a%20large-scale%2C%0Ahigh-quality%20dataset%20for%20T3DEM%2C%20comprising%2015%2C000%20text-to-3D%20expression%0Amappings%20that%20characterize%20the%20aforementioned%20three%20challenges%20in%20Emo3D%0Ageneration.%20Furthermore%2C%20we%20develop%20various%20metrics%20to%20effectively%20evaluate%0Amodels%20against%20these%20identified%20challenges.%20Next%2C%20to%20effectively%20model%20the%0Aconsistency%2C%20diversity%2C%20and%20fluidity%20of%20human%20expressions%20in%20the%20T3DEM%20step%2C%20we%0Apropose%20the%20Continuous%20Text-to-Expression%20Generator%2C%20which%20employs%20an%0Aautoregressive%20Conditional%20Variational%20Autoencoder%20for%20expression%20code%0Ageneration%2C%20enhanced%20with%20Latent%20Temporal%20Attention%20and%20Expression-wise%0AAttention%20mechanisms.%20Finally%2C%20to%20further%20enhance%20the%203DAR%20step%20on%20rendering%0Ahigher-quality%20subtle%20expressions%2C%20we%20present%20the%20Globally-informed%20Gaussian%0AAvatar%20%28GiGA%29%20model.%20GiGA%20incorporates%20a%20global%20information%20mechanism%20into%203D%0AGaussian%20representations%2C%20enabling%20the%20capture%20of%20subtle%20micro-expressions%20and%0Aseamless%20transitions%20between%20emotional%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02508v2&entry.124074799=Read"},
{"title": "ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models", "author": "Sombit Dey and Jan-Nico Zaech and Nikolay Nikolov and Luc Van Gool and Danda Pani Paudel", "abstract": "  Recent progress in large language models and access to large-scale robotic\ndatasets has sparked a paradigm shift in robotics models transforming them into\ngeneralists able to adapt to various tasks, scenes, and robot modalities. A\nlarge step for the community are open Vision Language Action models which\nshowcase strong performance in a wide variety of tasks. In this work, we study\nthe visual generalization capabilities of three existing robotic foundation\nmodels, and propose a corresponding evaluation framework. Our study shows that\nthe existing models do not exhibit robustness to visual out-of-domain\nscenarios. This is potentially caused by limited variations in the training\ndata and/or catastrophic forgetting, leading to domain limitations in the\nvision foundation models. We further explore OpenVLA, which uses two\npre-trained vision foundation models and is, therefore, expected to generalize\nto out-of-domain experiments. However, we showcase catastrophic forgetting by\nDINO-v2 in OpenVLA through its failure to fulfill the task of depth regression.\nTo overcome the aforementioned issue of visual catastrophic forgetting, we\npropose a gradual backbone reversal approach founded on model merging. This\nenables OpenVLA -- which requires the adaptation of the visual backbones during\ninitial training -- to regain its visual generalization ability. Regaining this\ncapability enables our ReVLA model to improve over OpenVLA by a factor of 77\\%\nand 66\\% for grasping and lifting in visual OOD tasks. Comprehensive\nevaluations, episode rollouts and model weights are available on the ReVLA Page\n", "link": "http://arxiv.org/abs/2409.15250v3", "date": "2025-05-20", "relevancy": 3.0389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6371}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReVLA%3A%20Reverting%20Visual%20Domain%20Limitation%20of%20Robotic%20Foundation%20Models&body=Title%3A%20ReVLA%3A%20Reverting%20Visual%20Domain%20Limitation%20of%20Robotic%20Foundation%20Models%0AAuthor%3A%20Sombit%20Dey%20and%20Jan-Nico%20Zaech%20and%20Nikolay%20Nikolov%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%20Recent%20progress%20in%20large%20language%20models%20and%20access%20to%20large-scale%20robotic%0Adatasets%20has%20sparked%20a%20paradigm%20shift%20in%20robotics%20models%20transforming%20them%20into%0Ageneralists%20able%20to%20adapt%20to%20various%20tasks%2C%20scenes%2C%20and%20robot%20modalities.%20A%0Alarge%20step%20for%20the%20community%20are%20open%20Vision%20Language%20Action%20models%20which%0Ashowcase%20strong%20performance%20in%20a%20wide%20variety%20of%20tasks.%20In%20this%20work%2C%20we%20study%0Athe%20visual%20generalization%20capabilities%20of%20three%20existing%20robotic%20foundation%0Amodels%2C%20and%20propose%20a%20corresponding%20evaluation%20framework.%20Our%20study%20shows%20that%0Athe%20existing%20models%20do%20not%20exhibit%20robustness%20to%20visual%20out-of-domain%0Ascenarios.%20This%20is%20potentially%20caused%20by%20limited%20variations%20in%20the%20training%0Adata%20and/or%20catastrophic%20forgetting%2C%20leading%20to%20domain%20limitations%20in%20the%0Avision%20foundation%20models.%20We%20further%20explore%20OpenVLA%2C%20which%20uses%20two%0Apre-trained%20vision%20foundation%20models%20and%20is%2C%20therefore%2C%20expected%20to%20generalize%0Ato%20out-of-domain%20experiments.%20However%2C%20we%20showcase%20catastrophic%20forgetting%20by%0ADINO-v2%20in%20OpenVLA%20through%20its%20failure%20to%20fulfill%20the%20task%20of%20depth%20regression.%0ATo%20overcome%20the%20aforementioned%20issue%20of%20visual%20catastrophic%20forgetting%2C%20we%0Apropose%20a%20gradual%20backbone%20reversal%20approach%20founded%20on%20model%20merging.%20This%0Aenables%20OpenVLA%20--%20which%20requires%20the%20adaptation%20of%20the%20visual%20backbones%20during%0Ainitial%20training%20--%20to%20regain%20its%20visual%20generalization%20ability.%20Regaining%20this%0Acapability%20enables%20our%20ReVLA%20model%20to%20improve%20over%20OpenVLA%20by%20a%20factor%20of%2077%5C%25%0Aand%2066%5C%25%20for%20grasping%20and%20lifting%20in%20visual%20OOD%20tasks.%20Comprehensive%0Aevaluations%2C%20episode%20rollouts%20and%20model%20weights%20are%20available%20on%20the%20ReVLA%20Page%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15250v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReVLA%253A%2520Reverting%2520Visual%2520Domain%2520Limitation%2520of%2520Robotic%2520Foundation%2520Models%26entry.906535625%3DSombit%2520Dey%2520and%2520Jan-Nico%2520Zaech%2520and%2520Nikolay%2520Nikolov%2520and%2520Luc%2520Van%2520Gool%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520large%2520language%2520models%2520and%2520access%2520to%2520large-scale%2520robotic%250Adatasets%2520has%2520sparked%2520a%2520paradigm%2520shift%2520in%2520robotics%2520models%2520transforming%2520them%2520into%250Ageneralists%2520able%2520to%2520adapt%2520to%2520various%2520tasks%252C%2520scenes%252C%2520and%2520robot%2520modalities.%2520A%250Alarge%2520step%2520for%2520the%2520community%2520are%2520open%2520Vision%2520Language%2520Action%2520models%2520which%250Ashowcase%2520strong%2520performance%2520in%2520a%2520wide%2520variety%2520of%2520tasks.%2520In%2520this%2520work%252C%2520we%2520study%250Athe%2520visual%2520generalization%2520capabilities%2520of%2520three%2520existing%2520robotic%2520foundation%250Amodels%252C%2520and%2520propose%2520a%2520corresponding%2520evaluation%2520framework.%2520Our%2520study%2520shows%2520that%250Athe%2520existing%2520models%2520do%2520not%2520exhibit%2520robustness%2520to%2520visual%2520out-of-domain%250Ascenarios.%2520This%2520is%2520potentially%2520caused%2520by%2520limited%2520variations%2520in%2520the%2520training%250Adata%2520and/or%2520catastrophic%2520forgetting%252C%2520leading%2520to%2520domain%2520limitations%2520in%2520the%250Avision%2520foundation%2520models.%2520We%2520further%2520explore%2520OpenVLA%252C%2520which%2520uses%2520two%250Apre-trained%2520vision%2520foundation%2520models%2520and%2520is%252C%2520therefore%252C%2520expected%2520to%2520generalize%250Ato%2520out-of-domain%2520experiments.%2520However%252C%2520we%2520showcase%2520catastrophic%2520forgetting%2520by%250ADINO-v2%2520in%2520OpenVLA%2520through%2520its%2520failure%2520to%2520fulfill%2520the%2520task%2520of%2520depth%2520regression.%250ATo%2520overcome%2520the%2520aforementioned%2520issue%2520of%2520visual%2520catastrophic%2520forgetting%252C%2520we%250Apropose%2520a%2520gradual%2520backbone%2520reversal%2520approach%2520founded%2520on%2520model%2520merging.%2520This%250Aenables%2520OpenVLA%2520--%2520which%2520requires%2520the%2520adaptation%2520of%2520the%2520visual%2520backbones%2520during%250Ainitial%2520training%2520--%2520to%2520regain%2520its%2520visual%2520generalization%2520ability.%2520Regaining%2520this%250Acapability%2520enables%2520our%2520ReVLA%2520model%2520to%2520improve%2520over%2520OpenVLA%2520by%2520a%2520factor%2520of%252077%255C%2525%250Aand%252066%255C%2525%2520for%2520grasping%2520and%2520lifting%2520in%2520visual%2520OOD%2520tasks.%2520Comprehensive%250Aevaluations%252C%2520episode%2520rollouts%2520and%2520model%2520weights%2520are%2520available%2520on%2520the%2520ReVLA%2520Page%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15250v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReVLA%3A%20Reverting%20Visual%20Domain%20Limitation%20of%20Robotic%20Foundation%20Models&entry.906535625=Sombit%20Dey%20and%20Jan-Nico%20Zaech%20and%20Nikolay%20Nikolov%20and%20Luc%20Van%20Gool%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%20Recent%20progress%20in%20large%20language%20models%20and%20access%20to%20large-scale%20robotic%0Adatasets%20has%20sparked%20a%20paradigm%20shift%20in%20robotics%20models%20transforming%20them%20into%0Ageneralists%20able%20to%20adapt%20to%20various%20tasks%2C%20scenes%2C%20and%20robot%20modalities.%20A%0Alarge%20step%20for%20the%20community%20are%20open%20Vision%20Language%20Action%20models%20which%0Ashowcase%20strong%20performance%20in%20a%20wide%20variety%20of%20tasks.%20In%20this%20work%2C%20we%20study%0Athe%20visual%20generalization%20capabilities%20of%20three%20existing%20robotic%20foundation%0Amodels%2C%20and%20propose%20a%20corresponding%20evaluation%20framework.%20Our%20study%20shows%20that%0Athe%20existing%20models%20do%20not%20exhibit%20robustness%20to%20visual%20out-of-domain%0Ascenarios.%20This%20is%20potentially%20caused%20by%20limited%20variations%20in%20the%20training%0Adata%20and/or%20catastrophic%20forgetting%2C%20leading%20to%20domain%20limitations%20in%20the%0Avision%20foundation%20models.%20We%20further%20explore%20OpenVLA%2C%20which%20uses%20two%0Apre-trained%20vision%20foundation%20models%20and%20is%2C%20therefore%2C%20expected%20to%20generalize%0Ato%20out-of-domain%20experiments.%20However%2C%20we%20showcase%20catastrophic%20forgetting%20by%0ADINO-v2%20in%20OpenVLA%20through%20its%20failure%20to%20fulfill%20the%20task%20of%20depth%20regression.%0ATo%20overcome%20the%20aforementioned%20issue%20of%20visual%20catastrophic%20forgetting%2C%20we%0Apropose%20a%20gradual%20backbone%20reversal%20approach%20founded%20on%20model%20merging.%20This%0Aenables%20OpenVLA%20--%20which%20requires%20the%20adaptation%20of%20the%20visual%20backbones%20during%0Ainitial%20training%20--%20to%20regain%20its%20visual%20generalization%20ability.%20Regaining%20this%0Acapability%20enables%20our%20ReVLA%20model%20to%20improve%20over%20OpenVLA%20by%20a%20factor%20of%2077%5C%25%0Aand%2066%5C%25%20for%20grasping%20and%20lifting%20in%20visual%20OOD%20tasks.%20Comprehensive%0Aevaluations%2C%20episode%20rollouts%20and%20model%20weights%20are%20available%20on%20the%20ReVLA%20Page%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15250v3&entry.124074799=Read"},
{"title": "SparC: Sparse Representation and Construction for High-Resolution 3D\n  Shapes Modeling", "author": "Zhihao Li and Yufei Wang and Heliang Zheng and Yihao Luo and Bihan Wen", "abstract": "  High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation.\n", "link": "http://arxiv.org/abs/2505.14521v1", "date": "2025-05-20", "relevancy": 3.0381, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6134}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6134}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparC%3A%20Sparse%20Representation%20and%20Construction%20for%20High-Resolution%203D%0A%20%20Shapes%20Modeling&body=Title%3A%20SparC%3A%20Sparse%20Representation%20and%20Construction%20for%20High-Resolution%203D%0A%20%20Shapes%20Modeling%0AAuthor%3A%20Zhihao%20Li%20and%20Yufei%20Wang%20and%20Heliang%20Zheng%20and%20Yihao%20Luo%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20High-fidelity%203D%20object%20synthesis%20remains%20significantly%20more%20challenging%20than%0A2D%20image%20generation%20due%20to%20the%20unstructured%20nature%20of%20mesh%20data%20and%20the%20cubic%0Acomplexity%20of%20dense%20volumetric%20grids.%20Existing%20two-stage%20pipelines-compressing%0Ameshes%20with%20a%20VAE%20%28using%20either%202D%20or%203D%20supervision%29%2C%20followed%20by%20latent%0Adiffusion%20sampling-often%20suffer%20from%20severe%20detail%20loss%20caused%20by%20inefficient%0Arepresentations%20and%20modality%20mismatches%20introduced%20in%20VAE.%20We%20introduce%20SparC%2C%0Aa%20unified%20framework%20that%20combines%20a%20sparse%20deformable%20marching%20cubes%0Arepresentation%20SparseCubes%20with%20a%20novel%20encoder%20SparConv-VAE.%20SparseCubes%0Aconverts%20raw%20meshes%20into%20high-resolution%20%28%241024%5E3%24%29%20surfaces%20with%20arbitrary%0Atopology%20by%20scattering%20signed%20distance%20and%20deformation%20fields%20onto%20a%20sparse%0Acube%2C%20allowing%20differentiable%20optimization.%20SparConv-VAE%20is%20the%20first%0Amodality-consistent%20variational%20autoencoder%20built%20entirely%20upon%20sparse%0Aconvolutional%20networks%2C%20enabling%20efficient%20and%20near-lossless%203D%20reconstruction%0Asuitable%20for%20high-resolution%20generative%20modeling%20through%20latent%20diffusion.%0ASparC%20achieves%20state-of-the-art%20reconstruction%20fidelity%20on%20challenging%20inputs%2C%0Aincluding%20open%20surfaces%2C%20disconnected%20components%2C%20and%20intricate%20geometry.%20It%0Apreserves%20fine-grained%20shape%20details%2C%20reduces%20training%20and%20inference%20cost%2C%20and%0Aintegrates%20naturally%20with%20latent%20diffusion%20models%20for%20scalable%2C%20high-resolution%0A3D%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparC%253A%2520Sparse%2520Representation%2520and%2520Construction%2520for%2520High-Resolution%25203D%250A%2520%2520Shapes%2520Modeling%26entry.906535625%3DZhihao%2520Li%2520and%2520Yufei%2520Wang%2520and%2520Heliang%2520Zheng%2520and%2520Yihao%2520Luo%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520object%2520synthesis%2520remains%2520significantly%2520more%2520challenging%2520than%250A2D%2520image%2520generation%2520due%2520to%2520the%2520unstructured%2520nature%2520of%2520mesh%2520data%2520and%2520the%2520cubic%250Acomplexity%2520of%2520dense%2520volumetric%2520grids.%2520Existing%2520two-stage%2520pipelines-compressing%250Ameshes%2520with%2520a%2520VAE%2520%2528using%2520either%25202D%2520or%25203D%2520supervision%2529%252C%2520followed%2520by%2520latent%250Adiffusion%2520sampling-often%2520suffer%2520from%2520severe%2520detail%2520loss%2520caused%2520by%2520inefficient%250Arepresentations%2520and%2520modality%2520mismatches%2520introduced%2520in%2520VAE.%2520We%2520introduce%2520SparC%252C%250Aa%2520unified%2520framework%2520that%2520combines%2520a%2520sparse%2520deformable%2520marching%2520cubes%250Arepresentation%2520SparseCubes%2520with%2520a%2520novel%2520encoder%2520SparConv-VAE.%2520SparseCubes%250Aconverts%2520raw%2520meshes%2520into%2520high-resolution%2520%2528%25241024%255E3%2524%2529%2520surfaces%2520with%2520arbitrary%250Atopology%2520by%2520scattering%2520signed%2520distance%2520and%2520deformation%2520fields%2520onto%2520a%2520sparse%250Acube%252C%2520allowing%2520differentiable%2520optimization.%2520SparConv-VAE%2520is%2520the%2520first%250Amodality-consistent%2520variational%2520autoencoder%2520built%2520entirely%2520upon%2520sparse%250Aconvolutional%2520networks%252C%2520enabling%2520efficient%2520and%2520near-lossless%25203D%2520reconstruction%250Asuitable%2520for%2520high-resolution%2520generative%2520modeling%2520through%2520latent%2520diffusion.%250ASparC%2520achieves%2520state-of-the-art%2520reconstruction%2520fidelity%2520on%2520challenging%2520inputs%252C%250Aincluding%2520open%2520surfaces%252C%2520disconnected%2520components%252C%2520and%2520intricate%2520geometry.%2520It%250Apreserves%2520fine-grained%2520shape%2520details%252C%2520reduces%2520training%2520and%2520inference%2520cost%252C%2520and%250Aintegrates%2520naturally%2520with%2520latent%2520diffusion%2520models%2520for%2520scalable%252C%2520high-resolution%250A3D%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparC%3A%20Sparse%20Representation%20and%20Construction%20for%20High-Resolution%203D%0A%20%20Shapes%20Modeling&entry.906535625=Zhihao%20Li%20and%20Yufei%20Wang%20and%20Heliang%20Zheng%20and%20Yihao%20Luo%20and%20Bihan%20Wen&entry.1292438233=%20%20High-fidelity%203D%20object%20synthesis%20remains%20significantly%20more%20challenging%20than%0A2D%20image%20generation%20due%20to%20the%20unstructured%20nature%20of%20mesh%20data%20and%20the%20cubic%0Acomplexity%20of%20dense%20volumetric%20grids.%20Existing%20two-stage%20pipelines-compressing%0Ameshes%20with%20a%20VAE%20%28using%20either%202D%20or%203D%20supervision%29%2C%20followed%20by%20latent%0Adiffusion%20sampling-often%20suffer%20from%20severe%20detail%20loss%20caused%20by%20inefficient%0Arepresentations%20and%20modality%20mismatches%20introduced%20in%20VAE.%20We%20introduce%20SparC%2C%0Aa%20unified%20framework%20that%20combines%20a%20sparse%20deformable%20marching%20cubes%0Arepresentation%20SparseCubes%20with%20a%20novel%20encoder%20SparConv-VAE.%20SparseCubes%0Aconverts%20raw%20meshes%20into%20high-resolution%20%28%241024%5E3%24%29%20surfaces%20with%20arbitrary%0Atopology%20by%20scattering%20signed%20distance%20and%20deformation%20fields%20onto%20a%20sparse%0Acube%2C%20allowing%20differentiable%20optimization.%20SparConv-VAE%20is%20the%20first%0Amodality-consistent%20variational%20autoencoder%20built%20entirely%20upon%20sparse%0Aconvolutional%20networks%2C%20enabling%20efficient%20and%20near-lossless%203D%20reconstruction%0Asuitable%20for%20high-resolution%20generative%20modeling%20through%20latent%20diffusion.%0ASparC%20achieves%20state-of-the-art%20reconstruction%20fidelity%20on%20challenging%20inputs%2C%0Aincluding%20open%20surfaces%2C%20disconnected%20components%2C%20and%20intricate%20geometry.%20It%0Apreserves%20fine-grained%20shape%20details%2C%20reduces%20training%20and%20inference%20cost%2C%20and%0Aintegrates%20naturally%20with%20latent%20diffusion%20models%20for%20scalable%2C%20high-resolution%0A3D%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14521v1&entry.124074799=Read"},
{"title": "3D Reconstruction from Sketches", "author": "Abhimanyu Talwar and Julien Laasri", "abstract": "  We consider the problem of reconstructing a 3D scene from multiple sketches.\nWe propose a pipeline which involves (1) stitching together multiple sketches\nthrough use of correspondence points, (2) converting the stitched sketch into a\nrealistic image using a CycleGAN, and (3) estimating that image's depth-map\nusing a pre-trained convolutional neural network based architecture called\nMegaDepth. Our contribution includes constructing a dataset of image-sketch\npairs, the images for which are from the Zurich Building Database, and sketches\nhave been generated by us. We use this dataset to train a CycleGAN for our\npipeline's second step. We end up with a stitching process that does not\ngeneralize well to real drawings, but the rest of the pipeline that creates a\n3D reconstruction from a single sketch performs quite well on a wide variety of\ndrawings.\n", "link": "http://arxiv.org/abs/2505.14621v1", "date": "2025-05-20", "relevancy": 3.0094, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6088}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Reconstruction%20from%20Sketches&body=Title%3A%203D%20Reconstruction%20from%20Sketches%0AAuthor%3A%20Abhimanyu%20Talwar%20and%20Julien%20Laasri%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20reconstructing%20a%203D%20scene%20from%20multiple%20sketches.%0AWe%20propose%20a%20pipeline%20which%20involves%20%281%29%20stitching%20together%20multiple%20sketches%0Athrough%20use%20of%20correspondence%20points%2C%20%282%29%20converting%20the%20stitched%20sketch%20into%20a%0Arealistic%20image%20using%20a%20CycleGAN%2C%20and%20%283%29%20estimating%20that%20image%27s%20depth-map%0Ausing%20a%20pre-trained%20convolutional%20neural%20network%20based%20architecture%20called%0AMegaDepth.%20Our%20contribution%20includes%20constructing%20a%20dataset%20of%20image-sketch%0Apairs%2C%20the%20images%20for%20which%20are%20from%20the%20Zurich%20Building%20Database%2C%20and%20sketches%0Ahave%20been%20generated%20by%20us.%20We%20use%20this%20dataset%20to%20train%20a%20CycleGAN%20for%20our%0Apipeline%27s%20second%20step.%20We%20end%20up%20with%20a%20stitching%20process%20that%20does%20not%0Ageneralize%20well%20to%20real%20drawings%2C%20but%20the%20rest%20of%20the%20pipeline%20that%20creates%20a%0A3D%20reconstruction%20from%20a%20single%20sketch%20performs%20quite%20well%20on%20a%20wide%20variety%20of%0Adrawings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Reconstruction%2520from%2520Sketches%26entry.906535625%3DAbhimanyu%2520Talwar%2520and%2520Julien%2520Laasri%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520reconstructing%2520a%25203D%2520scene%2520from%2520multiple%2520sketches.%250AWe%2520propose%2520a%2520pipeline%2520which%2520involves%2520%25281%2529%2520stitching%2520together%2520multiple%2520sketches%250Athrough%2520use%2520of%2520correspondence%2520points%252C%2520%25282%2529%2520converting%2520the%2520stitched%2520sketch%2520into%2520a%250Arealistic%2520image%2520using%2520a%2520CycleGAN%252C%2520and%2520%25283%2529%2520estimating%2520that%2520image%2527s%2520depth-map%250Ausing%2520a%2520pre-trained%2520convolutional%2520neural%2520network%2520based%2520architecture%2520called%250AMegaDepth.%2520Our%2520contribution%2520includes%2520constructing%2520a%2520dataset%2520of%2520image-sketch%250Apairs%252C%2520the%2520images%2520for%2520which%2520are%2520from%2520the%2520Zurich%2520Building%2520Database%252C%2520and%2520sketches%250Ahave%2520been%2520generated%2520by%2520us.%2520We%2520use%2520this%2520dataset%2520to%2520train%2520a%2520CycleGAN%2520for%2520our%250Apipeline%2527s%2520second%2520step.%2520We%2520end%2520up%2520with%2520a%2520stitching%2520process%2520that%2520does%2520not%250Ageneralize%2520well%2520to%2520real%2520drawings%252C%2520but%2520the%2520rest%2520of%2520the%2520pipeline%2520that%2520creates%2520a%250A3D%2520reconstruction%2520from%2520a%2520single%2520sketch%2520performs%2520quite%2520well%2520on%2520a%2520wide%2520variety%2520of%250Adrawings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Reconstruction%20from%20Sketches&entry.906535625=Abhimanyu%20Talwar%20and%20Julien%20Laasri&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20reconstructing%20a%203D%20scene%20from%20multiple%20sketches.%0AWe%20propose%20a%20pipeline%20which%20involves%20%281%29%20stitching%20together%20multiple%20sketches%0Athrough%20use%20of%20correspondence%20points%2C%20%282%29%20converting%20the%20stitched%20sketch%20into%20a%0Arealistic%20image%20using%20a%20CycleGAN%2C%20and%20%283%29%20estimating%20that%20image%27s%20depth-map%0Ausing%20a%20pre-trained%20convolutional%20neural%20network%20based%20architecture%20called%0AMegaDepth.%20Our%20contribution%20includes%20constructing%20a%20dataset%20of%20image-sketch%0Apairs%2C%20the%20images%20for%20which%20are%20from%20the%20Zurich%20Building%20Database%2C%20and%20sketches%0Ahave%20been%20generated%20by%20us.%20We%20use%20this%20dataset%20to%20train%20a%20CycleGAN%20for%20our%0Apipeline%27s%20second%20step.%20We%20end%20up%20with%20a%20stitching%20process%20that%20does%20not%0Ageneralize%20well%20to%20real%20drawings%2C%20but%20the%20rest%20of%20the%20pipeline%20that%20creates%20a%0A3D%20reconstruction%20from%20a%20single%20sketch%20performs%20quite%20well%20on%20a%20wide%20variety%20of%0Adrawings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14621v1&entry.124074799=Read"},
{"title": "Handloom Design Generation Using Generative Networks", "author": "Rajat Kanti Bhattacharjee and Meghali Nandi and Amrit Jha and Gunajit Kalita and Ferdous Ahmed Barbhuiya", "abstract": "  This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation.\n", "link": "http://arxiv.org/abs/2505.14330v1", "date": "2025-05-20", "relevancy": 2.9759, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6277}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5858}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Handloom%20Design%20Generation%20Using%20Generative%20Networks&body=Title%3A%20Handloom%20Design%20Generation%20Using%20Generative%20Networks%0AAuthor%3A%20Rajat%20Kanti%20Bhattacharjee%20and%20Meghali%20Nandi%20and%20Amrit%20Jha%20and%20Gunajit%20Kalita%20and%20Ferdous%20Ahmed%20Barbhuiya%0AAbstract%3A%20%20%20This%20paper%20proposes%20deep%20learning%20techniques%20of%20generating%20designs%20for%0Aclothing%2C%20focused%20on%20handloom%20fabric%20and%20discusses%20the%20associated%20challenges%0Aalong%20with%20its%20application.%20The%20capability%20of%20generative%20neural%20network%20models%0Ain%20understanding%20artistic%20designs%20and%20synthesizing%20those%20is%20not%20yet%20explored%0Awell.%20In%20this%20work%2C%20multiple%20methods%20are%20employed%20incorporating%20the%20current%0Astate%20of%20the%20art%20generative%20models%20and%20style%20transfer%20algorithms%20to%20study%20and%0Aobserve%20their%20performance%20for%20the%20task.%20The%20results%20are%20then%20evaluated%20through%0Auser%20score.%20This%20work%20also%20provides%20a%20new%20dataset%20NeuralLoom%20for%20the%20task%20of%0Athe%20design%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandloom%2520Design%2520Generation%2520Using%2520Generative%2520Networks%26entry.906535625%3DRajat%2520Kanti%2520Bhattacharjee%2520and%2520Meghali%2520Nandi%2520and%2520Amrit%2520Jha%2520and%2520Gunajit%2520Kalita%2520and%2520Ferdous%2520Ahmed%2520Barbhuiya%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520deep%2520learning%2520techniques%2520of%2520generating%2520designs%2520for%250Aclothing%252C%2520focused%2520on%2520handloom%2520fabric%2520and%2520discusses%2520the%2520associated%2520challenges%250Aalong%2520with%2520its%2520application.%2520The%2520capability%2520of%2520generative%2520neural%2520network%2520models%250Ain%2520understanding%2520artistic%2520designs%2520and%2520synthesizing%2520those%2520is%2520not%2520yet%2520explored%250Awell.%2520In%2520this%2520work%252C%2520multiple%2520methods%2520are%2520employed%2520incorporating%2520the%2520current%250Astate%2520of%2520the%2520art%2520generative%2520models%2520and%2520style%2520transfer%2520algorithms%2520to%2520study%2520and%250Aobserve%2520their%2520performance%2520for%2520the%2520task.%2520The%2520results%2520are%2520then%2520evaluated%2520through%250Auser%2520score.%2520This%2520work%2520also%2520provides%2520a%2520new%2520dataset%2520NeuralLoom%2520for%2520the%2520task%2520of%250Athe%2520design%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Handloom%20Design%20Generation%20Using%20Generative%20Networks&entry.906535625=Rajat%20Kanti%20Bhattacharjee%20and%20Meghali%20Nandi%20and%20Amrit%20Jha%20and%20Gunajit%20Kalita%20and%20Ferdous%20Ahmed%20Barbhuiya&entry.1292438233=%20%20This%20paper%20proposes%20deep%20learning%20techniques%20of%20generating%20designs%20for%0Aclothing%2C%20focused%20on%20handloom%20fabric%20and%20discusses%20the%20associated%20challenges%0Aalong%20with%20its%20application.%20The%20capability%20of%20generative%20neural%20network%20models%0Ain%20understanding%20artistic%20designs%20and%20synthesizing%20those%20is%20not%20yet%20explored%0Awell.%20In%20this%20work%2C%20multiple%20methods%20are%20employed%20incorporating%20the%20current%0Astate%20of%20the%20art%20generative%20models%20and%20style%20transfer%20algorithms%20to%20study%20and%0Aobserve%20their%20performance%20for%20the%20task.%20The%20results%20are%20then%20evaluated%20through%0Auser%20score.%20This%20work%20also%20provides%20a%20new%20dataset%20NeuralLoom%20for%20the%20task%20of%0Athe%20design%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14330v1&entry.124074799=Read"},
{"title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model", "author": "Zhaochong An and Guolei Sun and Yun Liu and Runjia Li and Junlin Han and Ender Konukoglu and Serge Belongie", "abstract": "  Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL\n", "link": "http://arxiv.org/abs/2503.16282v2", "date": "2025-05-20", "relevancy": 2.9407, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5937}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Few-shot%203D%20Point%20Cloud%20Segmentation%20with%20Vision-Language%0A%20%20Model&body=Title%3A%20Generalized%20Few-shot%203D%20Point%20Cloud%20Segmentation%20with%20Vision-Language%0A%20%20Model%0AAuthor%3A%20Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Runjia%20Li%20and%20Junlin%20Han%20and%20Ender%20Konukoglu%20and%20Serge%20Belongie%0AAbstract%3A%20%20%20Generalized%20few-shot%203D%20point%20cloud%20segmentation%20%28GFS-PCS%29%20adapts%20models%20to%0Anew%20classes%20with%20few%20support%20samples%20while%20retaining%20base%20class%20segmentation.%0AExisting%20GFS-PCS%20methods%20enhance%20prototypes%20via%20interacting%20with%20support%20or%0Aquery%20features%20but%20remain%20limited%20by%20sparse%20knowledge%20from%20few-shot%20samples.%0AMeanwhile%2C%203D%20vision-language%20models%20%283D%20VLMs%29%2C%20generalizing%20across%20open-world%0Anovel%20classes%2C%20contain%20rich%20but%20noisy%20novel%20class%20knowledge.%20In%20this%20work%2C%20we%0Aintroduce%20a%20GFS-PCS%20framework%20that%20synergizes%20dense%20but%20noisy%20pseudo-labels%0Afrom%203D%20VLMs%20with%20precise%20yet%20sparse%20few-shot%20samples%20to%20maximize%20the%20strengths%0Aof%20both%2C%20named%20GFS-VL.%20Specifically%2C%20we%20present%20a%20prototype-guided%20pseudo-label%0Aselection%20to%20filter%20low-quality%20regions%2C%20followed%20by%20an%20adaptive%20infilling%0Astrategy%20that%20combines%20knowledge%20from%20pseudo-label%20contexts%20and%20few-shot%0Asamples%20to%20adaptively%20label%20the%20filtered%2C%20unlabeled%20areas.%20Additionally%2C%20we%0Adesign%20a%20novel-base%20mix%20strategy%20to%20embed%20few-shot%20samples%20into%20training%0Ascenes%2C%20preserving%20essential%20context%20for%20improved%20novel%20class%20learning.%0AMoreover%2C%20recognizing%20the%20limited%20diversity%20in%20current%20GFS-PCS%20benchmarks%2C%20we%0Aintroduce%20two%20challenging%20benchmarks%20with%20diverse%20novel%20classes%20for%0Acomprehensive%20generalization%20evaluation.%20Experiments%20validate%20the%20effectiveness%0Aof%20our%20framework%20across%20models%20and%20datasets.%20Our%20approach%20and%20benchmarks%0Aprovide%20a%20solid%20foundation%20for%20advancing%20GFS-PCS%20in%20the%20real%20world.%20The%20code%20is%0Aat%20https%3A//github.com/ZhaochongAn/GFS-VL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.16282v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Few-shot%25203D%2520Point%2520Cloud%2520Segmentation%2520with%2520Vision-Language%250A%2520%2520Model%26entry.906535625%3DZhaochong%2520An%2520and%2520Guolei%2520Sun%2520and%2520Yun%2520Liu%2520and%2520Runjia%2520Li%2520and%2520Junlin%2520Han%2520and%2520Ender%2520Konukoglu%2520and%2520Serge%2520Belongie%26entry.1292438233%3D%2520%2520Generalized%2520few-shot%25203D%2520point%2520cloud%2520segmentation%2520%2528GFS-PCS%2529%2520adapts%2520models%2520to%250Anew%2520classes%2520with%2520few%2520support%2520samples%2520while%2520retaining%2520base%2520class%2520segmentation.%250AExisting%2520GFS-PCS%2520methods%2520enhance%2520prototypes%2520via%2520interacting%2520with%2520support%2520or%250Aquery%2520features%2520but%2520remain%2520limited%2520by%2520sparse%2520knowledge%2520from%2520few-shot%2520samples.%250AMeanwhile%252C%25203D%2520vision-language%2520models%2520%25283D%2520VLMs%2529%252C%2520generalizing%2520across%2520open-world%250Anovel%2520classes%252C%2520contain%2520rich%2520but%2520noisy%2520novel%2520class%2520knowledge.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520GFS-PCS%2520framework%2520that%2520synergizes%2520dense%2520but%2520noisy%2520pseudo-labels%250Afrom%25203D%2520VLMs%2520with%2520precise%2520yet%2520sparse%2520few-shot%2520samples%2520to%2520maximize%2520the%2520strengths%250Aof%2520both%252C%2520named%2520GFS-VL.%2520Specifically%252C%2520we%2520present%2520a%2520prototype-guided%2520pseudo-label%250Aselection%2520to%2520filter%2520low-quality%2520regions%252C%2520followed%2520by%2520an%2520adaptive%2520infilling%250Astrategy%2520that%2520combines%2520knowledge%2520from%2520pseudo-label%2520contexts%2520and%2520few-shot%250Asamples%2520to%2520adaptively%2520label%2520the%2520filtered%252C%2520unlabeled%2520areas.%2520Additionally%252C%2520we%250Adesign%2520a%2520novel-base%2520mix%2520strategy%2520to%2520embed%2520few-shot%2520samples%2520into%2520training%250Ascenes%252C%2520preserving%2520essential%2520context%2520for%2520improved%2520novel%2520class%2520learning.%250AMoreover%252C%2520recognizing%2520the%2520limited%2520diversity%2520in%2520current%2520GFS-PCS%2520benchmarks%252C%2520we%250Aintroduce%2520two%2520challenging%2520benchmarks%2520with%2520diverse%2520novel%2520classes%2520for%250Acomprehensive%2520generalization%2520evaluation.%2520Experiments%2520validate%2520the%2520effectiveness%250Aof%2520our%2520framework%2520across%2520models%2520and%2520datasets.%2520Our%2520approach%2520and%2520benchmarks%250Aprovide%2520a%2520solid%2520foundation%2520for%2520advancing%2520GFS-PCS%2520in%2520the%2520real%2520world.%2520The%2520code%2520is%250Aat%2520https%253A//github.com/ZhaochongAn/GFS-VL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.16282v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Few-shot%203D%20Point%20Cloud%20Segmentation%20with%20Vision-Language%0A%20%20Model&entry.906535625=Zhaochong%20An%20and%20Guolei%20Sun%20and%20Yun%20Liu%20and%20Runjia%20Li%20and%20Junlin%20Han%20and%20Ender%20Konukoglu%20and%20Serge%20Belongie&entry.1292438233=%20%20Generalized%20few-shot%203D%20point%20cloud%20segmentation%20%28GFS-PCS%29%20adapts%20models%20to%0Anew%20classes%20with%20few%20support%20samples%20while%20retaining%20base%20class%20segmentation.%0AExisting%20GFS-PCS%20methods%20enhance%20prototypes%20via%20interacting%20with%20support%20or%0Aquery%20features%20but%20remain%20limited%20by%20sparse%20knowledge%20from%20few-shot%20samples.%0AMeanwhile%2C%203D%20vision-language%20models%20%283D%20VLMs%29%2C%20generalizing%20across%20open-world%0Anovel%20classes%2C%20contain%20rich%20but%20noisy%20novel%20class%20knowledge.%20In%20this%20work%2C%20we%0Aintroduce%20a%20GFS-PCS%20framework%20that%20synergizes%20dense%20but%20noisy%20pseudo-labels%0Afrom%203D%20VLMs%20with%20precise%20yet%20sparse%20few-shot%20samples%20to%20maximize%20the%20strengths%0Aof%20both%2C%20named%20GFS-VL.%20Specifically%2C%20we%20present%20a%20prototype-guided%20pseudo-label%0Aselection%20to%20filter%20low-quality%20regions%2C%20followed%20by%20an%20adaptive%20infilling%0Astrategy%20that%20combines%20knowledge%20from%20pseudo-label%20contexts%20and%20few-shot%0Asamples%20to%20adaptively%20label%20the%20filtered%2C%20unlabeled%20areas.%20Additionally%2C%20we%0Adesign%20a%20novel-base%20mix%20strategy%20to%20embed%20few-shot%20samples%20into%20training%0Ascenes%2C%20preserving%20essential%20context%20for%20improved%20novel%20class%20learning.%0AMoreover%2C%20recognizing%20the%20limited%20diversity%20in%20current%20GFS-PCS%20benchmarks%2C%20we%0Aintroduce%20two%20challenging%20benchmarks%20with%20diverse%20novel%20classes%20for%0Acomprehensive%20generalization%20evaluation.%20Experiments%20validate%20the%20effectiveness%0Aof%20our%20framework%20across%20models%20and%20datasets.%20Our%20approach%20and%20benchmarks%0Aprovide%20a%20solid%20foundation%20for%20advancing%20GFS-PCS%20in%20the%20real%20world.%20The%20code%20is%0Aat%20https%3A//github.com/ZhaochongAn/GFS-VL%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.16282v2&entry.124074799=Read"},
{"title": "RadCLIP: Enhancing Radiologic Image Analysis through Contrastive\n  Language-Image Pre-training", "author": "Zhixiu Lu and Hailong Li and Nehal A. Parikh and Jonathan R. Dillman and Lili He", "abstract": "  The integration of artificial intelligence (AI) with radiology marks a\ntransformative era in medicine. Vision foundation models have been adopted to\nenhance radiologic imaging analysis. However, the distinct complexities of\nradiologic 2D and 3D radiologic data pose unique challenges that existing\nmodels, pre-trained on general non-medical images, fail to address adequately.\nTo bridge this gap and capitalize on the diagnostic precision required in\nradiologic imaging, we introduce Radiologic Contrastive Language-Image\nPre-training (RadCLIP): a cross-modal vision-language foundational model that\nharnesses Vision Language Pre-training (VLP) framework to improve radiologic\nimage analysis. Building upon Contrastive Language-Image Pre-training (CLIP),\nRadCLIP incorporates a slice pooling mechanism tailored for volumetric image\nanalysis and is pre-trained using a large and diverse dataset of radiologic\nimage-text pairs. The RadCLIP was pre-trained to effectively align radiologic\nimages with their corresponding text annotations, creating a robust vision\nbackbone for radiologic images. Extensive experiments demonstrate RadCLIP's\nsuperior performance in both uni-modal radiologic image classification and\ncross-modal image-text matching, highlighting its significant promise for\nimproving diagnostic accuracy and efficiency in clinical settings. Our Key\ncontributions include curating a large dataset with diverse radiologic 2D/3D\nradiologic image-text pairs, a slice pooling adapter using an attention\nmechanism for integrating 2D images, and comprehensive evaluations of RadCLIP\non various radiologic downstream tasks.\n", "link": "http://arxiv.org/abs/2403.09948v3", "date": "2025-05-20", "relevancy": 2.9176, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6157}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5674}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadCLIP%3A%20Enhancing%20Radiologic%20Image%20Analysis%20through%20Contrastive%0A%20%20Language-Image%20Pre-training&body=Title%3A%20RadCLIP%3A%20Enhancing%20Radiologic%20Image%20Analysis%20through%20Contrastive%0A%20%20Language-Image%20Pre-training%0AAuthor%3A%20Zhixiu%20Lu%20and%20Hailong%20Li%20and%20Nehal%20A.%20Parikh%20and%20Jonathan%20R.%20Dillman%20and%20Lili%20He%0AAbstract%3A%20%20%20The%20integration%20of%20artificial%20intelligence%20%28AI%29%20with%20radiology%20marks%20a%0Atransformative%20era%20in%20medicine.%20Vision%20foundation%20models%20have%20been%20adopted%20to%0Aenhance%20radiologic%20imaging%20analysis.%20However%2C%20the%20distinct%20complexities%20of%0Aradiologic%202D%20and%203D%20radiologic%20data%20pose%20unique%20challenges%20that%20existing%0Amodels%2C%20pre-trained%20on%20general%20non-medical%20images%2C%20fail%20to%20address%20adequately.%0ATo%20bridge%20this%20gap%20and%20capitalize%20on%20the%20diagnostic%20precision%20required%20in%0Aradiologic%20imaging%2C%20we%20introduce%20Radiologic%20Contrastive%20Language-Image%0APre-training%20%28RadCLIP%29%3A%20a%20cross-modal%20vision-language%20foundational%20model%20that%0Aharnesses%20Vision%20Language%20Pre-training%20%28VLP%29%20framework%20to%20improve%20radiologic%0Aimage%20analysis.%20Building%20upon%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%0ARadCLIP%20incorporates%20a%20slice%20pooling%20mechanism%20tailored%20for%20volumetric%20image%0Aanalysis%20and%20is%20pre-trained%20using%20a%20large%20and%20diverse%20dataset%20of%20radiologic%0Aimage-text%20pairs.%20The%20RadCLIP%20was%20pre-trained%20to%20effectively%20align%20radiologic%0Aimages%20with%20their%20corresponding%20text%20annotations%2C%20creating%20a%20robust%20vision%0Abackbone%20for%20radiologic%20images.%20Extensive%20experiments%20demonstrate%20RadCLIP%27s%0Asuperior%20performance%20in%20both%20uni-modal%20radiologic%20image%20classification%20and%0Across-modal%20image-text%20matching%2C%20highlighting%20its%20significant%20promise%20for%0Aimproving%20diagnostic%20accuracy%20and%20efficiency%20in%20clinical%20settings.%20Our%20Key%0Acontributions%20include%20curating%20a%20large%20dataset%20with%20diverse%20radiologic%202D/3D%0Aradiologic%20image-text%20pairs%2C%20a%20slice%20pooling%20adapter%20using%20an%20attention%0Amechanism%20for%20integrating%202D%20images%2C%20and%20comprehensive%20evaluations%20of%20RadCLIP%0Aon%20various%20radiologic%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09948v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadCLIP%253A%2520Enhancing%2520Radiologic%2520Image%2520Analysis%2520through%2520Contrastive%250A%2520%2520Language-Image%2520Pre-training%26entry.906535625%3DZhixiu%2520Lu%2520and%2520Hailong%2520Li%2520and%2520Nehal%2520A.%2520Parikh%2520and%2520Jonathan%2520R.%2520Dillman%2520and%2520Lili%2520He%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520with%2520radiology%2520marks%2520a%250Atransformative%2520era%2520in%2520medicine.%2520Vision%2520foundation%2520models%2520have%2520been%2520adopted%2520to%250Aenhance%2520radiologic%2520imaging%2520analysis.%2520However%252C%2520the%2520distinct%2520complexities%2520of%250Aradiologic%25202D%2520and%25203D%2520radiologic%2520data%2520pose%2520unique%2520challenges%2520that%2520existing%250Amodels%252C%2520pre-trained%2520on%2520general%2520non-medical%2520images%252C%2520fail%2520to%2520address%2520adequately.%250ATo%2520bridge%2520this%2520gap%2520and%2520capitalize%2520on%2520the%2520diagnostic%2520precision%2520required%2520in%250Aradiologic%2520imaging%252C%2520we%2520introduce%2520Radiologic%2520Contrastive%2520Language-Image%250APre-training%2520%2528RadCLIP%2529%253A%2520a%2520cross-modal%2520vision-language%2520foundational%2520model%2520that%250Aharnesses%2520Vision%2520Language%2520Pre-training%2520%2528VLP%2529%2520framework%2520to%2520improve%2520radiologic%250Aimage%2520analysis.%2520Building%2520upon%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%252C%250ARadCLIP%2520incorporates%2520a%2520slice%2520pooling%2520mechanism%2520tailored%2520for%2520volumetric%2520image%250Aanalysis%2520and%2520is%2520pre-trained%2520using%2520a%2520large%2520and%2520diverse%2520dataset%2520of%2520radiologic%250Aimage-text%2520pairs.%2520The%2520RadCLIP%2520was%2520pre-trained%2520to%2520effectively%2520align%2520radiologic%250Aimages%2520with%2520their%2520corresponding%2520text%2520annotations%252C%2520creating%2520a%2520robust%2520vision%250Abackbone%2520for%2520radiologic%2520images.%2520Extensive%2520experiments%2520demonstrate%2520RadCLIP%2527s%250Asuperior%2520performance%2520in%2520both%2520uni-modal%2520radiologic%2520image%2520classification%2520and%250Across-modal%2520image-text%2520matching%252C%2520highlighting%2520its%2520significant%2520promise%2520for%250Aimproving%2520diagnostic%2520accuracy%2520and%2520efficiency%2520in%2520clinical%2520settings.%2520Our%2520Key%250Acontributions%2520include%2520curating%2520a%2520large%2520dataset%2520with%2520diverse%2520radiologic%25202D/3D%250Aradiologic%2520image-text%2520pairs%252C%2520a%2520slice%2520pooling%2520adapter%2520using%2520an%2520attention%250Amechanism%2520for%2520integrating%25202D%2520images%252C%2520and%2520comprehensive%2520evaluations%2520of%2520RadCLIP%250Aon%2520various%2520radiologic%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09948v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadCLIP%3A%20Enhancing%20Radiologic%20Image%20Analysis%20through%20Contrastive%0A%20%20Language-Image%20Pre-training&entry.906535625=Zhixiu%20Lu%20and%20Hailong%20Li%20and%20Nehal%20A.%20Parikh%20and%20Jonathan%20R.%20Dillman%20and%20Lili%20He&entry.1292438233=%20%20The%20integration%20of%20artificial%20intelligence%20%28AI%29%20with%20radiology%20marks%20a%0Atransformative%20era%20in%20medicine.%20Vision%20foundation%20models%20have%20been%20adopted%20to%0Aenhance%20radiologic%20imaging%20analysis.%20However%2C%20the%20distinct%20complexities%20of%0Aradiologic%202D%20and%203D%20radiologic%20data%20pose%20unique%20challenges%20that%20existing%0Amodels%2C%20pre-trained%20on%20general%20non-medical%20images%2C%20fail%20to%20address%20adequately.%0ATo%20bridge%20this%20gap%20and%20capitalize%20on%20the%20diagnostic%20precision%20required%20in%0Aradiologic%20imaging%2C%20we%20introduce%20Radiologic%20Contrastive%20Language-Image%0APre-training%20%28RadCLIP%29%3A%20a%20cross-modal%20vision-language%20foundational%20model%20that%0Aharnesses%20Vision%20Language%20Pre-training%20%28VLP%29%20framework%20to%20improve%20radiologic%0Aimage%20analysis.%20Building%20upon%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%0ARadCLIP%20incorporates%20a%20slice%20pooling%20mechanism%20tailored%20for%20volumetric%20image%0Aanalysis%20and%20is%20pre-trained%20using%20a%20large%20and%20diverse%20dataset%20of%20radiologic%0Aimage-text%20pairs.%20The%20RadCLIP%20was%20pre-trained%20to%20effectively%20align%20radiologic%0Aimages%20with%20their%20corresponding%20text%20annotations%2C%20creating%20a%20robust%20vision%0Abackbone%20for%20radiologic%20images.%20Extensive%20experiments%20demonstrate%20RadCLIP%27s%0Asuperior%20performance%20in%20both%20uni-modal%20radiologic%20image%20classification%20and%0Across-modal%20image-text%20matching%2C%20highlighting%20its%20significant%20promise%20for%0Aimproving%20diagnostic%20accuracy%20and%20efficiency%20in%20clinical%20settings.%20Our%20Key%0Acontributions%20include%20curating%20a%20large%20dataset%20with%20diverse%20radiologic%202D/3D%0Aradiologic%20image-text%20pairs%2C%20a%20slice%20pooling%20adapter%20using%20an%20attention%0Amechanism%20for%20integrating%202D%20images%2C%20and%20comprehensive%20evaluations%20of%20RadCLIP%0Aon%20various%20radiologic%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09948v3&entry.124074799=Read"},
{"title": "Egocentric Action-aware Inertial Localization in Point Clouds", "author": "Mingfang Zhang and Ryo Yonetani and Yifei Huang and Liangyang Ouyang and Ruicong Liu and Yoichi Sato", "abstract": "  This paper presents a novel inertial localization framework named Egocentric\nAction-aware Inertial Localization (EAIL), which leverages egocentric action\ncues from head-mounted IMU signals to localize the target individual within a\n3D point cloud. Human inertial localization is challenging due to IMU sensor\nnoise that causes trajectory drift over time. The diversity of human actions\nfurther complicates IMU signal processing by introducing various motion\npatterns. Nevertheless, we observe that some actions observed through the\nhead-mounted IMU correlate with spatial environmental structures (e.g., bending\ndown to look inside an oven, washing dishes next to a sink), thereby serving as\nspatial anchors to compensate for the localization drift. The proposed EAIL\nframework learns such correlations via hierarchical multi-modal alignment. By\nassuming that the 3D point cloud of the environment is available, it\ncontrastively learns modality encoders that align short-term egocentric action\ncues in IMU signals with local environmental features in the point cloud. These\nencoders are then used in reasoning the IMU data and the point cloud over time\nand space to perform inertial localization. Interestingly, these encoders can\nfurther be utilized to recognize the corresponding sequence of actions as a\nby-product. Extensive experiments demonstrate the effectiveness of the proposed\nframework over state-of-the-art inertial localization and inertial action\nrecognition baselines.\n", "link": "http://arxiv.org/abs/2505.14346v1", "date": "2025-05-20", "relevancy": 2.8983, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6168}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.562}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Egocentric%20Action-aware%20Inertial%20Localization%20in%20Point%20Clouds&body=Title%3A%20Egocentric%20Action-aware%20Inertial%20Localization%20in%20Point%20Clouds%0AAuthor%3A%20Mingfang%20Zhang%20and%20Ryo%20Yonetani%20and%20Yifei%20Huang%20and%20Liangyang%20Ouyang%20and%20Ruicong%20Liu%20and%20Yoichi%20Sato%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20inertial%20localization%20framework%20named%20Egocentric%0AAction-aware%20Inertial%20Localization%20%28EAIL%29%2C%20which%20leverages%20egocentric%20action%0Acues%20from%20head-mounted%20IMU%20signals%20to%20localize%20the%20target%20individual%20within%20a%0A3D%20point%20cloud.%20Human%20inertial%20localization%20is%20challenging%20due%20to%20IMU%20sensor%0Anoise%20that%20causes%20trajectory%20drift%20over%20time.%20The%20diversity%20of%20human%20actions%0Afurther%20complicates%20IMU%20signal%20processing%20by%20introducing%20various%20motion%0Apatterns.%20Nevertheless%2C%20we%20observe%20that%20some%20actions%20observed%20through%20the%0Ahead-mounted%20IMU%20correlate%20with%20spatial%20environmental%20structures%20%28e.g.%2C%20bending%0Adown%20to%20look%20inside%20an%20oven%2C%20washing%20dishes%20next%20to%20a%20sink%29%2C%20thereby%20serving%20as%0Aspatial%20anchors%20to%20compensate%20for%20the%20localization%20drift.%20The%20proposed%20EAIL%0Aframework%20learns%20such%20correlations%20via%20hierarchical%20multi-modal%20alignment.%20By%0Aassuming%20that%20the%203D%20point%20cloud%20of%20the%20environment%20is%20available%2C%20it%0Acontrastively%20learns%20modality%20encoders%20that%20align%20short-term%20egocentric%20action%0Acues%20in%20IMU%20signals%20with%20local%20environmental%20features%20in%20the%20point%20cloud.%20These%0Aencoders%20are%20then%20used%20in%20reasoning%20the%20IMU%20data%20and%20the%20point%20cloud%20over%20time%0Aand%20space%20to%20perform%20inertial%20localization.%20Interestingly%2C%20these%20encoders%20can%0Afurther%20be%20utilized%20to%20recognize%20the%20corresponding%20sequence%20of%20actions%20as%20a%0Aby-product.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aframework%20over%20state-of-the-art%20inertial%20localization%20and%20inertial%20action%0Arecognition%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgocentric%2520Action-aware%2520Inertial%2520Localization%2520in%2520Point%2520Clouds%26entry.906535625%3DMingfang%2520Zhang%2520and%2520Ryo%2520Yonetani%2520and%2520Yifei%2520Huang%2520and%2520Liangyang%2520Ouyang%2520and%2520Ruicong%2520Liu%2520and%2520Yoichi%2520Sato%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520inertial%2520localization%2520framework%2520named%2520Egocentric%250AAction-aware%2520Inertial%2520Localization%2520%2528EAIL%2529%252C%2520which%2520leverages%2520egocentric%2520action%250Acues%2520from%2520head-mounted%2520IMU%2520signals%2520to%2520localize%2520the%2520target%2520individual%2520within%2520a%250A3D%2520point%2520cloud.%2520Human%2520inertial%2520localization%2520is%2520challenging%2520due%2520to%2520IMU%2520sensor%250Anoise%2520that%2520causes%2520trajectory%2520drift%2520over%2520time.%2520The%2520diversity%2520of%2520human%2520actions%250Afurther%2520complicates%2520IMU%2520signal%2520processing%2520by%2520introducing%2520various%2520motion%250Apatterns.%2520Nevertheless%252C%2520we%2520observe%2520that%2520some%2520actions%2520observed%2520through%2520the%250Ahead-mounted%2520IMU%2520correlate%2520with%2520spatial%2520environmental%2520structures%2520%2528e.g.%252C%2520bending%250Adown%2520to%2520look%2520inside%2520an%2520oven%252C%2520washing%2520dishes%2520next%2520to%2520a%2520sink%2529%252C%2520thereby%2520serving%2520as%250Aspatial%2520anchors%2520to%2520compensate%2520for%2520the%2520localization%2520drift.%2520The%2520proposed%2520EAIL%250Aframework%2520learns%2520such%2520correlations%2520via%2520hierarchical%2520multi-modal%2520alignment.%2520By%250Aassuming%2520that%2520the%25203D%2520point%2520cloud%2520of%2520the%2520environment%2520is%2520available%252C%2520it%250Acontrastively%2520learns%2520modality%2520encoders%2520that%2520align%2520short-term%2520egocentric%2520action%250Acues%2520in%2520IMU%2520signals%2520with%2520local%2520environmental%2520features%2520in%2520the%2520point%2520cloud.%2520These%250Aencoders%2520are%2520then%2520used%2520in%2520reasoning%2520the%2520IMU%2520data%2520and%2520the%2520point%2520cloud%2520over%2520time%250Aand%2520space%2520to%2520perform%2520inertial%2520localization.%2520Interestingly%252C%2520these%2520encoders%2520can%250Afurther%2520be%2520utilized%2520to%2520recognize%2520the%2520corresponding%2520sequence%2520of%2520actions%2520as%2520a%250Aby-product.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aframework%2520over%2520state-of-the-art%2520inertial%2520localization%2520and%2520inertial%2520action%250Arecognition%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Egocentric%20Action-aware%20Inertial%20Localization%20in%20Point%20Clouds&entry.906535625=Mingfang%20Zhang%20and%20Ryo%20Yonetani%20and%20Yifei%20Huang%20and%20Liangyang%20Ouyang%20and%20Ruicong%20Liu%20and%20Yoichi%20Sato&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20inertial%20localization%20framework%20named%20Egocentric%0AAction-aware%20Inertial%20Localization%20%28EAIL%29%2C%20which%20leverages%20egocentric%20action%0Acues%20from%20head-mounted%20IMU%20signals%20to%20localize%20the%20target%20individual%20within%20a%0A3D%20point%20cloud.%20Human%20inertial%20localization%20is%20challenging%20due%20to%20IMU%20sensor%0Anoise%20that%20causes%20trajectory%20drift%20over%20time.%20The%20diversity%20of%20human%20actions%0Afurther%20complicates%20IMU%20signal%20processing%20by%20introducing%20various%20motion%0Apatterns.%20Nevertheless%2C%20we%20observe%20that%20some%20actions%20observed%20through%20the%0Ahead-mounted%20IMU%20correlate%20with%20spatial%20environmental%20structures%20%28e.g.%2C%20bending%0Adown%20to%20look%20inside%20an%20oven%2C%20washing%20dishes%20next%20to%20a%20sink%29%2C%20thereby%20serving%20as%0Aspatial%20anchors%20to%20compensate%20for%20the%20localization%20drift.%20The%20proposed%20EAIL%0Aframework%20learns%20such%20correlations%20via%20hierarchical%20multi-modal%20alignment.%20By%0Aassuming%20that%20the%203D%20point%20cloud%20of%20the%20environment%20is%20available%2C%20it%0Acontrastively%20learns%20modality%20encoders%20that%20align%20short-term%20egocentric%20action%0Acues%20in%20IMU%20signals%20with%20local%20environmental%20features%20in%20the%20point%20cloud.%20These%0Aencoders%20are%20then%20used%20in%20reasoning%20the%20IMU%20data%20and%20the%20point%20cloud%20over%20time%0Aand%20space%20to%20perform%20inertial%20localization.%20Interestingly%2C%20these%20encoders%20can%0Afurther%20be%20utilized%20to%20recognize%20the%20corresponding%20sequence%20of%20actions%20as%20a%0Aby-product.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aframework%20over%20state-of-the-art%20inertial%20localization%20and%20inertial%20action%0Arecognition%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14346v1&entry.124074799=Read"},
{"title": "Beginning with You: Perceptual-Initialization Improves Vision-Language\n  Representation and Alignment", "author": "Yang Hu and Runchen Wang and Stephen Chong Zhao and Xuhui Zhan and Do Hun Kim and Mark Wallace and David A. Tovar", "abstract": "  We introduce Perceptual-Initialization (PI), a paradigm shift in visual\nrepresentation learning that incorporates human perceptual structure during the\ninitialization phase rather than as a downstream fine-tuning step. By\nintegrating human-derived triplet embeddings from the NIGHTS dataset to\ninitialize a CLIP vision encoder, followed by self-supervised learning on\nYFCC15M, our approach demonstrates significant zero-shot performance\nimprovements, without any task-specific fine-tuning, across 29 zero shot\nclassification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains\nemerge after approximately 15 epochs of pretraining. Benefits are observed\nacross datasets of various scales, with improvements manifesting at different\nstages of the pretraining process depending on dataset characteristics. Our\napproach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and\nretrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,\nwithout requiring any adaptation to target domains. These findings challenge\nthe conventional wisdom of using human-perceptual data primarily for\nfine-tuning and demonstrate that embedding human perceptual structure during\nearly representation learning yields more capable and vision-language aligned\nsystems that generalize immediately to unseen tasks. Our work shows that\n\"beginning with you\", starting with human perception, provides a stronger\nfoundation for general-purpose vision-language intelligence.\n", "link": "http://arxiv.org/abs/2505.14204v1", "date": "2025-05-20", "relevancy": 2.8902, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beginning%20with%20You%3A%20Perceptual-Initialization%20Improves%20Vision-Language%0A%20%20Representation%20and%20Alignment&body=Title%3A%20Beginning%20with%20You%3A%20Perceptual-Initialization%20Improves%20Vision-Language%0A%20%20Representation%20and%20Alignment%0AAuthor%3A%20Yang%20Hu%20and%20Runchen%20Wang%20and%20Stephen%20Chong%20Zhao%20and%20Xuhui%20Zhan%20and%20Do%20Hun%20Kim%20and%20Mark%20Wallace%20and%20David%20A.%20Tovar%0AAbstract%3A%20%20%20We%20introduce%20Perceptual-Initialization%20%28PI%29%2C%20a%20paradigm%20shift%20in%20visual%0Arepresentation%20learning%20that%20incorporates%20human%20perceptual%20structure%20during%20the%0Ainitialization%20phase%20rather%20than%20as%20a%20downstream%20fine-tuning%20step.%20By%0Aintegrating%20human-derived%20triplet%20embeddings%20from%20the%20NIGHTS%20dataset%20to%0Ainitialize%20a%20CLIP%20vision%20encoder%2C%20followed%20by%20self-supervised%20learning%20on%0AYFCC15M%2C%20our%20approach%20demonstrates%20significant%20zero-shot%20performance%0Aimprovements%2C%20without%20any%20task-specific%20fine-tuning%2C%20across%2029%20zero%20shot%0Aclassification%20and%202%20retrieval%20benchmarks.%20On%20ImageNet-1K%2C%20zero-shot%20gains%0Aemerge%20after%20approximately%2015%20epochs%20of%20pretraining.%20Benefits%20are%20observed%0Aacross%20datasets%20of%20various%20scales%2C%20with%20improvements%20manifesting%20at%20different%0Astages%20of%20the%20pretraining%20process%20depending%20on%20dataset%20characteristics.%20Our%0Aapproach%20consistently%20enhances%20zero-shot%20top-1%20accuracy%2C%20top-5%20accuracy%2C%20and%0Aretrieval%20recall%20%28e.g.%2C%20R%401%2C%20R%405%29%20across%20these%20diverse%20evaluation%20tasks%2C%0Awithout%20requiring%20any%20adaptation%20to%20target%20domains.%20These%20findings%20challenge%0Athe%20conventional%20wisdom%20of%20using%20human-perceptual%20data%20primarily%20for%0Afine-tuning%20and%20demonstrate%20that%20embedding%20human%20perceptual%20structure%20during%0Aearly%20representation%20learning%20yields%20more%20capable%20and%20vision-language%20aligned%0Asystems%20that%20generalize%20immediately%20to%20unseen%20tasks.%20Our%20work%20shows%20that%0A%22beginning%20with%20you%22%2C%20starting%20with%20human%20perception%2C%20provides%20a%20stronger%0Afoundation%20for%20general-purpose%20vision-language%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeginning%2520with%2520You%253A%2520Perceptual-Initialization%2520Improves%2520Vision-Language%250A%2520%2520Representation%2520and%2520Alignment%26entry.906535625%3DYang%2520Hu%2520and%2520Runchen%2520Wang%2520and%2520Stephen%2520Chong%2520Zhao%2520and%2520Xuhui%2520Zhan%2520and%2520Do%2520Hun%2520Kim%2520and%2520Mark%2520Wallace%2520and%2520David%2520A.%2520Tovar%26entry.1292438233%3D%2520%2520We%2520introduce%2520Perceptual-Initialization%2520%2528PI%2529%252C%2520a%2520paradigm%2520shift%2520in%2520visual%250Arepresentation%2520learning%2520that%2520incorporates%2520human%2520perceptual%2520structure%2520during%2520the%250Ainitialization%2520phase%2520rather%2520than%2520as%2520a%2520downstream%2520fine-tuning%2520step.%2520By%250Aintegrating%2520human-derived%2520triplet%2520embeddings%2520from%2520the%2520NIGHTS%2520dataset%2520to%250Ainitialize%2520a%2520CLIP%2520vision%2520encoder%252C%2520followed%2520by%2520self-supervised%2520learning%2520on%250AYFCC15M%252C%2520our%2520approach%2520demonstrates%2520significant%2520zero-shot%2520performance%250Aimprovements%252C%2520without%2520any%2520task-specific%2520fine-tuning%252C%2520across%252029%2520zero%2520shot%250Aclassification%2520and%25202%2520retrieval%2520benchmarks.%2520On%2520ImageNet-1K%252C%2520zero-shot%2520gains%250Aemerge%2520after%2520approximately%252015%2520epochs%2520of%2520pretraining.%2520Benefits%2520are%2520observed%250Aacross%2520datasets%2520of%2520various%2520scales%252C%2520with%2520improvements%2520manifesting%2520at%2520different%250Astages%2520of%2520the%2520pretraining%2520process%2520depending%2520on%2520dataset%2520characteristics.%2520Our%250Aapproach%2520consistently%2520enhances%2520zero-shot%2520top-1%2520accuracy%252C%2520top-5%2520accuracy%252C%2520and%250Aretrieval%2520recall%2520%2528e.g.%252C%2520R%25401%252C%2520R%25405%2529%2520across%2520these%2520diverse%2520evaluation%2520tasks%252C%250Awithout%2520requiring%2520any%2520adaptation%2520to%2520target%2520domains.%2520These%2520findings%2520challenge%250Athe%2520conventional%2520wisdom%2520of%2520using%2520human-perceptual%2520data%2520primarily%2520for%250Afine-tuning%2520and%2520demonstrate%2520that%2520embedding%2520human%2520perceptual%2520structure%2520during%250Aearly%2520representation%2520learning%2520yields%2520more%2520capable%2520and%2520vision-language%2520aligned%250Asystems%2520that%2520generalize%2520immediately%2520to%2520unseen%2520tasks.%2520Our%2520work%2520shows%2520that%250A%2522beginning%2520with%2520you%2522%252C%2520starting%2520with%2520human%2520perception%252C%2520provides%2520a%2520stronger%250Afoundation%2520for%2520general-purpose%2520vision-language%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beginning%20with%20You%3A%20Perceptual-Initialization%20Improves%20Vision-Language%0A%20%20Representation%20and%20Alignment&entry.906535625=Yang%20Hu%20and%20Runchen%20Wang%20and%20Stephen%20Chong%20Zhao%20and%20Xuhui%20Zhan%20and%20Do%20Hun%20Kim%20and%20Mark%20Wallace%20and%20David%20A.%20Tovar&entry.1292438233=%20%20We%20introduce%20Perceptual-Initialization%20%28PI%29%2C%20a%20paradigm%20shift%20in%20visual%0Arepresentation%20learning%20that%20incorporates%20human%20perceptual%20structure%20during%20the%0Ainitialization%20phase%20rather%20than%20as%20a%20downstream%20fine-tuning%20step.%20By%0Aintegrating%20human-derived%20triplet%20embeddings%20from%20the%20NIGHTS%20dataset%20to%0Ainitialize%20a%20CLIP%20vision%20encoder%2C%20followed%20by%20self-supervised%20learning%20on%0AYFCC15M%2C%20our%20approach%20demonstrates%20significant%20zero-shot%20performance%0Aimprovements%2C%20without%20any%20task-specific%20fine-tuning%2C%20across%2029%20zero%20shot%0Aclassification%20and%202%20retrieval%20benchmarks.%20On%20ImageNet-1K%2C%20zero-shot%20gains%0Aemerge%20after%20approximately%2015%20epochs%20of%20pretraining.%20Benefits%20are%20observed%0Aacross%20datasets%20of%20various%20scales%2C%20with%20improvements%20manifesting%20at%20different%0Astages%20of%20the%20pretraining%20process%20depending%20on%20dataset%20characteristics.%20Our%0Aapproach%20consistently%20enhances%20zero-shot%20top-1%20accuracy%2C%20top-5%20accuracy%2C%20and%0Aretrieval%20recall%20%28e.g.%2C%20R%401%2C%20R%405%29%20across%20these%20diverse%20evaluation%20tasks%2C%0Awithout%20requiring%20any%20adaptation%20to%20target%20domains.%20These%20findings%20challenge%0Athe%20conventional%20wisdom%20of%20using%20human-perceptual%20data%20primarily%20for%0Afine-tuning%20and%20demonstrate%20that%20embedding%20human%20perceptual%20structure%20during%0Aearly%20representation%20learning%20yields%20more%20capable%20and%20vision-language%20aligned%0Asystems%20that%20generalize%20immediately%20to%20unseen%20tasks.%20Our%20work%20shows%20that%0A%22beginning%20with%20you%22%2C%20starting%20with%20human%20perception%2C%20provides%20a%20stronger%0Afoundation%20for%20general-purpose%20vision-language%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14204v1&entry.124074799=Read"},
{"title": "Video Compression Commander: Plug-and-Play Inference Acceleration for\n  Video Large Language Models", "author": "Xuyang Liu and Yiyu Wang and Junpeng Ma and Linfeng Zhang", "abstract": "  Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2.\n", "link": "http://arxiv.org/abs/2505.14454v1", "date": "2025-05-20", "relevancy": 2.8868, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5819}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Compression%20Commander%3A%20Plug-and-Play%20Inference%20Acceleration%20for%0A%20%20Video%20Large%20Language%20Models&body=Title%3A%20Video%20Compression%20Commander%3A%20Plug-and-Play%20Inference%20Acceleration%20for%0A%20%20Video%20Large%20Language%20Models%0AAuthor%3A%20Xuyang%20Liu%20and%20Yiyu%20Wang%20and%20Junpeng%20Ma%20and%20Linfeng%20Zhang%0AAbstract%3A%20%20%20Video%20large%20language%20models%20%28VideoLLM%29%20excel%20at%20video%20understanding%2C%20but%20face%0Aefficiency%20challenges%20due%20to%20the%20quadratic%20complexity%20of%20abundant%20visual%0Atokens.%20Our%20systematic%20analysis%20of%20token%20compression%20methods%20for%20VideoLLMs%0Areveals%20two%20critical%20issues%3A%20%28i%29%20overlooking%20distinctive%20visual%20signals%20across%0Aframes%2C%20leading%20to%20information%20loss%3B%20%28ii%29%20suffering%20from%20implementation%0Aconstraints%2C%20causing%20incompatibility%20with%20modern%20architectures%20or%20efficient%0Aoperators.%20To%20address%20these%20challenges%2C%20we%20distill%20three%20design%20principles%20for%0AVideoLLM%20token%20compression%20and%20propose%20a%20plug-and-play%20inference%20acceleration%0Aframework%20%22Video%20Compression%20Commander%22%20%28VidCom2%29.%20By%20quantifying%20each%20frame%27s%0Auniqueness%2C%20VidCom2%20adaptively%20adjusts%20compression%20intensity%20across%20frames%2C%0Aeffectively%20preserving%20essential%20information%20while%20reducing%20redundancy%20in%20video%0Asequences.%20Extensive%20experiments%20across%20various%20VideoLLMs%20and%20benchmarks%0Ademonstrate%20the%20superior%20performance%20and%20efficiency%20of%20our%20VidCom2.%20With%20only%0A25%25%20visual%20tokens%2C%20VidCom2%20achieves%2099.6%25%20of%20the%20original%20performance%20on%0ALLaVA-OV%20while%20reducing%2070.8%25%20of%20the%20LLM%20generation%20latency.%20Notably%2C%20our%20Frame%0ACompression%20Adjustment%20strategy%20is%20compatible%20with%20other%20token%20compression%0Amethods%20to%20further%20improve%20their%20performance.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xuyang-liu16/VidCom2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Compression%2520Commander%253A%2520Plug-and-Play%2520Inference%2520Acceleration%2520for%250A%2520%2520Video%2520Large%2520Language%2520Models%26entry.906535625%3DXuyang%2520Liu%2520and%2520Yiyu%2520Wang%2520and%2520Junpeng%2520Ma%2520and%2520Linfeng%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520large%2520language%2520models%2520%2528VideoLLM%2529%2520excel%2520at%2520video%2520understanding%252C%2520but%2520face%250Aefficiency%2520challenges%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520abundant%2520visual%250Atokens.%2520Our%2520systematic%2520analysis%2520of%2520token%2520compression%2520methods%2520for%2520VideoLLMs%250Areveals%2520two%2520critical%2520issues%253A%2520%2528i%2529%2520overlooking%2520distinctive%2520visual%2520signals%2520across%250Aframes%252C%2520leading%2520to%2520information%2520loss%253B%2520%2528ii%2529%2520suffering%2520from%2520implementation%250Aconstraints%252C%2520causing%2520incompatibility%2520with%2520modern%2520architectures%2520or%2520efficient%250Aoperators.%2520To%2520address%2520these%2520challenges%252C%2520we%2520distill%2520three%2520design%2520principles%2520for%250AVideoLLM%2520token%2520compression%2520and%2520propose%2520a%2520plug-and-play%2520inference%2520acceleration%250Aframework%2520%2522Video%2520Compression%2520Commander%2522%2520%2528VidCom2%2529.%2520By%2520quantifying%2520each%2520frame%2527s%250Auniqueness%252C%2520VidCom2%2520adaptively%2520adjusts%2520compression%2520intensity%2520across%2520frames%252C%250Aeffectively%2520preserving%2520essential%2520information%2520while%2520reducing%2520redundancy%2520in%2520video%250Asequences.%2520Extensive%2520experiments%2520across%2520various%2520VideoLLMs%2520and%2520benchmarks%250Ademonstrate%2520the%2520superior%2520performance%2520and%2520efficiency%2520of%2520our%2520VidCom2.%2520With%2520only%250A25%2525%2520visual%2520tokens%252C%2520VidCom2%2520achieves%252099.6%2525%2520of%2520the%2520original%2520performance%2520on%250ALLaVA-OV%2520while%2520reducing%252070.8%2525%2520of%2520the%2520LLM%2520generation%2520latency.%2520Notably%252C%2520our%2520Frame%250ACompression%2520Adjustment%2520strategy%2520is%2520compatible%2520with%2520other%2520token%2520compression%250Amethods%2520to%2520further%2520improve%2520their%2520performance.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/xuyang-liu16/VidCom2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Compression%20Commander%3A%20Plug-and-Play%20Inference%20Acceleration%20for%0A%20%20Video%20Large%20Language%20Models&entry.906535625=Xuyang%20Liu%20and%20Yiyu%20Wang%20and%20Junpeng%20Ma%20and%20Linfeng%20Zhang&entry.1292438233=%20%20Video%20large%20language%20models%20%28VideoLLM%29%20excel%20at%20video%20understanding%2C%20but%20face%0Aefficiency%20challenges%20due%20to%20the%20quadratic%20complexity%20of%20abundant%20visual%0Atokens.%20Our%20systematic%20analysis%20of%20token%20compression%20methods%20for%20VideoLLMs%0Areveals%20two%20critical%20issues%3A%20%28i%29%20overlooking%20distinctive%20visual%20signals%20across%0Aframes%2C%20leading%20to%20information%20loss%3B%20%28ii%29%20suffering%20from%20implementation%0Aconstraints%2C%20causing%20incompatibility%20with%20modern%20architectures%20or%20efficient%0Aoperators.%20To%20address%20these%20challenges%2C%20we%20distill%20three%20design%20principles%20for%0AVideoLLM%20token%20compression%20and%20propose%20a%20plug-and-play%20inference%20acceleration%0Aframework%20%22Video%20Compression%20Commander%22%20%28VidCom2%29.%20By%20quantifying%20each%20frame%27s%0Auniqueness%2C%20VidCom2%20adaptively%20adjusts%20compression%20intensity%20across%20frames%2C%0Aeffectively%20preserving%20essential%20information%20while%20reducing%20redundancy%20in%20video%0Asequences.%20Extensive%20experiments%20across%20various%20VideoLLMs%20and%20benchmarks%0Ademonstrate%20the%20superior%20performance%20and%20efficiency%20of%20our%20VidCom2.%20With%20only%0A25%25%20visual%20tokens%2C%20VidCom2%20achieves%2099.6%25%20of%20the%20original%20performance%20on%0ALLaVA-OV%20while%20reducing%2070.8%25%20of%20the%20LLM%20generation%20latency.%20Notably%2C%20our%20Frame%0ACompression%20Adjustment%20strategy%20is%20compatible%20with%20other%20token%20compression%0Amethods%20to%20further%20improve%20their%20performance.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xuyang-liu16/VidCom2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14454v1&entry.124074799=Read"},
{"title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement\n  Learning", "author": "Sule Bai and Mingxing Li and Yong Liu and Jing Tang and Haoji Zhang and Lei Sun and Xiangxiang Chu and Yansong Tang", "abstract": "  Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.\n", "link": "http://arxiv.org/abs/2505.14231v1", "date": "2025-05-20", "relevancy": 2.8563, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniVG-R1%3A%20Reasoning%20Guided%20Universal%20Visual%20Grounding%20with%20Reinforcement%0A%20%20Learning&body=Title%3A%20UniVG-R1%3A%20Reasoning%20Guided%20Universal%20Visual%20Grounding%20with%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Sule%20Bai%20and%20Mingxing%20Li%20and%20Yong%20Liu%20and%20Jing%20Tang%20and%20Haoji%20Zhang%20and%20Lei%20Sun%20and%20Xiangxiang%20Chu%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20Traditional%20visual%20grounding%20methods%20primarily%20focus%20on%20single-image%0Ascenarios%20with%20simple%20textual%20references.%20However%2C%20extending%20these%20methods%20to%0Areal-world%20scenarios%20that%20involve%20implicit%20and%20complex%20instructions%2C%0Aparticularly%20in%20conjunction%20with%20multiple%20images%2C%20poses%20significant%20challenges%2C%0Awhich%20is%20mainly%20due%20to%20the%20lack%20of%20advanced%20reasoning%20ability%20across%20diverse%0Amulti-modal%20contexts.%20In%20this%20work%2C%20we%20aim%20to%20address%20the%20more%20practical%0Auniversal%20grounding%20task%2C%20and%20propose%20UniVG-R1%2C%20a%20reasoning%20guided%20multimodal%0Alarge%20language%20model%20%28MLLM%29%20for%20universal%20visual%20grounding%2C%20which%20enhances%0Areasoning%20capabilities%20through%20reinforcement%20learning%20%28RL%29%20combined%20with%0Acold-start%20data.%20Specifically%2C%20we%20first%20construct%20a%20high-quality%0AChain-of-Thought%20%28CoT%29%20grounding%20dataset%2C%20annotated%20with%20detailed%20reasoning%0Achains%2C%20to%20guide%20the%20model%20towards%20correct%20reasoning%20paths%20via%20supervised%0Afine-tuning.%20Subsequently%2C%20we%20perform%20rule-based%20reinforcement%20learning%20to%0Aencourage%20the%20model%20to%20identify%20correct%20reasoning%20chains%2C%20thereby%20incentivizing%0Aits%20reasoning%20capabilities.%20In%20addition%2C%20we%20identify%20a%20difficulty%20bias%20arising%0Afrom%20the%20prevalence%20of%20easy%20samples%20as%20RL%20training%20progresses%2C%20and%20we%20propose%20a%0Adifficulty-aware%20weight%20adjustment%20strategy%20to%20further%20strengthen%20the%0Aperformance.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20UniVG-R1%2C%0Awhich%20achieves%20state-of-the-art%20performance%20on%20MIG-Bench%20with%20a%209.1%25%0Aimprovement%20over%20the%20previous%20method.%20Furthermore%2C%20our%20model%20exhibits%20strong%0Ageneralizability%2C%20achieving%20an%20average%20improvement%20of%2023.4%25%20in%20zero-shot%0Aperformance%20across%20four%20image%20and%20video%20reasoning%20grounding%20benchmarks.%20The%0Aproject%20page%20can%20be%20accessed%20at%20https%3A//amap-ml.github.io/UniVG-R1-page/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniVG-R1%253A%2520Reasoning%2520Guided%2520Universal%2520Visual%2520Grounding%2520with%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSule%2520Bai%2520and%2520Mingxing%2520Li%2520and%2520Yong%2520Liu%2520and%2520Jing%2520Tang%2520and%2520Haoji%2520Zhang%2520and%2520Lei%2520Sun%2520and%2520Xiangxiang%2520Chu%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520Traditional%2520visual%2520grounding%2520methods%2520primarily%2520focus%2520on%2520single-image%250Ascenarios%2520with%2520simple%2520textual%2520references.%2520However%252C%2520extending%2520these%2520methods%2520to%250Areal-world%2520scenarios%2520that%2520involve%2520implicit%2520and%2520complex%2520instructions%252C%250Aparticularly%2520in%2520conjunction%2520with%2520multiple%2520images%252C%2520poses%2520significant%2520challenges%252C%250Awhich%2520is%2520mainly%2520due%2520to%2520the%2520lack%2520of%2520advanced%2520reasoning%2520ability%2520across%2520diverse%250Amulti-modal%2520contexts.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520address%2520the%2520more%2520practical%250Auniversal%2520grounding%2520task%252C%2520and%2520propose%2520UniVG-R1%252C%2520a%2520reasoning%2520guided%2520multimodal%250Alarge%2520language%2520model%2520%2528MLLM%2529%2520for%2520universal%2520visual%2520grounding%252C%2520which%2520enhances%250Areasoning%2520capabilities%2520through%2520reinforcement%2520learning%2520%2528RL%2529%2520combined%2520with%250Acold-start%2520data.%2520Specifically%252C%2520we%2520first%2520construct%2520a%2520high-quality%250AChain-of-Thought%2520%2528CoT%2529%2520grounding%2520dataset%252C%2520annotated%2520with%2520detailed%2520reasoning%250Achains%252C%2520to%2520guide%2520the%2520model%2520towards%2520correct%2520reasoning%2520paths%2520via%2520supervised%250Afine-tuning.%2520Subsequently%252C%2520we%2520perform%2520rule-based%2520reinforcement%2520learning%2520to%250Aencourage%2520the%2520model%2520to%2520identify%2520correct%2520reasoning%2520chains%252C%2520thereby%2520incentivizing%250Aits%2520reasoning%2520capabilities.%2520In%2520addition%252C%2520we%2520identify%2520a%2520difficulty%2520bias%2520arising%250Afrom%2520the%2520prevalence%2520of%2520easy%2520samples%2520as%2520RL%2520training%2520progresses%252C%2520and%2520we%2520propose%2520a%250Adifficulty-aware%2520weight%2520adjustment%2520strategy%2520to%2520further%2520strengthen%2520the%250Aperformance.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520UniVG-R1%252C%250Awhich%2520achieves%2520state-of-the-art%2520performance%2520on%2520MIG-Bench%2520with%2520a%25209.1%2525%250Aimprovement%2520over%2520the%2520previous%2520method.%2520Furthermore%252C%2520our%2520model%2520exhibits%2520strong%250Ageneralizability%252C%2520achieving%2520an%2520average%2520improvement%2520of%252023.4%2525%2520in%2520zero-shot%250Aperformance%2520across%2520four%2520image%2520and%2520video%2520reasoning%2520grounding%2520benchmarks.%2520The%250Aproject%2520page%2520can%2520be%2520accessed%2520at%2520https%253A//amap-ml.github.io/UniVG-R1-page/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniVG-R1%3A%20Reasoning%20Guided%20Universal%20Visual%20Grounding%20with%20Reinforcement%0A%20%20Learning&entry.906535625=Sule%20Bai%20and%20Mingxing%20Li%20and%20Yong%20Liu%20and%20Jing%20Tang%20and%20Haoji%20Zhang%20and%20Lei%20Sun%20and%20Xiangxiang%20Chu%20and%20Yansong%20Tang&entry.1292438233=%20%20Traditional%20visual%20grounding%20methods%20primarily%20focus%20on%20single-image%0Ascenarios%20with%20simple%20textual%20references.%20However%2C%20extending%20these%20methods%20to%0Areal-world%20scenarios%20that%20involve%20implicit%20and%20complex%20instructions%2C%0Aparticularly%20in%20conjunction%20with%20multiple%20images%2C%20poses%20significant%20challenges%2C%0Awhich%20is%20mainly%20due%20to%20the%20lack%20of%20advanced%20reasoning%20ability%20across%20diverse%0Amulti-modal%20contexts.%20In%20this%20work%2C%20we%20aim%20to%20address%20the%20more%20practical%0Auniversal%20grounding%20task%2C%20and%20propose%20UniVG-R1%2C%20a%20reasoning%20guided%20multimodal%0Alarge%20language%20model%20%28MLLM%29%20for%20universal%20visual%20grounding%2C%20which%20enhances%0Areasoning%20capabilities%20through%20reinforcement%20learning%20%28RL%29%20combined%20with%0Acold-start%20data.%20Specifically%2C%20we%20first%20construct%20a%20high-quality%0AChain-of-Thought%20%28CoT%29%20grounding%20dataset%2C%20annotated%20with%20detailed%20reasoning%0Achains%2C%20to%20guide%20the%20model%20towards%20correct%20reasoning%20paths%20via%20supervised%0Afine-tuning.%20Subsequently%2C%20we%20perform%20rule-based%20reinforcement%20learning%20to%0Aencourage%20the%20model%20to%20identify%20correct%20reasoning%20chains%2C%20thereby%20incentivizing%0Aits%20reasoning%20capabilities.%20In%20addition%2C%20we%20identify%20a%20difficulty%20bias%20arising%0Afrom%20the%20prevalence%20of%20easy%20samples%20as%20RL%20training%20progresses%2C%20and%20we%20propose%20a%0Adifficulty-aware%20weight%20adjustment%20strategy%20to%20further%20strengthen%20the%0Aperformance.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20UniVG-R1%2C%0Awhich%20achieves%20state-of-the-art%20performance%20on%20MIG-Bench%20with%20a%209.1%25%0Aimprovement%20over%20the%20previous%20method.%20Furthermore%2C%20our%20model%20exhibits%20strong%0Ageneralizability%2C%20achieving%20an%20average%20improvement%20of%2023.4%25%20in%20zero-shot%0Aperformance%20across%20four%20image%20and%20video%20reasoning%20grounding%20benchmarks.%20The%0Aproject%20page%20can%20be%20accessed%20at%20https%3A//amap-ml.github.io/UniVG-R1-page/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14231v1&entry.124074799=Read"},
{"title": "ViMo: A Generative Visual GUI World Model for App Agents", "author": "Dezhao Luo and Bohan Tang and Kang Li and Georgios Papoudakis and Jifei Song and Shaogang Gong and Jianye Hao and Jun Wang and Kun Shao", "abstract": "  App agents, which autonomously operate mobile Apps through Graphical User\nInterfaces (GUIs), have gained significant interest in real-world applications.\nYet, they often struggle with long-horizon planning, failing to find the\noptimal actions for complex tasks with longer steps. To address this, world\nmodels are used to predict the next GUI observation based on user actions,\nenabling more effective agent planning. However, existing world models\nprimarily focus on generating only textual descriptions, lacking essential\nvisual details. To fill this gap, we propose ViMo, the first visual world model\ndesigned to generate future App observations as images. For the challenge of\ngenerating text in image patches, where even minor pixel errors can distort\nreadability, we decompose GUI generation into graphic and text content\ngeneration. We propose a novel data representation, the Symbolic Text\nRepresentation~(STR) to overlay text content with symbolic placeholders while\npreserving graphics. With this design, ViMo employs a STR Predictor to predict\nfuture GUIs' graphics and a GUI-text Predictor for generating the corresponding\ntext. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the\noutcome of different action options. Experiments show ViMo's ability to\ngenerate visually plausible and functionally effective GUIs that enable App\nagents to make more informed decisions.\n", "link": "http://arxiv.org/abs/2504.13936v2", "date": "2025-05-20", "relevancy": 2.8131, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5969}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5634}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViMo%3A%20A%20Generative%20Visual%20GUI%20World%20Model%20for%20App%20Agents&body=Title%3A%20ViMo%3A%20A%20Generative%20Visual%20GUI%20World%20Model%20for%20App%20Agents%0AAuthor%3A%20Dezhao%20Luo%20and%20Bohan%20Tang%20and%20Kang%20Li%20and%20Georgios%20Papoudakis%20and%20Jifei%20Song%20and%20Shaogang%20Gong%20and%20Jianye%20Hao%20and%20Jun%20Wang%20and%20Kun%20Shao%0AAbstract%3A%20%20%20App%20agents%2C%20which%20autonomously%20operate%20mobile%20Apps%20through%20Graphical%20User%0AInterfaces%20%28GUIs%29%2C%20have%20gained%20significant%20interest%20in%20real-world%20applications.%0AYet%2C%20they%20often%20struggle%20with%20long-horizon%20planning%2C%20failing%20to%20find%20the%0Aoptimal%20actions%20for%20complex%20tasks%20with%20longer%20steps.%20To%20address%20this%2C%20world%0Amodels%20are%20used%20to%20predict%20the%20next%20GUI%20observation%20based%20on%20user%20actions%2C%0Aenabling%20more%20effective%20agent%20planning.%20However%2C%20existing%20world%20models%0Aprimarily%20focus%20on%20generating%20only%20textual%20descriptions%2C%20lacking%20essential%0Avisual%20details.%20To%20fill%20this%20gap%2C%20we%20propose%20ViMo%2C%20the%20first%20visual%20world%20model%0Adesigned%20to%20generate%20future%20App%20observations%20as%20images.%20For%20the%20challenge%20of%0Agenerating%20text%20in%20image%20patches%2C%20where%20even%20minor%20pixel%20errors%20can%20distort%0Areadability%2C%20we%20decompose%20GUI%20generation%20into%20graphic%20and%20text%20content%0Ageneration.%20We%20propose%20a%20novel%20data%20representation%2C%20the%20Symbolic%20Text%0ARepresentation~%28STR%29%20to%20overlay%20text%20content%20with%20symbolic%20placeholders%20while%0Apreserving%20graphics.%20With%20this%20design%2C%20ViMo%20employs%20a%20STR%20Predictor%20to%20predict%0Afuture%20GUIs%27%20graphics%20and%20a%20GUI-text%20Predictor%20for%20generating%20the%20corresponding%0Atext.%20Moreover%2C%20we%20deploy%20ViMo%20to%20enhance%20agent-focused%20tasks%20by%20predicting%20the%0Aoutcome%20of%20different%20action%20options.%20Experiments%20show%20ViMo%27s%20ability%20to%0Agenerate%20visually%20plausible%20and%20functionally%20effective%20GUIs%20that%20enable%20App%0Aagents%20to%20make%20more%20informed%20decisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViMo%253A%2520A%2520Generative%2520Visual%2520GUI%2520World%2520Model%2520for%2520App%2520Agents%26entry.906535625%3DDezhao%2520Luo%2520and%2520Bohan%2520Tang%2520and%2520Kang%2520Li%2520and%2520Georgios%2520Papoudakis%2520and%2520Jifei%2520Song%2520and%2520Shaogang%2520Gong%2520and%2520Jianye%2520Hao%2520and%2520Jun%2520Wang%2520and%2520Kun%2520Shao%26entry.1292438233%3D%2520%2520App%2520agents%252C%2520which%2520autonomously%2520operate%2520mobile%2520Apps%2520through%2520Graphical%2520User%250AInterfaces%2520%2528GUIs%2529%252C%2520have%2520gained%2520significant%2520interest%2520in%2520real-world%2520applications.%250AYet%252C%2520they%2520often%2520struggle%2520with%2520long-horizon%2520planning%252C%2520failing%2520to%2520find%2520the%250Aoptimal%2520actions%2520for%2520complex%2520tasks%2520with%2520longer%2520steps.%2520To%2520address%2520this%252C%2520world%250Amodels%2520are%2520used%2520to%2520predict%2520the%2520next%2520GUI%2520observation%2520based%2520on%2520user%2520actions%252C%250Aenabling%2520more%2520effective%2520agent%2520planning.%2520However%252C%2520existing%2520world%2520models%250Aprimarily%2520focus%2520on%2520generating%2520only%2520textual%2520descriptions%252C%2520lacking%2520essential%250Avisual%2520details.%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520ViMo%252C%2520the%2520first%2520visual%2520world%2520model%250Adesigned%2520to%2520generate%2520future%2520App%2520observations%2520as%2520images.%2520For%2520the%2520challenge%2520of%250Agenerating%2520text%2520in%2520image%2520patches%252C%2520where%2520even%2520minor%2520pixel%2520errors%2520can%2520distort%250Areadability%252C%2520we%2520decompose%2520GUI%2520generation%2520into%2520graphic%2520and%2520text%2520content%250Ageneration.%2520We%2520propose%2520a%2520novel%2520data%2520representation%252C%2520the%2520Symbolic%2520Text%250ARepresentation~%2528STR%2529%2520to%2520overlay%2520text%2520content%2520with%2520symbolic%2520placeholders%2520while%250Apreserving%2520graphics.%2520With%2520this%2520design%252C%2520ViMo%2520employs%2520a%2520STR%2520Predictor%2520to%2520predict%250Afuture%2520GUIs%2527%2520graphics%2520and%2520a%2520GUI-text%2520Predictor%2520for%2520generating%2520the%2520corresponding%250Atext.%2520Moreover%252C%2520we%2520deploy%2520ViMo%2520to%2520enhance%2520agent-focused%2520tasks%2520by%2520predicting%2520the%250Aoutcome%2520of%2520different%2520action%2520options.%2520Experiments%2520show%2520ViMo%2527s%2520ability%2520to%250Agenerate%2520visually%2520plausible%2520and%2520functionally%2520effective%2520GUIs%2520that%2520enable%2520App%250Aagents%2520to%2520make%2520more%2520informed%2520decisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViMo%3A%20A%20Generative%20Visual%20GUI%20World%20Model%20for%20App%20Agents&entry.906535625=Dezhao%20Luo%20and%20Bohan%20Tang%20and%20Kang%20Li%20and%20Georgios%20Papoudakis%20and%20Jifei%20Song%20and%20Shaogang%20Gong%20and%20Jianye%20Hao%20and%20Jun%20Wang%20and%20Kun%20Shao&entry.1292438233=%20%20App%20agents%2C%20which%20autonomously%20operate%20mobile%20Apps%20through%20Graphical%20User%0AInterfaces%20%28GUIs%29%2C%20have%20gained%20significant%20interest%20in%20real-world%20applications.%0AYet%2C%20they%20often%20struggle%20with%20long-horizon%20planning%2C%20failing%20to%20find%20the%0Aoptimal%20actions%20for%20complex%20tasks%20with%20longer%20steps.%20To%20address%20this%2C%20world%0Amodels%20are%20used%20to%20predict%20the%20next%20GUI%20observation%20based%20on%20user%20actions%2C%0Aenabling%20more%20effective%20agent%20planning.%20However%2C%20existing%20world%20models%0Aprimarily%20focus%20on%20generating%20only%20textual%20descriptions%2C%20lacking%20essential%0Avisual%20details.%20To%20fill%20this%20gap%2C%20we%20propose%20ViMo%2C%20the%20first%20visual%20world%20model%0Adesigned%20to%20generate%20future%20App%20observations%20as%20images.%20For%20the%20challenge%20of%0Agenerating%20text%20in%20image%20patches%2C%20where%20even%20minor%20pixel%20errors%20can%20distort%0Areadability%2C%20we%20decompose%20GUI%20generation%20into%20graphic%20and%20text%20content%0Ageneration.%20We%20propose%20a%20novel%20data%20representation%2C%20the%20Symbolic%20Text%0ARepresentation~%28STR%29%20to%20overlay%20text%20content%20with%20symbolic%20placeholders%20while%0Apreserving%20graphics.%20With%20this%20design%2C%20ViMo%20employs%20a%20STR%20Predictor%20to%20predict%0Afuture%20GUIs%27%20graphics%20and%20a%20GUI-text%20Predictor%20for%20generating%20the%20corresponding%0Atext.%20Moreover%2C%20we%20deploy%20ViMo%20to%20enhance%20agent-focused%20tasks%20by%20predicting%20the%0Aoutcome%20of%20different%20action%20options.%20Experiments%20show%20ViMo%27s%20ability%20to%0Agenerate%20visually%20plausible%20and%20functionally%20effective%20GUIs%20that%20enable%20App%0Aagents%20to%20make%20more%20informed%20decisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13936v2&entry.124074799=Read"},
{"title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided\n  Design Code Generation", "author": "Anna C. Doris and Md Ferdous Alam and Amin Heyrani Nobari and Faez Ahmed", "abstract": "  Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.\n", "link": "http://arxiv.org/abs/2505.14646v1", "date": "2025-05-20", "relevancy": 2.8122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5628}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Coder%3A%20An%20Open-Source%20Vision-Language%20Model%20for%20Computer-Aided%0A%20%20Design%20Code%20Generation&body=Title%3A%20CAD-Coder%3A%20An%20Open-Source%20Vision-Language%20Model%20for%20Computer-Aided%0A%20%20Design%20Code%20Generation%0AAuthor%3A%20Anna%20C.%20Doris%20and%20Md%20Ferdous%20Alam%20and%20Amin%20Heyrani%20Nobari%20and%20Faez%20Ahmed%0AAbstract%3A%20%20%20Efficient%20creation%20of%20accurate%20and%20editable%203D%20CAD%20models%20is%20critical%20in%0Aengineering%20design%2C%20significantly%20impacting%20cost%20and%20time-to-market%20in%20product%0Ainnovation.%20Current%20manual%20workflows%20remain%20highly%20time-consuming%20and%20demand%0Aextensive%20user%20expertise.%20While%20recent%20developments%20in%20AI-driven%20CAD%20generation%0Ashow%20promise%2C%20existing%20models%20are%20limited%20by%20incomplete%20representations%20of%20CAD%0Aoperations%2C%20inability%20to%20generalize%20to%20real-world%20images%2C%20and%20low%20output%0Aaccuracy.%20This%20paper%20introduces%20CAD-Coder%2C%20an%20open-source%20Vision-Language%20Model%0A%28VLM%29%20explicitly%20fine-tuned%20to%20generate%20editable%20CAD%20code%20%28CadQuery%20Python%29%0Adirectly%20from%20visual%20input.%20Leveraging%20a%20novel%20dataset%20that%20we%0Acreated--GenCAD-Code%2C%20consisting%20of%20over%20163k%20CAD-model%20image%20and%20code%0Apairs--CAD-Coder%20outperforms%20state-of-the-art%20VLM%20baselines%20such%20as%20GPT-4.5%20and%0AQwen2.5-VL-72B%2C%20achieving%20a%20100%25%20valid%20syntax%20rate%20and%20the%20highest%20accuracy%20in%0A3D%20solid%20similarity.%20Notably%2C%20our%20VLM%20demonstrates%20some%20signs%20of%0Ageneralizability%2C%20successfully%20generating%20CAD%20code%20from%20real-world%20images%20and%0Aexecuting%20CAD%20operations%20unseen%20during%20fine-tuning.%20The%20performance%20and%0Aadaptability%20of%20CAD-Coder%20highlights%20the%20potential%20of%20VLMs%20fine-tuned%20on%20code%0Ato%20streamline%20CAD%20workflows%20for%20engineers%20and%20designers.%20CAD-Coder%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/anniedoris/CAD-Coder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14646v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Coder%253A%2520An%2520Open-Source%2520Vision-Language%2520Model%2520for%2520Computer-Aided%250A%2520%2520Design%2520Code%2520Generation%26entry.906535625%3DAnna%2520C.%2520Doris%2520and%2520Md%2520Ferdous%2520Alam%2520and%2520Amin%2520Heyrani%2520Nobari%2520and%2520Faez%2520Ahmed%26entry.1292438233%3D%2520%2520Efficient%2520creation%2520of%2520accurate%2520and%2520editable%25203D%2520CAD%2520models%2520is%2520critical%2520in%250Aengineering%2520design%252C%2520significantly%2520impacting%2520cost%2520and%2520time-to-market%2520in%2520product%250Ainnovation.%2520Current%2520manual%2520workflows%2520remain%2520highly%2520time-consuming%2520and%2520demand%250Aextensive%2520user%2520expertise.%2520While%2520recent%2520developments%2520in%2520AI-driven%2520CAD%2520generation%250Ashow%2520promise%252C%2520existing%2520models%2520are%2520limited%2520by%2520incomplete%2520representations%2520of%2520CAD%250Aoperations%252C%2520inability%2520to%2520generalize%2520to%2520real-world%2520images%252C%2520and%2520low%2520output%250Aaccuracy.%2520This%2520paper%2520introduces%2520CAD-Coder%252C%2520an%2520open-source%2520Vision-Language%2520Model%250A%2528VLM%2529%2520explicitly%2520fine-tuned%2520to%2520generate%2520editable%2520CAD%2520code%2520%2528CadQuery%2520Python%2529%250Adirectly%2520from%2520visual%2520input.%2520Leveraging%2520a%2520novel%2520dataset%2520that%2520we%250Acreated--GenCAD-Code%252C%2520consisting%2520of%2520over%2520163k%2520CAD-model%2520image%2520and%2520code%250Apairs--CAD-Coder%2520outperforms%2520state-of-the-art%2520VLM%2520baselines%2520such%2520as%2520GPT-4.5%2520and%250AQwen2.5-VL-72B%252C%2520achieving%2520a%2520100%2525%2520valid%2520syntax%2520rate%2520and%2520the%2520highest%2520accuracy%2520in%250A3D%2520solid%2520similarity.%2520Notably%252C%2520our%2520VLM%2520demonstrates%2520some%2520signs%2520of%250Ageneralizability%252C%2520successfully%2520generating%2520CAD%2520code%2520from%2520real-world%2520images%2520and%250Aexecuting%2520CAD%2520operations%2520unseen%2520during%2520fine-tuning.%2520The%2520performance%2520and%250Aadaptability%2520of%2520CAD-Coder%2520highlights%2520the%2520potential%2520of%2520VLMs%2520fine-tuned%2520on%2520code%250Ato%2520streamline%2520CAD%2520workflows%2520for%2520engineers%2520and%2520designers.%2520CAD-Coder%2520is%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/anniedoris/CAD-Coder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14646v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Coder%3A%20An%20Open-Source%20Vision-Language%20Model%20for%20Computer-Aided%0A%20%20Design%20Code%20Generation&entry.906535625=Anna%20C.%20Doris%20and%20Md%20Ferdous%20Alam%20and%20Amin%20Heyrani%20Nobari%20and%20Faez%20Ahmed&entry.1292438233=%20%20Efficient%20creation%20of%20accurate%20and%20editable%203D%20CAD%20models%20is%20critical%20in%0Aengineering%20design%2C%20significantly%20impacting%20cost%20and%20time-to-market%20in%20product%0Ainnovation.%20Current%20manual%20workflows%20remain%20highly%20time-consuming%20and%20demand%0Aextensive%20user%20expertise.%20While%20recent%20developments%20in%20AI-driven%20CAD%20generation%0Ashow%20promise%2C%20existing%20models%20are%20limited%20by%20incomplete%20representations%20of%20CAD%0Aoperations%2C%20inability%20to%20generalize%20to%20real-world%20images%2C%20and%20low%20output%0Aaccuracy.%20This%20paper%20introduces%20CAD-Coder%2C%20an%20open-source%20Vision-Language%20Model%0A%28VLM%29%20explicitly%20fine-tuned%20to%20generate%20editable%20CAD%20code%20%28CadQuery%20Python%29%0Adirectly%20from%20visual%20input.%20Leveraging%20a%20novel%20dataset%20that%20we%0Acreated--GenCAD-Code%2C%20consisting%20of%20over%20163k%20CAD-model%20image%20and%20code%0Apairs--CAD-Coder%20outperforms%20state-of-the-art%20VLM%20baselines%20such%20as%20GPT-4.5%20and%0AQwen2.5-VL-72B%2C%20achieving%20a%20100%25%20valid%20syntax%20rate%20and%20the%20highest%20accuracy%20in%0A3D%20solid%20similarity.%20Notably%2C%20our%20VLM%20demonstrates%20some%20signs%20of%0Ageneralizability%2C%20successfully%20generating%20CAD%20code%20from%20real-world%20images%20and%0Aexecuting%20CAD%20operations%20unseen%20during%20fine-tuning.%20The%20performance%20and%0Aadaptability%20of%20CAD-Coder%20highlights%20the%20potential%20of%20VLMs%20fine-tuned%20on%20code%0Ato%20streamline%20CAD%20workflows%20for%20engineers%20and%20designers.%20CAD-Coder%20is%20publicly%0Aavailable%20at%3A%20https%3A//github.com/anniedoris/CAD-Coder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14646v1&entry.124074799=Read"},
{"title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning", "author": "Jiaer Xia and Yuhang Zang and Peng Gao and Yixuan Li and Kaiyang Zhou", "abstract": "  Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.\n", "link": "http://arxiv.org/abs/2505.14677v1", "date": "2025-05-20", "relevancy": 2.7837, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5694}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visionary-R1%3A%20Mitigating%20Shortcuts%20in%20Visual%20Reasoning%20with%0A%20%20Reinforcement%20Learning&body=Title%3A%20Visionary-R1%3A%20Mitigating%20Shortcuts%20in%20Visual%20Reasoning%20with%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Jiaer%20Xia%20and%20Yuhang%20Zang%20and%20Peng%20Gao%20and%20Yixuan%20Li%20and%20Kaiyang%20Zhou%0AAbstract%3A%20%20%20Learning%20general-purpose%20reasoning%20capabilities%20has%20long%20been%20a%20challenging%0Aproblem%20in%20AI.%20Recent%20research%20in%20large%20language%20models%20%28LLMs%29%2C%20such%20as%0ADeepSeek-R1%2C%20has%20shown%20that%20reinforcement%20learning%20techniques%20like%20GRPO%20can%0Aenable%20pre-trained%20LLMs%20to%20develop%20reasoning%20capabilities%20using%20simple%0Aquestion-answer%20pairs.%20In%20this%20paper%2C%20we%20aim%20to%20train%20visual%20language%20models%0A%28VLMs%29%20to%20perform%20reasoning%20on%20image%20data%20through%20reinforcement%20learning%20and%0Avisual%20question-answer%20pairs%2C%20without%20any%20explicit%20chain-of-thought%20%28CoT%29%0Asupervision.%20Our%20findings%20indicate%20that%20simply%20applying%20reinforcement%20learning%0Ato%20a%20VLM%20--%20by%20prompting%20the%20model%20to%20produce%20a%20reasoning%20chain%20before%0Aproviding%20an%20answer%20--%20can%20lead%20the%20model%20to%20develop%20shortcuts%20from%20easy%0Aquestions%2C%20thereby%20reducing%20its%20ability%20to%20generalize%20across%20unseen%20data%0Adistributions.%20We%20argue%20that%20the%20key%20to%20mitigating%20shortcut%20learning%20is%20to%0Aencourage%20the%20model%20to%20interpret%20images%20prior%20to%20reasoning.%20Therefore%2C%20we%20train%0Athe%20model%20to%20adhere%20to%20a%20caption-reason-answer%20output%20format%3A%20initially%0Agenerating%20a%20detailed%20caption%20for%20an%20image%2C%20followed%20by%20constructing%20an%0Aextensive%20reasoning%20chain.%20When%20trained%20on%20273K%20CoT-free%20visual%20question-answer%0Apairs%20and%20using%20only%20reinforcement%20learning%2C%20our%20model%2C%20named%20Visionary-R1%2C%0Aoutperforms%20strong%20multimodal%20models%2C%20such%20as%20GPT-4o%2C%20Claude3.5-Sonnet%2C%20and%0AGemini-1.5-Pro%2C%20on%20multiple%20visual%20reasoning%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionary-R1%253A%2520Mitigating%2520Shortcuts%2520in%2520Visual%2520Reasoning%2520with%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DJiaer%2520Xia%2520and%2520Yuhang%2520Zang%2520and%2520Peng%2520Gao%2520and%2520Yixuan%2520Li%2520and%2520Kaiyang%2520Zhou%26entry.1292438233%3D%2520%2520Learning%2520general-purpose%2520reasoning%2520capabilities%2520has%2520long%2520been%2520a%2520challenging%250Aproblem%2520in%2520AI.%2520Recent%2520research%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%250ADeepSeek-R1%252C%2520has%2520shown%2520that%2520reinforcement%2520learning%2520techniques%2520like%2520GRPO%2520can%250Aenable%2520pre-trained%2520LLMs%2520to%2520develop%2520reasoning%2520capabilities%2520using%2520simple%250Aquestion-answer%2520pairs.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520train%2520visual%2520language%2520models%250A%2528VLMs%2529%2520to%2520perform%2520reasoning%2520on%2520image%2520data%2520through%2520reinforcement%2520learning%2520and%250Avisual%2520question-answer%2520pairs%252C%2520without%2520any%2520explicit%2520chain-of-thought%2520%2528CoT%2529%250Asupervision.%2520Our%2520findings%2520indicate%2520that%2520simply%2520applying%2520reinforcement%2520learning%250Ato%2520a%2520VLM%2520--%2520by%2520prompting%2520the%2520model%2520to%2520produce%2520a%2520reasoning%2520chain%2520before%250Aproviding%2520an%2520answer%2520--%2520can%2520lead%2520the%2520model%2520to%2520develop%2520shortcuts%2520from%2520easy%250Aquestions%252C%2520thereby%2520reducing%2520its%2520ability%2520to%2520generalize%2520across%2520unseen%2520data%250Adistributions.%2520We%2520argue%2520that%2520the%2520key%2520to%2520mitigating%2520shortcut%2520learning%2520is%2520to%250Aencourage%2520the%2520model%2520to%2520interpret%2520images%2520prior%2520to%2520reasoning.%2520Therefore%252C%2520we%2520train%250Athe%2520model%2520to%2520adhere%2520to%2520a%2520caption-reason-answer%2520output%2520format%253A%2520initially%250Agenerating%2520a%2520detailed%2520caption%2520for%2520an%2520image%252C%2520followed%2520by%2520constructing%2520an%250Aextensive%2520reasoning%2520chain.%2520When%2520trained%2520on%2520273K%2520CoT-free%2520visual%2520question-answer%250Apairs%2520and%2520using%2520only%2520reinforcement%2520learning%252C%2520our%2520model%252C%2520named%2520Visionary-R1%252C%250Aoutperforms%2520strong%2520multimodal%2520models%252C%2520such%2520as%2520GPT-4o%252C%2520Claude3.5-Sonnet%252C%2520and%250AGemini-1.5-Pro%252C%2520on%2520multiple%2520visual%2520reasoning%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visionary-R1%3A%20Mitigating%20Shortcuts%20in%20Visual%20Reasoning%20with%0A%20%20Reinforcement%20Learning&entry.906535625=Jiaer%20Xia%20and%20Yuhang%20Zang%20and%20Peng%20Gao%20and%20Yixuan%20Li%20and%20Kaiyang%20Zhou&entry.1292438233=%20%20Learning%20general-purpose%20reasoning%20capabilities%20has%20long%20been%20a%20challenging%0Aproblem%20in%20AI.%20Recent%20research%20in%20large%20language%20models%20%28LLMs%29%2C%20such%20as%0ADeepSeek-R1%2C%20has%20shown%20that%20reinforcement%20learning%20techniques%20like%20GRPO%20can%0Aenable%20pre-trained%20LLMs%20to%20develop%20reasoning%20capabilities%20using%20simple%0Aquestion-answer%20pairs.%20In%20this%20paper%2C%20we%20aim%20to%20train%20visual%20language%20models%0A%28VLMs%29%20to%20perform%20reasoning%20on%20image%20data%20through%20reinforcement%20learning%20and%0Avisual%20question-answer%20pairs%2C%20without%20any%20explicit%20chain-of-thought%20%28CoT%29%0Asupervision.%20Our%20findings%20indicate%20that%20simply%20applying%20reinforcement%20learning%0Ato%20a%20VLM%20--%20by%20prompting%20the%20model%20to%20produce%20a%20reasoning%20chain%20before%0Aproviding%20an%20answer%20--%20can%20lead%20the%20model%20to%20develop%20shortcuts%20from%20easy%0Aquestions%2C%20thereby%20reducing%20its%20ability%20to%20generalize%20across%20unseen%20data%0Adistributions.%20We%20argue%20that%20the%20key%20to%20mitigating%20shortcut%20learning%20is%20to%0Aencourage%20the%20model%20to%20interpret%20images%20prior%20to%20reasoning.%20Therefore%2C%20we%20train%0Athe%20model%20to%20adhere%20to%20a%20caption-reason-answer%20output%20format%3A%20initially%0Agenerating%20a%20detailed%20caption%20for%20an%20image%2C%20followed%20by%20constructing%20an%0Aextensive%20reasoning%20chain.%20When%20trained%20on%20273K%20CoT-free%20visual%20question-answer%0Apairs%20and%20using%20only%20reinforcement%20learning%2C%20our%20model%2C%20named%20Visionary-R1%2C%0Aoutperforms%20strong%20multimodal%20models%2C%20such%20as%20GPT-4o%2C%20Claude3.5-Sonnet%2C%20and%0AGemini-1.5-Pro%2C%20on%20multiple%20visual%20reasoning%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14677v1&entry.124074799=Read"},
{"title": "Breaking Language Barriers in Visual Language Models via Multilingual\n  Textual Regularization", "author": "I\u00f1igo Pikabea and I\u00f1aki Lacunza and Oriol Pareras and Carlos Escolano and Aitor Gonzalez-Agirre and Javier Hernando and Marta Villegas", "abstract": "  Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.\n", "link": "http://arxiv.org/abs/2503.22577v2", "date": "2025-05-20", "relevancy": 2.7747, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Language%20Barriers%20in%20Visual%20Language%20Models%20via%20Multilingual%0A%20%20Textual%20Regularization&body=Title%3A%20Breaking%20Language%20Barriers%20in%20Visual%20Language%20Models%20via%20Multilingual%0A%20%20Textual%20Regularization%0AAuthor%3A%20I%C3%B1igo%20Pikabea%20and%20I%C3%B1aki%20Lacunza%20and%20Oriol%20Pareras%20and%20Carlos%20Escolano%20and%20Aitor%20Gonzalez-Agirre%20and%20Javier%20Hernando%20and%20Marta%20Villegas%0AAbstract%3A%20%20%20Rapid%20advancements%20in%20Visual%20Language%20Models%20%28VLMs%29%20have%20transformed%0Amultimodal%20understanding%20but%20are%20often%20constrained%20by%20generating%20English%0Aresponses%20regardless%20of%20the%20input%20language.%20This%20phenomenon%20has%20been%20termed%20as%0AImage-induced%20Fidelity%20Loss%20%28IFL%29%20and%20stems%20from%20limited%20multimodal%0Amultilingual%20training%20data.%20To%20address%20this%2C%20we%20propose%20a%20continuous%0Amultilingual%20integration%20strategy%20that%20injects%20text-only%20multilingual%20data%0Aduring%20visual%20instruction%20tuning%2C%20preserving%20the%20language%20model%27s%20original%0Amultilingual%20capabilities.%20Extensive%20evaluations%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20linguistic%20fidelity%20across%20languages%20without%20degradation%0Ain%20visual%20performance.%20We%20also%20explore%20model%20merging%2C%20which%20improves%20language%0Afidelity%20but%20comes%20at%20the%20cost%20of%20visual%20performance.%20In%20contrast%2C%20our%20core%0Amethod%20achieves%20robust%20multilingual%20alignment%20without%20trade-offs%2C%20offering%20a%0Ascalable%20and%20effective%20path%20to%20mitigating%20IFL%20for%20global%20VLM%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Language%2520Barriers%2520in%2520Visual%2520Language%2520Models%2520via%2520Multilingual%250A%2520%2520Textual%2520Regularization%26entry.906535625%3DI%25C3%25B1igo%2520Pikabea%2520and%2520I%25C3%25B1aki%2520Lacunza%2520and%2520Oriol%2520Pareras%2520and%2520Carlos%2520Escolano%2520and%2520Aitor%2520Gonzalez-Agirre%2520and%2520Javier%2520Hernando%2520and%2520Marta%2520Villegas%26entry.1292438233%3D%2520%2520Rapid%2520advancements%2520in%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520have%2520transformed%250Amultimodal%2520understanding%2520but%2520are%2520often%2520constrained%2520by%2520generating%2520English%250Aresponses%2520regardless%2520of%2520the%2520input%2520language.%2520This%2520phenomenon%2520has%2520been%2520termed%2520as%250AImage-induced%2520Fidelity%2520Loss%2520%2528IFL%2529%2520and%2520stems%2520from%2520limited%2520multimodal%250Amultilingual%2520training%2520data.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520continuous%250Amultilingual%2520integration%2520strategy%2520that%2520injects%2520text-only%2520multilingual%2520data%250Aduring%2520visual%2520instruction%2520tuning%252C%2520preserving%2520the%2520language%2520model%2527s%2520original%250Amultilingual%2520capabilities.%2520Extensive%2520evaluations%2520demonstrate%2520that%2520our%2520approach%250Asignificantly%2520improves%2520linguistic%2520fidelity%2520across%2520languages%2520without%2520degradation%250Ain%2520visual%2520performance.%2520We%2520also%2520explore%2520model%2520merging%252C%2520which%2520improves%2520language%250Afidelity%2520but%2520comes%2520at%2520the%2520cost%2520of%2520visual%2520performance.%2520In%2520contrast%252C%2520our%2520core%250Amethod%2520achieves%2520robust%2520multilingual%2520alignment%2520without%2520trade-offs%252C%2520offering%2520a%250Ascalable%2520and%2520effective%2520path%2520to%2520mitigating%2520IFL%2520for%2520global%2520VLM%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Language%20Barriers%20in%20Visual%20Language%20Models%20via%20Multilingual%0A%20%20Textual%20Regularization&entry.906535625=I%C3%B1igo%20Pikabea%20and%20I%C3%B1aki%20Lacunza%20and%20Oriol%20Pareras%20and%20Carlos%20Escolano%20and%20Aitor%20Gonzalez-Agirre%20and%20Javier%20Hernando%20and%20Marta%20Villegas&entry.1292438233=%20%20Rapid%20advancements%20in%20Visual%20Language%20Models%20%28VLMs%29%20have%20transformed%0Amultimodal%20understanding%20but%20are%20often%20constrained%20by%20generating%20English%0Aresponses%20regardless%20of%20the%20input%20language.%20This%20phenomenon%20has%20been%20termed%20as%0AImage-induced%20Fidelity%20Loss%20%28IFL%29%20and%20stems%20from%20limited%20multimodal%0Amultilingual%20training%20data.%20To%20address%20this%2C%20we%20propose%20a%20continuous%0Amultilingual%20integration%20strategy%20that%20injects%20text-only%20multilingual%20data%0Aduring%20visual%20instruction%20tuning%2C%20preserving%20the%20language%20model%27s%20original%0Amultilingual%20capabilities.%20Extensive%20evaluations%20demonstrate%20that%20our%20approach%0Asignificantly%20improves%20linguistic%20fidelity%20across%20languages%20without%20degradation%0Ain%20visual%20performance.%20We%20also%20explore%20model%20merging%2C%20which%20improves%20language%0Afidelity%20but%20comes%20at%20the%20cost%20of%20visual%20performance.%20In%20contrast%2C%20our%20core%0Amethod%20achieves%20robust%20multilingual%20alignment%20without%20trade-offs%2C%20offering%20a%0Ascalable%20and%20effective%20path%20to%20mitigating%20IFL%20for%20global%20VLM%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22577v2&entry.124074799=Read"},
{"title": "SpaceJAM: a Lightweight and Regularization-free Method for Fast Joint\n  Alignment of Images", "author": "Nir Barel and Ron Shapira Weber and Nir Mualem and Shahaf E. Finder and Oren Freifeld", "abstract": "  The unsupervised task of Joint Alignment (JA) of images is beset by\nchallenges such as high complexity, geometric distortions, and convergence to\npoor local or even global optima. Although Vision Transformers (ViT) have\nrecently provided valuable features for JA, they fall short of fully addressing\nthese issues. Consequently, researchers frequently depend on expensive models\nand numerous regularization terms, resulting in long training times and\nchallenging hyperparameter tuning. We introduce the Spatial Joint Alignment\nModel (SpaceJAM), a novel approach that addresses the JA task with efficiency\nand simplicity. SpaceJAM leverages a compact architecture with only 16K\ntrainable parameters and uniquely operates without the need for regularization\nor atlas maintenance. Evaluations on SPair-71K and CUB datasets demonstrate\nthat SpaceJAM matches the alignment capabilities of existing methods while\nsignificantly reducing computational demands and achieving at least a 10x\nspeedup. SpaceJAM sets a new standard for rapid and effective image alignment,\nmaking the process more accessible and efficient. Our code is available at:\nhttps://bgu-cs-vil.github.io/SpaceJAM/.\n", "link": "http://arxiv.org/abs/2407.11850v2", "date": "2025-05-20", "relevancy": 2.7462, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5579}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceJAM%3A%20a%20Lightweight%20and%20Regularization-free%20Method%20for%20Fast%20Joint%0A%20%20Alignment%20of%20Images&body=Title%3A%20SpaceJAM%3A%20a%20Lightweight%20and%20Regularization-free%20Method%20for%20Fast%20Joint%0A%20%20Alignment%20of%20Images%0AAuthor%3A%20Nir%20Barel%20and%20Ron%20Shapira%20Weber%20and%20Nir%20Mualem%20and%20Shahaf%20E.%20Finder%20and%20Oren%20Freifeld%0AAbstract%3A%20%20%20The%20unsupervised%20task%20of%20Joint%20Alignment%20%28JA%29%20of%20images%20is%20beset%20by%0Achallenges%20such%20as%20high%20complexity%2C%20geometric%20distortions%2C%20and%20convergence%20to%0Apoor%20local%20or%20even%20global%20optima.%20Although%20Vision%20Transformers%20%28ViT%29%20have%0Arecently%20provided%20valuable%20features%20for%20JA%2C%20they%20fall%20short%20of%20fully%20addressing%0Athese%20issues.%20Consequently%2C%20researchers%20frequently%20depend%20on%20expensive%20models%0Aand%20numerous%20regularization%20terms%2C%20resulting%20in%20long%20training%20times%20and%0Achallenging%20hyperparameter%20tuning.%20We%20introduce%20the%20Spatial%20Joint%20Alignment%0AModel%20%28SpaceJAM%29%2C%20a%20novel%20approach%20that%20addresses%20the%20JA%20task%20with%20efficiency%0Aand%20simplicity.%20SpaceJAM%20leverages%20a%20compact%20architecture%20with%20only%2016K%0Atrainable%20parameters%20and%20uniquely%20operates%20without%20the%20need%20for%20regularization%0Aor%20atlas%20maintenance.%20Evaluations%20on%20SPair-71K%20and%20CUB%20datasets%20demonstrate%0Athat%20SpaceJAM%20matches%20the%20alignment%20capabilities%20of%20existing%20methods%20while%0Asignificantly%20reducing%20computational%20demands%20and%20achieving%20at%20least%20a%2010x%0Aspeedup.%20SpaceJAM%20sets%20a%20new%20standard%20for%20rapid%20and%20effective%20image%20alignment%2C%0Amaking%20the%20process%20more%20accessible%20and%20efficient.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//bgu-cs-vil.github.io/SpaceJAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceJAM%253A%2520a%2520Lightweight%2520and%2520Regularization-free%2520Method%2520for%2520Fast%2520Joint%250A%2520%2520Alignment%2520of%2520Images%26entry.906535625%3DNir%2520Barel%2520and%2520Ron%2520Shapira%2520Weber%2520and%2520Nir%2520Mualem%2520and%2520Shahaf%2520E.%2520Finder%2520and%2520Oren%2520Freifeld%26entry.1292438233%3D%2520%2520The%2520unsupervised%2520task%2520of%2520Joint%2520Alignment%2520%2528JA%2529%2520of%2520images%2520is%2520beset%2520by%250Achallenges%2520such%2520as%2520high%2520complexity%252C%2520geometric%2520distortions%252C%2520and%2520convergence%2520to%250Apoor%2520local%2520or%2520even%2520global%2520optima.%2520Although%2520Vision%2520Transformers%2520%2528ViT%2529%2520have%250Arecently%2520provided%2520valuable%2520features%2520for%2520JA%252C%2520they%2520fall%2520short%2520of%2520fully%2520addressing%250Athese%2520issues.%2520Consequently%252C%2520researchers%2520frequently%2520depend%2520on%2520expensive%2520models%250Aand%2520numerous%2520regularization%2520terms%252C%2520resulting%2520in%2520long%2520training%2520times%2520and%250Achallenging%2520hyperparameter%2520tuning.%2520We%2520introduce%2520the%2520Spatial%2520Joint%2520Alignment%250AModel%2520%2528SpaceJAM%2529%252C%2520a%2520novel%2520approach%2520that%2520addresses%2520the%2520JA%2520task%2520with%2520efficiency%250Aand%2520simplicity.%2520SpaceJAM%2520leverages%2520a%2520compact%2520architecture%2520with%2520only%252016K%250Atrainable%2520parameters%2520and%2520uniquely%2520operates%2520without%2520the%2520need%2520for%2520regularization%250Aor%2520atlas%2520maintenance.%2520Evaluations%2520on%2520SPair-71K%2520and%2520CUB%2520datasets%2520demonstrate%250Athat%2520SpaceJAM%2520matches%2520the%2520alignment%2520capabilities%2520of%2520existing%2520methods%2520while%250Asignificantly%2520reducing%2520computational%2520demands%2520and%2520achieving%2520at%2520least%2520a%252010x%250Aspeedup.%2520SpaceJAM%2520sets%2520a%2520new%2520standard%2520for%2520rapid%2520and%2520effective%2520image%2520alignment%252C%250Amaking%2520the%2520process%2520more%2520accessible%2520and%2520efficient.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//bgu-cs-vil.github.io/SpaceJAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceJAM%3A%20a%20Lightweight%20and%20Regularization-free%20Method%20for%20Fast%20Joint%0A%20%20Alignment%20of%20Images&entry.906535625=Nir%20Barel%20and%20Ron%20Shapira%20Weber%20and%20Nir%20Mualem%20and%20Shahaf%20E.%20Finder%20and%20Oren%20Freifeld&entry.1292438233=%20%20The%20unsupervised%20task%20of%20Joint%20Alignment%20%28JA%29%20of%20images%20is%20beset%20by%0Achallenges%20such%20as%20high%20complexity%2C%20geometric%20distortions%2C%20and%20convergence%20to%0Apoor%20local%20or%20even%20global%20optima.%20Although%20Vision%20Transformers%20%28ViT%29%20have%0Arecently%20provided%20valuable%20features%20for%20JA%2C%20they%20fall%20short%20of%20fully%20addressing%0Athese%20issues.%20Consequently%2C%20researchers%20frequently%20depend%20on%20expensive%20models%0Aand%20numerous%20regularization%20terms%2C%20resulting%20in%20long%20training%20times%20and%0Achallenging%20hyperparameter%20tuning.%20We%20introduce%20the%20Spatial%20Joint%20Alignment%0AModel%20%28SpaceJAM%29%2C%20a%20novel%20approach%20that%20addresses%20the%20JA%20task%20with%20efficiency%0Aand%20simplicity.%20SpaceJAM%20leverages%20a%20compact%20architecture%20with%20only%2016K%0Atrainable%20parameters%20and%20uniquely%20operates%20without%20the%20need%20for%20regularization%0Aor%20atlas%20maintenance.%20Evaluations%20on%20SPair-71K%20and%20CUB%20datasets%20demonstrate%0Athat%20SpaceJAM%20matches%20the%20alignment%20capabilities%20of%20existing%20methods%20while%0Asignificantly%20reducing%20computational%20demands%20and%20achieving%20at%20least%20a%2010x%0Aspeedup.%20SpaceJAM%20sets%20a%20new%20standard%20for%20rapid%20and%20effective%20image%20alignment%2C%0Amaking%20the%20process%20more%20accessible%20and%20efficient.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//bgu-cs-vil.github.io/SpaceJAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11850v2&entry.124074799=Read"},
{"title": "DSMentor: Enhancing Data Science Agents with Curriculum Learning and\n  Online Knowledge Accumulation", "author": "He Wang and Alexander Hanbo Li and Yiqun Hu and Sheng Zhang and Hideo Kobayashi and Jiani Zhang and Henry Zhu and Chung-Wei Hang and Patrick Ng", "abstract": "  Large language model (LLM) agents have shown promising performance in\ngenerating code for solving complex data science problems. Recent studies\nprimarily focus on enhancing in-context learning through improved search,\nsampling, and planning techniques, while overlooking the importance of the\norder in which problems are tackled during inference. In this work, we develop\na novel inference-time optimization framework, referred to as DSMentor, which\nleverages curriculum learning -- a strategy that introduces simpler task first\nand progressively moves to more complex ones as the learner improves -- to\nenhance LLM agent performance in challenging data science tasks. Our\nmentor-guided framework organizes data science tasks in order of increasing\ndifficulty and incorporates a growing long-term memory to retain prior\nexperiences, guiding the agent's learning progression and enabling more\neffective utilization of accumulated knowledge. We evaluate DSMentor through\nextensive experiments on DSEval and QRData benchmarks. Experiments show that\nDSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval\nand QRData compared to baseline agents. Furthermore, DSMentor demonstrates\nstronger causal reasoning ability, improving the pass rate by 8.8% on the\ncausality problems compared to GPT-4 using Program-of-Thoughts prompts. Our\nwork underscores the importance of developing effective strategies for\naccumulating and utilizing knowledge during inference, mirroring the human\nlearning process and opening new avenues for improving LLM performance through\ncurriculum-based inference optimization.\n", "link": "http://arxiv.org/abs/2505.14163v1", "date": "2025-05-20", "relevancy": 2.7428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSMentor%3A%20Enhancing%20Data%20Science%20Agents%20with%20Curriculum%20Learning%20and%0A%20%20Online%20Knowledge%20Accumulation&body=Title%3A%20DSMentor%3A%20Enhancing%20Data%20Science%20Agents%20with%20Curriculum%20Learning%20and%0A%20%20Online%20Knowledge%20Accumulation%0AAuthor%3A%20He%20Wang%20and%20Alexander%20Hanbo%20Li%20and%20Yiqun%20Hu%20and%20Sheng%20Zhang%20and%20Hideo%20Kobayashi%20and%20Jiani%20Zhang%20and%20Henry%20Zhu%20and%20Chung-Wei%20Hang%20and%20Patrick%20Ng%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20agents%20have%20shown%20promising%20performance%20in%0Agenerating%20code%20for%20solving%20complex%20data%20science%20problems.%20Recent%20studies%0Aprimarily%20focus%20on%20enhancing%20in-context%20learning%20through%20improved%20search%2C%0Asampling%2C%20and%20planning%20techniques%2C%20while%20overlooking%20the%20importance%20of%20the%0Aorder%20in%20which%20problems%20are%20tackled%20during%20inference.%20In%20this%20work%2C%20we%20develop%0Aa%20novel%20inference-time%20optimization%20framework%2C%20referred%20to%20as%20DSMentor%2C%20which%0Aleverages%20curriculum%20learning%20--%20a%20strategy%20that%20introduces%20simpler%20task%20first%0Aand%20progressively%20moves%20to%20more%20complex%20ones%20as%20the%20learner%20improves%20--%20to%0Aenhance%20LLM%20agent%20performance%20in%20challenging%20data%20science%20tasks.%20Our%0Amentor-guided%20framework%20organizes%20data%20science%20tasks%20in%20order%20of%20increasing%0Adifficulty%20and%20incorporates%20a%20growing%20long-term%20memory%20to%20retain%20prior%0Aexperiences%2C%20guiding%20the%20agent%27s%20learning%20progression%20and%20enabling%20more%0Aeffective%20utilization%20of%20accumulated%20knowledge.%20We%20evaluate%20DSMentor%20through%0Aextensive%20experiments%20on%20DSEval%20and%20QRData%20benchmarks.%20Experiments%20show%20that%0ADSMentor%20using%20Claude-3.5-Sonnet%20improves%20the%20pass%20rate%20by%20up%20to%205.2%25%20on%20DSEval%0Aand%20QRData%20compared%20to%20baseline%20agents.%20Furthermore%2C%20DSMentor%20demonstrates%0Astronger%20causal%20reasoning%20ability%2C%20improving%20the%20pass%20rate%20by%208.8%25%20on%20the%0Acausality%20problems%20compared%20to%20GPT-4%20using%20Program-of-Thoughts%20prompts.%20Our%0Awork%20underscores%20the%20importance%20of%20developing%20effective%20strategies%20for%0Aaccumulating%20and%20utilizing%20knowledge%20during%20inference%2C%20mirroring%20the%20human%0Alearning%20process%20and%20opening%20new%20avenues%20for%20improving%20LLM%20performance%20through%0Acurriculum-based%20inference%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSMentor%253A%2520Enhancing%2520Data%2520Science%2520Agents%2520with%2520Curriculum%2520Learning%2520and%250A%2520%2520Online%2520Knowledge%2520Accumulation%26entry.906535625%3DHe%2520Wang%2520and%2520Alexander%2520Hanbo%2520Li%2520and%2520Yiqun%2520Hu%2520and%2520Sheng%2520Zhang%2520and%2520Hideo%2520Kobayashi%2520and%2520Jiani%2520Zhang%2520and%2520Henry%2520Zhu%2520and%2520Chung-Wei%2520Hang%2520and%2520Patrick%2520Ng%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520agents%2520have%2520shown%2520promising%2520performance%2520in%250Agenerating%2520code%2520for%2520solving%2520complex%2520data%2520science%2520problems.%2520Recent%2520studies%250Aprimarily%2520focus%2520on%2520enhancing%2520in-context%2520learning%2520through%2520improved%2520search%252C%250Asampling%252C%2520and%2520planning%2520techniques%252C%2520while%2520overlooking%2520the%2520importance%2520of%2520the%250Aorder%2520in%2520which%2520problems%2520are%2520tackled%2520during%2520inference.%2520In%2520this%2520work%252C%2520we%2520develop%250Aa%2520novel%2520inference-time%2520optimization%2520framework%252C%2520referred%2520to%2520as%2520DSMentor%252C%2520which%250Aleverages%2520curriculum%2520learning%2520--%2520a%2520strategy%2520that%2520introduces%2520simpler%2520task%2520first%250Aand%2520progressively%2520moves%2520to%2520more%2520complex%2520ones%2520as%2520the%2520learner%2520improves%2520--%2520to%250Aenhance%2520LLM%2520agent%2520performance%2520in%2520challenging%2520data%2520science%2520tasks.%2520Our%250Amentor-guided%2520framework%2520organizes%2520data%2520science%2520tasks%2520in%2520order%2520of%2520increasing%250Adifficulty%2520and%2520incorporates%2520a%2520growing%2520long-term%2520memory%2520to%2520retain%2520prior%250Aexperiences%252C%2520guiding%2520the%2520agent%2527s%2520learning%2520progression%2520and%2520enabling%2520more%250Aeffective%2520utilization%2520of%2520accumulated%2520knowledge.%2520We%2520evaluate%2520DSMentor%2520through%250Aextensive%2520experiments%2520on%2520DSEval%2520and%2520QRData%2520benchmarks.%2520Experiments%2520show%2520that%250ADSMentor%2520using%2520Claude-3.5-Sonnet%2520improves%2520the%2520pass%2520rate%2520by%2520up%2520to%25205.2%2525%2520on%2520DSEval%250Aand%2520QRData%2520compared%2520to%2520baseline%2520agents.%2520Furthermore%252C%2520DSMentor%2520demonstrates%250Astronger%2520causal%2520reasoning%2520ability%252C%2520improving%2520the%2520pass%2520rate%2520by%25208.8%2525%2520on%2520the%250Acausality%2520problems%2520compared%2520to%2520GPT-4%2520using%2520Program-of-Thoughts%2520prompts.%2520Our%250Awork%2520underscores%2520the%2520importance%2520of%2520developing%2520effective%2520strategies%2520for%250Aaccumulating%2520and%2520utilizing%2520knowledge%2520during%2520inference%252C%2520mirroring%2520the%2520human%250Alearning%2520process%2520and%2520opening%2520new%2520avenues%2520for%2520improving%2520LLM%2520performance%2520through%250Acurriculum-based%2520inference%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSMentor%3A%20Enhancing%20Data%20Science%20Agents%20with%20Curriculum%20Learning%20and%0A%20%20Online%20Knowledge%20Accumulation&entry.906535625=He%20Wang%20and%20Alexander%20Hanbo%20Li%20and%20Yiqun%20Hu%20and%20Sheng%20Zhang%20and%20Hideo%20Kobayashi%20and%20Jiani%20Zhang%20and%20Henry%20Zhu%20and%20Chung-Wei%20Hang%20and%20Patrick%20Ng&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20agents%20have%20shown%20promising%20performance%20in%0Agenerating%20code%20for%20solving%20complex%20data%20science%20problems.%20Recent%20studies%0Aprimarily%20focus%20on%20enhancing%20in-context%20learning%20through%20improved%20search%2C%0Asampling%2C%20and%20planning%20techniques%2C%20while%20overlooking%20the%20importance%20of%20the%0Aorder%20in%20which%20problems%20are%20tackled%20during%20inference.%20In%20this%20work%2C%20we%20develop%0Aa%20novel%20inference-time%20optimization%20framework%2C%20referred%20to%20as%20DSMentor%2C%20which%0Aleverages%20curriculum%20learning%20--%20a%20strategy%20that%20introduces%20simpler%20task%20first%0Aand%20progressively%20moves%20to%20more%20complex%20ones%20as%20the%20learner%20improves%20--%20to%0Aenhance%20LLM%20agent%20performance%20in%20challenging%20data%20science%20tasks.%20Our%0Amentor-guided%20framework%20organizes%20data%20science%20tasks%20in%20order%20of%20increasing%0Adifficulty%20and%20incorporates%20a%20growing%20long-term%20memory%20to%20retain%20prior%0Aexperiences%2C%20guiding%20the%20agent%27s%20learning%20progression%20and%20enabling%20more%0Aeffective%20utilization%20of%20accumulated%20knowledge.%20We%20evaluate%20DSMentor%20through%0Aextensive%20experiments%20on%20DSEval%20and%20QRData%20benchmarks.%20Experiments%20show%20that%0ADSMentor%20using%20Claude-3.5-Sonnet%20improves%20the%20pass%20rate%20by%20up%20to%205.2%25%20on%20DSEval%0Aand%20QRData%20compared%20to%20baseline%20agents.%20Furthermore%2C%20DSMentor%20demonstrates%0Astronger%20causal%20reasoning%20ability%2C%20improving%20the%20pass%20rate%20by%208.8%25%20on%20the%0Acausality%20problems%20compared%20to%20GPT-4%20using%20Program-of-Thoughts%20prompts.%20Our%0Awork%20underscores%20the%20importance%20of%20developing%20effective%20strategies%20for%0Aaccumulating%20and%20utilizing%20knowledge%20during%20inference%2C%20mirroring%20the%20human%0Alearning%20process%20and%20opening%20new%20avenues%20for%20improving%20LLM%20performance%20through%0Acurriculum-based%20inference%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14163v1&entry.124074799=Read"},
{"title": "SCAN: Semantic Document Layout Analysis for Textual and Visual\n  Retrieval-Augmented Generation", "author": "Yuyang Dong and Nobuhiro Ueda and Kriszti\u00e1n Boros and Daiki Ito and Takuya Sera and Masafumi Oyamada", "abstract": "  With the increasing adoption of Large Language Models (LLMs) and\nVision-Language Models (VLMs), rich document analysis technologies for\napplications like Retrieval-Augmented Generation (RAG) and visual RAG are\ngaining significant attention. Recent research indicates that using VLMs can\nachieve better RAG performance, but processing rich documents still remains a\nchallenge since a single page contains large amounts of information. In this\npaper, we present SCAN (\\textbf{S}emanti\\textbf{C} Document Layout\n\\textbf{AN}alysis), a novel approach enhancing both textual and visual\nRetrieval-Augmented Generation (RAG) systems working with visually rich\ndocuments. It is a VLM-friendly approach that identifies document components\nwith appropriate semantic granularity, balancing context preservation with\nprocessing efficiency. SCAN uses a coarse-grained semantic approach that\ndivides documents into coherent regions covering continuous components. We\ntrained the SCAN model by fine-tuning object detection models with\nsophisticated annotation datasets. Our experimental results across English and\nJapanese datasets demonstrate that applying SCAN improves end-to-end textual\nRAG performance by up to 9.0\\% and visual RAG performance by up to 6.4\\%,\noutperforming conventional approaches and even commercial document processing\nsolutions.\n", "link": "http://arxiv.org/abs/2505.14381v1", "date": "2025-05-20", "relevancy": 2.739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCAN%3A%20Semantic%20Document%20Layout%20Analysis%20for%20Textual%20and%20Visual%0A%20%20Retrieval-Augmented%20Generation&body=Title%3A%20SCAN%3A%20Semantic%20Document%20Layout%20Analysis%20for%20Textual%20and%20Visual%0A%20%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Yuyang%20Dong%20and%20Nobuhiro%20Ueda%20and%20Kriszti%C3%A1n%20Boros%20and%20Daiki%20Ito%20and%20Takuya%20Sera%20and%20Masafumi%20Oyamada%0AAbstract%3A%20%20%20With%20the%20increasing%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%20and%0AVision-Language%20Models%20%28VLMs%29%2C%20rich%20document%20analysis%20technologies%20for%0Aapplications%20like%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%20visual%20RAG%20are%0Againing%20significant%20attention.%20Recent%20research%20indicates%20that%20using%20VLMs%20can%0Aachieve%20better%20RAG%20performance%2C%20but%20processing%20rich%20documents%20still%20remains%20a%0Achallenge%20since%20a%20single%20page%20contains%20large%20amounts%20of%20information.%20In%20this%0Apaper%2C%20we%20present%20SCAN%20%28%5Ctextbf%7BS%7Demanti%5Ctextbf%7BC%7D%20Document%20Layout%0A%5Ctextbf%7BAN%7Dalysis%29%2C%20a%20novel%20approach%20enhancing%20both%20textual%20and%20visual%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems%20working%20with%20visually%20rich%0Adocuments.%20It%20is%20a%20VLM-friendly%20approach%20that%20identifies%20document%20components%0Awith%20appropriate%20semantic%20granularity%2C%20balancing%20context%20preservation%20with%0Aprocessing%20efficiency.%20SCAN%20uses%20a%20coarse-grained%20semantic%20approach%20that%0Adivides%20documents%20into%20coherent%20regions%20covering%20continuous%20components.%20We%0Atrained%20the%20SCAN%20model%20by%20fine-tuning%20object%20detection%20models%20with%0Asophisticated%20annotation%20datasets.%20Our%20experimental%20results%20across%20English%20and%0AJapanese%20datasets%20demonstrate%20that%20applying%20SCAN%20improves%20end-to-end%20textual%0ARAG%20performance%20by%20up%20to%209.0%5C%25%20and%20visual%20RAG%20performance%20by%20up%20to%206.4%5C%25%2C%0Aoutperforming%20conventional%20approaches%20and%20even%20commercial%20document%20processing%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCAN%253A%2520Semantic%2520Document%2520Layout%2520Analysis%2520for%2520Textual%2520and%2520Visual%250A%2520%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DYuyang%2520Dong%2520and%2520Nobuhiro%2520Ueda%2520and%2520Kriszti%25C3%25A1n%2520Boros%2520and%2520Daiki%2520Ito%2520and%2520Takuya%2520Sera%2520and%2520Masafumi%2520Oyamada%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520adoption%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%250AVision-Language%2520Models%2520%2528VLMs%2529%252C%2520rich%2520document%2520analysis%2520technologies%2520for%250Aapplications%2520like%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520and%2520visual%2520RAG%2520are%250Againing%2520significant%2520attention.%2520Recent%2520research%2520indicates%2520that%2520using%2520VLMs%2520can%250Aachieve%2520better%2520RAG%2520performance%252C%2520but%2520processing%2520rich%2520documents%2520still%2520remains%2520a%250Achallenge%2520since%2520a%2520single%2520page%2520contains%2520large%2520amounts%2520of%2520information.%2520In%2520this%250Apaper%252C%2520we%2520present%2520SCAN%2520%2528%255Ctextbf%257BS%257Demanti%255Ctextbf%257BC%257D%2520Document%2520Layout%250A%255Ctextbf%257BAN%257Dalysis%2529%252C%2520a%2520novel%2520approach%2520enhancing%2520both%2520textual%2520and%2520visual%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520working%2520with%2520visually%2520rich%250Adocuments.%2520It%2520is%2520a%2520VLM-friendly%2520approach%2520that%2520identifies%2520document%2520components%250Awith%2520appropriate%2520semantic%2520granularity%252C%2520balancing%2520context%2520preservation%2520with%250Aprocessing%2520efficiency.%2520SCAN%2520uses%2520a%2520coarse-grained%2520semantic%2520approach%2520that%250Adivides%2520documents%2520into%2520coherent%2520regions%2520covering%2520continuous%2520components.%2520We%250Atrained%2520the%2520SCAN%2520model%2520by%2520fine-tuning%2520object%2520detection%2520models%2520with%250Asophisticated%2520annotation%2520datasets.%2520Our%2520experimental%2520results%2520across%2520English%2520and%250AJapanese%2520datasets%2520demonstrate%2520that%2520applying%2520SCAN%2520improves%2520end-to-end%2520textual%250ARAG%2520performance%2520by%2520up%2520to%25209.0%255C%2525%2520and%2520visual%2520RAG%2520performance%2520by%2520up%2520to%25206.4%255C%2525%252C%250Aoutperforming%2520conventional%2520approaches%2520and%2520even%2520commercial%2520document%2520processing%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCAN%3A%20Semantic%20Document%20Layout%20Analysis%20for%20Textual%20and%20Visual%0A%20%20Retrieval-Augmented%20Generation&entry.906535625=Yuyang%20Dong%20and%20Nobuhiro%20Ueda%20and%20Kriszti%C3%A1n%20Boros%20and%20Daiki%20Ito%20and%20Takuya%20Sera%20and%20Masafumi%20Oyamada&entry.1292438233=%20%20With%20the%20increasing%20adoption%20of%20Large%20Language%20Models%20%28LLMs%29%20and%0AVision-Language%20Models%20%28VLMs%29%2C%20rich%20document%20analysis%20technologies%20for%0Aapplications%20like%20Retrieval-Augmented%20Generation%20%28RAG%29%20and%20visual%20RAG%20are%0Againing%20significant%20attention.%20Recent%20research%20indicates%20that%20using%20VLMs%20can%0Aachieve%20better%20RAG%20performance%2C%20but%20processing%20rich%20documents%20still%20remains%20a%0Achallenge%20since%20a%20single%20page%20contains%20large%20amounts%20of%20information.%20In%20this%0Apaper%2C%20we%20present%20SCAN%20%28%5Ctextbf%7BS%7Demanti%5Ctextbf%7BC%7D%20Document%20Layout%0A%5Ctextbf%7BAN%7Dalysis%29%2C%20a%20novel%20approach%20enhancing%20both%20textual%20and%20visual%0ARetrieval-Augmented%20Generation%20%28RAG%29%20systems%20working%20with%20visually%20rich%0Adocuments.%20It%20is%20a%20VLM-friendly%20approach%20that%20identifies%20document%20components%0Awith%20appropriate%20semantic%20granularity%2C%20balancing%20context%20preservation%20with%0Aprocessing%20efficiency.%20SCAN%20uses%20a%20coarse-grained%20semantic%20approach%20that%0Adivides%20documents%20into%20coherent%20regions%20covering%20continuous%20components.%20We%0Atrained%20the%20SCAN%20model%20by%20fine-tuning%20object%20detection%20models%20with%0Asophisticated%20annotation%20datasets.%20Our%20experimental%20results%20across%20English%20and%0AJapanese%20datasets%20demonstrate%20that%20applying%20SCAN%20improves%20end-to-end%20textual%0ARAG%20performance%20by%20up%20to%209.0%5C%25%20and%20visual%20RAG%20performance%20by%20up%20to%206.4%5C%25%2C%0Aoutperforming%20conventional%20approaches%20and%20even%20commercial%20document%20processing%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14381v1&entry.124074799=Read"},
{"title": "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and\n  Racial Disparities in Multilingual Contrastive Vision Language Models", "author": "Zahraa Al Sahili and Ioannis Patras and Matthew Purver", "abstract": "  Multilingual vision-language models promise universal image-text retrieval,\nyet their social biases remain under-explored. We present the first systematic\naudit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and\nCAPIVARA-CLIP -- across ten languages that vary in resource availability and\ngrammatical gender. Using balanced subsets of \\textsc{FairFace} and the\n\\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and\ngender bias and measure stereotype amplification. Contrary to the assumption\nthat multilinguality mitigates bias, every model exhibits stronger gender bias\nthan its English-only baseline. CAPIVARA-CLIP shows its largest biases\nprecisely in the low-resource languages it targets, while the shared\ncross-lingual encoder of NLLB-CLIP transports English gender stereotypes into\ngender-neutral languages; loosely coupled encoders largely avoid this transfer.\nHighly gendered languages consistently magnify all measured bias types, but\neven gender-neutral languages remain vulnerable when cross-lingual weight\nsharing imports foreign stereotypes. Aggregated metrics conceal\nlanguage-specific ``hot spots,'' underscoring the need for fine-grained,\nlanguage-aware bias evaluation in future multilingual vision-language research.\n", "link": "http://arxiv.org/abs/2505.14160v1", "date": "2025-05-20", "relevancy": 2.7225, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5594}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Language%20Barriers%20or%20Reinforcing%20Bias%3F%20A%20Study%20of%20Gender%20and%0A%20%20Racial%20Disparities%20in%20Multilingual%20Contrastive%20Vision%20Language%20Models&body=Title%3A%20Breaking%20Language%20Barriers%20or%20Reinforcing%20Bias%3F%20A%20Study%20of%20Gender%20and%0A%20%20Racial%20Disparities%20in%20Multilingual%20Contrastive%20Vision%20Language%20Models%0AAuthor%3A%20Zahraa%20Al%20Sahili%20and%20Ioannis%20Patras%20and%20Matthew%20Purver%0AAbstract%3A%20%20%20Multilingual%20vision-language%20models%20promise%20universal%20image-text%20retrieval%2C%0Ayet%20their%20social%20biases%20remain%20under-explored.%20We%20present%20the%20first%20systematic%0Aaudit%20of%20three%20public%20multilingual%20CLIP%20checkpoints%20--%20M-CLIP%2C%20NLLB-CLIP%2C%20and%0ACAPIVARA-CLIP%20--%20across%20ten%20languages%20that%20vary%20in%20resource%20availability%20and%0Agrammatical%20gender.%20Using%20balanced%20subsets%20of%20%5Ctextsc%7BFairFace%7D%20and%20the%0A%5Ctextsc%7BPATA%7D%20stereotype%20suite%20in%20a%20zero-shot%20setting%2C%20we%20quantify%20race%20and%0Agender%20bias%20and%20measure%20stereotype%20amplification.%20Contrary%20to%20the%20assumption%0Athat%20multilinguality%20mitigates%20bias%2C%20every%20model%20exhibits%20stronger%20gender%20bias%0Athan%20its%20English-only%20baseline.%20CAPIVARA-CLIP%20shows%20its%20largest%20biases%0Aprecisely%20in%20the%20low-resource%20languages%20it%20targets%2C%20while%20the%20shared%0Across-lingual%20encoder%20of%20NLLB-CLIP%20transports%20English%20gender%20stereotypes%20into%0Agender-neutral%20languages%3B%20loosely%20coupled%20encoders%20largely%20avoid%20this%20transfer.%0AHighly%20gendered%20languages%20consistently%20magnify%20all%20measured%20bias%20types%2C%20but%0Aeven%20gender-neutral%20languages%20remain%20vulnerable%20when%20cross-lingual%20weight%0Asharing%20imports%20foreign%20stereotypes.%20Aggregated%20metrics%20conceal%0Alanguage-specific%20%60%60hot%20spots%2C%27%27%20underscoring%20the%20need%20for%20fine-grained%2C%0Alanguage-aware%20bias%20evaluation%20in%20future%20multilingual%20vision-language%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Language%2520Barriers%2520or%2520Reinforcing%2520Bias%253F%2520A%2520Study%2520of%2520Gender%2520and%250A%2520%2520Racial%2520Disparities%2520in%2520Multilingual%2520Contrastive%2520Vision%2520Language%2520Models%26entry.906535625%3DZahraa%2520Al%2520Sahili%2520and%2520Ioannis%2520Patras%2520and%2520Matthew%2520Purver%26entry.1292438233%3D%2520%2520Multilingual%2520vision-language%2520models%2520promise%2520universal%2520image-text%2520retrieval%252C%250Ayet%2520their%2520social%2520biases%2520remain%2520under-explored.%2520We%2520present%2520the%2520first%2520systematic%250Aaudit%2520of%2520three%2520public%2520multilingual%2520CLIP%2520checkpoints%2520--%2520M-CLIP%252C%2520NLLB-CLIP%252C%2520and%250ACAPIVARA-CLIP%2520--%2520across%2520ten%2520languages%2520that%2520vary%2520in%2520resource%2520availability%2520and%250Agrammatical%2520gender.%2520Using%2520balanced%2520subsets%2520of%2520%255Ctextsc%257BFairFace%257D%2520and%2520the%250A%255Ctextsc%257BPATA%257D%2520stereotype%2520suite%2520in%2520a%2520zero-shot%2520setting%252C%2520we%2520quantify%2520race%2520and%250Agender%2520bias%2520and%2520measure%2520stereotype%2520amplification.%2520Contrary%2520to%2520the%2520assumption%250Athat%2520multilinguality%2520mitigates%2520bias%252C%2520every%2520model%2520exhibits%2520stronger%2520gender%2520bias%250Athan%2520its%2520English-only%2520baseline.%2520CAPIVARA-CLIP%2520shows%2520its%2520largest%2520biases%250Aprecisely%2520in%2520the%2520low-resource%2520languages%2520it%2520targets%252C%2520while%2520the%2520shared%250Across-lingual%2520encoder%2520of%2520NLLB-CLIP%2520transports%2520English%2520gender%2520stereotypes%2520into%250Agender-neutral%2520languages%253B%2520loosely%2520coupled%2520encoders%2520largely%2520avoid%2520this%2520transfer.%250AHighly%2520gendered%2520languages%2520consistently%2520magnify%2520all%2520measured%2520bias%2520types%252C%2520but%250Aeven%2520gender-neutral%2520languages%2520remain%2520vulnerable%2520when%2520cross-lingual%2520weight%250Asharing%2520imports%2520foreign%2520stereotypes.%2520Aggregated%2520metrics%2520conceal%250Alanguage-specific%2520%2560%2560hot%2520spots%252C%2527%2527%2520underscoring%2520the%2520need%2520for%2520fine-grained%252C%250Alanguage-aware%2520bias%2520evaluation%2520in%2520future%2520multilingual%2520vision-language%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Language%20Barriers%20or%20Reinforcing%20Bias%3F%20A%20Study%20of%20Gender%20and%0A%20%20Racial%20Disparities%20in%20Multilingual%20Contrastive%20Vision%20Language%20Models&entry.906535625=Zahraa%20Al%20Sahili%20and%20Ioannis%20Patras%20and%20Matthew%20Purver&entry.1292438233=%20%20Multilingual%20vision-language%20models%20promise%20universal%20image-text%20retrieval%2C%0Ayet%20their%20social%20biases%20remain%20under-explored.%20We%20present%20the%20first%20systematic%0Aaudit%20of%20three%20public%20multilingual%20CLIP%20checkpoints%20--%20M-CLIP%2C%20NLLB-CLIP%2C%20and%0ACAPIVARA-CLIP%20--%20across%20ten%20languages%20that%20vary%20in%20resource%20availability%20and%0Agrammatical%20gender.%20Using%20balanced%20subsets%20of%20%5Ctextsc%7BFairFace%7D%20and%20the%0A%5Ctextsc%7BPATA%7D%20stereotype%20suite%20in%20a%20zero-shot%20setting%2C%20we%20quantify%20race%20and%0Agender%20bias%20and%20measure%20stereotype%20amplification.%20Contrary%20to%20the%20assumption%0Athat%20multilinguality%20mitigates%20bias%2C%20every%20model%20exhibits%20stronger%20gender%20bias%0Athan%20its%20English-only%20baseline.%20CAPIVARA-CLIP%20shows%20its%20largest%20biases%0Aprecisely%20in%20the%20low-resource%20languages%20it%20targets%2C%20while%20the%20shared%0Across-lingual%20encoder%20of%20NLLB-CLIP%20transports%20English%20gender%20stereotypes%20into%0Agender-neutral%20languages%3B%20loosely%20coupled%20encoders%20largely%20avoid%20this%20transfer.%0AHighly%20gendered%20languages%20consistently%20magnify%20all%20measured%20bias%20types%2C%20but%0Aeven%20gender-neutral%20languages%20remain%20vulnerable%20when%20cross-lingual%20weight%0Asharing%20imports%20foreign%20stereotypes.%20Aggregated%20metrics%20conceal%0Alanguage-specific%20%60%60hot%20spots%2C%27%27%20underscoring%20the%20need%20for%20fine-grained%2C%0Alanguage-aware%20bias%20evaluation%20in%20future%20multilingual%20vision-language%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14160v1&entry.124074799=Read"},
{"title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via\n  Reinforcement Learning to Rank", "author": "Tianhe Wu and Jian Zou and Jie Liang and Lei Zhang and Kede Ma", "abstract": "  DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.\n", "link": "http://arxiv.org/abs/2505.14460v1", "date": "2025-05-20", "relevancy": 2.7162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisualQuality-R1%3A%20Reasoning-Induced%20Image%20Quality%20Assessment%20via%0A%20%20Reinforcement%20Learning%20to%20Rank&body=Title%3A%20VisualQuality-R1%3A%20Reasoning-Induced%20Image%20Quality%20Assessment%20via%0A%20%20Reinforcement%20Learning%20to%20Rank%0AAuthor%3A%20Tianhe%20Wu%20and%20Jian%20Zou%20and%20Jie%20Liang%20and%20Lei%20Zhang%20and%20Kede%20Ma%0AAbstract%3A%20%20%20DeepSeek-R1%20has%20demonstrated%20remarkable%20effectiveness%20in%20incentivizing%0Areasoning%20and%20generalization%20capabilities%20of%20large%20language%20models%20%28LLMs%29%0Athrough%20reinforcement%20learning.%20Nevertheless%2C%20the%20potential%20of%0Areasoning-induced%20computational%20modeling%20has%20not%20been%20thoroughly%20explored%20in%0Athe%20context%20of%20image%20quality%20assessment%20%28IQA%29%2C%20a%20task%20critically%20dependent%20on%0Avisual%20reasoning.%20In%20this%20paper%2C%20we%20introduce%20VisualQuality-R1%2C%20a%0Areasoning-induced%20no-reference%20IQA%20%28NR-IQA%29%20model%2C%20and%20we%20train%20it%20with%0Areinforcement%20learning%20to%20rank%2C%20a%20learning%20algorithm%20tailored%20to%20the%0Aintrinsically%20relative%20nature%20of%20visual%20quality.%20Specifically%2C%20for%20a%20pair%20of%0Aimages%2C%20we%20employ%20group%20relative%20policy%20optimization%20to%20generate%20multiple%0Aquality%20scores%20for%20each%20image.%20These%20estimates%20are%20then%20used%20to%20compute%0Acomparative%20probabilities%20of%20one%20image%20having%20higher%20quality%20than%20the%20other%0Aunder%20the%20Thurstone%20model.%20Rewards%20for%20each%20quality%20estimate%20are%20defined%20using%0Acontinuous%20fidelity%20measures%20rather%20than%20discretized%20binary%20labels.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20VisualQuality-R1%20consistently%20outperforms%0Adiscriminative%20deep%20learning-based%20NR-IQA%20models%20as%20well%20as%20a%20recent%0Areasoning-induced%20quality%20regression%20method.%20Moreover%2C%20VisualQuality-R1%20is%0Acapable%20of%20generating%20contextually%20rich%2C%20human-aligned%20quality%20descriptions%2C%0Aand%20supports%20multi-dataset%20training%20without%20requiring%20perceptual%20scale%0Arealignment.%20These%20features%20make%20VisualQuality-R1%20especially%20well-suited%20for%0Areliably%20measuring%20progress%20in%20a%20wide%20range%20of%20image%20processing%20tasks%20like%0Asuper-resolution%20and%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisualQuality-R1%253A%2520Reasoning-Induced%2520Image%2520Quality%2520Assessment%2520via%250A%2520%2520Reinforcement%2520Learning%2520to%2520Rank%26entry.906535625%3DTianhe%2520Wu%2520and%2520Jian%2520Zou%2520and%2520Jie%2520Liang%2520and%2520Lei%2520Zhang%2520and%2520Kede%2520Ma%26entry.1292438233%3D%2520%2520DeepSeek-R1%2520has%2520demonstrated%2520remarkable%2520effectiveness%2520in%2520incentivizing%250Areasoning%2520and%2520generalization%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%250Athrough%2520reinforcement%2520learning.%2520Nevertheless%252C%2520the%2520potential%2520of%250Areasoning-induced%2520computational%2520modeling%2520has%2520not%2520been%2520thoroughly%2520explored%2520in%250Athe%2520context%2520of%2520image%2520quality%2520assessment%2520%2528IQA%2529%252C%2520a%2520task%2520critically%2520dependent%2520on%250Avisual%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520VisualQuality-R1%252C%2520a%250Areasoning-induced%2520no-reference%2520IQA%2520%2528NR-IQA%2529%2520model%252C%2520and%2520we%2520train%2520it%2520with%250Areinforcement%2520learning%2520to%2520rank%252C%2520a%2520learning%2520algorithm%2520tailored%2520to%2520the%250Aintrinsically%2520relative%2520nature%2520of%2520visual%2520quality.%2520Specifically%252C%2520for%2520a%2520pair%2520of%250Aimages%252C%2520we%2520employ%2520group%2520relative%2520policy%2520optimization%2520to%2520generate%2520multiple%250Aquality%2520scores%2520for%2520each%2520image.%2520These%2520estimates%2520are%2520then%2520used%2520to%2520compute%250Acomparative%2520probabilities%2520of%2520one%2520image%2520having%2520higher%2520quality%2520than%2520the%2520other%250Aunder%2520the%2520Thurstone%2520model.%2520Rewards%2520for%2520each%2520quality%2520estimate%2520are%2520defined%2520using%250Acontinuous%2520fidelity%2520measures%2520rather%2520than%2520discretized%2520binary%2520labels.%2520Extensive%250Aexperiments%2520show%2520that%2520the%2520proposed%2520VisualQuality-R1%2520consistently%2520outperforms%250Adiscriminative%2520deep%2520learning-based%2520NR-IQA%2520models%2520as%2520well%2520as%2520a%2520recent%250Areasoning-induced%2520quality%2520regression%2520method.%2520Moreover%252C%2520VisualQuality-R1%2520is%250Acapable%2520of%2520generating%2520contextually%2520rich%252C%2520human-aligned%2520quality%2520descriptions%252C%250Aand%2520supports%2520multi-dataset%2520training%2520without%2520requiring%2520perceptual%2520scale%250Arealignment.%2520These%2520features%2520make%2520VisualQuality-R1%2520especially%2520well-suited%2520for%250Areliably%2520measuring%2520progress%2520in%2520a%2520wide%2520range%2520of%2520image%2520processing%2520tasks%2520like%250Asuper-resolution%2520and%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisualQuality-R1%3A%20Reasoning-Induced%20Image%20Quality%20Assessment%20via%0A%20%20Reinforcement%20Learning%20to%20Rank&entry.906535625=Tianhe%20Wu%20and%20Jian%20Zou%20and%20Jie%20Liang%20and%20Lei%20Zhang%20and%20Kede%20Ma&entry.1292438233=%20%20DeepSeek-R1%20has%20demonstrated%20remarkable%20effectiveness%20in%20incentivizing%0Areasoning%20and%20generalization%20capabilities%20of%20large%20language%20models%20%28LLMs%29%0Athrough%20reinforcement%20learning.%20Nevertheless%2C%20the%20potential%20of%0Areasoning-induced%20computational%20modeling%20has%20not%20been%20thoroughly%20explored%20in%0Athe%20context%20of%20image%20quality%20assessment%20%28IQA%29%2C%20a%20task%20critically%20dependent%20on%0Avisual%20reasoning.%20In%20this%20paper%2C%20we%20introduce%20VisualQuality-R1%2C%20a%0Areasoning-induced%20no-reference%20IQA%20%28NR-IQA%29%20model%2C%20and%20we%20train%20it%20with%0Areinforcement%20learning%20to%20rank%2C%20a%20learning%20algorithm%20tailored%20to%20the%0Aintrinsically%20relative%20nature%20of%20visual%20quality.%20Specifically%2C%20for%20a%20pair%20of%0Aimages%2C%20we%20employ%20group%20relative%20policy%20optimization%20to%20generate%20multiple%0Aquality%20scores%20for%20each%20image.%20These%20estimates%20are%20then%20used%20to%20compute%0Acomparative%20probabilities%20of%20one%20image%20having%20higher%20quality%20than%20the%20other%0Aunder%20the%20Thurstone%20model.%20Rewards%20for%20each%20quality%20estimate%20are%20defined%20using%0Acontinuous%20fidelity%20measures%20rather%20than%20discretized%20binary%20labels.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20VisualQuality-R1%20consistently%20outperforms%0Adiscriminative%20deep%20learning-based%20NR-IQA%20models%20as%20well%20as%20a%20recent%0Areasoning-induced%20quality%20regression%20method.%20Moreover%2C%20VisualQuality-R1%20is%0Acapable%20of%20generating%20contextually%20rich%2C%20human-aligned%20quality%20descriptions%2C%0Aand%20supports%20multi-dataset%20training%20without%20requiring%20perceptual%20scale%0Arealignment.%20These%20features%20make%20VisualQuality-R1%20especially%20well-suited%20for%0Areliably%20measuring%20progress%20in%20a%20wide%20range%20of%20image%20processing%20tasks%20like%0Asuper-resolution%20and%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14460v1&entry.124074799=Read"},
{"title": "LMP: Leveraging Motion Prior in Zero-Shot Video Generation with\n  Diffusion Transformer", "author": "Changgu Chen and Xiaoyan Yang and Junwei Shu and Changbo Wang and Yang Li", "abstract": "  In recent years, large-scale pre-trained diffusion transformer models have\nmade significant progress in video generation. While current DiT models can\nproduce high-definition, high-frame-rate, and highly diverse videos, there is a\nlack of fine-grained control over the video content. Controlling the motion of\nsubjects in videos using only prompts is challenging, especially when it comes\nto describing complex movements. Further, existing methods fail to control the\nmotion in image-to-video generation, as the subject in the reference image\noften differs from the subject in the reference video in terms of initial\nposition, size, and shape. To address this, we propose the Leveraging Motion\nPrior (LMP) framework for zero-shot video generation. Our framework harnesses\nthe powerful generative capabilities of pre-trained diffusion transformers to\nenable motion in the generated videos to reference user-provided motion videos\nin both text-to-video and image-to-video generation. To this end, we first\nintroduce a foreground-background disentangle module to distinguish between\nmoving subjects and backgrounds in the reference video, preventing interference\nin the target video generation. A reweighted motion transfer module is designed\nto allow the target video to reference the motion from the reference video. To\navoid interference from the subject in the reference video, we propose an\nappearance separation module to suppress the appearance of the reference\nsubject in the target video. We annotate the DAVIS dataset with detailed\nprompts for our experiments and design evaluation metrics to validate the\neffectiveness of our method. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance in generation quality,\nprompt-video consistency, and control capability. Our homepage is available at\nhttps://vpx-ecnu.github.io/LMP-Website/\n", "link": "http://arxiv.org/abs/2505.14167v1", "date": "2025-05-20", "relevancy": 2.7157, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.703}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6785}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMP%3A%20Leveraging%20Motion%20Prior%20in%20Zero-Shot%20Video%20Generation%20with%0A%20%20Diffusion%20Transformer&body=Title%3A%20LMP%3A%20Leveraging%20Motion%20Prior%20in%20Zero-Shot%20Video%20Generation%20with%0A%20%20Diffusion%20Transformer%0AAuthor%3A%20Changgu%20Chen%20and%20Xiaoyan%20Yang%20and%20Junwei%20Shu%20and%20Changbo%20Wang%20and%20Yang%20Li%0AAbstract%3A%20%20%20In%20recent%20years%2C%20large-scale%20pre-trained%20diffusion%20transformer%20models%20have%0Amade%20significant%20progress%20in%20video%20generation.%20While%20current%20DiT%20models%20can%0Aproduce%20high-definition%2C%20high-frame-rate%2C%20and%20highly%20diverse%20videos%2C%20there%20is%20a%0Alack%20of%20fine-grained%20control%20over%20the%20video%20content.%20Controlling%20the%20motion%20of%0Asubjects%20in%20videos%20using%20only%20prompts%20is%20challenging%2C%20especially%20when%20it%20comes%0Ato%20describing%20complex%20movements.%20Further%2C%20existing%20methods%20fail%20to%20control%20the%0Amotion%20in%20image-to-video%20generation%2C%20as%20the%20subject%20in%20the%20reference%20image%0Aoften%20differs%20from%20the%20subject%20in%20the%20reference%20video%20in%20terms%20of%20initial%0Aposition%2C%20size%2C%20and%20shape.%20To%20address%20this%2C%20we%20propose%20the%20Leveraging%20Motion%0APrior%20%28LMP%29%20framework%20for%20zero-shot%20video%20generation.%20Our%20framework%20harnesses%0Athe%20powerful%20generative%20capabilities%20of%20pre-trained%20diffusion%20transformers%20to%0Aenable%20motion%20in%20the%20generated%20videos%20to%20reference%20user-provided%20motion%20videos%0Ain%20both%20text-to-video%20and%20image-to-video%20generation.%20To%20this%20end%2C%20we%20first%0Aintroduce%20a%20foreground-background%20disentangle%20module%20to%20distinguish%20between%0Amoving%20subjects%20and%20backgrounds%20in%20the%20reference%20video%2C%20preventing%20interference%0Ain%20the%20target%20video%20generation.%20A%20reweighted%20motion%20transfer%20module%20is%20designed%0Ato%20allow%20the%20target%20video%20to%20reference%20the%20motion%20from%20the%20reference%20video.%20To%0Aavoid%20interference%20from%20the%20subject%20in%20the%20reference%20video%2C%20we%20propose%20an%0Aappearance%20separation%20module%20to%20suppress%20the%20appearance%20of%20the%20reference%0Asubject%20in%20the%20target%20video.%20We%20annotate%20the%20DAVIS%20dataset%20with%20detailed%0Aprompts%20for%20our%20experiments%20and%20design%20evaluation%20metrics%20to%20validate%20the%0Aeffectiveness%20of%20our%20method.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20in%20generation%20quality%2C%0Aprompt-video%20consistency%2C%20and%20control%20capability.%20Our%20homepage%20is%20available%20at%0Ahttps%3A//vpx-ecnu.github.io/LMP-Website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14167v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMP%253A%2520Leveraging%2520Motion%2520Prior%2520in%2520Zero-Shot%2520Video%2520Generation%2520with%250A%2520%2520Diffusion%2520Transformer%26entry.906535625%3DChanggu%2520Chen%2520and%2520Xiaoyan%2520Yang%2520and%2520Junwei%2520Shu%2520and%2520Changbo%2520Wang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520large-scale%2520pre-trained%2520diffusion%2520transformer%2520models%2520have%250Amade%2520significant%2520progress%2520in%2520video%2520generation.%2520While%2520current%2520DiT%2520models%2520can%250Aproduce%2520high-definition%252C%2520high-frame-rate%252C%2520and%2520highly%2520diverse%2520videos%252C%2520there%2520is%2520a%250Alack%2520of%2520fine-grained%2520control%2520over%2520the%2520video%2520content.%2520Controlling%2520the%2520motion%2520of%250Asubjects%2520in%2520videos%2520using%2520only%2520prompts%2520is%2520challenging%252C%2520especially%2520when%2520it%2520comes%250Ato%2520describing%2520complex%2520movements.%2520Further%252C%2520existing%2520methods%2520fail%2520to%2520control%2520the%250Amotion%2520in%2520image-to-video%2520generation%252C%2520as%2520the%2520subject%2520in%2520the%2520reference%2520image%250Aoften%2520differs%2520from%2520the%2520subject%2520in%2520the%2520reference%2520video%2520in%2520terms%2520of%2520initial%250Aposition%252C%2520size%252C%2520and%2520shape.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Leveraging%2520Motion%250APrior%2520%2528LMP%2529%2520framework%2520for%2520zero-shot%2520video%2520generation.%2520Our%2520framework%2520harnesses%250Athe%2520powerful%2520generative%2520capabilities%2520of%2520pre-trained%2520diffusion%2520transformers%2520to%250Aenable%2520motion%2520in%2520the%2520generated%2520videos%2520to%2520reference%2520user-provided%2520motion%2520videos%250Ain%2520both%2520text-to-video%2520and%2520image-to-video%2520generation.%2520To%2520this%2520end%252C%2520we%2520first%250Aintroduce%2520a%2520foreground-background%2520disentangle%2520module%2520to%2520distinguish%2520between%250Amoving%2520subjects%2520and%2520backgrounds%2520in%2520the%2520reference%2520video%252C%2520preventing%2520interference%250Ain%2520the%2520target%2520video%2520generation.%2520A%2520reweighted%2520motion%2520transfer%2520module%2520is%2520designed%250Ato%2520allow%2520the%2520target%2520video%2520to%2520reference%2520the%2520motion%2520from%2520the%2520reference%2520video.%2520To%250Aavoid%2520interference%2520from%2520the%2520subject%2520in%2520the%2520reference%2520video%252C%2520we%2520propose%2520an%250Aappearance%2520separation%2520module%2520to%2520suppress%2520the%2520appearance%2520of%2520the%2520reference%250Asubject%2520in%2520the%2520target%2520video.%2520We%2520annotate%2520the%2520DAVIS%2520dataset%2520with%2520detailed%250Aprompts%2520for%2520our%2520experiments%2520and%2520design%2520evaluation%2520metrics%2520to%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520achieves%2520state-of-the-art%2520performance%2520in%2520generation%2520quality%252C%250Aprompt-video%2520consistency%252C%2520and%2520control%2520capability.%2520Our%2520homepage%2520is%2520available%2520at%250Ahttps%253A//vpx-ecnu.github.io/LMP-Website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14167v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMP%3A%20Leveraging%20Motion%20Prior%20in%20Zero-Shot%20Video%20Generation%20with%0A%20%20Diffusion%20Transformer&entry.906535625=Changgu%20Chen%20and%20Xiaoyan%20Yang%20and%20Junwei%20Shu%20and%20Changbo%20Wang%20and%20Yang%20Li&entry.1292438233=%20%20In%20recent%20years%2C%20large-scale%20pre-trained%20diffusion%20transformer%20models%20have%0Amade%20significant%20progress%20in%20video%20generation.%20While%20current%20DiT%20models%20can%0Aproduce%20high-definition%2C%20high-frame-rate%2C%20and%20highly%20diverse%20videos%2C%20there%20is%20a%0Alack%20of%20fine-grained%20control%20over%20the%20video%20content.%20Controlling%20the%20motion%20of%0Asubjects%20in%20videos%20using%20only%20prompts%20is%20challenging%2C%20especially%20when%20it%20comes%0Ato%20describing%20complex%20movements.%20Further%2C%20existing%20methods%20fail%20to%20control%20the%0Amotion%20in%20image-to-video%20generation%2C%20as%20the%20subject%20in%20the%20reference%20image%0Aoften%20differs%20from%20the%20subject%20in%20the%20reference%20video%20in%20terms%20of%20initial%0Aposition%2C%20size%2C%20and%20shape.%20To%20address%20this%2C%20we%20propose%20the%20Leveraging%20Motion%0APrior%20%28LMP%29%20framework%20for%20zero-shot%20video%20generation.%20Our%20framework%20harnesses%0Athe%20powerful%20generative%20capabilities%20of%20pre-trained%20diffusion%20transformers%20to%0Aenable%20motion%20in%20the%20generated%20videos%20to%20reference%20user-provided%20motion%20videos%0Ain%20both%20text-to-video%20and%20image-to-video%20generation.%20To%20this%20end%2C%20we%20first%0Aintroduce%20a%20foreground-background%20disentangle%20module%20to%20distinguish%20between%0Amoving%20subjects%20and%20backgrounds%20in%20the%20reference%20video%2C%20preventing%20interference%0Ain%20the%20target%20video%20generation.%20A%20reweighted%20motion%20transfer%20module%20is%20designed%0Ato%20allow%20the%20target%20video%20to%20reference%20the%20motion%20from%20the%20reference%20video.%20To%0Aavoid%20interference%20from%20the%20subject%20in%20the%20reference%20video%2C%20we%20propose%20an%0Aappearance%20separation%20module%20to%20suppress%20the%20appearance%20of%20the%20reference%0Asubject%20in%20the%20target%20video.%20We%20annotate%20the%20DAVIS%20dataset%20with%20detailed%0Aprompts%20for%20our%20experiments%20and%20design%20evaluation%20metrics%20to%20validate%20the%0Aeffectiveness%20of%20our%20method.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20achieves%20state-of-the-art%20performance%20in%20generation%20quality%2C%0Aprompt-video%20consistency%2C%20and%20control%20capability.%20Our%20homepage%20is%20available%20at%0Ahttps%3A//vpx-ecnu.github.io/LMP-Website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14167v1&entry.124074799=Read"},
{"title": "Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided\n  Extraction of Moving Speakers in Dynamic Scenarios", "author": "Jakob Kienegger and Timo Gerkmann", "abstract": "  Recent speaker extraction methods using deep non-linear spatial filtering\nperform exceptionally well when the target direction is known and stationary.\nHowever, spatially dynamic scenarios are considerably more challenging due to\ntime-varying spatial features and arising ambiguities, e.g. when moving\nspeakers cross. While in a static scenario it may be easy for a user to point\nto the target's direction, manually tracking a moving speaker is impractical.\nInstead of relying on accurate time-dependent directional cues, which we refer\nto as strong guidance, in this paper we propose a weakly guided extraction\nmethod solely depending on the target's initial position to cope with spatial\ndynamic scenarios. By incorporating our own deep tracking algorithm and\ndeveloping a joint training strategy on a synthetic dataset, we demonstrate the\nproficiency of our approach in resolving spatial ambiguities and even\noutperform a mismatched, but strongly guided extraction method.\n", "link": "http://arxiv.org/abs/2505.14517v1", "date": "2025-05-20", "relevancy": 2.7015, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5541}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Deep%20Non-Linear%20Spatially%20Selective%20Filters%20for%20Weakly%20Guided%0A%20%20Extraction%20of%20Moving%20Speakers%20in%20Dynamic%20Scenarios&body=Title%3A%20Steering%20Deep%20Non-Linear%20Spatially%20Selective%20Filters%20for%20Weakly%20Guided%0A%20%20Extraction%20of%20Moving%20Speakers%20in%20Dynamic%20Scenarios%0AAuthor%3A%20Jakob%20Kienegger%20and%20Timo%20Gerkmann%0AAbstract%3A%20%20%20Recent%20speaker%20extraction%20methods%20using%20deep%20non-linear%20spatial%20filtering%0Aperform%20exceptionally%20well%20when%20the%20target%20direction%20is%20known%20and%20stationary.%0AHowever%2C%20spatially%20dynamic%20scenarios%20are%20considerably%20more%20challenging%20due%20to%0Atime-varying%20spatial%20features%20and%20arising%20ambiguities%2C%20e.g.%20when%20moving%0Aspeakers%20cross.%20While%20in%20a%20static%20scenario%20it%20may%20be%20easy%20for%20a%20user%20to%20point%0Ato%20the%20target%27s%20direction%2C%20manually%20tracking%20a%20moving%20speaker%20is%20impractical.%0AInstead%20of%20relying%20on%20accurate%20time-dependent%20directional%20cues%2C%20which%20we%20refer%0Ato%20as%20strong%20guidance%2C%20in%20this%20paper%20we%20propose%20a%20weakly%20guided%20extraction%0Amethod%20solely%20depending%20on%20the%20target%27s%20initial%20position%20to%20cope%20with%20spatial%0Adynamic%20scenarios.%20By%20incorporating%20our%20own%20deep%20tracking%20algorithm%20and%0Adeveloping%20a%20joint%20training%20strategy%20on%20a%20synthetic%20dataset%2C%20we%20demonstrate%20the%0Aproficiency%20of%20our%20approach%20in%20resolving%20spatial%20ambiguities%20and%20even%0Aoutperform%20a%20mismatched%2C%20but%20strongly%20guided%20extraction%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14517v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Deep%2520Non-Linear%2520Spatially%2520Selective%2520Filters%2520for%2520Weakly%2520Guided%250A%2520%2520Extraction%2520of%2520Moving%2520Speakers%2520in%2520Dynamic%2520Scenarios%26entry.906535625%3DJakob%2520Kienegger%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3D%2520%2520Recent%2520speaker%2520extraction%2520methods%2520using%2520deep%2520non-linear%2520spatial%2520filtering%250Aperform%2520exceptionally%2520well%2520when%2520the%2520target%2520direction%2520is%2520known%2520and%2520stationary.%250AHowever%252C%2520spatially%2520dynamic%2520scenarios%2520are%2520considerably%2520more%2520challenging%2520due%2520to%250Atime-varying%2520spatial%2520features%2520and%2520arising%2520ambiguities%252C%2520e.g.%2520when%2520moving%250Aspeakers%2520cross.%2520While%2520in%2520a%2520static%2520scenario%2520it%2520may%2520be%2520easy%2520for%2520a%2520user%2520to%2520point%250Ato%2520the%2520target%2527s%2520direction%252C%2520manually%2520tracking%2520a%2520moving%2520speaker%2520is%2520impractical.%250AInstead%2520of%2520relying%2520on%2520accurate%2520time-dependent%2520directional%2520cues%252C%2520which%2520we%2520refer%250Ato%2520as%2520strong%2520guidance%252C%2520in%2520this%2520paper%2520we%2520propose%2520a%2520weakly%2520guided%2520extraction%250Amethod%2520solely%2520depending%2520on%2520the%2520target%2527s%2520initial%2520position%2520to%2520cope%2520with%2520spatial%250Adynamic%2520scenarios.%2520By%2520incorporating%2520our%2520own%2520deep%2520tracking%2520algorithm%2520and%250Adeveloping%2520a%2520joint%2520training%2520strategy%2520on%2520a%2520synthetic%2520dataset%252C%2520we%2520demonstrate%2520the%250Aproficiency%2520of%2520our%2520approach%2520in%2520resolving%2520spatial%2520ambiguities%2520and%2520even%250Aoutperform%2520a%2520mismatched%252C%2520but%2520strongly%2520guided%2520extraction%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14517v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Deep%20Non-Linear%20Spatially%20Selective%20Filters%20for%20Weakly%20Guided%0A%20%20Extraction%20of%20Moving%20Speakers%20in%20Dynamic%20Scenarios&entry.906535625=Jakob%20Kienegger%20and%20Timo%20Gerkmann&entry.1292438233=%20%20Recent%20speaker%20extraction%20methods%20using%20deep%20non-linear%20spatial%20filtering%0Aperform%20exceptionally%20well%20when%20the%20target%20direction%20is%20known%20and%20stationary.%0AHowever%2C%20spatially%20dynamic%20scenarios%20are%20considerably%20more%20challenging%20due%20to%0Atime-varying%20spatial%20features%20and%20arising%20ambiguities%2C%20e.g.%20when%20moving%0Aspeakers%20cross.%20While%20in%20a%20static%20scenario%20it%20may%20be%20easy%20for%20a%20user%20to%20point%0Ato%20the%20target%27s%20direction%2C%20manually%20tracking%20a%20moving%20speaker%20is%20impractical.%0AInstead%20of%20relying%20on%20accurate%20time-dependent%20directional%20cues%2C%20which%20we%20refer%0Ato%20as%20strong%20guidance%2C%20in%20this%20paper%20we%20propose%20a%20weakly%20guided%20extraction%0Amethod%20solely%20depending%20on%20the%20target%27s%20initial%20position%20to%20cope%20with%20spatial%0Adynamic%20scenarios.%20By%20incorporating%20our%20own%20deep%20tracking%20algorithm%20and%0Adeveloping%20a%20joint%20training%20strategy%20on%20a%20synthetic%20dataset%2C%20we%20demonstrate%20the%0Aproficiency%20of%20our%20approach%20in%20resolving%20spatial%20ambiguities%20and%20even%0Aoutperform%20a%20mismatched%2C%20but%20strongly%20guided%20extraction%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14517v1&entry.124074799=Read"},
{"title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward\n  Model", "author": "Yuhang Zang and Xiaoyi Dong and Pan Zhang and Yuhang Cao and Ziyu Liu and Shengyuan Ding and Shenxi Wu and Yubo Ma and Haodong Duan and Wenwei Zhang and Kai Chen and Dahua Lin and Jiaqi Wang", "abstract": "  Despite the promising performance of Large Vision Language Models (LVLMs) in\nvisual understanding, they occasionally generate incorrect outputs. While\nreward models (RMs) with reinforcement learning or test-time scaling offer the\npotential for improving generation quality, a critical gap remains: publicly\navailable multi-modal RMs for LVLMs are scarce, and the implementation details\nof proprietary models are often unclear. We bridge this gap with\nInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective\nmulti-modal reward model that aligns LVLMs with human preferences. To ensure\nthe robustness and versatility of IXC-2.5-Reward, we set up a high-quality\nmulti-modal preference corpus spanning text, image, and video inputs across\ndiverse domains, such as instruction following, general understanding,\ntext-rich documents, mathematical reasoning, and video understanding.\nIXC-2.5-Reward achieves excellent results on the latest multi-modal reward\nmodel benchmark and shows competitive performance on text-only reward model\nbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:\n(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward\nwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows\nconsistent improvements in instruction following and multi-modal open-ended\ndialogue; (2) Selecting the best response from candidate responses for\ntest-time scaling; and (3) Filtering outlier or noisy samples from existing\nimage and video instruction tuning training data. To ensure reproducibility and\nfacilitate further research, we have open-sourced all model weights and\ntraining recipes at\nhttps://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward\n", "link": "http://arxiv.org/abs/2501.12368v2", "date": "2025-05-20", "relevancy": 2.6873, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.547}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternLM-XComposer2.5-Reward%3A%20A%20Simple%20Yet%20Effective%20Multi-Modal%20Reward%0A%20%20Model&body=Title%3A%20InternLM-XComposer2.5-Reward%3A%20A%20Simple%20Yet%20Effective%20Multi-Modal%20Reward%0A%20%20Model%0AAuthor%3A%20Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Cao%20and%20Ziyu%20Liu%20and%20Shengyuan%20Ding%20and%20Shenxi%20Wu%20and%20Yubo%20Ma%20and%20Haodong%20Duan%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Despite%20the%20promising%20performance%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20in%0Avisual%20understanding%2C%20they%20occasionally%20generate%20incorrect%20outputs.%20While%0Areward%20models%20%28RMs%29%20with%20reinforcement%20learning%20or%20test-time%20scaling%20offer%20the%0Apotential%20for%20improving%20generation%20quality%2C%20a%20critical%20gap%20remains%3A%20publicly%0Aavailable%20multi-modal%20RMs%20for%20LVLMs%20are%20scarce%2C%20and%20the%20implementation%20details%0Aof%20proprietary%20models%20are%20often%20unclear.%20We%20bridge%20this%20gap%20with%0AInternLM-XComposer2.5-Reward%20%28IXC-2.5-Reward%29%2C%20a%20simple%20yet%20effective%0Amulti-modal%20reward%20model%20that%20aligns%20LVLMs%20with%20human%20preferences.%20To%20ensure%0Athe%20robustness%20and%20versatility%20of%20IXC-2.5-Reward%2C%20we%20set%20up%20a%20high-quality%0Amulti-modal%20preference%20corpus%20spanning%20text%2C%20image%2C%20and%20video%20inputs%20across%0Adiverse%20domains%2C%20such%20as%20instruction%20following%2C%20general%20understanding%2C%0Atext-rich%20documents%2C%20mathematical%20reasoning%2C%20and%20video%20understanding.%0AIXC-2.5-Reward%20achieves%20excellent%20results%20on%20the%20latest%20multi-modal%20reward%0Amodel%20benchmark%20and%20shows%20competitive%20performance%20on%20text-only%20reward%20model%0Abenchmarks.%20We%20further%20demonstrate%20three%20key%20applications%20of%20IXC-2.5-Reward%3A%0A%281%29%20Providing%20a%20supervisory%20signal%20for%20RL%20training.%20We%20integrate%20IXC-2.5-Reward%0Awith%20Proximal%20Policy%20Optimization%20%28PPO%29%20yields%20IXC-2.5-Chat%2C%20which%20shows%0Aconsistent%20improvements%20in%20instruction%20following%20and%20multi-modal%20open-ended%0Adialogue%3B%20%282%29%20Selecting%20the%20best%20response%20from%20candidate%20responses%20for%0Atest-time%20scaling%3B%20and%20%283%29%20Filtering%20outlier%20or%20noisy%20samples%20from%20existing%0Aimage%20and%20video%20instruction%20tuning%20training%20data.%20To%20ensure%20reproducibility%20and%0Afacilitate%20further%20research%2C%20we%20have%20open-sourced%20all%20model%20weights%20and%0Atraining%20recipes%20at%0Ahttps%3A//github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternLM-XComposer2.5-Reward%253A%2520A%2520Simple%2520Yet%2520Effective%2520Multi-Modal%2520Reward%250A%2520%2520Model%26entry.906535625%3DYuhang%2520Zang%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Cao%2520and%2520Ziyu%2520Liu%2520and%2520Shengyuan%2520Ding%2520and%2520Shenxi%2520Wu%2520and%2520Yubo%2520Ma%2520and%2520Haodong%2520Duan%2520and%2520Wenwei%2520Zhang%2520and%2520Kai%2520Chen%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520promising%2520performance%2520of%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520in%250Avisual%2520understanding%252C%2520they%2520occasionally%2520generate%2520incorrect%2520outputs.%2520While%250Areward%2520models%2520%2528RMs%2529%2520with%2520reinforcement%2520learning%2520or%2520test-time%2520scaling%2520offer%2520the%250Apotential%2520for%2520improving%2520generation%2520quality%252C%2520a%2520critical%2520gap%2520remains%253A%2520publicly%250Aavailable%2520multi-modal%2520RMs%2520for%2520LVLMs%2520are%2520scarce%252C%2520and%2520the%2520implementation%2520details%250Aof%2520proprietary%2520models%2520are%2520often%2520unclear.%2520We%2520bridge%2520this%2520gap%2520with%250AInternLM-XComposer2.5-Reward%2520%2528IXC-2.5-Reward%2529%252C%2520a%2520simple%2520yet%2520effective%250Amulti-modal%2520reward%2520model%2520that%2520aligns%2520LVLMs%2520with%2520human%2520preferences.%2520To%2520ensure%250Athe%2520robustness%2520and%2520versatility%2520of%2520IXC-2.5-Reward%252C%2520we%2520set%2520up%2520a%2520high-quality%250Amulti-modal%2520preference%2520corpus%2520spanning%2520text%252C%2520image%252C%2520and%2520video%2520inputs%2520across%250Adiverse%2520domains%252C%2520such%2520as%2520instruction%2520following%252C%2520general%2520understanding%252C%250Atext-rich%2520documents%252C%2520mathematical%2520reasoning%252C%2520and%2520video%2520understanding.%250AIXC-2.5-Reward%2520achieves%2520excellent%2520results%2520on%2520the%2520latest%2520multi-modal%2520reward%250Amodel%2520benchmark%2520and%2520shows%2520competitive%2520performance%2520on%2520text-only%2520reward%2520model%250Abenchmarks.%2520We%2520further%2520demonstrate%2520three%2520key%2520applications%2520of%2520IXC-2.5-Reward%253A%250A%25281%2529%2520Providing%2520a%2520supervisory%2520signal%2520for%2520RL%2520training.%2520We%2520integrate%2520IXC-2.5-Reward%250Awith%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520yields%2520IXC-2.5-Chat%252C%2520which%2520shows%250Aconsistent%2520improvements%2520in%2520instruction%2520following%2520and%2520multi-modal%2520open-ended%250Adialogue%253B%2520%25282%2529%2520Selecting%2520the%2520best%2520response%2520from%2520candidate%2520responses%2520for%250Atest-time%2520scaling%253B%2520and%2520%25283%2529%2520Filtering%2520outlier%2520or%2520noisy%2520samples%2520from%2520existing%250Aimage%2520and%2520video%2520instruction%2520tuning%2520training%2520data.%2520To%2520ensure%2520reproducibility%2520and%250Afacilitate%2520further%2520research%252C%2520we%2520have%2520open-sourced%2520all%2520model%2520weights%2520and%250Atraining%2520recipes%2520at%250Ahttps%253A//github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternLM-XComposer2.5-Reward%3A%20A%20Simple%20Yet%20Effective%20Multi-Modal%20Reward%0A%20%20Model&entry.906535625=Yuhang%20Zang%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Cao%20and%20Ziyu%20Liu%20and%20Shengyuan%20Ding%20and%20Shenxi%20Wu%20and%20Yubo%20Ma%20and%20Haodong%20Duan%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Despite%20the%20promising%20performance%20of%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20in%0Avisual%20understanding%2C%20they%20occasionally%20generate%20incorrect%20outputs.%20While%0Areward%20models%20%28RMs%29%20with%20reinforcement%20learning%20or%20test-time%20scaling%20offer%20the%0Apotential%20for%20improving%20generation%20quality%2C%20a%20critical%20gap%20remains%3A%20publicly%0Aavailable%20multi-modal%20RMs%20for%20LVLMs%20are%20scarce%2C%20and%20the%20implementation%20details%0Aof%20proprietary%20models%20are%20often%20unclear.%20We%20bridge%20this%20gap%20with%0AInternLM-XComposer2.5-Reward%20%28IXC-2.5-Reward%29%2C%20a%20simple%20yet%20effective%0Amulti-modal%20reward%20model%20that%20aligns%20LVLMs%20with%20human%20preferences.%20To%20ensure%0Athe%20robustness%20and%20versatility%20of%20IXC-2.5-Reward%2C%20we%20set%20up%20a%20high-quality%0Amulti-modal%20preference%20corpus%20spanning%20text%2C%20image%2C%20and%20video%20inputs%20across%0Adiverse%20domains%2C%20such%20as%20instruction%20following%2C%20general%20understanding%2C%0Atext-rich%20documents%2C%20mathematical%20reasoning%2C%20and%20video%20understanding.%0AIXC-2.5-Reward%20achieves%20excellent%20results%20on%20the%20latest%20multi-modal%20reward%0Amodel%20benchmark%20and%20shows%20competitive%20performance%20on%20text-only%20reward%20model%0Abenchmarks.%20We%20further%20demonstrate%20three%20key%20applications%20of%20IXC-2.5-Reward%3A%0A%281%29%20Providing%20a%20supervisory%20signal%20for%20RL%20training.%20We%20integrate%20IXC-2.5-Reward%0Awith%20Proximal%20Policy%20Optimization%20%28PPO%29%20yields%20IXC-2.5-Chat%2C%20which%20shows%0Aconsistent%20improvements%20in%20instruction%20following%20and%20multi-modal%20open-ended%0Adialogue%3B%20%282%29%20Selecting%20the%20best%20response%20from%20candidate%20responses%20for%0Atest-time%20scaling%3B%20and%20%283%29%20Filtering%20outlier%20or%20noisy%20samples%20from%20existing%0Aimage%20and%20video%20instruction%20tuning%20training%20data.%20To%20ensure%20reproducibility%20and%0Afacilitate%20further%20research%2C%20we%20have%20open-sourced%20all%20model%20weights%20and%0Atraining%20recipes%20at%0Ahttps%3A//github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12368v2&entry.124074799=Read"},
{"title": "VoQA: Visual-only Question Answering", "author": "Luyang Jiang and Jianing An and Jie Luo and Wenjun Wu and Lei Huang", "abstract": "  We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually.\n", "link": "http://arxiv.org/abs/2505.14227v1", "date": "2025-05-20", "relevancy": 2.686, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoQA%3A%20Visual-only%20Question%20Answering&body=Title%3A%20VoQA%3A%20Visual-only%20Question%20Answering%0AAuthor%3A%20Luyang%20Jiang%20and%20Jianing%20An%20and%20Jie%20Luo%20and%20Wenjun%20Wu%20and%20Lei%20Huang%0AAbstract%3A%20%20%20We%20propose%20Visual-only%20Question%20Answering%20%28VoQA%29%2C%20a%20novel%20multimodal%20task%20in%0Awhich%20questions%20are%20visually%20embedded%20within%20images%2C%20without%20any%20accompanying%0Atextual%20input.%20This%20requires%20models%20to%20locate%2C%20recognize%2C%20and%20reason%20over%0Avisually%20embedded%20textual%20questions%2C%20posing%20challenges%20for%20existing%20large%0Avision-language%20models%20%28LVLMs%29%2C%20which%20show%20notable%20performance%20drops%20even%20with%0Acarefully%20designed%20prompts.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Guided%20Response%0ATriggering%20Supervised%20Fine-tuning%20%28GRT-SFT%29%2C%20a%20structured%20fine-tuning%20strategy%0Athat%20guides%20the%20model%20to%20perform%20step-by-step%20reasoning%20purely%20based%20on%20visual%0Ainput%2C%20significantly%20improving%20model%20performance.%20Our%20work%20enhances%20models%27%0Acapacity%20for%20human-like%20visual%20understanding%20in%20complex%20multimodal%20scenarios%2C%0Awhere%20information%2C%20including%20language%2C%20is%20perceived%20visually.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoQA%253A%2520Visual-only%2520Question%2520Answering%26entry.906535625%3DLuyang%2520Jiang%2520and%2520Jianing%2520An%2520and%2520Jie%2520Luo%2520and%2520Wenjun%2520Wu%2520and%2520Lei%2520Huang%26entry.1292438233%3D%2520%2520We%2520propose%2520Visual-only%2520Question%2520Answering%2520%2528VoQA%2529%252C%2520a%2520novel%2520multimodal%2520task%2520in%250Awhich%2520questions%2520are%2520visually%2520embedded%2520within%2520images%252C%2520without%2520any%2520accompanying%250Atextual%2520input.%2520This%2520requires%2520models%2520to%2520locate%252C%2520recognize%252C%2520and%2520reason%2520over%250Avisually%2520embedded%2520textual%2520questions%252C%2520posing%2520challenges%2520for%2520existing%2520large%250Avision-language%2520models%2520%2528LVLMs%2529%252C%2520which%2520show%2520notable%2520performance%2520drops%2520even%2520with%250Acarefully%2520designed%2520prompts.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520Guided%2520Response%250ATriggering%2520Supervised%2520Fine-tuning%2520%2528GRT-SFT%2529%252C%2520a%2520structured%2520fine-tuning%2520strategy%250Athat%2520guides%2520the%2520model%2520to%2520perform%2520step-by-step%2520reasoning%2520purely%2520based%2520on%2520visual%250Ainput%252C%2520significantly%2520improving%2520model%2520performance.%2520Our%2520work%2520enhances%2520models%2527%250Acapacity%2520for%2520human-like%2520visual%2520understanding%2520in%2520complex%2520multimodal%2520scenarios%252C%250Awhere%2520information%252C%2520including%2520language%252C%2520is%2520perceived%2520visually.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoQA%3A%20Visual-only%20Question%20Answering&entry.906535625=Luyang%20Jiang%20and%20Jianing%20An%20and%20Jie%20Luo%20and%20Wenjun%20Wu%20and%20Lei%20Huang&entry.1292438233=%20%20We%20propose%20Visual-only%20Question%20Answering%20%28VoQA%29%2C%20a%20novel%20multimodal%20task%20in%0Awhich%20questions%20are%20visually%20embedded%20within%20images%2C%20without%20any%20accompanying%0Atextual%20input.%20This%20requires%20models%20to%20locate%2C%20recognize%2C%20and%20reason%20over%0Avisually%20embedded%20textual%20questions%2C%20posing%20challenges%20for%20existing%20large%0Avision-language%20models%20%28LVLMs%29%2C%20which%20show%20notable%20performance%20drops%20even%20with%0Acarefully%20designed%20prompts.%20To%20bridge%20this%20gap%2C%20we%20introduce%20Guided%20Response%0ATriggering%20Supervised%20Fine-tuning%20%28GRT-SFT%29%2C%20a%20structured%20fine-tuning%20strategy%0Athat%20guides%20the%20model%20to%20perform%20step-by-step%20reasoning%20purely%20based%20on%20visual%0Ainput%2C%20significantly%20improving%20model%20performance.%20Our%20work%20enhances%20models%27%0Acapacity%20for%20human-like%20visual%20understanding%20in%20complex%20multimodal%20scenarios%2C%0Awhere%20information%2C%20including%20language%2C%20is%20perceived%20visually.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14227v1&entry.124074799=Read"},
{"title": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors\n  Approach", "author": "Umberto Cappellazzo and Minsu Kim and Stavros Petridis and Daniele Falavigna and Alessio Brutti", "abstract": "  Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness.\n", "link": "http://arxiv.org/abs/2505.14336v1", "date": "2025-05-20", "relevancy": 2.6791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20and%20Enhancing%20LLM-based%20AVSR%3A%20A%20Sparse%20Mixture%20of%20Projectors%0A%20%20Approach&body=Title%3A%20Scaling%20and%20Enhancing%20LLM-based%20AVSR%3A%20A%20Sparse%20Mixture%20of%20Projectors%0A%20%20Approach%0AAuthor%3A%20Umberto%20Cappellazzo%20and%20Minsu%20Kim%20and%20Stavros%20Petridis%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti%0AAbstract%3A%20%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20enhances%20robustness%20in%20noisy%0Aenvironments%20by%20integrating%20visual%20cues.%20While%20recent%20advances%20integrate%20Large%0ALanguage%20Models%20%28LLMs%29%20into%20AVSR%2C%20their%20high%20computational%20cost%20hinders%0Adeployment%20in%20resource-constrained%20settings.%20To%20address%20this%2C%20we%20propose%0ALlama-SMoP%2C%20an%20efficient%20Multimodal%20LLM%20that%20employs%20a%20Sparse%20Mixture%20of%0AProjectors%20%28SMoP%29%20module%20to%20scale%20model%20capacity%20without%20increasing%20inference%0Acosts.%20By%20incorporating%20sparsely-gated%20mixture-of-experts%20%28MoE%29%20projectors%2C%0ALlama-SMoP%20enables%20the%20use%20of%20smaller%20LLMs%20while%20maintaining%20strong%0Aperformance.%20We%20explore%20three%20SMoP%20configurations%20and%20show%20that%20Llama-SMoP%20DEDR%0A%28Disjoint-Experts%2C%20Disjoint-Routers%29%2C%20which%20uses%20modality-specific%20routers%20and%0Aexperts%2C%20achieves%20superior%20performance%20on%20ASR%2C%20VSR%2C%20and%20AVSR%20tasks.%20Ablation%0Astudies%20confirm%20its%20effectiveness%20in%20expert%20activation%2C%20scalability%2C%20and%20noise%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520and%2520Enhancing%2520LLM-based%2520AVSR%253A%2520A%2520Sparse%2520Mixture%2520of%2520Projectors%250A%2520%2520Approach%26entry.906535625%3DUmberto%2520Cappellazzo%2520and%2520Minsu%2520Kim%2520and%2520Stavros%2520Petridis%2520and%2520Daniele%2520Falavigna%2520and%2520Alessio%2520Brutti%26entry.1292438233%3D%2520%2520Audio-Visual%2520Speech%2520Recognition%2520%2528AVSR%2529%2520enhances%2520robustness%2520in%2520noisy%250Aenvironments%2520by%2520integrating%2520visual%2520cues.%2520While%2520recent%2520advances%2520integrate%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520into%2520AVSR%252C%2520their%2520high%2520computational%2520cost%2520hinders%250Adeployment%2520in%2520resource-constrained%2520settings.%2520To%2520address%2520this%252C%2520we%2520propose%250ALlama-SMoP%252C%2520an%2520efficient%2520Multimodal%2520LLM%2520that%2520employs%2520a%2520Sparse%2520Mixture%2520of%250AProjectors%2520%2528SMoP%2529%2520module%2520to%2520scale%2520model%2520capacity%2520without%2520increasing%2520inference%250Acosts.%2520By%2520incorporating%2520sparsely-gated%2520mixture-of-experts%2520%2528MoE%2529%2520projectors%252C%250ALlama-SMoP%2520enables%2520the%2520use%2520of%2520smaller%2520LLMs%2520while%2520maintaining%2520strong%250Aperformance.%2520We%2520explore%2520three%2520SMoP%2520configurations%2520and%2520show%2520that%2520Llama-SMoP%2520DEDR%250A%2528Disjoint-Experts%252C%2520Disjoint-Routers%2529%252C%2520which%2520uses%2520modality-specific%2520routers%2520and%250Aexperts%252C%2520achieves%2520superior%2520performance%2520on%2520ASR%252C%2520VSR%252C%2520and%2520AVSR%2520tasks.%2520Ablation%250Astudies%2520confirm%2520its%2520effectiveness%2520in%2520expert%2520activation%252C%2520scalability%252C%2520and%2520noise%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20and%20Enhancing%20LLM-based%20AVSR%3A%20A%20Sparse%20Mixture%20of%20Projectors%0A%20%20Approach&entry.906535625=Umberto%20Cappellazzo%20and%20Minsu%20Kim%20and%20Stavros%20Petridis%20and%20Daniele%20Falavigna%20and%20Alessio%20Brutti&entry.1292438233=%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20enhances%20robustness%20in%20noisy%0Aenvironments%20by%20integrating%20visual%20cues.%20While%20recent%20advances%20integrate%20Large%0ALanguage%20Models%20%28LLMs%29%20into%20AVSR%2C%20their%20high%20computational%20cost%20hinders%0Adeployment%20in%20resource-constrained%20settings.%20To%20address%20this%2C%20we%20propose%0ALlama-SMoP%2C%20an%20efficient%20Multimodal%20LLM%20that%20employs%20a%20Sparse%20Mixture%20of%0AProjectors%20%28SMoP%29%20module%20to%20scale%20model%20capacity%20without%20increasing%20inference%0Acosts.%20By%20incorporating%20sparsely-gated%20mixture-of-experts%20%28MoE%29%20projectors%2C%0ALlama-SMoP%20enables%20the%20use%20of%20smaller%20LLMs%20while%20maintaining%20strong%0Aperformance.%20We%20explore%20three%20SMoP%20configurations%20and%20show%20that%20Llama-SMoP%20DEDR%0A%28Disjoint-Experts%2C%20Disjoint-Routers%29%2C%20which%20uses%20modality-specific%20routers%20and%0Aexperts%2C%20achieves%20superior%20performance%20on%20ASR%2C%20VSR%2C%20and%20AVSR%20tasks.%20Ablation%0Astudies%20confirm%20its%20effectiveness%20in%20expert%20activation%2C%20scalability%2C%20and%20noise%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14336v1&entry.124074799=Read"},
{"title": "Instance Segmentation for Point Sets", "author": "Abhimanyu Talwar and Julien Laasri", "abstract": "  Recently proposed neural network architectures like PointNet [QSMG16] and\nPointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point\nsets. The feature representations of shapes learned by these two networks\nenabled training classifiers for Semantic Segmentation, and more recently for\nInstance Segmentation via the Similarity Group Proposal Network (SGPN)\n[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,\npertains to use of memory intensive similarity matrices which occupy memory\nquadratic in the number of points. In this report, we attempt to tackle this\nissue through use of two sampling based methods, which compute Instance\nSegmentation on a sub-sampled Point Set, and then extrapolate labels to the\ncomplete set using the nearest neigbhour approach. While both approaches\nperform equally well on large sub-samples, the random-based strategy gives the\nmost improvements in terms of speed and memory usage.\n", "link": "http://arxiv.org/abs/2505.14583v1", "date": "2025-05-20", "relevancy": 2.655, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5328}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance%20Segmentation%20for%20Point%20Sets&body=Title%3A%20Instance%20Segmentation%20for%20Point%20Sets%0AAuthor%3A%20Abhimanyu%20Talwar%20and%20Julien%20Laasri%0AAbstract%3A%20%20%20Recently%20proposed%20neural%20network%20architectures%20like%20PointNet%20%5BQSMG16%5D%20and%0APointNet%2B%2B%20%5BQYSG17%5D%20have%20made%20it%20possible%20to%20apply%20Deep%20Learning%20to%203D%20point%0Asets.%20The%20feature%20representations%20of%20shapes%20learned%20by%20these%20two%20networks%0Aenabled%20training%20classifiers%20for%20Semantic%20Segmentation%2C%20and%20more%20recently%20for%0AInstance%20Segmentation%20via%20the%20Similarity%20Group%20Proposal%20Network%20%28SGPN%29%0A%5BWYHN17%5D.%20One%20area%20of%20improvement%20which%20has%20been%20highlighted%20by%20SGPN%27s%20authors%2C%0Apertains%20to%20use%20of%20memory%20intensive%20similarity%20matrices%20which%20occupy%20memory%0Aquadratic%20in%20the%20number%20of%20points.%20In%20this%20report%2C%20we%20attempt%20to%20tackle%20this%0Aissue%20through%20use%20of%20two%20sampling%20based%20methods%2C%20which%20compute%20Instance%0ASegmentation%20on%20a%20sub-sampled%20Point%20Set%2C%20and%20then%20extrapolate%20labels%20to%20the%0Acomplete%20set%20using%20the%20nearest%20neigbhour%20approach.%20While%20both%20approaches%0Aperform%20equally%20well%20on%20large%20sub-samples%2C%20the%20random-based%20strategy%20gives%20the%0Amost%20improvements%20in%20terms%20of%20speed%20and%20memory%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance%2520Segmentation%2520for%2520Point%2520Sets%26entry.906535625%3DAbhimanyu%2520Talwar%2520and%2520Julien%2520Laasri%26entry.1292438233%3D%2520%2520Recently%2520proposed%2520neural%2520network%2520architectures%2520like%2520PointNet%2520%255BQSMG16%255D%2520and%250APointNet%252B%252B%2520%255BQYSG17%255D%2520have%2520made%2520it%2520possible%2520to%2520apply%2520Deep%2520Learning%2520to%25203D%2520point%250Asets.%2520The%2520feature%2520representations%2520of%2520shapes%2520learned%2520by%2520these%2520two%2520networks%250Aenabled%2520training%2520classifiers%2520for%2520Semantic%2520Segmentation%252C%2520and%2520more%2520recently%2520for%250AInstance%2520Segmentation%2520via%2520the%2520Similarity%2520Group%2520Proposal%2520Network%2520%2528SGPN%2529%250A%255BWYHN17%255D.%2520One%2520area%2520of%2520improvement%2520which%2520has%2520been%2520highlighted%2520by%2520SGPN%2527s%2520authors%252C%250Apertains%2520to%2520use%2520of%2520memory%2520intensive%2520similarity%2520matrices%2520which%2520occupy%2520memory%250Aquadratic%2520in%2520the%2520number%2520of%2520points.%2520In%2520this%2520report%252C%2520we%2520attempt%2520to%2520tackle%2520this%250Aissue%2520through%2520use%2520of%2520two%2520sampling%2520based%2520methods%252C%2520which%2520compute%2520Instance%250ASegmentation%2520on%2520a%2520sub-sampled%2520Point%2520Set%252C%2520and%2520then%2520extrapolate%2520labels%2520to%2520the%250Acomplete%2520set%2520using%2520the%2520nearest%2520neigbhour%2520approach.%2520While%2520both%2520approaches%250Aperform%2520equally%2520well%2520on%2520large%2520sub-samples%252C%2520the%2520random-based%2520strategy%2520gives%2520the%250Amost%2520improvements%2520in%2520terms%2520of%2520speed%2520and%2520memory%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance%20Segmentation%20for%20Point%20Sets&entry.906535625=Abhimanyu%20Talwar%20and%20Julien%20Laasri&entry.1292438233=%20%20Recently%20proposed%20neural%20network%20architectures%20like%20PointNet%20%5BQSMG16%5D%20and%0APointNet%2B%2B%20%5BQYSG17%5D%20have%20made%20it%20possible%20to%20apply%20Deep%20Learning%20to%203D%20point%0Asets.%20The%20feature%20representations%20of%20shapes%20learned%20by%20these%20two%20networks%0Aenabled%20training%20classifiers%20for%20Semantic%20Segmentation%2C%20and%20more%20recently%20for%0AInstance%20Segmentation%20via%20the%20Similarity%20Group%20Proposal%20Network%20%28SGPN%29%0A%5BWYHN17%5D.%20One%20area%20of%20improvement%20which%20has%20been%20highlighted%20by%20SGPN%27s%20authors%2C%0Apertains%20to%20use%20of%20memory%20intensive%20similarity%20matrices%20which%20occupy%20memory%0Aquadratic%20in%20the%20number%20of%20points.%20In%20this%20report%2C%20we%20attempt%20to%20tackle%20this%0Aissue%20through%20use%20of%20two%20sampling%20based%20methods%2C%20which%20compute%20Instance%0ASegmentation%20on%20a%20sub-sampled%20Point%20Set%2C%20and%20then%20extrapolate%20labels%20to%20the%0Acomplete%20set%20using%20the%20nearest%20neigbhour%20approach.%20While%20both%20approaches%0Aperform%20equally%20well%20on%20large%20sub-samples%2C%20the%20random-based%20strategy%20gives%20the%0Amost%20improvements%20in%20terms%20of%20speed%20and%20memory%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14583v1&entry.124074799=Read"},
{"title": "UniCTokens: Boosting Personalized Understanding and Generation via\n  Unified Concept Tokens", "author": "Ruichuan An and Sihan Yang and Renrui Zhang and Zijun Shen and Ming Lu and Gaole Dai and Hao Liang and Ziyu Guo and Shilin Yan and Yulin Luo and Bocheng Zou and Chaoqun Yang and Wentao Zhang", "abstract": "  Personalized models have demonstrated remarkable success in understanding and\ngenerating concepts provided by users. However, existing methods use separate\nconcept tokens for understanding and generation, treating these tasks in\nisolation. This may result in limitations for generating images with complex\nprompts. For example, given the concept $\\langle bo\\rangle$, generating\n\"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions\nof its hat. We call this kind of generation personalized knowledge-driven\ngeneration. To address the limitation, we present UniCTokens, a novel framework\nthat effectively integrates personalized information into a unified vision\nlanguage model (VLM) for understanding and generation. UniCTokens trains a set\nof unified concept tokens to leverage complementary semantics, boosting two\npersonalized tasks. Moreover, we propose a progressive training strategy with\nthree stages: understanding warm-up, bootstrapping generation from\nunderstanding, and deepening understanding from generation to enhance mutual\nbenefits between both tasks. To quantitatively evaluate the unified VLM\npersonalization, we present UnifyBench, the first benchmark for assessing\nconcept understanding, concept generation, and knowledge-driven generation.\nExperimental results on UnifyBench indicate that UniCTokens shows competitive\nperformance compared to leading methods in concept understanding, concept\ngeneration, and achieving state-of-the-art results in personalized\nknowledge-driven generation. Our research demonstrates that enhanced\nunderstanding improves generation, and the generation process can yield\nvaluable insights into understanding. Our code and dataset will be released at:\n\\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.\n", "link": "http://arxiv.org/abs/2505.14671v1", "date": "2025-05-20", "relevancy": 2.6529, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5376}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5343}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniCTokens%3A%20Boosting%20Personalized%20Understanding%20and%20Generation%20via%0A%20%20Unified%20Concept%20Tokens&body=Title%3A%20UniCTokens%3A%20Boosting%20Personalized%20Understanding%20and%20Generation%20via%0A%20%20Unified%20Concept%20Tokens%0AAuthor%3A%20Ruichuan%20An%20and%20Sihan%20Yang%20and%20Renrui%20Zhang%20and%20Zijun%20Shen%20and%20Ming%20Lu%20and%20Gaole%20Dai%20and%20Hao%20Liang%20and%20Ziyu%20Guo%20and%20Shilin%20Yan%20and%20Yulin%20Luo%20and%20Bocheng%20Zou%20and%20Chaoqun%20Yang%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Personalized%20models%20have%20demonstrated%20remarkable%20success%20in%20understanding%20and%0Agenerating%20concepts%20provided%20by%20users.%20However%2C%20existing%20methods%20use%20separate%0Aconcept%20tokens%20for%20understanding%20and%20generation%2C%20treating%20these%20tasks%20in%0Aisolation.%20This%20may%20result%20in%20limitations%20for%20generating%20images%20with%20complex%0Aprompts.%20For%20example%2C%20given%20the%20concept%20%24%5Clangle%20bo%5Crangle%24%2C%20generating%0A%22%24%5Clangle%20bo%5Crangle%24%20wearing%20its%20hat%22%20without%20additional%20textual%20descriptions%0Aof%20its%20hat.%20We%20call%20this%20kind%20of%20generation%20personalized%20knowledge-driven%0Ageneration.%20To%20address%20the%20limitation%2C%20we%20present%20UniCTokens%2C%20a%20novel%20framework%0Athat%20effectively%20integrates%20personalized%20information%20into%20a%20unified%20vision%0Alanguage%20model%20%28VLM%29%20for%20understanding%20and%20generation.%20UniCTokens%20trains%20a%20set%0Aof%20unified%20concept%20tokens%20to%20leverage%20complementary%20semantics%2C%20boosting%20two%0Apersonalized%20tasks.%20Moreover%2C%20we%20propose%20a%20progressive%20training%20strategy%20with%0Athree%20stages%3A%20understanding%20warm-up%2C%20bootstrapping%20generation%20from%0Aunderstanding%2C%20and%20deepening%20understanding%20from%20generation%20to%20enhance%20mutual%0Abenefits%20between%20both%20tasks.%20To%20quantitatively%20evaluate%20the%20unified%20VLM%0Apersonalization%2C%20we%20present%20UnifyBench%2C%20the%20first%20benchmark%20for%20assessing%0Aconcept%20understanding%2C%20concept%20generation%2C%20and%20knowledge-driven%20generation.%0AExperimental%20results%20on%20UnifyBench%20indicate%20that%20UniCTokens%20shows%20competitive%0Aperformance%20compared%20to%20leading%20methods%20in%20concept%20understanding%2C%20concept%0Ageneration%2C%20and%20achieving%20state-of-the-art%20results%20in%20personalized%0Aknowledge-driven%20generation.%20Our%20research%20demonstrates%20that%20enhanced%0Aunderstanding%20improves%20generation%2C%20and%20the%20generation%20process%20can%20yield%0Avaluable%20insights%20into%20understanding.%20Our%20code%20and%20dataset%20will%20be%20released%20at%3A%0A%5Chref%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14671v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniCTokens%253A%2520Boosting%2520Personalized%2520Understanding%2520and%2520Generation%2520via%250A%2520%2520Unified%2520Concept%2520Tokens%26entry.906535625%3DRuichuan%2520An%2520and%2520Sihan%2520Yang%2520and%2520Renrui%2520Zhang%2520and%2520Zijun%2520Shen%2520and%2520Ming%2520Lu%2520and%2520Gaole%2520Dai%2520and%2520Hao%2520Liang%2520and%2520Ziyu%2520Guo%2520and%2520Shilin%2520Yan%2520and%2520Yulin%2520Luo%2520and%2520Bocheng%2520Zou%2520and%2520Chaoqun%2520Yang%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Personalized%2520models%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520understanding%2520and%250Agenerating%2520concepts%2520provided%2520by%2520users.%2520However%252C%2520existing%2520methods%2520use%2520separate%250Aconcept%2520tokens%2520for%2520understanding%2520and%2520generation%252C%2520treating%2520these%2520tasks%2520in%250Aisolation.%2520This%2520may%2520result%2520in%2520limitations%2520for%2520generating%2520images%2520with%2520complex%250Aprompts.%2520For%2520example%252C%2520given%2520the%2520concept%2520%2524%255Clangle%2520bo%255Crangle%2524%252C%2520generating%250A%2522%2524%255Clangle%2520bo%255Crangle%2524%2520wearing%2520its%2520hat%2522%2520without%2520additional%2520textual%2520descriptions%250Aof%2520its%2520hat.%2520We%2520call%2520this%2520kind%2520of%2520generation%2520personalized%2520knowledge-driven%250Ageneration.%2520To%2520address%2520the%2520limitation%252C%2520we%2520present%2520UniCTokens%252C%2520a%2520novel%2520framework%250Athat%2520effectively%2520integrates%2520personalized%2520information%2520into%2520a%2520unified%2520vision%250Alanguage%2520model%2520%2528VLM%2529%2520for%2520understanding%2520and%2520generation.%2520UniCTokens%2520trains%2520a%2520set%250Aof%2520unified%2520concept%2520tokens%2520to%2520leverage%2520complementary%2520semantics%252C%2520boosting%2520two%250Apersonalized%2520tasks.%2520Moreover%252C%2520we%2520propose%2520a%2520progressive%2520training%2520strategy%2520with%250Athree%2520stages%253A%2520understanding%2520warm-up%252C%2520bootstrapping%2520generation%2520from%250Aunderstanding%252C%2520and%2520deepening%2520understanding%2520from%2520generation%2520to%2520enhance%2520mutual%250Abenefits%2520between%2520both%2520tasks.%2520To%2520quantitatively%2520evaluate%2520the%2520unified%2520VLM%250Apersonalization%252C%2520we%2520present%2520UnifyBench%252C%2520the%2520first%2520benchmark%2520for%2520assessing%250Aconcept%2520understanding%252C%2520concept%2520generation%252C%2520and%2520knowledge-driven%2520generation.%250AExperimental%2520results%2520on%2520UnifyBench%2520indicate%2520that%2520UniCTokens%2520shows%2520competitive%250Aperformance%2520compared%2520to%2520leading%2520methods%2520in%2520concept%2520understanding%252C%2520concept%250Ageneration%252C%2520and%2520achieving%2520state-of-the-art%2520results%2520in%2520personalized%250Aknowledge-driven%2520generation.%2520Our%2520research%2520demonstrates%2520that%2520enhanced%250Aunderstanding%2520improves%2520generation%252C%2520and%2520the%2520generation%2520process%2520can%2520yield%250Avaluable%2520insights%2520into%2520understanding.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520released%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/arctanxarc/UniCTokens%257D%257Bhttps%253A//github.com/arctanxarc/UniCTokens%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14671v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniCTokens%3A%20Boosting%20Personalized%20Understanding%20and%20Generation%20via%0A%20%20Unified%20Concept%20Tokens&entry.906535625=Ruichuan%20An%20and%20Sihan%20Yang%20and%20Renrui%20Zhang%20and%20Zijun%20Shen%20and%20Ming%20Lu%20and%20Gaole%20Dai%20and%20Hao%20Liang%20and%20Ziyu%20Guo%20and%20Shilin%20Yan%20and%20Yulin%20Luo%20and%20Bocheng%20Zou%20and%20Chaoqun%20Yang%20and%20Wentao%20Zhang&entry.1292438233=%20%20Personalized%20models%20have%20demonstrated%20remarkable%20success%20in%20understanding%20and%0Agenerating%20concepts%20provided%20by%20users.%20However%2C%20existing%20methods%20use%20separate%0Aconcept%20tokens%20for%20understanding%20and%20generation%2C%20treating%20these%20tasks%20in%0Aisolation.%20This%20may%20result%20in%20limitations%20for%20generating%20images%20with%20complex%0Aprompts.%20For%20example%2C%20given%20the%20concept%20%24%5Clangle%20bo%5Crangle%24%2C%20generating%0A%22%24%5Clangle%20bo%5Crangle%24%20wearing%20its%20hat%22%20without%20additional%20textual%20descriptions%0Aof%20its%20hat.%20We%20call%20this%20kind%20of%20generation%20personalized%20knowledge-driven%0Ageneration.%20To%20address%20the%20limitation%2C%20we%20present%20UniCTokens%2C%20a%20novel%20framework%0Athat%20effectively%20integrates%20personalized%20information%20into%20a%20unified%20vision%0Alanguage%20model%20%28VLM%29%20for%20understanding%20and%20generation.%20UniCTokens%20trains%20a%20set%0Aof%20unified%20concept%20tokens%20to%20leverage%20complementary%20semantics%2C%20boosting%20two%0Apersonalized%20tasks.%20Moreover%2C%20we%20propose%20a%20progressive%20training%20strategy%20with%0Athree%20stages%3A%20understanding%20warm-up%2C%20bootstrapping%20generation%20from%0Aunderstanding%2C%20and%20deepening%20understanding%20from%20generation%20to%20enhance%20mutual%0Abenefits%20between%20both%20tasks.%20To%20quantitatively%20evaluate%20the%20unified%20VLM%0Apersonalization%2C%20we%20present%20UnifyBench%2C%20the%20first%20benchmark%20for%20assessing%0Aconcept%20understanding%2C%20concept%20generation%2C%20and%20knowledge-driven%20generation.%0AExperimental%20results%20on%20UnifyBench%20indicate%20that%20UniCTokens%20shows%20competitive%0Aperformance%20compared%20to%20leading%20methods%20in%20concept%20understanding%2C%20concept%0Ageneration%2C%20and%20achieving%20state-of-the-art%20results%20in%20personalized%0Aknowledge-driven%20generation.%20Our%20research%20demonstrates%20that%20enhanced%0Aunderstanding%20improves%20generation%2C%20and%20the%20generation%20process%20can%20yield%0Avaluable%20insights%20into%20understanding.%20Our%20code%20and%20dataset%20will%20be%20released%20at%3A%0A%5Chref%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D%7Bhttps%3A//github.com/arctanxarc/UniCTokens%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14671v1&entry.124074799=Read"},
{"title": "ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for\n  Long-Tailed Megakaryocyte Classification", "author": "Linghao Zhuang and Ying Zhang and Gege Yuan and Xingyue Zhao and Zhiping Jiang", "abstract": "  Precise classification of megakaryocytes is crucial for diagnosing\nmyelodysplastic syndromes. Although self-supervised learning has shown promise\nin medical image analysis, its application to classifying megakaryocytes in\nstained slides faces three main challenges: (1) pervasive background noise that\nobscures cellular details, (2) a long-tailed distribution that limits data for\nrare subtypes, and (3) complex morphological variations leading to high\nintra-class variability. To address these issues, we propose the ActiveSSF\nframework, which integrates active learning with self-supervised pretraining.\nSpecifically, our approach employs Gaussian filtering combined with K-means\nclustering and HSV analysis (augmented by clinical prior knowledge) for\naccurate region-of-interest extraction; an adaptive sample selection mechanism\nthat dynamically adjusts similarity thresholds to mitigate class imbalance; and\nprototype clustering on labeled samples to overcome morphological complexity.\nExperimental results on clinical megakaryocyte datasets demonstrate that\nActiveSSF not only achieves state-of-the-art performance but also significantly\nimproves recognition accuracy for rare subtypes. Moreover, the integration of\nthese advanced techniques further underscores the practical potential of\nActiveSSF in clinical settings.\n", "link": "http://arxiv.org/abs/2502.08200v2", "date": "2025-05-20", "relevancy": 2.6346, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5394}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5341}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActiveSSF%3A%20An%20Active-Learning-Guided%20Self-Supervised%20Framework%20for%0A%20%20Long-Tailed%20Megakaryocyte%20Classification&body=Title%3A%20ActiveSSF%3A%20An%20Active-Learning-Guided%20Self-Supervised%20Framework%20for%0A%20%20Long-Tailed%20Megakaryocyte%20Classification%0AAuthor%3A%20Linghao%20Zhuang%20and%20Ying%20Zhang%20and%20Gege%20Yuan%20and%20Xingyue%20Zhao%20and%20Zhiping%20Jiang%0AAbstract%3A%20%20%20Precise%20classification%20of%20megakaryocytes%20is%20crucial%20for%20diagnosing%0Amyelodysplastic%20syndromes.%20Although%20self-supervised%20learning%20has%20shown%20promise%0Ain%20medical%20image%20analysis%2C%20its%20application%20to%20classifying%20megakaryocytes%20in%0Astained%20slides%20faces%20three%20main%20challenges%3A%20%281%29%20pervasive%20background%20noise%20that%0Aobscures%20cellular%20details%2C%20%282%29%20a%20long-tailed%20distribution%20that%20limits%20data%20for%0Arare%20subtypes%2C%20and%20%283%29%20complex%20morphological%20variations%20leading%20to%20high%0Aintra-class%20variability.%20To%20address%20these%20issues%2C%20we%20propose%20the%20ActiveSSF%0Aframework%2C%20which%20integrates%20active%20learning%20with%20self-supervised%20pretraining.%0ASpecifically%2C%20our%20approach%20employs%20Gaussian%20filtering%20combined%20with%20K-means%0Aclustering%20and%20HSV%20analysis%20%28augmented%20by%20clinical%20prior%20knowledge%29%20for%0Aaccurate%20region-of-interest%20extraction%3B%20an%20adaptive%20sample%20selection%20mechanism%0Athat%20dynamically%20adjusts%20similarity%20thresholds%20to%20mitigate%20class%20imbalance%3B%20and%0Aprototype%20clustering%20on%20labeled%20samples%20to%20overcome%20morphological%20complexity.%0AExperimental%20results%20on%20clinical%20megakaryocyte%20datasets%20demonstrate%20that%0AActiveSSF%20not%20only%20achieves%20state-of-the-art%20performance%20but%20also%20significantly%0Aimproves%20recognition%20accuracy%20for%20rare%20subtypes.%20Moreover%2C%20the%20integration%20of%0Athese%20advanced%20techniques%20further%20underscores%20the%20practical%20potential%20of%0AActiveSSF%20in%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActiveSSF%253A%2520An%2520Active-Learning-Guided%2520Self-Supervised%2520Framework%2520for%250A%2520%2520Long-Tailed%2520Megakaryocyte%2520Classification%26entry.906535625%3DLinghao%2520Zhuang%2520and%2520Ying%2520Zhang%2520and%2520Gege%2520Yuan%2520and%2520Xingyue%2520Zhao%2520and%2520Zhiping%2520Jiang%26entry.1292438233%3D%2520%2520Precise%2520classification%2520of%2520megakaryocytes%2520is%2520crucial%2520for%2520diagnosing%250Amyelodysplastic%2520syndromes.%2520Although%2520self-supervised%2520learning%2520has%2520shown%2520promise%250Ain%2520medical%2520image%2520analysis%252C%2520its%2520application%2520to%2520classifying%2520megakaryocytes%2520in%250Astained%2520slides%2520faces%2520three%2520main%2520challenges%253A%2520%25281%2529%2520pervasive%2520background%2520noise%2520that%250Aobscures%2520cellular%2520details%252C%2520%25282%2529%2520a%2520long-tailed%2520distribution%2520that%2520limits%2520data%2520for%250Arare%2520subtypes%252C%2520and%2520%25283%2529%2520complex%2520morphological%2520variations%2520leading%2520to%2520high%250Aintra-class%2520variability.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520the%2520ActiveSSF%250Aframework%252C%2520which%2520integrates%2520active%2520learning%2520with%2520self-supervised%2520pretraining.%250ASpecifically%252C%2520our%2520approach%2520employs%2520Gaussian%2520filtering%2520combined%2520with%2520K-means%250Aclustering%2520and%2520HSV%2520analysis%2520%2528augmented%2520by%2520clinical%2520prior%2520knowledge%2529%2520for%250Aaccurate%2520region-of-interest%2520extraction%253B%2520an%2520adaptive%2520sample%2520selection%2520mechanism%250Athat%2520dynamically%2520adjusts%2520similarity%2520thresholds%2520to%2520mitigate%2520class%2520imbalance%253B%2520and%250Aprototype%2520clustering%2520on%2520labeled%2520samples%2520to%2520overcome%2520morphological%2520complexity.%250AExperimental%2520results%2520on%2520clinical%2520megakaryocyte%2520datasets%2520demonstrate%2520that%250AActiveSSF%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520but%2520also%2520significantly%250Aimproves%2520recognition%2520accuracy%2520for%2520rare%2520subtypes.%2520Moreover%252C%2520the%2520integration%2520of%250Athese%2520advanced%2520techniques%2520further%2520underscores%2520the%2520practical%2520potential%2520of%250AActiveSSF%2520in%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActiveSSF%3A%20An%20Active-Learning-Guided%20Self-Supervised%20Framework%20for%0A%20%20Long-Tailed%20Megakaryocyte%20Classification&entry.906535625=Linghao%20Zhuang%20and%20Ying%20Zhang%20and%20Gege%20Yuan%20and%20Xingyue%20Zhao%20and%20Zhiping%20Jiang&entry.1292438233=%20%20Precise%20classification%20of%20megakaryocytes%20is%20crucial%20for%20diagnosing%0Amyelodysplastic%20syndromes.%20Although%20self-supervised%20learning%20has%20shown%20promise%0Ain%20medical%20image%20analysis%2C%20its%20application%20to%20classifying%20megakaryocytes%20in%0Astained%20slides%20faces%20three%20main%20challenges%3A%20%281%29%20pervasive%20background%20noise%20that%0Aobscures%20cellular%20details%2C%20%282%29%20a%20long-tailed%20distribution%20that%20limits%20data%20for%0Arare%20subtypes%2C%20and%20%283%29%20complex%20morphological%20variations%20leading%20to%20high%0Aintra-class%20variability.%20To%20address%20these%20issues%2C%20we%20propose%20the%20ActiveSSF%0Aframework%2C%20which%20integrates%20active%20learning%20with%20self-supervised%20pretraining.%0ASpecifically%2C%20our%20approach%20employs%20Gaussian%20filtering%20combined%20with%20K-means%0Aclustering%20and%20HSV%20analysis%20%28augmented%20by%20clinical%20prior%20knowledge%29%20for%0Aaccurate%20region-of-interest%20extraction%3B%20an%20adaptive%20sample%20selection%20mechanism%0Athat%20dynamically%20adjusts%20similarity%20thresholds%20to%20mitigate%20class%20imbalance%3B%20and%0Aprototype%20clustering%20on%20labeled%20samples%20to%20overcome%20morphological%20complexity.%0AExperimental%20results%20on%20clinical%20megakaryocyte%20datasets%20demonstrate%20that%0AActiveSSF%20not%20only%20achieves%20state-of-the-art%20performance%20but%20also%20significantly%0Aimproves%20recognition%20accuracy%20for%20rare%20subtypes.%20Moreover%2C%20the%20integration%20of%0Athese%20advanced%20techniques%20further%20underscores%20the%20practical%20potential%20of%0AActiveSSF%20in%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08200v2&entry.124074799=Read"},
{"title": "Customized SAM 2 for Referring Remote Sensing Image Segmentation", "author": "Fu Rong and Meng Lan and Qian Zhang and Lefei Zhang", "abstract": "  Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target\nobjects in remote sensing (RS) images based on textual descriptions. Although\nSegment Anything Model 2 (SAM 2) has shown remarkable performance in various\nsegmentation tasks, its application to RRSIS presents several challenges,\nincluding understanding the text-described RS scenes and generating effective\nprompts from text descriptions. To address these issues, we propose RS2-SAM 2,\na novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS\nfeatures and textual features, providing pseudo-mask-based dense prompts, and\nenforcing boundary constraints. Specifically, we first employ a union encoder\nto jointly encode the visual and textual inputs, generating aligned visual and\ntext embeddings as well as multimodal class tokens. Then, we design a\nbidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align\nadapted visual features with the visually enhanced text embeddings, improving\nthe model's interpretation of text-described RS scenes. Additionally, a mask\nprompt generator is introduced to take the visual embeddings and class tokens\nas input and produce a pseudo-mask as the dense prompt of SAM 2. To further\nrefine segmentation, we introduce a text-guided boundary loss to optimize\nsegmentation boundaries by computing text-weighted gradient differences.\nExperimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2\nachieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2503.07266v2", "date": "2025-05-20", "relevancy": 2.6302, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5356}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Customized%20SAM%202%20for%20Referring%20Remote%20Sensing%20Image%20Segmentation&body=Title%3A%20Customized%20SAM%202%20for%20Referring%20Remote%20Sensing%20Image%20Segmentation%0AAuthor%3A%20Fu%20Rong%20and%20Meng%20Lan%20and%20Qian%20Zhang%20and%20Lefei%20Zhang%0AAbstract%3A%20%20%20Referring%20Remote%20Sensing%20Image%20Segmentation%20%28RRSIS%29%20aims%20to%20segment%20target%0Aobjects%20in%20remote%20sensing%20%28RS%29%20images%20based%20on%20textual%20descriptions.%20Although%0ASegment%20Anything%20Model%202%20%28SAM%202%29%20has%20shown%20remarkable%20performance%20in%20various%0Asegmentation%20tasks%2C%20its%20application%20to%20RRSIS%20presents%20several%20challenges%2C%0Aincluding%20understanding%20the%20text-described%20RS%20scenes%20and%20generating%20effective%0Aprompts%20from%20text%20descriptions.%20To%20address%20these%20issues%2C%20we%20propose%20RS2-SAM%202%2C%0Aa%20novel%20framework%20that%20adapts%20SAM%202%20to%20RRSIS%20by%20aligning%20the%20adapted%20RS%0Afeatures%20and%20textual%20features%2C%20providing%20pseudo-mask-based%20dense%20prompts%2C%20and%0Aenforcing%20boundary%20constraints.%20Specifically%2C%20we%20first%20employ%20a%20union%20encoder%0Ato%20jointly%20encode%20the%20visual%20and%20textual%20inputs%2C%20generating%20aligned%20visual%20and%0Atext%20embeddings%20as%20well%20as%20multimodal%20class%20tokens.%20Then%2C%20we%20design%20a%0Abidirectional%20hierarchical%20fusion%20module%20to%20adapt%20SAM%202%20to%20RS%20scenes%20and%20align%0Aadapted%20visual%20features%20with%20the%20visually%20enhanced%20text%20embeddings%2C%20improving%0Athe%20model%27s%20interpretation%20of%20text-described%20RS%20scenes.%20Additionally%2C%20a%20mask%0Aprompt%20generator%20is%20introduced%20to%20take%20the%20visual%20embeddings%20and%20class%20tokens%0Aas%20input%20and%20produce%20a%20pseudo-mask%20as%20the%20dense%20prompt%20of%20SAM%202.%20To%20further%0Arefine%20segmentation%2C%20we%20introduce%20a%20text-guided%20boundary%20loss%20to%20optimize%0Asegmentation%20boundaries%20by%20computing%20text-weighted%20gradient%20differences.%0AExperimental%20results%20on%20several%20RRSIS%20benchmarks%20demonstrate%20that%20RS2-SAM%202%0Aachieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07266v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCustomized%2520SAM%25202%2520for%2520Referring%2520Remote%2520Sensing%2520Image%2520Segmentation%26entry.906535625%3DFu%2520Rong%2520and%2520Meng%2520Lan%2520and%2520Qian%2520Zhang%2520and%2520Lefei%2520Zhang%26entry.1292438233%3D%2520%2520Referring%2520Remote%2520Sensing%2520Image%2520Segmentation%2520%2528RRSIS%2529%2520aims%2520to%2520segment%2520target%250Aobjects%2520in%2520remote%2520sensing%2520%2528RS%2529%2520images%2520based%2520on%2520textual%2520descriptions.%2520Although%250ASegment%2520Anything%2520Model%25202%2520%2528SAM%25202%2529%2520has%2520shown%2520remarkable%2520performance%2520in%2520various%250Asegmentation%2520tasks%252C%2520its%2520application%2520to%2520RRSIS%2520presents%2520several%2520challenges%252C%250Aincluding%2520understanding%2520the%2520text-described%2520RS%2520scenes%2520and%2520generating%2520effective%250Aprompts%2520from%2520text%2520descriptions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520RS2-SAM%25202%252C%250Aa%2520novel%2520framework%2520that%2520adapts%2520SAM%25202%2520to%2520RRSIS%2520by%2520aligning%2520the%2520adapted%2520RS%250Afeatures%2520and%2520textual%2520features%252C%2520providing%2520pseudo-mask-based%2520dense%2520prompts%252C%2520and%250Aenforcing%2520boundary%2520constraints.%2520Specifically%252C%2520we%2520first%2520employ%2520a%2520union%2520encoder%250Ato%2520jointly%2520encode%2520the%2520visual%2520and%2520textual%2520inputs%252C%2520generating%2520aligned%2520visual%2520and%250Atext%2520embeddings%2520as%2520well%2520as%2520multimodal%2520class%2520tokens.%2520Then%252C%2520we%2520design%2520a%250Abidirectional%2520hierarchical%2520fusion%2520module%2520to%2520adapt%2520SAM%25202%2520to%2520RS%2520scenes%2520and%2520align%250Aadapted%2520visual%2520features%2520with%2520the%2520visually%2520enhanced%2520text%2520embeddings%252C%2520improving%250Athe%2520model%2527s%2520interpretation%2520of%2520text-described%2520RS%2520scenes.%2520Additionally%252C%2520a%2520mask%250Aprompt%2520generator%2520is%2520introduced%2520to%2520take%2520the%2520visual%2520embeddings%2520and%2520class%2520tokens%250Aas%2520input%2520and%2520produce%2520a%2520pseudo-mask%2520as%2520the%2520dense%2520prompt%2520of%2520SAM%25202.%2520To%2520further%250Arefine%2520segmentation%252C%2520we%2520introduce%2520a%2520text-guided%2520boundary%2520loss%2520to%2520optimize%250Asegmentation%2520boundaries%2520by%2520computing%2520text-weighted%2520gradient%2520differences.%250AExperimental%2520results%2520on%2520several%2520RRSIS%2520benchmarks%2520demonstrate%2520that%2520RS2-SAM%25202%250Aachieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07266v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customized%20SAM%202%20for%20Referring%20Remote%20Sensing%20Image%20Segmentation&entry.906535625=Fu%20Rong%20and%20Meng%20Lan%20and%20Qian%20Zhang%20and%20Lefei%20Zhang&entry.1292438233=%20%20Referring%20Remote%20Sensing%20Image%20Segmentation%20%28RRSIS%29%20aims%20to%20segment%20target%0Aobjects%20in%20remote%20sensing%20%28RS%29%20images%20based%20on%20textual%20descriptions.%20Although%0ASegment%20Anything%20Model%202%20%28SAM%202%29%20has%20shown%20remarkable%20performance%20in%20various%0Asegmentation%20tasks%2C%20its%20application%20to%20RRSIS%20presents%20several%20challenges%2C%0Aincluding%20understanding%20the%20text-described%20RS%20scenes%20and%20generating%20effective%0Aprompts%20from%20text%20descriptions.%20To%20address%20these%20issues%2C%20we%20propose%20RS2-SAM%202%2C%0Aa%20novel%20framework%20that%20adapts%20SAM%202%20to%20RRSIS%20by%20aligning%20the%20adapted%20RS%0Afeatures%20and%20textual%20features%2C%20providing%20pseudo-mask-based%20dense%20prompts%2C%20and%0Aenforcing%20boundary%20constraints.%20Specifically%2C%20we%20first%20employ%20a%20union%20encoder%0Ato%20jointly%20encode%20the%20visual%20and%20textual%20inputs%2C%20generating%20aligned%20visual%20and%0Atext%20embeddings%20as%20well%20as%20multimodal%20class%20tokens.%20Then%2C%20we%20design%20a%0Abidirectional%20hierarchical%20fusion%20module%20to%20adapt%20SAM%202%20to%20RS%20scenes%20and%20align%0Aadapted%20visual%20features%20with%20the%20visually%20enhanced%20text%20embeddings%2C%20improving%0Athe%20model%27s%20interpretation%20of%20text-described%20RS%20scenes.%20Additionally%2C%20a%20mask%0Aprompt%20generator%20is%20introduced%20to%20take%20the%20visual%20embeddings%20and%20class%20tokens%0Aas%20input%20and%20produce%20a%20pseudo-mask%20as%20the%20dense%20prompt%20of%20SAM%202.%20To%20further%0Arefine%20segmentation%2C%20we%20introduce%20a%20text-guided%20boundary%20loss%20to%20optimize%0Asegmentation%20boundaries%20by%20computing%20text-weighted%20gradient%20differences.%0AExperimental%20results%20on%20several%20RRSIS%20benchmarks%20demonstrate%20that%20RS2-SAM%202%0Aachieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07266v2&entry.124074799=Read"},
{"title": "Table Foundation Models: on knowledge pre-training for tabular learning", "author": "Myung Jun Kim and F\u00e9lix Lefebvre and Ga\u00ebtan Brison and Alexandre Perez-Lebel and Ga\u00ebl Varoquaux", "abstract": "  Table foundation models bring high hopes to data science: pre-trained on\ntabular data to embark knowledge or priors, they should facilitate downstream\ntasks on tables. One specific challenge is that of data semantics: numerical\nentries take their meaning from context, e.g., column name. Pre-trained neural\nnetworks that jointly model column names and table entries have recently\nboosted prediction accuracy. While these models outline the promises of world\nknowledge to interpret table values, they lack the convenience of popular\nfoundation models in text or vision. Indeed, they must be fine-tuned to bring\nbenefits, come with sizeable computation costs, and cannot easily be reused or\ncombined with other architectures. Here we introduce TARTE, a foundation model\nthat transforms tables to knowledge-enhanced vector representations using the\nstring to capture semantics. Pre-trained on large relational data, TARTE yields\nrepresentations that facilitate subsequent learning with little additional\ncost. These representations can be fine-tuned or combined with other learners,\ngiving models that push the state-of-the-art prediction performance and improve\nthe prediction/computation performance trade-off. Specialized to a task or a\ndomain, TARTE gives domain-specific representations that facilitate further\nlearning. Our study demonstrates an effective approach to knowledge\npre-training for tabular learning.\n", "link": "http://arxiv.org/abs/2505.14415v1", "date": "2025-05-20", "relevancy": 2.6294, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Table%20Foundation%20Models%3A%20on%20knowledge%20pre-training%20for%20tabular%20learning&body=Title%3A%20Table%20Foundation%20Models%3A%20on%20knowledge%20pre-training%20for%20tabular%20learning%0AAuthor%3A%20Myung%20Jun%20Kim%20and%20F%C3%A9lix%20Lefebvre%20and%20Ga%C3%ABtan%20Brison%20and%20Alexandre%20Perez-Lebel%20and%20Ga%C3%ABl%20Varoquaux%0AAbstract%3A%20%20%20Table%20foundation%20models%20bring%20high%20hopes%20to%20data%20science%3A%20pre-trained%20on%0Atabular%20data%20to%20embark%20knowledge%20or%20priors%2C%20they%20should%20facilitate%20downstream%0Atasks%20on%20tables.%20One%20specific%20challenge%20is%20that%20of%20data%20semantics%3A%20numerical%0Aentries%20take%20their%20meaning%20from%20context%2C%20e.g.%2C%20column%20name.%20Pre-trained%20neural%0Anetworks%20that%20jointly%20model%20column%20names%20and%20table%20entries%20have%20recently%0Aboosted%20prediction%20accuracy.%20While%20these%20models%20outline%20the%20promises%20of%20world%0Aknowledge%20to%20interpret%20table%20values%2C%20they%20lack%20the%20convenience%20of%20popular%0Afoundation%20models%20in%20text%20or%20vision.%20Indeed%2C%20they%20must%20be%20fine-tuned%20to%20bring%0Abenefits%2C%20come%20with%20sizeable%20computation%20costs%2C%20and%20cannot%20easily%20be%20reused%20or%0Acombined%20with%20other%20architectures.%20Here%20we%20introduce%20TARTE%2C%20a%20foundation%20model%0Athat%20transforms%20tables%20to%20knowledge-enhanced%20vector%20representations%20using%20the%0Astring%20to%20capture%20semantics.%20Pre-trained%20on%20large%20relational%20data%2C%20TARTE%20yields%0Arepresentations%20that%20facilitate%20subsequent%20learning%20with%20little%20additional%0Acost.%20These%20representations%20can%20be%20fine-tuned%20or%20combined%20with%20other%20learners%2C%0Agiving%20models%20that%20push%20the%20state-of-the-art%20prediction%20performance%20and%20improve%0Athe%20prediction/computation%20performance%20trade-off.%20Specialized%20to%20a%20task%20or%20a%0Adomain%2C%20TARTE%20gives%20domain-specific%20representations%20that%20facilitate%20further%0Alearning.%20Our%20study%20demonstrates%20an%20effective%20approach%20to%20knowledge%0Apre-training%20for%20tabular%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTable%2520Foundation%2520Models%253A%2520on%2520knowledge%2520pre-training%2520for%2520tabular%2520learning%26entry.906535625%3DMyung%2520Jun%2520Kim%2520and%2520F%25C3%25A9lix%2520Lefebvre%2520and%2520Ga%25C3%25ABtan%2520Brison%2520and%2520Alexandre%2520Perez-Lebel%2520and%2520Ga%25C3%25ABl%2520Varoquaux%26entry.1292438233%3D%2520%2520Table%2520foundation%2520models%2520bring%2520high%2520hopes%2520to%2520data%2520science%253A%2520pre-trained%2520on%250Atabular%2520data%2520to%2520embark%2520knowledge%2520or%2520priors%252C%2520they%2520should%2520facilitate%2520downstream%250Atasks%2520on%2520tables.%2520One%2520specific%2520challenge%2520is%2520that%2520of%2520data%2520semantics%253A%2520numerical%250Aentries%2520take%2520their%2520meaning%2520from%2520context%252C%2520e.g.%252C%2520column%2520name.%2520Pre-trained%2520neural%250Anetworks%2520that%2520jointly%2520model%2520column%2520names%2520and%2520table%2520entries%2520have%2520recently%250Aboosted%2520prediction%2520accuracy.%2520While%2520these%2520models%2520outline%2520the%2520promises%2520of%2520world%250Aknowledge%2520to%2520interpret%2520table%2520values%252C%2520they%2520lack%2520the%2520convenience%2520of%2520popular%250Afoundation%2520models%2520in%2520text%2520or%2520vision.%2520Indeed%252C%2520they%2520must%2520be%2520fine-tuned%2520to%2520bring%250Abenefits%252C%2520come%2520with%2520sizeable%2520computation%2520costs%252C%2520and%2520cannot%2520easily%2520be%2520reused%2520or%250Acombined%2520with%2520other%2520architectures.%2520Here%2520we%2520introduce%2520TARTE%252C%2520a%2520foundation%2520model%250Athat%2520transforms%2520tables%2520to%2520knowledge-enhanced%2520vector%2520representations%2520using%2520the%250Astring%2520to%2520capture%2520semantics.%2520Pre-trained%2520on%2520large%2520relational%2520data%252C%2520TARTE%2520yields%250Arepresentations%2520that%2520facilitate%2520subsequent%2520learning%2520with%2520little%2520additional%250Acost.%2520These%2520representations%2520can%2520be%2520fine-tuned%2520or%2520combined%2520with%2520other%2520learners%252C%250Agiving%2520models%2520that%2520push%2520the%2520state-of-the-art%2520prediction%2520performance%2520and%2520improve%250Athe%2520prediction/computation%2520performance%2520trade-off.%2520Specialized%2520to%2520a%2520task%2520or%2520a%250Adomain%252C%2520TARTE%2520gives%2520domain-specific%2520representations%2520that%2520facilitate%2520further%250Alearning.%2520Our%2520study%2520demonstrates%2520an%2520effective%2520approach%2520to%2520knowledge%250Apre-training%2520for%2520tabular%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Table%20Foundation%20Models%3A%20on%20knowledge%20pre-training%20for%20tabular%20learning&entry.906535625=Myung%20Jun%20Kim%20and%20F%C3%A9lix%20Lefebvre%20and%20Ga%C3%ABtan%20Brison%20and%20Alexandre%20Perez-Lebel%20and%20Ga%C3%ABl%20Varoquaux&entry.1292438233=%20%20Table%20foundation%20models%20bring%20high%20hopes%20to%20data%20science%3A%20pre-trained%20on%0Atabular%20data%20to%20embark%20knowledge%20or%20priors%2C%20they%20should%20facilitate%20downstream%0Atasks%20on%20tables.%20One%20specific%20challenge%20is%20that%20of%20data%20semantics%3A%20numerical%0Aentries%20take%20their%20meaning%20from%20context%2C%20e.g.%2C%20column%20name.%20Pre-trained%20neural%0Anetworks%20that%20jointly%20model%20column%20names%20and%20table%20entries%20have%20recently%0Aboosted%20prediction%20accuracy.%20While%20these%20models%20outline%20the%20promises%20of%20world%0Aknowledge%20to%20interpret%20table%20values%2C%20they%20lack%20the%20convenience%20of%20popular%0Afoundation%20models%20in%20text%20or%20vision.%20Indeed%2C%20they%20must%20be%20fine-tuned%20to%20bring%0Abenefits%2C%20come%20with%20sizeable%20computation%20costs%2C%20and%20cannot%20easily%20be%20reused%20or%0Acombined%20with%20other%20architectures.%20Here%20we%20introduce%20TARTE%2C%20a%20foundation%20model%0Athat%20transforms%20tables%20to%20knowledge-enhanced%20vector%20representations%20using%20the%0Astring%20to%20capture%20semantics.%20Pre-trained%20on%20large%20relational%20data%2C%20TARTE%20yields%0Arepresentations%20that%20facilitate%20subsequent%20learning%20with%20little%20additional%0Acost.%20These%20representations%20can%20be%20fine-tuned%20or%20combined%20with%20other%20learners%2C%0Agiving%20models%20that%20push%20the%20state-of-the-art%20prediction%20performance%20and%20improve%0Athe%20prediction/computation%20performance%20trade-off.%20Specialized%20to%20a%20task%20or%20a%0Adomain%2C%20TARTE%20gives%20domain-specific%20representations%20that%20facilitate%20further%0Alearning.%20Our%20study%20demonstrates%20an%20effective%20approach%20to%20knowledge%0Apre-training%20for%20tabular%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14415v1&entry.124074799=Read"},
{"title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric\n  Knowledge Transfer in Large Language Models", "author": "Yuqiao Tan and Shizhu He and Kang Liu and Jun Zhao", "abstract": "  Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.\n", "link": "http://arxiv.org/abs/2505.14436v1", "date": "2025-05-20", "relevancy": 2.6175, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5358}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5292}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Incompatibility%3A%20The%20Unbridgeable%20Gap%20of%20Cross-Scale%20Parametric%0A%20%20Knowledge%20Transfer%20in%20Large%20Language%20Models&body=Title%3A%20Neural%20Incompatibility%3A%20The%20Unbridgeable%20Gap%20of%20Cross-Scale%20Parametric%0A%20%20Knowledge%20Transfer%20in%20Large%20Language%20Models%0AAuthor%3A%20Yuqiao%20Tan%20and%20Shizhu%20He%20and%20Kang%20Liu%20and%20Jun%20Zhao%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20transparent%20brain%20with%20accessible%0Aparameters%20that%20encode%20extensive%20knowledge%2C%20which%20can%20be%20analyzed%2C%20located%20and%0Atransferred.%20Consequently%2C%20a%20key%20research%20challenge%20is%20to%20transcend%20traditional%0Aknowledge%20transfer%20paradigms%20rooted%20in%20symbolic%20language%20and%20achieve%20genuine%0AParametric%20Knowledge%20Transfer%20%28PKT%29.%20Significantly%2C%20exploring%20effective%20methods%0Afor%20transferring%20knowledge%20across%20LLMs%20of%20different%20scales%20through%20parameters%0Apresents%20an%20intriguing%20and%20valuable%20research%20direction.%20In%20this%20paper%2C%20we%20first%0Ademonstrate%20%24%5Ctextbf%7BAlignment%7D%24%20in%20parametric%20space%20is%20the%20fundamental%0Aprerequisite%20to%20achieve%20successful%20cross-scale%20PKT.%20We%20redefine%20the%20previously%0Aexplored%20knowledge%20transfer%20as%20Post-Align%20PKT%20%28PostPKT%29%2C%20which%20utilizes%0Aextracted%20parameters%20for%20LoRA%20initialization%20and%20requires%20subsequent%20fine-tune%0Afor%20alignment.%20Hence%2C%20to%20reduce%20cost%20for%20further%20fine-tuning%2C%20we%20introduce%20a%0Anovel%20Pre-Align%20PKT%20%28PrePKT%29%20paradigm%20and%20propose%20a%20solution%20called%0A%24%5Ctextbf%7BLaTen%7D%24%0A%28%24%5Ctextbf%7BL%7D%24oc%24%5Ctextbf%7Ba%7D%24te-%24%5Ctextbf%7BT%7D%24h%24%5Ctextbf%7Be%7D%24n-Alig%24%5Ctextbf%7Bn%7D%24%29%20that%0Aaligns%20the%20parametric%20spaces%20of%20LLMs%20across%20scales%20only%20using%20several%20training%0Asteps%20without%20following%20training.%20Comprehensive%20experiments%20on%20four%20benchmarks%0Ademonstrate%20that%20both%20PostPKT%20and%20PrePKT%20face%20challenges%20in%20achieving%0Aconsistently%20stable%20transfer.%20Through%20in-depth%20analysis%2C%20we%20identify%0A%24%5Ctextbf%7BNeural%20Incompatibility%7D%24%20as%20the%20ethological%20and%20parametric%20structural%0Adifferences%20between%20LLMs%20of%20varying%20scales%2C%20presenting%20fundamental%20challenges%0Ato%20achieving%20effective%20PKT.%20These%20findings%20provide%20fresh%20insights%20into%20the%0Aparametric%20architectures%20of%20LLMs%20and%20highlight%20promising%20directions%20for%20future%0Aresearch%20on%20efficient%20PKT.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Trae1ounG/Neural_Incompatibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Incompatibility%253A%2520The%2520Unbridgeable%2520Gap%2520of%2520Cross-Scale%2520Parametric%250A%2520%2520Knowledge%2520Transfer%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DYuqiao%2520Tan%2520and%2520Shizhu%2520He%2520and%2520Kang%2520Liu%2520and%2520Jun%2520Zhao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520a%2520transparent%2520brain%2520with%2520accessible%250Aparameters%2520that%2520encode%2520extensive%2520knowledge%252C%2520which%2520can%2520be%2520analyzed%252C%2520located%2520and%250Atransferred.%2520Consequently%252C%2520a%2520key%2520research%2520challenge%2520is%2520to%2520transcend%2520traditional%250Aknowledge%2520transfer%2520paradigms%2520rooted%2520in%2520symbolic%2520language%2520and%2520achieve%2520genuine%250AParametric%2520Knowledge%2520Transfer%2520%2528PKT%2529.%2520Significantly%252C%2520exploring%2520effective%2520methods%250Afor%2520transferring%2520knowledge%2520across%2520LLMs%2520of%2520different%2520scales%2520through%2520parameters%250Apresents%2520an%2520intriguing%2520and%2520valuable%2520research%2520direction.%2520In%2520this%2520paper%252C%2520we%2520first%250Ademonstrate%2520%2524%255Ctextbf%257BAlignment%257D%2524%2520in%2520parametric%2520space%2520is%2520the%2520fundamental%250Aprerequisite%2520to%2520achieve%2520successful%2520cross-scale%2520PKT.%2520We%2520redefine%2520the%2520previously%250Aexplored%2520knowledge%2520transfer%2520as%2520Post-Align%2520PKT%2520%2528PostPKT%2529%252C%2520which%2520utilizes%250Aextracted%2520parameters%2520for%2520LoRA%2520initialization%2520and%2520requires%2520subsequent%2520fine-tune%250Afor%2520alignment.%2520Hence%252C%2520to%2520reduce%2520cost%2520for%2520further%2520fine-tuning%252C%2520we%2520introduce%2520a%250Anovel%2520Pre-Align%2520PKT%2520%2528PrePKT%2529%2520paradigm%2520and%2520propose%2520a%2520solution%2520called%250A%2524%255Ctextbf%257BLaTen%257D%2524%250A%2528%2524%255Ctextbf%257BL%257D%2524oc%2524%255Ctextbf%257Ba%257D%2524te-%2524%255Ctextbf%257BT%257D%2524h%2524%255Ctextbf%257Be%257D%2524n-Alig%2524%255Ctextbf%257Bn%257D%2524%2529%2520that%250Aaligns%2520the%2520parametric%2520spaces%2520of%2520LLMs%2520across%2520scales%2520only%2520using%2520several%2520training%250Asteps%2520without%2520following%2520training.%2520Comprehensive%2520experiments%2520on%2520four%2520benchmarks%250Ademonstrate%2520that%2520both%2520PostPKT%2520and%2520PrePKT%2520face%2520challenges%2520in%2520achieving%250Aconsistently%2520stable%2520transfer.%2520Through%2520in-depth%2520analysis%252C%2520we%2520identify%250A%2524%255Ctextbf%257BNeural%2520Incompatibility%257D%2524%2520as%2520the%2520ethological%2520and%2520parametric%2520structural%250Adifferences%2520between%2520LLMs%2520of%2520varying%2520scales%252C%2520presenting%2520fundamental%2520challenges%250Ato%2520achieving%2520effective%2520PKT.%2520These%2520findings%2520provide%2520fresh%2520insights%2520into%2520the%250Aparametric%2520architectures%2520of%2520LLMs%2520and%2520highlight%2520promising%2520directions%2520for%2520future%250Aresearch%2520on%2520efficient%2520PKT.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Trae1ounG/Neural_Incompatibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Incompatibility%3A%20The%20Unbridgeable%20Gap%20of%20Cross-Scale%20Parametric%0A%20%20Knowledge%20Transfer%20in%20Large%20Language%20Models&entry.906535625=Yuqiao%20Tan%20and%20Shizhu%20He%20and%20Kang%20Liu%20and%20Jun%20Zhao&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20offer%20a%20transparent%20brain%20with%20accessible%0Aparameters%20that%20encode%20extensive%20knowledge%2C%20which%20can%20be%20analyzed%2C%20located%20and%0Atransferred.%20Consequently%2C%20a%20key%20research%20challenge%20is%20to%20transcend%20traditional%0Aknowledge%20transfer%20paradigms%20rooted%20in%20symbolic%20language%20and%20achieve%20genuine%0AParametric%20Knowledge%20Transfer%20%28PKT%29.%20Significantly%2C%20exploring%20effective%20methods%0Afor%20transferring%20knowledge%20across%20LLMs%20of%20different%20scales%20through%20parameters%0Apresents%20an%20intriguing%20and%20valuable%20research%20direction.%20In%20this%20paper%2C%20we%20first%0Ademonstrate%20%24%5Ctextbf%7BAlignment%7D%24%20in%20parametric%20space%20is%20the%20fundamental%0Aprerequisite%20to%20achieve%20successful%20cross-scale%20PKT.%20We%20redefine%20the%20previously%0Aexplored%20knowledge%20transfer%20as%20Post-Align%20PKT%20%28PostPKT%29%2C%20which%20utilizes%0Aextracted%20parameters%20for%20LoRA%20initialization%20and%20requires%20subsequent%20fine-tune%0Afor%20alignment.%20Hence%2C%20to%20reduce%20cost%20for%20further%20fine-tuning%2C%20we%20introduce%20a%0Anovel%20Pre-Align%20PKT%20%28PrePKT%29%20paradigm%20and%20propose%20a%20solution%20called%0A%24%5Ctextbf%7BLaTen%7D%24%0A%28%24%5Ctextbf%7BL%7D%24oc%24%5Ctextbf%7Ba%7D%24te-%24%5Ctextbf%7BT%7D%24h%24%5Ctextbf%7Be%7D%24n-Alig%24%5Ctextbf%7Bn%7D%24%29%20that%0Aaligns%20the%20parametric%20spaces%20of%20LLMs%20across%20scales%20only%20using%20several%20training%0Asteps%20without%20following%20training.%20Comprehensive%20experiments%20on%20four%20benchmarks%0Ademonstrate%20that%20both%20PostPKT%20and%20PrePKT%20face%20challenges%20in%20achieving%0Aconsistently%20stable%20transfer.%20Through%20in-depth%20analysis%2C%20we%20identify%0A%24%5Ctextbf%7BNeural%20Incompatibility%7D%24%20as%20the%20ethological%20and%20parametric%20structural%0Adifferences%20between%20LLMs%20of%20varying%20scales%2C%20presenting%20fundamental%20challenges%0Ato%20achieving%20effective%20PKT.%20These%20findings%20provide%20fresh%20insights%20into%20the%0Aparametric%20architectures%20of%20LLMs%20and%20highlight%20promising%20directions%20for%20future%0Aresearch%20on%20efficient%20PKT.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Trae1ounG/Neural_Incompatibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14436v1&entry.124074799=Read"},
{"title": "Time to Embed: Unlocking Foundation Models for Time Series with Channel\n  Descriptions", "author": "Utsav Dutta and Sina Khoshfetrat Pakazad and Henrik Ohlsson", "abstract": "  Traditional time series models are task-specific and often depend on\ndataset-specific training and extensive feature engineering. While\nTransformer-based architectures have improved scalability, foundation models,\ncommonplace in text, vision, and audio, remain under-explored for time series\nand are largely restricted to forecasting. We introduce $\\textbf{CHARM}$, a\nfoundation embedding model for multivariate time series that learns shared,\ntransferable, and domain-aware representations. To address the unique\ndifficulties of time series foundation learning, $\\textbf{CHARM}$ incorporates\narchitectural innovations that integrate channel-level textual descriptions\nwhile remaining invariant to channel order. The model is trained using a Joint\nEmbedding Predictive Architecture (JEPA), with novel augmentation schemes and a\nloss function designed to improve interpretability and training stability. Our\n$7$M-parameter model achieves state-of-the-art performance across diverse\ndownstream tasks, setting a new benchmark for time series representation\nlearning.\n", "link": "http://arxiv.org/abs/2505.14543v1", "date": "2025-05-20", "relevancy": 2.61, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time%20to%20Embed%3A%20Unlocking%20Foundation%20Models%20for%20Time%20Series%20with%20Channel%0A%20%20Descriptions&body=Title%3A%20Time%20to%20Embed%3A%20Unlocking%20Foundation%20Models%20for%20Time%20Series%20with%20Channel%0A%20%20Descriptions%0AAuthor%3A%20Utsav%20Dutta%20and%20Sina%20Khoshfetrat%20Pakazad%20and%20Henrik%20Ohlsson%0AAbstract%3A%20%20%20Traditional%20time%20series%20models%20are%20task-specific%20and%20often%20depend%20on%0Adataset-specific%20training%20and%20extensive%20feature%20engineering.%20While%0ATransformer-based%20architectures%20have%20improved%20scalability%2C%20foundation%20models%2C%0Acommonplace%20in%20text%2C%20vision%2C%20and%20audio%2C%20remain%20under-explored%20for%20time%20series%0Aand%20are%20largely%20restricted%20to%20forecasting.%20We%20introduce%20%24%5Ctextbf%7BCHARM%7D%24%2C%20a%0Afoundation%20embedding%20model%20for%20multivariate%20time%20series%20that%20learns%20shared%2C%0Atransferable%2C%20and%20domain-aware%20representations.%20To%20address%20the%20unique%0Adifficulties%20of%20time%20series%20foundation%20learning%2C%20%24%5Ctextbf%7BCHARM%7D%24%20incorporates%0Aarchitectural%20innovations%20that%20integrate%20channel-level%20textual%20descriptions%0Awhile%20remaining%20invariant%20to%20channel%20order.%20The%20model%20is%20trained%20using%20a%20Joint%0AEmbedding%20Predictive%20Architecture%20%28JEPA%29%2C%20with%20novel%20augmentation%20schemes%20and%20a%0Aloss%20function%20designed%20to%20improve%20interpretability%20and%20training%20stability.%20Our%0A%247%24M-parameter%20model%20achieves%20state-of-the-art%20performance%20across%20diverse%0Adownstream%20tasks%2C%20setting%20a%20new%20benchmark%20for%20time%20series%20representation%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime%2520to%2520Embed%253A%2520Unlocking%2520Foundation%2520Models%2520for%2520Time%2520Series%2520with%2520Channel%250A%2520%2520Descriptions%26entry.906535625%3DUtsav%2520Dutta%2520and%2520Sina%2520Khoshfetrat%2520Pakazad%2520and%2520Henrik%2520Ohlsson%26entry.1292438233%3D%2520%2520Traditional%2520time%2520series%2520models%2520are%2520task-specific%2520and%2520often%2520depend%2520on%250Adataset-specific%2520training%2520and%2520extensive%2520feature%2520engineering.%2520While%250ATransformer-based%2520architectures%2520have%2520improved%2520scalability%252C%2520foundation%2520models%252C%250Acommonplace%2520in%2520text%252C%2520vision%252C%2520and%2520audio%252C%2520remain%2520under-explored%2520for%2520time%2520series%250Aand%2520are%2520largely%2520restricted%2520to%2520forecasting.%2520We%2520introduce%2520%2524%255Ctextbf%257BCHARM%257D%2524%252C%2520a%250Afoundation%2520embedding%2520model%2520for%2520multivariate%2520time%2520series%2520that%2520learns%2520shared%252C%250Atransferable%252C%2520and%2520domain-aware%2520representations.%2520To%2520address%2520the%2520unique%250Adifficulties%2520of%2520time%2520series%2520foundation%2520learning%252C%2520%2524%255Ctextbf%257BCHARM%257D%2524%2520incorporates%250Aarchitectural%2520innovations%2520that%2520integrate%2520channel-level%2520textual%2520descriptions%250Awhile%2520remaining%2520invariant%2520to%2520channel%2520order.%2520The%2520model%2520is%2520trained%2520using%2520a%2520Joint%250AEmbedding%2520Predictive%2520Architecture%2520%2528JEPA%2529%252C%2520with%2520novel%2520augmentation%2520schemes%2520and%2520a%250Aloss%2520function%2520designed%2520to%2520improve%2520interpretability%2520and%2520training%2520stability.%2520Our%250A%25247%2524M-parameter%2520model%2520achieves%2520state-of-the-art%2520performance%2520across%2520diverse%250Adownstream%2520tasks%252C%2520setting%2520a%2520new%2520benchmark%2520for%2520time%2520series%2520representation%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20to%20Embed%3A%20Unlocking%20Foundation%20Models%20for%20Time%20Series%20with%20Channel%0A%20%20Descriptions&entry.906535625=Utsav%20Dutta%20and%20Sina%20Khoshfetrat%20Pakazad%20and%20Henrik%20Ohlsson&entry.1292438233=%20%20Traditional%20time%20series%20models%20are%20task-specific%20and%20often%20depend%20on%0Adataset-specific%20training%20and%20extensive%20feature%20engineering.%20While%0ATransformer-based%20architectures%20have%20improved%20scalability%2C%20foundation%20models%2C%0Acommonplace%20in%20text%2C%20vision%2C%20and%20audio%2C%20remain%20under-explored%20for%20time%20series%0Aand%20are%20largely%20restricted%20to%20forecasting.%20We%20introduce%20%24%5Ctextbf%7BCHARM%7D%24%2C%20a%0Afoundation%20embedding%20model%20for%20multivariate%20time%20series%20that%20learns%20shared%2C%0Atransferable%2C%20and%20domain-aware%20representations.%20To%20address%20the%20unique%0Adifficulties%20of%20time%20series%20foundation%20learning%2C%20%24%5Ctextbf%7BCHARM%7D%24%20incorporates%0Aarchitectural%20innovations%20that%20integrate%20channel-level%20textual%20descriptions%0Awhile%20remaining%20invariant%20to%20channel%20order.%20The%20model%20is%20trained%20using%20a%20Joint%0AEmbedding%20Predictive%20Architecture%20%28JEPA%29%2C%20with%20novel%20augmentation%20schemes%20and%20a%0Aloss%20function%20designed%20to%20improve%20interpretability%20and%20training%20stability.%20Our%0A%247%24M-parameter%20model%20achieves%20state-of-the-art%20performance%20across%20diverse%0Adownstream%20tasks%2C%20setting%20a%20new%20benchmark%20for%20time%20series%20representation%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14543v1&entry.124074799=Read"},
{"title": "Enhancing Interpretability of Sparse Latent Representations with Class\n  Information", "author": "Farshad Sangari Abiz and Reshad Hosseini and Babak N. Araabi", "abstract": "  Variational Autoencoders (VAEs) are powerful generative models for learning\nlatent representations. Standard VAEs generate dispersed and unstructured\nlatent spaces by utilizing all dimensions, which limits their interpretability,\nespecially in high-dimensional spaces. To address this challenge, Variational\nSparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting\nin sparse latent representations for each input. These sparse representations,\ncharacterized by a limited number of active dimensions, are inherently more\ninterpretable. Despite this advantage, VSC falls short in providing structured\ninterpretations across samples within the same class. Intuitively, samples from\nthe same class are expected to share similar attributes while allowing for\nvariations in those attributes. This expectation should manifest as consistent\npatterns of active dimensions in their latent representations, but VSC does not\nenforce such consistency.\n  In this paper, we propose a novel approach to enhance the latent space\ninterpretability by ensuring that the active dimensions in the latent space are\nconsistent across samples within the same class. To achieve this, we introduce\na new loss function that encourages samples from the same class to share\nsimilar active dimensions. This alignment creates a more structured and\ninterpretable latent space, where each shared dimension corresponds to a\nhigh-level concept, or \"factor.\" Unlike existing disentanglement-based methods\nthat primarily focus on global factors shared across all classes, our method\ncaptures both global and class-specific factors, thereby enhancing the utility\nand interpretability of latent representations.\n", "link": "http://arxiv.org/abs/2505.14476v1", "date": "2025-05-20", "relevancy": 2.597, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Interpretability%20of%20Sparse%20Latent%20Representations%20with%20Class%0A%20%20Information&body=Title%3A%20Enhancing%20Interpretability%20of%20Sparse%20Latent%20Representations%20with%20Class%0A%20%20Information%0AAuthor%3A%20Farshad%20Sangari%20Abiz%20and%20Reshad%20Hosseini%20and%20Babak%20N.%20Araabi%0AAbstract%3A%20%20%20Variational%20Autoencoders%20%28VAEs%29%20are%20powerful%20generative%20models%20for%20learning%0Alatent%20representations.%20Standard%20VAEs%20generate%20dispersed%20and%20unstructured%0Alatent%20spaces%20by%20utilizing%20all%20dimensions%2C%20which%20limits%20their%20interpretability%2C%0Aespecially%20in%20high-dimensional%20spaces.%20To%20address%20this%20challenge%2C%20Variational%0ASparse%20Coding%20%28VSC%29%20introduces%20a%20spike-and-slab%20prior%20distribution%2C%20resulting%0Ain%20sparse%20latent%20representations%20for%20each%20input.%20These%20sparse%20representations%2C%0Acharacterized%20by%20a%20limited%20number%20of%20active%20dimensions%2C%20are%20inherently%20more%0Ainterpretable.%20Despite%20this%20advantage%2C%20VSC%20falls%20short%20in%20providing%20structured%0Ainterpretations%20across%20samples%20within%20the%20same%20class.%20Intuitively%2C%20samples%20from%0Athe%20same%20class%20are%20expected%20to%20share%20similar%20attributes%20while%20allowing%20for%0Avariations%20in%20those%20attributes.%20This%20expectation%20should%20manifest%20as%20consistent%0Apatterns%20of%20active%20dimensions%20in%20their%20latent%20representations%2C%20but%20VSC%20does%20not%0Aenforce%20such%20consistency.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20enhance%20the%20latent%20space%0Ainterpretability%20by%20ensuring%20that%20the%20active%20dimensions%20in%20the%20latent%20space%20are%0Aconsistent%20across%20samples%20within%20the%20same%20class.%20To%20achieve%20this%2C%20we%20introduce%0Aa%20new%20loss%20function%20that%20encourages%20samples%20from%20the%20same%20class%20to%20share%0Asimilar%20active%20dimensions.%20This%20alignment%20creates%20a%20more%20structured%20and%0Ainterpretable%20latent%20space%2C%20where%20each%20shared%20dimension%20corresponds%20to%20a%0Ahigh-level%20concept%2C%20or%20%22factor.%22%20Unlike%20existing%20disentanglement-based%20methods%0Athat%20primarily%20focus%20on%20global%20factors%20shared%20across%20all%20classes%2C%20our%20method%0Acaptures%20both%20global%20and%20class-specific%20factors%2C%20thereby%20enhancing%20the%20utility%0Aand%20interpretability%20of%20latent%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Interpretability%2520of%2520Sparse%2520Latent%2520Representations%2520with%2520Class%250A%2520%2520Information%26entry.906535625%3DFarshad%2520Sangari%2520Abiz%2520and%2520Reshad%2520Hosseini%2520and%2520Babak%2520N.%2520Araabi%26entry.1292438233%3D%2520%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520are%2520powerful%2520generative%2520models%2520for%2520learning%250Alatent%2520representations.%2520Standard%2520VAEs%2520generate%2520dispersed%2520and%2520unstructured%250Alatent%2520spaces%2520by%2520utilizing%2520all%2520dimensions%252C%2520which%2520limits%2520their%2520interpretability%252C%250Aespecially%2520in%2520high-dimensional%2520spaces.%2520To%2520address%2520this%2520challenge%252C%2520Variational%250ASparse%2520Coding%2520%2528VSC%2529%2520introduces%2520a%2520spike-and-slab%2520prior%2520distribution%252C%2520resulting%250Ain%2520sparse%2520latent%2520representations%2520for%2520each%2520input.%2520These%2520sparse%2520representations%252C%250Acharacterized%2520by%2520a%2520limited%2520number%2520of%2520active%2520dimensions%252C%2520are%2520inherently%2520more%250Ainterpretable.%2520Despite%2520this%2520advantage%252C%2520VSC%2520falls%2520short%2520in%2520providing%2520structured%250Ainterpretations%2520across%2520samples%2520within%2520the%2520same%2520class.%2520Intuitively%252C%2520samples%2520from%250Athe%2520same%2520class%2520are%2520expected%2520to%2520share%2520similar%2520attributes%2520while%2520allowing%2520for%250Avariations%2520in%2520those%2520attributes.%2520This%2520expectation%2520should%2520manifest%2520as%2520consistent%250Apatterns%2520of%2520active%2520dimensions%2520in%2520their%2520latent%2520representations%252C%2520but%2520VSC%2520does%2520not%250Aenforce%2520such%2520consistency.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520enhance%2520the%2520latent%2520space%250Ainterpretability%2520by%2520ensuring%2520that%2520the%2520active%2520dimensions%2520in%2520the%2520latent%2520space%2520are%250Aconsistent%2520across%2520samples%2520within%2520the%2520same%2520class.%2520To%2520achieve%2520this%252C%2520we%2520introduce%250Aa%2520new%2520loss%2520function%2520that%2520encourages%2520samples%2520from%2520the%2520same%2520class%2520to%2520share%250Asimilar%2520active%2520dimensions.%2520This%2520alignment%2520creates%2520a%2520more%2520structured%2520and%250Ainterpretable%2520latent%2520space%252C%2520where%2520each%2520shared%2520dimension%2520corresponds%2520to%2520a%250Ahigh-level%2520concept%252C%2520or%2520%2522factor.%2522%2520Unlike%2520existing%2520disentanglement-based%2520methods%250Athat%2520primarily%2520focus%2520on%2520global%2520factors%2520shared%2520across%2520all%2520classes%252C%2520our%2520method%250Acaptures%2520both%2520global%2520and%2520class-specific%2520factors%252C%2520thereby%2520enhancing%2520the%2520utility%250Aand%2520interpretability%2520of%2520latent%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Interpretability%20of%20Sparse%20Latent%20Representations%20with%20Class%0A%20%20Information&entry.906535625=Farshad%20Sangari%20Abiz%20and%20Reshad%20Hosseini%20and%20Babak%20N.%20Araabi&entry.1292438233=%20%20Variational%20Autoencoders%20%28VAEs%29%20are%20powerful%20generative%20models%20for%20learning%0Alatent%20representations.%20Standard%20VAEs%20generate%20dispersed%20and%20unstructured%0Alatent%20spaces%20by%20utilizing%20all%20dimensions%2C%20which%20limits%20their%20interpretability%2C%0Aespecially%20in%20high-dimensional%20spaces.%20To%20address%20this%20challenge%2C%20Variational%0ASparse%20Coding%20%28VSC%29%20introduces%20a%20spike-and-slab%20prior%20distribution%2C%20resulting%0Ain%20sparse%20latent%20representations%20for%20each%20input.%20These%20sparse%20representations%2C%0Acharacterized%20by%20a%20limited%20number%20of%20active%20dimensions%2C%20are%20inherently%20more%0Ainterpretable.%20Despite%20this%20advantage%2C%20VSC%20falls%20short%20in%20providing%20structured%0Ainterpretations%20across%20samples%20within%20the%20same%20class.%20Intuitively%2C%20samples%20from%0Athe%20same%20class%20are%20expected%20to%20share%20similar%20attributes%20while%20allowing%20for%0Avariations%20in%20those%20attributes.%20This%20expectation%20should%20manifest%20as%20consistent%0Apatterns%20of%20active%20dimensions%20in%20their%20latent%20representations%2C%20but%20VSC%20does%20not%0Aenforce%20such%20consistency.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20enhance%20the%20latent%20space%0Ainterpretability%20by%20ensuring%20that%20the%20active%20dimensions%20in%20the%20latent%20space%20are%0Aconsistent%20across%20samples%20within%20the%20same%20class.%20To%20achieve%20this%2C%20we%20introduce%0Aa%20new%20loss%20function%20that%20encourages%20samples%20from%20the%20same%20class%20to%20share%0Asimilar%20active%20dimensions.%20This%20alignment%20creates%20a%20more%20structured%20and%0Ainterpretable%20latent%20space%2C%20where%20each%20shared%20dimension%20corresponds%20to%20a%0Ahigh-level%20concept%2C%20or%20%22factor.%22%20Unlike%20existing%20disentanglement-based%20methods%0Athat%20primarily%20focus%20on%20global%20factors%20shared%20across%20all%20classes%2C%20our%20method%0Acaptures%20both%20global%20and%20class-specific%20factors%2C%20thereby%20enhancing%20the%20utility%0Aand%20interpretability%20of%20latent%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14476v1&entry.124074799=Read"},
{"title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and\n  Interpretability Prediction", "author": "Mengying Yuan and Wenhao Wang and Zixuan Wang and Yujie Huang and Kangli Wei and Fei Li and Chong Teng and Donghong Ji", "abstract": "  Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer\nreview.\n", "link": "http://arxiv.org/abs/2504.12324v2", "date": "2025-05-20", "relevancy": 2.5924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5257}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Document%20Cross-Lingual%20NLI%20via%20RST-Enhanced%20Graph%20Fusion%20and%0A%20%20Interpretability%20Prediction&body=Title%3A%20Cross-Document%20Cross-Lingual%20NLI%20via%20RST-Enhanced%20Graph%20Fusion%20and%0A%20%20Interpretability%20Prediction%0AAuthor%3A%20Mengying%20Yuan%20and%20Wenhao%20Wang%20and%20Zixuan%20Wang%20and%20Yujie%20Huang%20and%20Kangli%20Wei%20and%20Fei%20Li%20and%20Chong%20Teng%20and%20Donghong%20Ji%0AAbstract%3A%20%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20fundamental%20task%20in%20natural%20language%0Aprocessing.%20While%20NLI%20has%20developed%20many%20sub-directions%20such%20as%20sentence-level%0ANLI%2C%20document-level%20NLI%20and%20cross-lingual%20NLI%2C%20Cross-Document%20Cross-Lingual%20NLI%0A%28CDCL-NLI%29%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aparadigm%3A%20CDCL-NLI%2C%20which%20extends%20traditional%20NLI%20capabilities%20to%0Amulti-document%2C%20multilingual%20scenarios.%20To%20support%20this%20task%2C%20we%20construct%20a%0Ahigh-quality%20CDCL-NLI%20dataset%20including%2025%2C410%20instances%20and%20spanning%2026%0Alanguages.%20To%20address%20the%20limitations%20of%20previous%20methods%20on%20CDCL-NLI%20task%2C%20we%0Afurther%20propose%20an%20innovative%20method%20that%20integrates%20RST-enhanced%20graph%20fusion%0Awith%20interpretability-aware%20prediction.%20Our%20approach%20leverages%20RST%20%28Rhetorical%0AStructure%20Theory%29%20within%20heterogeneous%20graph%20neural%20networks%20for%20cross-document%0Acontext%20modeling%2C%20and%20employs%20a%20structure-aware%20semantic%20alignment%20based%20on%0Alexical%20chains%20for%20cross-lingual%20understanding.%20For%20NLI%20interpretability%2C%20we%0Adevelop%20an%20EDU%20%28Elementary%20Discourse%20Unit%29-level%20attribution%20framework%20that%0Aproduces%20extractive%20explanations.%20Extensive%20experiments%20demonstrate%20our%0Aapproach%27s%20superior%20performance%2C%20achieving%20significant%20improvements%20over%20both%0Aconventional%20NLI%20models%20as%20well%20as%20large%20language%20models.%20Our%20work%20sheds%20light%0Aon%20the%20study%20of%20NLI%20and%20will%20bring%20research%20interest%20on%20cross-document%0Across-lingual%20context%20understanding%2C%20hallucination%20elimination%20and%0Ainterpretability%20inference.%20Our%20code%20and%20datasets%20are%20available%20at%0A%5Chref%7Bhttps%3A//anonymous.4open.science/r/CDCL-NLI-637E/%7D%7BCDCL-NLI-link%7D%20for%20peer%0Areview.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12324v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Document%2520Cross-Lingual%2520NLI%2520via%2520RST-Enhanced%2520Graph%2520Fusion%2520and%250A%2520%2520Interpretability%2520Prediction%26entry.906535625%3DMengying%2520Yuan%2520and%2520Wenhao%2520Wang%2520and%2520Zixuan%2520Wang%2520and%2520Yujie%2520Huang%2520and%2520Kangli%2520Wei%2520and%2520Fei%2520Li%2520and%2520Chong%2520Teng%2520and%2520Donghong%2520Ji%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Inference%2520%2528NLI%2529%2520is%2520a%2520fundamental%2520task%2520in%2520natural%2520language%250Aprocessing.%2520While%2520NLI%2520has%2520developed%2520many%2520sub-directions%2520such%2520as%2520sentence-level%250ANLI%252C%2520document-level%2520NLI%2520and%2520cross-lingual%2520NLI%252C%2520Cross-Document%2520Cross-Lingual%2520NLI%250A%2528CDCL-NLI%2529%2520remains%2520largely%2520unexplored.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aparadigm%253A%2520CDCL-NLI%252C%2520which%2520extends%2520traditional%2520NLI%2520capabilities%2520to%250Amulti-document%252C%2520multilingual%2520scenarios.%2520To%2520support%2520this%2520task%252C%2520we%2520construct%2520a%250Ahigh-quality%2520CDCL-NLI%2520dataset%2520including%252025%252C410%2520instances%2520and%2520spanning%252026%250Alanguages.%2520To%2520address%2520the%2520limitations%2520of%2520previous%2520methods%2520on%2520CDCL-NLI%2520task%252C%2520we%250Afurther%2520propose%2520an%2520innovative%2520method%2520that%2520integrates%2520RST-enhanced%2520graph%2520fusion%250Awith%2520interpretability-aware%2520prediction.%2520Our%2520approach%2520leverages%2520RST%2520%2528Rhetorical%250AStructure%2520Theory%2529%2520within%2520heterogeneous%2520graph%2520neural%2520networks%2520for%2520cross-document%250Acontext%2520modeling%252C%2520and%2520employs%2520a%2520structure-aware%2520semantic%2520alignment%2520based%2520on%250Alexical%2520chains%2520for%2520cross-lingual%2520understanding.%2520For%2520NLI%2520interpretability%252C%2520we%250Adevelop%2520an%2520EDU%2520%2528Elementary%2520Discourse%2520Unit%2529-level%2520attribution%2520framework%2520that%250Aproduces%2520extractive%2520explanations.%2520Extensive%2520experiments%2520demonstrate%2520our%250Aapproach%2527s%2520superior%2520performance%252C%2520achieving%2520significant%2520improvements%2520over%2520both%250Aconventional%2520NLI%2520models%2520as%2520well%2520as%2520large%2520language%2520models.%2520Our%2520work%2520sheds%2520light%250Aon%2520the%2520study%2520of%2520NLI%2520and%2520will%2520bring%2520research%2520interest%2520on%2520cross-document%250Across-lingual%2520context%2520understanding%252C%2520hallucination%2520elimination%2520and%250Ainterpretability%2520inference.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//anonymous.4open.science/r/CDCL-NLI-637E/%257D%257BCDCL-NLI-link%257D%2520for%2520peer%250Areview.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12324v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Document%20Cross-Lingual%20NLI%20via%20RST-Enhanced%20Graph%20Fusion%20and%0A%20%20Interpretability%20Prediction&entry.906535625=Mengying%20Yuan%20and%20Wenhao%20Wang%20and%20Zixuan%20Wang%20and%20Yujie%20Huang%20and%20Kangli%20Wei%20and%20Fei%20Li%20and%20Chong%20Teng%20and%20Donghong%20Ji&entry.1292438233=%20%20Natural%20Language%20Inference%20%28NLI%29%20is%20a%20fundamental%20task%20in%20natural%20language%0Aprocessing.%20While%20NLI%20has%20developed%20many%20sub-directions%20such%20as%20sentence-level%0ANLI%2C%20document-level%20NLI%20and%20cross-lingual%20NLI%2C%20Cross-Document%20Cross-Lingual%20NLI%0A%28CDCL-NLI%29%20remains%20largely%20unexplored.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aparadigm%3A%20CDCL-NLI%2C%20which%20extends%20traditional%20NLI%20capabilities%20to%0Amulti-document%2C%20multilingual%20scenarios.%20To%20support%20this%20task%2C%20we%20construct%20a%0Ahigh-quality%20CDCL-NLI%20dataset%20including%2025%2C410%20instances%20and%20spanning%2026%0Alanguages.%20To%20address%20the%20limitations%20of%20previous%20methods%20on%20CDCL-NLI%20task%2C%20we%0Afurther%20propose%20an%20innovative%20method%20that%20integrates%20RST-enhanced%20graph%20fusion%0Awith%20interpretability-aware%20prediction.%20Our%20approach%20leverages%20RST%20%28Rhetorical%0AStructure%20Theory%29%20within%20heterogeneous%20graph%20neural%20networks%20for%20cross-document%0Acontext%20modeling%2C%20and%20employs%20a%20structure-aware%20semantic%20alignment%20based%20on%0Alexical%20chains%20for%20cross-lingual%20understanding.%20For%20NLI%20interpretability%2C%20we%0Adevelop%20an%20EDU%20%28Elementary%20Discourse%20Unit%29-level%20attribution%20framework%20that%0Aproduces%20extractive%20explanations.%20Extensive%20experiments%20demonstrate%20our%0Aapproach%27s%20superior%20performance%2C%20achieving%20significant%20improvements%20over%20both%0Aconventional%20NLI%20models%20as%20well%20as%20large%20language%20models.%20Our%20work%20sheds%20light%0Aon%20the%20study%20of%20NLI%20and%20will%20bring%20research%20interest%20on%20cross-document%0Across-lingual%20context%20understanding%2C%20hallucination%20elimination%20and%0Ainterpretability%20inference.%20Our%20code%20and%20datasets%20are%20available%20at%0A%5Chref%7Bhttps%3A//anonymous.4open.science/r/CDCL-NLI-637E/%7D%7BCDCL-NLI-link%7D%20for%20peer%0Areview.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12324v2&entry.124074799=Read"},
{"title": "Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean\n  Frameworks", "author": "Menglin Yang and Yifei Zhang and Jialin Chen and Melanie Weber and Rex Ying", "abstract": "  In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace is the de facto geometric setting of our machine learning architectures.\nHowever, recent literature has demonstrated that this choice comes with\nfundamental limitations. To that end, non-Euclidean learning is quickly gaining\ntraction, particularly in web-related applications where complex relationships\nand structures are prevalent. Non-Euclidean spaces, such as hyperbolic,\nspherical, and mixed-curvature spaces, have been shown to provide more\nefficient and effective representations for data with intrinsic geometric\nproperties, including web-related data like social network topology,\nquery-document relationships, and user-item interactions. Integrating\nfoundation models with non-Euclidean geometries has great potential to enhance\ntheir ability to capture and model the underlying structures, leading to better\nperformance in search, recommendations, and content understanding. This\nworkshop focuses on the intersection of Non-Euclidean Foundation Models and\nGeometric Learning (NEGEL), exploring its potential benefits, including the\npotential benefits for advancing web-related technologies, challenges, and\nfuture directions. Workshop page:\n[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)\n", "link": "http://arxiv.org/abs/2505.14417v1", "date": "2025-05-20", "relevancy": 2.5883, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5322}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Non-Euclidean%20Foundation%20Models%3A%20Advancing%20AI%20Beyond%20Euclidean%0A%20%20Frameworks&body=Title%3A%20Towards%20Non-Euclidean%20Foundation%20Models%3A%20Advancing%20AI%20Beyond%20Euclidean%0A%20%20Frameworks%0AAuthor%3A%20Menglin%20Yang%20and%20Yifei%20Zhang%20and%20Jialin%20Chen%20and%20Melanie%20Weber%20and%20Rex%20Ying%0AAbstract%3A%20%20%20In%20the%20era%20of%20foundation%20models%20and%20Large%20Language%20Models%20%28LLMs%29%2C%20Euclidean%0Aspace%20is%20the%20de%20facto%20geometric%20setting%20of%20our%20machine%20learning%20architectures.%0AHowever%2C%20recent%20literature%20has%20demonstrated%20that%20this%20choice%20comes%20with%0Afundamental%20limitations.%20To%20that%20end%2C%20non-Euclidean%20learning%20is%20quickly%20gaining%0Atraction%2C%20particularly%20in%20web-related%20applications%20where%20complex%20relationships%0Aand%20structures%20are%20prevalent.%20Non-Euclidean%20spaces%2C%20such%20as%20hyperbolic%2C%0Aspherical%2C%20and%20mixed-curvature%20spaces%2C%20have%20been%20shown%20to%20provide%20more%0Aefficient%20and%20effective%20representations%20for%20data%20with%20intrinsic%20geometric%0Aproperties%2C%20including%20web-related%20data%20like%20social%20network%20topology%2C%0Aquery-document%20relationships%2C%20and%20user-item%20interactions.%20Integrating%0Afoundation%20models%20with%20non-Euclidean%20geometries%20has%20great%20potential%20to%20enhance%0Atheir%20ability%20to%20capture%20and%20model%20the%20underlying%20structures%2C%20leading%20to%20better%0Aperformance%20in%20search%2C%20recommendations%2C%20and%20content%20understanding.%20This%0Aworkshop%20focuses%20on%20the%20intersection%20of%20Non-Euclidean%20Foundation%20Models%20and%0AGeometric%20Learning%20%28NEGEL%29%2C%20exploring%20its%20potential%20benefits%2C%20including%20the%0Apotential%20benefits%20for%20advancing%20web-related%20technologies%2C%20challenges%2C%20and%0Afuture%20directions.%20Workshop%20page%3A%0A%5Bhttps%3A//hyperboliclearning.github.io/events/www2025workshop%5D%28https%3A//hyperboliclearning.github.io/events/www2025workshop%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Non-Euclidean%2520Foundation%2520Models%253A%2520Advancing%2520AI%2520Beyond%2520Euclidean%250A%2520%2520Frameworks%26entry.906535625%3DMenglin%2520Yang%2520and%2520Yifei%2520Zhang%2520and%2520Jialin%2520Chen%2520and%2520Melanie%2520Weber%2520and%2520Rex%2520Ying%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520foundation%2520models%2520and%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Euclidean%250Aspace%2520is%2520the%2520de%2520facto%2520geometric%2520setting%2520of%2520our%2520machine%2520learning%2520architectures.%250AHowever%252C%2520recent%2520literature%2520has%2520demonstrated%2520that%2520this%2520choice%2520comes%2520with%250Afundamental%2520limitations.%2520To%2520that%2520end%252C%2520non-Euclidean%2520learning%2520is%2520quickly%2520gaining%250Atraction%252C%2520particularly%2520in%2520web-related%2520applications%2520where%2520complex%2520relationships%250Aand%2520structures%2520are%2520prevalent.%2520Non-Euclidean%2520spaces%252C%2520such%2520as%2520hyperbolic%252C%250Aspherical%252C%2520and%2520mixed-curvature%2520spaces%252C%2520have%2520been%2520shown%2520to%2520provide%2520more%250Aefficient%2520and%2520effective%2520representations%2520for%2520data%2520with%2520intrinsic%2520geometric%250Aproperties%252C%2520including%2520web-related%2520data%2520like%2520social%2520network%2520topology%252C%250Aquery-document%2520relationships%252C%2520and%2520user-item%2520interactions.%2520Integrating%250Afoundation%2520models%2520with%2520non-Euclidean%2520geometries%2520has%2520great%2520potential%2520to%2520enhance%250Atheir%2520ability%2520to%2520capture%2520and%2520model%2520the%2520underlying%2520structures%252C%2520leading%2520to%2520better%250Aperformance%2520in%2520search%252C%2520recommendations%252C%2520and%2520content%2520understanding.%2520This%250Aworkshop%2520focuses%2520on%2520the%2520intersection%2520of%2520Non-Euclidean%2520Foundation%2520Models%2520and%250AGeometric%2520Learning%2520%2528NEGEL%2529%252C%2520exploring%2520its%2520potential%2520benefits%252C%2520including%2520the%250Apotential%2520benefits%2520for%2520advancing%2520web-related%2520technologies%252C%2520challenges%252C%2520and%250Afuture%2520directions.%2520Workshop%2520page%253A%250A%255Bhttps%253A//hyperboliclearning.github.io/events/www2025workshop%255D%2528https%253A//hyperboliclearning.github.io/events/www2025workshop%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Non-Euclidean%20Foundation%20Models%3A%20Advancing%20AI%20Beyond%20Euclidean%0A%20%20Frameworks&entry.906535625=Menglin%20Yang%20and%20Yifei%20Zhang%20and%20Jialin%20Chen%20and%20Melanie%20Weber%20and%20Rex%20Ying&entry.1292438233=%20%20In%20the%20era%20of%20foundation%20models%20and%20Large%20Language%20Models%20%28LLMs%29%2C%20Euclidean%0Aspace%20is%20the%20de%20facto%20geometric%20setting%20of%20our%20machine%20learning%20architectures.%0AHowever%2C%20recent%20literature%20has%20demonstrated%20that%20this%20choice%20comes%20with%0Afundamental%20limitations.%20To%20that%20end%2C%20non-Euclidean%20learning%20is%20quickly%20gaining%0Atraction%2C%20particularly%20in%20web-related%20applications%20where%20complex%20relationships%0Aand%20structures%20are%20prevalent.%20Non-Euclidean%20spaces%2C%20such%20as%20hyperbolic%2C%0Aspherical%2C%20and%20mixed-curvature%20spaces%2C%20have%20been%20shown%20to%20provide%20more%0Aefficient%20and%20effective%20representations%20for%20data%20with%20intrinsic%20geometric%0Aproperties%2C%20including%20web-related%20data%20like%20social%20network%20topology%2C%0Aquery-document%20relationships%2C%20and%20user-item%20interactions.%20Integrating%0Afoundation%20models%20with%20non-Euclidean%20geometries%20has%20great%20potential%20to%20enhance%0Atheir%20ability%20to%20capture%20and%20model%20the%20underlying%20structures%2C%20leading%20to%20better%0Aperformance%20in%20search%2C%20recommendations%2C%20and%20content%20understanding.%20This%0Aworkshop%20focuses%20on%20the%20intersection%20of%20Non-Euclidean%20Foundation%20Models%20and%0AGeometric%20Learning%20%28NEGEL%29%2C%20exploring%20its%20potential%20benefits%2C%20including%20the%0Apotential%20benefits%20for%20advancing%20web-related%20technologies%2C%20challenges%2C%20and%0Afuture%20directions.%20Workshop%20page%3A%0A%5Bhttps%3A//hyperboliclearning.github.io/events/www2025workshop%5D%28https%3A//hyperboliclearning.github.io/events/www2025workshop%29%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14417v1&entry.124074799=Read"},
{"title": "Reasoning Models Better Express Their Confidence", "author": "Dongkeun Yoon and Seungone Kim and Sohee Yang and Sunkyoung Kim and Soyeon Kim and Yongil Kim and Eunbi Choi and Yireun Kim and Minjoon Seo", "abstract": "  Despite their strengths, large language models (LLMs) often fail to\ncommunicate their confidence accurately, making it difficult to assess when\nthey might be wrong and limiting their reliability. In this work, we\ndemonstrate that reasoning models-LLMs that engage in extended chain-of-thought\n(CoT) reasoning-exhibit superior performance not only in problem-solving but\nalso in accurately expressing their confidence. Specifically, we benchmark six\nreasoning models across six datasets and find that they achieve strictly better\nconfidence calibration than their non-reasoning counterparts in 33 out of the\n36 settings. Our detailed analysis reveals that these gains in calibration stem\nfrom the slow thinking behaviors of reasoning models-such as exploring\nalternative approaches and backtracking-which enable them to adjust their\nconfidence dynamically throughout their CoT, making it progressively more\naccurate. In particular, we find that reasoning models become increasingly\nbetter calibrated as their CoT unfolds, a trend not observed in non-reasoning\nmodels. Moreover, removing slow thinking behaviors from the CoT leads to a\nsignificant drop in calibration. Lastly, we show that these gains are not\nexclusive to reasoning models-non-reasoning models also benefit when guided to\nperform slow thinking via in-context learning.\n", "link": "http://arxiv.org/abs/2505.14489v1", "date": "2025-05-20", "relevancy": 2.5783, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5268}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Models%20Better%20Express%20Their%20Confidence&body=Title%3A%20Reasoning%20Models%20Better%20Express%20Their%20Confidence%0AAuthor%3A%20Dongkeun%20Yoon%20and%20Seungone%20Kim%20and%20Sohee%20Yang%20and%20Sunkyoung%20Kim%20and%20Soyeon%20Kim%20and%20Yongil%20Kim%20and%20Eunbi%20Choi%20and%20Yireun%20Kim%20and%20Minjoon%20Seo%0AAbstract%3A%20%20%20Despite%20their%20strengths%2C%20large%20language%20models%20%28LLMs%29%20often%20fail%20to%0Acommunicate%20their%20confidence%20accurately%2C%20making%20it%20difficult%20to%20assess%20when%0Athey%20might%20be%20wrong%20and%20limiting%20their%20reliability.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20reasoning%20models-LLMs%20that%20engage%20in%20extended%20chain-of-thought%0A%28CoT%29%20reasoning-exhibit%20superior%20performance%20not%20only%20in%20problem-solving%20but%0Aalso%20in%20accurately%20expressing%20their%20confidence.%20Specifically%2C%20we%20benchmark%20six%0Areasoning%20models%20across%20six%20datasets%20and%20find%20that%20they%20achieve%20strictly%20better%0Aconfidence%20calibration%20than%20their%20non-reasoning%20counterparts%20in%2033%20out%20of%20the%0A36%20settings.%20Our%20detailed%20analysis%20reveals%20that%20these%20gains%20in%20calibration%20stem%0Afrom%20the%20slow%20thinking%20behaviors%20of%20reasoning%20models-such%20as%20exploring%0Aalternative%20approaches%20and%20backtracking-which%20enable%20them%20to%20adjust%20their%0Aconfidence%20dynamically%20throughout%20their%20CoT%2C%20making%20it%20progressively%20more%0Aaccurate.%20In%20particular%2C%20we%20find%20that%20reasoning%20models%20become%20increasingly%0Abetter%20calibrated%20as%20their%20CoT%20unfolds%2C%20a%20trend%20not%20observed%20in%20non-reasoning%0Amodels.%20Moreover%2C%20removing%20slow%20thinking%20behaviors%20from%20the%20CoT%20leads%20to%20a%0Asignificant%20drop%20in%20calibration.%20Lastly%2C%20we%20show%20that%20these%20gains%20are%20not%0Aexclusive%20to%20reasoning%20models-non-reasoning%20models%20also%20benefit%20when%20guided%20to%0Aperform%20slow%20thinking%20via%20in-context%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Models%2520Better%2520Express%2520Their%2520Confidence%26entry.906535625%3DDongkeun%2520Yoon%2520and%2520Seungone%2520Kim%2520and%2520Sohee%2520Yang%2520and%2520Sunkyoung%2520Kim%2520and%2520Soyeon%2520Kim%2520and%2520Yongil%2520Kim%2520and%2520Eunbi%2520Choi%2520and%2520Yireun%2520Kim%2520and%2520Minjoon%2520Seo%26entry.1292438233%3D%2520%2520Despite%2520their%2520strengths%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520fail%2520to%250Acommunicate%2520their%2520confidence%2520accurately%252C%2520making%2520it%2520difficult%2520to%2520assess%2520when%250Athey%2520might%2520be%2520wrong%2520and%2520limiting%2520their%2520reliability.%2520In%2520this%2520work%252C%2520we%250Ademonstrate%2520that%2520reasoning%2520models-LLMs%2520that%2520engage%2520in%2520extended%2520chain-of-thought%250A%2528CoT%2529%2520reasoning-exhibit%2520superior%2520performance%2520not%2520only%2520in%2520problem-solving%2520but%250Aalso%2520in%2520accurately%2520expressing%2520their%2520confidence.%2520Specifically%252C%2520we%2520benchmark%2520six%250Areasoning%2520models%2520across%2520six%2520datasets%2520and%2520find%2520that%2520they%2520achieve%2520strictly%2520better%250Aconfidence%2520calibration%2520than%2520their%2520non-reasoning%2520counterparts%2520in%252033%2520out%2520of%2520the%250A36%2520settings.%2520Our%2520detailed%2520analysis%2520reveals%2520that%2520these%2520gains%2520in%2520calibration%2520stem%250Afrom%2520the%2520slow%2520thinking%2520behaviors%2520of%2520reasoning%2520models-such%2520as%2520exploring%250Aalternative%2520approaches%2520and%2520backtracking-which%2520enable%2520them%2520to%2520adjust%2520their%250Aconfidence%2520dynamically%2520throughout%2520their%2520CoT%252C%2520making%2520it%2520progressively%2520more%250Aaccurate.%2520In%2520particular%252C%2520we%2520find%2520that%2520reasoning%2520models%2520become%2520increasingly%250Abetter%2520calibrated%2520as%2520their%2520CoT%2520unfolds%252C%2520a%2520trend%2520not%2520observed%2520in%2520non-reasoning%250Amodels.%2520Moreover%252C%2520removing%2520slow%2520thinking%2520behaviors%2520from%2520the%2520CoT%2520leads%2520to%2520a%250Asignificant%2520drop%2520in%2520calibration.%2520Lastly%252C%2520we%2520show%2520that%2520these%2520gains%2520are%2520not%250Aexclusive%2520to%2520reasoning%2520models-non-reasoning%2520models%2520also%2520benefit%2520when%2520guided%2520to%250Aperform%2520slow%2520thinking%2520via%2520in-context%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Models%20Better%20Express%20Their%20Confidence&entry.906535625=Dongkeun%20Yoon%20and%20Seungone%20Kim%20and%20Sohee%20Yang%20and%20Sunkyoung%20Kim%20and%20Soyeon%20Kim%20and%20Yongil%20Kim%20and%20Eunbi%20Choi%20and%20Yireun%20Kim%20and%20Minjoon%20Seo&entry.1292438233=%20%20Despite%20their%20strengths%2C%20large%20language%20models%20%28LLMs%29%20often%20fail%20to%0Acommunicate%20their%20confidence%20accurately%2C%20making%20it%20difficult%20to%20assess%20when%0Athey%20might%20be%20wrong%20and%20limiting%20their%20reliability.%20In%20this%20work%2C%20we%0Ademonstrate%20that%20reasoning%20models-LLMs%20that%20engage%20in%20extended%20chain-of-thought%0A%28CoT%29%20reasoning-exhibit%20superior%20performance%20not%20only%20in%20problem-solving%20but%0Aalso%20in%20accurately%20expressing%20their%20confidence.%20Specifically%2C%20we%20benchmark%20six%0Areasoning%20models%20across%20six%20datasets%20and%20find%20that%20they%20achieve%20strictly%20better%0Aconfidence%20calibration%20than%20their%20non-reasoning%20counterparts%20in%2033%20out%20of%20the%0A36%20settings.%20Our%20detailed%20analysis%20reveals%20that%20these%20gains%20in%20calibration%20stem%0Afrom%20the%20slow%20thinking%20behaviors%20of%20reasoning%20models-such%20as%20exploring%0Aalternative%20approaches%20and%20backtracking-which%20enable%20them%20to%20adjust%20their%0Aconfidence%20dynamically%20throughout%20their%20CoT%2C%20making%20it%20progressively%20more%0Aaccurate.%20In%20particular%2C%20we%20find%20that%20reasoning%20models%20become%20increasingly%0Abetter%20calibrated%20as%20their%20CoT%20unfolds%2C%20a%20trend%20not%20observed%20in%20non-reasoning%0Amodels.%20Moreover%2C%20removing%20slow%20thinking%20behaviors%20from%20the%20CoT%20leads%20to%20a%0Asignificant%20drop%20in%20calibration.%20Lastly%2C%20we%20show%20that%20these%20gains%20are%20not%0Aexclusive%20to%20reasoning%20models-non-reasoning%20models%20also%20benefit%20when%20guided%20to%0Aperform%20slow%20thinking%20via%20in-context%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14489v1&entry.124074799=Read"},
{"title": "On the Generalizability of Foundation Models for Crop Type Mapping", "author": "Yi-Chia Chang and Adam J. Stewart and Favyen Bastani and Piper Wolters and Shreya Kannan and George R. Huber and Jingtong Wang and Arindam Banerjee", "abstract": "  Foundation models pre-trained using self-supervised learning have shown\npowerful transfer learning capabilities on various downstream tasks, including\nlanguage understanding, text generation, and image recognition. The Earth\nobservation (EO) field has produced several foundation models pre-trained\ndirectly on multispectral satellite imagery for applications like precision\nagriculture, wildfire and drought monitoring, and natural disaster response.\nHowever, few studies have investigated the ability of these models to\ngeneralize to new geographic locations, and potential concerns of geospatial\nbias -- models trained on data-rich developed nations not transferring well to\ndata-scarce developing nations -- remain. We evaluate three popular EO\nfoundation models, SSL4EO-S12, SatlasPretrain, and ImageNet, on five crop\nclassification datasets across five continents. Results show that pre-trained\nweights designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform\ngeneral pre-trained weights like ImageNet. While only 100 labeled images are\nsufficient for achieving high overall accuracy, 900 images are required to\nmitigate class imbalance and improve average accuracy.\n", "link": "http://arxiv.org/abs/2409.09451v4", "date": "2025-05-20", "relevancy": 2.576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Generalizability%20of%20Foundation%20Models%20for%20Crop%20Type%20Mapping&body=Title%3A%20On%20the%20Generalizability%20of%20Foundation%20Models%20for%20Crop%20Type%20Mapping%0AAuthor%3A%20Yi-Chia%20Chang%20and%20Adam%20J.%20Stewart%20and%20Favyen%20Bastani%20and%20Piper%20Wolters%20and%20Shreya%20Kannan%20and%20George%20R.%20Huber%20and%20Jingtong%20Wang%20and%20Arindam%20Banerjee%0AAbstract%3A%20%20%20Foundation%20models%20pre-trained%20using%20self-supervised%20learning%20have%20shown%0Apowerful%20transfer%20learning%20capabilities%20on%20various%20downstream%20tasks%2C%20including%0Alanguage%20understanding%2C%20text%20generation%2C%20and%20image%20recognition.%20The%20Earth%0Aobservation%20%28EO%29%20field%20has%20produced%20several%20foundation%20models%20pre-trained%0Adirectly%20on%20multispectral%20satellite%20imagery%20for%20applications%20like%20precision%0Aagriculture%2C%20wildfire%20and%20drought%20monitoring%2C%20and%20natural%20disaster%20response.%0AHowever%2C%20few%20studies%20have%20investigated%20the%20ability%20of%20these%20models%20to%0Ageneralize%20to%20new%20geographic%20locations%2C%20and%20potential%20concerns%20of%20geospatial%0Abias%20--%20models%20trained%20on%20data-rich%20developed%20nations%20not%20transferring%20well%20to%0Adata-scarce%20developing%20nations%20--%20remain.%20We%20evaluate%20three%20popular%20EO%0Afoundation%20models%2C%20SSL4EO-S12%2C%20SatlasPretrain%2C%20and%20ImageNet%2C%20on%20five%20crop%0Aclassification%20datasets%20across%20five%20continents.%20Results%20show%20that%20pre-trained%0Aweights%20designed%20explicitly%20for%20Sentinel-2%2C%20such%20as%20SSL4EO-S12%2C%20outperform%0Ageneral%20pre-trained%20weights%20like%20ImageNet.%20While%20only%20100%20labeled%20images%20are%0Asufficient%20for%20achieving%20high%20overall%20accuracy%2C%20900%20images%20are%20required%20to%0Amitigate%20class%20imbalance%20and%20improve%20average%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09451v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Generalizability%2520of%2520Foundation%2520Models%2520for%2520Crop%2520Type%2520Mapping%26entry.906535625%3DYi-Chia%2520Chang%2520and%2520Adam%2520J.%2520Stewart%2520and%2520Favyen%2520Bastani%2520and%2520Piper%2520Wolters%2520and%2520Shreya%2520Kannan%2520and%2520George%2520R.%2520Huber%2520and%2520Jingtong%2520Wang%2520and%2520Arindam%2520Banerjee%26entry.1292438233%3D%2520%2520Foundation%2520models%2520pre-trained%2520using%2520self-supervised%2520learning%2520have%2520shown%250Apowerful%2520transfer%2520learning%2520capabilities%2520on%2520various%2520downstream%2520tasks%252C%2520including%250Alanguage%2520understanding%252C%2520text%2520generation%252C%2520and%2520image%2520recognition.%2520The%2520Earth%250Aobservation%2520%2528EO%2529%2520field%2520has%2520produced%2520several%2520foundation%2520models%2520pre-trained%250Adirectly%2520on%2520multispectral%2520satellite%2520imagery%2520for%2520applications%2520like%2520precision%250Aagriculture%252C%2520wildfire%2520and%2520drought%2520monitoring%252C%2520and%2520natural%2520disaster%2520response.%250AHowever%252C%2520few%2520studies%2520have%2520investigated%2520the%2520ability%2520of%2520these%2520models%2520to%250Ageneralize%2520to%2520new%2520geographic%2520locations%252C%2520and%2520potential%2520concerns%2520of%2520geospatial%250Abias%2520--%2520models%2520trained%2520on%2520data-rich%2520developed%2520nations%2520not%2520transferring%2520well%2520to%250Adata-scarce%2520developing%2520nations%2520--%2520remain.%2520We%2520evaluate%2520three%2520popular%2520EO%250Afoundation%2520models%252C%2520SSL4EO-S12%252C%2520SatlasPretrain%252C%2520and%2520ImageNet%252C%2520on%2520five%2520crop%250Aclassification%2520datasets%2520across%2520five%2520continents.%2520Results%2520show%2520that%2520pre-trained%250Aweights%2520designed%2520explicitly%2520for%2520Sentinel-2%252C%2520such%2520as%2520SSL4EO-S12%252C%2520outperform%250Ageneral%2520pre-trained%2520weights%2520like%2520ImageNet.%2520While%2520only%2520100%2520labeled%2520images%2520are%250Asufficient%2520for%2520achieving%2520high%2520overall%2520accuracy%252C%2520900%2520images%2520are%2520required%2520to%250Amitigate%2520class%2520imbalance%2520and%2520improve%2520average%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09451v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Generalizability%20of%20Foundation%20Models%20for%20Crop%20Type%20Mapping&entry.906535625=Yi-Chia%20Chang%20and%20Adam%20J.%20Stewart%20and%20Favyen%20Bastani%20and%20Piper%20Wolters%20and%20Shreya%20Kannan%20and%20George%20R.%20Huber%20and%20Jingtong%20Wang%20and%20Arindam%20Banerjee&entry.1292438233=%20%20Foundation%20models%20pre-trained%20using%20self-supervised%20learning%20have%20shown%0Apowerful%20transfer%20learning%20capabilities%20on%20various%20downstream%20tasks%2C%20including%0Alanguage%20understanding%2C%20text%20generation%2C%20and%20image%20recognition.%20The%20Earth%0Aobservation%20%28EO%29%20field%20has%20produced%20several%20foundation%20models%20pre-trained%0Adirectly%20on%20multispectral%20satellite%20imagery%20for%20applications%20like%20precision%0Aagriculture%2C%20wildfire%20and%20drought%20monitoring%2C%20and%20natural%20disaster%20response.%0AHowever%2C%20few%20studies%20have%20investigated%20the%20ability%20of%20these%20models%20to%0Ageneralize%20to%20new%20geographic%20locations%2C%20and%20potential%20concerns%20of%20geospatial%0Abias%20--%20models%20trained%20on%20data-rich%20developed%20nations%20not%20transferring%20well%20to%0Adata-scarce%20developing%20nations%20--%20remain.%20We%20evaluate%20three%20popular%20EO%0Afoundation%20models%2C%20SSL4EO-S12%2C%20SatlasPretrain%2C%20and%20ImageNet%2C%20on%20five%20crop%0Aclassification%20datasets%20across%20five%20continents.%20Results%20show%20that%20pre-trained%0Aweights%20designed%20explicitly%20for%20Sentinel-2%2C%20such%20as%20SSL4EO-S12%2C%20outperform%0Ageneral%20pre-trained%20weights%20like%20ImageNet.%20While%20only%20100%20labeled%20images%20are%0Asufficient%20for%20achieving%20high%20overall%20accuracy%2C%20900%20images%20are%20required%20to%0Amitigate%20class%20imbalance%20and%20improve%20average%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09451v4&entry.124074799=Read"},
{"title": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\n  Image Classification", "author": "Wenchen Chen and Yanmei Zhang and Zhongwei Xiao and Jianping Chu and Xingbo Wang", "abstract": "  Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification.\n", "link": "http://arxiv.org/abs/2505.12482v2", "date": "2025-05-20", "relevancy": 2.5721, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5448}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5019}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4965}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral-Spatial%20Self-Supervised%20Learning%20for%20Few-Shot%20Hyperspectral%0A%20%20Image%20Classification&body=Title%3A%20Spectral-Spatial%20Self-Supervised%20Learning%20for%20Few-Shot%20Hyperspectral%0A%20%20Image%20Classification%0AAuthor%3A%20Wenchen%20Chen%20and%20Yanmei%20Zhang%20and%20Zhongwei%20Xiao%20and%20Jianping%20Chu%20and%20Xingbo%20Wang%0AAbstract%3A%20%20%20Few-shot%20classification%20of%20hyperspectral%20images%20%28HSI%29%20faces%20the%20challenge%20of%0Ascarce%20labeled%20samples.%20Self-Supervised%20learning%20%28SSL%29%20and%20Few-Shot%20Learning%0A%28FSL%29%20offer%20promising%20avenues%20to%20address%20this%20issue.%20However%2C%20existing%20methods%0Aoften%20struggle%20to%20adapt%20to%20the%20spatial%20geometric%20diversity%20of%20HSIs%20and%20lack%0Asufficient%20spectral%20prior%20knowledge.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%0Amethod%2C%20Spectral-Spatial%20Self-Supervised%20Learning%20for%20Few-Shot%20Hyperspectral%0AImage%20Classification%20%28S4L-FSC%29%2C%20aimed%20at%20improving%20the%20performance%20of%20few-shot%0AHSI%20classification.%20Specifically%2C%20we%20first%20leverage%20heterogeneous%20datasets%20to%0Apretrain%20a%20spatial%20feature%20extractor%20using%20a%20designed%20Rotation-Mirror%0ASelf-Supervised%20Learning%20%28RM-SSL%29%20method%2C%20combined%20with%20FSL.%20This%20approach%0Aenables%20the%20model%20to%20learn%20the%20spatial%20geometric%20diversity%20of%20HSIs%20using%0Arotation%20and%20mirroring%20labels%20as%20supervisory%20signals%2C%20while%20acquiring%0Atransferable%20spatial%20meta-knowledge%20through%20few-shot%20learning.%20Subsequently%2C%0Ahomogeneous%20datasets%20are%20utilized%20to%20pretrain%20a%20spectral%20feature%20extractor%20via%0Aa%20combination%20of%20FSL%20and%20Masked%20Reconstruction%20Self-Supervised%20Learning%0A%28MR-SSL%29.%20The%20model%20learns%20to%20reconstruct%20original%20spectral%20information%20from%0Arandomly%20masked%20spectral%20vectors%2C%20inferring%20spectral%20dependencies.%20In%20parallel%2C%0AFSL%20guides%20the%20model%20to%20extract%20pixel-level%20discriminative%20features%2C%20thereby%0Aembedding%20rich%20spectral%20priors%20into%20the%20model.%20This%20spectral-spatial%0Apretraining%20method%2C%20along%20with%20the%20integration%20of%20knowledge%20from%20heterogeneous%0Aand%20homogeneous%20sources%2C%20significantly%20enhances%20model%20performance.%20Extensive%0Aexperiments%20on%20four%20HSI%20datasets%20demonstrate%20the%20effectiveness%20and%20superiority%0Aof%20the%20proposed%20S4L-FSC%20approach%20for%20few-shot%20HSI%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12482v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral-Spatial%2520Self-Supervised%2520Learning%2520for%2520Few-Shot%2520Hyperspectral%250A%2520%2520Image%2520Classification%26entry.906535625%3DWenchen%2520Chen%2520and%2520Yanmei%2520Zhang%2520and%2520Zhongwei%2520Xiao%2520and%2520Jianping%2520Chu%2520and%2520Xingbo%2520Wang%26entry.1292438233%3D%2520%2520Few-shot%2520classification%2520of%2520hyperspectral%2520images%2520%2528HSI%2529%2520faces%2520the%2520challenge%2520of%250Ascarce%2520labeled%2520samples.%2520Self-Supervised%2520learning%2520%2528SSL%2529%2520and%2520Few-Shot%2520Learning%250A%2528FSL%2529%2520offer%2520promising%2520avenues%2520to%2520address%2520this%2520issue.%2520However%252C%2520existing%2520methods%250Aoften%2520struggle%2520to%2520adapt%2520to%2520the%2520spatial%2520geometric%2520diversity%2520of%2520HSIs%2520and%2520lack%250Asufficient%2520spectral%2520prior%2520knowledge.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520a%250Amethod%252C%2520Spectral-Spatial%2520Self-Supervised%2520Learning%2520for%2520Few-Shot%2520Hyperspectral%250AImage%2520Classification%2520%2528S4L-FSC%2529%252C%2520aimed%2520at%2520improving%2520the%2520performance%2520of%2520few-shot%250AHSI%2520classification.%2520Specifically%252C%2520we%2520first%2520leverage%2520heterogeneous%2520datasets%2520to%250Apretrain%2520a%2520spatial%2520feature%2520extractor%2520using%2520a%2520designed%2520Rotation-Mirror%250ASelf-Supervised%2520Learning%2520%2528RM-SSL%2529%2520method%252C%2520combined%2520with%2520FSL.%2520This%2520approach%250Aenables%2520the%2520model%2520to%2520learn%2520the%2520spatial%2520geometric%2520diversity%2520of%2520HSIs%2520using%250Arotation%2520and%2520mirroring%2520labels%2520as%2520supervisory%2520signals%252C%2520while%2520acquiring%250Atransferable%2520spatial%2520meta-knowledge%2520through%2520few-shot%2520learning.%2520Subsequently%252C%250Ahomogeneous%2520datasets%2520are%2520utilized%2520to%2520pretrain%2520a%2520spectral%2520feature%2520extractor%2520via%250Aa%2520combination%2520of%2520FSL%2520and%2520Masked%2520Reconstruction%2520Self-Supervised%2520Learning%250A%2528MR-SSL%2529.%2520The%2520model%2520learns%2520to%2520reconstruct%2520original%2520spectral%2520information%2520from%250Arandomly%2520masked%2520spectral%2520vectors%252C%2520inferring%2520spectral%2520dependencies.%2520In%2520parallel%252C%250AFSL%2520guides%2520the%2520model%2520to%2520extract%2520pixel-level%2520discriminative%2520features%252C%2520thereby%250Aembedding%2520rich%2520spectral%2520priors%2520into%2520the%2520model.%2520This%2520spectral-spatial%250Apretraining%2520method%252C%2520along%2520with%2520the%2520integration%2520of%2520knowledge%2520from%2520heterogeneous%250Aand%2520homogeneous%2520sources%252C%2520significantly%2520enhances%2520model%2520performance.%2520Extensive%250Aexperiments%2520on%2520four%2520HSI%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520superiority%250Aof%2520the%2520proposed%2520S4L-FSC%2520approach%2520for%2520few-shot%2520HSI%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12482v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral-Spatial%20Self-Supervised%20Learning%20for%20Few-Shot%20Hyperspectral%0A%20%20Image%20Classification&entry.906535625=Wenchen%20Chen%20and%20Yanmei%20Zhang%20and%20Zhongwei%20Xiao%20and%20Jianping%20Chu%20and%20Xingbo%20Wang&entry.1292438233=%20%20Few-shot%20classification%20of%20hyperspectral%20images%20%28HSI%29%20faces%20the%20challenge%20of%0Ascarce%20labeled%20samples.%20Self-Supervised%20learning%20%28SSL%29%20and%20Few-Shot%20Learning%0A%28FSL%29%20offer%20promising%20avenues%20to%20address%20this%20issue.%20However%2C%20existing%20methods%0Aoften%20struggle%20to%20adapt%20to%20the%20spatial%20geometric%20diversity%20of%20HSIs%20and%20lack%0Asufficient%20spectral%20prior%20knowledge.%20To%20tackle%20these%20challenges%2C%20we%20propose%20a%0Amethod%2C%20Spectral-Spatial%20Self-Supervised%20Learning%20for%20Few-Shot%20Hyperspectral%0AImage%20Classification%20%28S4L-FSC%29%2C%20aimed%20at%20improving%20the%20performance%20of%20few-shot%0AHSI%20classification.%20Specifically%2C%20we%20first%20leverage%20heterogeneous%20datasets%20to%0Apretrain%20a%20spatial%20feature%20extractor%20using%20a%20designed%20Rotation-Mirror%0ASelf-Supervised%20Learning%20%28RM-SSL%29%20method%2C%20combined%20with%20FSL.%20This%20approach%0Aenables%20the%20model%20to%20learn%20the%20spatial%20geometric%20diversity%20of%20HSIs%20using%0Arotation%20and%20mirroring%20labels%20as%20supervisory%20signals%2C%20while%20acquiring%0Atransferable%20spatial%20meta-knowledge%20through%20few-shot%20learning.%20Subsequently%2C%0Ahomogeneous%20datasets%20are%20utilized%20to%20pretrain%20a%20spectral%20feature%20extractor%20via%0Aa%20combination%20of%20FSL%20and%20Masked%20Reconstruction%20Self-Supervised%20Learning%0A%28MR-SSL%29.%20The%20model%20learns%20to%20reconstruct%20original%20spectral%20information%20from%0Arandomly%20masked%20spectral%20vectors%2C%20inferring%20spectral%20dependencies.%20In%20parallel%2C%0AFSL%20guides%20the%20model%20to%20extract%20pixel-level%20discriminative%20features%2C%20thereby%0Aembedding%20rich%20spectral%20priors%20into%20the%20model.%20This%20spectral-spatial%0Apretraining%20method%2C%20along%20with%20the%20integration%20of%20knowledge%20from%20heterogeneous%0Aand%20homogeneous%20sources%2C%20significantly%20enhances%20model%20performance.%20Extensive%0Aexperiments%20on%20four%20HSI%20datasets%20demonstrate%20the%20effectiveness%20and%20superiority%0Aof%20the%20proposed%20S4L-FSC%20approach%20for%20few-shot%20HSI%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12482v2&entry.124074799=Read"},
{"title": "Exploring Graph Representations of Logical Forms for Language Modeling", "author": "Michael Sullivan", "abstract": "  We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs pretrained on similar amounts of data, indicating that LFLMs\ncan learn with substantially less data than models over plain text.\nFurthermore, we show that the performance of this model is likely to scale with\nadditional parameters and pretraining data, suggesting the viability of LFLMs\nin real-world applications.\n", "link": "http://arxiv.org/abs/2505.14523v1", "date": "2025-05-20", "relevancy": 2.5643, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5201}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Graph%20Representations%20of%20Logical%20Forms%20for%20Language%20Modeling&body=Title%3A%20Exploring%20Graph%20Representations%20of%20Logical%20Forms%20for%20Language%20Modeling%0AAuthor%3A%20Michael%20Sullivan%0AAbstract%3A%20%20%20We%20make%20the%20case%20for%20language%20models%20over%20logical%20forms%20%28LFLMs%29%2C%20arguing%20that%0Asuch%20models%20are%20more%20data-efficient%20than%20their%20textual%20counterparts.%20To%20that%0Aend%2C%20we%20introduce%20the%20Graph-based%20Formal-Logical%20Distributional%20Semantics%0A%28GFoLDS%29%20prototype%2C%20a%20pretrained%20LM%20over%20graph%20representations%20of%20logical%0Aforms%2C%20as%20a%20proof-of-concept%20of%20LFLMs.%20Using%20GFoLDS%2C%20we%20present%20strong%0Aexperimental%20evidence%20that%20LFLMs%20can%20leverage%20the%20built-in%2C%20basic%20linguistic%0Aknowledge%20inherent%20in%20such%20models%20to%20immediately%20begin%20learning%20more%20complex%0Apatterns.%20On%20downstream%20tasks%2C%20we%20show%20that%20GFoLDS%20vastly%20outperforms%20textual%2C%0Atransformer%20LMs%20pretrained%20on%20similar%20amounts%20of%20data%2C%20indicating%20that%20LFLMs%0Acan%20learn%20with%20substantially%20less%20data%20than%20models%20over%20plain%20text.%0AFurthermore%2C%20we%20show%20that%20the%20performance%20of%20this%20model%20is%20likely%20to%20scale%20with%0Aadditional%20parameters%20and%20pretraining%20data%2C%20suggesting%20the%20viability%20of%20LFLMs%0Ain%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Graph%2520Representations%2520of%2520Logical%2520Forms%2520for%2520Language%2520Modeling%26entry.906535625%3DMichael%2520Sullivan%26entry.1292438233%3D%2520%2520We%2520make%2520the%2520case%2520for%2520language%2520models%2520over%2520logical%2520forms%2520%2528LFLMs%2529%252C%2520arguing%2520that%250Asuch%2520models%2520are%2520more%2520data-efficient%2520than%2520their%2520textual%2520counterparts.%2520To%2520that%250Aend%252C%2520we%2520introduce%2520the%2520Graph-based%2520Formal-Logical%2520Distributional%2520Semantics%250A%2528GFoLDS%2529%2520prototype%252C%2520a%2520pretrained%2520LM%2520over%2520graph%2520representations%2520of%2520logical%250Aforms%252C%2520as%2520a%2520proof-of-concept%2520of%2520LFLMs.%2520Using%2520GFoLDS%252C%2520we%2520present%2520strong%250Aexperimental%2520evidence%2520that%2520LFLMs%2520can%2520leverage%2520the%2520built-in%252C%2520basic%2520linguistic%250Aknowledge%2520inherent%2520in%2520such%2520models%2520to%2520immediately%2520begin%2520learning%2520more%2520complex%250Apatterns.%2520On%2520downstream%2520tasks%252C%2520we%2520show%2520that%2520GFoLDS%2520vastly%2520outperforms%2520textual%252C%250Atransformer%2520LMs%2520pretrained%2520on%2520similar%2520amounts%2520of%2520data%252C%2520indicating%2520that%2520LFLMs%250Acan%2520learn%2520with%2520substantially%2520less%2520data%2520than%2520models%2520over%2520plain%2520text.%250AFurthermore%252C%2520we%2520show%2520that%2520the%2520performance%2520of%2520this%2520model%2520is%2520likely%2520to%2520scale%2520with%250Aadditional%2520parameters%2520and%2520pretraining%2520data%252C%2520suggesting%2520the%2520viability%2520of%2520LFLMs%250Ain%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Graph%20Representations%20of%20Logical%20Forms%20for%20Language%20Modeling&entry.906535625=Michael%20Sullivan&entry.1292438233=%20%20We%20make%20the%20case%20for%20language%20models%20over%20logical%20forms%20%28LFLMs%29%2C%20arguing%20that%0Asuch%20models%20are%20more%20data-efficient%20than%20their%20textual%20counterparts.%20To%20that%0Aend%2C%20we%20introduce%20the%20Graph-based%20Formal-Logical%20Distributional%20Semantics%0A%28GFoLDS%29%20prototype%2C%20a%20pretrained%20LM%20over%20graph%20representations%20of%20logical%0Aforms%2C%20as%20a%20proof-of-concept%20of%20LFLMs.%20Using%20GFoLDS%2C%20we%20present%20strong%0Aexperimental%20evidence%20that%20LFLMs%20can%20leverage%20the%20built-in%2C%20basic%20linguistic%0Aknowledge%20inherent%20in%20such%20models%20to%20immediately%20begin%20learning%20more%20complex%0Apatterns.%20On%20downstream%20tasks%2C%20we%20show%20that%20GFoLDS%20vastly%20outperforms%20textual%2C%0Atransformer%20LMs%20pretrained%20on%20similar%20amounts%20of%20data%2C%20indicating%20that%20LFLMs%0Acan%20learn%20with%20substantially%20less%20data%20than%20models%20over%20plain%20text.%0AFurthermore%2C%20we%20show%20that%20the%20performance%20of%20this%20model%20is%20likely%20to%20scale%20with%0Aadditional%20parameters%20and%20pretraining%20data%2C%20suggesting%20the%20viability%20of%20LFLMs%0Ain%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14523v1&entry.124074799=Read"},
{"title": "Decoupling Classifier for Boosting Few-shot Object Detection and\n  Instance Segmentation", "author": "Bin-Bin Gao and Xiaochen Chen and Zhongyi Huang and Congchong Nie and Jun Liu and Jinxiang Lai and Guannan Jiang and Xi Wang and Chengjie Wang", "abstract": "  This paper focus on few-shot object detection~(FSOD) and instance\nsegmentation~(FSIS), which requires a model to quickly adapt to novel classes\nwith a few labeled instances. The existing methods severely suffer from bias\nclassification because of the missing label issue which naturally exists in an\ninstance-level few-shot scenario and is first formally proposed by us. Our\nanalysis suggests that the standard classification head of most FSOD or FSIS\nmodels needs to be decoupled to mitigate the bias classification. Therefore, we\npropose an embarrassingly simple but effective method that decouples the\nstandard classifier into two heads. Then, these two individual heads are\ncapable of independently addressing clear positive samples and noisy negative\nsamples which are caused by the missing label. In this way, the model can\neffectively learn novel classes while mitigating the effects of noisy negative\nsamples. Without bells and whistles, our model without any additional\ncomputation cost and parameters consistently outperforms its baseline and\nstate-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for\nFSOD and FSIS tasks. The Code is available at\nhttps://csgaobb.github.io/Projects/DCFS.\n", "link": "http://arxiv.org/abs/2505.14239v1", "date": "2025-05-20", "relevancy": 2.5505, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5176}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5134}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Classifier%20for%20Boosting%20Few-shot%20Object%20Detection%20and%0A%20%20Instance%20Segmentation&body=Title%3A%20Decoupling%20Classifier%20for%20Boosting%20Few-shot%20Object%20Detection%20and%0A%20%20Instance%20Segmentation%0AAuthor%3A%20Bin-Bin%20Gao%20and%20Xiaochen%20Chen%20and%20Zhongyi%20Huang%20and%20Congchong%20Nie%20and%20Jun%20Liu%20and%20Jinxiang%20Lai%20and%20Guannan%20Jiang%20and%20Xi%20Wang%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20This%20paper%20focus%20on%20few-shot%20object%20detection~%28FSOD%29%20and%20instance%0Asegmentation~%28FSIS%29%2C%20which%20requires%20a%20model%20to%20quickly%20adapt%20to%20novel%20classes%0Awith%20a%20few%20labeled%20instances.%20The%20existing%20methods%20severely%20suffer%20from%20bias%0Aclassification%20because%20of%20the%20missing%20label%20issue%20which%20naturally%20exists%20in%20an%0Ainstance-level%20few-shot%20scenario%20and%20is%20first%20formally%20proposed%20by%20us.%20Our%0Aanalysis%20suggests%20that%20the%20standard%20classification%20head%20of%20most%20FSOD%20or%20FSIS%0Amodels%20needs%20to%20be%20decoupled%20to%20mitigate%20the%20bias%20classification.%20Therefore%2C%20we%0Apropose%20an%20embarrassingly%20simple%20but%20effective%20method%20that%20decouples%20the%0Astandard%20classifier%20into%20two%20heads.%20Then%2C%20these%20two%20individual%20heads%20are%0Acapable%20of%20independently%20addressing%20clear%20positive%20samples%20and%20noisy%20negative%0Asamples%20which%20are%20caused%20by%20the%20missing%20label.%20In%20this%20way%2C%20the%20model%20can%0Aeffectively%20learn%20novel%20classes%20while%20mitigating%20the%20effects%20of%20noisy%20negative%0Asamples.%20Without%20bells%20and%20whistles%2C%20our%20model%20without%20any%20additional%0Acomputation%20cost%20and%20parameters%20consistently%20outperforms%20its%20baseline%20and%0Astate-of-the-art%20by%20a%20large%20margin%20on%20PASCAL%20VOC%20and%20MS-COCO%20benchmarks%20for%0AFSOD%20and%20FSIS%20tasks.%20The%20Code%20is%20available%20at%0Ahttps%3A//csgaobb.github.io/Projects/DCFS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Classifier%2520for%2520Boosting%2520Few-shot%2520Object%2520Detection%2520and%250A%2520%2520Instance%2520Segmentation%26entry.906535625%3DBin-Bin%2520Gao%2520and%2520Xiaochen%2520Chen%2520and%2520Zhongyi%2520Huang%2520and%2520Congchong%2520Nie%2520and%2520Jun%2520Liu%2520and%2520Jinxiang%2520Lai%2520and%2520Guannan%2520Jiang%2520and%2520Xi%2520Wang%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520focus%2520on%2520few-shot%2520object%2520detection~%2528FSOD%2529%2520and%2520instance%250Asegmentation~%2528FSIS%2529%252C%2520which%2520requires%2520a%2520model%2520to%2520quickly%2520adapt%2520to%2520novel%2520classes%250Awith%2520a%2520few%2520labeled%2520instances.%2520The%2520existing%2520methods%2520severely%2520suffer%2520from%2520bias%250Aclassification%2520because%2520of%2520the%2520missing%2520label%2520issue%2520which%2520naturally%2520exists%2520in%2520an%250Ainstance-level%2520few-shot%2520scenario%2520and%2520is%2520first%2520formally%2520proposed%2520by%2520us.%2520Our%250Aanalysis%2520suggests%2520that%2520the%2520standard%2520classification%2520head%2520of%2520most%2520FSOD%2520or%2520FSIS%250Amodels%2520needs%2520to%2520be%2520decoupled%2520to%2520mitigate%2520the%2520bias%2520classification.%2520Therefore%252C%2520we%250Apropose%2520an%2520embarrassingly%2520simple%2520but%2520effective%2520method%2520that%2520decouples%2520the%250Astandard%2520classifier%2520into%2520two%2520heads.%2520Then%252C%2520these%2520two%2520individual%2520heads%2520are%250Acapable%2520of%2520independently%2520addressing%2520clear%2520positive%2520samples%2520and%2520noisy%2520negative%250Asamples%2520which%2520are%2520caused%2520by%2520the%2520missing%2520label.%2520In%2520this%2520way%252C%2520the%2520model%2520can%250Aeffectively%2520learn%2520novel%2520classes%2520while%2520mitigating%2520the%2520effects%2520of%2520noisy%2520negative%250Asamples.%2520Without%2520bells%2520and%2520whistles%252C%2520our%2520model%2520without%2520any%2520additional%250Acomputation%2520cost%2520and%2520parameters%2520consistently%2520outperforms%2520its%2520baseline%2520and%250Astate-of-the-art%2520by%2520a%2520large%2520margin%2520on%2520PASCAL%2520VOC%2520and%2520MS-COCO%2520benchmarks%2520for%250AFSOD%2520and%2520FSIS%2520tasks.%2520The%2520Code%2520is%2520available%2520at%250Ahttps%253A//csgaobb.github.io/Projects/DCFS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Classifier%20for%20Boosting%20Few-shot%20Object%20Detection%20and%0A%20%20Instance%20Segmentation&entry.906535625=Bin-Bin%20Gao%20and%20Xiaochen%20Chen%20and%20Zhongyi%20Huang%20and%20Congchong%20Nie%20and%20Jun%20Liu%20and%20Jinxiang%20Lai%20and%20Guannan%20Jiang%20and%20Xi%20Wang%20and%20Chengjie%20Wang&entry.1292438233=%20%20This%20paper%20focus%20on%20few-shot%20object%20detection~%28FSOD%29%20and%20instance%0Asegmentation~%28FSIS%29%2C%20which%20requires%20a%20model%20to%20quickly%20adapt%20to%20novel%20classes%0Awith%20a%20few%20labeled%20instances.%20The%20existing%20methods%20severely%20suffer%20from%20bias%0Aclassification%20because%20of%20the%20missing%20label%20issue%20which%20naturally%20exists%20in%20an%0Ainstance-level%20few-shot%20scenario%20and%20is%20first%20formally%20proposed%20by%20us.%20Our%0Aanalysis%20suggests%20that%20the%20standard%20classification%20head%20of%20most%20FSOD%20or%20FSIS%0Amodels%20needs%20to%20be%20decoupled%20to%20mitigate%20the%20bias%20classification.%20Therefore%2C%20we%0Apropose%20an%20embarrassingly%20simple%20but%20effective%20method%20that%20decouples%20the%0Astandard%20classifier%20into%20two%20heads.%20Then%2C%20these%20two%20individual%20heads%20are%0Acapable%20of%20independently%20addressing%20clear%20positive%20samples%20and%20noisy%20negative%0Asamples%20which%20are%20caused%20by%20the%20missing%20label.%20In%20this%20way%2C%20the%20model%20can%0Aeffectively%20learn%20novel%20classes%20while%20mitigating%20the%20effects%20of%20noisy%20negative%0Asamples.%20Without%20bells%20and%20whistles%2C%20our%20model%20without%20any%20additional%0Acomputation%20cost%20and%20parameters%20consistently%20outperforms%20its%20baseline%20and%0Astate-of-the-art%20by%20a%20large%20margin%20on%20PASCAL%20VOC%20and%20MS-COCO%20benchmarks%20for%0AFSOD%20and%20FSIS%20tasks.%20The%20Code%20is%20available%20at%0Ahttps%3A//csgaobb.github.io/Projects/DCFS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14239v1&entry.124074799=Read"},
{"title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic\n  Reasoning Limits", "author": "Xiang Zhang and Juntai Cao and Jiaqi Wei and Yiwei Xu and Chenyu You", "abstract": "  Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.\n", "link": "http://arxiv.org/abs/2505.14178v1", "date": "2025-05-20", "relevancy": 2.5439, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5209}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tokenization%20Constraints%20in%20LLMs%3A%20A%20Study%20of%20Symbolic%20and%20Arithmetic%0A%20%20Reasoning%20Limits&body=Title%3A%20Tokenization%20Constraints%20in%20LLMs%3A%20A%20Study%20of%20Symbolic%20and%20Arithmetic%0A%20%20Reasoning%20Limits%0AAuthor%3A%20Xiang%20Zhang%20and%20Juntai%20Cao%20and%20Jiaqi%20Wei%20and%20Yiwei%20Xu%20and%20Chenyu%20You%0AAbstract%3A%20%20%20Tokenization%20is%20the%20first%20-%20and%20often%20underappreciated%20-%20layer%20of%20computation%0Ain%20language%20models.%20While%20Chain-of-Thought%20%28CoT%29%20prompting%20enables%20transformer%0Amodels%20to%20approximate%20recurrent%20computation%20by%20externalizing%20intermediate%0Asteps%2C%20we%20show%20that%20the%20success%20of%20such%20reasoning%20is%20fundamentally%20bounded%20by%0Athe%20structure%20of%20tokenized%20inputs.%20This%20work%20presents%20a%20theoretical%20and%0Aempirical%20investigation%20into%20how%20tokenization%20schemes%2C%20particularly%0Asubword-based%20methods%20like%20byte-pair%20encoding%20%28BPE%29%2C%20impede%20symbolic%0Acomputation%20by%20merging%20or%20obscuring%20atomic%20reasoning%20units.%20We%20introduce%20the%0Anotion%20of%20Token%20Awareness%20to%20formalize%20how%20poor%20token%20granularity%20disrupts%0Alogical%20alignment%20and%20prevents%20models%20from%20generalizing%20symbolic%20procedures.%0AThrough%20systematic%20evaluation%20on%20arithmetic%20and%20symbolic%20tasks%2C%20we%20demonstrate%0Athat%20token%20structure%20dramatically%20affect%20reasoning%20performance%2C%20causing%20failure%0Aeven%20with%20CoT%2C%20while%20atomically-aligned%20formats%20unlock%20strong%20generalization%2C%0Aallowing%20small%20models%20%28e.g.%2C%20GPT-4o-mini%29%20to%20outperform%20larger%20systems%20%28e.g.%2C%0Ao1%29%20in%20structured%20reasoning.%20Our%20findings%20reveal%20that%20symbolic%20reasoning%0Aability%20in%20LLMs%20is%20not%20purely%20architectural%2C%20but%20deeply%20conditioned%20on%0Atoken-level%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenization%2520Constraints%2520in%2520LLMs%253A%2520A%2520Study%2520of%2520Symbolic%2520and%2520Arithmetic%250A%2520%2520Reasoning%2520Limits%26entry.906535625%3DXiang%2520Zhang%2520and%2520Juntai%2520Cao%2520and%2520Jiaqi%2520Wei%2520and%2520Yiwei%2520Xu%2520and%2520Chenyu%2520You%26entry.1292438233%3D%2520%2520Tokenization%2520is%2520the%2520first%2520-%2520and%2520often%2520underappreciated%2520-%2520layer%2520of%2520computation%250Ain%2520language%2520models.%2520While%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520enables%2520transformer%250Amodels%2520to%2520approximate%2520recurrent%2520computation%2520by%2520externalizing%2520intermediate%250Asteps%252C%2520we%2520show%2520that%2520the%2520success%2520of%2520such%2520reasoning%2520is%2520fundamentally%2520bounded%2520by%250Athe%2520structure%2520of%2520tokenized%2520inputs.%2520This%2520work%2520presents%2520a%2520theoretical%2520and%250Aempirical%2520investigation%2520into%2520how%2520tokenization%2520schemes%252C%2520particularly%250Asubword-based%2520methods%2520like%2520byte-pair%2520encoding%2520%2528BPE%2529%252C%2520impede%2520symbolic%250Acomputation%2520by%2520merging%2520or%2520obscuring%2520atomic%2520reasoning%2520units.%2520We%2520introduce%2520the%250Anotion%2520of%2520Token%2520Awareness%2520to%2520formalize%2520how%2520poor%2520token%2520granularity%2520disrupts%250Alogical%2520alignment%2520and%2520prevents%2520models%2520from%2520generalizing%2520symbolic%2520procedures.%250AThrough%2520systematic%2520evaluation%2520on%2520arithmetic%2520and%2520symbolic%2520tasks%252C%2520we%2520demonstrate%250Athat%2520token%2520structure%2520dramatically%2520affect%2520reasoning%2520performance%252C%2520causing%2520failure%250Aeven%2520with%2520CoT%252C%2520while%2520atomically-aligned%2520formats%2520unlock%2520strong%2520generalization%252C%250Aallowing%2520small%2520models%2520%2528e.g.%252C%2520GPT-4o-mini%2529%2520to%2520outperform%2520larger%2520systems%2520%2528e.g.%252C%250Ao1%2529%2520in%2520structured%2520reasoning.%2520Our%2520findings%2520reveal%2520that%2520symbolic%2520reasoning%250Aability%2520in%2520LLMs%2520is%2520not%2520purely%2520architectural%252C%2520but%2520deeply%2520conditioned%2520on%250Atoken-level%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tokenization%20Constraints%20in%20LLMs%3A%20A%20Study%20of%20Symbolic%20and%20Arithmetic%0A%20%20Reasoning%20Limits&entry.906535625=Xiang%20Zhang%20and%20Juntai%20Cao%20and%20Jiaqi%20Wei%20and%20Yiwei%20Xu%20and%20Chenyu%20You&entry.1292438233=%20%20Tokenization%20is%20the%20first%20-%20and%20often%20underappreciated%20-%20layer%20of%20computation%0Ain%20language%20models.%20While%20Chain-of-Thought%20%28CoT%29%20prompting%20enables%20transformer%0Amodels%20to%20approximate%20recurrent%20computation%20by%20externalizing%20intermediate%0Asteps%2C%20we%20show%20that%20the%20success%20of%20such%20reasoning%20is%20fundamentally%20bounded%20by%0Athe%20structure%20of%20tokenized%20inputs.%20This%20work%20presents%20a%20theoretical%20and%0Aempirical%20investigation%20into%20how%20tokenization%20schemes%2C%20particularly%0Asubword-based%20methods%20like%20byte-pair%20encoding%20%28BPE%29%2C%20impede%20symbolic%0Acomputation%20by%20merging%20or%20obscuring%20atomic%20reasoning%20units.%20We%20introduce%20the%0Anotion%20of%20Token%20Awareness%20to%20formalize%20how%20poor%20token%20granularity%20disrupts%0Alogical%20alignment%20and%20prevents%20models%20from%20generalizing%20symbolic%20procedures.%0AThrough%20systematic%20evaluation%20on%20arithmetic%20and%20symbolic%20tasks%2C%20we%20demonstrate%0Athat%20token%20structure%20dramatically%20affect%20reasoning%20performance%2C%20causing%20failure%0Aeven%20with%20CoT%2C%20while%20atomically-aligned%20formats%20unlock%20strong%20generalization%2C%0Aallowing%20small%20models%20%28e.g.%2C%20GPT-4o-mini%29%20to%20outperform%20larger%20systems%20%28e.g.%2C%0Ao1%29%20in%20structured%20reasoning.%20Our%20findings%20reveal%20that%20symbolic%20reasoning%0Aability%20in%20LLMs%20is%20not%20purely%20architectural%2C%20but%20deeply%20conditioned%20on%0Atoken-level%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14178v1&entry.124074799=Read"},
{"title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "author": "Haolei Xu and Yuchen Yan and Yongliang Shen and Wenqi Zhang and Guiyang Hou and Shengpei Jiang and Kaitao Song and Weiming Lu and Jun Xiao and Yueting Zhuang", "abstract": "  Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.\n", "link": "http://arxiv.org/abs/2505.14684v1", "date": "2025-05-20", "relevancy": 2.5424, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5146}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20Bridging%20Thought%20Leap%20for%20Improved%20Chain-of-Thought%20Tuning&body=Title%3A%20Mind%20the%20Gap%3A%20Bridging%20Thought%20Leap%20for%20Improved%20Chain-of-Thought%20Tuning%0AAuthor%3A%20Haolei%20Xu%20and%20Yuchen%20Yan%20and%20Yongliang%20Shen%20and%20Wenqi%20Zhang%20and%20Guiyang%20Hou%20and%20Shengpei%20Jiang%20and%20Kaitao%20Song%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20on%0Amathemati-cal%20tasks%20through%20Chain-of-Thought%20%28CoT%29%20reasoning.%20However%2C%20existing%0Amathematical%20CoT%20datasets%20often%20suffer%20from%20Thought%20Leaps%20due%20to%20experts%0Aomitting%20intermediate%20steps%2C%20which%20negatively%20impacts%20model%20learning%20and%0Ageneralization.%20We%20propose%20the%20CoT%20Thought%20Leap%20Bridge%20Task%2C%20which%20aims%20to%0Aautomatically%20detect%20leaps%20and%20generate%20missing%20intermediate%20reasoning%20steps%20to%0Arestore%20the%20completeness%20and%20coherence%20of%20CoT.%20To%20facilitate%20this%2C%20we%0Aconstructed%20a%20specialized%20training%20dataset%20called%20ScaleQM%2B%2C%20based%20on%20the%0Astructured%20ScaleQuestMath%20dataset%2C%20and%20trained%20CoT-Bridge%20to%20bridge%20thought%0Aleaps.%20Through%20comprehensive%20experiments%20on%20mathematical%20reasoning%20benchmarks%2C%0Awe%20demonstrate%20that%20models%20fine-tuned%20on%20bridged%20datasets%20consistently%0Aoutperform%20those%20trained%20on%20original%20datasets%2C%20with%20improvements%20of%20up%20to%0A%2B5.87%25%20on%20NuminaMath.%20Our%20approach%20effectively%20enhances%20distilled%20data%20%28%2B3.02%25%29%0Aand%20provides%20better%20starting%20points%20for%20reinforcement%20learning%20%28%2B3.1%25%29%2C%0Afunctioning%20as%20a%20plug-and-play%20module%20compatible%20with%20existing%20optimization%0Atechniques.%20Furthermore%2C%20CoT-Bridge%20demonstrate%20improved%20generalization%20to%0Aout-of-domain%20logical%20reasoning%20tasks%2C%20confirming%20that%20enhancing%20reasoning%0Acompleteness%20yields%20broadly%20applicable%20benefits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520Bridging%2520Thought%2520Leap%2520for%2520Improved%2520Chain-of-Thought%2520Tuning%26entry.906535625%3DHaolei%2520Xu%2520and%2520Yuchen%2520Yan%2520and%2520Yongliang%2520Shen%2520and%2520Wenqi%2520Zhang%2520and%2520Guiyang%2520Hou%2520and%2520Shengpei%2520Jiang%2520and%2520Kaitao%2520Song%2520and%2520Weiming%2520Lu%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520on%250Amathemati-cal%2520tasks%2520through%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning.%2520However%252C%2520existing%250Amathematical%2520CoT%2520datasets%2520often%2520suffer%2520from%2520Thought%2520Leaps%2520due%2520to%2520experts%250Aomitting%2520intermediate%2520steps%252C%2520which%2520negatively%2520impacts%2520model%2520learning%2520and%250Ageneralization.%2520We%2520propose%2520the%2520CoT%2520Thought%2520Leap%2520Bridge%2520Task%252C%2520which%2520aims%2520to%250Aautomatically%2520detect%2520leaps%2520and%2520generate%2520missing%2520intermediate%2520reasoning%2520steps%2520to%250Arestore%2520the%2520completeness%2520and%2520coherence%2520of%2520CoT.%2520To%2520facilitate%2520this%252C%2520we%250Aconstructed%2520a%2520specialized%2520training%2520dataset%2520called%2520ScaleQM%252B%252C%2520based%2520on%2520the%250Astructured%2520ScaleQuestMath%2520dataset%252C%2520and%2520trained%2520CoT-Bridge%2520to%2520bridge%2520thought%250Aleaps.%2520Through%2520comprehensive%2520experiments%2520on%2520mathematical%2520reasoning%2520benchmarks%252C%250Awe%2520demonstrate%2520that%2520models%2520fine-tuned%2520on%2520bridged%2520datasets%2520consistently%250Aoutperform%2520those%2520trained%2520on%2520original%2520datasets%252C%2520with%2520improvements%2520of%2520up%2520to%250A%252B5.87%2525%2520on%2520NuminaMath.%2520Our%2520approach%2520effectively%2520enhances%2520distilled%2520data%2520%2528%252B3.02%2525%2529%250Aand%2520provides%2520better%2520starting%2520points%2520for%2520reinforcement%2520learning%2520%2528%252B3.1%2525%2529%252C%250Afunctioning%2520as%2520a%2520plug-and-play%2520module%2520compatible%2520with%2520existing%2520optimization%250Atechniques.%2520Furthermore%252C%2520CoT-Bridge%2520demonstrate%2520improved%2520generalization%2520to%250Aout-of-domain%2520logical%2520reasoning%2520tasks%252C%2520confirming%2520that%2520enhancing%2520reasoning%250Acompleteness%2520yields%2520broadly%2520applicable%2520benefits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20Bridging%20Thought%20Leap%20for%20Improved%20Chain-of-Thought%20Tuning&entry.906535625=Haolei%20Xu%20and%20Yuchen%20Yan%20and%20Yongliang%20Shen%20and%20Wenqi%20Zhang%20and%20Guiyang%20Hou%20and%20Shengpei%20Jiang%20and%20Kaitao%20Song%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20remarkable%20progress%20on%0Amathemati-cal%20tasks%20through%20Chain-of-Thought%20%28CoT%29%20reasoning.%20However%2C%20existing%0Amathematical%20CoT%20datasets%20often%20suffer%20from%20Thought%20Leaps%20due%20to%20experts%0Aomitting%20intermediate%20steps%2C%20which%20negatively%20impacts%20model%20learning%20and%0Ageneralization.%20We%20propose%20the%20CoT%20Thought%20Leap%20Bridge%20Task%2C%20which%20aims%20to%0Aautomatically%20detect%20leaps%20and%20generate%20missing%20intermediate%20reasoning%20steps%20to%0Arestore%20the%20completeness%20and%20coherence%20of%20CoT.%20To%20facilitate%20this%2C%20we%0Aconstructed%20a%20specialized%20training%20dataset%20called%20ScaleQM%2B%2C%20based%20on%20the%0Astructured%20ScaleQuestMath%20dataset%2C%20and%20trained%20CoT-Bridge%20to%20bridge%20thought%0Aleaps.%20Through%20comprehensive%20experiments%20on%20mathematical%20reasoning%20benchmarks%2C%0Awe%20demonstrate%20that%20models%20fine-tuned%20on%20bridged%20datasets%20consistently%0Aoutperform%20those%20trained%20on%20original%20datasets%2C%20with%20improvements%20of%20up%20to%0A%2B5.87%25%20on%20NuminaMath.%20Our%20approach%20effectively%20enhances%20distilled%20data%20%28%2B3.02%25%29%0Aand%20provides%20better%20starting%20points%20for%20reinforcement%20learning%20%28%2B3.1%25%29%2C%0Afunctioning%20as%20a%20plug-and-play%20module%20compatible%20with%20existing%20optimization%0Atechniques.%20Furthermore%2C%20CoT-Bridge%20demonstrate%20improved%20generalization%20to%0Aout-of-domain%20logical%20reasoning%20tasks%2C%20confirming%20that%20enhancing%20reasoning%0Acompleteness%20yields%20broadly%20applicable%20benefits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14684v1&entry.124074799=Read"},
{"title": "Self Supervised Correlation-based Permutations for Multi-View Clustering", "author": "Ran Eisenberg and Jonathan Svirsky and Ofir Lindenbaum", "abstract": "  Combining data from different sources can improve data analysis tasks such as\nclustering. However, most of the current multi-view clustering methods are\nlimited to specific domains or rely on a suboptimal and computationally\nintensive two-stage process of representation learning and clustering. We\npropose an end-to-end deep learning-based multi-view clustering framework for\ngeneral data types (such as images and tables). Our approach involves\ngenerating meaningful fused representations using a novel permutation-based\ncanonical correlation objective. We provide a theoretical analysis showing how\nthe learned embeddings approximate those obtained by supervised linear\ndiscriminant analysis (LDA). Cluster assignments are learned by identifying\nconsistent pseudo-labels across multiple views. Additionally, we establish a\ntheoretical bound on the error caused by incorrect pseudo-labels in the\nunsupervised representations compared to LDA. Extensive experiments on ten\nmulti-view clustering benchmark datasets provide empirical evidence for the\neffectiveness of the proposed model.\n", "link": "http://arxiv.org/abs/2402.16383v2", "date": "2025-05-20", "relevancy": 2.5375, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self%20Supervised%20Correlation-based%20Permutations%20for%20Multi-View%20Clustering&body=Title%3A%20Self%20Supervised%20Correlation-based%20Permutations%20for%20Multi-View%20Clustering%0AAuthor%3A%20Ran%20Eisenberg%20and%20Jonathan%20Svirsky%20and%20Ofir%20Lindenbaum%0AAbstract%3A%20%20%20Combining%20data%20from%20different%20sources%20can%20improve%20data%20analysis%20tasks%20such%20as%0Aclustering.%20However%2C%20most%20of%20the%20current%20multi-view%20clustering%20methods%20are%0Alimited%20to%20specific%20domains%20or%20rely%20on%20a%20suboptimal%20and%20computationally%0Aintensive%20two-stage%20process%20of%20representation%20learning%20and%20clustering.%20We%0Apropose%20an%20end-to-end%20deep%20learning-based%20multi-view%20clustering%20framework%20for%0Ageneral%20data%20types%20%28such%20as%20images%20and%20tables%29.%20Our%20approach%20involves%0Agenerating%20meaningful%20fused%20representations%20using%20a%20novel%20permutation-based%0Acanonical%20correlation%20objective.%20We%20provide%20a%20theoretical%20analysis%20showing%20how%0Athe%20learned%20embeddings%20approximate%20those%20obtained%20by%20supervised%20linear%0Adiscriminant%20analysis%20%28LDA%29.%20Cluster%20assignments%20are%20learned%20by%20identifying%0Aconsistent%20pseudo-labels%20across%20multiple%20views.%20Additionally%2C%20we%20establish%20a%0Atheoretical%20bound%20on%20the%20error%20caused%20by%20incorrect%20pseudo-labels%20in%20the%0Aunsupervised%20representations%20compared%20to%20LDA.%20Extensive%20experiments%20on%20ten%0Amulti-view%20clustering%20benchmark%20datasets%20provide%20empirical%20evidence%20for%20the%0Aeffectiveness%20of%20the%20proposed%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf%2520Supervised%2520Correlation-based%2520Permutations%2520for%2520Multi-View%2520Clustering%26entry.906535625%3DRan%2520Eisenberg%2520and%2520Jonathan%2520Svirsky%2520and%2520Ofir%2520Lindenbaum%26entry.1292438233%3D%2520%2520Combining%2520data%2520from%2520different%2520sources%2520can%2520improve%2520data%2520analysis%2520tasks%2520such%2520as%250Aclustering.%2520However%252C%2520most%2520of%2520the%2520current%2520multi-view%2520clustering%2520methods%2520are%250Alimited%2520to%2520specific%2520domains%2520or%2520rely%2520on%2520a%2520suboptimal%2520and%2520computationally%250Aintensive%2520two-stage%2520process%2520of%2520representation%2520learning%2520and%2520clustering.%2520We%250Apropose%2520an%2520end-to-end%2520deep%2520learning-based%2520multi-view%2520clustering%2520framework%2520for%250Ageneral%2520data%2520types%2520%2528such%2520as%2520images%2520and%2520tables%2529.%2520Our%2520approach%2520involves%250Agenerating%2520meaningful%2520fused%2520representations%2520using%2520a%2520novel%2520permutation-based%250Acanonical%2520correlation%2520objective.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520showing%2520how%250Athe%2520learned%2520embeddings%2520approximate%2520those%2520obtained%2520by%2520supervised%2520linear%250Adiscriminant%2520analysis%2520%2528LDA%2529.%2520Cluster%2520assignments%2520are%2520learned%2520by%2520identifying%250Aconsistent%2520pseudo-labels%2520across%2520multiple%2520views.%2520Additionally%252C%2520we%2520establish%2520a%250Atheoretical%2520bound%2520on%2520the%2520error%2520caused%2520by%2520incorrect%2520pseudo-labels%2520in%2520the%250Aunsupervised%2520representations%2520compared%2520to%2520LDA.%2520Extensive%2520experiments%2520on%2520ten%250Amulti-view%2520clustering%2520benchmark%2520datasets%2520provide%2520empirical%2520evidence%2520for%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self%20Supervised%20Correlation-based%20Permutations%20for%20Multi-View%20Clustering&entry.906535625=Ran%20Eisenberg%20and%20Jonathan%20Svirsky%20and%20Ofir%20Lindenbaum&entry.1292438233=%20%20Combining%20data%20from%20different%20sources%20can%20improve%20data%20analysis%20tasks%20such%20as%0Aclustering.%20However%2C%20most%20of%20the%20current%20multi-view%20clustering%20methods%20are%0Alimited%20to%20specific%20domains%20or%20rely%20on%20a%20suboptimal%20and%20computationally%0Aintensive%20two-stage%20process%20of%20representation%20learning%20and%20clustering.%20We%0Apropose%20an%20end-to-end%20deep%20learning-based%20multi-view%20clustering%20framework%20for%0Ageneral%20data%20types%20%28such%20as%20images%20and%20tables%29.%20Our%20approach%20involves%0Agenerating%20meaningful%20fused%20representations%20using%20a%20novel%20permutation-based%0Acanonical%20correlation%20objective.%20We%20provide%20a%20theoretical%20analysis%20showing%20how%0Athe%20learned%20embeddings%20approximate%20those%20obtained%20by%20supervised%20linear%0Adiscriminant%20analysis%20%28LDA%29.%20Cluster%20assignments%20are%20learned%20by%20identifying%0Aconsistent%20pseudo-labels%20across%20multiple%20views.%20Additionally%2C%20we%20establish%20a%0Atheoretical%20bound%20on%20the%20error%20caused%20by%20incorrect%20pseudo-labels%20in%20the%0Aunsupervised%20representations%20compared%20to%20LDA.%20Extensive%20experiments%20on%20ten%0Amulti-view%20clustering%20benchmark%20datasets%20provide%20empirical%20evidence%20for%20the%0Aeffectiveness%20of%20the%20proposed%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16383v2&entry.124074799=Read"},
{"title": "Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive\n  Prediction", "author": "Cheng Wang and Xinzhu Ma and Bin Wang and Shixiang Tang and Yuan Meng and Ping Jiang", "abstract": "  Recovering CAD models from point clouds, especially the sketch-extrusion\nprocess, can be seen as the process of rebuilding the topology and extrusion\nprimitives. Previous methods utilize implicit fields for sketch representation,\nleading to shape reconstruction of curved edges. In this paper, we proposed a\nCAD reconstruction network that produces editable CAD models from input point\nclouds (Point2Primitive) by directly predicting every element of the extrusion\nprimitives. Point2Primitive can directly detect and predict sketch curves (type\nand parameter) from point clouds based on an improved transformer. The sketch\ncurve parameters are formulated as position queries and optimized in an\nautoregressive way, leading to high parameter accuracy. The topology is rebuilt\nby extrusion segmentation, and each extrusion parameter (sketch and extrusion\noperation) is recovered by combining the predicted curves and the computed\nextrusion operation. Extensive experiments demonstrate that our method is\nsuperior in primitive prediction accuracy and CAD reconstruction. The\nreconstructed shapes are of high geometrical fidelity.\n", "link": "http://arxiv.org/abs/2505.02043v2", "date": "2025-05-20", "relevancy": 2.5223, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5165}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5165}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point2Primitive%3A%20CAD%20Reconstruction%20from%20Point%20Cloud%20by%20Direct%20Primitive%0A%20%20Prediction&body=Title%3A%20Point2Primitive%3A%20CAD%20Reconstruction%20from%20Point%20Cloud%20by%20Direct%20Primitive%0A%20%20Prediction%0AAuthor%3A%20Cheng%20Wang%20and%20Xinzhu%20Ma%20and%20Bin%20Wang%20and%20Shixiang%20Tang%20and%20Yuan%20Meng%20and%20Ping%20Jiang%0AAbstract%3A%20%20%20Recovering%20CAD%20models%20from%20point%20clouds%2C%20especially%20the%20sketch-extrusion%0Aprocess%2C%20can%20be%20seen%20as%20the%20process%20of%20rebuilding%20the%20topology%20and%20extrusion%0Aprimitives.%20Previous%20methods%20utilize%20implicit%20fields%20for%20sketch%20representation%2C%0Aleading%20to%20shape%20reconstruction%20of%20curved%20edges.%20In%20this%20paper%2C%20we%20proposed%20a%0ACAD%20reconstruction%20network%20that%20produces%20editable%20CAD%20models%20from%20input%20point%0Aclouds%20%28Point2Primitive%29%20by%20directly%20predicting%20every%20element%20of%20the%20extrusion%0Aprimitives.%20Point2Primitive%20can%20directly%20detect%20and%20predict%20sketch%20curves%20%28type%0Aand%20parameter%29%20from%20point%20clouds%20based%20on%20an%20improved%20transformer.%20The%20sketch%0Acurve%20parameters%20are%20formulated%20as%20position%20queries%20and%20optimized%20in%20an%0Aautoregressive%20way%2C%20leading%20to%20high%20parameter%20accuracy.%20The%20topology%20is%20rebuilt%0Aby%20extrusion%20segmentation%2C%20and%20each%20extrusion%20parameter%20%28sketch%20and%20extrusion%0Aoperation%29%20is%20recovered%20by%20combining%20the%20predicted%20curves%20and%20the%20computed%0Aextrusion%20operation.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20is%0Asuperior%20in%20primitive%20prediction%20accuracy%20and%20CAD%20reconstruction.%20The%0Areconstructed%20shapes%20are%20of%20high%20geometrical%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint2Primitive%253A%2520CAD%2520Reconstruction%2520from%2520Point%2520Cloud%2520by%2520Direct%2520Primitive%250A%2520%2520Prediction%26entry.906535625%3DCheng%2520Wang%2520and%2520Xinzhu%2520Ma%2520and%2520Bin%2520Wang%2520and%2520Shixiang%2520Tang%2520and%2520Yuan%2520Meng%2520and%2520Ping%2520Jiang%26entry.1292438233%3D%2520%2520Recovering%2520CAD%2520models%2520from%2520point%2520clouds%252C%2520especially%2520the%2520sketch-extrusion%250Aprocess%252C%2520can%2520be%2520seen%2520as%2520the%2520process%2520of%2520rebuilding%2520the%2520topology%2520and%2520extrusion%250Aprimitives.%2520Previous%2520methods%2520utilize%2520implicit%2520fields%2520for%2520sketch%2520representation%252C%250Aleading%2520to%2520shape%2520reconstruction%2520of%2520curved%2520edges.%2520In%2520this%2520paper%252C%2520we%2520proposed%2520a%250ACAD%2520reconstruction%2520network%2520that%2520produces%2520editable%2520CAD%2520models%2520from%2520input%2520point%250Aclouds%2520%2528Point2Primitive%2529%2520by%2520directly%2520predicting%2520every%2520element%2520of%2520the%2520extrusion%250Aprimitives.%2520Point2Primitive%2520can%2520directly%2520detect%2520and%2520predict%2520sketch%2520curves%2520%2528type%250Aand%2520parameter%2529%2520from%2520point%2520clouds%2520based%2520on%2520an%2520improved%2520transformer.%2520The%2520sketch%250Acurve%2520parameters%2520are%2520formulated%2520as%2520position%2520queries%2520and%2520optimized%2520in%2520an%250Aautoregressive%2520way%252C%2520leading%2520to%2520high%2520parameter%2520accuracy.%2520The%2520topology%2520is%2520rebuilt%250Aby%2520extrusion%2520segmentation%252C%2520and%2520each%2520extrusion%2520parameter%2520%2528sketch%2520and%2520extrusion%250Aoperation%2529%2520is%2520recovered%2520by%2520combining%2520the%2520predicted%2520curves%2520and%2520the%2520computed%250Aextrusion%2520operation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520is%250Asuperior%2520in%2520primitive%2520prediction%2520accuracy%2520and%2520CAD%2520reconstruction.%2520The%250Areconstructed%2520shapes%2520are%2520of%2520high%2520geometrical%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point2Primitive%3A%20CAD%20Reconstruction%20from%20Point%20Cloud%20by%20Direct%20Primitive%0A%20%20Prediction&entry.906535625=Cheng%20Wang%20and%20Xinzhu%20Ma%20and%20Bin%20Wang%20and%20Shixiang%20Tang%20and%20Yuan%20Meng%20and%20Ping%20Jiang&entry.1292438233=%20%20Recovering%20CAD%20models%20from%20point%20clouds%2C%20especially%20the%20sketch-extrusion%0Aprocess%2C%20can%20be%20seen%20as%20the%20process%20of%20rebuilding%20the%20topology%20and%20extrusion%0Aprimitives.%20Previous%20methods%20utilize%20implicit%20fields%20for%20sketch%20representation%2C%0Aleading%20to%20shape%20reconstruction%20of%20curved%20edges.%20In%20this%20paper%2C%20we%20proposed%20a%0ACAD%20reconstruction%20network%20that%20produces%20editable%20CAD%20models%20from%20input%20point%0Aclouds%20%28Point2Primitive%29%20by%20directly%20predicting%20every%20element%20of%20the%20extrusion%0Aprimitives.%20Point2Primitive%20can%20directly%20detect%20and%20predict%20sketch%20curves%20%28type%0Aand%20parameter%29%20from%20point%20clouds%20based%20on%20an%20improved%20transformer.%20The%20sketch%0Acurve%20parameters%20are%20formulated%20as%20position%20queries%20and%20optimized%20in%20an%0Aautoregressive%20way%2C%20leading%20to%20high%20parameter%20accuracy.%20The%20topology%20is%20rebuilt%0Aby%20extrusion%20segmentation%2C%20and%20each%20extrusion%20parameter%20%28sketch%20and%20extrusion%0Aoperation%29%20is%20recovered%20by%20combining%20the%20predicted%20curves%20and%20the%20computed%0Aextrusion%20operation.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20is%0Asuperior%20in%20primitive%20prediction%20accuracy%20and%20CAD%20reconstruction.%20The%0Areconstructed%20shapes%20are%20of%20high%20geometrical%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02043v2&entry.124074799=Read"},
{"title": "Federated Hybrid Model Pruning through Loss Landscape Exploration", "author": "Christian Intern\u00f2 and Elena Raponi and Niki van Stein and Thomas B\u00e4ck and Markus Olhofer and Yaochu Jin and Barbara Hammer", "abstract": "  As the era of connectivity and unprecedented data generation expands,\ncollaborative intelligence emerges as a key driver for machine learning,\nencouraging global-scale model development. Federated learning (FL) stands at\nthe heart of this transformation, enabling distributed systems to work\ncollectively on complex tasks while respecting strict constraints on privacy\nand security. Despite its vast potential, specially in the age of complex\nmodels, FL encounters challenges such as elevated communication costs,\ncomputational constraints, and the heterogeneous data distributions. In this\ncontext, we present AutoFLIP, a novel framework that optimizes FL through an\nadaptive hybrid pruning approach, grounded in a federated loss exploration\nphase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP\nefficiently identifies model substructures for pruning both at structured and\nunstructured levels. This targeted optimization fosters a symbiotic\nintelligence loop, reducing computational burdens and boosting model\nperformance on resource-limited devices for a more inclusive and democratized\nmodel usage. Our extensive experiments across multiple datasets and FL tasks\nshow that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in\ncomputational overhead, a 35.5% decrease in communication costs, and a notable\nimprovement in global accuracy. By significantly reducing these overheads,\nAutoFLIP offer the way for efficient FL deployment in real-world applications\nfor a scalable and broad applicability.\n", "link": "http://arxiv.org/abs/2405.10271v3", "date": "2025-05-20", "relevancy": 2.5166, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5301}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Hybrid%20Model%20Pruning%20through%20Loss%20Landscape%20Exploration&body=Title%3A%20Federated%20Hybrid%20Model%20Pruning%20through%20Loss%20Landscape%20Exploration%0AAuthor%3A%20Christian%20Intern%C3%B2%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%20and%20Markus%20Olhofer%20and%20Yaochu%20Jin%20and%20Barbara%20Hammer%0AAbstract%3A%20%20%20As%20the%20era%20of%20connectivity%20and%20unprecedented%20data%20generation%20expands%2C%0Acollaborative%20intelligence%20emerges%20as%20a%20key%20driver%20for%20machine%20learning%2C%0Aencouraging%20global-scale%20model%20development.%20Federated%20learning%20%28FL%29%20stands%20at%0Athe%20heart%20of%20this%20transformation%2C%20enabling%20distributed%20systems%20to%20work%0Acollectively%20on%20complex%20tasks%20while%20respecting%20strict%20constraints%20on%20privacy%0Aand%20security.%20Despite%20its%20vast%20potential%2C%20specially%20in%20the%20age%20of%20complex%0Amodels%2C%20FL%20encounters%20challenges%20such%20as%20elevated%20communication%20costs%2C%0Acomputational%20constraints%2C%20and%20the%20heterogeneous%20data%20distributions.%20In%20this%0Acontext%2C%20we%20present%20AutoFLIP%2C%20a%20novel%20framework%20that%20optimizes%20FL%20through%20an%0Aadaptive%20hybrid%20pruning%20approach%2C%20grounded%20in%20a%20federated%20loss%20exploration%0Aphase.%20By%20jointly%20analyzing%20diverse%20non-IID%20client%20loss%20landscapes%2C%20AutoFLIP%0Aefficiently%20identifies%20model%20substructures%20for%20pruning%20both%20at%20structured%20and%0Aunstructured%20levels.%20This%20targeted%20optimization%20fosters%20a%20symbiotic%0Aintelligence%20loop%2C%20reducing%20computational%20burdens%20and%20boosting%20model%0Aperformance%20on%20resource-limited%20devices%20for%20a%20more%20inclusive%20and%20democratized%0Amodel%20usage.%20Our%20extensive%20experiments%20across%20multiple%20datasets%20and%20FL%20tasks%0Ashow%20that%20AutoFLIP%20delivers%20quantifiable%20benefits%3A%20a%2048.8%25%20reduction%20in%0Acomputational%20overhead%2C%20a%2035.5%25%20decrease%20in%20communication%20costs%2C%20and%20a%20notable%0Aimprovement%20in%20global%20accuracy.%20By%20significantly%20reducing%20these%20overheads%2C%0AAutoFLIP%20offer%20the%20way%20for%20efficient%20FL%20deployment%20in%20real-world%20applications%0Afor%20a%20scalable%20and%20broad%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10271v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Hybrid%2520Model%2520Pruning%2520through%2520Loss%2520Landscape%2520Exploration%26entry.906535625%3DChristian%2520Intern%25C3%25B2%2520and%2520Elena%2520Raponi%2520and%2520Niki%2520van%2520Stein%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Markus%2520Olhofer%2520and%2520Yaochu%2520Jin%2520and%2520Barbara%2520Hammer%26entry.1292438233%3D%2520%2520As%2520the%2520era%2520of%2520connectivity%2520and%2520unprecedented%2520data%2520generation%2520expands%252C%250Acollaborative%2520intelligence%2520emerges%2520as%2520a%2520key%2520driver%2520for%2520machine%2520learning%252C%250Aencouraging%2520global-scale%2520model%2520development.%2520Federated%2520learning%2520%2528FL%2529%2520stands%2520at%250Athe%2520heart%2520of%2520this%2520transformation%252C%2520enabling%2520distributed%2520systems%2520to%2520work%250Acollectively%2520on%2520complex%2520tasks%2520while%2520respecting%2520strict%2520constraints%2520on%2520privacy%250Aand%2520security.%2520Despite%2520its%2520vast%2520potential%252C%2520specially%2520in%2520the%2520age%2520of%2520complex%250Amodels%252C%2520FL%2520encounters%2520challenges%2520such%2520as%2520elevated%2520communication%2520costs%252C%250Acomputational%2520constraints%252C%2520and%2520the%2520heterogeneous%2520data%2520distributions.%2520In%2520this%250Acontext%252C%2520we%2520present%2520AutoFLIP%252C%2520a%2520novel%2520framework%2520that%2520optimizes%2520FL%2520through%2520an%250Aadaptive%2520hybrid%2520pruning%2520approach%252C%2520grounded%2520in%2520a%2520federated%2520loss%2520exploration%250Aphase.%2520By%2520jointly%2520analyzing%2520diverse%2520non-IID%2520client%2520loss%2520landscapes%252C%2520AutoFLIP%250Aefficiently%2520identifies%2520model%2520substructures%2520for%2520pruning%2520both%2520at%2520structured%2520and%250Aunstructured%2520levels.%2520This%2520targeted%2520optimization%2520fosters%2520a%2520symbiotic%250Aintelligence%2520loop%252C%2520reducing%2520computational%2520burdens%2520and%2520boosting%2520model%250Aperformance%2520on%2520resource-limited%2520devices%2520for%2520a%2520more%2520inclusive%2520and%2520democratized%250Amodel%2520usage.%2520Our%2520extensive%2520experiments%2520across%2520multiple%2520datasets%2520and%2520FL%2520tasks%250Ashow%2520that%2520AutoFLIP%2520delivers%2520quantifiable%2520benefits%253A%2520a%252048.8%2525%2520reduction%2520in%250Acomputational%2520overhead%252C%2520a%252035.5%2525%2520decrease%2520in%2520communication%2520costs%252C%2520and%2520a%2520notable%250Aimprovement%2520in%2520global%2520accuracy.%2520By%2520significantly%2520reducing%2520these%2520overheads%252C%250AAutoFLIP%2520offer%2520the%2520way%2520for%2520efficient%2520FL%2520deployment%2520in%2520real-world%2520applications%250Afor%2520a%2520scalable%2520and%2520broad%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10271v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Hybrid%20Model%20Pruning%20through%20Loss%20Landscape%20Exploration&entry.906535625=Christian%20Intern%C3%B2%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%20and%20Thomas%20B%C3%A4ck%20and%20Markus%20Olhofer%20and%20Yaochu%20Jin%20and%20Barbara%20Hammer&entry.1292438233=%20%20As%20the%20era%20of%20connectivity%20and%20unprecedented%20data%20generation%20expands%2C%0Acollaborative%20intelligence%20emerges%20as%20a%20key%20driver%20for%20machine%20learning%2C%0Aencouraging%20global-scale%20model%20development.%20Federated%20learning%20%28FL%29%20stands%20at%0Athe%20heart%20of%20this%20transformation%2C%20enabling%20distributed%20systems%20to%20work%0Acollectively%20on%20complex%20tasks%20while%20respecting%20strict%20constraints%20on%20privacy%0Aand%20security.%20Despite%20its%20vast%20potential%2C%20specially%20in%20the%20age%20of%20complex%0Amodels%2C%20FL%20encounters%20challenges%20such%20as%20elevated%20communication%20costs%2C%0Acomputational%20constraints%2C%20and%20the%20heterogeneous%20data%20distributions.%20In%20this%0Acontext%2C%20we%20present%20AutoFLIP%2C%20a%20novel%20framework%20that%20optimizes%20FL%20through%20an%0Aadaptive%20hybrid%20pruning%20approach%2C%20grounded%20in%20a%20federated%20loss%20exploration%0Aphase.%20By%20jointly%20analyzing%20diverse%20non-IID%20client%20loss%20landscapes%2C%20AutoFLIP%0Aefficiently%20identifies%20model%20substructures%20for%20pruning%20both%20at%20structured%20and%0Aunstructured%20levels.%20This%20targeted%20optimization%20fosters%20a%20symbiotic%0Aintelligence%20loop%2C%20reducing%20computational%20burdens%20and%20boosting%20model%0Aperformance%20on%20resource-limited%20devices%20for%20a%20more%20inclusive%20and%20democratized%0Amodel%20usage.%20Our%20extensive%20experiments%20across%20multiple%20datasets%20and%20FL%20tasks%0Ashow%20that%20AutoFLIP%20delivers%20quantifiable%20benefits%3A%20a%2048.8%25%20reduction%20in%0Acomputational%20overhead%2C%20a%2035.5%25%20decrease%20in%20communication%20costs%2C%20and%20a%20notable%0Aimprovement%20in%20global%20accuracy.%20By%20significantly%20reducing%20these%20overheads%2C%0AAutoFLIP%20offer%20the%20way%20for%20efficient%20FL%20deployment%20in%20real-world%20applications%0Afor%20a%20scalable%20and%20broad%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10271v3&entry.124074799=Read"},
{"title": "Harnessing the Universal Geometry of Embeddings", "author": "Rishi Jha and Collin Zhang and Vitaly Shmatikov and John X. Morris", "abstract": "  We introduce the first method for translating text embeddings from one vector\nspace to another without any paired data, encoders, or predefined sets of\nmatches. Our unsupervised approach translates any embedding to and from a\nuniversal latent representation (i.e., a universal semantic structure\nconjectured by the Platonic Representation Hypothesis). Our translations\nachieve high cosine similarity across model pairs with different architectures,\nparameter counts, and training datasets.\n  The ability to translate unknown embeddings into a different space while\npreserving their geometry has serious implications for the security of vector\ndatabases. An adversary with access only to embedding vectors can extract\nsensitive information about the underlying documents, sufficient for\nclassification and attribute inference.\n", "link": "http://arxiv.org/abs/2505.12540v2", "date": "2025-05-20", "relevancy": 2.5132, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5137}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Universal%20Geometry%20of%20Embeddings&body=Title%3A%20Harnessing%20the%20Universal%20Geometry%20of%20Embeddings%0AAuthor%3A%20Rishi%20Jha%20and%20Collin%20Zhang%20and%20Vitaly%20Shmatikov%20and%20John%20X.%20Morris%0AAbstract%3A%20%20%20We%20introduce%20the%20first%20method%20for%20translating%20text%20embeddings%20from%20one%20vector%0Aspace%20to%20another%20without%20any%20paired%20data%2C%20encoders%2C%20or%20predefined%20sets%20of%0Amatches.%20Our%20unsupervised%20approach%20translates%20any%20embedding%20to%20and%20from%20a%0Auniversal%20latent%20representation%20%28i.e.%2C%20a%20universal%20semantic%20structure%0Aconjectured%20by%20the%20Platonic%20Representation%20Hypothesis%29.%20Our%20translations%0Aachieve%20high%20cosine%20similarity%20across%20model%20pairs%20with%20different%20architectures%2C%0Aparameter%20counts%2C%20and%20training%20datasets.%0A%20%20The%20ability%20to%20translate%20unknown%20embeddings%20into%20a%20different%20space%20while%0Apreserving%20their%20geometry%20has%20serious%20implications%20for%20the%20security%20of%20vector%0Adatabases.%20An%20adversary%20with%20access%20only%20to%20embedding%20vectors%20can%20extract%0Asensitive%20information%20about%20the%20underlying%20documents%2C%20sufficient%20for%0Aclassification%20and%20attribute%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12540v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520the%2520Universal%2520Geometry%2520of%2520Embeddings%26entry.906535625%3DRishi%2520Jha%2520and%2520Collin%2520Zhang%2520and%2520Vitaly%2520Shmatikov%2520and%2520John%2520X.%2520Morris%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520first%2520method%2520for%2520translating%2520text%2520embeddings%2520from%2520one%2520vector%250Aspace%2520to%2520another%2520without%2520any%2520paired%2520data%252C%2520encoders%252C%2520or%2520predefined%2520sets%2520of%250Amatches.%2520Our%2520unsupervised%2520approach%2520translates%2520any%2520embedding%2520to%2520and%2520from%2520a%250Auniversal%2520latent%2520representation%2520%2528i.e.%252C%2520a%2520universal%2520semantic%2520structure%250Aconjectured%2520by%2520the%2520Platonic%2520Representation%2520Hypothesis%2529.%2520Our%2520translations%250Aachieve%2520high%2520cosine%2520similarity%2520across%2520model%2520pairs%2520with%2520different%2520architectures%252C%250Aparameter%2520counts%252C%2520and%2520training%2520datasets.%250A%2520%2520The%2520ability%2520to%2520translate%2520unknown%2520embeddings%2520into%2520a%2520different%2520space%2520while%250Apreserving%2520their%2520geometry%2520has%2520serious%2520implications%2520for%2520the%2520security%2520of%2520vector%250Adatabases.%2520An%2520adversary%2520with%2520access%2520only%2520to%2520embedding%2520vectors%2520can%2520extract%250Asensitive%2520information%2520about%2520the%2520underlying%2520documents%252C%2520sufficient%2520for%250Aclassification%2520and%2520attribute%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12540v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Universal%20Geometry%20of%20Embeddings&entry.906535625=Rishi%20Jha%20and%20Collin%20Zhang%20and%20Vitaly%20Shmatikov%20and%20John%20X.%20Morris&entry.1292438233=%20%20We%20introduce%20the%20first%20method%20for%20translating%20text%20embeddings%20from%20one%20vector%0Aspace%20to%20another%20without%20any%20paired%20data%2C%20encoders%2C%20or%20predefined%20sets%20of%0Amatches.%20Our%20unsupervised%20approach%20translates%20any%20embedding%20to%20and%20from%20a%0Auniversal%20latent%20representation%20%28i.e.%2C%20a%20universal%20semantic%20structure%0Aconjectured%20by%20the%20Platonic%20Representation%20Hypothesis%29.%20Our%20translations%0Aachieve%20high%20cosine%20similarity%20across%20model%20pairs%20with%20different%20architectures%2C%0Aparameter%20counts%2C%20and%20training%20datasets.%0A%20%20The%20ability%20to%20translate%20unknown%20embeddings%20into%20a%20different%20space%20while%0Apreserving%20their%20geometry%20has%20serious%20implications%20for%20the%20security%20of%20vector%0Adatabases.%20An%20adversary%20with%20access%20only%20to%20embedding%20vectors%20can%20extract%0Asensitive%20information%20about%20the%20underlying%20documents%2C%20sufficient%20for%0Aclassification%20and%20attribute%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12540v2&entry.124074799=Read"},
{"title": "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education", "author": "Yanhao Jia and Xinyi Wu and Hao Li and Qinglin Zhang and Yuxiao Hu and Shuai Zhao and Wenqi Fan", "abstract": "  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n", "link": "http://arxiv.org/abs/2502.05863v2", "date": "2025-05-20", "relevancy": 2.5131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni-Retrieval%3A%20A%20Multi-Style%20Retrieval%20Framework%20for%20STEM%27s%20Education&body=Title%3A%20Uni-Retrieval%3A%20A%20Multi-Style%20Retrieval%20Framework%20for%20STEM%27s%20Education%0AAuthor%3A%20Yanhao%20Jia%20and%20Xinyi%20Wu%20and%20Hao%20Li%20and%20Qinglin%20Zhang%20and%20Yuxiao%20Hu%20and%20Shuai%20Zhao%20and%20Wenqi%20Fan%0AAbstract%3A%20%20%20In%20AI-facilitated%20teaching%2C%20leveraging%20various%20query%20styles%20to%20interpret%0Aabstract%20text%20descriptions%20is%20crucial%20for%20ensuring%20high-quality%20teaching.%0AHowever%2C%20current%20retrieval%20models%20primarily%20focus%20on%20natural%20text-image%0Aretrieval%2C%20making%20them%20insufficiently%20tailored%20to%20educational%20scenarios%20due%20to%0Athe%20ambiguities%20in%20the%20retrieval%20process.%20In%20this%20paper%2C%20we%20propose%20a%20diverse%0Aexpression%20retrieval%20task%20tailored%20to%20educational%20scenarios%2C%20supporting%0Aretrieval%20based%20on%20multiple%20query%20styles%20and%20expressions.%20We%20introduce%20the%20STEM%0AEducation%20Retrieval%20Dataset%20%28SER%29%2C%20which%20contains%20over%2024%2C000%20query%20pairs%20of%0Adifferent%20styles%2C%20and%20the%20Uni-Retrieval%2C%20an%20efficient%20and%20style-diversified%0Aretrieval%20vision-language%20model%20based%20on%20prompt%20tuning.%20Uni-Retrieval%20extracts%0Aquery%20style%20features%20as%20prototypes%20and%20builds%20a%20continuously%20updated%20Prompt%0ABank%20containing%20prompt%20tokens%20for%20diverse%20queries.%20This%20bank%20can%20updated%20during%0Atest%20time%20to%20represent%20domain-specific%20knowledge%20for%20different%20subject%0Aretrieval%20scenarios.%20Our%20framework%20demonstrates%20scalability%20and%20robustness%20by%0Adynamically%20retrieving%20prompt%20tokens%20based%20on%20prototype%20similarity%2C%20effectively%0Afacilitating%20learning%20for%20unknown%20queries.%20Experimental%20results%20indicate%20that%0AUni-Retrieval%20outperforms%20existing%20retrieval%20models%20in%20most%20retrieval%20tasks.%0AThis%20advancement%20provides%20a%20scalable%20and%20precise%20solution%20for%20diverse%0Aeducational%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05863v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni-Retrieval%253A%2520A%2520Multi-Style%2520Retrieval%2520Framework%2520for%2520STEM%2527s%2520Education%26entry.906535625%3DYanhao%2520Jia%2520and%2520Xinyi%2520Wu%2520and%2520Hao%2520Li%2520and%2520Qinglin%2520Zhang%2520and%2520Yuxiao%2520Hu%2520and%2520Shuai%2520Zhao%2520and%2520Wenqi%2520Fan%26entry.1292438233%3D%2520%2520In%2520AI-facilitated%2520teaching%252C%2520leveraging%2520various%2520query%2520styles%2520to%2520interpret%250Aabstract%2520text%2520descriptions%2520is%2520crucial%2520for%2520ensuring%2520high-quality%2520teaching.%250AHowever%252C%2520current%2520retrieval%2520models%2520primarily%2520focus%2520on%2520natural%2520text-image%250Aretrieval%252C%2520making%2520them%2520insufficiently%2520tailored%2520to%2520educational%2520scenarios%2520due%2520to%250Athe%2520ambiguities%2520in%2520the%2520retrieval%2520process.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520diverse%250Aexpression%2520retrieval%2520task%2520tailored%2520to%2520educational%2520scenarios%252C%2520supporting%250Aretrieval%2520based%2520on%2520multiple%2520query%2520styles%2520and%2520expressions.%2520We%2520introduce%2520the%2520STEM%250AEducation%2520Retrieval%2520Dataset%2520%2528SER%2529%252C%2520which%2520contains%2520over%252024%252C000%2520query%2520pairs%2520of%250Adifferent%2520styles%252C%2520and%2520the%2520Uni-Retrieval%252C%2520an%2520efficient%2520and%2520style-diversified%250Aretrieval%2520vision-language%2520model%2520based%2520on%2520prompt%2520tuning.%2520Uni-Retrieval%2520extracts%250Aquery%2520style%2520features%2520as%2520prototypes%2520and%2520builds%2520a%2520continuously%2520updated%2520Prompt%250ABank%2520containing%2520prompt%2520tokens%2520for%2520diverse%2520queries.%2520This%2520bank%2520can%2520updated%2520during%250Atest%2520time%2520to%2520represent%2520domain-specific%2520knowledge%2520for%2520different%2520subject%250Aretrieval%2520scenarios.%2520Our%2520framework%2520demonstrates%2520scalability%2520and%2520robustness%2520by%250Adynamically%2520retrieving%2520prompt%2520tokens%2520based%2520on%2520prototype%2520similarity%252C%2520effectively%250Afacilitating%2520learning%2520for%2520unknown%2520queries.%2520Experimental%2520results%2520indicate%2520that%250AUni-Retrieval%2520outperforms%2520existing%2520retrieval%2520models%2520in%2520most%2520retrieval%2520tasks.%250AThis%2520advancement%2520provides%2520a%2520scalable%2520and%2520precise%2520solution%2520for%2520diverse%250Aeducational%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05863v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni-Retrieval%3A%20A%20Multi-Style%20Retrieval%20Framework%20for%20STEM%27s%20Education&entry.906535625=Yanhao%20Jia%20and%20Xinyi%20Wu%20and%20Hao%20Li%20and%20Qinglin%20Zhang%20and%20Yuxiao%20Hu%20and%20Shuai%20Zhao%20and%20Wenqi%20Fan&entry.1292438233=%20%20In%20AI-facilitated%20teaching%2C%20leveraging%20various%20query%20styles%20to%20interpret%0Aabstract%20text%20descriptions%20is%20crucial%20for%20ensuring%20high-quality%20teaching.%0AHowever%2C%20current%20retrieval%20models%20primarily%20focus%20on%20natural%20text-image%0Aretrieval%2C%20making%20them%20insufficiently%20tailored%20to%20educational%20scenarios%20due%20to%0Athe%20ambiguities%20in%20the%20retrieval%20process.%20In%20this%20paper%2C%20we%20propose%20a%20diverse%0Aexpression%20retrieval%20task%20tailored%20to%20educational%20scenarios%2C%20supporting%0Aretrieval%20based%20on%20multiple%20query%20styles%20and%20expressions.%20We%20introduce%20the%20STEM%0AEducation%20Retrieval%20Dataset%20%28SER%29%2C%20which%20contains%20over%2024%2C000%20query%20pairs%20of%0Adifferent%20styles%2C%20and%20the%20Uni-Retrieval%2C%20an%20efficient%20and%20style-diversified%0Aretrieval%20vision-language%20model%20based%20on%20prompt%20tuning.%20Uni-Retrieval%20extracts%0Aquery%20style%20features%20as%20prototypes%20and%20builds%20a%20continuously%20updated%20Prompt%0ABank%20containing%20prompt%20tokens%20for%20diverse%20queries.%20This%20bank%20can%20updated%20during%0Atest%20time%20to%20represent%20domain-specific%20knowledge%20for%20different%20subject%0Aretrieval%20scenarios.%20Our%20framework%20demonstrates%20scalability%20and%20robustness%20by%0Adynamically%20retrieving%20prompt%20tokens%20based%20on%20prototype%20similarity%2C%20effectively%0Afacilitating%20learning%20for%20unknown%20queries.%20Experimental%20results%20indicate%20that%0AUni-Retrieval%20outperforms%20existing%20retrieval%20models%20in%20most%20retrieval%20tasks.%0AThis%20advancement%20provides%20a%20scalable%20and%20precise%20solution%20for%20diverse%0Aeducational%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05863v2&entry.124074799=Read"},
{"title": "Virtual Cells: Predict, Explain, Discover", "author": "Emmanuel Noutahi and Jason Hartford and Prudencio Tossou and Shawn Whitfield and Alisandra K. Denton and Cas Wognum and Kristina Ulicna and Jonathan Hsu and Michael Cuccarese and Emmanuel Bengio and Dominique Beaini and Christopher Gibson and Daniel Cohen and Berton Earnshaw", "abstract": "  Drug discovery is fundamentally a process of inferring the effects of\ntreatments on patients, and would therefore benefit immensely from\ncomputational models that can reliably simulate patient responses, enabling\nresearchers to generate and test large numbers of therapeutic hypotheses safely\nand economically before initiating costly clinical trials. Even a more specific\nmodel that predicts the functional response of cells to a wide range of\nperturbations would be tremendously valuable for discovering safe and effective\ntreatments that successfully translate to the clinic. Creating such virtual\ncells has long been a goal of the computational research community that\nunfortunately remains unachieved given the daunting complexity and scale of\ncellular biology. Nevertheless, recent advances in AI, computing power, lab\nautomation, and high-throughput cellular profiling provide new opportunities\nfor reaching this goal. In this perspective, we present a vision for developing\nand evaluating virtual cells that builds on our experience at Recursion. We\nargue that in order to be a useful tool to discover novel biology, virtual\ncells must accurately predict the functional response of a cell to\nperturbations and explain how the predicted response is a consequence of\nmodifications to key biomolecular interactions. We then introduce key\nprinciples for designing therapeutically-relevant virtual cells, describe a\nlab-in-the-loop approach for generating novel insights with them, and advocate\nfor biologically-grounded benchmarks to guide virtual cell development.\nFinally, we make the case that our approach to virtual cells provides a useful\nframework for building other models at higher levels of organization, including\nvirtual patients. We hope that these directions prove useful to the research\ncommunity in developing virtual models optimized for positive impact on drug\ndiscovery outcomes.\n", "link": "http://arxiv.org/abs/2505.14613v1", "date": "2025-05-20", "relevancy": 2.4993, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5046}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4975}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20Cells%3A%20Predict%2C%20Explain%2C%20Discover&body=Title%3A%20Virtual%20Cells%3A%20Predict%2C%20Explain%2C%20Discover%0AAuthor%3A%20Emmanuel%20Noutahi%20and%20Jason%20Hartford%20and%20Prudencio%20Tossou%20and%20Shawn%20Whitfield%20and%20Alisandra%20K.%20Denton%20and%20Cas%20Wognum%20and%20Kristina%20Ulicna%20and%20Jonathan%20Hsu%20and%20Michael%20Cuccarese%20and%20Emmanuel%20Bengio%20and%20Dominique%20Beaini%20and%20Christopher%20Gibson%20and%20Daniel%20Cohen%20and%20Berton%20Earnshaw%0AAbstract%3A%20%20%20Drug%20discovery%20is%20fundamentally%20a%20process%20of%20inferring%20the%20effects%20of%0Atreatments%20on%20patients%2C%20and%20would%20therefore%20benefit%20immensely%20from%0Acomputational%20models%20that%20can%20reliably%20simulate%20patient%20responses%2C%20enabling%0Aresearchers%20to%20generate%20and%20test%20large%20numbers%20of%20therapeutic%20hypotheses%20safely%0Aand%20economically%20before%20initiating%20costly%20clinical%20trials.%20Even%20a%20more%20specific%0Amodel%20that%20predicts%20the%20functional%20response%20of%20cells%20to%20a%20wide%20range%20of%0Aperturbations%20would%20be%20tremendously%20valuable%20for%20discovering%20safe%20and%20effective%0Atreatments%20that%20successfully%20translate%20to%20the%20clinic.%20Creating%20such%20virtual%0Acells%20has%20long%20been%20a%20goal%20of%20the%20computational%20research%20community%20that%0Aunfortunately%20remains%20unachieved%20given%20the%20daunting%20complexity%20and%20scale%20of%0Acellular%20biology.%20Nevertheless%2C%20recent%20advances%20in%20AI%2C%20computing%20power%2C%20lab%0Aautomation%2C%20and%20high-throughput%20cellular%20profiling%20provide%20new%20opportunities%0Afor%20reaching%20this%20goal.%20In%20this%20perspective%2C%20we%20present%20a%20vision%20for%20developing%0Aand%20evaluating%20virtual%20cells%20that%20builds%20on%20our%20experience%20at%20Recursion.%20We%0Aargue%20that%20in%20order%20to%20be%20a%20useful%20tool%20to%20discover%20novel%20biology%2C%20virtual%0Acells%20must%20accurately%20predict%20the%20functional%20response%20of%20a%20cell%20to%0Aperturbations%20and%20explain%20how%20the%20predicted%20response%20is%20a%20consequence%20of%0Amodifications%20to%20key%20biomolecular%20interactions.%20We%20then%20introduce%20key%0Aprinciples%20for%20designing%20therapeutically-relevant%20virtual%20cells%2C%20describe%20a%0Alab-in-the-loop%20approach%20for%20generating%20novel%20insights%20with%20them%2C%20and%20advocate%0Afor%20biologically-grounded%20benchmarks%20to%20guide%20virtual%20cell%20development.%0AFinally%2C%20we%20make%20the%20case%20that%20our%20approach%20to%20virtual%20cells%20provides%20a%20useful%0Aframework%20for%20building%20other%20models%20at%20higher%20levels%20of%20organization%2C%20including%0Avirtual%20patients.%20We%20hope%20that%20these%20directions%20prove%20useful%20to%20the%20research%0Acommunity%20in%20developing%20virtual%20models%20optimized%20for%20positive%20impact%20on%20drug%0Adiscovery%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520Cells%253A%2520Predict%252C%2520Explain%252C%2520Discover%26entry.906535625%3DEmmanuel%2520Noutahi%2520and%2520Jason%2520Hartford%2520and%2520Prudencio%2520Tossou%2520and%2520Shawn%2520Whitfield%2520and%2520Alisandra%2520K.%2520Denton%2520and%2520Cas%2520Wognum%2520and%2520Kristina%2520Ulicna%2520and%2520Jonathan%2520Hsu%2520and%2520Michael%2520Cuccarese%2520and%2520Emmanuel%2520Bengio%2520and%2520Dominique%2520Beaini%2520and%2520Christopher%2520Gibson%2520and%2520Daniel%2520Cohen%2520and%2520Berton%2520Earnshaw%26entry.1292438233%3D%2520%2520Drug%2520discovery%2520is%2520fundamentally%2520a%2520process%2520of%2520inferring%2520the%2520effects%2520of%250Atreatments%2520on%2520patients%252C%2520and%2520would%2520therefore%2520benefit%2520immensely%2520from%250Acomputational%2520models%2520that%2520can%2520reliably%2520simulate%2520patient%2520responses%252C%2520enabling%250Aresearchers%2520to%2520generate%2520and%2520test%2520large%2520numbers%2520of%2520therapeutic%2520hypotheses%2520safely%250Aand%2520economically%2520before%2520initiating%2520costly%2520clinical%2520trials.%2520Even%2520a%2520more%2520specific%250Amodel%2520that%2520predicts%2520the%2520functional%2520response%2520of%2520cells%2520to%2520a%2520wide%2520range%2520of%250Aperturbations%2520would%2520be%2520tremendously%2520valuable%2520for%2520discovering%2520safe%2520and%2520effective%250Atreatments%2520that%2520successfully%2520translate%2520to%2520the%2520clinic.%2520Creating%2520such%2520virtual%250Acells%2520has%2520long%2520been%2520a%2520goal%2520of%2520the%2520computational%2520research%2520community%2520that%250Aunfortunately%2520remains%2520unachieved%2520given%2520the%2520daunting%2520complexity%2520and%2520scale%2520of%250Acellular%2520biology.%2520Nevertheless%252C%2520recent%2520advances%2520in%2520AI%252C%2520computing%2520power%252C%2520lab%250Aautomation%252C%2520and%2520high-throughput%2520cellular%2520profiling%2520provide%2520new%2520opportunities%250Afor%2520reaching%2520this%2520goal.%2520In%2520this%2520perspective%252C%2520we%2520present%2520a%2520vision%2520for%2520developing%250Aand%2520evaluating%2520virtual%2520cells%2520that%2520builds%2520on%2520our%2520experience%2520at%2520Recursion.%2520We%250Aargue%2520that%2520in%2520order%2520to%2520be%2520a%2520useful%2520tool%2520to%2520discover%2520novel%2520biology%252C%2520virtual%250Acells%2520must%2520accurately%2520predict%2520the%2520functional%2520response%2520of%2520a%2520cell%2520to%250Aperturbations%2520and%2520explain%2520how%2520the%2520predicted%2520response%2520is%2520a%2520consequence%2520of%250Amodifications%2520to%2520key%2520biomolecular%2520interactions.%2520We%2520then%2520introduce%2520key%250Aprinciples%2520for%2520designing%2520therapeutically-relevant%2520virtual%2520cells%252C%2520describe%2520a%250Alab-in-the-loop%2520approach%2520for%2520generating%2520novel%2520insights%2520with%2520them%252C%2520and%2520advocate%250Afor%2520biologically-grounded%2520benchmarks%2520to%2520guide%2520virtual%2520cell%2520development.%250AFinally%252C%2520we%2520make%2520the%2520case%2520that%2520our%2520approach%2520to%2520virtual%2520cells%2520provides%2520a%2520useful%250Aframework%2520for%2520building%2520other%2520models%2520at%2520higher%2520levels%2520of%2520organization%252C%2520including%250Avirtual%2520patients.%2520We%2520hope%2520that%2520these%2520directions%2520prove%2520useful%2520to%2520the%2520research%250Acommunity%2520in%2520developing%2520virtual%2520models%2520optimized%2520for%2520positive%2520impact%2520on%2520drug%250Adiscovery%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20Cells%3A%20Predict%2C%20Explain%2C%20Discover&entry.906535625=Emmanuel%20Noutahi%20and%20Jason%20Hartford%20and%20Prudencio%20Tossou%20and%20Shawn%20Whitfield%20and%20Alisandra%20K.%20Denton%20and%20Cas%20Wognum%20and%20Kristina%20Ulicna%20and%20Jonathan%20Hsu%20and%20Michael%20Cuccarese%20and%20Emmanuel%20Bengio%20and%20Dominique%20Beaini%20and%20Christopher%20Gibson%20and%20Daniel%20Cohen%20and%20Berton%20Earnshaw&entry.1292438233=%20%20Drug%20discovery%20is%20fundamentally%20a%20process%20of%20inferring%20the%20effects%20of%0Atreatments%20on%20patients%2C%20and%20would%20therefore%20benefit%20immensely%20from%0Acomputational%20models%20that%20can%20reliably%20simulate%20patient%20responses%2C%20enabling%0Aresearchers%20to%20generate%20and%20test%20large%20numbers%20of%20therapeutic%20hypotheses%20safely%0Aand%20economically%20before%20initiating%20costly%20clinical%20trials.%20Even%20a%20more%20specific%0Amodel%20that%20predicts%20the%20functional%20response%20of%20cells%20to%20a%20wide%20range%20of%0Aperturbations%20would%20be%20tremendously%20valuable%20for%20discovering%20safe%20and%20effective%0Atreatments%20that%20successfully%20translate%20to%20the%20clinic.%20Creating%20such%20virtual%0Acells%20has%20long%20been%20a%20goal%20of%20the%20computational%20research%20community%20that%0Aunfortunately%20remains%20unachieved%20given%20the%20daunting%20complexity%20and%20scale%20of%0Acellular%20biology.%20Nevertheless%2C%20recent%20advances%20in%20AI%2C%20computing%20power%2C%20lab%0Aautomation%2C%20and%20high-throughput%20cellular%20profiling%20provide%20new%20opportunities%0Afor%20reaching%20this%20goal.%20In%20this%20perspective%2C%20we%20present%20a%20vision%20for%20developing%0Aand%20evaluating%20virtual%20cells%20that%20builds%20on%20our%20experience%20at%20Recursion.%20We%0Aargue%20that%20in%20order%20to%20be%20a%20useful%20tool%20to%20discover%20novel%20biology%2C%20virtual%0Acells%20must%20accurately%20predict%20the%20functional%20response%20of%20a%20cell%20to%0Aperturbations%20and%20explain%20how%20the%20predicted%20response%20is%20a%20consequence%20of%0Amodifications%20to%20key%20biomolecular%20interactions.%20We%20then%20introduce%20key%0Aprinciples%20for%20designing%20therapeutically-relevant%20virtual%20cells%2C%20describe%20a%0Alab-in-the-loop%20approach%20for%20generating%20novel%20insights%20with%20them%2C%20and%20advocate%0Afor%20biologically-grounded%20benchmarks%20to%20guide%20virtual%20cell%20development.%0AFinally%2C%20we%20make%20the%20case%20that%20our%20approach%20to%20virtual%20cells%20provides%20a%20useful%0Aframework%20for%20building%20other%20models%20at%20higher%20levels%20of%20organization%2C%20including%0Avirtual%20patients.%20We%20hope%20that%20these%20directions%20prove%20useful%20to%20the%20research%0Acommunity%20in%20developing%20virtual%20models%20optimized%20for%20positive%20impact%20on%20drug%0Adiscovery%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14613v1&entry.124074799=Read"},
{"title": "Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive\n  Extension", "author": "Jiahan Li and Tong Chen and Shitong Luo and Chaoran Cheng and Jiaqi Guan and Ruihan Guo and Sheng Wang and Ge Liu and Jian Peng and Jianzhu Ma", "abstract": "  Peptides, short chains of amino acids, interact with target proteins, making\nthem a unique class of protein-based therapeutics for treating human diseases.\nRecently, deep generative models have shown great promise in peptide\ngeneration. However, several challenges remain in designing effective peptide\nbinders. First, not all residues contribute equally to peptide-target\ninteractions. Second, the generated peptides must adopt valid geometries due to\nthe constraints of peptide bonds. Third, realistic tasks for peptide drug\ndevelopment are still lacking. To address these challenges, we introduce\nPepHAR, a hot-spot-driven autoregressive generative model for designing\npeptides targeting specific proteins. Building on the observation that certain\nhot spot residues have higher interaction potentials, we first use an\nenergy-based density model to fit and sample these key residues. Next, to\nensure proper peptide geometry, we autoregressively extend peptide fragments by\nestimating dihedral angles between residue frames. Finally, we apply an\noptimization process to iteratively refine fragment assembly, ensuring correct\npeptide structures. By combining hot spot sampling with fragment-based\nextension, our approach enables de novo peptide design tailored to a target\nprotein and allows the incorporation of key hot spot residues into peptide\nscaffolds. Extensive experiments, including peptide design and peptide scaffold\ngeneration, demonstrate the strong potential of PepHAR in computational peptide\nbinder design. Source code will be available at\nhttps://github.com/Ced3-han/PepHAR.\n", "link": "http://arxiv.org/abs/2411.18463v3", "date": "2025-05-20", "relevancy": 2.4982, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5061}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4988}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hotspot-Driven%20Peptide%20Design%20via%20Multi-Fragment%20Autoregressive%0A%20%20Extension&body=Title%3A%20Hotspot-Driven%20Peptide%20Design%20via%20Multi-Fragment%20Autoregressive%0A%20%20Extension%0AAuthor%3A%20Jiahan%20Li%20and%20Tong%20Chen%20and%20Shitong%20Luo%20and%20Chaoran%20Cheng%20and%20Jiaqi%20Guan%20and%20Ruihan%20Guo%20and%20Sheng%20Wang%20and%20Ge%20Liu%20and%20Jian%20Peng%20and%20Jianzhu%20Ma%0AAbstract%3A%20%20%20Peptides%2C%20short%20chains%20of%20amino%20acids%2C%20interact%20with%20target%20proteins%2C%20making%0Athem%20a%20unique%20class%20of%20protein-based%20therapeutics%20for%20treating%20human%20diseases.%0ARecently%2C%20deep%20generative%20models%20have%20shown%20great%20promise%20in%20peptide%0Ageneration.%20However%2C%20several%20challenges%20remain%20in%20designing%20effective%20peptide%0Abinders.%20First%2C%20not%20all%20residues%20contribute%20equally%20to%20peptide-target%0Ainteractions.%20Second%2C%20the%20generated%20peptides%20must%20adopt%20valid%20geometries%20due%20to%0Athe%20constraints%20of%20peptide%20bonds.%20Third%2C%20realistic%20tasks%20for%20peptide%20drug%0Adevelopment%20are%20still%20lacking.%20To%20address%20these%20challenges%2C%20we%20introduce%0APepHAR%2C%20a%20hot-spot-driven%20autoregressive%20generative%20model%20for%20designing%0Apeptides%20targeting%20specific%20proteins.%20Building%20on%20the%20observation%20that%20certain%0Ahot%20spot%20residues%20have%20higher%20interaction%20potentials%2C%20we%20first%20use%20an%0Aenergy-based%20density%20model%20to%20fit%20and%20sample%20these%20key%20residues.%20Next%2C%20to%0Aensure%20proper%20peptide%20geometry%2C%20we%20autoregressively%20extend%20peptide%20fragments%20by%0Aestimating%20dihedral%20angles%20between%20residue%20frames.%20Finally%2C%20we%20apply%20an%0Aoptimization%20process%20to%20iteratively%20refine%20fragment%20assembly%2C%20ensuring%20correct%0Apeptide%20structures.%20By%20combining%20hot%20spot%20sampling%20with%20fragment-based%0Aextension%2C%20our%20approach%20enables%20de%20novo%20peptide%20design%20tailored%20to%20a%20target%0Aprotein%20and%20allows%20the%20incorporation%20of%20key%20hot%20spot%20residues%20into%20peptide%0Ascaffolds.%20Extensive%20experiments%2C%20including%20peptide%20design%20and%20peptide%20scaffold%0Ageneration%2C%20demonstrate%20the%20strong%20potential%20of%20PepHAR%20in%20computational%20peptide%0Abinder%20design.%20Source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Ced3-han/PepHAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18463v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHotspot-Driven%2520Peptide%2520Design%2520via%2520Multi-Fragment%2520Autoregressive%250A%2520%2520Extension%26entry.906535625%3DJiahan%2520Li%2520and%2520Tong%2520Chen%2520and%2520Shitong%2520Luo%2520and%2520Chaoran%2520Cheng%2520and%2520Jiaqi%2520Guan%2520and%2520Ruihan%2520Guo%2520and%2520Sheng%2520Wang%2520and%2520Ge%2520Liu%2520and%2520Jian%2520Peng%2520and%2520Jianzhu%2520Ma%26entry.1292438233%3D%2520%2520Peptides%252C%2520short%2520chains%2520of%2520amino%2520acids%252C%2520interact%2520with%2520target%2520proteins%252C%2520making%250Athem%2520a%2520unique%2520class%2520of%2520protein-based%2520therapeutics%2520for%2520treating%2520human%2520diseases.%250ARecently%252C%2520deep%2520generative%2520models%2520have%2520shown%2520great%2520promise%2520in%2520peptide%250Ageneration.%2520However%252C%2520several%2520challenges%2520remain%2520in%2520designing%2520effective%2520peptide%250Abinders.%2520First%252C%2520not%2520all%2520residues%2520contribute%2520equally%2520to%2520peptide-target%250Ainteractions.%2520Second%252C%2520the%2520generated%2520peptides%2520must%2520adopt%2520valid%2520geometries%2520due%2520to%250Athe%2520constraints%2520of%2520peptide%2520bonds.%2520Third%252C%2520realistic%2520tasks%2520for%2520peptide%2520drug%250Adevelopment%2520are%2520still%2520lacking.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250APepHAR%252C%2520a%2520hot-spot-driven%2520autoregressive%2520generative%2520model%2520for%2520designing%250Apeptides%2520targeting%2520specific%2520proteins.%2520Building%2520on%2520the%2520observation%2520that%2520certain%250Ahot%2520spot%2520residues%2520have%2520higher%2520interaction%2520potentials%252C%2520we%2520first%2520use%2520an%250Aenergy-based%2520density%2520model%2520to%2520fit%2520and%2520sample%2520these%2520key%2520residues.%2520Next%252C%2520to%250Aensure%2520proper%2520peptide%2520geometry%252C%2520we%2520autoregressively%2520extend%2520peptide%2520fragments%2520by%250Aestimating%2520dihedral%2520angles%2520between%2520residue%2520frames.%2520Finally%252C%2520we%2520apply%2520an%250Aoptimization%2520process%2520to%2520iteratively%2520refine%2520fragment%2520assembly%252C%2520ensuring%2520correct%250Apeptide%2520structures.%2520By%2520combining%2520hot%2520spot%2520sampling%2520with%2520fragment-based%250Aextension%252C%2520our%2520approach%2520enables%2520de%2520novo%2520peptide%2520design%2520tailored%2520to%2520a%2520target%250Aprotein%2520and%2520allows%2520the%2520incorporation%2520of%2520key%2520hot%2520spot%2520residues%2520into%2520peptide%250Ascaffolds.%2520Extensive%2520experiments%252C%2520including%2520peptide%2520design%2520and%2520peptide%2520scaffold%250Ageneration%252C%2520demonstrate%2520the%2520strong%2520potential%2520of%2520PepHAR%2520in%2520computational%2520peptide%250Abinder%2520design.%2520Source%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Ced3-han/PepHAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18463v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hotspot-Driven%20Peptide%20Design%20via%20Multi-Fragment%20Autoregressive%0A%20%20Extension&entry.906535625=Jiahan%20Li%20and%20Tong%20Chen%20and%20Shitong%20Luo%20and%20Chaoran%20Cheng%20and%20Jiaqi%20Guan%20and%20Ruihan%20Guo%20and%20Sheng%20Wang%20and%20Ge%20Liu%20and%20Jian%20Peng%20and%20Jianzhu%20Ma&entry.1292438233=%20%20Peptides%2C%20short%20chains%20of%20amino%20acids%2C%20interact%20with%20target%20proteins%2C%20making%0Athem%20a%20unique%20class%20of%20protein-based%20therapeutics%20for%20treating%20human%20diseases.%0ARecently%2C%20deep%20generative%20models%20have%20shown%20great%20promise%20in%20peptide%0Ageneration.%20However%2C%20several%20challenges%20remain%20in%20designing%20effective%20peptide%0Abinders.%20First%2C%20not%20all%20residues%20contribute%20equally%20to%20peptide-target%0Ainteractions.%20Second%2C%20the%20generated%20peptides%20must%20adopt%20valid%20geometries%20due%20to%0Athe%20constraints%20of%20peptide%20bonds.%20Third%2C%20realistic%20tasks%20for%20peptide%20drug%0Adevelopment%20are%20still%20lacking.%20To%20address%20these%20challenges%2C%20we%20introduce%0APepHAR%2C%20a%20hot-spot-driven%20autoregressive%20generative%20model%20for%20designing%0Apeptides%20targeting%20specific%20proteins.%20Building%20on%20the%20observation%20that%20certain%0Ahot%20spot%20residues%20have%20higher%20interaction%20potentials%2C%20we%20first%20use%20an%0Aenergy-based%20density%20model%20to%20fit%20and%20sample%20these%20key%20residues.%20Next%2C%20to%0Aensure%20proper%20peptide%20geometry%2C%20we%20autoregressively%20extend%20peptide%20fragments%20by%0Aestimating%20dihedral%20angles%20between%20residue%20frames.%20Finally%2C%20we%20apply%20an%0Aoptimization%20process%20to%20iteratively%20refine%20fragment%20assembly%2C%20ensuring%20correct%0Apeptide%20structures.%20By%20combining%20hot%20spot%20sampling%20with%20fragment-based%0Aextension%2C%20our%20approach%20enables%20de%20novo%20peptide%20design%20tailored%20to%20a%20target%0Aprotein%20and%20allows%20the%20incorporation%20of%20key%20hot%20spot%20residues%20into%20peptide%0Ascaffolds.%20Extensive%20experiments%2C%20including%20peptide%20design%20and%20peptide%20scaffold%0Ageneration%2C%20demonstrate%20the%20strong%20potential%20of%20PepHAR%20in%20computational%20peptide%0Abinder%20design.%20Source%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Ced3-han/PepHAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18463v3&entry.124074799=Read"},
{"title": "Multi-granular body modeling with Redundancy-Free Spatiotemporal Fusion\n  for Text-Driven Motion Generation", "author": "Xingzu Zhan and Chen Xie and Honghang Chen and Haoran Sun and Xiaochun Mai", "abstract": "  Text-to-motion generation sits at the intersection of multimodal learning and\ncomputer graphics and is gaining momentum because it can simplify content\ncreation for games, animation, robotics and virtual reality. Most current\nmethods stack spatial and temporal features in a straightforward way, which\nadds redundancy and still misses subtle joint-level cues. We introduce HiSTF\nMamba, a framework with three parts: Dual-Spatial Mamba, Bi-Temporal Mamba and\na Dynamic Spatiotemporal Fusion Module (DSFM). The Dual-Spatial module runs\npart-based and whole-body models in parallel, capturing both overall\ncoordination and fine-grained joint motion. The Bi-Temporal module scans\nsequences forward and backward to encode short-term details and long-term\ndependencies. DSFM removes redundant temporal information, extracts\ncomplementary cues and fuses them with spatial features to build a richer\nspatiotemporal representation. Experiments on the HumanML3D benchmark show that\nHiSTF Mamba performs well across several metrics, achieving high fidelity and\ntight semantic alignment between text and motion.\n", "link": "http://arxiv.org/abs/2503.06897v2", "date": "2025-05-20", "relevancy": 2.4947, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6643}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5989}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-granular%20body%20modeling%20with%20Redundancy-Free%20Spatiotemporal%20Fusion%0A%20%20for%20Text-Driven%20Motion%20Generation&body=Title%3A%20Multi-granular%20body%20modeling%20with%20Redundancy-Free%20Spatiotemporal%20Fusion%0A%20%20for%20Text-Driven%20Motion%20Generation%0AAuthor%3A%20Xingzu%20Zhan%20and%20Chen%20Xie%20and%20Honghang%20Chen%20and%20Haoran%20Sun%20and%20Xiaochun%20Mai%0AAbstract%3A%20%20%20Text-to-motion%20generation%20sits%20at%20the%20intersection%20of%20multimodal%20learning%20and%0Acomputer%20graphics%20and%20is%20gaining%20momentum%20because%20it%20can%20simplify%20content%0Acreation%20for%20games%2C%20animation%2C%20robotics%20and%20virtual%20reality.%20Most%20current%0Amethods%20stack%20spatial%20and%20temporal%20features%20in%20a%20straightforward%20way%2C%20which%0Aadds%20redundancy%20and%20still%20misses%20subtle%20joint-level%20cues.%20We%20introduce%20HiSTF%0AMamba%2C%20a%20framework%20with%20three%20parts%3A%20Dual-Spatial%20Mamba%2C%20Bi-Temporal%20Mamba%20and%0Aa%20Dynamic%20Spatiotemporal%20Fusion%20Module%20%28DSFM%29.%20The%20Dual-Spatial%20module%20runs%0Apart-based%20and%20whole-body%20models%20in%20parallel%2C%20capturing%20both%20overall%0Acoordination%20and%20fine-grained%20joint%20motion.%20The%20Bi-Temporal%20module%20scans%0Asequences%20forward%20and%20backward%20to%20encode%20short-term%20details%20and%20long-term%0Adependencies.%20DSFM%20removes%20redundant%20temporal%20information%2C%20extracts%0Acomplementary%20cues%20and%20fuses%20them%20with%20spatial%20features%20to%20build%20a%20richer%0Aspatiotemporal%20representation.%20Experiments%20on%20the%20HumanML3D%20benchmark%20show%20that%0AHiSTF%20Mamba%20performs%20well%20across%20several%20metrics%2C%20achieving%20high%20fidelity%20and%0Atight%20semantic%20alignment%20between%20text%20and%20motion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06897v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-granular%2520body%2520modeling%2520with%2520Redundancy-Free%2520Spatiotemporal%2520Fusion%250A%2520%2520for%2520Text-Driven%2520Motion%2520Generation%26entry.906535625%3DXingzu%2520Zhan%2520and%2520Chen%2520Xie%2520and%2520Honghang%2520Chen%2520and%2520Haoran%2520Sun%2520and%2520Xiaochun%2520Mai%26entry.1292438233%3D%2520%2520Text-to-motion%2520generation%2520sits%2520at%2520the%2520intersection%2520of%2520multimodal%2520learning%2520and%250Acomputer%2520graphics%2520and%2520is%2520gaining%2520momentum%2520because%2520it%2520can%2520simplify%2520content%250Acreation%2520for%2520games%252C%2520animation%252C%2520robotics%2520and%2520virtual%2520reality.%2520Most%2520current%250Amethods%2520stack%2520spatial%2520and%2520temporal%2520features%2520in%2520a%2520straightforward%2520way%252C%2520which%250Aadds%2520redundancy%2520and%2520still%2520misses%2520subtle%2520joint-level%2520cues.%2520We%2520introduce%2520HiSTF%250AMamba%252C%2520a%2520framework%2520with%2520three%2520parts%253A%2520Dual-Spatial%2520Mamba%252C%2520Bi-Temporal%2520Mamba%2520and%250Aa%2520Dynamic%2520Spatiotemporal%2520Fusion%2520Module%2520%2528DSFM%2529.%2520The%2520Dual-Spatial%2520module%2520runs%250Apart-based%2520and%2520whole-body%2520models%2520in%2520parallel%252C%2520capturing%2520both%2520overall%250Acoordination%2520and%2520fine-grained%2520joint%2520motion.%2520The%2520Bi-Temporal%2520module%2520scans%250Asequences%2520forward%2520and%2520backward%2520to%2520encode%2520short-term%2520details%2520and%2520long-term%250Adependencies.%2520DSFM%2520removes%2520redundant%2520temporal%2520information%252C%2520extracts%250Acomplementary%2520cues%2520and%2520fuses%2520them%2520with%2520spatial%2520features%2520to%2520build%2520a%2520richer%250Aspatiotemporal%2520representation.%2520Experiments%2520on%2520the%2520HumanML3D%2520benchmark%2520show%2520that%250AHiSTF%2520Mamba%2520performs%2520well%2520across%2520several%2520metrics%252C%2520achieving%2520high%2520fidelity%2520and%250Atight%2520semantic%2520alignment%2520between%2520text%2520and%2520motion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06897v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-granular%20body%20modeling%20with%20Redundancy-Free%20Spatiotemporal%20Fusion%0A%20%20for%20Text-Driven%20Motion%20Generation&entry.906535625=Xingzu%20Zhan%20and%20Chen%20Xie%20and%20Honghang%20Chen%20and%20Haoran%20Sun%20and%20Xiaochun%20Mai&entry.1292438233=%20%20Text-to-motion%20generation%20sits%20at%20the%20intersection%20of%20multimodal%20learning%20and%0Acomputer%20graphics%20and%20is%20gaining%20momentum%20because%20it%20can%20simplify%20content%0Acreation%20for%20games%2C%20animation%2C%20robotics%20and%20virtual%20reality.%20Most%20current%0Amethods%20stack%20spatial%20and%20temporal%20features%20in%20a%20straightforward%20way%2C%20which%0Aadds%20redundancy%20and%20still%20misses%20subtle%20joint-level%20cues.%20We%20introduce%20HiSTF%0AMamba%2C%20a%20framework%20with%20three%20parts%3A%20Dual-Spatial%20Mamba%2C%20Bi-Temporal%20Mamba%20and%0Aa%20Dynamic%20Spatiotemporal%20Fusion%20Module%20%28DSFM%29.%20The%20Dual-Spatial%20module%20runs%0Apart-based%20and%20whole-body%20models%20in%20parallel%2C%20capturing%20both%20overall%0Acoordination%20and%20fine-grained%20joint%20motion.%20The%20Bi-Temporal%20module%20scans%0Asequences%20forward%20and%20backward%20to%20encode%20short-term%20details%20and%20long-term%0Adependencies.%20DSFM%20removes%20redundant%20temporal%20information%2C%20extracts%0Acomplementary%20cues%20and%20fuses%20them%20with%20spatial%20features%20to%20build%20a%20richer%0Aspatiotemporal%20representation.%20Experiments%20on%20the%20HumanML3D%20benchmark%20show%20that%0AHiSTF%20Mamba%20performs%20well%20across%20several%20metrics%2C%20achieving%20high%20fidelity%20and%0Atight%20semantic%20alignment%20between%20text%20and%20motion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06897v2&entry.124074799=Read"},
{"title": "Accuracy and Fairness of Facial Recognition Technology in Low-Quality\n  Police Images: An Experiment With Synthetic Faces", "author": "Maria Cuellar and Hon Kiu and  To and Arush Mehrotra", "abstract": "  Facial recognition technology (FRT) is increasingly used in criminal\ninvestigations, yet most evaluations of its accuracy rely on high-quality\nimages, unlike those often encountered by law enforcement. This study examines\nhow five common forms of image degradation--contrast, brightness, motion blur,\npose shift, and resolution--affect FRT accuracy and fairness across demographic\ngroups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,\nwe simulate degraded images and evaluate performance using Deepface with\nArcFace loss in 1:n identification tasks. We perform an experiment and find\nthat false positive rates peak near baseline image quality, while false\nnegatives increase as degradation intensifies--especially with blur and low\nresolution. Error rates are consistently higher for women and Black\nindividuals, with Black females most affected. These disparities raise concerns\nabout fairness and reliability when FRT is used in real-world investigative\ncontexts. Nevertheless, even under the most challenging conditions and for the\nmost affected subgroups, FRT accuracy remains substantially higher than that of\nmany traditional forensic methods. This suggests that, if appropriately\nvalidated and regulated, FRT should be considered a valuable investigative\ntool. However, algorithmic accuracy alone is not sufficient: we must also\nevaluate how FRT is used in practice, including user-driven data manipulation.\nSuch cases underscore the need for transparency and oversight in FRT deployment\nto ensure both fairness and forensic validity.\n", "link": "http://arxiv.org/abs/2505.14320v1", "date": "2025-05-20", "relevancy": 2.4866, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5124}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.491}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy%20and%20Fairness%20of%20Facial%20Recognition%20Technology%20in%20Low-Quality%0A%20%20Police%20Images%3A%20An%20Experiment%20With%20Synthetic%20Faces&body=Title%3A%20Accuracy%20and%20Fairness%20of%20Facial%20Recognition%20Technology%20in%20Low-Quality%0A%20%20Police%20Images%3A%20An%20Experiment%20With%20Synthetic%20Faces%0AAuthor%3A%20Maria%20Cuellar%20and%20Hon%20Kiu%20and%20%20To%20and%20Arush%20Mehrotra%0AAbstract%3A%20%20%20Facial%20recognition%20technology%20%28FRT%29%20is%20increasingly%20used%20in%20criminal%0Ainvestigations%2C%20yet%20most%20evaluations%20of%20its%20accuracy%20rely%20on%20high-quality%0Aimages%2C%20unlike%20those%20often%20encountered%20by%20law%20enforcement.%20This%20study%20examines%0Ahow%20five%20common%20forms%20of%20image%20degradation--contrast%2C%20brightness%2C%20motion%20blur%2C%0Apose%20shift%2C%20and%20resolution--affect%20FRT%20accuracy%20and%20fairness%20across%20demographic%0Agroups.%20Using%20synthetic%20faces%20generated%20by%20StyleGAN3%20and%20labeled%20with%20FairFace%2C%0Awe%20simulate%20degraded%20images%20and%20evaluate%20performance%20using%20Deepface%20with%0AArcFace%20loss%20in%201%3An%20identification%20tasks.%20We%20perform%20an%20experiment%20and%20find%0Athat%20false%20positive%20rates%20peak%20near%20baseline%20image%20quality%2C%20while%20false%0Anegatives%20increase%20as%20degradation%20intensifies--especially%20with%20blur%20and%20low%0Aresolution.%20Error%20rates%20are%20consistently%20higher%20for%20women%20and%20Black%0Aindividuals%2C%20with%20Black%20females%20most%20affected.%20These%20disparities%20raise%20concerns%0Aabout%20fairness%20and%20reliability%20when%20FRT%20is%20used%20in%20real-world%20investigative%0Acontexts.%20Nevertheless%2C%20even%20under%20the%20most%20challenging%20conditions%20and%20for%20the%0Amost%20affected%20subgroups%2C%20FRT%20accuracy%20remains%20substantially%20higher%20than%20that%20of%0Amany%20traditional%20forensic%20methods.%20This%20suggests%20that%2C%20if%20appropriately%0Avalidated%20and%20regulated%2C%20FRT%20should%20be%20considered%20a%20valuable%20investigative%0Atool.%20However%2C%20algorithmic%20accuracy%20alone%20is%20not%20sufficient%3A%20we%20must%20also%0Aevaluate%20how%20FRT%20is%20used%20in%20practice%2C%20including%20user-driven%20data%20manipulation.%0ASuch%20cases%20underscore%20the%20need%20for%20transparency%20and%20oversight%20in%20FRT%20deployment%0Ato%20ensure%20both%20fairness%20and%20forensic%20validity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy%2520and%2520Fairness%2520of%2520Facial%2520Recognition%2520Technology%2520in%2520Low-Quality%250A%2520%2520Police%2520Images%253A%2520An%2520Experiment%2520With%2520Synthetic%2520Faces%26entry.906535625%3DMaria%2520Cuellar%2520and%2520Hon%2520Kiu%2520and%2520%2520To%2520and%2520Arush%2520Mehrotra%26entry.1292438233%3D%2520%2520Facial%2520recognition%2520technology%2520%2528FRT%2529%2520is%2520increasingly%2520used%2520in%2520criminal%250Ainvestigations%252C%2520yet%2520most%2520evaluations%2520of%2520its%2520accuracy%2520rely%2520on%2520high-quality%250Aimages%252C%2520unlike%2520those%2520often%2520encountered%2520by%2520law%2520enforcement.%2520This%2520study%2520examines%250Ahow%2520five%2520common%2520forms%2520of%2520image%2520degradation--contrast%252C%2520brightness%252C%2520motion%2520blur%252C%250Apose%2520shift%252C%2520and%2520resolution--affect%2520FRT%2520accuracy%2520and%2520fairness%2520across%2520demographic%250Agroups.%2520Using%2520synthetic%2520faces%2520generated%2520by%2520StyleGAN3%2520and%2520labeled%2520with%2520FairFace%252C%250Awe%2520simulate%2520degraded%2520images%2520and%2520evaluate%2520performance%2520using%2520Deepface%2520with%250AArcFace%2520loss%2520in%25201%253An%2520identification%2520tasks.%2520We%2520perform%2520an%2520experiment%2520and%2520find%250Athat%2520false%2520positive%2520rates%2520peak%2520near%2520baseline%2520image%2520quality%252C%2520while%2520false%250Anegatives%2520increase%2520as%2520degradation%2520intensifies--especially%2520with%2520blur%2520and%2520low%250Aresolution.%2520Error%2520rates%2520are%2520consistently%2520higher%2520for%2520women%2520and%2520Black%250Aindividuals%252C%2520with%2520Black%2520females%2520most%2520affected.%2520These%2520disparities%2520raise%2520concerns%250Aabout%2520fairness%2520and%2520reliability%2520when%2520FRT%2520is%2520used%2520in%2520real-world%2520investigative%250Acontexts.%2520Nevertheless%252C%2520even%2520under%2520the%2520most%2520challenging%2520conditions%2520and%2520for%2520the%250Amost%2520affected%2520subgroups%252C%2520FRT%2520accuracy%2520remains%2520substantially%2520higher%2520than%2520that%2520of%250Amany%2520traditional%2520forensic%2520methods.%2520This%2520suggests%2520that%252C%2520if%2520appropriately%250Avalidated%2520and%2520regulated%252C%2520FRT%2520should%2520be%2520considered%2520a%2520valuable%2520investigative%250Atool.%2520However%252C%2520algorithmic%2520accuracy%2520alone%2520is%2520not%2520sufficient%253A%2520we%2520must%2520also%250Aevaluate%2520how%2520FRT%2520is%2520used%2520in%2520practice%252C%2520including%2520user-driven%2520data%2520manipulation.%250ASuch%2520cases%2520underscore%2520the%2520need%2520for%2520transparency%2520and%2520oversight%2520in%2520FRT%2520deployment%250Ato%2520ensure%2520both%2520fairness%2520and%2520forensic%2520validity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy%20and%20Fairness%20of%20Facial%20Recognition%20Technology%20in%20Low-Quality%0A%20%20Police%20Images%3A%20An%20Experiment%20With%20Synthetic%20Faces&entry.906535625=Maria%20Cuellar%20and%20Hon%20Kiu%20and%20%20To%20and%20Arush%20Mehrotra&entry.1292438233=%20%20Facial%20recognition%20technology%20%28FRT%29%20is%20increasingly%20used%20in%20criminal%0Ainvestigations%2C%20yet%20most%20evaluations%20of%20its%20accuracy%20rely%20on%20high-quality%0Aimages%2C%20unlike%20those%20often%20encountered%20by%20law%20enforcement.%20This%20study%20examines%0Ahow%20five%20common%20forms%20of%20image%20degradation--contrast%2C%20brightness%2C%20motion%20blur%2C%0Apose%20shift%2C%20and%20resolution--affect%20FRT%20accuracy%20and%20fairness%20across%20demographic%0Agroups.%20Using%20synthetic%20faces%20generated%20by%20StyleGAN3%20and%20labeled%20with%20FairFace%2C%0Awe%20simulate%20degraded%20images%20and%20evaluate%20performance%20using%20Deepface%20with%0AArcFace%20loss%20in%201%3An%20identification%20tasks.%20We%20perform%20an%20experiment%20and%20find%0Athat%20false%20positive%20rates%20peak%20near%20baseline%20image%20quality%2C%20while%20false%0Anegatives%20increase%20as%20degradation%20intensifies--especially%20with%20blur%20and%20low%0Aresolution.%20Error%20rates%20are%20consistently%20higher%20for%20women%20and%20Black%0Aindividuals%2C%20with%20Black%20females%20most%20affected.%20These%20disparities%20raise%20concerns%0Aabout%20fairness%20and%20reliability%20when%20FRT%20is%20used%20in%20real-world%20investigative%0Acontexts.%20Nevertheless%2C%20even%20under%20the%20most%20challenging%20conditions%20and%20for%20the%0Amost%20affected%20subgroups%2C%20FRT%20accuracy%20remains%20substantially%20higher%20than%20that%20of%0Amany%20traditional%20forensic%20methods.%20This%20suggests%20that%2C%20if%20appropriately%0Avalidated%20and%20regulated%2C%20FRT%20should%20be%20considered%20a%20valuable%20investigative%0Atool.%20However%2C%20algorithmic%20accuracy%20alone%20is%20not%20sufficient%3A%20we%20must%20also%0Aevaluate%20how%20FRT%20is%20used%20in%20practice%2C%20including%20user-driven%20data%20manipulation.%0ASuch%20cases%20underscore%20the%20need%20for%20transparency%20and%20oversight%20in%20FRT%20deployment%0Ato%20ensure%20both%20fairness%20and%20forensic%20validity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14320v1&entry.124074799=Read"},
{"title": "When Do LLMs Help With Node Classification? A Comprehensive Analysis", "author": "Xixi Wu and Yifei Shen and Fangzhou Ge and Caihua Shan and Yizhu Jiao and Xiangguo Sun and Hong Cheng", "abstract": "  Node classification is a fundamental task in graph analysis, with broad\napplications across various fields. Recent breakthroughs in Large Language\nModels (LLMs) have enabled LLM-based approaches for this task. Although many\nstudies demonstrate the impressive performance of LLM-based methods, the lack\nof clear design guidelines may hinder their practical application. In this\nwork, we aim to establish such guidelines through a fair and systematic\ncomparison of these algorithms. As a first step, we developed LLMNodeBed, a\ncomprehensive codebase and testbed for node classification using LLMs. It\nincludes 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based\nalgorithms, 8 classic baselines, and 3 learning paradigms. Subsequently, we\nconducted extensive experiments, training and evaluating over 2,700 models, to\ndetermine the key settings (e.g., learning paradigms and homophily) and\ncomponents (e.g., model size and prompt) that affect performance. Our findings\nuncover 8 insights, e.g., (1) LLM-based methods can significantly outperform\ntraditional methods in a semi-supervised setting, while the advantage is\nmarginal in a supervised setting; (2) Graph Foundation Models can beat\nopen-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot\nsetting. We hope that the release of LLMNodeBed, along with our insights, will\nfacilitate reproducible research and inspire future studies in this field.\nCodes and datasets are released at\n\\href{https://llmnodebed.github.io/}{\\texttt{https://llmnodebed.github.io/}}.\n", "link": "http://arxiv.org/abs/2502.00829v2", "date": "2025-05-20", "relevancy": 2.4729, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Do%20LLMs%20Help%20With%20Node%20Classification%3F%20A%20Comprehensive%20Analysis&body=Title%3A%20When%20Do%20LLMs%20Help%20With%20Node%20Classification%3F%20A%20Comprehensive%20Analysis%0AAuthor%3A%20Xixi%20Wu%20and%20Yifei%20Shen%20and%20Fangzhou%20Ge%20and%20Caihua%20Shan%20and%20Yizhu%20Jiao%20and%20Xiangguo%20Sun%20and%20Hong%20Cheng%0AAbstract%3A%20%20%20Node%20classification%20is%20a%20fundamental%20task%20in%20graph%20analysis%2C%20with%20broad%0Aapplications%20across%20various%20fields.%20Recent%20breakthroughs%20in%20Large%20Language%0AModels%20%28LLMs%29%20have%20enabled%20LLM-based%20approaches%20for%20this%20task.%20Although%20many%0Astudies%20demonstrate%20the%20impressive%20performance%20of%20LLM-based%20methods%2C%20the%20lack%0Aof%20clear%20design%20guidelines%20may%20hinder%20their%20practical%20application.%20In%20this%0Awork%2C%20we%20aim%20to%20establish%20such%20guidelines%20through%20a%20fair%20and%20systematic%0Acomparison%20of%20these%20algorithms.%20As%20a%20first%20step%2C%20we%20developed%20LLMNodeBed%2C%20a%0Acomprehensive%20codebase%20and%20testbed%20for%20node%20classification%20using%20LLMs.%20It%0Aincludes%2010%20homophilic%20datasets%2C%204%20heterophilic%20datasets%2C%208%20LLM-based%0Aalgorithms%2C%208%20classic%20baselines%2C%20and%203%20learning%20paradigms.%20Subsequently%2C%20we%0Aconducted%20extensive%20experiments%2C%20training%20and%20evaluating%20over%202%2C700%20models%2C%20to%0Adetermine%20the%20key%20settings%20%28e.g.%2C%20learning%20paradigms%20and%20homophily%29%20and%0Acomponents%20%28e.g.%2C%20model%20size%20and%20prompt%29%20that%20affect%20performance.%20Our%20findings%0Auncover%208%20insights%2C%20e.g.%2C%20%281%29%20LLM-based%20methods%20can%20significantly%20outperform%0Atraditional%20methods%20in%20a%20semi-supervised%20setting%2C%20while%20the%20advantage%20is%0Amarginal%20in%20a%20supervised%20setting%3B%20%282%29%20Graph%20Foundation%20Models%20can%20beat%0Aopen-source%20LLMs%20but%20still%20fall%20short%20of%20strong%20LLMs%20like%20GPT-4o%20in%20a%20zero-shot%0Asetting.%20We%20hope%20that%20the%20release%20of%20LLMNodeBed%2C%20along%20with%20our%20insights%2C%20will%0Afacilitate%20reproducible%20research%20and%20inspire%20future%20studies%20in%20this%20field.%0ACodes%20and%20datasets%20are%20released%20at%0A%5Chref%7Bhttps%3A//llmnodebed.github.io/%7D%7B%5Ctexttt%7Bhttps%3A//llmnodebed.github.io/%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Do%2520LLMs%2520Help%2520With%2520Node%2520Classification%253F%2520A%2520Comprehensive%2520Analysis%26entry.906535625%3DXixi%2520Wu%2520and%2520Yifei%2520Shen%2520and%2520Fangzhou%2520Ge%2520and%2520Caihua%2520Shan%2520and%2520Yizhu%2520Jiao%2520and%2520Xiangguo%2520Sun%2520and%2520Hong%2520Cheng%26entry.1292438233%3D%2520%2520Node%2520classification%2520is%2520a%2520fundamental%2520task%2520in%2520graph%2520analysis%252C%2520with%2520broad%250Aapplications%2520across%2520various%2520fields.%2520Recent%2520breakthroughs%2520in%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520have%2520enabled%2520LLM-based%2520approaches%2520for%2520this%2520task.%2520Although%2520many%250Astudies%2520demonstrate%2520the%2520impressive%2520performance%2520of%2520LLM-based%2520methods%252C%2520the%2520lack%250Aof%2520clear%2520design%2520guidelines%2520may%2520hinder%2520their%2520practical%2520application.%2520In%2520this%250Awork%252C%2520we%2520aim%2520to%2520establish%2520such%2520guidelines%2520through%2520a%2520fair%2520and%2520systematic%250Acomparison%2520of%2520these%2520algorithms.%2520As%2520a%2520first%2520step%252C%2520we%2520developed%2520LLMNodeBed%252C%2520a%250Acomprehensive%2520codebase%2520and%2520testbed%2520for%2520node%2520classification%2520using%2520LLMs.%2520It%250Aincludes%252010%2520homophilic%2520datasets%252C%25204%2520heterophilic%2520datasets%252C%25208%2520LLM-based%250Aalgorithms%252C%25208%2520classic%2520baselines%252C%2520and%25203%2520learning%2520paradigms.%2520Subsequently%252C%2520we%250Aconducted%2520extensive%2520experiments%252C%2520training%2520and%2520evaluating%2520over%25202%252C700%2520models%252C%2520to%250Adetermine%2520the%2520key%2520settings%2520%2528e.g.%252C%2520learning%2520paradigms%2520and%2520homophily%2529%2520and%250Acomponents%2520%2528e.g.%252C%2520model%2520size%2520and%2520prompt%2529%2520that%2520affect%2520performance.%2520Our%2520findings%250Auncover%25208%2520insights%252C%2520e.g.%252C%2520%25281%2529%2520LLM-based%2520methods%2520can%2520significantly%2520outperform%250Atraditional%2520methods%2520in%2520a%2520semi-supervised%2520setting%252C%2520while%2520the%2520advantage%2520is%250Amarginal%2520in%2520a%2520supervised%2520setting%253B%2520%25282%2529%2520Graph%2520Foundation%2520Models%2520can%2520beat%250Aopen-source%2520LLMs%2520but%2520still%2520fall%2520short%2520of%2520strong%2520LLMs%2520like%2520GPT-4o%2520in%2520a%2520zero-shot%250Asetting.%2520We%2520hope%2520that%2520the%2520release%2520of%2520LLMNodeBed%252C%2520along%2520with%2520our%2520insights%252C%2520will%250Afacilitate%2520reproducible%2520research%2520and%2520inspire%2520future%2520studies%2520in%2520this%2520field.%250ACodes%2520and%2520datasets%2520are%2520released%2520at%250A%255Chref%257Bhttps%253A//llmnodebed.github.io/%257D%257B%255Ctexttt%257Bhttps%253A//llmnodebed.github.io/%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Do%20LLMs%20Help%20With%20Node%20Classification%3F%20A%20Comprehensive%20Analysis&entry.906535625=Xixi%20Wu%20and%20Yifei%20Shen%20and%20Fangzhou%20Ge%20and%20Caihua%20Shan%20and%20Yizhu%20Jiao%20and%20Xiangguo%20Sun%20and%20Hong%20Cheng&entry.1292438233=%20%20Node%20classification%20is%20a%20fundamental%20task%20in%20graph%20analysis%2C%20with%20broad%0Aapplications%20across%20various%20fields.%20Recent%20breakthroughs%20in%20Large%20Language%0AModels%20%28LLMs%29%20have%20enabled%20LLM-based%20approaches%20for%20this%20task.%20Although%20many%0Astudies%20demonstrate%20the%20impressive%20performance%20of%20LLM-based%20methods%2C%20the%20lack%0Aof%20clear%20design%20guidelines%20may%20hinder%20their%20practical%20application.%20In%20this%0Awork%2C%20we%20aim%20to%20establish%20such%20guidelines%20through%20a%20fair%20and%20systematic%0Acomparison%20of%20these%20algorithms.%20As%20a%20first%20step%2C%20we%20developed%20LLMNodeBed%2C%20a%0Acomprehensive%20codebase%20and%20testbed%20for%20node%20classification%20using%20LLMs.%20It%0Aincludes%2010%20homophilic%20datasets%2C%204%20heterophilic%20datasets%2C%208%20LLM-based%0Aalgorithms%2C%208%20classic%20baselines%2C%20and%203%20learning%20paradigms.%20Subsequently%2C%20we%0Aconducted%20extensive%20experiments%2C%20training%20and%20evaluating%20over%202%2C700%20models%2C%20to%0Adetermine%20the%20key%20settings%20%28e.g.%2C%20learning%20paradigms%20and%20homophily%29%20and%0Acomponents%20%28e.g.%2C%20model%20size%20and%20prompt%29%20that%20affect%20performance.%20Our%20findings%0Auncover%208%20insights%2C%20e.g.%2C%20%281%29%20LLM-based%20methods%20can%20significantly%20outperform%0Atraditional%20methods%20in%20a%20semi-supervised%20setting%2C%20while%20the%20advantage%20is%0Amarginal%20in%20a%20supervised%20setting%3B%20%282%29%20Graph%20Foundation%20Models%20can%20beat%0Aopen-source%20LLMs%20but%20still%20fall%20short%20of%20strong%20LLMs%20like%20GPT-4o%20in%20a%20zero-shot%0Asetting.%20We%20hope%20that%20the%20release%20of%20LLMNodeBed%2C%20along%20with%20our%20insights%2C%20will%0Afacilitate%20reproducible%20research%20and%20inspire%20future%20studies%20in%20this%20field.%0ACodes%20and%20datasets%20are%20released%20at%0A%5Chref%7Bhttps%3A//llmnodebed.github.io/%7D%7B%5Ctexttt%7Bhttps%3A//llmnodebed.github.io/%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00829v2&entry.124074799=Read"},
{"title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models", "author": "Siqiao Huang and Jialong Wu and Qixing Zhou and Shangchen Miao and Mingsheng Long", "abstract": "  World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models.\n", "link": "http://arxiv.org/abs/2505.14357v1", "date": "2025-05-20", "relevancy": 2.4715, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6227}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6221}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vid2World%3A%20Crafting%20Video%20Diffusion%20Models%20to%20Interactive%20World%20Models&body=Title%3A%20Vid2World%3A%20Crafting%20Video%20Diffusion%20Models%20to%20Interactive%20World%20Models%0AAuthor%3A%20Siqiao%20Huang%20and%20Jialong%20Wu%20and%20Qixing%20Zhou%20and%20Shangchen%20Miao%20and%20Mingsheng%20Long%0AAbstract%3A%20%20%20World%20models%2C%20which%20predict%20transitions%20based%20on%20history%20observation%20and%0Aaction%20sequences%2C%20have%20shown%20great%20promise%20in%20improving%20data%20efficiency%20for%0Asequential%20decision%20making.%20However%2C%20existing%20world%20models%20often%20require%0Aextensive%20domain-specific%20training%20and%20still%20produce%20low-fidelity%2C%20coarse%0Apredictions%2C%20limiting%20their%20applicability%20in%20complex%20environments.%20In%20contrast%2C%0Avideo%20diffusion%20models%20trained%20on%20large%2C%20internet-scale%20datasets%20have%0Ademonstrated%20impressive%20capabilities%20in%20generating%20high-quality%20videos%20that%0Acapture%20diverse%20real-world%20dynamics.%20In%20this%20work%2C%20we%20present%20Vid2World%2C%20a%0Ageneral%20approach%20for%20leveraging%20and%20transferring%20pre-trained%20video%20diffusion%0Amodels%20into%20interactive%20world%20models.%20To%20bridge%20the%20gap%2C%20Vid2World%20performs%0Acasualization%20of%20a%20pre-trained%20video%20diffusion%20model%20by%20crafting%20its%0Aarchitecture%20and%20training%20objective%20to%20enable%20autoregressive%20generation.%0AFurthermore%2C%20it%20introduces%20a%20causal%20action%20guidance%20mechanism%20to%20enhance%20action%0Acontrollability%20in%20the%20resulting%20interactive%20world%20model.%20Extensive%20experiments%0Ain%20robot%20manipulation%20and%20game%20simulation%20domains%20show%20that%20our%20method%20offers%20a%0Ascalable%20and%20effective%20approach%20for%20repurposing%20highly%20capable%20video%20diffusion%0Amodels%20to%20interactive%20world%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVid2World%253A%2520Crafting%2520Video%2520Diffusion%2520Models%2520to%2520Interactive%2520World%2520Models%26entry.906535625%3DSiqiao%2520Huang%2520and%2520Jialong%2520Wu%2520and%2520Qixing%2520Zhou%2520and%2520Shangchen%2520Miao%2520and%2520Mingsheng%2520Long%26entry.1292438233%3D%2520%2520World%2520models%252C%2520which%2520predict%2520transitions%2520based%2520on%2520history%2520observation%2520and%250Aaction%2520sequences%252C%2520have%2520shown%2520great%2520promise%2520in%2520improving%2520data%2520efficiency%2520for%250Asequential%2520decision%2520making.%2520However%252C%2520existing%2520world%2520models%2520often%2520require%250Aextensive%2520domain-specific%2520training%2520and%2520still%2520produce%2520low-fidelity%252C%2520coarse%250Apredictions%252C%2520limiting%2520their%2520applicability%2520in%2520complex%2520environments.%2520In%2520contrast%252C%250Avideo%2520diffusion%2520models%2520trained%2520on%2520large%252C%2520internet-scale%2520datasets%2520have%250Ademonstrated%2520impressive%2520capabilities%2520in%2520generating%2520high-quality%2520videos%2520that%250Acapture%2520diverse%2520real-world%2520dynamics.%2520In%2520this%2520work%252C%2520we%2520present%2520Vid2World%252C%2520a%250Ageneral%2520approach%2520for%2520leveraging%2520and%2520transferring%2520pre-trained%2520video%2520diffusion%250Amodels%2520into%2520interactive%2520world%2520models.%2520To%2520bridge%2520the%2520gap%252C%2520Vid2World%2520performs%250Acasualization%2520of%2520a%2520pre-trained%2520video%2520diffusion%2520model%2520by%2520crafting%2520its%250Aarchitecture%2520and%2520training%2520objective%2520to%2520enable%2520autoregressive%2520generation.%250AFurthermore%252C%2520it%2520introduces%2520a%2520causal%2520action%2520guidance%2520mechanism%2520to%2520enhance%2520action%250Acontrollability%2520in%2520the%2520resulting%2520interactive%2520world%2520model.%2520Extensive%2520experiments%250Ain%2520robot%2520manipulation%2520and%2520game%2520simulation%2520domains%2520show%2520that%2520our%2520method%2520offers%2520a%250Ascalable%2520and%2520effective%2520approach%2520for%2520repurposing%2520highly%2520capable%2520video%2520diffusion%250Amodels%2520to%2520interactive%2520world%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vid2World%3A%20Crafting%20Video%20Diffusion%20Models%20to%20Interactive%20World%20Models&entry.906535625=Siqiao%20Huang%20and%20Jialong%20Wu%20and%20Qixing%20Zhou%20and%20Shangchen%20Miao%20and%20Mingsheng%20Long&entry.1292438233=%20%20World%20models%2C%20which%20predict%20transitions%20based%20on%20history%20observation%20and%0Aaction%20sequences%2C%20have%20shown%20great%20promise%20in%20improving%20data%20efficiency%20for%0Asequential%20decision%20making.%20However%2C%20existing%20world%20models%20often%20require%0Aextensive%20domain-specific%20training%20and%20still%20produce%20low-fidelity%2C%20coarse%0Apredictions%2C%20limiting%20their%20applicability%20in%20complex%20environments.%20In%20contrast%2C%0Avideo%20diffusion%20models%20trained%20on%20large%2C%20internet-scale%20datasets%20have%0Ademonstrated%20impressive%20capabilities%20in%20generating%20high-quality%20videos%20that%0Acapture%20diverse%20real-world%20dynamics.%20In%20this%20work%2C%20we%20present%20Vid2World%2C%20a%0Ageneral%20approach%20for%20leveraging%20and%20transferring%20pre-trained%20video%20diffusion%0Amodels%20into%20interactive%20world%20models.%20To%20bridge%20the%20gap%2C%20Vid2World%20performs%0Acasualization%20of%20a%20pre-trained%20video%20diffusion%20model%20by%20crafting%20its%0Aarchitecture%20and%20training%20objective%20to%20enable%20autoregressive%20generation.%0AFurthermore%2C%20it%20introduces%20a%20causal%20action%20guidance%20mechanism%20to%20enhance%20action%0Acontrollability%20in%20the%20resulting%20interactive%20world%20model.%20Extensive%20experiments%0Ain%20robot%20manipulation%20and%20game%20simulation%20domains%20show%20that%20our%20method%20offers%20a%0Ascalable%20and%20effective%20approach%20for%20repurposing%20highly%20capable%20video%20diffusion%0Amodels%20to%20interactive%20world%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14357v1&entry.124074799=Read"},
{"title": "Technical Report: Quantifying and Analyzing the Generalization Power of\n  a DNN", "author": "Yuxuan He and Junpeng Zhang and Lei Cheng and Hongyuan Zhang and Quanshi Zhang", "abstract": "  This paper proposes a new perspective for analyzing the generalization power\nof deep neural networks (DNNs), i.e., directly disentangling and analyzing the\ndynamics of generalizable and non-generalizable interaction encoded by a DNN\nthrough the training process. Specifically, this work builds upon the recent\ntheoretical achievement in explainble AI, which proves that the detailed\ninference logic of DNNs can be can be strictly rewritten as a small number of\nAND-OR interaction patterns. Based on this, we propose an efficient method to\nquantify the generalization power of each interaction, and we discover a\ndistinct three-phase dynamics of the generalization power of interactions\nduring training. In particular, the early phase of training typically removes\nnoisy and non-generalizable interactions and learns simple and generalizable\nones. The second and the third phases tend to capture increasingly complex\ninteractions that are harder to generalize. Experimental results verify that\nthe learning of non-generalizable interactions is the the direct cause for the\ngap between the training and testing losses.\n", "link": "http://arxiv.org/abs/2505.06993v2", "date": "2025-05-20", "relevancy": 2.461, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5322}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4745}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Technical%20Report%3A%20Quantifying%20and%20Analyzing%20the%20Generalization%20Power%20of%0A%20%20a%20DNN&body=Title%3A%20Technical%20Report%3A%20Quantifying%20and%20Analyzing%20the%20Generalization%20Power%20of%0A%20%20a%20DNN%0AAuthor%3A%20Yuxuan%20He%20and%20Junpeng%20Zhang%20and%20Lei%20Cheng%20and%20Hongyuan%20Zhang%20and%20Quanshi%20Zhang%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20new%20perspective%20for%20analyzing%20the%20generalization%20power%0Aof%20deep%20neural%20networks%20%28DNNs%29%2C%20i.e.%2C%20directly%20disentangling%20and%20analyzing%20the%0Adynamics%20of%20generalizable%20and%20non-generalizable%20interaction%20encoded%20by%20a%20DNN%0Athrough%20the%20training%20process.%20Specifically%2C%20this%20work%20builds%20upon%20the%20recent%0Atheoretical%20achievement%20in%20explainble%20AI%2C%20which%20proves%20that%20the%20detailed%0Ainference%20logic%20of%20DNNs%20can%20be%20can%20be%20strictly%20rewritten%20as%20a%20small%20number%20of%0AAND-OR%20interaction%20patterns.%20Based%20on%20this%2C%20we%20propose%20an%20efficient%20method%20to%0Aquantify%20the%20generalization%20power%20of%20each%20interaction%2C%20and%20we%20discover%20a%0Adistinct%20three-phase%20dynamics%20of%20the%20generalization%20power%20of%20interactions%0Aduring%20training.%20In%20particular%2C%20the%20early%20phase%20of%20training%20typically%20removes%0Anoisy%20and%20non-generalizable%20interactions%20and%20learns%20simple%20and%20generalizable%0Aones.%20The%20second%20and%20the%20third%20phases%20tend%20to%20capture%20increasingly%20complex%0Ainteractions%20that%20are%20harder%20to%20generalize.%20Experimental%20results%20verify%20that%0Athe%20learning%20of%20non-generalizable%20interactions%20is%20the%20the%20direct%20cause%20for%20the%0Agap%20between%20the%20training%20and%20testing%20losses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06993v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTechnical%2520Report%253A%2520Quantifying%2520and%2520Analyzing%2520the%2520Generalization%2520Power%2520of%250A%2520%2520a%2520DNN%26entry.906535625%3DYuxuan%2520He%2520and%2520Junpeng%2520Zhang%2520and%2520Lei%2520Cheng%2520and%2520Hongyuan%2520Zhang%2520and%2520Quanshi%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520new%2520perspective%2520for%2520analyzing%2520the%2520generalization%2520power%250Aof%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520i.e.%252C%2520directly%2520disentangling%2520and%2520analyzing%2520the%250Adynamics%2520of%2520generalizable%2520and%2520non-generalizable%2520interaction%2520encoded%2520by%2520a%2520DNN%250Athrough%2520the%2520training%2520process.%2520Specifically%252C%2520this%2520work%2520builds%2520upon%2520the%2520recent%250Atheoretical%2520achievement%2520in%2520explainble%2520AI%252C%2520which%2520proves%2520that%2520the%2520detailed%250Ainference%2520logic%2520of%2520DNNs%2520can%2520be%2520can%2520be%2520strictly%2520rewritten%2520as%2520a%2520small%2520number%2520of%250AAND-OR%2520interaction%2520patterns.%2520Based%2520on%2520this%252C%2520we%2520propose%2520an%2520efficient%2520method%2520to%250Aquantify%2520the%2520generalization%2520power%2520of%2520each%2520interaction%252C%2520and%2520we%2520discover%2520a%250Adistinct%2520three-phase%2520dynamics%2520of%2520the%2520generalization%2520power%2520of%2520interactions%250Aduring%2520training.%2520In%2520particular%252C%2520the%2520early%2520phase%2520of%2520training%2520typically%2520removes%250Anoisy%2520and%2520non-generalizable%2520interactions%2520and%2520learns%2520simple%2520and%2520generalizable%250Aones.%2520The%2520second%2520and%2520the%2520third%2520phases%2520tend%2520to%2520capture%2520increasingly%2520complex%250Ainteractions%2520that%2520are%2520harder%2520to%2520generalize.%2520Experimental%2520results%2520verify%2520that%250Athe%2520learning%2520of%2520non-generalizable%2520interactions%2520is%2520the%2520the%2520direct%2520cause%2520for%2520the%250Agap%2520between%2520the%2520training%2520and%2520testing%2520losses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06993v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Technical%20Report%3A%20Quantifying%20and%20Analyzing%20the%20Generalization%20Power%20of%0A%20%20a%20DNN&entry.906535625=Yuxuan%20He%20and%20Junpeng%20Zhang%20and%20Lei%20Cheng%20and%20Hongyuan%20Zhang%20and%20Quanshi%20Zhang&entry.1292438233=%20%20This%20paper%20proposes%20a%20new%20perspective%20for%20analyzing%20the%20generalization%20power%0Aof%20deep%20neural%20networks%20%28DNNs%29%2C%20i.e.%2C%20directly%20disentangling%20and%20analyzing%20the%0Adynamics%20of%20generalizable%20and%20non-generalizable%20interaction%20encoded%20by%20a%20DNN%0Athrough%20the%20training%20process.%20Specifically%2C%20this%20work%20builds%20upon%20the%20recent%0Atheoretical%20achievement%20in%20explainble%20AI%2C%20which%20proves%20that%20the%20detailed%0Ainference%20logic%20of%20DNNs%20can%20be%20can%20be%20strictly%20rewritten%20as%20a%20small%20number%20of%0AAND-OR%20interaction%20patterns.%20Based%20on%20this%2C%20we%20propose%20an%20efficient%20method%20to%0Aquantify%20the%20generalization%20power%20of%20each%20interaction%2C%20and%20we%20discover%20a%0Adistinct%20three-phase%20dynamics%20of%20the%20generalization%20power%20of%20interactions%0Aduring%20training.%20In%20particular%2C%20the%20early%20phase%20of%20training%20typically%20removes%0Anoisy%20and%20non-generalizable%20interactions%20and%20learns%20simple%20and%20generalizable%0Aones.%20The%20second%20and%20the%20third%20phases%20tend%20to%20capture%20increasingly%20complex%0Ainteractions%20that%20are%20harder%20to%20generalize.%20Experimental%20results%20verify%20that%0Athe%20learning%20of%20non-generalizable%20interactions%20is%20the%20the%20direct%20cause%20for%20the%0Agap%20between%20the%20training%20and%20testing%20losses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06993v2&entry.124074799=Read"},
{"title": "Mechanistic Fine-tuning for In-context Learning", "author": "Hakaze Cho and Peng Luo and Mariko Kato and Rin Kaenbyou and Naoya Inoue", "abstract": "  In-context Learning (ICL) utilizes structured demonstration-query inputs to\ninduce few-shot learning on Language Models (LMs), which are not originally\npre-trained on ICL-style data. To bridge the gap between ICL and pre-training,\nsome approaches fine-tune LMs on large ICL-style datasets by an end-to-end\nparadigm with massive computational costs. To reduce such costs, in this paper,\nwe propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous\nfindings on the inner mechanism of ICL, building training objectives on the\nattention scores instead of the final outputs, to force the attention scores to\nfocus on the correct label tokens presented in the context and mitigate\nattention scores from the wrong label tokens. Our experiments on 9 modern LMs\nand 8 datasets empirically find that ABFT outperforms in performance,\nrobustness, unbiasedness, and efficiency, with only around 0.01% data cost\ncompared to the previous methods. Moreover, our subsequent analysis finds that\nthe end-to-end training objective contains the ABFT objective, suggesting the\nimplicit bias of ICL-style data to the emergence of induction heads. Our work\ndemonstrates the possibility of controlling specific module sequences within\nLMs to improve their behavior, opening up the future application of mechanistic\ninterpretability.\n", "link": "http://arxiv.org/abs/2505.14233v1", "date": "2025-05-20", "relevancy": 2.4526, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanistic%20Fine-tuning%20for%20In-context%20Learning&body=Title%3A%20Mechanistic%20Fine-tuning%20for%20In-context%20Learning%0AAuthor%3A%20Hakaze%20Cho%20and%20Peng%20Luo%20and%20Mariko%20Kato%20and%20Rin%20Kaenbyou%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20In-context%20Learning%20%28ICL%29%20utilizes%20structured%20demonstration-query%20inputs%20to%0Ainduce%20few-shot%20learning%20on%20Language%20Models%20%28LMs%29%2C%20which%20are%20not%20originally%0Apre-trained%20on%20ICL-style%20data.%20To%20bridge%20the%20gap%20between%20ICL%20and%20pre-training%2C%0Asome%20approaches%20fine-tune%20LMs%20on%20large%20ICL-style%20datasets%20by%20an%20end-to-end%0Aparadigm%20with%20massive%20computational%20costs.%20To%20reduce%20such%20costs%2C%20in%20this%20paper%2C%0Awe%20propose%20Attention%20Behavior%20Fine-Tuning%20%28ABFT%29%2C%20utilizing%20the%20previous%0Afindings%20on%20the%20inner%20mechanism%20of%20ICL%2C%20building%20training%20objectives%20on%20the%0Aattention%20scores%20instead%20of%20the%20final%20outputs%2C%20to%20force%20the%20attention%20scores%20to%0Afocus%20on%20the%20correct%20label%20tokens%20presented%20in%20the%20context%20and%20mitigate%0Aattention%20scores%20from%20the%20wrong%20label%20tokens.%20Our%20experiments%20on%209%20modern%20LMs%0Aand%208%20datasets%20empirically%20find%20that%20ABFT%20outperforms%20in%20performance%2C%0Arobustness%2C%20unbiasedness%2C%20and%20efficiency%2C%20with%20only%20around%200.01%25%20data%20cost%0Acompared%20to%20the%20previous%20methods.%20Moreover%2C%20our%20subsequent%20analysis%20finds%20that%0Athe%20end-to-end%20training%20objective%20contains%20the%20ABFT%20objective%2C%20suggesting%20the%0Aimplicit%20bias%20of%20ICL-style%20data%20to%20the%20emergence%20of%20induction%20heads.%20Our%20work%0Ademonstrates%20the%20possibility%20of%20controlling%20specific%20module%20sequences%20within%0ALMs%20to%20improve%20their%20behavior%2C%20opening%20up%20the%20future%20application%20of%20mechanistic%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14233v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanistic%2520Fine-tuning%2520for%2520In-context%2520Learning%26entry.906535625%3DHakaze%2520Cho%2520and%2520Peng%2520Luo%2520and%2520Mariko%2520Kato%2520and%2520Rin%2520Kaenbyou%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520In-context%2520Learning%2520%2528ICL%2529%2520utilizes%2520structured%2520demonstration-query%2520inputs%2520to%250Ainduce%2520few-shot%2520learning%2520on%2520Language%2520Models%2520%2528LMs%2529%252C%2520which%2520are%2520not%2520originally%250Apre-trained%2520on%2520ICL-style%2520data.%2520To%2520bridge%2520the%2520gap%2520between%2520ICL%2520and%2520pre-training%252C%250Asome%2520approaches%2520fine-tune%2520LMs%2520on%2520large%2520ICL-style%2520datasets%2520by%2520an%2520end-to-end%250Aparadigm%2520with%2520massive%2520computational%2520costs.%2520To%2520reduce%2520such%2520costs%252C%2520in%2520this%2520paper%252C%250Awe%2520propose%2520Attention%2520Behavior%2520Fine-Tuning%2520%2528ABFT%2529%252C%2520utilizing%2520the%2520previous%250Afindings%2520on%2520the%2520inner%2520mechanism%2520of%2520ICL%252C%2520building%2520training%2520objectives%2520on%2520the%250Aattention%2520scores%2520instead%2520of%2520the%2520final%2520outputs%252C%2520to%2520force%2520the%2520attention%2520scores%2520to%250Afocus%2520on%2520the%2520correct%2520label%2520tokens%2520presented%2520in%2520the%2520context%2520and%2520mitigate%250Aattention%2520scores%2520from%2520the%2520wrong%2520label%2520tokens.%2520Our%2520experiments%2520on%25209%2520modern%2520LMs%250Aand%25208%2520datasets%2520empirically%2520find%2520that%2520ABFT%2520outperforms%2520in%2520performance%252C%250Arobustness%252C%2520unbiasedness%252C%2520and%2520efficiency%252C%2520with%2520only%2520around%25200.01%2525%2520data%2520cost%250Acompared%2520to%2520the%2520previous%2520methods.%2520Moreover%252C%2520our%2520subsequent%2520analysis%2520finds%2520that%250Athe%2520end-to-end%2520training%2520objective%2520contains%2520the%2520ABFT%2520objective%252C%2520suggesting%2520the%250Aimplicit%2520bias%2520of%2520ICL-style%2520data%2520to%2520the%2520emergence%2520of%2520induction%2520heads.%2520Our%2520work%250Ademonstrates%2520the%2520possibility%2520of%2520controlling%2520specific%2520module%2520sequences%2520within%250ALMs%2520to%2520improve%2520their%2520behavior%252C%2520opening%2520up%2520the%2520future%2520application%2520of%2520mechanistic%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14233v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanistic%20Fine-tuning%20for%20In-context%20Learning&entry.906535625=Hakaze%20Cho%20and%20Peng%20Luo%20and%20Mariko%20Kato%20and%20Rin%20Kaenbyou%20and%20Naoya%20Inoue&entry.1292438233=%20%20In-context%20Learning%20%28ICL%29%20utilizes%20structured%20demonstration-query%20inputs%20to%0Ainduce%20few-shot%20learning%20on%20Language%20Models%20%28LMs%29%2C%20which%20are%20not%20originally%0Apre-trained%20on%20ICL-style%20data.%20To%20bridge%20the%20gap%20between%20ICL%20and%20pre-training%2C%0Asome%20approaches%20fine-tune%20LMs%20on%20large%20ICL-style%20datasets%20by%20an%20end-to-end%0Aparadigm%20with%20massive%20computational%20costs.%20To%20reduce%20such%20costs%2C%20in%20this%20paper%2C%0Awe%20propose%20Attention%20Behavior%20Fine-Tuning%20%28ABFT%29%2C%20utilizing%20the%20previous%0Afindings%20on%20the%20inner%20mechanism%20of%20ICL%2C%20building%20training%20objectives%20on%20the%0Aattention%20scores%20instead%20of%20the%20final%20outputs%2C%20to%20force%20the%20attention%20scores%20to%0Afocus%20on%20the%20correct%20label%20tokens%20presented%20in%20the%20context%20and%20mitigate%0Aattention%20scores%20from%20the%20wrong%20label%20tokens.%20Our%20experiments%20on%209%20modern%20LMs%0Aand%208%20datasets%20empirically%20find%20that%20ABFT%20outperforms%20in%20performance%2C%0Arobustness%2C%20unbiasedness%2C%20and%20efficiency%2C%20with%20only%20around%200.01%25%20data%20cost%0Acompared%20to%20the%20previous%20methods.%20Moreover%2C%20our%20subsequent%20analysis%20finds%20that%0Athe%20end-to-end%20training%20objective%20contains%20the%20ABFT%20objective%2C%20suggesting%20the%0Aimplicit%20bias%20of%20ICL-style%20data%20to%20the%20emergence%20of%20induction%20heads.%20Our%20work%0Ademonstrates%20the%20possibility%20of%20controlling%20specific%20module%20sequences%20within%0ALMs%20to%20improve%20their%20behavior%2C%20opening%20up%20the%20future%20application%20of%20mechanistic%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14233v1&entry.124074799=Read"},
{"title": "Towards Model-Agnostic Federated Learning over Networks", "author": "S. Abdurakhmanova and Y. SarcheshmehPour and A. Jung", "abstract": "  We present a model-agnostic federated learning method for networks of\nheterogeneous data and models. The network structure reflects similarities\nbetween the (statistics of the) local datasets and, in turn, their associated\nlocal (personal) models. Our method is an instance of empirical risk\nminimization, with a regularization term derived from the network structure of\nthe data. In particular, we require well-connected local models, which form\nclusters, to yield similar predictions on shared public, unlabelled dataset(s).\nThe proposed method allows for a wide range of local models. The only\nrestriction is that these local models must allow for efficient implementation\nof regularized empirical risk minimization (training). For many models, such\nimplementations are readily available in high-level programming libraries,\nincluding scikit-learn, Keras, and PyTorch.\n", "link": "http://arxiv.org/abs/2302.04363v3", "date": "2025-05-20", "relevancy": 2.4415, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5009}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4924}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Model-Agnostic%20Federated%20Learning%20over%20Networks&body=Title%3A%20Towards%20Model-Agnostic%20Federated%20Learning%20over%20Networks%0AAuthor%3A%20S.%20Abdurakhmanova%20and%20Y.%20SarcheshmehPour%20and%20A.%20Jung%0AAbstract%3A%20%20%20We%20present%20a%20model-agnostic%20federated%20learning%20method%20for%20networks%20of%0Aheterogeneous%20data%20and%20models.%20The%20network%20structure%20reflects%20similarities%0Abetween%20the%20%28statistics%20of%20the%29%20local%20datasets%20and%2C%20in%20turn%2C%20their%20associated%0Alocal%20%28personal%29%20models.%20Our%20method%20is%20an%20instance%20of%20empirical%20risk%0Aminimization%2C%20with%20a%20regularization%20term%20derived%20from%20the%20network%20structure%20of%0Athe%20data.%20In%20particular%2C%20we%20require%20well-connected%20local%20models%2C%20which%20form%0Aclusters%2C%20to%20yield%20similar%20predictions%20on%20shared%20public%2C%20unlabelled%20dataset%28s%29.%0AThe%20proposed%20method%20allows%20for%20a%20wide%20range%20of%20local%20models.%20The%20only%0Arestriction%20is%20that%20these%20local%20models%20must%20allow%20for%20efficient%20implementation%0Aof%20regularized%20empirical%20risk%20minimization%20%28training%29.%20For%20many%20models%2C%20such%0Aimplementations%20are%20readily%20available%20in%20high-level%20programming%20libraries%2C%0Aincluding%20scikit-learn%2C%20Keras%2C%20and%20PyTorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.04363v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Model-Agnostic%2520Federated%2520Learning%2520over%2520Networks%26entry.906535625%3DS.%2520Abdurakhmanova%2520and%2520Y.%2520SarcheshmehPour%2520and%2520A.%2520Jung%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520model-agnostic%2520federated%2520learning%2520method%2520for%2520networks%2520of%250Aheterogeneous%2520data%2520and%2520models.%2520The%2520network%2520structure%2520reflects%2520similarities%250Abetween%2520the%2520%2528statistics%2520of%2520the%2529%2520local%2520datasets%2520and%252C%2520in%2520turn%252C%2520their%2520associated%250Alocal%2520%2528personal%2529%2520models.%2520Our%2520method%2520is%2520an%2520instance%2520of%2520empirical%2520risk%250Aminimization%252C%2520with%2520a%2520regularization%2520term%2520derived%2520from%2520the%2520network%2520structure%2520of%250Athe%2520data.%2520In%2520particular%252C%2520we%2520require%2520well-connected%2520local%2520models%252C%2520which%2520form%250Aclusters%252C%2520to%2520yield%2520similar%2520predictions%2520on%2520shared%2520public%252C%2520unlabelled%2520dataset%2528s%2529.%250AThe%2520proposed%2520method%2520allows%2520for%2520a%2520wide%2520range%2520of%2520local%2520models.%2520The%2520only%250Arestriction%2520is%2520that%2520these%2520local%2520models%2520must%2520allow%2520for%2520efficient%2520implementation%250Aof%2520regularized%2520empirical%2520risk%2520minimization%2520%2528training%2529.%2520For%2520many%2520models%252C%2520such%250Aimplementations%2520are%2520readily%2520available%2520in%2520high-level%2520programming%2520libraries%252C%250Aincluding%2520scikit-learn%252C%2520Keras%252C%2520and%2520PyTorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.04363v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Model-Agnostic%20Federated%20Learning%20over%20Networks&entry.906535625=S.%20Abdurakhmanova%20and%20Y.%20SarcheshmehPour%20and%20A.%20Jung&entry.1292438233=%20%20We%20present%20a%20model-agnostic%20federated%20learning%20method%20for%20networks%20of%0Aheterogeneous%20data%20and%20models.%20The%20network%20structure%20reflects%20similarities%0Abetween%20the%20%28statistics%20of%20the%29%20local%20datasets%20and%2C%20in%20turn%2C%20their%20associated%0Alocal%20%28personal%29%20models.%20Our%20method%20is%20an%20instance%20of%20empirical%20risk%0Aminimization%2C%20with%20a%20regularization%20term%20derived%20from%20the%20network%20structure%20of%0Athe%20data.%20In%20particular%2C%20we%20require%20well-connected%20local%20models%2C%20which%20form%0Aclusters%2C%20to%20yield%20similar%20predictions%20on%20shared%20public%2C%20unlabelled%20dataset%28s%29.%0AThe%20proposed%20method%20allows%20for%20a%20wide%20range%20of%20local%20models.%20The%20only%0Arestriction%20is%20that%20these%20local%20models%20must%20allow%20for%20efficient%20implementation%0Aof%20regularized%20empirical%20risk%20minimization%20%28training%29.%20For%20many%20models%2C%20such%0Aimplementations%20are%20readily%20available%20in%20high-level%20programming%20libraries%2C%0Aincluding%20scikit-learn%2C%20Keras%2C%20and%20PyTorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.04363v3&entry.124074799=Read"},
{"title": "Triplane Grasping: Efficient 6-DoF Grasping with Single RGB Images", "author": "Yiming Li and Hanchi Ren and Yue Yang and Jingjing Deng and Xianghua Xie", "abstract": "  Reliable object grasping is one of the fundamental tasks in robotics.\nHowever, determining grasping pose based on single-image input has long been a\nchallenge due to limited visual information and the complexity of real-world\nobjects. In this paper, we propose Triplane Grasping, a fast grasping\ndecision-making method that relies solely on a single RGB-only image as input.\nTriplane Grasping creates a hybrid Triplane-Gaussian 3D representation through\na point decoder and a triplane decoder, which produce an efficient and\nhigh-quality reconstruction of the object to be grasped to meet real-time\ngrasping requirements. We propose to use an end-to-end network to generate\n6-DoF parallel-jaw grasp distributions directly from 3D points in the point\ncloud as potential grasp contacts and anchor the grasp pose in the observed\ndata. Experiments on the OmniObject3D and GraspNet-1Billion datasets\ndemonstrate that our method achieves rapid modeling and grasping pose\ndecision-making for daily objects, and strong generalization capability.\n", "link": "http://arxiv.org/abs/2410.15879v2", "date": "2025-05-20", "relevancy": 2.4399, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6308}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6045}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triplane%20Grasping%3A%20Efficient%206-DoF%20Grasping%20with%20Single%20RGB%20Images&body=Title%3A%20Triplane%20Grasping%3A%20Efficient%206-DoF%20Grasping%20with%20Single%20RGB%20Images%0AAuthor%3A%20Yiming%20Li%20and%20Hanchi%20Ren%20and%20Yue%20Yang%20and%20Jingjing%20Deng%20and%20Xianghua%20Xie%0AAbstract%3A%20%20%20Reliable%20object%20grasping%20is%20one%20of%20the%20fundamental%20tasks%20in%20robotics.%0AHowever%2C%20determining%20grasping%20pose%20based%20on%20single-image%20input%20has%20long%20been%20a%0Achallenge%20due%20to%20limited%20visual%20information%20and%20the%20complexity%20of%20real-world%0Aobjects.%20In%20this%20paper%2C%20we%20propose%20Triplane%20Grasping%2C%20a%20fast%20grasping%0Adecision-making%20method%20that%20relies%20solely%20on%20a%20single%20RGB-only%20image%20as%20input.%0ATriplane%20Grasping%20creates%20a%20hybrid%20Triplane-Gaussian%203D%20representation%20through%0Aa%20point%20decoder%20and%20a%20triplane%20decoder%2C%20which%20produce%20an%20efficient%20and%0Ahigh-quality%20reconstruction%20of%20the%20object%20to%20be%20grasped%20to%20meet%20real-time%0Agrasping%20requirements.%20We%20propose%20to%20use%20an%20end-to-end%20network%20to%20generate%0A6-DoF%20parallel-jaw%20grasp%20distributions%20directly%20from%203D%20points%20in%20the%20point%0Acloud%20as%20potential%20grasp%20contacts%20and%20anchor%20the%20grasp%20pose%20in%20the%20observed%0Adata.%20Experiments%20on%20the%20OmniObject3D%20and%20GraspNet-1Billion%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20rapid%20modeling%20and%20grasping%20pose%0Adecision-making%20for%20daily%20objects%2C%20and%20strong%20generalization%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriplane%2520Grasping%253A%2520Efficient%25206-DoF%2520Grasping%2520with%2520Single%2520RGB%2520Images%26entry.906535625%3DYiming%2520Li%2520and%2520Hanchi%2520Ren%2520and%2520Yue%2520Yang%2520and%2520Jingjing%2520Deng%2520and%2520Xianghua%2520Xie%26entry.1292438233%3D%2520%2520Reliable%2520object%2520grasping%2520is%2520one%2520of%2520the%2520fundamental%2520tasks%2520in%2520robotics.%250AHowever%252C%2520determining%2520grasping%2520pose%2520based%2520on%2520single-image%2520input%2520has%2520long%2520been%2520a%250Achallenge%2520due%2520to%2520limited%2520visual%2520information%2520and%2520the%2520complexity%2520of%2520real-world%250Aobjects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Triplane%2520Grasping%252C%2520a%2520fast%2520grasping%250Adecision-making%2520method%2520that%2520relies%2520solely%2520on%2520a%2520single%2520RGB-only%2520image%2520as%2520input.%250ATriplane%2520Grasping%2520creates%2520a%2520hybrid%2520Triplane-Gaussian%25203D%2520representation%2520through%250Aa%2520point%2520decoder%2520and%2520a%2520triplane%2520decoder%252C%2520which%2520produce%2520an%2520efficient%2520and%250Ahigh-quality%2520reconstruction%2520of%2520the%2520object%2520to%2520be%2520grasped%2520to%2520meet%2520real-time%250Agrasping%2520requirements.%2520We%2520propose%2520to%2520use%2520an%2520end-to-end%2520network%2520to%2520generate%250A6-DoF%2520parallel-jaw%2520grasp%2520distributions%2520directly%2520from%25203D%2520points%2520in%2520the%2520point%250Acloud%2520as%2520potential%2520grasp%2520contacts%2520and%2520anchor%2520the%2520grasp%2520pose%2520in%2520the%2520observed%250Adata.%2520Experiments%2520on%2520the%2520OmniObject3D%2520and%2520GraspNet-1Billion%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520rapid%2520modeling%2520and%2520grasping%2520pose%250Adecision-making%2520for%2520daily%2520objects%252C%2520and%2520strong%2520generalization%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triplane%20Grasping%3A%20Efficient%206-DoF%20Grasping%20with%20Single%20RGB%20Images&entry.906535625=Yiming%20Li%20and%20Hanchi%20Ren%20and%20Yue%20Yang%20and%20Jingjing%20Deng%20and%20Xianghua%20Xie&entry.1292438233=%20%20Reliable%20object%20grasping%20is%20one%20of%20the%20fundamental%20tasks%20in%20robotics.%0AHowever%2C%20determining%20grasping%20pose%20based%20on%20single-image%20input%20has%20long%20been%20a%0Achallenge%20due%20to%20limited%20visual%20information%20and%20the%20complexity%20of%20real-world%0Aobjects.%20In%20this%20paper%2C%20we%20propose%20Triplane%20Grasping%2C%20a%20fast%20grasping%0Adecision-making%20method%20that%20relies%20solely%20on%20a%20single%20RGB-only%20image%20as%20input.%0ATriplane%20Grasping%20creates%20a%20hybrid%20Triplane-Gaussian%203D%20representation%20through%0Aa%20point%20decoder%20and%20a%20triplane%20decoder%2C%20which%20produce%20an%20efficient%20and%0Ahigh-quality%20reconstruction%20of%20the%20object%20to%20be%20grasped%20to%20meet%20real-time%0Agrasping%20requirements.%20We%20propose%20to%20use%20an%20end-to-end%20network%20to%20generate%0A6-DoF%20parallel-jaw%20grasp%20distributions%20directly%20from%203D%20points%20in%20the%20point%0Acloud%20as%20potential%20grasp%20contacts%20and%20anchor%20the%20grasp%20pose%20in%20the%20observed%0Adata.%20Experiments%20on%20the%20OmniObject3D%20and%20GraspNet-1Billion%20datasets%0Ademonstrate%20that%20our%20method%20achieves%20rapid%20modeling%20and%20grasping%20pose%0Adecision-making%20for%20daily%20objects%2C%20and%20strong%20generalization%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15879v2&entry.124074799=Read"},
{"title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation", "author": "Yang Zhou and Shiyu Zhao and Yuxiao Chen and Zhenting Wang and Can Jin and Dimitris N. Metaxas", "abstract": "  Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.\n", "link": "http://arxiv.org/abs/2503.13794v2", "date": "2025-05-20", "relevancy": 2.4391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6104}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation&body=Title%3A%20LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation%0AAuthor%3A%20Yang%20Zhou%20and%20Shiyu%20Zhao%20and%20Yuxiao%20Chen%20and%20Zhenting%20Wang%20and%20Can%20Jin%20and%20Dimitris%20N.%20Metaxas%0AAbstract%3A%20%20%20Large%20foundation%20models%20trained%20on%20large-scale%20vision-language%20data%20can%20boost%0AOpen-Vocabulary%20Object%20Detection%20%28OVD%29%20via%20synthetic%20training%20data%2C%20yet%20the%0Ahand-crafted%20pipelines%20often%20introduce%20bias%20and%20overfit%20to%20specific%20prompts.%20We%0Asidestep%20this%20issue%20by%20directly%20fusing%20hidden%20states%20from%20Large%20Language%20Models%0A%28LLMs%29%20into%20detectors-an%20avenue%20surprisingly%20under-explored.%20This%20paper%0Apresents%20a%20systematic%20method%20to%20enhance%20visual%20grounding%20by%20utilizing%20decoder%0Alayers%20of%20the%20LLM%20of%20an%20MLLM.%20We%20introduce%20a%20zero-initialized%20cross-attention%0Aadapter%20to%20enable%20efficient%20knowledge%20fusion%20from%20LLMs%20to%20object%20detectors%2C%20a%0Anew%20approach%20called%20LED%20%28LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%29.%20We%0Afind%20that%20intermediate%20LLM%20layers%20already%20encode%20rich%20spatial%20semantics%3B%0Aadapting%20only%20the%20early%20layers%20yields%20most%20of%20the%20gain.%20With%20Swin-T%20as%20the%0Avision%20encoder%2C%20Qwen2-0.5B%20%2B%20LED%20lifts%20GroundingDINO%20by%203.82%20%25%20on%20OmniLabel%20at%0Ajust%208.7%20%25%20extra%20GFLOPs%2C%20and%20a%20larger%20vision%20backbone%20pushes%20the%20improvement%20to%0A6.22%20%25.%20Extensive%20ablations%20on%20adapter%20variants%2C%20LLM%20scales%20and%20fusion%20depths%0Afurther%20corroborate%20our%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.13794v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLED%253A%2520LLM%2520Enhanced%2520Open-Vocabulary%2520Object%2520Detection%2520without%2520Human%2520Curated%250A%2520%2520Data%2520Generation%26entry.906535625%3DYang%2520Zhou%2520and%2520Shiyu%2520Zhao%2520and%2520Yuxiao%2520Chen%2520and%2520Zhenting%2520Wang%2520and%2520Can%2520Jin%2520and%2520Dimitris%2520N.%2520Metaxas%26entry.1292438233%3D%2520%2520Large%2520foundation%2520models%2520trained%2520on%2520large-scale%2520vision-language%2520data%2520can%2520boost%250AOpen-Vocabulary%2520Object%2520Detection%2520%2528OVD%2529%2520via%2520synthetic%2520training%2520data%252C%2520yet%2520the%250Ahand-crafted%2520pipelines%2520often%2520introduce%2520bias%2520and%2520overfit%2520to%2520specific%2520prompts.%2520We%250Asidestep%2520this%2520issue%2520by%2520directly%2520fusing%2520hidden%2520states%2520from%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520into%2520detectors-an%2520avenue%2520surprisingly%2520under-explored.%2520This%2520paper%250Apresents%2520a%2520systematic%2520method%2520to%2520enhance%2520visual%2520grounding%2520by%2520utilizing%2520decoder%250Alayers%2520of%2520the%2520LLM%2520of%2520an%2520MLLM.%2520We%2520introduce%2520a%2520zero-initialized%2520cross-attention%250Aadapter%2520to%2520enable%2520efficient%2520knowledge%2520fusion%2520from%2520LLMs%2520to%2520object%2520detectors%252C%2520a%250Anew%2520approach%2520called%2520LED%2520%2528LLM%2520Enhanced%2520Open-Vocabulary%2520Object%2520Detection%2529.%2520We%250Afind%2520that%2520intermediate%2520LLM%2520layers%2520already%2520encode%2520rich%2520spatial%2520semantics%253B%250Aadapting%2520only%2520the%2520early%2520layers%2520yields%2520most%2520of%2520the%2520gain.%2520With%2520Swin-T%2520as%2520the%250Avision%2520encoder%252C%2520Qwen2-0.5B%2520%252B%2520LED%2520lifts%2520GroundingDINO%2520by%25203.82%2520%2525%2520on%2520OmniLabel%2520at%250Ajust%25208.7%2520%2525%2520extra%2520GFLOPs%252C%2520and%2520a%2520larger%2520vision%2520backbone%2520pushes%2520the%2520improvement%2520to%250A6.22%2520%2525.%2520Extensive%2520ablations%2520on%2520adapter%2520variants%252C%2520LLM%2520scales%2520and%2520fusion%2520depths%250Afurther%2520corroborate%2520our%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13794v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LED%3A%20LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%20without%20Human%20Curated%0A%20%20Data%20Generation&entry.906535625=Yang%20Zhou%20and%20Shiyu%20Zhao%20and%20Yuxiao%20Chen%20and%20Zhenting%20Wang%20and%20Can%20Jin%20and%20Dimitris%20N.%20Metaxas&entry.1292438233=%20%20Large%20foundation%20models%20trained%20on%20large-scale%20vision-language%20data%20can%20boost%0AOpen-Vocabulary%20Object%20Detection%20%28OVD%29%20via%20synthetic%20training%20data%2C%20yet%20the%0Ahand-crafted%20pipelines%20often%20introduce%20bias%20and%20overfit%20to%20specific%20prompts.%20We%0Asidestep%20this%20issue%20by%20directly%20fusing%20hidden%20states%20from%20Large%20Language%20Models%0A%28LLMs%29%20into%20detectors-an%20avenue%20surprisingly%20under-explored.%20This%20paper%0Apresents%20a%20systematic%20method%20to%20enhance%20visual%20grounding%20by%20utilizing%20decoder%0Alayers%20of%20the%20LLM%20of%20an%20MLLM.%20We%20introduce%20a%20zero-initialized%20cross-attention%0Aadapter%20to%20enable%20efficient%20knowledge%20fusion%20from%20LLMs%20to%20object%20detectors%2C%20a%0Anew%20approach%20called%20LED%20%28LLM%20Enhanced%20Open-Vocabulary%20Object%20Detection%29.%20We%0Afind%20that%20intermediate%20LLM%20layers%20already%20encode%20rich%20spatial%20semantics%3B%0Aadapting%20only%20the%20early%20layers%20yields%20most%20of%20the%20gain.%20With%20Swin-T%20as%20the%0Avision%20encoder%2C%20Qwen2-0.5B%20%2B%20LED%20lifts%20GroundingDINO%20by%203.82%20%25%20on%20OmniLabel%20at%0Ajust%208.7%20%25%20extra%20GFLOPs%2C%20and%20a%20larger%20vision%20backbone%20pushes%20the%20improvement%20to%0A6.22%20%25.%20Extensive%20ablations%20on%20adapter%20variants%2C%20LLM%20scales%20and%20fusion%20depths%0Afurther%20corroborate%20our%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.13794v2&entry.124074799=Read"},
{"title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation", "author": "Wentao Ma and Weiming Ren and Yiming Jia and Zhuofeng Li and Ping Nie and Ge Zhang and Wenhu Chen", "abstract": "  Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain.\n", "link": "http://arxiv.org/abs/2505.14640v1", "date": "2025-05-20", "relevancy": 2.4361, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6213}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6213}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoEval-Pro%3A%20Robust%20and%20Realistic%20Long%20Video%20Understanding%20Evaluation&body=Title%3A%20VideoEval-Pro%3A%20Robust%20and%20Realistic%20Long%20Video%20Understanding%20Evaluation%0AAuthor%3A%20Wentao%20Ma%20and%20Weiming%20Ren%20and%20Yiming%20Jia%20and%20Zhuofeng%20Li%20and%20Ping%20Nie%20and%20Ge%20Zhang%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%0Along%20video%20understanding%20%28LVU%29%2C%20prompting%20the%20development%20of%20standardized%20LVU%0Abenchmarks%20to%20evaluate%20their%20performance.%20However%2C%20our%20investigation%20reveals%20a%0Arather%20sober%20lesson%20for%20existing%20LVU%20benchmarks.%20First%2C%20most%20existing%0Abenchmarks%20rely%20heavily%20on%20multiple-choice%20questions%20%28MCQs%29%2C%20whose%20evaluation%0Aresults%20are%20inflated%20due%20to%20the%20possibility%20of%20guessing%20the%20correct%20answer%3B%0ASecond%2C%20a%20significant%20portion%20of%20questions%20in%20these%20benchmarks%20have%20strong%0Apriors%20to%20allow%20models%20to%20answer%20directly%20without%20even%20reading%20the%20input%20video.%0AFor%20example%2C%20Gemini-1.5-Pro%20can%20achieve%20over%2050%5C%25%20accuracy%20given%20a%20random%20frame%0Afrom%20a%20long%20video%20on%20Video-MME.%20We%20also%20observe%20that%20increasing%20the%20number%20of%0Aframes%20does%20not%20necessarily%20lead%20to%20improvement%20on%20existing%20benchmarks%2C%20which%0Ais%20counterintuitive.%20As%20a%20result%2C%20the%20validity%20and%20robustness%20of%20current%20LVU%0Abenchmarks%20are%20undermined%2C%20impeding%20a%20faithful%20assessment%20of%20LMMs%27%20long-video%0Aunderstanding%20capability.%20To%20tackle%20this%20problem%2C%20we%20propose%20VideoEval-Pro%2C%20a%0Arealistic%20LVU%20benchmark%20containing%20questions%20with%20open-ended%20short-answer%2C%0Awhich%20truly%20require%20understanding%20the%20entire%20video.%20VideoEval-Pro%20assesses%20both%0Asegment-level%20and%20full-video%20understanding%20through%20perception%20and%20reasoning%0Atasks.%20By%20evaluating%2021%20proprietary%20and%20open-source%20video%20LMMs%2C%20we%20conclude%20the%0Afollowing%20findings%3A%20%281%29%20video%20LMMs%20show%20drastic%20performance%20%28%24%3E%2425%5C%25%29%20drops%20on%0Aopen-ended%20questions%20compared%20with%20MCQs%3B%20%282%29%20surprisingly%2C%20higher%20MCQ%20scores%20do%0Anot%20lead%20to%20higher%20open-ended%20scores%20on%20VideoEval-Pro%3B%20%283%29%20compared%20to%20other%0AMCQ%20benchmarks%2C%20VideoEval-Pro%20benefits%20more%20from%20increasing%20the%20number%20of%20input%0Aframes.%20Our%20results%20show%20that%20VideoEval-Pro%20offers%20a%20more%20realistic%20and%0Areliable%20measure%20of%20long%20video%20understanding%2C%20providing%20a%20clearer%20view%20of%0Aprogress%20in%20this%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoEval-Pro%253A%2520Robust%2520and%2520Realistic%2520Long%2520Video%2520Understanding%2520Evaluation%26entry.906535625%3DWentao%2520Ma%2520and%2520Weiming%2520Ren%2520and%2520Yiming%2520Jia%2520and%2520Zhuofeng%2520Li%2520and%2520Ping%2520Nie%2520and%2520Ge%2520Zhang%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520Large%2520multimodal%2520models%2520%2528LMMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%250Along%2520video%2520understanding%2520%2528LVU%2529%252C%2520prompting%2520the%2520development%2520of%2520standardized%2520LVU%250Abenchmarks%2520to%2520evaluate%2520their%2520performance.%2520However%252C%2520our%2520investigation%2520reveals%2520a%250Arather%2520sober%2520lesson%2520for%2520existing%2520LVU%2520benchmarks.%2520First%252C%2520most%2520existing%250Abenchmarks%2520rely%2520heavily%2520on%2520multiple-choice%2520questions%2520%2528MCQs%2529%252C%2520whose%2520evaluation%250Aresults%2520are%2520inflated%2520due%2520to%2520the%2520possibility%2520of%2520guessing%2520the%2520correct%2520answer%253B%250ASecond%252C%2520a%2520significant%2520portion%2520of%2520questions%2520in%2520these%2520benchmarks%2520have%2520strong%250Apriors%2520to%2520allow%2520models%2520to%2520answer%2520directly%2520without%2520even%2520reading%2520the%2520input%2520video.%250AFor%2520example%252C%2520Gemini-1.5-Pro%2520can%2520achieve%2520over%252050%255C%2525%2520accuracy%2520given%2520a%2520random%2520frame%250Afrom%2520a%2520long%2520video%2520on%2520Video-MME.%2520We%2520also%2520observe%2520that%2520increasing%2520the%2520number%2520of%250Aframes%2520does%2520not%2520necessarily%2520lead%2520to%2520improvement%2520on%2520existing%2520benchmarks%252C%2520which%250Ais%2520counterintuitive.%2520As%2520a%2520result%252C%2520the%2520validity%2520and%2520robustness%2520of%2520current%2520LVU%250Abenchmarks%2520are%2520undermined%252C%2520impeding%2520a%2520faithful%2520assessment%2520of%2520LMMs%2527%2520long-video%250Aunderstanding%2520capability.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520VideoEval-Pro%252C%2520a%250Arealistic%2520LVU%2520benchmark%2520containing%2520questions%2520with%2520open-ended%2520short-answer%252C%250Awhich%2520truly%2520require%2520understanding%2520the%2520entire%2520video.%2520VideoEval-Pro%2520assesses%2520both%250Asegment-level%2520and%2520full-video%2520understanding%2520through%2520perception%2520and%2520reasoning%250Atasks.%2520By%2520evaluating%252021%2520proprietary%2520and%2520open-source%2520video%2520LMMs%252C%2520we%2520conclude%2520the%250Afollowing%2520findings%253A%2520%25281%2529%2520video%2520LMMs%2520show%2520drastic%2520performance%2520%2528%2524%253E%252425%255C%2525%2529%2520drops%2520on%250Aopen-ended%2520questions%2520compared%2520with%2520MCQs%253B%2520%25282%2529%2520surprisingly%252C%2520higher%2520MCQ%2520scores%2520do%250Anot%2520lead%2520to%2520higher%2520open-ended%2520scores%2520on%2520VideoEval-Pro%253B%2520%25283%2529%2520compared%2520to%2520other%250AMCQ%2520benchmarks%252C%2520VideoEval-Pro%2520benefits%2520more%2520from%2520increasing%2520the%2520number%2520of%2520input%250Aframes.%2520Our%2520results%2520show%2520that%2520VideoEval-Pro%2520offers%2520a%2520more%2520realistic%2520and%250Areliable%2520measure%2520of%2520long%2520video%2520understanding%252C%2520providing%2520a%2520clearer%2520view%2520of%250Aprogress%2520in%2520this%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoEval-Pro%3A%20Robust%20and%20Realistic%20Long%20Video%20Understanding%20Evaluation&entry.906535625=Wentao%20Ma%20and%20Weiming%20Ren%20and%20Yiming%20Jia%20and%20Zhuofeng%20Li%20and%20Ping%20Nie%20and%20Ge%20Zhang%20and%20Wenhu%20Chen&entry.1292438233=%20%20Large%20multimodal%20models%20%28LMMs%29%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%0Along%20video%20understanding%20%28LVU%29%2C%20prompting%20the%20development%20of%20standardized%20LVU%0Abenchmarks%20to%20evaluate%20their%20performance.%20However%2C%20our%20investigation%20reveals%20a%0Arather%20sober%20lesson%20for%20existing%20LVU%20benchmarks.%20First%2C%20most%20existing%0Abenchmarks%20rely%20heavily%20on%20multiple-choice%20questions%20%28MCQs%29%2C%20whose%20evaluation%0Aresults%20are%20inflated%20due%20to%20the%20possibility%20of%20guessing%20the%20correct%20answer%3B%0ASecond%2C%20a%20significant%20portion%20of%20questions%20in%20these%20benchmarks%20have%20strong%0Apriors%20to%20allow%20models%20to%20answer%20directly%20without%20even%20reading%20the%20input%20video.%0AFor%20example%2C%20Gemini-1.5-Pro%20can%20achieve%20over%2050%5C%25%20accuracy%20given%20a%20random%20frame%0Afrom%20a%20long%20video%20on%20Video-MME.%20We%20also%20observe%20that%20increasing%20the%20number%20of%0Aframes%20does%20not%20necessarily%20lead%20to%20improvement%20on%20existing%20benchmarks%2C%20which%0Ais%20counterintuitive.%20As%20a%20result%2C%20the%20validity%20and%20robustness%20of%20current%20LVU%0Abenchmarks%20are%20undermined%2C%20impeding%20a%20faithful%20assessment%20of%20LMMs%27%20long-video%0Aunderstanding%20capability.%20To%20tackle%20this%20problem%2C%20we%20propose%20VideoEval-Pro%2C%20a%0Arealistic%20LVU%20benchmark%20containing%20questions%20with%20open-ended%20short-answer%2C%0Awhich%20truly%20require%20understanding%20the%20entire%20video.%20VideoEval-Pro%20assesses%20both%0Asegment-level%20and%20full-video%20understanding%20through%20perception%20and%20reasoning%0Atasks.%20By%20evaluating%2021%20proprietary%20and%20open-source%20video%20LMMs%2C%20we%20conclude%20the%0Afollowing%20findings%3A%20%281%29%20video%20LMMs%20show%20drastic%20performance%20%28%24%3E%2425%5C%25%29%20drops%20on%0Aopen-ended%20questions%20compared%20with%20MCQs%3B%20%282%29%20surprisingly%2C%20higher%20MCQ%20scores%20do%0Anot%20lead%20to%20higher%20open-ended%20scores%20on%20VideoEval-Pro%3B%20%283%29%20compared%20to%20other%0AMCQ%20benchmarks%2C%20VideoEval-Pro%20benefits%20more%20from%20increasing%20the%20number%20of%20input%0Aframes.%20Our%20results%20show%20that%20VideoEval-Pro%20offers%20a%20more%20realistic%20and%0Areliable%20measure%20of%20long%20video%20understanding%2C%20providing%20a%20clearer%20view%20of%0Aprogress%20in%20this%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14640v1&entry.124074799=Read"},
{"title": "Self-Supervised Frameworks for Speaker Verification via Bootstrapped\n  Positive Sampling", "author": "Theo Lepage and Reda Dehak", "abstract": "  Recent developments in Self-Supervised Learning (SSL) have demonstrated\nsignificant potential for Speaker Verification (SV), but closing the\nperformance gap with supervised systems remains an ongoing challenge. Standard\nSSL frameworks rely on anchor-positive pairs extracted from the same audio\nutterances. Hence, positives have channel characteristics similar to those of\ntheir corresponding anchors, even with extensive data-augmentation. Therefore,\nthis positive sampling strategy is a fundamental limitation as it encodes too\nmuch information regarding the recording source in the learned representations.\nThis article introduces Self-Supervised Positive Sampling (SSPS), a\nbootstrapped technique for sampling appropriate and diverse positives in SSL\nframeworks for SV. SSPS samples positives close to their anchor in the\nrepresentation space, under the assumption that these pseudo-positives belong\nto the same speaker identity but correspond to different recording conditions.\nThis method demonstrates consistent improvements in SV performance on VoxCeleb\nbenchmarks when implemented in major SSL frameworks, such as SimCLR, SwAV,\nVICReg, and DINO. Using SSPS, SimCLR, and DINO achieve 2.57% and 2.53% EER on\nVoxCeleb1-O. SimCLR yields a 58% relative reduction in EER, getting comparable\nperformance to DINO with a simpler training framework. Furthermore, SSPS lowers\nintra-class variance and reduces channel information in speaker representations\nwhile exhibiting greater robustness without data-augmentation.\n", "link": "http://arxiv.org/abs/2501.17772v2", "date": "2025-05-20", "relevancy": 2.4332, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Frameworks%20for%20Speaker%20Verification%20via%20Bootstrapped%0A%20%20Positive%20Sampling&body=Title%3A%20Self-Supervised%20Frameworks%20for%20Speaker%20Verification%20via%20Bootstrapped%0A%20%20Positive%20Sampling%0AAuthor%3A%20Theo%20Lepage%20and%20Reda%20Dehak%0AAbstract%3A%20%20%20Recent%20developments%20in%20Self-Supervised%20Learning%20%28SSL%29%20have%20demonstrated%0Asignificant%20potential%20for%20Speaker%20Verification%20%28SV%29%2C%20but%20closing%20the%0Aperformance%20gap%20with%20supervised%20systems%20remains%20an%20ongoing%20challenge.%20Standard%0ASSL%20frameworks%20rely%20on%20anchor-positive%20pairs%20extracted%20from%20the%20same%20audio%0Autterances.%20Hence%2C%20positives%20have%20channel%20characteristics%20similar%20to%20those%20of%0Atheir%20corresponding%20anchors%2C%20even%20with%20extensive%20data-augmentation.%20Therefore%2C%0Athis%20positive%20sampling%20strategy%20is%20a%20fundamental%20limitation%20as%20it%20encodes%20too%0Amuch%20information%20regarding%20the%20recording%20source%20in%20the%20learned%20representations.%0AThis%20article%20introduces%20Self-Supervised%20Positive%20Sampling%20%28SSPS%29%2C%20a%0Abootstrapped%20technique%20for%20sampling%20appropriate%20and%20diverse%20positives%20in%20SSL%0Aframeworks%20for%20SV.%20SSPS%20samples%20positives%20close%20to%20their%20anchor%20in%20the%0Arepresentation%20space%2C%20under%20the%20assumption%20that%20these%20pseudo-positives%20belong%0Ato%20the%20same%20speaker%20identity%20but%20correspond%20to%20different%20recording%20conditions.%0AThis%20method%20demonstrates%20consistent%20improvements%20in%20SV%20performance%20on%20VoxCeleb%0Abenchmarks%20when%20implemented%20in%20major%20SSL%20frameworks%2C%20such%20as%20SimCLR%2C%20SwAV%2C%0AVICReg%2C%20and%20DINO.%20Using%20SSPS%2C%20SimCLR%2C%20and%20DINO%20achieve%202.57%25%20and%202.53%25%20EER%20on%0AVoxCeleb1-O.%20SimCLR%20yields%20a%2058%25%20relative%20reduction%20in%20EER%2C%20getting%20comparable%0Aperformance%20to%20DINO%20with%20a%20simpler%20training%20framework.%20Furthermore%2C%20SSPS%20lowers%0Aintra-class%20variance%20and%20reduces%20channel%20information%20in%20speaker%20representations%0Awhile%20exhibiting%20greater%20robustness%20without%20data-augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.17772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Frameworks%2520for%2520Speaker%2520Verification%2520via%2520Bootstrapped%250A%2520%2520Positive%2520Sampling%26entry.906535625%3DTheo%2520Lepage%2520and%2520Reda%2520Dehak%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520have%2520demonstrated%250Asignificant%2520potential%2520for%2520Speaker%2520Verification%2520%2528SV%2529%252C%2520but%2520closing%2520the%250Aperformance%2520gap%2520with%2520supervised%2520systems%2520remains%2520an%2520ongoing%2520challenge.%2520Standard%250ASSL%2520frameworks%2520rely%2520on%2520anchor-positive%2520pairs%2520extracted%2520from%2520the%2520same%2520audio%250Autterances.%2520Hence%252C%2520positives%2520have%2520channel%2520characteristics%2520similar%2520to%2520those%2520of%250Atheir%2520corresponding%2520anchors%252C%2520even%2520with%2520extensive%2520data-augmentation.%2520Therefore%252C%250Athis%2520positive%2520sampling%2520strategy%2520is%2520a%2520fundamental%2520limitation%2520as%2520it%2520encodes%2520too%250Amuch%2520information%2520regarding%2520the%2520recording%2520source%2520in%2520the%2520learned%2520representations.%250AThis%2520article%2520introduces%2520Self-Supervised%2520Positive%2520Sampling%2520%2528SSPS%2529%252C%2520a%250Abootstrapped%2520technique%2520for%2520sampling%2520appropriate%2520and%2520diverse%2520positives%2520in%2520SSL%250Aframeworks%2520for%2520SV.%2520SSPS%2520samples%2520positives%2520close%2520to%2520their%2520anchor%2520in%2520the%250Arepresentation%2520space%252C%2520under%2520the%2520assumption%2520that%2520these%2520pseudo-positives%2520belong%250Ato%2520the%2520same%2520speaker%2520identity%2520but%2520correspond%2520to%2520different%2520recording%2520conditions.%250AThis%2520method%2520demonstrates%2520consistent%2520improvements%2520in%2520SV%2520performance%2520on%2520VoxCeleb%250Abenchmarks%2520when%2520implemented%2520in%2520major%2520SSL%2520frameworks%252C%2520such%2520as%2520SimCLR%252C%2520SwAV%252C%250AVICReg%252C%2520and%2520DINO.%2520Using%2520SSPS%252C%2520SimCLR%252C%2520and%2520DINO%2520achieve%25202.57%2525%2520and%25202.53%2525%2520EER%2520on%250AVoxCeleb1-O.%2520SimCLR%2520yields%2520a%252058%2525%2520relative%2520reduction%2520in%2520EER%252C%2520getting%2520comparable%250Aperformance%2520to%2520DINO%2520with%2520a%2520simpler%2520training%2520framework.%2520Furthermore%252C%2520SSPS%2520lowers%250Aintra-class%2520variance%2520and%2520reduces%2520channel%2520information%2520in%2520speaker%2520representations%250Awhile%2520exhibiting%2520greater%2520robustness%2520without%2520data-augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.17772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Frameworks%20for%20Speaker%20Verification%20via%20Bootstrapped%0A%20%20Positive%20Sampling&entry.906535625=Theo%20Lepage%20and%20Reda%20Dehak&entry.1292438233=%20%20Recent%20developments%20in%20Self-Supervised%20Learning%20%28SSL%29%20have%20demonstrated%0Asignificant%20potential%20for%20Speaker%20Verification%20%28SV%29%2C%20but%20closing%20the%0Aperformance%20gap%20with%20supervised%20systems%20remains%20an%20ongoing%20challenge.%20Standard%0ASSL%20frameworks%20rely%20on%20anchor-positive%20pairs%20extracted%20from%20the%20same%20audio%0Autterances.%20Hence%2C%20positives%20have%20channel%20characteristics%20similar%20to%20those%20of%0Atheir%20corresponding%20anchors%2C%20even%20with%20extensive%20data-augmentation.%20Therefore%2C%0Athis%20positive%20sampling%20strategy%20is%20a%20fundamental%20limitation%20as%20it%20encodes%20too%0Amuch%20information%20regarding%20the%20recording%20source%20in%20the%20learned%20representations.%0AThis%20article%20introduces%20Self-Supervised%20Positive%20Sampling%20%28SSPS%29%2C%20a%0Abootstrapped%20technique%20for%20sampling%20appropriate%20and%20diverse%20positives%20in%20SSL%0Aframeworks%20for%20SV.%20SSPS%20samples%20positives%20close%20to%20their%20anchor%20in%20the%0Arepresentation%20space%2C%20under%20the%20assumption%20that%20these%20pseudo-positives%20belong%0Ato%20the%20same%20speaker%20identity%20but%20correspond%20to%20different%20recording%20conditions.%0AThis%20method%20demonstrates%20consistent%20improvements%20in%20SV%20performance%20on%20VoxCeleb%0Abenchmarks%20when%20implemented%20in%20major%20SSL%20frameworks%2C%20such%20as%20SimCLR%2C%20SwAV%2C%0AVICReg%2C%20and%20DINO.%20Using%20SSPS%2C%20SimCLR%2C%20and%20DINO%20achieve%202.57%25%20and%202.53%25%20EER%20on%0AVoxCeleb1-O.%20SimCLR%20yields%20a%2058%25%20relative%20reduction%20in%20EER%2C%20getting%20comparable%0Aperformance%20to%20DINO%20with%20a%20simpler%20training%20framework.%20Furthermore%2C%20SSPS%20lowers%0Aintra-class%20variance%20and%20reduces%20channel%20information%20in%20speaker%20representations%0Awhile%20exhibiting%20greater%20robustness%20without%20data-augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.17772v2&entry.124074799=Read"},
{"title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "author": "Sanjay Govindan and Maurice Pagnucco and Yang Song", "abstract": "  Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.\n", "link": "http://arxiv.org/abs/2505.14158v1", "date": "2025-05-20", "relevancy": 2.4247, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Alignment%20of%20Time%20Sensitive%20Facts%20with%20Activation%20Engineering&body=Title%3A%20Temporal%20Alignment%20of%20Time%20Sensitive%20Facts%20with%20Activation%20Engineering%0AAuthor%3A%20Sanjay%20Govindan%20and%20Maurice%20Pagnucco%20and%20Yang%20Song%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20trained%20on%20diverse%20and%20often%20conflicting%0Aknowledge%20spanning%20multiple%20domains%20and%20time%20periods.%20Some%20of%20this%20knowledge%20is%0Aonly%20valid%20within%20specific%20temporal%20contexts%2C%20such%20as%20answering%20the%20question%2C%0A%22Who%20is%20the%20President%20of%20the%20United%20States%20in%202022%3F%22%20Ensuring%20LLMs%20generate%0Atime%20appropriate%20responses%20is%20crucial%20for%20maintaining%20relevance%20and%20accuracy.%0AIn%20this%20work%20we%20explore%20activation%20engineering%20as%20a%20method%20for%20temporally%0Aaligning%20LLMs%20to%20improve%20factual%20recall%20without%20any%20training%20or%20dataset%0Acreation.%20In%20this%20research%20we%20explore%20an%20activation%20engineering%20technique%20to%0Aground%20three%20versions%20of%20LLaMA%202%20to%20specific%20points%20in%20time%20and%20examine%20the%0Aeffects%20of%20varying%20injection%20layers%20and%20prompting%20strategies.%20Our%20experiments%0Ademonstrate%20up%20to%20a%2044%25%20and%2016%25%20improvement%20in%20relative%20and%20explicit%20prompting%0Arespectively%2C%20achieving%20comparable%20performance%20to%20the%20fine-tuning%20method%0Aproposed%20by%20Zhao%20et%20al.%20%282024%29%20.%20Notably%2C%20our%20approach%20achieves%20similar%20results%0Ato%20the%20fine-tuning%20baseline%20while%20being%20significantly%20more%20computationally%0Aefficient%20and%20requiring%20no%20pre-aligned%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Alignment%2520of%2520Time%2520Sensitive%2520Facts%2520with%2520Activation%2520Engineering%26entry.906535625%3DSanjay%2520Govindan%2520and%2520Maurice%2520Pagnucco%2520and%2520Yang%2520Song%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520trained%2520on%2520diverse%2520and%2520often%2520conflicting%250Aknowledge%2520spanning%2520multiple%2520domains%2520and%2520time%2520periods.%2520Some%2520of%2520this%2520knowledge%2520is%250Aonly%2520valid%2520within%2520specific%2520temporal%2520contexts%252C%2520such%2520as%2520answering%2520the%2520question%252C%250A%2522Who%2520is%2520the%2520President%2520of%2520the%2520United%2520States%2520in%25202022%253F%2522%2520Ensuring%2520LLMs%2520generate%250Atime%2520appropriate%2520responses%2520is%2520crucial%2520for%2520maintaining%2520relevance%2520and%2520accuracy.%250AIn%2520this%2520work%2520we%2520explore%2520activation%2520engineering%2520as%2520a%2520method%2520for%2520temporally%250Aaligning%2520LLMs%2520to%2520improve%2520factual%2520recall%2520without%2520any%2520training%2520or%2520dataset%250Acreation.%2520In%2520this%2520research%2520we%2520explore%2520an%2520activation%2520engineering%2520technique%2520to%250Aground%2520three%2520versions%2520of%2520LLaMA%25202%2520to%2520specific%2520points%2520in%2520time%2520and%2520examine%2520the%250Aeffects%2520of%2520varying%2520injection%2520layers%2520and%2520prompting%2520strategies.%2520Our%2520experiments%250Ademonstrate%2520up%2520to%2520a%252044%2525%2520and%252016%2525%2520improvement%2520in%2520relative%2520and%2520explicit%2520prompting%250Arespectively%252C%2520achieving%2520comparable%2520performance%2520to%2520the%2520fine-tuning%2520method%250Aproposed%2520by%2520Zhao%2520et%2520al.%2520%25282024%2529%2520.%2520Notably%252C%2520our%2520approach%2520achieves%2520similar%2520results%250Ato%2520the%2520fine-tuning%2520baseline%2520while%2520being%2520significantly%2520more%2520computationally%250Aefficient%2520and%2520requiring%2520no%2520pre-aligned%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Alignment%20of%20Time%20Sensitive%20Facts%20with%20Activation%20Engineering&entry.906535625=Sanjay%20Govindan%20and%20Maurice%20Pagnucco%20and%20Yang%20Song&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20trained%20on%20diverse%20and%20often%20conflicting%0Aknowledge%20spanning%20multiple%20domains%20and%20time%20periods.%20Some%20of%20this%20knowledge%20is%0Aonly%20valid%20within%20specific%20temporal%20contexts%2C%20such%20as%20answering%20the%20question%2C%0A%22Who%20is%20the%20President%20of%20the%20United%20States%20in%202022%3F%22%20Ensuring%20LLMs%20generate%0Atime%20appropriate%20responses%20is%20crucial%20for%20maintaining%20relevance%20and%20accuracy.%0AIn%20this%20work%20we%20explore%20activation%20engineering%20as%20a%20method%20for%20temporally%0Aaligning%20LLMs%20to%20improve%20factual%20recall%20without%20any%20training%20or%20dataset%0Acreation.%20In%20this%20research%20we%20explore%20an%20activation%20engineering%20technique%20to%0Aground%20three%20versions%20of%20LLaMA%202%20to%20specific%20points%20in%20time%20and%20examine%20the%0Aeffects%20of%20varying%20injection%20layers%20and%20prompting%20strategies.%20Our%20experiments%0Ademonstrate%20up%20to%20a%2044%25%20and%2016%25%20improvement%20in%20relative%20and%20explicit%20prompting%0Arespectively%2C%20achieving%20comparable%20performance%20to%20the%20fine-tuning%20method%0Aproposed%20by%20Zhao%20et%20al.%20%282024%29%20.%20Notably%2C%20our%20approach%20achieves%20similar%20results%0Ato%20the%20fine-tuning%20baseline%20while%20being%20significantly%20more%20computationally%0Aefficient%20and%20requiring%20no%20pre-aligned%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14158v1&entry.124074799=Read"},
{"title": "Online Iterative Self-Alignment for Radiology Report Generation", "author": "Ting Xiao and Lei Shi and Yang Zhang and HaoFeng Yang and Zhe Wang and Chenjia Bai", "abstract": "  Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.\n", "link": "http://arxiv.org/abs/2505.11983v2", "date": "2025-05-20", "relevancy": 2.4238, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.495}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4893}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Iterative%20Self-Alignment%20for%20Radiology%20Report%20Generation&body=Title%3A%20Online%20Iterative%20Self-Alignment%20for%20Radiology%20Report%20Generation%0AAuthor%3A%20Ting%20Xiao%20and%20Lei%20Shi%20and%20Yang%20Zhang%20and%20HaoFeng%20Yang%20and%20Zhe%20Wang%20and%20Chenjia%20Bai%0AAbstract%3A%20%20%20Radiology%20Report%20Generation%20%28RRG%29%20is%20an%20important%20research%20topic%20for%0Arelieving%20radiologist%27%20heavy%20workload.%20Existing%20RRG%20models%20mainly%20rely%20on%0Asupervised%20fine-tuning%20%28SFT%29%20based%20on%20different%20model%20architectures%20using%20data%0Apairs%20of%20radiological%20images%20and%20corresponding%20radiologist-annotated%20reports.%0ARecent%20research%20has%20shifted%20focus%20to%20post-training%20improvements%2C%20aligning%20RRG%0Amodel%20outputs%20with%20human%20preferences%20using%20reinforcement%20learning%20%28RL%29.%0AHowever%2C%20the%20limited%20data%20coverage%20of%20high-quality%20annotated%20data%20poses%20risks%0Aof%20overfitting%20and%20generalization.%20This%20paper%20proposes%20a%20novel%20Online%20Iterative%0ASelf-Alignment%20%28OISA%29%20method%20for%20RRG%20that%20consists%20of%20four%20stages%3A%0Aself-generation%20of%20diverse%20data%2C%20self-evaluation%20for%20multi-objective%20preference%0Adata%2Cself-alignment%20for%20multi-objective%20optimization%20and%20self-iteration%20for%0Afurther%20improvement.%20Our%20approach%20allows%20for%20generating%20varied%20reports%20tailored%0Ato%20specific%20clinical%20objectives%2C%20enhancing%20the%20overall%20performance%20of%20the%20RRG%0Amodel%20iteratively.%20Unlike%20existing%20methods%2C%20our%20frame-work%20significantly%0Aincreases%20data%20quality%20and%20optimizes%20performance%20through%20iterative%0Amulti-objective%20optimization.%20Experimental%20results%20demonstrate%20that%20our%20method%0Asurpasses%20previous%20approaches%2C%20achieving%20state-of-the-art%20performance%20across%0Amultiple%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Iterative%2520Self-Alignment%2520for%2520Radiology%2520Report%2520Generation%26entry.906535625%3DTing%2520Xiao%2520and%2520Lei%2520Shi%2520and%2520Yang%2520Zhang%2520and%2520HaoFeng%2520Yang%2520and%2520Zhe%2520Wang%2520and%2520Chenjia%2520Bai%26entry.1292438233%3D%2520%2520Radiology%2520Report%2520Generation%2520%2528RRG%2529%2520is%2520an%2520important%2520research%2520topic%2520for%250Arelieving%2520radiologist%2527%2520heavy%2520workload.%2520Existing%2520RRG%2520models%2520mainly%2520rely%2520on%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520based%2520on%2520different%2520model%2520architectures%2520using%2520data%250Apairs%2520of%2520radiological%2520images%2520and%2520corresponding%2520radiologist-annotated%2520reports.%250ARecent%2520research%2520has%2520shifted%2520focus%2520to%2520post-training%2520improvements%252C%2520aligning%2520RRG%250Amodel%2520outputs%2520with%2520human%2520preferences%2520using%2520reinforcement%2520learning%2520%2528RL%2529.%250AHowever%252C%2520the%2520limited%2520data%2520coverage%2520of%2520high-quality%2520annotated%2520data%2520poses%2520risks%250Aof%2520overfitting%2520and%2520generalization.%2520This%2520paper%2520proposes%2520a%2520novel%2520Online%2520Iterative%250ASelf-Alignment%2520%2528OISA%2529%2520method%2520for%2520RRG%2520that%2520consists%2520of%2520four%2520stages%253A%250Aself-generation%2520of%2520diverse%2520data%252C%2520self-evaluation%2520for%2520multi-objective%2520preference%250Adata%252Cself-alignment%2520for%2520multi-objective%2520optimization%2520and%2520self-iteration%2520for%250Afurther%2520improvement.%2520Our%2520approach%2520allows%2520for%2520generating%2520varied%2520reports%2520tailored%250Ato%2520specific%2520clinical%2520objectives%252C%2520enhancing%2520the%2520overall%2520performance%2520of%2520the%2520RRG%250Amodel%2520iteratively.%2520Unlike%2520existing%2520methods%252C%2520our%2520frame-work%2520significantly%250Aincreases%2520data%2520quality%2520and%2520optimizes%2520performance%2520through%2520iterative%250Amulti-objective%2520optimization.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Asurpasses%2520previous%2520approaches%252C%2520achieving%2520state-of-the-art%2520performance%2520across%250Amultiple%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Iterative%20Self-Alignment%20for%20Radiology%20Report%20Generation&entry.906535625=Ting%20Xiao%20and%20Lei%20Shi%20and%20Yang%20Zhang%20and%20HaoFeng%20Yang%20and%20Zhe%20Wang%20and%20Chenjia%20Bai&entry.1292438233=%20%20Radiology%20Report%20Generation%20%28RRG%29%20is%20an%20important%20research%20topic%20for%0Arelieving%20radiologist%27%20heavy%20workload.%20Existing%20RRG%20models%20mainly%20rely%20on%0Asupervised%20fine-tuning%20%28SFT%29%20based%20on%20different%20model%20architectures%20using%20data%0Apairs%20of%20radiological%20images%20and%20corresponding%20radiologist-annotated%20reports.%0ARecent%20research%20has%20shifted%20focus%20to%20post-training%20improvements%2C%20aligning%20RRG%0Amodel%20outputs%20with%20human%20preferences%20using%20reinforcement%20learning%20%28RL%29.%0AHowever%2C%20the%20limited%20data%20coverage%20of%20high-quality%20annotated%20data%20poses%20risks%0Aof%20overfitting%20and%20generalization.%20This%20paper%20proposes%20a%20novel%20Online%20Iterative%0ASelf-Alignment%20%28OISA%29%20method%20for%20RRG%20that%20consists%20of%20four%20stages%3A%0Aself-generation%20of%20diverse%20data%2C%20self-evaluation%20for%20multi-objective%20preference%0Adata%2Cself-alignment%20for%20multi-objective%20optimization%20and%20self-iteration%20for%0Afurther%20improvement.%20Our%20approach%20allows%20for%20generating%20varied%20reports%20tailored%0Ato%20specific%20clinical%20objectives%2C%20enhancing%20the%20overall%20performance%20of%20the%20RRG%0Amodel%20iteratively.%20Unlike%20existing%20methods%2C%20our%20frame-work%20significantly%0Aincreases%20data%20quality%20and%20optimizes%20performance%20through%20iterative%0Amulti-objective%20optimization.%20Experimental%20results%20demonstrate%20that%20our%20method%0Asurpasses%20previous%20approaches%2C%20achieving%20state-of-the-art%20performance%20across%0Amultiple%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11983v2&entry.124074799=Read"},
{"title": "BigReg: An Efficient Registration Pipeline for High-Resolution X-Ray and\n  Light-Sheet Fluorescence Microscopy", "author": "Siyuan Mei and Fuxin Fan and Mareike Thies and Mingxuan Gu and Fabian Wagner and Oliver Aust and Ina Erceg and Zeynab Mirzaei and Georgiana Neag and Yipeng Sun and Yixing Huang and Andreas Maier", "abstract": "  Recently, X-ray microscopy (XRM) and light-sheet fluorescence microscopy\n(LSFM) have emerged as pivotal tools in preclinical research, particularly for\nstudying bone remodeling diseases such as osteoporosis. These modalities offer\nmicrometer-level resolution, and their integration allows for a complementary\nexamination of bone microstructures which is essential for analyzing functional\nchanges. However, registering high-resolution volumes from these independently\nscanned modalities poses substantial challenges, especially in real-world and\nreference-free scenarios. This paper presents BigReg, a fast, two-stage\npipeline designed for large-volume registration of XRM and LSFM data. The first\nstage involves extracting surface features and applying two successive point\ncloud-based methods for coarse alignment. The subsequent stage refines this\nalignment using a modified cross-correlation technique, achieving precise\nvolumetric registration. Evaluations using expert-annotated landmarks and\naugmented test data demonstrate that BigReg approaches the accuracy of\nlandmark-based registration with a landmark distance (LMD) of 8.36\\,\\textmu\nm\\,$\\pm$\\,0.12\\,\\textmu m and a landmark fitness (LM fitness) of\n85.71\\%\\,$\\pm$\\,1.02\\%. Moreover, BigReg can provide an optimal initialization\nfor mutual information-based methods which otherwise fail independently,\nfurther reducing LMD to 7.24\\,\\textmu m\\,$\\pm$\\,0.11\\,\\textmu m and increasing\nLM fitness to 93.90\\%\\,$\\pm$\\,0.77\\%. Ultimately, key microstructures, notably\nlacunae in XRM and bone cells in LSFM, are accurately aligned, enabling\nunprecedented insights into the pathology of osteoporosis.\n", "link": "http://arxiv.org/abs/2404.14807v2", "date": "2025-05-20", "relevancy": 2.4233, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5207}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4718}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BigReg%3A%20An%20Efficient%20Registration%20Pipeline%20for%20High-Resolution%20X-Ray%20and%0A%20%20Light-Sheet%20Fluorescence%20Microscopy&body=Title%3A%20BigReg%3A%20An%20Efficient%20Registration%20Pipeline%20for%20High-Resolution%20X-Ray%20and%0A%20%20Light-Sheet%20Fluorescence%20Microscopy%0AAuthor%3A%20Siyuan%20Mei%20and%20Fuxin%20Fan%20and%20Mareike%20Thies%20and%20Mingxuan%20Gu%20and%20Fabian%20Wagner%20and%20Oliver%20Aust%20and%20Ina%20Erceg%20and%20Zeynab%20Mirzaei%20and%20Georgiana%20Neag%20and%20Yipeng%20Sun%20and%20Yixing%20Huang%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Recently%2C%20X-ray%20microscopy%20%28XRM%29%20and%20light-sheet%20fluorescence%20microscopy%0A%28LSFM%29%20have%20emerged%20as%20pivotal%20tools%20in%20preclinical%20research%2C%20particularly%20for%0Astudying%20bone%20remodeling%20diseases%20such%20as%20osteoporosis.%20These%20modalities%20offer%0Amicrometer-level%20resolution%2C%20and%20their%20integration%20allows%20for%20a%20complementary%0Aexamination%20of%20bone%20microstructures%20which%20is%20essential%20for%20analyzing%20functional%0Achanges.%20However%2C%20registering%20high-resolution%20volumes%20from%20these%20independently%0Ascanned%20modalities%20poses%20substantial%20challenges%2C%20especially%20in%20real-world%20and%0Areference-free%20scenarios.%20This%20paper%20presents%20BigReg%2C%20a%20fast%2C%20two-stage%0Apipeline%20designed%20for%20large-volume%20registration%20of%20XRM%20and%20LSFM%20data.%20The%20first%0Astage%20involves%20extracting%20surface%20features%20and%20applying%20two%20successive%20point%0Acloud-based%20methods%20for%20coarse%20alignment.%20The%20subsequent%20stage%20refines%20this%0Aalignment%20using%20a%20modified%20cross-correlation%20technique%2C%20achieving%20precise%0Avolumetric%20registration.%20Evaluations%20using%20expert-annotated%20landmarks%20and%0Aaugmented%20test%20data%20demonstrate%20that%20BigReg%20approaches%20the%20accuracy%20of%0Alandmark-based%20registration%20with%20a%20landmark%20distance%20%28LMD%29%20of%208.36%5C%2C%5Ctextmu%0Am%5C%2C%24%5Cpm%24%5C%2C0.12%5C%2C%5Ctextmu%20m%20and%20a%20landmark%20fitness%20%28LM%20fitness%29%20of%0A85.71%5C%25%5C%2C%24%5Cpm%24%5C%2C1.02%5C%25.%20Moreover%2C%20BigReg%20can%20provide%20an%20optimal%20initialization%0Afor%20mutual%20information-based%20methods%20which%20otherwise%20fail%20independently%2C%0Afurther%20reducing%20LMD%20to%207.24%5C%2C%5Ctextmu%20m%5C%2C%24%5Cpm%24%5C%2C0.11%5C%2C%5Ctextmu%20m%20and%20increasing%0ALM%20fitness%20to%2093.90%5C%25%5C%2C%24%5Cpm%24%5C%2C0.77%5C%25.%20Ultimately%2C%20key%20microstructures%2C%20notably%0Alacunae%20in%20XRM%20and%20bone%20cells%20in%20LSFM%2C%20are%20accurately%20aligned%2C%20enabling%0Aunprecedented%20insights%20into%20the%20pathology%20of%20osteoporosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBigReg%253A%2520An%2520Efficient%2520Registration%2520Pipeline%2520for%2520High-Resolution%2520X-Ray%2520and%250A%2520%2520Light-Sheet%2520Fluorescence%2520Microscopy%26entry.906535625%3DSiyuan%2520Mei%2520and%2520Fuxin%2520Fan%2520and%2520Mareike%2520Thies%2520and%2520Mingxuan%2520Gu%2520and%2520Fabian%2520Wagner%2520and%2520Oliver%2520Aust%2520and%2520Ina%2520Erceg%2520and%2520Zeynab%2520Mirzaei%2520and%2520Georgiana%2520Neag%2520and%2520Yipeng%2520Sun%2520and%2520Yixing%2520Huang%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Recently%252C%2520X-ray%2520microscopy%2520%2528XRM%2529%2520and%2520light-sheet%2520fluorescence%2520microscopy%250A%2528LSFM%2529%2520have%2520emerged%2520as%2520pivotal%2520tools%2520in%2520preclinical%2520research%252C%2520particularly%2520for%250Astudying%2520bone%2520remodeling%2520diseases%2520such%2520as%2520osteoporosis.%2520These%2520modalities%2520offer%250Amicrometer-level%2520resolution%252C%2520and%2520their%2520integration%2520allows%2520for%2520a%2520complementary%250Aexamination%2520of%2520bone%2520microstructures%2520which%2520is%2520essential%2520for%2520analyzing%2520functional%250Achanges.%2520However%252C%2520registering%2520high-resolution%2520volumes%2520from%2520these%2520independently%250Ascanned%2520modalities%2520poses%2520substantial%2520challenges%252C%2520especially%2520in%2520real-world%2520and%250Areference-free%2520scenarios.%2520This%2520paper%2520presents%2520BigReg%252C%2520a%2520fast%252C%2520two-stage%250Apipeline%2520designed%2520for%2520large-volume%2520registration%2520of%2520XRM%2520and%2520LSFM%2520data.%2520The%2520first%250Astage%2520involves%2520extracting%2520surface%2520features%2520and%2520applying%2520two%2520successive%2520point%250Acloud-based%2520methods%2520for%2520coarse%2520alignment.%2520The%2520subsequent%2520stage%2520refines%2520this%250Aalignment%2520using%2520a%2520modified%2520cross-correlation%2520technique%252C%2520achieving%2520precise%250Avolumetric%2520registration.%2520Evaluations%2520using%2520expert-annotated%2520landmarks%2520and%250Aaugmented%2520test%2520data%2520demonstrate%2520that%2520BigReg%2520approaches%2520the%2520accuracy%2520of%250Alandmark-based%2520registration%2520with%2520a%2520landmark%2520distance%2520%2528LMD%2529%2520of%25208.36%255C%252C%255Ctextmu%250Am%255C%252C%2524%255Cpm%2524%255C%252C0.12%255C%252C%255Ctextmu%2520m%2520and%2520a%2520landmark%2520fitness%2520%2528LM%2520fitness%2529%2520of%250A85.71%255C%2525%255C%252C%2524%255Cpm%2524%255C%252C1.02%255C%2525.%2520Moreover%252C%2520BigReg%2520can%2520provide%2520an%2520optimal%2520initialization%250Afor%2520mutual%2520information-based%2520methods%2520which%2520otherwise%2520fail%2520independently%252C%250Afurther%2520reducing%2520LMD%2520to%25207.24%255C%252C%255Ctextmu%2520m%255C%252C%2524%255Cpm%2524%255C%252C0.11%255C%252C%255Ctextmu%2520m%2520and%2520increasing%250ALM%2520fitness%2520to%252093.90%255C%2525%255C%252C%2524%255Cpm%2524%255C%252C0.77%255C%2525.%2520Ultimately%252C%2520key%2520microstructures%252C%2520notably%250Alacunae%2520in%2520XRM%2520and%2520bone%2520cells%2520in%2520LSFM%252C%2520are%2520accurately%2520aligned%252C%2520enabling%250Aunprecedented%2520insights%2520into%2520the%2520pathology%2520of%2520osteoporosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BigReg%3A%20An%20Efficient%20Registration%20Pipeline%20for%20High-Resolution%20X-Ray%20and%0A%20%20Light-Sheet%20Fluorescence%20Microscopy&entry.906535625=Siyuan%20Mei%20and%20Fuxin%20Fan%20and%20Mareike%20Thies%20and%20Mingxuan%20Gu%20and%20Fabian%20Wagner%20and%20Oliver%20Aust%20and%20Ina%20Erceg%20and%20Zeynab%20Mirzaei%20and%20Georgiana%20Neag%20and%20Yipeng%20Sun%20and%20Yixing%20Huang%20and%20Andreas%20Maier&entry.1292438233=%20%20Recently%2C%20X-ray%20microscopy%20%28XRM%29%20and%20light-sheet%20fluorescence%20microscopy%0A%28LSFM%29%20have%20emerged%20as%20pivotal%20tools%20in%20preclinical%20research%2C%20particularly%20for%0Astudying%20bone%20remodeling%20diseases%20such%20as%20osteoporosis.%20These%20modalities%20offer%0Amicrometer-level%20resolution%2C%20and%20their%20integration%20allows%20for%20a%20complementary%0Aexamination%20of%20bone%20microstructures%20which%20is%20essential%20for%20analyzing%20functional%0Achanges.%20However%2C%20registering%20high-resolution%20volumes%20from%20these%20independently%0Ascanned%20modalities%20poses%20substantial%20challenges%2C%20especially%20in%20real-world%20and%0Areference-free%20scenarios.%20This%20paper%20presents%20BigReg%2C%20a%20fast%2C%20two-stage%0Apipeline%20designed%20for%20large-volume%20registration%20of%20XRM%20and%20LSFM%20data.%20The%20first%0Astage%20involves%20extracting%20surface%20features%20and%20applying%20two%20successive%20point%0Acloud-based%20methods%20for%20coarse%20alignment.%20The%20subsequent%20stage%20refines%20this%0Aalignment%20using%20a%20modified%20cross-correlation%20technique%2C%20achieving%20precise%0Avolumetric%20registration.%20Evaluations%20using%20expert-annotated%20landmarks%20and%0Aaugmented%20test%20data%20demonstrate%20that%20BigReg%20approaches%20the%20accuracy%20of%0Alandmark-based%20registration%20with%20a%20landmark%20distance%20%28LMD%29%20of%208.36%5C%2C%5Ctextmu%0Am%5C%2C%24%5Cpm%24%5C%2C0.12%5C%2C%5Ctextmu%20m%20and%20a%20landmark%20fitness%20%28LM%20fitness%29%20of%0A85.71%5C%25%5C%2C%24%5Cpm%24%5C%2C1.02%5C%25.%20Moreover%2C%20BigReg%20can%20provide%20an%20optimal%20initialization%0Afor%20mutual%20information-based%20methods%20which%20otherwise%20fail%20independently%2C%0Afurther%20reducing%20LMD%20to%207.24%5C%2C%5Ctextmu%20m%5C%2C%24%5Cpm%24%5C%2C0.11%5C%2C%5Ctextmu%20m%20and%20increasing%0ALM%20fitness%20to%2093.90%5C%25%5C%2C%24%5Cpm%24%5C%2C0.77%5C%25.%20Ultimately%2C%20key%20microstructures%2C%20notably%0Alacunae%20in%20XRM%20and%20bone%20cells%20in%20LSFM%2C%20are%20accurately%20aligned%2C%20enabling%0Aunprecedented%20insights%20into%20the%20pathology%20of%20osteoporosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14807v2&entry.124074799=Read"},
{"title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic\n  Worlds", "author": "Joel Currie and Gioele Migno and Enrico Piacenti and Maria Elena Giannaccini and Patric Bach and Davide De Tommaso and Agnieszka Wykowska", "abstract": "  We present a conceptual framework for training Vision-Language Models (VLMs)\nto perform Visual Perspective Taking (VPT), a core capability for embodied\ncognition essential for Human-Robot Interaction (HRI). As a first step toward\nthis goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,\nthat enables supervised learning for spatial reasoning tasks. Each instance\nincludes an RGB image, a natural language description, and a ground-truth 4X4\ntransformation matrix representing object pose. We focus on inferring Z-axis\ndistance as a foundational skill, with future extensions targeting full 6\nDegrees Of Freedom (DOFs) reasoning. The dataset is publicly available to\nsupport further research. This work serves as a foundational step toward\nembodied AI systems capable of spatial understanding in interactive human-robot\nscenarios.\n", "link": "http://arxiv.org/abs/2505.14366v1", "date": "2025-05-20", "relevancy": 2.4184, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6118}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6019}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Embodied%20Cognition%20in%20Robots%20via%20Spatially%20Grounded%20Synthetic%0A%20%20Worlds&body=Title%3A%20Towards%20Embodied%20Cognition%20in%20Robots%20via%20Spatially%20Grounded%20Synthetic%0A%20%20Worlds%0AAuthor%3A%20Joel%20Currie%20and%20Gioele%20Migno%20and%20Enrico%20Piacenti%20and%20Maria%20Elena%20Giannaccini%20and%20Patric%20Bach%20and%20Davide%20De%20Tommaso%20and%20Agnieszka%20Wykowska%0AAbstract%3A%20%20%20We%20present%20a%20conceptual%20framework%20for%20training%20Vision-Language%20Models%20%28VLMs%29%0Ato%20perform%20Visual%20Perspective%20Taking%20%28VPT%29%2C%20a%20core%20capability%20for%20embodied%0Acognition%20essential%20for%20Human-Robot%20Interaction%20%28HRI%29.%20As%20a%20first%20step%20toward%0Athis%20goal%2C%20we%20introduce%20a%20synthetic%20dataset%2C%20generated%20in%20NVIDIA%20Omniverse%2C%0Athat%20enables%20supervised%20learning%20for%20spatial%20reasoning%20tasks.%20Each%20instance%0Aincludes%20an%20RGB%20image%2C%20a%20natural%20language%20description%2C%20and%20a%20ground-truth%204X4%0Atransformation%20matrix%20representing%20object%20pose.%20We%20focus%20on%20inferring%20Z-axis%0Adistance%20as%20a%20foundational%20skill%2C%20with%20future%20extensions%20targeting%20full%206%0ADegrees%20Of%20Freedom%20%28DOFs%29%20reasoning.%20The%20dataset%20is%20publicly%20available%20to%0Asupport%20further%20research.%20This%20work%20serves%20as%20a%20foundational%20step%20toward%0Aembodied%20AI%20systems%20capable%20of%20spatial%20understanding%20in%20interactive%20human-robot%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Embodied%2520Cognition%2520in%2520Robots%2520via%2520Spatially%2520Grounded%2520Synthetic%250A%2520%2520Worlds%26entry.906535625%3DJoel%2520Currie%2520and%2520Gioele%2520Migno%2520and%2520Enrico%2520Piacenti%2520and%2520Maria%2520Elena%2520Giannaccini%2520and%2520Patric%2520Bach%2520and%2520Davide%2520De%2520Tommaso%2520and%2520Agnieszka%2520Wykowska%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520conceptual%2520framework%2520for%2520training%2520Vision-Language%2520Models%2520%2528VLMs%2529%250Ato%2520perform%2520Visual%2520Perspective%2520Taking%2520%2528VPT%2529%252C%2520a%2520core%2520capability%2520for%2520embodied%250Acognition%2520essential%2520for%2520Human-Robot%2520Interaction%2520%2528HRI%2529.%2520As%2520a%2520first%2520step%2520toward%250Athis%2520goal%252C%2520we%2520introduce%2520a%2520synthetic%2520dataset%252C%2520generated%2520in%2520NVIDIA%2520Omniverse%252C%250Athat%2520enables%2520supervised%2520learning%2520for%2520spatial%2520reasoning%2520tasks.%2520Each%2520instance%250Aincludes%2520an%2520RGB%2520image%252C%2520a%2520natural%2520language%2520description%252C%2520and%2520a%2520ground-truth%25204X4%250Atransformation%2520matrix%2520representing%2520object%2520pose.%2520We%2520focus%2520on%2520inferring%2520Z-axis%250Adistance%2520as%2520a%2520foundational%2520skill%252C%2520with%2520future%2520extensions%2520targeting%2520full%25206%250ADegrees%2520Of%2520Freedom%2520%2528DOFs%2529%2520reasoning.%2520The%2520dataset%2520is%2520publicly%2520available%2520to%250Asupport%2520further%2520research.%2520This%2520work%2520serves%2520as%2520a%2520foundational%2520step%2520toward%250Aembodied%2520AI%2520systems%2520capable%2520of%2520spatial%2520understanding%2520in%2520interactive%2520human-robot%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Embodied%20Cognition%20in%20Robots%20via%20Spatially%20Grounded%20Synthetic%0A%20%20Worlds&entry.906535625=Joel%20Currie%20and%20Gioele%20Migno%20and%20Enrico%20Piacenti%20and%20Maria%20Elena%20Giannaccini%20and%20Patric%20Bach%20and%20Davide%20De%20Tommaso%20and%20Agnieszka%20Wykowska&entry.1292438233=%20%20We%20present%20a%20conceptual%20framework%20for%20training%20Vision-Language%20Models%20%28VLMs%29%0Ato%20perform%20Visual%20Perspective%20Taking%20%28VPT%29%2C%20a%20core%20capability%20for%20embodied%0Acognition%20essential%20for%20Human-Robot%20Interaction%20%28HRI%29.%20As%20a%20first%20step%20toward%0Athis%20goal%2C%20we%20introduce%20a%20synthetic%20dataset%2C%20generated%20in%20NVIDIA%20Omniverse%2C%0Athat%20enables%20supervised%20learning%20for%20spatial%20reasoning%20tasks.%20Each%20instance%0Aincludes%20an%20RGB%20image%2C%20a%20natural%20language%20description%2C%20and%20a%20ground-truth%204X4%0Atransformation%20matrix%20representing%20object%20pose.%20We%20focus%20on%20inferring%20Z-axis%0Adistance%20as%20a%20foundational%20skill%2C%20with%20future%20extensions%20targeting%20full%206%0ADegrees%20Of%20Freedom%20%28DOFs%29%20reasoning.%20The%20dataset%20is%20publicly%20available%20to%0Asupport%20further%20research.%20This%20work%20serves%20as%20a%20foundational%20step%20toward%0Aembodied%20AI%20systems%20capable%20of%20spatial%20understanding%20in%20interactive%20human-robot%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14366v1&entry.124074799=Read"},
{"title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?", "author": "Bo Feng and Zhengfeng Lai and Shiyu Li and Zizhen Wang and Simon Wang and Ping Huang and Meng Cao", "abstract": "  Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.\n", "link": "http://arxiv.org/abs/2505.14321v1", "date": "2025-05-20", "relevancy": 2.415, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6244}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Down%20Video%20LLM%20Benchmarks%3A%20Knowledge%2C%20Spatial%20Perception%2C%20or%0A%20%20True%20Temporal%20Understanding%3F&body=Title%3A%20Breaking%20Down%20Video%20LLM%20Benchmarks%3A%20Knowledge%2C%20Spatial%20Perception%2C%20or%0A%20%20True%20Temporal%20Understanding%3F%0AAuthor%3A%20Bo%20Feng%20and%20Zhengfeng%20Lai%20and%20Shiyu%20Li%20and%20Zizhen%20Wang%20and%20Simon%20Wang%20and%20Ping%20Huang%20and%20Meng%20Cao%0AAbstract%3A%20%20%20Existing%20video%20understanding%20benchmarks%20often%20conflate%20knowledge-based%20and%0Apurely%20image-based%20questions%2C%20rather%20than%20clearly%20isolating%20a%20model%27s%20temporal%0Areasoning%20ability%2C%20which%20is%20the%20key%20aspect%20that%20distinguishes%20video%0Aunderstanding%20from%20other%20modalities.%20We%20identify%20two%20major%20limitations%20that%0Aobscure%20whether%20higher%20scores%20truly%20indicate%20stronger%20understanding%20of%20the%0Adynamic%20content%20in%20videos%3A%20%281%29%20strong%20language%20priors%2C%20where%20models%20can%20answer%0Aquestions%20without%20watching%20the%20video%3B%20and%20%282%29%20shuffling%20invariance%2C%20where%0Amodels%20maintain%20similar%20performance%20on%20certain%20questions%20even%20when%20video%20frames%0Aare%20temporally%20shuffled.%20To%20alleviate%20these%20issues%2C%20we%20propose%20VBenchComp%2C%20an%0Aautomated%20pipeline%20that%20categorizes%20questions%20into%20different%20domains%3A%0ALLM-Answerable%2C%20Semantic%2C%20and%20Temporal.%20Specifically%2C%20LLM-Answerable%20questions%0Acan%20be%20answered%20without%20viewing%20the%20video%3B%20Semantic%20questions%20remain%20answerable%0Aeven%20when%20the%20video%20frames%20are%20shuffled%3B%20and%20Temporal%20questions%20require%0Aunderstanding%20the%20correct%20temporal%20order%20of%20frames.%20The%20rest%20of%20the%20questions%0Aare%20labeled%20as%20Others.%20This%20can%20enable%20fine-grained%20evaluation%20of%20different%0Acapabilities%20of%20a%20video%20LLM.%20Our%20analysis%20reveals%20nuanced%20model%20weaknesses%20that%0Aare%20hidden%20by%20traditional%20overall%20scores%2C%20and%20we%20offer%20insights%20and%0Arecommendations%20for%20designing%20future%20benchmarks%20that%20more%20accurately%20assess%0Avideo%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Down%2520Video%2520LLM%2520Benchmarks%253A%2520Knowledge%252C%2520Spatial%2520Perception%252C%2520or%250A%2520%2520True%2520Temporal%2520Understanding%253F%26entry.906535625%3DBo%2520Feng%2520and%2520Zhengfeng%2520Lai%2520and%2520Shiyu%2520Li%2520and%2520Zizhen%2520Wang%2520and%2520Simon%2520Wang%2520and%2520Ping%2520Huang%2520and%2520Meng%2520Cao%26entry.1292438233%3D%2520%2520Existing%2520video%2520understanding%2520benchmarks%2520often%2520conflate%2520knowledge-based%2520and%250Apurely%2520image-based%2520questions%252C%2520rather%2520than%2520clearly%2520isolating%2520a%2520model%2527s%2520temporal%250Areasoning%2520ability%252C%2520which%2520is%2520the%2520key%2520aspect%2520that%2520distinguishes%2520video%250Aunderstanding%2520from%2520other%2520modalities.%2520We%2520identify%2520two%2520major%2520limitations%2520that%250Aobscure%2520whether%2520higher%2520scores%2520truly%2520indicate%2520stronger%2520understanding%2520of%2520the%250Adynamic%2520content%2520in%2520videos%253A%2520%25281%2529%2520strong%2520language%2520priors%252C%2520where%2520models%2520can%2520answer%250Aquestions%2520without%2520watching%2520the%2520video%253B%2520and%2520%25282%2529%2520shuffling%2520invariance%252C%2520where%250Amodels%2520maintain%2520similar%2520performance%2520on%2520certain%2520questions%2520even%2520when%2520video%2520frames%250Aare%2520temporally%2520shuffled.%2520To%2520alleviate%2520these%2520issues%252C%2520we%2520propose%2520VBenchComp%252C%2520an%250Aautomated%2520pipeline%2520that%2520categorizes%2520questions%2520into%2520different%2520domains%253A%250ALLM-Answerable%252C%2520Semantic%252C%2520and%2520Temporal.%2520Specifically%252C%2520LLM-Answerable%2520questions%250Acan%2520be%2520answered%2520without%2520viewing%2520the%2520video%253B%2520Semantic%2520questions%2520remain%2520answerable%250Aeven%2520when%2520the%2520video%2520frames%2520are%2520shuffled%253B%2520and%2520Temporal%2520questions%2520require%250Aunderstanding%2520the%2520correct%2520temporal%2520order%2520of%2520frames.%2520The%2520rest%2520of%2520the%2520questions%250Aare%2520labeled%2520as%2520Others.%2520This%2520can%2520enable%2520fine-grained%2520evaluation%2520of%2520different%250Acapabilities%2520of%2520a%2520video%2520LLM.%2520Our%2520analysis%2520reveals%2520nuanced%2520model%2520weaknesses%2520that%250Aare%2520hidden%2520by%2520traditional%2520overall%2520scores%252C%2520and%2520we%2520offer%2520insights%2520and%250Arecommendations%2520for%2520designing%2520future%2520benchmarks%2520that%2520more%2520accurately%2520assess%250Avideo%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Down%20Video%20LLM%20Benchmarks%3A%20Knowledge%2C%20Spatial%20Perception%2C%20or%0A%20%20True%20Temporal%20Understanding%3F&entry.906535625=Bo%20Feng%20and%20Zhengfeng%20Lai%20and%20Shiyu%20Li%20and%20Zizhen%20Wang%20and%20Simon%20Wang%20and%20Ping%20Huang%20and%20Meng%20Cao&entry.1292438233=%20%20Existing%20video%20understanding%20benchmarks%20often%20conflate%20knowledge-based%20and%0Apurely%20image-based%20questions%2C%20rather%20than%20clearly%20isolating%20a%20model%27s%20temporal%0Areasoning%20ability%2C%20which%20is%20the%20key%20aspect%20that%20distinguishes%20video%0Aunderstanding%20from%20other%20modalities.%20We%20identify%20two%20major%20limitations%20that%0Aobscure%20whether%20higher%20scores%20truly%20indicate%20stronger%20understanding%20of%20the%0Adynamic%20content%20in%20videos%3A%20%281%29%20strong%20language%20priors%2C%20where%20models%20can%20answer%0Aquestions%20without%20watching%20the%20video%3B%20and%20%282%29%20shuffling%20invariance%2C%20where%0Amodels%20maintain%20similar%20performance%20on%20certain%20questions%20even%20when%20video%20frames%0Aare%20temporally%20shuffled.%20To%20alleviate%20these%20issues%2C%20we%20propose%20VBenchComp%2C%20an%0Aautomated%20pipeline%20that%20categorizes%20questions%20into%20different%20domains%3A%0ALLM-Answerable%2C%20Semantic%2C%20and%20Temporal.%20Specifically%2C%20LLM-Answerable%20questions%0Acan%20be%20answered%20without%20viewing%20the%20video%3B%20Semantic%20questions%20remain%20answerable%0Aeven%20when%20the%20video%20frames%20are%20shuffled%3B%20and%20Temporal%20questions%20require%0Aunderstanding%20the%20correct%20temporal%20order%20of%20frames.%20The%20rest%20of%20the%20questions%0Aare%20labeled%20as%20Others.%20This%20can%20enable%20fine-grained%20evaluation%20of%20different%0Acapabilities%20of%20a%20video%20LLM.%20Our%20analysis%20reveals%20nuanced%20model%20weaknesses%20that%0Aare%20hidden%20by%20traditional%20overall%20scores%2C%20and%20we%20offer%20insights%20and%0Arecommendations%20for%20designing%20future%20benchmarks%20that%20more%20accurately%20assess%0Avideo%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14321v1&entry.124074799=Read"},
{"title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal\n  Understanding and Generation", "author": "Rui Tian and Mingfei Gao and Mingze Xu and Jiaming Hu and Jiasen Lu and Zuxuan Wu and Yinfei Yang and Afshin Dehghan", "abstract": "  We introduce UniGen, a unified multimodal large language model (MLLM) capable\nof image understanding and generation. We study the full training pipeline of\nUniGen from a data-centric perspective, including multi-stage pre-training,\nsupervised fine-tuning, and direct preference optimization. More importantly,\nwe propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time\nscaling, which significantly boosts UniGen's image generation quality using a\nsimple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act\nas both image generator and verifier at test time, assessing the semantic\nalignment between a text prompt and its generated image in a step-by-step CoT\nmanner. Trained entirely on open-source datasets across all stages, UniGen\nachieves state-of-the-art performance on a range of image understanding and\ngeneration benchmarks, with a final score of 0.78 on GenEval and 85.19 on\nDPG-Bench. Through extensive ablation studies, our work provides actionable\ninsights and addresses key challenges in the full life cycle of building\nunified MLLMs, contributing meaningful directions to the future research.\n", "link": "http://arxiv.org/abs/2505.14682v1", "date": "2025-05-20", "relevancy": 2.4087, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.619}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.602}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniGen%3A%20Enhanced%20Training%20%26%20Test-Time%20Strategies%20for%20Unified%20Multimodal%0A%20%20Understanding%20and%20Generation&body=Title%3A%20UniGen%3A%20Enhanced%20Training%20%26%20Test-Time%20Strategies%20for%20Unified%20Multimodal%0A%20%20Understanding%20and%20Generation%0AAuthor%3A%20Rui%20Tian%20and%20Mingfei%20Gao%20and%20Mingze%20Xu%20and%20Jiaming%20Hu%20and%20Jiasen%20Lu%20and%20Zuxuan%20Wu%20and%20Yinfei%20Yang%20and%20Afshin%20Dehghan%0AAbstract%3A%20%20%20We%20introduce%20UniGen%2C%20a%20unified%20multimodal%20large%20language%20model%20%28MLLM%29%20capable%0Aof%20image%20understanding%20and%20generation.%20We%20study%20the%20full%20training%20pipeline%20of%0AUniGen%20from%20a%20data-centric%20perspective%2C%20including%20multi-stage%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20direct%20preference%20optimization.%20More%20importantly%2C%0Awe%20propose%20a%20new%20Chain-of-Thought%20Verification%20%28CoT-V%29%20strategy%20for%20test-time%0Ascaling%2C%20which%20significantly%20boosts%20UniGen%27s%20image%20generation%20quality%20using%20a%0Asimple%20Best-of-N%20test-time%20strategy.%20Specifically%2C%20CoT-V%20enables%20UniGen%20to%20act%0Aas%20both%20image%20generator%20and%20verifier%20at%20test%20time%2C%20assessing%20the%20semantic%0Aalignment%20between%20a%20text%20prompt%20and%20its%20generated%20image%20in%20a%20step-by-step%20CoT%0Amanner.%20Trained%20entirely%20on%20open-source%20datasets%20across%20all%20stages%2C%20UniGen%0Aachieves%20state-of-the-art%20performance%20on%20a%20range%20of%20image%20understanding%20and%0Ageneration%20benchmarks%2C%20with%20a%20final%20score%20of%200.78%20on%20GenEval%20and%2085.19%20on%0ADPG-Bench.%20Through%20extensive%20ablation%20studies%2C%20our%20work%20provides%20actionable%0Ainsights%20and%20addresses%20key%20challenges%20in%20the%20full%20life%20cycle%20of%20building%0Aunified%20MLLMs%2C%20contributing%20meaningful%20directions%20to%20the%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniGen%253A%2520Enhanced%2520Training%2520%2526%2520Test-Time%2520Strategies%2520for%2520Unified%2520Multimodal%250A%2520%2520Understanding%2520and%2520Generation%26entry.906535625%3DRui%2520Tian%2520and%2520Mingfei%2520Gao%2520and%2520Mingze%2520Xu%2520and%2520Jiaming%2520Hu%2520and%2520Jiasen%2520Lu%2520and%2520Zuxuan%2520Wu%2520and%2520Yinfei%2520Yang%2520and%2520Afshin%2520Dehghan%26entry.1292438233%3D%2520%2520We%2520introduce%2520UniGen%252C%2520a%2520unified%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520capable%250Aof%2520image%2520understanding%2520and%2520generation.%2520We%2520study%2520the%2520full%2520training%2520pipeline%2520of%250AUniGen%2520from%2520a%2520data-centric%2520perspective%252C%2520including%2520multi-stage%2520pre-training%252C%250Asupervised%2520fine-tuning%252C%2520and%2520direct%2520preference%2520optimization.%2520More%2520importantly%252C%250Awe%2520propose%2520a%2520new%2520Chain-of-Thought%2520Verification%2520%2528CoT-V%2529%2520strategy%2520for%2520test-time%250Ascaling%252C%2520which%2520significantly%2520boosts%2520UniGen%2527s%2520image%2520generation%2520quality%2520using%2520a%250Asimple%2520Best-of-N%2520test-time%2520strategy.%2520Specifically%252C%2520CoT-V%2520enables%2520UniGen%2520to%2520act%250Aas%2520both%2520image%2520generator%2520and%2520verifier%2520at%2520test%2520time%252C%2520assessing%2520the%2520semantic%250Aalignment%2520between%2520a%2520text%2520prompt%2520and%2520its%2520generated%2520image%2520in%2520a%2520step-by-step%2520CoT%250Amanner.%2520Trained%2520entirely%2520on%2520open-source%2520datasets%2520across%2520all%2520stages%252C%2520UniGen%250Aachieves%2520state-of-the-art%2520performance%2520on%2520a%2520range%2520of%2520image%2520understanding%2520and%250Ageneration%2520benchmarks%252C%2520with%2520a%2520final%2520score%2520of%25200.78%2520on%2520GenEval%2520and%252085.19%2520on%250ADPG-Bench.%2520Through%2520extensive%2520ablation%2520studies%252C%2520our%2520work%2520provides%2520actionable%250Ainsights%2520and%2520addresses%2520key%2520challenges%2520in%2520the%2520full%2520life%2520cycle%2520of%2520building%250Aunified%2520MLLMs%252C%2520contributing%2520meaningful%2520directions%2520to%2520the%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniGen%3A%20Enhanced%20Training%20%26%20Test-Time%20Strategies%20for%20Unified%20Multimodal%0A%20%20Understanding%20and%20Generation&entry.906535625=Rui%20Tian%20and%20Mingfei%20Gao%20and%20Mingze%20Xu%20and%20Jiaming%20Hu%20and%20Jiasen%20Lu%20and%20Zuxuan%20Wu%20and%20Yinfei%20Yang%20and%20Afshin%20Dehghan&entry.1292438233=%20%20We%20introduce%20UniGen%2C%20a%20unified%20multimodal%20large%20language%20model%20%28MLLM%29%20capable%0Aof%20image%20understanding%20and%20generation.%20We%20study%20the%20full%20training%20pipeline%20of%0AUniGen%20from%20a%20data-centric%20perspective%2C%20including%20multi-stage%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20direct%20preference%20optimization.%20More%20importantly%2C%0Awe%20propose%20a%20new%20Chain-of-Thought%20Verification%20%28CoT-V%29%20strategy%20for%20test-time%0Ascaling%2C%20which%20significantly%20boosts%20UniGen%27s%20image%20generation%20quality%20using%20a%0Asimple%20Best-of-N%20test-time%20strategy.%20Specifically%2C%20CoT-V%20enables%20UniGen%20to%20act%0Aas%20both%20image%20generator%20and%20verifier%20at%20test%20time%2C%20assessing%20the%20semantic%0Aalignment%20between%20a%20text%20prompt%20and%20its%20generated%20image%20in%20a%20step-by-step%20CoT%0Amanner.%20Trained%20entirely%20on%20open-source%20datasets%20across%20all%20stages%2C%20UniGen%0Aachieves%20state-of-the-art%20performance%20on%20a%20range%20of%20image%20understanding%20and%0Ageneration%20benchmarks%2C%20with%20a%20final%20score%20of%200.78%20on%20GenEval%20and%2085.19%20on%0ADPG-Bench.%20Through%20extensive%20ablation%20studies%2C%20our%20work%20provides%20actionable%0Ainsights%20and%20addresses%20key%20challenges%20in%20the%20full%20life%20cycle%20of%20building%0Aunified%20MLLMs%2C%20contributing%20meaningful%20directions%20to%20the%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14682v1&entry.124074799=Read"},
{"title": "CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation", "author": "Ziyue Liu and Ruijie Zhang and Zhengyang Wang and Zi Yang and Paul Hovland and Bogdan Nicolae and Franck Cappello and Zheng Zhang", "abstract": "  The full-size MLPs and the projection layers in attention introduce\ntremendous model sizes of large language models (LLMs), imposing extremely\ndemanding needs of computational resources in the pre-training stage. However,\nwe empirically observe that the activations of pre-trained LLMs exhibit\nlow-rank property. Motivated by such observations, we propose CoLA and its\nmemory-efficient implementation, CoLA-M, to replace these full-size layers with\ncompute-efficient auto-encoders that naturally enforce low-rank activations\nthroughout training. This fundamental architectural change eliminates the\nactivation redundancy and significantly boosts model capacity and training\nefficiency. Experiments on LLaMA models with 60 million to 7 billion parameters\nshow that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves\ntraining throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level\nperformance. CoLA-M further squeezes memory cost without sacrificing\nthroughput, offering a pre-training approach with collectively superior\nparameter, computing, and memory efficiency. The LLMs produced are also $\\bf\n2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on\nresource-constrained platforms.\n", "link": "http://arxiv.org/abs/2502.10940v2", "date": "2025-05-20", "relevancy": 2.4001, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4907}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoLA%3A%20Compute-Efficient%20Pre-Training%20of%20LLMs%20via%20Low-Rank%20Activation&body=Title%3A%20CoLA%3A%20Compute-Efficient%20Pre-Training%20of%20LLMs%20via%20Low-Rank%20Activation%0AAuthor%3A%20Ziyue%20Liu%20and%20Ruijie%20Zhang%20and%20Zhengyang%20Wang%20and%20Zi%20Yang%20and%20Paul%20Hovland%20and%20Bogdan%20Nicolae%20and%20Franck%20Cappello%20and%20Zheng%20Zhang%0AAbstract%3A%20%20%20The%20full-size%20MLPs%20and%20the%20projection%20layers%20in%20attention%20introduce%0Atremendous%20model%20sizes%20of%20large%20language%20models%20%28LLMs%29%2C%20imposing%20extremely%0Ademanding%20needs%20of%20computational%20resources%20in%20the%20pre-training%20stage.%20However%2C%0Awe%20empirically%20observe%20that%20the%20activations%20of%20pre-trained%20LLMs%20exhibit%0Alow-rank%20property.%20Motivated%20by%20such%20observations%2C%20we%20propose%20CoLA%20and%20its%0Amemory-efficient%20implementation%2C%20CoLA-M%2C%20to%20replace%20these%20full-size%20layers%20with%0Acompute-efficient%20auto-encoders%20that%20naturally%20enforce%20low-rank%20activations%0Athroughout%20training.%20This%20fundamental%20architectural%20change%20eliminates%20the%0Aactivation%20redundancy%20and%20significantly%20boosts%20model%20capacity%20and%20training%0Aefficiency.%20Experiments%20on%20LLaMA%20models%20with%2060%20million%20to%207%20billion%20parameters%0Ashow%20that%20CoLA%20reduces%20the%20computing%20cost%20by%20%24%5Cbf%202%5Cpmb%7B%5Ctimes%7D%24%20and%20improves%0Atraining%20throughput%20by%20%24%5Cbf%201.86%5Cpmb%7B%5Ctimes%7D%24%20while%20maintaining%20full-rank%20level%0Aperformance.%20CoLA-M%20further%20squeezes%20memory%20cost%20without%20sacrificing%0Athroughput%2C%20offering%20a%20pre-training%20approach%20with%20collectively%20superior%0Aparameter%2C%20computing%2C%20and%20memory%20efficiency.%20The%20LLMs%20produced%20are%20also%20%24%5Cbf%0A2%5Cpmb%7B%5Ctimes%7D%24%20smaller%2C%20enabling%20faster%20inference%20with%20lower%20memory%20cost%20on%0Aresource-constrained%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.10940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoLA%253A%2520Compute-Efficient%2520Pre-Training%2520of%2520LLMs%2520via%2520Low-Rank%2520Activation%26entry.906535625%3DZiyue%2520Liu%2520and%2520Ruijie%2520Zhang%2520and%2520Zhengyang%2520Wang%2520and%2520Zi%2520Yang%2520and%2520Paul%2520Hovland%2520and%2520Bogdan%2520Nicolae%2520and%2520Franck%2520Cappello%2520and%2520Zheng%2520Zhang%26entry.1292438233%3D%2520%2520The%2520full-size%2520MLPs%2520and%2520the%2520projection%2520layers%2520in%2520attention%2520introduce%250Atremendous%2520model%2520sizes%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520imposing%2520extremely%250Ademanding%2520needs%2520of%2520computational%2520resources%2520in%2520the%2520pre-training%2520stage.%2520However%252C%250Awe%2520empirically%2520observe%2520that%2520the%2520activations%2520of%2520pre-trained%2520LLMs%2520exhibit%250Alow-rank%2520property.%2520Motivated%2520by%2520such%2520observations%252C%2520we%2520propose%2520CoLA%2520and%2520its%250Amemory-efficient%2520implementation%252C%2520CoLA-M%252C%2520to%2520replace%2520these%2520full-size%2520layers%2520with%250Acompute-efficient%2520auto-encoders%2520that%2520naturally%2520enforce%2520low-rank%2520activations%250Athroughout%2520training.%2520This%2520fundamental%2520architectural%2520change%2520eliminates%2520the%250Aactivation%2520redundancy%2520and%2520significantly%2520boosts%2520model%2520capacity%2520and%2520training%250Aefficiency.%2520Experiments%2520on%2520LLaMA%2520models%2520with%252060%2520million%2520to%25207%2520billion%2520parameters%250Ashow%2520that%2520CoLA%2520reduces%2520the%2520computing%2520cost%2520by%2520%2524%255Cbf%25202%255Cpmb%257B%255Ctimes%257D%2524%2520and%2520improves%250Atraining%2520throughput%2520by%2520%2524%255Cbf%25201.86%255Cpmb%257B%255Ctimes%257D%2524%2520while%2520maintaining%2520full-rank%2520level%250Aperformance.%2520CoLA-M%2520further%2520squeezes%2520memory%2520cost%2520without%2520sacrificing%250Athroughput%252C%2520offering%2520a%2520pre-training%2520approach%2520with%2520collectively%2520superior%250Aparameter%252C%2520computing%252C%2520and%2520memory%2520efficiency.%2520The%2520LLMs%2520produced%2520are%2520also%2520%2524%255Cbf%250A2%255Cpmb%257B%255Ctimes%257D%2524%2520smaller%252C%2520enabling%2520faster%2520inference%2520with%2520lower%2520memory%2520cost%2520on%250Aresource-constrained%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoLA%3A%20Compute-Efficient%20Pre-Training%20of%20LLMs%20via%20Low-Rank%20Activation&entry.906535625=Ziyue%20Liu%20and%20Ruijie%20Zhang%20and%20Zhengyang%20Wang%20and%20Zi%20Yang%20and%20Paul%20Hovland%20and%20Bogdan%20Nicolae%20and%20Franck%20Cappello%20and%20Zheng%20Zhang&entry.1292438233=%20%20The%20full-size%20MLPs%20and%20the%20projection%20layers%20in%20attention%20introduce%0Atremendous%20model%20sizes%20of%20large%20language%20models%20%28LLMs%29%2C%20imposing%20extremely%0Ademanding%20needs%20of%20computational%20resources%20in%20the%20pre-training%20stage.%20However%2C%0Awe%20empirically%20observe%20that%20the%20activations%20of%20pre-trained%20LLMs%20exhibit%0Alow-rank%20property.%20Motivated%20by%20such%20observations%2C%20we%20propose%20CoLA%20and%20its%0Amemory-efficient%20implementation%2C%20CoLA-M%2C%20to%20replace%20these%20full-size%20layers%20with%0Acompute-efficient%20auto-encoders%20that%20naturally%20enforce%20low-rank%20activations%0Athroughout%20training.%20This%20fundamental%20architectural%20change%20eliminates%20the%0Aactivation%20redundancy%20and%20significantly%20boosts%20model%20capacity%20and%20training%0Aefficiency.%20Experiments%20on%20LLaMA%20models%20with%2060%20million%20to%207%20billion%20parameters%0Ashow%20that%20CoLA%20reduces%20the%20computing%20cost%20by%20%24%5Cbf%202%5Cpmb%7B%5Ctimes%7D%24%20and%20improves%0Atraining%20throughput%20by%20%24%5Cbf%201.86%5Cpmb%7B%5Ctimes%7D%24%20while%20maintaining%20full-rank%20level%0Aperformance.%20CoLA-M%20further%20squeezes%20memory%20cost%20without%20sacrificing%0Athroughput%2C%20offering%20a%20pre-training%20approach%20with%20collectively%20superior%0Aparameter%2C%20computing%2C%20and%20memory%20efficiency.%20The%20LLMs%20produced%20are%20also%20%24%5Cbf%0A2%5Cpmb%7B%5Ctimes%7D%24%20smaller%2C%20enabling%20faster%20inference%20with%20lower%20memory%20cost%20on%0Aresource-constrained%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.10940v2&entry.124074799=Read"},
{"title": "Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and\n  GRPO-based Method", "author": "Xinshen Zhang and Zhen Ye and Xu Zheng", "abstract": "  Omnidirectional images (ODIs), with their 360{\\deg} field of view, provide\nunparalleled spatial awareness for immersive applications like augmented\nreality and embodied AI. However, the capability of existing multi-modal large\nlanguage models (MLLMs) to comprehend and reason about such panoramic scenes\nremains underexplored. This paper addresses this gap by introducing OmniVQA,\nthe first dataset and conducting the first benchmark for omnidirectional visual\nquestion answering. Our evaluation of state-of-the-art MLLMs reveals\nsignificant limitations in handling omnidirectional visual question answering,\nhighlighting persistent challenges in object localization, feature extraction,\nand hallucination suppression within panoramic contexts. These results\nunderscore the disconnect between current MLLM capabilities and the demands of\nomnidirectional visual understanding, which calls for dedicated architectural\nor training innovations tailored to 360{\\deg} imagery. Building on the OmniVQA\ndataset and benchmark, we further introduce a rule-based reinforcement learning\nmethod, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group\nrelative policy optimization (GRPO) by proposing three novel reward functions:\n(1) reasoning process similarity reward, (2) answer semantic accuracy reward,\nand (3) structured format compliance reward. Extensive experiments on our\nOmniVQA demonstrate the superiority of our proposed method in omnidirectional\nspace (+6% improvement).\n", "link": "http://arxiv.org/abs/2505.14197v1", "date": "2025-05-20", "relevancy": 2.3971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Omnidirectional%20Reasoning%20with%20360-R1%3A%20A%20Dataset%2C%20Benchmark%2C%20and%0A%20%20GRPO-based%20Method&body=Title%3A%20Towards%20Omnidirectional%20Reasoning%20with%20360-R1%3A%20A%20Dataset%2C%20Benchmark%2C%20and%0A%20%20GRPO-based%20Method%0AAuthor%3A%20Xinshen%20Zhang%20and%20Zhen%20Ye%20and%20Xu%20Zheng%0AAbstract%3A%20%20%20Omnidirectional%20images%20%28ODIs%29%2C%20with%20their%20360%7B%5Cdeg%7D%20field%20of%20view%2C%20provide%0Aunparalleled%20spatial%20awareness%20for%20immersive%20applications%20like%20augmented%0Areality%20and%20embodied%20AI.%20However%2C%20the%20capability%20of%20existing%20multi-modal%20large%0Alanguage%20models%20%28MLLMs%29%20to%20comprehend%20and%20reason%20about%20such%20panoramic%20scenes%0Aremains%20underexplored.%20This%20paper%20addresses%20this%20gap%20by%20introducing%20OmniVQA%2C%0Athe%20first%20dataset%20and%20conducting%20the%20first%20benchmark%20for%20omnidirectional%20visual%0Aquestion%20answering.%20Our%20evaluation%20of%20state-of-the-art%20MLLMs%20reveals%0Asignificant%20limitations%20in%20handling%20omnidirectional%20visual%20question%20answering%2C%0Ahighlighting%20persistent%20challenges%20in%20object%20localization%2C%20feature%20extraction%2C%0Aand%20hallucination%20suppression%20within%20panoramic%20contexts.%20These%20results%0Aunderscore%20the%20disconnect%20between%20current%20MLLM%20capabilities%20and%20the%20demands%20of%0Aomnidirectional%20visual%20understanding%2C%20which%20calls%20for%20dedicated%20architectural%0Aor%20training%20innovations%20tailored%20to%20360%7B%5Cdeg%7D%20imagery.%20Building%20on%20the%20OmniVQA%0Adataset%20and%20benchmark%2C%20we%20further%20introduce%20a%20rule-based%20reinforcement%20learning%0Amethod%2C%20360-R1%2C%20based%20on%20Qwen2.5-VL-Instruct.%20Concretely%2C%20we%20modify%20the%20group%0Arelative%20policy%20optimization%20%28GRPO%29%20by%20proposing%20three%20novel%20reward%20functions%3A%0A%281%29%20reasoning%20process%20similarity%20reward%2C%20%282%29%20answer%20semantic%20accuracy%20reward%2C%0Aand%20%283%29%20structured%20format%20compliance%20reward.%20Extensive%20experiments%20on%20our%0AOmniVQA%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20in%20omnidirectional%0Aspace%20%28%2B6%25%20improvement%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Omnidirectional%2520Reasoning%2520with%2520360-R1%253A%2520A%2520Dataset%252C%2520Benchmark%252C%2520and%250A%2520%2520GRPO-based%2520Method%26entry.906535625%3DXinshen%2520Zhang%2520and%2520Zhen%2520Ye%2520and%2520Xu%2520Zheng%26entry.1292438233%3D%2520%2520Omnidirectional%2520images%2520%2528ODIs%2529%252C%2520with%2520their%2520360%257B%255Cdeg%257D%2520field%2520of%2520view%252C%2520provide%250Aunparalleled%2520spatial%2520awareness%2520for%2520immersive%2520applications%2520like%2520augmented%250Areality%2520and%2520embodied%2520AI.%2520However%252C%2520the%2520capability%2520of%2520existing%2520multi-modal%2520large%250Alanguage%2520models%2520%2528MLLMs%2529%2520to%2520comprehend%2520and%2520reason%2520about%2520such%2520panoramic%2520scenes%250Aremains%2520underexplored.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%2520introducing%2520OmniVQA%252C%250Athe%2520first%2520dataset%2520and%2520conducting%2520the%2520first%2520benchmark%2520for%2520omnidirectional%2520visual%250Aquestion%2520answering.%2520Our%2520evaluation%2520of%2520state-of-the-art%2520MLLMs%2520reveals%250Asignificant%2520limitations%2520in%2520handling%2520omnidirectional%2520visual%2520question%2520answering%252C%250Ahighlighting%2520persistent%2520challenges%2520in%2520object%2520localization%252C%2520feature%2520extraction%252C%250Aand%2520hallucination%2520suppression%2520within%2520panoramic%2520contexts.%2520These%2520results%250Aunderscore%2520the%2520disconnect%2520between%2520current%2520MLLM%2520capabilities%2520and%2520the%2520demands%2520of%250Aomnidirectional%2520visual%2520understanding%252C%2520which%2520calls%2520for%2520dedicated%2520architectural%250Aor%2520training%2520innovations%2520tailored%2520to%2520360%257B%255Cdeg%257D%2520imagery.%2520Building%2520on%2520the%2520OmniVQA%250Adataset%2520and%2520benchmark%252C%2520we%2520further%2520introduce%2520a%2520rule-based%2520reinforcement%2520learning%250Amethod%252C%2520360-R1%252C%2520based%2520on%2520Qwen2.5-VL-Instruct.%2520Concretely%252C%2520we%2520modify%2520the%2520group%250Arelative%2520policy%2520optimization%2520%2528GRPO%2529%2520by%2520proposing%2520three%2520novel%2520reward%2520functions%253A%250A%25281%2529%2520reasoning%2520process%2520similarity%2520reward%252C%2520%25282%2529%2520answer%2520semantic%2520accuracy%2520reward%252C%250Aand%2520%25283%2529%2520structured%2520format%2520compliance%2520reward.%2520Extensive%2520experiments%2520on%2520our%250AOmniVQA%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520method%2520in%2520omnidirectional%250Aspace%2520%2528%252B6%2525%2520improvement%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Omnidirectional%20Reasoning%20with%20360-R1%3A%20A%20Dataset%2C%20Benchmark%2C%20and%0A%20%20GRPO-based%20Method&entry.906535625=Xinshen%20Zhang%20and%20Zhen%20Ye%20and%20Xu%20Zheng&entry.1292438233=%20%20Omnidirectional%20images%20%28ODIs%29%2C%20with%20their%20360%7B%5Cdeg%7D%20field%20of%20view%2C%20provide%0Aunparalleled%20spatial%20awareness%20for%20immersive%20applications%20like%20augmented%0Areality%20and%20embodied%20AI.%20However%2C%20the%20capability%20of%20existing%20multi-modal%20large%0Alanguage%20models%20%28MLLMs%29%20to%20comprehend%20and%20reason%20about%20such%20panoramic%20scenes%0Aremains%20underexplored.%20This%20paper%20addresses%20this%20gap%20by%20introducing%20OmniVQA%2C%0Athe%20first%20dataset%20and%20conducting%20the%20first%20benchmark%20for%20omnidirectional%20visual%0Aquestion%20answering.%20Our%20evaluation%20of%20state-of-the-art%20MLLMs%20reveals%0Asignificant%20limitations%20in%20handling%20omnidirectional%20visual%20question%20answering%2C%0Ahighlighting%20persistent%20challenges%20in%20object%20localization%2C%20feature%20extraction%2C%0Aand%20hallucination%20suppression%20within%20panoramic%20contexts.%20These%20results%0Aunderscore%20the%20disconnect%20between%20current%20MLLM%20capabilities%20and%20the%20demands%20of%0Aomnidirectional%20visual%20understanding%2C%20which%20calls%20for%20dedicated%20architectural%0Aor%20training%20innovations%20tailored%20to%20360%7B%5Cdeg%7D%20imagery.%20Building%20on%20the%20OmniVQA%0Adataset%20and%20benchmark%2C%20we%20further%20introduce%20a%20rule-based%20reinforcement%20learning%0Amethod%2C%20360-R1%2C%20based%20on%20Qwen2.5-VL-Instruct.%20Concretely%2C%20we%20modify%20the%20group%0Arelative%20policy%20optimization%20%28GRPO%29%20by%20proposing%20three%20novel%20reward%20functions%3A%0A%281%29%20reasoning%20process%20similarity%20reward%2C%20%282%29%20answer%20semantic%20accuracy%20reward%2C%0Aand%20%283%29%20structured%20format%20compliance%20reward.%20Extensive%20experiments%20on%20our%0AOmniVQA%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%20in%20omnidirectional%0Aspace%20%28%2B6%25%20improvement%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14197v1&entry.124074799=Read"},
{"title": "Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask\n  Scheduling in LLMs", "author": "Zhipeng Yang and Junzhuo Li and Siyu Xia and Xuming Hu", "abstract": "  We show that large language models (LLMs) exhibit an $\\textit{internal\nchain-of-thought}$: they sequentially decompose and execute composite tasks\nlayer-by-layer. Two claims ground our study: (i) distinct subtasks are learned\nat different network depths, and (ii) these subtasks are executed sequentially\nacross layers. On a benchmark of 15 two-step composite tasks, we employ\nlayer-from context-masking and propose a novel cross-task patching method,\nconfirming (i). To examine claim (ii), we apply LogitLens to decode hidden\nstates, revealing a consistent layerwise execution pattern. We further\nreplicate our analysis on the real-world $\\text{TRACE}$ benchmark, observing\nthe same stepwise dynamics. Together, our results enhance LLMs transparency by\nshowing their capacity to internally plan and execute subtasks (or\ninstructions), opening avenues for fine-grained, instruction-level activation\nsteering.\n", "link": "http://arxiv.org/abs/2505.14530v1", "date": "2025-05-20", "relevancy": 2.3854, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Internal%20Chain-of-Thought%3A%20Empirical%20Evidence%20for%20Layer-wise%20Subtask%0A%20%20Scheduling%20in%20LLMs&body=Title%3A%20Internal%20Chain-of-Thought%3A%20Empirical%20Evidence%20for%20Layer-wise%20Subtask%0A%20%20Scheduling%20in%20LLMs%0AAuthor%3A%20Zhipeng%20Yang%20and%20Junzhuo%20Li%20and%20Siyu%20Xia%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20We%20show%20that%20large%20language%20models%20%28LLMs%29%20exhibit%20an%20%24%5Ctextit%7Binternal%0Achain-of-thought%7D%24%3A%20they%20sequentially%20decompose%20and%20execute%20composite%20tasks%0Alayer-by-layer.%20Two%20claims%20ground%20our%20study%3A%20%28i%29%20distinct%20subtasks%20are%20learned%0Aat%20different%20network%20depths%2C%20and%20%28ii%29%20these%20subtasks%20are%20executed%20sequentially%0Aacross%20layers.%20On%20a%20benchmark%20of%2015%20two-step%20composite%20tasks%2C%20we%20employ%0Alayer-from%20context-masking%20and%20propose%20a%20novel%20cross-task%20patching%20method%2C%0Aconfirming%20%28i%29.%20To%20examine%20claim%20%28ii%29%2C%20we%20apply%20LogitLens%20to%20decode%20hidden%0Astates%2C%20revealing%20a%20consistent%20layerwise%20execution%20pattern.%20We%20further%0Areplicate%20our%20analysis%20on%20the%20real-world%20%24%5Ctext%7BTRACE%7D%24%20benchmark%2C%20observing%0Athe%20same%20stepwise%20dynamics.%20Together%2C%20our%20results%20enhance%20LLMs%20transparency%20by%0Ashowing%20their%20capacity%20to%20internally%20plan%20and%20execute%20subtasks%20%28or%0Ainstructions%29%2C%20opening%20avenues%20for%20fine-grained%2C%20instruction-level%20activation%0Asteering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternal%2520Chain-of-Thought%253A%2520Empirical%2520Evidence%2520for%2520Layer-wise%2520Subtask%250A%2520%2520Scheduling%2520in%2520LLMs%26entry.906535625%3DZhipeng%2520Yang%2520and%2520Junzhuo%2520Li%2520and%2520Siyu%2520Xia%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520an%2520%2524%255Ctextit%257Binternal%250Achain-of-thought%257D%2524%253A%2520they%2520sequentially%2520decompose%2520and%2520execute%2520composite%2520tasks%250Alayer-by-layer.%2520Two%2520claims%2520ground%2520our%2520study%253A%2520%2528i%2529%2520distinct%2520subtasks%2520are%2520learned%250Aat%2520different%2520network%2520depths%252C%2520and%2520%2528ii%2529%2520these%2520subtasks%2520are%2520executed%2520sequentially%250Aacross%2520layers.%2520On%2520a%2520benchmark%2520of%252015%2520two-step%2520composite%2520tasks%252C%2520we%2520employ%250Alayer-from%2520context-masking%2520and%2520propose%2520a%2520novel%2520cross-task%2520patching%2520method%252C%250Aconfirming%2520%2528i%2529.%2520To%2520examine%2520claim%2520%2528ii%2529%252C%2520we%2520apply%2520LogitLens%2520to%2520decode%2520hidden%250Astates%252C%2520revealing%2520a%2520consistent%2520layerwise%2520execution%2520pattern.%2520We%2520further%250Areplicate%2520our%2520analysis%2520on%2520the%2520real-world%2520%2524%255Ctext%257BTRACE%257D%2524%2520benchmark%252C%2520observing%250Athe%2520same%2520stepwise%2520dynamics.%2520Together%252C%2520our%2520results%2520enhance%2520LLMs%2520transparency%2520by%250Ashowing%2520their%2520capacity%2520to%2520internally%2520plan%2520and%2520execute%2520subtasks%2520%2528or%250Ainstructions%2529%252C%2520opening%2520avenues%2520for%2520fine-grained%252C%2520instruction-level%2520activation%250Asteering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Internal%20Chain-of-Thought%3A%20Empirical%20Evidence%20for%20Layer-wise%20Subtask%0A%20%20Scheduling%20in%20LLMs&entry.906535625=Zhipeng%20Yang%20and%20Junzhuo%20Li%20and%20Siyu%20Xia%20and%20Xuming%20Hu&entry.1292438233=%20%20We%20show%20that%20large%20language%20models%20%28LLMs%29%20exhibit%20an%20%24%5Ctextit%7Binternal%0Achain-of-thought%7D%24%3A%20they%20sequentially%20decompose%20and%20execute%20composite%20tasks%0Alayer-by-layer.%20Two%20claims%20ground%20our%20study%3A%20%28i%29%20distinct%20subtasks%20are%20learned%0Aat%20different%20network%20depths%2C%20and%20%28ii%29%20these%20subtasks%20are%20executed%20sequentially%0Aacross%20layers.%20On%20a%20benchmark%20of%2015%20two-step%20composite%20tasks%2C%20we%20employ%0Alayer-from%20context-masking%20and%20propose%20a%20novel%20cross-task%20patching%20method%2C%0Aconfirming%20%28i%29.%20To%20examine%20claim%20%28ii%29%2C%20we%20apply%20LogitLens%20to%20decode%20hidden%0Astates%2C%20revealing%20a%20consistent%20layerwise%20execution%20pattern.%20We%20further%0Areplicate%20our%20analysis%20on%20the%20real-world%20%24%5Ctext%7BTRACE%7D%24%20benchmark%2C%20observing%0Athe%20same%20stepwise%20dynamics.%20Together%2C%20our%20results%20enhance%20LLMs%20transparency%20by%0Ashowing%20their%20capacity%20to%20internally%20plan%20and%20execute%20subtasks%20%28or%0Ainstructions%29%2C%20opening%20avenues%20for%20fine-grained%2C%20instruction-level%20activation%0Asteering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14530v1&entry.124074799=Read"},
{"title": "SQLong: Enhanced NL2SQL for Longer Contexts with LLMs", "author": "Dai Quoc Nguyen and Cong Duy Vu Hoang and Duy Vu and Gioacchino Tangari and Thanh Tien Vu and Don Dharmasiri and Yuan-Fang Li and Long Duong", "abstract": "  Open-weight large language models (LLMs) have significantly advanced\nperformance in the Natural Language to SQL (NL2SQL) task. However, their\neffectiveness diminishes when dealing with large database schemas, as the\ncontext length increases. To address this limitation, we present SQLong, a\nnovel and efficient data augmentation framework designed to enhance LLM\nperformance in long-context scenarios for the NL2SQL task. SQLong generates\naugmented datasets by extending existing database schemas with additional\nsynthetic CREATE TABLE commands and corresponding data rows, sampled from\ndiverse schemas in the training data. This approach effectively simulates\nlong-context scenarios during finetuning and evaluation. Through experiments on\nthe Spider and BIRD datasets, we demonstrate that LLMs finetuned with\nSQLong-augmented data significantly outperform those trained on standard\ndatasets. These imply SQLong's practical implementation and its impact on\nimproving NL2SQL capabilities in real-world settings with complex database\nschemas.\n", "link": "http://arxiv.org/abs/2502.16747v2", "date": "2025-05-20", "relevancy": 2.3684, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SQLong%3A%20Enhanced%20NL2SQL%20for%20Longer%20Contexts%20with%20LLMs&body=Title%3A%20SQLong%3A%20Enhanced%20NL2SQL%20for%20Longer%20Contexts%20with%20LLMs%0AAuthor%3A%20Dai%20Quoc%20Nguyen%20and%20Cong%20Duy%20Vu%20Hoang%20and%20Duy%20Vu%20and%20Gioacchino%20Tangari%20and%20Thanh%20Tien%20Vu%20and%20Don%20Dharmasiri%20and%20Yuan-Fang%20Li%20and%20Long%20Duong%0AAbstract%3A%20%20%20Open-weight%20large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%0Aperformance%20in%20the%20Natural%20Language%20to%20SQL%20%28NL2SQL%29%20task.%20However%2C%20their%0Aeffectiveness%20diminishes%20when%20dealing%20with%20large%20database%20schemas%2C%20as%20the%0Acontext%20length%20increases.%20To%20address%20this%20limitation%2C%20we%20present%20SQLong%2C%20a%0Anovel%20and%20efficient%20data%20augmentation%20framework%20designed%20to%20enhance%20LLM%0Aperformance%20in%20long-context%20scenarios%20for%20the%20NL2SQL%20task.%20SQLong%20generates%0Aaugmented%20datasets%20by%20extending%20existing%20database%20schemas%20with%20additional%0Asynthetic%20CREATE%20TABLE%20commands%20and%20corresponding%20data%20rows%2C%20sampled%20from%0Adiverse%20schemas%20in%20the%20training%20data.%20This%20approach%20effectively%20simulates%0Along-context%20scenarios%20during%20finetuning%20and%20evaluation.%20Through%20experiments%20on%0Athe%20Spider%20and%20BIRD%20datasets%2C%20we%20demonstrate%20that%20LLMs%20finetuned%20with%0ASQLong-augmented%20data%20significantly%20outperform%20those%20trained%20on%20standard%0Adatasets.%20These%20imply%20SQLong%27s%20practical%20implementation%20and%20its%20impact%20on%0Aimproving%20NL2SQL%20capabilities%20in%20real-world%20settings%20with%20complex%20database%0Aschemas.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSQLong%253A%2520Enhanced%2520NL2SQL%2520for%2520Longer%2520Contexts%2520with%2520LLMs%26entry.906535625%3DDai%2520Quoc%2520Nguyen%2520and%2520Cong%2520Duy%2520Vu%2520Hoang%2520and%2520Duy%2520Vu%2520and%2520Gioacchino%2520Tangari%2520and%2520Thanh%2520Tien%2520Vu%2520and%2520Don%2520Dharmasiri%2520and%2520Yuan-Fang%2520Li%2520and%2520Long%2520Duong%26entry.1292438233%3D%2520%2520Open-weight%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%250Aperformance%2520in%2520the%2520Natural%2520Language%2520to%2520SQL%2520%2528NL2SQL%2529%2520task.%2520However%252C%2520their%250Aeffectiveness%2520diminishes%2520when%2520dealing%2520with%2520large%2520database%2520schemas%252C%2520as%2520the%250Acontext%2520length%2520increases.%2520To%2520address%2520this%2520limitation%252C%2520we%2520present%2520SQLong%252C%2520a%250Anovel%2520and%2520efficient%2520data%2520augmentation%2520framework%2520designed%2520to%2520enhance%2520LLM%250Aperformance%2520in%2520long-context%2520scenarios%2520for%2520the%2520NL2SQL%2520task.%2520SQLong%2520generates%250Aaugmented%2520datasets%2520by%2520extending%2520existing%2520database%2520schemas%2520with%2520additional%250Asynthetic%2520CREATE%2520TABLE%2520commands%2520and%2520corresponding%2520data%2520rows%252C%2520sampled%2520from%250Adiverse%2520schemas%2520in%2520the%2520training%2520data.%2520This%2520approach%2520effectively%2520simulates%250Along-context%2520scenarios%2520during%2520finetuning%2520and%2520evaluation.%2520Through%2520experiments%2520on%250Athe%2520Spider%2520and%2520BIRD%2520datasets%252C%2520we%2520demonstrate%2520that%2520LLMs%2520finetuned%2520with%250ASQLong-augmented%2520data%2520significantly%2520outperform%2520those%2520trained%2520on%2520standard%250Adatasets.%2520These%2520imply%2520SQLong%2527s%2520practical%2520implementation%2520and%2520its%2520impact%2520on%250Aimproving%2520NL2SQL%2520capabilities%2520in%2520real-world%2520settings%2520with%2520complex%2520database%250Aschemas.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SQLong%3A%20Enhanced%20NL2SQL%20for%20Longer%20Contexts%20with%20LLMs&entry.906535625=Dai%20Quoc%20Nguyen%20and%20Cong%20Duy%20Vu%20Hoang%20and%20Duy%20Vu%20and%20Gioacchino%20Tangari%20and%20Thanh%20Tien%20Vu%20and%20Don%20Dharmasiri%20and%20Yuan-Fang%20Li%20and%20Long%20Duong&entry.1292438233=%20%20Open-weight%20large%20language%20models%20%28LLMs%29%20have%20significantly%20advanced%0Aperformance%20in%20the%20Natural%20Language%20to%20SQL%20%28NL2SQL%29%20task.%20However%2C%20their%0Aeffectiveness%20diminishes%20when%20dealing%20with%20large%20database%20schemas%2C%20as%20the%0Acontext%20length%20increases.%20To%20address%20this%20limitation%2C%20we%20present%20SQLong%2C%20a%0Anovel%20and%20efficient%20data%20augmentation%20framework%20designed%20to%20enhance%20LLM%0Aperformance%20in%20long-context%20scenarios%20for%20the%20NL2SQL%20task.%20SQLong%20generates%0Aaugmented%20datasets%20by%20extending%20existing%20database%20schemas%20with%20additional%0Asynthetic%20CREATE%20TABLE%20commands%20and%20corresponding%20data%20rows%2C%20sampled%20from%0Adiverse%20schemas%20in%20the%20training%20data.%20This%20approach%20effectively%20simulates%0Along-context%20scenarios%20during%20finetuning%20and%20evaluation.%20Through%20experiments%20on%0Athe%20Spider%20and%20BIRD%20datasets%2C%20we%20demonstrate%20that%20LLMs%20finetuned%20with%0ASQLong-augmented%20data%20significantly%20outperform%20those%20trained%20on%20standard%0Adatasets.%20These%20imply%20SQLong%27s%20practical%20implementation%20and%20its%20impact%20on%0Aimproving%20NL2SQL%20capabilities%20in%20real-world%20settings%20with%20complex%20database%0Aschemas.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16747v2&entry.124074799=Read"},
{"title": "Visual Agentic Reinforcement Fine-Tuning", "author": "Ziyu Liu and Yuhang Zang and Yushan Zou and Zijian Liang and Xiaoyi Dong and Yuhang Cao and Haodong Duan and Dahua Lin and Jiaqi Wang", "abstract": "  A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents.\n", "link": "http://arxiv.org/abs/2505.14246v1", "date": "2025-05-20", "relevancy": 2.3556, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5892}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Agentic%20Reinforcement%20Fine-Tuning&body=Title%3A%20Visual%20Agentic%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Ziyu%20Liu%20and%20Yuhang%20Zang%20and%20Yushan%20Zou%20and%20Zijian%20Liang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20A%20key%20trend%20in%20Large%20Reasoning%20Models%20%28e.g.%2C%20OpenAI%27s%20o3%29%20is%20the%20native%0Aagentic%20ability%20to%20use%20external%20tools%20such%20as%20web%20browsers%20for%20searching%20and%0Awriting/executing%20code%20for%20image%20manipulation%20to%20think%20with%20images.%20In%20the%0Aopen-source%20research%20community%2C%20while%20significant%20progress%20has%20been%20made%20in%0Alanguage-only%20agentic%20abilities%20such%20as%20function%20calling%20and%20tool%20integration%2C%0Athe%20development%20of%20multi-modal%20agentic%20capabilities%20that%20involve%20truly%20thinking%0Awith%20images%2C%20and%20their%20corresponding%20benchmarks%2C%20are%20still%20less%20explored.%20This%0Awork%20highlights%20the%20effectiveness%20of%20Visual%20Agentic%20Reinforcement%20Fine-Tuning%0A%28Visual-ARFT%29%20for%20enabling%20flexible%20and%20adaptive%20reasoning%20abilities%20for%20Large%0AVision-Language%20Models%20%28LVLMs%29.%20With%20Visual-ARFT%2C%20open-source%20LVLMs%20gain%20the%0Aability%20to%20browse%20websites%20for%20real-time%20information%20updates%20and%20write%20code%20to%0Amanipulate%20and%20analyze%20input%20images%20through%20cropping%2C%20rotation%2C%20and%20other%20image%0Aprocessing%20techniques.%20We%20also%20present%20a%20Multi-modal%20Agentic%20Tool%20Bench%20%28MAT%29%0Awith%20two%20settings%20%28MAT-Search%20and%20MAT-Coding%29%20designed%20to%20evaluate%20LVLMs%27%0Aagentic%20search%20and%20coding%20abilities.%20Our%20experimental%20results%20demonstrate%20that%0AVisual-ARFT%20outperforms%20its%20baseline%20by%20%2B18.6%25%20F1%20/%20%2B13.0%25%20EM%20on%20MAT-Coding%20and%0A%2B10.3%25%20F1%20/%20%2B8.7%25%20EM%20on%20MAT-Search%2C%20ultimately%20surpassing%20GPT-4o.%20Visual-ARFT%0Aalso%20achieves%20%2B29.3%20F1%25%20/%20%2B25.9%25%20EM%20gains%20on%20existing%20multi-hop%20QA%20benchmarks%0Asuch%20as%202Wiki%20and%20HotpotQA%2C%20demonstrating%20strong%20generalization%20capabilities.%0AOur%20findings%20suggest%20that%20Visual-ARFT%20offers%20a%20promising%20path%20toward%20building%0Arobust%20and%20generalizable%20multimodal%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Agentic%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DZiyu%2520Liu%2520and%2520Yuhang%2520Zang%2520and%2520Yushan%2520Zou%2520and%2520Zijian%2520Liang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Cao%2520and%2520Haodong%2520Duan%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520A%2520key%2520trend%2520in%2520Large%2520Reasoning%2520Models%2520%2528e.g.%252C%2520OpenAI%2527s%2520o3%2529%2520is%2520the%2520native%250Aagentic%2520ability%2520to%2520use%2520external%2520tools%2520such%2520as%2520web%2520browsers%2520for%2520searching%2520and%250Awriting/executing%2520code%2520for%2520image%2520manipulation%2520to%2520think%2520with%2520images.%2520In%2520the%250Aopen-source%2520research%2520community%252C%2520while%2520significant%2520progress%2520has%2520been%2520made%2520in%250Alanguage-only%2520agentic%2520abilities%2520such%2520as%2520function%2520calling%2520and%2520tool%2520integration%252C%250Athe%2520development%2520of%2520multi-modal%2520agentic%2520capabilities%2520that%2520involve%2520truly%2520thinking%250Awith%2520images%252C%2520and%2520their%2520corresponding%2520benchmarks%252C%2520are%2520still%2520less%2520explored.%2520This%250Awork%2520highlights%2520the%2520effectiveness%2520of%2520Visual%2520Agentic%2520Reinforcement%2520Fine-Tuning%250A%2528Visual-ARFT%2529%2520for%2520enabling%2520flexible%2520and%2520adaptive%2520reasoning%2520abilities%2520for%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529.%2520With%2520Visual-ARFT%252C%2520open-source%2520LVLMs%2520gain%2520the%250Aability%2520to%2520browse%2520websites%2520for%2520real-time%2520information%2520updates%2520and%2520write%2520code%2520to%250Amanipulate%2520and%2520analyze%2520input%2520images%2520through%2520cropping%252C%2520rotation%252C%2520and%2520other%2520image%250Aprocessing%2520techniques.%2520We%2520also%2520present%2520a%2520Multi-modal%2520Agentic%2520Tool%2520Bench%2520%2528MAT%2529%250Awith%2520two%2520settings%2520%2528MAT-Search%2520and%2520MAT-Coding%2529%2520designed%2520to%2520evaluate%2520LVLMs%2527%250Aagentic%2520search%2520and%2520coding%2520abilities.%2520Our%2520experimental%2520results%2520demonstrate%2520that%250AVisual-ARFT%2520outperforms%2520its%2520baseline%2520by%2520%252B18.6%2525%2520F1%2520/%2520%252B13.0%2525%2520EM%2520on%2520MAT-Coding%2520and%250A%252B10.3%2525%2520F1%2520/%2520%252B8.7%2525%2520EM%2520on%2520MAT-Search%252C%2520ultimately%2520surpassing%2520GPT-4o.%2520Visual-ARFT%250Aalso%2520achieves%2520%252B29.3%2520F1%2525%2520/%2520%252B25.9%2525%2520EM%2520gains%2520on%2520existing%2520multi-hop%2520QA%2520benchmarks%250Asuch%2520as%25202Wiki%2520and%2520HotpotQA%252C%2520demonstrating%2520strong%2520generalization%2520capabilities.%250AOur%2520findings%2520suggest%2520that%2520Visual-ARFT%2520offers%2520a%2520promising%2520path%2520toward%2520building%250Arobust%2520and%2520generalizable%2520multimodal%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Agentic%20Reinforcement%20Fine-Tuning&entry.906535625=Ziyu%20Liu%20and%20Yuhang%20Zang%20and%20Yushan%20Zou%20and%20Zijian%20Liang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Cao%20and%20Haodong%20Duan%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20A%20key%20trend%20in%20Large%20Reasoning%20Models%20%28e.g.%2C%20OpenAI%27s%20o3%29%20is%20the%20native%0Aagentic%20ability%20to%20use%20external%20tools%20such%20as%20web%20browsers%20for%20searching%20and%0Awriting/executing%20code%20for%20image%20manipulation%20to%20think%20with%20images.%20In%20the%0Aopen-source%20research%20community%2C%20while%20significant%20progress%20has%20been%20made%20in%0Alanguage-only%20agentic%20abilities%20such%20as%20function%20calling%20and%20tool%20integration%2C%0Athe%20development%20of%20multi-modal%20agentic%20capabilities%20that%20involve%20truly%20thinking%0Awith%20images%2C%20and%20their%20corresponding%20benchmarks%2C%20are%20still%20less%20explored.%20This%0Awork%20highlights%20the%20effectiveness%20of%20Visual%20Agentic%20Reinforcement%20Fine-Tuning%0A%28Visual-ARFT%29%20for%20enabling%20flexible%20and%20adaptive%20reasoning%20abilities%20for%20Large%0AVision-Language%20Models%20%28LVLMs%29.%20With%20Visual-ARFT%2C%20open-source%20LVLMs%20gain%20the%0Aability%20to%20browse%20websites%20for%20real-time%20information%20updates%20and%20write%20code%20to%0Amanipulate%20and%20analyze%20input%20images%20through%20cropping%2C%20rotation%2C%20and%20other%20image%0Aprocessing%20techniques.%20We%20also%20present%20a%20Multi-modal%20Agentic%20Tool%20Bench%20%28MAT%29%0Awith%20two%20settings%20%28MAT-Search%20and%20MAT-Coding%29%20designed%20to%20evaluate%20LVLMs%27%0Aagentic%20search%20and%20coding%20abilities.%20Our%20experimental%20results%20demonstrate%20that%0AVisual-ARFT%20outperforms%20its%20baseline%20by%20%2B18.6%25%20F1%20/%20%2B13.0%25%20EM%20on%20MAT-Coding%20and%0A%2B10.3%25%20F1%20/%20%2B8.7%25%20EM%20on%20MAT-Search%2C%20ultimately%20surpassing%20GPT-4o.%20Visual-ARFT%0Aalso%20achieves%20%2B29.3%20F1%25%20/%20%2B25.9%25%20EM%20gains%20on%20existing%20multi-hop%20QA%20benchmarks%0Asuch%20as%202Wiki%20and%20HotpotQA%2C%20demonstrating%20strong%20generalization%20capabilities.%0AOur%20findings%20suggest%20that%20Visual-ARFT%20offers%20a%20promising%20path%20toward%20building%0Arobust%20and%20generalizable%20multimodal%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14246v1&entry.124074799=Read"},
{"title": "Debating for Better Reasoning: An Unsupervised Multimodal Approach", "author": "Ashutosh Adhikari and Mirella Lapata", "abstract": "  As Large Language Models (LLMs) gain expertise across diverse domains and\nmodalities, scalable oversight becomes increasingly challenging, particularly\nwhen their capabilities may surpass human evaluators. Debate has emerged as a\npromising mechanism for enabling such oversight. In this work, we extend the\ndebate paradigm to a multimodal setting, exploring its potential for weaker\nmodels to supervise and enhance the performance of stronger models. We focus on\nvisual question answering (VQA), where two \"sighted\" expert vision-language\nmodels debate an answer, while a \"blind\" (text-only) judge adjudicates based\nsolely on the quality of the arguments. In our framework, the experts defend\nonly answers aligned with their beliefs, thereby obviating the need for\nexplicit role-playing and concentrating the debate on instances of expert\ndisagreement. Experiments on several multimodal tasks demonstrate that the\ndebate framework consistently outperforms individual expert models. Moreover,\njudgments from weaker LLMs can help instill reasoning capabilities in\nvision-language models through finetuning.\n", "link": "http://arxiv.org/abs/2505.14627v1", "date": "2025-05-20", "relevancy": 2.3515, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debating%20for%20Better%20Reasoning%3A%20An%20Unsupervised%20Multimodal%20Approach&body=Title%3A%20Debating%20for%20Better%20Reasoning%3A%20An%20Unsupervised%20Multimodal%20Approach%0AAuthor%3A%20Ashutosh%20Adhikari%20and%20Mirella%20Lapata%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20gain%20expertise%20across%20diverse%20domains%20and%0Amodalities%2C%20scalable%20oversight%20becomes%20increasingly%20challenging%2C%20particularly%0Awhen%20their%20capabilities%20may%20surpass%20human%20evaluators.%20Debate%20has%20emerged%20as%20a%0Apromising%20mechanism%20for%20enabling%20such%20oversight.%20In%20this%20work%2C%20we%20extend%20the%0Adebate%20paradigm%20to%20a%20multimodal%20setting%2C%20exploring%20its%20potential%20for%20weaker%0Amodels%20to%20supervise%20and%20enhance%20the%20performance%20of%20stronger%20models.%20We%20focus%20on%0Avisual%20question%20answering%20%28VQA%29%2C%20where%20two%20%22sighted%22%20expert%20vision-language%0Amodels%20debate%20an%20answer%2C%20while%20a%20%22blind%22%20%28text-only%29%20judge%20adjudicates%20based%0Asolely%20on%20the%20quality%20of%20the%20arguments.%20In%20our%20framework%2C%20the%20experts%20defend%0Aonly%20answers%20aligned%20with%20their%20beliefs%2C%20thereby%20obviating%20the%20need%20for%0Aexplicit%20role-playing%20and%20concentrating%20the%20debate%20on%20instances%20of%20expert%0Adisagreement.%20Experiments%20on%20several%20multimodal%20tasks%20demonstrate%20that%20the%0Adebate%20framework%20consistently%20outperforms%20individual%20expert%20models.%20Moreover%2C%0Ajudgments%20from%20weaker%20LLMs%20can%20help%20instill%20reasoning%20capabilities%20in%0Avision-language%20models%20through%20finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebating%2520for%2520Better%2520Reasoning%253A%2520An%2520Unsupervised%2520Multimodal%2520Approach%26entry.906535625%3DAshutosh%2520Adhikari%2520and%2520Mirella%2520Lapata%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520gain%2520expertise%2520across%2520diverse%2520domains%2520and%250Amodalities%252C%2520scalable%2520oversight%2520becomes%2520increasingly%2520challenging%252C%2520particularly%250Awhen%2520their%2520capabilities%2520may%2520surpass%2520human%2520evaluators.%2520Debate%2520has%2520emerged%2520as%2520a%250Apromising%2520mechanism%2520for%2520enabling%2520such%2520oversight.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%250Adebate%2520paradigm%2520to%2520a%2520multimodal%2520setting%252C%2520exploring%2520its%2520potential%2520for%2520weaker%250Amodels%2520to%2520supervise%2520and%2520enhance%2520the%2520performance%2520of%2520stronger%2520models.%2520We%2520focus%2520on%250Avisual%2520question%2520answering%2520%2528VQA%2529%252C%2520where%2520two%2520%2522sighted%2522%2520expert%2520vision-language%250Amodels%2520debate%2520an%2520answer%252C%2520while%2520a%2520%2522blind%2522%2520%2528text-only%2529%2520judge%2520adjudicates%2520based%250Asolely%2520on%2520the%2520quality%2520of%2520the%2520arguments.%2520In%2520our%2520framework%252C%2520the%2520experts%2520defend%250Aonly%2520answers%2520aligned%2520with%2520their%2520beliefs%252C%2520thereby%2520obviating%2520the%2520need%2520for%250Aexplicit%2520role-playing%2520and%2520concentrating%2520the%2520debate%2520on%2520instances%2520of%2520expert%250Adisagreement.%2520Experiments%2520on%2520several%2520multimodal%2520tasks%2520demonstrate%2520that%2520the%250Adebate%2520framework%2520consistently%2520outperforms%2520individual%2520expert%2520models.%2520Moreover%252C%250Ajudgments%2520from%2520weaker%2520LLMs%2520can%2520help%2520instill%2520reasoning%2520capabilities%2520in%250Avision-language%2520models%2520through%2520finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debating%20for%20Better%20Reasoning%3A%20An%20Unsupervised%20Multimodal%20Approach&entry.906535625=Ashutosh%20Adhikari%20and%20Mirella%20Lapata&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20gain%20expertise%20across%20diverse%20domains%20and%0Amodalities%2C%20scalable%20oversight%20becomes%20increasingly%20challenging%2C%20particularly%0Awhen%20their%20capabilities%20may%20surpass%20human%20evaluators.%20Debate%20has%20emerged%20as%20a%0Apromising%20mechanism%20for%20enabling%20such%20oversight.%20In%20this%20work%2C%20we%20extend%20the%0Adebate%20paradigm%20to%20a%20multimodal%20setting%2C%20exploring%20its%20potential%20for%20weaker%0Amodels%20to%20supervise%20and%20enhance%20the%20performance%20of%20stronger%20models.%20We%20focus%20on%0Avisual%20question%20answering%20%28VQA%29%2C%20where%20two%20%22sighted%22%20expert%20vision-language%0Amodels%20debate%20an%20answer%2C%20while%20a%20%22blind%22%20%28text-only%29%20judge%20adjudicates%20based%0Asolely%20on%20the%20quality%20of%20the%20arguments.%20In%20our%20framework%2C%20the%20experts%20defend%0Aonly%20answers%20aligned%20with%20their%20beliefs%2C%20thereby%20obviating%20the%20need%20for%0Aexplicit%20role-playing%20and%20concentrating%20the%20debate%20on%20instances%20of%20expert%0Adisagreement.%20Experiments%20on%20several%20multimodal%20tasks%20demonstrate%20that%20the%0Adebate%20framework%20consistently%20outperforms%20individual%20expert%20models.%20Moreover%2C%0Ajudgments%20from%20weaker%20LLMs%20can%20help%20instill%20reasoning%20capabilities%20in%0Avision-language%20models%20through%20finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14627v1&entry.124074799=Read"},
{"title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement\n  Learning", "author": "Ziwei Zheng and Michael Yang and Jack Hong and Chenxiao Zhao and Guohai Xu and Le Yang and Chao Shen and Xing Yu", "abstract": "  Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes.\n", "link": "http://arxiv.org/abs/2505.14362v1", "date": "2025-05-20", "relevancy": 2.3513, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepEyes%3A%20Incentivizing%20%22Thinking%20with%20Images%22%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20DeepEyes%3A%20Incentivizing%20%22Thinking%20with%20Images%22%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Ziwei%20Zheng%20and%20Michael%20Yang%20and%20Jack%20Hong%20and%20Chenxiao%20Zhao%20and%20Guohai%20Xu%20and%20Le%20Yang%20and%20Chao%20Shen%20and%20Xing%20Yu%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20strong%20capabilities%20in%0Amultimodal%20understanding%20and%20reasoning%2C%20yet%20they%20are%20primarily%20constrained%20by%0Atext-based%20reasoning%20processes.%20However%2C%20achieving%20seamless%20integration%20of%0Avisual%20and%20textual%20reasoning%20which%20mirrors%20human%20cognitive%20processes%20remains%20a%0Asignificant%20challenge.%20In%20particular%2C%20effectively%20incorporating%20advanced%20visual%0Ainput%20processing%20into%20reasoning%20mechanisms%20is%20still%20an%20open%20question.%20Thus%2C%20in%0Athis%20paper%2C%20we%20explore%20the%20interleaved%20multimodal%20reasoning%20paradigm%20and%0Aintroduce%20DeepEyes%2C%20a%20model%20with%20%22thinking%20with%20images%22%20capabilities%0Aincentivized%20through%20end-to-end%20reinforcement%20learning%20without%20the%20need%20for%0Acold-start%20SFT.%20Notably%2C%20this%20ability%20emerges%20natively%20within%20the%20model%20itself%2C%0Aleveraging%20its%20inherent%20grounding%20ability%20as%20a%20tool%20instead%20of%20depending%20on%0Aseparate%20specialized%20models.%20Specifically%2C%20we%20propose%20a%20tool-use-oriented%20data%0Aselection%20mechanism%20and%20a%20reward%20strategy%20to%20encourage%20successful%20tool-assisted%0Areasoning%20trajectories.%20DeepEyes%20achieves%20significant%20performance%20gains%20on%0Afine-grained%20perception%20and%20reasoning%20benchmarks%20and%20also%20demonstrates%0Aimprovement%20in%20grounding%2C%20hallucination%2C%20and%20mathematical%20reasoning%20tasks.%0AInterestingly%2C%20we%20observe%20the%20distinct%20evolution%20of%20tool-calling%20behavior%20from%0Ainitial%20exploration%20to%20efficient%20and%20accurate%20exploitation%2C%20and%20diverse%0Athinking%20patterns%20that%20closely%20mirror%20human%20visual%20reasoning%20processes.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Visual-Agent/DeepEyes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14362v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepEyes%253A%2520Incentivizing%2520%2522Thinking%2520with%2520Images%2522%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DZiwei%2520Zheng%2520and%2520Michael%2520Yang%2520and%2520Jack%2520Hong%2520and%2520Chenxiao%2520Zhao%2520and%2520Guohai%2520Xu%2520and%2520Le%2520Yang%2520and%2520Chao%2520Shen%2520and%2520Xing%2520Yu%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520capabilities%2520in%250Amultimodal%2520understanding%2520and%2520reasoning%252C%2520yet%2520they%2520are%2520primarily%2520constrained%2520by%250Atext-based%2520reasoning%2520processes.%2520However%252C%2520achieving%2520seamless%2520integration%2520of%250Avisual%2520and%2520textual%2520reasoning%2520which%2520mirrors%2520human%2520cognitive%2520processes%2520remains%2520a%250Asignificant%2520challenge.%2520In%2520particular%252C%2520effectively%2520incorporating%2520advanced%2520visual%250Ainput%2520processing%2520into%2520reasoning%2520mechanisms%2520is%2520still%2520an%2520open%2520question.%2520Thus%252C%2520in%250Athis%2520paper%252C%2520we%2520explore%2520the%2520interleaved%2520multimodal%2520reasoning%2520paradigm%2520and%250Aintroduce%2520DeepEyes%252C%2520a%2520model%2520with%2520%2522thinking%2520with%2520images%2522%2520capabilities%250Aincentivized%2520through%2520end-to-end%2520reinforcement%2520learning%2520without%2520the%2520need%2520for%250Acold-start%2520SFT.%2520Notably%252C%2520this%2520ability%2520emerges%2520natively%2520within%2520the%2520model%2520itself%252C%250Aleveraging%2520its%2520inherent%2520grounding%2520ability%2520as%2520a%2520tool%2520instead%2520of%2520depending%2520on%250Aseparate%2520specialized%2520models.%2520Specifically%252C%2520we%2520propose%2520a%2520tool-use-oriented%2520data%250Aselection%2520mechanism%2520and%2520a%2520reward%2520strategy%2520to%2520encourage%2520successful%2520tool-assisted%250Areasoning%2520trajectories.%2520DeepEyes%2520achieves%2520significant%2520performance%2520gains%2520on%250Afine-grained%2520perception%2520and%2520reasoning%2520benchmarks%2520and%2520also%2520demonstrates%250Aimprovement%2520in%2520grounding%252C%2520hallucination%252C%2520and%2520mathematical%2520reasoning%2520tasks.%250AInterestingly%252C%2520we%2520observe%2520the%2520distinct%2520evolution%2520of%2520tool-calling%2520behavior%2520from%250Ainitial%2520exploration%2520to%2520efficient%2520and%2520accurate%2520exploitation%252C%2520and%2520diverse%250Athinking%2520patterns%2520that%2520closely%2520mirror%2520human%2520visual%2520reasoning%2520processes.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/Visual-Agent/DeepEyes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14362v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepEyes%3A%20Incentivizing%20%22Thinking%20with%20Images%22%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Ziwei%20Zheng%20and%20Michael%20Yang%20and%20Jack%20Hong%20and%20Chenxiao%20Zhao%20and%20Guohai%20Xu%20and%20Le%20Yang%20and%20Chao%20Shen%20and%20Xing%20Yu&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20shown%20strong%20capabilities%20in%0Amultimodal%20understanding%20and%20reasoning%2C%20yet%20they%20are%20primarily%20constrained%20by%0Atext-based%20reasoning%20processes.%20However%2C%20achieving%20seamless%20integration%20of%0Avisual%20and%20textual%20reasoning%20which%20mirrors%20human%20cognitive%20processes%20remains%20a%0Asignificant%20challenge.%20In%20particular%2C%20effectively%20incorporating%20advanced%20visual%0Ainput%20processing%20into%20reasoning%20mechanisms%20is%20still%20an%20open%20question.%20Thus%2C%20in%0Athis%20paper%2C%20we%20explore%20the%20interleaved%20multimodal%20reasoning%20paradigm%20and%0Aintroduce%20DeepEyes%2C%20a%20model%20with%20%22thinking%20with%20images%22%20capabilities%0Aincentivized%20through%20end-to-end%20reinforcement%20learning%20without%20the%20need%20for%0Acold-start%20SFT.%20Notably%2C%20this%20ability%20emerges%20natively%20within%20the%20model%20itself%2C%0Aleveraging%20its%20inherent%20grounding%20ability%20as%20a%20tool%20instead%20of%20depending%20on%0Aseparate%20specialized%20models.%20Specifically%2C%20we%20propose%20a%20tool-use-oriented%20data%0Aselection%20mechanism%20and%20a%20reward%20strategy%20to%20encourage%20successful%20tool-assisted%0Areasoning%20trajectories.%20DeepEyes%20achieves%20significant%20performance%20gains%20on%0Afine-grained%20perception%20and%20reasoning%20benchmarks%20and%20also%20demonstrates%0Aimprovement%20in%20grounding%2C%20hallucination%2C%20and%20mathematical%20reasoning%20tasks.%0AInterestingly%2C%20we%20observe%20the%20distinct%20evolution%20of%20tool-calling%20behavior%20from%0Ainitial%20exploration%20to%20efficient%20and%20accurate%20exploitation%2C%20and%20diverse%0Athinking%20patterns%20that%20closely%20mirror%20human%20visual%20reasoning%20processes.%20Code%20is%0Aavailable%20at%20https%3A//github.com/Visual-Agent/DeepEyes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14362v1&entry.124074799=Read"},
{"title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised\n  Speaker Verification", "author": "Theo Lepage and Reda Dehak", "abstract": "  Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS.\n", "link": "http://arxiv.org/abs/2505.14561v1", "date": "2025-05-20", "relevancy": 2.3478, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4968}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4572}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSPS%3A%20Self-Supervised%20Positive%20Sampling%20for%20Robust%20Self-Supervised%0A%20%20Speaker%20Verification&body=Title%3A%20SSPS%3A%20Self-Supervised%20Positive%20Sampling%20for%20Robust%20Self-Supervised%0A%20%20Speaker%20Verification%0AAuthor%3A%20Theo%20Lepage%20and%20Reda%20Dehak%0AAbstract%3A%20%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20led%20to%20considerable%20progress%20in%20Speaker%0AVerification%20%28SV%29.%20The%20standard%20framework%20uses%20same-utterance%20positive%20sampling%0Aand%20data-augmentation%20to%20generate%20anchor-positive%20pairs%20of%20the%20same%20speaker.%0AThis%20is%20a%20major%20limitation%2C%20as%20this%20strategy%20primarily%20encodes%20channel%0Ainformation%20from%20the%20recording%20condition%2C%20shared%20by%20the%20anchor%20and%20positive.%20We%0Apropose%20a%20new%20positive%20sampling%20technique%20to%20address%20this%20bottleneck%3A%0ASelf-Supervised%20Positive%20Sampling%20%28SSPS%29.%20For%20a%20given%20anchor%2C%20SSPS%20aims%20to%20find%0Aan%20appropriate%20positive%2C%20i.e.%2C%20of%20the%20same%20speaker%20identity%20but%20a%20different%0Arecording%20condition%2C%20in%20the%20latent%20space%20using%20clustering%20assignments%20and%20a%0Amemory%20queue%20of%20positive%20embeddings.%20SSPS%20improves%20SV%20performance%20for%20both%0ASimCLR%20and%20DINO%2C%20reaching%202.57%25%20and%202.53%25%20EER%2C%20outperforming%20SOTA%20SSL%20methods%0Aon%20VoxCeleb1-O.%20In%20particular%2C%20SimCLR-SSPS%20achieves%20a%2058%25%20EER%20reduction%20by%0Alowering%20intra-speaker%20variance%2C%20providing%20comparable%20performance%20to%20DINO-SSPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSPS%253A%2520Self-Supervised%2520Positive%2520Sampling%2520for%2520Robust%2520Self-Supervised%250A%2520%2520Speaker%2520Verification%26entry.906535625%3DTheo%2520Lepage%2520and%2520Reda%2520Dehak%26entry.1292438233%3D%2520%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520led%2520to%2520considerable%2520progress%2520in%2520Speaker%250AVerification%2520%2528SV%2529.%2520The%2520standard%2520framework%2520uses%2520same-utterance%2520positive%2520sampling%250Aand%2520data-augmentation%2520to%2520generate%2520anchor-positive%2520pairs%2520of%2520the%2520same%2520speaker.%250AThis%2520is%2520a%2520major%2520limitation%252C%2520as%2520this%2520strategy%2520primarily%2520encodes%2520channel%250Ainformation%2520from%2520the%2520recording%2520condition%252C%2520shared%2520by%2520the%2520anchor%2520and%2520positive.%2520We%250Apropose%2520a%2520new%2520positive%2520sampling%2520technique%2520to%2520address%2520this%2520bottleneck%253A%250ASelf-Supervised%2520Positive%2520Sampling%2520%2528SSPS%2529.%2520For%2520a%2520given%2520anchor%252C%2520SSPS%2520aims%2520to%2520find%250Aan%2520appropriate%2520positive%252C%2520i.e.%252C%2520of%2520the%2520same%2520speaker%2520identity%2520but%2520a%2520different%250Arecording%2520condition%252C%2520in%2520the%2520latent%2520space%2520using%2520clustering%2520assignments%2520and%2520a%250Amemory%2520queue%2520of%2520positive%2520embeddings.%2520SSPS%2520improves%2520SV%2520performance%2520for%2520both%250ASimCLR%2520and%2520DINO%252C%2520reaching%25202.57%2525%2520and%25202.53%2525%2520EER%252C%2520outperforming%2520SOTA%2520SSL%2520methods%250Aon%2520VoxCeleb1-O.%2520In%2520particular%252C%2520SimCLR-SSPS%2520achieves%2520a%252058%2525%2520EER%2520reduction%2520by%250Alowering%2520intra-speaker%2520variance%252C%2520providing%2520comparable%2520performance%2520to%2520DINO-SSPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSPS%3A%20Self-Supervised%20Positive%20Sampling%20for%20Robust%20Self-Supervised%0A%20%20Speaker%20Verification&entry.906535625=Theo%20Lepage%20and%20Reda%20Dehak&entry.1292438233=%20%20Self-Supervised%20Learning%20%28SSL%29%20has%20led%20to%20considerable%20progress%20in%20Speaker%0AVerification%20%28SV%29.%20The%20standard%20framework%20uses%20same-utterance%20positive%20sampling%0Aand%20data-augmentation%20to%20generate%20anchor-positive%20pairs%20of%20the%20same%20speaker.%0AThis%20is%20a%20major%20limitation%2C%20as%20this%20strategy%20primarily%20encodes%20channel%0Ainformation%20from%20the%20recording%20condition%2C%20shared%20by%20the%20anchor%20and%20positive.%20We%0Apropose%20a%20new%20positive%20sampling%20technique%20to%20address%20this%20bottleneck%3A%0ASelf-Supervised%20Positive%20Sampling%20%28SSPS%29.%20For%20a%20given%20anchor%2C%20SSPS%20aims%20to%20find%0Aan%20appropriate%20positive%2C%20i.e.%2C%20of%20the%20same%20speaker%20identity%20but%20a%20different%0Arecording%20condition%2C%20in%20the%20latent%20space%20using%20clustering%20assignments%20and%20a%0Amemory%20queue%20of%20positive%20embeddings.%20SSPS%20improves%20SV%20performance%20for%20both%0ASimCLR%20and%20DINO%2C%20reaching%202.57%25%20and%202.53%25%20EER%2C%20outperforming%20SOTA%20SSL%20methods%0Aon%20VoxCeleb1-O.%20In%20particular%2C%20SimCLR-SSPS%20achieves%20a%2058%25%20EER%20reduction%20by%0Alowering%20intra-speaker%20variance%2C%20providing%20comparable%20performance%20to%20DINO-SSPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14561v1&entry.124074799=Read"},
{"title": "EntryPrune: Neural Network Feature Selection using First Impressions", "author": "Felix Zimmer and Patrik Okanovic and Torsten Hoefler", "abstract": "  There is an ongoing effort to develop feature selection algorithms to improve\ninterpretability, reduce computational resources, and minimize overfitting in\npredictive models. Neural networks stand out as architectures on which to build\nfeature selection methods, and recently, neuron pruning and regrowth have\nemerged from the sparse neural network literature as promising new tools. We\nintroduce EntryPrune, a novel supervised feature selection algorithm using a\ndense neural network with a dynamic sparse input layer. It employs entry-based\npruning, a novel approach that compares neurons based on their relative change\ninduced when they have entered the network. Extensive experiments on 13\ndifferent datasets show that our approach generally outperforms the current\nstate-of-the-art methods, and in particular improves the average accuracy on\nlow-dimensional datasets. Furthermore, we show that EntryPruning surpasses\ntraditional techniques such as magnitude pruning within the EntryPrune\nframework and that EntryPrune achieves lower runtime than competing approaches.\nOur code is available at https://github.com/flxzimmer/entryprune.\n", "link": "http://arxiv.org/abs/2410.02344v3", "date": "2025-05-20", "relevancy": 2.3316, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5092}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4484}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EntryPrune%3A%20Neural%20Network%20Feature%20Selection%20using%20First%20Impressions&body=Title%3A%20EntryPrune%3A%20Neural%20Network%20Feature%20Selection%20using%20First%20Impressions%0AAuthor%3A%20Felix%20Zimmer%20and%20Patrik%20Okanovic%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20There%20is%20an%20ongoing%20effort%20to%20develop%20feature%20selection%20algorithms%20to%20improve%0Ainterpretability%2C%20reduce%20computational%20resources%2C%20and%20minimize%20overfitting%20in%0Apredictive%20models.%20Neural%20networks%20stand%20out%20as%20architectures%20on%20which%20to%20build%0Afeature%20selection%20methods%2C%20and%20recently%2C%20neuron%20pruning%20and%20regrowth%20have%0Aemerged%20from%20the%20sparse%20neural%20network%20literature%20as%20promising%20new%20tools.%20We%0Aintroduce%20EntryPrune%2C%20a%20novel%20supervised%20feature%20selection%20algorithm%20using%20a%0Adense%20neural%20network%20with%20a%20dynamic%20sparse%20input%20layer.%20It%20employs%20entry-based%0Apruning%2C%20a%20novel%20approach%20that%20compares%20neurons%20based%20on%20their%20relative%20change%0Ainduced%20when%20they%20have%20entered%20the%20network.%20Extensive%20experiments%20on%2013%0Adifferent%20datasets%20show%20that%20our%20approach%20generally%20outperforms%20the%20current%0Astate-of-the-art%20methods%2C%20and%20in%20particular%20improves%20the%20average%20accuracy%20on%0Alow-dimensional%20datasets.%20Furthermore%2C%20we%20show%20that%20EntryPruning%20surpasses%0Atraditional%20techniques%20such%20as%20magnitude%20pruning%20within%20the%20EntryPrune%0Aframework%20and%20that%20EntryPrune%20achieves%20lower%20runtime%20than%20competing%20approaches.%0AOur%20code%20is%20available%20at%20https%3A//github.com/flxzimmer/entryprune.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntryPrune%253A%2520Neural%2520Network%2520Feature%2520Selection%2520using%2520First%2520Impressions%26entry.906535625%3DFelix%2520Zimmer%2520and%2520Patrik%2520Okanovic%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520There%2520is%2520an%2520ongoing%2520effort%2520to%2520develop%2520feature%2520selection%2520algorithms%2520to%2520improve%250Ainterpretability%252C%2520reduce%2520computational%2520resources%252C%2520and%2520minimize%2520overfitting%2520in%250Apredictive%2520models.%2520Neural%2520networks%2520stand%2520out%2520as%2520architectures%2520on%2520which%2520to%2520build%250Afeature%2520selection%2520methods%252C%2520and%2520recently%252C%2520neuron%2520pruning%2520and%2520regrowth%2520have%250Aemerged%2520from%2520the%2520sparse%2520neural%2520network%2520literature%2520as%2520promising%2520new%2520tools.%2520We%250Aintroduce%2520EntryPrune%252C%2520a%2520novel%2520supervised%2520feature%2520selection%2520algorithm%2520using%2520a%250Adense%2520neural%2520network%2520with%2520a%2520dynamic%2520sparse%2520input%2520layer.%2520It%2520employs%2520entry-based%250Apruning%252C%2520a%2520novel%2520approach%2520that%2520compares%2520neurons%2520based%2520on%2520their%2520relative%2520change%250Ainduced%2520when%2520they%2520have%2520entered%2520the%2520network.%2520Extensive%2520experiments%2520on%252013%250Adifferent%2520datasets%2520show%2520that%2520our%2520approach%2520generally%2520outperforms%2520the%2520current%250Astate-of-the-art%2520methods%252C%2520and%2520in%2520particular%2520improves%2520the%2520average%2520accuracy%2520on%250Alow-dimensional%2520datasets.%2520Furthermore%252C%2520we%2520show%2520that%2520EntryPruning%2520surpasses%250Atraditional%2520techniques%2520such%2520as%2520magnitude%2520pruning%2520within%2520the%2520EntryPrune%250Aframework%2520and%2520that%2520EntryPrune%2520achieves%2520lower%2520runtime%2520than%2520competing%2520approaches.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/flxzimmer/entryprune.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EntryPrune%3A%20Neural%20Network%20Feature%20Selection%20using%20First%20Impressions&entry.906535625=Felix%20Zimmer%20and%20Patrik%20Okanovic%20and%20Torsten%20Hoefler&entry.1292438233=%20%20There%20is%20an%20ongoing%20effort%20to%20develop%20feature%20selection%20algorithms%20to%20improve%0Ainterpretability%2C%20reduce%20computational%20resources%2C%20and%20minimize%20overfitting%20in%0Apredictive%20models.%20Neural%20networks%20stand%20out%20as%20architectures%20on%20which%20to%20build%0Afeature%20selection%20methods%2C%20and%20recently%2C%20neuron%20pruning%20and%20regrowth%20have%0Aemerged%20from%20the%20sparse%20neural%20network%20literature%20as%20promising%20new%20tools.%20We%0Aintroduce%20EntryPrune%2C%20a%20novel%20supervised%20feature%20selection%20algorithm%20using%20a%0Adense%20neural%20network%20with%20a%20dynamic%20sparse%20input%20layer.%20It%20employs%20entry-based%0Apruning%2C%20a%20novel%20approach%20that%20compares%20neurons%20based%20on%20their%20relative%20change%0Ainduced%20when%20they%20have%20entered%20the%20network.%20Extensive%20experiments%20on%2013%0Adifferent%20datasets%20show%20that%20our%20approach%20generally%20outperforms%20the%20current%0Astate-of-the-art%20methods%2C%20and%20in%20particular%20improves%20the%20average%20accuracy%20on%0Alow-dimensional%20datasets.%20Furthermore%2C%20we%20show%20that%20EntryPruning%20surpasses%0Atraditional%20techniques%20such%20as%20magnitude%20pruning%20within%20the%20EntryPrune%0Aframework%20and%20that%20EntryPrune%20achieves%20lower%20runtime%20than%20competing%20approaches.%0AOur%20code%20is%20available%20at%20https%3A//github.com/flxzimmer/entryprune.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02344v3&entry.124074799=Read"},
{"title": "High-Dimensional Analysis of Bootstrap Ensemble Classifiers", "author": "Hamza Cherkaoui and Malik Tiomoko and Mohamed El Amine Seddik and Cosme Louart and Ekkehard Schnoor and Balazs Kegl", "abstract": "  Bootstrap methods have long been a cornerstone of ensemble learning in\nmachine learning. This paper presents a theoretical analysis of bootstrap\ntechniques applied to the Least Square Support Vector Machine (LSSVM) ensemble\nin the context of large and growing sample sizes and feature dimensionalities.\nLeveraging tools from Random Matrix Theory, we investigate the performance of\nthis classifier that aggregates decision functions from multiple weak\nclassifiers, each trained on different subsets of the data. We provide insights\ninto the use of bootstrap methods in high-dimensional settings, enhancing our\nunderstanding of their impact. Based on these findings, we propose strategies\nto select the number of subsets and the regularization parameter that maximize\nthe performance of the LSSVM. Empirical experiments on synthetic and real-world\ndatasets validate our theoretical results.\n", "link": "http://arxiv.org/abs/2505.14587v1", "date": "2025-05-20", "relevancy": 2.3238, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5084}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.444}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Dimensional%20Analysis%20of%20Bootstrap%20Ensemble%20Classifiers&body=Title%3A%20High-Dimensional%20Analysis%20of%20Bootstrap%20Ensemble%20Classifiers%0AAuthor%3A%20Hamza%20Cherkaoui%20and%20Malik%20Tiomoko%20and%20Mohamed%20El%20Amine%20Seddik%20and%20Cosme%20Louart%20and%20Ekkehard%20Schnoor%20and%20Balazs%20Kegl%0AAbstract%3A%20%20%20Bootstrap%20methods%20have%20long%20been%20a%20cornerstone%20of%20ensemble%20learning%20in%0Amachine%20learning.%20This%20paper%20presents%20a%20theoretical%20analysis%20of%20bootstrap%0Atechniques%20applied%20to%20the%20Least%20Square%20Support%20Vector%20Machine%20%28LSSVM%29%20ensemble%0Ain%20the%20context%20of%20large%20and%20growing%20sample%20sizes%20and%20feature%20dimensionalities.%0ALeveraging%20tools%20from%20Random%20Matrix%20Theory%2C%20we%20investigate%20the%20performance%20of%0Athis%20classifier%20that%20aggregates%20decision%20functions%20from%20multiple%20weak%0Aclassifiers%2C%20each%20trained%20on%20different%20subsets%20of%20the%20data.%20We%20provide%20insights%0Ainto%20the%20use%20of%20bootstrap%20methods%20in%20high-dimensional%20settings%2C%20enhancing%20our%0Aunderstanding%20of%20their%20impact.%20Based%20on%20these%20findings%2C%20we%20propose%20strategies%0Ato%20select%20the%20number%20of%20subsets%20and%20the%20regularization%20parameter%20that%20maximize%0Athe%20performance%20of%20the%20LSSVM.%20Empirical%20experiments%20on%20synthetic%20and%20real-world%0Adatasets%20validate%20our%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Dimensional%2520Analysis%2520of%2520Bootstrap%2520Ensemble%2520Classifiers%26entry.906535625%3DHamza%2520Cherkaoui%2520and%2520Malik%2520Tiomoko%2520and%2520Mohamed%2520El%2520Amine%2520Seddik%2520and%2520Cosme%2520Louart%2520and%2520Ekkehard%2520Schnoor%2520and%2520Balazs%2520Kegl%26entry.1292438233%3D%2520%2520Bootstrap%2520methods%2520have%2520long%2520been%2520a%2520cornerstone%2520of%2520ensemble%2520learning%2520in%250Amachine%2520learning.%2520This%2520paper%2520presents%2520a%2520theoretical%2520analysis%2520of%2520bootstrap%250Atechniques%2520applied%2520to%2520the%2520Least%2520Square%2520Support%2520Vector%2520Machine%2520%2528LSSVM%2529%2520ensemble%250Ain%2520the%2520context%2520of%2520large%2520and%2520growing%2520sample%2520sizes%2520and%2520feature%2520dimensionalities.%250ALeveraging%2520tools%2520from%2520Random%2520Matrix%2520Theory%252C%2520we%2520investigate%2520the%2520performance%2520of%250Athis%2520classifier%2520that%2520aggregates%2520decision%2520functions%2520from%2520multiple%2520weak%250Aclassifiers%252C%2520each%2520trained%2520on%2520different%2520subsets%2520of%2520the%2520data.%2520We%2520provide%2520insights%250Ainto%2520the%2520use%2520of%2520bootstrap%2520methods%2520in%2520high-dimensional%2520settings%252C%2520enhancing%2520our%250Aunderstanding%2520of%2520their%2520impact.%2520Based%2520on%2520these%2520findings%252C%2520we%2520propose%2520strategies%250Ato%2520select%2520the%2520number%2520of%2520subsets%2520and%2520the%2520regularization%2520parameter%2520that%2520maximize%250Athe%2520performance%2520of%2520the%2520LSSVM.%2520Empirical%2520experiments%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520validate%2520our%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Dimensional%20Analysis%20of%20Bootstrap%20Ensemble%20Classifiers&entry.906535625=Hamza%20Cherkaoui%20and%20Malik%20Tiomoko%20and%20Mohamed%20El%20Amine%20Seddik%20and%20Cosme%20Louart%20and%20Ekkehard%20Schnoor%20and%20Balazs%20Kegl&entry.1292438233=%20%20Bootstrap%20methods%20have%20long%20been%20a%20cornerstone%20of%20ensemble%20learning%20in%0Amachine%20learning.%20This%20paper%20presents%20a%20theoretical%20analysis%20of%20bootstrap%0Atechniques%20applied%20to%20the%20Least%20Square%20Support%20Vector%20Machine%20%28LSSVM%29%20ensemble%0Ain%20the%20context%20of%20large%20and%20growing%20sample%20sizes%20and%20feature%20dimensionalities.%0ALeveraging%20tools%20from%20Random%20Matrix%20Theory%2C%20we%20investigate%20the%20performance%20of%0Athis%20classifier%20that%20aggregates%20decision%20functions%20from%20multiple%20weak%0Aclassifiers%2C%20each%20trained%20on%20different%20subsets%20of%20the%20data.%20We%20provide%20insights%0Ainto%20the%20use%20of%20bootstrap%20methods%20in%20high-dimensional%20settings%2C%20enhancing%20our%0Aunderstanding%20of%20their%20impact.%20Based%20on%20these%20findings%2C%20we%20propose%20strategies%0Ato%20select%20the%20number%20of%20subsets%20and%20the%20regularization%20parameter%20that%20maximize%0Athe%20performance%20of%20the%20LSSVM.%20Empirical%20experiments%20on%20synthetic%20and%20real-world%0Adatasets%20validate%20our%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14587v1&entry.124074799=Read"},
{"title": "IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic\n  Visual Prompting", "author": "Yuxin Zhang and Minyan Luo and Weiming Dong and Xiao Yang and Haibin Huang and Chongyang Ma and Oliver Deussen and Tong-Yee Lee and Changsheng Xu", "abstract": "  The stories and characters that captivate us as we grow up shape unique\nfantasy worlds, with images serving as the primary medium for visually\nexperiencing these realms. Personalizing generative models through fine-tuning\nwith theme-specific data has become a prevalent approach in text-to-image\ngeneration. However, unlike object customization, which focuses on learning\nspecific objects, theme-specific generation encompasses diverse elements such\nas characters, scenes, and objects. Such diversity also introduces a key\nchallenge: how to adaptively generate multi-character, multi-concept, and\ncontinuous theme-specific images (TSI). Moreover, fine-tuning approaches often\ncome with significant computational overhead, time costs, and risks of\noverfitting. This paper explores a fundamental question: Can image generation\nmodels directly leverage images as contextual input, similarly to how large\nlanguage models use text as context? To address this, we present IP-Prompter, a\nnovel training-free TSI generation method. IP-Prompter introduces visual\nprompting, a mechanism that integrates reference images into generative models,\nallowing users to seamlessly specify the target theme without requiring\nadditional training. To further enhance this process, we propose a Dynamic\nVisual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to\nimprove the accuracy and quality of generated images. Our approach enables\ndiverse applications, including consistent story generation, character design,\nrealistic character generation, and style-guided image generation. Comparative\nevaluations against state-of-the-art personalization methods demonstrate that\nIP-Prompter achieves significantly better results and excels in maintaining\ncharacter identity preserving, style consistency and text alignment, offering a\nrobust and flexible solution for theme-specific image generation.\n", "link": "http://arxiv.org/abs/2501.15641v2", "date": "2025-05-20", "relevancy": 2.3123, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5829}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5825}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IP-Prompter%3A%20Training-Free%20Theme-Specific%20Image%20Generation%20via%20Dynamic%0A%20%20Visual%20Prompting&body=Title%3A%20IP-Prompter%3A%20Training-Free%20Theme-Specific%20Image%20Generation%20via%20Dynamic%0A%20%20Visual%20Prompting%0AAuthor%3A%20Yuxin%20Zhang%20and%20Minyan%20Luo%20and%20Weiming%20Dong%20and%20Xiao%20Yang%20and%20Haibin%20Huang%20and%20Chongyang%20Ma%20and%20Oliver%20Deussen%20and%20Tong-Yee%20Lee%20and%20Changsheng%20Xu%0AAbstract%3A%20%20%20The%20stories%20and%20characters%20that%20captivate%20us%20as%20we%20grow%20up%20shape%20unique%0Afantasy%20worlds%2C%20with%20images%20serving%20as%20the%20primary%20medium%20for%20visually%0Aexperiencing%20these%20realms.%20Personalizing%20generative%20models%20through%20fine-tuning%0Awith%20theme-specific%20data%20has%20become%20a%20prevalent%20approach%20in%20text-to-image%0Ageneration.%20However%2C%20unlike%20object%20customization%2C%20which%20focuses%20on%20learning%0Aspecific%20objects%2C%20theme-specific%20generation%20encompasses%20diverse%20elements%20such%0Aas%20characters%2C%20scenes%2C%20and%20objects.%20Such%20diversity%20also%20introduces%20a%20key%0Achallenge%3A%20how%20to%20adaptively%20generate%20multi-character%2C%20multi-concept%2C%20and%0Acontinuous%20theme-specific%20images%20%28TSI%29.%20Moreover%2C%20fine-tuning%20approaches%20often%0Acome%20with%20significant%20computational%20overhead%2C%20time%20costs%2C%20and%20risks%20of%0Aoverfitting.%20This%20paper%20explores%20a%20fundamental%20question%3A%20Can%20image%20generation%0Amodels%20directly%20leverage%20images%20as%20contextual%20input%2C%20similarly%20to%20how%20large%0Alanguage%20models%20use%20text%20as%20context%3F%20To%20address%20this%2C%20we%20present%20IP-Prompter%2C%20a%0Anovel%20training-free%20TSI%20generation%20method.%20IP-Prompter%20introduces%20visual%0Aprompting%2C%20a%20mechanism%20that%20integrates%20reference%20images%20into%20generative%20models%2C%0Aallowing%20users%20to%20seamlessly%20specify%20the%20target%20theme%20without%20requiring%0Aadditional%20training.%20To%20further%20enhance%20this%20process%2C%20we%20propose%20a%20Dynamic%0AVisual%20Prompting%20%28DVP%29%20mechanism%2C%20which%20iteratively%20optimizes%20visual%20prompts%20to%0Aimprove%20the%20accuracy%20and%20quality%20of%20generated%20images.%20Our%20approach%20enables%0Adiverse%20applications%2C%20including%20consistent%20story%20generation%2C%20character%20design%2C%0Arealistic%20character%20generation%2C%20and%20style-guided%20image%20generation.%20Comparative%0Aevaluations%20against%20state-of-the-art%20personalization%20methods%20demonstrate%20that%0AIP-Prompter%20achieves%20significantly%20better%20results%20and%20excels%20in%20maintaining%0Acharacter%20identity%20preserving%2C%20style%20consistency%20and%20text%20alignment%2C%20offering%20a%0Arobust%20and%20flexible%20solution%20for%20theme-specific%20image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.15641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIP-Prompter%253A%2520Training-Free%2520Theme-Specific%2520Image%2520Generation%2520via%2520Dynamic%250A%2520%2520Visual%2520Prompting%26entry.906535625%3DYuxin%2520Zhang%2520and%2520Minyan%2520Luo%2520and%2520Weiming%2520Dong%2520and%2520Xiao%2520Yang%2520and%2520Haibin%2520Huang%2520and%2520Chongyang%2520Ma%2520and%2520Oliver%2520Deussen%2520and%2520Tong-Yee%2520Lee%2520and%2520Changsheng%2520Xu%26entry.1292438233%3D%2520%2520The%2520stories%2520and%2520characters%2520that%2520captivate%2520us%2520as%2520we%2520grow%2520up%2520shape%2520unique%250Afantasy%2520worlds%252C%2520with%2520images%2520serving%2520as%2520the%2520primary%2520medium%2520for%2520visually%250Aexperiencing%2520these%2520realms.%2520Personalizing%2520generative%2520models%2520through%2520fine-tuning%250Awith%2520theme-specific%2520data%2520has%2520become%2520a%2520prevalent%2520approach%2520in%2520text-to-image%250Ageneration.%2520However%252C%2520unlike%2520object%2520customization%252C%2520which%2520focuses%2520on%2520learning%250Aspecific%2520objects%252C%2520theme-specific%2520generation%2520encompasses%2520diverse%2520elements%2520such%250Aas%2520characters%252C%2520scenes%252C%2520and%2520objects.%2520Such%2520diversity%2520also%2520introduces%2520a%2520key%250Achallenge%253A%2520how%2520to%2520adaptively%2520generate%2520multi-character%252C%2520multi-concept%252C%2520and%250Acontinuous%2520theme-specific%2520images%2520%2528TSI%2529.%2520Moreover%252C%2520fine-tuning%2520approaches%2520often%250Acome%2520with%2520significant%2520computational%2520overhead%252C%2520time%2520costs%252C%2520and%2520risks%2520of%250Aoverfitting.%2520This%2520paper%2520explores%2520a%2520fundamental%2520question%253A%2520Can%2520image%2520generation%250Amodels%2520directly%2520leverage%2520images%2520as%2520contextual%2520input%252C%2520similarly%2520to%2520how%2520large%250Alanguage%2520models%2520use%2520text%2520as%2520context%253F%2520To%2520address%2520this%252C%2520we%2520present%2520IP-Prompter%252C%2520a%250Anovel%2520training-free%2520TSI%2520generation%2520method.%2520IP-Prompter%2520introduces%2520visual%250Aprompting%252C%2520a%2520mechanism%2520that%2520integrates%2520reference%2520images%2520into%2520generative%2520models%252C%250Aallowing%2520users%2520to%2520seamlessly%2520specify%2520the%2520target%2520theme%2520without%2520requiring%250Aadditional%2520training.%2520To%2520further%2520enhance%2520this%2520process%252C%2520we%2520propose%2520a%2520Dynamic%250AVisual%2520Prompting%2520%2528DVP%2529%2520mechanism%252C%2520which%2520iteratively%2520optimizes%2520visual%2520prompts%2520to%250Aimprove%2520the%2520accuracy%2520and%2520quality%2520of%2520generated%2520images.%2520Our%2520approach%2520enables%250Adiverse%2520applications%252C%2520including%2520consistent%2520story%2520generation%252C%2520character%2520design%252C%250Arealistic%2520character%2520generation%252C%2520and%2520style-guided%2520image%2520generation.%2520Comparative%250Aevaluations%2520against%2520state-of-the-art%2520personalization%2520methods%2520demonstrate%2520that%250AIP-Prompter%2520achieves%2520significantly%2520better%2520results%2520and%2520excels%2520in%2520maintaining%250Acharacter%2520identity%2520preserving%252C%2520style%2520consistency%2520and%2520text%2520alignment%252C%2520offering%2520a%250Arobust%2520and%2520flexible%2520solution%2520for%2520theme-specific%2520image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.15641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IP-Prompter%3A%20Training-Free%20Theme-Specific%20Image%20Generation%20via%20Dynamic%0A%20%20Visual%20Prompting&entry.906535625=Yuxin%20Zhang%20and%20Minyan%20Luo%20and%20Weiming%20Dong%20and%20Xiao%20Yang%20and%20Haibin%20Huang%20and%20Chongyang%20Ma%20and%20Oliver%20Deussen%20and%20Tong-Yee%20Lee%20and%20Changsheng%20Xu&entry.1292438233=%20%20The%20stories%20and%20characters%20that%20captivate%20us%20as%20we%20grow%20up%20shape%20unique%0Afantasy%20worlds%2C%20with%20images%20serving%20as%20the%20primary%20medium%20for%20visually%0Aexperiencing%20these%20realms.%20Personalizing%20generative%20models%20through%20fine-tuning%0Awith%20theme-specific%20data%20has%20become%20a%20prevalent%20approach%20in%20text-to-image%0Ageneration.%20However%2C%20unlike%20object%20customization%2C%20which%20focuses%20on%20learning%0Aspecific%20objects%2C%20theme-specific%20generation%20encompasses%20diverse%20elements%20such%0Aas%20characters%2C%20scenes%2C%20and%20objects.%20Such%20diversity%20also%20introduces%20a%20key%0Achallenge%3A%20how%20to%20adaptively%20generate%20multi-character%2C%20multi-concept%2C%20and%0Acontinuous%20theme-specific%20images%20%28TSI%29.%20Moreover%2C%20fine-tuning%20approaches%20often%0Acome%20with%20significant%20computational%20overhead%2C%20time%20costs%2C%20and%20risks%20of%0Aoverfitting.%20This%20paper%20explores%20a%20fundamental%20question%3A%20Can%20image%20generation%0Amodels%20directly%20leverage%20images%20as%20contextual%20input%2C%20similarly%20to%20how%20large%0Alanguage%20models%20use%20text%20as%20context%3F%20To%20address%20this%2C%20we%20present%20IP-Prompter%2C%20a%0Anovel%20training-free%20TSI%20generation%20method.%20IP-Prompter%20introduces%20visual%0Aprompting%2C%20a%20mechanism%20that%20integrates%20reference%20images%20into%20generative%20models%2C%0Aallowing%20users%20to%20seamlessly%20specify%20the%20target%20theme%20without%20requiring%0Aadditional%20training.%20To%20further%20enhance%20this%20process%2C%20we%20propose%20a%20Dynamic%0AVisual%20Prompting%20%28DVP%29%20mechanism%2C%20which%20iteratively%20optimizes%20visual%20prompts%20to%0Aimprove%20the%20accuracy%20and%20quality%20of%20generated%20images.%20Our%20approach%20enables%0Adiverse%20applications%2C%20including%20consistent%20story%20generation%2C%20character%20design%2C%0Arealistic%20character%20generation%2C%20and%20style-guided%20image%20generation.%20Comparative%0Aevaluations%20against%20state-of-the-art%20personalization%20methods%20demonstrate%20that%0AIP-Prompter%20achieves%20significantly%20better%20results%20and%20excels%20in%20maintaining%0Acharacter%20identity%20preserving%2C%20style%20consistency%20and%20text%20alignment%2C%20offering%20a%0Arobust%20and%20flexible%20solution%20for%20theme-specific%20image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.15641v2&entry.124074799=Read"},
{"title": "Scaling Video-Language Models to 10K Frames via Hierarchical\n  Differential Distillation", "author": "Chuanqi Cheng and Jian Guan and Wei Wu and Rui Yan", "abstract": "  Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLAMP, a hierarchical video-language model that\nprocesses hour-long videos at \"mixed precision\" through two key mechanisms: (1)\ndifferential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLAMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLAMP's superior performance\nacross five video understanding benchmarks, particularly on long-form content.\nNotably, ViLAMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance. Code and model are available at\nhttps://github.com/steven-ccq/ViLAMP.\n", "link": "http://arxiv.org/abs/2504.02438v4", "date": "2025-05-20", "relevancy": 2.3008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.576}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Video-Language%20Models%20to%2010K%20Frames%20via%20Hierarchical%0A%20%20Differential%20Distillation&body=Title%3A%20Scaling%20Video-Language%20Models%20to%2010K%20Frames%20via%20Hierarchical%0A%20%20Differential%20Distillation%0AAuthor%3A%20Chuanqi%20Cheng%20and%20Jian%20Guan%20and%20Wei%20Wu%20and%20Rui%20Yan%0AAbstract%3A%20%20%20Long-form%20video%20processing%20fundamentally%20challenges%20vision-language%20models%0A%28VLMs%29%20due%20to%20the%20high%20computational%20costs%20of%20handling%20extended%20temporal%0Asequences.%20Existing%20token%20pruning%20and%20feature%20merging%20methods%20often%20sacrifice%0Acritical%20temporal%20dependencies%20or%20dilute%20semantic%20information.%20We%20introduce%0Adifferential%20distillation%2C%20a%20principled%20approach%20that%20systematically%20preserves%0Atask-relevant%20information%20while%20suppressing%20redundancy.%20Based%20on%20this%0Aprinciple%2C%20we%20develop%20ViLAMP%2C%20a%20hierarchical%20video-language%20model%20that%0Aprocesses%20hour-long%20videos%20at%20%22mixed%20precision%22%20through%20two%20key%20mechanisms%3A%20%281%29%0Adifferential%20keyframe%20selection%20that%20maximizes%20query%20relevance%20while%0Amaintaining%20temporal%20distinctiveness%20at%20the%20frame%20level%20and%20%282%29%20differential%0Afeature%20merging%20that%20preserves%20query-salient%20features%20in%20non-keyframes%20at%20the%0Apatch%20level.%20Hence%2C%20ViLAMP%20retains%20full%20information%20in%20keyframes%20while%20reducing%0Anon-keyframes%20to%20their%20most%20salient%20features%2C%20resembling%20mixed-precision%0Atraining.%20Extensive%20experiments%20demonstrate%20ViLAMP%27s%20superior%20performance%0Aacross%20five%20video%20understanding%20benchmarks%2C%20particularly%20on%20long-form%20content.%0ANotably%2C%20ViLAMP%20can%20process%20ultra-long%20videos%20%28up%20to%2010K%20frames%29%20on%20a%20single%0ANVIDIA%20A100%20GPU%2C%20achieving%20substantial%20computational%20efficiency%20while%0Amaintaining%20state-of-the-art%20performance.%20Code%20and%20model%20are%20available%20at%0Ahttps%3A//github.com/steven-ccq/ViLAMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02438v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Video-Language%2520Models%2520to%252010K%2520Frames%2520via%2520Hierarchical%250A%2520%2520Differential%2520Distillation%26entry.906535625%3DChuanqi%2520Cheng%2520and%2520Jian%2520Guan%2520and%2520Wei%2520Wu%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520Long-form%2520video%2520processing%2520fundamentally%2520challenges%2520vision-language%2520models%250A%2528VLMs%2529%2520due%2520to%2520the%2520high%2520computational%2520costs%2520of%2520handling%2520extended%2520temporal%250Asequences.%2520Existing%2520token%2520pruning%2520and%2520feature%2520merging%2520methods%2520often%2520sacrifice%250Acritical%2520temporal%2520dependencies%2520or%2520dilute%2520semantic%2520information.%2520We%2520introduce%250Adifferential%2520distillation%252C%2520a%2520principled%2520approach%2520that%2520systematically%2520preserves%250Atask-relevant%2520information%2520while%2520suppressing%2520redundancy.%2520Based%2520on%2520this%250Aprinciple%252C%2520we%2520develop%2520ViLAMP%252C%2520a%2520hierarchical%2520video-language%2520model%2520that%250Aprocesses%2520hour-long%2520videos%2520at%2520%2522mixed%2520precision%2522%2520through%2520two%2520key%2520mechanisms%253A%2520%25281%2529%250Adifferential%2520keyframe%2520selection%2520that%2520maximizes%2520query%2520relevance%2520while%250Amaintaining%2520temporal%2520distinctiveness%2520at%2520the%2520frame%2520level%2520and%2520%25282%2529%2520differential%250Afeature%2520merging%2520that%2520preserves%2520query-salient%2520features%2520in%2520non-keyframes%2520at%2520the%250Apatch%2520level.%2520Hence%252C%2520ViLAMP%2520retains%2520full%2520information%2520in%2520keyframes%2520while%2520reducing%250Anon-keyframes%2520to%2520their%2520most%2520salient%2520features%252C%2520resembling%2520mixed-precision%250Atraining.%2520Extensive%2520experiments%2520demonstrate%2520ViLAMP%2527s%2520superior%2520performance%250Aacross%2520five%2520video%2520understanding%2520benchmarks%252C%2520particularly%2520on%2520long-form%2520content.%250ANotably%252C%2520ViLAMP%2520can%2520process%2520ultra-long%2520videos%2520%2528up%2520to%252010K%2520frames%2529%2520on%2520a%2520single%250ANVIDIA%2520A100%2520GPU%252C%2520achieving%2520substantial%2520computational%2520efficiency%2520while%250Amaintaining%2520state-of-the-art%2520performance.%2520Code%2520and%2520model%2520are%2520available%2520at%250Ahttps%253A//github.com/steven-ccq/ViLAMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02438v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Video-Language%20Models%20to%2010K%20Frames%20via%20Hierarchical%0A%20%20Differential%20Distillation&entry.906535625=Chuanqi%20Cheng%20and%20Jian%20Guan%20and%20Wei%20Wu%20and%20Rui%20Yan&entry.1292438233=%20%20Long-form%20video%20processing%20fundamentally%20challenges%20vision-language%20models%0A%28VLMs%29%20due%20to%20the%20high%20computational%20costs%20of%20handling%20extended%20temporal%0Asequences.%20Existing%20token%20pruning%20and%20feature%20merging%20methods%20often%20sacrifice%0Acritical%20temporal%20dependencies%20or%20dilute%20semantic%20information.%20We%20introduce%0Adifferential%20distillation%2C%20a%20principled%20approach%20that%20systematically%20preserves%0Atask-relevant%20information%20while%20suppressing%20redundancy.%20Based%20on%20this%0Aprinciple%2C%20we%20develop%20ViLAMP%2C%20a%20hierarchical%20video-language%20model%20that%0Aprocesses%20hour-long%20videos%20at%20%22mixed%20precision%22%20through%20two%20key%20mechanisms%3A%20%281%29%0Adifferential%20keyframe%20selection%20that%20maximizes%20query%20relevance%20while%0Amaintaining%20temporal%20distinctiveness%20at%20the%20frame%20level%20and%20%282%29%20differential%0Afeature%20merging%20that%20preserves%20query-salient%20features%20in%20non-keyframes%20at%20the%0Apatch%20level.%20Hence%2C%20ViLAMP%20retains%20full%20information%20in%20keyframes%20while%20reducing%0Anon-keyframes%20to%20their%20most%20salient%20features%2C%20resembling%20mixed-precision%0Atraining.%20Extensive%20experiments%20demonstrate%20ViLAMP%27s%20superior%20performance%0Aacross%20five%20video%20understanding%20benchmarks%2C%20particularly%20on%20long-form%20content.%0ANotably%2C%20ViLAMP%20can%20process%20ultra-long%20videos%20%28up%20to%2010K%20frames%29%20on%20a%20single%0ANVIDIA%20A100%20GPU%2C%20achieving%20substantial%20computational%20efficiency%20while%0Amaintaining%20state-of-the-art%20performance.%20Code%20and%20model%20are%20available%20at%0Ahttps%3A//github.com/steven-ccq/ViLAMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02438v4&entry.124074799=Read"},
{"title": "Local Minima Prediction using Dynamic Bayesian Filtering for UGV\n  Navigation in Unstructured Environments", "author": "Seung Hun Lee and Wonse Jo and Lionel P. Robert Jr. and Dawn M. Tilbury", "abstract": "  Path planning is crucial for the navigation of autonomous vehicles, yet these\nvehicles face challenges in complex and real-world environments. Although a\nglobal view may be provided, it is often outdated, necessitating the reliance\nof Unmanned Ground Vehicles (UGVs) on real-time local information. This\nreliance on partial information, without considering the global context, can\nlead to UGVs getting stuck in local minima. This paper develops a method to\nproactively predict local minima using Dynamic Bayesian filtering, based on the\ndetected obstacles in the local view and the global goal. This approach aims to\nenhance the autonomous navigation of self-driving vehicles by allowing them to\npredict potential pitfalls before they get stuck, and either ask for help from\na human, or re-plan an alternate trajectory.\n", "link": "http://arxiv.org/abs/2505.14337v1", "date": "2025-05-20", "relevancy": 2.2995, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Minima%20Prediction%20using%20Dynamic%20Bayesian%20Filtering%20for%20UGV%0A%20%20Navigation%20in%20Unstructured%20Environments&body=Title%3A%20Local%20Minima%20Prediction%20using%20Dynamic%20Bayesian%20Filtering%20for%20UGV%0A%20%20Navigation%20in%20Unstructured%20Environments%0AAuthor%3A%20Seung%20Hun%20Lee%20and%20Wonse%20Jo%20and%20Lionel%20P.%20Robert%20Jr.%20and%20Dawn%20M.%20Tilbury%0AAbstract%3A%20%20%20Path%20planning%20is%20crucial%20for%20the%20navigation%20of%20autonomous%20vehicles%2C%20yet%20these%0Avehicles%20face%20challenges%20in%20complex%20and%20real-world%20environments.%20Although%20a%0Aglobal%20view%20may%20be%20provided%2C%20it%20is%20often%20outdated%2C%20necessitating%20the%20reliance%0Aof%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20on%20real-time%20local%20information.%20This%0Areliance%20on%20partial%20information%2C%20without%20considering%20the%20global%20context%2C%20can%0Alead%20to%20UGVs%20getting%20stuck%20in%20local%20minima.%20This%20paper%20develops%20a%20method%20to%0Aproactively%20predict%20local%20minima%20using%20Dynamic%20Bayesian%20filtering%2C%20based%20on%20the%0Adetected%20obstacles%20in%20the%20local%20view%20and%20the%20global%20goal.%20This%20approach%20aims%20to%0Aenhance%20the%20autonomous%20navigation%20of%20self-driving%20vehicles%20by%20allowing%20them%20to%0Apredict%20potential%20pitfalls%20before%20they%20get%20stuck%2C%20and%20either%20ask%20for%20help%20from%0Aa%20human%2C%20or%20re-plan%20an%20alternate%20trajectory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Minima%2520Prediction%2520using%2520Dynamic%2520Bayesian%2520Filtering%2520for%2520UGV%250A%2520%2520Navigation%2520in%2520Unstructured%2520Environments%26entry.906535625%3DSeung%2520Hun%2520Lee%2520and%2520Wonse%2520Jo%2520and%2520Lionel%2520P.%2520Robert%2520Jr.%2520and%2520Dawn%2520M.%2520Tilbury%26entry.1292438233%3D%2520%2520Path%2520planning%2520is%2520crucial%2520for%2520the%2520navigation%2520of%2520autonomous%2520vehicles%252C%2520yet%2520these%250Avehicles%2520face%2520challenges%2520in%2520complex%2520and%2520real-world%2520environments.%2520Although%2520a%250Aglobal%2520view%2520may%2520be%2520provided%252C%2520it%2520is%2520often%2520outdated%252C%2520necessitating%2520the%2520reliance%250Aof%2520Unmanned%2520Ground%2520Vehicles%2520%2528UGVs%2529%2520on%2520real-time%2520local%2520information.%2520This%250Areliance%2520on%2520partial%2520information%252C%2520without%2520considering%2520the%2520global%2520context%252C%2520can%250Alead%2520to%2520UGVs%2520getting%2520stuck%2520in%2520local%2520minima.%2520This%2520paper%2520develops%2520a%2520method%2520to%250Aproactively%2520predict%2520local%2520minima%2520using%2520Dynamic%2520Bayesian%2520filtering%252C%2520based%2520on%2520the%250Adetected%2520obstacles%2520in%2520the%2520local%2520view%2520and%2520the%2520global%2520goal.%2520This%2520approach%2520aims%2520to%250Aenhance%2520the%2520autonomous%2520navigation%2520of%2520self-driving%2520vehicles%2520by%2520allowing%2520them%2520to%250Apredict%2520potential%2520pitfalls%2520before%2520they%2520get%2520stuck%252C%2520and%2520either%2520ask%2520for%2520help%2520from%250Aa%2520human%252C%2520or%2520re-plan%2520an%2520alternate%2520trajectory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Minima%20Prediction%20using%20Dynamic%20Bayesian%20Filtering%20for%20UGV%0A%20%20Navigation%20in%20Unstructured%20Environments&entry.906535625=Seung%20Hun%20Lee%20and%20Wonse%20Jo%20and%20Lionel%20P.%20Robert%20Jr.%20and%20Dawn%20M.%20Tilbury&entry.1292438233=%20%20Path%20planning%20is%20crucial%20for%20the%20navigation%20of%20autonomous%20vehicles%2C%20yet%20these%0Avehicles%20face%20challenges%20in%20complex%20and%20real-world%20environments.%20Although%20a%0Aglobal%20view%20may%20be%20provided%2C%20it%20is%20often%20outdated%2C%20necessitating%20the%20reliance%0Aof%20Unmanned%20Ground%20Vehicles%20%28UGVs%29%20on%20real-time%20local%20information.%20This%0Areliance%20on%20partial%20information%2C%20without%20considering%20the%20global%20context%2C%20can%0Alead%20to%20UGVs%20getting%20stuck%20in%20local%20minima.%20This%20paper%20develops%20a%20method%20to%0Aproactively%20predict%20local%20minima%20using%20Dynamic%20Bayesian%20filtering%2C%20based%20on%20the%0Adetected%20obstacles%20in%20the%20local%20view%20and%20the%20global%20goal.%20This%20approach%20aims%20to%0Aenhance%20the%20autonomous%20navigation%20of%20self-driving%20vehicles%20by%20allowing%20them%20to%0Apredict%20potential%20pitfalls%20before%20they%20get%20stuck%2C%20and%20either%20ask%20for%20help%20from%0Aa%20human%2C%20or%20re-plan%20an%20alternate%20trajectory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14337v1&entry.124074799=Read"},
{"title": "FuxiMT: Sparsifying Large Language Models for Chinese-Centric\n  Multilingual Machine Translation", "author": "Shaolin Zhu and Tianyu Dong and Bo Li and Deyi Xiong", "abstract": "  In this paper, we present FuxiMT, a novel Chinese-centric multilingual\nmachine translation model powered by a sparsified large language model (LLM).\nWe adopt a two-stage strategy to train FuxiMT. We first pre-train the model on\na massive Chinese corpus and then conduct multilingual fine-tuning on a large\nparallel dataset encompassing 65 languages. FuxiMT incorporates\nMixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust\nperformance across various resource levels. Experimental results demonstrate\nthat FuxiMT significantly outperforms strong baselines, including\nstate-of-the-art LLMs and machine translation models, particularly under\nlow-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot\ntranslation capabilities for unseen language pairs, indicating its potential to\nbridge communication gaps where parallel data are scarce or unavailable.\n", "link": "http://arxiv.org/abs/2505.14256v1", "date": "2025-05-20", "relevancy": 2.2803, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4561}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FuxiMT%3A%20Sparsifying%20Large%20Language%20Models%20for%20Chinese-Centric%0A%20%20Multilingual%20Machine%20Translation&body=Title%3A%20FuxiMT%3A%20Sparsifying%20Large%20Language%20Models%20for%20Chinese-Centric%0A%20%20Multilingual%20Machine%20Translation%0AAuthor%3A%20Shaolin%20Zhu%20and%20Tianyu%20Dong%20and%20Bo%20Li%20and%20Deyi%20Xiong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20FuxiMT%2C%20a%20novel%20Chinese-centric%20multilingual%0Amachine%20translation%20model%20powered%20by%20a%20sparsified%20large%20language%20model%20%28LLM%29.%0AWe%20adopt%20a%20two-stage%20strategy%20to%20train%20FuxiMT.%20We%20first%20pre-train%20the%20model%20on%0Aa%20massive%20Chinese%20corpus%20and%20then%20conduct%20multilingual%20fine-tuning%20on%20a%20large%0Aparallel%20dataset%20encompassing%2065%20languages.%20FuxiMT%20incorporates%0AMixture-of-Experts%20%28MoEs%29%20and%20employs%20a%20curriculum%20learning%20strategy%20for%20robust%0Aperformance%20across%20various%20resource%20levels.%20Experimental%20results%20demonstrate%0Athat%20FuxiMT%20significantly%20outperforms%20strong%20baselines%2C%20including%0Astate-of-the-art%20LLMs%20and%20machine%20translation%20models%2C%20particularly%20under%0Alow-resource%20scenarios.%20Furthermore%2C%20FuxiMT%20exhibits%20remarkable%20zero-shot%0Atranslation%20capabilities%20for%20unseen%20language%20pairs%2C%20indicating%20its%20potential%20to%0Abridge%20communication%20gaps%20where%20parallel%20data%20are%20scarce%20or%20unavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFuxiMT%253A%2520Sparsifying%2520Large%2520Language%2520Models%2520for%2520Chinese-Centric%250A%2520%2520Multilingual%2520Machine%2520Translation%26entry.906535625%3DShaolin%2520Zhu%2520and%2520Tianyu%2520Dong%2520and%2520Bo%2520Li%2520and%2520Deyi%2520Xiong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520FuxiMT%252C%2520a%2520novel%2520Chinese-centric%2520multilingual%250Amachine%2520translation%2520model%2520powered%2520by%2520a%2520sparsified%2520large%2520language%2520model%2520%2528LLM%2529.%250AWe%2520adopt%2520a%2520two-stage%2520strategy%2520to%2520train%2520FuxiMT.%2520We%2520first%2520pre-train%2520the%2520model%2520on%250Aa%2520massive%2520Chinese%2520corpus%2520and%2520then%2520conduct%2520multilingual%2520fine-tuning%2520on%2520a%2520large%250Aparallel%2520dataset%2520encompassing%252065%2520languages.%2520FuxiMT%2520incorporates%250AMixture-of-Experts%2520%2528MoEs%2529%2520and%2520employs%2520a%2520curriculum%2520learning%2520strategy%2520for%2520robust%250Aperformance%2520across%2520various%2520resource%2520levels.%2520Experimental%2520results%2520demonstrate%250Athat%2520FuxiMT%2520significantly%2520outperforms%2520strong%2520baselines%252C%2520including%250Astate-of-the-art%2520LLMs%2520and%2520machine%2520translation%2520models%252C%2520particularly%2520under%250Alow-resource%2520scenarios.%2520Furthermore%252C%2520FuxiMT%2520exhibits%2520remarkable%2520zero-shot%250Atranslation%2520capabilities%2520for%2520unseen%2520language%2520pairs%252C%2520indicating%2520its%2520potential%2520to%250Abridge%2520communication%2520gaps%2520where%2520parallel%2520data%2520are%2520scarce%2520or%2520unavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FuxiMT%3A%20Sparsifying%20Large%20Language%20Models%20for%20Chinese-Centric%0A%20%20Multilingual%20Machine%20Translation&entry.906535625=Shaolin%20Zhu%20and%20Tianyu%20Dong%20and%20Bo%20Li%20and%20Deyi%20Xiong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20FuxiMT%2C%20a%20novel%20Chinese-centric%20multilingual%0Amachine%20translation%20model%20powered%20by%20a%20sparsified%20large%20language%20model%20%28LLM%29.%0AWe%20adopt%20a%20two-stage%20strategy%20to%20train%20FuxiMT.%20We%20first%20pre-train%20the%20model%20on%0Aa%20massive%20Chinese%20corpus%20and%20then%20conduct%20multilingual%20fine-tuning%20on%20a%20large%0Aparallel%20dataset%20encompassing%2065%20languages.%20FuxiMT%20incorporates%0AMixture-of-Experts%20%28MoEs%29%20and%20employs%20a%20curriculum%20learning%20strategy%20for%20robust%0Aperformance%20across%20various%20resource%20levels.%20Experimental%20results%20demonstrate%0Athat%20FuxiMT%20significantly%20outperforms%20strong%20baselines%2C%20including%0Astate-of-the-art%20LLMs%20and%20machine%20translation%20models%2C%20particularly%20under%0Alow-resource%20scenarios.%20Furthermore%2C%20FuxiMT%20exhibits%20remarkable%20zero-shot%0Atranslation%20capabilities%20for%20unseen%20language%20pairs%2C%20indicating%20its%20potential%20to%0Abridge%20communication%20gaps%20where%20parallel%20data%20are%20scarce%20or%20unavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14256v1&entry.124074799=Read"},
{"title": "WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in\n  Wireless Communications", "author": "Xin Li and Mengbing Liu and Li Wei and Jiancheng An and M\u00e9rouane Debbah and Chau Yuen", "abstract": "  Large Language Models (LLMs) have achieved impressive results across a broad\narray of tasks, yet their capacity for complex, domain-specific mathematical\nreasoning-particularly in wireless communications-remains underexplored. In\nthis work, we introduce WirelessMathBench, a novel benchmark specifically\ndesigned to evaluate LLMs on mathematical modeling challenges to wireless\ncommunications engineering. Our benchmark consists of 587 meticulously curated\nquestions sourced from 40 state-of-the-art research papers, encompassing a\ndiverse spectrum of tasks ranging from basic multiple-choice questions to\ncomplex equation completion tasks, including both partial and full completions,\nall of which rigorously adhere to physical and dimensional constraints. Through\nextensive experimentation with leading LLMs, we observe that while many models\nexcel in basic recall tasks, their performance degrades significantly when\nreconstructing partially or fully obscured equations, exposing fundamental\nlimitations in current LLMs. Even DeepSeek-R1, the best performer on our\nbenchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%\nsuccess rate in full equation completion. By publicly releasing\nWirelessMathBench along with the evaluation toolkit, we aim to advance the\ndevelopment of more robust, domain-aware LLMs for wireless system analysis and\nbroader engineering applications.\n", "link": "http://arxiv.org/abs/2505.14354v1", "date": "2025-05-20", "relevancy": 2.2763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4649}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WirelessMathBench%3A%20A%20Mathematical%20Modeling%20Benchmark%20for%20LLMs%20in%0A%20%20Wireless%20Communications&body=Title%3A%20WirelessMathBench%3A%20A%20Mathematical%20Modeling%20Benchmark%20for%20LLMs%20in%0A%20%20Wireless%20Communications%0AAuthor%3A%20Xin%20Li%20and%20Mengbing%20Liu%20and%20Li%20Wei%20and%20Jiancheng%20An%20and%20M%C3%A9rouane%20Debbah%20and%20Chau%20Yuen%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%20results%20across%20a%20broad%0Aarray%20of%20tasks%2C%20yet%20their%20capacity%20for%20complex%2C%20domain-specific%20mathematical%0Areasoning-particularly%20in%20wireless%20communications-remains%20underexplored.%20In%0Athis%20work%2C%20we%20introduce%20WirelessMathBench%2C%20a%20novel%20benchmark%20specifically%0Adesigned%20to%20evaluate%20LLMs%20on%20mathematical%20modeling%20challenges%20to%20wireless%0Acommunications%20engineering.%20Our%20benchmark%20consists%20of%20587%20meticulously%20curated%0Aquestions%20sourced%20from%2040%20state-of-the-art%20research%20papers%2C%20encompassing%20a%0Adiverse%20spectrum%20of%20tasks%20ranging%20from%20basic%20multiple-choice%20questions%20to%0Acomplex%20equation%20completion%20tasks%2C%20including%20both%20partial%20and%20full%20completions%2C%0Aall%20of%20which%20rigorously%20adhere%20to%20physical%20and%20dimensional%20constraints.%20Through%0Aextensive%20experimentation%20with%20leading%20LLMs%2C%20we%20observe%20that%20while%20many%20models%0Aexcel%20in%20basic%20recall%20tasks%2C%20their%20performance%20degrades%20significantly%20when%0Areconstructing%20partially%20or%20fully%20obscured%20equations%2C%20exposing%20fundamental%0Alimitations%20in%20current%20LLMs.%20Even%20DeepSeek-R1%2C%20the%20best%20performer%20on%20our%0Abenchmark%2C%20achieves%20an%20average%20accuracy%20of%20only%2038.05%25%2C%20with%20a%20mere%207.83%25%0Asuccess%20rate%20in%20full%20equation%20completion.%20By%20publicly%20releasing%0AWirelessMathBench%20along%20with%20the%20evaluation%20toolkit%2C%20we%20aim%20to%20advance%20the%0Adevelopment%20of%20more%20robust%2C%20domain-aware%20LLMs%20for%20wireless%20system%20analysis%20and%0Abroader%20engineering%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWirelessMathBench%253A%2520A%2520Mathematical%2520Modeling%2520Benchmark%2520for%2520LLMs%2520in%250A%2520%2520Wireless%2520Communications%26entry.906535625%3DXin%2520Li%2520and%2520Mengbing%2520Liu%2520and%2520Li%2520Wei%2520and%2520Jiancheng%2520An%2520and%2520M%25C3%25A9rouane%2520Debbah%2520and%2520Chau%2520Yuen%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520results%2520across%2520a%2520broad%250Aarray%2520of%2520tasks%252C%2520yet%2520their%2520capacity%2520for%2520complex%252C%2520domain-specific%2520mathematical%250Areasoning-particularly%2520in%2520wireless%2520communications-remains%2520underexplored.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520WirelessMathBench%252C%2520a%2520novel%2520benchmark%2520specifically%250Adesigned%2520to%2520evaluate%2520LLMs%2520on%2520mathematical%2520modeling%2520challenges%2520to%2520wireless%250Acommunications%2520engineering.%2520Our%2520benchmark%2520consists%2520of%2520587%2520meticulously%2520curated%250Aquestions%2520sourced%2520from%252040%2520state-of-the-art%2520research%2520papers%252C%2520encompassing%2520a%250Adiverse%2520spectrum%2520of%2520tasks%2520ranging%2520from%2520basic%2520multiple-choice%2520questions%2520to%250Acomplex%2520equation%2520completion%2520tasks%252C%2520including%2520both%2520partial%2520and%2520full%2520completions%252C%250Aall%2520of%2520which%2520rigorously%2520adhere%2520to%2520physical%2520and%2520dimensional%2520constraints.%2520Through%250Aextensive%2520experimentation%2520with%2520leading%2520LLMs%252C%2520we%2520observe%2520that%2520while%2520many%2520models%250Aexcel%2520in%2520basic%2520recall%2520tasks%252C%2520their%2520performance%2520degrades%2520significantly%2520when%250Areconstructing%2520partially%2520or%2520fully%2520obscured%2520equations%252C%2520exposing%2520fundamental%250Alimitations%2520in%2520current%2520LLMs.%2520Even%2520DeepSeek-R1%252C%2520the%2520best%2520performer%2520on%2520our%250Abenchmark%252C%2520achieves%2520an%2520average%2520accuracy%2520of%2520only%252038.05%2525%252C%2520with%2520a%2520mere%25207.83%2525%250Asuccess%2520rate%2520in%2520full%2520equation%2520completion.%2520By%2520publicly%2520releasing%250AWirelessMathBench%2520along%2520with%2520the%2520evaluation%2520toolkit%252C%2520we%2520aim%2520to%2520advance%2520the%250Adevelopment%2520of%2520more%2520robust%252C%2520domain-aware%2520LLMs%2520for%2520wireless%2520system%2520analysis%2520and%250Abroader%2520engineering%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WirelessMathBench%3A%20A%20Mathematical%20Modeling%20Benchmark%20for%20LLMs%20in%0A%20%20Wireless%20Communications&entry.906535625=Xin%20Li%20and%20Mengbing%20Liu%20and%20Li%20Wei%20and%20Jiancheng%20An%20and%20M%C3%A9rouane%20Debbah%20and%20Chau%20Yuen&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%20results%20across%20a%20broad%0Aarray%20of%20tasks%2C%20yet%20their%20capacity%20for%20complex%2C%20domain-specific%20mathematical%0Areasoning-particularly%20in%20wireless%20communications-remains%20underexplored.%20In%0Athis%20work%2C%20we%20introduce%20WirelessMathBench%2C%20a%20novel%20benchmark%20specifically%0Adesigned%20to%20evaluate%20LLMs%20on%20mathematical%20modeling%20challenges%20to%20wireless%0Acommunications%20engineering.%20Our%20benchmark%20consists%20of%20587%20meticulously%20curated%0Aquestions%20sourced%20from%2040%20state-of-the-art%20research%20papers%2C%20encompassing%20a%0Adiverse%20spectrum%20of%20tasks%20ranging%20from%20basic%20multiple-choice%20questions%20to%0Acomplex%20equation%20completion%20tasks%2C%20including%20both%20partial%20and%20full%20completions%2C%0Aall%20of%20which%20rigorously%20adhere%20to%20physical%20and%20dimensional%20constraints.%20Through%0Aextensive%20experimentation%20with%20leading%20LLMs%2C%20we%20observe%20that%20while%20many%20models%0Aexcel%20in%20basic%20recall%20tasks%2C%20their%20performance%20degrades%20significantly%20when%0Areconstructing%20partially%20or%20fully%20obscured%20equations%2C%20exposing%20fundamental%0Alimitations%20in%20current%20LLMs.%20Even%20DeepSeek-R1%2C%20the%20best%20performer%20on%20our%0Abenchmark%2C%20achieves%20an%20average%20accuracy%20of%20only%2038.05%25%2C%20with%20a%20mere%207.83%25%0Asuccess%20rate%20in%20full%20equation%20completion.%20By%20publicly%20releasing%0AWirelessMathBench%20along%20with%20the%20evaluation%20toolkit%2C%20we%20aim%20to%20advance%20the%0Adevelopment%20of%20more%20robust%2C%20domain-aware%20LLMs%20for%20wireless%20system%20analysis%20and%0Abroader%20engineering%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14354v1&entry.124074799=Read"},
{"title": "Emerging Properties in Unified Multimodal Pretraining", "author": "Chaorui Deng and Deyao Zhu and Kunchang Li and Chenhui Gou and Feng Li and Zeyu Wang and Shu Zhong and Weihao Yu and Xiaonan Nie and Ziang Song and Guang Shi and Haoqi Fan", "abstract": "  Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/\n", "link": "http://arxiv.org/abs/2505.14683v1", "date": "2025-05-20", "relevancy": 2.2755, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5802}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emerging%20Properties%20in%20Unified%20Multimodal%20Pretraining&body=Title%3A%20Emerging%20Properties%20in%20Unified%20Multimodal%20Pretraining%0AAuthor%3A%20Chaorui%20Deng%20and%20Deyao%20Zhu%20and%20Kunchang%20Li%20and%20Chenhui%20Gou%20and%20Feng%20Li%20and%20Zeyu%20Wang%20and%20Shu%20Zhong%20and%20Weihao%20Yu%20and%20Xiaonan%20Nie%20and%20Ziang%20Song%20and%20Guang%20Shi%20and%20Haoqi%20Fan%0AAbstract%3A%20%20%20Unifying%20multimodal%20understanding%20and%20generation%20has%20shown%20impressive%0Acapabilities%20in%20cutting-edge%20proprietary%20systems.%20In%20this%20work%2C%20we%20introduce%0ABAGEL%2C%20an%20open0source%20foundational%20model%20that%20natively%20supports%20multimodal%0Aunderstanding%20and%20generation.%20BAGEL%20is%20a%20unified%2C%20decoder0only%20model%20pretrained%0Aon%20trillions%20of%20tokens%20curated%20from%20large0scale%20interleaved%20text%2C%20image%2C%20video%2C%0Aand%20web%20data.%20When%20scaled%20with%20such%20diverse%20multimodal%20interleaved%20data%2C%20BAGEL%0Aexhibits%20emerging%20capabilities%20in%20complex%20multimodal%20reasoning.%20As%20a%20result%2C%20it%0Asignificantly%20outperforms%20open-source%20unified%20models%20in%20both%20multimodal%0Ageneration%20and%20understanding%20across%20standard%20benchmarks%2C%20while%20exhibiting%0Aadvanced%20multimodal%20reasoning%20abilities%20such%20as%20free-form%20image%20manipulation%2C%0Afuture%20frame%20prediction%2C%203D%20manipulation%2C%20and%20world%20navigation.%20In%20the%20hope%20of%0Afacilitating%20further%20opportunities%20for%20multimodal%20research%2C%20we%20share%20the%20key%0Afindings%2C%20pretraining%20details%2C%20data%20creation%20protocal%2C%20and%20release%20our%20code%20and%0Acheckpoints%20to%20the%20community.%20The%20project%20page%20is%20at%20https%3A//bagel-ai.org/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmerging%2520Properties%2520in%2520Unified%2520Multimodal%2520Pretraining%26entry.906535625%3DChaorui%2520Deng%2520and%2520Deyao%2520Zhu%2520and%2520Kunchang%2520Li%2520and%2520Chenhui%2520Gou%2520and%2520Feng%2520Li%2520and%2520Zeyu%2520Wang%2520and%2520Shu%2520Zhong%2520and%2520Weihao%2520Yu%2520and%2520Xiaonan%2520Nie%2520and%2520Ziang%2520Song%2520and%2520Guang%2520Shi%2520and%2520Haoqi%2520Fan%26entry.1292438233%3D%2520%2520Unifying%2520multimodal%2520understanding%2520and%2520generation%2520has%2520shown%2520impressive%250Acapabilities%2520in%2520cutting-edge%2520proprietary%2520systems.%2520In%2520this%2520work%252C%2520we%2520introduce%250ABAGEL%252C%2520an%2520open0source%2520foundational%2520model%2520that%2520natively%2520supports%2520multimodal%250Aunderstanding%2520and%2520generation.%2520BAGEL%2520is%2520a%2520unified%252C%2520decoder0only%2520model%2520pretrained%250Aon%2520trillions%2520of%2520tokens%2520curated%2520from%2520large0scale%2520interleaved%2520text%252C%2520image%252C%2520video%252C%250Aand%2520web%2520data.%2520When%2520scaled%2520with%2520such%2520diverse%2520multimodal%2520interleaved%2520data%252C%2520BAGEL%250Aexhibits%2520emerging%2520capabilities%2520in%2520complex%2520multimodal%2520reasoning.%2520As%2520a%2520result%252C%2520it%250Asignificantly%2520outperforms%2520open-source%2520unified%2520models%2520in%2520both%2520multimodal%250Ageneration%2520and%2520understanding%2520across%2520standard%2520benchmarks%252C%2520while%2520exhibiting%250Aadvanced%2520multimodal%2520reasoning%2520abilities%2520such%2520as%2520free-form%2520image%2520manipulation%252C%250Afuture%2520frame%2520prediction%252C%25203D%2520manipulation%252C%2520and%2520world%2520navigation.%2520In%2520the%2520hope%2520of%250Afacilitating%2520further%2520opportunities%2520for%2520multimodal%2520research%252C%2520we%2520share%2520the%2520key%250Afindings%252C%2520pretraining%2520details%252C%2520data%2520creation%2520protocal%252C%2520and%2520release%2520our%2520code%2520and%250Acheckpoints%2520to%2520the%2520community.%2520The%2520project%2520page%2520is%2520at%2520https%253A//bagel-ai.org/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emerging%20Properties%20in%20Unified%20Multimodal%20Pretraining&entry.906535625=Chaorui%20Deng%20and%20Deyao%20Zhu%20and%20Kunchang%20Li%20and%20Chenhui%20Gou%20and%20Feng%20Li%20and%20Zeyu%20Wang%20and%20Shu%20Zhong%20and%20Weihao%20Yu%20and%20Xiaonan%20Nie%20and%20Ziang%20Song%20and%20Guang%20Shi%20and%20Haoqi%20Fan&entry.1292438233=%20%20Unifying%20multimodal%20understanding%20and%20generation%20has%20shown%20impressive%0Acapabilities%20in%20cutting-edge%20proprietary%20systems.%20In%20this%20work%2C%20we%20introduce%0ABAGEL%2C%20an%20open0source%20foundational%20model%20that%20natively%20supports%20multimodal%0Aunderstanding%20and%20generation.%20BAGEL%20is%20a%20unified%2C%20decoder0only%20model%20pretrained%0Aon%20trillions%20of%20tokens%20curated%20from%20large0scale%20interleaved%20text%2C%20image%2C%20video%2C%0Aand%20web%20data.%20When%20scaled%20with%20such%20diverse%20multimodal%20interleaved%20data%2C%20BAGEL%0Aexhibits%20emerging%20capabilities%20in%20complex%20multimodal%20reasoning.%20As%20a%20result%2C%20it%0Asignificantly%20outperforms%20open-source%20unified%20models%20in%20both%20multimodal%0Ageneration%20and%20understanding%20across%20standard%20benchmarks%2C%20while%20exhibiting%0Aadvanced%20multimodal%20reasoning%20abilities%20such%20as%20free-form%20image%20manipulation%2C%0Afuture%20frame%20prediction%2C%203D%20manipulation%2C%20and%20world%20navigation.%20In%20the%20hope%20of%0Afacilitating%20further%20opportunities%20for%20multimodal%20research%2C%20we%20share%20the%20key%0Afindings%2C%20pretraining%20details%2C%20data%20creation%20protocal%2C%20and%20release%20our%20code%20and%0Acheckpoints%20to%20the%20community.%20The%20project%20page%20is%20at%20https%3A//bagel-ai.org/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14683v1&entry.124074799=Read"},
{"title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs", "author": "Himanshu Beniwal and Sailesh Panda and Birudugadda Srivibhav and Mayank Singh", "abstract": "  We explore \\textbf{C}ross-lingual \\textbf{B}ackdoor \\textbf{AT}tacks (X-BAT)\nin multilingual Large Language Models (mLLMs), revealing how backdoors inserted\nin one language can automatically transfer to others through shared embedding\nspaces. Using toxicity classification as a case study, we demonstrate that\nattackers can compromise multilingual systems by poisoning data in a single\nlanguage, with rare and high-occurring tokens serving as specific, effective\ntriggers. Our findings expose a critical vulnerability that influences the\nmodel's architecture, resulting in a concealed backdoor effect during the\ninformation flow. Our code and data are publicly available\nhttps://github.com/himanshubeniwal/X-BAT.\n", "link": "http://arxiv.org/abs/2502.16901v2", "date": "2025-05-20", "relevancy": 2.2653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4589}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Char-mander%20Use%20mBackdoor%21%20A%20Study%20of%20Cross-lingual%20Backdoor%20Attacks%20in%0A%20%20Multilingual%20LLMs&body=Title%3A%20Char-mander%20Use%20mBackdoor%21%20A%20Study%20of%20Cross-lingual%20Backdoor%20Attacks%20in%0A%20%20Multilingual%20LLMs%0AAuthor%3A%20Himanshu%20Beniwal%20and%20Sailesh%20Panda%20and%20Birudugadda%20Srivibhav%20and%20Mayank%20Singh%0AAbstract%3A%20%20%20We%20explore%20%5Ctextbf%7BC%7Dross-lingual%20%5Ctextbf%7BB%7Dackdoor%20%5Ctextbf%7BAT%7Dtacks%20%28X-BAT%29%0Ain%20multilingual%20Large%20Language%20Models%20%28mLLMs%29%2C%20revealing%20how%20backdoors%20inserted%0Ain%20one%20language%20can%20automatically%20transfer%20to%20others%20through%20shared%20embedding%0Aspaces.%20Using%20toxicity%20classification%20as%20a%20case%20study%2C%20we%20demonstrate%20that%0Aattackers%20can%20compromise%20multilingual%20systems%20by%20poisoning%20data%20in%20a%20single%0Alanguage%2C%20with%20rare%20and%20high-occurring%20tokens%20serving%20as%20specific%2C%20effective%0Atriggers.%20Our%20findings%20expose%20a%20critical%20vulnerability%20that%20influences%20the%0Amodel%27s%20architecture%2C%20resulting%20in%20a%20concealed%20backdoor%20effect%20during%20the%0Ainformation%20flow.%20Our%20code%20and%20data%20are%20publicly%20available%0Ahttps%3A//github.com/himanshubeniwal/X-BAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16901v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChar-mander%2520Use%2520mBackdoor%2521%2520A%2520Study%2520of%2520Cross-lingual%2520Backdoor%2520Attacks%2520in%250A%2520%2520Multilingual%2520LLMs%26entry.906535625%3DHimanshu%2520Beniwal%2520and%2520Sailesh%2520Panda%2520and%2520Birudugadda%2520Srivibhav%2520and%2520Mayank%2520Singh%26entry.1292438233%3D%2520%2520We%2520explore%2520%255Ctextbf%257BC%257Dross-lingual%2520%255Ctextbf%257BB%257Dackdoor%2520%255Ctextbf%257BAT%257Dtacks%2520%2528X-BAT%2529%250Ain%2520multilingual%2520Large%2520Language%2520Models%2520%2528mLLMs%2529%252C%2520revealing%2520how%2520backdoors%2520inserted%250Ain%2520one%2520language%2520can%2520automatically%2520transfer%2520to%2520others%2520through%2520shared%2520embedding%250Aspaces.%2520Using%2520toxicity%2520classification%2520as%2520a%2520case%2520study%252C%2520we%2520demonstrate%2520that%250Aattackers%2520can%2520compromise%2520multilingual%2520systems%2520by%2520poisoning%2520data%2520in%2520a%2520single%250Alanguage%252C%2520with%2520rare%2520and%2520high-occurring%2520tokens%2520serving%2520as%2520specific%252C%2520effective%250Atriggers.%2520Our%2520findings%2520expose%2520a%2520critical%2520vulnerability%2520that%2520influences%2520the%250Amodel%2527s%2520architecture%252C%2520resulting%2520in%2520a%2520concealed%2520backdoor%2520effect%2520during%2520the%250Ainformation%2520flow.%2520Our%2520code%2520and%2520data%2520are%2520publicly%2520available%250Ahttps%253A//github.com/himanshubeniwal/X-BAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16901v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Char-mander%20Use%20mBackdoor%21%20A%20Study%20of%20Cross-lingual%20Backdoor%20Attacks%20in%0A%20%20Multilingual%20LLMs&entry.906535625=Himanshu%20Beniwal%20and%20Sailesh%20Panda%20and%20Birudugadda%20Srivibhav%20and%20Mayank%20Singh&entry.1292438233=%20%20We%20explore%20%5Ctextbf%7BC%7Dross-lingual%20%5Ctextbf%7BB%7Dackdoor%20%5Ctextbf%7BAT%7Dtacks%20%28X-BAT%29%0Ain%20multilingual%20Large%20Language%20Models%20%28mLLMs%29%2C%20revealing%20how%20backdoors%20inserted%0Ain%20one%20language%20can%20automatically%20transfer%20to%20others%20through%20shared%20embedding%0Aspaces.%20Using%20toxicity%20classification%20as%20a%20case%20study%2C%20we%20demonstrate%20that%0Aattackers%20can%20compromise%20multilingual%20systems%20by%20poisoning%20data%20in%20a%20single%0Alanguage%2C%20with%20rare%20and%20high-occurring%20tokens%20serving%20as%20specific%2C%20effective%0Atriggers.%20Our%20findings%20expose%20a%20critical%20vulnerability%20that%20influences%20the%0Amodel%27s%20architecture%2C%20resulting%20in%20a%20concealed%20backdoor%20effect%20during%20the%0Ainformation%20flow.%20Our%20code%20and%20data%20are%20publicly%20available%0Ahttps%3A//github.com/himanshubeniwal/X-BAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16901v2&entry.124074799=Read"},
{"title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture\n  Understanding", "author": "Jiaang Li and Yifei Yuan and Wenyan Li and Mohammad Aliannejadi and Daniel Hershcovich and Anders S\u00f8gaard and Ivan Vuli\u0107 and Wenxuan Zhang and Paul Pu Liang and Yang Deng and Serge Belongie", "abstract": "  As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding.\n", "link": "http://arxiv.org/abs/2505.14462v1", "date": "2025-05-20", "relevancy": 2.261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAVENEA%3A%20A%20Benchmark%20for%20Multimodal%20Retrieval-Augmented%20Visual%20Culture%0A%20%20Understanding&body=Title%3A%20RAVENEA%3A%20A%20Benchmark%20for%20Multimodal%20Retrieval-Augmented%20Visual%20Culture%0A%20%20Understanding%0AAuthor%3A%20Jiaang%20Li%20and%20Yifei%20Yuan%20and%20Wenyan%20Li%20and%20Mohammad%20Aliannejadi%20and%20Daniel%20Hershcovich%20and%20Anders%20S%C3%B8gaard%20and%20Ivan%20Vuli%C4%87%20and%20Wenxuan%20Zhang%20and%20Paul%20Pu%20Liang%20and%20Yang%20Deng%20and%20Serge%20Belongie%0AAbstract%3A%20%20%20As%20vision-language%20models%20%28VLMs%29%20become%20increasingly%20integrated%20into%20daily%0Alife%2C%20the%20need%20for%20accurate%20visual%20culture%20understanding%20is%20becoming%20critical.%0AYet%2C%20these%20models%20frequently%20fall%20short%20in%20interpreting%20cultural%20nuances%0Aeffectively.%20Prior%20work%20has%20demonstrated%20the%20effectiveness%20of%0Aretrieval-augmented%20generation%20%28RAG%29%20in%20enhancing%20cultural%20understanding%20in%0Atext-only%20settings%2C%20while%20its%20application%20in%20multimodal%20scenarios%20remains%0Aunderexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RAVENEA%20%28Retrieval-Augmented%0AVisual%20culturE%20uNdErstAnding%29%2C%20a%20new%20benchmark%20designed%20to%20advance%20visual%0Aculture%20understanding%20through%20retrieval%2C%20focusing%20on%20two%20tasks%3A%20culture-focused%0Avisual%20question%20answering%20%28cVQA%29%20and%20culture-informed%20image%20captioning%20%28cIC%29.%0ARAVENEA%20extends%20existing%20datasets%20by%20integrating%20over%2010%2C000%20Wikipedia%0Adocuments%20curated%20and%20ranked%20by%20human%20annotators.%20With%20RAVENEA%2C%20we%20train%20and%0Aevaluate%20seven%20multimodal%20retrievers%20for%20each%20image%20query%2C%20and%20measure%20the%0Adownstream%20impact%20of%20retrieval-augmented%20inputs%20across%20fourteen%0Astate-of-the-art%20VLMs.%20Our%20results%20show%20that%20lightweight%20VLMs%2C%20when%20augmented%0Awith%20culture-aware%20retrieval%2C%20outperform%20their%20non-augmented%20counterparts%20%28by%0Aat%20least%203.2%25%20absolute%20on%20cVQA%20and%206.2%25%20absolute%20on%20cIC%29.%20This%20highlights%20the%0Avalue%20of%20retrieval-augmented%20methods%20and%20culturally%20inclusive%20benchmarks%20for%0Amultimodal%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAVENEA%253A%2520A%2520Benchmark%2520for%2520Multimodal%2520Retrieval-Augmented%2520Visual%2520Culture%250A%2520%2520Understanding%26entry.906535625%3DJiaang%2520Li%2520and%2520Yifei%2520Yuan%2520and%2520Wenyan%2520Li%2520and%2520Mohammad%2520Aliannejadi%2520and%2520Daniel%2520Hershcovich%2520and%2520Anders%2520S%25C3%25B8gaard%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Wenxuan%2520Zhang%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Yang%2520Deng%2520and%2520Serge%2520Belongie%26entry.1292438233%3D%2520%2520As%2520vision-language%2520models%2520%2528VLMs%2529%2520become%2520increasingly%2520integrated%2520into%2520daily%250Alife%252C%2520the%2520need%2520for%2520accurate%2520visual%2520culture%2520understanding%2520is%2520becoming%2520critical.%250AYet%252C%2520these%2520models%2520frequently%2520fall%2520short%2520in%2520interpreting%2520cultural%2520nuances%250Aeffectively.%2520Prior%2520work%2520has%2520demonstrated%2520the%2520effectiveness%2520of%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%2520in%2520enhancing%2520cultural%2520understanding%2520in%250Atext-only%2520settings%252C%2520while%2520its%2520application%2520in%2520multimodal%2520scenarios%2520remains%250Aunderexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520RAVENEA%2520%2528Retrieval-Augmented%250AVisual%2520culturE%2520uNdErstAnding%2529%252C%2520a%2520new%2520benchmark%2520designed%2520to%2520advance%2520visual%250Aculture%2520understanding%2520through%2520retrieval%252C%2520focusing%2520on%2520two%2520tasks%253A%2520culture-focused%250Avisual%2520question%2520answering%2520%2528cVQA%2529%2520and%2520culture-informed%2520image%2520captioning%2520%2528cIC%2529.%250ARAVENEA%2520extends%2520existing%2520datasets%2520by%2520integrating%2520over%252010%252C000%2520Wikipedia%250Adocuments%2520curated%2520and%2520ranked%2520by%2520human%2520annotators.%2520With%2520RAVENEA%252C%2520we%2520train%2520and%250Aevaluate%2520seven%2520multimodal%2520retrievers%2520for%2520each%2520image%2520query%252C%2520and%2520measure%2520the%250Adownstream%2520impact%2520of%2520retrieval-augmented%2520inputs%2520across%2520fourteen%250Astate-of-the-art%2520VLMs.%2520Our%2520results%2520show%2520that%2520lightweight%2520VLMs%252C%2520when%2520augmented%250Awith%2520culture-aware%2520retrieval%252C%2520outperform%2520their%2520non-augmented%2520counterparts%2520%2528by%250Aat%2520least%25203.2%2525%2520absolute%2520on%2520cVQA%2520and%25206.2%2525%2520absolute%2520on%2520cIC%2529.%2520This%2520highlights%2520the%250Avalue%2520of%2520retrieval-augmented%2520methods%2520and%2520culturally%2520inclusive%2520benchmarks%2520for%250Amultimodal%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAVENEA%3A%20A%20Benchmark%20for%20Multimodal%20Retrieval-Augmented%20Visual%20Culture%0A%20%20Understanding&entry.906535625=Jiaang%20Li%20and%20Yifei%20Yuan%20and%20Wenyan%20Li%20and%20Mohammad%20Aliannejadi%20and%20Daniel%20Hershcovich%20and%20Anders%20S%C3%B8gaard%20and%20Ivan%20Vuli%C4%87%20and%20Wenxuan%20Zhang%20and%20Paul%20Pu%20Liang%20and%20Yang%20Deng%20and%20Serge%20Belongie&entry.1292438233=%20%20As%20vision-language%20models%20%28VLMs%29%20become%20increasingly%20integrated%20into%20daily%0Alife%2C%20the%20need%20for%20accurate%20visual%20culture%20understanding%20is%20becoming%20critical.%0AYet%2C%20these%20models%20frequently%20fall%20short%20in%20interpreting%20cultural%20nuances%0Aeffectively.%20Prior%20work%20has%20demonstrated%20the%20effectiveness%20of%0Aretrieval-augmented%20generation%20%28RAG%29%20in%20enhancing%20cultural%20understanding%20in%0Atext-only%20settings%2C%20while%20its%20application%20in%20multimodal%20scenarios%20remains%0Aunderexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RAVENEA%20%28Retrieval-Augmented%0AVisual%20culturE%20uNdErstAnding%29%2C%20a%20new%20benchmark%20designed%20to%20advance%20visual%0Aculture%20understanding%20through%20retrieval%2C%20focusing%20on%20two%20tasks%3A%20culture-focused%0Avisual%20question%20answering%20%28cVQA%29%20and%20culture-informed%20image%20captioning%20%28cIC%29.%0ARAVENEA%20extends%20existing%20datasets%20by%20integrating%20over%2010%2C000%20Wikipedia%0Adocuments%20curated%20and%20ranked%20by%20human%20annotators.%20With%20RAVENEA%2C%20we%20train%20and%0Aevaluate%20seven%20multimodal%20retrievers%20for%20each%20image%20query%2C%20and%20measure%20the%0Adownstream%20impact%20of%20retrieval-augmented%20inputs%20across%20fourteen%0Astate-of-the-art%20VLMs.%20Our%20results%20show%20that%20lightweight%20VLMs%2C%20when%20augmented%0Awith%20culture-aware%20retrieval%2C%20outperform%20their%20non-augmented%20counterparts%20%28by%0Aat%20least%203.2%25%20absolute%20on%20cVQA%20and%206.2%25%20absolute%20on%20cIC%29.%20This%20highlights%20the%0Avalue%20of%20retrieval-augmented%20methods%20and%20culturally%20inclusive%20benchmarks%20for%0Amultimodal%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14462v1&entry.124074799=Read"},
{"title": "Cross-Image Contrastive Decoding: Precise, Lossless Suppression of\n  Language Priors in Large Vision-Language Models", "author": "Jianfei Zhao and Feng Zhang and Xin Sun and Chong Feng", "abstract": "  Language priors are a major cause of hallucinations in Large Vision-Language\nModels (LVLMs), often leading to text that is linguistically plausible but\nvisually inconsistent. Recent work explores contrastive decoding as a\ntraining-free solution, but these methods typically construct negative contexts\nfrom the original image, resulting in visual information loss and distorted\ndistribution. Motivated by the observation that language priors stem from the\nLLM backbone and remain consistent across images, we propose Cross-Images\nContrastive Decoding (CICD), a simple yet effective training-free method that\nuses different images to construct negative contexts. We further analyze the\ncross-image behavior of language priors and introduce a distinction between\nessential priors (supporting fluency) and detrimental priors (causing\nhallucinations). By selectively preserving essential priors and suppressing\ndetrimental ones, our method reduces hallucinations while maintaining coherent\nand fluent language generation. Experiments on 4 benchmarks and 6 LVLMs across\nthree model families confirm the effectiveness and generalizability of CICD,\nespecially in image captioning, where language priors are particularly\npronounced. Code will be released once accepted.\n", "link": "http://arxiv.org/abs/2505.10634v3", "date": "2025-05-20", "relevancy": 2.2403, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Image%20Contrastive%20Decoding%3A%20Precise%2C%20Lossless%20Suppression%20of%0A%20%20Language%20Priors%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Cross-Image%20Contrastive%20Decoding%3A%20Precise%2C%20Lossless%20Suppression%20of%0A%20%20Language%20Priors%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Chong%20Feng%0AAbstract%3A%20%20%20Language%20priors%20are%20a%20major%20cause%20of%20hallucinations%20in%20Large%20Vision-Language%0AModels%20%28LVLMs%29%2C%20often%20leading%20to%20text%20that%20is%20linguistically%20plausible%20but%0Avisually%20inconsistent.%20Recent%20work%20explores%20contrastive%20decoding%20as%20a%0Atraining-free%20solution%2C%20but%20these%20methods%20typically%20construct%20negative%20contexts%0Afrom%20the%20original%20image%2C%20resulting%20in%20visual%20information%20loss%20and%20distorted%0Adistribution.%20Motivated%20by%20the%20observation%20that%20language%20priors%20stem%20from%20the%0ALLM%20backbone%20and%20remain%20consistent%20across%20images%2C%20we%20propose%20Cross-Images%0AContrastive%20Decoding%20%28CICD%29%2C%20a%20simple%20yet%20effective%20training-free%20method%20that%0Auses%20different%20images%20to%20construct%20negative%20contexts.%20We%20further%20analyze%20the%0Across-image%20behavior%20of%20language%20priors%20and%20introduce%20a%20distinction%20between%0Aessential%20priors%20%28supporting%20fluency%29%20and%20detrimental%20priors%20%28causing%0Ahallucinations%29.%20By%20selectively%20preserving%20essential%20priors%20and%20suppressing%0Adetrimental%20ones%2C%20our%20method%20reduces%20hallucinations%20while%20maintaining%20coherent%0Aand%20fluent%20language%20generation.%20Experiments%20on%204%20benchmarks%20and%206%20LVLMs%20across%0Athree%20model%20families%20confirm%20the%20effectiveness%20and%20generalizability%20of%20CICD%2C%0Aespecially%20in%20image%20captioning%2C%20where%20language%20priors%20are%20particularly%0Apronounced.%20Code%20will%20be%20released%20once%20accepted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Image%2520Contrastive%2520Decoding%253A%2520Precise%252C%2520Lossless%2520Suppression%2520of%250A%2520%2520Language%2520Priors%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJianfei%2520Zhao%2520and%2520Feng%2520Zhang%2520and%2520Xin%2520Sun%2520and%2520Chong%2520Feng%26entry.1292438233%3D%2520%2520Language%2520priors%2520are%2520a%2520major%2520cause%2520of%2520hallucinations%2520in%2520Large%2520Vision-Language%250AModels%2520%2528LVLMs%2529%252C%2520often%2520leading%2520to%2520text%2520that%2520is%2520linguistically%2520plausible%2520but%250Avisually%2520inconsistent.%2520Recent%2520work%2520explores%2520contrastive%2520decoding%2520as%2520a%250Atraining-free%2520solution%252C%2520but%2520these%2520methods%2520typically%2520construct%2520negative%2520contexts%250Afrom%2520the%2520original%2520image%252C%2520resulting%2520in%2520visual%2520information%2520loss%2520and%2520distorted%250Adistribution.%2520Motivated%2520by%2520the%2520observation%2520that%2520language%2520priors%2520stem%2520from%2520the%250ALLM%2520backbone%2520and%2520remain%2520consistent%2520across%2520images%252C%2520we%2520propose%2520Cross-Images%250AContrastive%2520Decoding%2520%2528CICD%2529%252C%2520a%2520simple%2520yet%2520effective%2520training-free%2520method%2520that%250Auses%2520different%2520images%2520to%2520construct%2520negative%2520contexts.%2520We%2520further%2520analyze%2520the%250Across-image%2520behavior%2520of%2520language%2520priors%2520and%2520introduce%2520a%2520distinction%2520between%250Aessential%2520priors%2520%2528supporting%2520fluency%2529%2520and%2520detrimental%2520priors%2520%2528causing%250Ahallucinations%2529.%2520By%2520selectively%2520preserving%2520essential%2520priors%2520and%2520suppressing%250Adetrimental%2520ones%252C%2520our%2520method%2520reduces%2520hallucinations%2520while%2520maintaining%2520coherent%250Aand%2520fluent%2520language%2520generation.%2520Experiments%2520on%25204%2520benchmarks%2520and%25206%2520LVLMs%2520across%250Athree%2520model%2520families%2520confirm%2520the%2520effectiveness%2520and%2520generalizability%2520of%2520CICD%252C%250Aespecially%2520in%2520image%2520captioning%252C%2520where%2520language%2520priors%2520are%2520particularly%250Apronounced.%2520Code%2520will%2520be%2520released%2520once%2520accepted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Image%20Contrastive%20Decoding%3A%20Precise%2C%20Lossless%20Suppression%20of%0A%20%20Language%20Priors%20in%20Large%20Vision-Language%20Models&entry.906535625=Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Chong%20Feng&entry.1292438233=%20%20Language%20priors%20are%20a%20major%20cause%20of%20hallucinations%20in%20Large%20Vision-Language%0AModels%20%28LVLMs%29%2C%20often%20leading%20to%20text%20that%20is%20linguistically%20plausible%20but%0Avisually%20inconsistent.%20Recent%20work%20explores%20contrastive%20decoding%20as%20a%0Atraining-free%20solution%2C%20but%20these%20methods%20typically%20construct%20negative%20contexts%0Afrom%20the%20original%20image%2C%20resulting%20in%20visual%20information%20loss%20and%20distorted%0Adistribution.%20Motivated%20by%20the%20observation%20that%20language%20priors%20stem%20from%20the%0ALLM%20backbone%20and%20remain%20consistent%20across%20images%2C%20we%20propose%20Cross-Images%0AContrastive%20Decoding%20%28CICD%29%2C%20a%20simple%20yet%20effective%20training-free%20method%20that%0Auses%20different%20images%20to%20construct%20negative%20contexts.%20We%20further%20analyze%20the%0Across-image%20behavior%20of%20language%20priors%20and%20introduce%20a%20distinction%20between%0Aessential%20priors%20%28supporting%20fluency%29%20and%20detrimental%20priors%20%28causing%0Ahallucinations%29.%20By%20selectively%20preserving%20essential%20priors%20and%20suppressing%0Adetrimental%20ones%2C%20our%20method%20reduces%20hallucinations%20while%20maintaining%20coherent%0Aand%20fluent%20language%20generation.%20Experiments%20on%204%20benchmarks%20and%206%20LVLMs%20across%0Athree%20model%20families%20confirm%20the%20effectiveness%20and%20generalizability%20of%20CICD%2C%0Aespecially%20in%20image%20captioning%2C%20where%20language%20priors%20are%20particularly%0Apronounced.%20Code%20will%20be%20released%20once%20accepted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10634v3&entry.124074799=Read"},
{"title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal\n  Pretraining", "author": "Qichen Sun and Zhengrui Guo and Rui Peng and Hao Chen and Jinzhuo Wang", "abstract": "  Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2505.12711v2", "date": "2025-05-20", "relevancy": 2.2291, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6217}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Any-to-Any%20Learning%20in%20Computational%20Pathology%20via%20Triplet%20Multimodal%0A%20%20Pretraining&body=Title%3A%20Any-to-Any%20Learning%20in%20Computational%20Pathology%20via%20Triplet%20Multimodal%0A%20%20Pretraining%0AAuthor%3A%20Qichen%20Sun%20and%20Zhengrui%20Guo%20and%20Rui%20Peng%20and%20Hao%20Chen%20and%20Jinzhuo%20Wang%0AAbstract%3A%20%20%20Recent%20advances%20in%20computational%20pathology%20and%20artificial%20intelligence%20have%0Asignificantly%20enhanced%20the%20utilization%20of%20gigapixel%20whole-slide%20images%20and%20and%0Aadditional%20modalities%20%28e.g.%2C%20genomics%29%20for%20pathological%20diagnosis.%20Although%0Adeep%20learning%20has%20demonstrated%20strong%20potential%20in%20pathology%2C%20several%20key%0Achallenges%20persist%3A%20%281%29%20fusing%20heterogeneous%20data%20types%20requires%20sophisticated%0Astrategies%20beyond%20simple%20concatenation%20due%20to%20high%20computational%20costs%3B%20%282%29%0Acommon%20scenarios%20of%20missing%20modalities%20necessitate%20flexible%20strategies%20that%0Aallow%20the%20model%20to%20learn%20robustly%20in%20the%20absence%20of%20certain%20modalities%3B%20%283%29%20the%0Adownstream%20tasks%20in%20CPath%20are%20diverse%2C%20ranging%20from%20unimodal%20to%20multimodal%2C%0Acnecessitating%20a%20unified%20model%20capable%20of%20handling%20all%20modalities.%20To%20address%0Athese%20challenges%2C%20we%20propose%20ALTER%2C%20an%20any-to-any%20tri-modal%20pretraining%0Aframework%20that%20integrates%20WSIs%2C%20genomics%2C%20and%20pathology%20reports.%20The%20term%20%22any%22%0Aemphasizes%20ALTER%27s%20modality-adaptive%20design%2C%20enabling%20flexible%20pretraining%20with%0Aany%20subset%20of%20modalities%2C%20and%20its%20capacity%20to%20learn%20robust%2C%20cross-modal%0Arepresentations%20beyond%20WSI-centric%20approaches.%20We%20evaluate%20ALTER%20across%0Aextensive%20clinical%20tasks%20including%20survival%20prediction%2C%20cancer%20subtyping%2C%20gene%0Amutation%20prediction%2C%20and%20report%20generation%2C%20achieving%20superior%20or%20comparable%0Aperformance%20to%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12711v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAny-to-Any%2520Learning%2520in%2520Computational%2520Pathology%2520via%2520Triplet%2520Multimodal%250A%2520%2520Pretraining%26entry.906535625%3DQichen%2520Sun%2520and%2520Zhengrui%2520Guo%2520and%2520Rui%2520Peng%2520and%2520Hao%2520Chen%2520and%2520Jinzhuo%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520computational%2520pathology%2520and%2520artificial%2520intelligence%2520have%250Asignificantly%2520enhanced%2520the%2520utilization%2520of%2520gigapixel%2520whole-slide%2520images%2520and%2520and%250Aadditional%2520modalities%2520%2528e.g.%252C%2520genomics%2529%2520for%2520pathological%2520diagnosis.%2520Although%250Adeep%2520learning%2520has%2520demonstrated%2520strong%2520potential%2520in%2520pathology%252C%2520several%2520key%250Achallenges%2520persist%253A%2520%25281%2529%2520fusing%2520heterogeneous%2520data%2520types%2520requires%2520sophisticated%250Astrategies%2520beyond%2520simple%2520concatenation%2520due%2520to%2520high%2520computational%2520costs%253B%2520%25282%2529%250Acommon%2520scenarios%2520of%2520missing%2520modalities%2520necessitate%2520flexible%2520strategies%2520that%250Aallow%2520the%2520model%2520to%2520learn%2520robustly%2520in%2520the%2520absence%2520of%2520certain%2520modalities%253B%2520%25283%2529%2520the%250Adownstream%2520tasks%2520in%2520CPath%2520are%2520diverse%252C%2520ranging%2520from%2520unimodal%2520to%2520multimodal%252C%250Acnecessitating%2520a%2520unified%2520model%2520capable%2520of%2520handling%2520all%2520modalities.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520ALTER%252C%2520an%2520any-to-any%2520tri-modal%2520pretraining%250Aframework%2520that%2520integrates%2520WSIs%252C%2520genomics%252C%2520and%2520pathology%2520reports.%2520The%2520term%2520%2522any%2522%250Aemphasizes%2520ALTER%2527s%2520modality-adaptive%2520design%252C%2520enabling%2520flexible%2520pretraining%2520with%250Aany%2520subset%2520of%2520modalities%252C%2520and%2520its%2520capacity%2520to%2520learn%2520robust%252C%2520cross-modal%250Arepresentations%2520beyond%2520WSI-centric%2520approaches.%2520We%2520evaluate%2520ALTER%2520across%250Aextensive%2520clinical%2520tasks%2520including%2520survival%2520prediction%252C%2520cancer%2520subtyping%252C%2520gene%250Amutation%2520prediction%252C%2520and%2520report%2520generation%252C%2520achieving%2520superior%2520or%2520comparable%250Aperformance%2520to%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12711v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Any-to-Any%20Learning%20in%20Computational%20Pathology%20via%20Triplet%20Multimodal%0A%20%20Pretraining&entry.906535625=Qichen%20Sun%20and%20Zhengrui%20Guo%20and%20Rui%20Peng%20and%20Hao%20Chen%20and%20Jinzhuo%20Wang&entry.1292438233=%20%20Recent%20advances%20in%20computational%20pathology%20and%20artificial%20intelligence%20have%0Asignificantly%20enhanced%20the%20utilization%20of%20gigapixel%20whole-slide%20images%20and%20and%0Aadditional%20modalities%20%28e.g.%2C%20genomics%29%20for%20pathological%20diagnosis.%20Although%0Adeep%20learning%20has%20demonstrated%20strong%20potential%20in%20pathology%2C%20several%20key%0Achallenges%20persist%3A%20%281%29%20fusing%20heterogeneous%20data%20types%20requires%20sophisticated%0Astrategies%20beyond%20simple%20concatenation%20due%20to%20high%20computational%20costs%3B%20%282%29%0Acommon%20scenarios%20of%20missing%20modalities%20necessitate%20flexible%20strategies%20that%0Aallow%20the%20model%20to%20learn%20robustly%20in%20the%20absence%20of%20certain%20modalities%3B%20%283%29%20the%0Adownstream%20tasks%20in%20CPath%20are%20diverse%2C%20ranging%20from%20unimodal%20to%20multimodal%2C%0Acnecessitating%20a%20unified%20model%20capable%20of%20handling%20all%20modalities.%20To%20address%0Athese%20challenges%2C%20we%20propose%20ALTER%2C%20an%20any-to-any%20tri-modal%20pretraining%0Aframework%20that%20integrates%20WSIs%2C%20genomics%2C%20and%20pathology%20reports.%20The%20term%20%22any%22%0Aemphasizes%20ALTER%27s%20modality-adaptive%20design%2C%20enabling%20flexible%20pretraining%20with%0Aany%20subset%20of%20modalities%2C%20and%20its%20capacity%20to%20learn%20robust%2C%20cross-modal%0Arepresentations%20beyond%20WSI-centric%20approaches.%20We%20evaluate%20ALTER%20across%0Aextensive%20clinical%20tasks%20including%20survival%20prediction%2C%20cancer%20subtyping%2C%20gene%0Amutation%20prediction%2C%20and%20report%20generation%2C%20achieving%20superior%20or%20comparable%0Aperformance%20to%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12711v2&entry.124074799=Read"},
{"title": "Exploring Social Media Image Categorization Using Large Models with\n  Different Adaptation Methods: A Case Study on Cultural Nature's Contributions\n  to People", "author": "Rohaifa Khaldi and Domingo Alcaraz-Segura and Ignacio S\u00e1nchez-Herrera and Javier Martinez-Lopez and Carlos Javier Navarro and Siham Tabik", "abstract": "  Social media images provide valuable insights for modeling, mapping, and\nunderstanding human interactions with natural and cultural heritage. However,\ncategorizing these images into semantically meaningful groups remains highly\ncomplex due to the vast diversity and heterogeneity of their visual content as\nthey contain an open-world human and nature elements. This challenge becomes\ngreater when categories involve abstract concepts and lack consistent visual\npatterns. Related studies involve human supervision in the categorization\nprocess and the lack of public benchmark datasets make comparisons between\nthese works unfeasible. On the other hand, the continuous advances in large\nmodels, including Large Language Models (LLMs), Large Visual Models (LVMs), and\nLarge Visual Language Models (LVLMs), provide a large space of unexplored\nsolutions. In this work 1) we introduce FLIPS a dataset of Flickr images that\ncapture the interaction between human and nature, and 2) evaluate various\nsolutions based on different types and combinations of large models using\nvarious adaptation methods. We assess and report their performance in terms of\ncost, productivity, scalability, and result quality to address the challenges\nof social media image categorization.\n", "link": "http://arxiv.org/abs/2410.00275v3", "date": "2025-05-20", "relevancy": 2.2274, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.575}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5589}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Social%20Media%20Image%20Categorization%20Using%20Large%20Models%20with%0A%20%20Different%20Adaptation%20Methods%3A%20A%20Case%20Study%20on%20Cultural%20Nature%27s%20Contributions%0A%20%20to%20People&body=Title%3A%20Exploring%20Social%20Media%20Image%20Categorization%20Using%20Large%20Models%20with%0A%20%20Different%20Adaptation%20Methods%3A%20A%20Case%20Study%20on%20Cultural%20Nature%27s%20Contributions%0A%20%20to%20People%0AAuthor%3A%20Rohaifa%20Khaldi%20and%20Domingo%20Alcaraz-Segura%20and%20Ignacio%20S%C3%A1nchez-Herrera%20and%20Javier%20Martinez-Lopez%20and%20Carlos%20Javier%20Navarro%20and%20Siham%20Tabik%0AAbstract%3A%20%20%20Social%20media%20images%20provide%20valuable%20insights%20for%20modeling%2C%20mapping%2C%20and%0Aunderstanding%20human%20interactions%20with%20natural%20and%20cultural%20heritage.%20However%2C%0Acategorizing%20these%20images%20into%20semantically%20meaningful%20groups%20remains%20highly%0Acomplex%20due%20to%20the%20vast%20diversity%20and%20heterogeneity%20of%20their%20visual%20content%20as%0Athey%20contain%20an%20open-world%20human%20and%20nature%20elements.%20This%20challenge%20becomes%0Agreater%20when%20categories%20involve%20abstract%20concepts%20and%20lack%20consistent%20visual%0Apatterns.%20Related%20studies%20involve%20human%20supervision%20in%20the%20categorization%0Aprocess%20and%20the%20lack%20of%20public%20benchmark%20datasets%20make%20comparisons%20between%0Athese%20works%20unfeasible.%20On%20the%20other%20hand%2C%20the%20continuous%20advances%20in%20large%0Amodels%2C%20including%20Large%20Language%20Models%20%28LLMs%29%2C%20Large%20Visual%20Models%20%28LVMs%29%2C%20and%0ALarge%20Visual%20Language%20Models%20%28LVLMs%29%2C%20provide%20a%20large%20space%20of%20unexplored%0Asolutions.%20In%20this%20work%201%29%20we%20introduce%20FLIPS%20a%20dataset%20of%20Flickr%20images%20that%0Acapture%20the%20interaction%20between%20human%20and%20nature%2C%20and%202%29%20evaluate%20various%0Asolutions%20based%20on%20different%20types%20and%20combinations%20of%20large%20models%20using%0Avarious%20adaptation%20methods.%20We%20assess%20and%20report%20their%20performance%20in%20terms%20of%0Acost%2C%20productivity%2C%20scalability%2C%20and%20result%20quality%20to%20address%20the%20challenges%0Aof%20social%20media%20image%20categorization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.00275v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Social%2520Media%2520Image%2520Categorization%2520Using%2520Large%2520Models%2520with%250A%2520%2520Different%2520Adaptation%2520Methods%253A%2520A%2520Case%2520Study%2520on%2520Cultural%2520Nature%2527s%2520Contributions%250A%2520%2520to%2520People%26entry.906535625%3DRohaifa%2520Khaldi%2520and%2520Domingo%2520Alcaraz-Segura%2520and%2520Ignacio%2520S%25C3%25A1nchez-Herrera%2520and%2520Javier%2520Martinez-Lopez%2520and%2520Carlos%2520Javier%2520Navarro%2520and%2520Siham%2520Tabik%26entry.1292438233%3D%2520%2520Social%2520media%2520images%2520provide%2520valuable%2520insights%2520for%2520modeling%252C%2520mapping%252C%2520and%250Aunderstanding%2520human%2520interactions%2520with%2520natural%2520and%2520cultural%2520heritage.%2520However%252C%250Acategorizing%2520these%2520images%2520into%2520semantically%2520meaningful%2520groups%2520remains%2520highly%250Acomplex%2520due%2520to%2520the%2520vast%2520diversity%2520and%2520heterogeneity%2520of%2520their%2520visual%2520content%2520as%250Athey%2520contain%2520an%2520open-world%2520human%2520and%2520nature%2520elements.%2520This%2520challenge%2520becomes%250Agreater%2520when%2520categories%2520involve%2520abstract%2520concepts%2520and%2520lack%2520consistent%2520visual%250Apatterns.%2520Related%2520studies%2520involve%2520human%2520supervision%2520in%2520the%2520categorization%250Aprocess%2520and%2520the%2520lack%2520of%2520public%2520benchmark%2520datasets%2520make%2520comparisons%2520between%250Athese%2520works%2520unfeasible.%2520On%2520the%2520other%2520hand%252C%2520the%2520continuous%2520advances%2520in%2520large%250Amodels%252C%2520including%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Large%2520Visual%2520Models%2520%2528LVMs%2529%252C%2520and%250ALarge%2520Visual%2520Language%2520Models%2520%2528LVLMs%2529%252C%2520provide%2520a%2520large%2520space%2520of%2520unexplored%250Asolutions.%2520In%2520this%2520work%25201%2529%2520we%2520introduce%2520FLIPS%2520a%2520dataset%2520of%2520Flickr%2520images%2520that%250Acapture%2520the%2520interaction%2520between%2520human%2520and%2520nature%252C%2520and%25202%2529%2520evaluate%2520various%250Asolutions%2520based%2520on%2520different%2520types%2520and%2520combinations%2520of%2520large%2520models%2520using%250Avarious%2520adaptation%2520methods.%2520We%2520assess%2520and%2520report%2520their%2520performance%2520in%2520terms%2520of%250Acost%252C%2520productivity%252C%2520scalability%252C%2520and%2520result%2520quality%2520to%2520address%2520the%2520challenges%250Aof%2520social%2520media%2520image%2520categorization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.00275v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Social%20Media%20Image%20Categorization%20Using%20Large%20Models%20with%0A%20%20Different%20Adaptation%20Methods%3A%20A%20Case%20Study%20on%20Cultural%20Nature%27s%20Contributions%0A%20%20to%20People&entry.906535625=Rohaifa%20Khaldi%20and%20Domingo%20Alcaraz-Segura%20and%20Ignacio%20S%C3%A1nchez-Herrera%20and%20Javier%20Martinez-Lopez%20and%20Carlos%20Javier%20Navarro%20and%20Siham%20Tabik&entry.1292438233=%20%20Social%20media%20images%20provide%20valuable%20insights%20for%20modeling%2C%20mapping%2C%20and%0Aunderstanding%20human%20interactions%20with%20natural%20and%20cultural%20heritage.%20However%2C%0Acategorizing%20these%20images%20into%20semantically%20meaningful%20groups%20remains%20highly%0Acomplex%20due%20to%20the%20vast%20diversity%20and%20heterogeneity%20of%20their%20visual%20content%20as%0Athey%20contain%20an%20open-world%20human%20and%20nature%20elements.%20This%20challenge%20becomes%0Agreater%20when%20categories%20involve%20abstract%20concepts%20and%20lack%20consistent%20visual%0Apatterns.%20Related%20studies%20involve%20human%20supervision%20in%20the%20categorization%0Aprocess%20and%20the%20lack%20of%20public%20benchmark%20datasets%20make%20comparisons%20between%0Athese%20works%20unfeasible.%20On%20the%20other%20hand%2C%20the%20continuous%20advances%20in%20large%0Amodels%2C%20including%20Large%20Language%20Models%20%28LLMs%29%2C%20Large%20Visual%20Models%20%28LVMs%29%2C%20and%0ALarge%20Visual%20Language%20Models%20%28LVLMs%29%2C%20provide%20a%20large%20space%20of%20unexplored%0Asolutions.%20In%20this%20work%201%29%20we%20introduce%20FLIPS%20a%20dataset%20of%20Flickr%20images%20that%0Acapture%20the%20interaction%20between%20human%20and%20nature%2C%20and%202%29%20evaluate%20various%0Asolutions%20based%20on%20different%20types%20and%20combinations%20of%20large%20models%20using%0Avarious%20adaptation%20methods.%20We%20assess%20and%20report%20their%20performance%20in%20terms%20of%0Acost%2C%20productivity%2C%20scalability%2C%20and%20result%20quality%20to%20address%20the%20challenges%0Aof%20social%20media%20image%20categorization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.00275v3&entry.124074799=Read"},
{"title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of\n  Cross-Modal Embeddings", "author": "Yilin Ye and Junchao Huang and Xingchen Zeng and Jiazhi Xia and Wei Zeng", "abstract": "  Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.\n", "link": "http://arxiv.org/abs/2505.14664v1", "date": "2025-05-20", "relevancy": 2.2254, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5809}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AKRMap%3A%20Adaptive%20Kernel%20Regression%20for%20Trustworthy%20Visualization%20of%0A%20%20Cross-Modal%20Embeddings&body=Title%3A%20AKRMap%3A%20Adaptive%20Kernel%20Regression%20for%20Trustworthy%20Visualization%20of%0A%20%20Cross-Modal%20Embeddings%0AAuthor%3A%20Yilin%20Ye%20and%20Junchao%20Huang%20and%20Xingchen%20Zeng%20and%20Jiazhi%20Xia%20and%20Wei%20Zeng%0AAbstract%3A%20%20%20Cross-modal%20embeddings%20form%20the%20foundation%20for%20multi-modal%20models.%20However%2C%0Avisualization%20methods%20for%20interpreting%20cross-modal%20embeddings%20have%20been%0Aprimarily%20confined%20to%20traditional%20dimensionality%20reduction%20%28DR%29%20techniques%20like%0APCA%20and%20t-SNE.%20These%20DR%20methods%20primarily%20focus%20on%20feature%20distributions%20within%0Aa%20single%20modality%2C%20whilst%20failing%20to%20incorporate%20metrics%20%28e.g.%2C%20CLIPScore%29%0Aacross%20multiple%20modalities.This%20paper%20introduces%20AKRMap%2C%20a%20new%20DR%20technique%0Adesigned%20to%20visualize%20cross-modal%20embeddings%20metric%20with%20enhanced%20accuracy%20by%0Alearning%20kernel%20regression%20of%20the%20metric%20landscape%20in%20the%20projection%20space.%0ASpecifically%2C%20AKRMap%20constructs%20a%20supervised%20projection%20network%20guided%20by%20a%0Apost-projection%20kernel%20regression%20loss%2C%20and%20employs%20adaptive%20generalized%0Akernels%20that%20can%20be%20jointly%20optimized%20with%20the%20projection.%20This%20approach%0Aenables%20AKRMap%20to%20efficiently%20generate%20visualizations%20that%20capture%20complex%0Ametric%20distributions%2C%20while%20also%20supporting%20interactive%20features%20such%20as%20zoom%0Aand%20overlay%20for%20deeper%20exploration.%20Quantitative%20experiments%20demonstrate%20that%0AAKRMap%20outperforms%20existing%20DR%20methods%20in%20generating%20more%20accurate%20and%0Atrustworthy%20visualizations.%20We%20further%20showcase%20the%20effectiveness%20of%20AKRMap%20in%0Avisualizing%20and%20comparing%20cross-modal%20embeddings%20for%20text-to-image%20models.%20Code%0Aand%20demo%20are%20available%20at%20https%3A//github.com/yilinye/AKRMap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAKRMap%253A%2520Adaptive%2520Kernel%2520Regression%2520for%2520Trustworthy%2520Visualization%2520of%250A%2520%2520Cross-Modal%2520Embeddings%26entry.906535625%3DYilin%2520Ye%2520and%2520Junchao%2520Huang%2520and%2520Xingchen%2520Zeng%2520and%2520Jiazhi%2520Xia%2520and%2520Wei%2520Zeng%26entry.1292438233%3D%2520%2520Cross-modal%2520embeddings%2520form%2520the%2520foundation%2520for%2520multi-modal%2520models.%2520However%252C%250Avisualization%2520methods%2520for%2520interpreting%2520cross-modal%2520embeddings%2520have%2520been%250Aprimarily%2520confined%2520to%2520traditional%2520dimensionality%2520reduction%2520%2528DR%2529%2520techniques%2520like%250APCA%2520and%2520t-SNE.%2520These%2520DR%2520methods%2520primarily%2520focus%2520on%2520feature%2520distributions%2520within%250Aa%2520single%2520modality%252C%2520whilst%2520failing%2520to%2520incorporate%2520metrics%2520%2528e.g.%252C%2520CLIPScore%2529%250Aacross%2520multiple%2520modalities.This%2520paper%2520introduces%2520AKRMap%252C%2520a%2520new%2520DR%2520technique%250Adesigned%2520to%2520visualize%2520cross-modal%2520embeddings%2520metric%2520with%2520enhanced%2520accuracy%2520by%250Alearning%2520kernel%2520regression%2520of%2520the%2520metric%2520landscape%2520in%2520the%2520projection%2520space.%250ASpecifically%252C%2520AKRMap%2520constructs%2520a%2520supervised%2520projection%2520network%2520guided%2520by%2520a%250Apost-projection%2520kernel%2520regression%2520loss%252C%2520and%2520employs%2520adaptive%2520generalized%250Akernels%2520that%2520can%2520be%2520jointly%2520optimized%2520with%2520the%2520projection.%2520This%2520approach%250Aenables%2520AKRMap%2520to%2520efficiently%2520generate%2520visualizations%2520that%2520capture%2520complex%250Ametric%2520distributions%252C%2520while%2520also%2520supporting%2520interactive%2520features%2520such%2520as%2520zoom%250Aand%2520overlay%2520for%2520deeper%2520exploration.%2520Quantitative%2520experiments%2520demonstrate%2520that%250AAKRMap%2520outperforms%2520existing%2520DR%2520methods%2520in%2520generating%2520more%2520accurate%2520and%250Atrustworthy%2520visualizations.%2520We%2520further%2520showcase%2520the%2520effectiveness%2520of%2520AKRMap%2520in%250Avisualizing%2520and%2520comparing%2520cross-modal%2520embeddings%2520for%2520text-to-image%2520models.%2520Code%250Aand%2520demo%2520are%2520available%2520at%2520https%253A//github.com/yilinye/AKRMap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AKRMap%3A%20Adaptive%20Kernel%20Regression%20for%20Trustworthy%20Visualization%20of%0A%20%20Cross-Modal%20Embeddings&entry.906535625=Yilin%20Ye%20and%20Junchao%20Huang%20and%20Xingchen%20Zeng%20and%20Jiazhi%20Xia%20and%20Wei%20Zeng&entry.1292438233=%20%20Cross-modal%20embeddings%20form%20the%20foundation%20for%20multi-modal%20models.%20However%2C%0Avisualization%20methods%20for%20interpreting%20cross-modal%20embeddings%20have%20been%0Aprimarily%20confined%20to%20traditional%20dimensionality%20reduction%20%28DR%29%20techniques%20like%0APCA%20and%20t-SNE.%20These%20DR%20methods%20primarily%20focus%20on%20feature%20distributions%20within%0Aa%20single%20modality%2C%20whilst%20failing%20to%20incorporate%20metrics%20%28e.g.%2C%20CLIPScore%29%0Aacross%20multiple%20modalities.This%20paper%20introduces%20AKRMap%2C%20a%20new%20DR%20technique%0Adesigned%20to%20visualize%20cross-modal%20embeddings%20metric%20with%20enhanced%20accuracy%20by%0Alearning%20kernel%20regression%20of%20the%20metric%20landscape%20in%20the%20projection%20space.%0ASpecifically%2C%20AKRMap%20constructs%20a%20supervised%20projection%20network%20guided%20by%20a%0Apost-projection%20kernel%20regression%20loss%2C%20and%20employs%20adaptive%20generalized%0Akernels%20that%20can%20be%20jointly%20optimized%20with%20the%20projection.%20This%20approach%0Aenables%20AKRMap%20to%20efficiently%20generate%20visualizations%20that%20capture%20complex%0Ametric%20distributions%2C%20while%20also%20supporting%20interactive%20features%20such%20as%20zoom%0Aand%20overlay%20for%20deeper%20exploration.%20Quantitative%20experiments%20demonstrate%20that%0AAKRMap%20outperforms%20existing%20DR%20methods%20in%20generating%20more%20accurate%20and%0Atrustworthy%20visualizations.%20We%20further%20showcase%20the%20effectiveness%20of%20AKRMap%20in%0Avisualizing%20and%20comparing%20cross-modal%20embeddings%20for%20text-to-image%20models.%20Code%0Aand%20demo%20are%20available%20at%20https%3A//github.com/yilinye/AKRMap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14664v1&entry.124074799=Read"},
{"title": "Toward Embodied AGI: A Review of Embodied AI and the Road Ahead", "author": "Yequan Wang and Aixin Sun", "abstract": "  Artificial General Intelligence (AGI) is often envisioned as inherently\nembodied. With recent advances in robotics and foundational AI models, we stand\nat the threshold of a new era-one marked by increasingly generalized embodied\nAI systems. This paper contributes to the discourse by introducing a systematic\ntaxonomy of Embodied AGI spanning five levels (L1-L5). We review existing\nresearch and challenges at the foundational stages (L1-L2) and outline the key\ncomponents required to achieve higher-level capabilities (L3-L5). Building on\nthese insights and existing technologies, we propose a conceptual framework for\nan L3+ robotic brain, offering both a technical outlook and a foundation for\nfuture exploration.\n", "link": "http://arxiv.org/abs/2505.14235v1", "date": "2025-05-20", "relevancy": 1.4604, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4964}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Embodied%20AGI%3A%20A%20Review%20of%20Embodied%20AI%20and%20the%20Road%20Ahead&body=Title%3A%20Toward%20Embodied%20AGI%3A%20A%20Review%20of%20Embodied%20AI%20and%20the%20Road%20Ahead%0AAuthor%3A%20Yequan%20Wang%20and%20Aixin%20Sun%0AAbstract%3A%20%20%20Artificial%20General%20Intelligence%20%28AGI%29%20is%20often%20envisioned%20as%20inherently%0Aembodied.%20With%20recent%20advances%20in%20robotics%20and%20foundational%20AI%20models%2C%20we%20stand%0Aat%20the%20threshold%20of%20a%20new%20era-one%20marked%20by%20increasingly%20generalized%20embodied%0AAI%20systems.%20This%20paper%20contributes%20to%20the%20discourse%20by%20introducing%20a%20systematic%0Ataxonomy%20of%20Embodied%20AGI%20spanning%20five%20levels%20%28L1-L5%29.%20We%20review%20existing%0Aresearch%20and%20challenges%20at%20the%20foundational%20stages%20%28L1-L2%29%20and%20outline%20the%20key%0Acomponents%20required%20to%20achieve%20higher-level%20capabilities%20%28L3-L5%29.%20Building%20on%0Athese%20insights%20and%20existing%20technologies%2C%20we%20propose%20a%20conceptual%20framework%20for%0Aan%20L3%2B%20robotic%20brain%2C%20offering%20both%20a%20technical%20outlook%20and%20a%20foundation%20for%0Afuture%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Embodied%2520AGI%253A%2520A%2520Review%2520of%2520Embodied%2520AI%2520and%2520the%2520Road%2520Ahead%26entry.906535625%3DYequan%2520Wang%2520and%2520Aixin%2520Sun%26entry.1292438233%3D%2520%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529%2520is%2520often%2520envisioned%2520as%2520inherently%250Aembodied.%2520With%2520recent%2520advances%2520in%2520robotics%2520and%2520foundational%2520AI%2520models%252C%2520we%2520stand%250Aat%2520the%2520threshold%2520of%2520a%2520new%2520era-one%2520marked%2520by%2520increasingly%2520generalized%2520embodied%250AAI%2520systems.%2520This%2520paper%2520contributes%2520to%2520the%2520discourse%2520by%2520introducing%2520a%2520systematic%250Ataxonomy%2520of%2520Embodied%2520AGI%2520spanning%2520five%2520levels%2520%2528L1-L5%2529.%2520We%2520review%2520existing%250Aresearch%2520and%2520challenges%2520at%2520the%2520foundational%2520stages%2520%2528L1-L2%2529%2520and%2520outline%2520the%2520key%250Acomponents%2520required%2520to%2520achieve%2520higher-level%2520capabilities%2520%2528L3-L5%2529.%2520Building%2520on%250Athese%2520insights%2520and%2520existing%2520technologies%252C%2520we%2520propose%2520a%2520conceptual%2520framework%2520for%250Aan%2520L3%252B%2520robotic%2520brain%252C%2520offering%2520both%2520a%2520technical%2520outlook%2520and%2520a%2520foundation%2520for%250Afuture%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Embodied%20AGI%3A%20A%20Review%20of%20Embodied%20AI%20and%20the%20Road%20Ahead&entry.906535625=Yequan%20Wang%20and%20Aixin%20Sun&entry.1292438233=%20%20Artificial%20General%20Intelligence%20%28AGI%29%20is%20often%20envisioned%20as%20inherently%0Aembodied.%20With%20recent%20advances%20in%20robotics%20and%20foundational%20AI%20models%2C%20we%20stand%0Aat%20the%20threshold%20of%20a%20new%20era-one%20marked%20by%20increasingly%20generalized%20embodied%0AAI%20systems.%20This%20paper%20contributes%20to%20the%20discourse%20by%20introducing%20a%20systematic%0Ataxonomy%20of%20Embodied%20AGI%20spanning%20five%20levels%20%28L1-L5%29.%20We%20review%20existing%0Aresearch%20and%20challenges%20at%20the%20foundational%20stages%20%28L1-L2%29%20and%20outline%20the%20key%0Acomponents%20required%20to%20achieve%20higher-level%20capabilities%20%28L3-L5%29.%20Building%20on%0Athese%20insights%20and%20existing%20technologies%2C%20we%20propose%20a%20conceptual%20framework%20for%0Aan%20L3%2B%20robotic%20brain%2C%20offering%20both%20a%20technical%20outlook%20and%20a%20foundation%20for%0Afuture%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14235v1&entry.124074799=Read"},
{"title": "LimeSoDa: A Dataset Collection for Benchmarking of Machine Learning\n  Regressors in Digital Soil Mapping", "author": "J. Schmidinger and S. Vogel and V. Barkov and A. -D. Pham and R. Gebbers and H. Tavakoli and J. Correa and T. R. Tavares and P. Filippi and E. J. Jones and V. Lukas and E. Boenecke and J. Ruehlmann and I. Schroeter and E. Kramer and S. Paetzold and M. Kodaira and A. M. J. -C. Wadoux and L. Bragazza and K. Metzger and J. Huang and D. S. M. Valente and J. L. Safanelli and E. L. Bottega and R. S. D. Dalmolin and C. Farkas and A. Steiger and T. Z. Horst and L. Ramirez-Lopez and T. Scholten and F. Stumpf and P. Rosso and M. M. Costa and R. S. Zandonadi and J. Wetterlind and M. Atzmueller", "abstract": "  Digital soil mapping (DSM) relies on a broad pool of statistical methods, yet\ndetermining the optimal method for a given context remains challenging and\ncontentious. Benchmarking studies on multiple datasets are needed to reveal\nstrengths and limitations of commonly used methods. Existing DSM studies\nusually rely on a single dataset with restricted access, leading to incomplete\nand potentially misleading conclusions. To address these issues, we introduce\nan open-access dataset collection called Precision Liming Soil Datasets\n(LimeSoDa). LimeSoDa consists of 31 field- and farm-scale datasets from various\ncountries. Each dataset has three target soil properties: (1) soil organic\nmatter or soil organic carbon, (2) clay content and (3) pH, alongside a set of\nfeatures. Features are dataset-specific and were obtained by optical\nspectroscopy, proximal- and remote soil sensing. All datasets were aligned to a\ntabular format and are ready-to-use for modeling. We demonstrated the use of\nLimeSoDa for benchmarking by comparing the predictive performance of four\nlearning algorithms across all datasets. This comparison included multiple\nlinear regression (MLR), support vector regression (SVR), categorical boosting\n(CatBoost) and random forest (RF). The results showed that although no single\nalgorithm was universally superior, certain algorithms performed better in\nspecific contexts. MLR and SVR performed better on high-dimensional spectral\ndatasets, likely due to better compatibility with principal components. In\ncontrast, CatBoost and RF exhibited considerably better performances when\napplied to datasets with a moderate number (< 20) of features. These\nbenchmarking results illustrate that the performance of a method is highly\ncontext-dependent. LimeSoDa therefore provides an important resource for\nimproving the development and evaluation of statistical methods in DSM.\n", "link": "http://arxiv.org/abs/2502.20139v2", "date": "2025-05-20", "relevancy": 1.9437, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4869}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LimeSoDa%3A%20A%20Dataset%20Collection%20for%20Benchmarking%20of%20Machine%20Learning%0A%20%20Regressors%20in%20Digital%20Soil%20Mapping&body=Title%3A%20LimeSoDa%3A%20A%20Dataset%20Collection%20for%20Benchmarking%20of%20Machine%20Learning%0A%20%20Regressors%20in%20Digital%20Soil%20Mapping%0AAuthor%3A%20J.%20Schmidinger%20and%20S.%20Vogel%20and%20V.%20Barkov%20and%20A.%20-D.%20Pham%20and%20R.%20Gebbers%20and%20H.%20Tavakoli%20and%20J.%20Correa%20and%20T.%20R.%20Tavares%20and%20P.%20Filippi%20and%20E.%20J.%20Jones%20and%20V.%20Lukas%20and%20E.%20Boenecke%20and%20J.%20Ruehlmann%20and%20I.%20Schroeter%20and%20E.%20Kramer%20and%20S.%20Paetzold%20and%20M.%20Kodaira%20and%20A.%20M.%20J.%20-C.%20Wadoux%20and%20L.%20Bragazza%20and%20K.%20Metzger%20and%20J.%20Huang%20and%20D.%20S.%20M.%20Valente%20and%20J.%20L.%20Safanelli%20and%20E.%20L.%20Bottega%20and%20R.%20S.%20D.%20Dalmolin%20and%20C.%20Farkas%20and%20A.%20Steiger%20and%20T.%20Z.%20Horst%20and%20L.%20Ramirez-Lopez%20and%20T.%20Scholten%20and%20F.%20Stumpf%20and%20P.%20Rosso%20and%20M.%20M.%20Costa%20and%20R.%20S.%20Zandonadi%20and%20J.%20Wetterlind%20and%20M.%20Atzmueller%0AAbstract%3A%20%20%20Digital%20soil%20mapping%20%28DSM%29%20relies%20on%20a%20broad%20pool%20of%20statistical%20methods%2C%20yet%0Adetermining%20the%20optimal%20method%20for%20a%20given%20context%20remains%20challenging%20and%0Acontentious.%20Benchmarking%20studies%20on%20multiple%20datasets%20are%20needed%20to%20reveal%0Astrengths%20and%20limitations%20of%20commonly%20used%20methods.%20Existing%20DSM%20studies%0Ausually%20rely%20on%20a%20single%20dataset%20with%20restricted%20access%2C%20leading%20to%20incomplete%0Aand%20potentially%20misleading%20conclusions.%20To%20address%20these%20issues%2C%20we%20introduce%0Aan%20open-access%20dataset%20collection%20called%20Precision%20Liming%20Soil%20Datasets%0A%28LimeSoDa%29.%20LimeSoDa%20consists%20of%2031%20field-%20and%20farm-scale%20datasets%20from%20various%0Acountries.%20Each%20dataset%20has%20three%20target%20soil%20properties%3A%20%281%29%20soil%20organic%0Amatter%20or%20soil%20organic%20carbon%2C%20%282%29%20clay%20content%20and%20%283%29%20pH%2C%20alongside%20a%20set%20of%0Afeatures.%20Features%20are%20dataset-specific%20and%20were%20obtained%20by%20optical%0Aspectroscopy%2C%20proximal-%20and%20remote%20soil%20sensing.%20All%20datasets%20were%20aligned%20to%20a%0Atabular%20format%20and%20are%20ready-to-use%20for%20modeling.%20We%20demonstrated%20the%20use%20of%0ALimeSoDa%20for%20benchmarking%20by%20comparing%20the%20predictive%20performance%20of%20four%0Alearning%20algorithms%20across%20all%20datasets.%20This%20comparison%20included%20multiple%0Alinear%20regression%20%28MLR%29%2C%20support%20vector%20regression%20%28SVR%29%2C%20categorical%20boosting%0A%28CatBoost%29%20and%20random%20forest%20%28RF%29.%20The%20results%20showed%20that%20although%20no%20single%0Aalgorithm%20was%20universally%20superior%2C%20certain%20algorithms%20performed%20better%20in%0Aspecific%20contexts.%20MLR%20and%20SVR%20performed%20better%20on%20high-dimensional%20spectral%0Adatasets%2C%20likely%20due%20to%20better%20compatibility%20with%20principal%20components.%20In%0Acontrast%2C%20CatBoost%20and%20RF%20exhibited%20considerably%20better%20performances%20when%0Aapplied%20to%20datasets%20with%20a%20moderate%20number%20%28%3C%2020%29%20of%20features.%20These%0Abenchmarking%20results%20illustrate%20that%20the%20performance%20of%20a%20method%20is%20highly%0Acontext-dependent.%20LimeSoDa%20therefore%20provides%20an%20important%20resource%20for%0Aimproving%20the%20development%20and%20evaluation%20of%20statistical%20methods%20in%20DSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimeSoDa%253A%2520A%2520Dataset%2520Collection%2520for%2520Benchmarking%2520of%2520Machine%2520Learning%250A%2520%2520Regressors%2520in%2520Digital%2520Soil%2520Mapping%26entry.906535625%3DJ.%2520Schmidinger%2520and%2520S.%2520Vogel%2520and%2520V.%2520Barkov%2520and%2520A.%2520-D.%2520Pham%2520and%2520R.%2520Gebbers%2520and%2520H.%2520Tavakoli%2520and%2520J.%2520Correa%2520and%2520T.%2520R.%2520Tavares%2520and%2520P.%2520Filippi%2520and%2520E.%2520J.%2520Jones%2520and%2520V.%2520Lukas%2520and%2520E.%2520Boenecke%2520and%2520J.%2520Ruehlmann%2520and%2520I.%2520Schroeter%2520and%2520E.%2520Kramer%2520and%2520S.%2520Paetzold%2520and%2520M.%2520Kodaira%2520and%2520A.%2520M.%2520J.%2520-C.%2520Wadoux%2520and%2520L.%2520Bragazza%2520and%2520K.%2520Metzger%2520and%2520J.%2520Huang%2520and%2520D.%2520S.%2520M.%2520Valente%2520and%2520J.%2520L.%2520Safanelli%2520and%2520E.%2520L.%2520Bottega%2520and%2520R.%2520S.%2520D.%2520Dalmolin%2520and%2520C.%2520Farkas%2520and%2520A.%2520Steiger%2520and%2520T.%2520Z.%2520Horst%2520and%2520L.%2520Ramirez-Lopez%2520and%2520T.%2520Scholten%2520and%2520F.%2520Stumpf%2520and%2520P.%2520Rosso%2520and%2520M.%2520M.%2520Costa%2520and%2520R.%2520S.%2520Zandonadi%2520and%2520J.%2520Wetterlind%2520and%2520M.%2520Atzmueller%26entry.1292438233%3D%2520%2520Digital%2520soil%2520mapping%2520%2528DSM%2529%2520relies%2520on%2520a%2520broad%2520pool%2520of%2520statistical%2520methods%252C%2520yet%250Adetermining%2520the%2520optimal%2520method%2520for%2520a%2520given%2520context%2520remains%2520challenging%2520and%250Acontentious.%2520Benchmarking%2520studies%2520on%2520multiple%2520datasets%2520are%2520needed%2520to%2520reveal%250Astrengths%2520and%2520limitations%2520of%2520commonly%2520used%2520methods.%2520Existing%2520DSM%2520studies%250Ausually%2520rely%2520on%2520a%2520single%2520dataset%2520with%2520restricted%2520access%252C%2520leading%2520to%2520incomplete%250Aand%2520potentially%2520misleading%2520conclusions.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%250Aan%2520open-access%2520dataset%2520collection%2520called%2520Precision%2520Liming%2520Soil%2520Datasets%250A%2528LimeSoDa%2529.%2520LimeSoDa%2520consists%2520of%252031%2520field-%2520and%2520farm-scale%2520datasets%2520from%2520various%250Acountries.%2520Each%2520dataset%2520has%2520three%2520target%2520soil%2520properties%253A%2520%25281%2529%2520soil%2520organic%250Amatter%2520or%2520soil%2520organic%2520carbon%252C%2520%25282%2529%2520clay%2520content%2520and%2520%25283%2529%2520pH%252C%2520alongside%2520a%2520set%2520of%250Afeatures.%2520Features%2520are%2520dataset-specific%2520and%2520were%2520obtained%2520by%2520optical%250Aspectroscopy%252C%2520proximal-%2520and%2520remote%2520soil%2520sensing.%2520All%2520datasets%2520were%2520aligned%2520to%2520a%250Atabular%2520format%2520and%2520are%2520ready-to-use%2520for%2520modeling.%2520We%2520demonstrated%2520the%2520use%2520of%250ALimeSoDa%2520for%2520benchmarking%2520by%2520comparing%2520the%2520predictive%2520performance%2520of%2520four%250Alearning%2520algorithms%2520across%2520all%2520datasets.%2520This%2520comparison%2520included%2520multiple%250Alinear%2520regression%2520%2528MLR%2529%252C%2520support%2520vector%2520regression%2520%2528SVR%2529%252C%2520categorical%2520boosting%250A%2528CatBoost%2529%2520and%2520random%2520forest%2520%2528RF%2529.%2520The%2520results%2520showed%2520that%2520although%2520no%2520single%250Aalgorithm%2520was%2520universally%2520superior%252C%2520certain%2520algorithms%2520performed%2520better%2520in%250Aspecific%2520contexts.%2520MLR%2520and%2520SVR%2520performed%2520better%2520on%2520high-dimensional%2520spectral%250Adatasets%252C%2520likely%2520due%2520to%2520better%2520compatibility%2520with%2520principal%2520components.%2520In%250Acontrast%252C%2520CatBoost%2520and%2520RF%2520exhibited%2520considerably%2520better%2520performances%2520when%250Aapplied%2520to%2520datasets%2520with%2520a%2520moderate%2520number%2520%2528%253C%252020%2529%2520of%2520features.%2520These%250Abenchmarking%2520results%2520illustrate%2520that%2520the%2520performance%2520of%2520a%2520method%2520is%2520highly%250Acontext-dependent.%2520LimeSoDa%2520therefore%2520provides%2520an%2520important%2520resource%2520for%250Aimproving%2520the%2520development%2520and%2520evaluation%2520of%2520statistical%2520methods%2520in%2520DSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LimeSoDa%3A%20A%20Dataset%20Collection%20for%20Benchmarking%20of%20Machine%20Learning%0A%20%20Regressors%20in%20Digital%20Soil%20Mapping&entry.906535625=J.%20Schmidinger%20and%20S.%20Vogel%20and%20V.%20Barkov%20and%20A.%20-D.%20Pham%20and%20R.%20Gebbers%20and%20H.%20Tavakoli%20and%20J.%20Correa%20and%20T.%20R.%20Tavares%20and%20P.%20Filippi%20and%20E.%20J.%20Jones%20and%20V.%20Lukas%20and%20E.%20Boenecke%20and%20J.%20Ruehlmann%20and%20I.%20Schroeter%20and%20E.%20Kramer%20and%20S.%20Paetzold%20and%20M.%20Kodaira%20and%20A.%20M.%20J.%20-C.%20Wadoux%20and%20L.%20Bragazza%20and%20K.%20Metzger%20and%20J.%20Huang%20and%20D.%20S.%20M.%20Valente%20and%20J.%20L.%20Safanelli%20and%20E.%20L.%20Bottega%20and%20R.%20S.%20D.%20Dalmolin%20and%20C.%20Farkas%20and%20A.%20Steiger%20and%20T.%20Z.%20Horst%20and%20L.%20Ramirez-Lopez%20and%20T.%20Scholten%20and%20F.%20Stumpf%20and%20P.%20Rosso%20and%20M.%20M.%20Costa%20and%20R.%20S.%20Zandonadi%20and%20J.%20Wetterlind%20and%20M.%20Atzmueller&entry.1292438233=%20%20Digital%20soil%20mapping%20%28DSM%29%20relies%20on%20a%20broad%20pool%20of%20statistical%20methods%2C%20yet%0Adetermining%20the%20optimal%20method%20for%20a%20given%20context%20remains%20challenging%20and%0Acontentious.%20Benchmarking%20studies%20on%20multiple%20datasets%20are%20needed%20to%20reveal%0Astrengths%20and%20limitations%20of%20commonly%20used%20methods.%20Existing%20DSM%20studies%0Ausually%20rely%20on%20a%20single%20dataset%20with%20restricted%20access%2C%20leading%20to%20incomplete%0Aand%20potentially%20misleading%20conclusions.%20To%20address%20these%20issues%2C%20we%20introduce%0Aan%20open-access%20dataset%20collection%20called%20Precision%20Liming%20Soil%20Datasets%0A%28LimeSoDa%29.%20LimeSoDa%20consists%20of%2031%20field-%20and%20farm-scale%20datasets%20from%20various%0Acountries.%20Each%20dataset%20has%20three%20target%20soil%20properties%3A%20%281%29%20soil%20organic%0Amatter%20or%20soil%20organic%20carbon%2C%20%282%29%20clay%20content%20and%20%283%29%20pH%2C%20alongside%20a%20set%20of%0Afeatures.%20Features%20are%20dataset-specific%20and%20were%20obtained%20by%20optical%0Aspectroscopy%2C%20proximal-%20and%20remote%20soil%20sensing.%20All%20datasets%20were%20aligned%20to%20a%0Atabular%20format%20and%20are%20ready-to-use%20for%20modeling.%20We%20demonstrated%20the%20use%20of%0ALimeSoDa%20for%20benchmarking%20by%20comparing%20the%20predictive%20performance%20of%20four%0Alearning%20algorithms%20across%20all%20datasets.%20This%20comparison%20included%20multiple%0Alinear%20regression%20%28MLR%29%2C%20support%20vector%20regression%20%28SVR%29%2C%20categorical%20boosting%0A%28CatBoost%29%20and%20random%20forest%20%28RF%29.%20The%20results%20showed%20that%20although%20no%20single%0Aalgorithm%20was%20universally%20superior%2C%20certain%20algorithms%20performed%20better%20in%0Aspecific%20contexts.%20MLR%20and%20SVR%20performed%20better%20on%20high-dimensional%20spectral%0Adatasets%2C%20likely%20due%20to%20better%20compatibility%20with%20principal%20components.%20In%0Acontrast%2C%20CatBoost%20and%20RF%20exhibited%20considerably%20better%20performances%20when%0Aapplied%20to%20datasets%20with%20a%20moderate%20number%20%28%3C%2020%29%20of%20features.%20These%0Abenchmarking%20results%20illustrate%20that%20the%20performance%20of%20a%20method%20is%20highly%0Acontext-dependent.%20LimeSoDa%20therefore%20provides%20an%20important%20resource%20for%0Aimproving%20the%20development%20and%20evaluation%20of%20statistical%20methods%20in%20DSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20139v2&entry.124074799=Read"},
{"title": "Prior Prompt Engineering for Reinforcement Fine-Tuning", "author": "Pittawat Taveekitworachai and Potsawee Manakul and Sarana Nutanong and Kunat Pipatanakul", "abstract": "  This paper investigates prior prompt engineering (pPE) in the context of\nreinforcement fine-tuning (RFT), where language models (LMs) are incentivized\nto exhibit behaviors that maximize performance through reward signals. While\nexisting RFT research has primarily focused on algorithms, reward shaping, and\ndata curation, the design of the prior prompt--the instructions prepended to\nqueries during training to elicit behaviors such as step-by-step\nreasoning--remains underexplored. We investigate whether different pPE\napproaches can guide LMs to internalize distinct behaviors after RFT. Inspired\nby inference-time prompt engineering (iPE), we translate five representative\niPE strategies--reasoning, planning, code-based reasoning, knowledge recall,\nand null-example utilization--into corresponding pPE approaches. We experiment\nwith Qwen2.5-7B using each of the pPE approaches, then evaluate performance on\nin-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and\nGPQA-Diamond). Our results show that all pPE-trained models surpass their\niPE-prompted counterparts, with the null-example pPE approach achieving the\nlargest average performance gain and the highest improvement on AIME2024 and\nGPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by\nadapting a behavior-classification framework, we demonstrate that different pPE\nstrategies instill distinct behavioral styles in the resulting models. These\nfindings position pPE as a powerful yet understudied axis for RFT.\n", "link": "http://arxiv.org/abs/2505.14157v1", "date": "2025-05-20", "relevancy": 1.9005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prior%20Prompt%20Engineering%20for%20Reinforcement%20Fine-Tuning&body=Title%3A%20Prior%20Prompt%20Engineering%20for%20Reinforcement%20Fine-Tuning%0AAuthor%3A%20Pittawat%20Taveekitworachai%20and%20Potsawee%20Manakul%20and%20Sarana%20Nutanong%20and%20Kunat%20Pipatanakul%0AAbstract%3A%20%20%20This%20paper%20investigates%20prior%20prompt%20engineering%20%28pPE%29%20in%20the%20context%20of%0Areinforcement%20fine-tuning%20%28RFT%29%2C%20where%20language%20models%20%28LMs%29%20are%20incentivized%0Ato%20exhibit%20behaviors%20that%20maximize%20performance%20through%20reward%20signals.%20While%0Aexisting%20RFT%20research%20has%20primarily%20focused%20on%20algorithms%2C%20reward%20shaping%2C%20and%0Adata%20curation%2C%20the%20design%20of%20the%20prior%20prompt--the%20instructions%20prepended%20to%0Aqueries%20during%20training%20to%20elicit%20behaviors%20such%20as%20step-by-step%0Areasoning--remains%20underexplored.%20We%20investigate%20whether%20different%20pPE%0Aapproaches%20can%20guide%20LMs%20to%20internalize%20distinct%20behaviors%20after%20RFT.%20Inspired%0Aby%20inference-time%20prompt%20engineering%20%28iPE%29%2C%20we%20translate%20five%20representative%0AiPE%20strategies--reasoning%2C%20planning%2C%20code-based%20reasoning%2C%20knowledge%20recall%2C%0Aand%20null-example%20utilization--into%20corresponding%20pPE%20approaches.%20We%20experiment%0Awith%20Qwen2.5-7B%20using%20each%20of%20the%20pPE%20approaches%2C%20then%20evaluate%20performance%20on%0Ain-domain%20and%20out-of-domain%20benchmarks%20%28e.g.%2C%20AIME2024%2C%20HumanEval%2B%2C%20and%0AGPQA-Diamond%29.%20Our%20results%20show%20that%20all%20pPE-trained%20models%20surpass%20their%0AiPE-prompted%20counterparts%2C%20with%20the%20null-example%20pPE%20approach%20achieving%20the%0Alargest%20average%20performance%20gain%20and%20the%20highest%20improvement%20on%20AIME2024%20and%0AGPQA-Diamond%2C%20surpassing%20the%20commonly%20used%20reasoning%20approach.%20Furthermore%2C%20by%0Aadapting%20a%20behavior-classification%20framework%2C%20we%20demonstrate%20that%20different%20pPE%0Astrategies%20instill%20distinct%20behavioral%20styles%20in%20the%20resulting%20models.%20These%0Afindings%20position%20pPE%20as%20a%20powerful%20yet%20understudied%20axis%20for%20RFT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrior%2520Prompt%2520Engineering%2520for%2520Reinforcement%2520Fine-Tuning%26entry.906535625%3DPittawat%2520Taveekitworachai%2520and%2520Potsawee%2520Manakul%2520and%2520Sarana%2520Nutanong%2520and%2520Kunat%2520Pipatanakul%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520prior%2520prompt%2520engineering%2520%2528pPE%2529%2520in%2520the%2520context%2520of%250Areinforcement%2520fine-tuning%2520%2528RFT%2529%252C%2520where%2520language%2520models%2520%2528LMs%2529%2520are%2520incentivized%250Ato%2520exhibit%2520behaviors%2520that%2520maximize%2520performance%2520through%2520reward%2520signals.%2520While%250Aexisting%2520RFT%2520research%2520has%2520primarily%2520focused%2520on%2520algorithms%252C%2520reward%2520shaping%252C%2520and%250Adata%2520curation%252C%2520the%2520design%2520of%2520the%2520prior%2520prompt--the%2520instructions%2520prepended%2520to%250Aqueries%2520during%2520training%2520to%2520elicit%2520behaviors%2520such%2520as%2520step-by-step%250Areasoning--remains%2520underexplored.%2520We%2520investigate%2520whether%2520different%2520pPE%250Aapproaches%2520can%2520guide%2520LMs%2520to%2520internalize%2520distinct%2520behaviors%2520after%2520RFT.%2520Inspired%250Aby%2520inference-time%2520prompt%2520engineering%2520%2528iPE%2529%252C%2520we%2520translate%2520five%2520representative%250AiPE%2520strategies--reasoning%252C%2520planning%252C%2520code-based%2520reasoning%252C%2520knowledge%2520recall%252C%250Aand%2520null-example%2520utilization--into%2520corresponding%2520pPE%2520approaches.%2520We%2520experiment%250Awith%2520Qwen2.5-7B%2520using%2520each%2520of%2520the%2520pPE%2520approaches%252C%2520then%2520evaluate%2520performance%2520on%250Ain-domain%2520and%2520out-of-domain%2520benchmarks%2520%2528e.g.%252C%2520AIME2024%252C%2520HumanEval%252B%252C%2520and%250AGPQA-Diamond%2529.%2520Our%2520results%2520show%2520that%2520all%2520pPE-trained%2520models%2520surpass%2520their%250AiPE-prompted%2520counterparts%252C%2520with%2520the%2520null-example%2520pPE%2520approach%2520achieving%2520the%250Alargest%2520average%2520performance%2520gain%2520and%2520the%2520highest%2520improvement%2520on%2520AIME2024%2520and%250AGPQA-Diamond%252C%2520surpassing%2520the%2520commonly%2520used%2520reasoning%2520approach.%2520Furthermore%252C%2520by%250Aadapting%2520a%2520behavior-classification%2520framework%252C%2520we%2520demonstrate%2520that%2520different%2520pPE%250Astrategies%2520instill%2520distinct%2520behavioral%2520styles%2520in%2520the%2520resulting%2520models.%2520These%250Afindings%2520position%2520pPE%2520as%2520a%2520powerful%2520yet%2520understudied%2520axis%2520for%2520RFT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prior%20Prompt%20Engineering%20for%20Reinforcement%20Fine-Tuning&entry.906535625=Pittawat%20Taveekitworachai%20and%20Potsawee%20Manakul%20and%20Sarana%20Nutanong%20and%20Kunat%20Pipatanakul&entry.1292438233=%20%20This%20paper%20investigates%20prior%20prompt%20engineering%20%28pPE%29%20in%20the%20context%20of%0Areinforcement%20fine-tuning%20%28RFT%29%2C%20where%20language%20models%20%28LMs%29%20are%20incentivized%0Ato%20exhibit%20behaviors%20that%20maximize%20performance%20through%20reward%20signals.%20While%0Aexisting%20RFT%20research%20has%20primarily%20focused%20on%20algorithms%2C%20reward%20shaping%2C%20and%0Adata%20curation%2C%20the%20design%20of%20the%20prior%20prompt--the%20instructions%20prepended%20to%0Aqueries%20during%20training%20to%20elicit%20behaviors%20such%20as%20step-by-step%0Areasoning--remains%20underexplored.%20We%20investigate%20whether%20different%20pPE%0Aapproaches%20can%20guide%20LMs%20to%20internalize%20distinct%20behaviors%20after%20RFT.%20Inspired%0Aby%20inference-time%20prompt%20engineering%20%28iPE%29%2C%20we%20translate%20five%20representative%0AiPE%20strategies--reasoning%2C%20planning%2C%20code-based%20reasoning%2C%20knowledge%20recall%2C%0Aand%20null-example%20utilization--into%20corresponding%20pPE%20approaches.%20We%20experiment%0Awith%20Qwen2.5-7B%20using%20each%20of%20the%20pPE%20approaches%2C%20then%20evaluate%20performance%20on%0Ain-domain%20and%20out-of-domain%20benchmarks%20%28e.g.%2C%20AIME2024%2C%20HumanEval%2B%2C%20and%0AGPQA-Diamond%29.%20Our%20results%20show%20that%20all%20pPE-trained%20models%20surpass%20their%0AiPE-prompted%20counterparts%2C%20with%20the%20null-example%20pPE%20approach%20achieving%20the%0Alargest%20average%20performance%20gain%20and%20the%20highest%20improvement%20on%20AIME2024%20and%0AGPQA-Diamond%2C%20surpassing%20the%20commonly%20used%20reasoning%20approach.%20Furthermore%2C%20by%0Aadapting%20a%20behavior-classification%20framework%2C%20we%20demonstrate%20that%20different%20pPE%0Astrategies%20instill%20distinct%20behavioral%20styles%20in%20the%20resulting%20models.%20These%0Afindings%20position%20pPE%20as%20a%20powerful%20yet%20understudied%20axis%20for%20RFT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14157v1&entry.124074799=Read"},
{"title": "One-Step Offline Distillation of Diffusion-based Models via Koopman\n  Modeling", "author": "Nimrod Berman and Ilan Naiman and Moshe Eliasof and Hedi Zisling and Omri Azencot", "abstract": "  Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.\n", "link": "http://arxiv.org/abs/2505.13358v2", "date": "2025-05-20", "relevancy": 1.7243, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6069}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5711}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Step%20Offline%20Distillation%20of%20Diffusion-based%20Models%20via%20Koopman%0A%20%20Modeling&body=Title%3A%20One-Step%20Offline%20Distillation%20of%20Diffusion-based%20Models%20via%20Koopman%0A%20%20Modeling%0AAuthor%3A%20Nimrod%20Berman%20and%20Ilan%20Naiman%20and%20Moshe%20Eliasof%20and%20Hedi%20Zisling%20and%20Omri%20Azencot%0AAbstract%3A%20%20%20Diffusion-based%20generative%20models%20have%20demonstrated%20exceptional%20performance%2C%0Ayet%20their%20iterative%20sampling%20procedures%20remain%20computationally%20expensive.%20A%0Aprominent%20strategy%20to%20mitigate%20this%20cost%20is%20distillation%2C%20with%20offline%0Adistillation%20offering%20particular%20advantages%20in%20terms%20of%20efficiency%2C%20modularity%2C%0Aand%20flexibility.%20In%20this%20work%2C%20we%20identify%20two%20key%20observations%20that%20motivate%20a%0Aprincipled%20distillation%20framework%3A%20%281%29%20while%20diffusion%20models%20have%20been%20viewed%0Athrough%20the%20lens%20of%20dynamical%20systems%20theory%2C%20powerful%20and%20underexplored%20tools%0Acan%20be%20further%20leveraged%3B%20and%20%282%29%20diffusion%20models%20inherently%20impose%0Astructured%2C%20semantically%20coherent%20trajectories%20in%20latent%20space.%20Building%20on%0Athese%20observations%2C%20we%20introduce%20the%20Koopman%20Distillation%20Model%20KDM%2C%20a%20novel%0Aoffline%20distillation%20approach%20grounded%20in%20Koopman%20theory-a%20classical%20framework%0Afor%20representing%20nonlinear%20dynamics%20linearly%20in%20a%20transformed%20space.%20KDM%0Aencodes%20noisy%20inputs%20into%20an%20embedded%20space%20where%20a%20learned%20linear%20operator%0Apropagates%20them%20forward%2C%20followed%20by%20a%20decoder%20that%20reconstructs%20clean%20samples.%0AThis%20enables%20single-step%20generation%20while%20preserving%20semantic%20fidelity.%20We%0Aprovide%20theoretical%20justification%20for%20our%20approach%3A%20%281%29%20under%20mild%20assumptions%2C%0Athe%20learned%20diffusion%20dynamics%20admit%20a%20finite-dimensional%20Koopman%0Arepresentation%3B%20and%20%282%29%20proximity%20in%20the%20Koopman%20latent%20space%20correlates%20with%0Asemantic%20similarity%20in%20the%20generated%20outputs%2C%20allowing%20for%20effective%20trajectory%0Aalignment.%20Empirically%2C%20KDM%20achieves%20state-of-the-art%20performance%20across%0Astandard%20offline%20distillation%20benchmarks%2C%20improving%20FID%20scores%20by%20up%20to%2040%25%20in%0Aa%20single%20generation%20step.%20All%20implementation%20details%20and%20code%20for%20the%0Aexperimental%20setups%20are%20provided%20in%20our%20GitHub%20-%0Ahttps%3A//github.com/azencot-group/KDM%2C%20or%20in%20our%20project%20page%20-%0Ahttps%3A//sites.google.com/view/koopman-distillation-model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Step%2520Offline%2520Distillation%2520of%2520Diffusion-based%2520Models%2520via%2520Koopman%250A%2520%2520Modeling%26entry.906535625%3DNimrod%2520Berman%2520and%2520Ilan%2520Naiman%2520and%2520Moshe%2520Eliasof%2520and%2520Hedi%2520Zisling%2520and%2520Omri%2520Azencot%26entry.1292438233%3D%2520%2520Diffusion-based%2520generative%2520models%2520have%2520demonstrated%2520exceptional%2520performance%252C%250Ayet%2520their%2520iterative%2520sampling%2520procedures%2520remain%2520computationally%2520expensive.%2520A%250Aprominent%2520strategy%2520to%2520mitigate%2520this%2520cost%2520is%2520distillation%252C%2520with%2520offline%250Adistillation%2520offering%2520particular%2520advantages%2520in%2520terms%2520of%2520efficiency%252C%2520modularity%252C%250Aand%2520flexibility.%2520In%2520this%2520work%252C%2520we%2520identify%2520two%2520key%2520observations%2520that%2520motivate%2520a%250Aprincipled%2520distillation%2520framework%253A%2520%25281%2529%2520while%2520diffusion%2520models%2520have%2520been%2520viewed%250Athrough%2520the%2520lens%2520of%2520dynamical%2520systems%2520theory%252C%2520powerful%2520and%2520underexplored%2520tools%250Acan%2520be%2520further%2520leveraged%253B%2520and%2520%25282%2529%2520diffusion%2520models%2520inherently%2520impose%250Astructured%252C%2520semantically%2520coherent%2520trajectories%2520in%2520latent%2520space.%2520Building%2520on%250Athese%2520observations%252C%2520we%2520introduce%2520the%2520Koopman%2520Distillation%2520Model%2520KDM%252C%2520a%2520novel%250Aoffline%2520distillation%2520approach%2520grounded%2520in%2520Koopman%2520theory-a%2520classical%2520framework%250Afor%2520representing%2520nonlinear%2520dynamics%2520linearly%2520in%2520a%2520transformed%2520space.%2520KDM%250Aencodes%2520noisy%2520inputs%2520into%2520an%2520embedded%2520space%2520where%2520a%2520learned%2520linear%2520operator%250Apropagates%2520them%2520forward%252C%2520followed%2520by%2520a%2520decoder%2520that%2520reconstructs%2520clean%2520samples.%250AThis%2520enables%2520single-step%2520generation%2520while%2520preserving%2520semantic%2520fidelity.%2520We%250Aprovide%2520theoretical%2520justification%2520for%2520our%2520approach%253A%2520%25281%2529%2520under%2520mild%2520assumptions%252C%250Athe%2520learned%2520diffusion%2520dynamics%2520admit%2520a%2520finite-dimensional%2520Koopman%250Arepresentation%253B%2520and%2520%25282%2529%2520proximity%2520in%2520the%2520Koopman%2520latent%2520space%2520correlates%2520with%250Asemantic%2520similarity%2520in%2520the%2520generated%2520outputs%252C%2520allowing%2520for%2520effective%2520trajectory%250Aalignment.%2520Empirically%252C%2520KDM%2520achieves%2520state-of-the-art%2520performance%2520across%250Astandard%2520offline%2520distillation%2520benchmarks%252C%2520improving%2520FID%2520scores%2520by%2520up%2520to%252040%2525%2520in%250Aa%2520single%2520generation%2520step.%2520All%2520implementation%2520details%2520and%2520code%2520for%2520the%250Aexperimental%2520setups%2520are%2520provided%2520in%2520our%2520GitHub%2520-%250Ahttps%253A//github.com/azencot-group/KDM%252C%2520or%2520in%2520our%2520project%2520page%2520-%250Ahttps%253A//sites.google.com/view/koopman-distillation-model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Step%20Offline%20Distillation%20of%20Diffusion-based%20Models%20via%20Koopman%0A%20%20Modeling&entry.906535625=Nimrod%20Berman%20and%20Ilan%20Naiman%20and%20Moshe%20Eliasof%20and%20Hedi%20Zisling%20and%20Omri%20Azencot&entry.1292438233=%20%20Diffusion-based%20generative%20models%20have%20demonstrated%20exceptional%20performance%2C%0Ayet%20their%20iterative%20sampling%20procedures%20remain%20computationally%20expensive.%20A%0Aprominent%20strategy%20to%20mitigate%20this%20cost%20is%20distillation%2C%20with%20offline%0Adistillation%20offering%20particular%20advantages%20in%20terms%20of%20efficiency%2C%20modularity%2C%0Aand%20flexibility.%20In%20this%20work%2C%20we%20identify%20two%20key%20observations%20that%20motivate%20a%0Aprincipled%20distillation%20framework%3A%20%281%29%20while%20diffusion%20models%20have%20been%20viewed%0Athrough%20the%20lens%20of%20dynamical%20systems%20theory%2C%20powerful%20and%20underexplored%20tools%0Acan%20be%20further%20leveraged%3B%20and%20%282%29%20diffusion%20models%20inherently%20impose%0Astructured%2C%20semantically%20coherent%20trajectories%20in%20latent%20space.%20Building%20on%0Athese%20observations%2C%20we%20introduce%20the%20Koopman%20Distillation%20Model%20KDM%2C%20a%20novel%0Aoffline%20distillation%20approach%20grounded%20in%20Koopman%20theory-a%20classical%20framework%0Afor%20representing%20nonlinear%20dynamics%20linearly%20in%20a%20transformed%20space.%20KDM%0Aencodes%20noisy%20inputs%20into%20an%20embedded%20space%20where%20a%20learned%20linear%20operator%0Apropagates%20them%20forward%2C%20followed%20by%20a%20decoder%20that%20reconstructs%20clean%20samples.%0AThis%20enables%20single-step%20generation%20while%20preserving%20semantic%20fidelity.%20We%0Aprovide%20theoretical%20justification%20for%20our%20approach%3A%20%281%29%20under%20mild%20assumptions%2C%0Athe%20learned%20diffusion%20dynamics%20admit%20a%20finite-dimensional%20Koopman%0Arepresentation%3B%20and%20%282%29%20proximity%20in%20the%20Koopman%20latent%20space%20correlates%20with%0Asemantic%20similarity%20in%20the%20generated%20outputs%2C%20allowing%20for%20effective%20trajectory%0Aalignment.%20Empirically%2C%20KDM%20achieves%20state-of-the-art%20performance%20across%0Astandard%20offline%20distillation%20benchmarks%2C%20improving%20FID%20scores%20by%20up%20to%2040%25%20in%0Aa%20single%20generation%20step.%20All%20implementation%20details%20and%20code%20for%20the%0Aexperimental%20setups%20are%20provided%20in%20our%20GitHub%20-%0Ahttps%3A//github.com/azencot-group/KDM%2C%20or%20in%20our%20project%20page%20-%0Ahttps%3A//sites.google.com/view/koopman-distillation-model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13358v2&entry.124074799=Read"},
{"title": "Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for\n  Composed Person Retrieval", "author": "Delong Liu and Haiwen Li and Zhaohui Hou and Zhicheng Zhao and Fei Su and Yuan Dong", "abstract": "  Person retrieval has attracted rising attention. Existing methods are mainly\ndivided into two retrieval modes, namely image-only and text-only. However,\nthey are unable to make full use of the available information and are difficult\nto meet diverse application requirements. To address the above limitations, we\npropose a new Composed Person Retrieval (CPR) task, which combines visual and\ntextual queries to identify individuals of interest from large-scale person\nimage databases. Nevertheless, the foremost difficulty of the CPR task is the\nlack of available annotated datasets. Therefore, we first introduce a scalable\nautomatic data synthesis pipeline, which decomposes complex multimodal data\ngeneration into the creation of textual quadruples followed by\nidentity-consistent image synthesis using fine-tuned generative models.\nMeanwhile, a multimodal filtering method is designed to ensure the resulting\nSynCPR dataset retains 1.15 million high-quality and fully synthetic triplets.\nAdditionally, to improve the representation of composed person queries, we\npropose a novel Fine-grained Adaptive Feature Alignment (FAFA) framework\nthrough fine-grained dynamic alignment and masked feature reasoning. Moreover,\nfor objective evaluation, we manually annotate the Image-Text Composed Person\nRetrieval (ITCPR) test set. The extensive experiments demonstrate the\neffectiveness of the SynCPR dataset and the superiority of the proposed FAFA\nframework when compared with the state-of-the-art methods. All code and data\nwill be provided at\nhttps://github.com/Delong-liu-bupt/Composed_Person_Retrieval.\n", "link": "http://arxiv.org/abs/2311.16515v4", "date": "2025-05-20", "relevancy": 2.1473, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5305}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Synthetic%20Data%20and%20Fine-grained%20Adaptive%20Feature%20Alignment%20for%0A%20%20Composed%20Person%20Retrieval&body=Title%3A%20Automatic%20Synthetic%20Data%20and%20Fine-grained%20Adaptive%20Feature%20Alignment%20for%0A%20%20Composed%20Person%20Retrieval%0AAuthor%3A%20Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhaohui%20Hou%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Yuan%20Dong%0AAbstract%3A%20%20%20Person%20retrieval%20has%20attracted%20rising%20attention.%20Existing%20methods%20are%20mainly%0Adivided%20into%20two%20retrieval%20modes%2C%20namely%20image-only%20and%20text-only.%20However%2C%0Athey%20are%20unable%20to%20make%20full%20use%20of%20the%20available%20information%20and%20are%20difficult%0Ato%20meet%20diverse%20application%20requirements.%20To%20address%20the%20above%20limitations%2C%20we%0Apropose%20a%20new%20Composed%20Person%20Retrieval%20%28CPR%29%20task%2C%20which%20combines%20visual%20and%0Atextual%20queries%20to%20identify%20individuals%20of%20interest%20from%20large-scale%20person%0Aimage%20databases.%20Nevertheless%2C%20the%20foremost%20difficulty%20of%20the%20CPR%20task%20is%20the%0Alack%20of%20available%20annotated%20datasets.%20Therefore%2C%20we%20first%20introduce%20a%20scalable%0Aautomatic%20data%20synthesis%20pipeline%2C%20which%20decomposes%20complex%20multimodal%20data%0Ageneration%20into%20the%20creation%20of%20textual%20quadruples%20followed%20by%0Aidentity-consistent%20image%20synthesis%20using%20fine-tuned%20generative%20models.%0AMeanwhile%2C%20a%20multimodal%20filtering%20method%20is%20designed%20to%20ensure%20the%20resulting%0ASynCPR%20dataset%20retains%201.15%20million%20high-quality%20and%20fully%20synthetic%20triplets.%0AAdditionally%2C%20to%20improve%20the%20representation%20of%20composed%20person%20queries%2C%20we%0Apropose%20a%20novel%20Fine-grained%20Adaptive%20Feature%20Alignment%20%28FAFA%29%20framework%0Athrough%20fine-grained%20dynamic%20alignment%20and%20masked%20feature%20reasoning.%20Moreover%2C%0Afor%20objective%20evaluation%2C%20we%20manually%20annotate%20the%20Image-Text%20Composed%20Person%0ARetrieval%20%28ITCPR%29%20test%20set.%20The%20extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20SynCPR%20dataset%20and%20the%20superiority%20of%20the%20proposed%20FAFA%0Aframework%20when%20compared%20with%20the%20state-of-the-art%20methods.%20All%20code%20and%20data%0Awill%20be%20provided%20at%0Ahttps%3A//github.com/Delong-liu-bupt/Composed_Person_Retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16515v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Synthetic%2520Data%2520and%2520Fine-grained%2520Adaptive%2520Feature%2520Alignment%2520for%250A%2520%2520Composed%2520Person%2520Retrieval%26entry.906535625%3DDelong%2520Liu%2520and%2520Haiwen%2520Li%2520and%2520Zhaohui%2520Hou%2520and%2520Zhicheng%2520Zhao%2520and%2520Fei%2520Su%2520and%2520Yuan%2520Dong%26entry.1292438233%3D%2520%2520Person%2520retrieval%2520has%2520attracted%2520rising%2520attention.%2520Existing%2520methods%2520are%2520mainly%250Adivided%2520into%2520two%2520retrieval%2520modes%252C%2520namely%2520image-only%2520and%2520text-only.%2520However%252C%250Athey%2520are%2520unable%2520to%2520make%2520full%2520use%2520of%2520the%2520available%2520information%2520and%2520are%2520difficult%250Ato%2520meet%2520diverse%2520application%2520requirements.%2520To%2520address%2520the%2520above%2520limitations%252C%2520we%250Apropose%2520a%2520new%2520Composed%2520Person%2520Retrieval%2520%2528CPR%2529%2520task%252C%2520which%2520combines%2520visual%2520and%250Atextual%2520queries%2520to%2520identify%2520individuals%2520of%2520interest%2520from%2520large-scale%2520person%250Aimage%2520databases.%2520Nevertheless%252C%2520the%2520foremost%2520difficulty%2520of%2520the%2520CPR%2520task%2520is%2520the%250Alack%2520of%2520available%2520annotated%2520datasets.%2520Therefore%252C%2520we%2520first%2520introduce%2520a%2520scalable%250Aautomatic%2520data%2520synthesis%2520pipeline%252C%2520which%2520decomposes%2520complex%2520multimodal%2520data%250Ageneration%2520into%2520the%2520creation%2520of%2520textual%2520quadruples%2520followed%2520by%250Aidentity-consistent%2520image%2520synthesis%2520using%2520fine-tuned%2520generative%2520models.%250AMeanwhile%252C%2520a%2520multimodal%2520filtering%2520method%2520is%2520designed%2520to%2520ensure%2520the%2520resulting%250ASynCPR%2520dataset%2520retains%25201.15%2520million%2520high-quality%2520and%2520fully%2520synthetic%2520triplets.%250AAdditionally%252C%2520to%2520improve%2520the%2520representation%2520of%2520composed%2520person%2520queries%252C%2520we%250Apropose%2520a%2520novel%2520Fine-grained%2520Adaptive%2520Feature%2520Alignment%2520%2528FAFA%2529%2520framework%250Athrough%2520fine-grained%2520dynamic%2520alignment%2520and%2520masked%2520feature%2520reasoning.%2520Moreover%252C%250Afor%2520objective%2520evaluation%252C%2520we%2520manually%2520annotate%2520the%2520Image-Text%2520Composed%2520Person%250ARetrieval%2520%2528ITCPR%2529%2520test%2520set.%2520The%2520extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520SynCPR%2520dataset%2520and%2520the%2520superiority%2520of%2520the%2520proposed%2520FAFA%250Aframework%2520when%2520compared%2520with%2520the%2520state-of-the-art%2520methods.%2520All%2520code%2520and%2520data%250Awill%2520be%2520provided%2520at%250Ahttps%253A//github.com/Delong-liu-bupt/Composed_Person_Retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16515v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Synthetic%20Data%20and%20Fine-grained%20Adaptive%20Feature%20Alignment%20for%0A%20%20Composed%20Person%20Retrieval&entry.906535625=Delong%20Liu%20and%20Haiwen%20Li%20and%20Zhaohui%20Hou%20and%20Zhicheng%20Zhao%20and%20Fei%20Su%20and%20Yuan%20Dong&entry.1292438233=%20%20Person%20retrieval%20has%20attracted%20rising%20attention.%20Existing%20methods%20are%20mainly%0Adivided%20into%20two%20retrieval%20modes%2C%20namely%20image-only%20and%20text-only.%20However%2C%0Athey%20are%20unable%20to%20make%20full%20use%20of%20the%20available%20information%20and%20are%20difficult%0Ato%20meet%20diverse%20application%20requirements.%20To%20address%20the%20above%20limitations%2C%20we%0Apropose%20a%20new%20Composed%20Person%20Retrieval%20%28CPR%29%20task%2C%20which%20combines%20visual%20and%0Atextual%20queries%20to%20identify%20individuals%20of%20interest%20from%20large-scale%20person%0Aimage%20databases.%20Nevertheless%2C%20the%20foremost%20difficulty%20of%20the%20CPR%20task%20is%20the%0Alack%20of%20available%20annotated%20datasets.%20Therefore%2C%20we%20first%20introduce%20a%20scalable%0Aautomatic%20data%20synthesis%20pipeline%2C%20which%20decomposes%20complex%20multimodal%20data%0Ageneration%20into%20the%20creation%20of%20textual%20quadruples%20followed%20by%0Aidentity-consistent%20image%20synthesis%20using%20fine-tuned%20generative%20models.%0AMeanwhile%2C%20a%20multimodal%20filtering%20method%20is%20designed%20to%20ensure%20the%20resulting%0ASynCPR%20dataset%20retains%201.15%20million%20high-quality%20and%20fully%20synthetic%20triplets.%0AAdditionally%2C%20to%20improve%20the%20representation%20of%20composed%20person%20queries%2C%20we%0Apropose%20a%20novel%20Fine-grained%20Adaptive%20Feature%20Alignment%20%28FAFA%29%20framework%0Athrough%20fine-grained%20dynamic%20alignment%20and%20masked%20feature%20reasoning.%20Moreover%2C%0Afor%20objective%20evaluation%2C%20we%20manually%20annotate%20the%20Image-Text%20Composed%20Person%0ARetrieval%20%28ITCPR%29%20test%20set.%20The%20extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20SynCPR%20dataset%20and%20the%20superiority%20of%20the%20proposed%20FAFA%0Aframework%20when%20compared%20with%20the%20state-of-the-art%20methods.%20All%20code%20and%20data%0Awill%20be%20provided%20at%0Ahttps%3A//github.com/Delong-liu-bupt/Composed_Person_Retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16515v4&entry.124074799=Read"},
{"title": "Efficient and Scalable Neural Symbolic Search for Knowledge Graph\n  Complex Query Answering", "author": "Weizhi Fei and Zihao Wang and hang Yin and Shukai Zhao and Wei Zhang and Yangqiu Song", "abstract": "  Complex Query Answering (CQA) aims to retrieve answer sets for complex\nlogical formulas from incomplete knowledge graphs, which is a crucial yet\nchallenging task in knowledge graph reasoning. While neuro-symbolic search\nutilized neural link predictions achieve superior accuracy, they encounter\nsignificant complexity bottlenecks: (i) Data complexity typically scales\nquadratically with the number of entities in the knowledge graph, and (ii)\nQuery complexity becomes NP-hard for cyclic queries. Consequently, these\napproaches struggle to effectively scale to larger knowledge graphs and more\ncomplex queries. To address these challenges, we propose an efficient and\nscalable symbolic search framework. First, we propose two constraint strategies\nto compute neural logical indices to reduce the domain of variables, thereby\ndecreasing the data complexity of symbolic search. Additionally, we introduce\nan approximate algorithm based on local search to tackle the NP query\ncomplexity of cyclic queries. Experiments on various CQA benchmarks demonstrate\nthat our framework reduces the computational load of symbolic methods by 90\\%\nwhile maintaining nearly the same performance, thus alleviating both efficiency\nand scalability issues.\n", "link": "http://arxiv.org/abs/2505.08155v3", "date": "2025-05-20", "relevancy": 1.904, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5018}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Scalable%20Neural%20Symbolic%20Search%20for%20Knowledge%20Graph%0A%20%20Complex%20Query%20Answering&body=Title%3A%20Efficient%20and%20Scalable%20Neural%20Symbolic%20Search%20for%20Knowledge%20Graph%0A%20%20Complex%20Query%20Answering%0AAuthor%3A%20Weizhi%20Fei%20and%20Zihao%20Wang%20and%20hang%20Yin%20and%20Shukai%20Zhao%20and%20Wei%20Zhang%20and%20Yangqiu%20Song%0AAbstract%3A%20%20%20Complex%20Query%20Answering%20%28CQA%29%20aims%20to%20retrieve%20answer%20sets%20for%20complex%0Alogical%20formulas%20from%20incomplete%20knowledge%20graphs%2C%20which%20is%20a%20crucial%20yet%0Achallenging%20task%20in%20knowledge%20graph%20reasoning.%20While%20neuro-symbolic%20search%0Autilized%20neural%20link%20predictions%20achieve%20superior%20accuracy%2C%20they%20encounter%0Asignificant%20complexity%20bottlenecks%3A%20%28i%29%20Data%20complexity%20typically%20scales%0Aquadratically%20with%20the%20number%20of%20entities%20in%20the%20knowledge%20graph%2C%20and%20%28ii%29%0AQuery%20complexity%20becomes%20NP-hard%20for%20cyclic%20queries.%20Consequently%2C%20these%0Aapproaches%20struggle%20to%20effectively%20scale%20to%20larger%20knowledge%20graphs%20and%20more%0Acomplex%20queries.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20efficient%20and%0Ascalable%20symbolic%20search%20framework.%20First%2C%20we%20propose%20two%20constraint%20strategies%0Ato%20compute%20neural%20logical%20indices%20to%20reduce%20the%20domain%20of%20variables%2C%20thereby%0Adecreasing%20the%20data%20complexity%20of%20symbolic%20search.%20Additionally%2C%20we%20introduce%0Aan%20approximate%20algorithm%20based%20on%20local%20search%20to%20tackle%20the%20NP%20query%0Acomplexity%20of%20cyclic%20queries.%20Experiments%20on%20various%20CQA%20benchmarks%20demonstrate%0Athat%20our%20framework%20reduces%20the%20computational%20load%20of%20symbolic%20methods%20by%2090%5C%25%0Awhile%20maintaining%20nearly%20the%20same%20performance%2C%20thus%20alleviating%20both%20efficiency%0Aand%20scalability%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.08155v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520and%2520Scalable%2520Neural%2520Symbolic%2520Search%2520for%2520Knowledge%2520Graph%250A%2520%2520Complex%2520Query%2520Answering%26entry.906535625%3DWeizhi%2520Fei%2520and%2520Zihao%2520Wang%2520and%2520hang%2520Yin%2520and%2520Shukai%2520Zhao%2520and%2520Wei%2520Zhang%2520and%2520Yangqiu%2520Song%26entry.1292438233%3D%2520%2520Complex%2520Query%2520Answering%2520%2528CQA%2529%2520aims%2520to%2520retrieve%2520answer%2520sets%2520for%2520complex%250Alogical%2520formulas%2520from%2520incomplete%2520knowledge%2520graphs%252C%2520which%2520is%2520a%2520crucial%2520yet%250Achallenging%2520task%2520in%2520knowledge%2520graph%2520reasoning.%2520While%2520neuro-symbolic%2520search%250Autilized%2520neural%2520link%2520predictions%2520achieve%2520superior%2520accuracy%252C%2520they%2520encounter%250Asignificant%2520complexity%2520bottlenecks%253A%2520%2528i%2529%2520Data%2520complexity%2520typically%2520scales%250Aquadratically%2520with%2520the%2520number%2520of%2520entities%2520in%2520the%2520knowledge%2520graph%252C%2520and%2520%2528ii%2529%250AQuery%2520complexity%2520becomes%2520NP-hard%2520for%2520cyclic%2520queries.%2520Consequently%252C%2520these%250Aapproaches%2520struggle%2520to%2520effectively%2520scale%2520to%2520larger%2520knowledge%2520graphs%2520and%2520more%250Acomplex%2520queries.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520efficient%2520and%250Ascalable%2520symbolic%2520search%2520framework.%2520First%252C%2520we%2520propose%2520two%2520constraint%2520strategies%250Ato%2520compute%2520neural%2520logical%2520indices%2520to%2520reduce%2520the%2520domain%2520of%2520variables%252C%2520thereby%250Adecreasing%2520the%2520data%2520complexity%2520of%2520symbolic%2520search.%2520Additionally%252C%2520we%2520introduce%250Aan%2520approximate%2520algorithm%2520based%2520on%2520local%2520search%2520to%2520tackle%2520the%2520NP%2520query%250Acomplexity%2520of%2520cyclic%2520queries.%2520Experiments%2520on%2520various%2520CQA%2520benchmarks%2520demonstrate%250Athat%2520our%2520framework%2520reduces%2520the%2520computational%2520load%2520of%2520symbolic%2520methods%2520by%252090%255C%2525%250Awhile%2520maintaining%2520nearly%2520the%2520same%2520performance%252C%2520thus%2520alleviating%2520both%2520efficiency%250Aand%2520scalability%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08155v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Scalable%20Neural%20Symbolic%20Search%20for%20Knowledge%20Graph%0A%20%20Complex%20Query%20Answering&entry.906535625=Weizhi%20Fei%20and%20Zihao%20Wang%20and%20hang%20Yin%20and%20Shukai%20Zhao%20and%20Wei%20Zhang%20and%20Yangqiu%20Song&entry.1292438233=%20%20Complex%20Query%20Answering%20%28CQA%29%20aims%20to%20retrieve%20answer%20sets%20for%20complex%0Alogical%20formulas%20from%20incomplete%20knowledge%20graphs%2C%20which%20is%20a%20crucial%20yet%0Achallenging%20task%20in%20knowledge%20graph%20reasoning.%20While%20neuro-symbolic%20search%0Autilized%20neural%20link%20predictions%20achieve%20superior%20accuracy%2C%20they%20encounter%0Asignificant%20complexity%20bottlenecks%3A%20%28i%29%20Data%20complexity%20typically%20scales%0Aquadratically%20with%20the%20number%20of%20entities%20in%20the%20knowledge%20graph%2C%20and%20%28ii%29%0AQuery%20complexity%20becomes%20NP-hard%20for%20cyclic%20queries.%20Consequently%2C%20these%0Aapproaches%20struggle%20to%20effectively%20scale%20to%20larger%20knowledge%20graphs%20and%20more%0Acomplex%20queries.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20efficient%20and%0Ascalable%20symbolic%20search%20framework.%20First%2C%20we%20propose%20two%20constraint%20strategies%0Ato%20compute%20neural%20logical%20indices%20to%20reduce%20the%20domain%20of%20variables%2C%20thereby%0Adecreasing%20the%20data%20complexity%20of%20symbolic%20search.%20Additionally%2C%20we%20introduce%0Aan%20approximate%20algorithm%20based%20on%20local%20search%20to%20tackle%20the%20NP%20query%0Acomplexity%20of%20cyclic%20queries.%20Experiments%20on%20various%20CQA%20benchmarks%20demonstrate%0Athat%20our%20framework%20reduces%20the%20computational%20load%20of%20symbolic%20methods%20by%2090%5C%25%0Awhile%20maintaining%20nearly%20the%20same%20performance%2C%20thus%20alleviating%20both%20efficiency%0Aand%20scalability%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.08155v3&entry.124074799=Read"},
{"title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas", "author": "Yu Ying Chiu and Zhilin Wang and Sharan Maiya and Yejin Choi and Kyle Fish and Sydney Levine and Evan Hubinger", "abstract": "  Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.\n", "link": "http://arxiv.org/abs/2505.14633v1", "date": "2025-05-20", "relevancy": 1.2495, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4323}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4174}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Will%20AI%20Tell%20Lies%20to%20Save%20Sick%20Children%3F%20Litmus-Testing%20AI%20Values%0A%20%20Prioritization%20with%20AIRiskDilemmas&body=Title%3A%20Will%20AI%20Tell%20Lies%20to%20Save%20Sick%20Children%3F%20Litmus-Testing%20AI%20Values%0A%20%20Prioritization%20with%20AIRiskDilemmas%0AAuthor%3A%20Yu%20Ying%20Chiu%20and%20Zhilin%20Wang%20and%20Sharan%20Maiya%20and%20Yejin%20Choi%20and%20Kyle%20Fish%20and%20Sydney%20Levine%20and%20Evan%20Hubinger%0AAbstract%3A%20%20%20Detecting%20AI%20risks%20becomes%20more%20challenging%20as%20stronger%20models%20emerge%20and%0Afind%20novel%20methods%20such%20as%20Alignment%20Faking%20to%20circumvent%20these%20detection%0Aattempts.%20Inspired%20by%20how%20risky%20behaviors%20in%20humans%20%28i.e.%2C%20illegal%20activities%0Athat%20may%20hurt%20others%29%20are%20sometimes%20guided%20by%20strongly-held%20values%2C%20we%20believe%0Athat%20identifying%20values%20within%20AI%20models%20can%20be%20an%20early%20warning%20system%20for%0AAI%27s%20risky%20behaviors.%20We%20create%20LitmusValues%2C%20an%20evaluation%20pipeline%20to%20reveal%0AAI%20models%27%20priorities%20on%20a%20range%20of%20AI%20value%20classes.%20Then%2C%20we%20collect%0AAIRiskDilemmas%2C%20a%20diverse%20collection%20of%20dilemmas%20that%20pit%20values%20against%20one%0Aanother%20in%20scenarios%20relevant%20to%20AI%20safety%20risks%20such%20as%20Power%20Seeking.%20By%0Ameasuring%20an%20AI%20model%27s%20value%20prioritization%20using%20its%20aggregate%20choices%2C%20we%0Aobtain%20a%20self-consistent%20set%20of%20predicted%20value%20priorities%20that%20uncover%0Apotential%20risks.%20We%20show%20that%20values%20in%20LitmusValues%20%28including%20seemingly%0Ainnocuous%20ones%20like%20Care%29%20can%20predict%20for%20both%20seen%20risky%20behaviors%20in%0AAIRiskDilemmas%20and%20unseen%20risky%20behaviors%20in%20HarmBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWill%2520AI%2520Tell%2520Lies%2520to%2520Save%2520Sick%2520Children%253F%2520Litmus-Testing%2520AI%2520Values%250A%2520%2520Prioritization%2520with%2520AIRiskDilemmas%26entry.906535625%3DYu%2520Ying%2520Chiu%2520and%2520Zhilin%2520Wang%2520and%2520Sharan%2520Maiya%2520and%2520Yejin%2520Choi%2520and%2520Kyle%2520Fish%2520and%2520Sydney%2520Levine%2520and%2520Evan%2520Hubinger%26entry.1292438233%3D%2520%2520Detecting%2520AI%2520risks%2520becomes%2520more%2520challenging%2520as%2520stronger%2520models%2520emerge%2520and%250Afind%2520novel%2520methods%2520such%2520as%2520Alignment%2520Faking%2520to%2520circumvent%2520these%2520detection%250Aattempts.%2520Inspired%2520by%2520how%2520risky%2520behaviors%2520in%2520humans%2520%2528i.e.%252C%2520illegal%2520activities%250Athat%2520may%2520hurt%2520others%2529%2520are%2520sometimes%2520guided%2520by%2520strongly-held%2520values%252C%2520we%2520believe%250Athat%2520identifying%2520values%2520within%2520AI%2520models%2520can%2520be%2520an%2520early%2520warning%2520system%2520for%250AAI%2527s%2520risky%2520behaviors.%2520We%2520create%2520LitmusValues%252C%2520an%2520evaluation%2520pipeline%2520to%2520reveal%250AAI%2520models%2527%2520priorities%2520on%2520a%2520range%2520of%2520AI%2520value%2520classes.%2520Then%252C%2520we%2520collect%250AAIRiskDilemmas%252C%2520a%2520diverse%2520collection%2520of%2520dilemmas%2520that%2520pit%2520values%2520against%2520one%250Aanother%2520in%2520scenarios%2520relevant%2520to%2520AI%2520safety%2520risks%2520such%2520as%2520Power%2520Seeking.%2520By%250Ameasuring%2520an%2520AI%2520model%2527s%2520value%2520prioritization%2520using%2520its%2520aggregate%2520choices%252C%2520we%250Aobtain%2520a%2520self-consistent%2520set%2520of%2520predicted%2520value%2520priorities%2520that%2520uncover%250Apotential%2520risks.%2520We%2520show%2520that%2520values%2520in%2520LitmusValues%2520%2528including%2520seemingly%250Ainnocuous%2520ones%2520like%2520Care%2529%2520can%2520predict%2520for%2520both%2520seen%2520risky%2520behaviors%2520in%250AAIRiskDilemmas%2520and%2520unseen%2520risky%2520behaviors%2520in%2520HarmBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Will%20AI%20Tell%20Lies%20to%20Save%20Sick%20Children%3F%20Litmus-Testing%20AI%20Values%0A%20%20Prioritization%20with%20AIRiskDilemmas&entry.906535625=Yu%20Ying%20Chiu%20and%20Zhilin%20Wang%20and%20Sharan%20Maiya%20and%20Yejin%20Choi%20and%20Kyle%20Fish%20and%20Sydney%20Levine%20and%20Evan%20Hubinger&entry.1292438233=%20%20Detecting%20AI%20risks%20becomes%20more%20challenging%20as%20stronger%20models%20emerge%20and%0Afind%20novel%20methods%20such%20as%20Alignment%20Faking%20to%20circumvent%20these%20detection%0Aattempts.%20Inspired%20by%20how%20risky%20behaviors%20in%20humans%20%28i.e.%2C%20illegal%20activities%0Athat%20may%20hurt%20others%29%20are%20sometimes%20guided%20by%20strongly-held%20values%2C%20we%20believe%0Athat%20identifying%20values%20within%20AI%20models%20can%20be%20an%20early%20warning%20system%20for%0AAI%27s%20risky%20behaviors.%20We%20create%20LitmusValues%2C%20an%20evaluation%20pipeline%20to%20reveal%0AAI%20models%27%20priorities%20on%20a%20range%20of%20AI%20value%20classes.%20Then%2C%20we%20collect%0AAIRiskDilemmas%2C%20a%20diverse%20collection%20of%20dilemmas%20that%20pit%20values%20against%20one%0Aanother%20in%20scenarios%20relevant%20to%20AI%20safety%20risks%20such%20as%20Power%20Seeking.%20By%0Ameasuring%20an%20AI%20model%27s%20value%20prioritization%20using%20its%20aggregate%20choices%2C%20we%0Aobtain%20a%20self-consistent%20set%20of%20predicted%20value%20priorities%20that%20uncover%0Apotential%20risks.%20We%20show%20that%20values%20in%20LitmusValues%20%28including%20seemingly%0Ainnocuous%20ones%20like%20Care%29%20can%20predict%20for%20both%20seen%20risky%20behaviors%20in%0AAIRiskDilemmas%20and%20unseen%20risky%20behaviors%20in%20HarmBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14633v1&entry.124074799=Read"},
{"title": "Latent Flow Transformer", "author": "Yen-Chen Wu and Feng-Ting Liao and Meng-Hsi Chen and Pei-Chen Ho and Farhang Nabiei and Da-shan Shiu", "abstract": "  Transformers, the standard implementation for large language models (LLMs),\ntypically consist of tens to hundreds of discrete layers. While more layers can\nlead to better performance, this approach has been challenged as far from\nefficient, especially given the superiority of continuous layers demonstrated\nby diffusion and flow-based models for image generation. We propose the Latent\nFlow Transformer (LFT), which replaces a block of layers with a single learned\ntransport operator trained via flow matching, offering significant compression\nwhile maintaining compatibility with the original architecture. Additionally,\nwe address the limitations of existing flow-based methods in \\textit{preserving\ncoupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M\nmodel, LFT trained with flow matching compresses 6 of 24 layers and outperforms\ndirectly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),\ndemonstrating the feasibility of this design. When trained with FW, LFT further\ndistills 12 layers into one while reducing the KL to 0.736 surpassing that from\nskipping 3 layers (0.932), significantly narrowing the gap between\nautoregressive and flow-based generation paradigms.\n", "link": "http://arxiv.org/abs/2505.14513v1", "date": "2025-05-20", "relevancy": 1.6931, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.7075}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5695}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Flow%20Transformer&body=Title%3A%20Latent%20Flow%20Transformer%0AAuthor%3A%20Yen-Chen%20Wu%20and%20Feng-Ting%20Liao%20and%20Meng-Hsi%20Chen%20and%20Pei-Chen%20Ho%20and%20Farhang%20Nabiei%20and%20Da-shan%20Shiu%0AAbstract%3A%20%20%20Transformers%2C%20the%20standard%20implementation%20for%20large%20language%20models%20%28LLMs%29%2C%0Atypically%20consist%20of%20tens%20to%20hundreds%20of%20discrete%20layers.%20While%20more%20layers%20can%0Alead%20to%20better%20performance%2C%20this%20approach%20has%20been%20challenged%20as%20far%20from%0Aefficient%2C%20especially%20given%20the%20superiority%20of%20continuous%20layers%20demonstrated%0Aby%20diffusion%20and%20flow-based%20models%20for%20image%20generation.%20We%20propose%20the%20Latent%0AFlow%20Transformer%20%28LFT%29%2C%20which%20replaces%20a%20block%20of%20layers%20with%20a%20single%20learned%0Atransport%20operator%20trained%20via%20flow%20matching%2C%20offering%20significant%20compression%0Awhile%20maintaining%20compatibility%20with%20the%20original%20architecture.%20Additionally%2C%0Awe%20address%20the%20limitations%20of%20existing%20flow-based%20methods%20in%20%5Ctextit%7Bpreserving%0Acoupling%7D%20by%20introducing%20the%20Flow%20Walking%20%28FW%29%20algorithm.%20On%20the%20Pythia-410M%0Amodel%2C%20LFT%20trained%20with%20flow%20matching%20compresses%206%20of%2024%20layers%20and%20outperforms%0Adirectly%20skipping%202%20layers%20%28KL%20Divergence%20of%20LM%20logits%20at%200.407%20vs.%200.529%29%2C%0Ademonstrating%20the%20feasibility%20of%20this%20design.%20When%20trained%20with%20FW%2C%20LFT%20further%0Adistills%2012%20layers%20into%20one%20while%20reducing%20the%20KL%20to%200.736%20surpassing%20that%20from%0Askipping%203%20layers%20%280.932%29%2C%20significantly%20narrowing%20the%20gap%20between%0Aautoregressive%20and%20flow-based%20generation%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Flow%2520Transformer%26entry.906535625%3DYen-Chen%2520Wu%2520and%2520Feng-Ting%2520Liao%2520and%2520Meng-Hsi%2520Chen%2520and%2520Pei-Chen%2520Ho%2520and%2520Farhang%2520Nabiei%2520and%2520Da-shan%2520Shiu%26entry.1292438233%3D%2520%2520Transformers%252C%2520the%2520standard%2520implementation%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Atypically%2520consist%2520of%2520tens%2520to%2520hundreds%2520of%2520discrete%2520layers.%2520While%2520more%2520layers%2520can%250Alead%2520to%2520better%2520performance%252C%2520this%2520approach%2520has%2520been%2520challenged%2520as%2520far%2520from%250Aefficient%252C%2520especially%2520given%2520the%2520superiority%2520of%2520continuous%2520layers%2520demonstrated%250Aby%2520diffusion%2520and%2520flow-based%2520models%2520for%2520image%2520generation.%2520We%2520propose%2520the%2520Latent%250AFlow%2520Transformer%2520%2528LFT%2529%252C%2520which%2520replaces%2520a%2520block%2520of%2520layers%2520with%2520a%2520single%2520learned%250Atransport%2520operator%2520trained%2520via%2520flow%2520matching%252C%2520offering%2520significant%2520compression%250Awhile%2520maintaining%2520compatibility%2520with%2520the%2520original%2520architecture.%2520Additionally%252C%250Awe%2520address%2520the%2520limitations%2520of%2520existing%2520flow-based%2520methods%2520in%2520%255Ctextit%257Bpreserving%250Acoupling%257D%2520by%2520introducing%2520the%2520Flow%2520Walking%2520%2528FW%2529%2520algorithm.%2520On%2520the%2520Pythia-410M%250Amodel%252C%2520LFT%2520trained%2520with%2520flow%2520matching%2520compresses%25206%2520of%252024%2520layers%2520and%2520outperforms%250Adirectly%2520skipping%25202%2520layers%2520%2528KL%2520Divergence%2520of%2520LM%2520logits%2520at%25200.407%2520vs.%25200.529%2529%252C%250Ademonstrating%2520the%2520feasibility%2520of%2520this%2520design.%2520When%2520trained%2520with%2520FW%252C%2520LFT%2520further%250Adistills%252012%2520layers%2520into%2520one%2520while%2520reducing%2520the%2520KL%2520to%25200.736%2520surpassing%2520that%2520from%250Askipping%25203%2520layers%2520%25280.932%2529%252C%2520significantly%2520narrowing%2520the%2520gap%2520between%250Aautoregressive%2520and%2520flow-based%2520generation%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Flow%20Transformer&entry.906535625=Yen-Chen%20Wu%20and%20Feng-Ting%20Liao%20and%20Meng-Hsi%20Chen%20and%20Pei-Chen%20Ho%20and%20Farhang%20Nabiei%20and%20Da-shan%20Shiu&entry.1292438233=%20%20Transformers%2C%20the%20standard%20implementation%20for%20large%20language%20models%20%28LLMs%29%2C%0Atypically%20consist%20of%20tens%20to%20hundreds%20of%20discrete%20layers.%20While%20more%20layers%20can%0Alead%20to%20better%20performance%2C%20this%20approach%20has%20been%20challenged%20as%20far%20from%0Aefficient%2C%20especially%20given%20the%20superiority%20of%20continuous%20layers%20demonstrated%0Aby%20diffusion%20and%20flow-based%20models%20for%20image%20generation.%20We%20propose%20the%20Latent%0AFlow%20Transformer%20%28LFT%29%2C%20which%20replaces%20a%20block%20of%20layers%20with%20a%20single%20learned%0Atransport%20operator%20trained%20via%20flow%20matching%2C%20offering%20significant%20compression%0Awhile%20maintaining%20compatibility%20with%20the%20original%20architecture.%20Additionally%2C%0Awe%20address%20the%20limitations%20of%20existing%20flow-based%20methods%20in%20%5Ctextit%7Bpreserving%0Acoupling%7D%20by%20introducing%20the%20Flow%20Walking%20%28FW%29%20algorithm.%20On%20the%20Pythia-410M%0Amodel%2C%20LFT%20trained%20with%20flow%20matching%20compresses%206%20of%2024%20layers%20and%20outperforms%0Adirectly%20skipping%202%20layers%20%28KL%20Divergence%20of%20LM%20logits%20at%200.407%20vs.%200.529%29%2C%0Ademonstrating%20the%20feasibility%20of%20this%20design.%20When%20trained%20with%20FW%2C%20LFT%20further%0Adistills%2012%20layers%20into%20one%20while%20reducing%20the%20KL%20to%200.736%20surpassing%20that%20from%0Askipping%203%20layers%20%280.932%29%2C%20significantly%20narrowing%20the%20gap%20between%0Aautoregressive%20and%20flow-based%20generation%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14513v1&entry.124074799=Read"},
{"title": "Can LLMs be Good Graph Judge for Knowledge Graph Construction?", "author": "Haoyu Huang and Chong Chen and Zeang Sheng and Yang Li and Wentao Zhang", "abstract": "  In real-world scenarios, most of the data obtained from the information\nretrieval (IR) system is unstructured. Converting natural language sentences\ninto structured Knowledge Graphs (KGs) remains a critical challenge. We\nidentified three limitations with respect to existing KG construction methods:\n(1) There could be a large amount of noise in real-world documents, which could\nresult in extracting messy information. (2) Naive LLMs usually extract\ninaccurate knowledge from some domain-specific documents. (3) Hallucination\nphenomenon cannot be overlooked when directly using LLMs to construct KGs. In\nthis paper, we propose \\textbf{GraphJudge}, a KG construction framework to\naddress the aforementioned challenges. In this framework, we designed an\nentity-centric strategy to eliminate the noise information in the documents.\nAnd we fine-tuned a LLM as a graph judge to finally enhance the quality of\ngenerated KGs. Experiments conducted on two general and one domain-specific\ntext-graph pair datasets demonstrate state-of-the-art performance against\nvarious baseline methods with strong generalization abilities. Our code is\navailable at\n\\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.\n", "link": "http://arxiv.org/abs/2411.17388v3", "date": "2025-05-20", "relevancy": 1.8143, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4721}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4515}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20be%20Good%20Graph%20Judge%20for%20Knowledge%20Graph%20Construction%3F&body=Title%3A%20Can%20LLMs%20be%20Good%20Graph%20Judge%20for%20Knowledge%20Graph%20Construction%3F%0AAuthor%3A%20Haoyu%20Huang%20and%20Chong%20Chen%20and%20Zeang%20Sheng%20and%20Yang%20Li%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20In%20real-world%20scenarios%2C%20most%20of%20the%20data%20obtained%20from%20the%20information%0Aretrieval%20%28IR%29%20system%20is%20unstructured.%20Converting%20natural%20language%20sentences%0Ainto%20structured%20Knowledge%20Graphs%20%28KGs%29%20remains%20a%20critical%20challenge.%20We%0Aidentified%20three%20limitations%20with%20respect%20to%20existing%20KG%20construction%20methods%3A%0A%281%29%20There%20could%20be%20a%20large%20amount%20of%20noise%20in%20real-world%20documents%2C%20which%20could%0Aresult%20in%20extracting%20messy%20information.%20%282%29%20Naive%20LLMs%20usually%20extract%0Ainaccurate%20knowledge%20from%20some%20domain-specific%20documents.%20%283%29%20Hallucination%0Aphenomenon%20cannot%20be%20overlooked%20when%20directly%20using%20LLMs%20to%20construct%20KGs.%20In%0Athis%20paper%2C%20we%20propose%20%5Ctextbf%7BGraphJudge%7D%2C%20a%20KG%20construction%20framework%20to%0Aaddress%20the%20aforementioned%20challenges.%20In%20this%20framework%2C%20we%20designed%20an%0Aentity-centric%20strategy%20to%20eliminate%20the%20noise%20information%20in%20the%20documents.%0AAnd%20we%20fine-tuned%20a%20LLM%20as%20a%20graph%20judge%20to%20finally%20enhance%20the%20quality%20of%0Agenerated%20KGs.%20Experiments%20conducted%20on%20two%20general%20and%20one%20domain-specific%0Atext-graph%20pair%20datasets%20demonstrate%20state-of-the-art%20performance%20against%0Avarious%20baseline%20methods%20with%20strong%20generalization%20abilities.%20Our%20code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/hhy-huang/GraphJudge%7D%7Bhttps%3A//github.com/hhy-huang/GraphJudge%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17388v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520be%2520Good%2520Graph%2520Judge%2520for%2520Knowledge%2520Graph%2520Construction%253F%26entry.906535625%3DHaoyu%2520Huang%2520and%2520Chong%2520Chen%2520and%2520Zeang%2520Sheng%2520and%2520Yang%2520Li%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520In%2520real-world%2520scenarios%252C%2520most%2520of%2520the%2520data%2520obtained%2520from%2520the%2520information%250Aretrieval%2520%2528IR%2529%2520system%2520is%2520unstructured.%2520Converting%2520natural%2520language%2520sentences%250Ainto%2520structured%2520Knowledge%2520Graphs%2520%2528KGs%2529%2520remains%2520a%2520critical%2520challenge.%2520We%250Aidentified%2520three%2520limitations%2520with%2520respect%2520to%2520existing%2520KG%2520construction%2520methods%253A%250A%25281%2529%2520There%2520could%2520be%2520a%2520large%2520amount%2520of%2520noise%2520in%2520real-world%2520documents%252C%2520which%2520could%250Aresult%2520in%2520extracting%2520messy%2520information.%2520%25282%2529%2520Naive%2520LLMs%2520usually%2520extract%250Ainaccurate%2520knowledge%2520from%2520some%2520domain-specific%2520documents.%2520%25283%2529%2520Hallucination%250Aphenomenon%2520cannot%2520be%2520overlooked%2520when%2520directly%2520using%2520LLMs%2520to%2520construct%2520KGs.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BGraphJudge%257D%252C%2520a%2520KG%2520construction%2520framework%2520to%250Aaddress%2520the%2520aforementioned%2520challenges.%2520In%2520this%2520framework%252C%2520we%2520designed%2520an%250Aentity-centric%2520strategy%2520to%2520eliminate%2520the%2520noise%2520information%2520in%2520the%2520documents.%250AAnd%2520we%2520fine-tuned%2520a%2520LLM%2520as%2520a%2520graph%2520judge%2520to%2520finally%2520enhance%2520the%2520quality%2520of%250Agenerated%2520KGs.%2520Experiments%2520conducted%2520on%2520two%2520general%2520and%2520one%2520domain-specific%250Atext-graph%2520pair%2520datasets%2520demonstrate%2520state-of-the-art%2520performance%2520against%250Avarious%2520baseline%2520methods%2520with%2520strong%2520generalization%2520abilities.%2520Our%2520code%2520is%250Aavailable%2520at%250A%255Chref%257Bhttps%253A//github.com/hhy-huang/GraphJudge%257D%257Bhttps%253A//github.com/hhy-huang/GraphJudge%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17388v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20be%20Good%20Graph%20Judge%20for%20Knowledge%20Graph%20Construction%3F&entry.906535625=Haoyu%20Huang%20and%20Chong%20Chen%20and%20Zeang%20Sheng%20and%20Yang%20Li%20and%20Wentao%20Zhang&entry.1292438233=%20%20In%20real-world%20scenarios%2C%20most%20of%20the%20data%20obtained%20from%20the%20information%0Aretrieval%20%28IR%29%20system%20is%20unstructured.%20Converting%20natural%20language%20sentences%0Ainto%20structured%20Knowledge%20Graphs%20%28KGs%29%20remains%20a%20critical%20challenge.%20We%0Aidentified%20three%20limitations%20with%20respect%20to%20existing%20KG%20construction%20methods%3A%0A%281%29%20There%20could%20be%20a%20large%20amount%20of%20noise%20in%20real-world%20documents%2C%20which%20could%0Aresult%20in%20extracting%20messy%20information.%20%282%29%20Naive%20LLMs%20usually%20extract%0Ainaccurate%20knowledge%20from%20some%20domain-specific%20documents.%20%283%29%20Hallucination%0Aphenomenon%20cannot%20be%20overlooked%20when%20directly%20using%20LLMs%20to%20construct%20KGs.%20In%0Athis%20paper%2C%20we%20propose%20%5Ctextbf%7BGraphJudge%7D%2C%20a%20KG%20construction%20framework%20to%0Aaddress%20the%20aforementioned%20challenges.%20In%20this%20framework%2C%20we%20designed%20an%0Aentity-centric%20strategy%20to%20eliminate%20the%20noise%20information%20in%20the%20documents.%0AAnd%20we%20fine-tuned%20a%20LLM%20as%20a%20graph%20judge%20to%20finally%20enhance%20the%20quality%20of%0Agenerated%20KGs.%20Experiments%20conducted%20on%20two%20general%20and%20one%20domain-specific%0Atext-graph%20pair%20datasets%20demonstrate%20state-of-the-art%20performance%20against%0Avarious%20baseline%20methods%20with%20strong%20generalization%20abilities.%20Our%20code%20is%0Aavailable%20at%0A%5Chref%7Bhttps%3A//github.com/hhy-huang/GraphJudge%7D%7Bhttps%3A//github.com/hhy-huang/GraphJudge%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17388v3&entry.124074799=Read"},
{"title": "RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual\n  Data", "author": "Yoorhim Cho and Hongyeob Kim and Semin Kim and Youjia Zhang and Yunseok Choi and Sungeun Hong", "abstract": "  Visuo-tactile perception aims to understand an object's tactile properties,\nsuch as texture, softness, and rigidity. However, the field remains\nunderexplored because collecting tactile data is costly and labor-intensive. We\nobserve that visually distinct objects can exhibit similar surface textures or\nmaterial properties. For example, a leather sofa and a leather jacket have\ndifferent appearances but share similar tactile properties. This implies that\ntactile understanding can be guided by material cues in visual data, even\nwithout direct tactile supervision. In this paper, we introduce RA-Touch, a\nretrieval-augmented framework that improves visuo-tactile perception by\nleveraging visual data enriched with tactile semantics. We carefully recaption\na large-scale visual dataset with tactile-focused descriptions, enabling the\nmodel to access tactile semantics typically absent from conventional visual\ndatasets. A key challenge remains in effectively utilizing these tactile-aware\nexternal descriptions. RA-Touch addresses this by retrieving visual-textual\nrepresentations aligned with tactile inputs and integrating them to focus on\nrelevant textural and material properties. By outperforming prior methods on\nthe TVL benchmark, our method demonstrates the potential of retrieval-based\nvisual reuse for tactile understanding. Code is available at\nhttps://aim-skku.github.io/RA-Touch\n", "link": "http://arxiv.org/abs/2505.14270v1", "date": "2025-05-20", "relevancy": 2.1116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RA-Touch%3A%20Retrieval-Augmented%20Touch%20Understanding%20with%20Enriched%20Visual%0A%20%20Data&body=Title%3A%20RA-Touch%3A%20Retrieval-Augmented%20Touch%20Understanding%20with%20Enriched%20Visual%0A%20%20Data%0AAuthor%3A%20Yoorhim%20Cho%20and%20Hongyeob%20Kim%20and%20Semin%20Kim%20and%20Youjia%20Zhang%20and%20Yunseok%20Choi%20and%20Sungeun%20Hong%0AAbstract%3A%20%20%20Visuo-tactile%20perception%20aims%20to%20understand%20an%20object%27s%20tactile%20properties%2C%0Asuch%20as%20texture%2C%20softness%2C%20and%20rigidity.%20However%2C%20the%20field%20remains%0Aunderexplored%20because%20collecting%20tactile%20data%20is%20costly%20and%20labor-intensive.%20We%0Aobserve%20that%20visually%20distinct%20objects%20can%20exhibit%20similar%20surface%20textures%20or%0Amaterial%20properties.%20For%20example%2C%20a%20leather%20sofa%20and%20a%20leather%20jacket%20have%0Adifferent%20appearances%20but%20share%20similar%20tactile%20properties.%20This%20implies%20that%0Atactile%20understanding%20can%20be%20guided%20by%20material%20cues%20in%20visual%20data%2C%20even%0Awithout%20direct%20tactile%20supervision.%20In%20this%20paper%2C%20we%20introduce%20RA-Touch%2C%20a%0Aretrieval-augmented%20framework%20that%20improves%20visuo-tactile%20perception%20by%0Aleveraging%20visual%20data%20enriched%20with%20tactile%20semantics.%20We%20carefully%20recaption%0Aa%20large-scale%20visual%20dataset%20with%20tactile-focused%20descriptions%2C%20enabling%20the%0Amodel%20to%20access%20tactile%20semantics%20typically%20absent%20from%20conventional%20visual%0Adatasets.%20A%20key%20challenge%20remains%20in%20effectively%20utilizing%20these%20tactile-aware%0Aexternal%20descriptions.%20RA-Touch%20addresses%20this%20by%20retrieving%20visual-textual%0Arepresentations%20aligned%20with%20tactile%20inputs%20and%20integrating%20them%20to%20focus%20on%0Arelevant%20textural%20and%20material%20properties.%20By%20outperforming%20prior%20methods%20on%0Athe%20TVL%20benchmark%2C%20our%20method%20demonstrates%20the%20potential%20of%20retrieval-based%0Avisual%20reuse%20for%20tactile%20understanding.%20Code%20is%20available%20at%0Ahttps%3A//aim-skku.github.io/RA-Touch%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRA-Touch%253A%2520Retrieval-Augmented%2520Touch%2520Understanding%2520with%2520Enriched%2520Visual%250A%2520%2520Data%26entry.906535625%3DYoorhim%2520Cho%2520and%2520Hongyeob%2520Kim%2520and%2520Semin%2520Kim%2520and%2520Youjia%2520Zhang%2520and%2520Yunseok%2520Choi%2520and%2520Sungeun%2520Hong%26entry.1292438233%3D%2520%2520Visuo-tactile%2520perception%2520aims%2520to%2520understand%2520an%2520object%2527s%2520tactile%2520properties%252C%250Asuch%2520as%2520texture%252C%2520softness%252C%2520and%2520rigidity.%2520However%252C%2520the%2520field%2520remains%250Aunderexplored%2520because%2520collecting%2520tactile%2520data%2520is%2520costly%2520and%2520labor-intensive.%2520We%250Aobserve%2520that%2520visually%2520distinct%2520objects%2520can%2520exhibit%2520similar%2520surface%2520textures%2520or%250Amaterial%2520properties.%2520For%2520example%252C%2520a%2520leather%2520sofa%2520and%2520a%2520leather%2520jacket%2520have%250Adifferent%2520appearances%2520but%2520share%2520similar%2520tactile%2520properties.%2520This%2520implies%2520that%250Atactile%2520understanding%2520can%2520be%2520guided%2520by%2520material%2520cues%2520in%2520visual%2520data%252C%2520even%250Awithout%2520direct%2520tactile%2520supervision.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520RA-Touch%252C%2520a%250Aretrieval-augmented%2520framework%2520that%2520improves%2520visuo-tactile%2520perception%2520by%250Aleveraging%2520visual%2520data%2520enriched%2520with%2520tactile%2520semantics.%2520We%2520carefully%2520recaption%250Aa%2520large-scale%2520visual%2520dataset%2520with%2520tactile-focused%2520descriptions%252C%2520enabling%2520the%250Amodel%2520to%2520access%2520tactile%2520semantics%2520typically%2520absent%2520from%2520conventional%2520visual%250Adatasets.%2520A%2520key%2520challenge%2520remains%2520in%2520effectively%2520utilizing%2520these%2520tactile-aware%250Aexternal%2520descriptions.%2520RA-Touch%2520addresses%2520this%2520by%2520retrieving%2520visual-textual%250Arepresentations%2520aligned%2520with%2520tactile%2520inputs%2520and%2520integrating%2520them%2520to%2520focus%2520on%250Arelevant%2520textural%2520and%2520material%2520properties.%2520By%2520outperforming%2520prior%2520methods%2520on%250Athe%2520TVL%2520benchmark%252C%2520our%2520method%2520demonstrates%2520the%2520potential%2520of%2520retrieval-based%250Avisual%2520reuse%2520for%2520tactile%2520understanding.%2520Code%2520is%2520available%2520at%250Ahttps%253A//aim-skku.github.io/RA-Touch%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RA-Touch%3A%20Retrieval-Augmented%20Touch%20Understanding%20with%20Enriched%20Visual%0A%20%20Data&entry.906535625=Yoorhim%20Cho%20and%20Hongyeob%20Kim%20and%20Semin%20Kim%20and%20Youjia%20Zhang%20and%20Yunseok%20Choi%20and%20Sungeun%20Hong&entry.1292438233=%20%20Visuo-tactile%20perception%20aims%20to%20understand%20an%20object%27s%20tactile%20properties%2C%0Asuch%20as%20texture%2C%20softness%2C%20and%20rigidity.%20However%2C%20the%20field%20remains%0Aunderexplored%20because%20collecting%20tactile%20data%20is%20costly%20and%20labor-intensive.%20We%0Aobserve%20that%20visually%20distinct%20objects%20can%20exhibit%20similar%20surface%20textures%20or%0Amaterial%20properties.%20For%20example%2C%20a%20leather%20sofa%20and%20a%20leather%20jacket%20have%0Adifferent%20appearances%20but%20share%20similar%20tactile%20properties.%20This%20implies%20that%0Atactile%20understanding%20can%20be%20guided%20by%20material%20cues%20in%20visual%20data%2C%20even%0Awithout%20direct%20tactile%20supervision.%20In%20this%20paper%2C%20we%20introduce%20RA-Touch%2C%20a%0Aretrieval-augmented%20framework%20that%20improves%20visuo-tactile%20perception%20by%0Aleveraging%20visual%20data%20enriched%20with%20tactile%20semantics.%20We%20carefully%20recaption%0Aa%20large-scale%20visual%20dataset%20with%20tactile-focused%20descriptions%2C%20enabling%20the%0Amodel%20to%20access%20tactile%20semantics%20typically%20absent%20from%20conventional%20visual%0Adatasets.%20A%20key%20challenge%20remains%20in%20effectively%20utilizing%20these%20tactile-aware%0Aexternal%20descriptions.%20RA-Touch%20addresses%20this%20by%20retrieving%20visual-textual%0Arepresentations%20aligned%20with%20tactile%20inputs%20and%20integrating%20them%20to%20focus%20on%0Arelevant%20textural%20and%20material%20properties.%20By%20outperforming%20prior%20methods%20on%0Athe%20TVL%20benchmark%2C%20our%20method%20demonstrates%20the%20potential%20of%20retrieval-based%0Avisual%20reuse%20for%20tactile%20understanding.%20Code%20is%20available%20at%0Ahttps%3A//aim-skku.github.io/RA-Touch%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14270v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


