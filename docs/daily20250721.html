<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250720.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations", "author": "Yu Wei and Jiahui Zhang and Xiaoqin Zhang and Ling Shao and Shijian Lu", "abstract": "  COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.\n", "link": "http://arxiv.org/abs/2507.13891v1", "date": "2025-07-18", "relevancy": 3.2259, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6781}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCR-GS%3A%20COLMAP-Free%203D%20Gaussian%20Splatting%20via%20Pose%20Co-Regularizations&body=Title%3A%20PCR-GS%3A%20COLMAP-Free%203D%20Gaussian%20Splatting%20via%20Pose%20Co-Regularizations%0AAuthor%3A%20Yu%20Wei%20and%20Jiahui%20Zhang%20and%20Xiaoqin%20Zhang%20and%20Ling%20Shao%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20COLMAP-free%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20recently%20attracted%20increasing%0Aattention%20due%20to%20its%20remarkable%20performance%20in%20reconstructing%20high-quality%203D%0Ascenes%20from%20unposed%20images%20or%20videos.%20However%2C%20it%20often%20struggles%20to%20handle%0Ascenes%20with%20complex%20camera%20trajectories%20as%20featured%20by%20drastic%20rotation%20and%0Atranslation%20across%20adjacent%20camera%20views%2C%20leading%20to%20degraded%20estimation%20of%0Acamera%20poses%20and%20further%20local%20minima%20in%20joint%20optimization%20of%20camera%20poses%20and%0A3D-GS.%20We%20propose%20PCR-GS%2C%20an%20innovative%20COLMAP-free%203DGS%20technique%20that%0Aachieves%20superior%203D%20scene%20modeling%20and%20camera%20pose%20estimation%20via%20camera%20pose%0Aco-regularization.%20PCR-GS%20achieves%20regularization%20from%20two%20perspectives.%20The%0Afirst%20is%20feature%20reprojection%20regularization%20which%20extracts%20view-robust%20DINO%0Afeatures%20from%20adjacent%20camera%20views%20and%20aligns%20their%20semantic%20information%20for%0Acamera%20pose%20regularization.%20The%20second%20is%20wavelet-based%20frequency%0Aregularization%20which%20exploits%20discrepancy%20in%20high-frequency%20details%20to%20further%0Aoptimize%20the%20rotation%20matrix%20in%20camera%20poses.%20Extensive%20experiments%20over%0Amultiple%20real-world%20scenes%20show%20that%20the%20proposed%20PCR-GS%20achieves%20superior%0Apose-free%203D-GS%20scene%20modeling%20under%20dramatic%20changes%20of%20camera%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCR-GS%253A%2520COLMAP-Free%25203D%2520Gaussian%2520Splatting%2520via%2520Pose%2520Co-Regularizations%26entry.906535625%3DYu%2520Wei%2520and%2520Jiahui%2520Zhang%2520and%2520Xiaoqin%2520Zhang%2520and%2520Ling%2520Shao%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520COLMAP-free%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520has%2520recently%2520attracted%2520increasing%250Aattention%2520due%2520to%2520its%2520remarkable%2520performance%2520in%2520reconstructing%2520high-quality%25203D%250Ascenes%2520from%2520unposed%2520images%2520or%2520videos.%2520However%252C%2520it%2520often%2520struggles%2520to%2520handle%250Ascenes%2520with%2520complex%2520camera%2520trajectories%2520as%2520featured%2520by%2520drastic%2520rotation%2520and%250Atranslation%2520across%2520adjacent%2520camera%2520views%252C%2520leading%2520to%2520degraded%2520estimation%2520of%250Acamera%2520poses%2520and%2520further%2520local%2520minima%2520in%2520joint%2520optimization%2520of%2520camera%2520poses%2520and%250A3D-GS.%2520We%2520propose%2520PCR-GS%252C%2520an%2520innovative%2520COLMAP-free%25203DGS%2520technique%2520that%250Aachieves%2520superior%25203D%2520scene%2520modeling%2520and%2520camera%2520pose%2520estimation%2520via%2520camera%2520pose%250Aco-regularization.%2520PCR-GS%2520achieves%2520regularization%2520from%2520two%2520perspectives.%2520The%250Afirst%2520is%2520feature%2520reprojection%2520regularization%2520which%2520extracts%2520view-robust%2520DINO%250Afeatures%2520from%2520adjacent%2520camera%2520views%2520and%2520aligns%2520their%2520semantic%2520information%2520for%250Acamera%2520pose%2520regularization.%2520The%2520second%2520is%2520wavelet-based%2520frequency%250Aregularization%2520which%2520exploits%2520discrepancy%2520in%2520high-frequency%2520details%2520to%2520further%250Aoptimize%2520the%2520rotation%2520matrix%2520in%2520camera%2520poses.%2520Extensive%2520experiments%2520over%250Amultiple%2520real-world%2520scenes%2520show%2520that%2520the%2520proposed%2520PCR-GS%2520achieves%2520superior%250Apose-free%25203D-GS%2520scene%2520modeling%2520under%2520dramatic%2520changes%2520of%2520camera%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCR-GS%3A%20COLMAP-Free%203D%20Gaussian%20Splatting%20via%20Pose%20Co-Regularizations&entry.906535625=Yu%20Wei%20and%20Jiahui%20Zhang%20and%20Xiaoqin%20Zhang%20and%20Ling%20Shao%20and%20Shijian%20Lu&entry.1292438233=%20%20COLMAP-free%203D%20Gaussian%20Splatting%20%283D-GS%29%20has%20recently%20attracted%20increasing%0Aattention%20due%20to%20its%20remarkable%20performance%20in%20reconstructing%20high-quality%203D%0Ascenes%20from%20unposed%20images%20or%20videos.%20However%2C%20it%20often%20struggles%20to%20handle%0Ascenes%20with%20complex%20camera%20trajectories%20as%20featured%20by%20drastic%20rotation%20and%0Atranslation%20across%20adjacent%20camera%20views%2C%20leading%20to%20degraded%20estimation%20of%0Acamera%20poses%20and%20further%20local%20minima%20in%20joint%20optimization%20of%20camera%20poses%20and%0A3D-GS.%20We%20propose%20PCR-GS%2C%20an%20innovative%20COLMAP-free%203DGS%20technique%20that%0Aachieves%20superior%203D%20scene%20modeling%20and%20camera%20pose%20estimation%20via%20camera%20pose%0Aco-regularization.%20PCR-GS%20achieves%20regularization%20from%20two%20perspectives.%20The%0Afirst%20is%20feature%20reprojection%20regularization%20which%20extracts%20view-robust%20DINO%0Afeatures%20from%20adjacent%20camera%20views%20and%20aligns%20their%20semantic%20information%20for%0Acamera%20pose%20regularization.%20The%20second%20is%20wavelet-based%20frequency%0Aregularization%20which%20exploits%20discrepancy%20in%20high-frequency%20details%20to%20further%0Aoptimize%20the%20rotation%20matrix%20in%20camera%20poses.%20Extensive%20experiments%20over%0Amultiple%20real-world%20scenes%20show%20that%20the%20proposed%20PCR-GS%20achieves%20superior%0Apose-free%203D-GS%20scene%20modeling%20under%20dramatic%20changes%20of%20camera%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13891v1&entry.124074799=Read"},
{"title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D\n  Object Detection", "author": "Yujian Mo and Yan Wu and Junqiao Zhao and Jijun Wang and Yinghao Hu and Jun Yan", "abstract": "  Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.\n", "link": "http://arxiv.org/abs/2507.13899v1", "date": "2025-07-18", "relevancy": 3.1288, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.631}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20LiDAR%20Point%20Features%20with%20Foundation%20Model%20Priors%20for%203D%0A%20%20Object%20Detection&body=Title%3A%20Enhancing%20LiDAR%20Point%20Features%20with%20Foundation%20Model%20Priors%20for%203D%0A%20%20Object%20Detection%0AAuthor%3A%20Yujian%20Mo%20and%20Yan%20Wu%20and%20Junqiao%20Zhao%20and%20Jijun%20Wang%20and%20Yinghao%20Hu%20and%20Jun%20Yan%0AAbstract%3A%20%20%20Recent%20advances%20in%20foundation%20models%20have%20opened%20up%20new%20possibilities%20for%0Aenhancing%203D%20perception.%20In%20particular%2C%20DepthAnything%20offers%20dense%20and%20reliable%0Ageometric%20priors%20from%20monocular%20RGB%20images%2C%20which%20can%20complement%20sparse%20LiDAR%0Adata%20in%20autonomous%20driving%20scenarios.%20However%2C%20such%20priors%20remain%20underutilized%0Ain%20LiDAR-based%203D%20object%20detection.%20In%20this%20paper%2C%20we%20address%20the%20limited%0Aexpressiveness%20of%20raw%20LiDAR%20point%20features%2C%20especially%20the%20weak%20discriminative%0Acapability%20of%20the%20reflectance%20attribute%2C%20by%20introducing%20depth%20priors%20predicted%0Aby%20DepthAnything.%20These%20priors%20are%20fused%20with%20the%20original%20LiDAR%20attributes%20to%0Aenrich%20each%20point%27s%20representation.%20To%20leverage%20the%20enhanced%20point%20features%2C%20we%0Apropose%20a%20point-wise%20feature%20extraction%20module.%20Then%2C%20a%20Dual-Path%20RoI%20feature%0Aextraction%20framework%20is%20employed%2C%20comprising%20a%20voxel-based%20branch%20for%20global%0Asemantic%20context%20and%20a%20point-based%20branch%20for%20fine-grained%20structural%20details.%0ATo%20effectively%20integrate%20the%20complementary%20RoI%20features%2C%20we%20introduce%20a%0Abidirectional%20gated%20RoI%20feature%20fusion%20module%20that%20balances%20global%20and%20local%0Acues.%20Extensive%20experiments%20on%20the%20KITTI%20benchmark%20show%20that%20our%20method%0Aconsistently%20improves%20detection%20accuracy%2C%20demonstrating%20the%20value%20of%0Aincorporating%20visual%20foundation%20model%20priors%20into%20LiDAR-based%203D%20object%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520LiDAR%2520Point%2520Features%2520with%2520Foundation%2520Model%2520Priors%2520for%25203D%250A%2520%2520Object%2520Detection%26entry.906535625%3DYujian%2520Mo%2520and%2520Yan%2520Wu%2520and%2520Junqiao%2520Zhao%2520and%2520Jijun%2520Wang%2520and%2520Yinghao%2520Hu%2520and%2520Jun%2520Yan%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520foundation%2520models%2520have%2520opened%2520up%2520new%2520possibilities%2520for%250Aenhancing%25203D%2520perception.%2520In%2520particular%252C%2520DepthAnything%2520offers%2520dense%2520and%2520reliable%250Ageometric%2520priors%2520from%2520monocular%2520RGB%2520images%252C%2520which%2520can%2520complement%2520sparse%2520LiDAR%250Adata%2520in%2520autonomous%2520driving%2520scenarios.%2520However%252C%2520such%2520priors%2520remain%2520underutilized%250Ain%2520LiDAR-based%25203D%2520object%2520detection.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520limited%250Aexpressiveness%2520of%2520raw%2520LiDAR%2520point%2520features%252C%2520especially%2520the%2520weak%2520discriminative%250Acapability%2520of%2520the%2520reflectance%2520attribute%252C%2520by%2520introducing%2520depth%2520priors%2520predicted%250Aby%2520DepthAnything.%2520These%2520priors%2520are%2520fused%2520with%2520the%2520original%2520LiDAR%2520attributes%2520to%250Aenrich%2520each%2520point%2527s%2520representation.%2520To%2520leverage%2520the%2520enhanced%2520point%2520features%252C%2520we%250Apropose%2520a%2520point-wise%2520feature%2520extraction%2520module.%2520Then%252C%2520a%2520Dual-Path%2520RoI%2520feature%250Aextraction%2520framework%2520is%2520employed%252C%2520comprising%2520a%2520voxel-based%2520branch%2520for%2520global%250Asemantic%2520context%2520and%2520a%2520point-based%2520branch%2520for%2520fine-grained%2520structural%2520details.%250ATo%2520effectively%2520integrate%2520the%2520complementary%2520RoI%2520features%252C%2520we%2520introduce%2520a%250Abidirectional%2520gated%2520RoI%2520feature%2520fusion%2520module%2520that%2520balances%2520global%2520and%2520local%250Acues.%2520Extensive%2520experiments%2520on%2520the%2520KITTI%2520benchmark%2520show%2520that%2520our%2520method%250Aconsistently%2520improves%2520detection%2520accuracy%252C%2520demonstrating%2520the%2520value%2520of%250Aincorporating%2520visual%2520foundation%2520model%2520priors%2520into%2520LiDAR-based%25203D%2520object%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20LiDAR%20Point%20Features%20with%20Foundation%20Model%20Priors%20for%203D%0A%20%20Object%20Detection&entry.906535625=Yujian%20Mo%20and%20Yan%20Wu%20and%20Junqiao%20Zhao%20and%20Jijun%20Wang%20and%20Yinghao%20Hu%20and%20Jun%20Yan&entry.1292438233=%20%20Recent%20advances%20in%20foundation%20models%20have%20opened%20up%20new%20possibilities%20for%0Aenhancing%203D%20perception.%20In%20particular%2C%20DepthAnything%20offers%20dense%20and%20reliable%0Ageometric%20priors%20from%20monocular%20RGB%20images%2C%20which%20can%20complement%20sparse%20LiDAR%0Adata%20in%20autonomous%20driving%20scenarios.%20However%2C%20such%20priors%20remain%20underutilized%0Ain%20LiDAR-based%203D%20object%20detection.%20In%20this%20paper%2C%20we%20address%20the%20limited%0Aexpressiveness%20of%20raw%20LiDAR%20point%20features%2C%20especially%20the%20weak%20discriminative%0Acapability%20of%20the%20reflectance%20attribute%2C%20by%20introducing%20depth%20priors%20predicted%0Aby%20DepthAnything.%20These%20priors%20are%20fused%20with%20the%20original%20LiDAR%20attributes%20to%0Aenrich%20each%20point%27s%20representation.%20To%20leverage%20the%20enhanced%20point%20features%2C%20we%0Apropose%20a%20point-wise%20feature%20extraction%20module.%20Then%2C%20a%20Dual-Path%20RoI%20feature%0Aextraction%20framework%20is%20employed%2C%20comprising%20a%20voxel-based%20branch%20for%20global%0Asemantic%20context%20and%20a%20point-based%20branch%20for%20fine-grained%20structural%20details.%0ATo%20effectively%20integrate%20the%20complementary%20RoI%20features%2C%20we%20introduce%20a%0Abidirectional%20gated%20RoI%20feature%20fusion%20module%20that%20balances%20global%20and%20local%0Acues.%20Extensive%20experiments%20on%20the%20KITTI%20benchmark%20show%20that%20our%20method%0Aconsistently%20improves%20detection%20accuracy%2C%20demonstrating%20the%20value%20of%0Aincorporating%20visual%20foundation%20model%20priors%20into%20LiDAR-based%203D%20object%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13899v1&entry.124074799=Read"},
{"title": "Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model\n  via Cross-modal Alignment", "author": "Angelos Zavras and Dimitrios Michail and Beg\u00fcm Demir and Ioannis Papoutsis", "abstract": "  Deep Learning (DL) is undergoing a paradigm shift with the emergence of\nfoundation models. In this work, we focus on Contrastive Language-Image\nPre-training (CLIP), a Vision-Language foundation model that achieves high\naccuracy across various image classification tasks and often rivals fully\nsupervised baselines, despite not being explicitly trained for those tasks.\nNevertheless, there are still domains where zero-shot CLIP performance is far\nfrom optimal, such as Remote Sensing (RS) and medical imagery. These domains do\nnot only exhibit fundamentally different distributions compared to natural\nimages, but also commonly rely on complementary modalities, beyond RGB, to\nderive meaningful insights. To this end, we propose a methodology to align\ndistinct RS image modalities with the visual and textual modalities of CLIP.\nOur two-stage procedure addresses the aforementioned distribution shift,\nextends the zero-shot capabilities of CLIP and enriches CLIP's shared embedding\nspace with domain-specific knowledge. Initially, we robustly fine-tune CLIP\naccording to the PAINT (Ilharco et al., 2022) patching protocol, in order to\ndeal with the distribution shift. Building upon this foundation, we facilitate\nthe cross-modal alignment of a RS modality encoder by distilling knowledge from\nthe CLIP visual and textual encoders. We empirically show that both patching\nand cross-modal alignment translate to significant performance gains, across\nseveral RS imagery classification and cross-modal retrieval benchmark datasets.\nNotably, these enhancements are achieved without the reliance on textual\ndescriptions, without introducing any task-specific parameters, without\ntraining from scratch and without catastrophic forgetting. We make our code\nimplementation and weights for all experiments publicly available at\nhttps://github.com/Orion-AI-Lab/MindTheModalityGap.\n", "link": "http://arxiv.org/abs/2402.09816v2", "date": "2025-07-18", "relevancy": 3.0881, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.634}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Modality%20Gap%3A%20Towards%20a%20Remote%20Sensing%20Vision-Language%20Model%0A%20%20via%20Cross-modal%20Alignment&body=Title%3A%20Mind%20the%20Modality%20Gap%3A%20Towards%20a%20Remote%20Sensing%20Vision-Language%20Model%0A%20%20via%20Cross-modal%20Alignment%0AAuthor%3A%20Angelos%20Zavras%20and%20Dimitrios%20Michail%20and%20Beg%C3%BCm%20Demir%20and%20Ioannis%20Papoutsis%0AAbstract%3A%20%20%20Deep%20Learning%20%28DL%29%20is%20undergoing%20a%20paradigm%20shift%20with%20the%20emergence%20of%0Afoundation%20models.%20In%20this%20work%2C%20we%20focus%20on%20Contrastive%20Language-Image%0APre-training%20%28CLIP%29%2C%20a%20Vision-Language%20foundation%20model%20that%20achieves%20high%0Aaccuracy%20across%20various%20image%20classification%20tasks%20and%20often%20rivals%20fully%0Asupervised%20baselines%2C%20despite%20not%20being%20explicitly%20trained%20for%20those%20tasks.%0ANevertheless%2C%20there%20are%20still%20domains%20where%20zero-shot%20CLIP%20performance%20is%20far%0Afrom%20optimal%2C%20such%20as%20Remote%20Sensing%20%28RS%29%20and%20medical%20imagery.%20These%20domains%20do%0Anot%20only%20exhibit%20fundamentally%20different%20distributions%20compared%20to%20natural%0Aimages%2C%20but%20also%20commonly%20rely%20on%20complementary%20modalities%2C%20beyond%20RGB%2C%20to%0Aderive%20meaningful%20insights.%20To%20this%20end%2C%20we%20propose%20a%20methodology%20to%20align%0Adistinct%20RS%20image%20modalities%20with%20the%20visual%20and%20textual%20modalities%20of%20CLIP.%0AOur%20two-stage%20procedure%20addresses%20the%20aforementioned%20distribution%20shift%2C%0Aextends%20the%20zero-shot%20capabilities%20of%20CLIP%20and%20enriches%20CLIP%27s%20shared%20embedding%0Aspace%20with%20domain-specific%20knowledge.%20Initially%2C%20we%20robustly%20fine-tune%20CLIP%0Aaccording%20to%20the%20PAINT%20%28Ilharco%20et%20al.%2C%202022%29%20patching%20protocol%2C%20in%20order%20to%0Adeal%20with%20the%20distribution%20shift.%20Building%20upon%20this%20foundation%2C%20we%20facilitate%0Athe%20cross-modal%20alignment%20of%20a%20RS%20modality%20encoder%20by%20distilling%20knowledge%20from%0Athe%20CLIP%20visual%20and%20textual%20encoders.%20We%20empirically%20show%20that%20both%20patching%0Aand%20cross-modal%20alignment%20translate%20to%20significant%20performance%20gains%2C%20across%0Aseveral%20RS%20imagery%20classification%20and%20cross-modal%20retrieval%20benchmark%20datasets.%0ANotably%2C%20these%20enhancements%20are%20achieved%20without%20the%20reliance%20on%20textual%0Adescriptions%2C%20without%20introducing%20any%20task-specific%20parameters%2C%20without%0Atraining%20from%20scratch%20and%20without%20catastrophic%20forgetting.%20We%20make%20our%20code%0Aimplementation%20and%20weights%20for%20all%20experiments%20publicly%20available%20at%0Ahttps%3A//github.com/Orion-AI-Lab/MindTheModalityGap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09816v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Modality%2520Gap%253A%2520Towards%2520a%2520Remote%2520Sensing%2520Vision-Language%2520Model%250A%2520%2520via%2520Cross-modal%2520Alignment%26entry.906535625%3DAngelos%2520Zavras%2520and%2520Dimitrios%2520Michail%2520and%2520Beg%25C3%25BCm%2520Demir%2520and%2520Ioannis%2520Papoutsis%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520%2528DL%2529%2520is%2520undergoing%2520a%2520paradigm%2520shift%2520with%2520the%2520emergence%2520of%250Afoundation%2520models.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520Contrastive%2520Language-Image%250APre-training%2520%2528CLIP%2529%252C%2520a%2520Vision-Language%2520foundation%2520model%2520that%2520achieves%2520high%250Aaccuracy%2520across%2520various%2520image%2520classification%2520tasks%2520and%2520often%2520rivals%2520fully%250Asupervised%2520baselines%252C%2520despite%2520not%2520being%2520explicitly%2520trained%2520for%2520those%2520tasks.%250ANevertheless%252C%2520there%2520are%2520still%2520domains%2520where%2520zero-shot%2520CLIP%2520performance%2520is%2520far%250Afrom%2520optimal%252C%2520such%2520as%2520Remote%2520Sensing%2520%2528RS%2529%2520and%2520medical%2520imagery.%2520These%2520domains%2520do%250Anot%2520only%2520exhibit%2520fundamentally%2520different%2520distributions%2520compared%2520to%2520natural%250Aimages%252C%2520but%2520also%2520commonly%2520rely%2520on%2520complementary%2520modalities%252C%2520beyond%2520RGB%252C%2520to%250Aderive%2520meaningful%2520insights.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520methodology%2520to%2520align%250Adistinct%2520RS%2520image%2520modalities%2520with%2520the%2520visual%2520and%2520textual%2520modalities%2520of%2520CLIP.%250AOur%2520two-stage%2520procedure%2520addresses%2520the%2520aforementioned%2520distribution%2520shift%252C%250Aextends%2520the%2520zero-shot%2520capabilities%2520of%2520CLIP%2520and%2520enriches%2520CLIP%2527s%2520shared%2520embedding%250Aspace%2520with%2520domain-specific%2520knowledge.%2520Initially%252C%2520we%2520robustly%2520fine-tune%2520CLIP%250Aaccording%2520to%2520the%2520PAINT%2520%2528Ilharco%2520et%2520al.%252C%25202022%2529%2520patching%2520protocol%252C%2520in%2520order%2520to%250Adeal%2520with%2520the%2520distribution%2520shift.%2520Building%2520upon%2520this%2520foundation%252C%2520we%2520facilitate%250Athe%2520cross-modal%2520alignment%2520of%2520a%2520RS%2520modality%2520encoder%2520by%2520distilling%2520knowledge%2520from%250Athe%2520CLIP%2520visual%2520and%2520textual%2520encoders.%2520We%2520empirically%2520show%2520that%2520both%2520patching%250Aand%2520cross-modal%2520alignment%2520translate%2520to%2520significant%2520performance%2520gains%252C%2520across%250Aseveral%2520RS%2520imagery%2520classification%2520and%2520cross-modal%2520retrieval%2520benchmark%2520datasets.%250ANotably%252C%2520these%2520enhancements%2520are%2520achieved%2520without%2520the%2520reliance%2520on%2520textual%250Adescriptions%252C%2520without%2520introducing%2520any%2520task-specific%2520parameters%252C%2520without%250Atraining%2520from%2520scratch%2520and%2520without%2520catastrophic%2520forgetting.%2520We%2520make%2520our%2520code%250Aimplementation%2520and%2520weights%2520for%2520all%2520experiments%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Orion-AI-Lab/MindTheModalityGap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09816v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Modality%20Gap%3A%20Towards%20a%20Remote%20Sensing%20Vision-Language%20Model%0A%20%20via%20Cross-modal%20Alignment&entry.906535625=Angelos%20Zavras%20and%20Dimitrios%20Michail%20and%20Beg%C3%BCm%20Demir%20and%20Ioannis%20Papoutsis&entry.1292438233=%20%20Deep%20Learning%20%28DL%29%20is%20undergoing%20a%20paradigm%20shift%20with%20the%20emergence%20of%0Afoundation%20models.%20In%20this%20work%2C%20we%20focus%20on%20Contrastive%20Language-Image%0APre-training%20%28CLIP%29%2C%20a%20Vision-Language%20foundation%20model%20that%20achieves%20high%0Aaccuracy%20across%20various%20image%20classification%20tasks%20and%20often%20rivals%20fully%0Asupervised%20baselines%2C%20despite%20not%20being%20explicitly%20trained%20for%20those%20tasks.%0ANevertheless%2C%20there%20are%20still%20domains%20where%20zero-shot%20CLIP%20performance%20is%20far%0Afrom%20optimal%2C%20such%20as%20Remote%20Sensing%20%28RS%29%20and%20medical%20imagery.%20These%20domains%20do%0Anot%20only%20exhibit%20fundamentally%20different%20distributions%20compared%20to%20natural%0Aimages%2C%20but%20also%20commonly%20rely%20on%20complementary%20modalities%2C%20beyond%20RGB%2C%20to%0Aderive%20meaningful%20insights.%20To%20this%20end%2C%20we%20propose%20a%20methodology%20to%20align%0Adistinct%20RS%20image%20modalities%20with%20the%20visual%20and%20textual%20modalities%20of%20CLIP.%0AOur%20two-stage%20procedure%20addresses%20the%20aforementioned%20distribution%20shift%2C%0Aextends%20the%20zero-shot%20capabilities%20of%20CLIP%20and%20enriches%20CLIP%27s%20shared%20embedding%0Aspace%20with%20domain-specific%20knowledge.%20Initially%2C%20we%20robustly%20fine-tune%20CLIP%0Aaccording%20to%20the%20PAINT%20%28Ilharco%20et%20al.%2C%202022%29%20patching%20protocol%2C%20in%20order%20to%0Adeal%20with%20the%20distribution%20shift.%20Building%20upon%20this%20foundation%2C%20we%20facilitate%0Athe%20cross-modal%20alignment%20of%20a%20RS%20modality%20encoder%20by%20distilling%20knowledge%20from%0Athe%20CLIP%20visual%20and%20textual%20encoders.%20We%20empirically%20show%20that%20both%20patching%0Aand%20cross-modal%20alignment%20translate%20to%20significant%20performance%20gains%2C%20across%0Aseveral%20RS%20imagery%20classification%20and%20cross-modal%20retrieval%20benchmark%20datasets.%0ANotably%2C%20these%20enhancements%20are%20achieved%20without%20the%20reliance%20on%20textual%0Adescriptions%2C%20without%20introducing%20any%20task-specific%20parameters%2C%20without%0Atraining%20from%20scratch%20and%20without%20catastrophic%20forgetting.%20We%20make%20our%20code%0Aimplementation%20and%20weights%20for%20all%20experiments%20publicly%20available%20at%0Ahttps%3A//github.com/Orion-AI-Lab/MindTheModalityGap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09816v2&entry.124074799=Read"},
{"title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes\n  Without Visual Feature via Connected \u03b4-Overlap Graphs", "author": "Yung-Hong Sun and Ting-Hung Lin and Jiangang Chen and Hongrui Jiang and Yu Hen Hu", "abstract": "  Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.\n", "link": "http://arxiv.org/abs/2507.14095v1", "date": "2025-07-18", "relevancy": 3.0849, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6347}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-DOG%3A%20Training-Free%20Multi-View%20Multi-Object%20Association%20in%20Dense%20Scenes%0A%20%20Without%20Visual%20Feature%20via%20Connected%20%CE%B4-Overlap%20Graphs&body=Title%3A%20C-DOG%3A%20Training-Free%20Multi-View%20Multi-Object%20Association%20in%20Dense%20Scenes%0A%20%20Without%20Visual%20Feature%20via%20Connected%20%CE%B4-Overlap%20Graphs%0AAuthor%3A%20Yung-Hong%20Sun%20and%20Ting-Hung%20Lin%20and%20Jiangang%20Chen%20and%20Hongrui%20Jiang%20and%20Yu%20Hen%20Hu%0AAbstract%3A%20%20%20Multi-view%20multi-object%20association%20is%20a%20fundamental%20step%20in%203D%0Areconstruction%20pipelines%2C%20enabling%20consistent%20grouping%20of%20object%20instances%0Aacross%20multiple%20camera%20views.%20Existing%20methods%20often%20rely%20on%20appearance%0Afeatures%20or%20geometric%20constraints%20such%20as%20epipolar%20consistency.%20However%2C%20these%0Aapproaches%20can%20fail%20when%20objects%20are%20visually%20indistinguishable%20or%20observations%0Aare%20corrupted%20by%20noise.%20We%20propose%20C-DOG%2C%20a%20training-free%20framework%20that%20serves%0Aas%20an%20intermediate%20module%20bridging%20object%20detection%20%28or%20pose%20estimation%29%20and%203D%0Areconstruction%2C%20without%20relying%20on%20visual%20features.%20It%20combines%20connected%0Adelta-overlap%20graph%20modeling%20with%20epipolar%20geometry%20to%20robustly%20associate%0Adetections%20across%20views.%20Each%202D%20observation%20is%20represented%20as%20a%20graph%20node%2C%0Awith%20edges%20weighted%20by%20epipolar%20consistency.%20A%20delta-neighbor-overlap%0Aclustering%20step%20identifies%20strongly%20consistent%20groups%20while%20tolerating%20noise%0Aand%20partial%20connectivity.%20To%20further%20improve%20robustness%2C%20we%20incorporate%0AInterquartile%20Range%20%28IQR%29-based%20filtering%20and%20a%203D%20back-projection%20error%0Acriterion%20to%20eliminate%20inconsistent%20observations.%20Extensive%20experiments%20on%0Asynthetic%20benchmarks%20demonstrate%20that%20C-DOG%20outperforms%20geometry-based%0Abaselines%20and%20remains%20robust%20under%20challenging%20conditions%2C%20including%20high%0Aobject%20density%2C%20without%20visual%20features%2C%20and%20limited%20camera%20overlap%2C%20making%20it%0Awell-suited%20for%20scalable%203D%20reconstruction%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-DOG%253A%2520Training-Free%2520Multi-View%2520Multi-Object%2520Association%2520in%2520Dense%2520Scenes%250A%2520%2520Without%2520Visual%2520Feature%2520via%2520Connected%2520%25CE%25B4-Overlap%2520Graphs%26entry.906535625%3DYung-Hong%2520Sun%2520and%2520Ting-Hung%2520Lin%2520and%2520Jiangang%2520Chen%2520and%2520Hongrui%2520Jiang%2520and%2520Yu%2520Hen%2520Hu%26entry.1292438233%3D%2520%2520Multi-view%2520multi-object%2520association%2520is%2520a%2520fundamental%2520step%2520in%25203D%250Areconstruction%2520pipelines%252C%2520enabling%2520consistent%2520grouping%2520of%2520object%2520instances%250Aacross%2520multiple%2520camera%2520views.%2520Existing%2520methods%2520often%2520rely%2520on%2520appearance%250Afeatures%2520or%2520geometric%2520constraints%2520such%2520as%2520epipolar%2520consistency.%2520However%252C%2520these%250Aapproaches%2520can%2520fail%2520when%2520objects%2520are%2520visually%2520indistinguishable%2520or%2520observations%250Aare%2520corrupted%2520by%2520noise.%2520We%2520propose%2520C-DOG%252C%2520a%2520training-free%2520framework%2520that%2520serves%250Aas%2520an%2520intermediate%2520module%2520bridging%2520object%2520detection%2520%2528or%2520pose%2520estimation%2529%2520and%25203D%250Areconstruction%252C%2520without%2520relying%2520on%2520visual%2520features.%2520It%2520combines%2520connected%250Adelta-overlap%2520graph%2520modeling%2520with%2520epipolar%2520geometry%2520to%2520robustly%2520associate%250Adetections%2520across%2520views.%2520Each%25202D%2520observation%2520is%2520represented%2520as%2520a%2520graph%2520node%252C%250Awith%2520edges%2520weighted%2520by%2520epipolar%2520consistency.%2520A%2520delta-neighbor-overlap%250Aclustering%2520step%2520identifies%2520strongly%2520consistent%2520groups%2520while%2520tolerating%2520noise%250Aand%2520partial%2520connectivity.%2520To%2520further%2520improve%2520robustness%252C%2520we%2520incorporate%250AInterquartile%2520Range%2520%2528IQR%2529-based%2520filtering%2520and%2520a%25203D%2520back-projection%2520error%250Acriterion%2520to%2520eliminate%2520inconsistent%2520observations.%2520Extensive%2520experiments%2520on%250Asynthetic%2520benchmarks%2520demonstrate%2520that%2520C-DOG%2520outperforms%2520geometry-based%250Abaselines%2520and%2520remains%2520robust%2520under%2520challenging%2520conditions%252C%2520including%2520high%250Aobject%2520density%252C%2520without%2520visual%2520features%252C%2520and%2520limited%2520camera%2520overlap%252C%2520making%2520it%250Awell-suited%2520for%2520scalable%25203D%2520reconstruction%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-DOG%3A%20Training-Free%20Multi-View%20Multi-Object%20Association%20in%20Dense%20Scenes%0A%20%20Without%20Visual%20Feature%20via%20Connected%20%CE%B4-Overlap%20Graphs&entry.906535625=Yung-Hong%20Sun%20and%20Ting-Hung%20Lin%20and%20Jiangang%20Chen%20and%20Hongrui%20Jiang%20and%20Yu%20Hen%20Hu&entry.1292438233=%20%20Multi-view%20multi-object%20association%20is%20a%20fundamental%20step%20in%203D%0Areconstruction%20pipelines%2C%20enabling%20consistent%20grouping%20of%20object%20instances%0Aacross%20multiple%20camera%20views.%20Existing%20methods%20often%20rely%20on%20appearance%0Afeatures%20or%20geometric%20constraints%20such%20as%20epipolar%20consistency.%20However%2C%20these%0Aapproaches%20can%20fail%20when%20objects%20are%20visually%20indistinguishable%20or%20observations%0Aare%20corrupted%20by%20noise.%20We%20propose%20C-DOG%2C%20a%20training-free%20framework%20that%20serves%0Aas%20an%20intermediate%20module%20bridging%20object%20detection%20%28or%20pose%20estimation%29%20and%203D%0Areconstruction%2C%20without%20relying%20on%20visual%20features.%20It%20combines%20connected%0Adelta-overlap%20graph%20modeling%20with%20epipolar%20geometry%20to%20robustly%20associate%0Adetections%20across%20views.%20Each%202D%20observation%20is%20represented%20as%20a%20graph%20node%2C%0Awith%20edges%20weighted%20by%20epipolar%20consistency.%20A%20delta-neighbor-overlap%0Aclustering%20step%20identifies%20strongly%20consistent%20groups%20while%20tolerating%20noise%0Aand%20partial%20connectivity.%20To%20further%20improve%20robustness%2C%20we%20incorporate%0AInterquartile%20Range%20%28IQR%29-based%20filtering%20and%20a%203D%20back-projection%20error%0Acriterion%20to%20eliminate%20inconsistent%20observations.%20Extensive%20experiments%20on%0Asynthetic%20benchmarks%20demonstrate%20that%20C-DOG%20outperforms%20geometry-based%0Abaselines%20and%20remains%20robust%20under%20challenging%20conditions%2C%20including%20high%0Aobject%20density%2C%20without%20visual%20features%2C%20and%20limited%20camera%20overlap%2C%20making%20it%0Awell-suited%20for%20scalable%203D%20reconstruction%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14095v1&entry.124074799=Read"},
{"title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in\n  Vision-Language Models", "author": "Francesco Ortu and Zhijing Jin and Diego Doimo and Alberto Cazzaniga", "abstract": "  Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.\n", "link": "http://arxiv.org/abs/2507.13868v1", "date": "2025-07-18", "relevancy": 2.9454, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5948}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Seeing%20Overrides%20Knowing%3A%20Disentangling%20Knowledge%20Conflicts%20in%0A%20%20Vision-Language%20Models&body=Title%3A%20When%20Seeing%20Overrides%20Knowing%3A%20Disentangling%20Knowledge%20Conflicts%20in%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Francesco%20Ortu%20and%20Zhijing%20Jin%20and%20Diego%20Doimo%20and%20Alberto%20Cazzaniga%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20increasingly%20leverage%20diverse%20knowledge%20sources%0Ato%20address%20complex%20tasks%2C%20often%20encountering%20conflicts%20between%20their%20internal%0Aparametric%20knowledge%20and%20external%20information.%20Knowledge%20conflicts%20can%20result%0Ain%20hallucinations%20and%20unreliable%20responses%2C%20but%20the%20mechanisms%20governing%20such%0Ainteractions%20remain%20unknown.%20To%20address%20this%20gap%2C%20we%20analyze%20the%20mechanisms%0Athat%20VLMs%20use%20to%20resolve%20cross-modal%20conflicts%20by%20introducing%20a%20dataset%20of%0Amultimodal%20counterfactual%20queries%20that%20deliberately%20contradict%20internal%0Acommonsense%20knowledge.%20We%20localize%20with%20logit%20inspection%20a%20small%20set%20of%20heads%0Athat%20control%20the%20conflict.%20Moreover%2C%20by%20modifying%20these%20heads%2C%20we%20can%20steer%20the%0Amodel%20towards%20its%20internal%20knowledge%20or%20the%20visual%20inputs.%20Finally%2C%20we%20show%0Athat%20attention%20from%20such%20heads%20pinpoints%20localized%20image%20regions%20driving%20visual%0Aoverrides%2C%20outperforming%20gradient-based%20attribution%20in%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Seeing%2520Overrides%2520Knowing%253A%2520Disentangling%2520Knowledge%2520Conflicts%2520in%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DFrancesco%2520Ortu%2520and%2520Zhijing%2520Jin%2520and%2520Diego%2520Doimo%2520and%2520Alberto%2520Cazzaniga%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520increasingly%2520leverage%2520diverse%2520knowledge%2520sources%250Ato%2520address%2520complex%2520tasks%252C%2520often%2520encountering%2520conflicts%2520between%2520their%2520internal%250Aparametric%2520knowledge%2520and%2520external%2520information.%2520Knowledge%2520conflicts%2520can%2520result%250Ain%2520hallucinations%2520and%2520unreliable%2520responses%252C%2520but%2520the%2520mechanisms%2520governing%2520such%250Ainteractions%2520remain%2520unknown.%2520To%2520address%2520this%2520gap%252C%2520we%2520analyze%2520the%2520mechanisms%250Athat%2520VLMs%2520use%2520to%2520resolve%2520cross-modal%2520conflicts%2520by%2520introducing%2520a%2520dataset%2520of%250Amultimodal%2520counterfactual%2520queries%2520that%2520deliberately%2520contradict%2520internal%250Acommonsense%2520knowledge.%2520We%2520localize%2520with%2520logit%2520inspection%2520a%2520small%2520set%2520of%2520heads%250Athat%2520control%2520the%2520conflict.%2520Moreover%252C%2520by%2520modifying%2520these%2520heads%252C%2520we%2520can%2520steer%2520the%250Amodel%2520towards%2520its%2520internal%2520knowledge%2520or%2520the%2520visual%2520inputs.%2520Finally%252C%2520we%2520show%250Athat%2520attention%2520from%2520such%2520heads%2520pinpoints%2520localized%2520image%2520regions%2520driving%2520visual%250Aoverrides%252C%2520outperforming%2520gradient-based%2520attribution%2520in%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Seeing%20Overrides%20Knowing%3A%20Disentangling%20Knowledge%20Conflicts%20in%0A%20%20Vision-Language%20Models&entry.906535625=Francesco%20Ortu%20and%20Zhijing%20Jin%20and%20Diego%20Doimo%20and%20Alberto%20Cazzaniga&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20increasingly%20leverage%20diverse%20knowledge%20sources%0Ato%20address%20complex%20tasks%2C%20often%20encountering%20conflicts%20between%20their%20internal%0Aparametric%20knowledge%20and%20external%20information.%20Knowledge%20conflicts%20can%20result%0Ain%20hallucinations%20and%20unreliable%20responses%2C%20but%20the%20mechanisms%20governing%20such%0Ainteractions%20remain%20unknown.%20To%20address%20this%20gap%2C%20we%20analyze%20the%20mechanisms%0Athat%20VLMs%20use%20to%20resolve%20cross-modal%20conflicts%20by%20introducing%20a%20dataset%20of%0Amultimodal%20counterfactual%20queries%20that%20deliberately%20contradict%20internal%0Acommonsense%20knowledge.%20We%20localize%20with%20logit%20inspection%20a%20small%20set%20of%20heads%0Athat%20control%20the%20conflict.%20Moreover%2C%20by%20modifying%20these%20heads%2C%20we%20can%20steer%20the%0Amodel%20towards%20its%20internal%20knowledge%20or%20the%20visual%20inputs.%20Finally%2C%20we%20show%0Athat%20attention%20from%20such%20heads%20pinpoints%20localized%20image%20regions%20driving%20visual%0Aoverrides%2C%20outperforming%20gradient-based%20attribution%20in%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13868v1&entry.124074799=Read"},
{"title": "GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote\n  Sensing Image Parsing", "author": "Xianzhi Ma and Jianhui Li and Changhua Pei and Hao Liu", "abstract": "  The application of Vision-Language Models (VLMs) in remote sensing (RS) image\nunderstanding has achieved notable progress, demonstrating the basic ability to\nrecognize and describe geographical entities. However, existing RS-VLMs are\nmostly limited to image-level and region-level tasks, lacking the capability to\nhandle pixel-level tasks and performing poorly in small-object recognition\nscenarios. Moreover, RS-VLMs consume significant computational resources when\nprocessing high-resolution RS images, further restricting their practical\napplicability. In this context, we propose GeoMag (Geographical Magnifier), an\nend-to-end general-purpose large model framework for RS. GeoMag dynamically\nfocuses the attention scope based on prompt semantics to effectively perform\nremote sensing image parsing across multiple levels of granularity. This method\nintroduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and\nPrompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the\nspatial resolution of task-irrelevant regions while enhancing the visual\nrepresentation of task-relevant areas. This approach improves the model's\nperception of critical target regions, suppresses background redundancy, and\nreduces the computational cost of interpreting high-resolution RS imagery.\nExtensive comparative experiments on 10 benchmarks demonstrate that GeoMag not\nonly excels in handling pixel-level tasks but also maintains competitive\nperformance across tasks of other granularities compared to existing RS-VLMs.\n", "link": "http://arxiv.org/abs/2507.05887v2", "date": "2025-07-18", "relevancy": 2.8683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMag%3A%20A%20Vision-Language%20Model%20for%20Pixel-level%20Fine-Grained%20Remote%0A%20%20Sensing%20Image%20Parsing&body=Title%3A%20GeoMag%3A%20A%20Vision-Language%20Model%20for%20Pixel-level%20Fine-Grained%20Remote%0A%20%20Sensing%20Image%20Parsing%0AAuthor%3A%20Xianzhi%20Ma%20and%20Jianhui%20Li%20and%20Changhua%20Pei%20and%20Hao%20Liu%0AAbstract%3A%20%20%20The%20application%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20remote%20sensing%20%28RS%29%20image%0Aunderstanding%20has%20achieved%20notable%20progress%2C%20demonstrating%20the%20basic%20ability%20to%0Arecognize%20and%20describe%20geographical%20entities.%20However%2C%20existing%20RS-VLMs%20are%0Amostly%20limited%20to%20image-level%20and%20region-level%20tasks%2C%20lacking%20the%20capability%20to%0Ahandle%20pixel-level%20tasks%20and%20performing%20poorly%20in%20small-object%20recognition%0Ascenarios.%20Moreover%2C%20RS-VLMs%20consume%20significant%20computational%20resources%20when%0Aprocessing%20high-resolution%20RS%20images%2C%20further%20restricting%20their%20practical%0Aapplicability.%20In%20this%20context%2C%20we%20propose%20GeoMag%20%28Geographical%20Magnifier%29%2C%20an%0Aend-to-end%20general-purpose%20large%20model%20framework%20for%20RS.%20GeoMag%20dynamically%0Afocuses%20the%20attention%20scope%20based%20on%20prompt%20semantics%20to%20effectively%20perform%0Aremote%20sensing%20image%20parsing%20across%20multiple%20levels%20of%20granularity.%20This%20method%0Aintroduces%20Task-driven%20Multi-granularity%20Resolution%20Adjustment%20%28TMRA%29%20and%0APrompt-guided%20Semantic-aware%20Cropping%20%28PSC%29%2C%20which%20adaptively%20reduce%20the%0Aspatial%20resolution%20of%20task-irrelevant%20regions%20while%20enhancing%20the%20visual%0Arepresentation%20of%20task-relevant%20areas.%20This%20approach%20improves%20the%20model%27s%0Aperception%20of%20critical%20target%20regions%2C%20suppresses%20background%20redundancy%2C%20and%0Areduces%20the%20computational%20cost%20of%20interpreting%20high-resolution%20RS%20imagery.%0AExtensive%20comparative%20experiments%20on%2010%20benchmarks%20demonstrate%20that%20GeoMag%20not%0Aonly%20excels%20in%20handling%20pixel-level%20tasks%20but%20also%20maintains%20competitive%0Aperformance%20across%20tasks%20of%20other%20granularities%20compared%20to%20existing%20RS-VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMag%253A%2520A%2520Vision-Language%2520Model%2520for%2520Pixel-level%2520Fine-Grained%2520Remote%250A%2520%2520Sensing%2520Image%2520Parsing%26entry.906535625%3DXianzhi%2520Ma%2520and%2520Jianhui%2520Li%2520and%2520Changhua%2520Pei%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520in%2520remote%2520sensing%2520%2528RS%2529%2520image%250Aunderstanding%2520has%2520achieved%2520notable%2520progress%252C%2520demonstrating%2520the%2520basic%2520ability%2520to%250Arecognize%2520and%2520describe%2520geographical%2520entities.%2520However%252C%2520existing%2520RS-VLMs%2520are%250Amostly%2520limited%2520to%2520image-level%2520and%2520region-level%2520tasks%252C%2520lacking%2520the%2520capability%2520to%250Ahandle%2520pixel-level%2520tasks%2520and%2520performing%2520poorly%2520in%2520small-object%2520recognition%250Ascenarios.%2520Moreover%252C%2520RS-VLMs%2520consume%2520significant%2520computational%2520resources%2520when%250Aprocessing%2520high-resolution%2520RS%2520images%252C%2520further%2520restricting%2520their%2520practical%250Aapplicability.%2520In%2520this%2520context%252C%2520we%2520propose%2520GeoMag%2520%2528Geographical%2520Magnifier%2529%252C%2520an%250Aend-to-end%2520general-purpose%2520large%2520model%2520framework%2520for%2520RS.%2520GeoMag%2520dynamically%250Afocuses%2520the%2520attention%2520scope%2520based%2520on%2520prompt%2520semantics%2520to%2520effectively%2520perform%250Aremote%2520sensing%2520image%2520parsing%2520across%2520multiple%2520levels%2520of%2520granularity.%2520This%2520method%250Aintroduces%2520Task-driven%2520Multi-granularity%2520Resolution%2520Adjustment%2520%2528TMRA%2529%2520and%250APrompt-guided%2520Semantic-aware%2520Cropping%2520%2528PSC%2529%252C%2520which%2520adaptively%2520reduce%2520the%250Aspatial%2520resolution%2520of%2520task-irrelevant%2520regions%2520while%2520enhancing%2520the%2520visual%250Arepresentation%2520of%2520task-relevant%2520areas.%2520This%2520approach%2520improves%2520the%2520model%2527s%250Aperception%2520of%2520critical%2520target%2520regions%252C%2520suppresses%2520background%2520redundancy%252C%2520and%250Areduces%2520the%2520computational%2520cost%2520of%2520interpreting%2520high-resolution%2520RS%2520imagery.%250AExtensive%2520comparative%2520experiments%2520on%252010%2520benchmarks%2520demonstrate%2520that%2520GeoMag%2520not%250Aonly%2520excels%2520in%2520handling%2520pixel-level%2520tasks%2520but%2520also%2520maintains%2520competitive%250Aperformance%2520across%2520tasks%2520of%2520other%2520granularities%2520compared%2520to%2520existing%2520RS-VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMag%3A%20A%20Vision-Language%20Model%20for%20Pixel-level%20Fine-Grained%20Remote%0A%20%20Sensing%20Image%20Parsing&entry.906535625=Xianzhi%20Ma%20and%20Jianhui%20Li%20and%20Changhua%20Pei%20and%20Hao%20Liu&entry.1292438233=%20%20The%20application%20of%20Vision-Language%20Models%20%28VLMs%29%20in%20remote%20sensing%20%28RS%29%20image%0Aunderstanding%20has%20achieved%20notable%20progress%2C%20demonstrating%20the%20basic%20ability%20to%0Arecognize%20and%20describe%20geographical%20entities.%20However%2C%20existing%20RS-VLMs%20are%0Amostly%20limited%20to%20image-level%20and%20region-level%20tasks%2C%20lacking%20the%20capability%20to%0Ahandle%20pixel-level%20tasks%20and%20performing%20poorly%20in%20small-object%20recognition%0Ascenarios.%20Moreover%2C%20RS-VLMs%20consume%20significant%20computational%20resources%20when%0Aprocessing%20high-resolution%20RS%20images%2C%20further%20restricting%20their%20practical%0Aapplicability.%20In%20this%20context%2C%20we%20propose%20GeoMag%20%28Geographical%20Magnifier%29%2C%20an%0Aend-to-end%20general-purpose%20large%20model%20framework%20for%20RS.%20GeoMag%20dynamically%0Afocuses%20the%20attention%20scope%20based%20on%20prompt%20semantics%20to%20effectively%20perform%0Aremote%20sensing%20image%20parsing%20across%20multiple%20levels%20of%20granularity.%20This%20method%0Aintroduces%20Task-driven%20Multi-granularity%20Resolution%20Adjustment%20%28TMRA%29%20and%0APrompt-guided%20Semantic-aware%20Cropping%20%28PSC%29%2C%20which%20adaptively%20reduce%20the%0Aspatial%20resolution%20of%20task-irrelevant%20regions%20while%20enhancing%20the%20visual%0Arepresentation%20of%20task-relevant%20areas.%20This%20approach%20improves%20the%20model%27s%0Aperception%20of%20critical%20target%20regions%2C%20suppresses%20background%20redundancy%2C%20and%0Areduces%20the%20computational%20cost%20of%20interpreting%20high-resolution%20RS%20imagery.%0AExtensive%20comparative%20experiments%20on%2010%20benchmarks%20demonstrate%20that%20GeoMag%20not%0Aonly%20excels%20in%20handling%20pixel-level%20tasks%20but%20also%20maintains%20competitive%0Aperformance%20across%20tasks%20of%20other%20granularities%20compared%20to%20existing%20RS-VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05887v2&entry.124074799=Read"},
{"title": "Large-Vocabulary Segmentation for Medical Images with Text Prompts", "author": "Ziheng Zhao and Yao Zhang and Chaoyi Wu and Xiaoman Zhang and Xiao Zhou and Ya Zhang and Yanfeng Wang and Weidi Xie", "abstract": "  This paper aims to build a model that can Segment Anything in 3D medical\nimages, driven by medical terminologies as Text prompts, termed as SAT. Our\nmain contributions are three-fold: (i) We construct the first multimodal\nknowledge tree on human anatomy, including 6502 anatomical terminologies; Then,\nwe build the largest and most comprehensive segmentation dataset for training,\ncollecting over 22K 3D scans from 72 datasets, across 497 classes, with careful\nstandardization on both image and label space; (ii) We propose to inject\nmedical knowledge into a text encoder via contrastive learning and formulate a\nlarge-vocabulary segmentation model that can be prompted by medical\nterminologies in text form; (iii) We train SAT-Nano (110M parameters) and\nSAT-Pro (447M parameters). SAT-Pro achieves comparable performance to 72\nnnU-Nets -- the strongest specialist models trained on each dataset (over 2.2B\nparameters combined) -- over 497 categories. Compared with the interactive\napproach MedSAM, SAT-Pro consistently outperforms across all 7 human body\nregions with +7.1% average Dice Similarity Coefficient (DSC) improvement, while\nshowing enhanced scalability and robustness. On 2 external (cross-center)\ndatasets, SAT-Pro achieves higher performance than all baselines (+3.7% average\nDSC), demonstrating superior generalization ability.\n", "link": "http://arxiv.org/abs/2312.17183v5", "date": "2025-07-18", "relevancy": 2.8585, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-Vocabulary%20Segmentation%20for%20Medical%20Images%20with%20Text%20Prompts&body=Title%3A%20Large-Vocabulary%20Segmentation%20for%20Medical%20Images%20with%20Text%20Prompts%0AAuthor%3A%20Ziheng%20Zhao%20and%20Yao%20Zhang%20and%20Chaoyi%20Wu%20and%20Xiaoman%20Zhang%20and%20Xiao%20Zhou%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20build%20a%20model%20that%20can%20Segment%20Anything%20in%203D%20medical%0Aimages%2C%20driven%20by%20medical%20terminologies%20as%20Text%20prompts%2C%20termed%20as%20SAT.%20Our%0Amain%20contributions%20are%20three-fold%3A%20%28i%29%20We%20construct%20the%20first%20multimodal%0Aknowledge%20tree%20on%20human%20anatomy%2C%20including%206502%20anatomical%20terminologies%3B%20Then%2C%0Awe%20build%20the%20largest%20and%20most%20comprehensive%20segmentation%20dataset%20for%20training%2C%0Acollecting%20over%2022K%203D%20scans%20from%2072%20datasets%2C%20across%20497%20classes%2C%20with%20careful%0Astandardization%20on%20both%20image%20and%20label%20space%3B%20%28ii%29%20We%20propose%20to%20inject%0Amedical%20knowledge%20into%20a%20text%20encoder%20via%20contrastive%20learning%20and%20formulate%20a%0Alarge-vocabulary%20segmentation%20model%20that%20can%20be%20prompted%20by%20medical%0Aterminologies%20in%20text%20form%3B%20%28iii%29%20We%20train%20SAT-Nano%20%28110M%20parameters%29%20and%0ASAT-Pro%20%28447M%20parameters%29.%20SAT-Pro%20achieves%20comparable%20performance%20to%2072%0AnnU-Nets%20--%20the%20strongest%20specialist%20models%20trained%20on%20each%20dataset%20%28over%202.2B%0Aparameters%20combined%29%20--%20over%20497%20categories.%20Compared%20with%20the%20interactive%0Aapproach%20MedSAM%2C%20SAT-Pro%20consistently%20outperforms%20across%20all%207%20human%20body%0Aregions%20with%20%2B7.1%25%20average%20Dice%20Similarity%20Coefficient%20%28DSC%29%20improvement%2C%20while%0Ashowing%20enhanced%20scalability%20and%20robustness.%20On%202%20external%20%28cross-center%29%0Adatasets%2C%20SAT-Pro%20achieves%20higher%20performance%20than%20all%20baselines%20%28%2B3.7%25%20average%0ADSC%29%2C%20demonstrating%20superior%20generalization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17183v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-Vocabulary%2520Segmentation%2520for%2520Medical%2520Images%2520with%2520Text%2520Prompts%26entry.906535625%3DZiheng%2520Zhao%2520and%2520Yao%2520Zhang%2520and%2520Chaoyi%2520Wu%2520and%2520Xiaoman%2520Zhang%2520and%2520Xiao%2520Zhou%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520build%2520a%2520model%2520that%2520can%2520Segment%2520Anything%2520in%25203D%2520medical%250Aimages%252C%2520driven%2520by%2520medical%2520terminologies%2520as%2520Text%2520prompts%252C%2520termed%2520as%2520SAT.%2520Our%250Amain%2520contributions%2520are%2520three-fold%253A%2520%2528i%2529%2520We%2520construct%2520the%2520first%2520multimodal%250Aknowledge%2520tree%2520on%2520human%2520anatomy%252C%2520including%25206502%2520anatomical%2520terminologies%253B%2520Then%252C%250Awe%2520build%2520the%2520largest%2520and%2520most%2520comprehensive%2520segmentation%2520dataset%2520for%2520training%252C%250Acollecting%2520over%252022K%25203D%2520scans%2520from%252072%2520datasets%252C%2520across%2520497%2520classes%252C%2520with%2520careful%250Astandardization%2520on%2520both%2520image%2520and%2520label%2520space%253B%2520%2528ii%2529%2520We%2520propose%2520to%2520inject%250Amedical%2520knowledge%2520into%2520a%2520text%2520encoder%2520via%2520contrastive%2520learning%2520and%2520formulate%2520a%250Alarge-vocabulary%2520segmentation%2520model%2520that%2520can%2520be%2520prompted%2520by%2520medical%250Aterminologies%2520in%2520text%2520form%253B%2520%2528iii%2529%2520We%2520train%2520SAT-Nano%2520%2528110M%2520parameters%2529%2520and%250ASAT-Pro%2520%2528447M%2520parameters%2529.%2520SAT-Pro%2520achieves%2520comparable%2520performance%2520to%252072%250AnnU-Nets%2520--%2520the%2520strongest%2520specialist%2520models%2520trained%2520on%2520each%2520dataset%2520%2528over%25202.2B%250Aparameters%2520combined%2529%2520--%2520over%2520497%2520categories.%2520Compared%2520with%2520the%2520interactive%250Aapproach%2520MedSAM%252C%2520SAT-Pro%2520consistently%2520outperforms%2520across%2520all%25207%2520human%2520body%250Aregions%2520with%2520%252B7.1%2525%2520average%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520improvement%252C%2520while%250Ashowing%2520enhanced%2520scalability%2520and%2520robustness.%2520On%25202%2520external%2520%2528cross-center%2529%250Adatasets%252C%2520SAT-Pro%2520achieves%2520higher%2520performance%2520than%2520all%2520baselines%2520%2528%252B3.7%2525%2520average%250ADSC%2529%252C%2520demonstrating%2520superior%2520generalization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.17183v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-Vocabulary%20Segmentation%20for%20Medical%20Images%20with%20Text%20Prompts&entry.906535625=Ziheng%20Zhao%20and%20Yao%20Zhang%20and%20Chaoyi%20Wu%20and%20Xiaoman%20Zhang%20and%20Xiao%20Zhou%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=%20%20This%20paper%20aims%20to%20build%20a%20model%20that%20can%20Segment%20Anything%20in%203D%20medical%0Aimages%2C%20driven%20by%20medical%20terminologies%20as%20Text%20prompts%2C%20termed%20as%20SAT.%20Our%0Amain%20contributions%20are%20three-fold%3A%20%28i%29%20We%20construct%20the%20first%20multimodal%0Aknowledge%20tree%20on%20human%20anatomy%2C%20including%206502%20anatomical%20terminologies%3B%20Then%2C%0Awe%20build%20the%20largest%20and%20most%20comprehensive%20segmentation%20dataset%20for%20training%2C%0Acollecting%20over%2022K%203D%20scans%20from%2072%20datasets%2C%20across%20497%20classes%2C%20with%20careful%0Astandardization%20on%20both%20image%20and%20label%20space%3B%20%28ii%29%20We%20propose%20to%20inject%0Amedical%20knowledge%20into%20a%20text%20encoder%20via%20contrastive%20learning%20and%20formulate%20a%0Alarge-vocabulary%20segmentation%20model%20that%20can%20be%20prompted%20by%20medical%0Aterminologies%20in%20text%20form%3B%20%28iii%29%20We%20train%20SAT-Nano%20%28110M%20parameters%29%20and%0ASAT-Pro%20%28447M%20parameters%29.%20SAT-Pro%20achieves%20comparable%20performance%20to%2072%0AnnU-Nets%20--%20the%20strongest%20specialist%20models%20trained%20on%20each%20dataset%20%28over%202.2B%0Aparameters%20combined%29%20--%20over%20497%20categories.%20Compared%20with%20the%20interactive%0Aapproach%20MedSAM%2C%20SAT-Pro%20consistently%20outperforms%20across%20all%207%20human%20body%0Aregions%20with%20%2B7.1%25%20average%20Dice%20Similarity%20Coefficient%20%28DSC%29%20improvement%2C%20while%0Ashowing%20enhanced%20scalability%20and%20robustness.%20On%202%20external%20%28cross-center%29%0Adatasets%2C%20SAT-Pro%20achieves%20higher%20performance%20than%20all%20baselines%20%28%2B3.7%25%20average%0ADSC%29%2C%20demonstrating%20superior%20generalization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17183v5&entry.124074799=Read"},
{"title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic\n  Surveillance Environments", "author": "Hayat Ullah and Abbas Khan and Arslan Munir and Hari Kalva", "abstract": "  Realistic human surveillance datasets are crucial for training and evaluating\ncomputer vision models under real-world conditions, facilitating the\ndevelopment of robust algorithms for human and human-interacting object\ndetection in complex environments. These datasets need to offer diverse and\nchallenging data to enable a comprehensive assessment of model performance and\nthe creation of more reliable surveillance systems for public safety. To this\nend, we present two visual object detection benchmarks named OD-VIRAT Large and\nOD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance\nimagery. The video sequences in both benchmarks cover 10 different scenes of\nhuman surveillance recorded from significant height and distance. The proposed\nbenchmarks offer rich annotations of bounding boxes and categories, where\nOD-VIRAT Large has 8.7 million annotated instances in 599,996 images and\nOD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also\nfocuses on benchmarking state-of-the-art object detection architectures,\nincluding RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object\ndetection-specific variant of VIRAT dataset. To the best of our knowledge, it\nis the first work to examine the performance of these recently published\nstate-of-the-art object detection architectures on realistic surveillance\nimagery under challenging conditions such as complex backgrounds, occluded\nobjects, and small-scale objects. The proposed benchmarking and experimental\nsettings will help in providing insights concerning the performance of selected\nobject detection models and set the base for developing more efficient and\nrobust object detection architectures.\n", "link": "http://arxiv.org/abs/2507.12396v2", "date": "2025-07-18", "relevancy": 2.8048, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OD-VIRAT%3A%20A%20Large-Scale%20Benchmark%20for%20Object%20Detection%20in%20Realistic%0A%20%20Surveillance%20Environments&body=Title%3A%20OD-VIRAT%3A%20A%20Large-Scale%20Benchmark%20for%20Object%20Detection%20in%20Realistic%0A%20%20Surveillance%20Environments%0AAuthor%3A%20Hayat%20Ullah%20and%20Abbas%20Khan%20and%20Arslan%20Munir%20and%20Hari%20Kalva%0AAbstract%3A%20%20%20Realistic%20human%20surveillance%20datasets%20are%20crucial%20for%20training%20and%20evaluating%0Acomputer%20vision%20models%20under%20real-world%20conditions%2C%20facilitating%20the%0Adevelopment%20of%20robust%20algorithms%20for%20human%20and%20human-interacting%20object%0Adetection%20in%20complex%20environments.%20These%20datasets%20need%20to%20offer%20diverse%20and%0Achallenging%20data%20to%20enable%20a%20comprehensive%20assessment%20of%20model%20performance%20and%0Athe%20creation%20of%20more%20reliable%20surveillance%20systems%20for%20public%20safety.%20To%20this%0Aend%2C%20we%20present%20two%20visual%20object%20detection%20benchmarks%20named%20OD-VIRAT%20Large%20and%0AOD-VIRAT%20Tiny%2C%20aiming%20at%20advancing%20visual%20understanding%20tasks%20in%20surveillance%0Aimagery.%20The%20video%20sequences%20in%20both%20benchmarks%20cover%2010%20different%20scenes%20of%0Ahuman%20surveillance%20recorded%20from%20significant%20height%20and%20distance.%20The%20proposed%0Abenchmarks%20offer%20rich%20annotations%20of%20bounding%20boxes%20and%20categories%2C%20where%0AOD-VIRAT%20Large%20has%208.7%20million%20annotated%20instances%20in%20599%2C996%20images%20and%0AOD-VIRAT%20Tiny%20has%20288%2C901%20annotated%20instances%20in%2019%2C860%20images.%20This%20work%20also%0Afocuses%20on%20benchmarking%20state-of-the-art%20object%20detection%20architectures%2C%0Aincluding%20RETMDET%2C%20YOLOX%2C%20RetinaNet%2C%20DETR%2C%20and%20Deformable-DETR%20on%20this%20object%0Adetection-specific%20variant%20of%20VIRAT%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20it%0Ais%20the%20first%20work%20to%20examine%20the%20performance%20of%20these%20recently%20published%0Astate-of-the-art%20object%20detection%20architectures%20on%20realistic%20surveillance%0Aimagery%20under%20challenging%20conditions%20such%20as%20complex%20backgrounds%2C%20occluded%0Aobjects%2C%20and%20small-scale%20objects.%20The%20proposed%20benchmarking%20and%20experimental%0Asettings%20will%20help%20in%20providing%20insights%20concerning%20the%20performance%20of%20selected%0Aobject%20detection%20models%20and%20set%20the%20base%20for%20developing%20more%20efficient%20and%0Arobust%20object%20detection%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOD-VIRAT%253A%2520A%2520Large-Scale%2520Benchmark%2520for%2520Object%2520Detection%2520in%2520Realistic%250A%2520%2520Surveillance%2520Environments%26entry.906535625%3DHayat%2520Ullah%2520and%2520Abbas%2520Khan%2520and%2520Arslan%2520Munir%2520and%2520Hari%2520Kalva%26entry.1292438233%3D%2520%2520Realistic%2520human%2520surveillance%2520datasets%2520are%2520crucial%2520for%2520training%2520and%2520evaluating%250Acomputer%2520vision%2520models%2520under%2520real-world%2520conditions%252C%2520facilitating%2520the%250Adevelopment%2520of%2520robust%2520algorithms%2520for%2520human%2520and%2520human-interacting%2520object%250Adetection%2520in%2520complex%2520environments.%2520These%2520datasets%2520need%2520to%2520offer%2520diverse%2520and%250Achallenging%2520data%2520to%2520enable%2520a%2520comprehensive%2520assessment%2520of%2520model%2520performance%2520and%250Athe%2520creation%2520of%2520more%2520reliable%2520surveillance%2520systems%2520for%2520public%2520safety.%2520To%2520this%250Aend%252C%2520we%2520present%2520two%2520visual%2520object%2520detection%2520benchmarks%2520named%2520OD-VIRAT%2520Large%2520and%250AOD-VIRAT%2520Tiny%252C%2520aiming%2520at%2520advancing%2520visual%2520understanding%2520tasks%2520in%2520surveillance%250Aimagery.%2520The%2520video%2520sequences%2520in%2520both%2520benchmarks%2520cover%252010%2520different%2520scenes%2520of%250Ahuman%2520surveillance%2520recorded%2520from%2520significant%2520height%2520and%2520distance.%2520The%2520proposed%250Abenchmarks%2520offer%2520rich%2520annotations%2520of%2520bounding%2520boxes%2520and%2520categories%252C%2520where%250AOD-VIRAT%2520Large%2520has%25208.7%2520million%2520annotated%2520instances%2520in%2520599%252C996%2520images%2520and%250AOD-VIRAT%2520Tiny%2520has%2520288%252C901%2520annotated%2520instances%2520in%252019%252C860%2520images.%2520This%2520work%2520also%250Afocuses%2520on%2520benchmarking%2520state-of-the-art%2520object%2520detection%2520architectures%252C%250Aincluding%2520RETMDET%252C%2520YOLOX%252C%2520RetinaNet%252C%2520DETR%252C%2520and%2520Deformable-DETR%2520on%2520this%2520object%250Adetection-specific%2520variant%2520of%2520VIRAT%2520dataset.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520it%250Ais%2520the%2520first%2520work%2520to%2520examine%2520the%2520performance%2520of%2520these%2520recently%2520published%250Astate-of-the-art%2520object%2520detection%2520architectures%2520on%2520realistic%2520surveillance%250Aimagery%2520under%2520challenging%2520conditions%2520such%2520as%2520complex%2520backgrounds%252C%2520occluded%250Aobjects%252C%2520and%2520small-scale%2520objects.%2520The%2520proposed%2520benchmarking%2520and%2520experimental%250Asettings%2520will%2520help%2520in%2520providing%2520insights%2520concerning%2520the%2520performance%2520of%2520selected%250Aobject%2520detection%2520models%2520and%2520set%2520the%2520base%2520for%2520developing%2520more%2520efficient%2520and%250Arobust%2520object%2520detection%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OD-VIRAT%3A%20A%20Large-Scale%20Benchmark%20for%20Object%20Detection%20in%20Realistic%0A%20%20Surveillance%20Environments&entry.906535625=Hayat%20Ullah%20and%20Abbas%20Khan%20and%20Arslan%20Munir%20and%20Hari%20Kalva&entry.1292438233=%20%20Realistic%20human%20surveillance%20datasets%20are%20crucial%20for%20training%20and%20evaluating%0Acomputer%20vision%20models%20under%20real-world%20conditions%2C%20facilitating%20the%0Adevelopment%20of%20robust%20algorithms%20for%20human%20and%20human-interacting%20object%0Adetection%20in%20complex%20environments.%20These%20datasets%20need%20to%20offer%20diverse%20and%0Achallenging%20data%20to%20enable%20a%20comprehensive%20assessment%20of%20model%20performance%20and%0Athe%20creation%20of%20more%20reliable%20surveillance%20systems%20for%20public%20safety.%20To%20this%0Aend%2C%20we%20present%20two%20visual%20object%20detection%20benchmarks%20named%20OD-VIRAT%20Large%20and%0AOD-VIRAT%20Tiny%2C%20aiming%20at%20advancing%20visual%20understanding%20tasks%20in%20surveillance%0Aimagery.%20The%20video%20sequences%20in%20both%20benchmarks%20cover%2010%20different%20scenes%20of%0Ahuman%20surveillance%20recorded%20from%20significant%20height%20and%20distance.%20The%20proposed%0Abenchmarks%20offer%20rich%20annotations%20of%20bounding%20boxes%20and%20categories%2C%20where%0AOD-VIRAT%20Large%20has%208.7%20million%20annotated%20instances%20in%20599%2C996%20images%20and%0AOD-VIRAT%20Tiny%20has%20288%2C901%20annotated%20instances%20in%2019%2C860%20images.%20This%20work%20also%0Afocuses%20on%20benchmarking%20state-of-the-art%20object%20detection%20architectures%2C%0Aincluding%20RETMDET%2C%20YOLOX%2C%20RetinaNet%2C%20DETR%2C%20and%20Deformable-DETR%20on%20this%20object%0Adetection-specific%20variant%20of%20VIRAT%20dataset.%20To%20the%20best%20of%20our%20knowledge%2C%20it%0Ais%20the%20first%20work%20to%20examine%20the%20performance%20of%20these%20recently%20published%0Astate-of-the-art%20object%20detection%20architectures%20on%20realistic%20surveillance%0Aimagery%20under%20challenging%20conditions%20such%20as%20complex%20backgrounds%2C%20occluded%0Aobjects%2C%20and%20small-scale%20objects.%20The%20proposed%20benchmarking%20and%20experimental%0Asettings%20will%20help%20in%20providing%20insights%20concerning%20the%20performance%20of%20selected%0Aobject%20detection%20models%20and%20set%20the%20base%20for%20developing%20more%20efficient%20and%0Arobust%20object%20detection%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12396v2&entry.124074799=Read"},
{"title": "Software architecture and manual for novel versatile CT image analysis\n  toolbox -- AnatomyArchive", "author": "Lei Xu and Torkel B Brismar", "abstract": "  We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.\n", "link": "http://arxiv.org/abs/2507.13901v1", "date": "2025-07-18", "relevancy": 2.7855, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5751}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Software%20architecture%20and%20manual%20for%20novel%20versatile%20CT%20image%20analysis%0A%20%20toolbox%20--%20AnatomyArchive&body=Title%3A%20Software%20architecture%20and%20manual%20for%20novel%20versatile%20CT%20image%20analysis%0A%20%20toolbox%20--%20AnatomyArchive%0AAuthor%3A%20Lei%20Xu%20and%20Torkel%20B%20Brismar%0AAbstract%3A%20%20%20We%20have%20developed%20a%20novel%20CT%20image%20analysis%20package%20named%20AnatomyArchive%2C%0Abuilt%20on%20top%20of%20the%20recent%20full%20body%20segmentation%20model%20TotalSegmentator.%20It%0Aprovides%20automatic%20target%20volume%20selection%20and%20deselection%20capabilities%0Aaccording%20to%20user-configured%20anatomies%20for%20volumetric%20upper-%20and%20lower-bounds.%0AIt%20has%20a%20knowledge%20graph-based%20and%20time%20efficient%20tool%20for%20anatomy%20segmentation%0Amask%20management%20and%20medical%20image%20database%20maintenance.%20AnatomyArchive%20enables%0Aautomatic%20body%20volume%20cropping%2C%20as%20well%20as%20automatic%20arm-detection%20and%0Aexclusion%2C%20for%20more%20precise%20body%20composition%20analysis%20in%20both%202D%20and%203D%0Aformats.%20It%20provides%20robust%20voxel-based%20radiomic%20feature%20extraction%2C%20feature%0Avisualization%2C%20and%20an%20integrated%20toolchain%20for%20statistical%20tests%20and%20analysis.%0AA%20python-based%20GPU-accelerated%20nearly%20photo-realistic%20segmentation-integrated%0Acomposite%20cinematic%20rendering%20is%20also%20included.%20We%20present%20here%20its%20software%0Aarchitecture%20design%2C%20illustrate%20its%20workflow%20and%20working%20principle%20of%0Aalgorithms%20as%20well%20provide%20a%20few%20examples%20on%20how%20the%20software%20can%20be%20used%20to%0Aassist%20development%20of%20modern%20machine%20learning%20models.%20Open-source%20codes%20will%20be%0Areleased%20at%20https%3A//github.com/lxu-medai/AnatomyArchive%20for%20only%20research%20and%0Aeducational%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftware%2520architecture%2520and%2520manual%2520for%2520novel%2520versatile%2520CT%2520image%2520analysis%250A%2520%2520toolbox%2520--%2520AnatomyArchive%26entry.906535625%3DLei%2520Xu%2520and%2520Torkel%2520B%2520Brismar%26entry.1292438233%3D%2520%2520We%2520have%2520developed%2520a%2520novel%2520CT%2520image%2520analysis%2520package%2520named%2520AnatomyArchive%252C%250Abuilt%2520on%2520top%2520of%2520the%2520recent%2520full%2520body%2520segmentation%2520model%2520TotalSegmentator.%2520It%250Aprovides%2520automatic%2520target%2520volume%2520selection%2520and%2520deselection%2520capabilities%250Aaccording%2520to%2520user-configured%2520anatomies%2520for%2520volumetric%2520upper-%2520and%2520lower-bounds.%250AIt%2520has%2520a%2520knowledge%2520graph-based%2520and%2520time%2520efficient%2520tool%2520for%2520anatomy%2520segmentation%250Amask%2520management%2520and%2520medical%2520image%2520database%2520maintenance.%2520AnatomyArchive%2520enables%250Aautomatic%2520body%2520volume%2520cropping%252C%2520as%2520well%2520as%2520automatic%2520arm-detection%2520and%250Aexclusion%252C%2520for%2520more%2520precise%2520body%2520composition%2520analysis%2520in%2520both%25202D%2520and%25203D%250Aformats.%2520It%2520provides%2520robust%2520voxel-based%2520radiomic%2520feature%2520extraction%252C%2520feature%250Avisualization%252C%2520and%2520an%2520integrated%2520toolchain%2520for%2520statistical%2520tests%2520and%2520analysis.%250AA%2520python-based%2520GPU-accelerated%2520nearly%2520photo-realistic%2520segmentation-integrated%250Acomposite%2520cinematic%2520rendering%2520is%2520also%2520included.%2520We%2520present%2520here%2520its%2520software%250Aarchitecture%2520design%252C%2520illustrate%2520its%2520workflow%2520and%2520working%2520principle%2520of%250Aalgorithms%2520as%2520well%2520provide%2520a%2520few%2520examples%2520on%2520how%2520the%2520software%2520can%2520be%2520used%2520to%250Aassist%2520development%2520of%2520modern%2520machine%2520learning%2520models.%2520Open-source%2520codes%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/lxu-medai/AnatomyArchive%2520for%2520only%2520research%2520and%250Aeducational%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Software%20architecture%20and%20manual%20for%20novel%20versatile%20CT%20image%20analysis%0A%20%20toolbox%20--%20AnatomyArchive&entry.906535625=Lei%20Xu%20and%20Torkel%20B%20Brismar&entry.1292438233=%20%20We%20have%20developed%20a%20novel%20CT%20image%20analysis%20package%20named%20AnatomyArchive%2C%0Abuilt%20on%20top%20of%20the%20recent%20full%20body%20segmentation%20model%20TotalSegmentator.%20It%0Aprovides%20automatic%20target%20volume%20selection%20and%20deselection%20capabilities%0Aaccording%20to%20user-configured%20anatomies%20for%20volumetric%20upper-%20and%20lower-bounds.%0AIt%20has%20a%20knowledge%20graph-based%20and%20time%20efficient%20tool%20for%20anatomy%20segmentation%0Amask%20management%20and%20medical%20image%20database%20maintenance.%20AnatomyArchive%20enables%0Aautomatic%20body%20volume%20cropping%2C%20as%20well%20as%20automatic%20arm-detection%20and%0Aexclusion%2C%20for%20more%20precise%20body%20composition%20analysis%20in%20both%202D%20and%203D%0Aformats.%20It%20provides%20robust%20voxel-based%20radiomic%20feature%20extraction%2C%20feature%0Avisualization%2C%20and%20an%20integrated%20toolchain%20for%20statistical%20tests%20and%20analysis.%0AA%20python-based%20GPU-accelerated%20nearly%20photo-realistic%20segmentation-integrated%0Acomposite%20cinematic%20rendering%20is%20also%20included.%20We%20present%20here%20its%20software%0Aarchitecture%20design%2C%20illustrate%20its%20workflow%20and%20working%20principle%20of%0Aalgorithms%20as%20well%20provide%20a%20few%20examples%20on%20how%20the%20software%20can%20be%20used%20to%0Aassist%20development%20of%20modern%20machine%20learning%20models.%20Open-source%20codes%20will%20be%0Areleased%20at%20https%3A//github.com/lxu-medai/AnatomyArchive%20for%20only%20research%20and%0Aeducational%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13901v1&entry.124074799=Read"},
{"title": "DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial\n  Training", "author": "Jacob Piland and Chris Sweet and Adam Czajka", "abstract": "  Class Activation Mapping (CAM) and its gradient-based variants (e.g.,\nGradCAM) have become standard tools for explaining Convolutional Neural Network\n(CNN) predictions. However, these approaches typically focus on individual\nlogits, while for neural networks using softmax, the class membership\nprobability estimates depend \\textit{only} on the \\textit{differences} between\nlogits, not on their absolute values. This disconnect leaves standard CAMs\nvulnerable to adversarial manipulation, such as passive fooling, where a model\nis trained to produce misleading CAMs without affecting decision performance.\nWe introduce \\textbf{Salience-Hoax Activation Maps (SHAMs)}, an\n\\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM\nrobustness under adversarial conditions. To address the passive fooling\nvulnerability, we then propose \\textbf{DiffGradCAM}, a novel, lightweight, and\ncontrastive approach to class activation mapping that is both non-suceptible to\npassive fooling, but also matches the output of standard CAM methods such as\nGradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a\nnew framework for probing and improving the robustness of saliency-based\nexplanations. We validate both contributions across multi-class tasks with few\nand many classes.\n", "link": "http://arxiv.org/abs/2506.08514v2", "date": "2025-07-18", "relevancy": 2.7774, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.564}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5601}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffGradCAM%3A%20A%20Universal%20Class%20Activation%20Map%20Resistant%20to%20Adversarial%0A%20%20Training&body=Title%3A%20DiffGradCAM%3A%20A%20Universal%20Class%20Activation%20Map%20Resistant%20to%20Adversarial%0A%20%20Training%0AAuthor%3A%20Jacob%20Piland%20and%20Chris%20Sweet%20and%20Adam%20Czajka%0AAbstract%3A%20%20%20Class%20Activation%20Mapping%20%28CAM%29%20and%20its%20gradient-based%20variants%20%28e.g.%2C%0AGradCAM%29%20have%20become%20standard%20tools%20for%20explaining%20Convolutional%20Neural%20Network%0A%28CNN%29%20predictions.%20However%2C%20these%20approaches%20typically%20focus%20on%20individual%0Alogits%2C%20while%20for%20neural%20networks%20using%20softmax%2C%20the%20class%20membership%0Aprobability%20estimates%20depend%20%5Ctextit%7Bonly%7D%20on%20the%20%5Ctextit%7Bdifferences%7D%20between%0Alogits%2C%20not%20on%20their%20absolute%20values.%20This%20disconnect%20leaves%20standard%20CAMs%0Avulnerable%20to%20adversarial%20manipulation%2C%20such%20as%20passive%20fooling%2C%20where%20a%20model%0Ais%20trained%20to%20produce%20misleading%20CAMs%20without%20affecting%20decision%20performance.%0AWe%20introduce%20%5Ctextbf%7BSalience-Hoax%20Activation%20Maps%20%28SHAMs%29%7D%2C%20an%0A%5Cemph%7Bentropy-aware%20form%20of%20passive%20fooling%7D%20that%20serves%20as%20a%20benchmark%20for%20CAM%0Arobustness%20under%20adversarial%20conditions.%20To%20address%20the%20passive%20fooling%0Avulnerability%2C%20we%20then%20propose%20%5Ctextbf%7BDiffGradCAM%7D%2C%20a%20novel%2C%20lightweight%2C%20and%0Acontrastive%20approach%20to%20class%20activation%20mapping%20that%20is%20both%20non-suceptible%20to%0Apassive%20fooling%2C%20but%20also%20matches%20the%20output%20of%20standard%20CAM%20methods%20such%20as%0AGradCAM%20in%20the%20non-adversarial%20case.%20Together%2C%20SHAM%20and%20DiffGradCAM%20establish%20a%0Anew%20framework%20for%20probing%20and%20improving%20the%20robustness%20of%20saliency-based%0Aexplanations.%20We%20validate%20both%20contributions%20across%20multi-class%20tasks%20with%20few%0Aand%20many%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffGradCAM%253A%2520A%2520Universal%2520Class%2520Activation%2520Map%2520Resistant%2520to%2520Adversarial%250A%2520%2520Training%26entry.906535625%3DJacob%2520Piland%2520and%2520Chris%2520Sweet%2520and%2520Adam%2520Czajka%26entry.1292438233%3D%2520%2520Class%2520Activation%2520Mapping%2520%2528CAM%2529%2520and%2520its%2520gradient-based%2520variants%2520%2528e.g.%252C%250AGradCAM%2529%2520have%2520become%2520standard%2520tools%2520for%2520explaining%2520Convolutional%2520Neural%2520Network%250A%2528CNN%2529%2520predictions.%2520However%252C%2520these%2520approaches%2520typically%2520focus%2520on%2520individual%250Alogits%252C%2520while%2520for%2520neural%2520networks%2520using%2520softmax%252C%2520the%2520class%2520membership%250Aprobability%2520estimates%2520depend%2520%255Ctextit%257Bonly%257D%2520on%2520the%2520%255Ctextit%257Bdifferences%257D%2520between%250Alogits%252C%2520not%2520on%2520their%2520absolute%2520values.%2520This%2520disconnect%2520leaves%2520standard%2520CAMs%250Avulnerable%2520to%2520adversarial%2520manipulation%252C%2520such%2520as%2520passive%2520fooling%252C%2520where%2520a%2520model%250Ais%2520trained%2520to%2520produce%2520misleading%2520CAMs%2520without%2520affecting%2520decision%2520performance.%250AWe%2520introduce%2520%255Ctextbf%257BSalience-Hoax%2520Activation%2520Maps%2520%2528SHAMs%2529%257D%252C%2520an%250A%255Cemph%257Bentropy-aware%2520form%2520of%2520passive%2520fooling%257D%2520that%2520serves%2520as%2520a%2520benchmark%2520for%2520CAM%250Arobustness%2520under%2520adversarial%2520conditions.%2520To%2520address%2520the%2520passive%2520fooling%250Avulnerability%252C%2520we%2520then%2520propose%2520%255Ctextbf%257BDiffGradCAM%257D%252C%2520a%2520novel%252C%2520lightweight%252C%2520and%250Acontrastive%2520approach%2520to%2520class%2520activation%2520mapping%2520that%2520is%2520both%2520non-suceptible%2520to%250Apassive%2520fooling%252C%2520but%2520also%2520matches%2520the%2520output%2520of%2520standard%2520CAM%2520methods%2520such%2520as%250AGradCAM%2520in%2520the%2520non-adversarial%2520case.%2520Together%252C%2520SHAM%2520and%2520DiffGradCAM%2520establish%2520a%250Anew%2520framework%2520for%2520probing%2520and%2520improving%2520the%2520robustness%2520of%2520saliency-based%250Aexplanations.%2520We%2520validate%2520both%2520contributions%2520across%2520multi-class%2520tasks%2520with%2520few%250Aand%2520many%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffGradCAM%3A%20A%20Universal%20Class%20Activation%20Map%20Resistant%20to%20Adversarial%0A%20%20Training&entry.906535625=Jacob%20Piland%20and%20Chris%20Sweet%20and%20Adam%20Czajka&entry.1292438233=%20%20Class%20Activation%20Mapping%20%28CAM%29%20and%20its%20gradient-based%20variants%20%28e.g.%2C%0AGradCAM%29%20have%20become%20standard%20tools%20for%20explaining%20Convolutional%20Neural%20Network%0A%28CNN%29%20predictions.%20However%2C%20these%20approaches%20typically%20focus%20on%20individual%0Alogits%2C%20while%20for%20neural%20networks%20using%20softmax%2C%20the%20class%20membership%0Aprobability%20estimates%20depend%20%5Ctextit%7Bonly%7D%20on%20the%20%5Ctextit%7Bdifferences%7D%20between%0Alogits%2C%20not%20on%20their%20absolute%20values.%20This%20disconnect%20leaves%20standard%20CAMs%0Avulnerable%20to%20adversarial%20manipulation%2C%20such%20as%20passive%20fooling%2C%20where%20a%20model%0Ais%20trained%20to%20produce%20misleading%20CAMs%20without%20affecting%20decision%20performance.%0AWe%20introduce%20%5Ctextbf%7BSalience-Hoax%20Activation%20Maps%20%28SHAMs%29%7D%2C%20an%0A%5Cemph%7Bentropy-aware%20form%20of%20passive%20fooling%7D%20that%20serves%20as%20a%20benchmark%20for%20CAM%0Arobustness%20under%20adversarial%20conditions.%20To%20address%20the%20passive%20fooling%0Avulnerability%2C%20we%20then%20propose%20%5Ctextbf%7BDiffGradCAM%7D%2C%20a%20novel%2C%20lightweight%2C%20and%0Acontrastive%20approach%20to%20class%20activation%20mapping%20that%20is%20both%20non-suceptible%20to%0Apassive%20fooling%2C%20but%20also%20matches%20the%20output%20of%20standard%20CAM%20methods%20such%20as%0AGradCAM%20in%20the%20non-adversarial%20case.%20Together%2C%20SHAM%20and%20DiffGradCAM%20establish%20a%0Anew%20framework%20for%20probing%20and%20improving%20the%20robustness%20of%20saliency-based%0Aexplanations.%20We%20validate%20both%20contributions%20across%20multi-class%20tasks%20with%20few%0Aand%20many%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08514v2&entry.124074799=Read"},
{"title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation\n  Learning", "author": "Shashanka Venkataramanan and Valentinos Pariza and Mohammadreza Salehi and Lukas Knobel and Spyros Gidaris and Elias Ramzi and Andrei Bursuc and Yuki M. Asano", "abstract": "  We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.\n", "link": "http://arxiv.org/abs/2507.14137v1", "date": "2025-07-18", "relevancy": 2.766, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5555}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%0A%20%20Learning&body=Title%3A%20Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%0A%20%20Learning%0AAuthor%3A%20Shashanka%20Venkataramanan%20and%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Lukas%20Knobel%20and%20Spyros%20Gidaris%20and%20Elias%20Ramzi%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20%20%20We%20present%20Franca%20%28pronounced%20Fran-ka%29%3A%20free%20one%3B%20the%20first%20fully%20open-source%0A%28data%2C%20code%2C%20weights%29%20vision%20foundation%20model%20that%20matches%20and%20in%20many%20cases%0Asurpasses%20the%20performance%20of%20state-of-the-art%20proprietary%20models%2C%20e.g.%2C%20DINOv2%2C%0ACLIP%2C%20SigLIPv2%2C%20etc.%20Our%20approach%20is%20grounded%20in%20a%20transparent%20training%0Apipeline%20inspired%20by%20Web-SSL%20and%20uses%20publicly%20available%20data%3A%20ImageNet-21K%20and%0Aa%20subset%20of%20ReLAION-2B.%20Beyond%20model%20release%2C%20we%20tackle%20critical%20limitations%20in%0ASSL%20clustering%20methods.%20While%20modern%20models%20rely%20on%20assigning%20image%20features%20to%0Alarge%20codebooks%20via%20clustering%20algorithms%20like%20Sinkhorn-Knopp%2C%20they%20fail%20to%0Aaccount%20for%20the%20inherent%20ambiguity%20in%20clustering%20semantics.%20To%20address%20this%2C%20we%0Aintroduce%20a%20parameter-efficient%2C%20multi-head%20clustering%20projector%20based%20on%0Anested%20Matryoshka%20representations.%20This%20design%20progressively%20refines%20features%0Ainto%20increasingly%20fine-grained%20clusters%20without%20increasing%20the%20model%20size%2C%0Aenabling%20both%20performance%20and%20memory%20efficiency.%20Additionally%2C%20we%20propose%20a%0Anovel%20positional%20disentanglement%20strategy%20that%20explicitly%20removes%20positional%0Abiases%20from%20dense%20representations%2C%20thereby%20improving%20the%20encoding%20of%20semantic%0Acontent.%20This%20leads%20to%20consistent%20gains%20on%20several%20downstream%20benchmarks%2C%0Ademonstrating%20the%20utility%20of%20cleaner%20feature%20spaces.%20Our%20contributions%0Aestablish%20a%20new%20standard%20for%20transparent%2C%20high-performance%20vision%20models%20and%0Aopen%20a%20path%20toward%20more%20reproducible%20and%20generalizable%20foundation%20models%20for%0Athe%20broader%20AI%20community.%20The%20code%20and%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/valeoai/Franca.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFranca%253A%2520Nested%2520Matryoshka%2520Clustering%2520for%2520Scalable%2520Visual%2520Representation%250A%2520%2520Learning%26entry.906535625%3DShashanka%2520Venkataramanan%2520and%2520Valentinos%2520Pariza%2520and%2520Mohammadreza%2520Salehi%2520and%2520Lukas%2520Knobel%2520and%2520Spyros%2520Gidaris%2520and%2520Elias%2520Ramzi%2520and%2520Andrei%2520Bursuc%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3D%2520%2520We%2520present%2520Franca%2520%2528pronounced%2520Fran-ka%2529%253A%2520free%2520one%253B%2520the%2520first%2520fully%2520open-source%250A%2528data%252C%2520code%252C%2520weights%2529%2520vision%2520foundation%2520model%2520that%2520matches%2520and%2520in%2520many%2520cases%250Asurpasses%2520the%2520performance%2520of%2520state-of-the-art%2520proprietary%2520models%252C%2520e.g.%252C%2520DINOv2%252C%250ACLIP%252C%2520SigLIPv2%252C%2520etc.%2520Our%2520approach%2520is%2520grounded%2520in%2520a%2520transparent%2520training%250Apipeline%2520inspired%2520by%2520Web-SSL%2520and%2520uses%2520publicly%2520available%2520data%253A%2520ImageNet-21K%2520and%250Aa%2520subset%2520of%2520ReLAION-2B.%2520Beyond%2520model%2520release%252C%2520we%2520tackle%2520critical%2520limitations%2520in%250ASSL%2520clustering%2520methods.%2520While%2520modern%2520models%2520rely%2520on%2520assigning%2520image%2520features%2520to%250Alarge%2520codebooks%2520via%2520clustering%2520algorithms%2520like%2520Sinkhorn-Knopp%252C%2520they%2520fail%2520to%250Aaccount%2520for%2520the%2520inherent%2520ambiguity%2520in%2520clustering%2520semantics.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520parameter-efficient%252C%2520multi-head%2520clustering%2520projector%2520based%2520on%250Anested%2520Matryoshka%2520representations.%2520This%2520design%2520progressively%2520refines%2520features%250Ainto%2520increasingly%2520fine-grained%2520clusters%2520without%2520increasing%2520the%2520model%2520size%252C%250Aenabling%2520both%2520performance%2520and%2520memory%2520efficiency.%2520Additionally%252C%2520we%2520propose%2520a%250Anovel%2520positional%2520disentanglement%2520strategy%2520that%2520explicitly%2520removes%2520positional%250Abiases%2520from%2520dense%2520representations%252C%2520thereby%2520improving%2520the%2520encoding%2520of%2520semantic%250Acontent.%2520This%2520leads%2520to%2520consistent%2520gains%2520on%2520several%2520downstream%2520benchmarks%252C%250Ademonstrating%2520the%2520utility%2520of%2520cleaner%2520feature%2520spaces.%2520Our%2520contributions%250Aestablish%2520a%2520new%2520standard%2520for%2520transparent%252C%2520high-performance%2520vision%2520models%2520and%250Aopen%2520a%2520path%2520toward%2520more%2520reproducible%2520and%2520generalizable%2520foundation%2520models%2520for%250Athe%2520broader%2520AI%2520community.%2520The%2520code%2520and%2520model%2520checkpoints%2520are%2520available%2520at%250Ahttps%253A//github.com/valeoai/Franca.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Franca%3A%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%0A%20%20Learning&entry.906535625=Shashanka%20Venkataramanan%20and%20Valentinos%20Pariza%20and%20Mohammadreza%20Salehi%20and%20Lukas%20Knobel%20and%20Spyros%20Gidaris%20and%20Elias%20Ramzi%20and%20Andrei%20Bursuc%20and%20Yuki%20M.%20Asano&entry.1292438233=%20%20We%20present%20Franca%20%28pronounced%20Fran-ka%29%3A%20free%20one%3B%20the%20first%20fully%20open-source%0A%28data%2C%20code%2C%20weights%29%20vision%20foundation%20model%20that%20matches%20and%20in%20many%20cases%0Asurpasses%20the%20performance%20of%20state-of-the-art%20proprietary%20models%2C%20e.g.%2C%20DINOv2%2C%0ACLIP%2C%20SigLIPv2%2C%20etc.%20Our%20approach%20is%20grounded%20in%20a%20transparent%20training%0Apipeline%20inspired%20by%20Web-SSL%20and%20uses%20publicly%20available%20data%3A%20ImageNet-21K%20and%0Aa%20subset%20of%20ReLAION-2B.%20Beyond%20model%20release%2C%20we%20tackle%20critical%20limitations%20in%0ASSL%20clustering%20methods.%20While%20modern%20models%20rely%20on%20assigning%20image%20features%20to%0Alarge%20codebooks%20via%20clustering%20algorithms%20like%20Sinkhorn-Knopp%2C%20they%20fail%20to%0Aaccount%20for%20the%20inherent%20ambiguity%20in%20clustering%20semantics.%20To%20address%20this%2C%20we%0Aintroduce%20a%20parameter-efficient%2C%20multi-head%20clustering%20projector%20based%20on%0Anested%20Matryoshka%20representations.%20This%20design%20progressively%20refines%20features%0Ainto%20increasingly%20fine-grained%20clusters%20without%20increasing%20the%20model%20size%2C%0Aenabling%20both%20performance%20and%20memory%20efficiency.%20Additionally%2C%20we%20propose%20a%0Anovel%20positional%20disentanglement%20strategy%20that%20explicitly%20removes%20positional%0Abiases%20from%20dense%20representations%2C%20thereby%20improving%20the%20encoding%20of%20semantic%0Acontent.%20This%20leads%20to%20consistent%20gains%20on%20several%20downstream%20benchmarks%2C%0Ademonstrating%20the%20utility%20of%20cleaner%20feature%20spaces.%20Our%20contributions%0Aestablish%20a%20new%20standard%20for%20transparent%2C%20high-performance%20vision%20models%20and%0Aopen%20a%20path%20toward%20more%20reproducible%20and%20generalizable%20foundation%20models%20for%0Athe%20broader%20AI%20community.%20The%20code%20and%20model%20checkpoints%20are%20available%20at%0Ahttps%3A//github.com/valeoai/Franca.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14137v1&entry.124074799=Read"},
{"title": "SecurePose: Automated Face Blurring and Human Movement Kinematics\n  Extraction from Videos Recorded in Clinical Settings", "author": "Rishabh Bajpai and Bhooma Aravamuthan", "abstract": "  Movement disorder diagnosis often relies on expert evaluation of patient\nvideos, but sharing these videos poses privacy risks. Current methods for\nde-identifying videos, such as blurring faces, are often manual, inconsistent,\nor inaccurate. Furthermore, these methods can compromise objective kinematic\nanalysis - a crucial component of diagnosis. To address these challenges, we\ndeveloped SecurePose, an open-source software that simultaneously provides\nreliable de-identification and automated kinematic extraction from videos\nrecorded in clinic settings using smartphones/tablets. SecurePose utilizes pose\nestimation (using OpenPose) to extract full body kinematics, track individuals,\nidentify the patient, and then accurately blur faces in the videos. We\nvalidated SecurePose on gait videos recorded in outpatient clinic visits of 116\nchildren with cerebral palsy, assessing both the accuracy of its\nde-identification compared to the ground truth (manual blurring) and the\nreliability of the intermediate steps of kinematics extraction. Results\ndemonstrate that SecurePose outperformed six existing methods in automated face\ndetection and achieved comparable accuracy to robust manual blurring, but in\nsignificantly less time (91.08% faster). Ten experienced researchers also\nconfirmed SecurePose's usability via System Usability Scale scores. These\nfindings validate SecurePose as a practical and effective tool for protecting\npatient privacy while enabling accurate kinematics extraction in clinical\nsettings.\n", "link": "http://arxiv.org/abs/2402.14143v2", "date": "2025-07-18", "relevancy": 2.7515, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5636}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5594}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SecurePose%3A%20Automated%20Face%20Blurring%20and%20Human%20Movement%20Kinematics%0A%20%20Extraction%20from%20Videos%20Recorded%20in%20Clinical%20Settings&body=Title%3A%20SecurePose%3A%20Automated%20Face%20Blurring%20and%20Human%20Movement%20Kinematics%0A%20%20Extraction%20from%20Videos%20Recorded%20in%20Clinical%20Settings%0AAuthor%3A%20Rishabh%20Bajpai%20and%20Bhooma%20Aravamuthan%0AAbstract%3A%20%20%20Movement%20disorder%20diagnosis%20often%20relies%20on%20expert%20evaluation%20of%20patient%0Avideos%2C%20but%20sharing%20these%20videos%20poses%20privacy%20risks.%20Current%20methods%20for%0Ade-identifying%20videos%2C%20such%20as%20blurring%20faces%2C%20are%20often%20manual%2C%20inconsistent%2C%0Aor%20inaccurate.%20Furthermore%2C%20these%20methods%20can%20compromise%20objective%20kinematic%0Aanalysis%20-%20a%20crucial%20component%20of%20diagnosis.%20To%20address%20these%20challenges%2C%20we%0Adeveloped%20SecurePose%2C%20an%20open-source%20software%20that%20simultaneously%20provides%0Areliable%20de-identification%20and%20automated%20kinematic%20extraction%20from%20videos%0Arecorded%20in%20clinic%20settings%20using%20smartphones/tablets.%20SecurePose%20utilizes%20pose%0Aestimation%20%28using%20OpenPose%29%20to%20extract%20full%20body%20kinematics%2C%20track%20individuals%2C%0Aidentify%20the%20patient%2C%20and%20then%20accurately%20blur%20faces%20in%20the%20videos.%20We%0Avalidated%20SecurePose%20on%20gait%20videos%20recorded%20in%20outpatient%20clinic%20visits%20of%20116%0Achildren%20with%20cerebral%20palsy%2C%20assessing%20both%20the%20accuracy%20of%20its%0Ade-identification%20compared%20to%20the%20ground%20truth%20%28manual%20blurring%29%20and%20the%0Areliability%20of%20the%20intermediate%20steps%20of%20kinematics%20extraction.%20Results%0Ademonstrate%20that%20SecurePose%20outperformed%20six%20existing%20methods%20in%20automated%20face%0Adetection%20and%20achieved%20comparable%20accuracy%20to%20robust%20manual%20blurring%2C%20but%20in%0Asignificantly%20less%20time%20%2891.08%25%20faster%29.%20Ten%20experienced%20researchers%20also%0Aconfirmed%20SecurePose%27s%20usability%20via%20System%20Usability%20Scale%20scores.%20These%0Afindings%20validate%20SecurePose%20as%20a%20practical%20and%20effective%20tool%20for%20protecting%0Apatient%20privacy%20while%20enabling%20accurate%20kinematics%20extraction%20in%20clinical%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecurePose%253A%2520Automated%2520Face%2520Blurring%2520and%2520Human%2520Movement%2520Kinematics%250A%2520%2520Extraction%2520from%2520Videos%2520Recorded%2520in%2520Clinical%2520Settings%26entry.906535625%3DRishabh%2520Bajpai%2520and%2520Bhooma%2520Aravamuthan%26entry.1292438233%3D%2520%2520Movement%2520disorder%2520diagnosis%2520often%2520relies%2520on%2520expert%2520evaluation%2520of%2520patient%250Avideos%252C%2520but%2520sharing%2520these%2520videos%2520poses%2520privacy%2520risks.%2520Current%2520methods%2520for%250Ade-identifying%2520videos%252C%2520such%2520as%2520blurring%2520faces%252C%2520are%2520often%2520manual%252C%2520inconsistent%252C%250Aor%2520inaccurate.%2520Furthermore%252C%2520these%2520methods%2520can%2520compromise%2520objective%2520kinematic%250Aanalysis%2520-%2520a%2520crucial%2520component%2520of%2520diagnosis.%2520To%2520address%2520these%2520challenges%252C%2520we%250Adeveloped%2520SecurePose%252C%2520an%2520open-source%2520software%2520that%2520simultaneously%2520provides%250Areliable%2520de-identification%2520and%2520automated%2520kinematic%2520extraction%2520from%2520videos%250Arecorded%2520in%2520clinic%2520settings%2520using%2520smartphones/tablets.%2520SecurePose%2520utilizes%2520pose%250Aestimation%2520%2528using%2520OpenPose%2529%2520to%2520extract%2520full%2520body%2520kinematics%252C%2520track%2520individuals%252C%250Aidentify%2520the%2520patient%252C%2520and%2520then%2520accurately%2520blur%2520faces%2520in%2520the%2520videos.%2520We%250Avalidated%2520SecurePose%2520on%2520gait%2520videos%2520recorded%2520in%2520outpatient%2520clinic%2520visits%2520of%2520116%250Achildren%2520with%2520cerebral%2520palsy%252C%2520assessing%2520both%2520the%2520accuracy%2520of%2520its%250Ade-identification%2520compared%2520to%2520the%2520ground%2520truth%2520%2528manual%2520blurring%2529%2520and%2520the%250Areliability%2520of%2520the%2520intermediate%2520steps%2520of%2520kinematics%2520extraction.%2520Results%250Ademonstrate%2520that%2520SecurePose%2520outperformed%2520six%2520existing%2520methods%2520in%2520automated%2520face%250Adetection%2520and%2520achieved%2520comparable%2520accuracy%2520to%2520robust%2520manual%2520blurring%252C%2520but%2520in%250Asignificantly%2520less%2520time%2520%252891.08%2525%2520faster%2529.%2520Ten%2520experienced%2520researchers%2520also%250Aconfirmed%2520SecurePose%2527s%2520usability%2520via%2520System%2520Usability%2520Scale%2520scores.%2520These%250Afindings%2520validate%2520SecurePose%2520as%2520a%2520practical%2520and%2520effective%2520tool%2520for%2520protecting%250Apatient%2520privacy%2520while%2520enabling%2520accurate%2520kinematics%2520extraction%2520in%2520clinical%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SecurePose%3A%20Automated%20Face%20Blurring%20and%20Human%20Movement%20Kinematics%0A%20%20Extraction%20from%20Videos%20Recorded%20in%20Clinical%20Settings&entry.906535625=Rishabh%20Bajpai%20and%20Bhooma%20Aravamuthan&entry.1292438233=%20%20Movement%20disorder%20diagnosis%20often%20relies%20on%20expert%20evaluation%20of%20patient%0Avideos%2C%20but%20sharing%20these%20videos%20poses%20privacy%20risks.%20Current%20methods%20for%0Ade-identifying%20videos%2C%20such%20as%20blurring%20faces%2C%20are%20often%20manual%2C%20inconsistent%2C%0Aor%20inaccurate.%20Furthermore%2C%20these%20methods%20can%20compromise%20objective%20kinematic%0Aanalysis%20-%20a%20crucial%20component%20of%20diagnosis.%20To%20address%20these%20challenges%2C%20we%0Adeveloped%20SecurePose%2C%20an%20open-source%20software%20that%20simultaneously%20provides%0Areliable%20de-identification%20and%20automated%20kinematic%20extraction%20from%20videos%0Arecorded%20in%20clinic%20settings%20using%20smartphones/tablets.%20SecurePose%20utilizes%20pose%0Aestimation%20%28using%20OpenPose%29%20to%20extract%20full%20body%20kinematics%2C%20track%20individuals%2C%0Aidentify%20the%20patient%2C%20and%20then%20accurately%20blur%20faces%20in%20the%20videos.%20We%0Avalidated%20SecurePose%20on%20gait%20videos%20recorded%20in%20outpatient%20clinic%20visits%20of%20116%0Achildren%20with%20cerebral%20palsy%2C%20assessing%20both%20the%20accuracy%20of%20its%0Ade-identification%20compared%20to%20the%20ground%20truth%20%28manual%20blurring%29%20and%20the%0Areliability%20of%20the%20intermediate%20steps%20of%20kinematics%20extraction.%20Results%0Ademonstrate%20that%20SecurePose%20outperformed%20six%20existing%20methods%20in%20automated%20face%0Adetection%20and%20achieved%20comparable%20accuracy%20to%20robust%20manual%20blurring%2C%20but%20in%0Asignificantly%20less%20time%20%2891.08%25%20faster%29.%20Ten%20experienced%20researchers%20also%0Aconfirmed%20SecurePose%27s%20usability%20via%20System%20Usability%20Scale%20scores.%20These%0Afindings%20validate%20SecurePose%20as%20a%20practical%20and%20effective%20tool%20for%20protecting%0Apatient%20privacy%20while%20enabling%20accurate%20kinematics%20extraction%20in%20clinical%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14143v2&entry.124074799=Read"},
{"title": "Interpretable Imitation Learning via Generative Adversarial STL\n  Inference and Control", "author": "Wenliang Liu and Danyang Li and Erfan Aasi and Daniela Rus and Roberto Tron and Calin Belta", "abstract": "  Imitation learning methods have demonstrated considerable success in teaching\nautonomous systems complex tasks through expert demonstrations. However, a\nlimitation of these methods is their lack of interpretability, particularly in\nunderstanding the specific task the learning agent aims to accomplish. In this\npaper, we propose a novel imitation learning method that combines Signal\nTemporal Logic (STL) inference and control synthesis, enabling the explicit\nrepresentation of the task as an STL formula. This approach not only provides a\nclear understanding of the task but also supports the integration of human\nknowledge and allows for adaptation to out-of-distribution scenarios by\nmanually adjusting the STL formulas and fine-tuning the policy. We employ a\nGenerative Adversarial Network (GAN)-inspired approach to train both the\ninference and policy networks, effectively narrowing the gap between expert and\nlearned policies. The efficiency of our algorithm is demonstrated through\nsimulations, showcasing its practical applicability and adaptability.\n", "link": "http://arxiv.org/abs/2402.10310v2", "date": "2025-07-18", "relevancy": 2.7023, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5596}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5556}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Imitation%20Learning%20via%20Generative%20Adversarial%20STL%0A%20%20Inference%20and%20Control&body=Title%3A%20Interpretable%20Imitation%20Learning%20via%20Generative%20Adversarial%20STL%0A%20%20Inference%20and%20Control%0AAuthor%3A%20Wenliang%20Liu%20and%20Danyang%20Li%20and%20Erfan%20Aasi%20and%20Daniela%20Rus%20and%20Roberto%20Tron%20and%20Calin%20Belta%0AAbstract%3A%20%20%20Imitation%20learning%20methods%20have%20demonstrated%20considerable%20success%20in%20teaching%0Aautonomous%20systems%20complex%20tasks%20through%20expert%20demonstrations.%20However%2C%20a%0Alimitation%20of%20these%20methods%20is%20their%20lack%20of%20interpretability%2C%20particularly%20in%0Aunderstanding%20the%20specific%20task%20the%20learning%20agent%20aims%20to%20accomplish.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20imitation%20learning%20method%20that%20combines%20Signal%0ATemporal%20Logic%20%28STL%29%20inference%20and%20control%20synthesis%2C%20enabling%20the%20explicit%0Arepresentation%20of%20the%20task%20as%20an%20STL%20formula.%20This%20approach%20not%20only%20provides%20a%0Aclear%20understanding%20of%20the%20task%20but%20also%20supports%20the%20integration%20of%20human%0Aknowledge%20and%20allows%20for%20adaptation%20to%20out-of-distribution%20scenarios%20by%0Amanually%20adjusting%20the%20STL%20formulas%20and%20fine-tuning%20the%20policy.%20We%20employ%20a%0AGenerative%20Adversarial%20Network%20%28GAN%29-inspired%20approach%20to%20train%20both%20the%0Ainference%20and%20policy%20networks%2C%20effectively%20narrowing%20the%20gap%20between%20expert%20and%0Alearned%20policies.%20The%20efficiency%20of%20our%20algorithm%20is%20demonstrated%20through%0Asimulations%2C%20showcasing%20its%20practical%20applicability%20and%20adaptability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10310v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Imitation%2520Learning%2520via%2520Generative%2520Adversarial%2520STL%250A%2520%2520Inference%2520and%2520Control%26entry.906535625%3DWenliang%2520Liu%2520and%2520Danyang%2520Li%2520and%2520Erfan%2520Aasi%2520and%2520Daniela%2520Rus%2520and%2520Roberto%2520Tron%2520and%2520Calin%2520Belta%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520methods%2520have%2520demonstrated%2520considerable%2520success%2520in%2520teaching%250Aautonomous%2520systems%2520complex%2520tasks%2520through%2520expert%2520demonstrations.%2520However%252C%2520a%250Alimitation%2520of%2520these%2520methods%2520is%2520their%2520lack%2520of%2520interpretability%252C%2520particularly%2520in%250Aunderstanding%2520the%2520specific%2520task%2520the%2520learning%2520agent%2520aims%2520to%2520accomplish.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520imitation%2520learning%2520method%2520that%2520combines%2520Signal%250ATemporal%2520Logic%2520%2528STL%2529%2520inference%2520and%2520control%2520synthesis%252C%2520enabling%2520the%2520explicit%250Arepresentation%2520of%2520the%2520task%2520as%2520an%2520STL%2520formula.%2520This%2520approach%2520not%2520only%2520provides%2520a%250Aclear%2520understanding%2520of%2520the%2520task%2520but%2520also%2520supports%2520the%2520integration%2520of%2520human%250Aknowledge%2520and%2520allows%2520for%2520adaptation%2520to%2520out-of-distribution%2520scenarios%2520by%250Amanually%2520adjusting%2520the%2520STL%2520formulas%2520and%2520fine-tuning%2520the%2520policy.%2520We%2520employ%2520a%250AGenerative%2520Adversarial%2520Network%2520%2528GAN%2529-inspired%2520approach%2520to%2520train%2520both%2520the%250Ainference%2520and%2520policy%2520networks%252C%2520effectively%2520narrowing%2520the%2520gap%2520between%2520expert%2520and%250Alearned%2520policies.%2520The%2520efficiency%2520of%2520our%2520algorithm%2520is%2520demonstrated%2520through%250Asimulations%252C%2520showcasing%2520its%2520practical%2520applicability%2520and%2520adaptability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10310v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Imitation%20Learning%20via%20Generative%20Adversarial%20STL%0A%20%20Inference%20and%20Control&entry.906535625=Wenliang%20Liu%20and%20Danyang%20Li%20and%20Erfan%20Aasi%20and%20Daniela%20Rus%20and%20Roberto%20Tron%20and%20Calin%20Belta&entry.1292438233=%20%20Imitation%20learning%20methods%20have%20demonstrated%20considerable%20success%20in%20teaching%0Aautonomous%20systems%20complex%20tasks%20through%20expert%20demonstrations.%20However%2C%20a%0Alimitation%20of%20these%20methods%20is%20their%20lack%20of%20interpretability%2C%20particularly%20in%0Aunderstanding%20the%20specific%20task%20the%20learning%20agent%20aims%20to%20accomplish.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20imitation%20learning%20method%20that%20combines%20Signal%0ATemporal%20Logic%20%28STL%29%20inference%20and%20control%20synthesis%2C%20enabling%20the%20explicit%0Arepresentation%20of%20the%20task%20as%20an%20STL%20formula.%20This%20approach%20not%20only%20provides%20a%0Aclear%20understanding%20of%20the%20task%20but%20also%20supports%20the%20integration%20of%20human%0Aknowledge%20and%20allows%20for%20adaptation%20to%20out-of-distribution%20scenarios%20by%0Amanually%20adjusting%20the%20STL%20formulas%20and%20fine-tuning%20the%20policy.%20We%20employ%20a%0AGenerative%20Adversarial%20Network%20%28GAN%29-inspired%20approach%20to%20train%20both%20the%0Ainference%20and%20policy%20networks%2C%20effectively%20narrowing%20the%20gap%20between%20expert%20and%0Alearned%20policies.%20The%20efficiency%20of%20our%20algorithm%20is%20demonstrated%20through%0Asimulations%2C%20showcasing%20its%20practical%20applicability%20and%20adaptability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10310v2&entry.124074799=Read"},
{"title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional\n  Framework and Benchmark Dataset", "author": "Sara Abdulaziz and Giacomo D'Amicantonio and Egor Bondarev", "abstract": "  Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.\n", "link": "http://arxiv.org/abs/2507.13981v1", "date": "2025-07-18", "relevancy": 2.6741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5205}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Human%20Visual%20Privacy%20Protection%3A%20A%20Three-Dimensional%0A%20%20Framework%20and%20Benchmark%20Dataset&body=Title%3A%20Evaluation%20of%20Human%20Visual%20Privacy%20Protection%3A%20A%20Three-Dimensional%0A%20%20Framework%20and%20Benchmark%20Dataset%0AAuthor%3A%20Sara%20Abdulaziz%20and%20Giacomo%20D%27Amicantonio%20and%20Egor%20Bondarev%0AAbstract%3A%20%20%20Recent%20advances%20in%20AI-powered%20surveillance%20have%20intensified%20concerns%20over%20the%0Acollection%20and%20processing%20of%20sensitive%20personal%20data.%20In%20response%2C%20research%20has%0Aincreasingly%20focused%20on%20privacy-by-design%20solutions%2C%20raising%20the%20need%20for%0Aobjective%20techniques%20to%20evaluate%20privacy%20protection.%20This%20paper%20presents%20a%0Acomprehensive%20framework%20for%20evaluating%20visual%20privacy-protection%20methods%20across%0Athree%20dimensions%3A%20privacy%2C%20utility%2C%20and%20practicality.%20In%20addition%2C%20it%0Aintroduces%20HR-VISPR%2C%20a%20publicly%20available%20human-centric%20dataset%20with%20biometric%2C%0Asoft-biometric%2C%20and%20non-biometric%20labels%20to%20train%20an%20interpretable%20privacy%0Ametric.%20We%20evaluate%2011%20privacy%20protection%20methods%2C%20ranging%20from%20conventional%0Atechniques%20to%20advanced%20deep-learning%20methods%2C%20through%20the%20proposed%20framework.%0AThe%20framework%20differentiates%20privacy%20levels%20in%20alignment%20with%20human%20visual%0Aperception%2C%20while%20highlighting%20trade-offs%20between%20privacy%2C%20utility%2C%20and%0Apracticality.%20This%20study%2C%20along%20with%20the%20HR-VISPR%20dataset%2C%20serves%20as%20an%0Ainsightful%20tool%20and%20offers%20a%20structured%20evaluation%20framework%20applicable%20across%0Adiverse%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Human%2520Visual%2520Privacy%2520Protection%253A%2520A%2520Three-Dimensional%250A%2520%2520Framework%2520and%2520Benchmark%2520Dataset%26entry.906535625%3DSara%2520Abdulaziz%2520and%2520Giacomo%2520D%2527Amicantonio%2520and%2520Egor%2520Bondarev%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520AI-powered%2520surveillance%2520have%2520intensified%2520concerns%2520over%2520the%250Acollection%2520and%2520processing%2520of%2520sensitive%2520personal%2520data.%2520In%2520response%252C%2520research%2520has%250Aincreasingly%2520focused%2520on%2520privacy-by-design%2520solutions%252C%2520raising%2520the%2520need%2520for%250Aobjective%2520techniques%2520to%2520evaluate%2520privacy%2520protection.%2520This%2520paper%2520presents%2520a%250Acomprehensive%2520framework%2520for%2520evaluating%2520visual%2520privacy-protection%2520methods%2520across%250Athree%2520dimensions%253A%2520privacy%252C%2520utility%252C%2520and%2520practicality.%2520In%2520addition%252C%2520it%250Aintroduces%2520HR-VISPR%252C%2520a%2520publicly%2520available%2520human-centric%2520dataset%2520with%2520biometric%252C%250Asoft-biometric%252C%2520and%2520non-biometric%2520labels%2520to%2520train%2520an%2520interpretable%2520privacy%250Ametric.%2520We%2520evaluate%252011%2520privacy%2520protection%2520methods%252C%2520ranging%2520from%2520conventional%250Atechniques%2520to%2520advanced%2520deep-learning%2520methods%252C%2520through%2520the%2520proposed%2520framework.%250AThe%2520framework%2520differentiates%2520privacy%2520levels%2520in%2520alignment%2520with%2520human%2520visual%250Aperception%252C%2520while%2520highlighting%2520trade-offs%2520between%2520privacy%252C%2520utility%252C%2520and%250Apracticality.%2520This%2520study%252C%2520along%2520with%2520the%2520HR-VISPR%2520dataset%252C%2520serves%2520as%2520an%250Ainsightful%2520tool%2520and%2520offers%2520a%2520structured%2520evaluation%2520framework%2520applicable%2520across%250Adiverse%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Human%20Visual%20Privacy%20Protection%3A%20A%20Three-Dimensional%0A%20%20Framework%20and%20Benchmark%20Dataset&entry.906535625=Sara%20Abdulaziz%20and%20Giacomo%20D%27Amicantonio%20and%20Egor%20Bondarev&entry.1292438233=%20%20Recent%20advances%20in%20AI-powered%20surveillance%20have%20intensified%20concerns%20over%20the%0Acollection%20and%20processing%20of%20sensitive%20personal%20data.%20In%20response%2C%20research%20has%0Aincreasingly%20focused%20on%20privacy-by-design%20solutions%2C%20raising%20the%20need%20for%0Aobjective%20techniques%20to%20evaluate%20privacy%20protection.%20This%20paper%20presents%20a%0Acomprehensive%20framework%20for%20evaluating%20visual%20privacy-protection%20methods%20across%0Athree%20dimensions%3A%20privacy%2C%20utility%2C%20and%20practicality.%20In%20addition%2C%20it%0Aintroduces%20HR-VISPR%2C%20a%20publicly%20available%20human-centric%20dataset%20with%20biometric%2C%0Asoft-biometric%2C%20and%20non-biometric%20labels%20to%20train%20an%20interpretable%20privacy%0Ametric.%20We%20evaluate%2011%20privacy%20protection%20methods%2C%20ranging%20from%20conventional%0Atechniques%20to%20advanced%20deep-learning%20methods%2C%20through%20the%20proposed%20framework.%0AThe%20framework%20differentiates%20privacy%20levels%20in%20alignment%20with%20human%20visual%0Aperception%2C%20while%20highlighting%20trade-offs%20between%20privacy%2C%20utility%2C%20and%0Apracticality.%20This%20study%2C%20along%20with%20the%20HR-VISPR%20dataset%2C%20serves%20as%20an%0Ainsightful%20tool%20and%20offers%20a%20structured%20evaluation%20framework%20applicable%20across%0Adiverse%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13981v1&entry.124074799=Read"},
{"title": "Structural Connectome Harmonization Using Deep Learning: The Strength of\n  Graph Neural Networks", "author": "Jagruti Patel and Thomas A. W. Bolton and Mikkel Sch\u00f6ttner and Anjali Tarun and Sebastien Tourbier and Yasser Alem\u00e0n-G\u00f2mez and Jonas Richiardi and Patric Hagmann", "abstract": "  Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.\n", "link": "http://arxiv.org/abs/2507.13992v1", "date": "2025-07-18", "relevancy": 2.6442, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Connectome%20Harmonization%20Using%20Deep%20Learning%3A%20The%20Strength%20of%0A%20%20Graph%20Neural%20Networks&body=Title%3A%20Structural%20Connectome%20Harmonization%20Using%20Deep%20Learning%3A%20The%20Strength%20of%0A%20%20Graph%20Neural%20Networks%0AAuthor%3A%20Jagruti%20Patel%20and%20Thomas%20A.%20W.%20Bolton%20and%20Mikkel%20Sch%C3%B6ttner%20and%20Anjali%20Tarun%20and%20Sebastien%20Tourbier%20and%20Yasser%20Alem%C3%A0n-G%C3%B2mez%20and%20Jonas%20Richiardi%20and%20Patric%20Hagmann%0AAbstract%3A%20%20%20Small%20sample%20sizes%20in%20neuroimaging%20in%20general%2C%20and%20in%20structural%20connectome%0A%28SC%29%20studies%20in%20particular%20limit%20the%20development%20of%20reliable%20biomarkers%20for%0Aneurological%20and%20psychiatric%20disorders%20-%20such%20as%20Alzheimer%27s%20disease%20and%0Aschizophrenia%20-%20by%20reducing%20statistical%20power%2C%20reliability%2C%20and%0Ageneralizability.%20Large-scale%20multi-site%20studies%20have%20exist%2C%20but%20they%20have%0Aacquisition-related%20biases%20due%20to%20scanner%20heterogeneity%2C%20compromising%20imaging%0Aconsistency%20and%20downstream%20analyses.%20While%20existing%20SC%20harmonization%20methods%20-%0Asuch%20as%20linear%20regression%20%28LR%29%2C%20ComBat%2C%20and%20deep%20learning%20techniques%20-%20mitigate%0Athese%20biases%2C%20they%20often%20rely%20on%20detailed%20metadata%2C%20traveling%20subjects%20%28TS%29%2C%20or%0Aoverlook%20the%20graph-topology%20of%20SCs.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Asite-conditioned%20deep%20harmonization%20framework%20that%20harmonizes%20SCs%20across%0Adiverse%20acquisition%20sites%20without%20requiring%20metadata%20or%20TS%20that%20we%20test%20in%20a%0Asimulated%20scenario%20based%20on%20the%20Human%20Connectome%20Dataset.%20Within%20this%0Aframework%2C%20we%20benchmark%20three%20deep%20architectures%20-%20a%20fully%20connected%0Aautoencoder%20%28AE%29%2C%20a%20convolutional%20AE%2C%20and%20a%20graph%20convolutional%20AE%20-%20against%20a%0Atop-performing%20LR%20baseline.%20While%20non-graph%20models%20excel%20in%20edge-weight%0Aprediction%20and%20edge%20existence%20detection%2C%20the%20graph%20AE%20demonstrates%20superior%0Apreservation%20of%20topological%20structure%20and%20subject-level%20individuality%2C%20as%0Areflected%20by%20graph%20metrics%20and%20fingerprinting%20accuracy%2C%20respectively.%20Although%0Athe%20LR%20baseline%20achieves%20the%20highest%20numerical%20performance%20by%20explicitly%0Amodeling%20acquisition%20parameters%2C%20it%20lacks%20applicability%20to%20real-world%0Amulti-site%20use%20cases%20as%20detailed%20acquisition%20metadata%20is%20often%20unavailable.%20Our%0Aresults%20highlight%20the%20critical%20role%20of%20model%20architecture%20in%20SC%20harmonization%0Aperformance%20and%20demonstrate%20that%20graph-based%20approaches%20are%20particularly%0Awell-suited%20for%20structure-aware%2C%20domain-generalizable%20SC%20harmonization%20in%0Alarge-scale%20multi-site%20SC%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Connectome%2520Harmonization%2520Using%2520Deep%2520Learning%253A%2520The%2520Strength%2520of%250A%2520%2520Graph%2520Neural%2520Networks%26entry.906535625%3DJagruti%2520Patel%2520and%2520Thomas%2520A.%2520W.%2520Bolton%2520and%2520Mikkel%2520Sch%25C3%25B6ttner%2520and%2520Anjali%2520Tarun%2520and%2520Sebastien%2520Tourbier%2520and%2520Yasser%2520Alem%25C3%25A0n-G%25C3%25B2mez%2520and%2520Jonas%2520Richiardi%2520and%2520Patric%2520Hagmann%26entry.1292438233%3D%2520%2520Small%2520sample%2520sizes%2520in%2520neuroimaging%2520in%2520general%252C%2520and%2520in%2520structural%2520connectome%250A%2528SC%2529%2520studies%2520in%2520particular%2520limit%2520the%2520development%2520of%2520reliable%2520biomarkers%2520for%250Aneurological%2520and%2520psychiatric%2520disorders%2520-%2520such%2520as%2520Alzheimer%2527s%2520disease%2520and%250Aschizophrenia%2520-%2520by%2520reducing%2520statistical%2520power%252C%2520reliability%252C%2520and%250Ageneralizability.%2520Large-scale%2520multi-site%2520studies%2520have%2520exist%252C%2520but%2520they%2520have%250Aacquisition-related%2520biases%2520due%2520to%2520scanner%2520heterogeneity%252C%2520compromising%2520imaging%250Aconsistency%2520and%2520downstream%2520analyses.%2520While%2520existing%2520SC%2520harmonization%2520methods%2520-%250Asuch%2520as%2520linear%2520regression%2520%2528LR%2529%252C%2520ComBat%252C%2520and%2520deep%2520learning%2520techniques%2520-%2520mitigate%250Athese%2520biases%252C%2520they%2520often%2520rely%2520on%2520detailed%2520metadata%252C%2520traveling%2520subjects%2520%2528TS%2529%252C%2520or%250Aoverlook%2520the%2520graph-topology%2520of%2520SCs.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Asite-conditioned%2520deep%2520harmonization%2520framework%2520that%2520harmonizes%2520SCs%2520across%250Adiverse%2520acquisition%2520sites%2520without%2520requiring%2520metadata%2520or%2520TS%2520that%2520we%2520test%2520in%2520a%250Asimulated%2520scenario%2520based%2520on%2520the%2520Human%2520Connectome%2520Dataset.%2520Within%2520this%250Aframework%252C%2520we%2520benchmark%2520three%2520deep%2520architectures%2520-%2520a%2520fully%2520connected%250Aautoencoder%2520%2528AE%2529%252C%2520a%2520convolutional%2520AE%252C%2520and%2520a%2520graph%2520convolutional%2520AE%2520-%2520against%2520a%250Atop-performing%2520LR%2520baseline.%2520While%2520non-graph%2520models%2520excel%2520in%2520edge-weight%250Aprediction%2520and%2520edge%2520existence%2520detection%252C%2520the%2520graph%2520AE%2520demonstrates%2520superior%250Apreservation%2520of%2520topological%2520structure%2520and%2520subject-level%2520individuality%252C%2520as%250Areflected%2520by%2520graph%2520metrics%2520and%2520fingerprinting%2520accuracy%252C%2520respectively.%2520Although%250Athe%2520LR%2520baseline%2520achieves%2520the%2520highest%2520numerical%2520performance%2520by%2520explicitly%250Amodeling%2520acquisition%2520parameters%252C%2520it%2520lacks%2520applicability%2520to%2520real-world%250Amulti-site%2520use%2520cases%2520as%2520detailed%2520acquisition%2520metadata%2520is%2520often%2520unavailable.%2520Our%250Aresults%2520highlight%2520the%2520critical%2520role%2520of%2520model%2520architecture%2520in%2520SC%2520harmonization%250Aperformance%2520and%2520demonstrate%2520that%2520graph-based%2520approaches%2520are%2520particularly%250Awell-suited%2520for%2520structure-aware%252C%2520domain-generalizable%2520SC%2520harmonization%2520in%250Alarge-scale%2520multi-site%2520SC%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Connectome%20Harmonization%20Using%20Deep%20Learning%3A%20The%20Strength%20of%0A%20%20Graph%20Neural%20Networks&entry.906535625=Jagruti%20Patel%20and%20Thomas%20A.%20W.%20Bolton%20and%20Mikkel%20Sch%C3%B6ttner%20and%20Anjali%20Tarun%20and%20Sebastien%20Tourbier%20and%20Yasser%20Alem%C3%A0n-G%C3%B2mez%20and%20Jonas%20Richiardi%20and%20Patric%20Hagmann&entry.1292438233=%20%20Small%20sample%20sizes%20in%20neuroimaging%20in%20general%2C%20and%20in%20structural%20connectome%0A%28SC%29%20studies%20in%20particular%20limit%20the%20development%20of%20reliable%20biomarkers%20for%0Aneurological%20and%20psychiatric%20disorders%20-%20such%20as%20Alzheimer%27s%20disease%20and%0Aschizophrenia%20-%20by%20reducing%20statistical%20power%2C%20reliability%2C%20and%0Ageneralizability.%20Large-scale%20multi-site%20studies%20have%20exist%2C%20but%20they%20have%0Aacquisition-related%20biases%20due%20to%20scanner%20heterogeneity%2C%20compromising%20imaging%0Aconsistency%20and%20downstream%20analyses.%20While%20existing%20SC%20harmonization%20methods%20-%0Asuch%20as%20linear%20regression%20%28LR%29%2C%20ComBat%2C%20and%20deep%20learning%20techniques%20-%20mitigate%0Athese%20biases%2C%20they%20often%20rely%20on%20detailed%20metadata%2C%20traveling%20subjects%20%28TS%29%2C%20or%0Aoverlook%20the%20graph-topology%20of%20SCs.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Asite-conditioned%20deep%20harmonization%20framework%20that%20harmonizes%20SCs%20across%0Adiverse%20acquisition%20sites%20without%20requiring%20metadata%20or%20TS%20that%20we%20test%20in%20a%0Asimulated%20scenario%20based%20on%20the%20Human%20Connectome%20Dataset.%20Within%20this%0Aframework%2C%20we%20benchmark%20three%20deep%20architectures%20-%20a%20fully%20connected%0Aautoencoder%20%28AE%29%2C%20a%20convolutional%20AE%2C%20and%20a%20graph%20convolutional%20AE%20-%20against%20a%0Atop-performing%20LR%20baseline.%20While%20non-graph%20models%20excel%20in%20edge-weight%0Aprediction%20and%20edge%20existence%20detection%2C%20the%20graph%20AE%20demonstrates%20superior%0Apreservation%20of%20topological%20structure%20and%20subject-level%20individuality%2C%20as%0Areflected%20by%20graph%20metrics%20and%20fingerprinting%20accuracy%2C%20respectively.%20Although%0Athe%20LR%20baseline%20achieves%20the%20highest%20numerical%20performance%20by%20explicitly%0Amodeling%20acquisition%20parameters%2C%20it%20lacks%20applicability%20to%20real-world%0Amulti-site%20use%20cases%20as%20detailed%20acquisition%20metadata%20is%20often%20unavailable.%20Our%0Aresults%20highlight%20the%20critical%20role%20of%20model%20architecture%20in%20SC%20harmonization%0Aperformance%20and%20demonstrate%20that%20graph-based%20approaches%20are%20particularly%0Awell-suited%20for%20structure-aware%2C%20domain-generalizable%20SC%20harmonization%20in%0Alarge-scale%20multi-site%20SC%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13992v1&entry.124074799=Read"},
{"title": "DUALRec: A Hybrid Sequential and Language Model Framework for\n  Context-Aware Movie Recommendation", "author": "Yitong Li and Raoul Grasman", "abstract": "  The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.\n", "link": "http://arxiv.org/abs/2507.13957v1", "date": "2025-07-18", "relevancy": 2.6185, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DUALRec%3A%20A%20Hybrid%20Sequential%20and%20Language%20Model%20Framework%20for%0A%20%20Context-Aware%20Movie%20Recommendation&body=Title%3A%20DUALRec%3A%20A%20Hybrid%20Sequential%20and%20Language%20Model%20Framework%20for%0A%20%20Context-Aware%20Movie%20Recommendation%0AAuthor%3A%20Yitong%20Li%20and%20Raoul%20Grasman%0AAbstract%3A%20%20%20The%20modern%20recommender%20systems%20are%20facing%20an%20increasing%20challenge%20of%0Amodelling%20and%20predicting%20the%20dynamic%20and%20context-rich%20user%20preferences.%0ATraditional%20collaborative%20filtering%20and%20content-based%20methods%20often%20struggle%20to%0Acapture%20the%20temporal%20patternings%20and%20evolving%20user%20intentions.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20gained%20gradual%20attention%20in%20recent%20years%2C%20by%20their%0Astrong%20semantic%20understanding%20and%20reasoning%20abilities%2C%20they%20are%20not%20inherently%0Adesigned%20to%20model%20chronologically%20evolving%20user%20preference%20and%20intentions.%20On%0Athe%20other%20hand%2C%20for%20sequential%20models%20like%20LSTM%20%28Long-Short-Term-Memory%29%20which%0Ais%20good%20at%20capturing%20the%20temporal%20dynamics%20of%20user%20behaviour%20and%20evolving%20user%0Apreference%20over%20time%2C%20but%20still%20lacks%20a%20rich%20semantic%20understanding%20for%0Acomprehensive%20recommendation%20generation.%20In%20this%20study%2C%20we%20propose%20DUALRec%0A%28Dynamic%20User-Aware%20Language-based%20Recommender%29%2C%20a%20novel%20recommender%20that%0Aleverages%20the%20complementary%20strength%20of%20both%20models%2C%20which%20combines%20the%0Atemporal%20modelling%20abilities%20of%20LSTM%20networks%20with%20semantic%20reasoning%20power%20of%0Athe%20fine-tuned%20Large%20Language%20Models.%20The%20LSTM%20component%20will%20capture%20users%0Aevolving%20preference%20through%20their%20viewing%20history%2C%20while%20the%20fine-tuned%20LLM%0Avariants%20will%20leverage%20these%20temporal%20user%20insights%20to%20generate%20next%20movies%0Athat%20users%20might%20enjoy.%20Experimental%20results%20on%20MovieLens-1M%20dataset%20shows%20that%0Athe%20DUALRec%20model%20outperforms%20a%20wide%20range%20of%20baseline%20models%2C%20with%0Acomprehensive%20evaluation%20matrices%20of%20Hit%20Rate%20%28HR%40k%29%2C%20Normalized%20Discounted%0ACumulative%20Gain%20%28NDCG%40k%29%2C%20and%20genre%20similarity%20metrics.%20This%20research%20proposes%0Aa%20novel%20architecture%20that%20bridges%20the%20gap%20between%20temporal%20sequence%20modeling%0Aand%20semantic%20reasoning%2C%20and%20offers%20a%20promising%20direction%20for%20developing%20more%0Aintelligent%20and%20context-aware%20recommenders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDUALRec%253A%2520A%2520Hybrid%2520Sequential%2520and%2520Language%2520Model%2520Framework%2520for%250A%2520%2520Context-Aware%2520Movie%2520Recommendation%26entry.906535625%3DYitong%2520Li%2520and%2520Raoul%2520Grasman%26entry.1292438233%3D%2520%2520The%2520modern%2520recommender%2520systems%2520are%2520facing%2520an%2520increasing%2520challenge%2520of%250Amodelling%2520and%2520predicting%2520the%2520dynamic%2520and%2520context-rich%2520user%2520preferences.%250ATraditional%2520collaborative%2520filtering%2520and%2520content-based%2520methods%2520often%2520struggle%2520to%250Acapture%2520the%2520temporal%2520patternings%2520and%2520evolving%2520user%2520intentions.%2520While%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520have%2520gained%2520gradual%2520attention%2520in%2520recent%2520years%252C%2520by%2520their%250Astrong%2520semantic%2520understanding%2520and%2520reasoning%2520abilities%252C%2520they%2520are%2520not%2520inherently%250Adesigned%2520to%2520model%2520chronologically%2520evolving%2520user%2520preference%2520and%2520intentions.%2520On%250Athe%2520other%2520hand%252C%2520for%2520sequential%2520models%2520like%2520LSTM%2520%2528Long-Short-Term-Memory%2529%2520which%250Ais%2520good%2520at%2520capturing%2520the%2520temporal%2520dynamics%2520of%2520user%2520behaviour%2520and%2520evolving%2520user%250Apreference%2520over%2520time%252C%2520but%2520still%2520lacks%2520a%2520rich%2520semantic%2520understanding%2520for%250Acomprehensive%2520recommendation%2520generation.%2520In%2520this%2520study%252C%2520we%2520propose%2520DUALRec%250A%2528Dynamic%2520User-Aware%2520Language-based%2520Recommender%2529%252C%2520a%2520novel%2520recommender%2520that%250Aleverages%2520the%2520complementary%2520strength%2520of%2520both%2520models%252C%2520which%2520combines%2520the%250Atemporal%2520modelling%2520abilities%2520of%2520LSTM%2520networks%2520with%2520semantic%2520reasoning%2520power%2520of%250Athe%2520fine-tuned%2520Large%2520Language%2520Models.%2520The%2520LSTM%2520component%2520will%2520capture%2520users%250Aevolving%2520preference%2520through%2520their%2520viewing%2520history%252C%2520while%2520the%2520fine-tuned%2520LLM%250Avariants%2520will%2520leverage%2520these%2520temporal%2520user%2520insights%2520to%2520generate%2520next%2520movies%250Athat%2520users%2520might%2520enjoy.%2520Experimental%2520results%2520on%2520MovieLens-1M%2520dataset%2520shows%2520that%250Athe%2520DUALRec%2520model%2520outperforms%2520a%2520wide%2520range%2520of%2520baseline%2520models%252C%2520with%250Acomprehensive%2520evaluation%2520matrices%2520of%2520Hit%2520Rate%2520%2528HR%2540k%2529%252C%2520Normalized%2520Discounted%250ACumulative%2520Gain%2520%2528NDCG%2540k%2529%252C%2520and%2520genre%2520similarity%2520metrics.%2520This%2520research%2520proposes%250Aa%2520novel%2520architecture%2520that%2520bridges%2520the%2520gap%2520between%2520temporal%2520sequence%2520modeling%250Aand%2520semantic%2520reasoning%252C%2520and%2520offers%2520a%2520promising%2520direction%2520for%2520developing%2520more%250Aintelligent%2520and%2520context-aware%2520recommenders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DUALRec%3A%20A%20Hybrid%20Sequential%20and%20Language%20Model%20Framework%20for%0A%20%20Context-Aware%20Movie%20Recommendation&entry.906535625=Yitong%20Li%20and%20Raoul%20Grasman&entry.1292438233=%20%20The%20modern%20recommender%20systems%20are%20facing%20an%20increasing%20challenge%20of%0Amodelling%20and%20predicting%20the%20dynamic%20and%20context-rich%20user%20preferences.%0ATraditional%20collaborative%20filtering%20and%20content-based%20methods%20often%20struggle%20to%0Acapture%20the%20temporal%20patternings%20and%20evolving%20user%20intentions.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20have%20gained%20gradual%20attention%20in%20recent%20years%2C%20by%20their%0Astrong%20semantic%20understanding%20and%20reasoning%20abilities%2C%20they%20are%20not%20inherently%0Adesigned%20to%20model%20chronologically%20evolving%20user%20preference%20and%20intentions.%20On%0Athe%20other%20hand%2C%20for%20sequential%20models%20like%20LSTM%20%28Long-Short-Term-Memory%29%20which%0Ais%20good%20at%20capturing%20the%20temporal%20dynamics%20of%20user%20behaviour%20and%20evolving%20user%0Apreference%20over%20time%2C%20but%20still%20lacks%20a%20rich%20semantic%20understanding%20for%0Acomprehensive%20recommendation%20generation.%20In%20this%20study%2C%20we%20propose%20DUALRec%0A%28Dynamic%20User-Aware%20Language-based%20Recommender%29%2C%20a%20novel%20recommender%20that%0Aleverages%20the%20complementary%20strength%20of%20both%20models%2C%20which%20combines%20the%0Atemporal%20modelling%20abilities%20of%20LSTM%20networks%20with%20semantic%20reasoning%20power%20of%0Athe%20fine-tuned%20Large%20Language%20Models.%20The%20LSTM%20component%20will%20capture%20users%0Aevolving%20preference%20through%20their%20viewing%20history%2C%20while%20the%20fine-tuned%20LLM%0Avariants%20will%20leverage%20these%20temporal%20user%20insights%20to%20generate%20next%20movies%0Athat%20users%20might%20enjoy.%20Experimental%20results%20on%20MovieLens-1M%20dataset%20shows%20that%0Athe%20DUALRec%20model%20outperforms%20a%20wide%20range%20of%20baseline%20models%2C%20with%0Acomprehensive%20evaluation%20matrices%20of%20Hit%20Rate%20%28HR%40k%29%2C%20Normalized%20Discounted%0ACumulative%20Gain%20%28NDCG%40k%29%2C%20and%20genre%20similarity%20metrics.%20This%20research%20proposes%0Aa%20novel%20architecture%20that%20bridges%20the%20gap%20between%20temporal%20sequence%20modeling%0Aand%20semantic%20reasoning%2C%20and%20offers%20a%20promising%20direction%20for%20developing%20more%0Aintelligent%20and%20context-aware%20recommenders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13957v1&entry.124074799=Read"},
{"title": "Toward Temporal Causal Representation Learning with Tensor Decomposition", "author": "Jianhong Chen and Meng Zhao and Mostafa Reisi Gahrooei and Xubo Yue", "abstract": "  Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.\n", "link": "http://arxiv.org/abs/2507.14126v1", "date": "2025-07-18", "relevancy": 2.5812, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5426}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5056}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Temporal%20Causal%20Representation%20Learning%20with%20Tensor%20Decomposition&body=Title%3A%20Toward%20Temporal%20Causal%20Representation%20Learning%20with%20Tensor%20Decomposition%0AAuthor%3A%20Jianhong%20Chen%20and%20Meng%20Zhao%20and%20Mostafa%20Reisi%20Gahrooei%20and%20Xubo%20Yue%0AAbstract%3A%20%20%20Temporal%20causal%20representation%20learning%20is%20a%20powerful%20tool%20for%20uncovering%0Acomplex%20patterns%20in%20observational%20studies%2C%20which%20are%20often%20represented%20as%0Alow-dimensional%20time%20series.%20However%2C%20in%20many%20real-world%20applications%2C%20data%20are%0Ahigh-dimensional%20with%20varying%20input%20lengths%20and%20naturally%20take%20the%20form%20of%0Airregular%20tensors.%20To%20analyze%20such%20data%2C%20irregular%20tensor%20decomposition%20is%0Acritical%20for%20extracting%20meaningful%20clusters%20that%20capture%20essential%20information.%0AIn%20this%20paper%2C%20we%20focus%20on%20modeling%20causal%20representation%20learning%20based%20on%20the%0Atransformed%20information.%20First%2C%20we%20present%20a%20novel%20causal%20formulation%20for%20a%20set%0Aof%20latent%20clusters.%20We%20then%20propose%20CaRTeD%2C%20a%20joint%20learning%20framework%20that%0Aintegrates%20temporal%20causal%20representation%20learning%20with%20irregular%20tensor%0Adecomposition.%20Notably%2C%20our%20framework%20provides%20a%20blueprint%20for%20downstream%20tasks%0Ausing%20the%20learned%20tensor%20factors%2C%20such%20as%20modeling%20latent%20structures%20and%0Aextracting%20causal%20information%2C%20and%20offers%20a%20more%20flexible%20regularization%20design%0Ato%20enhance%20tensor%20decomposition.%20Theoretically%2C%20we%20show%20that%20our%20algorithm%0Aconverges%20to%20a%20stationary%20point.%20More%20importantly%2C%20our%20results%20fill%20the%20gap%20in%0Atheoretical%20guarantees%20for%20the%20convergence%20of%20state-of-the-art%20irregular%20tensor%0Adecomposition.%20Experimental%20results%20on%20synthetic%20and%20real-world%20electronic%0Ahealth%20record%20%28EHR%29%20datasets%20%28MIMIC-III%29%2C%20with%20extensive%20benchmarks%20from%20both%0Aphenotyping%20and%20network%20recovery%20perspectives%2C%20demonstrate%20that%20our%20proposed%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20enhances%20the%20explainability%0Aof%20causal%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Temporal%2520Causal%2520Representation%2520Learning%2520with%2520Tensor%2520Decomposition%26entry.906535625%3DJianhong%2520Chen%2520and%2520Meng%2520Zhao%2520and%2520Mostafa%2520Reisi%2520Gahrooei%2520and%2520Xubo%2520Yue%26entry.1292438233%3D%2520%2520Temporal%2520causal%2520representation%2520learning%2520is%2520a%2520powerful%2520tool%2520for%2520uncovering%250Acomplex%2520patterns%2520in%2520observational%2520studies%252C%2520which%2520are%2520often%2520represented%2520as%250Alow-dimensional%2520time%2520series.%2520However%252C%2520in%2520many%2520real-world%2520applications%252C%2520data%2520are%250Ahigh-dimensional%2520with%2520varying%2520input%2520lengths%2520and%2520naturally%2520take%2520the%2520form%2520of%250Airregular%2520tensors.%2520To%2520analyze%2520such%2520data%252C%2520irregular%2520tensor%2520decomposition%2520is%250Acritical%2520for%2520extracting%2520meaningful%2520clusters%2520that%2520capture%2520essential%2520information.%250AIn%2520this%2520paper%252C%2520we%2520focus%2520on%2520modeling%2520causal%2520representation%2520learning%2520based%2520on%2520the%250Atransformed%2520information.%2520First%252C%2520we%2520present%2520a%2520novel%2520causal%2520formulation%2520for%2520a%2520set%250Aof%2520latent%2520clusters.%2520We%2520then%2520propose%2520CaRTeD%252C%2520a%2520joint%2520learning%2520framework%2520that%250Aintegrates%2520temporal%2520causal%2520representation%2520learning%2520with%2520irregular%2520tensor%250Adecomposition.%2520Notably%252C%2520our%2520framework%2520provides%2520a%2520blueprint%2520for%2520downstream%2520tasks%250Ausing%2520the%2520learned%2520tensor%2520factors%252C%2520such%2520as%2520modeling%2520latent%2520structures%2520and%250Aextracting%2520causal%2520information%252C%2520and%2520offers%2520a%2520more%2520flexible%2520regularization%2520design%250Ato%2520enhance%2520tensor%2520decomposition.%2520Theoretically%252C%2520we%2520show%2520that%2520our%2520algorithm%250Aconverges%2520to%2520a%2520stationary%2520point.%2520More%2520importantly%252C%2520our%2520results%2520fill%2520the%2520gap%2520in%250Atheoretical%2520guarantees%2520for%2520the%2520convergence%2520of%2520state-of-the-art%2520irregular%2520tensor%250Adecomposition.%2520Experimental%2520results%2520on%2520synthetic%2520and%2520real-world%2520electronic%250Ahealth%2520record%2520%2528EHR%2529%2520datasets%2520%2528MIMIC-III%2529%252C%2520with%2520extensive%2520benchmarks%2520from%2520both%250Aphenotyping%2520and%2520network%2520recovery%2520perspectives%252C%2520demonstrate%2520that%2520our%2520proposed%250Amethod%2520outperforms%2520state-of-the-art%2520techniques%2520and%2520enhances%2520the%2520explainability%250Aof%2520causal%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Temporal%20Causal%20Representation%20Learning%20with%20Tensor%20Decomposition&entry.906535625=Jianhong%20Chen%20and%20Meng%20Zhao%20and%20Mostafa%20Reisi%20Gahrooei%20and%20Xubo%20Yue&entry.1292438233=%20%20Temporal%20causal%20representation%20learning%20is%20a%20powerful%20tool%20for%20uncovering%0Acomplex%20patterns%20in%20observational%20studies%2C%20which%20are%20often%20represented%20as%0Alow-dimensional%20time%20series.%20However%2C%20in%20many%20real-world%20applications%2C%20data%20are%0Ahigh-dimensional%20with%20varying%20input%20lengths%20and%20naturally%20take%20the%20form%20of%0Airregular%20tensors.%20To%20analyze%20such%20data%2C%20irregular%20tensor%20decomposition%20is%0Acritical%20for%20extracting%20meaningful%20clusters%20that%20capture%20essential%20information.%0AIn%20this%20paper%2C%20we%20focus%20on%20modeling%20causal%20representation%20learning%20based%20on%20the%0Atransformed%20information.%20First%2C%20we%20present%20a%20novel%20causal%20formulation%20for%20a%20set%0Aof%20latent%20clusters.%20We%20then%20propose%20CaRTeD%2C%20a%20joint%20learning%20framework%20that%0Aintegrates%20temporal%20causal%20representation%20learning%20with%20irregular%20tensor%0Adecomposition.%20Notably%2C%20our%20framework%20provides%20a%20blueprint%20for%20downstream%20tasks%0Ausing%20the%20learned%20tensor%20factors%2C%20such%20as%20modeling%20latent%20structures%20and%0Aextracting%20causal%20information%2C%20and%20offers%20a%20more%20flexible%20regularization%20design%0Ato%20enhance%20tensor%20decomposition.%20Theoretically%2C%20we%20show%20that%20our%20algorithm%0Aconverges%20to%20a%20stationary%20point.%20More%20importantly%2C%20our%20results%20fill%20the%20gap%20in%0Atheoretical%20guarantees%20for%20the%20convergence%20of%20state-of-the-art%20irregular%20tensor%0Adecomposition.%20Experimental%20results%20on%20synthetic%20and%20real-world%20electronic%0Ahealth%20record%20%28EHR%29%20datasets%20%28MIMIC-III%29%2C%20with%20extensive%20benchmarks%20from%20both%0Aphenotyping%20and%20network%20recovery%20perspectives%2C%20demonstrate%20that%20our%20proposed%0Amethod%20outperforms%20state-of-the-art%20techniques%20and%20enhances%20the%20explainability%0Aof%20causal%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14126v1&entry.124074799=Read"},
{"title": "Byzantine-resilient federated online learning for Gaussian process\n  regression", "author": "Xu Zhang and Zhenyuan Yuan and Minghui Zhu", "abstract": "  In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.\n", "link": "http://arxiv.org/abs/2507.14021v1", "date": "2025-07-18", "relevancy": 2.5426, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5154}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5108}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Byzantine-resilient%20federated%20online%20learning%20for%20Gaussian%20process%0A%20%20regression&body=Title%3A%20Byzantine-resilient%20federated%20online%20learning%20for%20Gaussian%20process%0A%20%20regression%0AAuthor%3A%20Xu%20Zhang%20and%20Zhenyuan%20Yuan%20and%20Minghui%20Zhu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20Byzantine-resilient%20federated%20online%20learning%20for%0AGaussian%20process%20regression%20%28GPR%29.%20We%20develop%20a%20Byzantine-resilient%20federated%0AGPR%20algorithm%20that%20allows%20a%20cloud%20and%20a%20group%20of%20agents%20to%20collaboratively%0Alearn%20a%20latent%20function%20and%20improve%20the%20learning%20performances%20where%20some%20agents%0Aexhibit%20Byzantine%20failures%2C%20i.e.%2C%20arbitrary%20and%20potentially%20adversarial%0Abehavior.%20Each%20agent-based%20local%20GPR%20sends%20potentially%20compromised%20local%0Apredictions%20to%20the%20cloud%2C%20and%20the%20cloud-based%20aggregated%20GPR%20computes%20a%20global%0Amodel%20by%20a%20Byzantine-resilient%20product%20of%20experts%20aggregation%20rule.%20Then%20the%0Acloud%20broadcasts%20the%20current%20global%20model%20to%20all%20the%20agents.%20Agent-based%20fused%0AGPR%20refines%20local%20predictions%20by%20fusing%20the%20received%20global%20model%20with%20that%20of%0Athe%20agent-based%20local%20GPR.%20Moreover%2C%20we%20quantify%20the%20learning%20accuracy%0Aimprovements%20of%20the%20agent-based%20fused%20GPR%20over%20the%20agent-based%20local%20GPR.%0AExperiments%20on%20a%20toy%20example%20and%20two%20medium-scale%20real-world%20datasets%20are%0Aconducted%20to%20demonstrate%20the%20performances%20of%20the%20proposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByzantine-resilient%2520federated%2520online%2520learning%2520for%2520Gaussian%2520process%250A%2520%2520regression%26entry.906535625%3DXu%2520Zhang%2520and%2520Zhenyuan%2520Yuan%2520and%2520Minghui%2520Zhu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520Byzantine-resilient%2520federated%2520online%2520learning%2520for%250AGaussian%2520process%2520regression%2520%2528GPR%2529.%2520We%2520develop%2520a%2520Byzantine-resilient%2520federated%250AGPR%2520algorithm%2520that%2520allows%2520a%2520cloud%2520and%2520a%2520group%2520of%2520agents%2520to%2520collaboratively%250Alearn%2520a%2520latent%2520function%2520and%2520improve%2520the%2520learning%2520performances%2520where%2520some%2520agents%250Aexhibit%2520Byzantine%2520failures%252C%2520i.e.%252C%2520arbitrary%2520and%2520potentially%2520adversarial%250Abehavior.%2520Each%2520agent-based%2520local%2520GPR%2520sends%2520potentially%2520compromised%2520local%250Apredictions%2520to%2520the%2520cloud%252C%2520and%2520the%2520cloud-based%2520aggregated%2520GPR%2520computes%2520a%2520global%250Amodel%2520by%2520a%2520Byzantine-resilient%2520product%2520of%2520experts%2520aggregation%2520rule.%2520Then%2520the%250Acloud%2520broadcasts%2520the%2520current%2520global%2520model%2520to%2520all%2520the%2520agents.%2520Agent-based%2520fused%250AGPR%2520refines%2520local%2520predictions%2520by%2520fusing%2520the%2520received%2520global%2520model%2520with%2520that%2520of%250Athe%2520agent-based%2520local%2520GPR.%2520Moreover%252C%2520we%2520quantify%2520the%2520learning%2520accuracy%250Aimprovements%2520of%2520the%2520agent-based%2520fused%2520GPR%2520over%2520the%2520agent-based%2520local%2520GPR.%250AExperiments%2520on%2520a%2520toy%2520example%2520and%2520two%2520medium-scale%2520real-world%2520datasets%2520are%250Aconducted%2520to%2520demonstrate%2520the%2520performances%2520of%2520the%2520proposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Byzantine-resilient%20federated%20online%20learning%20for%20Gaussian%20process%0A%20%20regression&entry.906535625=Xu%20Zhang%20and%20Zhenyuan%20Yuan%20and%20Minghui%20Zhu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20Byzantine-resilient%20federated%20online%20learning%20for%0AGaussian%20process%20regression%20%28GPR%29.%20We%20develop%20a%20Byzantine-resilient%20federated%0AGPR%20algorithm%20that%20allows%20a%20cloud%20and%20a%20group%20of%20agents%20to%20collaboratively%0Alearn%20a%20latent%20function%20and%20improve%20the%20learning%20performances%20where%20some%20agents%0Aexhibit%20Byzantine%20failures%2C%20i.e.%2C%20arbitrary%20and%20potentially%20adversarial%0Abehavior.%20Each%20agent-based%20local%20GPR%20sends%20potentially%20compromised%20local%0Apredictions%20to%20the%20cloud%2C%20and%20the%20cloud-based%20aggregated%20GPR%20computes%20a%20global%0Amodel%20by%20a%20Byzantine-resilient%20product%20of%20experts%20aggregation%20rule.%20Then%20the%0Acloud%20broadcasts%20the%20current%20global%20model%20to%20all%20the%20agents.%20Agent-based%20fused%0AGPR%20refines%20local%20predictions%20by%20fusing%20the%20received%20global%20model%20with%20that%20of%0Athe%20agent-based%20local%20GPR.%20Moreover%2C%20we%20quantify%20the%20learning%20accuracy%0Aimprovements%20of%20the%20agent-based%20fused%20GPR%20over%20the%20agent-based%20local%20GPR.%0AExperiments%20on%20a%20toy%20example%20and%20two%20medium-scale%20real-world%20datasets%20are%0Aconducted%20to%20demonstrate%20the%20performances%20of%20the%20proposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14021v1&entry.124074799=Read"},
{"title": "Progressively Exploring and Exploiting Cost-Free Data to Break\n  Fine-Grained Classification Barriers", "author": "Li-Jun Zhao and Zhen-Duo Chen and Zhi-Yuan Xue and Xin Luo and Xin-Shun Xu", "abstract": "  Current fine-grained classification research primarily focuses on\nfine-grained feature learning. However, in real-world scenarios, fine-grained\ndata annotation is challenging, and the features and semantics are highly\ndiverse and frequently changing. These issues create inherent barriers between\ntraditional experimental settings and real-world applications, limiting the\neffectiveness of conventional fine-grained classification methods. Although\nsome recent studies have provided potential solutions to these issues, most of\nthem still rely on limited supervised information and thus fail to offer\neffective solutions. In this paper, based on theoretical analysis, we propose a\nnovel learning paradigm to break the barriers in fine-grained classification.\nThis paradigm enables the model to progressively learn during inference,\nthereby leveraging cost-free data to more accurately represent fine-grained\ncategories and adapt to dynamic semantic changes. On this basis, an efficient\nEXPloring and EXPloiting strategy and method (EXP2) is designed. Thereinto,\nuseful inference data samples are explored according to class representations\nand exploited to optimize classifiers. Experimental results demonstrate the\ngeneral effectiveness of our method, providing guidance for future in-depth\nunderstanding and exploration of real-world fine-grained classification.\n", "link": "http://arxiv.org/abs/2412.20383v2", "date": "2025-07-18", "relevancy": 2.542, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressively%20Exploring%20and%20Exploiting%20Cost-Free%20Data%20to%20Break%0A%20%20Fine-Grained%20Classification%20Barriers&body=Title%3A%20Progressively%20Exploring%20and%20Exploiting%20Cost-Free%20Data%20to%20Break%0A%20%20Fine-Grained%20Classification%20Barriers%0AAuthor%3A%20Li-Jun%20Zhao%20and%20Zhen-Duo%20Chen%20and%20Zhi-Yuan%20Xue%20and%20Xin%20Luo%20and%20Xin-Shun%20Xu%0AAbstract%3A%20%20%20Current%20fine-grained%20classification%20research%20primarily%20focuses%20on%0Afine-grained%20feature%20learning.%20However%2C%20in%20real-world%20scenarios%2C%20fine-grained%0Adata%20annotation%20is%20challenging%2C%20and%20the%20features%20and%20semantics%20are%20highly%0Adiverse%20and%20frequently%20changing.%20These%20issues%20create%20inherent%20barriers%20between%0Atraditional%20experimental%20settings%20and%20real-world%20applications%2C%20limiting%20the%0Aeffectiveness%20of%20conventional%20fine-grained%20classification%20methods.%20Although%0Asome%20recent%20studies%20have%20provided%20potential%20solutions%20to%20these%20issues%2C%20most%20of%0Athem%20still%20rely%20on%20limited%20supervised%20information%20and%20thus%20fail%20to%20offer%0Aeffective%20solutions.%20In%20this%20paper%2C%20based%20on%20theoretical%20analysis%2C%20we%20propose%20a%0Anovel%20learning%20paradigm%20to%20break%20the%20barriers%20in%20fine-grained%20classification.%0AThis%20paradigm%20enables%20the%20model%20to%20progressively%20learn%20during%20inference%2C%0Athereby%20leveraging%20cost-free%20data%20to%20more%20accurately%20represent%20fine-grained%0Acategories%20and%20adapt%20to%20dynamic%20semantic%20changes.%20On%20this%20basis%2C%20an%20efficient%0AEXPloring%20and%20EXPloiting%20strategy%20and%20method%20%28EXP2%29%20is%20designed.%20Thereinto%2C%0Auseful%20inference%20data%20samples%20are%20explored%20according%20to%20class%20representations%0Aand%20exploited%20to%20optimize%20classifiers.%20Experimental%20results%20demonstrate%20the%0Ageneral%20effectiveness%20of%20our%20method%2C%20providing%20guidance%20for%20future%20in-depth%0Aunderstanding%20and%20exploration%20of%20real-world%20fine-grained%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressively%2520Exploring%2520and%2520Exploiting%2520Cost-Free%2520Data%2520to%2520Break%250A%2520%2520Fine-Grained%2520Classification%2520Barriers%26entry.906535625%3DLi-Jun%2520Zhao%2520and%2520Zhen-Duo%2520Chen%2520and%2520Zhi-Yuan%2520Xue%2520and%2520Xin%2520Luo%2520and%2520Xin-Shun%2520Xu%26entry.1292438233%3D%2520%2520Current%2520fine-grained%2520classification%2520research%2520primarily%2520focuses%2520on%250Afine-grained%2520feature%2520learning.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520fine-grained%250Adata%2520annotation%2520is%2520challenging%252C%2520and%2520the%2520features%2520and%2520semantics%2520are%2520highly%250Adiverse%2520and%2520frequently%2520changing.%2520These%2520issues%2520create%2520inherent%2520barriers%2520between%250Atraditional%2520experimental%2520settings%2520and%2520real-world%2520applications%252C%2520limiting%2520the%250Aeffectiveness%2520of%2520conventional%2520fine-grained%2520classification%2520methods.%2520Although%250Asome%2520recent%2520studies%2520have%2520provided%2520potential%2520solutions%2520to%2520these%2520issues%252C%2520most%2520of%250Athem%2520still%2520rely%2520on%2520limited%2520supervised%2520information%2520and%2520thus%2520fail%2520to%2520offer%250Aeffective%2520solutions.%2520In%2520this%2520paper%252C%2520based%2520on%2520theoretical%2520analysis%252C%2520we%2520propose%2520a%250Anovel%2520learning%2520paradigm%2520to%2520break%2520the%2520barriers%2520in%2520fine-grained%2520classification.%250AThis%2520paradigm%2520enables%2520the%2520model%2520to%2520progressively%2520learn%2520during%2520inference%252C%250Athereby%2520leveraging%2520cost-free%2520data%2520to%2520more%2520accurately%2520represent%2520fine-grained%250Acategories%2520and%2520adapt%2520to%2520dynamic%2520semantic%2520changes.%2520On%2520this%2520basis%252C%2520an%2520efficient%250AEXPloring%2520and%2520EXPloiting%2520strategy%2520and%2520method%2520%2528EXP2%2529%2520is%2520designed.%2520Thereinto%252C%250Auseful%2520inference%2520data%2520samples%2520are%2520explored%2520according%2520to%2520class%2520representations%250Aand%2520exploited%2520to%2520optimize%2520classifiers.%2520Experimental%2520results%2520demonstrate%2520the%250Ageneral%2520effectiveness%2520of%2520our%2520method%252C%2520providing%2520guidance%2520for%2520future%2520in-depth%250Aunderstanding%2520and%2520exploration%2520of%2520real-world%2520fine-grained%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressively%20Exploring%20and%20Exploiting%20Cost-Free%20Data%20to%20Break%0A%20%20Fine-Grained%20Classification%20Barriers&entry.906535625=Li-Jun%20Zhao%20and%20Zhen-Duo%20Chen%20and%20Zhi-Yuan%20Xue%20and%20Xin%20Luo%20and%20Xin-Shun%20Xu&entry.1292438233=%20%20Current%20fine-grained%20classification%20research%20primarily%20focuses%20on%0Afine-grained%20feature%20learning.%20However%2C%20in%20real-world%20scenarios%2C%20fine-grained%0Adata%20annotation%20is%20challenging%2C%20and%20the%20features%20and%20semantics%20are%20highly%0Adiverse%20and%20frequently%20changing.%20These%20issues%20create%20inherent%20barriers%20between%0Atraditional%20experimental%20settings%20and%20real-world%20applications%2C%20limiting%20the%0Aeffectiveness%20of%20conventional%20fine-grained%20classification%20methods.%20Although%0Asome%20recent%20studies%20have%20provided%20potential%20solutions%20to%20these%20issues%2C%20most%20of%0Athem%20still%20rely%20on%20limited%20supervised%20information%20and%20thus%20fail%20to%20offer%0Aeffective%20solutions.%20In%20this%20paper%2C%20based%20on%20theoretical%20analysis%2C%20we%20propose%20a%0Anovel%20learning%20paradigm%20to%20break%20the%20barriers%20in%20fine-grained%20classification.%0AThis%20paradigm%20enables%20the%20model%20to%20progressively%20learn%20during%20inference%2C%0Athereby%20leveraging%20cost-free%20data%20to%20more%20accurately%20represent%20fine-grained%0Acategories%20and%20adapt%20to%20dynamic%20semantic%20changes.%20On%20this%20basis%2C%20an%20efficient%0AEXPloring%20and%20EXPloiting%20strategy%20and%20method%20%28EXP2%29%20is%20designed.%20Thereinto%2C%0Auseful%20inference%20data%20samples%20are%20explored%20according%20to%20class%20representations%0Aand%20exploited%20to%20optimize%20classifiers.%20Experimental%20results%20demonstrate%20the%0Ageneral%20effectiveness%20of%20our%20method%2C%20providing%20guidance%20for%20future%20in-depth%0Aunderstanding%20and%20exploration%20of%20real-world%20fine-grained%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20383v2&entry.124074799=Read"},
{"title": "Exploring Graph Representations of Logical Forms for Language Modeling", "author": "Michael Sullivan", "abstract": "  We make the case for language models over logical forms (LFLMs), arguing that\nsuch models are more data-efficient than their textual counterparts. To that\nend, we introduce the Graph-based Formal-Logical Distributional Semantics\n(GFoLDS) prototype, a pretrained LM over graph representations of logical\nforms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong\nexperimental evidence that LFLMs can leverage the built-in, basic linguistic\nknowledge inherent in such models to immediately begin learning more complex\npatterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,\ntransformer LMs (BERT) pretrained on the same data, indicating that LFLMs can\nlearn with substantially less data than models over plain text. Furthermore, we\nshow that the performance of this model is likely to scale with additional\nparameters and pretraining data, suggesting the viability of LFLMs in\nreal-world applications.\n", "link": "http://arxiv.org/abs/2505.14523v2", "date": "2025-07-18", "relevancy": 2.542, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Graph%20Representations%20of%20Logical%20Forms%20for%20Language%20Modeling&body=Title%3A%20Exploring%20Graph%20Representations%20of%20Logical%20Forms%20for%20Language%20Modeling%0AAuthor%3A%20Michael%20Sullivan%0AAbstract%3A%20%20%20We%20make%20the%20case%20for%20language%20models%20over%20logical%20forms%20%28LFLMs%29%2C%20arguing%20that%0Asuch%20models%20are%20more%20data-efficient%20than%20their%20textual%20counterparts.%20To%20that%0Aend%2C%20we%20introduce%20the%20Graph-based%20Formal-Logical%20Distributional%20Semantics%0A%28GFoLDS%29%20prototype%2C%20a%20pretrained%20LM%20over%20graph%20representations%20of%20logical%0Aforms%2C%20as%20a%20proof-of-concept%20of%20LFLMs.%20Using%20GFoLDS%2C%20we%20present%20strong%0Aexperimental%20evidence%20that%20LFLMs%20can%20leverage%20the%20built-in%2C%20basic%20linguistic%0Aknowledge%20inherent%20in%20such%20models%20to%20immediately%20begin%20learning%20more%20complex%0Apatterns.%20On%20downstream%20tasks%2C%20we%20show%20that%20GFoLDS%20vastly%20outperforms%20textual%2C%0Atransformer%20LMs%20%28BERT%29%20pretrained%20on%20the%20same%20data%2C%20indicating%20that%20LFLMs%20can%0Alearn%20with%20substantially%20less%20data%20than%20models%20over%20plain%20text.%20Furthermore%2C%20we%0Ashow%20that%20the%20performance%20of%20this%20model%20is%20likely%20to%20scale%20with%20additional%0Aparameters%20and%20pretraining%20data%2C%20suggesting%20the%20viability%20of%20LFLMs%20in%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.14523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Graph%2520Representations%2520of%2520Logical%2520Forms%2520for%2520Language%2520Modeling%26entry.906535625%3DMichael%2520Sullivan%26entry.1292438233%3D%2520%2520We%2520make%2520the%2520case%2520for%2520language%2520models%2520over%2520logical%2520forms%2520%2528LFLMs%2529%252C%2520arguing%2520that%250Asuch%2520models%2520are%2520more%2520data-efficient%2520than%2520their%2520textual%2520counterparts.%2520To%2520that%250Aend%252C%2520we%2520introduce%2520the%2520Graph-based%2520Formal-Logical%2520Distributional%2520Semantics%250A%2528GFoLDS%2529%2520prototype%252C%2520a%2520pretrained%2520LM%2520over%2520graph%2520representations%2520of%2520logical%250Aforms%252C%2520as%2520a%2520proof-of-concept%2520of%2520LFLMs.%2520Using%2520GFoLDS%252C%2520we%2520present%2520strong%250Aexperimental%2520evidence%2520that%2520LFLMs%2520can%2520leverage%2520the%2520built-in%252C%2520basic%2520linguistic%250Aknowledge%2520inherent%2520in%2520such%2520models%2520to%2520immediately%2520begin%2520learning%2520more%2520complex%250Apatterns.%2520On%2520downstream%2520tasks%252C%2520we%2520show%2520that%2520GFoLDS%2520vastly%2520outperforms%2520textual%252C%250Atransformer%2520LMs%2520%2528BERT%2529%2520pretrained%2520on%2520the%2520same%2520data%252C%2520indicating%2520that%2520LFLMs%2520can%250Alearn%2520with%2520substantially%2520less%2520data%2520than%2520models%2520over%2520plain%2520text.%2520Furthermore%252C%2520we%250Ashow%2520that%2520the%2520performance%2520of%2520this%2520model%2520is%2520likely%2520to%2520scale%2520with%2520additional%250Aparameters%2520and%2520pretraining%2520data%252C%2520suggesting%2520the%2520viability%2520of%2520LFLMs%2520in%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Graph%20Representations%20of%20Logical%20Forms%20for%20Language%20Modeling&entry.906535625=Michael%20Sullivan&entry.1292438233=%20%20We%20make%20the%20case%20for%20language%20models%20over%20logical%20forms%20%28LFLMs%29%2C%20arguing%20that%0Asuch%20models%20are%20more%20data-efficient%20than%20their%20textual%20counterparts.%20To%20that%0Aend%2C%20we%20introduce%20the%20Graph-based%20Formal-Logical%20Distributional%20Semantics%0A%28GFoLDS%29%20prototype%2C%20a%20pretrained%20LM%20over%20graph%20representations%20of%20logical%0Aforms%2C%20as%20a%20proof-of-concept%20of%20LFLMs.%20Using%20GFoLDS%2C%20we%20present%20strong%0Aexperimental%20evidence%20that%20LFLMs%20can%20leverage%20the%20built-in%2C%20basic%20linguistic%0Aknowledge%20inherent%20in%20such%20models%20to%20immediately%20begin%20learning%20more%20complex%0Apatterns.%20On%20downstream%20tasks%2C%20we%20show%20that%20GFoLDS%20vastly%20outperforms%20textual%2C%0Atransformer%20LMs%20%28BERT%29%20pretrained%20on%20the%20same%20data%2C%20indicating%20that%20LFLMs%20can%0Alearn%20with%20substantially%20less%20data%20than%20models%20over%20plain%20text.%20Furthermore%2C%20we%0Ashow%20that%20the%20performance%20of%20this%20model%20is%20likely%20to%20scale%20with%20additional%0Aparameters%20and%20pretraining%20data%2C%20suggesting%20the%20viability%20of%20LFLMs%20in%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.14523v2&entry.124074799=Read"},
{"title": "BeetleVerse: A Study on Taxonomic Classification of Ground Beetles", "author": "S M Rayeed and Alyson East and Samuel Stevens and Sydne Record and Charles V Stewart", "abstract": "  Ground beetles are a highly sensitive and speciose biological indicator,\nmaking them vital for monitoring biodiversity. However, they are currently an\nunderutilized resource due to the manual effort required by taxonomic experts\nto perform challenging species differentiations based on subtle morphological\ndifferences, precluding widespread applications. In this paper, we evaluate 12\nvision models on taxonomic classification across four diverse, long-tailed\ndatasets spanning over 230 genera and 1769 species, with images ranging from\ncontrolled laboratory settings to challenging field-collected (in-situ)\nphotographs. We further explore taxonomic classification in two important\nreal-world contexts: sample efficiency and domain adaptation. Our results show\nthat the Vision and Language Transformer combined with an MLP head is the best\nperforming model, with 97% accuracy at genus and 94% at species level. Sample\nefficiency analysis shows that we can reduce train data requirements by up to\n50% with minimal compromise in performance. The domain adaptation experiments\nreveal significant challenges when transferring models from lab to in-situ\nimages, highlighting a critical domain gap. Overall, our study lays a\nfoundation for large-scale automated taxonomic classification of beetles, and\nbeyond that, advances sample-efficient learning and cross-domain adaptation for\ndiverse long-tailed ecological datasets.\n", "link": "http://arxiv.org/abs/2504.13393v2", "date": "2025-07-18", "relevancy": 2.5217, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeetleVerse%3A%20A%20Study%20on%20Taxonomic%20Classification%20of%20Ground%20Beetles&body=Title%3A%20BeetleVerse%3A%20A%20Study%20on%20Taxonomic%20Classification%20of%20Ground%20Beetles%0AAuthor%3A%20S%20M%20Rayeed%20and%20Alyson%20East%20and%20Samuel%20Stevens%20and%20Sydne%20Record%20and%20Charles%20V%20Stewart%0AAbstract%3A%20%20%20Ground%20beetles%20are%20a%20highly%20sensitive%20and%20speciose%20biological%20indicator%2C%0Amaking%20them%20vital%20for%20monitoring%20biodiversity.%20However%2C%20they%20are%20currently%20an%0Aunderutilized%20resource%20due%20to%20the%20manual%20effort%20required%20by%20taxonomic%20experts%0Ato%20perform%20challenging%20species%20differentiations%20based%20on%20subtle%20morphological%0Adifferences%2C%20precluding%20widespread%20applications.%20In%20this%20paper%2C%20we%20evaluate%2012%0Avision%20models%20on%20taxonomic%20classification%20across%20four%20diverse%2C%20long-tailed%0Adatasets%20spanning%20over%20230%20genera%20and%201769%20species%2C%20with%20images%20ranging%20from%0Acontrolled%20laboratory%20settings%20to%20challenging%20field-collected%20%28in-situ%29%0Aphotographs.%20We%20further%20explore%20taxonomic%20classification%20in%20two%20important%0Areal-world%20contexts%3A%20sample%20efficiency%20and%20domain%20adaptation.%20Our%20results%20show%0Athat%20the%20Vision%20and%20Language%20Transformer%20combined%20with%20an%20MLP%20head%20is%20the%20best%0Aperforming%20model%2C%20with%2097%25%20accuracy%20at%20genus%20and%2094%25%20at%20species%20level.%20Sample%0Aefficiency%20analysis%20shows%20that%20we%20can%20reduce%20train%20data%20requirements%20by%20up%20to%0A50%25%20with%20minimal%20compromise%20in%20performance.%20The%20domain%20adaptation%20experiments%0Areveal%20significant%20challenges%20when%20transferring%20models%20from%20lab%20to%20in-situ%0Aimages%2C%20highlighting%20a%20critical%20domain%20gap.%20Overall%2C%20our%20study%20lays%20a%0Afoundation%20for%20large-scale%20automated%20taxonomic%20classification%20of%20beetles%2C%20and%0Abeyond%20that%2C%20advances%20sample-efficient%20learning%20and%20cross-domain%20adaptation%20for%0Adiverse%20long-tailed%20ecological%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeetleVerse%253A%2520A%2520Study%2520on%2520Taxonomic%2520Classification%2520of%2520Ground%2520Beetles%26entry.906535625%3DS%2520M%2520Rayeed%2520and%2520Alyson%2520East%2520and%2520Samuel%2520Stevens%2520and%2520Sydne%2520Record%2520and%2520Charles%2520V%2520Stewart%26entry.1292438233%3D%2520%2520Ground%2520beetles%2520are%2520a%2520highly%2520sensitive%2520and%2520speciose%2520biological%2520indicator%252C%250Amaking%2520them%2520vital%2520for%2520monitoring%2520biodiversity.%2520However%252C%2520they%2520are%2520currently%2520an%250Aunderutilized%2520resource%2520due%2520to%2520the%2520manual%2520effort%2520required%2520by%2520taxonomic%2520experts%250Ato%2520perform%2520challenging%2520species%2520differentiations%2520based%2520on%2520subtle%2520morphological%250Adifferences%252C%2520precluding%2520widespread%2520applications.%2520In%2520this%2520paper%252C%2520we%2520evaluate%252012%250Avision%2520models%2520on%2520taxonomic%2520classification%2520across%2520four%2520diverse%252C%2520long-tailed%250Adatasets%2520spanning%2520over%2520230%2520genera%2520and%25201769%2520species%252C%2520with%2520images%2520ranging%2520from%250Acontrolled%2520laboratory%2520settings%2520to%2520challenging%2520field-collected%2520%2528in-situ%2529%250Aphotographs.%2520We%2520further%2520explore%2520taxonomic%2520classification%2520in%2520two%2520important%250Areal-world%2520contexts%253A%2520sample%2520efficiency%2520and%2520domain%2520adaptation.%2520Our%2520results%2520show%250Athat%2520the%2520Vision%2520and%2520Language%2520Transformer%2520combined%2520with%2520an%2520MLP%2520head%2520is%2520the%2520best%250Aperforming%2520model%252C%2520with%252097%2525%2520accuracy%2520at%2520genus%2520and%252094%2525%2520at%2520species%2520level.%2520Sample%250Aefficiency%2520analysis%2520shows%2520that%2520we%2520can%2520reduce%2520train%2520data%2520requirements%2520by%2520up%2520to%250A50%2525%2520with%2520minimal%2520compromise%2520in%2520performance.%2520The%2520domain%2520adaptation%2520experiments%250Areveal%2520significant%2520challenges%2520when%2520transferring%2520models%2520from%2520lab%2520to%2520in-situ%250Aimages%252C%2520highlighting%2520a%2520critical%2520domain%2520gap.%2520Overall%252C%2520our%2520study%2520lays%2520a%250Afoundation%2520for%2520large-scale%2520automated%2520taxonomic%2520classification%2520of%2520beetles%252C%2520and%250Abeyond%2520that%252C%2520advances%2520sample-efficient%2520learning%2520and%2520cross-domain%2520adaptation%2520for%250Adiverse%2520long-tailed%2520ecological%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeetleVerse%3A%20A%20Study%20on%20Taxonomic%20Classification%20of%20Ground%20Beetles&entry.906535625=S%20M%20Rayeed%20and%20Alyson%20East%20and%20Samuel%20Stevens%20and%20Sydne%20Record%20and%20Charles%20V%20Stewart&entry.1292438233=%20%20Ground%20beetles%20are%20a%20highly%20sensitive%20and%20speciose%20biological%20indicator%2C%0Amaking%20them%20vital%20for%20monitoring%20biodiversity.%20However%2C%20they%20are%20currently%20an%0Aunderutilized%20resource%20due%20to%20the%20manual%20effort%20required%20by%20taxonomic%20experts%0Ato%20perform%20challenging%20species%20differentiations%20based%20on%20subtle%20morphological%0Adifferences%2C%20precluding%20widespread%20applications.%20In%20this%20paper%2C%20we%20evaluate%2012%0Avision%20models%20on%20taxonomic%20classification%20across%20four%20diverse%2C%20long-tailed%0Adatasets%20spanning%20over%20230%20genera%20and%201769%20species%2C%20with%20images%20ranging%20from%0Acontrolled%20laboratory%20settings%20to%20challenging%20field-collected%20%28in-situ%29%0Aphotographs.%20We%20further%20explore%20taxonomic%20classification%20in%20two%20important%0Areal-world%20contexts%3A%20sample%20efficiency%20and%20domain%20adaptation.%20Our%20results%20show%0Athat%20the%20Vision%20and%20Language%20Transformer%20combined%20with%20an%20MLP%20head%20is%20the%20best%0Aperforming%20model%2C%20with%2097%25%20accuracy%20at%20genus%20and%2094%25%20at%20species%20level.%20Sample%0Aefficiency%20analysis%20shows%20that%20we%20can%20reduce%20train%20data%20requirements%20by%20up%20to%0A50%25%20with%20minimal%20compromise%20in%20performance.%20The%20domain%20adaptation%20experiments%0Areveal%20significant%20challenges%20when%20transferring%20models%20from%20lab%20to%20in-situ%0Aimages%2C%20highlighting%20a%20critical%20domain%20gap.%20Overall%2C%20our%20study%20lays%20a%0Afoundation%20for%20large-scale%20automated%20taxonomic%20classification%20of%20beetles%2C%20and%0Abeyond%20that%2C%20advances%20sample-efficient%20learning%20and%20cross-domain%20adaptation%20for%0Adiverse%20long-tailed%20ecological%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13393v2&entry.124074799=Read"},
{"title": "Critiques of World Models", "author": "Eric Xing and Mingkai Deng and Jinyu Hou and Zhiting Hu", "abstract": "  World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model.\n", "link": "http://arxiv.org/abs/2507.05169v2", "date": "2025-07-18", "relevancy": 2.4951, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critiques%20of%20World%20Models&body=Title%3A%20Critiques%20of%20World%20Models%0AAuthor%3A%20Eric%20Xing%20and%20Mingkai%20Deng%20and%20Jinyu%20Hou%20and%20Zhiting%20Hu%0AAbstract%3A%20%20%20World%20Model%2C%20the%20supposed%20algorithmic%20surrogate%20of%20the%20real-world%20environment%0Awhich%20biological%20agents%20experience%20with%20and%20act%20upon%2C%20has%20been%20an%20emerging%0Atopic%20in%20recent%20years%20because%20of%20the%20rising%20needs%20to%20develop%20virtual%20agents%0Awith%20artificial%20%28general%29%20intelligence.%20There%20has%20been%20much%20debate%20on%20what%20a%0Aworld%20model%20really%20is%2C%20how%20to%20build%20it%2C%20how%20to%20use%20it%2C%20and%20how%20to%20evaluate%20it.%0AIn%20this%20essay%2C%20starting%20from%20the%20imagination%20in%20the%20famed%20Sci-Fi%20classic%20Dune%2C%0Aand%20drawing%20inspiration%20from%20the%20concept%20of%20%22hypothetical%20thinking%22%20in%0Apsychology%20literature%2C%20we%20offer%20critiques%20of%20several%20schools%20of%20thoughts%20on%0Aworld%20modeling%2C%20and%20argue%20the%20primary%20goal%20of%20a%20world%20model%20to%20be%20simulating%0Aall%20actionable%20possibilities%20of%20the%20real%20world%20for%20purposeful%20reasoning%20and%0Aacting.%20Building%20on%20the%20critiques%2C%20we%20propose%20a%20new%20architecture%20for%20a%0Ageneral-purpose%20world%20model%2C%20based%20on%20hierarchical%2C%20multi-level%2C%20and%20mixed%0Acontinuous/discrete%20representations%2C%20and%20a%20generative%20and%20self-supervision%0Alearning%20framework%2C%20with%20an%20outlook%20of%20a%20Physical%2C%20Agentic%2C%20and%20Nested%20%28PAN%29%0AAGI%20system%20enabled%20by%20such%20a%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.05169v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritiques%2520of%2520World%2520Models%26entry.906535625%3DEric%2520Xing%2520and%2520Mingkai%2520Deng%2520and%2520Jinyu%2520Hou%2520and%2520Zhiting%2520Hu%26entry.1292438233%3D%2520%2520World%2520Model%252C%2520the%2520supposed%2520algorithmic%2520surrogate%2520of%2520the%2520real-world%2520environment%250Awhich%2520biological%2520agents%2520experience%2520with%2520and%2520act%2520upon%252C%2520has%2520been%2520an%2520emerging%250Atopic%2520in%2520recent%2520years%2520because%2520of%2520the%2520rising%2520needs%2520to%2520develop%2520virtual%2520agents%250Awith%2520artificial%2520%2528general%2529%2520intelligence.%2520There%2520has%2520been%2520much%2520debate%2520on%2520what%2520a%250Aworld%2520model%2520really%2520is%252C%2520how%2520to%2520build%2520it%252C%2520how%2520to%2520use%2520it%252C%2520and%2520how%2520to%2520evaluate%2520it.%250AIn%2520this%2520essay%252C%2520starting%2520from%2520the%2520imagination%2520in%2520the%2520famed%2520Sci-Fi%2520classic%2520Dune%252C%250Aand%2520drawing%2520inspiration%2520from%2520the%2520concept%2520of%2520%2522hypothetical%2520thinking%2522%2520in%250Apsychology%2520literature%252C%2520we%2520offer%2520critiques%2520of%2520several%2520schools%2520of%2520thoughts%2520on%250Aworld%2520modeling%252C%2520and%2520argue%2520the%2520primary%2520goal%2520of%2520a%2520world%2520model%2520to%2520be%2520simulating%250Aall%2520actionable%2520possibilities%2520of%2520the%2520real%2520world%2520for%2520purposeful%2520reasoning%2520and%250Aacting.%2520Building%2520on%2520the%2520critiques%252C%2520we%2520propose%2520a%2520new%2520architecture%2520for%2520a%250Ageneral-purpose%2520world%2520model%252C%2520based%2520on%2520hierarchical%252C%2520multi-level%252C%2520and%2520mixed%250Acontinuous/discrete%2520representations%252C%2520and%2520a%2520generative%2520and%2520self-supervision%250Alearning%2520framework%252C%2520with%2520an%2520outlook%2520of%2520a%2520Physical%252C%2520Agentic%252C%2520and%2520Nested%2520%2528PAN%2529%250AAGI%2520system%2520enabled%2520by%2520such%2520a%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05169v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critiques%20of%20World%20Models&entry.906535625=Eric%20Xing%20and%20Mingkai%20Deng%20and%20Jinyu%20Hou%20and%20Zhiting%20Hu&entry.1292438233=%20%20World%20Model%2C%20the%20supposed%20algorithmic%20surrogate%20of%20the%20real-world%20environment%0Awhich%20biological%20agents%20experience%20with%20and%20act%20upon%2C%20has%20been%20an%20emerging%0Atopic%20in%20recent%20years%20because%20of%20the%20rising%20needs%20to%20develop%20virtual%20agents%0Awith%20artificial%20%28general%29%20intelligence.%20There%20has%20been%20much%20debate%20on%20what%20a%0Aworld%20model%20really%20is%2C%20how%20to%20build%20it%2C%20how%20to%20use%20it%2C%20and%20how%20to%20evaluate%20it.%0AIn%20this%20essay%2C%20starting%20from%20the%20imagination%20in%20the%20famed%20Sci-Fi%20classic%20Dune%2C%0Aand%20drawing%20inspiration%20from%20the%20concept%20of%20%22hypothetical%20thinking%22%20in%0Apsychology%20literature%2C%20we%20offer%20critiques%20of%20several%20schools%20of%20thoughts%20on%0Aworld%20modeling%2C%20and%20argue%20the%20primary%20goal%20of%20a%20world%20model%20to%20be%20simulating%0Aall%20actionable%20possibilities%20of%20the%20real%20world%20for%20purposeful%20reasoning%20and%0Aacting.%20Building%20on%20the%20critiques%2C%20we%20propose%20a%20new%20architecture%20for%20a%0Ageneral-purpose%20world%20model%2C%20based%20on%20hierarchical%2C%20multi-level%2C%20and%20mixed%0Acontinuous/discrete%20representations%2C%20and%20a%20generative%20and%20self-supervision%0Alearning%20framework%2C%20with%20an%20outlook%20of%20a%20Physical%2C%20Agentic%2C%20and%20Nested%20%28PAN%29%0AAGI%20system%20enabled%20by%20such%20a%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.05169v2&entry.124074799=Read"},
{"title": "Exploiting Primacy Effect To Improve Large Language Models", "author": "Bianca Raimondi and Maurizio Gabbrielli", "abstract": "  Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.\n", "link": "http://arxiv.org/abs/2507.13949v1", "date": "2025-07-18", "relevancy": 2.4904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5131}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Primacy%20Effect%20To%20Improve%20Large%20Language%20Models&body=Title%3A%20Exploiting%20Primacy%20Effect%20To%20Improve%20Large%20Language%20Models%0AAuthor%3A%20Bianca%20Raimondi%20and%20Maurizio%20Gabbrielli%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20essential%20in%20many%20Natural%20Language%0AProcessing%20%28NLP%29%20tasks%2C%20leveraging%20extensive%20pre-training%20and%20fine-tuning%20to%0Aachieve%20high%20accuracy.%20However%2C%20like%20humans%2C%20LLMs%20exhibit%20biases%2C%20particularly%0Apositional%20biases%20such%20as%20primacy%20and%20recency%20effects%2C%20which%20can%20influence%20the%0Aaccuracy%20of%20the%20answers.%20The%20primacy%20effect-where%20items%20presented%20first%20are%0Amore%20likely%20to%20be%20remembered%20or%20selected-plays%20a%20key%20role%20in%20Multiple%20Choice%0AQuestion%20Answering%20%28MCQA%29%2C%20where%20the%20order%20of%20answer%20options%20can%20affect%0Aprediction%20outcomes.%20This%20study%20focuses%20on%20primacy%20bias%20in%20fine-tuned%20LLMs%3A%20We%0Afirst%20show%20that%20fine-tuning%20amplifies%20this%20bias%2C%20probably%20due%20to%20exposure%20to%0Ahuman-like%20patterns.%20Hence%2C%20we%20strategically%20leverage%20this%20effect%20by%20reordering%0Aresponse%20options%20based%20on%20semantic%20similarity%20to%20the%20query%2C%20without%20requiring%0Aknowledge%20of%20the%20correct%20answer.%20Our%20experimental%20results%20show%20that%20this%0Aapproach%20significantly%20improves%20performance%20in%20MCQA.%20More%20generally%2C%20our%0Afindings%20underscore%20the%20dual%20nature%20of%20biases%20as%20both%20challenges%20and%0Aopportunities%2C%20offering%20insights%20for%20bias-aware%20model%20design%20and%20NLP%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Primacy%2520Effect%2520To%2520Improve%2520Large%2520Language%2520Models%26entry.906535625%3DBianca%2520Raimondi%2520and%2520Maurizio%2520Gabbrielli%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520essential%2520in%2520many%2520Natural%2520Language%250AProcessing%2520%2528NLP%2529%2520tasks%252C%2520leveraging%2520extensive%2520pre-training%2520and%2520fine-tuning%2520to%250Aachieve%2520high%2520accuracy.%2520However%252C%2520like%2520humans%252C%2520LLMs%2520exhibit%2520biases%252C%2520particularly%250Apositional%2520biases%2520such%2520as%2520primacy%2520and%2520recency%2520effects%252C%2520which%2520can%2520influence%2520the%250Aaccuracy%2520of%2520the%2520answers.%2520The%2520primacy%2520effect-where%2520items%2520presented%2520first%2520are%250Amore%2520likely%2520to%2520be%2520remembered%2520or%2520selected-plays%2520a%2520key%2520role%2520in%2520Multiple%2520Choice%250AQuestion%2520Answering%2520%2528MCQA%2529%252C%2520where%2520the%2520order%2520of%2520answer%2520options%2520can%2520affect%250Aprediction%2520outcomes.%2520This%2520study%2520focuses%2520on%2520primacy%2520bias%2520in%2520fine-tuned%2520LLMs%253A%2520We%250Afirst%2520show%2520that%2520fine-tuning%2520amplifies%2520this%2520bias%252C%2520probably%2520due%2520to%2520exposure%2520to%250Ahuman-like%2520patterns.%2520Hence%252C%2520we%2520strategically%2520leverage%2520this%2520effect%2520by%2520reordering%250Aresponse%2520options%2520based%2520on%2520semantic%2520similarity%2520to%2520the%2520query%252C%2520without%2520requiring%250Aknowledge%2520of%2520the%2520correct%2520answer.%2520Our%2520experimental%2520results%2520show%2520that%2520this%250Aapproach%2520significantly%2520improves%2520performance%2520in%2520MCQA.%2520More%2520generally%252C%2520our%250Afindings%2520underscore%2520the%2520dual%2520nature%2520of%2520biases%2520as%2520both%2520challenges%2520and%250Aopportunities%252C%2520offering%2520insights%2520for%2520bias-aware%2520model%2520design%2520and%2520NLP%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Primacy%20Effect%20To%20Improve%20Large%20Language%20Models&entry.906535625=Bianca%20Raimondi%20and%20Maurizio%20Gabbrielli&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20essential%20in%20many%20Natural%20Language%0AProcessing%20%28NLP%29%20tasks%2C%20leveraging%20extensive%20pre-training%20and%20fine-tuning%20to%0Aachieve%20high%20accuracy.%20However%2C%20like%20humans%2C%20LLMs%20exhibit%20biases%2C%20particularly%0Apositional%20biases%20such%20as%20primacy%20and%20recency%20effects%2C%20which%20can%20influence%20the%0Aaccuracy%20of%20the%20answers.%20The%20primacy%20effect-where%20items%20presented%20first%20are%0Amore%20likely%20to%20be%20remembered%20or%20selected-plays%20a%20key%20role%20in%20Multiple%20Choice%0AQuestion%20Answering%20%28MCQA%29%2C%20where%20the%20order%20of%20answer%20options%20can%20affect%0Aprediction%20outcomes.%20This%20study%20focuses%20on%20primacy%20bias%20in%20fine-tuned%20LLMs%3A%20We%0Afirst%20show%20that%20fine-tuning%20amplifies%20this%20bias%2C%20probably%20due%20to%20exposure%20to%0Ahuman-like%20patterns.%20Hence%2C%20we%20strategically%20leverage%20this%20effect%20by%20reordering%0Aresponse%20options%20based%20on%20semantic%20similarity%20to%20the%20query%2C%20without%20requiring%0Aknowledge%20of%20the%20correct%20answer.%20Our%20experimental%20results%20show%20that%20this%0Aapproach%20significantly%20improves%20performance%20in%20MCQA.%20More%20generally%2C%20our%0Afindings%20underscore%20the%20dual%20nature%20of%20biases%20as%20both%20challenges%20and%0Aopportunities%2C%20offering%20insights%20for%20bias-aware%20model%20design%20and%20NLP%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13949v1&entry.124074799=Read"},
{"title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language\n  Models", "author": "Lam Nguyen and Erika Barcelos and Roger French and Yinghui Wu", "abstract": "  Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.\n", "link": "http://arxiv.org/abs/2507.14032v1", "date": "2025-07-18", "relevancy": 2.4741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5022}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KROMA%3A%20Ontology%20Matching%20with%20Knowledge%20Retrieval%20and%20Large%20Language%0A%20%20Models&body=Title%3A%20KROMA%3A%20Ontology%20Matching%20with%20Knowledge%20Retrieval%20and%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Lam%20Nguyen%20and%20Erika%20Barcelos%20and%20Roger%20French%20and%20Yinghui%20Wu%0AAbstract%3A%20%20%20Ontology%20Matching%20%28OM%29%20is%20a%20cornerstone%20task%20of%20semantic%20interoperability%2C%0Ayet%20existing%20systems%20often%20rely%20on%20handcrafted%20rules%20or%20specialized%20models%20with%0Alimited%20adaptability.%20We%20present%20KROMA%2C%20a%20novel%20OM%20framework%20that%20harnesses%0ALarge%20Language%20Models%20%28LLMs%29%20within%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%0Apipeline%20to%20dynamically%20enrich%20the%20semantic%20context%20of%20OM%20tasks%20with%0Astructural%2C%20lexical%2C%20and%20definitional%20knowledge.%20To%20optimize%20both%20performance%0Aand%20efficiency%2C%20KROMA%20integrates%20a%20bisimilarity-based%20concept%20matching%20and%20a%0Alightweight%20ontology%20refinement%20step%2C%20which%20prune%20candidate%20concepts%20and%0Asubstantially%20reduce%20the%20communication%20overhead%20from%20invoking%20LLMs.%20Through%0Aexperiments%20on%20multiple%20benchmark%20datasets%2C%20we%20show%20that%20integrating%20knowledge%0Aretrieval%20with%20context-augmented%20LLMs%20significantly%20enhances%20ontology%20matching%2C%0Aoutperforming%20both%20classic%20OM%20systems%20and%20cutting-edge%20LLM-based%20approaches%0Awhile%20keeping%20communication%20overhead%20comparable.%20Our%20study%20highlights%20the%0Afeasibility%20and%20benefit%20of%20the%20proposed%20optimization%20techniques%20%28targeted%0Aknowledge%20retrieval%2C%20prompt%20enrichment%2C%20and%20ontology%20refinement%29%20for%20ontology%0Amatching%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKROMA%253A%2520Ontology%2520Matching%2520with%2520Knowledge%2520Retrieval%2520and%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DLam%2520Nguyen%2520and%2520Erika%2520Barcelos%2520and%2520Roger%2520French%2520and%2520Yinghui%2520Wu%26entry.1292438233%3D%2520%2520Ontology%2520Matching%2520%2528OM%2529%2520is%2520a%2520cornerstone%2520task%2520of%2520semantic%2520interoperability%252C%250Ayet%2520existing%2520systems%2520often%2520rely%2520on%2520handcrafted%2520rules%2520or%2520specialized%2520models%2520with%250Alimited%2520adaptability.%2520We%2520present%2520KROMA%252C%2520a%2520novel%2520OM%2520framework%2520that%2520harnesses%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520within%2520a%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%250Apipeline%2520to%2520dynamically%2520enrich%2520the%2520semantic%2520context%2520of%2520OM%2520tasks%2520with%250Astructural%252C%2520lexical%252C%2520and%2520definitional%2520knowledge.%2520To%2520optimize%2520both%2520performance%250Aand%2520efficiency%252C%2520KROMA%2520integrates%2520a%2520bisimilarity-based%2520concept%2520matching%2520and%2520a%250Alightweight%2520ontology%2520refinement%2520step%252C%2520which%2520prune%2520candidate%2520concepts%2520and%250Asubstantially%2520reduce%2520the%2520communication%2520overhead%2520from%2520invoking%2520LLMs.%2520Through%250Aexperiments%2520on%2520multiple%2520benchmark%2520datasets%252C%2520we%2520show%2520that%2520integrating%2520knowledge%250Aretrieval%2520with%2520context-augmented%2520LLMs%2520significantly%2520enhances%2520ontology%2520matching%252C%250Aoutperforming%2520both%2520classic%2520OM%2520systems%2520and%2520cutting-edge%2520LLM-based%2520approaches%250Awhile%2520keeping%2520communication%2520overhead%2520comparable.%2520Our%2520study%2520highlights%2520the%250Afeasibility%2520and%2520benefit%2520of%2520the%2520proposed%2520optimization%2520techniques%2520%2528targeted%250Aknowledge%2520retrieval%252C%2520prompt%2520enrichment%252C%2520and%2520ontology%2520refinement%2529%2520for%2520ontology%250Amatching%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KROMA%3A%20Ontology%20Matching%20with%20Knowledge%20Retrieval%20and%20Large%20Language%0A%20%20Models&entry.906535625=Lam%20Nguyen%20and%20Erika%20Barcelos%20and%20Roger%20French%20and%20Yinghui%20Wu&entry.1292438233=%20%20Ontology%20Matching%20%28OM%29%20is%20a%20cornerstone%20task%20of%20semantic%20interoperability%2C%0Ayet%20existing%20systems%20often%20rely%20on%20handcrafted%20rules%20or%20specialized%20models%20with%0Alimited%20adaptability.%20We%20present%20KROMA%2C%20a%20novel%20OM%20framework%20that%20harnesses%0ALarge%20Language%20Models%20%28LLMs%29%20within%20a%20Retrieval-Augmented%20Generation%20%28RAG%29%0Apipeline%20to%20dynamically%20enrich%20the%20semantic%20context%20of%20OM%20tasks%20with%0Astructural%2C%20lexical%2C%20and%20definitional%20knowledge.%20To%20optimize%20both%20performance%0Aand%20efficiency%2C%20KROMA%20integrates%20a%20bisimilarity-based%20concept%20matching%20and%20a%0Alightweight%20ontology%20refinement%20step%2C%20which%20prune%20candidate%20concepts%20and%0Asubstantially%20reduce%20the%20communication%20overhead%20from%20invoking%20LLMs.%20Through%0Aexperiments%20on%20multiple%20benchmark%20datasets%2C%20we%20show%20that%20integrating%20knowledge%0Aretrieval%20with%20context-augmented%20LLMs%20significantly%20enhances%20ontology%20matching%2C%0Aoutperforming%20both%20classic%20OM%20systems%20and%20cutting-edge%20LLM-based%20approaches%0Awhile%20keeping%20communication%20overhead%20comparable.%20Our%20study%20highlights%20the%0Afeasibility%20and%20benefit%20of%20the%20proposed%20optimization%20techniques%20%28targeted%0Aknowledge%20retrieval%2C%20prompt%20enrichment%2C%20and%20ontology%20refinement%29%20for%20ontology%0Amatching%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14032v1&entry.124074799=Read"},
{"title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "author": "Marzieh Gheisari and Auguste Genovesio", "abstract": "  Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.\n", "link": "http://arxiv.org/abs/2507.13934v1", "date": "2025-07-18", "relevancy": 2.4737, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6455}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6007}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiViD%3A%20Disentangled%20Video%20Diffusion%20for%20Static-Dynamic%20Factorization&body=Title%3A%20DiViD%3A%20Disentangled%20Video%20Diffusion%20for%20Static-Dynamic%20Factorization%0AAuthor%3A%20Marzieh%20Gheisari%20and%20Auguste%20Genovesio%0AAbstract%3A%20%20%20Unsupervised%20disentanglement%20of%20static%20appearance%20and%20dynamic%20motion%20in%20video%0Aremains%20a%20fundamental%20challenge%2C%20often%20hindered%20by%20information%20leakage%20and%0Ablurry%20reconstructions%20in%20existing%20VAE-%20and%20GAN-based%20approaches.%20We%20introduce%0ADiViD%2C%20the%20first%20end-to-end%20video%20diffusion%20framework%20for%20explicit%0Astatic-dynamic%20factorization.%20DiViD%27s%20sequence%20encoder%20extracts%20a%20global%20static%0Atoken%20from%20the%20first%20frame%20and%20per-frame%20dynamic%20tokens%2C%20explicitly%20removing%0Astatic%20content%20from%20the%20motion%20code.%20Its%20conditional%20DDPM%20decoder%20incorporates%0Athree%20key%20inductive%20biases%3A%20a%20shared-noise%20schedule%20for%20temporal%20consistency%2C%20a%0Atime-varying%20KL-based%20bottleneck%20that%20tightens%20at%20early%20timesteps%20%28compressing%0Astatic%20information%29%20and%20relaxes%20later%20%28enriching%20dynamics%29%2C%20and%20cross-attention%0Athat%20routes%20the%20global%20static%20token%20to%20all%20frames%20while%20keeping%20dynamic%20tokens%0Aframe-specific.%20An%20orthogonality%20regularizer%20further%20prevents%20residual%0Astatic-dynamic%20leakage.%20We%20evaluate%20DiViD%20on%20real-world%20benchmarks%20using%0Aswap-based%20accuracy%20and%20cross-leakage%20metrics.%20DiViD%20outperforms%0Astate-of-the-art%20sequential%20disentanglement%20methods%3A%20it%20achieves%20the%20highest%0Aswap-based%20joint%20accuracy%2C%20preserves%20static%20fidelity%20while%20improving%20dynamic%0Atransfer%2C%20and%20reduces%20average%20cross-leakage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiViD%253A%2520Disentangled%2520Video%2520Diffusion%2520for%2520Static-Dynamic%2520Factorization%26entry.906535625%3DMarzieh%2520Gheisari%2520and%2520Auguste%2520Genovesio%26entry.1292438233%3D%2520%2520Unsupervised%2520disentanglement%2520of%2520static%2520appearance%2520and%2520dynamic%2520motion%2520in%2520video%250Aremains%2520a%2520fundamental%2520challenge%252C%2520often%2520hindered%2520by%2520information%2520leakage%2520and%250Ablurry%2520reconstructions%2520in%2520existing%2520VAE-%2520and%2520GAN-based%2520approaches.%2520We%2520introduce%250ADiViD%252C%2520the%2520first%2520end-to-end%2520video%2520diffusion%2520framework%2520for%2520explicit%250Astatic-dynamic%2520factorization.%2520DiViD%2527s%2520sequence%2520encoder%2520extracts%2520a%2520global%2520static%250Atoken%2520from%2520the%2520first%2520frame%2520and%2520per-frame%2520dynamic%2520tokens%252C%2520explicitly%2520removing%250Astatic%2520content%2520from%2520the%2520motion%2520code.%2520Its%2520conditional%2520DDPM%2520decoder%2520incorporates%250Athree%2520key%2520inductive%2520biases%253A%2520a%2520shared-noise%2520schedule%2520for%2520temporal%2520consistency%252C%2520a%250Atime-varying%2520KL-based%2520bottleneck%2520that%2520tightens%2520at%2520early%2520timesteps%2520%2528compressing%250Astatic%2520information%2529%2520and%2520relaxes%2520later%2520%2528enriching%2520dynamics%2529%252C%2520and%2520cross-attention%250Athat%2520routes%2520the%2520global%2520static%2520token%2520to%2520all%2520frames%2520while%2520keeping%2520dynamic%2520tokens%250Aframe-specific.%2520An%2520orthogonality%2520regularizer%2520further%2520prevents%2520residual%250Astatic-dynamic%2520leakage.%2520We%2520evaluate%2520DiViD%2520on%2520real-world%2520benchmarks%2520using%250Aswap-based%2520accuracy%2520and%2520cross-leakage%2520metrics.%2520DiViD%2520outperforms%250Astate-of-the-art%2520sequential%2520disentanglement%2520methods%253A%2520it%2520achieves%2520the%2520highest%250Aswap-based%2520joint%2520accuracy%252C%2520preserves%2520static%2520fidelity%2520while%2520improving%2520dynamic%250Atransfer%252C%2520and%2520reduces%2520average%2520cross-leakage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiViD%3A%20Disentangled%20Video%20Diffusion%20for%20Static-Dynamic%20Factorization&entry.906535625=Marzieh%20Gheisari%20and%20Auguste%20Genovesio&entry.1292438233=%20%20Unsupervised%20disentanglement%20of%20static%20appearance%20and%20dynamic%20motion%20in%20video%0Aremains%20a%20fundamental%20challenge%2C%20often%20hindered%20by%20information%20leakage%20and%0Ablurry%20reconstructions%20in%20existing%20VAE-%20and%20GAN-based%20approaches.%20We%20introduce%0ADiViD%2C%20the%20first%20end-to-end%20video%20diffusion%20framework%20for%20explicit%0Astatic-dynamic%20factorization.%20DiViD%27s%20sequence%20encoder%20extracts%20a%20global%20static%0Atoken%20from%20the%20first%20frame%20and%20per-frame%20dynamic%20tokens%2C%20explicitly%20removing%0Astatic%20content%20from%20the%20motion%20code.%20Its%20conditional%20DDPM%20decoder%20incorporates%0Athree%20key%20inductive%20biases%3A%20a%20shared-noise%20schedule%20for%20temporal%20consistency%2C%20a%0Atime-varying%20KL-based%20bottleneck%20that%20tightens%20at%20early%20timesteps%20%28compressing%0Astatic%20information%29%20and%20relaxes%20later%20%28enriching%20dynamics%29%2C%20and%20cross-attention%0Athat%20routes%20the%20global%20static%20token%20to%20all%20frames%20while%20keeping%20dynamic%20tokens%0Aframe-specific.%20An%20orthogonality%20regularizer%20further%20prevents%20residual%0Astatic-dynamic%20leakage.%20We%20evaluate%20DiViD%20on%20real-world%20benchmarks%20using%0Aswap-based%20accuracy%20and%20cross-leakage%20metrics.%20DiViD%20outperforms%0Astate-of-the-art%20sequential%20disentanglement%20methods%3A%20it%20achieves%20the%20highest%0Aswap-based%20joint%20accuracy%2C%20preserves%20static%20fidelity%20while%20improving%20dynamic%0Atransfer%2C%20and%20reduces%20average%20cross-leakage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13934v1&entry.124074799=Read"},
{"title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation", "author": "Haoran Li and Yuli Tian and Kun Lan and Yong Liao and Lin Wang and Pan Hui and Peng Yuan Zhou", "abstract": "  Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.\n", "link": "http://arxiv.org/abs/2507.13985v1", "date": "2025-07-18", "relevancy": 2.4667, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6201}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6201}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamScene%3A%203D%20Gaussian-based%20End-to-end%20Text-to-3D%20Scene%20Generation&body=Title%3A%20DreamScene%3A%203D%20Gaussian-based%20End-to-end%20Text-to-3D%20Scene%20Generation%0AAuthor%3A%20Haoran%20Li%20and%20Yuli%20Tian%20and%20Kun%20Lan%20and%20Yong%20Liao%20and%20Lin%20Wang%20and%20Pan%20Hui%20and%20Peng%20Yuan%20Zhou%0AAbstract%3A%20%20%20Generating%203D%20scenes%20from%20natural%20language%20holds%20great%20promise%20for%0Aapplications%20in%20gaming%2C%20film%2C%20and%20design.%20However%2C%20existing%20methods%20struggle%0Awith%20automation%2C%203D%20consistency%2C%20and%20fine-grained%20control.%20We%20present%0ADreamScene%2C%20an%20end-to-end%20framework%20for%20high-quality%20and%20editable%203D%20scene%0Ageneration%20from%20text%20or%20dialogue.%20DreamScene%20begins%20with%20a%20scene%20planning%0Amodule%2C%20where%20a%20GPT-4%20agent%20infers%20object%20semantics%20and%20spatial%20constraints%20to%0Aconstruct%20a%20hybrid%20graph.%20A%20graph-based%20placement%20algorithm%20then%20produces%20a%0Astructured%2C%20collision-free%20layout.%20Based%20on%20this%20layout%2C%20Formation%20Pattern%0ASampling%20%28FPS%29%20generates%20object%20geometry%20using%20multi-timestep%20sampling%20and%0Areconstructive%20optimization%2C%20enabling%20fast%20and%20realistic%20synthesis.%20To%20ensure%0Aglobal%20consistent%2C%20DreamScene%20employs%20a%20progressive%20camera%20sampling%20strategy%0Atailored%20to%20both%20indoor%20and%20outdoor%20settings.%20Finally%2C%20the%20system%20supports%0Afine-grained%20scene%20editing%2C%20including%20object%20movement%2C%20appearance%20changes%2C%20and%0A4D%20dynamic%20motion.%20Experiments%20demonstrate%20that%20DreamScene%20surpasses%20prior%0Amethods%20in%20quality%2C%20consistency%2C%20and%20flexibility%2C%20offering%20a%20practical%20solution%0Afor%20open-domain%203D%20content%20creation.%20Code%20and%20demos%20are%20available%20at%0Ahttps%3A//dreamscene-project.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamScene%253A%25203D%2520Gaussian-based%2520End-to-end%2520Text-to-3D%2520Scene%2520Generation%26entry.906535625%3DHaoran%2520Li%2520and%2520Yuli%2520Tian%2520and%2520Kun%2520Lan%2520and%2520Yong%2520Liao%2520and%2520Lin%2520Wang%2520and%2520Pan%2520Hui%2520and%2520Peng%2520Yuan%2520Zhou%26entry.1292438233%3D%2520%2520Generating%25203D%2520scenes%2520from%2520natural%2520language%2520holds%2520great%2520promise%2520for%250Aapplications%2520in%2520gaming%252C%2520film%252C%2520and%2520design.%2520However%252C%2520existing%2520methods%2520struggle%250Awith%2520automation%252C%25203D%2520consistency%252C%2520and%2520fine-grained%2520control.%2520We%2520present%250ADreamScene%252C%2520an%2520end-to-end%2520framework%2520for%2520high-quality%2520and%2520editable%25203D%2520scene%250Ageneration%2520from%2520text%2520or%2520dialogue.%2520DreamScene%2520begins%2520with%2520a%2520scene%2520planning%250Amodule%252C%2520where%2520a%2520GPT-4%2520agent%2520infers%2520object%2520semantics%2520and%2520spatial%2520constraints%2520to%250Aconstruct%2520a%2520hybrid%2520graph.%2520A%2520graph-based%2520placement%2520algorithm%2520then%2520produces%2520a%250Astructured%252C%2520collision-free%2520layout.%2520Based%2520on%2520this%2520layout%252C%2520Formation%2520Pattern%250ASampling%2520%2528FPS%2529%2520generates%2520object%2520geometry%2520using%2520multi-timestep%2520sampling%2520and%250Areconstructive%2520optimization%252C%2520enabling%2520fast%2520and%2520realistic%2520synthesis.%2520To%2520ensure%250Aglobal%2520consistent%252C%2520DreamScene%2520employs%2520a%2520progressive%2520camera%2520sampling%2520strategy%250Atailored%2520to%2520both%2520indoor%2520and%2520outdoor%2520settings.%2520Finally%252C%2520the%2520system%2520supports%250Afine-grained%2520scene%2520editing%252C%2520including%2520object%2520movement%252C%2520appearance%2520changes%252C%2520and%250A4D%2520dynamic%2520motion.%2520Experiments%2520demonstrate%2520that%2520DreamScene%2520surpasses%2520prior%250Amethods%2520in%2520quality%252C%2520consistency%252C%2520and%2520flexibility%252C%2520offering%2520a%2520practical%2520solution%250Afor%2520open-domain%25203D%2520content%2520creation.%2520Code%2520and%2520demos%2520are%2520available%2520at%250Ahttps%253A//dreamscene-project.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamScene%3A%203D%20Gaussian-based%20End-to-end%20Text-to-3D%20Scene%20Generation&entry.906535625=Haoran%20Li%20and%20Yuli%20Tian%20and%20Kun%20Lan%20and%20Yong%20Liao%20and%20Lin%20Wang%20and%20Pan%20Hui%20and%20Peng%20Yuan%20Zhou&entry.1292438233=%20%20Generating%203D%20scenes%20from%20natural%20language%20holds%20great%20promise%20for%0Aapplications%20in%20gaming%2C%20film%2C%20and%20design.%20However%2C%20existing%20methods%20struggle%0Awith%20automation%2C%203D%20consistency%2C%20and%20fine-grained%20control.%20We%20present%0ADreamScene%2C%20an%20end-to-end%20framework%20for%20high-quality%20and%20editable%203D%20scene%0Ageneration%20from%20text%20or%20dialogue.%20DreamScene%20begins%20with%20a%20scene%20planning%0Amodule%2C%20where%20a%20GPT-4%20agent%20infers%20object%20semantics%20and%20spatial%20constraints%20to%0Aconstruct%20a%20hybrid%20graph.%20A%20graph-based%20placement%20algorithm%20then%20produces%20a%0Astructured%2C%20collision-free%20layout.%20Based%20on%20this%20layout%2C%20Formation%20Pattern%0ASampling%20%28FPS%29%20generates%20object%20geometry%20using%20multi-timestep%20sampling%20and%0Areconstructive%20optimization%2C%20enabling%20fast%20and%20realistic%20synthesis.%20To%20ensure%0Aglobal%20consistent%2C%20DreamScene%20employs%20a%20progressive%20camera%20sampling%20strategy%0Atailored%20to%20both%20indoor%20and%20outdoor%20settings.%20Finally%2C%20the%20system%20supports%0Afine-grained%20scene%20editing%2C%20including%20object%20movement%2C%20appearance%20changes%2C%20and%0A4D%20dynamic%20motion.%20Experiments%20demonstrate%20that%20DreamScene%20surpasses%20prior%0Amethods%20in%20quality%2C%20consistency%2C%20and%20flexibility%2C%20offering%20a%20practical%20solution%0Afor%20open-domain%203D%20content%20creation.%20Code%20and%20demos%20are%20available%20at%0Ahttps%3A//dreamscene-project.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13985v1&entry.124074799=Read"},
{"title": "Learning to Reason at the Frontier of Learnability", "author": "Thomas Foster and Anya Sims and Johannes Forkel and Mattie Fellows and Jakob Foerster", "abstract": "  Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.\n", "link": "http://arxiv.org/abs/2502.12272v5", "date": "2025-07-18", "relevancy": 2.4545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4918}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Reason%20at%20the%20Frontier%20of%20Learnability&body=Title%3A%20Learning%20to%20Reason%20at%20the%20Frontier%20of%20Learnability%0AAuthor%3A%20Thomas%20Foster%20and%20Anya%20Sims%20and%20Johannes%20Forkel%20and%20Mattie%20Fellows%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Reinforcement%20learning%20is%20now%20widely%20adopted%20as%20the%20final%20stage%20of%20large%0Alanguage%20model%20training%2C%20especially%20for%20reasoning-style%20tasks%20such%20as%20maths%0Aproblems.%20Typically%2C%20models%20attempt%20each%20question%20many%20times%20during%20a%20single%0Atraining%20step%20and%20attempt%20to%20learn%20from%20their%20successes%20and%20failures.%20However%2C%0Awe%20demonstrate%20that%20throughout%20training%20with%20two%20popular%20algorithms%20%28PPO%20and%0AVinePPO%29%20on%20two%20widely%20used%20datasets%2C%20many%20questions%20are%20either%20solved%20by%20all%0Aattempts%20-%20meaning%20they%20are%20already%20learned%20-%20or%20by%20none%20-%20providing%20no%0Ameaningful%20training%20signal.%20To%20address%20this%2C%20we%20adapt%20a%20method%20from%20the%0Areinforcement%20learning%20literature%20-%20sampling%20for%20learnability%20-%20and%20apply%20it%20to%0Athe%20reinforcement%20learning%20stage%20of%20LLM%20training.%20Our%20curriculum%20prioritises%0Aquestions%20with%20high%20variance%20of%20success%2C%20i.e.%20those%20where%20the%20agent%20sometimes%0Asucceeds%2C%20but%20not%20always.%20Our%20findings%20demonstrate%20that%20this%20curriculum%0Aconsistently%20boosts%20training%20performance%20across%20multiple%20algorithms%20and%0Adatasets%2C%20paving%20the%20way%20for%20more%20efficient%20and%20effective%20reinforcement%0Alearning%20with%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12272v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Reason%2520at%2520the%2520Frontier%2520of%2520Learnability%26entry.906535625%3DThomas%2520Foster%2520and%2520Anya%2520Sims%2520and%2520Johannes%2520Forkel%2520and%2520Mattie%2520Fellows%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520is%2520now%2520widely%2520adopted%2520as%2520the%2520final%2520stage%2520of%2520large%250Alanguage%2520model%2520training%252C%2520especially%2520for%2520reasoning-style%2520tasks%2520such%2520as%2520maths%250Aproblems.%2520Typically%252C%2520models%2520attempt%2520each%2520question%2520many%2520times%2520during%2520a%2520single%250Atraining%2520step%2520and%2520attempt%2520to%2520learn%2520from%2520their%2520successes%2520and%2520failures.%2520However%252C%250Awe%2520demonstrate%2520that%2520throughout%2520training%2520with%2520two%2520popular%2520algorithms%2520%2528PPO%2520and%250AVinePPO%2529%2520on%2520two%2520widely%2520used%2520datasets%252C%2520many%2520questions%2520are%2520either%2520solved%2520by%2520all%250Aattempts%2520-%2520meaning%2520they%2520are%2520already%2520learned%2520-%2520or%2520by%2520none%2520-%2520providing%2520no%250Ameaningful%2520training%2520signal.%2520To%2520address%2520this%252C%2520we%2520adapt%2520a%2520method%2520from%2520the%250Areinforcement%2520learning%2520literature%2520-%2520sampling%2520for%2520learnability%2520-%2520and%2520apply%2520it%2520to%250Athe%2520reinforcement%2520learning%2520stage%2520of%2520LLM%2520training.%2520Our%2520curriculum%2520prioritises%250Aquestions%2520with%2520high%2520variance%2520of%2520success%252C%2520i.e.%2520those%2520where%2520the%2520agent%2520sometimes%250Asucceeds%252C%2520but%2520not%2520always.%2520Our%2520findings%2520demonstrate%2520that%2520this%2520curriculum%250Aconsistently%2520boosts%2520training%2520performance%2520across%2520multiple%2520algorithms%2520and%250Adatasets%252C%2520paving%2520the%2520way%2520for%2520more%2520efficient%2520and%2520effective%2520reinforcement%250Alearning%2520with%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12272v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Reason%20at%20the%20Frontier%20of%20Learnability&entry.906535625=Thomas%20Foster%20and%20Anya%20Sims%20and%20Johannes%20Forkel%20and%20Mattie%20Fellows%20and%20Jakob%20Foerster&entry.1292438233=%20%20Reinforcement%20learning%20is%20now%20widely%20adopted%20as%20the%20final%20stage%20of%20large%0Alanguage%20model%20training%2C%20especially%20for%20reasoning-style%20tasks%20such%20as%20maths%0Aproblems.%20Typically%2C%20models%20attempt%20each%20question%20many%20times%20during%20a%20single%0Atraining%20step%20and%20attempt%20to%20learn%20from%20their%20successes%20and%20failures.%20However%2C%0Awe%20demonstrate%20that%20throughout%20training%20with%20two%20popular%20algorithms%20%28PPO%20and%0AVinePPO%29%20on%20two%20widely%20used%20datasets%2C%20many%20questions%20are%20either%20solved%20by%20all%0Aattempts%20-%20meaning%20they%20are%20already%20learned%20-%20or%20by%20none%20-%20providing%20no%0Ameaningful%20training%20signal.%20To%20address%20this%2C%20we%20adapt%20a%20method%20from%20the%0Areinforcement%20learning%20literature%20-%20sampling%20for%20learnability%20-%20and%20apply%20it%20to%0Athe%20reinforcement%20learning%20stage%20of%20LLM%20training.%20Our%20curriculum%20prioritises%0Aquestions%20with%20high%20variance%20of%20success%2C%20i.e.%20those%20where%20the%20agent%20sometimes%0Asucceeds%2C%20but%20not%20always.%20Our%20findings%20demonstrate%20that%20this%20curriculum%0Aconsistently%20boosts%20training%20performance%20across%20multiple%20algorithms%20and%0Adatasets%2C%20paving%20the%20way%20for%20more%20efficient%20and%20effective%20reinforcement%0Alearning%20with%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12272v5&entry.124074799=Read"},
{"title": "Convergent transformations of visual representation in brains and models", "author": "Pablo Marcos-Manch\u00f3n and Llu\u00eds Fuentemilla", "abstract": "  A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.\n", "link": "http://arxiv.org/abs/2507.13941v1", "date": "2025-07-18", "relevancy": 2.4493, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6236}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Convergent%20transformations%20of%20visual%20representation%20in%20brains%20and%20models&body=Title%3A%20Convergent%20transformations%20of%20visual%20representation%20in%20brains%20and%20models%0AAuthor%3A%20Pablo%20Marcos-Manch%C3%B3n%20and%20Llu%C3%ADs%20Fuentemilla%0AAbstract%3A%20%20%20A%20fundamental%20question%20in%20cognitive%20neuroscience%20is%20what%20shapes%20visual%0Aperception%3A%20the%20external%20world%27s%20structure%20or%20the%20brain%27s%20internal%0Aarchitecture.%20Although%20some%20perceptual%20variability%20can%20be%20traced%20to%20individual%0Adifferences%2C%20brain%20responses%20to%20naturalistic%20stimuli%20evoke%20similar%20activity%0Apatterns%20across%20individuals%2C%20suggesting%20a%20convergent%20representational%0Aprinciple.%20Here%2C%20we%20test%20if%20this%20stimulus-driven%20convergence%20follows%20a%20common%0Atrajectory%20across%20people%20and%20deep%20neural%20networks%20%28DNNs%29%20during%20its%0Atransformation%20from%20sensory%20to%20high-level%20internal%20representations.%20We%0Aintroduce%20a%20unified%20framework%20that%20traces%20representational%20flow%20by%20combining%0Ainter-subject%20similarity%20with%20alignment%20to%20model%20hierarchies.%20Applying%20this%0Aframework%20to%20three%20independent%20fMRI%20datasets%20of%20visual%20scene%20perception%2C%20we%0Areveal%20a%20cortex-wide%20network%2C%20conserved%20across%20individuals%2C%20organized%20into%20two%0Apathways%3A%20a%20medial-ventral%20stream%20for%20scene%20structure%20and%20a%20lateral-dorsal%0Astream%20tuned%20for%20social%20and%20biological%20content.%20This%20functional%20organization%20is%0Acaptured%20by%20the%20hierarchies%20of%20vision%20DNNs%20but%20not%20language%20models%2C%20reinforcing%0Athe%20specificity%20of%20the%20visual-to-semantic%20transformation.%20These%20findings%20show%20a%0Aconvergent%20computational%20solution%20for%20visual%20encoding%20in%20both%20human%20and%0Aartificial%20vision%2C%20driven%20by%20the%20structure%20of%20the%20external%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConvergent%2520transformations%2520of%2520visual%2520representation%2520in%2520brains%2520and%2520models%26entry.906535625%3DPablo%2520Marcos-Manch%25C3%25B3n%2520and%2520Llu%25C3%25ADs%2520Fuentemilla%26entry.1292438233%3D%2520%2520A%2520fundamental%2520question%2520in%2520cognitive%2520neuroscience%2520is%2520what%2520shapes%2520visual%250Aperception%253A%2520the%2520external%2520world%2527s%2520structure%2520or%2520the%2520brain%2527s%2520internal%250Aarchitecture.%2520Although%2520some%2520perceptual%2520variability%2520can%2520be%2520traced%2520to%2520individual%250Adifferences%252C%2520brain%2520responses%2520to%2520naturalistic%2520stimuli%2520evoke%2520similar%2520activity%250Apatterns%2520across%2520individuals%252C%2520suggesting%2520a%2520convergent%2520representational%250Aprinciple.%2520Here%252C%2520we%2520test%2520if%2520this%2520stimulus-driven%2520convergence%2520follows%2520a%2520common%250Atrajectory%2520across%2520people%2520and%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520during%2520its%250Atransformation%2520from%2520sensory%2520to%2520high-level%2520internal%2520representations.%2520We%250Aintroduce%2520a%2520unified%2520framework%2520that%2520traces%2520representational%2520flow%2520by%2520combining%250Ainter-subject%2520similarity%2520with%2520alignment%2520to%2520model%2520hierarchies.%2520Applying%2520this%250Aframework%2520to%2520three%2520independent%2520fMRI%2520datasets%2520of%2520visual%2520scene%2520perception%252C%2520we%250Areveal%2520a%2520cortex-wide%2520network%252C%2520conserved%2520across%2520individuals%252C%2520organized%2520into%2520two%250Apathways%253A%2520a%2520medial-ventral%2520stream%2520for%2520scene%2520structure%2520and%2520a%2520lateral-dorsal%250Astream%2520tuned%2520for%2520social%2520and%2520biological%2520content.%2520This%2520functional%2520organization%2520is%250Acaptured%2520by%2520the%2520hierarchies%2520of%2520vision%2520DNNs%2520but%2520not%2520language%2520models%252C%2520reinforcing%250Athe%2520specificity%2520of%2520the%2520visual-to-semantic%2520transformation.%2520These%2520findings%2520show%2520a%250Aconvergent%2520computational%2520solution%2520for%2520visual%2520encoding%2520in%2520both%2520human%2520and%250Aartificial%2520vision%252C%2520driven%2520by%2520the%2520structure%2520of%2520the%2520external%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergent%20transformations%20of%20visual%20representation%20in%20brains%20and%20models&entry.906535625=Pablo%20Marcos-Manch%C3%B3n%20and%20Llu%C3%ADs%20Fuentemilla&entry.1292438233=%20%20A%20fundamental%20question%20in%20cognitive%20neuroscience%20is%20what%20shapes%20visual%0Aperception%3A%20the%20external%20world%27s%20structure%20or%20the%20brain%27s%20internal%0Aarchitecture.%20Although%20some%20perceptual%20variability%20can%20be%20traced%20to%20individual%0Adifferences%2C%20brain%20responses%20to%20naturalistic%20stimuli%20evoke%20similar%20activity%0Apatterns%20across%20individuals%2C%20suggesting%20a%20convergent%20representational%0Aprinciple.%20Here%2C%20we%20test%20if%20this%20stimulus-driven%20convergence%20follows%20a%20common%0Atrajectory%20across%20people%20and%20deep%20neural%20networks%20%28DNNs%29%20during%20its%0Atransformation%20from%20sensory%20to%20high-level%20internal%20representations.%20We%0Aintroduce%20a%20unified%20framework%20that%20traces%20representational%20flow%20by%20combining%0Ainter-subject%20similarity%20with%20alignment%20to%20model%20hierarchies.%20Applying%20this%0Aframework%20to%20three%20independent%20fMRI%20datasets%20of%20visual%20scene%20perception%2C%20we%0Areveal%20a%20cortex-wide%20network%2C%20conserved%20across%20individuals%2C%20organized%20into%20two%0Apathways%3A%20a%20medial-ventral%20stream%20for%20scene%20structure%20and%20a%20lateral-dorsal%0Astream%20tuned%20for%20social%20and%20biological%20content.%20This%20functional%20organization%20is%0Acaptured%20by%20the%20hierarchies%20of%20vision%20DNNs%20but%20not%20language%20models%2C%20reinforcing%0Athe%20specificity%20of%20the%20visual-to-semantic%20transformation.%20These%20findings%20show%20a%0Aconvergent%20computational%20solution%20for%20visual%20encoding%20in%20both%20human%20and%0Aartificial%20vision%2C%20driven%20by%20the%20structure%20of%20the%20external%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13941v1&entry.124074799=Read"},
{"title": "How Far Have Medical Vision-Language Models Come? A Comprehensive\n  Benchmarking Study", "author": "Che Liu and Jiazhen Pan and Weixiang Shen and Wenjia Bai and Daniel Rueckert and Rossella Arcucci", "abstract": "  Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols.\n", "link": "http://arxiv.org/abs/2507.11200v2", "date": "2025-07-18", "relevancy": 2.4436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6308}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20Have%20Medical%20Vision-Language%20Models%20Come%3F%20A%20Comprehensive%0A%20%20Benchmarking%20Study&body=Title%3A%20How%20Far%20Have%20Medical%20Vision-Language%20Models%20Come%3F%20A%20Comprehensive%0A%20%20Benchmarking%20Study%0AAuthor%3A%20Che%20Liu%20and%20Jiazhen%20Pan%20and%20Weixiang%20Shen%20and%20Wenjia%20Bai%20and%20Daniel%20Rueckert%20and%20Rossella%20Arcucci%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20trained%20on%20web-scale%20corpora%20excel%20at%20natural%0Aimage%20tasks%20and%20are%20increasingly%20repurposed%20for%20healthcare%3B%20however%2C%20their%0Acompetence%20in%20medical%20tasks%20remains%20underexplored.%20We%20present%20a%20comprehensive%0Aevaluation%20of%20open-source%20general-purpose%20and%20medically%20specialised%20VLMs%2C%0Aranging%20from%203B%20to%2072B%20parameters%2C%20across%20eight%20benchmarks%3A%20MedXpert%2C%0AOmniMedVQA%2C%20PMC-VQA%2C%20PathVQA%2C%20MMMU%2C%20SLAKE%2C%20and%20VQA-RAD.%20To%20observe%20model%0Aperformance%20across%20different%20aspects%2C%20we%20first%20separate%20it%20into%20understanding%0Aand%20reasoning%20components.%20Three%20salient%20findings%20emerge.%20First%2C%20large%0Ageneral-purpose%20models%20already%20match%20or%20surpass%20medical-specific%20counterparts%0Aon%20several%20benchmarks%2C%20demonstrating%20strong%20zero-shot%20transfer%20from%20natural%20to%0Amedical%20images.%20Second%2C%20reasoning%20performance%20is%20consistently%20lower%20than%0Aunderstanding%2C%20highlighting%20a%20critical%20barrier%20to%20safe%20decision%20support.%20Third%2C%0Aperformance%20varies%20widely%20across%20benchmarks%2C%20reflecting%20differences%20in%20task%0Adesign%2C%20annotation%20quality%2C%20and%20knowledge%20demands.%20No%20model%20yet%20reaches%20the%0Areliability%20threshold%20for%20clinical%20deployment%2C%20underscoring%20the%20need%20for%0Astronger%20multimodal%20alignment%20and%20more%20rigorous%2C%20fine-grained%20evaluation%0Aprotocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520Have%2520Medical%2520Vision-Language%2520Models%2520Come%253F%2520A%2520Comprehensive%250A%2520%2520Benchmarking%2520Study%26entry.906535625%3DChe%2520Liu%2520and%2520Jiazhen%2520Pan%2520and%2520Weixiang%2520Shen%2520and%2520Wenjia%2520Bai%2520and%2520Daniel%2520Rueckert%2520and%2520Rossella%2520Arcucci%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520trained%2520on%2520web-scale%2520corpora%2520excel%2520at%2520natural%250Aimage%2520tasks%2520and%2520are%2520increasingly%2520repurposed%2520for%2520healthcare%253B%2520however%252C%2520their%250Acompetence%2520in%2520medical%2520tasks%2520remains%2520underexplored.%2520We%2520present%2520a%2520comprehensive%250Aevaluation%2520of%2520open-source%2520general-purpose%2520and%2520medically%2520specialised%2520VLMs%252C%250Aranging%2520from%25203B%2520to%252072B%2520parameters%252C%2520across%2520eight%2520benchmarks%253A%2520MedXpert%252C%250AOmniMedVQA%252C%2520PMC-VQA%252C%2520PathVQA%252C%2520MMMU%252C%2520SLAKE%252C%2520and%2520VQA-RAD.%2520To%2520observe%2520model%250Aperformance%2520across%2520different%2520aspects%252C%2520we%2520first%2520separate%2520it%2520into%2520understanding%250Aand%2520reasoning%2520components.%2520Three%2520salient%2520findings%2520emerge.%2520First%252C%2520large%250Ageneral-purpose%2520models%2520already%2520match%2520or%2520surpass%2520medical-specific%2520counterparts%250Aon%2520several%2520benchmarks%252C%2520demonstrating%2520strong%2520zero-shot%2520transfer%2520from%2520natural%2520to%250Amedical%2520images.%2520Second%252C%2520reasoning%2520performance%2520is%2520consistently%2520lower%2520than%250Aunderstanding%252C%2520highlighting%2520a%2520critical%2520barrier%2520to%2520safe%2520decision%2520support.%2520Third%252C%250Aperformance%2520varies%2520widely%2520across%2520benchmarks%252C%2520reflecting%2520differences%2520in%2520task%250Adesign%252C%2520annotation%2520quality%252C%2520and%2520knowledge%2520demands.%2520No%2520model%2520yet%2520reaches%2520the%250Areliability%2520threshold%2520for%2520clinical%2520deployment%252C%2520underscoring%2520the%2520need%2520for%250Astronger%2520multimodal%2520alignment%2520and%2520more%2520rigorous%252C%2520fine-grained%2520evaluation%250Aprotocols.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20Have%20Medical%20Vision-Language%20Models%20Come%3F%20A%20Comprehensive%0A%20%20Benchmarking%20Study&entry.906535625=Che%20Liu%20and%20Jiazhen%20Pan%20and%20Weixiang%20Shen%20and%20Wenjia%20Bai%20and%20Daniel%20Rueckert%20and%20Rossella%20Arcucci&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20trained%20on%20web-scale%20corpora%20excel%20at%20natural%0Aimage%20tasks%20and%20are%20increasingly%20repurposed%20for%20healthcare%3B%20however%2C%20their%0Acompetence%20in%20medical%20tasks%20remains%20underexplored.%20We%20present%20a%20comprehensive%0Aevaluation%20of%20open-source%20general-purpose%20and%20medically%20specialised%20VLMs%2C%0Aranging%20from%203B%20to%2072B%20parameters%2C%20across%20eight%20benchmarks%3A%20MedXpert%2C%0AOmniMedVQA%2C%20PMC-VQA%2C%20PathVQA%2C%20MMMU%2C%20SLAKE%2C%20and%20VQA-RAD.%20To%20observe%20model%0Aperformance%20across%20different%20aspects%2C%20we%20first%20separate%20it%20into%20understanding%0Aand%20reasoning%20components.%20Three%20salient%20findings%20emerge.%20First%2C%20large%0Ageneral-purpose%20models%20already%20match%20or%20surpass%20medical-specific%20counterparts%0Aon%20several%20benchmarks%2C%20demonstrating%20strong%20zero-shot%20transfer%20from%20natural%20to%0Amedical%20images.%20Second%2C%20reasoning%20performance%20is%20consistently%20lower%20than%0Aunderstanding%2C%20highlighting%20a%20critical%20barrier%20to%20safe%20decision%20support.%20Third%2C%0Aperformance%20varies%20widely%20across%20benchmarks%2C%20reflecting%20differences%20in%20task%0Adesign%2C%20annotation%20quality%2C%20and%20knowledge%20demands.%20No%20model%20yet%20reaches%20the%0Areliability%20threshold%20for%20clinical%20deployment%2C%20underscoring%20the%20need%20for%0Astronger%20multimodal%20alignment%20and%20more%20rigorous%2C%20fine-grained%20evaluation%0Aprotocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11200v2&entry.124074799=Read"},
{"title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene\n  Completion", "author": "Haoang Lu and Yuanqi Su and Xiaoning Zhang and Hao Hu", "abstract": "  In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.\n", "link": "http://arxiv.org/abs/2507.13801v1", "date": "2025-07-18", "relevancy": 2.4312, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Step%20Closer%3A%20Creating%20the%20Future%20to%20Boost%20Monocular%20Semantic%20Scene%0A%20%20Completion&body=Title%3A%20One%20Step%20Closer%3A%20Creating%20the%20Future%20to%20Boost%20Monocular%20Semantic%20Scene%0A%20%20Completion%0AAuthor%3A%20Haoang%20Lu%20and%20Yuanqi%20Su%20and%20Xiaoning%20Zhang%20and%20Hao%20Hu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20visual%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20has%20emerged%20as%20a%0Acritical%20perception%20task%20for%20autonomous%20driving%20due%20to%20its%20ability%20to%20infer%0Acomplete%203D%20scene%20layouts%20and%20semantics%20from%20single%202D%20images.%20However%2C%20in%0Areal-world%20traffic%20scenarios%2C%20a%20significant%20portion%20of%20the%20scene%20remains%0Aoccluded%20or%20outside%20the%20camera%27s%20field%20of%20view%20--%20a%20fundamental%20challenge%20that%0Aexisting%20monocular%20SSC%20methods%20fail%20to%20address%20adequately.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20Creating%20the%20Future%20SSC%20%28CF-SSC%29%2C%20a%20novel%20temporal%20SSC%0Aframework%20that%20leverages%20pseudo-future%20frame%20prediction%20to%20expand%20the%20model%27s%0Aeffective%20perceptual%20range.%20Our%20approach%20combines%20poses%20and%20depths%20to%20establish%0Aaccurate%203D%20correspondences%2C%20enabling%20geometrically-consistent%20fusion%20of%20past%2C%0Apresent%2C%20and%20predicted%20future%20frames%20in%203D%20space.%20Unlike%20conventional%20methods%0Athat%20rely%20on%20simple%20feature%20stacking%2C%20our%203D-aware%20architecture%20achieves%20more%0Arobust%20scene%20completion%20by%20explicitly%20modeling%20spatial-temporal%20relationships.%0AComprehensive%20experiments%20on%20SemanticKITTI%20and%20SSCBench-KITTI-360%20benchmarks%0Ademonstrate%20state-of-the-art%20performance%2C%20validating%20the%20effectiveness%20of%20our%0Aapproach%2C%20highlighting%20our%20method%27s%20ability%20to%20improve%20occlusion%20reasoning%20and%0A3D%20scene%20completion%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Step%2520Closer%253A%2520Creating%2520the%2520Future%2520to%2520Boost%2520Monocular%2520Semantic%2520Scene%250A%2520%2520Completion%26entry.906535625%3DHaoang%2520Lu%2520and%2520Yuanqi%2520Su%2520and%2520Xiaoning%2520Zhang%2520and%2520Hao%2520Hu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520visual%25203D%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520has%2520emerged%2520as%2520a%250Acritical%2520perception%2520task%2520for%2520autonomous%2520driving%2520due%2520to%2520its%2520ability%2520to%2520infer%250Acomplete%25203D%2520scene%2520layouts%2520and%2520semantics%2520from%2520single%25202D%2520images.%2520However%252C%2520in%250Areal-world%2520traffic%2520scenarios%252C%2520a%2520significant%2520portion%2520of%2520the%2520scene%2520remains%250Aoccluded%2520or%2520outside%2520the%2520camera%2527s%2520field%2520of%2520view%2520--%2520a%2520fundamental%2520challenge%2520that%250Aexisting%2520monocular%2520SSC%2520methods%2520fail%2520to%2520address%2520adequately.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520Creating%2520the%2520Future%2520SSC%2520%2528CF-SSC%2529%252C%2520a%2520novel%2520temporal%2520SSC%250Aframework%2520that%2520leverages%2520pseudo-future%2520frame%2520prediction%2520to%2520expand%2520the%2520model%2527s%250Aeffective%2520perceptual%2520range.%2520Our%2520approach%2520combines%2520poses%2520and%2520depths%2520to%2520establish%250Aaccurate%25203D%2520correspondences%252C%2520enabling%2520geometrically-consistent%2520fusion%2520of%2520past%252C%250Apresent%252C%2520and%2520predicted%2520future%2520frames%2520in%25203D%2520space.%2520Unlike%2520conventional%2520methods%250Athat%2520rely%2520on%2520simple%2520feature%2520stacking%252C%2520our%25203D-aware%2520architecture%2520achieves%2520more%250Arobust%2520scene%2520completion%2520by%2520explicitly%2520modeling%2520spatial-temporal%2520relationships.%250AComprehensive%2520experiments%2520on%2520SemanticKITTI%2520and%2520SSCBench-KITTI-360%2520benchmarks%250Ademonstrate%2520state-of-the-art%2520performance%252C%2520validating%2520the%2520effectiveness%2520of%2520our%250Aapproach%252C%2520highlighting%2520our%2520method%2527s%2520ability%2520to%2520improve%2520occlusion%2520reasoning%2520and%250A3D%2520scene%2520completion%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Step%20Closer%3A%20Creating%20the%20Future%20to%20Boost%20Monocular%20Semantic%20Scene%0A%20%20Completion&entry.906535625=Haoang%20Lu%20and%20Yuanqi%20Su%20and%20Xiaoning%20Zhang%20and%20Hao%20Hu&entry.1292438233=%20%20In%20recent%20years%2C%20visual%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20has%20emerged%20as%20a%0Acritical%20perception%20task%20for%20autonomous%20driving%20due%20to%20its%20ability%20to%20infer%0Acomplete%203D%20scene%20layouts%20and%20semantics%20from%20single%202D%20images.%20However%2C%20in%0Areal-world%20traffic%20scenarios%2C%20a%20significant%20portion%20of%20the%20scene%20remains%0Aoccluded%20or%20outside%20the%20camera%27s%20field%20of%20view%20--%20a%20fundamental%20challenge%20that%0Aexisting%20monocular%20SSC%20methods%20fail%20to%20address%20adequately.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20Creating%20the%20Future%20SSC%20%28CF-SSC%29%2C%20a%20novel%20temporal%20SSC%0Aframework%20that%20leverages%20pseudo-future%20frame%20prediction%20to%20expand%20the%20model%27s%0Aeffective%20perceptual%20range.%20Our%20approach%20combines%20poses%20and%20depths%20to%20establish%0Aaccurate%203D%20correspondences%2C%20enabling%20geometrically-consistent%20fusion%20of%20past%2C%0Apresent%2C%20and%20predicted%20future%20frames%20in%203D%20space.%20Unlike%20conventional%20methods%0Athat%20rely%20on%20simple%20feature%20stacking%2C%20our%203D-aware%20architecture%20achieves%20more%0Arobust%20scene%20completion%20by%20explicitly%20modeling%20spatial-temporal%20relationships.%0AComprehensive%20experiments%20on%20SemanticKITTI%20and%20SSCBench-KITTI-360%20benchmarks%0Ademonstrate%20state-of-the-art%20performance%2C%20validating%20the%20effectiveness%20of%20our%0Aapproach%2C%20highlighting%20our%20method%27s%20ability%20to%20improve%20occlusion%20reasoning%20and%0A3D%20scene%20completion%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13801v1&entry.124074799=Read"},
{"title": "Team of One: Cracking Complex Video QA with Model Synergy", "author": "Jun Xie and Zhaoran Zhao and Xiongjun Guan and Yingjian Zhu and Hongzhu Yi and Xinming Wang and Feng Chen and Zhepeng Wang", "abstract": "  We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.\n", "link": "http://arxiv.org/abs/2507.13820v1", "date": "2025-07-18", "relevancy": 2.4272, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6208}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Team%20of%20One%3A%20Cracking%20Complex%20Video%20QA%20with%20Model%20Synergy&body=Title%3A%20Team%20of%20One%3A%20Cracking%20Complex%20Video%20QA%20with%20Model%20Synergy%0AAuthor%3A%20Jun%20Xie%20and%20Zhaoran%20Zhao%20and%20Xiongjun%20Guan%20and%20Yingjian%20Zhu%20and%20Hongzhu%20Yi%20and%20Xinming%20Wang%20and%20Feng%20Chen%20and%20Zhepeng%20Wang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20framework%20for%20open-ended%20video%20question%20answering%20that%0Aenhances%20reasoning%20depth%20and%20robustness%20in%20complex%20real-world%20scenarios%2C%20as%0Abenchmarked%20on%20the%20CVRR-ES%20dataset.%20Existing%20Video-Large%20Multimodal%20Models%0A%28Video-LMMs%29%20often%20exhibit%20limited%20contextual%20understanding%2C%20weak%20temporal%0Amodeling%2C%20and%20poor%20generalization%20to%20ambiguous%20or%20compositional%20queries.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20prompting-and-response%20integration%0Amechanism%20that%20coordinates%20multiple%20heterogeneous%20Video-Language%20Models%20%28VLMs%29%0Avia%20structured%20chains%20of%20thought%2C%20each%20tailored%20to%20distinct%20reasoning%20pathways.%0AAn%20external%20Large%20Language%20Model%20%28LLM%29%20serves%20as%20an%20evaluator%20and%20integrator%2C%0Aselecting%20and%20fusing%20the%20most%20reliable%20responses.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20baselines%20across%0Aall%20evaluation%20metrics%2C%20showcasing%20superior%20generalization%20and%20robustness.%20Our%0Aapproach%20offers%20a%20lightweight%2C%20extensible%20strategy%20for%20advancing%20multimodal%0Areasoning%20without%20requiring%20model%20retraining%2C%20setting%20a%20strong%20foundation%20for%0Afuture%20Video-LMM%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeam%2520of%2520One%253A%2520Cracking%2520Complex%2520Video%2520QA%2520with%2520Model%2520Synergy%26entry.906535625%3DJun%2520Xie%2520and%2520Zhaoran%2520Zhao%2520and%2520Xiongjun%2520Guan%2520and%2520Yingjian%2520Zhu%2520and%2520Hongzhu%2520Yi%2520and%2520Xinming%2520Wang%2520and%2520Feng%2520Chen%2520and%2520Zhepeng%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520framework%2520for%2520open-ended%2520video%2520question%2520answering%2520that%250Aenhances%2520reasoning%2520depth%2520and%2520robustness%2520in%2520complex%2520real-world%2520scenarios%252C%2520as%250Abenchmarked%2520on%2520the%2520CVRR-ES%2520dataset.%2520Existing%2520Video-Large%2520Multimodal%2520Models%250A%2528Video-LMMs%2529%2520often%2520exhibit%2520limited%2520contextual%2520understanding%252C%2520weak%2520temporal%250Amodeling%252C%2520and%2520poor%2520generalization%2520to%2520ambiguous%2520or%2520compositional%2520queries.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520prompting-and-response%2520integration%250Amechanism%2520that%2520coordinates%2520multiple%2520heterogeneous%2520Video-Language%2520Models%2520%2528VLMs%2529%250Avia%2520structured%2520chains%2520of%2520thought%252C%2520each%2520tailored%2520to%2520distinct%2520reasoning%2520pathways.%250AAn%2520external%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520serves%2520as%2520an%2520evaluator%2520and%2520integrator%252C%250Aselecting%2520and%2520fusing%2520the%2520most%2520reliable%2520responses.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%2520baselines%2520across%250Aall%2520evaluation%2520metrics%252C%2520showcasing%2520superior%2520generalization%2520and%2520robustness.%2520Our%250Aapproach%2520offers%2520a%2520lightweight%252C%2520extensible%2520strategy%2520for%2520advancing%2520multimodal%250Areasoning%2520without%2520requiring%2520model%2520retraining%252C%2520setting%2520a%2520strong%2520foundation%2520for%250Afuture%2520Video-LMM%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Team%20of%20One%3A%20Cracking%20Complex%20Video%20QA%20with%20Model%20Synergy&entry.906535625=Jun%20Xie%20and%20Zhaoran%20Zhao%20and%20Xiongjun%20Guan%20and%20Yingjian%20Zhu%20and%20Hongzhu%20Yi%20and%20Xinming%20Wang%20and%20Feng%20Chen%20and%20Zhepeng%20Wang&entry.1292438233=%20%20We%20propose%20a%20novel%20framework%20for%20open-ended%20video%20question%20answering%20that%0Aenhances%20reasoning%20depth%20and%20robustness%20in%20complex%20real-world%20scenarios%2C%20as%0Abenchmarked%20on%20the%20CVRR-ES%20dataset.%20Existing%20Video-Large%20Multimodal%20Models%0A%28Video-LMMs%29%20often%20exhibit%20limited%20contextual%20understanding%2C%20weak%20temporal%0Amodeling%2C%20and%20poor%20generalization%20to%20ambiguous%20or%20compositional%20queries.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20a%20prompting-and-response%20integration%0Amechanism%20that%20coordinates%20multiple%20heterogeneous%20Video-Language%20Models%20%28VLMs%29%0Avia%20structured%20chains%20of%20thought%2C%20each%20tailored%20to%20distinct%20reasoning%20pathways.%0AAn%20external%20Large%20Language%20Model%20%28LLM%29%20serves%20as%20an%20evaluator%20and%20integrator%2C%0Aselecting%20and%20fusing%20the%20most%20reliable%20responses.%20Extensive%20experiments%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20baselines%20across%0Aall%20evaluation%20metrics%2C%20showcasing%20superior%20generalization%20and%20robustness.%20Our%0Aapproach%20offers%20a%20lightweight%2C%20extensible%20strategy%20for%20advancing%20multimodal%0Areasoning%20without%20requiring%20model%20retraining%2C%20setting%20a%20strong%20foundation%20for%0Afuture%20Video-LMM%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13820v1&entry.124074799=Read"},
{"title": "Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and\n  Interpretation", "author": "Elisa Sanchez-Bayona and Rodrigo Agerri", "abstract": "  Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources.\n", "link": "http://arxiv.org/abs/2404.07053v2", "date": "2025-07-18", "relevancy": 2.4235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta4XNLI%3A%20A%20Crosslingual%20Parallel%20Corpus%20for%20Metaphor%20Detection%20and%0A%20%20Interpretation&body=Title%3A%20Meta4XNLI%3A%20A%20Crosslingual%20Parallel%20Corpus%20for%20Metaphor%20Detection%20and%0A%20%20Interpretation%0AAuthor%3A%20Elisa%20Sanchez-Bayona%20and%20Rodrigo%20Agerri%0AAbstract%3A%20%20%20Metaphors%2C%20although%20occasionally%20unperceived%2C%20are%20ubiquitous%20in%20our%20everyday%0Alanguage.%20Thus%2C%20it%20is%20crucial%20for%20Language%20Models%20to%20be%20able%20to%20grasp%20the%0Aunderlying%20meaning%20of%20this%20kind%20of%20figurative%20language.%20In%20this%20work%2C%20we%0Apresent%20Meta4XNLI%2C%20a%20novel%20parallel%20dataset%20for%20the%20tasks%20of%20metaphor%20detection%0Aand%20interpretation%20that%20contains%20metaphor%20annotations%20in%20both%20Spanish%20and%0AEnglish.%20We%20investigate%20language%20models%27%20metaphor%20identification%20and%0Aunderstanding%20abilities%20through%20a%20series%20of%20monolingual%20and%20cross-lingual%0Aexperiments%20by%20leveraging%20our%20proposed%20corpus.%20In%20order%20to%20comprehend%20how%20these%0Anon-literal%20expressions%20affect%20models%27%20performance%2C%20we%20look%20over%20the%20results%0Aand%20perform%20an%20error%20analysis.%20Additionally%2C%20parallel%20data%20offers%20many%0Apotential%20opportunities%20to%20investigate%20metaphor%20transferability%20between%20these%0Alanguages%20and%20the%20impact%20of%20translation%20on%20the%20development%20of%20multilingual%0Aannotated%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07053v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta4XNLI%253A%2520A%2520Crosslingual%2520Parallel%2520Corpus%2520for%2520Metaphor%2520Detection%2520and%250A%2520%2520Interpretation%26entry.906535625%3DElisa%2520Sanchez-Bayona%2520and%2520Rodrigo%2520Agerri%26entry.1292438233%3D%2520%2520Metaphors%252C%2520although%2520occasionally%2520unperceived%252C%2520are%2520ubiquitous%2520in%2520our%2520everyday%250Alanguage.%2520Thus%252C%2520it%2520is%2520crucial%2520for%2520Language%2520Models%2520to%2520be%2520able%2520to%2520grasp%2520the%250Aunderlying%2520meaning%2520of%2520this%2520kind%2520of%2520figurative%2520language.%2520In%2520this%2520work%252C%2520we%250Apresent%2520Meta4XNLI%252C%2520a%2520novel%2520parallel%2520dataset%2520for%2520the%2520tasks%2520of%2520metaphor%2520detection%250Aand%2520interpretation%2520that%2520contains%2520metaphor%2520annotations%2520in%2520both%2520Spanish%2520and%250AEnglish.%2520We%2520investigate%2520language%2520models%2527%2520metaphor%2520identification%2520and%250Aunderstanding%2520abilities%2520through%2520a%2520series%2520of%2520monolingual%2520and%2520cross-lingual%250Aexperiments%2520by%2520leveraging%2520our%2520proposed%2520corpus.%2520In%2520order%2520to%2520comprehend%2520how%2520these%250Anon-literal%2520expressions%2520affect%2520models%2527%2520performance%252C%2520we%2520look%2520over%2520the%2520results%250Aand%2520perform%2520an%2520error%2520analysis.%2520Additionally%252C%2520parallel%2520data%2520offers%2520many%250Apotential%2520opportunities%2520to%2520investigate%2520metaphor%2520transferability%2520between%2520these%250Alanguages%2520and%2520the%2520impact%2520of%2520translation%2520on%2520the%2520development%2520of%2520multilingual%250Aannotated%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07053v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta4XNLI%3A%20A%20Crosslingual%20Parallel%20Corpus%20for%20Metaphor%20Detection%20and%0A%20%20Interpretation&entry.906535625=Elisa%20Sanchez-Bayona%20and%20Rodrigo%20Agerri&entry.1292438233=%20%20Metaphors%2C%20although%20occasionally%20unperceived%2C%20are%20ubiquitous%20in%20our%20everyday%0Alanguage.%20Thus%2C%20it%20is%20crucial%20for%20Language%20Models%20to%20be%20able%20to%20grasp%20the%0Aunderlying%20meaning%20of%20this%20kind%20of%20figurative%20language.%20In%20this%20work%2C%20we%0Apresent%20Meta4XNLI%2C%20a%20novel%20parallel%20dataset%20for%20the%20tasks%20of%20metaphor%20detection%0Aand%20interpretation%20that%20contains%20metaphor%20annotations%20in%20both%20Spanish%20and%0AEnglish.%20We%20investigate%20language%20models%27%20metaphor%20identification%20and%0Aunderstanding%20abilities%20through%20a%20series%20of%20monolingual%20and%20cross-lingual%0Aexperiments%20by%20leveraging%20our%20proposed%20corpus.%20In%20order%20to%20comprehend%20how%20these%0Anon-literal%20expressions%20affect%20models%27%20performance%2C%20we%20look%20over%20the%20results%0Aand%20perform%20an%20error%20analysis.%20Additionally%2C%20parallel%20data%20offers%20many%0Apotential%20opportunities%20to%20investigate%20metaphor%20transferability%20between%20these%0Alanguages%20and%20the%20impact%20of%20translation%20on%20the%20development%20of%20multilingual%0Aannotated%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07053v2&entry.124074799=Read"},
{"title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "author": "Hari Iyer and Neel Macwan and Atharva Jitendra Hude and Heejin Jeong and Shenghan Guo", "abstract": "  Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.\n", "link": "http://arxiv.org/abs/2507.14097v1", "date": "2025-07-18", "relevancy": 2.4092, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6342}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5848}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI-Driven%20High-Fidelity%20Human%20Motion%20Simulation&body=Title%3A%20Generative%20AI-Driven%20High-Fidelity%20Human%20Motion%20Simulation%0AAuthor%3A%20Hari%20Iyer%20and%20Neel%20Macwan%20and%20Atharva%20Jitendra%20Hude%20and%20Heejin%20Jeong%20and%20Shenghan%20Guo%0AAbstract%3A%20%20%20Human%20motion%20simulation%20%28HMS%29%20supports%20cost-effective%20evaluation%20of%20worker%0Abehavior%2C%20safety%2C%20and%20productivity%20in%20industrial%20tasks.%20However%2C%20existing%0Amethods%20often%20suffer%20from%20low%20motion%20fidelity.%20This%20study%20introduces%0AGenerative-AI-Enabled%20HMS%20%28G-AI-HMS%29%2C%20which%20integrates%20text-to-text%20and%0Atext-to-motion%20models%20to%20enhance%20simulation%20quality%20for%20physical%20tasks.%0AG-AI-HMS%20tackles%20two%20key%20challenges%3A%20%281%29%20translating%20task%20descriptions%20into%0Amotion-aware%20language%20using%20Large%20Language%20Models%20aligned%20with%20MotionGPT%27s%0Atraining%20vocabulary%2C%20and%20%282%29%20validating%20AI-enhanced%20motions%20against%20real%20human%0Amovements%20using%20computer%20vision.%20Posture%20estimation%20algorithms%20are%20applied%20to%0Areal-time%20videos%20to%20extract%20joint%20landmarks%2C%20and%20motion%20similarity%20metrics%20are%0Aused%20to%20compare%20them%20with%20AI-enhanced%20sequences.%20In%20a%20case%20study%20involving%0Aeight%20tasks%2C%20the%20AI-enhanced%20motions%20showed%20lower%20error%20than%20human%20created%0Adescriptions%20in%20most%20scenarios%2C%20performing%20better%20in%20six%20tasks%20based%20on%20spatial%0Aaccuracy%2C%20four%20tasks%20based%20on%20alignment%20after%20pose%20normalization%2C%20and%20seven%0Atasks%20based%20on%20overall%20temporal%20similarity.%20Statistical%20analysis%20showed%20that%0AAI-enhanced%20prompts%20significantly%20%28p%20%24%3C%24%200.0001%29%20reduced%20joint%20error%20and%0Atemporal%20misalignment%20while%20retaining%20comparable%20posture%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI-Driven%2520High-Fidelity%2520Human%2520Motion%2520Simulation%26entry.906535625%3DHari%2520Iyer%2520and%2520Neel%2520Macwan%2520and%2520Atharva%2520Jitendra%2520Hude%2520and%2520Heejin%2520Jeong%2520and%2520Shenghan%2520Guo%26entry.1292438233%3D%2520%2520Human%2520motion%2520simulation%2520%2528HMS%2529%2520supports%2520cost-effective%2520evaluation%2520of%2520worker%250Abehavior%252C%2520safety%252C%2520and%2520productivity%2520in%2520industrial%2520tasks.%2520However%252C%2520existing%250Amethods%2520often%2520suffer%2520from%2520low%2520motion%2520fidelity.%2520This%2520study%2520introduces%250AGenerative-AI-Enabled%2520HMS%2520%2528G-AI-HMS%2529%252C%2520which%2520integrates%2520text-to-text%2520and%250Atext-to-motion%2520models%2520to%2520enhance%2520simulation%2520quality%2520for%2520physical%2520tasks.%250AG-AI-HMS%2520tackles%2520two%2520key%2520challenges%253A%2520%25281%2529%2520translating%2520task%2520descriptions%2520into%250Amotion-aware%2520language%2520using%2520Large%2520Language%2520Models%2520aligned%2520with%2520MotionGPT%2527s%250Atraining%2520vocabulary%252C%2520and%2520%25282%2529%2520validating%2520AI-enhanced%2520motions%2520against%2520real%2520human%250Amovements%2520using%2520computer%2520vision.%2520Posture%2520estimation%2520algorithms%2520are%2520applied%2520to%250Areal-time%2520videos%2520to%2520extract%2520joint%2520landmarks%252C%2520and%2520motion%2520similarity%2520metrics%2520are%250Aused%2520to%2520compare%2520them%2520with%2520AI-enhanced%2520sequences.%2520In%2520a%2520case%2520study%2520involving%250Aeight%2520tasks%252C%2520the%2520AI-enhanced%2520motions%2520showed%2520lower%2520error%2520than%2520human%2520created%250Adescriptions%2520in%2520most%2520scenarios%252C%2520performing%2520better%2520in%2520six%2520tasks%2520based%2520on%2520spatial%250Aaccuracy%252C%2520four%2520tasks%2520based%2520on%2520alignment%2520after%2520pose%2520normalization%252C%2520and%2520seven%250Atasks%2520based%2520on%2520overall%2520temporal%2520similarity.%2520Statistical%2520analysis%2520showed%2520that%250AAI-enhanced%2520prompts%2520significantly%2520%2528p%2520%2524%253C%2524%25200.0001%2529%2520reduced%2520joint%2520error%2520and%250Atemporal%2520misalignment%2520while%2520retaining%2520comparable%2520posture%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI-Driven%20High-Fidelity%20Human%20Motion%20Simulation&entry.906535625=Hari%20Iyer%20and%20Neel%20Macwan%20and%20Atharva%20Jitendra%20Hude%20and%20Heejin%20Jeong%20and%20Shenghan%20Guo&entry.1292438233=%20%20Human%20motion%20simulation%20%28HMS%29%20supports%20cost-effective%20evaluation%20of%20worker%0Abehavior%2C%20safety%2C%20and%20productivity%20in%20industrial%20tasks.%20However%2C%20existing%0Amethods%20often%20suffer%20from%20low%20motion%20fidelity.%20This%20study%20introduces%0AGenerative-AI-Enabled%20HMS%20%28G-AI-HMS%29%2C%20which%20integrates%20text-to-text%20and%0Atext-to-motion%20models%20to%20enhance%20simulation%20quality%20for%20physical%20tasks.%0AG-AI-HMS%20tackles%20two%20key%20challenges%3A%20%281%29%20translating%20task%20descriptions%20into%0Amotion-aware%20language%20using%20Large%20Language%20Models%20aligned%20with%20MotionGPT%27s%0Atraining%20vocabulary%2C%20and%20%282%29%20validating%20AI-enhanced%20motions%20against%20real%20human%0Amovements%20using%20computer%20vision.%20Posture%20estimation%20algorithms%20are%20applied%20to%0Areal-time%20videos%20to%20extract%20joint%20landmarks%2C%20and%20motion%20similarity%20metrics%20are%0Aused%20to%20compare%20them%20with%20AI-enhanced%20sequences.%20In%20a%20case%20study%20involving%0Aeight%20tasks%2C%20the%20AI-enhanced%20motions%20showed%20lower%20error%20than%20human%20created%0Adescriptions%20in%20most%20scenarios%2C%20performing%20better%20in%20six%20tasks%20based%20on%20spatial%0Aaccuracy%2C%20four%20tasks%20based%20on%20alignment%20after%20pose%20normalization%2C%20and%20seven%0Atasks%20based%20on%20overall%20temporal%20similarity.%20Statistical%20analysis%20showed%20that%0AAI-enhanced%20prompts%20significantly%20%28p%20%24%3C%24%200.0001%29%20reduced%20joint%20error%20and%0Atemporal%20misalignment%20while%20retaining%20comparable%20posture%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14097v1&entry.124074799=Read"},
{"title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from\n  Few-Shot Input Views", "author": "Hsiang-Hui Hung and Huu-Phu Do and Yung-Hui Li and Ching-Chun Huang", "abstract": "  We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.\n", "link": "http://arxiv.org/abs/2507.13929v1", "date": "2025-07-18", "relevancy": 2.4057, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.606}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.606}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TimeNeRF%3A%20Building%20Generalizable%20Neural%20Radiance%20Fields%20across%20Time%20from%0A%20%20Few-Shot%20Input%20Views&body=Title%3A%20TimeNeRF%3A%20Building%20Generalizable%20Neural%20Radiance%20Fields%20across%20Time%20from%0A%20%20Few-Shot%20Input%20Views%0AAuthor%3A%20Hsiang-Hui%20Hung%20and%20Huu-Phu%20Do%20and%20Yung-Hui%20Li%20and%20Ching-Chun%20Huang%0AAbstract%3A%20%20%20We%20present%20TimeNeRF%2C%20a%20generalizable%20neural%20rendering%20approach%20for%20rendering%0Anovel%20views%20at%20arbitrary%20viewpoints%20and%20at%20arbitrary%20times%2C%20even%20with%20few%20input%0Aviews.%20For%20real-world%20applications%2C%20it%20is%20expensive%20to%20collect%20multiple%20views%0Aand%20inefficient%20to%20re-optimize%20for%20unseen%20scenes.%20Moreover%2C%20as%20the%20digital%0Arealm%2C%20particularly%20the%20metaverse%2C%20strives%20for%20increasingly%20immersive%0Aexperiences%2C%20the%20ability%20to%20model%203D%20environments%20that%20naturally%20transition%0Abetween%20day%20and%20night%20becomes%20paramount.%20While%20current%20techniques%20based%20on%0ANeural%20Radiance%20Fields%20%28NeRF%29%20have%20shown%20remarkable%20proficiency%20in%20synthesizing%0Anovel%20views%2C%20the%20exploration%20of%20NeRF%27s%20potential%20for%20temporal%203D%20scene%20modeling%0Aremains%20limited%2C%20with%20no%20dedicated%20datasets%20available%20for%20this%20purpose.%20To%20this%0Aend%2C%20our%20approach%20harnesses%20the%20strengths%20of%20multi-view%20stereo%2C%20neural%20radiance%0Afields%2C%20and%20disentanglement%20strategies%20across%20diverse%20datasets.%20This%20equips%20our%0Amodel%20with%20the%20capability%20for%20generalizability%20in%20a%20few-shot%20setting%2C%20allows%20us%0Ato%20construct%20an%20implicit%20content%20radiance%20field%20for%20scene%20representation%2C%20and%0Afurther%20enables%20the%20building%20of%20neural%20radiance%20fields%20at%20any%20arbitrary%20time.%0AFinally%2C%20we%20synthesize%20novel%20views%20of%20that%20time%20via%20volume%20rendering.%0AExperiments%20show%20that%20TimeNeRF%20can%20render%20novel%20views%20in%20a%20few-shot%20setting%0Awithout%20per-scene%20optimization.%20Most%20notably%2C%20it%20excels%20in%20creating%20realistic%0Anovel%20views%20that%20transition%20smoothly%20across%20different%20times%2C%20adeptly%20capturing%0Aintricate%20natural%20scene%20changes%20from%20dawn%20to%20dusk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeNeRF%253A%2520Building%2520Generalizable%2520Neural%2520Radiance%2520Fields%2520across%2520Time%2520from%250A%2520%2520Few-Shot%2520Input%2520Views%26entry.906535625%3DHsiang-Hui%2520Hung%2520and%2520Huu-Phu%2520Do%2520and%2520Yung-Hui%2520Li%2520and%2520Ching-Chun%2520Huang%26entry.1292438233%3D%2520%2520We%2520present%2520TimeNeRF%252C%2520a%2520generalizable%2520neural%2520rendering%2520approach%2520for%2520rendering%250Anovel%2520views%2520at%2520arbitrary%2520viewpoints%2520and%2520at%2520arbitrary%2520times%252C%2520even%2520with%2520few%2520input%250Aviews.%2520For%2520real-world%2520applications%252C%2520it%2520is%2520expensive%2520to%2520collect%2520multiple%2520views%250Aand%2520inefficient%2520to%2520re-optimize%2520for%2520unseen%2520scenes.%2520Moreover%252C%2520as%2520the%2520digital%250Arealm%252C%2520particularly%2520the%2520metaverse%252C%2520strives%2520for%2520increasingly%2520immersive%250Aexperiences%252C%2520the%2520ability%2520to%2520model%25203D%2520environments%2520that%2520naturally%2520transition%250Abetween%2520day%2520and%2520night%2520becomes%2520paramount.%2520While%2520current%2520techniques%2520based%2520on%250ANeural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520shown%2520remarkable%2520proficiency%2520in%2520synthesizing%250Anovel%2520views%252C%2520the%2520exploration%2520of%2520NeRF%2527s%2520potential%2520for%2520temporal%25203D%2520scene%2520modeling%250Aremains%2520limited%252C%2520with%2520no%2520dedicated%2520datasets%2520available%2520for%2520this%2520purpose.%2520To%2520this%250Aend%252C%2520our%2520approach%2520harnesses%2520the%2520strengths%2520of%2520multi-view%2520stereo%252C%2520neural%2520radiance%250Afields%252C%2520and%2520disentanglement%2520strategies%2520across%2520diverse%2520datasets.%2520This%2520equips%2520our%250Amodel%2520with%2520the%2520capability%2520for%2520generalizability%2520in%2520a%2520few-shot%2520setting%252C%2520allows%2520us%250Ato%2520construct%2520an%2520implicit%2520content%2520radiance%2520field%2520for%2520scene%2520representation%252C%2520and%250Afurther%2520enables%2520the%2520building%2520of%2520neural%2520radiance%2520fields%2520at%2520any%2520arbitrary%2520time.%250AFinally%252C%2520we%2520synthesize%2520novel%2520views%2520of%2520that%2520time%2520via%2520volume%2520rendering.%250AExperiments%2520show%2520that%2520TimeNeRF%2520can%2520render%2520novel%2520views%2520in%2520a%2520few-shot%2520setting%250Awithout%2520per-scene%2520optimization.%2520Most%2520notably%252C%2520it%2520excels%2520in%2520creating%2520realistic%250Anovel%2520views%2520that%2520transition%2520smoothly%2520across%2520different%2520times%252C%2520adeptly%2520capturing%250Aintricate%2520natural%2520scene%2520changes%2520from%2520dawn%2520to%2520dusk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TimeNeRF%3A%20Building%20Generalizable%20Neural%20Radiance%20Fields%20across%20Time%20from%0A%20%20Few-Shot%20Input%20Views&entry.906535625=Hsiang-Hui%20Hung%20and%20Huu-Phu%20Do%20and%20Yung-Hui%20Li%20and%20Ching-Chun%20Huang&entry.1292438233=%20%20We%20present%20TimeNeRF%2C%20a%20generalizable%20neural%20rendering%20approach%20for%20rendering%0Anovel%20views%20at%20arbitrary%20viewpoints%20and%20at%20arbitrary%20times%2C%20even%20with%20few%20input%0Aviews.%20For%20real-world%20applications%2C%20it%20is%20expensive%20to%20collect%20multiple%20views%0Aand%20inefficient%20to%20re-optimize%20for%20unseen%20scenes.%20Moreover%2C%20as%20the%20digital%0Arealm%2C%20particularly%20the%20metaverse%2C%20strives%20for%20increasingly%20immersive%0Aexperiences%2C%20the%20ability%20to%20model%203D%20environments%20that%20naturally%20transition%0Abetween%20day%20and%20night%20becomes%20paramount.%20While%20current%20techniques%20based%20on%0ANeural%20Radiance%20Fields%20%28NeRF%29%20have%20shown%20remarkable%20proficiency%20in%20synthesizing%0Anovel%20views%2C%20the%20exploration%20of%20NeRF%27s%20potential%20for%20temporal%203D%20scene%20modeling%0Aremains%20limited%2C%20with%20no%20dedicated%20datasets%20available%20for%20this%20purpose.%20To%20this%0Aend%2C%20our%20approach%20harnesses%20the%20strengths%20of%20multi-view%20stereo%2C%20neural%20radiance%0Afields%2C%20and%20disentanglement%20strategies%20across%20diverse%20datasets.%20This%20equips%20our%0Amodel%20with%20the%20capability%20for%20generalizability%20in%20a%20few-shot%20setting%2C%20allows%20us%0Ato%20construct%20an%20implicit%20content%20radiance%20field%20for%20scene%20representation%2C%20and%0Afurther%20enables%20the%20building%20of%20neural%20radiance%20fields%20at%20any%20arbitrary%20time.%0AFinally%2C%20we%20synthesize%20novel%20views%20of%20that%20time%20via%20volume%20rendering.%0AExperiments%20show%20that%20TimeNeRF%20can%20render%20novel%20views%20in%20a%20few-shot%20setting%0Awithout%20per-scene%20optimization.%20Most%20notably%2C%20it%20excels%20in%20creating%20realistic%0Anovel%20views%20that%20transition%20smoothly%20across%20different%20times%2C%20adeptly%20capturing%0Aintricate%20natural%20scene%20changes%20from%20dawn%20to%20dusk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13929v1&entry.124074799=Read"},
{"title": "PositionIC: Unified Position and Identity Consistency for Image\n  Customization", "author": "Junjie Hu and Tianyang Han and Kai Ma and Jialin Gao and Hao Dou and Song Yang and Xianhua He and Jianhui Zhang and Junfeng Luo and Xiaoming Wei and Wenqiang Zhang", "abstract": "  Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.\n", "link": "http://arxiv.org/abs/2507.13861v1", "date": "2025-07-18", "relevancy": 2.392, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6221}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5809}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PositionIC%3A%20Unified%20Position%20and%20Identity%20Consistency%20for%20Image%0A%20%20Customization&body=Title%3A%20PositionIC%3A%20Unified%20Position%20and%20Identity%20Consistency%20for%20Image%0A%20%20Customization%0AAuthor%3A%20Junjie%20Hu%20and%20Tianyang%20Han%20and%20Kai%20Ma%20and%20Jialin%20Gao%20and%20Hao%20Dou%20and%20Song%20Yang%20and%20Xianhua%20He%20and%20Jianhui%20Zhang%20and%20Junfeng%20Luo%20and%20Xiaoming%20Wei%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Recent%20subject-driven%20image%20customization%20has%20achieved%20significant%0Aadvancements%20in%20fidelity%2C%20yet%20fine-grained%20entity-level%20spatial%20control%20remains%0Aelusive%2C%20hindering%20the%20broader%20real-world%20application.%20This%20limitation%20is%0Amainly%20attributed%20to%20scalable%20datasets%20that%20bind%20identity%20with%20precise%0Apositional%20cues%20are%20absent.%20To%20this%20end%2C%20we%20introduce%20PositionIC%2C%20a%20unified%0Aframework%20that%20enforces%20position%20and%20identity%20consistency%20for%20multi-subject%0Acustomization.%20We%20construct%20a%20scalable%20synthesis%20pipeline%20that%20employs%20a%0Abidirectional%20generation%20paradigm%20to%20eliminate%20subject%20drift%20and%20maintain%0Asemantic%20coherence.%20On%20top%20of%20these%20data%2C%20we%20design%20a%20lightweight%20positional%0Amodulation%20layer%20that%20decouples%20spatial%20embeddings%20among%20subjects%2C%20enabling%0Aindependent%2C%20accurate%20placement%20while%20preserving%20visual%20fidelity.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20can%20achieve%20precise%20spatial%20control%0Awhile%20maintaining%20high%20consistency%20in%20image%20customization%20task.%20PositionIC%0Apaves%20the%20way%20for%20controllable%2C%20high-fidelity%20image%20customization%20in%0Aopen-world%2C%20multi-entity%20scenarios%20and%20will%20be%20released%20to%20foster%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPositionIC%253A%2520Unified%2520Position%2520and%2520Identity%2520Consistency%2520for%2520Image%250A%2520%2520Customization%26entry.906535625%3DJunjie%2520Hu%2520and%2520Tianyang%2520Han%2520and%2520Kai%2520Ma%2520and%2520Jialin%2520Gao%2520and%2520Hao%2520Dou%2520and%2520Song%2520Yang%2520and%2520Xianhua%2520He%2520and%2520Jianhui%2520Zhang%2520and%2520Junfeng%2520Luo%2520and%2520Xiaoming%2520Wei%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520subject-driven%2520image%2520customization%2520has%2520achieved%2520significant%250Aadvancements%2520in%2520fidelity%252C%2520yet%2520fine-grained%2520entity-level%2520spatial%2520control%2520remains%250Aelusive%252C%2520hindering%2520the%2520broader%2520real-world%2520application.%2520This%2520limitation%2520is%250Amainly%2520attributed%2520to%2520scalable%2520datasets%2520that%2520bind%2520identity%2520with%2520precise%250Apositional%2520cues%2520are%2520absent.%2520To%2520this%2520end%252C%2520we%2520introduce%2520PositionIC%252C%2520a%2520unified%250Aframework%2520that%2520enforces%2520position%2520and%2520identity%2520consistency%2520for%2520multi-subject%250Acustomization.%2520We%2520construct%2520a%2520scalable%2520synthesis%2520pipeline%2520that%2520employs%2520a%250Abidirectional%2520generation%2520paradigm%2520to%2520eliminate%2520subject%2520drift%2520and%2520maintain%250Asemantic%2520coherence.%2520On%2520top%2520of%2520these%2520data%252C%2520we%2520design%2520a%2520lightweight%2520positional%250Amodulation%2520layer%2520that%2520decouples%2520spatial%2520embeddings%2520among%2520subjects%252C%2520enabling%250Aindependent%252C%2520accurate%2520placement%2520while%2520preserving%2520visual%2520fidelity.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%2520precise%2520spatial%2520control%250Awhile%2520maintaining%2520high%2520consistency%2520in%2520image%2520customization%2520task.%2520PositionIC%250Apaves%2520the%2520way%2520for%2520controllable%252C%2520high-fidelity%2520image%2520customization%2520in%250Aopen-world%252C%2520multi-entity%2520scenarios%2520and%2520will%2520be%2520released%2520to%2520foster%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PositionIC%3A%20Unified%20Position%20and%20Identity%20Consistency%20for%20Image%0A%20%20Customization&entry.906535625=Junjie%20Hu%20and%20Tianyang%20Han%20and%20Kai%20Ma%20and%20Jialin%20Gao%20and%20Hao%20Dou%20and%20Song%20Yang%20and%20Xianhua%20He%20and%20Jianhui%20Zhang%20and%20Junfeng%20Luo%20and%20Xiaoming%20Wei%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Recent%20subject-driven%20image%20customization%20has%20achieved%20significant%0Aadvancements%20in%20fidelity%2C%20yet%20fine-grained%20entity-level%20spatial%20control%20remains%0Aelusive%2C%20hindering%20the%20broader%20real-world%20application.%20This%20limitation%20is%0Amainly%20attributed%20to%20scalable%20datasets%20that%20bind%20identity%20with%20precise%0Apositional%20cues%20are%20absent.%20To%20this%20end%2C%20we%20introduce%20PositionIC%2C%20a%20unified%0Aframework%20that%20enforces%20position%20and%20identity%20consistency%20for%20multi-subject%0Acustomization.%20We%20construct%20a%20scalable%20synthesis%20pipeline%20that%20employs%20a%0Abidirectional%20generation%20paradigm%20to%20eliminate%20subject%20drift%20and%20maintain%0Asemantic%20coherence.%20On%20top%20of%20these%20data%2C%20we%20design%20a%20lightweight%20positional%0Amodulation%20layer%20that%20decouples%20spatial%20embeddings%20among%20subjects%2C%20enabling%0Aindependent%2C%20accurate%20placement%20while%20preserving%20visual%20fidelity.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20approach%20can%20achieve%20precise%20spatial%20control%0Awhile%20maintaining%20high%20consistency%20in%20image%20customization%20task.%20PositionIC%0Apaves%20the%20way%20for%20controllable%2C%20high-fidelity%20image%20customization%20in%0Aopen-world%2C%20multi-entity%20scenarios%20and%20will%20be%20released%20to%20foster%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13861v1&entry.124074799=Read"},
{"title": "Hierarchical Multi-Stage Transformer Architecture for Context-Aware\n  Temporal Action Localization", "author": "Hayat Ullah and Arslan Munir and Oliver Nina", "abstract": "  Inspired by the recent success of transformers and multi-stage architectures\nin video recognition and object detection domains. We thoroughly explore the\nrich spatio-temporal properties of transformers within a multi-stage\narchitecture paradigm for the temporal action localization (TAL) task. This\nexploration led to the development of a hierarchical multi-stage transformer\narchitecture called PCL-Former, where each subtask is handled by a dedicated\ntransformer module with a specialized loss function. Specifically, the\nProposal-Former identifies candidate segments in an untrimmed video that may\ncontain actions, the Classification-Former classifies the action categories\nwithin those segments, and the Localization-Former precisely predicts the\ntemporal boundaries (i.e., start and end) of the action instances. To evaluate\nthe performance of our method, we have conducted extensive experiments on three\nchallenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.\nWe also conducted detailed ablation experiments to assess the impact of each\nindividual module of our PCL-Former. The obtained quantitative results validate\nthe effectiveness of the proposed PCL-Former, outperforming state-of-the-art\nTAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS\ndatasets, respectively.\n", "link": "http://arxiv.org/abs/2507.06411v2", "date": "2025-07-18", "relevancy": 2.389, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6226}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5889}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Multi-Stage%20Transformer%20Architecture%20for%20Context-Aware%0A%20%20Temporal%20Action%20Localization&body=Title%3A%20Hierarchical%20Multi-Stage%20Transformer%20Architecture%20for%20Context-Aware%0A%20%20Temporal%20Action%20Localization%0AAuthor%3A%20Hayat%20Ullah%20and%20Arslan%20Munir%20and%20Oliver%20Nina%0AAbstract%3A%20%20%20Inspired%20by%20the%20recent%20success%20of%20transformers%20and%20multi-stage%20architectures%0Ain%20video%20recognition%20and%20object%20detection%20domains.%20We%20thoroughly%20explore%20the%0Arich%20spatio-temporal%20properties%20of%20transformers%20within%20a%20multi-stage%0Aarchitecture%20paradigm%20for%20the%20temporal%20action%20localization%20%28TAL%29%20task.%20This%0Aexploration%20led%20to%20the%20development%20of%20a%20hierarchical%20multi-stage%20transformer%0Aarchitecture%20called%20PCL-Former%2C%20where%20each%20subtask%20is%20handled%20by%20a%20dedicated%0Atransformer%20module%20with%20a%20specialized%20loss%20function.%20Specifically%2C%20the%0AProposal-Former%20identifies%20candidate%20segments%20in%20an%20untrimmed%20video%20that%20may%0Acontain%20actions%2C%20the%20Classification-Former%20classifies%20the%20action%20categories%0Awithin%20those%20segments%2C%20and%20the%20Localization-Former%20precisely%20predicts%20the%0Atemporal%20boundaries%20%28i.e.%2C%20start%20and%20end%29%20of%20the%20action%20instances.%20To%20evaluate%0Athe%20performance%20of%20our%20method%2C%20we%20have%20conducted%20extensive%20experiments%20on%20three%0Achallenging%20benchmark%20datasets%3A%20THUMOS-14%2C%20ActivityNet-1.3%2C%20and%20HACS%20Segments.%0AWe%20also%20conducted%20detailed%20ablation%20experiments%20to%20assess%20the%20impact%20of%20each%0Aindividual%20module%20of%20our%20PCL-Former.%20The%20obtained%20quantitative%20results%20validate%0Athe%20effectiveness%20of%20the%20proposed%20PCL-Former%2C%20outperforming%20state-of-the-art%0ATAL%20approaches%20by%202.8%25%2C%201.2%25%2C%20and%204.8%25%20on%20THUMOS14%2C%20ActivityNet-1.3%2C%20and%20HACS%0Adatasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06411v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Multi-Stage%2520Transformer%2520Architecture%2520for%2520Context-Aware%250A%2520%2520Temporal%2520Action%2520Localization%26entry.906535625%3DHayat%2520Ullah%2520and%2520Arslan%2520Munir%2520and%2520Oliver%2520Nina%26entry.1292438233%3D%2520%2520Inspired%2520by%2520the%2520recent%2520success%2520of%2520transformers%2520and%2520multi-stage%2520architectures%250Ain%2520video%2520recognition%2520and%2520object%2520detection%2520domains.%2520We%2520thoroughly%2520explore%2520the%250Arich%2520spatio-temporal%2520properties%2520of%2520transformers%2520within%2520a%2520multi-stage%250Aarchitecture%2520paradigm%2520for%2520the%2520temporal%2520action%2520localization%2520%2528TAL%2529%2520task.%2520This%250Aexploration%2520led%2520to%2520the%2520development%2520of%2520a%2520hierarchical%2520multi-stage%2520transformer%250Aarchitecture%2520called%2520PCL-Former%252C%2520where%2520each%2520subtask%2520is%2520handled%2520by%2520a%2520dedicated%250Atransformer%2520module%2520with%2520a%2520specialized%2520loss%2520function.%2520Specifically%252C%2520the%250AProposal-Former%2520identifies%2520candidate%2520segments%2520in%2520an%2520untrimmed%2520video%2520that%2520may%250Acontain%2520actions%252C%2520the%2520Classification-Former%2520classifies%2520the%2520action%2520categories%250Awithin%2520those%2520segments%252C%2520and%2520the%2520Localization-Former%2520precisely%2520predicts%2520the%250Atemporal%2520boundaries%2520%2528i.e.%252C%2520start%2520and%2520end%2529%2520of%2520the%2520action%2520instances.%2520To%2520evaluate%250Athe%2520performance%2520of%2520our%2520method%252C%2520we%2520have%2520conducted%2520extensive%2520experiments%2520on%2520three%250Achallenging%2520benchmark%2520datasets%253A%2520THUMOS-14%252C%2520ActivityNet-1.3%252C%2520and%2520HACS%2520Segments.%250AWe%2520also%2520conducted%2520detailed%2520ablation%2520experiments%2520to%2520assess%2520the%2520impact%2520of%2520each%250Aindividual%2520module%2520of%2520our%2520PCL-Former.%2520The%2520obtained%2520quantitative%2520results%2520validate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520PCL-Former%252C%2520outperforming%2520state-of-the-art%250ATAL%2520approaches%2520by%25202.8%2525%252C%25201.2%2525%252C%2520and%25204.8%2525%2520on%2520THUMOS14%252C%2520ActivityNet-1.3%252C%2520and%2520HACS%250Adatasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06411v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Multi-Stage%20Transformer%20Architecture%20for%20Context-Aware%0A%20%20Temporal%20Action%20Localization&entry.906535625=Hayat%20Ullah%20and%20Arslan%20Munir%20and%20Oliver%20Nina&entry.1292438233=%20%20Inspired%20by%20the%20recent%20success%20of%20transformers%20and%20multi-stage%20architectures%0Ain%20video%20recognition%20and%20object%20detection%20domains.%20We%20thoroughly%20explore%20the%0Arich%20spatio-temporal%20properties%20of%20transformers%20within%20a%20multi-stage%0Aarchitecture%20paradigm%20for%20the%20temporal%20action%20localization%20%28TAL%29%20task.%20This%0Aexploration%20led%20to%20the%20development%20of%20a%20hierarchical%20multi-stage%20transformer%0Aarchitecture%20called%20PCL-Former%2C%20where%20each%20subtask%20is%20handled%20by%20a%20dedicated%0Atransformer%20module%20with%20a%20specialized%20loss%20function.%20Specifically%2C%20the%0AProposal-Former%20identifies%20candidate%20segments%20in%20an%20untrimmed%20video%20that%20may%0Acontain%20actions%2C%20the%20Classification-Former%20classifies%20the%20action%20categories%0Awithin%20those%20segments%2C%20and%20the%20Localization-Former%20precisely%20predicts%20the%0Atemporal%20boundaries%20%28i.e.%2C%20start%20and%20end%29%20of%20the%20action%20instances.%20To%20evaluate%0Athe%20performance%20of%20our%20method%2C%20we%20have%20conducted%20extensive%20experiments%20on%20three%0Achallenging%20benchmark%20datasets%3A%20THUMOS-14%2C%20ActivityNet-1.3%2C%20and%20HACS%20Segments.%0AWe%20also%20conducted%20detailed%20ablation%20experiments%20to%20assess%20the%20impact%20of%20each%0Aindividual%20module%20of%20our%20PCL-Former.%20The%20obtained%20quantitative%20results%20validate%0Athe%20effectiveness%20of%20the%20proposed%20PCL-Former%2C%20outperforming%20state-of-the-art%0ATAL%20approaches%20by%202.8%25%2C%201.2%25%2C%20and%204.8%25%20on%20THUMOS14%2C%20ActivityNet-1.3%2C%20and%20HACS%0Adatasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06411v2&entry.124074799=Read"},
{"title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography", "author": "Shravan Venkatraman and Pavan Kumar S and Rakesh Raj Madavan and Chandrakala S", "abstract": "  Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL\n", "link": "http://arxiv.org/abs/2507.14102v1", "date": "2025-07-18", "relevancy": 2.3572, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6305}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6163}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UGPL%3A%20Uncertainty-Guided%20Progressive%20Learning%20for%20Evidence-Based%0A%20%20Classification%20in%20Computed%20Tomography&body=Title%3A%20UGPL%3A%20Uncertainty-Guided%20Progressive%20Learning%20for%20Evidence-Based%0A%20%20Classification%20in%20Computed%20Tomography%0AAuthor%3A%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Rakesh%20Raj%20Madavan%20and%20Chandrakala%20S%0AAbstract%3A%20%20%20Accurate%20classification%20of%20computed%20tomography%20%28CT%29%20images%20is%20essential%20for%0Adiagnosis%20and%20treatment%20planning%2C%20but%20existing%20methods%20often%20struggle%20with%20the%0Asubtle%20and%20spatially%20diverse%20nature%20of%20pathological%20features.%20Current%0Aapproaches%20typically%20process%20images%20uniformly%2C%20limiting%20their%20ability%20to%20detect%0Alocalized%20abnormalities%20that%20require%20focused%20analysis.%20We%20introduce%20UGPL%2C%20an%0Auncertainty-guided%20progressive%20learning%20framework%20that%20performs%20a%0Aglobal-to-local%20analysis%20by%20first%20identifying%20regions%20of%20diagnostic%20ambiguity%0Aand%20then%20conducting%20detailed%20examination%20of%20these%20critical%20areas.%20Our%20approach%0Aemploys%20evidential%20deep%20learning%20to%20quantify%20predictive%20uncertainty%2C%20guiding%0Athe%20extraction%20of%20informative%20patches%20through%20a%20non-maximum%20suppression%0Amechanism%20that%20maintains%20spatial%20diversity.%20This%20progressive%20refinement%0Astrategy%2C%20combined%20with%20an%20adaptive%20fusion%20mechanism%2C%20enables%20UGPL%20to%20integrate%0Aboth%20contextual%20information%20and%20fine-grained%20details.%20Experiments%20across%20three%0ACT%20datasets%20demonstrate%20that%20UGPL%20consistently%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20improvements%20of%203.29%25%2C%202.46%25%2C%20and%208.08%25%20in%20accuracy%20for%0Akidney%20abnormality%2C%20lung%20cancer%2C%20and%20COVID-19%20detection%2C%20respectively.%20Our%0Aanalysis%20shows%20that%20the%20uncertainty-guided%20component%20provides%20substantial%0Abenefits%2C%20with%20performance%20dramatically%20increasing%20when%20the%20full%20progressive%0Alearning%20pipeline%20is%20implemented.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/shravan-18/UGPL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUGPL%253A%2520Uncertainty-Guided%2520Progressive%2520Learning%2520for%2520Evidence-Based%250A%2520%2520Classification%2520in%2520Computed%2520Tomography%26entry.906535625%3DShravan%2520Venkatraman%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Rakesh%2520Raj%2520Madavan%2520and%2520Chandrakala%2520S%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520computed%2520tomography%2520%2528CT%2529%2520images%2520is%2520essential%2520for%250Adiagnosis%2520and%2520treatment%2520planning%252C%2520but%2520existing%2520methods%2520often%2520struggle%2520with%2520the%250Asubtle%2520and%2520spatially%2520diverse%2520nature%2520of%2520pathological%2520features.%2520Current%250Aapproaches%2520typically%2520process%2520images%2520uniformly%252C%2520limiting%2520their%2520ability%2520to%2520detect%250Alocalized%2520abnormalities%2520that%2520require%2520focused%2520analysis.%2520We%2520introduce%2520UGPL%252C%2520an%250Auncertainty-guided%2520progressive%2520learning%2520framework%2520that%2520performs%2520a%250Aglobal-to-local%2520analysis%2520by%2520first%2520identifying%2520regions%2520of%2520diagnostic%2520ambiguity%250Aand%2520then%2520conducting%2520detailed%2520examination%2520of%2520these%2520critical%2520areas.%2520Our%2520approach%250Aemploys%2520evidential%2520deep%2520learning%2520to%2520quantify%2520predictive%2520uncertainty%252C%2520guiding%250Athe%2520extraction%2520of%2520informative%2520patches%2520through%2520a%2520non-maximum%2520suppression%250Amechanism%2520that%2520maintains%2520spatial%2520diversity.%2520This%2520progressive%2520refinement%250Astrategy%252C%2520combined%2520with%2520an%2520adaptive%2520fusion%2520mechanism%252C%2520enables%2520UGPL%2520to%2520integrate%250Aboth%2520contextual%2520information%2520and%2520fine-grained%2520details.%2520Experiments%2520across%2520three%250ACT%2520datasets%2520demonstrate%2520that%2520UGPL%2520consistently%2520outperforms%2520state-of-the-art%250Amethods%252C%2520achieving%2520improvements%2520of%25203.29%2525%252C%25202.46%2525%252C%2520and%25208.08%2525%2520in%2520accuracy%2520for%250Akidney%2520abnormality%252C%2520lung%2520cancer%252C%2520and%2520COVID-19%2520detection%252C%2520respectively.%2520Our%250Aanalysis%2520shows%2520that%2520the%2520uncertainty-guided%2520component%2520provides%2520substantial%250Abenefits%252C%2520with%2520performance%2520dramatically%2520increasing%2520when%2520the%2520full%2520progressive%250Alearning%2520pipeline%2520is%2520implemented.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/shravan-18/UGPL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UGPL%3A%20Uncertainty-Guided%20Progressive%20Learning%20for%20Evidence-Based%0A%20%20Classification%20in%20Computed%20Tomography&entry.906535625=Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Rakesh%20Raj%20Madavan%20and%20Chandrakala%20S&entry.1292438233=%20%20Accurate%20classification%20of%20computed%20tomography%20%28CT%29%20images%20is%20essential%20for%0Adiagnosis%20and%20treatment%20planning%2C%20but%20existing%20methods%20often%20struggle%20with%20the%0Asubtle%20and%20spatially%20diverse%20nature%20of%20pathological%20features.%20Current%0Aapproaches%20typically%20process%20images%20uniformly%2C%20limiting%20their%20ability%20to%20detect%0Alocalized%20abnormalities%20that%20require%20focused%20analysis.%20We%20introduce%20UGPL%2C%20an%0Auncertainty-guided%20progressive%20learning%20framework%20that%20performs%20a%0Aglobal-to-local%20analysis%20by%20first%20identifying%20regions%20of%20diagnostic%20ambiguity%0Aand%20then%20conducting%20detailed%20examination%20of%20these%20critical%20areas.%20Our%20approach%0Aemploys%20evidential%20deep%20learning%20to%20quantify%20predictive%20uncertainty%2C%20guiding%0Athe%20extraction%20of%20informative%20patches%20through%20a%20non-maximum%20suppression%0Amechanism%20that%20maintains%20spatial%20diversity.%20This%20progressive%20refinement%0Astrategy%2C%20combined%20with%20an%20adaptive%20fusion%20mechanism%2C%20enables%20UGPL%20to%20integrate%0Aboth%20contextual%20information%20and%20fine-grained%20details.%20Experiments%20across%20three%0ACT%20datasets%20demonstrate%20that%20UGPL%20consistently%20outperforms%20state-of-the-art%0Amethods%2C%20achieving%20improvements%20of%203.29%25%2C%202.46%25%2C%20and%208.08%25%20in%20accuracy%20for%0Akidney%20abnormality%2C%20lung%20cancer%2C%20and%20COVID-19%20detection%2C%20respectively.%20Our%0Aanalysis%20shows%20that%20the%20uncertainty-guided%20component%20provides%20substantial%0Abenefits%2C%20with%20performance%20dramatically%20increasing%20when%20the%20full%20progressive%0Alearning%20pipeline%20is%20implemented.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/shravan-18/UGPL%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14102v1&entry.124074799=Read"},
{"title": "A Survey of Dimension Estimation Methods", "author": "James A. D. Binnie and Pawe\u0142 D\u0142otko and John Harvey and Jakub Malinowski and Ka Man Yim", "abstract": "  It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.\n", "link": "http://arxiv.org/abs/2507.13887v1", "date": "2025-07-18", "relevancy": 2.3419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4742}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Dimension%20Estimation%20Methods&body=Title%3A%20A%20Survey%20of%20Dimension%20Estimation%20Methods%0AAuthor%3A%20James%20A.%20D.%20Binnie%20and%20Pawe%C5%82%20D%C5%82otko%20and%20John%20Harvey%20and%20Jakub%20Malinowski%20and%20Ka%20Man%20Yim%0AAbstract%3A%20%20%20It%20is%20a%20standard%20assumption%20that%20datasets%20in%20high%20dimension%20have%20an%20internal%0Astructure%20which%20means%20that%20they%20in%20fact%20lie%20on%2C%20or%20near%2C%20subsets%20of%20a%20lower%0Adimension.%20In%20many%20instances%20it%20is%20important%20to%20understand%20the%20real%20dimension%0Aof%20the%20data%2C%20hence%20the%20complexity%20of%20the%20dataset%20at%20hand.%20A%20great%20variety%20of%0Adimension%20estimators%20have%20been%20developed%20to%20find%20the%20intrinsic%20dimension%20of%20the%0Adata%20but%20there%20is%20little%20guidance%20on%20how%20to%20reliably%20use%20these%20estimators.%0A%20%20This%20survey%20reviews%20a%20wide%20range%20of%20dimension%20estimation%20methods%2C%0Acategorising%20them%20by%20the%20geometric%20information%20they%20exploit%3A%20tangential%0Aestimators%20which%20detect%20a%20local%20affine%20structure%3B%20parametric%20estimators%20which%0Arely%20on%20dimension-dependent%20probability%20distributions%3B%20and%20estimators%20which%20use%0Atopological%20or%20metric%20invariants.%0A%20%20The%20paper%20evaluates%20the%20performance%20of%20these%20methods%2C%20as%20well%20as%0Ainvestigating%20varying%20responses%20to%20curvature%20and%20noise.%20Key%20issues%20addressed%0Ainclude%20robustness%20to%20hyperparameter%20selection%2C%20sample%20size%20requirements%2C%0Aaccuracy%20in%20high%20dimensions%2C%20precision%2C%20and%20performance%20on%20non-linear%0Ageometries.%20In%20identifying%20the%20best%20hyperparameters%20for%20benchmark%20datasets%2C%0Aoverfitting%20is%20frequent%2C%20indicating%20that%20many%20estimators%20may%20not%20generalise%0Awell%20beyond%20the%20datasets%20on%20which%20they%20have%20been%20tested.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Dimension%2520Estimation%2520Methods%26entry.906535625%3DJames%2520A.%2520D.%2520Binnie%2520and%2520Pawe%25C5%2582%2520D%25C5%2582otko%2520and%2520John%2520Harvey%2520and%2520Jakub%2520Malinowski%2520and%2520Ka%2520Man%2520Yim%26entry.1292438233%3D%2520%2520It%2520is%2520a%2520standard%2520assumption%2520that%2520datasets%2520in%2520high%2520dimension%2520have%2520an%2520internal%250Astructure%2520which%2520means%2520that%2520they%2520in%2520fact%2520lie%2520on%252C%2520or%2520near%252C%2520subsets%2520of%2520a%2520lower%250Adimension.%2520In%2520many%2520instances%2520it%2520is%2520important%2520to%2520understand%2520the%2520real%2520dimension%250Aof%2520the%2520data%252C%2520hence%2520the%2520complexity%2520of%2520the%2520dataset%2520at%2520hand.%2520A%2520great%2520variety%2520of%250Adimension%2520estimators%2520have%2520been%2520developed%2520to%2520find%2520the%2520intrinsic%2520dimension%2520of%2520the%250Adata%2520but%2520there%2520is%2520little%2520guidance%2520on%2520how%2520to%2520reliably%2520use%2520these%2520estimators.%250A%2520%2520This%2520survey%2520reviews%2520a%2520wide%2520range%2520of%2520dimension%2520estimation%2520methods%252C%250Acategorising%2520them%2520by%2520the%2520geometric%2520information%2520they%2520exploit%253A%2520tangential%250Aestimators%2520which%2520detect%2520a%2520local%2520affine%2520structure%253B%2520parametric%2520estimators%2520which%250Arely%2520on%2520dimension-dependent%2520probability%2520distributions%253B%2520and%2520estimators%2520which%2520use%250Atopological%2520or%2520metric%2520invariants.%250A%2520%2520The%2520paper%2520evaluates%2520the%2520performance%2520of%2520these%2520methods%252C%2520as%2520well%2520as%250Ainvestigating%2520varying%2520responses%2520to%2520curvature%2520and%2520noise.%2520Key%2520issues%2520addressed%250Ainclude%2520robustness%2520to%2520hyperparameter%2520selection%252C%2520sample%2520size%2520requirements%252C%250Aaccuracy%2520in%2520high%2520dimensions%252C%2520precision%252C%2520and%2520performance%2520on%2520non-linear%250Ageometries.%2520In%2520identifying%2520the%2520best%2520hyperparameters%2520for%2520benchmark%2520datasets%252C%250Aoverfitting%2520is%2520frequent%252C%2520indicating%2520that%2520many%2520estimators%2520may%2520not%2520generalise%250Awell%2520beyond%2520the%2520datasets%2520on%2520which%2520they%2520have%2520been%2520tested.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Dimension%20Estimation%20Methods&entry.906535625=James%20A.%20D.%20Binnie%20and%20Pawe%C5%82%20D%C5%82otko%20and%20John%20Harvey%20and%20Jakub%20Malinowski%20and%20Ka%20Man%20Yim&entry.1292438233=%20%20It%20is%20a%20standard%20assumption%20that%20datasets%20in%20high%20dimension%20have%20an%20internal%0Astructure%20which%20means%20that%20they%20in%20fact%20lie%20on%2C%20or%20near%2C%20subsets%20of%20a%20lower%0Adimension.%20In%20many%20instances%20it%20is%20important%20to%20understand%20the%20real%20dimension%0Aof%20the%20data%2C%20hence%20the%20complexity%20of%20the%20dataset%20at%20hand.%20A%20great%20variety%20of%0Adimension%20estimators%20have%20been%20developed%20to%20find%20the%20intrinsic%20dimension%20of%20the%0Adata%20but%20there%20is%20little%20guidance%20on%20how%20to%20reliably%20use%20these%20estimators.%0A%20%20This%20survey%20reviews%20a%20wide%20range%20of%20dimension%20estimation%20methods%2C%0Acategorising%20them%20by%20the%20geometric%20information%20they%20exploit%3A%20tangential%0Aestimators%20which%20detect%20a%20local%20affine%20structure%3B%20parametric%20estimators%20which%0Arely%20on%20dimension-dependent%20probability%20distributions%3B%20and%20estimators%20which%20use%0Atopological%20or%20metric%20invariants.%0A%20%20The%20paper%20evaluates%20the%20performance%20of%20these%20methods%2C%20as%20well%20as%0Ainvestigating%20varying%20responses%20to%20curvature%20and%20noise.%20Key%20issues%20addressed%0Ainclude%20robustness%20to%20hyperparameter%20selection%2C%20sample%20size%20requirements%2C%0Aaccuracy%20in%20high%20dimensions%2C%20precision%2C%20and%20performance%20on%20non-linear%0Ageometries.%20In%20identifying%20the%20best%20hyperparameters%20for%20benchmark%20datasets%2C%0Aoverfitting%20is%20frequent%2C%20indicating%20that%20many%20estimators%20may%20not%20generalise%0Awell%20beyond%20the%20datasets%20on%20which%20they%20have%20been%20tested.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13887v1&entry.124074799=Read"},
{"title": "Consistency of Responses and Continuations Generated by Large Language\n  Models on Social Media", "author": "Wenlu Fan and Yuqi Zhu and Chenyang Wang and Bin Wang and Wentao Xu", "abstract": "  Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.\n", "link": "http://arxiv.org/abs/2501.08102v3", "date": "2025-07-18", "relevancy": 2.3294, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media&body=Title%3A%20Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media%0AAuthor%3A%20Wenlu%20Fan%20and%20Yuqi%20Zhu%20and%20Chenyang%20Wang%20and%20Bin%20Wang%20and%20Wentao%20Xu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20text%0Ageneration%2C%20yet%20their%20emotional%20consistency%20and%20semantic%20coherence%20in%20social%0Amedia%20contexts%20remain%20insufficiently%20understood.%20This%20study%20investigates%20how%0ALLMs%20handle%20emotional%20content%20and%20maintain%20semantic%20relationships%20through%0Acontinuation%20and%20response%20tasks%20using%20two%20open-source%20models%3A%20Gemma%20and%20Llama.%0ABy%20analyzing%20climate%20change%20discussions%20from%20Twitter%20and%20Reddit%2C%20we%20examine%0Aemotional%20transitions%2C%20intensity%20patterns%2C%20and%20semantic%20similarity%20between%0Ahuman-authored%20and%20LLM-generated%20content.%20Our%20findings%20reveal%20that%20while%20both%0Amodels%20maintain%20high%20semantic%20coherence%2C%20they%20exhibit%20distinct%20emotional%0Apatterns%3A%20Gemma%20shows%20a%20tendency%20toward%20negative%20emotion%20amplification%2C%0Aparticularly%20anger%2C%20while%20maintaining%20certain%20positive%20emotions%20like%20optimism.%0ALlama%20demonstrates%20superior%20emotional%20preservation%20across%20a%20broader%20spectrum%20of%0Aaffects.%20Both%20models%20systematically%20generate%20responses%20with%20attenuated%0Aemotional%20intensity%20compared%20to%20human-authored%20content%20and%20show%20a%20bias%20toward%0Apositive%20emotions%20in%20response%20tasks.%20Additionally%2C%20both%20models%20maintain%20strong%0Asemantic%20similarity%20with%20original%20texts%2C%20though%20performance%20varies%20between%0Acontinuation%20and%20response%20tasks.%20These%20findings%20provide%20insights%20into%20LLMs%27%0Aemotional%20and%20semantic%20processing%20capabilities%2C%20with%20implications%20for%20their%0Adeployment%20in%20social%20media%20contexts%20and%20human-AI%20interaction%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08102v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency%2520of%2520Responses%2520and%2520Continuations%2520Generated%2520by%2520Large%2520Language%250A%2520%2520Models%2520on%2520Social%2520Media%26entry.906535625%3DWenlu%2520Fan%2520and%2520Yuqi%2520Zhu%2520and%2520Chenyang%2520Wang%2520and%2520Bin%2520Wang%2520and%2520Wentao%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520remarkable%2520capabilities%2520in%2520text%250Ageneration%252C%2520yet%2520their%2520emotional%2520consistency%2520and%2520semantic%2520coherence%2520in%2520social%250Amedia%2520contexts%2520remain%2520insufficiently%2520understood.%2520This%2520study%2520investigates%2520how%250ALLMs%2520handle%2520emotional%2520content%2520and%2520maintain%2520semantic%2520relationships%2520through%250Acontinuation%2520and%2520response%2520tasks%2520using%2520two%2520open-source%2520models%253A%2520Gemma%2520and%2520Llama.%250ABy%2520analyzing%2520climate%2520change%2520discussions%2520from%2520Twitter%2520and%2520Reddit%252C%2520we%2520examine%250Aemotional%2520transitions%252C%2520intensity%2520patterns%252C%2520and%2520semantic%2520similarity%2520between%250Ahuman-authored%2520and%2520LLM-generated%2520content.%2520Our%2520findings%2520reveal%2520that%2520while%2520both%250Amodels%2520maintain%2520high%2520semantic%2520coherence%252C%2520they%2520exhibit%2520distinct%2520emotional%250Apatterns%253A%2520Gemma%2520shows%2520a%2520tendency%2520toward%2520negative%2520emotion%2520amplification%252C%250Aparticularly%2520anger%252C%2520while%2520maintaining%2520certain%2520positive%2520emotions%2520like%2520optimism.%250ALlama%2520demonstrates%2520superior%2520emotional%2520preservation%2520across%2520a%2520broader%2520spectrum%2520of%250Aaffects.%2520Both%2520models%2520systematically%2520generate%2520responses%2520with%2520attenuated%250Aemotional%2520intensity%2520compared%2520to%2520human-authored%2520content%2520and%2520show%2520a%2520bias%2520toward%250Apositive%2520emotions%2520in%2520response%2520tasks.%2520Additionally%252C%2520both%2520models%2520maintain%2520strong%250Asemantic%2520similarity%2520with%2520original%2520texts%252C%2520though%2520performance%2520varies%2520between%250Acontinuation%2520and%2520response%2520tasks.%2520These%2520findings%2520provide%2520insights%2520into%2520LLMs%2527%250Aemotional%2520and%2520semantic%2520processing%2520capabilities%252C%2520with%2520implications%2520for%2520their%250Adeployment%2520in%2520social%2520media%2520contexts%2520and%2520human-AI%2520interaction%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08102v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency%20of%20Responses%20and%20Continuations%20Generated%20by%20Large%20Language%0A%20%20Models%20on%20Social%20Media&entry.906535625=Wenlu%20Fan%20and%20Yuqi%20Zhu%20and%20Chenyang%20Wang%20and%20Bin%20Wang%20and%20Wentao%20Xu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20remarkable%20capabilities%20in%20text%0Ageneration%2C%20yet%20their%20emotional%20consistency%20and%20semantic%20coherence%20in%20social%0Amedia%20contexts%20remain%20insufficiently%20understood.%20This%20study%20investigates%20how%0ALLMs%20handle%20emotional%20content%20and%20maintain%20semantic%20relationships%20through%0Acontinuation%20and%20response%20tasks%20using%20two%20open-source%20models%3A%20Gemma%20and%20Llama.%0ABy%20analyzing%20climate%20change%20discussions%20from%20Twitter%20and%20Reddit%2C%20we%20examine%0Aemotional%20transitions%2C%20intensity%20patterns%2C%20and%20semantic%20similarity%20between%0Ahuman-authored%20and%20LLM-generated%20content.%20Our%20findings%20reveal%20that%20while%20both%0Amodels%20maintain%20high%20semantic%20coherence%2C%20they%20exhibit%20distinct%20emotional%0Apatterns%3A%20Gemma%20shows%20a%20tendency%20toward%20negative%20emotion%20amplification%2C%0Aparticularly%20anger%2C%20while%20maintaining%20certain%20positive%20emotions%20like%20optimism.%0ALlama%20demonstrates%20superior%20emotional%20preservation%20across%20a%20broader%20spectrum%20of%0Aaffects.%20Both%20models%20systematically%20generate%20responses%20with%20attenuated%0Aemotional%20intensity%20compared%20to%20human-authored%20content%20and%20show%20a%20bias%20toward%0Apositive%20emotions%20in%20response%20tasks.%20Additionally%2C%20both%20models%20maintain%20strong%0Asemantic%20similarity%20with%20original%20texts%2C%20though%20performance%20varies%20between%0Acontinuation%20and%20response%20tasks.%20These%20findings%20provide%20insights%20into%20LLMs%27%0Aemotional%20and%20semantic%20processing%20capabilities%2C%20with%20implications%20for%20their%0Adeployment%20in%20social%20media%20contexts%20and%20human-AI%20interaction%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08102v3&entry.124074799=Read"},
{"title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with\n  Adaptive Low-Rank Compensation", "author": "Weiqi Yang and Xu Zhou and Jingfu Guan and Hao Du and Tianyu Bai", "abstract": "  Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments.\n", "link": "http://arxiv.org/abs/2507.13803v1", "date": "2025-07-18", "relevancy": 2.3159, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5916}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAM-MAMBA%3A%20Holistic%20Feature%20Alignment%20for%20Wireless%20Perception%20with%0A%20%20Adaptive%20Low-Rank%20Compensation&body=Title%3A%20GRAM-MAMBA%3A%20Holistic%20Feature%20Alignment%20for%20Wireless%20Perception%20with%0A%20%20Adaptive%20Low-Rank%20Compensation%0AAuthor%3A%20Weiqi%20Yang%20and%20Xu%20Zhou%20and%20Jingfu%20Guan%20and%20Hao%20Du%20and%20Tianyu%20Bai%0AAbstract%3A%20%20%20Multi-modal%20fusion%20is%20crucial%20for%20Internet%20of%20Things%20%28IoT%29%20perception%2C%20widely%0Adeployed%20in%20smart%20homes%2C%20intelligent%20transport%2C%20industrial%20automation%2C%20and%0Ahealthcare.%20However%2C%20existing%20systems%20often%20face%20challenges%3A%20high%20model%0Acomplexity%20hinders%20deployment%20in%20resource-constrained%20environments%2C%0Aunidirectional%20modal%20alignment%20neglects%20inter-modal%20relationships%2C%20and%0Arobustness%20suffers%20when%20sensor%20data%20is%20missing.%20These%20issues%20impede%20efficient%0Aand%20robust%20multimodal%20perception%20in%20real-world%20IoT%20settings.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20GRAM-MAMBA.%20This%20framework%20utilizes%20the%0Alinear-complexity%20Mamba%20model%20for%20efficient%20sensor%20time-series%20processing%2C%0Acombined%20with%20an%20optimized%20GRAM%20matrix%20strategy%20for%20pairwise%20alignment%20among%0Amodalities%2C%20addressing%20the%20shortcomings%20of%20traditional%20single-modality%0Aalignment.%20Inspired%20by%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20we%20introduce%20an%20adaptive%0Alow-rank%20layer%20compensation%20strategy%20to%20handle%20missing%20modalities%0Apost-training.%20This%20strategy%20freezes%20the%20pre-trained%20model%20core%20and%20irrelevant%0Aadaptive%20layers%2C%20fine-tuning%20only%20those%20related%20to%20available%20modalities%20and%20the%0Afusion%20process.%20Extensive%20experiments%20validate%20GRAM-MAMBA%27s%20effectiveness.%20On%0Athe%20SPAWC2021%20indoor%20positioning%20dataset%2C%20the%20pre-trained%20model%20shows%20lower%0Aerror%20than%20baselines%3B%20adapting%20to%20missing%20modalities%20yields%20a%2024.5%25%20performance%0Aboost%20by%20training%20less%20than%200.2%25%20of%20parameters.%20On%20the%20USC-HAD%20human%20activity%0Arecognition%20dataset%2C%20it%20achieves%2093.55%25%20F1%20and%2093.81%25%20Overall%20Accuracy%20%28OA%29%2C%0Aoutperforming%20prior%20work%3B%20the%20update%20strategy%20increases%20F1%20by%2023%25%20while%0Atraining%20less%20than%200.3%25%20of%20parameters.%20These%20results%20highlight%20GRAM-MAMBA%27s%0Apotential%20for%20achieving%20efficient%20and%20robust%20multimodal%20perception%20in%0Aresource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAM-MAMBA%253A%2520Holistic%2520Feature%2520Alignment%2520for%2520Wireless%2520Perception%2520with%250A%2520%2520Adaptive%2520Low-Rank%2520Compensation%26entry.906535625%3DWeiqi%2520Yang%2520and%2520Xu%2520Zhou%2520and%2520Jingfu%2520Guan%2520and%2520Hao%2520Du%2520and%2520Tianyu%2520Bai%26entry.1292438233%3D%2520%2520Multi-modal%2520fusion%2520is%2520crucial%2520for%2520Internet%2520of%2520Things%2520%2528IoT%2529%2520perception%252C%2520widely%250Adeployed%2520in%2520smart%2520homes%252C%2520intelligent%2520transport%252C%2520industrial%2520automation%252C%2520and%250Ahealthcare.%2520However%252C%2520existing%2520systems%2520often%2520face%2520challenges%253A%2520high%2520model%250Acomplexity%2520hinders%2520deployment%2520in%2520resource-constrained%2520environments%252C%250Aunidirectional%2520modal%2520alignment%2520neglects%2520inter-modal%2520relationships%252C%2520and%250Arobustness%2520suffers%2520when%2520sensor%2520data%2520is%2520missing.%2520These%2520issues%2520impede%2520efficient%250Aand%2520robust%2520multimodal%2520perception%2520in%2520real-world%2520IoT%2520settings.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520GRAM-MAMBA.%2520This%2520framework%2520utilizes%2520the%250Alinear-complexity%2520Mamba%2520model%2520for%2520efficient%2520sensor%2520time-series%2520processing%252C%250Acombined%2520with%2520an%2520optimized%2520GRAM%2520matrix%2520strategy%2520for%2520pairwise%2520alignment%2520among%250Amodalities%252C%2520addressing%2520the%2520shortcomings%2520of%2520traditional%2520single-modality%250Aalignment.%2520Inspired%2520by%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520we%2520introduce%2520an%2520adaptive%250Alow-rank%2520layer%2520compensation%2520strategy%2520to%2520handle%2520missing%2520modalities%250Apost-training.%2520This%2520strategy%2520freezes%2520the%2520pre-trained%2520model%2520core%2520and%2520irrelevant%250Aadaptive%2520layers%252C%2520fine-tuning%2520only%2520those%2520related%2520to%2520available%2520modalities%2520and%2520the%250Afusion%2520process.%2520Extensive%2520experiments%2520validate%2520GRAM-MAMBA%2527s%2520effectiveness.%2520On%250Athe%2520SPAWC2021%2520indoor%2520positioning%2520dataset%252C%2520the%2520pre-trained%2520model%2520shows%2520lower%250Aerror%2520than%2520baselines%253B%2520adapting%2520to%2520missing%2520modalities%2520yields%2520a%252024.5%2525%2520performance%250Aboost%2520by%2520training%2520less%2520than%25200.2%2525%2520of%2520parameters.%2520On%2520the%2520USC-HAD%2520human%2520activity%250Arecognition%2520dataset%252C%2520it%2520achieves%252093.55%2525%2520F1%2520and%252093.81%2525%2520Overall%2520Accuracy%2520%2528OA%2529%252C%250Aoutperforming%2520prior%2520work%253B%2520the%2520update%2520strategy%2520increases%2520F1%2520by%252023%2525%2520while%250Atraining%2520less%2520than%25200.3%2525%2520of%2520parameters.%2520These%2520results%2520highlight%2520GRAM-MAMBA%2527s%250Apotential%2520for%2520achieving%2520efficient%2520and%2520robust%2520multimodal%2520perception%2520in%250Aresource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAM-MAMBA%3A%20Holistic%20Feature%20Alignment%20for%20Wireless%20Perception%20with%0A%20%20Adaptive%20Low-Rank%20Compensation&entry.906535625=Weiqi%20Yang%20and%20Xu%20Zhou%20and%20Jingfu%20Guan%20and%20Hao%20Du%20and%20Tianyu%20Bai&entry.1292438233=%20%20Multi-modal%20fusion%20is%20crucial%20for%20Internet%20of%20Things%20%28IoT%29%20perception%2C%20widely%0Adeployed%20in%20smart%20homes%2C%20intelligent%20transport%2C%20industrial%20automation%2C%20and%0Ahealthcare.%20However%2C%20existing%20systems%20often%20face%20challenges%3A%20high%20model%0Acomplexity%20hinders%20deployment%20in%20resource-constrained%20environments%2C%0Aunidirectional%20modal%20alignment%20neglects%20inter-modal%20relationships%2C%20and%0Arobustness%20suffers%20when%20sensor%20data%20is%20missing.%20These%20issues%20impede%20efficient%0Aand%20robust%20multimodal%20perception%20in%20real-world%20IoT%20settings.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20GRAM-MAMBA.%20This%20framework%20utilizes%20the%0Alinear-complexity%20Mamba%20model%20for%20efficient%20sensor%20time-series%20processing%2C%0Acombined%20with%20an%20optimized%20GRAM%20matrix%20strategy%20for%20pairwise%20alignment%20among%0Amodalities%2C%20addressing%20the%20shortcomings%20of%20traditional%20single-modality%0Aalignment.%20Inspired%20by%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20we%20introduce%20an%20adaptive%0Alow-rank%20layer%20compensation%20strategy%20to%20handle%20missing%20modalities%0Apost-training.%20This%20strategy%20freezes%20the%20pre-trained%20model%20core%20and%20irrelevant%0Aadaptive%20layers%2C%20fine-tuning%20only%20those%20related%20to%20available%20modalities%20and%20the%0Afusion%20process.%20Extensive%20experiments%20validate%20GRAM-MAMBA%27s%20effectiveness.%20On%0Athe%20SPAWC2021%20indoor%20positioning%20dataset%2C%20the%20pre-trained%20model%20shows%20lower%0Aerror%20than%20baselines%3B%20adapting%20to%20missing%20modalities%20yields%20a%2024.5%25%20performance%0Aboost%20by%20training%20less%20than%200.2%25%20of%20parameters.%20On%20the%20USC-HAD%20human%20activity%0Arecognition%20dataset%2C%20it%20achieves%2093.55%25%20F1%20and%2093.81%25%20Overall%20Accuracy%20%28OA%29%2C%0Aoutperforming%20prior%20work%3B%20the%20update%20strategy%20increases%20F1%20by%2023%25%20while%0Atraining%20less%20than%200.3%25%20of%20parameters.%20These%20results%20highlight%20GRAM-MAMBA%27s%0Apotential%20for%20achieving%20efficient%20and%20robust%20multimodal%20perception%20in%0Aresource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13803v1&entry.124074799=Read"},
{"title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing", "author": "Yingying Zhang and Lixiang Ru and Kang Wu and Lei Yu and Lei Liang and Yansheng Li and Jingdong Chen", "abstract": "  The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.\n", "link": "http://arxiv.org/abs/2507.13812v1", "date": "2025-07-18", "relevancy": 2.3072, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5776}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkySense%20V2%3A%20A%20Unified%20Foundation%20Model%20for%20Multi-modal%20Remote%20Sensing&body=Title%3A%20SkySense%20V2%3A%20A%20Unified%20Foundation%20Model%20for%20Multi-modal%20Remote%20Sensing%0AAuthor%3A%20Yingying%20Zhang%20and%20Lixiang%20Ru%20and%20Kang%20Wu%20and%20Lei%20Yu%20and%20Lei%20Liang%20and%20Yansheng%20Li%20and%20Jingdong%20Chen%0AAbstract%3A%20%20%20The%20multi-modal%20remote%20sensing%20foundation%20model%20%28MM-RSFM%29%20has%20significantly%0Aadvanced%20various%20Earth%20observation%20tasks%2C%20such%20as%20urban%20planning%2C%20environmental%0Amonitoring%2C%20and%20natural%20disaster%20management.%20However%2C%20most%20existing%20approaches%0Agenerally%20require%20the%20training%20of%20separate%20backbone%20networks%20for%20each%20data%0Amodality%2C%20leading%20to%20redundancy%20and%20inefficient%20parameter%20utilization.%0AMoreover%2C%20prevalent%20pre-training%20methods%20typically%20apply%20self-supervised%0Alearning%20%28SSL%29%20techniques%20from%20natural%20images%20without%20adequately%20accommodating%0Athe%20characteristics%20of%20remote%20sensing%20%28RS%29%20images%2C%20such%20as%20the%20complicated%0Asemantic%20distribution%20within%20a%20single%20RS%20image.%20In%20this%20work%2C%20we%20present%0ASkySense%20V2%2C%20a%20unified%20MM-RSFM%20that%20employs%20a%20single%20transformer%20backbone%20to%0Ahandle%20multiple%20modalities.%20This%20backbone%20is%20pre-trained%20with%20a%20novel%20SSL%0Astrategy%20tailored%20to%20the%20distinct%20traits%20of%20RS%20data.%20In%20particular%2C%20SkySense%20V2%0Aincorporates%20an%20innovative%20adaptive%20patch%20merging%20module%20and%20learnable%20modality%0Aprompt%20tokens%20to%20address%20challenges%20related%20to%20varying%20resolutions%20and%20limited%0Afeature%20diversity%20across%20modalities.%20In%20additional%2C%20we%20incorporate%20the%20mixture%0Aof%20experts%20%28MoE%29%20module%20to%20further%20enhance%20the%20performance%20of%20the%20foundation%0Amodel.%20SkySense%20V2%20demonstrates%20impressive%20generalization%20abilities%20through%20an%0Aextensive%20evaluation%20involving%2016%20datasets%20over%207%20tasks%2C%20outperforming%20SkySense%0Aby%20an%20average%20of%201.8%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkySense%2520V2%253A%2520A%2520Unified%2520Foundation%2520Model%2520for%2520Multi-modal%2520Remote%2520Sensing%26entry.906535625%3DYingying%2520Zhang%2520and%2520Lixiang%2520Ru%2520and%2520Kang%2520Wu%2520and%2520Lei%2520Yu%2520and%2520Lei%2520Liang%2520and%2520Yansheng%2520Li%2520and%2520Jingdong%2520Chen%26entry.1292438233%3D%2520%2520The%2520multi-modal%2520remote%2520sensing%2520foundation%2520model%2520%2528MM-RSFM%2529%2520has%2520significantly%250Aadvanced%2520various%2520Earth%2520observation%2520tasks%252C%2520such%2520as%2520urban%2520planning%252C%2520environmental%250Amonitoring%252C%2520and%2520natural%2520disaster%2520management.%2520However%252C%2520most%2520existing%2520approaches%250Agenerally%2520require%2520the%2520training%2520of%2520separate%2520backbone%2520networks%2520for%2520each%2520data%250Amodality%252C%2520leading%2520to%2520redundancy%2520and%2520inefficient%2520parameter%2520utilization.%250AMoreover%252C%2520prevalent%2520pre-training%2520methods%2520typically%2520apply%2520self-supervised%250Alearning%2520%2528SSL%2529%2520techniques%2520from%2520natural%2520images%2520without%2520adequately%2520accommodating%250Athe%2520characteristics%2520of%2520remote%2520sensing%2520%2528RS%2529%2520images%252C%2520such%2520as%2520the%2520complicated%250Asemantic%2520distribution%2520within%2520a%2520single%2520RS%2520image.%2520In%2520this%2520work%252C%2520we%2520present%250ASkySense%2520V2%252C%2520a%2520unified%2520MM-RSFM%2520that%2520employs%2520a%2520single%2520transformer%2520backbone%2520to%250Ahandle%2520multiple%2520modalities.%2520This%2520backbone%2520is%2520pre-trained%2520with%2520a%2520novel%2520SSL%250Astrategy%2520tailored%2520to%2520the%2520distinct%2520traits%2520of%2520RS%2520data.%2520In%2520particular%252C%2520SkySense%2520V2%250Aincorporates%2520an%2520innovative%2520adaptive%2520patch%2520merging%2520module%2520and%2520learnable%2520modality%250Aprompt%2520tokens%2520to%2520address%2520challenges%2520related%2520to%2520varying%2520resolutions%2520and%2520limited%250Afeature%2520diversity%2520across%2520modalities.%2520In%2520additional%252C%2520we%2520incorporate%2520the%2520mixture%250Aof%2520experts%2520%2528MoE%2529%2520module%2520to%2520further%2520enhance%2520the%2520performance%2520of%2520the%2520foundation%250Amodel.%2520SkySense%2520V2%2520demonstrates%2520impressive%2520generalization%2520abilities%2520through%2520an%250Aextensive%2520evaluation%2520involving%252016%2520datasets%2520over%25207%2520tasks%252C%2520outperforming%2520SkySense%250Aby%2520an%2520average%2520of%25201.8%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkySense%20V2%3A%20A%20Unified%20Foundation%20Model%20for%20Multi-modal%20Remote%20Sensing&entry.906535625=Yingying%20Zhang%20and%20Lixiang%20Ru%20and%20Kang%20Wu%20and%20Lei%20Yu%20and%20Lei%20Liang%20and%20Yansheng%20Li%20and%20Jingdong%20Chen&entry.1292438233=%20%20The%20multi-modal%20remote%20sensing%20foundation%20model%20%28MM-RSFM%29%20has%20significantly%0Aadvanced%20various%20Earth%20observation%20tasks%2C%20such%20as%20urban%20planning%2C%20environmental%0Amonitoring%2C%20and%20natural%20disaster%20management.%20However%2C%20most%20existing%20approaches%0Agenerally%20require%20the%20training%20of%20separate%20backbone%20networks%20for%20each%20data%0Amodality%2C%20leading%20to%20redundancy%20and%20inefficient%20parameter%20utilization.%0AMoreover%2C%20prevalent%20pre-training%20methods%20typically%20apply%20self-supervised%0Alearning%20%28SSL%29%20techniques%20from%20natural%20images%20without%20adequately%20accommodating%0Athe%20characteristics%20of%20remote%20sensing%20%28RS%29%20images%2C%20such%20as%20the%20complicated%0Asemantic%20distribution%20within%20a%20single%20RS%20image.%20In%20this%20work%2C%20we%20present%0ASkySense%20V2%2C%20a%20unified%20MM-RSFM%20that%20employs%20a%20single%20transformer%20backbone%20to%0Ahandle%20multiple%20modalities.%20This%20backbone%20is%20pre-trained%20with%20a%20novel%20SSL%0Astrategy%20tailored%20to%20the%20distinct%20traits%20of%20RS%20data.%20In%20particular%2C%20SkySense%20V2%0Aincorporates%20an%20innovative%20adaptive%20patch%20merging%20module%20and%20learnable%20modality%0Aprompt%20tokens%20to%20address%20challenges%20related%20to%20varying%20resolutions%20and%20limited%0Afeature%20diversity%20across%20modalities.%20In%20additional%2C%20we%20incorporate%20the%20mixture%0Aof%20experts%20%28MoE%29%20module%20to%20further%20enhance%20the%20performance%20of%20the%20foundation%0Amodel.%20SkySense%20V2%20demonstrates%20impressive%20generalization%20abilities%20through%20an%0Aextensive%20evaluation%20involving%2016%20datasets%20over%207%20tasks%2C%20outperforming%20SkySense%0Aby%20an%20average%20of%201.8%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13812v1&entry.124074799=Read"},
{"title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps\n  Using Large Language Models for Bridge Condition Assessment", "author": "Viraj Nishesh Darji and Callie C. Liao and Duoduo Liao", "abstract": "  Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.\n", "link": "http://arxiv.org/abs/2507.14107v1", "date": "2025-07-18", "relevancy": 2.2913, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Interpretation%20of%20Non-Destructive%20Evaluation%20Contour%20Maps%0A%20%20Using%20Large%20Language%20Models%20for%20Bridge%20Condition%20Assessment&body=Title%3A%20Automated%20Interpretation%20of%20Non-Destructive%20Evaluation%20Contour%20Maps%0A%20%20Using%20Large%20Language%20Models%20for%20Bridge%20Condition%20Assessment%0AAuthor%3A%20Viraj%20Nishesh%20Darji%20and%20Callie%20C.%20Liao%20and%20Duoduo%20Liao%0AAbstract%3A%20%20%20Bridge%20maintenance%20and%20safety%20are%20essential%20for%20transportation%20authorities%2C%0Aand%20Non-Destructive%20Evaluation%20%28NDE%29%20techniques%20are%20critical%20to%20assessing%0Astructural%20integrity.%20However%2C%20interpreting%20NDE%20data%20can%20be%20time-consuming%20and%0Arequires%20expertise%2C%20potentially%20delaying%20decision-making.%20Recent%20advancements%0Ain%20Large%20Language%20Models%20%28LLMs%29%20offer%20new%20ways%20to%20automate%20and%20improve%20this%0Aanalysis.%20This%20pilot%20study%20introduces%20a%20holistic%20assessment%20of%20LLM%20capabilities%0Afor%20interpreting%20NDE%20contour%20maps%20and%20demonstrates%20the%20effectiveness%20of%20LLMs%20in%0Aproviding%20detailed%20bridge%20condition%20analyses.%20It%20establishes%20a%20framework%20for%0Aintegrating%20LLMs%20into%20bridge%20inspection%20workflows%2C%20indicating%20that%20LLM-assisted%0Aanalysis%20can%20enhance%20efficiency%20without%20compromising%20accuracy.%20In%20this%20study%2C%0Aseveral%20LLMs%20are%20explored%20with%20prompts%20specifically%20designed%20to%20enhance%20the%0Aquality%20of%20image%20descriptions%2C%20which%20are%20applied%20to%20interpret%20five%20different%0ANDE%20contour%20maps%20obtained%20through%20technologies%20for%20assessing%20bridge%20conditions.%0AEach%20LLM%20model%20is%20evaluated%20based%20on%20its%20ability%20to%20produce%20detailed%0Adescriptions%2C%20identify%20defects%2C%20provide%20actionable%20recommendations%2C%20and%0Ademonstrate%20overall%20accuracy.%20The%20research%20indicates%20that%20four%20of%20the%20nine%0Amodels%20provide%20better%20image%20descriptions%2C%20effectively%20covering%20a%20wide%20range%20of%0Atopics%20related%20to%20the%20bridge%27s%20condition.%20The%20outputs%20from%20these%20four%20models%0Aare%20summarized%20using%20five%20different%20LLMs%20to%20form%20a%20comprehensive%20overview%20of%0Athe%20bridge.%20Notably%2C%20LLMs%20ChatGPT-4%20and%20Claude%203.5%20Sonnet%20generate%20more%0Aeffective%20summaries.%20The%20findings%20suggest%20that%20LLMs%20have%20the%20potential%20to%0Asignificantly%20improve%20efficiency%20and%20accuracy.%20This%20pilot%20study%20presents%20an%0Ainnovative%20approach%20that%20leverages%20LLMs%20for%20image%20captioning%20in%20parallel%20and%0Asummarization%2C%20enabling%20faster%20decision-making%20in%20bridge%20maintenance%20and%0Aenhancing%20infrastructure%20management%20and%20safety%20assessments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Interpretation%2520of%2520Non-Destructive%2520Evaluation%2520Contour%2520Maps%250A%2520%2520Using%2520Large%2520Language%2520Models%2520for%2520Bridge%2520Condition%2520Assessment%26entry.906535625%3DViraj%2520Nishesh%2520Darji%2520and%2520Callie%2520C.%2520Liao%2520and%2520Duoduo%2520Liao%26entry.1292438233%3D%2520%2520Bridge%2520maintenance%2520and%2520safety%2520are%2520essential%2520for%2520transportation%2520authorities%252C%250Aand%2520Non-Destructive%2520Evaluation%2520%2528NDE%2529%2520techniques%2520are%2520critical%2520to%2520assessing%250Astructural%2520integrity.%2520However%252C%2520interpreting%2520NDE%2520data%2520can%2520be%2520time-consuming%2520and%250Arequires%2520expertise%252C%2520potentially%2520delaying%2520decision-making.%2520Recent%2520advancements%250Ain%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520new%2520ways%2520to%2520automate%2520and%2520improve%2520this%250Aanalysis.%2520This%2520pilot%2520study%2520introduces%2520a%2520holistic%2520assessment%2520of%2520LLM%2520capabilities%250Afor%2520interpreting%2520NDE%2520contour%2520maps%2520and%2520demonstrates%2520the%2520effectiveness%2520of%2520LLMs%2520in%250Aproviding%2520detailed%2520bridge%2520condition%2520analyses.%2520It%2520establishes%2520a%2520framework%2520for%250Aintegrating%2520LLMs%2520into%2520bridge%2520inspection%2520workflows%252C%2520indicating%2520that%2520LLM-assisted%250Aanalysis%2520can%2520enhance%2520efficiency%2520without%2520compromising%2520accuracy.%2520In%2520this%2520study%252C%250Aseveral%2520LLMs%2520are%2520explored%2520with%2520prompts%2520specifically%2520designed%2520to%2520enhance%2520the%250Aquality%2520of%2520image%2520descriptions%252C%2520which%2520are%2520applied%2520to%2520interpret%2520five%2520different%250ANDE%2520contour%2520maps%2520obtained%2520through%2520technologies%2520for%2520assessing%2520bridge%2520conditions.%250AEach%2520LLM%2520model%2520is%2520evaluated%2520based%2520on%2520its%2520ability%2520to%2520produce%2520detailed%250Adescriptions%252C%2520identify%2520defects%252C%2520provide%2520actionable%2520recommendations%252C%2520and%250Ademonstrate%2520overall%2520accuracy.%2520The%2520research%2520indicates%2520that%2520four%2520of%2520the%2520nine%250Amodels%2520provide%2520better%2520image%2520descriptions%252C%2520effectively%2520covering%2520a%2520wide%2520range%2520of%250Atopics%2520related%2520to%2520the%2520bridge%2527s%2520condition.%2520The%2520outputs%2520from%2520these%2520four%2520models%250Aare%2520summarized%2520using%2520five%2520different%2520LLMs%2520to%2520form%2520a%2520comprehensive%2520overview%2520of%250Athe%2520bridge.%2520Notably%252C%2520LLMs%2520ChatGPT-4%2520and%2520Claude%25203.5%2520Sonnet%2520generate%2520more%250Aeffective%2520summaries.%2520The%2520findings%2520suggest%2520that%2520LLMs%2520have%2520the%2520potential%2520to%250Asignificantly%2520improve%2520efficiency%2520and%2520accuracy.%2520This%2520pilot%2520study%2520presents%2520an%250Ainnovative%2520approach%2520that%2520leverages%2520LLMs%2520for%2520image%2520captioning%2520in%2520parallel%2520and%250Asummarization%252C%2520enabling%2520faster%2520decision-making%2520in%2520bridge%2520maintenance%2520and%250Aenhancing%2520infrastructure%2520management%2520and%2520safety%2520assessments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Interpretation%20of%20Non-Destructive%20Evaluation%20Contour%20Maps%0A%20%20Using%20Large%20Language%20Models%20for%20Bridge%20Condition%20Assessment&entry.906535625=Viraj%20Nishesh%20Darji%20and%20Callie%20C.%20Liao%20and%20Duoduo%20Liao&entry.1292438233=%20%20Bridge%20maintenance%20and%20safety%20are%20essential%20for%20transportation%20authorities%2C%0Aand%20Non-Destructive%20Evaluation%20%28NDE%29%20techniques%20are%20critical%20to%20assessing%0Astructural%20integrity.%20However%2C%20interpreting%20NDE%20data%20can%20be%20time-consuming%20and%0Arequires%20expertise%2C%20potentially%20delaying%20decision-making.%20Recent%20advancements%0Ain%20Large%20Language%20Models%20%28LLMs%29%20offer%20new%20ways%20to%20automate%20and%20improve%20this%0Aanalysis.%20This%20pilot%20study%20introduces%20a%20holistic%20assessment%20of%20LLM%20capabilities%0Afor%20interpreting%20NDE%20contour%20maps%20and%20demonstrates%20the%20effectiveness%20of%20LLMs%20in%0Aproviding%20detailed%20bridge%20condition%20analyses.%20It%20establishes%20a%20framework%20for%0Aintegrating%20LLMs%20into%20bridge%20inspection%20workflows%2C%20indicating%20that%20LLM-assisted%0Aanalysis%20can%20enhance%20efficiency%20without%20compromising%20accuracy.%20In%20this%20study%2C%0Aseveral%20LLMs%20are%20explored%20with%20prompts%20specifically%20designed%20to%20enhance%20the%0Aquality%20of%20image%20descriptions%2C%20which%20are%20applied%20to%20interpret%20five%20different%0ANDE%20contour%20maps%20obtained%20through%20technologies%20for%20assessing%20bridge%20conditions.%0AEach%20LLM%20model%20is%20evaluated%20based%20on%20its%20ability%20to%20produce%20detailed%0Adescriptions%2C%20identify%20defects%2C%20provide%20actionable%20recommendations%2C%20and%0Ademonstrate%20overall%20accuracy.%20The%20research%20indicates%20that%20four%20of%20the%20nine%0Amodels%20provide%20better%20image%20descriptions%2C%20effectively%20covering%20a%20wide%20range%20of%0Atopics%20related%20to%20the%20bridge%27s%20condition.%20The%20outputs%20from%20these%20four%20models%0Aare%20summarized%20using%20five%20different%20LLMs%20to%20form%20a%20comprehensive%20overview%20of%0Athe%20bridge.%20Notably%2C%20LLMs%20ChatGPT-4%20and%20Claude%203.5%20Sonnet%20generate%20more%0Aeffective%20summaries.%20The%20findings%20suggest%20that%20LLMs%20have%20the%20potential%20to%0Asignificantly%20improve%20efficiency%20and%20accuracy.%20This%20pilot%20study%20presents%20an%0Ainnovative%20approach%20that%20leverages%20LLMs%20for%20image%20captioning%20in%20parallel%20and%0Asummarization%2C%20enabling%20faster%20decision-making%20in%20bridge%20maintenance%20and%0Aenhancing%20infrastructure%20management%20and%20safety%20assessments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14107v1&entry.124074799=Read"},
{"title": "Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models", "author": "Konstantin Donhauser and Kristina Ulicna and Gemma Elyse Moran and Aditya Ravuri and Kian Kenyon-Dean and Cian Eastwood and Jason Hartford", "abstract": "  Sparse dictionary learning (DL) has emerged as a powerful approach to extract\nsemantically meaningful concepts from the internals of large language models\n(LLMs) trained mainly in the text domain. In this work, we explore whether DL\ncan extract meaningful concepts from less human-interpretable scientific data,\nsuch as vision foundation models trained on cell microscopy images, where\nlimited prior knowledge exists about which high-level concepts should arise. We\npropose a novel combination of a sparse DL algorithm, Iterative Codebook\nFeature Learning (ICFL), with a PCA whitening pre-processing step derived from\ncontrol data. Using this combined approach, we successfully retrieve\nbiologically meaningful concepts, such as cell types and genetic perturbations.\nMoreover, we demonstrate how our method reveals subtle morphological changes\narising from human-interpretable interventions, offering a promising new\ndirection for scientific discovery via mechanistic interpretability in\nbioimaging.\n", "link": "http://arxiv.org/abs/2412.16247v3", "date": "2025-07-18", "relevancy": 2.2868, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5843}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20scientific%20discovery%20with%20dictionary%20learning%3A%20Extracting%0A%20%20biological%20concepts%20from%20microscopy%20foundation%20models&body=Title%3A%20Towards%20scientific%20discovery%20with%20dictionary%20learning%3A%20Extracting%0A%20%20biological%20concepts%20from%20microscopy%20foundation%20models%0AAuthor%3A%20Konstantin%20Donhauser%20and%20Kristina%20Ulicna%20and%20Gemma%20Elyse%20Moran%20and%20Aditya%20Ravuri%20and%20Kian%20Kenyon-Dean%20and%20Cian%20Eastwood%20and%20Jason%20Hartford%0AAbstract%3A%20%20%20Sparse%20dictionary%20learning%20%28DL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20extract%0Asemantically%20meaningful%20concepts%20from%20the%20internals%20of%20large%20language%20models%0A%28LLMs%29%20trained%20mainly%20in%20the%20text%20domain.%20In%20this%20work%2C%20we%20explore%20whether%20DL%0Acan%20extract%20meaningful%20concepts%20from%20less%20human-interpretable%20scientific%20data%2C%0Asuch%20as%20vision%20foundation%20models%20trained%20on%20cell%20microscopy%20images%2C%20where%0Alimited%20prior%20knowledge%20exists%20about%20which%20high-level%20concepts%20should%20arise.%20We%0Apropose%20a%20novel%20combination%20of%20a%20sparse%20DL%20algorithm%2C%20Iterative%20Codebook%0AFeature%20Learning%20%28ICFL%29%2C%20with%20a%20PCA%20whitening%20pre-processing%20step%20derived%20from%0Acontrol%20data.%20Using%20this%20combined%20approach%2C%20we%20successfully%20retrieve%0Abiologically%20meaningful%20concepts%2C%20such%20as%20cell%20types%20and%20genetic%20perturbations.%0AMoreover%2C%20we%20demonstrate%20how%20our%20method%20reveals%20subtle%20morphological%20changes%0Aarising%20from%20human-interpretable%20interventions%2C%20offering%20a%20promising%20new%0Adirection%20for%20scientific%20discovery%20via%20mechanistic%20interpretability%20in%0Abioimaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16247v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520scientific%2520discovery%2520with%2520dictionary%2520learning%253A%2520Extracting%250A%2520%2520biological%2520concepts%2520from%2520microscopy%2520foundation%2520models%26entry.906535625%3DKonstantin%2520Donhauser%2520and%2520Kristina%2520Ulicna%2520and%2520Gemma%2520Elyse%2520Moran%2520and%2520Aditya%2520Ravuri%2520and%2520Kian%2520Kenyon-Dean%2520and%2520Cian%2520Eastwood%2520and%2520Jason%2520Hartford%26entry.1292438233%3D%2520%2520Sparse%2520dictionary%2520learning%2520%2528DL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520approach%2520to%2520extract%250Asemantically%2520meaningful%2520concepts%2520from%2520the%2520internals%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520trained%2520mainly%2520in%2520the%2520text%2520domain.%2520In%2520this%2520work%252C%2520we%2520explore%2520whether%2520DL%250Acan%2520extract%2520meaningful%2520concepts%2520from%2520less%2520human-interpretable%2520scientific%2520data%252C%250Asuch%2520as%2520vision%2520foundation%2520models%2520trained%2520on%2520cell%2520microscopy%2520images%252C%2520where%250Alimited%2520prior%2520knowledge%2520exists%2520about%2520which%2520high-level%2520concepts%2520should%2520arise.%2520We%250Apropose%2520a%2520novel%2520combination%2520of%2520a%2520sparse%2520DL%2520algorithm%252C%2520Iterative%2520Codebook%250AFeature%2520Learning%2520%2528ICFL%2529%252C%2520with%2520a%2520PCA%2520whitening%2520pre-processing%2520step%2520derived%2520from%250Acontrol%2520data.%2520Using%2520this%2520combined%2520approach%252C%2520we%2520successfully%2520retrieve%250Abiologically%2520meaningful%2520concepts%252C%2520such%2520as%2520cell%2520types%2520and%2520genetic%2520perturbations.%250AMoreover%252C%2520we%2520demonstrate%2520how%2520our%2520method%2520reveals%2520subtle%2520morphological%2520changes%250Aarising%2520from%2520human-interpretable%2520interventions%252C%2520offering%2520a%2520promising%2520new%250Adirection%2520for%2520scientific%2520discovery%2520via%2520mechanistic%2520interpretability%2520in%250Abioimaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16247v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20scientific%20discovery%20with%20dictionary%20learning%3A%20Extracting%0A%20%20biological%20concepts%20from%20microscopy%20foundation%20models&entry.906535625=Konstantin%20Donhauser%20and%20Kristina%20Ulicna%20and%20Gemma%20Elyse%20Moran%20and%20Aditya%20Ravuri%20and%20Kian%20Kenyon-Dean%20and%20Cian%20Eastwood%20and%20Jason%20Hartford&entry.1292438233=%20%20Sparse%20dictionary%20learning%20%28DL%29%20has%20emerged%20as%20a%20powerful%20approach%20to%20extract%0Asemantically%20meaningful%20concepts%20from%20the%20internals%20of%20large%20language%20models%0A%28LLMs%29%20trained%20mainly%20in%20the%20text%20domain.%20In%20this%20work%2C%20we%20explore%20whether%20DL%0Acan%20extract%20meaningful%20concepts%20from%20less%20human-interpretable%20scientific%20data%2C%0Asuch%20as%20vision%20foundation%20models%20trained%20on%20cell%20microscopy%20images%2C%20where%0Alimited%20prior%20knowledge%20exists%20about%20which%20high-level%20concepts%20should%20arise.%20We%0Apropose%20a%20novel%20combination%20of%20a%20sparse%20DL%20algorithm%2C%20Iterative%20Codebook%0AFeature%20Learning%20%28ICFL%29%2C%20with%20a%20PCA%20whitening%20pre-processing%20step%20derived%20from%0Acontrol%20data.%20Using%20this%20combined%20approach%2C%20we%20successfully%20retrieve%0Abiologically%20meaningful%20concepts%2C%20such%20as%20cell%20types%20and%20genetic%20perturbations.%0AMoreover%2C%20we%20demonstrate%20how%20our%20method%20reveals%20subtle%20morphological%20changes%0Aarising%20from%20human-interpretable%20interventions%2C%20offering%20a%20promising%20new%0Adirection%20for%20scientific%20discovery%20via%20mechanistic%20interpretability%20in%0Abioimaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16247v3&entry.124074799=Read"},
{"title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time\n  Human-AI Collaboration", "author": "Xiyun Li and Yining Ding and Yuhua Jiang and Yunlong Zhao and Runpeng Xie and Shuang Xu and Yuanhua Ni and Yiqin Yang and Bo Xu", "abstract": "  Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.\n", "link": "http://arxiv.org/abs/2507.14088v1", "date": "2025-07-18", "relevancy": 2.2761, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5868}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPMT%3A%20Dual%20Process%20Multi-scale%20Theory%20of%20Mind%20Framework%20for%20Real-time%0A%20%20Human-AI%20Collaboration&body=Title%3A%20DPMT%3A%20Dual%20Process%20Multi-scale%20Theory%20of%20Mind%20Framework%20for%20Real-time%0A%20%20Human-AI%20Collaboration%0AAuthor%3A%20Xiyun%20Li%20and%20Yining%20Ding%20and%20Yuhua%20Jiang%20and%20Yunlong%20Zhao%20and%20Runpeng%20Xie%20and%20Shuang%20Xu%20and%20Yuanhua%20Ni%20and%20Yiqin%20Yang%20and%20Bo%20Xu%0AAbstract%3A%20%20%20Real-time%20human-artificial%20intelligence%20%28AI%29%20collaboration%20is%20crucial%20yet%0Achallenging%2C%20especially%20when%20AI%20agents%20must%20adapt%20to%20diverse%20and%20unseen%20human%0Abehaviors%20in%20dynamic%20scenarios.%20Existing%20large%20language%20model%20%28LLM%29%20agents%0Aoften%20fail%20to%20accurately%20model%20the%20complex%20human%20mental%20characteristics%20such%20as%0Adomain%20intentions%2C%20especially%20in%20the%20absence%20of%20direct%20communication.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20dual%20process%20multi-scale%20theory%20of%0Amind%20%28DPMT%29%20framework%2C%20drawing%20inspiration%20from%20cognitive%20science%20dual%20process%0Atheory.%20Our%20DPMT%20framework%20incorporates%20a%20multi-scale%20theory%20of%20mind%20%28ToM%29%0Amodule%20to%20facilitate%20robust%20human%20partner%20modeling%20through%20mental%0Acharacteristic%20reasoning.%20Experimental%20results%20demonstrate%20that%20DPMT%0Asignificantly%20enhances%20human-AI%20collaboration%2C%20and%20ablation%20studies%20further%0Avalidate%20the%20contributions%20of%20our%20multi-scale%20ToM%20in%20the%20slow%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14088v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPMT%253A%2520Dual%2520Process%2520Multi-scale%2520Theory%2520of%2520Mind%2520Framework%2520for%2520Real-time%250A%2520%2520Human-AI%2520Collaboration%26entry.906535625%3DXiyun%2520Li%2520and%2520Yining%2520Ding%2520and%2520Yuhua%2520Jiang%2520and%2520Yunlong%2520Zhao%2520and%2520Runpeng%2520Xie%2520and%2520Shuang%2520Xu%2520and%2520Yuanhua%2520Ni%2520and%2520Yiqin%2520Yang%2520and%2520Bo%2520Xu%26entry.1292438233%3D%2520%2520Real-time%2520human-artificial%2520intelligence%2520%2528AI%2529%2520collaboration%2520is%2520crucial%2520yet%250Achallenging%252C%2520especially%2520when%2520AI%2520agents%2520must%2520adapt%2520to%2520diverse%2520and%2520unseen%2520human%250Abehaviors%2520in%2520dynamic%2520scenarios.%2520Existing%2520large%2520language%2520model%2520%2528LLM%2529%2520agents%250Aoften%2520fail%2520to%2520accurately%2520model%2520the%2520complex%2520human%2520mental%2520characteristics%2520such%2520as%250Adomain%2520intentions%252C%2520especially%2520in%2520the%2520absence%2520of%2520direct%2520communication.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520dual%2520process%2520multi-scale%2520theory%2520of%250Amind%2520%2528DPMT%2529%2520framework%252C%2520drawing%2520inspiration%2520from%2520cognitive%2520science%2520dual%2520process%250Atheory.%2520Our%2520DPMT%2520framework%2520incorporates%2520a%2520multi-scale%2520theory%2520of%2520mind%2520%2528ToM%2529%250Amodule%2520to%2520facilitate%2520robust%2520human%2520partner%2520modeling%2520through%2520mental%250Acharacteristic%2520reasoning.%2520Experimental%2520results%2520demonstrate%2520that%2520DPMT%250Asignificantly%2520enhances%2520human-AI%2520collaboration%252C%2520and%2520ablation%2520studies%2520further%250Avalidate%2520the%2520contributions%2520of%2520our%2520multi-scale%2520ToM%2520in%2520the%2520slow%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14088v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPMT%3A%20Dual%20Process%20Multi-scale%20Theory%20of%20Mind%20Framework%20for%20Real-time%0A%20%20Human-AI%20Collaboration&entry.906535625=Xiyun%20Li%20and%20Yining%20Ding%20and%20Yuhua%20Jiang%20and%20Yunlong%20Zhao%20and%20Runpeng%20Xie%20and%20Shuang%20Xu%20and%20Yuanhua%20Ni%20and%20Yiqin%20Yang%20and%20Bo%20Xu&entry.1292438233=%20%20Real-time%20human-artificial%20intelligence%20%28AI%29%20collaboration%20is%20crucial%20yet%0Achallenging%2C%20especially%20when%20AI%20agents%20must%20adapt%20to%20diverse%20and%20unseen%20human%0Abehaviors%20in%20dynamic%20scenarios.%20Existing%20large%20language%20model%20%28LLM%29%20agents%0Aoften%20fail%20to%20accurately%20model%20the%20complex%20human%20mental%20characteristics%20such%20as%0Adomain%20intentions%2C%20especially%20in%20the%20absence%20of%20direct%20communication.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20novel%20dual%20process%20multi-scale%20theory%20of%0Amind%20%28DPMT%29%20framework%2C%20drawing%20inspiration%20from%20cognitive%20science%20dual%20process%0Atheory.%20Our%20DPMT%20framework%20incorporates%20a%20multi-scale%20theory%20of%20mind%20%28ToM%29%0Amodule%20to%20facilitate%20robust%20human%20partner%20modeling%20through%20mental%0Acharacteristic%20reasoning.%20Experimental%20results%20demonstrate%20that%20DPMT%0Asignificantly%20enhances%20human-AI%20collaboration%2C%20and%20ablation%20studies%20further%0Avalidate%20the%20contributions%20of%20our%20multi-scale%20ToM%20in%20the%20slow%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14088v1&entry.124074799=Read"},
{"title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right\n  Breast MRI Segmentation", "author": "Maximilian Rokuss and Benjamin Hamm and Yannick Kirchhoff and Klaus Maier-Hein", "abstract": "  We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider\n", "link": "http://arxiv.org/abs/2507.13830v1", "date": "2025-07-18", "relevancy": 2.2391, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4436}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divide%20and%20Conquer%3A%20A%20Large-Scale%20Dataset%20and%20Model%20for%20Left-Right%0A%20%20Breast%20MRI%20Segmentation&body=Title%3A%20Divide%20and%20Conquer%3A%20A%20Large-Scale%20Dataset%20and%20Model%20for%20Left-Right%0A%20%20Breast%20MRI%20Segmentation%0AAuthor%3A%20Maximilian%20Rokuss%20and%20Benjamin%20Hamm%20and%20Yannick%20Kirchhoff%20and%20Klaus%20Maier-Hein%0AAbstract%3A%20%20%20We%20introduce%20the%20first%20publicly%20available%20breast%20MRI%20dataset%20with%20explicit%0Aleft%20and%20right%20breast%20segmentation%20labels%2C%20encompassing%20more%20than%2013%2C000%0Aannotated%20cases.%20Alongside%20this%20dataset%2C%20we%20provide%20a%20robust%20deep-learning%0Amodel%20trained%20for%20left-right%20breast%20segmentation.%20This%20work%20addresses%20a%0Acritical%20gap%20in%20breast%20MRI%20analysis%20and%20offers%20a%20valuable%20resource%20for%20the%0Adevelopment%20of%20advanced%20tools%20in%20women%27s%20health.%20The%20dataset%20and%20trained%20model%0Aare%20publicly%20available%20at%3A%20www.github.com/MIC-DKFZ/BreastDivider%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivide%2520and%2520Conquer%253A%2520A%2520Large-Scale%2520Dataset%2520and%2520Model%2520for%2520Left-Right%250A%2520%2520Breast%2520MRI%2520Segmentation%26entry.906535625%3DMaximilian%2520Rokuss%2520and%2520Benjamin%2520Hamm%2520and%2520Yannick%2520Kirchhoff%2520and%2520Klaus%2520Maier-Hein%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520first%2520publicly%2520available%2520breast%2520MRI%2520dataset%2520with%2520explicit%250Aleft%2520and%2520right%2520breast%2520segmentation%2520labels%252C%2520encompassing%2520more%2520than%252013%252C000%250Aannotated%2520cases.%2520Alongside%2520this%2520dataset%252C%2520we%2520provide%2520a%2520robust%2520deep-learning%250Amodel%2520trained%2520for%2520left-right%2520breast%2520segmentation.%2520This%2520work%2520addresses%2520a%250Acritical%2520gap%2520in%2520breast%2520MRI%2520analysis%2520and%2520offers%2520a%2520valuable%2520resource%2520for%2520the%250Adevelopment%2520of%2520advanced%2520tools%2520in%2520women%2527s%2520health.%2520The%2520dataset%2520and%2520trained%2520model%250Aare%2520publicly%2520available%2520at%253A%2520www.github.com/MIC-DKFZ/BreastDivider%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divide%20and%20Conquer%3A%20A%20Large-Scale%20Dataset%20and%20Model%20for%20Left-Right%0A%20%20Breast%20MRI%20Segmentation&entry.906535625=Maximilian%20Rokuss%20and%20Benjamin%20Hamm%20and%20Yannick%20Kirchhoff%20and%20Klaus%20Maier-Hein&entry.1292438233=%20%20We%20introduce%20the%20first%20publicly%20available%20breast%20MRI%20dataset%20with%20explicit%0Aleft%20and%20right%20breast%20segmentation%20labels%2C%20encompassing%20more%20than%2013%2C000%0Aannotated%20cases.%20Alongside%20this%20dataset%2C%20we%20provide%20a%20robust%20deep-learning%0Amodel%20trained%20for%20left-right%20breast%20segmentation.%20This%20work%20addresses%20a%0Acritical%20gap%20in%20breast%20MRI%20analysis%20and%20offers%20a%20valuable%20resource%20for%20the%0Adevelopment%20of%20advanced%20tools%20in%20women%27s%20health.%20The%20dataset%20and%20trained%20model%0Aare%20publicly%20available%20at%3A%20www.github.com/MIC-DKFZ/BreastDivider%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13830v1&entry.124074799=Read"},
{"title": "Sparsification Under Siege: Defending Against Poisoning Attacks in\n  Communication-Efficient Federated Learning", "author": "Zhiyong Jin and Runhua Xu and Chao Li and Yizhong Liu and Jianxin Li", "abstract": "  Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy, yet it faces significant\nchallenges in communication efficiency and vulnerability to poisoning attacks.\nWhile sparsification techniques mitigate communication overhead by transmitting\nonly critical model parameters, they inadvertently amplify security risks:\nadversarial clients can exploit sparse updates to evade detection and degrade\nmodel performance. Existing defense mechanisms, designed for standard FL\ncommunication scenarios, are ineffective in addressing these vulnerabilities\nwithin sparsified FL. To bridge this gap, we propose FLARE, a novel federated\nlearning framework that integrates sparse index mask inspection and model\nupdate sign similarity analysis to detect and mitigate poisoning attacks in\nsparsified FL. Extensive experiments across multiple datasets and adversarial\nscenarios demonstrate that FLARE significantly outperforms existing defense\nstrategies, effectively securing sparsified FL against poisoning attacks while\nmaintaining communication efficiency.\n", "link": "http://arxiv.org/abs/2505.01454v3", "date": "2025-07-18", "relevancy": 2.2352, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4657}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning&body=Title%3A%20Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning%0AAuthor%3A%20Zhiyong%20Jin%20and%20Runhua%20Xu%20and%20Chao%20Li%20and%20Yizhong%20Liu%20and%20Jianxin%20Li%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%2C%20yet%20it%20faces%20significant%0Achallenges%20in%20communication%20efficiency%20and%20vulnerability%20to%20poisoning%20attacks.%0AWhile%20sparsification%20techniques%20mitigate%20communication%20overhead%20by%20transmitting%0Aonly%20critical%20model%20parameters%2C%20they%20inadvertently%20amplify%20security%20risks%3A%0Aadversarial%20clients%20can%20exploit%20sparse%20updates%20to%20evade%20detection%20and%20degrade%0Amodel%20performance.%20Existing%20defense%20mechanisms%2C%20designed%20for%20standard%20FL%0Acommunication%20scenarios%2C%20are%20ineffective%20in%20addressing%20these%20vulnerabilities%0Awithin%20sparsified%20FL.%20To%20bridge%20this%20gap%2C%20we%20propose%20FLARE%2C%20a%20novel%20federated%0Alearning%20framework%20that%20integrates%20sparse%20index%20mask%20inspection%20and%20model%0Aupdate%20sign%20similarity%20analysis%20to%20detect%20and%20mitigate%20poisoning%20attacks%20in%0Asparsified%20FL.%20Extensive%20experiments%20across%20multiple%20datasets%20and%20adversarial%0Ascenarios%20demonstrate%20that%20FLARE%20significantly%20outperforms%20existing%20defense%0Astrategies%2C%20effectively%20securing%20sparsified%20FL%20against%20poisoning%20attacks%20while%0Amaintaining%20communication%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01454v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsification%2520Under%2520Siege%253A%2520Defending%2520Against%2520Poisoning%2520Attacks%2520in%250A%2520%2520Communication-Efficient%2520Federated%2520Learning%26entry.906535625%3DZhiyong%2520Jin%2520and%2520Runhua%2520Xu%2520and%2520Chao%2520Li%2520and%2520Yizhong%2520Liu%2520and%2520Jianxin%2520Li%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520enables%2520collaborative%2520model%2520training%2520across%250Adistributed%2520clients%2520while%2520preserving%2520data%2520privacy%252C%2520yet%2520it%2520faces%2520significant%250Achallenges%2520in%2520communication%2520efficiency%2520and%2520vulnerability%2520to%2520poisoning%2520attacks.%250AWhile%2520sparsification%2520techniques%2520mitigate%2520communication%2520overhead%2520by%2520transmitting%250Aonly%2520critical%2520model%2520parameters%252C%2520they%2520inadvertently%2520amplify%2520security%2520risks%253A%250Aadversarial%2520clients%2520can%2520exploit%2520sparse%2520updates%2520to%2520evade%2520detection%2520and%2520degrade%250Amodel%2520performance.%2520Existing%2520defense%2520mechanisms%252C%2520designed%2520for%2520standard%2520FL%250Acommunication%2520scenarios%252C%2520are%2520ineffective%2520in%2520addressing%2520these%2520vulnerabilities%250Awithin%2520sparsified%2520FL.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520FLARE%252C%2520a%2520novel%2520federated%250Alearning%2520framework%2520that%2520integrates%2520sparse%2520index%2520mask%2520inspection%2520and%2520model%250Aupdate%2520sign%2520similarity%2520analysis%2520to%2520detect%2520and%2520mitigate%2520poisoning%2520attacks%2520in%250Asparsified%2520FL.%2520Extensive%2520experiments%2520across%2520multiple%2520datasets%2520and%2520adversarial%250Ascenarios%2520demonstrate%2520that%2520FLARE%2520significantly%2520outperforms%2520existing%2520defense%250Astrategies%252C%2520effectively%2520securing%2520sparsified%2520FL%2520against%2520poisoning%2520attacks%2520while%250Amaintaining%2520communication%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01454v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsification%20Under%20Siege%3A%20Defending%20Against%20Poisoning%20Attacks%20in%0A%20%20Communication-Efficient%20Federated%20Learning&entry.906535625=Zhiyong%20Jin%20and%20Runhua%20Xu%20and%20Chao%20Li%20and%20Yizhong%20Liu%20and%20Jianxin%20Li&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20enables%20collaborative%20model%20training%20across%0Adistributed%20clients%20while%20preserving%20data%20privacy%2C%20yet%20it%20faces%20significant%0Achallenges%20in%20communication%20efficiency%20and%20vulnerability%20to%20poisoning%20attacks.%0AWhile%20sparsification%20techniques%20mitigate%20communication%20overhead%20by%20transmitting%0Aonly%20critical%20model%20parameters%2C%20they%20inadvertently%20amplify%20security%20risks%3A%0Aadversarial%20clients%20can%20exploit%20sparse%20updates%20to%20evade%20detection%20and%20degrade%0Amodel%20performance.%20Existing%20defense%20mechanisms%2C%20designed%20for%20standard%20FL%0Acommunication%20scenarios%2C%20are%20ineffective%20in%20addressing%20these%20vulnerabilities%0Awithin%20sparsified%20FL.%20To%20bridge%20this%20gap%2C%20we%20propose%20FLARE%2C%20a%20novel%20federated%0Alearning%20framework%20that%20integrates%20sparse%20index%20mask%20inspection%20and%20model%0Aupdate%20sign%20similarity%20analysis%20to%20detect%20and%20mitigate%20poisoning%20attacks%20in%0Asparsified%20FL.%20Extensive%20experiments%20across%20multiple%20datasets%20and%20adversarial%0Ascenarios%20demonstrate%20that%20FLARE%20significantly%20outperforms%20existing%20defense%0Astrategies%2C%20effectively%20securing%20sparsified%20FL%20against%20poisoning%20attacks%20while%0Amaintaining%20communication%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01454v3&entry.124074799=Read"},
{"title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein\n  Conformational Space", "author": "Jingbo Liang and Bruna Jacobson", "abstract": "  Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.\n", "link": "http://arxiv.org/abs/2507.13950v1", "date": "2025-07-18", "relevancy": 2.2128, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5558}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5524}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDyGAN%3A%20Combining%20Molecular%20Dynamics%20With%20GANs%20to%20Investigate%20Protein%0A%20%20Conformational%20Space&body=Title%3A%20MoDyGAN%3A%20Combining%20Molecular%20Dynamics%20With%20GANs%20to%20Investigate%20Protein%0A%20%20Conformational%20Space%0AAuthor%3A%20Jingbo%20Liang%20and%20Bruna%20Jacobson%0AAbstract%3A%20%20%20Extensively%20exploring%20protein%20conformational%20landscapes%20remains%20a%20major%0Achallenge%20in%20computational%20biology%20due%20to%20the%20high%20computational%20cost%20involved%0Ain%20dynamic%20physics-based%20simulations.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Apipeline%2C%20MoDyGAN%2C%20that%20leverages%20molecular%20dynamics%20%28MD%29%20simulations%20and%0Agenerative%20adversarial%20networks%20%28GANs%29%20to%20explore%20protein%20conformational%0Aspaces.%20MoDyGAN%20contains%20a%20generator%20that%20maps%20Gaussian%20distributions%20into%0AMD-derived%20protein%20trajectories%2C%20and%20a%20refinement%20module%20that%20combines%20ensemble%0Alearning%20with%20a%20dual-discriminator%20to%20further%20improve%20the%20plausibility%20of%0Agenerated%20conformations.%20Central%20to%20our%20approach%20is%20an%20innovative%0Arepresentation%20technique%20that%20reversibly%20transforms%203D%20protein%20structures%20into%0A2D%20matrices%2C%20enabling%20the%20use%20of%20advanced%20image-based%20GAN%20architectures.%20We%20use%0Athree%20rigid%20proteins%20to%20demonstrate%20that%20MoDyGAN%20can%20generate%20plausible%20new%0Aconformations.%20We%20also%20use%20deca-alanine%20as%20a%20case%20study%20to%20show%20that%0Ainterpolations%20within%20the%20latent%20space%20closely%20align%20with%20trajectories%20obtained%0Afrom%20steered%20molecular%20dynamics%20%28SMD%29%20simulations.%20Our%20results%20suggest%20that%0Arepresenting%20proteins%20as%20image-like%20data%20unlocks%20new%20possibilities%20for%20applying%0Aadvanced%20deep%20learning%20techniques%20to%20biomolecular%20simulation%2C%20leading%20to%20an%0Aefficient%20sampling%20of%20conformational%20states.%20Additionally%2C%20the%20proposed%0Aframework%20holds%20strong%20potential%20for%20extension%20to%20other%20complex%203D%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDyGAN%253A%2520Combining%2520Molecular%2520Dynamics%2520With%2520GANs%2520to%2520Investigate%2520Protein%250A%2520%2520Conformational%2520Space%26entry.906535625%3DJingbo%2520Liang%2520and%2520Bruna%2520Jacobson%26entry.1292438233%3D%2520%2520Extensively%2520exploring%2520protein%2520conformational%2520landscapes%2520remains%2520a%2520major%250Achallenge%2520in%2520computational%2520biology%2520due%2520to%2520the%2520high%2520computational%2520cost%2520involved%250Ain%2520dynamic%2520physics-based%2520simulations.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Apipeline%252C%2520MoDyGAN%252C%2520that%2520leverages%2520molecular%2520dynamics%2520%2528MD%2529%2520simulations%2520and%250Agenerative%2520adversarial%2520networks%2520%2528GANs%2529%2520to%2520explore%2520protein%2520conformational%250Aspaces.%2520MoDyGAN%2520contains%2520a%2520generator%2520that%2520maps%2520Gaussian%2520distributions%2520into%250AMD-derived%2520protein%2520trajectories%252C%2520and%2520a%2520refinement%2520module%2520that%2520combines%2520ensemble%250Alearning%2520with%2520a%2520dual-discriminator%2520to%2520further%2520improve%2520the%2520plausibility%2520of%250Agenerated%2520conformations.%2520Central%2520to%2520our%2520approach%2520is%2520an%2520innovative%250Arepresentation%2520technique%2520that%2520reversibly%2520transforms%25203D%2520protein%2520structures%2520into%250A2D%2520matrices%252C%2520enabling%2520the%2520use%2520of%2520advanced%2520image-based%2520GAN%2520architectures.%2520We%2520use%250Athree%2520rigid%2520proteins%2520to%2520demonstrate%2520that%2520MoDyGAN%2520can%2520generate%2520plausible%2520new%250Aconformations.%2520We%2520also%2520use%2520deca-alanine%2520as%2520a%2520case%2520study%2520to%2520show%2520that%250Ainterpolations%2520within%2520the%2520latent%2520space%2520closely%2520align%2520with%2520trajectories%2520obtained%250Afrom%2520steered%2520molecular%2520dynamics%2520%2528SMD%2529%2520simulations.%2520Our%2520results%2520suggest%2520that%250Arepresenting%2520proteins%2520as%2520image-like%2520data%2520unlocks%2520new%2520possibilities%2520for%2520applying%250Aadvanced%2520deep%2520learning%2520techniques%2520to%2520biomolecular%2520simulation%252C%2520leading%2520to%2520an%250Aefficient%2520sampling%2520of%2520conformational%2520states.%2520Additionally%252C%2520the%2520proposed%250Aframework%2520holds%2520strong%2520potential%2520for%2520extension%2520to%2520other%2520complex%25203D%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDyGAN%3A%20Combining%20Molecular%20Dynamics%20With%20GANs%20to%20Investigate%20Protein%0A%20%20Conformational%20Space&entry.906535625=Jingbo%20Liang%20and%20Bruna%20Jacobson&entry.1292438233=%20%20Extensively%20exploring%20protein%20conformational%20landscapes%20remains%20a%20major%0Achallenge%20in%20computational%20biology%20due%20to%20the%20high%20computational%20cost%20involved%0Ain%20dynamic%20physics-based%20simulations.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Apipeline%2C%20MoDyGAN%2C%20that%20leverages%20molecular%20dynamics%20%28MD%29%20simulations%20and%0Agenerative%20adversarial%20networks%20%28GANs%29%20to%20explore%20protein%20conformational%0Aspaces.%20MoDyGAN%20contains%20a%20generator%20that%20maps%20Gaussian%20distributions%20into%0AMD-derived%20protein%20trajectories%2C%20and%20a%20refinement%20module%20that%20combines%20ensemble%0Alearning%20with%20a%20dual-discriminator%20to%20further%20improve%20the%20plausibility%20of%0Agenerated%20conformations.%20Central%20to%20our%20approach%20is%20an%20innovative%0Arepresentation%20technique%20that%20reversibly%20transforms%203D%20protein%20structures%20into%0A2D%20matrices%2C%20enabling%20the%20use%20of%20advanced%20image-based%20GAN%20architectures.%20We%20use%0Athree%20rigid%20proteins%20to%20demonstrate%20that%20MoDyGAN%20can%20generate%20plausible%20new%0Aconformations.%20We%20also%20use%20deca-alanine%20as%20a%20case%20study%20to%20show%20that%0Ainterpolations%20within%20the%20latent%20space%20closely%20align%20with%20trajectories%20obtained%0Afrom%20steered%20molecular%20dynamics%20%28SMD%29%20simulations.%20Our%20results%20suggest%20that%0Arepresenting%20proteins%20as%20image-like%20data%20unlocks%20new%20possibilities%20for%20applying%0Aadvanced%20deep%20learning%20techniques%20to%20biomolecular%20simulation%2C%20leading%20to%20an%0Aefficient%20sampling%20of%20conformational%20states.%20Additionally%2C%20the%20proposed%0Aframework%20holds%20strong%20potential%20for%20extension%20to%20other%20complex%203D%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13950v1&entry.124074799=Read"},
{"title": "NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized\n  Safe Multi-Agent Motion Planning", "author": "Qingyi Chen and Ahmed H. Qureshi", "abstract": "  Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in\nrobotics. Despite substantial advancements, existing methods often face a\ndilemma. Decentralized algorithms typically rely on predicting the behavior of\nother agents, sharing contracts, or maintaining communication for safety, while\ncentralized approaches struggle with scalability and real-time decision-making.\nTo address these challenges, we introduce Neural Hamilton-Jacobi Reachability\nLearning (HJR) for Decentralized Multi-Agent Motion Planning. Our method\nprovides scalable neural HJR modeling to tackle high-dimensional configuration\nspaces and capture worst-case collision and safety constraints between agents.\nWe further propose a decentralized trajectory optimization framework that\nincorporates the learned HJR solutions to solve MAMP tasks in real-time. We\ndemonstrate that our method is both scalable and data-efficient, enabling the\nsolution of MAMP problems in higher-dimensional scenarios with complex\ncollision constraints. Our approach generalizes across various dynamical\nsystems, including a 12-dimensional dual-arm setup, and outperforms a range of\nstate-of-the-art techniques in successfully addressing challenging MAMP tasks.\nVideo demonstrations are available at https://youtu.be/IZiePX0p1Mc.\n", "link": "http://arxiv.org/abs/2507.13940v1", "date": "2025-07-18", "relevancy": 2.2078, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5483}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeHMO%3A%20Neural%20Hamilton-Jacobi%20Reachability%20Learning%20for%20Decentralized%0A%20%20Safe%20Multi-Agent%20Motion%20Planning&body=Title%3A%20NeHMO%3A%20Neural%20Hamilton-Jacobi%20Reachability%20Learning%20for%20Decentralized%0A%20%20Safe%20Multi-Agent%20Motion%20Planning%0AAuthor%3A%20Qingyi%20Chen%20and%20Ahmed%20H.%20Qureshi%0AAbstract%3A%20%20%20Safe%20Multi-Agent%20Motion%20Planning%20%28MAMP%29%20is%20a%20significant%20challenge%20in%0Arobotics.%20Despite%20substantial%20advancements%2C%20existing%20methods%20often%20face%20a%0Adilemma.%20Decentralized%20algorithms%20typically%20rely%20on%20predicting%20the%20behavior%20of%0Aother%20agents%2C%20sharing%20contracts%2C%20or%20maintaining%20communication%20for%20safety%2C%20while%0Acentralized%20approaches%20struggle%20with%20scalability%20and%20real-time%20decision-making.%0ATo%20address%20these%20challenges%2C%20we%20introduce%20Neural%20Hamilton-Jacobi%20Reachability%0ALearning%20%28HJR%29%20for%20Decentralized%20Multi-Agent%20Motion%20Planning.%20Our%20method%0Aprovides%20scalable%20neural%20HJR%20modeling%20to%20tackle%20high-dimensional%20configuration%0Aspaces%20and%20capture%20worst-case%20collision%20and%20safety%20constraints%20between%20agents.%0AWe%20further%20propose%20a%20decentralized%20trajectory%20optimization%20framework%20that%0Aincorporates%20the%20learned%20HJR%20solutions%20to%20solve%20MAMP%20tasks%20in%20real-time.%20We%0Ademonstrate%20that%20our%20method%20is%20both%20scalable%20and%20data-efficient%2C%20enabling%20the%0Asolution%20of%20MAMP%20problems%20in%20higher-dimensional%20scenarios%20with%20complex%0Acollision%20constraints.%20Our%20approach%20generalizes%20across%20various%20dynamical%0Asystems%2C%20including%20a%2012-dimensional%20dual-arm%20setup%2C%20and%20outperforms%20a%20range%20of%0Astate-of-the-art%20techniques%20in%20successfully%20addressing%20challenging%20MAMP%20tasks.%0AVideo%20demonstrations%20are%20available%20at%20https%3A//youtu.be/IZiePX0p1Mc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeHMO%253A%2520Neural%2520Hamilton-Jacobi%2520Reachability%2520Learning%2520for%2520Decentralized%250A%2520%2520Safe%2520Multi-Agent%2520Motion%2520Planning%26entry.906535625%3DQingyi%2520Chen%2520and%2520Ahmed%2520H.%2520Qureshi%26entry.1292438233%3D%2520%2520Safe%2520Multi-Agent%2520Motion%2520Planning%2520%2528MAMP%2529%2520is%2520a%2520significant%2520challenge%2520in%250Arobotics.%2520Despite%2520substantial%2520advancements%252C%2520existing%2520methods%2520often%2520face%2520a%250Adilemma.%2520Decentralized%2520algorithms%2520typically%2520rely%2520on%2520predicting%2520the%2520behavior%2520of%250Aother%2520agents%252C%2520sharing%2520contracts%252C%2520or%2520maintaining%2520communication%2520for%2520safety%252C%2520while%250Acentralized%2520approaches%2520struggle%2520with%2520scalability%2520and%2520real-time%2520decision-making.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Neural%2520Hamilton-Jacobi%2520Reachability%250ALearning%2520%2528HJR%2529%2520for%2520Decentralized%2520Multi-Agent%2520Motion%2520Planning.%2520Our%2520method%250Aprovides%2520scalable%2520neural%2520HJR%2520modeling%2520to%2520tackle%2520high-dimensional%2520configuration%250Aspaces%2520and%2520capture%2520worst-case%2520collision%2520and%2520safety%2520constraints%2520between%2520agents.%250AWe%2520further%2520propose%2520a%2520decentralized%2520trajectory%2520optimization%2520framework%2520that%250Aincorporates%2520the%2520learned%2520HJR%2520solutions%2520to%2520solve%2520MAMP%2520tasks%2520in%2520real-time.%2520We%250Ademonstrate%2520that%2520our%2520method%2520is%2520both%2520scalable%2520and%2520data-efficient%252C%2520enabling%2520the%250Asolution%2520of%2520MAMP%2520problems%2520in%2520higher-dimensional%2520scenarios%2520with%2520complex%250Acollision%2520constraints.%2520Our%2520approach%2520generalizes%2520across%2520various%2520dynamical%250Asystems%252C%2520including%2520a%252012-dimensional%2520dual-arm%2520setup%252C%2520and%2520outperforms%2520a%2520range%2520of%250Astate-of-the-art%2520techniques%2520in%2520successfully%2520addressing%2520challenging%2520MAMP%2520tasks.%250AVideo%2520demonstrations%2520are%2520available%2520at%2520https%253A//youtu.be/IZiePX0p1Mc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeHMO%3A%20Neural%20Hamilton-Jacobi%20Reachability%20Learning%20for%20Decentralized%0A%20%20Safe%20Multi-Agent%20Motion%20Planning&entry.906535625=Qingyi%20Chen%20and%20Ahmed%20H.%20Qureshi&entry.1292438233=%20%20Safe%20Multi-Agent%20Motion%20Planning%20%28MAMP%29%20is%20a%20significant%20challenge%20in%0Arobotics.%20Despite%20substantial%20advancements%2C%20existing%20methods%20often%20face%20a%0Adilemma.%20Decentralized%20algorithms%20typically%20rely%20on%20predicting%20the%20behavior%20of%0Aother%20agents%2C%20sharing%20contracts%2C%20or%20maintaining%20communication%20for%20safety%2C%20while%0Acentralized%20approaches%20struggle%20with%20scalability%20and%20real-time%20decision-making.%0ATo%20address%20these%20challenges%2C%20we%20introduce%20Neural%20Hamilton-Jacobi%20Reachability%0ALearning%20%28HJR%29%20for%20Decentralized%20Multi-Agent%20Motion%20Planning.%20Our%20method%0Aprovides%20scalable%20neural%20HJR%20modeling%20to%20tackle%20high-dimensional%20configuration%0Aspaces%20and%20capture%20worst-case%20collision%20and%20safety%20constraints%20between%20agents.%0AWe%20further%20propose%20a%20decentralized%20trajectory%20optimization%20framework%20that%0Aincorporates%20the%20learned%20HJR%20solutions%20to%20solve%20MAMP%20tasks%20in%20real-time.%20We%0Ademonstrate%20that%20our%20method%20is%20both%20scalable%20and%20data-efficient%2C%20enabling%20the%0Asolution%20of%20MAMP%20problems%20in%20higher-dimensional%20scenarios%20with%20complex%0Acollision%20constraints.%20Our%20approach%20generalizes%20across%20various%20dynamical%0Asystems%2C%20including%20a%2012-dimensional%20dual-arm%20setup%2C%20and%20outperforms%20a%20range%20of%0Astate-of-the-art%20techniques%20in%20successfully%20addressing%20challenging%20MAMP%20tasks.%0AVideo%20demonstrations%20are%20available%20at%20https%3A//youtu.be/IZiePX0p1Mc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13940v1&entry.124074799=Read"},
{"title": "Large Language Models as Innovators: A Framework to Leverage Latent\n  Space Exploration for Novelty Discovery", "author": "Mateusz Bystro\u0144ski and Miko\u0142aj Ho\u0142ysz and Grzegorz Piotrowski and Nitesh V. Chawla and Tomasz Kajdanowicz", "abstract": "  Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.\n", "link": "http://arxiv.org/abs/2507.13874v1", "date": "2025-07-18", "relevancy": 2.1956, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5593}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20as%20Innovators%3A%20A%20Framework%20to%20Leverage%20Latent%0A%20%20Space%20Exploration%20for%20Novelty%20Discovery&body=Title%3A%20Large%20Language%20Models%20as%20Innovators%3A%20A%20Framework%20to%20Leverage%20Latent%0A%20%20Space%20Exploration%20for%20Novelty%20Discovery%0AAuthor%3A%20Mateusz%20Bystro%C5%84ski%20and%20Miko%C5%82aj%20Ho%C5%82ysz%20and%20Grzegorz%20Piotrowski%20and%20Nitesh%20V.%20Chawla%20and%20Tomasz%20Kajdanowicz%0AAbstract%3A%20%20%20Innovative%20idea%20generation%20remains%20a%20core%20challenge%20in%20AI%2C%20as%20large%20language%0Amodels%20%28LLMs%29%20often%20struggle%20to%20produce%20outputs%20that%20are%20both%20novel%20and%0Arelevant.%20Despite%20their%20fluency%2C%20LLMs%20tend%20to%20replicate%20patterns%20seen%20during%0Atraining%2C%20limiting%20their%20ability%20to%20diverge%20creatively%20without%20extensive%20prompt%0Aengineering.%20Prior%20work%20has%20addressed%20this%20through%20domain-specific%20heuristics%0Aand%20structured%20prompting%20pipelines%2C%20but%20such%20solutions%20are%20brittle%20and%0Adifficult%20to%20generalize.%20In%20this%20paper%2C%20we%20propose%20a%20model-agnostic%0Alatent-space%20ideation%20framework%20that%20enables%20controlled%2C%20scalable%20creativity%20by%0Anavigating%20the%20continuous%20embedding%20space%20of%20ideas.%20Unlike%20prior%20methods%2C%20our%0Aframework%20requires%20no%20handcrafted%20rules%20and%20adapts%20easily%20to%20different%20domains%2C%0Ainput%20formats%2C%20and%20creative%20tasks.%20This%20paper%20introduces%20an%20early-stage%0Aprototype%20of%20our%20method%2C%20outlining%20the%20conceptual%20framework%20and%20preliminary%0Aresults%20highlighting%20its%20potential%20as%20a%20general-purpose%20co-ideator%20for%20human-AI%0Acollaboration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13874v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520as%2520Innovators%253A%2520A%2520Framework%2520to%2520Leverage%2520Latent%250A%2520%2520Space%2520Exploration%2520for%2520Novelty%2520Discovery%26entry.906535625%3DMateusz%2520Bystro%25C5%2584ski%2520and%2520Miko%25C5%2582aj%2520Ho%25C5%2582ysz%2520and%2520Grzegorz%2520Piotrowski%2520and%2520Nitesh%2520V.%2520Chawla%2520and%2520Tomasz%2520Kajdanowicz%26entry.1292438233%3D%2520%2520Innovative%2520idea%2520generation%2520remains%2520a%2520core%2520challenge%2520in%2520AI%252C%2520as%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520often%2520struggle%2520to%2520produce%2520outputs%2520that%2520are%2520both%2520novel%2520and%250Arelevant.%2520Despite%2520their%2520fluency%252C%2520LLMs%2520tend%2520to%2520replicate%2520patterns%2520seen%2520during%250Atraining%252C%2520limiting%2520their%2520ability%2520to%2520diverge%2520creatively%2520without%2520extensive%2520prompt%250Aengineering.%2520Prior%2520work%2520has%2520addressed%2520this%2520through%2520domain-specific%2520heuristics%250Aand%2520structured%2520prompting%2520pipelines%252C%2520but%2520such%2520solutions%2520are%2520brittle%2520and%250Adifficult%2520to%2520generalize.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520model-agnostic%250Alatent-space%2520ideation%2520framework%2520that%2520enables%2520controlled%252C%2520scalable%2520creativity%2520by%250Anavigating%2520the%2520continuous%2520embedding%2520space%2520of%2520ideas.%2520Unlike%2520prior%2520methods%252C%2520our%250Aframework%2520requires%2520no%2520handcrafted%2520rules%2520and%2520adapts%2520easily%2520to%2520different%2520domains%252C%250Ainput%2520formats%252C%2520and%2520creative%2520tasks.%2520This%2520paper%2520introduces%2520an%2520early-stage%250Aprototype%2520of%2520our%2520method%252C%2520outlining%2520the%2520conceptual%2520framework%2520and%2520preliminary%250Aresults%2520highlighting%2520its%2520potential%2520as%2520a%2520general-purpose%2520co-ideator%2520for%2520human-AI%250Acollaboration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13874v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20as%20Innovators%3A%20A%20Framework%20to%20Leverage%20Latent%0A%20%20Space%20Exploration%20for%20Novelty%20Discovery&entry.906535625=Mateusz%20Bystro%C5%84ski%20and%20Miko%C5%82aj%20Ho%C5%82ysz%20and%20Grzegorz%20Piotrowski%20and%20Nitesh%20V.%20Chawla%20and%20Tomasz%20Kajdanowicz&entry.1292438233=%20%20Innovative%20idea%20generation%20remains%20a%20core%20challenge%20in%20AI%2C%20as%20large%20language%0Amodels%20%28LLMs%29%20often%20struggle%20to%20produce%20outputs%20that%20are%20both%20novel%20and%0Arelevant.%20Despite%20their%20fluency%2C%20LLMs%20tend%20to%20replicate%20patterns%20seen%20during%0Atraining%2C%20limiting%20their%20ability%20to%20diverge%20creatively%20without%20extensive%20prompt%0Aengineering.%20Prior%20work%20has%20addressed%20this%20through%20domain-specific%20heuristics%0Aand%20structured%20prompting%20pipelines%2C%20but%20such%20solutions%20are%20brittle%20and%0Adifficult%20to%20generalize.%20In%20this%20paper%2C%20we%20propose%20a%20model-agnostic%0Alatent-space%20ideation%20framework%20that%20enables%20controlled%2C%20scalable%20creativity%20by%0Anavigating%20the%20continuous%20embedding%20space%20of%20ideas.%20Unlike%20prior%20methods%2C%20our%0Aframework%20requires%20no%20handcrafted%20rules%20and%20adapts%20easily%20to%20different%20domains%2C%0Ainput%20formats%2C%20and%20creative%20tasks.%20This%20paper%20introduces%20an%20early-stage%0Aprototype%20of%20our%20method%2C%20outlining%20the%20conceptual%20framework%20and%20preliminary%0Aresults%20highlighting%20its%20potential%20as%20a%20general-purpose%20co-ideator%20for%20human-AI%0Acollaboration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13874v1&entry.124074799=Read"},
{"title": "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian", "author": "Wannita Takerngsaksiri and Chakkrit Tantithamthavorn and Micheal Fu and Jirat Pasuksmit and Kun Chen and Ming Wu", "abstract": "  Software engineers spend a significant amount of time reading code during the\nsoftware development process, especially in the age of large language models\n(LLMs) that can automatically generate code. However, little is known about the\nreadability of the LLM-generated code and whether it is still important from\npractitioners' perspectives in this new era. In this paper, we conduct a survey\nto explore the practitioners' perspectives on code readability in the age of\nLLMs and investigate the readability of our LLM-based software development\nagents framework, HULA, by comparing its generated code with human-written code\nin real-world scenarios. Overall, the findings underscore that (1) readability\nremains a critical aspect of software development; (2) the readability of our\nLLM-generated code is comparable to human-written code, fostering the\nestablishment of appropriate trust and driving the broad adoption of our\nLLM-powered software development platform.\n", "link": "http://arxiv.org/abs/2501.11264v3", "date": "2025-07-18", "relevancy": 2.1876, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4416}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code%20Readability%20in%20the%20Age%20of%20Large%20Language%20Models%3A%20An%20Industrial%20Case%0A%20%20Study%20from%20Atlassian&body=Title%3A%20Code%20Readability%20in%20the%20Age%20of%20Large%20Language%20Models%3A%20An%20Industrial%20Case%0A%20%20Study%20from%20Atlassian%0AAuthor%3A%20Wannita%20Takerngsaksiri%20and%20Chakkrit%20Tantithamthavorn%20and%20Micheal%20Fu%20and%20Jirat%20Pasuksmit%20and%20Kun%20Chen%20and%20Ming%20Wu%0AAbstract%3A%20%20%20Software%20engineers%20spend%20a%20significant%20amount%20of%20time%20reading%20code%20during%20the%0Asoftware%20development%20process%2C%20especially%20in%20the%20age%20of%20large%20language%20models%0A%28LLMs%29%20that%20can%20automatically%20generate%20code.%20However%2C%20little%20is%20known%20about%20the%0Areadability%20of%20the%20LLM-generated%20code%20and%20whether%20it%20is%20still%20important%20from%0Apractitioners%27%20perspectives%20in%20this%20new%20era.%20In%20this%20paper%2C%20we%20conduct%20a%20survey%0Ato%20explore%20the%20practitioners%27%20perspectives%20on%20code%20readability%20in%20the%20age%20of%0ALLMs%20and%20investigate%20the%20readability%20of%20our%20LLM-based%20software%20development%0Aagents%20framework%2C%20HULA%2C%20by%20comparing%20its%20generated%20code%20with%20human-written%20code%0Ain%20real-world%20scenarios.%20Overall%2C%20the%20findings%20underscore%20that%20%281%29%20readability%0Aremains%20a%20critical%20aspect%20of%20software%20development%3B%20%282%29%20the%20readability%20of%20our%0ALLM-generated%20code%20is%20comparable%20to%20human-written%20code%2C%20fostering%20the%0Aestablishment%20of%20appropriate%20trust%20and%20driving%20the%20broad%20adoption%20of%20our%0ALLM-powered%20software%20development%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11264v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode%2520Readability%2520in%2520the%2520Age%2520of%2520Large%2520Language%2520Models%253A%2520An%2520Industrial%2520Case%250A%2520%2520Study%2520from%2520Atlassian%26entry.906535625%3DWannita%2520Takerngsaksiri%2520and%2520Chakkrit%2520Tantithamthavorn%2520and%2520Micheal%2520Fu%2520and%2520Jirat%2520Pasuksmit%2520and%2520Kun%2520Chen%2520and%2520Ming%2520Wu%26entry.1292438233%3D%2520%2520Software%2520engineers%2520spend%2520a%2520significant%2520amount%2520of%2520time%2520reading%2520code%2520during%2520the%250Asoftware%2520development%2520process%252C%2520especially%2520in%2520the%2520age%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520that%2520can%2520automatically%2520generate%2520code.%2520However%252C%2520little%2520is%2520known%2520about%2520the%250Areadability%2520of%2520the%2520LLM-generated%2520code%2520and%2520whether%2520it%2520is%2520still%2520important%2520from%250Apractitioners%2527%2520perspectives%2520in%2520this%2520new%2520era.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520survey%250Ato%2520explore%2520the%2520practitioners%2527%2520perspectives%2520on%2520code%2520readability%2520in%2520the%2520age%2520of%250ALLMs%2520and%2520investigate%2520the%2520readability%2520of%2520our%2520LLM-based%2520software%2520development%250Aagents%2520framework%252C%2520HULA%252C%2520by%2520comparing%2520its%2520generated%2520code%2520with%2520human-written%2520code%250Ain%2520real-world%2520scenarios.%2520Overall%252C%2520the%2520findings%2520underscore%2520that%2520%25281%2529%2520readability%250Aremains%2520a%2520critical%2520aspect%2520of%2520software%2520development%253B%2520%25282%2529%2520the%2520readability%2520of%2520our%250ALLM-generated%2520code%2520is%2520comparable%2520to%2520human-written%2520code%252C%2520fostering%2520the%250Aestablishment%2520of%2520appropriate%2520trust%2520and%2520driving%2520the%2520broad%2520adoption%2520of%2520our%250ALLM-powered%2520software%2520development%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11264v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code%20Readability%20in%20the%20Age%20of%20Large%20Language%20Models%3A%20An%20Industrial%20Case%0A%20%20Study%20from%20Atlassian&entry.906535625=Wannita%20Takerngsaksiri%20and%20Chakkrit%20Tantithamthavorn%20and%20Micheal%20Fu%20and%20Jirat%20Pasuksmit%20and%20Kun%20Chen%20and%20Ming%20Wu&entry.1292438233=%20%20Software%20engineers%20spend%20a%20significant%20amount%20of%20time%20reading%20code%20during%20the%0Asoftware%20development%20process%2C%20especially%20in%20the%20age%20of%20large%20language%20models%0A%28LLMs%29%20that%20can%20automatically%20generate%20code.%20However%2C%20little%20is%20known%20about%20the%0Areadability%20of%20the%20LLM-generated%20code%20and%20whether%20it%20is%20still%20important%20from%0Apractitioners%27%20perspectives%20in%20this%20new%20era.%20In%20this%20paper%2C%20we%20conduct%20a%20survey%0Ato%20explore%20the%20practitioners%27%20perspectives%20on%20code%20readability%20in%20the%20age%20of%0ALLMs%20and%20investigate%20the%20readability%20of%20our%20LLM-based%20software%20development%0Aagents%20framework%2C%20HULA%2C%20by%20comparing%20its%20generated%20code%20with%20human-written%20code%0Ain%20real-world%20scenarios.%20Overall%2C%20the%20findings%20underscore%20that%20%281%29%20readability%0Aremains%20a%20critical%20aspect%20of%20software%20development%3B%20%282%29%20the%20readability%20of%20our%0ALLM-generated%20code%20is%20comparable%20to%20human-written%20code%2C%20fostering%20the%0Aestablishment%20of%20appropriate%20trust%20and%20driving%20the%20broad%20adoption%20of%20our%0ALLM-powered%20software%20development%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11264v3&entry.124074799=Read"},
{"title": "EdgeVLA: Efficient Vision-Language-Action Models", "author": "Pawe\u0142 Budzianowski and Wesley Maa and Matthew Freed and Jingxiang Mo and Winston Hsiao and Aaron Xie and Tomasz M\u0142oduchowski and Viraj Tipnis and Benjamin Bolte", "abstract": "  Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.\n", "link": "http://arxiv.org/abs/2507.14049v1", "date": "2025-07-18", "relevancy": 2.1874, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeVLA%3A%20Efficient%20Vision-Language-Action%20Models&body=Title%3A%20EdgeVLA%3A%20Efficient%20Vision-Language-Action%20Models%0AAuthor%3A%20Pawe%C5%82%20Budzianowski%20and%20Wesley%20Maa%20and%20Matthew%20Freed%20and%20Jingxiang%20Mo%20and%20Winston%20Hsiao%20and%20Aaron%20Xie%20and%20Tomasz%20M%C5%82oduchowski%20and%20Viraj%20Tipnis%20and%20Benjamin%20Bolte%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20a%20promising%20approach%20to%20address%0Athe%20data%20scarcity%20challenge%20in%20robotics%2C%20enabling%20the%20development%20of%0Ageneralizable%20visuomotor%20control%20policies.%20While%20models%20like%20OpenVLA%20showcase%0Athe%20potential%20of%20this%20paradigm%2C%20deploying%20large-scale%20VLMs%20on%0Aresource-constrained%20mobile%20manipulation%20systems%20remains%20a%20significant%20hurdle.%0AThis%20paper%20introduces%20Edge%20VLA%20%28EVLA%29%2C%20a%20novel%20approach%20designed%20to%0Asignificantly%20enhance%20the%20inference%20speed%20of%20Vision-Language-Action%20%28VLA%29%0Amodels.%20EVLA%20maintains%20the%20representational%20power%20of%20these%20models%20while%0Aenabling%20real-time%20performance%20on%20edge%20devices.%20We%20achieve%20this%20through%20two%20key%0Ainnovations%3A%201%29%20Eliminating%20the%20autoregressive%20requirement%20for%20end-effector%0Aposition%20prediction%2C%20leading%20to%20a%207x%20speedup%20in%20inference%2C%20and%202%29%20Leveraging%0Athe%20efficiency%20of%20Small%20Language%20Models%20%28SLMs%29%2C%20demonstrating%20comparable%0Atraining%20performance%20to%20larger%20models%20with%20significantly%20reduced%20computational%0Ademands.%20Our%20early%20results%20demonstrate%20that%20EVLA%20achieves%20comparable%20training%0Acharacteristics%20to%20OpenVLA%20while%20offering%20substantial%20gains%20in%20inference%20speed%0Aand%20memory%20efficiency.%20We%20release%20our%20model%20checkpoints%20and%20training%0A%5Chref%7Bhttps%3A//github.com/kscalelabs/evla%20%7D%7Bcodebase%7D%20to%20foster%20further%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeVLA%253A%2520Efficient%2520Vision-Language-Action%2520Models%26entry.906535625%3DPawe%25C5%2582%2520Budzianowski%2520and%2520Wesley%2520Maa%2520and%2520Matthew%2520Freed%2520and%2520Jingxiang%2520Mo%2520and%2520Winston%2520Hsiao%2520and%2520Aaron%2520Xie%2520and%2520Tomasz%2520M%25C5%2582oduchowski%2520and%2520Viraj%2520Tipnis%2520and%2520Benjamin%2520Bolte%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520address%250Athe%2520data%2520scarcity%2520challenge%2520in%2520robotics%252C%2520enabling%2520the%2520development%2520of%250Ageneralizable%2520visuomotor%2520control%2520policies.%2520While%2520models%2520like%2520OpenVLA%2520showcase%250Athe%2520potential%2520of%2520this%2520paradigm%252C%2520deploying%2520large-scale%2520VLMs%2520on%250Aresource-constrained%2520mobile%2520manipulation%2520systems%2520remains%2520a%2520significant%2520hurdle.%250AThis%2520paper%2520introduces%2520Edge%2520VLA%2520%2528EVLA%2529%252C%2520a%2520novel%2520approach%2520designed%2520to%250Asignificantly%2520enhance%2520the%2520inference%2520speed%2520of%2520Vision-Language-Action%2520%2528VLA%2529%250Amodels.%2520EVLA%2520maintains%2520the%2520representational%2520power%2520of%2520these%2520models%2520while%250Aenabling%2520real-time%2520performance%2520on%2520edge%2520devices.%2520We%2520achieve%2520this%2520through%2520two%2520key%250Ainnovations%253A%25201%2529%2520Eliminating%2520the%2520autoregressive%2520requirement%2520for%2520end-effector%250Aposition%2520prediction%252C%2520leading%2520to%2520a%25207x%2520speedup%2520in%2520inference%252C%2520and%25202%2529%2520Leveraging%250Athe%2520efficiency%2520of%2520Small%2520Language%2520Models%2520%2528SLMs%2529%252C%2520demonstrating%2520comparable%250Atraining%2520performance%2520to%2520larger%2520models%2520with%2520significantly%2520reduced%2520computational%250Ademands.%2520Our%2520early%2520results%2520demonstrate%2520that%2520EVLA%2520achieves%2520comparable%2520training%250Acharacteristics%2520to%2520OpenVLA%2520while%2520offering%2520substantial%2520gains%2520in%2520inference%2520speed%250Aand%2520memory%2520efficiency.%2520We%2520release%2520our%2520model%2520checkpoints%2520and%2520training%250A%255Chref%257Bhttps%253A//github.com/kscalelabs/evla%2520%257D%257Bcodebase%257D%2520to%2520foster%2520further%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeVLA%3A%20Efficient%20Vision-Language-Action%20Models&entry.906535625=Pawe%C5%82%20Budzianowski%20and%20Wesley%20Maa%20and%20Matthew%20Freed%20and%20Jingxiang%20Mo%20and%20Winston%20Hsiao%20and%20Aaron%20Xie%20and%20Tomasz%20M%C5%82oduchowski%20and%20Viraj%20Tipnis%20and%20Benjamin%20Bolte&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20a%20promising%20approach%20to%20address%0Athe%20data%20scarcity%20challenge%20in%20robotics%2C%20enabling%20the%20development%20of%0Ageneralizable%20visuomotor%20control%20policies.%20While%20models%20like%20OpenVLA%20showcase%0Athe%20potential%20of%20this%20paradigm%2C%20deploying%20large-scale%20VLMs%20on%0Aresource-constrained%20mobile%20manipulation%20systems%20remains%20a%20significant%20hurdle.%0AThis%20paper%20introduces%20Edge%20VLA%20%28EVLA%29%2C%20a%20novel%20approach%20designed%20to%0Asignificantly%20enhance%20the%20inference%20speed%20of%20Vision-Language-Action%20%28VLA%29%0Amodels.%20EVLA%20maintains%20the%20representational%20power%20of%20these%20models%20while%0Aenabling%20real-time%20performance%20on%20edge%20devices.%20We%20achieve%20this%20through%20two%20key%0Ainnovations%3A%201%29%20Eliminating%20the%20autoregressive%20requirement%20for%20end-effector%0Aposition%20prediction%2C%20leading%20to%20a%207x%20speedup%20in%20inference%2C%20and%202%29%20Leveraging%0Athe%20efficiency%20of%20Small%20Language%20Models%20%28SLMs%29%2C%20demonstrating%20comparable%0Atraining%20performance%20to%20larger%20models%20with%20significantly%20reduced%20computational%0Ademands.%20Our%20early%20results%20demonstrate%20that%20EVLA%20achieves%20comparable%20training%0Acharacteristics%20to%20OpenVLA%20while%20offering%20substantial%20gains%20in%20inference%20speed%0Aand%20memory%20efficiency.%20We%20release%20our%20model%20checkpoints%20and%20training%0A%5Chref%7Bhttps%3A//github.com/kscalelabs/evla%20%7D%7Bcodebase%7D%20to%20foster%20further%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14049v1&entry.124074799=Read"},
{"title": "Visual Grounding Methods for Efficient Interaction with Desktop\n  Graphical User Interfaces", "author": "El Hassane Ettifouri and Jessica L\u00f3pez Espejel and Laura Minkova and Tassnim Dardouri and Walid Dahhane", "abstract": "  Most visual grounding solutions primarily focus on realistic images. However,\napplications involving synthetic images, such as Graphical User Interfaces\n(GUIs), remain limited. This restricts the development of autonomous computer\nvision-powered artificial intelligence (AI) agents for automatic application\ninteraction. Enabling AI to effectively understand and interact with GUIs is\ncrucial to advancing automation in software testing, accessibility, and\nhuman-computer interaction. In this work, we explore Instruction Visual\nGrounding (IVG), a multi-modal approach to object identification within a GUI.\nMore precisely, given a natural language instruction and a GUI screen, IVG\nlocates the coordinates of the element on the screen where the instruction\nshould be executed. We propose two main methods: (1) IVGocr, which combines a\nLarge Language Model (LLM), an object detection model, and an Optical Character\nRecognition (OCR) module; and (2) IVGdirect, which uses a multimodal\narchitecture for end-to-end grounding. For each method, we introduce a\ndedicated dataset. In addition, we propose the Central Point Validation (CPV)\nmetric, a relaxed variant of the classical Central Proximity Score (CPS)\nmetric. Our final test dataset is publicly released to support future research.\n", "link": "http://arxiv.org/abs/2407.01558v3", "date": "2025-07-18", "relevancy": 2.1867, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5568}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5435}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Grounding%20Methods%20for%20Efficient%20Interaction%20with%20Desktop%0A%20%20Graphical%20User%20Interfaces&body=Title%3A%20Visual%20Grounding%20Methods%20for%20Efficient%20Interaction%20with%20Desktop%0A%20%20Graphical%20User%20Interfaces%0AAuthor%3A%20El%20Hassane%20Ettifouri%20and%20Jessica%20L%C3%B3pez%20Espejel%20and%20Laura%20Minkova%20and%20Tassnim%20Dardouri%20and%20Walid%20Dahhane%0AAbstract%3A%20%20%20Most%20visual%20grounding%20solutions%20primarily%20focus%20on%20realistic%20images.%20However%2C%0Aapplications%20involving%20synthetic%20images%2C%20such%20as%20Graphical%20User%20Interfaces%0A%28GUIs%29%2C%20remain%20limited.%20This%20restricts%20the%20development%20of%20autonomous%20computer%0Avision-powered%20artificial%20intelligence%20%28AI%29%20agents%20for%20automatic%20application%0Ainteraction.%20Enabling%20AI%20to%20effectively%20understand%20and%20interact%20with%20GUIs%20is%0Acrucial%20to%20advancing%20automation%20in%20software%20testing%2C%20accessibility%2C%20and%0Ahuman-computer%20interaction.%20In%20this%20work%2C%20we%20explore%20Instruction%20Visual%0AGrounding%20%28IVG%29%2C%20a%20multi-modal%20approach%20to%20object%20identification%20within%20a%20GUI.%0AMore%20precisely%2C%20given%20a%20natural%20language%20instruction%20and%20a%20GUI%20screen%2C%20IVG%0Alocates%20the%20coordinates%20of%20the%20element%20on%20the%20screen%20where%20the%20instruction%0Ashould%20be%20executed.%20We%20propose%20two%20main%20methods%3A%20%281%29%20IVGocr%2C%20which%20combines%20a%0ALarge%20Language%20Model%20%28LLM%29%2C%20an%20object%20detection%20model%2C%20and%20an%20Optical%20Character%0ARecognition%20%28OCR%29%20module%3B%20and%20%282%29%20IVGdirect%2C%20which%20uses%20a%20multimodal%0Aarchitecture%20for%20end-to-end%20grounding.%20For%20each%20method%2C%20we%20introduce%20a%0Adedicated%20dataset.%20In%20addition%2C%20we%20propose%20the%20Central%20Point%20Validation%20%28CPV%29%0Ametric%2C%20a%20relaxed%20variant%20of%20the%20classical%20Central%20Proximity%20Score%20%28CPS%29%0Ametric.%20Our%20final%20test%20dataset%20is%20publicly%20released%20to%20support%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01558v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Grounding%2520Methods%2520for%2520Efficient%2520Interaction%2520with%2520Desktop%250A%2520%2520Graphical%2520User%2520Interfaces%26entry.906535625%3DEl%2520Hassane%2520Ettifouri%2520and%2520Jessica%2520L%25C3%25B3pez%2520Espejel%2520and%2520Laura%2520Minkova%2520and%2520Tassnim%2520Dardouri%2520and%2520Walid%2520Dahhane%26entry.1292438233%3D%2520%2520Most%2520visual%2520grounding%2520solutions%2520primarily%2520focus%2520on%2520realistic%2520images.%2520However%252C%250Aapplications%2520involving%2520synthetic%2520images%252C%2520such%2520as%2520Graphical%2520User%2520Interfaces%250A%2528GUIs%2529%252C%2520remain%2520limited.%2520This%2520restricts%2520the%2520development%2520of%2520autonomous%2520computer%250Avision-powered%2520artificial%2520intelligence%2520%2528AI%2529%2520agents%2520for%2520automatic%2520application%250Ainteraction.%2520Enabling%2520AI%2520to%2520effectively%2520understand%2520and%2520interact%2520with%2520GUIs%2520is%250Acrucial%2520to%2520advancing%2520automation%2520in%2520software%2520testing%252C%2520accessibility%252C%2520and%250Ahuman-computer%2520interaction.%2520In%2520this%2520work%252C%2520we%2520explore%2520Instruction%2520Visual%250AGrounding%2520%2528IVG%2529%252C%2520a%2520multi-modal%2520approach%2520to%2520object%2520identification%2520within%2520a%2520GUI.%250AMore%2520precisely%252C%2520given%2520a%2520natural%2520language%2520instruction%2520and%2520a%2520GUI%2520screen%252C%2520IVG%250Alocates%2520the%2520coordinates%2520of%2520the%2520element%2520on%2520the%2520screen%2520where%2520the%2520instruction%250Ashould%2520be%2520executed.%2520We%2520propose%2520two%2520main%2520methods%253A%2520%25281%2529%2520IVGocr%252C%2520which%2520combines%2520a%250ALarge%2520Language%2520Model%2520%2528LLM%2529%252C%2520an%2520object%2520detection%2520model%252C%2520and%2520an%2520Optical%2520Character%250ARecognition%2520%2528OCR%2529%2520module%253B%2520and%2520%25282%2529%2520IVGdirect%252C%2520which%2520uses%2520a%2520multimodal%250Aarchitecture%2520for%2520end-to-end%2520grounding.%2520For%2520each%2520method%252C%2520we%2520introduce%2520a%250Adedicated%2520dataset.%2520In%2520addition%252C%2520we%2520propose%2520the%2520Central%2520Point%2520Validation%2520%2528CPV%2529%250Ametric%252C%2520a%2520relaxed%2520variant%2520of%2520the%2520classical%2520Central%2520Proximity%2520Score%2520%2528CPS%2529%250Ametric.%2520Our%2520final%2520test%2520dataset%2520is%2520publicly%2520released%2520to%2520support%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01558v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Grounding%20Methods%20for%20Efficient%20Interaction%20with%20Desktop%0A%20%20Graphical%20User%20Interfaces&entry.906535625=El%20Hassane%20Ettifouri%20and%20Jessica%20L%C3%B3pez%20Espejel%20and%20Laura%20Minkova%20and%20Tassnim%20Dardouri%20and%20Walid%20Dahhane&entry.1292438233=%20%20Most%20visual%20grounding%20solutions%20primarily%20focus%20on%20realistic%20images.%20However%2C%0Aapplications%20involving%20synthetic%20images%2C%20such%20as%20Graphical%20User%20Interfaces%0A%28GUIs%29%2C%20remain%20limited.%20This%20restricts%20the%20development%20of%20autonomous%20computer%0Avision-powered%20artificial%20intelligence%20%28AI%29%20agents%20for%20automatic%20application%0Ainteraction.%20Enabling%20AI%20to%20effectively%20understand%20and%20interact%20with%20GUIs%20is%0Acrucial%20to%20advancing%20automation%20in%20software%20testing%2C%20accessibility%2C%20and%0Ahuman-computer%20interaction.%20In%20this%20work%2C%20we%20explore%20Instruction%20Visual%0AGrounding%20%28IVG%29%2C%20a%20multi-modal%20approach%20to%20object%20identification%20within%20a%20GUI.%0AMore%20precisely%2C%20given%20a%20natural%20language%20instruction%20and%20a%20GUI%20screen%2C%20IVG%0Alocates%20the%20coordinates%20of%20the%20element%20on%20the%20screen%20where%20the%20instruction%0Ashould%20be%20executed.%20We%20propose%20two%20main%20methods%3A%20%281%29%20IVGocr%2C%20which%20combines%20a%0ALarge%20Language%20Model%20%28LLM%29%2C%20an%20object%20detection%20model%2C%20and%20an%20Optical%20Character%0ARecognition%20%28OCR%29%20module%3B%20and%20%282%29%20IVGdirect%2C%20which%20uses%20a%20multimodal%0Aarchitecture%20for%20end-to-end%20grounding.%20For%20each%20method%2C%20we%20introduce%20a%0Adedicated%20dataset.%20In%20addition%2C%20we%20propose%20the%20Central%20Point%20Validation%20%28CPV%29%0Ametric%2C%20a%20relaxed%20variant%20of%20the%20classical%20Central%20Proximity%20Score%20%28CPS%29%0Ametric.%20Our%20final%20test%20dataset%20is%20publicly%20released%20to%20support%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01558v3&entry.124074799=Read"},
{"title": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with\n  Benchmarking Results", "author": "Dawar Khan and Xinyu Liu and Omar Mena and Donggang Jia and Alexandre Kouyoumdjian and Ivan Viola", "abstract": "  The deployment of large language models (LLMs) on extended reality (XR)\ndevices has great potential to advance the field of human-AI interaction. In\nthe case of direct, on-device model inference, selecting the appropriate model\nand device for specific tasks remains challenging. In this paper, we present\nAIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running\non XR devices. To demonstrate the framework, we deploy 17 selected LLMs across\nfour XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision\nPro, and conduct an extensive evaluation. Our experimental setup measures four\nkey metrics: performance consistency, processing speed, memory usage, and\nbattery consumption. For each of the 68 model-device pairs, we assess\nperformance under varying string lengths, batch sizes, and thread counts,\nanalyzing the trade-offs for real-time XR applications. We propose a unified\nevaluation method based on the 3D Pareto Optimality theory to select the\noptimal device-model pairs from quality and speed objectives. Additionally, we\ncompare the efficiency of on-device LLMs with client-server and cloud-based\nsetups, and evaluate their accuracy on two interactive tasks. We believe our\nfindings offer valuable insight to guide future optimization efforts for LLM\ndeployment on XR devices. Our evaluation method can be used as standard\ngroundwork for further research and development in this emerging field. The\nsource code and supplementary materials are available at:\nwww.nanovis.org/AIvaluateXR.html\n", "link": "http://arxiv.org/abs/2502.15761v2", "date": "2025-07-18", "relevancy": 2.1779, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIvaluateXR%3A%20An%20Evaluation%20Framework%20for%20on-Device%20AI%20in%20XR%20with%0A%20%20Benchmarking%20Results&body=Title%3A%20AIvaluateXR%3A%20An%20Evaluation%20Framework%20for%20on-Device%20AI%20in%20XR%20with%0A%20%20Benchmarking%20Results%0AAuthor%3A%20Dawar%20Khan%20and%20Xinyu%20Liu%20and%20Omar%20Mena%20and%20Donggang%20Jia%20and%20Alexandre%20Kouyoumdjian%20and%20Ivan%20Viola%0AAbstract%3A%20%20%20The%20deployment%20of%20large%20language%20models%20%28LLMs%29%20on%20extended%20reality%20%28XR%29%0Adevices%20has%20great%20potential%20to%20advance%20the%20field%20of%20human-AI%20interaction.%20In%0Athe%20case%20of%20direct%2C%20on-device%20model%20inference%2C%20selecting%20the%20appropriate%20model%0Aand%20device%20for%20specific%20tasks%20remains%20challenging.%20In%20this%20paper%2C%20we%20present%0AAIvaluateXR%2C%20a%20comprehensive%20evaluation%20framework%20for%20benchmarking%20LLMs%20running%0Aon%20XR%20devices.%20To%20demonstrate%20the%20framework%2C%20we%20deploy%2017%20selected%20LLMs%20across%0Afour%20XR%20platforms%3A%20Magic%20Leap%202%2C%20Meta%20Quest%203%2C%20Vivo%20X100s%20Pro%2C%20and%20Apple%20Vision%0APro%2C%20and%20conduct%20an%20extensive%20evaluation.%20Our%20experimental%20setup%20measures%20four%0Akey%20metrics%3A%20performance%20consistency%2C%20processing%20speed%2C%20memory%20usage%2C%20and%0Abattery%20consumption.%20For%20each%20of%20the%2068%20model-device%20pairs%2C%20we%20assess%0Aperformance%20under%20varying%20string%20lengths%2C%20batch%20sizes%2C%20and%20thread%20counts%2C%0Aanalyzing%20the%20trade-offs%20for%20real-time%20XR%20applications.%20We%20propose%20a%20unified%0Aevaluation%20method%20based%20on%20the%203D%20Pareto%20Optimality%20theory%20to%20select%20the%0Aoptimal%20device-model%20pairs%20from%20quality%20and%20speed%20objectives.%20Additionally%2C%20we%0Acompare%20the%20efficiency%20of%20on-device%20LLMs%20with%20client-server%20and%20cloud-based%0Asetups%2C%20and%20evaluate%20their%20accuracy%20on%20two%20interactive%20tasks.%20We%20believe%20our%0Afindings%20offer%20valuable%20insight%20to%20guide%20future%20optimization%20efforts%20for%20LLM%0Adeployment%20on%20XR%20devices.%20Our%20evaluation%20method%20can%20be%20used%20as%20standard%0Agroundwork%20for%20further%20research%20and%20development%20in%20this%20emerging%20field.%20The%0Asource%20code%20and%20supplementary%20materials%20are%20available%20at%3A%0Awww.nanovis.org/AIvaluateXR.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIvaluateXR%253A%2520An%2520Evaluation%2520Framework%2520for%2520on-Device%2520AI%2520in%2520XR%2520with%250A%2520%2520Benchmarking%2520Results%26entry.906535625%3DDawar%2520Khan%2520and%2520Xinyu%2520Liu%2520and%2520Omar%2520Mena%2520and%2520Donggang%2520Jia%2520and%2520Alexandre%2520Kouyoumdjian%2520and%2520Ivan%2520Viola%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520extended%2520reality%2520%2528XR%2529%250Adevices%2520has%2520great%2520potential%2520to%2520advance%2520the%2520field%2520of%2520human-AI%2520interaction.%2520In%250Athe%2520case%2520of%2520direct%252C%2520on-device%2520model%2520inference%252C%2520selecting%2520the%2520appropriate%2520model%250Aand%2520device%2520for%2520specific%2520tasks%2520remains%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520present%250AAIvaluateXR%252C%2520a%2520comprehensive%2520evaluation%2520framework%2520for%2520benchmarking%2520LLMs%2520running%250Aon%2520XR%2520devices.%2520To%2520demonstrate%2520the%2520framework%252C%2520we%2520deploy%252017%2520selected%2520LLMs%2520across%250Afour%2520XR%2520platforms%253A%2520Magic%2520Leap%25202%252C%2520Meta%2520Quest%25203%252C%2520Vivo%2520X100s%2520Pro%252C%2520and%2520Apple%2520Vision%250APro%252C%2520and%2520conduct%2520an%2520extensive%2520evaluation.%2520Our%2520experimental%2520setup%2520measures%2520four%250Akey%2520metrics%253A%2520performance%2520consistency%252C%2520processing%2520speed%252C%2520memory%2520usage%252C%2520and%250Abattery%2520consumption.%2520For%2520each%2520of%2520the%252068%2520model-device%2520pairs%252C%2520we%2520assess%250Aperformance%2520under%2520varying%2520string%2520lengths%252C%2520batch%2520sizes%252C%2520and%2520thread%2520counts%252C%250Aanalyzing%2520the%2520trade-offs%2520for%2520real-time%2520XR%2520applications.%2520We%2520propose%2520a%2520unified%250Aevaluation%2520method%2520based%2520on%2520the%25203D%2520Pareto%2520Optimality%2520theory%2520to%2520select%2520the%250Aoptimal%2520device-model%2520pairs%2520from%2520quality%2520and%2520speed%2520objectives.%2520Additionally%252C%2520we%250Acompare%2520the%2520efficiency%2520of%2520on-device%2520LLMs%2520with%2520client-server%2520and%2520cloud-based%250Asetups%252C%2520and%2520evaluate%2520their%2520accuracy%2520on%2520two%2520interactive%2520tasks.%2520We%2520believe%2520our%250Afindings%2520offer%2520valuable%2520insight%2520to%2520guide%2520future%2520optimization%2520efforts%2520for%2520LLM%250Adeployment%2520on%2520XR%2520devices.%2520Our%2520evaluation%2520method%2520can%2520be%2520used%2520as%2520standard%250Agroundwork%2520for%2520further%2520research%2520and%2520development%2520in%2520this%2520emerging%2520field.%2520The%250Asource%2520code%2520and%2520supplementary%2520materials%2520are%2520available%2520at%253A%250Awww.nanovis.org/AIvaluateXR.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIvaluateXR%3A%20An%20Evaluation%20Framework%20for%20on-Device%20AI%20in%20XR%20with%0A%20%20Benchmarking%20Results&entry.906535625=Dawar%20Khan%20and%20Xinyu%20Liu%20and%20Omar%20Mena%20and%20Donggang%20Jia%20and%20Alexandre%20Kouyoumdjian%20and%20Ivan%20Viola&entry.1292438233=%20%20The%20deployment%20of%20large%20language%20models%20%28LLMs%29%20on%20extended%20reality%20%28XR%29%0Adevices%20has%20great%20potential%20to%20advance%20the%20field%20of%20human-AI%20interaction.%20In%0Athe%20case%20of%20direct%2C%20on-device%20model%20inference%2C%20selecting%20the%20appropriate%20model%0Aand%20device%20for%20specific%20tasks%20remains%20challenging.%20In%20this%20paper%2C%20we%20present%0AAIvaluateXR%2C%20a%20comprehensive%20evaluation%20framework%20for%20benchmarking%20LLMs%20running%0Aon%20XR%20devices.%20To%20demonstrate%20the%20framework%2C%20we%20deploy%2017%20selected%20LLMs%20across%0Afour%20XR%20platforms%3A%20Magic%20Leap%202%2C%20Meta%20Quest%203%2C%20Vivo%20X100s%20Pro%2C%20and%20Apple%20Vision%0APro%2C%20and%20conduct%20an%20extensive%20evaluation.%20Our%20experimental%20setup%20measures%20four%0Akey%20metrics%3A%20performance%20consistency%2C%20processing%20speed%2C%20memory%20usage%2C%20and%0Abattery%20consumption.%20For%20each%20of%20the%2068%20model-device%20pairs%2C%20we%20assess%0Aperformance%20under%20varying%20string%20lengths%2C%20batch%20sizes%2C%20and%20thread%20counts%2C%0Aanalyzing%20the%20trade-offs%20for%20real-time%20XR%20applications.%20We%20propose%20a%20unified%0Aevaluation%20method%20based%20on%20the%203D%20Pareto%20Optimality%20theory%20to%20select%20the%0Aoptimal%20device-model%20pairs%20from%20quality%20and%20speed%20objectives.%20Additionally%2C%20we%0Acompare%20the%20efficiency%20of%20on-device%20LLMs%20with%20client-server%20and%20cloud-based%0Asetups%2C%20and%20evaluate%20their%20accuracy%20on%20two%20interactive%20tasks.%20We%20believe%20our%0Afindings%20offer%20valuable%20insight%20to%20guide%20future%20optimization%20efforts%20for%20LLM%0Adeployment%20on%20XR%20devices.%20Our%20evaluation%20method%20can%20be%20used%20as%20standard%0Agroundwork%20for%20further%20research%20and%20development%20in%20this%20emerging%20field.%20The%0Asource%20code%20and%20supplementary%20materials%20are%20available%20at%3A%0Awww.nanovis.org/AIvaluateXR.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15761v2&entry.124074799=Read"},
{"title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual\n  Backpropagation", "author": "Xiaowen Ma and Chenyang Lin and Yao Zhang and Volker Tresp and Yunpu Ma", "abstract": "  Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework.\n", "link": "http://arxiv.org/abs/2506.09046v2", "date": "2025-07-18", "relevancy": 2.157, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5569}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20Neural%20Networks%3A%20Self-Evolving%20Multi-Agent%20Systems%20via%20Textual%0A%20%20Backpropagation&body=Title%3A%20Agentic%20Neural%20Networks%3A%20Self-Evolving%20Multi-Agent%20Systems%20via%20Textual%0A%20%20Backpropagation%0AAuthor%3A%20Xiaowen%20Ma%20and%20Chenyang%20Lin%20and%20Yao%20Zhang%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Leveraging%20multiple%20Large%20Language%20Models%28LLMs%29%20has%20proven%20effective%20for%0Aaddressing%20complex%2C%20high-dimensional%20tasks%2C%20but%20current%20approaches%20often%20rely%0Aon%20static%2C%20manually%20engineered%20multi-agent%20configurations.%20To%20overcome%20these%0Aconstraints%2C%20we%20present%20the%20Agentic%20Neural%20Network%28ANN%29%2C%20a%20framework%20that%0Aconceptualizes%20multi-agent%20collaboration%20as%20a%20layered%20neural%20network%0Aarchitecture.%20In%20this%20design%2C%20each%20agent%20operates%20as%20a%20node%2C%20and%20each%20layer%0Aforms%20a%20cooperative%20%22team%22%20focused%20on%20a%20specific%20subtask.%20Agentic%20Neural%0ANetwork%20follows%20a%20two-phase%20optimization%20strategy%3A%20%281%29%20Forward%20Phase-Drawing%0Ainspiration%20from%20neural%20network%20forward%20passes%2C%20tasks%20are%20dynamically%0Adecomposed%20into%20subtasks%2C%20and%20cooperative%20agent%20teams%20with%20suitable%20aggregation%0Amethods%20are%20constructed%20layer%20by%20layer.%20%282%29%20Backward%20Phase-Mirroring%0Abackpropagation%2C%20we%20refine%20both%20global%20and%20local%20collaboration%20through%0Aiterative%20feedback%2C%20allowing%20agents%20to%20self-evolve%20their%20roles%2C%20prompts%2C%20and%0Acoordination.%20This%20neuro-symbolic%20approach%20enables%20ANN%20to%20create%20new%20or%0Aspecialized%20agent%20teams%20post-training%2C%20delivering%20notable%20gains%20in%20accuracy%20and%0Aadaptability.%20Across%20four%20benchmark%20datasets%2C%20ANN%20surpasses%20leading%20multi-agent%0Abaselines%20under%20the%20same%20configurations%2C%20showing%20consistent%20performance%0Aimprovements.%20Our%20findings%20indicate%20that%20ANN%20provides%20a%20scalable%2C%20data-driven%0Aframework%20for%20multi-agent%20systems%2C%20combining%20the%20collaborative%20capabilities%20of%0ALLMs%20with%20the%20efficiency%20and%20flexibility%20of%20neural%20network%20principles.%20We%20plan%0Ato%20open-source%20the%20entire%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.09046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520Neural%2520Networks%253A%2520Self-Evolving%2520Multi-Agent%2520Systems%2520via%2520Textual%250A%2520%2520Backpropagation%26entry.906535625%3DXiaowen%2520Ma%2520and%2520Chenyang%2520Lin%2520and%2520Yao%2520Zhang%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Leveraging%2520multiple%2520Large%2520Language%2520Models%2528LLMs%2529%2520has%2520proven%2520effective%2520for%250Aaddressing%2520complex%252C%2520high-dimensional%2520tasks%252C%2520but%2520current%2520approaches%2520often%2520rely%250Aon%2520static%252C%2520manually%2520engineered%2520multi-agent%2520configurations.%2520To%2520overcome%2520these%250Aconstraints%252C%2520we%2520present%2520the%2520Agentic%2520Neural%2520Network%2528ANN%2529%252C%2520a%2520framework%2520that%250Aconceptualizes%2520multi-agent%2520collaboration%2520as%2520a%2520layered%2520neural%2520network%250Aarchitecture.%2520In%2520this%2520design%252C%2520each%2520agent%2520operates%2520as%2520a%2520node%252C%2520and%2520each%2520layer%250Aforms%2520a%2520cooperative%2520%2522team%2522%2520focused%2520on%2520a%2520specific%2520subtask.%2520Agentic%2520Neural%250ANetwork%2520follows%2520a%2520two-phase%2520optimization%2520strategy%253A%2520%25281%2529%2520Forward%2520Phase-Drawing%250Ainspiration%2520from%2520neural%2520network%2520forward%2520passes%252C%2520tasks%2520are%2520dynamically%250Adecomposed%2520into%2520subtasks%252C%2520and%2520cooperative%2520agent%2520teams%2520with%2520suitable%2520aggregation%250Amethods%2520are%2520constructed%2520layer%2520by%2520layer.%2520%25282%2529%2520Backward%2520Phase-Mirroring%250Abackpropagation%252C%2520we%2520refine%2520both%2520global%2520and%2520local%2520collaboration%2520through%250Aiterative%2520feedback%252C%2520allowing%2520agents%2520to%2520self-evolve%2520their%2520roles%252C%2520prompts%252C%2520and%250Acoordination.%2520This%2520neuro-symbolic%2520approach%2520enables%2520ANN%2520to%2520create%2520new%2520or%250Aspecialized%2520agent%2520teams%2520post-training%252C%2520delivering%2520notable%2520gains%2520in%2520accuracy%2520and%250Aadaptability.%2520Across%2520four%2520benchmark%2520datasets%252C%2520ANN%2520surpasses%2520leading%2520multi-agent%250Abaselines%2520under%2520the%2520same%2520configurations%252C%2520showing%2520consistent%2520performance%250Aimprovements.%2520Our%2520findings%2520indicate%2520that%2520ANN%2520provides%2520a%2520scalable%252C%2520data-driven%250Aframework%2520for%2520multi-agent%2520systems%252C%2520combining%2520the%2520collaborative%2520capabilities%2520of%250ALLMs%2520with%2520the%2520efficiency%2520and%2520flexibility%2520of%2520neural%2520network%2520principles.%2520We%2520plan%250Ato%2520open-source%2520the%2520entire%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20Neural%20Networks%3A%20Self-Evolving%20Multi-Agent%20Systems%20via%20Textual%0A%20%20Backpropagation&entry.906535625=Xiaowen%20Ma%20and%20Chenyang%20Lin%20and%20Yao%20Zhang%20and%20Volker%20Tresp%20and%20Yunpu%20Ma&entry.1292438233=%20%20Leveraging%20multiple%20Large%20Language%20Models%28LLMs%29%20has%20proven%20effective%20for%0Aaddressing%20complex%2C%20high-dimensional%20tasks%2C%20but%20current%20approaches%20often%20rely%0Aon%20static%2C%20manually%20engineered%20multi-agent%20configurations.%20To%20overcome%20these%0Aconstraints%2C%20we%20present%20the%20Agentic%20Neural%20Network%28ANN%29%2C%20a%20framework%20that%0Aconceptualizes%20multi-agent%20collaboration%20as%20a%20layered%20neural%20network%0Aarchitecture.%20In%20this%20design%2C%20each%20agent%20operates%20as%20a%20node%2C%20and%20each%20layer%0Aforms%20a%20cooperative%20%22team%22%20focused%20on%20a%20specific%20subtask.%20Agentic%20Neural%0ANetwork%20follows%20a%20two-phase%20optimization%20strategy%3A%20%281%29%20Forward%20Phase-Drawing%0Ainspiration%20from%20neural%20network%20forward%20passes%2C%20tasks%20are%20dynamically%0Adecomposed%20into%20subtasks%2C%20and%20cooperative%20agent%20teams%20with%20suitable%20aggregation%0Amethods%20are%20constructed%20layer%20by%20layer.%20%282%29%20Backward%20Phase-Mirroring%0Abackpropagation%2C%20we%20refine%20both%20global%20and%20local%20collaboration%20through%0Aiterative%20feedback%2C%20allowing%20agents%20to%20self-evolve%20their%20roles%2C%20prompts%2C%20and%0Acoordination.%20This%20neuro-symbolic%20approach%20enables%20ANN%20to%20create%20new%20or%0Aspecialized%20agent%20teams%20post-training%2C%20delivering%20notable%20gains%20in%20accuracy%20and%0Aadaptability.%20Across%20four%20benchmark%20datasets%2C%20ANN%20surpasses%20leading%20multi-agent%0Abaselines%20under%20the%20same%20configurations%2C%20showing%20consistent%20performance%0Aimprovements.%20Our%20findings%20indicate%20that%20ANN%20provides%20a%20scalable%2C%20data-driven%0Aframework%20for%20multi-agent%20systems%2C%20combining%20the%20collaborative%20capabilities%20of%0ALLMs%20with%20the%20efficiency%20and%20flexibility%20of%20neural%20network%20principles.%20We%20plan%0Ato%20open-source%20the%20entire%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.09046v2&entry.124074799=Read"},
{"title": "LOCUS: LOcalization with Channel Uncertainty and Sporadic Energy", "author": "Subrata Biswas and Mohammad Nur Hossain Khan and Violet Colwell and Jack Adiletta and Bashima Islam", "abstract": "  Accurate sound source localization (SSL), such as direction-of-arrival (DoA)\nestimation, relies on consistent multichannel data. However, batteryless\nsystems often suffer from missing data due to the stochastic nature of energy\nharvesting, degrading localization performance. We propose LOCUS, a deep\nlearning framework that recovers corrupted features in such settings. LOCUS\nintegrates three modules: (1) Information-Weighted Focus (InFo) to identify\ncorrupted regions, (2) Latent Feature Synthesizer (LaFS) to reconstruct missing\nfeatures, and (3) Guided Replacement (GRep) to restore data without altering\nvalid inputs. LOCUS significantly improves DoA accuracy under missing-channel\nconditions, achieving up to 36.91% error reduction on DCASE and LargeSet, and\n25.87-59.46% gains in real-world deployments. We release a 50-hour multichannel\ndataset to support future research on localization under energy constraints.\nOur code and data are available at: https://bashlab.github.io/locus_project/\n", "link": "http://arxiv.org/abs/2302.09409v3", "date": "2025-07-18", "relevancy": 2.155, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOCUS%3A%20LOcalization%20with%20Channel%20Uncertainty%20and%20Sporadic%20Energy&body=Title%3A%20LOCUS%3A%20LOcalization%20with%20Channel%20Uncertainty%20and%20Sporadic%20Energy%0AAuthor%3A%20Subrata%20Biswas%20and%20Mohammad%20Nur%20Hossain%20Khan%20and%20Violet%20Colwell%20and%20Jack%20Adiletta%20and%20Bashima%20Islam%0AAbstract%3A%20%20%20Accurate%20sound%20source%20localization%20%28SSL%29%2C%20such%20as%20direction-of-arrival%20%28DoA%29%0Aestimation%2C%20relies%20on%20consistent%20multichannel%20data.%20However%2C%20batteryless%0Asystems%20often%20suffer%20from%20missing%20data%20due%20to%20the%20stochastic%20nature%20of%20energy%0Aharvesting%2C%20degrading%20localization%20performance.%20We%20propose%20LOCUS%2C%20a%20deep%0Alearning%20framework%20that%20recovers%20corrupted%20features%20in%20such%20settings.%20LOCUS%0Aintegrates%20three%20modules%3A%20%281%29%20Information-Weighted%20Focus%20%28InFo%29%20to%20identify%0Acorrupted%20regions%2C%20%282%29%20Latent%20Feature%20Synthesizer%20%28LaFS%29%20to%20reconstruct%20missing%0Afeatures%2C%20and%20%283%29%20Guided%20Replacement%20%28GRep%29%20to%20restore%20data%20without%20altering%0Avalid%20inputs.%20LOCUS%20significantly%20improves%20DoA%20accuracy%20under%20missing-channel%0Aconditions%2C%20achieving%20up%20to%2036.91%25%20error%20reduction%20on%20DCASE%20and%20LargeSet%2C%20and%0A25.87-59.46%25%20gains%20in%20real-world%20deployments.%20We%20release%20a%2050-hour%20multichannel%0Adataset%20to%20support%20future%20research%20on%20localization%20under%20energy%20constraints.%0AOur%20code%20and%20data%20are%20available%20at%3A%20https%3A//bashlab.github.io/locus_project/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.09409v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOCUS%253A%2520LOcalization%2520with%2520Channel%2520Uncertainty%2520and%2520Sporadic%2520Energy%26entry.906535625%3DSubrata%2520Biswas%2520and%2520Mohammad%2520Nur%2520Hossain%2520Khan%2520and%2520Violet%2520Colwell%2520and%2520Jack%2520Adiletta%2520and%2520Bashima%2520Islam%26entry.1292438233%3D%2520%2520Accurate%2520sound%2520source%2520localization%2520%2528SSL%2529%252C%2520such%2520as%2520direction-of-arrival%2520%2528DoA%2529%250Aestimation%252C%2520relies%2520on%2520consistent%2520multichannel%2520data.%2520However%252C%2520batteryless%250Asystems%2520often%2520suffer%2520from%2520missing%2520data%2520due%2520to%2520the%2520stochastic%2520nature%2520of%2520energy%250Aharvesting%252C%2520degrading%2520localization%2520performance.%2520We%2520propose%2520LOCUS%252C%2520a%2520deep%250Alearning%2520framework%2520that%2520recovers%2520corrupted%2520features%2520in%2520such%2520settings.%2520LOCUS%250Aintegrates%2520three%2520modules%253A%2520%25281%2529%2520Information-Weighted%2520Focus%2520%2528InFo%2529%2520to%2520identify%250Acorrupted%2520regions%252C%2520%25282%2529%2520Latent%2520Feature%2520Synthesizer%2520%2528LaFS%2529%2520to%2520reconstruct%2520missing%250Afeatures%252C%2520and%2520%25283%2529%2520Guided%2520Replacement%2520%2528GRep%2529%2520to%2520restore%2520data%2520without%2520altering%250Avalid%2520inputs.%2520LOCUS%2520significantly%2520improves%2520DoA%2520accuracy%2520under%2520missing-channel%250Aconditions%252C%2520achieving%2520up%2520to%252036.91%2525%2520error%2520reduction%2520on%2520DCASE%2520and%2520LargeSet%252C%2520and%250A25.87-59.46%2525%2520gains%2520in%2520real-world%2520deployments.%2520We%2520release%2520a%252050-hour%2520multichannel%250Adataset%2520to%2520support%2520future%2520research%2520on%2520localization%2520under%2520energy%2520constraints.%250AOur%2520code%2520and%2520data%2520are%2520available%2520at%253A%2520https%253A//bashlab.github.io/locus_project/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.09409v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOCUS%3A%20LOcalization%20with%20Channel%20Uncertainty%20and%20Sporadic%20Energy&entry.906535625=Subrata%20Biswas%20and%20Mohammad%20Nur%20Hossain%20Khan%20and%20Violet%20Colwell%20and%20Jack%20Adiletta%20and%20Bashima%20Islam&entry.1292438233=%20%20Accurate%20sound%20source%20localization%20%28SSL%29%2C%20such%20as%20direction-of-arrival%20%28DoA%29%0Aestimation%2C%20relies%20on%20consistent%20multichannel%20data.%20However%2C%20batteryless%0Asystems%20often%20suffer%20from%20missing%20data%20due%20to%20the%20stochastic%20nature%20of%20energy%0Aharvesting%2C%20degrading%20localization%20performance.%20We%20propose%20LOCUS%2C%20a%20deep%0Alearning%20framework%20that%20recovers%20corrupted%20features%20in%20such%20settings.%20LOCUS%0Aintegrates%20three%20modules%3A%20%281%29%20Information-Weighted%20Focus%20%28InFo%29%20to%20identify%0Acorrupted%20regions%2C%20%282%29%20Latent%20Feature%20Synthesizer%20%28LaFS%29%20to%20reconstruct%20missing%0Afeatures%2C%20and%20%283%29%20Guided%20Replacement%20%28GRep%29%20to%20restore%20data%20without%20altering%0Avalid%20inputs.%20LOCUS%20significantly%20improves%20DoA%20accuracy%20under%20missing-channel%0Aconditions%2C%20achieving%20up%20to%2036.91%25%20error%20reduction%20on%20DCASE%20and%20LargeSet%2C%20and%0A25.87-59.46%25%20gains%20in%20real-world%20deployments.%20We%20release%20a%2050-hour%20multichannel%0Adataset%20to%20support%20future%20research%20on%20localization%20under%20energy%20constraints.%0AOur%20code%20and%20data%20are%20available%20at%3A%20https%3A//bashlab.github.io/locus_project/%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.09409v3&entry.124074799=Read"},
{"title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance\n  Imaging", "author": "Hao Fang and Hao Yu and Sihao Teng and Tao Zhang and Siyi Yuan and Huaiwu He and Zhe Liu and Yunjie Yang", "abstract": "  Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.\n", "link": "http://arxiv.org/abs/2507.14046v1", "date": "2025-07-18", "relevancy": 2.143, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5529}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5323}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D2IP%3A%20Deep%20Dynamic%20Image%20Prior%20for%203D%20Time-sequence%20Pulmonary%20Impedance%0A%20%20Imaging&body=Title%3A%20D2IP%3A%20Deep%20Dynamic%20Image%20Prior%20for%203D%20Time-sequence%20Pulmonary%20Impedance%0A%20%20Imaging%0AAuthor%3A%20Hao%20Fang%20and%20Hao%20Yu%20and%20Sihao%20Teng%20and%20Tao%20Zhang%20and%20Siyi%20Yuan%20and%20Huaiwu%20He%20and%20Zhe%20Liu%20and%20Yunjie%20Yang%0AAbstract%3A%20%20%20Unsupervised%20learning%20methods%2C%20such%20as%20Deep%20Image%20Prior%20%28DIP%29%2C%20have%20shown%0Agreat%20potential%20in%20tomographic%20imaging%20due%20to%20their%20training-data-free%20nature%0Aand%20high%20generalization%20capability.%20However%2C%20their%20reliance%20on%20numerous%20network%0Aparameter%20iterations%20results%20in%20high%20computational%20costs%2C%20limiting%20their%0Apractical%20application%2C%20particularly%20in%20complex%203D%20or%20time-sequence%20tomographic%0Aimaging%20tasks.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Deep%20Dynamic%20Image%0APrior%20%28D2IP%29%2C%20a%20novel%20framework%20for%203D%20time-sequence%20imaging.%20D2IP%20introduces%0Athree%20key%20strategies%20-%20Unsupervised%20Parameter%20Warm-Start%20%28UPWS%29%2C%20Temporal%0AParameter%20Propagation%20%28TPP%29%2C%20and%20a%20customized%20lightweight%20reconstruction%0Abackbone%2C%203D-FastResUNet%20-%20to%20accelerate%20convergence%2C%20enforce%20temporal%0Acoherence%2C%20and%20improve%20computational%20efficiency.%20Experimental%20results%20on%20both%0Asimulated%20and%20clinical%20pulmonary%20datasets%20demonstrate%20that%20D2IP%20enables%20fast%0Aand%20accurate%203D%20time-sequence%20Electrical%20Impedance%20Tomography%20%28tsEIT%29%0Areconstruction.%20Compared%20to%20state-of-the-art%20baselines%2C%20D2IP%20delivers%20superior%0Aimage%20quality%2C%20with%20a%2024.8%25%20increase%20in%20average%20MSSIM%20and%20an%208.1%25%20reduction%20in%0AERR%2C%20alongside%20significantly%20reduced%20computational%20time%20%287.1x%20faster%29%2C%0Ahighlighting%20its%20promise%20for%20clinical%20dynamic%20pulmonary%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD2IP%253A%2520Deep%2520Dynamic%2520Image%2520Prior%2520for%25203D%2520Time-sequence%2520Pulmonary%2520Impedance%250A%2520%2520Imaging%26entry.906535625%3DHao%2520Fang%2520and%2520Hao%2520Yu%2520and%2520Sihao%2520Teng%2520and%2520Tao%2520Zhang%2520and%2520Siyi%2520Yuan%2520and%2520Huaiwu%2520He%2520and%2520Zhe%2520Liu%2520and%2520Yunjie%2520Yang%26entry.1292438233%3D%2520%2520Unsupervised%2520learning%2520methods%252C%2520such%2520as%2520Deep%2520Image%2520Prior%2520%2528DIP%2529%252C%2520have%2520shown%250Agreat%2520potential%2520in%2520tomographic%2520imaging%2520due%2520to%2520their%2520training-data-free%2520nature%250Aand%2520high%2520generalization%2520capability.%2520However%252C%2520their%2520reliance%2520on%2520numerous%2520network%250Aparameter%2520iterations%2520results%2520in%2520high%2520computational%2520costs%252C%2520limiting%2520their%250Apractical%2520application%252C%2520particularly%2520in%2520complex%25203D%2520or%2520time-sequence%2520tomographic%250Aimaging%2520tasks.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520Deep%2520Dynamic%2520Image%250APrior%2520%2528D2IP%2529%252C%2520a%2520novel%2520framework%2520for%25203D%2520time-sequence%2520imaging.%2520D2IP%2520introduces%250Athree%2520key%2520strategies%2520-%2520Unsupervised%2520Parameter%2520Warm-Start%2520%2528UPWS%2529%252C%2520Temporal%250AParameter%2520Propagation%2520%2528TPP%2529%252C%2520and%2520a%2520customized%2520lightweight%2520reconstruction%250Abackbone%252C%25203D-FastResUNet%2520-%2520to%2520accelerate%2520convergence%252C%2520enforce%2520temporal%250Acoherence%252C%2520and%2520improve%2520computational%2520efficiency.%2520Experimental%2520results%2520on%2520both%250Asimulated%2520and%2520clinical%2520pulmonary%2520datasets%2520demonstrate%2520that%2520D2IP%2520enables%2520fast%250Aand%2520accurate%25203D%2520time-sequence%2520Electrical%2520Impedance%2520Tomography%2520%2528tsEIT%2529%250Areconstruction.%2520Compared%2520to%2520state-of-the-art%2520baselines%252C%2520D2IP%2520delivers%2520superior%250Aimage%2520quality%252C%2520with%2520a%252024.8%2525%2520increase%2520in%2520average%2520MSSIM%2520and%2520an%25208.1%2525%2520reduction%2520in%250AERR%252C%2520alongside%2520significantly%2520reduced%2520computational%2520time%2520%25287.1x%2520faster%2529%252C%250Ahighlighting%2520its%2520promise%2520for%2520clinical%2520dynamic%2520pulmonary%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D2IP%3A%20Deep%20Dynamic%20Image%20Prior%20for%203D%20Time-sequence%20Pulmonary%20Impedance%0A%20%20Imaging&entry.906535625=Hao%20Fang%20and%20Hao%20Yu%20and%20Sihao%20Teng%20and%20Tao%20Zhang%20and%20Siyi%20Yuan%20and%20Huaiwu%20He%20and%20Zhe%20Liu%20and%20Yunjie%20Yang&entry.1292438233=%20%20Unsupervised%20learning%20methods%2C%20such%20as%20Deep%20Image%20Prior%20%28DIP%29%2C%20have%20shown%0Agreat%20potential%20in%20tomographic%20imaging%20due%20to%20their%20training-data-free%20nature%0Aand%20high%20generalization%20capability.%20However%2C%20their%20reliance%20on%20numerous%20network%0Aparameter%20iterations%20results%20in%20high%20computational%20costs%2C%20limiting%20their%0Apractical%20application%2C%20particularly%20in%20complex%203D%20or%20time-sequence%20tomographic%0Aimaging%20tasks.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Deep%20Dynamic%20Image%0APrior%20%28D2IP%29%2C%20a%20novel%20framework%20for%203D%20time-sequence%20imaging.%20D2IP%20introduces%0Athree%20key%20strategies%20-%20Unsupervised%20Parameter%20Warm-Start%20%28UPWS%29%2C%20Temporal%0AParameter%20Propagation%20%28TPP%29%2C%20and%20a%20customized%20lightweight%20reconstruction%0Abackbone%2C%203D-FastResUNet%20-%20to%20accelerate%20convergence%2C%20enforce%20temporal%0Acoherence%2C%20and%20improve%20computational%20efficiency.%20Experimental%20results%20on%20both%0Asimulated%20and%20clinical%20pulmonary%20datasets%20demonstrate%20that%20D2IP%20enables%20fast%0Aand%20accurate%203D%20time-sequence%20Electrical%20Impedance%20Tomography%20%28tsEIT%29%0Areconstruction.%20Compared%20to%20state-of-the-art%20baselines%2C%20D2IP%20delivers%20superior%0Aimage%20quality%2C%20with%20a%2024.8%25%20increase%20in%20average%20MSSIM%20and%20an%208.1%25%20reduction%20in%0AERR%2C%20alongside%20significantly%20reduced%20computational%20time%20%287.1x%20faster%29%2C%0Ahighlighting%20its%20promise%20for%20clinical%20dynamic%20pulmonary%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14046v1&entry.124074799=Read"},
{"title": "Cycle-Consistent Multi-Graph Matching for Self-Supervised Annotation of\n  C.Elegans", "author": "Christoph Karg and Sebastian Stricker and Lisa Hutschenreiter and Bogdan Savchynskyy and Dagmar Kainmueller", "abstract": "  In this work we present a novel approach for unsupervised multi-graph\nmatching, which applies to problems for which a Gaussian distribution of\nkeypoint features can be assumed. We leverage cycle consistency as loss for\nself-supervised learning, and determine Gaussian parameters through Bayesian\nOptimization, yielding a highly efficient approach that scales to large\ndatasets. Our fully unsupervised approach enables us to reach the accuracy of\nstate-of-the-art supervised methodology for the biomedical use case of semantic\ncell annotation in 3D microscopy images of the worm C. elegans. To this end,\nour approach yields the first unsupervised atlas of C. elegans, i.e. a model of\nthe joint distribution of all of its cell nuclei, without the need for any\nground truth cell annotation. This advancement enables highly efficient\nsemantic annotation of cells in large microscopy datasets, overcoming a current\nkey bottleneck. Beyond C. elegans, our approach offers fully unsupervised\nconstruction of cell-level atlases for any model organism with a stereotyped\nbody plan down to the level of unique semantic cell labels, and thus bears the\npotential to catalyze respective biomedical studies in a range of further\nspecies.\n", "link": "http://arxiv.org/abs/2503.07348v2", "date": "2025-07-18", "relevancy": 2.1379, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5453}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5273}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cycle-Consistent%20Multi-Graph%20Matching%20for%20Self-Supervised%20Annotation%20of%0A%20%20C.Elegans&body=Title%3A%20Cycle-Consistent%20Multi-Graph%20Matching%20for%20Self-Supervised%20Annotation%20of%0A%20%20C.Elegans%0AAuthor%3A%20Christoph%20Karg%20and%20Sebastian%20Stricker%20and%20Lisa%20Hutschenreiter%20and%20Bogdan%20Savchynskyy%20and%20Dagmar%20Kainmueller%0AAbstract%3A%20%20%20In%20this%20work%20we%20present%20a%20novel%20approach%20for%20unsupervised%20multi-graph%0Amatching%2C%20which%20applies%20to%20problems%20for%20which%20a%20Gaussian%20distribution%20of%0Akeypoint%20features%20can%20be%20assumed.%20We%20leverage%20cycle%20consistency%20as%20loss%20for%0Aself-supervised%20learning%2C%20and%20determine%20Gaussian%20parameters%20through%20Bayesian%0AOptimization%2C%20yielding%20a%20highly%20efficient%20approach%20that%20scales%20to%20large%0Adatasets.%20Our%20fully%20unsupervised%20approach%20enables%20us%20to%20reach%20the%20accuracy%20of%0Astate-of-the-art%20supervised%20methodology%20for%20the%20biomedical%20use%20case%20of%20semantic%0Acell%20annotation%20in%203D%20microscopy%20images%20of%20the%20worm%20C.%20elegans.%20To%20this%20end%2C%0Aour%20approach%20yields%20the%20first%20unsupervised%20atlas%20of%20C.%20elegans%2C%20i.e.%20a%20model%20of%0Athe%20joint%20distribution%20of%20all%20of%20its%20cell%20nuclei%2C%20without%20the%20need%20for%20any%0Aground%20truth%20cell%20annotation.%20This%20advancement%20enables%20highly%20efficient%0Asemantic%20annotation%20of%20cells%20in%20large%20microscopy%20datasets%2C%20overcoming%20a%20current%0Akey%20bottleneck.%20Beyond%20C.%20elegans%2C%20our%20approach%20offers%20fully%20unsupervised%0Aconstruction%20of%20cell-level%20atlases%20for%20any%20model%20organism%20with%20a%20stereotyped%0Abody%20plan%20down%20to%20the%20level%20of%20unique%20semantic%20cell%20labels%2C%20and%20thus%20bears%20the%0Apotential%20to%20catalyze%20respective%20biomedical%20studies%20in%20a%20range%20of%20further%0Aspecies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.07348v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCycle-Consistent%2520Multi-Graph%2520Matching%2520for%2520Self-Supervised%2520Annotation%2520of%250A%2520%2520C.Elegans%26entry.906535625%3DChristoph%2520Karg%2520and%2520Sebastian%2520Stricker%2520and%2520Lisa%2520Hutschenreiter%2520and%2520Bogdan%2520Savchynskyy%2520and%2520Dagmar%2520Kainmueller%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520present%2520a%2520novel%2520approach%2520for%2520unsupervised%2520multi-graph%250Amatching%252C%2520which%2520applies%2520to%2520problems%2520for%2520which%2520a%2520Gaussian%2520distribution%2520of%250Akeypoint%2520features%2520can%2520be%2520assumed.%2520We%2520leverage%2520cycle%2520consistency%2520as%2520loss%2520for%250Aself-supervised%2520learning%252C%2520and%2520determine%2520Gaussian%2520parameters%2520through%2520Bayesian%250AOptimization%252C%2520yielding%2520a%2520highly%2520efficient%2520approach%2520that%2520scales%2520to%2520large%250Adatasets.%2520Our%2520fully%2520unsupervised%2520approach%2520enables%2520us%2520to%2520reach%2520the%2520accuracy%2520of%250Astate-of-the-art%2520supervised%2520methodology%2520for%2520the%2520biomedical%2520use%2520case%2520of%2520semantic%250Acell%2520annotation%2520in%25203D%2520microscopy%2520images%2520of%2520the%2520worm%2520C.%2520elegans.%2520To%2520this%2520end%252C%250Aour%2520approach%2520yields%2520the%2520first%2520unsupervised%2520atlas%2520of%2520C.%2520elegans%252C%2520i.e.%2520a%2520model%2520of%250Athe%2520joint%2520distribution%2520of%2520all%2520of%2520its%2520cell%2520nuclei%252C%2520without%2520the%2520need%2520for%2520any%250Aground%2520truth%2520cell%2520annotation.%2520This%2520advancement%2520enables%2520highly%2520efficient%250Asemantic%2520annotation%2520of%2520cells%2520in%2520large%2520microscopy%2520datasets%252C%2520overcoming%2520a%2520current%250Akey%2520bottleneck.%2520Beyond%2520C.%2520elegans%252C%2520our%2520approach%2520offers%2520fully%2520unsupervised%250Aconstruction%2520of%2520cell-level%2520atlases%2520for%2520any%2520model%2520organism%2520with%2520a%2520stereotyped%250Abody%2520plan%2520down%2520to%2520the%2520level%2520of%2520unique%2520semantic%2520cell%2520labels%252C%2520and%2520thus%2520bears%2520the%250Apotential%2520to%2520catalyze%2520respective%2520biomedical%2520studies%2520in%2520a%2520range%2520of%2520further%250Aspecies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07348v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cycle-Consistent%20Multi-Graph%20Matching%20for%20Self-Supervised%20Annotation%20of%0A%20%20C.Elegans&entry.906535625=Christoph%20Karg%20and%20Sebastian%20Stricker%20and%20Lisa%20Hutschenreiter%20and%20Bogdan%20Savchynskyy%20and%20Dagmar%20Kainmueller&entry.1292438233=%20%20In%20this%20work%20we%20present%20a%20novel%20approach%20for%20unsupervised%20multi-graph%0Amatching%2C%20which%20applies%20to%20problems%20for%20which%20a%20Gaussian%20distribution%20of%0Akeypoint%20features%20can%20be%20assumed.%20We%20leverage%20cycle%20consistency%20as%20loss%20for%0Aself-supervised%20learning%2C%20and%20determine%20Gaussian%20parameters%20through%20Bayesian%0AOptimization%2C%20yielding%20a%20highly%20efficient%20approach%20that%20scales%20to%20large%0Adatasets.%20Our%20fully%20unsupervised%20approach%20enables%20us%20to%20reach%20the%20accuracy%20of%0Astate-of-the-art%20supervised%20methodology%20for%20the%20biomedical%20use%20case%20of%20semantic%0Acell%20annotation%20in%203D%20microscopy%20images%20of%20the%20worm%20C.%20elegans.%20To%20this%20end%2C%0Aour%20approach%20yields%20the%20first%20unsupervised%20atlas%20of%20C.%20elegans%2C%20i.e.%20a%20model%20of%0Athe%20joint%20distribution%20of%20all%20of%20its%20cell%20nuclei%2C%20without%20the%20need%20for%20any%0Aground%20truth%20cell%20annotation.%20This%20advancement%20enables%20highly%20efficient%0Asemantic%20annotation%20of%20cells%20in%20large%20microscopy%20datasets%2C%20overcoming%20a%20current%0Akey%20bottleneck.%20Beyond%20C.%20elegans%2C%20our%20approach%20offers%20fully%20unsupervised%0Aconstruction%20of%20cell-level%20atlases%20for%20any%20model%20organism%20with%20a%20stereotyped%0Abody%20plan%20down%20to%20the%20level%20of%20unique%20semantic%20cell%20labels%2C%20and%20thus%20bears%20the%0Apotential%20to%20catalyze%20respective%20biomedical%20studies%20in%20a%20range%20of%20further%0Aspecies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.07348v2&entry.124074799=Read"},
{"title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on\n  Multi-Modal Large Models", "author": "Ningyong Wu and Jinzhi Wang and Wenhong Zhao and Chenzhan Yu and Zhigang Xiu and Duwei Dai", "abstract": "  The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.\n", "link": "http://arxiv.org/abs/2507.13993v1", "date": "2025-07-18", "relevancy": 2.1307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OrthoInsight%3A%20Rib%20Fracture%20Diagnosis%20and%20Report%20Generation%20Based%20on%0A%20%20Multi-Modal%20Large%20Models&body=Title%3A%20OrthoInsight%3A%20Rib%20Fracture%20Diagnosis%20and%20Report%20Generation%20Based%20on%0A%20%20Multi-Modal%20Large%20Models%0AAuthor%3A%20Ningyong%20Wu%20and%20Jinzhi%20Wang%20and%20Wenhong%20Zhao%20and%20Chenzhan%20Yu%20and%20Zhigang%20Xiu%20and%20Duwei%20Dai%0AAbstract%3A%20%20%20The%20growing%20volume%20of%20medical%20imaging%20data%20has%20increased%20the%20need%20for%0Aautomated%20diagnostic%20tools%2C%20especially%20for%20musculoskeletal%20injuries%20like%20rib%0Afractures%2C%20commonly%20detected%20via%20CT%20scans.%20Manual%20interpretation%20is%0Atime-consuming%20and%20error-prone.%20We%20propose%20OrthoInsight%2C%20a%20multi-modal%20deep%0Alearning%20framework%20for%20rib%20fracture%20diagnosis%20and%20report%20generation.%20It%0Aintegrates%20a%20YOLOv9%20model%20for%20fracture%20detection%2C%20a%20medical%20knowledge%20graph%20for%0Aretrieving%20clinical%20context%2C%20and%20a%20fine-tuned%20LLaVA%20language%20model%20for%0Agenerating%20diagnostic%20reports.%20OrthoInsight%20combines%20visual%20features%20from%20CT%0Aimages%20with%20expert%20textual%20data%20to%20deliver%20clinically%20useful%20outputs.%20Evaluated%0Aon%2028%2C675%20annotated%20CT%20images%20and%20expert%20reports%2C%20it%20achieves%20high%20performance%0Aacross%20Diagnostic%20Accuracy%2C%20Content%20Completeness%2C%20Logical%20Coherence%2C%20and%0AClinical%20Guidance%20Value%2C%20with%20an%20average%20score%20of%204.28%2C%20outperforming%20models%0Alike%20GPT-4%20and%20Claude-3.%20This%20study%20demonstrates%20the%20potential%20of%20multi-modal%0Alearning%20in%20transforming%20medical%20image%20analysis%20and%20providing%20effective%20support%0Afor%20radiologists.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrthoInsight%253A%2520Rib%2520Fracture%2520Diagnosis%2520and%2520Report%2520Generation%2520Based%2520on%250A%2520%2520Multi-Modal%2520Large%2520Models%26entry.906535625%3DNingyong%2520Wu%2520and%2520Jinzhi%2520Wang%2520and%2520Wenhong%2520Zhao%2520and%2520Chenzhan%2520Yu%2520and%2520Zhigang%2520Xiu%2520and%2520Duwei%2520Dai%26entry.1292438233%3D%2520%2520The%2520growing%2520volume%2520of%2520medical%2520imaging%2520data%2520has%2520increased%2520the%2520need%2520for%250Aautomated%2520diagnostic%2520tools%252C%2520especially%2520for%2520musculoskeletal%2520injuries%2520like%2520rib%250Afractures%252C%2520commonly%2520detected%2520via%2520CT%2520scans.%2520Manual%2520interpretation%2520is%250Atime-consuming%2520and%2520error-prone.%2520We%2520propose%2520OrthoInsight%252C%2520a%2520multi-modal%2520deep%250Alearning%2520framework%2520for%2520rib%2520fracture%2520diagnosis%2520and%2520report%2520generation.%2520It%250Aintegrates%2520a%2520YOLOv9%2520model%2520for%2520fracture%2520detection%252C%2520a%2520medical%2520knowledge%2520graph%2520for%250Aretrieving%2520clinical%2520context%252C%2520and%2520a%2520fine-tuned%2520LLaVA%2520language%2520model%2520for%250Agenerating%2520diagnostic%2520reports.%2520OrthoInsight%2520combines%2520visual%2520features%2520from%2520CT%250Aimages%2520with%2520expert%2520textual%2520data%2520to%2520deliver%2520clinically%2520useful%2520outputs.%2520Evaluated%250Aon%252028%252C675%2520annotated%2520CT%2520images%2520and%2520expert%2520reports%252C%2520it%2520achieves%2520high%2520performance%250Aacross%2520Diagnostic%2520Accuracy%252C%2520Content%2520Completeness%252C%2520Logical%2520Coherence%252C%2520and%250AClinical%2520Guidance%2520Value%252C%2520with%2520an%2520average%2520score%2520of%25204.28%252C%2520outperforming%2520models%250Alike%2520GPT-4%2520and%2520Claude-3.%2520This%2520study%2520demonstrates%2520the%2520potential%2520of%2520multi-modal%250Alearning%2520in%2520transforming%2520medical%2520image%2520analysis%2520and%2520providing%2520effective%2520support%250Afor%2520radiologists.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OrthoInsight%3A%20Rib%20Fracture%20Diagnosis%20and%20Report%20Generation%20Based%20on%0A%20%20Multi-Modal%20Large%20Models&entry.906535625=Ningyong%20Wu%20and%20Jinzhi%20Wang%20and%20Wenhong%20Zhao%20and%20Chenzhan%20Yu%20and%20Zhigang%20Xiu%20and%20Duwei%20Dai&entry.1292438233=%20%20The%20growing%20volume%20of%20medical%20imaging%20data%20has%20increased%20the%20need%20for%0Aautomated%20diagnostic%20tools%2C%20especially%20for%20musculoskeletal%20injuries%20like%20rib%0Afractures%2C%20commonly%20detected%20via%20CT%20scans.%20Manual%20interpretation%20is%0Atime-consuming%20and%20error-prone.%20We%20propose%20OrthoInsight%2C%20a%20multi-modal%20deep%0Alearning%20framework%20for%20rib%20fracture%20diagnosis%20and%20report%20generation.%20It%0Aintegrates%20a%20YOLOv9%20model%20for%20fracture%20detection%2C%20a%20medical%20knowledge%20graph%20for%0Aretrieving%20clinical%20context%2C%20and%20a%20fine-tuned%20LLaVA%20language%20model%20for%0Agenerating%20diagnostic%20reports.%20OrthoInsight%20combines%20visual%20features%20from%20CT%0Aimages%20with%20expert%20textual%20data%20to%20deliver%20clinically%20useful%20outputs.%20Evaluated%0Aon%2028%2C675%20annotated%20CT%20images%20and%20expert%20reports%2C%20it%20achieves%20high%20performance%0Aacross%20Diagnostic%20Accuracy%2C%20Content%20Completeness%2C%20Logical%20Coherence%2C%20and%0AClinical%20Guidance%20Value%2C%20with%20an%20average%20score%20of%204.28%2C%20outperforming%20models%0Alike%20GPT-4%20and%20Claude-3.%20This%20study%20demonstrates%20the%20potential%20of%20multi-modal%0Alearning%20in%20transforming%20medical%20image%20analysis%20and%20providing%20effective%20support%0Afor%20radiologists.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13993v1&entry.124074799=Read"},
{"title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models", "author": "Qiguang Chen and Libo Qin and Jinhao Liu and Dengyun Peng and Jiannan Guan and Peng Wang and Mengkang Hu and Yuhang Zhou and Te Gao and Wanxiang Che", "abstract": "  Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence.\n", "link": "http://arxiv.org/abs/2503.09567v5", "date": "2025-07-18", "relevancy": 2.1222, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Reasoning%20Era%3A%20A%20Survey%20of%20Long%20Chain-of-Thought%20for%20Reasoning%0A%20%20Large%20Language%20Models&body=Title%3A%20Towards%20Reasoning%20Era%3A%20A%20Survey%20of%20Long%20Chain-of-Thought%20for%20Reasoning%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Qiguang%20Chen%20and%20Libo%20Qin%20and%20Jinhao%20Liu%20and%20Dengyun%20Peng%20and%20Jiannan%20Guan%20and%20Peng%20Wang%20and%20Mengkang%20Hu%20and%20Yuhang%20Zhou%20and%20Te%20Gao%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Recent%20advancements%20in%20reasoning%20with%20large%20language%20models%20%28RLLMs%29%2C%20such%20as%0AOpenAI-O1%20and%20DeepSeek-R1%2C%20have%20demonstrated%20their%20impressive%20capabilities%20in%0Acomplex%20domains%20like%20mathematics%20and%20coding.%20A%20central%20factor%20in%20their%20success%0Alies%20in%20the%20application%20of%20long%20chain-of-thought%20%28Long%20CoT%29%20characteristics%2C%0Awhich%20enhance%20reasoning%20abilities%20and%20enable%20the%20solution%20of%20intricate%0Aproblems.%20However%2C%20despite%20these%20developments%2C%20a%20comprehensive%20survey%20on%20Long%0ACoT%20is%20still%20lacking%2C%20limiting%20our%20understanding%20of%20its%20distinctions%20from%0Atraditional%20short%20chain-of-thought%20%28Short%20CoT%29%20and%20complicating%20ongoing%20debates%0Aon%20issues%20like%20%22overthinking%22%20and%20%22inference-time%20scaling.%22%20This%20survey%20seeks%0Ato%20fill%20this%20gap%20by%20offering%20a%20unified%20perspective%20on%20Long%20CoT.%20%281%29%20We%20first%0Adistinguish%20Long%20CoT%20from%20Short%20CoT%20and%20introduce%20a%20novel%20taxonomy%20to%0Acategorize%20current%20reasoning%20paradigms.%20%282%29%20Next%2C%20we%20explore%20the%20key%0Acharacteristics%20of%20Long%20CoT%3A%20deep%20reasoning%2C%20extensive%20exploration%2C%20and%0Afeasible%20reflection%2C%20which%20enable%20models%20to%20handle%20more%20complex%20tasks%20and%0Aproduce%20more%20efficient%2C%20coherent%20outcomes%20compared%20to%20the%20shallower%20Short%20CoT.%0A%283%29%20We%20then%20investigate%20key%20phenomena%20such%20as%20the%20emergence%20of%20Long%20CoT%20with%0Athese%20characteristics%2C%20including%20overthinking%2C%20and%20inference-time%20scaling%2C%0Aoffering%20insights%20into%20how%20these%20processes%20manifest%20in%20practice.%20%284%29%20Finally%2C%0Awe%20identify%20significant%20research%20gaps%20and%20highlight%20promising%20future%0Adirections%2C%20including%20the%20integration%20of%20multi-modal%20reasoning%2C%20efficiency%0Aimprovements%2C%20and%20enhanced%20knowledge%20frameworks.%20By%20providing%20a%20structured%0Aoverview%2C%20this%20survey%20aims%20to%20inspire%20future%20research%20and%20further%20the%0Adevelopment%20of%20logical%20reasoning%20in%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09567v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Reasoning%2520Era%253A%2520A%2520Survey%2520of%2520Long%2520Chain-of-Thought%2520for%2520Reasoning%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DQiguang%2520Chen%2520and%2520Libo%2520Qin%2520and%2520Jinhao%2520Liu%2520and%2520Dengyun%2520Peng%2520and%2520Jiannan%2520Guan%2520and%2520Peng%2520Wang%2520and%2520Mengkang%2520Hu%2520and%2520Yuhang%2520Zhou%2520and%2520Te%2520Gao%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520reasoning%2520with%2520large%2520language%2520models%2520%2528RLLMs%2529%252C%2520such%2520as%250AOpenAI-O1%2520and%2520DeepSeek-R1%252C%2520have%2520demonstrated%2520their%2520impressive%2520capabilities%2520in%250Acomplex%2520domains%2520like%2520mathematics%2520and%2520coding.%2520A%2520central%2520factor%2520in%2520their%2520success%250Alies%2520in%2520the%2520application%2520of%2520long%2520chain-of-thought%2520%2528Long%2520CoT%2529%2520characteristics%252C%250Awhich%2520enhance%2520reasoning%2520abilities%2520and%2520enable%2520the%2520solution%2520of%2520intricate%250Aproblems.%2520However%252C%2520despite%2520these%2520developments%252C%2520a%2520comprehensive%2520survey%2520on%2520Long%250ACoT%2520is%2520still%2520lacking%252C%2520limiting%2520our%2520understanding%2520of%2520its%2520distinctions%2520from%250Atraditional%2520short%2520chain-of-thought%2520%2528Short%2520CoT%2529%2520and%2520complicating%2520ongoing%2520debates%250Aon%2520issues%2520like%2520%2522overthinking%2522%2520and%2520%2522inference-time%2520scaling.%2522%2520This%2520survey%2520seeks%250Ato%2520fill%2520this%2520gap%2520by%2520offering%2520a%2520unified%2520perspective%2520on%2520Long%2520CoT.%2520%25281%2529%2520We%2520first%250Adistinguish%2520Long%2520CoT%2520from%2520Short%2520CoT%2520and%2520introduce%2520a%2520novel%2520taxonomy%2520to%250Acategorize%2520current%2520reasoning%2520paradigms.%2520%25282%2529%2520Next%252C%2520we%2520explore%2520the%2520key%250Acharacteristics%2520of%2520Long%2520CoT%253A%2520deep%2520reasoning%252C%2520extensive%2520exploration%252C%2520and%250Afeasible%2520reflection%252C%2520which%2520enable%2520models%2520to%2520handle%2520more%2520complex%2520tasks%2520and%250Aproduce%2520more%2520efficient%252C%2520coherent%2520outcomes%2520compared%2520to%2520the%2520shallower%2520Short%2520CoT.%250A%25283%2529%2520We%2520then%2520investigate%2520key%2520phenomena%2520such%2520as%2520the%2520emergence%2520of%2520Long%2520CoT%2520with%250Athese%2520characteristics%252C%2520including%2520overthinking%252C%2520and%2520inference-time%2520scaling%252C%250Aoffering%2520insights%2520into%2520how%2520these%2520processes%2520manifest%2520in%2520practice.%2520%25284%2529%2520Finally%252C%250Awe%2520identify%2520significant%2520research%2520gaps%2520and%2520highlight%2520promising%2520future%250Adirections%252C%2520including%2520the%2520integration%2520of%2520multi-modal%2520reasoning%252C%2520efficiency%250Aimprovements%252C%2520and%2520enhanced%2520knowledge%2520frameworks.%2520By%2520providing%2520a%2520structured%250Aoverview%252C%2520this%2520survey%2520aims%2520to%2520inspire%2520future%2520research%2520and%2520further%2520the%250Adevelopment%2520of%2520logical%2520reasoning%2520in%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09567v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Reasoning%20Era%3A%20A%20Survey%20of%20Long%20Chain-of-Thought%20for%20Reasoning%0A%20%20Large%20Language%20Models&entry.906535625=Qiguang%20Chen%20and%20Libo%20Qin%20and%20Jinhao%20Liu%20and%20Dengyun%20Peng%20and%20Jiannan%20Guan%20and%20Peng%20Wang%20and%20Mengkang%20Hu%20and%20Yuhang%20Zhou%20and%20Te%20Gao%20and%20Wanxiang%20Che&entry.1292438233=%20%20Recent%20advancements%20in%20reasoning%20with%20large%20language%20models%20%28RLLMs%29%2C%20such%20as%0AOpenAI-O1%20and%20DeepSeek-R1%2C%20have%20demonstrated%20their%20impressive%20capabilities%20in%0Acomplex%20domains%20like%20mathematics%20and%20coding.%20A%20central%20factor%20in%20their%20success%0Alies%20in%20the%20application%20of%20long%20chain-of-thought%20%28Long%20CoT%29%20characteristics%2C%0Awhich%20enhance%20reasoning%20abilities%20and%20enable%20the%20solution%20of%20intricate%0Aproblems.%20However%2C%20despite%20these%20developments%2C%20a%20comprehensive%20survey%20on%20Long%0ACoT%20is%20still%20lacking%2C%20limiting%20our%20understanding%20of%20its%20distinctions%20from%0Atraditional%20short%20chain-of-thought%20%28Short%20CoT%29%20and%20complicating%20ongoing%20debates%0Aon%20issues%20like%20%22overthinking%22%20and%20%22inference-time%20scaling.%22%20This%20survey%20seeks%0Ato%20fill%20this%20gap%20by%20offering%20a%20unified%20perspective%20on%20Long%20CoT.%20%281%29%20We%20first%0Adistinguish%20Long%20CoT%20from%20Short%20CoT%20and%20introduce%20a%20novel%20taxonomy%20to%0Acategorize%20current%20reasoning%20paradigms.%20%282%29%20Next%2C%20we%20explore%20the%20key%0Acharacteristics%20of%20Long%20CoT%3A%20deep%20reasoning%2C%20extensive%20exploration%2C%20and%0Afeasible%20reflection%2C%20which%20enable%20models%20to%20handle%20more%20complex%20tasks%20and%0Aproduce%20more%20efficient%2C%20coherent%20outcomes%20compared%20to%20the%20shallower%20Short%20CoT.%0A%283%29%20We%20then%20investigate%20key%20phenomena%20such%20as%20the%20emergence%20of%20Long%20CoT%20with%0Athese%20characteristics%2C%20including%20overthinking%2C%20and%20inference-time%20scaling%2C%0Aoffering%20insights%20into%20how%20these%20processes%20manifest%20in%20practice.%20%284%29%20Finally%2C%0Awe%20identify%20significant%20research%20gaps%20and%20highlight%20promising%20future%0Adirections%2C%20including%20the%20integration%20of%20multi-modal%20reasoning%2C%20efficiency%0Aimprovements%2C%20and%20enhanced%20knowledge%20frameworks.%20By%20providing%20a%20structured%0Aoverview%2C%20this%20survey%20aims%20to%20inspire%20future%20research%20and%20further%20the%0Adevelopment%20of%20logical%20reasoning%20in%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09567v5&entry.124074799=Read"},
{"title": "Noradrenergic-inspired gain modulation attenuates the stability gap in\n  joint training", "author": "Alejandro Rodriguez-Garcia and Anindya Ghosh and Srikanth Ramaswamy", "abstract": "  Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.\n", "link": "http://arxiv.org/abs/2507.14056v1", "date": "2025-07-18", "relevancy": 2.1046, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5365}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noradrenergic-inspired%20gain%20modulation%20attenuates%20the%20stability%20gap%20in%0A%20%20joint%20training&body=Title%3A%20Noradrenergic-inspired%20gain%20modulation%20attenuates%20the%20stability%20gap%20in%0A%20%20joint%20training%0AAuthor%3A%20Alejandro%20Rodriguez-Garcia%20and%20Anindya%20Ghosh%20and%20Srikanth%20Ramaswamy%0AAbstract%3A%20%20%20Recent%20studies%20in%20continual%20learning%20have%20identified%20a%20transient%20drop%20in%0Aperformance%20on%20mastered%20tasks%20when%20assimilating%20new%20ones%2C%20known%20as%20the%0Astability%20gap.%20Such%20dynamics%20contradict%20the%20objectives%20of%20continual%20learning%2C%0Arevealing%20a%20lack%20of%20robustness%20in%20mitigating%20forgetting%2C%20and%20notably%2C%0Apersisting%20even%20under%20an%20ideal%20joint-loss%20regime.%20Examining%20this%20gap%20within%0Athis%20idealized%20joint%20training%20context%20is%20critical%20to%20isolate%20it%20from%20other%0Asources%20of%20forgetting.%20We%20argue%20that%20it%20reflects%20an%20imbalance%20between%20rapid%0Aadaptation%20and%20robust%20retention%20at%20task%20boundaries%2C%20underscoring%20the%20need%20to%0Ainvestigate%20mechanisms%20that%20reconcile%20plasticity%20and%20stability%20within%20continual%0Alearning%20frameworks.%20Biological%20brains%20navigate%20a%20similar%20dilemma%20by%20operating%0Aconcurrently%20on%20multiple%20timescales%2C%20leveraging%20neuromodulatory%20signals%20to%0Amodulate%20synaptic%20plasticity.%20However%2C%20artificial%20networks%20lack%20native%0Amultitimescale%20dynamics%2C%20and%20although%20optimizers%20like%20momentum-SGD%20and%20Adam%0Aintroduce%20implicit%20timescale%20regularization%2C%20they%20still%20exhibit%20stability%20gaps.%0AInspired%20by%20locus%20coeruleus%20mediated%20noradrenergic%20bursts%2C%20which%20transiently%0Aenhance%20neuronal%20gain%20under%20uncertainty%20to%20facilitate%20sensory%20assimilation%2C%20we%0Apropose%20uncertainty-modulated%20gain%20dynamics%20-%20an%20adaptive%20mechanism%20that%0Aapproximates%20a%20two-timescale%20optimizer%20and%20dynamically%20balances%20integration%20of%0Aknowledge%20with%20minimal%20interference%20on%20previously%20consolidated%20information.%20We%0Aevaluate%20our%20mechanism%20on%20domain-incremental%20and%20class-incremental%20variants%20of%0Athe%20MNIST%20and%20CIFAR%20benchmarks%20under%20joint%20training%2C%20demonstrating%20that%0Auncertainty-modulated%20gain%20dynamics%20effectively%20attenuate%20the%20stability%20gap.%0AFinally%2C%20our%20analysis%20elucidates%20how%20gain%20modulation%20replicates%20noradrenergic%0Afunctions%20in%20cortical%20circuits%2C%20offering%20mechanistic%20insights%20into%20reducing%0Astability%20gaps%20and%20enhance%20performance%20in%20continual%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoradrenergic-inspired%2520gain%2520modulation%2520attenuates%2520the%2520stability%2520gap%2520in%250A%2520%2520joint%2520training%26entry.906535625%3DAlejandro%2520Rodriguez-Garcia%2520and%2520Anindya%2520Ghosh%2520and%2520Srikanth%2520Ramaswamy%26entry.1292438233%3D%2520%2520Recent%2520studies%2520in%2520continual%2520learning%2520have%2520identified%2520a%2520transient%2520drop%2520in%250Aperformance%2520on%2520mastered%2520tasks%2520when%2520assimilating%2520new%2520ones%252C%2520known%2520as%2520the%250Astability%2520gap.%2520Such%2520dynamics%2520contradict%2520the%2520objectives%2520of%2520continual%2520learning%252C%250Arevealing%2520a%2520lack%2520of%2520robustness%2520in%2520mitigating%2520forgetting%252C%2520and%2520notably%252C%250Apersisting%2520even%2520under%2520an%2520ideal%2520joint-loss%2520regime.%2520Examining%2520this%2520gap%2520within%250Athis%2520idealized%2520joint%2520training%2520context%2520is%2520critical%2520to%2520isolate%2520it%2520from%2520other%250Asources%2520of%2520forgetting.%2520We%2520argue%2520that%2520it%2520reflects%2520an%2520imbalance%2520between%2520rapid%250Aadaptation%2520and%2520robust%2520retention%2520at%2520task%2520boundaries%252C%2520underscoring%2520the%2520need%2520to%250Ainvestigate%2520mechanisms%2520that%2520reconcile%2520plasticity%2520and%2520stability%2520within%2520continual%250Alearning%2520frameworks.%2520Biological%2520brains%2520navigate%2520a%2520similar%2520dilemma%2520by%2520operating%250Aconcurrently%2520on%2520multiple%2520timescales%252C%2520leveraging%2520neuromodulatory%2520signals%2520to%250Amodulate%2520synaptic%2520plasticity.%2520However%252C%2520artificial%2520networks%2520lack%2520native%250Amultitimescale%2520dynamics%252C%2520and%2520although%2520optimizers%2520like%2520momentum-SGD%2520and%2520Adam%250Aintroduce%2520implicit%2520timescale%2520regularization%252C%2520they%2520still%2520exhibit%2520stability%2520gaps.%250AInspired%2520by%2520locus%2520coeruleus%2520mediated%2520noradrenergic%2520bursts%252C%2520which%2520transiently%250Aenhance%2520neuronal%2520gain%2520under%2520uncertainty%2520to%2520facilitate%2520sensory%2520assimilation%252C%2520we%250Apropose%2520uncertainty-modulated%2520gain%2520dynamics%2520-%2520an%2520adaptive%2520mechanism%2520that%250Aapproximates%2520a%2520two-timescale%2520optimizer%2520and%2520dynamically%2520balances%2520integration%2520of%250Aknowledge%2520with%2520minimal%2520interference%2520on%2520previously%2520consolidated%2520information.%2520We%250Aevaluate%2520our%2520mechanism%2520on%2520domain-incremental%2520and%2520class-incremental%2520variants%2520of%250Athe%2520MNIST%2520and%2520CIFAR%2520benchmarks%2520under%2520joint%2520training%252C%2520demonstrating%2520that%250Auncertainty-modulated%2520gain%2520dynamics%2520effectively%2520attenuate%2520the%2520stability%2520gap.%250AFinally%252C%2520our%2520analysis%2520elucidates%2520how%2520gain%2520modulation%2520replicates%2520noradrenergic%250Afunctions%2520in%2520cortical%2520circuits%252C%2520offering%2520mechanistic%2520insights%2520into%2520reducing%250Astability%2520gaps%2520and%2520enhance%2520performance%2520in%2520continual%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noradrenergic-inspired%20gain%20modulation%20attenuates%20the%20stability%20gap%20in%0A%20%20joint%20training&entry.906535625=Alejandro%20Rodriguez-Garcia%20and%20Anindya%20Ghosh%20and%20Srikanth%20Ramaswamy&entry.1292438233=%20%20Recent%20studies%20in%20continual%20learning%20have%20identified%20a%20transient%20drop%20in%0Aperformance%20on%20mastered%20tasks%20when%20assimilating%20new%20ones%2C%20known%20as%20the%0Astability%20gap.%20Such%20dynamics%20contradict%20the%20objectives%20of%20continual%20learning%2C%0Arevealing%20a%20lack%20of%20robustness%20in%20mitigating%20forgetting%2C%20and%20notably%2C%0Apersisting%20even%20under%20an%20ideal%20joint-loss%20regime.%20Examining%20this%20gap%20within%0Athis%20idealized%20joint%20training%20context%20is%20critical%20to%20isolate%20it%20from%20other%0Asources%20of%20forgetting.%20We%20argue%20that%20it%20reflects%20an%20imbalance%20between%20rapid%0Aadaptation%20and%20robust%20retention%20at%20task%20boundaries%2C%20underscoring%20the%20need%20to%0Ainvestigate%20mechanisms%20that%20reconcile%20plasticity%20and%20stability%20within%20continual%0Alearning%20frameworks.%20Biological%20brains%20navigate%20a%20similar%20dilemma%20by%20operating%0Aconcurrently%20on%20multiple%20timescales%2C%20leveraging%20neuromodulatory%20signals%20to%0Amodulate%20synaptic%20plasticity.%20However%2C%20artificial%20networks%20lack%20native%0Amultitimescale%20dynamics%2C%20and%20although%20optimizers%20like%20momentum-SGD%20and%20Adam%0Aintroduce%20implicit%20timescale%20regularization%2C%20they%20still%20exhibit%20stability%20gaps.%0AInspired%20by%20locus%20coeruleus%20mediated%20noradrenergic%20bursts%2C%20which%20transiently%0Aenhance%20neuronal%20gain%20under%20uncertainty%20to%20facilitate%20sensory%20assimilation%2C%20we%0Apropose%20uncertainty-modulated%20gain%20dynamics%20-%20an%20adaptive%20mechanism%20that%0Aapproximates%20a%20two-timescale%20optimizer%20and%20dynamically%20balances%20integration%20of%0Aknowledge%20with%20minimal%20interference%20on%20previously%20consolidated%20information.%20We%0Aevaluate%20our%20mechanism%20on%20domain-incremental%20and%20class-incremental%20variants%20of%0Athe%20MNIST%20and%20CIFAR%20benchmarks%20under%20joint%20training%2C%20demonstrating%20that%0Auncertainty-modulated%20gain%20dynamics%20effectively%20attenuate%20the%20stability%20gap.%0AFinally%2C%20our%20analysis%20elucidates%20how%20gain%20modulation%20replicates%20noradrenergic%0Afunctions%20in%20cortical%20circuits%2C%20offering%20mechanistic%20insights%20into%20reducing%0Astability%20gaps%20and%20enhance%20performance%20in%20continual%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14056v1&entry.124074799=Read"},
{"title": "Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and\n  Re-Identification using Colored Point Clouds", "author": "Daniel Fusaro and Federico Magistri and Jens Behley and Alberto Pretto and Cyrill Stachniss", "abstract": "  Accurate and consistent fruit monitoring over time is a key step toward\nautomated agricultural production systems. However, this task is inherently\ndifficult due to variations in fruit size, shape, occlusion, orientation, and\nthe dynamic nature of orchards where fruits may appear or disappear between\nobservations. In this article, we propose a novel method for fruit instance\nsegmentation and re-identification on 3D terrestrial point clouds collected\nover time. Our approach directly operates on dense colored point clouds,\ncapturing fine-grained 3D spatial detail. We segment individual fruits using a\nlearning-based instance segmentation method applied directly to the point\ncloud. For each segmented fruit, we extract a compact and discriminative\ndescriptor using a 3D sparse convolutional neural network. To track fruits\nacross different times, we introduce an attention-based matching network that\nassociates fruits with their counterparts from previous sessions. Matching is\nperformed using a probabilistic assignment scheme, selecting the most likely\nassociations across time. We evaluate our approach on real-world datasets of\nstrawberries and apples, demonstrating that it outperforms existing methods in\nboth instance segmentation and temporal re-identification, enabling robust and\nprecise fruit monitoring across complex and dynamic orchard environments.\n", "link": "http://arxiv.org/abs/2411.07799v2", "date": "2025-07-18", "relevancy": 2.0964, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5234}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Horticultural%20Temporal%20Fruit%20Monitoring%20via%203D%20Instance%20Segmentation%20and%0A%20%20Re-Identification%20using%20Colored%20Point%20Clouds&body=Title%3A%20Horticultural%20Temporal%20Fruit%20Monitoring%20via%203D%20Instance%20Segmentation%20and%0A%20%20Re-Identification%20using%20Colored%20Point%20Clouds%0AAuthor%3A%20Daniel%20Fusaro%20and%20Federico%20Magistri%20and%20Jens%20Behley%20and%20Alberto%20Pretto%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Accurate%20and%20consistent%20fruit%20monitoring%20over%20time%20is%20a%20key%20step%20toward%0Aautomated%20agricultural%20production%20systems.%20However%2C%20this%20task%20is%20inherently%0Adifficult%20due%20to%20variations%20in%20fruit%20size%2C%20shape%2C%20occlusion%2C%20orientation%2C%20and%0Athe%20dynamic%20nature%20of%20orchards%20where%20fruits%20may%20appear%20or%20disappear%20between%0Aobservations.%20In%20this%20article%2C%20we%20propose%20a%20novel%20method%20for%20fruit%20instance%0Asegmentation%20and%20re-identification%20on%203D%20terrestrial%20point%20clouds%20collected%0Aover%20time.%20Our%20approach%20directly%20operates%20on%20dense%20colored%20point%20clouds%2C%0Acapturing%20fine-grained%203D%20spatial%20detail.%20We%20segment%20individual%20fruits%20using%20a%0Alearning-based%20instance%20segmentation%20method%20applied%20directly%20to%20the%20point%0Acloud.%20For%20each%20segmented%20fruit%2C%20we%20extract%20a%20compact%20and%20discriminative%0Adescriptor%20using%20a%203D%20sparse%20convolutional%20neural%20network.%20To%20track%20fruits%0Aacross%20different%20times%2C%20we%20introduce%20an%20attention-based%20matching%20network%20that%0Aassociates%20fruits%20with%20their%20counterparts%20from%20previous%20sessions.%20Matching%20is%0Aperformed%20using%20a%20probabilistic%20assignment%20scheme%2C%20selecting%20the%20most%20likely%0Aassociations%20across%20time.%20We%20evaluate%20our%20approach%20on%20real-world%20datasets%20of%0Astrawberries%20and%20apples%2C%20demonstrating%20that%20it%20outperforms%20existing%20methods%20in%0Aboth%20instance%20segmentation%20and%20temporal%20re-identification%2C%20enabling%20robust%20and%0Aprecise%20fruit%20monitoring%20across%20complex%20and%20dynamic%20orchard%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHorticultural%2520Temporal%2520Fruit%2520Monitoring%2520via%25203D%2520Instance%2520Segmentation%2520and%250A%2520%2520Re-Identification%2520using%2520Colored%2520Point%2520Clouds%26entry.906535625%3DDaniel%2520Fusaro%2520and%2520Federico%2520Magistri%2520and%2520Jens%2520Behley%2520and%2520Alberto%2520Pretto%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Accurate%2520and%2520consistent%2520fruit%2520monitoring%2520over%2520time%2520is%2520a%2520key%2520step%2520toward%250Aautomated%2520agricultural%2520production%2520systems.%2520However%252C%2520this%2520task%2520is%2520inherently%250Adifficult%2520due%2520to%2520variations%2520in%2520fruit%2520size%252C%2520shape%252C%2520occlusion%252C%2520orientation%252C%2520and%250Athe%2520dynamic%2520nature%2520of%2520orchards%2520where%2520fruits%2520may%2520appear%2520or%2520disappear%2520between%250Aobservations.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520fruit%2520instance%250Asegmentation%2520and%2520re-identification%2520on%25203D%2520terrestrial%2520point%2520clouds%2520collected%250Aover%2520time.%2520Our%2520approach%2520directly%2520operates%2520on%2520dense%2520colored%2520point%2520clouds%252C%250Acapturing%2520fine-grained%25203D%2520spatial%2520detail.%2520We%2520segment%2520individual%2520fruits%2520using%2520a%250Alearning-based%2520instance%2520segmentation%2520method%2520applied%2520directly%2520to%2520the%2520point%250Acloud.%2520For%2520each%2520segmented%2520fruit%252C%2520we%2520extract%2520a%2520compact%2520and%2520discriminative%250Adescriptor%2520using%2520a%25203D%2520sparse%2520convolutional%2520neural%2520network.%2520To%2520track%2520fruits%250Aacross%2520different%2520times%252C%2520we%2520introduce%2520an%2520attention-based%2520matching%2520network%2520that%250Aassociates%2520fruits%2520with%2520their%2520counterparts%2520from%2520previous%2520sessions.%2520Matching%2520is%250Aperformed%2520using%2520a%2520probabilistic%2520assignment%2520scheme%252C%2520selecting%2520the%2520most%2520likely%250Aassociations%2520across%2520time.%2520We%2520evaluate%2520our%2520approach%2520on%2520real-world%2520datasets%2520of%250Astrawberries%2520and%2520apples%252C%2520demonstrating%2520that%2520it%2520outperforms%2520existing%2520methods%2520in%250Aboth%2520instance%2520segmentation%2520and%2520temporal%2520re-identification%252C%2520enabling%2520robust%2520and%250Aprecise%2520fruit%2520monitoring%2520across%2520complex%2520and%2520dynamic%2520orchard%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Horticultural%20Temporal%20Fruit%20Monitoring%20via%203D%20Instance%20Segmentation%20and%0A%20%20Re-Identification%20using%20Colored%20Point%20Clouds&entry.906535625=Daniel%20Fusaro%20and%20Federico%20Magistri%20and%20Jens%20Behley%20and%20Alberto%20Pretto%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Accurate%20and%20consistent%20fruit%20monitoring%20over%20time%20is%20a%20key%20step%20toward%0Aautomated%20agricultural%20production%20systems.%20However%2C%20this%20task%20is%20inherently%0Adifficult%20due%20to%20variations%20in%20fruit%20size%2C%20shape%2C%20occlusion%2C%20orientation%2C%20and%0Athe%20dynamic%20nature%20of%20orchards%20where%20fruits%20may%20appear%20or%20disappear%20between%0Aobservations.%20In%20this%20article%2C%20we%20propose%20a%20novel%20method%20for%20fruit%20instance%0Asegmentation%20and%20re-identification%20on%203D%20terrestrial%20point%20clouds%20collected%0Aover%20time.%20Our%20approach%20directly%20operates%20on%20dense%20colored%20point%20clouds%2C%0Acapturing%20fine-grained%203D%20spatial%20detail.%20We%20segment%20individual%20fruits%20using%20a%0Alearning-based%20instance%20segmentation%20method%20applied%20directly%20to%20the%20point%0Acloud.%20For%20each%20segmented%20fruit%2C%20we%20extract%20a%20compact%20and%20discriminative%0Adescriptor%20using%20a%203D%20sparse%20convolutional%20neural%20network.%20To%20track%20fruits%0Aacross%20different%20times%2C%20we%20introduce%20an%20attention-based%20matching%20network%20that%0Aassociates%20fruits%20with%20their%20counterparts%20from%20previous%20sessions.%20Matching%20is%0Aperformed%20using%20a%20probabilistic%20assignment%20scheme%2C%20selecting%20the%20most%20likely%0Aassociations%20across%20time.%20We%20evaluate%20our%20approach%20on%20real-world%20datasets%20of%0Astrawberries%20and%20apples%2C%20demonstrating%20that%20it%20outperforms%20existing%20methods%20in%0Aboth%20instance%20segmentation%20and%20temporal%20re-identification%2C%20enabling%20robust%20and%0Aprecise%20fruit%20monitoring%20across%20complex%20and%20dynamic%20orchard%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07799v2&entry.124074799=Read"},
{"title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization\n  and Its Effects on Video Anomaly Detection", "author": "Sara Abdulaziz and Egor Bondarev", "abstract": "  Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.\n", "link": "http://arxiv.org/abs/2507.14083v1", "date": "2025-07-18", "relevancy": 2.0899, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5522}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5176}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unmasking%20Performance%20Gaps%3A%20A%20Comparative%20Study%20of%20Human%20Anonymization%0A%20%20and%20Its%20Effects%20on%20Video%20Anomaly%20Detection&body=Title%3A%20Unmasking%20Performance%20Gaps%3A%20A%20Comparative%20Study%20of%20Human%20Anonymization%0A%20%20and%20Its%20Effects%20on%20Video%20Anomaly%20Detection%0AAuthor%3A%20Sara%20Abdulaziz%20and%20Egor%20Bondarev%0AAbstract%3A%20%20%20Advancements%20in%20deep%20learning%20have%20improved%20anomaly%20detection%20in%20surveillance%0Avideos%2C%20yet%20they%20raise%20urgent%20privacy%20concerns%20due%20to%20the%20collection%20of%0Asensitive%20human%20data.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20analysis%20of%0Aanomaly%20detection%20performance%20under%20four%20human%20anonymization%20techniques%2C%0Aincluding%20blurring%2C%20masking%2C%20encryption%2C%20and%20avatar%20replacement%2C%20applied%20to%20the%0AUCF-Crime%20dataset.%20We%20evaluate%20four%20anomaly%20detection%20methods%2C%20MGFN%2C%20UR-DMU%2C%0ABN-WVAD%2C%20and%20PEL4VAD%2C%20on%20the%20anonymized%20UCF-Crime%20to%20reveal%20how%20each%20method%0Aresponds%20to%20different%20obfuscation%20techniques.%20Experimental%20results%20demonstrate%0Athat%20anomaly%20detection%20remains%20viable%20under%20anonymized%20data%20and%20is%20dependent%20on%0Athe%20algorithmic%20design%20and%20the%20learning%20strategy.%20For%20instance%2C%20under%20certain%0Aanonymization%20patterns%2C%20such%20as%20encryption%20and%20masking%2C%20some%20models%0Ainadvertently%20achieve%20higher%20AUC%20performance%20compared%20to%20raw%20data%2C%20due%20to%20the%0Astrong%20responsiveness%20of%20their%20algorithmic%20components%20to%20these%20noise%20patterns.%0AThese%20results%20highlight%20the%20algorithm-specific%20sensitivities%20to%20anonymization%0Aand%20emphasize%20the%20trade-off%20between%20preserving%20privacy%20and%20maintaining%0Adetection%20utility.%20Furthermore%2C%20we%20compare%20these%20conventional%20anonymization%0Atechniques%20with%20the%20emerging%20privacy-by-design%20solutions%2C%20highlighting%20an%20often%0Aoverlooked%20trade-off%20between%20robust%20privacy%20protection%20and%20utility%20flexibility.%0AThrough%20comprehensive%20experiments%20and%20analyses%2C%20this%20study%20provides%20a%0Acompelling%20benchmark%20and%20insights%20into%20balancing%20human%20privacy%20with%20the%20demands%0Aof%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnmasking%2520Performance%2520Gaps%253A%2520A%2520Comparative%2520Study%2520of%2520Human%2520Anonymization%250A%2520%2520and%2520Its%2520Effects%2520on%2520Video%2520Anomaly%2520Detection%26entry.906535625%3DSara%2520Abdulaziz%2520and%2520Egor%2520Bondarev%26entry.1292438233%3D%2520%2520Advancements%2520in%2520deep%2520learning%2520have%2520improved%2520anomaly%2520detection%2520in%2520surveillance%250Avideos%252C%2520yet%2520they%2520raise%2520urgent%2520privacy%2520concerns%2520due%2520to%2520the%2520collection%2520of%250Asensitive%2520human%2520data.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%2520analysis%2520of%250Aanomaly%2520detection%2520performance%2520under%2520four%2520human%2520anonymization%2520techniques%252C%250Aincluding%2520blurring%252C%2520masking%252C%2520encryption%252C%2520and%2520avatar%2520replacement%252C%2520applied%2520to%2520the%250AUCF-Crime%2520dataset.%2520We%2520evaluate%2520four%2520anomaly%2520detection%2520methods%252C%2520MGFN%252C%2520UR-DMU%252C%250ABN-WVAD%252C%2520and%2520PEL4VAD%252C%2520on%2520the%2520anonymized%2520UCF-Crime%2520to%2520reveal%2520how%2520each%2520method%250Aresponds%2520to%2520different%2520obfuscation%2520techniques.%2520Experimental%2520results%2520demonstrate%250Athat%2520anomaly%2520detection%2520remains%2520viable%2520under%2520anonymized%2520data%2520and%2520is%2520dependent%2520on%250Athe%2520algorithmic%2520design%2520and%2520the%2520learning%2520strategy.%2520For%2520instance%252C%2520under%2520certain%250Aanonymization%2520patterns%252C%2520such%2520as%2520encryption%2520and%2520masking%252C%2520some%2520models%250Ainadvertently%2520achieve%2520higher%2520AUC%2520performance%2520compared%2520to%2520raw%2520data%252C%2520due%2520to%2520the%250Astrong%2520responsiveness%2520of%2520their%2520algorithmic%2520components%2520to%2520these%2520noise%2520patterns.%250AThese%2520results%2520highlight%2520the%2520algorithm-specific%2520sensitivities%2520to%2520anonymization%250Aand%2520emphasize%2520the%2520trade-off%2520between%2520preserving%2520privacy%2520and%2520maintaining%250Adetection%2520utility.%2520Furthermore%252C%2520we%2520compare%2520these%2520conventional%2520anonymization%250Atechniques%2520with%2520the%2520emerging%2520privacy-by-design%2520solutions%252C%2520highlighting%2520an%2520often%250Aoverlooked%2520trade-off%2520between%2520robust%2520privacy%2520protection%2520and%2520utility%2520flexibility.%250AThrough%2520comprehensive%2520experiments%2520and%2520analyses%252C%2520this%2520study%2520provides%2520a%250Acompelling%2520benchmark%2520and%2520insights%2520into%2520balancing%2520human%2520privacy%2520with%2520the%2520demands%250Aof%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unmasking%20Performance%20Gaps%3A%20A%20Comparative%20Study%20of%20Human%20Anonymization%0A%20%20and%20Its%20Effects%20on%20Video%20Anomaly%20Detection&entry.906535625=Sara%20Abdulaziz%20and%20Egor%20Bondarev&entry.1292438233=%20%20Advancements%20in%20deep%20learning%20have%20improved%20anomaly%20detection%20in%20surveillance%0Avideos%2C%20yet%20they%20raise%20urgent%20privacy%20concerns%20due%20to%20the%20collection%20of%0Asensitive%20human%20data.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%20analysis%20of%0Aanomaly%20detection%20performance%20under%20four%20human%20anonymization%20techniques%2C%0Aincluding%20blurring%2C%20masking%2C%20encryption%2C%20and%20avatar%20replacement%2C%20applied%20to%20the%0AUCF-Crime%20dataset.%20We%20evaluate%20four%20anomaly%20detection%20methods%2C%20MGFN%2C%20UR-DMU%2C%0ABN-WVAD%2C%20and%20PEL4VAD%2C%20on%20the%20anonymized%20UCF-Crime%20to%20reveal%20how%20each%20method%0Aresponds%20to%20different%20obfuscation%20techniques.%20Experimental%20results%20demonstrate%0Athat%20anomaly%20detection%20remains%20viable%20under%20anonymized%20data%20and%20is%20dependent%20on%0Athe%20algorithmic%20design%20and%20the%20learning%20strategy.%20For%20instance%2C%20under%20certain%0Aanonymization%20patterns%2C%20such%20as%20encryption%20and%20masking%2C%20some%20models%0Ainadvertently%20achieve%20higher%20AUC%20performance%20compared%20to%20raw%20data%2C%20due%20to%20the%0Astrong%20responsiveness%20of%20their%20algorithmic%20components%20to%20these%20noise%20patterns.%0AThese%20results%20highlight%20the%20algorithm-specific%20sensitivities%20to%20anonymization%0Aand%20emphasize%20the%20trade-off%20between%20preserving%20privacy%20and%20maintaining%0Adetection%20utility.%20Furthermore%2C%20we%20compare%20these%20conventional%20anonymization%0Atechniques%20with%20the%20emerging%20privacy-by-design%20solutions%2C%20highlighting%20an%20often%0Aoverlooked%20trade-off%20between%20robust%20privacy%20protection%20and%20utility%20flexibility.%0AThrough%20comprehensive%20experiments%20and%20analyses%2C%20this%20study%20provides%20a%0Acompelling%20benchmark%20and%20insights%20into%20balancing%20human%20privacy%20with%20the%20demands%0Aof%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14083v1&entry.124074799=Read"},
{"title": "Improved DDIM Sampling with Moment Matching Gaussian Mixtures", "author": "Prasad Gabbur", "abstract": "  We propose using a Gaussian Mixture Model (GMM) as reverse transition\noperator (kernel) within the Denoising Diffusion Implicit Models (DDIM)\nframework, which is one of the most widely used approaches for accelerated\nsampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM).\nSpecifically we match the first and second order central moments of the DDPM\nforward marginals by constraining the parameters of the GMM. We see that moment\nmatching is sufficient to obtain samples with equal or better quality than the\noriginal DDIM with Gaussian kernels. We provide experimental results with\nunconditional models trained on CelebAHQ and FFHQ and class-conditional models\ntrained on ImageNet datasets respectively. Our results suggest that using the\nGMM kernel leads to significant improvements in the quality of the generated\nsamples when the number of sampling steps is small, as measured by FID and IS\nmetrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a\nFID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73\nrespectively with a Gaussian kernel.\n", "link": "http://arxiv.org/abs/2311.04938v3", "date": "2025-07-18", "relevancy": 2.0834, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5462}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5186}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20DDIM%20Sampling%20with%20Moment%20Matching%20Gaussian%20Mixtures&body=Title%3A%20Improved%20DDIM%20Sampling%20with%20Moment%20Matching%20Gaussian%20Mixtures%0AAuthor%3A%20Prasad%20Gabbur%0AAbstract%3A%20%20%20We%20propose%20using%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20as%20reverse%20transition%0Aoperator%20%28kernel%29%20within%20the%20Denoising%20Diffusion%20Implicit%20Models%20%28DDIM%29%0Aframework%2C%20which%20is%20one%20of%20the%20most%20widely%20used%20approaches%20for%20accelerated%0Asampling%20from%20pre-trained%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29.%0ASpecifically%20we%20match%20the%20first%20and%20second%20order%20central%20moments%20of%20the%20DDPM%0Aforward%20marginals%20by%20constraining%20the%20parameters%20of%20the%20GMM.%20We%20see%20that%20moment%0Amatching%20is%20sufficient%20to%20obtain%20samples%20with%20equal%20or%20better%20quality%20than%20the%0Aoriginal%20DDIM%20with%20Gaussian%20kernels.%20We%20provide%20experimental%20results%20with%0Aunconditional%20models%20trained%20on%20CelebAHQ%20and%20FFHQ%20and%20class-conditional%20models%0Atrained%20on%20ImageNet%20datasets%20respectively.%20Our%20results%20suggest%20that%20using%20the%0AGMM%20kernel%20leads%20to%20significant%20improvements%20in%20the%20quality%20of%20the%20generated%0Asamples%20when%20the%20number%20of%20sampling%20steps%20is%20small%2C%20as%20measured%20by%20FID%20and%20IS%0Ametrics.%20For%20example%20on%20ImageNet%20256x256%2C%20using%2010%20sampling%20steps%2C%20we%20achieve%20a%0AFID%20of%206.94%20and%20IS%20of%20207.85%20with%20a%20GMM%20kernel%20compared%20to%2010.15%20and%20196.73%0Arespectively%20with%20a%20Gaussian%20kernel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.04938v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520DDIM%2520Sampling%2520with%2520Moment%2520Matching%2520Gaussian%2520Mixtures%26entry.906535625%3DPrasad%2520Gabbur%26entry.1292438233%3D%2520%2520We%2520propose%2520using%2520a%2520Gaussian%2520Mixture%2520Model%2520%2528GMM%2529%2520as%2520reverse%2520transition%250Aoperator%2520%2528kernel%2529%2520within%2520the%2520Denoising%2520Diffusion%2520Implicit%2520Models%2520%2528DDIM%2529%250Aframework%252C%2520which%2520is%2520one%2520of%2520the%2520most%2520widely%2520used%2520approaches%2520for%2520accelerated%250Asampling%2520from%2520pre-trained%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPM%2529.%250ASpecifically%2520we%2520match%2520the%2520first%2520and%2520second%2520order%2520central%2520moments%2520of%2520the%2520DDPM%250Aforward%2520marginals%2520by%2520constraining%2520the%2520parameters%2520of%2520the%2520GMM.%2520We%2520see%2520that%2520moment%250Amatching%2520is%2520sufficient%2520to%2520obtain%2520samples%2520with%2520equal%2520or%2520better%2520quality%2520than%2520the%250Aoriginal%2520DDIM%2520with%2520Gaussian%2520kernels.%2520We%2520provide%2520experimental%2520results%2520with%250Aunconditional%2520models%2520trained%2520on%2520CelebAHQ%2520and%2520FFHQ%2520and%2520class-conditional%2520models%250Atrained%2520on%2520ImageNet%2520datasets%2520respectively.%2520Our%2520results%2520suggest%2520that%2520using%2520the%250AGMM%2520kernel%2520leads%2520to%2520significant%2520improvements%2520in%2520the%2520quality%2520of%2520the%2520generated%250Asamples%2520when%2520the%2520number%2520of%2520sampling%2520steps%2520is%2520small%252C%2520as%2520measured%2520by%2520FID%2520and%2520IS%250Ametrics.%2520For%2520example%2520on%2520ImageNet%2520256x256%252C%2520using%252010%2520sampling%2520steps%252C%2520we%2520achieve%2520a%250AFID%2520of%25206.94%2520and%2520IS%2520of%2520207.85%2520with%2520a%2520GMM%2520kernel%2520compared%2520to%252010.15%2520and%2520196.73%250Arespectively%2520with%2520a%2520Gaussian%2520kernel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.04938v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20DDIM%20Sampling%20with%20Moment%20Matching%20Gaussian%20Mixtures&entry.906535625=Prasad%20Gabbur&entry.1292438233=%20%20We%20propose%20using%20a%20Gaussian%20Mixture%20Model%20%28GMM%29%20as%20reverse%20transition%0Aoperator%20%28kernel%29%20within%20the%20Denoising%20Diffusion%20Implicit%20Models%20%28DDIM%29%0Aframework%2C%20which%20is%20one%20of%20the%20most%20widely%20used%20approaches%20for%20accelerated%0Asampling%20from%20pre-trained%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPM%29.%0ASpecifically%20we%20match%20the%20first%20and%20second%20order%20central%20moments%20of%20the%20DDPM%0Aforward%20marginals%20by%20constraining%20the%20parameters%20of%20the%20GMM.%20We%20see%20that%20moment%0Amatching%20is%20sufficient%20to%20obtain%20samples%20with%20equal%20or%20better%20quality%20than%20the%0Aoriginal%20DDIM%20with%20Gaussian%20kernels.%20We%20provide%20experimental%20results%20with%0Aunconditional%20models%20trained%20on%20CelebAHQ%20and%20FFHQ%20and%20class-conditional%20models%0Atrained%20on%20ImageNet%20datasets%20respectively.%20Our%20results%20suggest%20that%20using%20the%0AGMM%20kernel%20leads%20to%20significant%20improvements%20in%20the%20quality%20of%20the%20generated%0Asamples%20when%20the%20number%20of%20sampling%20steps%20is%20small%2C%20as%20measured%20by%20FID%20and%20IS%0Ametrics.%20For%20example%20on%20ImageNet%20256x256%2C%20using%2010%20sampling%20steps%2C%20we%20achieve%20a%0AFID%20of%206.94%20and%20IS%20of%20207.85%20with%20a%20GMM%20kernel%20compared%20to%2010.15%20and%20196.73%0Arespectively%20with%20a%20Gaussian%20kernel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.04938v3&entry.124074799=Read"},
{"title": "VLA-Mark: A cross modal watermark for large vision-language alignment\n  model", "author": "Shuliang Liu and Qi Zheng and Jesse Jiaxi Xu and Yibo Yan and He Geng and Aiwei Liu and Peijie Jiang and Jia Liu and Yik-Cheung Tam and Xuming Hu", "abstract": "  Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking\n", "link": "http://arxiv.org/abs/2507.14067v1", "date": "2025-07-18", "relevancy": 2.0798, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5282}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA-Mark%3A%20A%20cross%20modal%20watermark%20for%20large%20vision-language%20alignment%0A%20%20model&body=Title%3A%20VLA-Mark%3A%20A%20cross%20modal%20watermark%20for%20large%20vision-language%20alignment%0A%20%20model%0AAuthor%3A%20Shuliang%20Liu%20and%20Qi%20Zheng%20and%20Jesse%20Jiaxi%20Xu%20and%20Yibo%20Yan%20and%20He%20Geng%20and%20Aiwei%20Liu%20and%20Peijie%20Jiang%20and%20Jia%20Liu%20and%20Yik-Cheung%20Tam%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Vision-language%20models%20demand%20watermarking%20solutions%20that%20protect%0Aintellectual%20property%20without%20compromising%20multimodal%20coherence.%20Existing%20text%0Awatermarking%20methods%20disrupt%20visual-textual%20alignment%20through%20biased%20token%0Aselection%20and%20static%20strategies%2C%20leaving%20semantic-critical%20concepts%20vulnerable.%0AWe%20propose%20VLA-Mark%2C%20a%20vision-aligned%20framework%20that%20embeds%20detectable%0Awatermarks%20while%20preserving%20semantic%20fidelity%20through%20cross-modal%20coordination.%0AOur%20approach%20integrates%20multiscale%20visual-textual%20alignment%20metrics%2C%20combining%0Alocalized%20patch%20affinity%2C%20global%20semantic%20coherence%2C%20and%20contextual%20attention%0Apatterns%2C%20to%20guide%20watermark%20injection%20without%20model%20retraining.%20An%0Aentropy-sensitive%20mechanism%20dynamically%20balances%20watermark%20strength%20and%0Asemantic%20preservation%2C%20prioritizing%20visual%20grounding%20during%20low-uncertainty%0Ageneration%20phases.%20Experiments%20show%207.4%25%20lower%20PPL%20and%2026.6%25%20higher%20BLEU%20than%0Aconventional%20methods%2C%20with%20near-perfect%20detection%20%2898.8%25%20AUC%29.%20The%20framework%0Ademonstrates%2096.1%5C%25%20attack%20resilience%20against%20attacks%20such%20as%20paraphrasing%20and%0Asynonym%20substitution%2C%20while%20maintaining%20text-visual%20consistency%2C%20establishing%0Anew%20standards%20for%20quality-preserving%20multimodal%20watermarking%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA-Mark%253A%2520A%2520cross%2520modal%2520watermark%2520for%2520large%2520vision-language%2520alignment%250A%2520%2520model%26entry.906535625%3DShuliang%2520Liu%2520and%2520Qi%2520Zheng%2520and%2520Jesse%2520Jiaxi%2520Xu%2520and%2520Yibo%2520Yan%2520and%2520He%2520Geng%2520and%2520Aiwei%2520Liu%2520and%2520Peijie%2520Jiang%2520and%2520Jia%2520Liu%2520and%2520Yik-Cheung%2520Tam%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520demand%2520watermarking%2520solutions%2520that%2520protect%250Aintellectual%2520property%2520without%2520compromising%2520multimodal%2520coherence.%2520Existing%2520text%250Awatermarking%2520methods%2520disrupt%2520visual-textual%2520alignment%2520through%2520biased%2520token%250Aselection%2520and%2520static%2520strategies%252C%2520leaving%2520semantic-critical%2520concepts%2520vulnerable.%250AWe%2520propose%2520VLA-Mark%252C%2520a%2520vision-aligned%2520framework%2520that%2520embeds%2520detectable%250Awatermarks%2520while%2520preserving%2520semantic%2520fidelity%2520through%2520cross-modal%2520coordination.%250AOur%2520approach%2520integrates%2520multiscale%2520visual-textual%2520alignment%2520metrics%252C%2520combining%250Alocalized%2520patch%2520affinity%252C%2520global%2520semantic%2520coherence%252C%2520and%2520contextual%2520attention%250Apatterns%252C%2520to%2520guide%2520watermark%2520injection%2520without%2520model%2520retraining.%2520An%250Aentropy-sensitive%2520mechanism%2520dynamically%2520balances%2520watermark%2520strength%2520and%250Asemantic%2520preservation%252C%2520prioritizing%2520visual%2520grounding%2520during%2520low-uncertainty%250Ageneration%2520phases.%2520Experiments%2520show%25207.4%2525%2520lower%2520PPL%2520and%252026.6%2525%2520higher%2520BLEU%2520than%250Aconventional%2520methods%252C%2520with%2520near-perfect%2520detection%2520%252898.8%2525%2520AUC%2529.%2520The%2520framework%250Ademonstrates%252096.1%255C%2525%2520attack%2520resilience%2520against%2520attacks%2520such%2520as%2520paraphrasing%2520and%250Asynonym%2520substitution%252C%2520while%2520maintaining%2520text-visual%2520consistency%252C%2520establishing%250Anew%2520standards%2520for%2520quality-preserving%2520multimodal%2520watermarking%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA-Mark%3A%20A%20cross%20modal%20watermark%20for%20large%20vision-language%20alignment%0A%20%20model&entry.906535625=Shuliang%20Liu%20and%20Qi%20Zheng%20and%20Jesse%20Jiaxi%20Xu%20and%20Yibo%20Yan%20and%20He%20Geng%20and%20Aiwei%20Liu%20and%20Peijie%20Jiang%20and%20Jia%20Liu%20and%20Yik-Cheung%20Tam%20and%20Xuming%20Hu&entry.1292438233=%20%20Vision-language%20models%20demand%20watermarking%20solutions%20that%20protect%0Aintellectual%20property%20without%20compromising%20multimodal%20coherence.%20Existing%20text%0Awatermarking%20methods%20disrupt%20visual-textual%20alignment%20through%20biased%20token%0Aselection%20and%20static%20strategies%2C%20leaving%20semantic-critical%20concepts%20vulnerable.%0AWe%20propose%20VLA-Mark%2C%20a%20vision-aligned%20framework%20that%20embeds%20detectable%0Awatermarks%20while%20preserving%20semantic%20fidelity%20through%20cross-modal%20coordination.%0AOur%20approach%20integrates%20multiscale%20visual-textual%20alignment%20metrics%2C%20combining%0Alocalized%20patch%20affinity%2C%20global%20semantic%20coherence%2C%20and%20contextual%20attention%0Apatterns%2C%20to%20guide%20watermark%20injection%20without%20model%20retraining.%20An%0Aentropy-sensitive%20mechanism%20dynamically%20balances%20watermark%20strength%20and%0Asemantic%20preservation%2C%20prioritizing%20visual%20grounding%20during%20low-uncertainty%0Ageneration%20phases.%20Experiments%20show%207.4%25%20lower%20PPL%20and%2026.6%25%20higher%20BLEU%20than%0Aconventional%20methods%2C%20with%20near-perfect%20detection%20%2898.8%25%20AUC%29.%20The%20framework%0Ademonstrates%2096.1%5C%25%20attack%20resilience%20against%20attacks%20such%20as%20paraphrasing%20and%0Asynonym%20substitution%2C%20while%20maintaining%20text-visual%20consistency%2C%20establishing%0Anew%20standards%20for%20quality-preserving%20multimodal%20watermarking%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14067v1&entry.124074799=Read"},
{"title": "Efficient Temporal Tokenization for Mobility Prediction with Large\n  Language Models", "author": "Haoyu He and Haozheng Luo and Yan Chen and Qi R. Wang", "abstract": "  We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2507.14017v1", "date": "2025-07-18", "relevancy": 2.0648, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5171}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Temporal%20Tokenization%20for%20Mobility%20Prediction%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Efficient%20Temporal%20Tokenization%20for%20Mobility%20Prediction%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Haoyu%20He%20and%20Haozheng%20Luo%20and%20Yan%20Chen%20and%20Qi%20R.%20Wang%0AAbstract%3A%20%20%20We%20introduce%20RHYTHM%20%28Reasoning%20with%20Hierarchical%20Temporal%20Tokenization%20for%0AHuman%20Mobility%29%2C%20a%20framework%20that%20leverages%20large%20language%20models%20%28LLMs%29%20as%0Aspatio-temporal%20predictors%20and%20trajectory%20reasoners.%20RHYTHM%20partitions%0Atrajectories%20into%20daily%20segments%20encoded%20as%20discrete%20tokens%20with%20hierarchical%0Aattention%2C%20capturing%20both%20daily%20and%20weekly%20dependencies%20while%20substantially%0Areducing%20the%20sequence%20length.%20Token%20representations%20are%20enriched%20with%0Apre-computed%20prompt%20embeddings%20via%20a%20frozen%20LLM%2C%20enhancing%20the%20model%27s%20ability%0Ato%20capture%20interdependencies%20without%20extensive%20computational%20overhead.%20By%0Afreezing%20the%20LLM%20backbone%2C%20RHYTHM%20achieves%20significant%20computational%0Aefficiency.%20Evaluation%20on%20three%20real-world%20datasets%20demonstrates%20a%202.4%25%0Aimprovement%20in%20accuracy%2C%205.0%25%20increase%20on%20weekends%2C%20and%2024.6%25%20reduction%20in%0Atraining%20time%20compared%20to%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14017v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Temporal%2520Tokenization%2520for%2520Mobility%2520Prediction%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHaoyu%2520He%2520and%2520Haozheng%2520Luo%2520and%2520Yan%2520Chen%2520and%2520Qi%2520R.%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520RHYTHM%2520%2528Reasoning%2520with%2520Hierarchical%2520Temporal%2520Tokenization%2520for%250AHuman%2520Mobility%2529%252C%2520a%2520framework%2520that%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520as%250Aspatio-temporal%2520predictors%2520and%2520trajectory%2520reasoners.%2520RHYTHM%2520partitions%250Atrajectories%2520into%2520daily%2520segments%2520encoded%2520as%2520discrete%2520tokens%2520with%2520hierarchical%250Aattention%252C%2520capturing%2520both%2520daily%2520and%2520weekly%2520dependencies%2520while%2520substantially%250Areducing%2520the%2520sequence%2520length.%2520Token%2520representations%2520are%2520enriched%2520with%250Apre-computed%2520prompt%2520embeddings%2520via%2520a%2520frozen%2520LLM%252C%2520enhancing%2520the%2520model%2527s%2520ability%250Ato%2520capture%2520interdependencies%2520without%2520extensive%2520computational%2520overhead.%2520By%250Afreezing%2520the%2520LLM%2520backbone%252C%2520RHYTHM%2520achieves%2520significant%2520computational%250Aefficiency.%2520Evaluation%2520on%2520three%2520real-world%2520datasets%2520demonstrates%2520a%25202.4%2525%250Aimprovement%2520in%2520accuracy%252C%25205.0%2525%2520increase%2520on%2520weekends%252C%2520and%252024.6%2525%2520reduction%2520in%250Atraining%2520time%2520compared%2520to%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14017v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Temporal%20Tokenization%20for%20Mobility%20Prediction%20with%20Large%0A%20%20Language%20Models&entry.906535625=Haoyu%20He%20and%20Haozheng%20Luo%20and%20Yan%20Chen%20and%20Qi%20R.%20Wang&entry.1292438233=%20%20We%20introduce%20RHYTHM%20%28Reasoning%20with%20Hierarchical%20Temporal%20Tokenization%20for%0AHuman%20Mobility%29%2C%20a%20framework%20that%20leverages%20large%20language%20models%20%28LLMs%29%20as%0Aspatio-temporal%20predictors%20and%20trajectory%20reasoners.%20RHYTHM%20partitions%0Atrajectories%20into%20daily%20segments%20encoded%20as%20discrete%20tokens%20with%20hierarchical%0Aattention%2C%20capturing%20both%20daily%20and%20weekly%20dependencies%20while%20substantially%0Areducing%20the%20sequence%20length.%20Token%20representations%20are%20enriched%20with%0Apre-computed%20prompt%20embeddings%20via%20a%20frozen%20LLM%2C%20enhancing%20the%20model%27s%20ability%0Ato%20capture%20interdependencies%20without%20extensive%20computational%20overhead.%20By%0Afreezing%20the%20LLM%20backbone%2C%20RHYTHM%20achieves%20significant%20computational%0Aefficiency.%20Evaluation%20on%20three%20real-world%20datasets%20demonstrates%20a%202.4%25%0Aimprovement%20in%20accuracy%2C%205.0%25%20increase%20on%20weekends%2C%20and%2024.6%25%20reduction%20in%0Atraining%20time%20compared%20to%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14017v1&entry.124074799=Read"},
{"title": "DONUT: Physics-aware Machine Learning for Real-time X-ray\n  Nanodiffraction Analysis", "author": "Aileen Luo and Tao Zhou and Ming Du and Martin V. Holt and Andrej Singer and Mathew J. Cherukara", "abstract": "  Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.\n", "link": "http://arxiv.org/abs/2507.14038v1", "date": "2025-07-18", "relevancy": 2.0617, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5093}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DONUT%3A%20Physics-aware%20Machine%20Learning%20for%20Real-time%20X-ray%0A%20%20Nanodiffraction%20Analysis&body=Title%3A%20DONUT%3A%20Physics-aware%20Machine%20Learning%20for%20Real-time%20X-ray%0A%20%20Nanodiffraction%20Analysis%0AAuthor%3A%20Aileen%20Luo%20and%20Tao%20Zhou%20and%20Ming%20Du%20and%20Martin%20V.%20Holt%20and%20Andrej%20Singer%20and%20Mathew%20J.%20Cherukara%0AAbstract%3A%20%20%20Coherent%20X-ray%20scattering%20techniques%20are%20critical%20for%20investigating%20the%0Afundamental%20structural%20properties%20of%20materials%20at%20the%20nanoscale.%20While%0Aadvancements%20have%20made%20these%20experiments%20more%20accessible%2C%20real-time%20analysis%0Aremains%20a%20significant%20bottleneck%2C%20often%20hindered%20by%20artifacts%20and%20computational%0Ademands.%20In%20scanning%20X-ray%20nanodiffraction%20microscopy%2C%20which%20is%20widely%20used%20to%0Aspatially%20resolve%20structural%20heterogeneities%2C%20this%20challenge%20is%20compounded%20by%0Athe%20convolution%20of%20the%20divergent%20beam%20with%20the%20sample%27s%20local%20structure.%20To%0Aaddress%20this%2C%20we%20introduce%20DONUT%20%28Diffraction%20with%20Optics%20for%20Nanobeam%20by%0AUnsupervised%20Training%29%2C%20a%20physics-aware%20neural%20network%20designed%20for%20the%20rapid%0Aand%20automated%20analysis%20of%20nanobeam%20diffraction%20data.%20By%20incorporating%20a%0Adifferentiable%20geometric%20diffraction%20model%20directly%20into%20its%20architecture%2C%0ADONUT%20learns%20to%20predict%20crystal%20lattice%20strain%20and%20orientation%20in%20real-time.%0ACrucially%2C%20this%20is%20achieved%20without%20reliance%20on%20labeled%20datasets%20or%0Apre-training%2C%20overcoming%20a%20fundamental%20limitation%20for%20supervised%20machine%0Alearning%20in%20X-ray%20science.%20We%20demonstrate%20experimentally%20that%20DONUT%20accurately%0Aextracts%20all%20features%20within%20the%20data%20over%20200%20times%20more%20efficiently%20than%0Aconventional%20fitting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDONUT%253A%2520Physics-aware%2520Machine%2520Learning%2520for%2520Real-time%2520X-ray%250A%2520%2520Nanodiffraction%2520Analysis%26entry.906535625%3DAileen%2520Luo%2520and%2520Tao%2520Zhou%2520and%2520Ming%2520Du%2520and%2520Martin%2520V.%2520Holt%2520and%2520Andrej%2520Singer%2520and%2520Mathew%2520J.%2520Cherukara%26entry.1292438233%3D%2520%2520Coherent%2520X-ray%2520scattering%2520techniques%2520are%2520critical%2520for%2520investigating%2520the%250Afundamental%2520structural%2520properties%2520of%2520materials%2520at%2520the%2520nanoscale.%2520While%250Aadvancements%2520have%2520made%2520these%2520experiments%2520more%2520accessible%252C%2520real-time%2520analysis%250Aremains%2520a%2520significant%2520bottleneck%252C%2520often%2520hindered%2520by%2520artifacts%2520and%2520computational%250Ademands.%2520In%2520scanning%2520X-ray%2520nanodiffraction%2520microscopy%252C%2520which%2520is%2520widely%2520used%2520to%250Aspatially%2520resolve%2520structural%2520heterogeneities%252C%2520this%2520challenge%2520is%2520compounded%2520by%250Athe%2520convolution%2520of%2520the%2520divergent%2520beam%2520with%2520the%2520sample%2527s%2520local%2520structure.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520DONUT%2520%2528Diffraction%2520with%2520Optics%2520for%2520Nanobeam%2520by%250AUnsupervised%2520Training%2529%252C%2520a%2520physics-aware%2520neural%2520network%2520designed%2520for%2520the%2520rapid%250Aand%2520automated%2520analysis%2520of%2520nanobeam%2520diffraction%2520data.%2520By%2520incorporating%2520a%250Adifferentiable%2520geometric%2520diffraction%2520model%2520directly%2520into%2520its%2520architecture%252C%250ADONUT%2520learns%2520to%2520predict%2520crystal%2520lattice%2520strain%2520and%2520orientation%2520in%2520real-time.%250ACrucially%252C%2520this%2520is%2520achieved%2520without%2520reliance%2520on%2520labeled%2520datasets%2520or%250Apre-training%252C%2520overcoming%2520a%2520fundamental%2520limitation%2520for%2520supervised%2520machine%250Alearning%2520in%2520X-ray%2520science.%2520We%2520demonstrate%2520experimentally%2520that%2520DONUT%2520accurately%250Aextracts%2520all%2520features%2520within%2520the%2520data%2520over%2520200%2520times%2520more%2520efficiently%2520than%250Aconventional%2520fitting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DONUT%3A%20Physics-aware%20Machine%20Learning%20for%20Real-time%20X-ray%0A%20%20Nanodiffraction%20Analysis&entry.906535625=Aileen%20Luo%20and%20Tao%20Zhou%20and%20Ming%20Du%20and%20Martin%20V.%20Holt%20and%20Andrej%20Singer%20and%20Mathew%20J.%20Cherukara&entry.1292438233=%20%20Coherent%20X-ray%20scattering%20techniques%20are%20critical%20for%20investigating%20the%0Afundamental%20structural%20properties%20of%20materials%20at%20the%20nanoscale.%20While%0Aadvancements%20have%20made%20these%20experiments%20more%20accessible%2C%20real-time%20analysis%0Aremains%20a%20significant%20bottleneck%2C%20often%20hindered%20by%20artifacts%20and%20computational%0Ademands.%20In%20scanning%20X-ray%20nanodiffraction%20microscopy%2C%20which%20is%20widely%20used%20to%0Aspatially%20resolve%20structural%20heterogeneities%2C%20this%20challenge%20is%20compounded%20by%0Athe%20convolution%20of%20the%20divergent%20beam%20with%20the%20sample%27s%20local%20structure.%20To%0Aaddress%20this%2C%20we%20introduce%20DONUT%20%28Diffraction%20with%20Optics%20for%20Nanobeam%20by%0AUnsupervised%20Training%29%2C%20a%20physics-aware%20neural%20network%20designed%20for%20the%20rapid%0Aand%20automated%20analysis%20of%20nanobeam%20diffraction%20data.%20By%20incorporating%20a%0Adifferentiable%20geometric%20diffraction%20model%20directly%20into%20its%20architecture%2C%0ADONUT%20learns%20to%20predict%20crystal%20lattice%20strain%20and%20orientation%20in%20real-time.%0ACrucially%2C%20this%20is%20achieved%20without%20reliance%20on%20labeled%20datasets%20or%0Apre-training%2C%20overcoming%20a%20fundamental%20limitation%20for%20supervised%20machine%0Alearning%20in%20X-ray%20science.%20We%20demonstrate%20experimentally%20that%20DONUT%20accurately%0Aextracts%20all%20features%20within%20the%20data%20over%20200%20times%20more%20efficiently%20than%0Aconventional%20fitting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14038v1&entry.124074799=Read"},
{"title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning", "author": "Qitao Tan and Jun Liu and Zheng Zhan and Caiwei Ding and Yanzhi Wang and Xiaolong Ma and Jaewoo Lee and Jin Lu and Geng Yuan", "abstract": "  Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose Divergence-driven\nZeroth-Order (DiZO) optimization. DiZO conducts divergence-driven layer\nadaptation by incorporating projections to ZO updates, generating\ndiverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning. Our code is released at\nhttps://anonymous.4open.science/r/DiZO-E86D.\n", "link": "http://arxiv.org/abs/2502.03304v2", "date": "2025-07-18", "relevancy": 2.0556, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5172}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmony%20in%20Divergence%3A%20Towards%20Fast%2C%20Accurate%2C%20and%20Memory-efficient%0A%20%20Zeroth-order%20LLM%20Fine-tuning&body=Title%3A%20Harmony%20in%20Divergence%3A%20Towards%20Fast%2C%20Accurate%2C%20and%20Memory-efficient%0A%20%20Zeroth-order%20LLM%20Fine-tuning%0AAuthor%3A%20Qitao%20Tan%20and%20Jun%20Liu%20and%20Zheng%20Zhan%20and%20Caiwei%20Ding%20and%20Yanzhi%20Wang%20and%20Xiaolong%20Ma%20and%20Jaewoo%20Lee%20and%20Jin%20Lu%20and%20Geng%20Yuan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20across%20various%20tasks%2C%20but%20standard%0Afirst-order%20%28FO%29%20fine-tuning%20demands%20considerable%20memory%2C%20significantly%0Alimiting%20real-world%20deployment.%20Recently%2C%20zeroth-order%20%28ZO%29%20optimization%20stood%0Aout%20as%20a%20promising%20memory-efficient%20training%20paradigm%2C%20avoiding%20backward%20passes%0Aand%20relying%20solely%20on%20forward%20passes%20for%20gradient%20estimation%2C%20making%20it%0Aattractive%20for%20resource-constrained%20scenarios.%20However%2C%20ZO%20method%20lags%20far%0Abehind%20FO%20method%20in%20both%20convergence%20speed%20and%20accuracy.%20To%20bridge%20the%20gap%2C%20we%0Aintroduce%20a%20novel%20layer-wise%20divergence%20analysis%20that%20uncovers%20the%20distinct%0Aupdate%20pattern%20of%20FO%20and%20ZO%20optimization.%20Aiming%20to%20resemble%20the%20learning%0Acapacity%20of%20FO%20method%20from%20the%20findings%2C%20we%20propose%20Divergence-driven%0AZeroth-Order%20%28DiZO%29%20optimization.%20DiZO%20conducts%20divergence-driven%20layer%0Aadaptation%20by%20incorporating%20projections%20to%20ZO%20updates%2C%20generating%0Adiverse-magnitude%20updates%20precisely%20scaled%20to%20layer-wise%20individual%0Aoptimization%20needs.%20Our%20results%20demonstrate%20that%20DiZO%20significantly%20reduces%20the%0Aneeded%20iterations%20for%20convergence%20without%20sacrificing%20throughput%2C%20cutting%0Atraining%20GPU%20hours%20by%20up%20to%2048%25%20on%20various%20datasets.%20Moreover%2C%20DiZO%0Aconsistently%20outperforms%20the%20representative%20ZO%20baselines%20in%20fine-tuning%0ARoBERTa-large%2C%20OPT-series%2C%20and%20Llama-series%20on%20downstream%20tasks%20and%2C%20in%20some%0Acases%2C%20even%20surpasses%20memory-intensive%20FO%20fine-tuning.%20Our%20code%20is%20released%20at%0Ahttps%3A//anonymous.4open.science/r/DiZO-E86D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03304v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmony%2520in%2520Divergence%253A%2520Towards%2520Fast%252C%2520Accurate%252C%2520and%2520Memory-efficient%250A%2520%2520Zeroth-order%2520LLM%2520Fine-tuning%26entry.906535625%3DQitao%2520Tan%2520and%2520Jun%2520Liu%2520and%2520Zheng%2520Zhan%2520and%2520Caiwei%2520Ding%2520and%2520Yanzhi%2520Wang%2520and%2520Xiaolong%2520Ma%2520and%2520Jaewoo%2520Lee%2520and%2520Jin%2520Lu%2520and%2520Geng%2520Yuan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520across%2520various%2520tasks%252C%2520but%2520standard%250Afirst-order%2520%2528FO%2529%2520fine-tuning%2520demands%2520considerable%2520memory%252C%2520significantly%250Alimiting%2520real-world%2520deployment.%2520Recently%252C%2520zeroth-order%2520%2528ZO%2529%2520optimization%2520stood%250Aout%2520as%2520a%2520promising%2520memory-efficient%2520training%2520paradigm%252C%2520avoiding%2520backward%2520passes%250Aand%2520relying%2520solely%2520on%2520forward%2520passes%2520for%2520gradient%2520estimation%252C%2520making%2520it%250Aattractive%2520for%2520resource-constrained%2520scenarios.%2520However%252C%2520ZO%2520method%2520lags%2520far%250Abehind%2520FO%2520method%2520in%2520both%2520convergence%2520speed%2520and%2520accuracy.%2520To%2520bridge%2520the%2520gap%252C%2520we%250Aintroduce%2520a%2520novel%2520layer-wise%2520divergence%2520analysis%2520that%2520uncovers%2520the%2520distinct%250Aupdate%2520pattern%2520of%2520FO%2520and%2520ZO%2520optimization.%2520Aiming%2520to%2520resemble%2520the%2520learning%250Acapacity%2520of%2520FO%2520method%2520from%2520the%2520findings%252C%2520we%2520propose%2520Divergence-driven%250AZeroth-Order%2520%2528DiZO%2529%2520optimization.%2520DiZO%2520conducts%2520divergence-driven%2520layer%250Aadaptation%2520by%2520incorporating%2520projections%2520to%2520ZO%2520updates%252C%2520generating%250Adiverse-magnitude%2520updates%2520precisely%2520scaled%2520to%2520layer-wise%2520individual%250Aoptimization%2520needs.%2520Our%2520results%2520demonstrate%2520that%2520DiZO%2520significantly%2520reduces%2520the%250Aneeded%2520iterations%2520for%2520convergence%2520without%2520sacrificing%2520throughput%252C%2520cutting%250Atraining%2520GPU%2520hours%2520by%2520up%2520to%252048%2525%2520on%2520various%2520datasets.%2520Moreover%252C%2520DiZO%250Aconsistently%2520outperforms%2520the%2520representative%2520ZO%2520baselines%2520in%2520fine-tuning%250ARoBERTa-large%252C%2520OPT-series%252C%2520and%2520Llama-series%2520on%2520downstream%2520tasks%2520and%252C%2520in%2520some%250Acases%252C%2520even%2520surpasses%2520memory-intensive%2520FO%2520fine-tuning.%2520Our%2520code%2520is%2520released%2520at%250Ahttps%253A//anonymous.4open.science/r/DiZO-E86D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03304v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmony%20in%20Divergence%3A%20Towards%20Fast%2C%20Accurate%2C%20and%20Memory-efficient%0A%20%20Zeroth-order%20LLM%20Fine-tuning&entry.906535625=Qitao%20Tan%20and%20Jun%20Liu%20and%20Zheng%20Zhan%20and%20Caiwei%20Ding%20and%20Yanzhi%20Wang%20and%20Xiaolong%20Ma%20and%20Jaewoo%20Lee%20and%20Jin%20Lu%20and%20Geng%20Yuan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20across%20various%20tasks%2C%20but%20standard%0Afirst-order%20%28FO%29%20fine-tuning%20demands%20considerable%20memory%2C%20significantly%0Alimiting%20real-world%20deployment.%20Recently%2C%20zeroth-order%20%28ZO%29%20optimization%20stood%0Aout%20as%20a%20promising%20memory-efficient%20training%20paradigm%2C%20avoiding%20backward%20passes%0Aand%20relying%20solely%20on%20forward%20passes%20for%20gradient%20estimation%2C%20making%20it%0Aattractive%20for%20resource-constrained%20scenarios.%20However%2C%20ZO%20method%20lags%20far%0Abehind%20FO%20method%20in%20both%20convergence%20speed%20and%20accuracy.%20To%20bridge%20the%20gap%2C%20we%0Aintroduce%20a%20novel%20layer-wise%20divergence%20analysis%20that%20uncovers%20the%20distinct%0Aupdate%20pattern%20of%20FO%20and%20ZO%20optimization.%20Aiming%20to%20resemble%20the%20learning%0Acapacity%20of%20FO%20method%20from%20the%20findings%2C%20we%20propose%20Divergence-driven%0AZeroth-Order%20%28DiZO%29%20optimization.%20DiZO%20conducts%20divergence-driven%20layer%0Aadaptation%20by%20incorporating%20projections%20to%20ZO%20updates%2C%20generating%0Adiverse-magnitude%20updates%20precisely%20scaled%20to%20layer-wise%20individual%0Aoptimization%20needs.%20Our%20results%20demonstrate%20that%20DiZO%20significantly%20reduces%20the%0Aneeded%20iterations%20for%20convergence%20without%20sacrificing%20throughput%2C%20cutting%0Atraining%20GPU%20hours%20by%20up%20to%2048%25%20on%20various%20datasets.%20Moreover%2C%20DiZO%0Aconsistently%20outperforms%20the%20representative%20ZO%20baselines%20in%20fine-tuning%0ARoBERTa-large%2C%20OPT-series%2C%20and%20Llama-series%20on%20downstream%20tasks%20and%2C%20in%20some%0Acases%2C%20even%20surpasses%20memory-intensive%20FO%20fine-tuning.%20Our%20code%20is%20released%20at%0Ahttps%3A//anonymous.4open.science/r/DiZO-E86D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03304v2&entry.124074799=Read"},
{"title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and\n  Optimized Segmentation Model", "author": "Ji-Yan Wu and Zheng Yong Poh and Anoop C. Patil and Bongsoo Park and Giovanni Volpe and Daisuke Urano", "abstract": "  Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.\n", "link": "http://arxiv.org/abs/2507.14013v1", "date": "2025-07-18", "relevancy": 2.0536, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Plant%20Nutrient%20Deficiencies%20Using%20Multi-Spectral%20Imaging%20and%0A%20%20Optimized%20Segmentation%20Model&body=Title%3A%20Analysis%20of%20Plant%20Nutrient%20Deficiencies%20Using%20Multi-Spectral%20Imaging%20and%0A%20%20Optimized%20Segmentation%20Model%0AAuthor%3A%20Ji-Yan%20Wu%20and%20Zheng%20Yong%20Poh%20and%20Anoop%20C.%20Patil%20and%20Bongsoo%20Park%20and%20Giovanni%20Volpe%20and%20Daisuke%20Urano%0AAbstract%3A%20%20%20Accurate%20detection%20of%20nutrient%20deficiency%20in%20plant%20leaves%20is%20essential%20for%0Aprecision%20agriculture%2C%20enabling%20early%20intervention%20in%20fertilization%2C%20disease%2C%0Aand%20stress%20management.%20This%20study%20presents%20a%20deep%20learning%20framework%20for%20leaf%0Aanomaly%20segmentation%20using%20multispectral%20imaging%20and%20an%20enhanced%20YOLOv5%20model%0Awith%20a%20transformer-based%20attention%20head.%20The%20model%20is%20tailored%20for%20processing%0Anine-channel%20multispectral%20input%20and%20uses%20self-attention%20mechanisms%20to%20better%0Acapture%20subtle%2C%20spatially-distributed%20symptoms.%20The%20plants%20in%20the%20experiments%0Awere%20grown%20under%20controlled%20nutrient%20stress%20conditions%20for%20evaluation.%20We%20carry%0Aout%20extensive%20experiments%20to%20benchmark%20the%20proposed%20model%20against%20the%20baseline%0AYOLOv5.%20Extensive%20experiments%20show%20that%20the%20proposed%20model%20significantly%0Aoutperforms%20the%20baseline%20YOLOv5%2C%20with%20an%20average%20Dice%20score%20and%20IoU%0A%28Intersection%20over%20Union%29%20improvement%20of%20about%2012%25.%20In%20particular%2C%20this%20model%0Ais%20effective%20in%20detecting%20challenging%20symptoms%20like%20chlorosis%20and%20pigment%0Aaccumulation.%20These%20results%20highlight%20the%20promise%20of%20combining%20multi-spectral%0Aimaging%20with%20spectral-spatial%20feature%20learning%20for%20advancing%20plant%20phenotyping%0Aand%20precision%20agriculture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Plant%2520Nutrient%2520Deficiencies%2520Using%2520Multi-Spectral%2520Imaging%2520and%250A%2520%2520Optimized%2520Segmentation%2520Model%26entry.906535625%3DJi-Yan%2520Wu%2520and%2520Zheng%2520Yong%2520Poh%2520and%2520Anoop%2520C.%2520Patil%2520and%2520Bongsoo%2520Park%2520and%2520Giovanni%2520Volpe%2520and%2520Daisuke%2520Urano%26entry.1292438233%3D%2520%2520Accurate%2520detection%2520of%2520nutrient%2520deficiency%2520in%2520plant%2520leaves%2520is%2520essential%2520for%250Aprecision%2520agriculture%252C%2520enabling%2520early%2520intervention%2520in%2520fertilization%252C%2520disease%252C%250Aand%2520stress%2520management.%2520This%2520study%2520presents%2520a%2520deep%2520learning%2520framework%2520for%2520leaf%250Aanomaly%2520segmentation%2520using%2520multispectral%2520imaging%2520and%2520an%2520enhanced%2520YOLOv5%2520model%250Awith%2520a%2520transformer-based%2520attention%2520head.%2520The%2520model%2520is%2520tailored%2520for%2520processing%250Anine-channel%2520multispectral%2520input%2520and%2520uses%2520self-attention%2520mechanisms%2520to%2520better%250Acapture%2520subtle%252C%2520spatially-distributed%2520symptoms.%2520The%2520plants%2520in%2520the%2520experiments%250Awere%2520grown%2520under%2520controlled%2520nutrient%2520stress%2520conditions%2520for%2520evaluation.%2520We%2520carry%250Aout%2520extensive%2520experiments%2520to%2520benchmark%2520the%2520proposed%2520model%2520against%2520the%2520baseline%250AYOLOv5.%2520Extensive%2520experiments%2520show%2520that%2520the%2520proposed%2520model%2520significantly%250Aoutperforms%2520the%2520baseline%2520YOLOv5%252C%2520with%2520an%2520average%2520Dice%2520score%2520and%2520IoU%250A%2528Intersection%2520over%2520Union%2529%2520improvement%2520of%2520about%252012%2525.%2520In%2520particular%252C%2520this%2520model%250Ais%2520effective%2520in%2520detecting%2520challenging%2520symptoms%2520like%2520chlorosis%2520and%2520pigment%250Aaccumulation.%2520These%2520results%2520highlight%2520the%2520promise%2520of%2520combining%2520multi-spectral%250Aimaging%2520with%2520spectral-spatial%2520feature%2520learning%2520for%2520advancing%2520plant%2520phenotyping%250Aand%2520precision%2520agriculture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Plant%20Nutrient%20Deficiencies%20Using%20Multi-Spectral%20Imaging%20and%0A%20%20Optimized%20Segmentation%20Model&entry.906535625=Ji-Yan%20Wu%20and%20Zheng%20Yong%20Poh%20and%20Anoop%20C.%20Patil%20and%20Bongsoo%20Park%20and%20Giovanni%20Volpe%20and%20Daisuke%20Urano&entry.1292438233=%20%20Accurate%20detection%20of%20nutrient%20deficiency%20in%20plant%20leaves%20is%20essential%20for%0Aprecision%20agriculture%2C%20enabling%20early%20intervention%20in%20fertilization%2C%20disease%2C%0Aand%20stress%20management.%20This%20study%20presents%20a%20deep%20learning%20framework%20for%20leaf%0Aanomaly%20segmentation%20using%20multispectral%20imaging%20and%20an%20enhanced%20YOLOv5%20model%0Awith%20a%20transformer-based%20attention%20head.%20The%20model%20is%20tailored%20for%20processing%0Anine-channel%20multispectral%20input%20and%20uses%20self-attention%20mechanisms%20to%20better%0Acapture%20subtle%2C%20spatially-distributed%20symptoms.%20The%20plants%20in%20the%20experiments%0Awere%20grown%20under%20controlled%20nutrient%20stress%20conditions%20for%20evaluation.%20We%20carry%0Aout%20extensive%20experiments%20to%20benchmark%20the%20proposed%20model%20against%20the%20baseline%0AYOLOv5.%20Extensive%20experiments%20show%20that%20the%20proposed%20model%20significantly%0Aoutperforms%20the%20baseline%20YOLOv5%2C%20with%20an%20average%20Dice%20score%20and%20IoU%0A%28Intersection%20over%20Union%29%20improvement%20of%20about%2012%25.%20In%20particular%2C%20this%20model%0Ais%20effective%20in%20detecting%20challenging%20symptoms%20like%20chlorosis%20and%20pigment%0Aaccumulation.%20These%20results%20highlight%20the%20promise%20of%20combining%20multi-spectral%0Aimaging%20with%20spectral-spatial%20feature%20learning%20for%20advancing%20plant%20phenotyping%0Aand%20precision%20agriculture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14013v1&entry.124074799=Read"},
{"title": "Revisiting Data Augmentation for Ultrasound Images", "author": "Adam Tupper and Christian Gagn\u00e9", "abstract": "  Data augmentation is a widely used and effective technique to improve the\ngeneralization performance of deep neural networks. Yet, despite often facing\nlimited data availability when working with medical images, it is frequently\nunderutilized. This appears to come from a gap in our collective understanding\nof the efficacy of different augmentation techniques across different tasks and\nmodalities. One modality where this is especially true is ultrasound imaging.\nThis work addresses this gap by analyzing the effectiveness of different\naugmentation techniques at improving model performance across a wide range of\nultrasound image analysis tasks. To achieve this, we introduce a new\nstandardized benchmark of 14 ultrasound image classification and semantic\nsegmentation tasks from 10 different sources and covering 11 body regions. Our\nresults demonstrate that many of the augmentations commonly used for tasks on\nnatural images are also effective on ultrasound images, even more so than\naugmentations developed specifically for ultrasound images in some cases. We\nalso show that diverse augmentation using TrivialAugment, which is widely used\nfor natural images, is also effective for ultrasound images. Moreover, our\nproposed methodology represents a structured approach for assessing various\ndata augmentations that can be applied to other contexts and modalities.\n", "link": "http://arxiv.org/abs/2501.13193v2", "date": "2025-07-18", "relevancy": 2.0445, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.529}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5076}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Data%20Augmentation%20for%20Ultrasound%20Images&body=Title%3A%20Revisiting%20Data%20Augmentation%20for%20Ultrasound%20Images%0AAuthor%3A%20Adam%20Tupper%20and%20Christian%20Gagn%C3%A9%0AAbstract%3A%20%20%20Data%20augmentation%20is%20a%20widely%20used%20and%20effective%20technique%20to%20improve%20the%0Ageneralization%20performance%20of%20deep%20neural%20networks.%20Yet%2C%20despite%20often%20facing%0Alimited%20data%20availability%20when%20working%20with%20medical%20images%2C%20it%20is%20frequently%0Aunderutilized.%20This%20appears%20to%20come%20from%20a%20gap%20in%20our%20collective%20understanding%0Aof%20the%20efficacy%20of%20different%20augmentation%20techniques%20across%20different%20tasks%20and%0Amodalities.%20One%20modality%20where%20this%20is%20especially%20true%20is%20ultrasound%20imaging.%0AThis%20work%20addresses%20this%20gap%20by%20analyzing%20the%20effectiveness%20of%20different%0Aaugmentation%20techniques%20at%20improving%20model%20performance%20across%20a%20wide%20range%20of%0Aultrasound%20image%20analysis%20tasks.%20To%20achieve%20this%2C%20we%20introduce%20a%20new%0Astandardized%20benchmark%20of%2014%20ultrasound%20image%20classification%20and%20semantic%0Asegmentation%20tasks%20from%2010%20different%20sources%20and%20covering%2011%20body%20regions.%20Our%0Aresults%20demonstrate%20that%20many%20of%20the%20augmentations%20commonly%20used%20for%20tasks%20on%0Anatural%20images%20are%20also%20effective%20on%20ultrasound%20images%2C%20even%20more%20so%20than%0Aaugmentations%20developed%20specifically%20for%20ultrasound%20images%20in%20some%20cases.%20We%0Aalso%20show%20that%20diverse%20augmentation%20using%20TrivialAugment%2C%20which%20is%20widely%20used%0Afor%20natural%20images%2C%20is%20also%20effective%20for%20ultrasound%20images.%20Moreover%2C%20our%0Aproposed%20methodology%20represents%20a%20structured%20approach%20for%20assessing%20various%0Adata%20augmentations%20that%20can%20be%20applied%20to%20other%20contexts%20and%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13193v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Data%2520Augmentation%2520for%2520Ultrasound%2520Images%26entry.906535625%3DAdam%2520Tupper%2520and%2520Christian%2520Gagn%25C3%25A9%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520a%2520widely%2520used%2520and%2520effective%2520technique%2520to%2520improve%2520the%250Ageneralization%2520performance%2520of%2520deep%2520neural%2520networks.%2520Yet%252C%2520despite%2520often%2520facing%250Alimited%2520data%2520availability%2520when%2520working%2520with%2520medical%2520images%252C%2520it%2520is%2520frequently%250Aunderutilized.%2520This%2520appears%2520to%2520come%2520from%2520a%2520gap%2520in%2520our%2520collective%2520understanding%250Aof%2520the%2520efficacy%2520of%2520different%2520augmentation%2520techniques%2520across%2520different%2520tasks%2520and%250Amodalities.%2520One%2520modality%2520where%2520this%2520is%2520especially%2520true%2520is%2520ultrasound%2520imaging.%250AThis%2520work%2520addresses%2520this%2520gap%2520by%2520analyzing%2520the%2520effectiveness%2520of%2520different%250Aaugmentation%2520techniques%2520at%2520improving%2520model%2520performance%2520across%2520a%2520wide%2520range%2520of%250Aultrasound%2520image%2520analysis%2520tasks.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520new%250Astandardized%2520benchmark%2520of%252014%2520ultrasound%2520image%2520classification%2520and%2520semantic%250Asegmentation%2520tasks%2520from%252010%2520different%2520sources%2520and%2520covering%252011%2520body%2520regions.%2520Our%250Aresults%2520demonstrate%2520that%2520many%2520of%2520the%2520augmentations%2520commonly%2520used%2520for%2520tasks%2520on%250Anatural%2520images%2520are%2520also%2520effective%2520on%2520ultrasound%2520images%252C%2520even%2520more%2520so%2520than%250Aaugmentations%2520developed%2520specifically%2520for%2520ultrasound%2520images%2520in%2520some%2520cases.%2520We%250Aalso%2520show%2520that%2520diverse%2520augmentation%2520using%2520TrivialAugment%252C%2520which%2520is%2520widely%2520used%250Afor%2520natural%2520images%252C%2520is%2520also%2520effective%2520for%2520ultrasound%2520images.%2520Moreover%252C%2520our%250Aproposed%2520methodology%2520represents%2520a%2520structured%2520approach%2520for%2520assessing%2520various%250Adata%2520augmentations%2520that%2520can%2520be%2520applied%2520to%2520other%2520contexts%2520and%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13193v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Data%20Augmentation%20for%20Ultrasound%20Images&entry.906535625=Adam%20Tupper%20and%20Christian%20Gagn%C3%A9&entry.1292438233=%20%20Data%20augmentation%20is%20a%20widely%20used%20and%20effective%20technique%20to%20improve%20the%0Ageneralization%20performance%20of%20deep%20neural%20networks.%20Yet%2C%20despite%20often%20facing%0Alimited%20data%20availability%20when%20working%20with%20medical%20images%2C%20it%20is%20frequently%0Aunderutilized.%20This%20appears%20to%20come%20from%20a%20gap%20in%20our%20collective%20understanding%0Aof%20the%20efficacy%20of%20different%20augmentation%20techniques%20across%20different%20tasks%20and%0Amodalities.%20One%20modality%20where%20this%20is%20especially%20true%20is%20ultrasound%20imaging.%0AThis%20work%20addresses%20this%20gap%20by%20analyzing%20the%20effectiveness%20of%20different%0Aaugmentation%20techniques%20at%20improving%20model%20performance%20across%20a%20wide%20range%20of%0Aultrasound%20image%20analysis%20tasks.%20To%20achieve%20this%2C%20we%20introduce%20a%20new%0Astandardized%20benchmark%20of%2014%20ultrasound%20image%20classification%20and%20semantic%0Asegmentation%20tasks%20from%2010%20different%20sources%20and%20covering%2011%20body%20regions.%20Our%0Aresults%20demonstrate%20that%20many%20of%20the%20augmentations%20commonly%20used%20for%20tasks%20on%0Anatural%20images%20are%20also%20effective%20on%20ultrasound%20images%2C%20even%20more%20so%20than%0Aaugmentations%20developed%20specifically%20for%20ultrasound%20images%20in%20some%20cases.%20We%0Aalso%20show%20that%20diverse%20augmentation%20using%20TrivialAugment%2C%20which%20is%20widely%20used%0Afor%20natural%20images%2C%20is%20also%20effective%20for%20ultrasound%20images.%20Moreover%2C%20our%0Aproposed%20methodology%20represents%20a%20structured%20approach%20for%20assessing%20various%0Adata%20augmentations%20that%20can%20be%20applied%20to%20other%20contexts%20and%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13193v2&entry.124074799=Read"},
{"title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of\n  Melanoma in H&E Images", "author": "Jiaqi Lv and Yijie Zhu and Carmen Guadalupe Colin Tenorio and Brinder Singh Chohan and Mark Eastwood and Shan E Ahmed Raza", "abstract": "  Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.\n", "link": "http://arxiv.org/abs/2507.13974v1", "date": "2025-07-18", "relevancy": 2.026, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5071}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5067}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Pathology%20Foundation%20Models%20for%20Panoptic%20Segmentation%20of%0A%20%20Melanoma%20in%20H%26E%20Images&body=Title%3A%20Leveraging%20Pathology%20Foundation%20Models%20for%20Panoptic%20Segmentation%20of%0A%20%20Melanoma%20in%20H%26E%20Images%0AAuthor%3A%20Jiaqi%20Lv%20and%20Yijie%20Zhu%20and%20Carmen%20Guadalupe%20Colin%20Tenorio%20and%20Brinder%20Singh%20Chohan%20and%20Mark%20Eastwood%20and%20Shan%20E%20Ahmed%20Raza%0AAbstract%3A%20%20%20Melanoma%20is%20an%20aggressive%20form%20of%20skin%20cancer%20with%20rapid%20progression%20and%20high%0Ametastatic%20potential.%20Accurate%20characterisation%20of%20tissue%20morphology%20in%0Amelanoma%20is%20crucial%20for%20prognosis%20and%20treatment%20planning.%20However%2C%20manual%0Asegmentation%20of%20tissue%20regions%20from%20haematoxylin%20and%20eosin%20%28H%26E%29%20stained%0Awhole-slide%20images%20%28WSIs%29%20is%20labour-intensive%20and%20prone%20to%20inter-observer%0Avariability%2C%20this%20motivates%20the%20need%20for%20reliable%20automated%20tissue%20segmentation%0Amethods.%20In%20this%20study%2C%20we%20propose%20a%20novel%20deep%20learning%20network%20for%20the%0Asegmentation%20of%20five%20tissue%20classes%20in%20melanoma%20H%26E%20images.%20Our%20approach%0Aleverages%20Virchow2%2C%20a%20pathology%20foundation%20model%20trained%20on%203.1%20million%0Ahistopathology%20images%20as%20a%20feature%20extractor.%20These%20features%20are%20fused%20with%20the%0Aoriginal%20RGB%20images%20and%20subsequently%20processed%20by%20an%20encoder-decoder%0Asegmentation%20network%20%28Efficient-UNet%29%20to%20produce%20accurate%20segmentation%20maps.%0AThe%20proposed%20model%20achieved%20first%20place%20in%20the%20tissue%20segmentation%20task%20of%20the%0APUMA%20Grand%20Challenge%2C%20demonstrating%20robust%20performance%20and%20generalizability.%0AOur%20results%20show%20the%20potential%20and%20efficacy%20of%20incorporating%20pathology%0Afoundation%20models%20into%20segmentation%20networks%20to%20accelerate%20computational%0Apathology%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Pathology%2520Foundation%2520Models%2520for%2520Panoptic%2520Segmentation%2520of%250A%2520%2520Melanoma%2520in%2520H%2526E%2520Images%26entry.906535625%3DJiaqi%2520Lv%2520and%2520Yijie%2520Zhu%2520and%2520Carmen%2520Guadalupe%2520Colin%2520Tenorio%2520and%2520Brinder%2520Singh%2520Chohan%2520and%2520Mark%2520Eastwood%2520and%2520Shan%2520E%2520Ahmed%2520Raza%26entry.1292438233%3D%2520%2520Melanoma%2520is%2520an%2520aggressive%2520form%2520of%2520skin%2520cancer%2520with%2520rapid%2520progression%2520and%2520high%250Ametastatic%2520potential.%2520Accurate%2520characterisation%2520of%2520tissue%2520morphology%2520in%250Amelanoma%2520is%2520crucial%2520for%2520prognosis%2520and%2520treatment%2520planning.%2520However%252C%2520manual%250Asegmentation%2520of%2520tissue%2520regions%2520from%2520haematoxylin%2520and%2520eosin%2520%2528H%2526E%2529%2520stained%250Awhole-slide%2520images%2520%2528WSIs%2529%2520is%2520labour-intensive%2520and%2520prone%2520to%2520inter-observer%250Avariability%252C%2520this%2520motivates%2520the%2520need%2520for%2520reliable%2520automated%2520tissue%2520segmentation%250Amethods.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520deep%2520learning%2520network%2520for%2520the%250Asegmentation%2520of%2520five%2520tissue%2520classes%2520in%2520melanoma%2520H%2526E%2520images.%2520Our%2520approach%250Aleverages%2520Virchow2%252C%2520a%2520pathology%2520foundation%2520model%2520trained%2520on%25203.1%2520million%250Ahistopathology%2520images%2520as%2520a%2520feature%2520extractor.%2520These%2520features%2520are%2520fused%2520with%2520the%250Aoriginal%2520RGB%2520images%2520and%2520subsequently%2520processed%2520by%2520an%2520encoder-decoder%250Asegmentation%2520network%2520%2528Efficient-UNet%2529%2520to%2520produce%2520accurate%2520segmentation%2520maps.%250AThe%2520proposed%2520model%2520achieved%2520first%2520place%2520in%2520the%2520tissue%2520segmentation%2520task%2520of%2520the%250APUMA%2520Grand%2520Challenge%252C%2520demonstrating%2520robust%2520performance%2520and%2520generalizability.%250AOur%2520results%2520show%2520the%2520potential%2520and%2520efficacy%2520of%2520incorporating%2520pathology%250Afoundation%2520models%2520into%2520segmentation%2520networks%2520to%2520accelerate%2520computational%250Apathology%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Pathology%20Foundation%20Models%20for%20Panoptic%20Segmentation%20of%0A%20%20Melanoma%20in%20H%26E%20Images&entry.906535625=Jiaqi%20Lv%20and%20Yijie%20Zhu%20and%20Carmen%20Guadalupe%20Colin%20Tenorio%20and%20Brinder%20Singh%20Chohan%20and%20Mark%20Eastwood%20and%20Shan%20E%20Ahmed%20Raza&entry.1292438233=%20%20Melanoma%20is%20an%20aggressive%20form%20of%20skin%20cancer%20with%20rapid%20progression%20and%20high%0Ametastatic%20potential.%20Accurate%20characterisation%20of%20tissue%20morphology%20in%0Amelanoma%20is%20crucial%20for%20prognosis%20and%20treatment%20planning.%20However%2C%20manual%0Asegmentation%20of%20tissue%20regions%20from%20haematoxylin%20and%20eosin%20%28H%26E%29%20stained%0Awhole-slide%20images%20%28WSIs%29%20is%20labour-intensive%20and%20prone%20to%20inter-observer%0Avariability%2C%20this%20motivates%20the%20need%20for%20reliable%20automated%20tissue%20segmentation%0Amethods.%20In%20this%20study%2C%20we%20propose%20a%20novel%20deep%20learning%20network%20for%20the%0Asegmentation%20of%20five%20tissue%20classes%20in%20melanoma%20H%26E%20images.%20Our%20approach%0Aleverages%20Virchow2%2C%20a%20pathology%20foundation%20model%20trained%20on%203.1%20million%0Ahistopathology%20images%20as%20a%20feature%20extractor.%20These%20features%20are%20fused%20with%20the%0Aoriginal%20RGB%20images%20and%20subsequently%20processed%20by%20an%20encoder-decoder%0Asegmentation%20network%20%28Efficient-UNet%29%20to%20produce%20accurate%20segmentation%20maps.%0AThe%20proposed%20model%20achieved%20first%20place%20in%20the%20tissue%20segmentation%20task%20of%20the%0APUMA%20Grand%20Challenge%2C%20demonstrating%20robust%20performance%20and%20generalizability.%0AOur%20results%20show%20the%20potential%20and%20efficacy%20of%20incorporating%20pathology%0Afoundation%20models%20into%20segmentation%20networks%20to%20accelerate%20computational%0Apathology%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13974v1&entry.124074799=Read"},
{"title": "Training-free Token Reduction for Vision Mamba", "author": "Qiankun Ma and Ziyao Zhang and Chi Su and Jie Chen and Zhen Song and Hairong Zheng and Wen Gao", "abstract": "  Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.\n", "link": "http://arxiv.org/abs/2507.14042v1", "date": "2025-07-18", "relevancy": 2.0258, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5337}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-free%20Token%20Reduction%20for%20Vision%20Mamba&body=Title%3A%20Training-free%20Token%20Reduction%20for%20Vision%20Mamba%0AAuthor%3A%20Qiankun%20Ma%20and%20Ziyao%20Zhang%20and%20Chi%20Su%20and%20Jie%20Chen%20and%20Zhen%20Song%20and%20Hairong%20Zheng%20and%20Wen%20Gao%0AAbstract%3A%20%20%20Vision%20Mamba%20has%20emerged%20as%20a%20strong%20competitor%20to%20Vision%20Transformers%20%28ViTs%29%0Adue%20to%20its%20ability%20to%20efficiently%20capture%20long-range%20dependencies%20with%20linear%0Acomputational%20complexity.%20While%20token%20reduction%2C%20an%20effective%20compression%0Atechnique%20in%20ViTs%2C%20has%20rarely%20been%20explored%20in%20Vision%20Mamba.%20Exploring%20Vision%0AMamba%27s%20efficiency%20is%20essential%20for%20enabling%20broader%20applications.%20However%2C%20we%0Afind%20that%20directly%20applying%20existing%20token%20reduction%20techniques%20for%20ViTs%20to%0AVision%20Mamba%20leads%20to%20significant%20performance%20degradation.%20This%20is%20primarily%0Abecause%20Mamba%20is%20a%20sequence%20model%20without%20attention%20mechanisms%2C%20whereas%20most%0Atoken%20reduction%20techniques%20for%20ViTs%20rely%20on%20attention%20mechanisms%20for%20importance%0Ameasurement%20and%20overlook%20the%20order%20of%20compressed%20tokens.%20In%20this%20paper%2C%20we%0Ainvestigate%20a%20Mamba%20structure-aware%20importance%20score%20to%20evaluate%20token%0Aimportance%20in%20a%20simple%20and%20effective%20manner.%20Building%20on%20this%20score%2C%20we%20further%0Apropose%20MTR%2C%20a%20training-free%20%5Ctextbf%7BM%7Damba%20%5Ctextbf%7BT%7Doken%20%5Ctextbf%7BR%7Deduction%0Aframework.%20Without%20the%20need%20for%20training%20or%20additional%20tuning%20parameters%2C%20our%0Amethod%20can%20be%20seamlessly%20integrated%20as%20a%20plug-and-play%20component%20across%20various%0AMamba%20models.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20significantly%0Areduces%20computational%20workload%20while%20minimizing%20performance%20impact%20across%0Avarious%20tasks%20and%20multiple%20backbones.%20Notably%2C%20MTR%20reduces%20FLOPs%20by%0Aapproximately%2040%5C%25%20on%20the%20Vim-B%20backbone%2C%20with%20only%20a%201.6%5C%25%20drop%20in%20ImageNet%0Aperformance%20without%20retraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-free%2520Token%2520Reduction%2520for%2520Vision%2520Mamba%26entry.906535625%3DQiankun%2520Ma%2520and%2520Ziyao%2520Zhang%2520and%2520Chi%2520Su%2520and%2520Jie%2520Chen%2520and%2520Zhen%2520Song%2520and%2520Hairong%2520Zheng%2520and%2520Wen%2520Gao%26entry.1292438233%3D%2520%2520Vision%2520Mamba%2520has%2520emerged%2520as%2520a%2520strong%2520competitor%2520to%2520Vision%2520Transformers%2520%2528ViTs%2529%250Adue%2520to%2520its%2520ability%2520to%2520efficiently%2520capture%2520long-range%2520dependencies%2520with%2520linear%250Acomputational%2520complexity.%2520While%2520token%2520reduction%252C%2520an%2520effective%2520compression%250Atechnique%2520in%2520ViTs%252C%2520has%2520rarely%2520been%2520explored%2520in%2520Vision%2520Mamba.%2520Exploring%2520Vision%250AMamba%2527s%2520efficiency%2520is%2520essential%2520for%2520enabling%2520broader%2520applications.%2520However%252C%2520we%250Afind%2520that%2520directly%2520applying%2520existing%2520token%2520reduction%2520techniques%2520for%2520ViTs%2520to%250AVision%2520Mamba%2520leads%2520to%2520significant%2520performance%2520degradation.%2520This%2520is%2520primarily%250Abecause%2520Mamba%2520is%2520a%2520sequence%2520model%2520without%2520attention%2520mechanisms%252C%2520whereas%2520most%250Atoken%2520reduction%2520techniques%2520for%2520ViTs%2520rely%2520on%2520attention%2520mechanisms%2520for%2520importance%250Ameasurement%2520and%2520overlook%2520the%2520order%2520of%2520compressed%2520tokens.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520a%2520Mamba%2520structure-aware%2520importance%2520score%2520to%2520evaluate%2520token%250Aimportance%2520in%2520a%2520simple%2520and%2520effective%2520manner.%2520Building%2520on%2520this%2520score%252C%2520we%2520further%250Apropose%2520MTR%252C%2520a%2520training-free%2520%255Ctextbf%257BM%257Damba%2520%255Ctextbf%257BT%257Doken%2520%255Ctextbf%257BR%257Deduction%250Aframework.%2520Without%2520the%2520need%2520for%2520training%2520or%2520additional%2520tuning%2520parameters%252C%2520our%250Amethod%2520can%2520be%2520seamlessly%2520integrated%2520as%2520a%2520plug-and-play%2520component%2520across%2520various%250AMamba%2520models.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520significantly%250Areduces%2520computational%2520workload%2520while%2520minimizing%2520performance%2520impact%2520across%250Avarious%2520tasks%2520and%2520multiple%2520backbones.%2520Notably%252C%2520MTR%2520reduces%2520FLOPs%2520by%250Aapproximately%252040%255C%2525%2520on%2520the%2520Vim-B%2520backbone%252C%2520with%2520only%2520a%25201.6%255C%2525%2520drop%2520in%2520ImageNet%250Aperformance%2520without%2520retraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-free%20Token%20Reduction%20for%20Vision%20Mamba&entry.906535625=Qiankun%20Ma%20and%20Ziyao%20Zhang%20and%20Chi%20Su%20and%20Jie%20Chen%20and%20Zhen%20Song%20and%20Hairong%20Zheng%20and%20Wen%20Gao&entry.1292438233=%20%20Vision%20Mamba%20has%20emerged%20as%20a%20strong%20competitor%20to%20Vision%20Transformers%20%28ViTs%29%0Adue%20to%20its%20ability%20to%20efficiently%20capture%20long-range%20dependencies%20with%20linear%0Acomputational%20complexity.%20While%20token%20reduction%2C%20an%20effective%20compression%0Atechnique%20in%20ViTs%2C%20has%20rarely%20been%20explored%20in%20Vision%20Mamba.%20Exploring%20Vision%0AMamba%27s%20efficiency%20is%20essential%20for%20enabling%20broader%20applications.%20However%2C%20we%0Afind%20that%20directly%20applying%20existing%20token%20reduction%20techniques%20for%20ViTs%20to%0AVision%20Mamba%20leads%20to%20significant%20performance%20degradation.%20This%20is%20primarily%0Abecause%20Mamba%20is%20a%20sequence%20model%20without%20attention%20mechanisms%2C%20whereas%20most%0Atoken%20reduction%20techniques%20for%20ViTs%20rely%20on%20attention%20mechanisms%20for%20importance%0Ameasurement%20and%20overlook%20the%20order%20of%20compressed%20tokens.%20In%20this%20paper%2C%20we%0Ainvestigate%20a%20Mamba%20structure-aware%20importance%20score%20to%20evaluate%20token%0Aimportance%20in%20a%20simple%20and%20effective%20manner.%20Building%20on%20this%20score%2C%20we%20further%0Apropose%20MTR%2C%20a%20training-free%20%5Ctextbf%7BM%7Damba%20%5Ctextbf%7BT%7Doken%20%5Ctextbf%7BR%7Deduction%0Aframework.%20Without%20the%20need%20for%20training%20or%20additional%20tuning%20parameters%2C%20our%0Amethod%20can%20be%20seamlessly%20integrated%20as%20a%20plug-and-play%20component%20across%20various%0AMamba%20models.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20significantly%0Areduces%20computational%20workload%20while%20minimizing%20performance%20impact%20across%0Avarious%20tasks%20and%20multiple%20backbones.%20Notably%2C%20MTR%20reduces%20FLOPs%20by%0Aapproximately%2040%5C%25%20on%20the%20Vim-B%20backbone%2C%20with%20only%20a%201.6%5C%25%20drop%20in%20ImageNet%0Aperformance%20without%20retraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14042v1&entry.124074799=Read"},
{"title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term\n  Temporal Dependencies", "author": "Itay Katav and Aryeh Kontorovich", "abstract": "  Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub\n", "link": "http://arxiv.org/abs/2507.13998v1", "date": "2025-07-18", "relevancy": 2.0224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.515}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5103}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParallelTime%3A%20Dynamically%20Weighting%20the%20Balance%20of%20Short-%20and%20Long-Term%0A%20%20Temporal%20Dependencies&body=Title%3A%20ParallelTime%3A%20Dynamically%20Weighting%20the%20Balance%20of%20Short-%20and%20Long-Term%0A%20%20Temporal%20Dependencies%0AAuthor%3A%20Itay%20Katav%20and%20Aryeh%20Kontorovich%0AAbstract%3A%20%20%20Modern%20multivariate%20time%20series%20forecasting%20primarily%20relies%20on%20two%0Aarchitectures%3A%20the%20Transformer%20with%20attention%20mechanism%20and%20Mamba.%20In%20natural%0Alanguage%20processing%2C%20an%20approach%20has%20been%20used%20that%20combines%20local%20window%0Aattention%20for%20capturing%20short-term%20dependencies%20and%20Mamba%20for%20capturing%0Along-term%20dependencies%2C%20with%20their%20outputs%20averaged%20to%20assign%20equal%20weight%20to%0Aboth.%20We%20find%20that%20for%20time-series%20forecasting%20tasks%2C%20assigning%20equal%20weight%20to%0Along-term%20and%20short-term%20dependencies%20is%20not%20optimal.%20To%20mitigate%20this%2C%20we%0Apropose%20a%20dynamic%20weighting%20mechanism%2C%20ParallelTime%20Weighter%2C%20which%20calculates%0Ainterdependent%20weights%20for%20long-term%20and%20short-term%20dependencies%20for%20each%20token%0Abased%20on%20the%20input%20and%20the%20model%27s%20knowledge.%20Furthermore%2C%20we%20introduce%20the%0AParallelTime%20architecture%2C%20which%20incorporates%20the%20ParallelTime%20Weighter%0Amechanism%20to%20deliver%20state-of-the-art%20performance%20across%20diverse%20benchmarks.%0AOur%20architecture%20demonstrates%20robustness%2C%20achieves%20lower%20FLOPs%2C%20requires%20fewer%0Aparameters%2C%20scales%20effectively%20to%20longer%20prediction%20horizons%2C%20and%20significantly%0Aoutperforms%20existing%20methods.%20These%20advances%20highlight%20a%20promising%20path%20for%0Afuture%20developments%20of%20parallel%20Attention-Mamba%20in%20time%20series%20forecasting.%20The%0Aimplementation%20is%20readily%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/itay1551/ParallelTime%7D%7BParallelTime%20GitHub%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallelTime%253A%2520Dynamically%2520Weighting%2520the%2520Balance%2520of%2520Short-%2520and%2520Long-Term%250A%2520%2520Temporal%2520Dependencies%26entry.906535625%3DItay%2520Katav%2520and%2520Aryeh%2520Kontorovich%26entry.1292438233%3D%2520%2520Modern%2520multivariate%2520time%2520series%2520forecasting%2520primarily%2520relies%2520on%2520two%250Aarchitectures%253A%2520the%2520Transformer%2520with%2520attention%2520mechanism%2520and%2520Mamba.%2520In%2520natural%250Alanguage%2520processing%252C%2520an%2520approach%2520has%2520been%2520used%2520that%2520combines%2520local%2520window%250Aattention%2520for%2520capturing%2520short-term%2520dependencies%2520and%2520Mamba%2520for%2520capturing%250Along-term%2520dependencies%252C%2520with%2520their%2520outputs%2520averaged%2520to%2520assign%2520equal%2520weight%2520to%250Aboth.%2520We%2520find%2520that%2520for%2520time-series%2520forecasting%2520tasks%252C%2520assigning%2520equal%2520weight%2520to%250Along-term%2520and%2520short-term%2520dependencies%2520is%2520not%2520optimal.%2520To%2520mitigate%2520this%252C%2520we%250Apropose%2520a%2520dynamic%2520weighting%2520mechanism%252C%2520ParallelTime%2520Weighter%252C%2520which%2520calculates%250Ainterdependent%2520weights%2520for%2520long-term%2520and%2520short-term%2520dependencies%2520for%2520each%2520token%250Abased%2520on%2520the%2520input%2520and%2520the%2520model%2527s%2520knowledge.%2520Furthermore%252C%2520we%2520introduce%2520the%250AParallelTime%2520architecture%252C%2520which%2520incorporates%2520the%2520ParallelTime%2520Weighter%250Amechanism%2520to%2520deliver%2520state-of-the-art%2520performance%2520across%2520diverse%2520benchmarks.%250AOur%2520architecture%2520demonstrates%2520robustness%252C%2520achieves%2520lower%2520FLOPs%252C%2520requires%2520fewer%250Aparameters%252C%2520scales%2520effectively%2520to%2520longer%2520prediction%2520horizons%252C%2520and%2520significantly%250Aoutperforms%2520existing%2520methods.%2520These%2520advances%2520highlight%2520a%2520promising%2520path%2520for%250Afuture%2520developments%2520of%2520parallel%2520Attention-Mamba%2520in%2520time%2520series%2520forecasting.%2520The%250Aimplementation%2520is%2520readily%2520available%2520at%253A%250A%255Chref%257Bhttps%253A//github.com/itay1551/ParallelTime%257D%257BParallelTime%2520GitHub%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParallelTime%3A%20Dynamically%20Weighting%20the%20Balance%20of%20Short-%20and%20Long-Term%0A%20%20Temporal%20Dependencies&entry.906535625=Itay%20Katav%20and%20Aryeh%20Kontorovich&entry.1292438233=%20%20Modern%20multivariate%20time%20series%20forecasting%20primarily%20relies%20on%20two%0Aarchitectures%3A%20the%20Transformer%20with%20attention%20mechanism%20and%20Mamba.%20In%20natural%0Alanguage%20processing%2C%20an%20approach%20has%20been%20used%20that%20combines%20local%20window%0Aattention%20for%20capturing%20short-term%20dependencies%20and%20Mamba%20for%20capturing%0Along-term%20dependencies%2C%20with%20their%20outputs%20averaged%20to%20assign%20equal%20weight%20to%0Aboth.%20We%20find%20that%20for%20time-series%20forecasting%20tasks%2C%20assigning%20equal%20weight%20to%0Along-term%20and%20short-term%20dependencies%20is%20not%20optimal.%20To%20mitigate%20this%2C%20we%0Apropose%20a%20dynamic%20weighting%20mechanism%2C%20ParallelTime%20Weighter%2C%20which%20calculates%0Ainterdependent%20weights%20for%20long-term%20and%20short-term%20dependencies%20for%20each%20token%0Abased%20on%20the%20input%20and%20the%20model%27s%20knowledge.%20Furthermore%2C%20we%20introduce%20the%0AParallelTime%20architecture%2C%20which%20incorporates%20the%20ParallelTime%20Weighter%0Amechanism%20to%20deliver%20state-of-the-art%20performance%20across%20diverse%20benchmarks.%0AOur%20architecture%20demonstrates%20robustness%2C%20achieves%20lower%20FLOPs%2C%20requires%20fewer%0Aparameters%2C%20scales%20effectively%20to%20longer%20prediction%20horizons%2C%20and%20significantly%0Aoutperforms%20existing%20methods.%20These%20advances%20highlight%20a%20promising%20path%20for%0Afuture%20developments%20of%20parallel%20Attention-Mamba%20in%20time%20series%20forecasting.%20The%0Aimplementation%20is%20readily%20available%20at%3A%0A%5Chref%7Bhttps%3A//github.com/itay1551/ParallelTime%7D%7BParallelTime%20GitHub%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13998v1&entry.124074799=Read"},
{"title": "Equivalent and Compact Representations of Neural Network Controllers\n  With Decision Trees", "author": "Kevin Chang and Nathan Dahlin and Rahul Jain and Pierluigi Nuzzo", "abstract": "  Over the past decade, neural network (NN)-based controllers have demonstrated\nremarkable efficacy in a variety of decision-making tasks. However, their\nblack-box nature and the risk of unexpected behaviors pose a challenge to their\ndeployment in real-world systems requiring strong guarantees of correctness and\nsafety. We address these limitations by investigating the transformation of\nNN-based controllers into equivalent soft decision tree (SDT)-based controllers\nand its impact on verifiability. In contrast to existing work, we focus on\ndiscrete-output NN controllers including rectified linear unit (ReLU)\nactivation functions as well as argmax operations. We then devise an exact yet\nefficient transformation algorithm which automatically prunes redundant\nbranches. We first demonstrate the practical efficacy of the transformation\nalgorithm applied to an autonomous driving NN controller within OpenAI Gym's\nCarRacing environment. Subsequently, we evaluate our approach using two\nbenchmarks from the OpenAI Gym environment. Our results indicate that the SDT\ntransformation can benefit formal verification, showing runtime improvements of\nup to $21 \\times$ and $2 \\times$ for MountainCar-v0 and CartPole-v1,\nrespectively.\n", "link": "http://arxiv.org/abs/2304.06049v3", "date": "2025-07-18", "relevancy": 2.0119, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5056}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5017}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivalent%20and%20Compact%20Representations%20of%20Neural%20Network%20Controllers%0A%20%20With%20Decision%20Trees&body=Title%3A%20Equivalent%20and%20Compact%20Representations%20of%20Neural%20Network%20Controllers%0A%20%20With%20Decision%20Trees%0AAuthor%3A%20Kevin%20Chang%20and%20Nathan%20Dahlin%20and%20Rahul%20Jain%20and%20Pierluigi%20Nuzzo%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20neural%20network%20%28NN%29-based%20controllers%20have%20demonstrated%0Aremarkable%20efficacy%20in%20a%20variety%20of%20decision-making%20tasks.%20However%2C%20their%0Ablack-box%20nature%20and%20the%20risk%20of%20unexpected%20behaviors%20pose%20a%20challenge%20to%20their%0Adeployment%20in%20real-world%20systems%20requiring%20strong%20guarantees%20of%20correctness%20and%0Asafety.%20We%20address%20these%20limitations%20by%20investigating%20the%20transformation%20of%0ANN-based%20controllers%20into%20equivalent%20soft%20decision%20tree%20%28SDT%29-based%20controllers%0Aand%20its%20impact%20on%20verifiability.%20In%20contrast%20to%20existing%20work%2C%20we%20focus%20on%0Adiscrete-output%20NN%20controllers%20including%20rectified%20linear%20unit%20%28ReLU%29%0Aactivation%20functions%20as%20well%20as%20argmax%20operations.%20We%20then%20devise%20an%20exact%20yet%0Aefficient%20transformation%20algorithm%20which%20automatically%20prunes%20redundant%0Abranches.%20We%20first%20demonstrate%20the%20practical%20efficacy%20of%20the%20transformation%0Aalgorithm%20applied%20to%20an%20autonomous%20driving%20NN%20controller%20within%20OpenAI%20Gym%27s%0ACarRacing%20environment.%20Subsequently%2C%20we%20evaluate%20our%20approach%20using%20two%0Abenchmarks%20from%20the%20OpenAI%20Gym%20environment.%20Our%20results%20indicate%20that%20the%20SDT%0Atransformation%20can%20benefit%20formal%20verification%2C%20showing%20runtime%20improvements%20of%0Aup%20to%20%2421%20%5Ctimes%24%20and%20%242%20%5Ctimes%24%20for%20MountainCar-v0%20and%20CartPole-v1%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06049v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivalent%2520and%2520Compact%2520Representations%2520of%2520Neural%2520Network%2520Controllers%250A%2520%2520With%2520Decision%2520Trees%26entry.906535625%3DKevin%2520Chang%2520and%2520Nathan%2520Dahlin%2520and%2520Rahul%2520Jain%2520and%2520Pierluigi%2520Nuzzo%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520neural%2520network%2520%2528NN%2529-based%2520controllers%2520have%2520demonstrated%250Aremarkable%2520efficacy%2520in%2520a%2520variety%2520of%2520decision-making%2520tasks.%2520However%252C%2520their%250Ablack-box%2520nature%2520and%2520the%2520risk%2520of%2520unexpected%2520behaviors%2520pose%2520a%2520challenge%2520to%2520their%250Adeployment%2520in%2520real-world%2520systems%2520requiring%2520strong%2520guarantees%2520of%2520correctness%2520and%250Asafety.%2520We%2520address%2520these%2520limitations%2520by%2520investigating%2520the%2520transformation%2520of%250ANN-based%2520controllers%2520into%2520equivalent%2520soft%2520decision%2520tree%2520%2528SDT%2529-based%2520controllers%250Aand%2520its%2520impact%2520on%2520verifiability.%2520In%2520contrast%2520to%2520existing%2520work%252C%2520we%2520focus%2520on%250Adiscrete-output%2520NN%2520controllers%2520including%2520rectified%2520linear%2520unit%2520%2528ReLU%2529%250Aactivation%2520functions%2520as%2520well%2520as%2520argmax%2520operations.%2520We%2520then%2520devise%2520an%2520exact%2520yet%250Aefficient%2520transformation%2520algorithm%2520which%2520automatically%2520prunes%2520redundant%250Abranches.%2520We%2520first%2520demonstrate%2520the%2520practical%2520efficacy%2520of%2520the%2520transformation%250Aalgorithm%2520applied%2520to%2520an%2520autonomous%2520driving%2520NN%2520controller%2520within%2520OpenAI%2520Gym%2527s%250ACarRacing%2520environment.%2520Subsequently%252C%2520we%2520evaluate%2520our%2520approach%2520using%2520two%250Abenchmarks%2520from%2520the%2520OpenAI%2520Gym%2520environment.%2520Our%2520results%2520indicate%2520that%2520the%2520SDT%250Atransformation%2520can%2520benefit%2520formal%2520verification%252C%2520showing%2520runtime%2520improvements%2520of%250Aup%2520to%2520%252421%2520%255Ctimes%2524%2520and%2520%25242%2520%255Ctimes%2524%2520for%2520MountainCar-v0%2520and%2520CartPole-v1%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06049v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivalent%20and%20Compact%20Representations%20of%20Neural%20Network%20Controllers%0A%20%20With%20Decision%20Trees&entry.906535625=Kevin%20Chang%20and%20Nathan%20Dahlin%20and%20Rahul%20Jain%20and%20Pierluigi%20Nuzzo&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20neural%20network%20%28NN%29-based%20controllers%20have%20demonstrated%0Aremarkable%20efficacy%20in%20a%20variety%20of%20decision-making%20tasks.%20However%2C%20their%0Ablack-box%20nature%20and%20the%20risk%20of%20unexpected%20behaviors%20pose%20a%20challenge%20to%20their%0Adeployment%20in%20real-world%20systems%20requiring%20strong%20guarantees%20of%20correctness%20and%0Asafety.%20We%20address%20these%20limitations%20by%20investigating%20the%20transformation%20of%0ANN-based%20controllers%20into%20equivalent%20soft%20decision%20tree%20%28SDT%29-based%20controllers%0Aand%20its%20impact%20on%20verifiability.%20In%20contrast%20to%20existing%20work%2C%20we%20focus%20on%0Adiscrete-output%20NN%20controllers%20including%20rectified%20linear%20unit%20%28ReLU%29%0Aactivation%20functions%20as%20well%20as%20argmax%20operations.%20We%20then%20devise%20an%20exact%20yet%0Aefficient%20transformation%20algorithm%20which%20automatically%20prunes%20redundant%0Abranches.%20We%20first%20demonstrate%20the%20practical%20efficacy%20of%20the%20transformation%0Aalgorithm%20applied%20to%20an%20autonomous%20driving%20NN%20controller%20within%20OpenAI%20Gym%27s%0ACarRacing%20environment.%20Subsequently%2C%20we%20evaluate%20our%20approach%20using%20two%0Abenchmarks%20from%20the%20OpenAI%20Gym%20environment.%20Our%20results%20indicate%20that%20the%20SDT%0Atransformation%20can%20benefit%20formal%20verification%2C%20showing%20runtime%20improvements%20of%0Aup%20to%20%2421%20%5Ctimes%24%20and%20%242%20%5Ctimes%24%20for%20MountainCar-v0%20and%20CartPole-v1%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06049v3&entry.124074799=Read"},
{"title": "Exploiting Label Skewness for Spiking Neural Networks in Federated\n  Learning", "author": "Di Yu and Xin Du and Linshan Jiang and Huijing Zhang and Shuiguang Deng", "abstract": "  The energy efficiency of deep spiking neural networks (SNNs) aligns with the\nconstraints of resource-limited edge devices, positioning SNNs as a promising\nfoundation for intelligent applications leveraging the extensive data collected\nby these devices. To address data privacy concerns when deploying SNNs on edge\ndevices, federated learning (FL) facilitates collaborative model training by\nleveraging data distributed across edge devices without transmitting local data\nto a central server. However, existing FL approaches struggle with label-skewed\ndata across devices, which leads to drift in local SNN models and degrades the\nperformance of the global SNN model. In this paper, we propose a novel\nframework called FedLEC, which incorporates intra-client label weight\ncalibration to balance the learning intensity across local labels and\ninter-client knowledge distillation to mitigate local SNN model bias caused by\nlabel absence. Extensive experiments with three different structured SNNs\nacross five datasets (i.e., three non-neuromorphic and two neuromorphic\ndatasets) demonstrate the efficiency of FedLEC. Compared to eight\nstate-of-the-art FL algorithms, FedLEC achieves an average accuracy improvement\nof approximately 11.59% for the global SNN model under various label skew\ndistribution settings.\n", "link": "http://arxiv.org/abs/2412.17305v3", "date": "2025-07-18", "relevancy": 2.0081, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5407}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4793}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Label%20Skewness%20for%20Spiking%20Neural%20Networks%20in%20Federated%0A%20%20Learning&body=Title%3A%20Exploiting%20Label%20Skewness%20for%20Spiking%20Neural%20Networks%20in%20Federated%0A%20%20Learning%0AAuthor%3A%20Di%20Yu%20and%20Xin%20Du%20and%20Linshan%20Jiang%20and%20Huijing%20Zhang%20and%20Shuiguang%20Deng%0AAbstract%3A%20%20%20The%20energy%20efficiency%20of%20deep%20spiking%20neural%20networks%20%28SNNs%29%20aligns%20with%20the%0Aconstraints%20of%20resource-limited%20edge%20devices%2C%20positioning%20SNNs%20as%20a%20promising%0Afoundation%20for%20intelligent%20applications%20leveraging%20the%20extensive%20data%20collected%0Aby%20these%20devices.%20To%20address%20data%20privacy%20concerns%20when%20deploying%20SNNs%20on%20edge%0Adevices%2C%20federated%20learning%20%28FL%29%20facilitates%20collaborative%20model%20training%20by%0Aleveraging%20data%20distributed%20across%20edge%20devices%20without%20transmitting%20local%20data%0Ato%20a%20central%20server.%20However%2C%20existing%20FL%20approaches%20struggle%20with%20label-skewed%0Adata%20across%20devices%2C%20which%20leads%20to%20drift%20in%20local%20SNN%20models%20and%20degrades%20the%0Aperformance%20of%20the%20global%20SNN%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%20called%20FedLEC%2C%20which%20incorporates%20intra-client%20label%20weight%0Acalibration%20to%20balance%20the%20learning%20intensity%20across%20local%20labels%20and%0Ainter-client%20knowledge%20distillation%20to%20mitigate%20local%20SNN%20model%20bias%20caused%20by%0Alabel%20absence.%20Extensive%20experiments%20with%20three%20different%20structured%20SNNs%0Aacross%20five%20datasets%20%28i.e.%2C%20three%20non-neuromorphic%20and%20two%20neuromorphic%0Adatasets%29%20demonstrate%20the%20efficiency%20of%20FedLEC.%20Compared%20to%20eight%0Astate-of-the-art%20FL%20algorithms%2C%20FedLEC%20achieves%20an%20average%20accuracy%20improvement%0Aof%20approximately%2011.59%25%20for%20the%20global%20SNN%20model%20under%20various%20label%20skew%0Adistribution%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17305v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Label%2520Skewness%2520for%2520Spiking%2520Neural%2520Networks%2520in%2520Federated%250A%2520%2520Learning%26entry.906535625%3DDi%2520Yu%2520and%2520Xin%2520Du%2520and%2520Linshan%2520Jiang%2520and%2520Huijing%2520Zhang%2520and%2520Shuiguang%2520Deng%26entry.1292438233%3D%2520%2520The%2520energy%2520efficiency%2520of%2520deep%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520aligns%2520with%2520the%250Aconstraints%2520of%2520resource-limited%2520edge%2520devices%252C%2520positioning%2520SNNs%2520as%2520a%2520promising%250Afoundation%2520for%2520intelligent%2520applications%2520leveraging%2520the%2520extensive%2520data%2520collected%250Aby%2520these%2520devices.%2520To%2520address%2520data%2520privacy%2520concerns%2520when%2520deploying%2520SNNs%2520on%2520edge%250Adevices%252C%2520federated%2520learning%2520%2528FL%2529%2520facilitates%2520collaborative%2520model%2520training%2520by%250Aleveraging%2520data%2520distributed%2520across%2520edge%2520devices%2520without%2520transmitting%2520local%2520data%250Ato%2520a%2520central%2520server.%2520However%252C%2520existing%2520FL%2520approaches%2520struggle%2520with%2520label-skewed%250Adata%2520across%2520devices%252C%2520which%2520leads%2520to%2520drift%2520in%2520local%2520SNN%2520models%2520and%2520degrades%2520the%250Aperformance%2520of%2520the%2520global%2520SNN%2520model.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520called%2520FedLEC%252C%2520which%2520incorporates%2520intra-client%2520label%2520weight%250Acalibration%2520to%2520balance%2520the%2520learning%2520intensity%2520across%2520local%2520labels%2520and%250Ainter-client%2520knowledge%2520distillation%2520to%2520mitigate%2520local%2520SNN%2520model%2520bias%2520caused%2520by%250Alabel%2520absence.%2520Extensive%2520experiments%2520with%2520three%2520different%2520structured%2520SNNs%250Aacross%2520five%2520datasets%2520%2528i.e.%252C%2520three%2520non-neuromorphic%2520and%2520two%2520neuromorphic%250Adatasets%2529%2520demonstrate%2520the%2520efficiency%2520of%2520FedLEC.%2520Compared%2520to%2520eight%250Astate-of-the-art%2520FL%2520algorithms%252C%2520FedLEC%2520achieves%2520an%2520average%2520accuracy%2520improvement%250Aof%2520approximately%252011.59%2525%2520for%2520the%2520global%2520SNN%2520model%2520under%2520various%2520label%2520skew%250Adistribution%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17305v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Label%20Skewness%20for%20Spiking%20Neural%20Networks%20in%20Federated%0A%20%20Learning&entry.906535625=Di%20Yu%20and%20Xin%20Du%20and%20Linshan%20Jiang%20and%20Huijing%20Zhang%20and%20Shuiguang%20Deng&entry.1292438233=%20%20The%20energy%20efficiency%20of%20deep%20spiking%20neural%20networks%20%28SNNs%29%20aligns%20with%20the%0Aconstraints%20of%20resource-limited%20edge%20devices%2C%20positioning%20SNNs%20as%20a%20promising%0Afoundation%20for%20intelligent%20applications%20leveraging%20the%20extensive%20data%20collected%0Aby%20these%20devices.%20To%20address%20data%20privacy%20concerns%20when%20deploying%20SNNs%20on%20edge%0Adevices%2C%20federated%20learning%20%28FL%29%20facilitates%20collaborative%20model%20training%20by%0Aleveraging%20data%20distributed%20across%20edge%20devices%20without%20transmitting%20local%20data%0Ato%20a%20central%20server.%20However%2C%20existing%20FL%20approaches%20struggle%20with%20label-skewed%0Adata%20across%20devices%2C%20which%20leads%20to%20drift%20in%20local%20SNN%20models%20and%20degrades%20the%0Aperformance%20of%20the%20global%20SNN%20model.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Aframework%20called%20FedLEC%2C%20which%20incorporates%20intra-client%20label%20weight%0Acalibration%20to%20balance%20the%20learning%20intensity%20across%20local%20labels%20and%0Ainter-client%20knowledge%20distillation%20to%20mitigate%20local%20SNN%20model%20bias%20caused%20by%0Alabel%20absence.%20Extensive%20experiments%20with%20three%20different%20structured%20SNNs%0Aacross%20five%20datasets%20%28i.e.%2C%20three%20non-neuromorphic%20and%20two%20neuromorphic%0Adatasets%29%20demonstrate%20the%20efficiency%20of%20FedLEC.%20Compared%20to%20eight%0Astate-of-the-art%20FL%20algorithms%2C%20FedLEC%20achieves%20an%20average%20accuracy%20improvement%0Aof%20approximately%2011.59%25%20for%20the%20global%20SNN%20model%20under%20various%20label%20skew%0Adistribution%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17305v3&entry.124074799=Read"},
{"title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis\n  using Sentinel-1 Data", "author": "Luigi Russo and Francesco Mauro and Babak Memar and Alessandro Sebastianelli and Silvia Liberata Ullo and Paolo Gamba", "abstract": "  Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.\n", "link": "http://arxiv.org/abs/2507.13852v1", "date": "2025-07-18", "relevancy": 2.0066, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5248}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4916}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Quantum-assisted%20Attention%20U-Net%20for%20Building%20Segmentation%20over%20Tunis%0A%20%20using%20Sentinel-1%20Data&body=Title%3A%20A%20Quantum-assisted%20Attention%20U-Net%20for%20Building%20Segmentation%20over%20Tunis%0A%20%20using%20Sentinel-1%20Data%0AAuthor%3A%20Luigi%20Russo%20and%20Francesco%20Mauro%20and%20Babak%20Memar%20and%20Alessandro%20Sebastianelli%20and%20Silvia%20Liberata%20Ullo%20and%20Paolo%20Gamba%0AAbstract%3A%20%20%20Building%20segmentation%20in%20urban%20areas%20is%20essential%20in%20fields%20such%20as%20urban%0Aplanning%2C%20disaster%20response%2C%20and%20population%20mapping.%20Yet%20accurately%20segmenting%0Abuildings%20in%20dense%20urban%20regions%20presents%20challenges%20due%20to%20the%20large%20size%20and%0Ahigh%20resolution%20of%20satellite%20images.%20This%20study%20investigates%20the%20use%20of%20a%0AQuanvolutional%20pre-processing%20to%20enhance%20the%20capability%20of%20the%20Attention%20U-Net%0Amodel%20in%20the%20building%20segmentation.%20Specifically%2C%20this%20paper%20focuses%20on%20the%0Aurban%20landscape%20of%20Tunis%2C%20utilizing%20Sentinel-1%20Synthetic%20Aperture%20Radar%20%28SAR%29%0Aimagery.%20In%20this%20work%2C%20Quanvolution%20was%20used%20to%20extract%20more%20informative%0Afeature%20maps%20that%20capture%20essential%20structural%20details%20in%20radar%20imagery%2C%0Aproving%20beneficial%20for%20accurate%20building%20segmentation.%20Preliminary%20results%0Aindicate%20that%20proposed%20methodology%20achieves%20comparable%20test%20accuracy%20to%20the%0Astandard%20Attention%20U-Net%20model%20while%20significantly%20reducing%20network%20parameters.%0AThis%20result%20aligns%20with%20findings%20from%20previous%20works%2C%20confirming%20that%0AQuanvolution%20not%20only%20maintains%20model%20accuracy%20but%20also%20increases%20computational%0Aefficiency.%20These%20promising%20outcomes%20highlight%20the%20potential%20of%0Aquantum-assisted%20Deep%20Learning%20frameworks%20for%20large-scale%20building%20segmentation%0Ain%20urban%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Quantum-assisted%2520Attention%2520U-Net%2520for%2520Building%2520Segmentation%2520over%2520Tunis%250A%2520%2520using%2520Sentinel-1%2520Data%26entry.906535625%3DLuigi%2520Russo%2520and%2520Francesco%2520Mauro%2520and%2520Babak%2520Memar%2520and%2520Alessandro%2520Sebastianelli%2520and%2520Silvia%2520Liberata%2520Ullo%2520and%2520Paolo%2520Gamba%26entry.1292438233%3D%2520%2520Building%2520segmentation%2520in%2520urban%2520areas%2520is%2520essential%2520in%2520fields%2520such%2520as%2520urban%250Aplanning%252C%2520disaster%2520response%252C%2520and%2520population%2520mapping.%2520Yet%2520accurately%2520segmenting%250Abuildings%2520in%2520dense%2520urban%2520regions%2520presents%2520challenges%2520due%2520to%2520the%2520large%2520size%2520and%250Ahigh%2520resolution%2520of%2520satellite%2520images.%2520This%2520study%2520investigates%2520the%2520use%2520of%2520a%250AQuanvolutional%2520pre-processing%2520to%2520enhance%2520the%2520capability%2520of%2520the%2520Attention%2520U-Net%250Amodel%2520in%2520the%2520building%2520segmentation.%2520Specifically%252C%2520this%2520paper%2520focuses%2520on%2520the%250Aurban%2520landscape%2520of%2520Tunis%252C%2520utilizing%2520Sentinel-1%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%250Aimagery.%2520In%2520this%2520work%252C%2520Quanvolution%2520was%2520used%2520to%2520extract%2520more%2520informative%250Afeature%2520maps%2520that%2520capture%2520essential%2520structural%2520details%2520in%2520radar%2520imagery%252C%250Aproving%2520beneficial%2520for%2520accurate%2520building%2520segmentation.%2520Preliminary%2520results%250Aindicate%2520that%2520proposed%2520methodology%2520achieves%2520comparable%2520test%2520accuracy%2520to%2520the%250Astandard%2520Attention%2520U-Net%2520model%2520while%2520significantly%2520reducing%2520network%2520parameters.%250AThis%2520result%2520aligns%2520with%2520findings%2520from%2520previous%2520works%252C%2520confirming%2520that%250AQuanvolution%2520not%2520only%2520maintains%2520model%2520accuracy%2520but%2520also%2520increases%2520computational%250Aefficiency.%2520These%2520promising%2520outcomes%2520highlight%2520the%2520potential%2520of%250Aquantum-assisted%2520Deep%2520Learning%2520frameworks%2520for%2520large-scale%2520building%2520segmentation%250Ain%2520urban%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Quantum-assisted%20Attention%20U-Net%20for%20Building%20Segmentation%20over%20Tunis%0A%20%20using%20Sentinel-1%20Data&entry.906535625=Luigi%20Russo%20and%20Francesco%20Mauro%20and%20Babak%20Memar%20and%20Alessandro%20Sebastianelli%20and%20Silvia%20Liberata%20Ullo%20and%20Paolo%20Gamba&entry.1292438233=%20%20Building%20segmentation%20in%20urban%20areas%20is%20essential%20in%20fields%20such%20as%20urban%0Aplanning%2C%20disaster%20response%2C%20and%20population%20mapping.%20Yet%20accurately%20segmenting%0Abuildings%20in%20dense%20urban%20regions%20presents%20challenges%20due%20to%20the%20large%20size%20and%0Ahigh%20resolution%20of%20satellite%20images.%20This%20study%20investigates%20the%20use%20of%20a%0AQuanvolutional%20pre-processing%20to%20enhance%20the%20capability%20of%20the%20Attention%20U-Net%0Amodel%20in%20the%20building%20segmentation.%20Specifically%2C%20this%20paper%20focuses%20on%20the%0Aurban%20landscape%20of%20Tunis%2C%20utilizing%20Sentinel-1%20Synthetic%20Aperture%20Radar%20%28SAR%29%0Aimagery.%20In%20this%20work%2C%20Quanvolution%20was%20used%20to%20extract%20more%20informative%0Afeature%20maps%20that%20capture%20essential%20structural%20details%20in%20radar%20imagery%2C%0Aproving%20beneficial%20for%20accurate%20building%20segmentation.%20Preliminary%20results%0Aindicate%20that%20proposed%20methodology%20achieves%20comparable%20test%20accuracy%20to%20the%0Astandard%20Attention%20U-Net%20model%20while%20significantly%20reducing%20network%20parameters.%0AThis%20result%20aligns%20with%20findings%20from%20previous%20works%2C%20confirming%20that%0AQuanvolution%20not%20only%20maintains%20model%20accuracy%20but%20also%20increases%20computational%0Aefficiency.%20These%20promising%20outcomes%20highlight%20the%20potential%20of%0Aquantum-assisted%20Deep%20Learning%20frameworks%20for%20large-scale%20building%20segmentation%0Ain%20urban%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13852v1&entry.124074799=Read"},
{"title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep\n  Learning and Visual Explanations", "author": "Yong Feng and Xiaolei Zhang and Shijin Feng and Yong Zhao and Yihan Chen", "abstract": "  Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.\n", "link": "http://arxiv.org/abs/2507.14010v1", "date": "2025-07-18", "relevancy": 1.9978, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.509}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4983}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Classification%20and%20Segmentation%20of%20Tunnel%20Cracks%20Based%20on%20Deep%0A%20%20Learning%20and%20Visual%20Explanations&body=Title%3A%20Automatic%20Classification%20and%20Segmentation%20of%20Tunnel%20Cracks%20Based%20on%20Deep%0A%20%20Learning%20and%20Visual%20Explanations%0AAuthor%3A%20Yong%20Feng%20and%20Xiaolei%20Zhang%20and%20Shijin%20Feng%20and%20Yong%20Zhao%20and%20Yihan%20Chen%0AAbstract%3A%20%20%20Tunnel%20lining%20crack%20is%20a%20crucial%20indicator%20of%20tunnels%27%20safety%20status.%20Aiming%0Ato%20classify%20and%20segment%20tunnel%20cracks%20with%20enhanced%20accuracy%20and%20efficiency%2C%0Athis%20study%20proposes%20a%20two-step%20deep%20learning-based%20method.%20An%20automatic%20tunnel%0Aimage%20classification%20model%20is%20developed%20using%20the%20DenseNet-169%20in%20the%20first%0Astep.%20The%20proposed%20crack%20segmentation%20model%20in%20the%20second%20step%20is%20based%20on%20the%0ADeepLabV3%2B%2C%20whose%20internal%20logic%20is%20evaluated%20via%20a%20score-weighted%20visual%0Aexplanation%20technique.%20Proposed%20method%20combines%20tunnel%20image%20classification%20and%0Asegmentation%20together%2C%20so%20that%20the%20selected%20images%20containing%20cracks%20from%20the%0Afirst%20step%20are%20segmented%20in%20the%20second%20step%20to%20improve%20the%20detection%20accuracy%0Aand%20efficiency.%20The%20superior%20performances%20of%20the%20two-step%20method%20are%20validated%0Aby%20experiments.%20The%20results%20show%20that%20the%20accuracy%20and%20frames%20per%20second%20%28FPS%29%0Aof%20the%20tunnel%20crack%20classification%20model%20are%2092.23%25%20and%2039.80%2C%20respectively%2C%0Awhich%20are%20higher%20than%20other%20convolutional%20neural%20networks%20%28CNN%29%20based%20and%0ATransformer%20based%20models.%20Also%2C%20the%20intersection%20over%20union%20%28IoU%29%20and%20F1%20score%0Aof%20the%20tunnel%20crack%20segmentation%20model%20are%2057.01%25%20and%2067.44%25%2C%20respectively%2C%0Aoutperforming%20other%20state-of-the-art%20models.%20Moreover%2C%20the%20provided%20visual%0Aexplanations%20in%20this%20study%20are%20conducive%20to%20understanding%20the%20%22black%20box%22%20of%0Adeep%20learning-based%20models.%20The%20developed%20two-stage%20deep%20learning-based%20method%0Aintegrating%20visual%20explanations%20provides%20a%20basis%20for%20fast%20and%20accurate%0Aquantitative%20assessment%20of%20tunnel%20health%20status.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Classification%2520and%2520Segmentation%2520of%2520Tunnel%2520Cracks%2520Based%2520on%2520Deep%250A%2520%2520Learning%2520and%2520Visual%2520Explanations%26entry.906535625%3DYong%2520Feng%2520and%2520Xiaolei%2520Zhang%2520and%2520Shijin%2520Feng%2520and%2520Yong%2520Zhao%2520and%2520Yihan%2520Chen%26entry.1292438233%3D%2520%2520Tunnel%2520lining%2520crack%2520is%2520a%2520crucial%2520indicator%2520of%2520tunnels%2527%2520safety%2520status.%2520Aiming%250Ato%2520classify%2520and%2520segment%2520tunnel%2520cracks%2520with%2520enhanced%2520accuracy%2520and%2520efficiency%252C%250Athis%2520study%2520proposes%2520a%2520two-step%2520deep%2520learning-based%2520method.%2520An%2520automatic%2520tunnel%250Aimage%2520classification%2520model%2520is%2520developed%2520using%2520the%2520DenseNet-169%2520in%2520the%2520first%250Astep.%2520The%2520proposed%2520crack%2520segmentation%2520model%2520in%2520the%2520second%2520step%2520is%2520based%2520on%2520the%250ADeepLabV3%252B%252C%2520whose%2520internal%2520logic%2520is%2520evaluated%2520via%2520a%2520score-weighted%2520visual%250Aexplanation%2520technique.%2520Proposed%2520method%2520combines%2520tunnel%2520image%2520classification%2520and%250Asegmentation%2520together%252C%2520so%2520that%2520the%2520selected%2520images%2520containing%2520cracks%2520from%2520the%250Afirst%2520step%2520are%2520segmented%2520in%2520the%2520second%2520step%2520to%2520improve%2520the%2520detection%2520accuracy%250Aand%2520efficiency.%2520The%2520superior%2520performances%2520of%2520the%2520two-step%2520method%2520are%2520validated%250Aby%2520experiments.%2520The%2520results%2520show%2520that%2520the%2520accuracy%2520and%2520frames%2520per%2520second%2520%2528FPS%2529%250Aof%2520the%2520tunnel%2520crack%2520classification%2520model%2520are%252092.23%2525%2520and%252039.80%252C%2520respectively%252C%250Awhich%2520are%2520higher%2520than%2520other%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%2520based%2520and%250ATransformer%2520based%2520models.%2520Also%252C%2520the%2520intersection%2520over%2520union%2520%2528IoU%2529%2520and%2520F1%2520score%250Aof%2520the%2520tunnel%2520crack%2520segmentation%2520model%2520are%252057.01%2525%2520and%252067.44%2525%252C%2520respectively%252C%250Aoutperforming%2520other%2520state-of-the-art%2520models.%2520Moreover%252C%2520the%2520provided%2520visual%250Aexplanations%2520in%2520this%2520study%2520are%2520conducive%2520to%2520understanding%2520the%2520%2522black%2520box%2522%2520of%250Adeep%2520learning-based%2520models.%2520The%2520developed%2520two-stage%2520deep%2520learning-based%2520method%250Aintegrating%2520visual%2520explanations%2520provides%2520a%2520basis%2520for%2520fast%2520and%2520accurate%250Aquantitative%2520assessment%2520of%2520tunnel%2520health%2520status.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Classification%20and%20Segmentation%20of%20Tunnel%20Cracks%20Based%20on%20Deep%0A%20%20Learning%20and%20Visual%20Explanations&entry.906535625=Yong%20Feng%20and%20Xiaolei%20Zhang%20and%20Shijin%20Feng%20and%20Yong%20Zhao%20and%20Yihan%20Chen&entry.1292438233=%20%20Tunnel%20lining%20crack%20is%20a%20crucial%20indicator%20of%20tunnels%27%20safety%20status.%20Aiming%0Ato%20classify%20and%20segment%20tunnel%20cracks%20with%20enhanced%20accuracy%20and%20efficiency%2C%0Athis%20study%20proposes%20a%20two-step%20deep%20learning-based%20method.%20An%20automatic%20tunnel%0Aimage%20classification%20model%20is%20developed%20using%20the%20DenseNet-169%20in%20the%20first%0Astep.%20The%20proposed%20crack%20segmentation%20model%20in%20the%20second%20step%20is%20based%20on%20the%0ADeepLabV3%2B%2C%20whose%20internal%20logic%20is%20evaluated%20via%20a%20score-weighted%20visual%0Aexplanation%20technique.%20Proposed%20method%20combines%20tunnel%20image%20classification%20and%0Asegmentation%20together%2C%20so%20that%20the%20selected%20images%20containing%20cracks%20from%20the%0Afirst%20step%20are%20segmented%20in%20the%20second%20step%20to%20improve%20the%20detection%20accuracy%0Aand%20efficiency.%20The%20superior%20performances%20of%20the%20two-step%20method%20are%20validated%0Aby%20experiments.%20The%20results%20show%20that%20the%20accuracy%20and%20frames%20per%20second%20%28FPS%29%0Aof%20the%20tunnel%20crack%20classification%20model%20are%2092.23%25%20and%2039.80%2C%20respectively%2C%0Awhich%20are%20higher%20than%20other%20convolutional%20neural%20networks%20%28CNN%29%20based%20and%0ATransformer%20based%20models.%20Also%2C%20the%20intersection%20over%20union%20%28IoU%29%20and%20F1%20score%0Aof%20the%20tunnel%20crack%20segmentation%20model%20are%2057.01%25%20and%2067.44%25%2C%20respectively%2C%0Aoutperforming%20other%20state-of-the-art%20models.%20Moreover%2C%20the%20provided%20visual%0Aexplanations%20in%20this%20study%20are%20conducive%20to%20understanding%20the%20%22black%20box%22%20of%0Adeep%20learning-based%20models.%20The%20developed%20two-stage%20deep%20learning-based%20method%0Aintegrating%20visual%20explanations%20provides%20a%20basis%20for%20fast%20and%20accurate%0Aquantitative%20assessment%20of%20tunnel%20health%20status.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14010v1&entry.124074799=Read"},
{"title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning", "author": "Xiaoya Li and Xiaofei Sun and Albert Wang and Jiwei Li and Chris Shum", "abstract": "  The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.\n", "link": "http://arxiv.org/abs/2507.14111v1", "date": "2025-07-18", "relevancy": 1.9943, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5219}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4965}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CUDA-L1%3A%20Improving%20CUDA%20Optimization%20via%20Contrastive%20Reinforcement%0A%20%20Learning&body=Title%3A%20CUDA-L1%3A%20Improving%20CUDA%20Optimization%20via%20Contrastive%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Xiaoya%20Li%20and%20Xiaofei%20Sun%20and%20Albert%20Wang%20and%20Jiwei%20Li%20and%20Chris%20Shum%0AAbstract%3A%20%20%20The%20exponential%20growth%20in%20demand%20for%20GPU%20computing%20resources%2C%20driven%20by%20the%0Arapid%20advancement%20of%20Large%20Language%20Models%2C%20has%20created%20an%20urgent%20need%20for%0Aautomated%20CUDA%20optimization%20strategies.%20While%20recent%20advances%20in%20LLMs%20show%0Apromise%20for%20code%20generation%2C%20current%20SOTA%20models%20%28e.g.%20R1%2C%20o1%29%20achieve%20low%0Asuccess%20rates%20in%20improving%20CUDA%20speed.%20In%20this%20paper%2C%20we%20introduce%20CUDA-L1%2C%20an%0Aautomated%20reinforcement%20learning%20framework%20for%20CUDA%20optimization.%0A%20%20CUDA-L1%20achieves%20performance%20improvements%20on%20the%20CUDA%20optimization%20task%3A%0Atrained%20on%20NVIDIA%20A100%2C%20it%20delivers%20an%20average%20speedup%20of%20x17.7%20across%20all%20250%0ACUDA%20kernels%20of%20KernelBench%2C%20with%20peak%20speedups%20reaching%20x449.%20Furthermore%2C%20the%0Amodel%20also%20demonstrates%20excellent%20portability%20across%20GPU%20architectures%2C%0Aachieving%20average%20speedups%20of%20x17.8%20on%20H100%2C%20x19.0%20on%20RTX%203090%2C%20x16.5%20on%20L40%2C%0Ax14.7%20on%20H800%2C%20and%20x13.9%20on%20H20%20despite%20being%20optimized%20specifically%20for%20A100.%0ABeyond%20these%20benchmark%20results%2C%20CUDA-L1%20demonstrates%20several%20remarkable%0Aproperties%3A%201%29%20Discovers%20a%20variety%20of%20CUDA%20optimization%20techniques%20and%20learns%0Ato%20combine%20them%20strategically%20to%20achieve%20optimal%20performance%3B%202%29%20Uncovers%0Afundamental%20principles%20of%20CUDA%20optimization%3B%203%29%20Identifies%20non-obvious%0Aperformance%20bottlenecks%20and%20rejects%20seemingly%20beneficial%20optimizations%20that%0Aharm%20performance.%0A%20%20The%20capabilities%20of%20CUDA-L1%20demonstrate%20that%20reinforcement%20learning%20can%0Atransform%20an%20initially%20poor-performing%20LLM%20into%20an%20effective%20CUDA%20optimizer%0Athrough%20speedup-based%20reward%20signals%20alone%2C%20without%20human%20expertise%20or%20domain%0Aknowledge.%20More%20importantly%2C%20the%20trained%20RL%20model%20extend%20the%20acquired%20reasoning%0Aabilities%20to%20new%20kernels.%20This%20paradigm%20opens%20possibilities%20for%20automated%0Aoptimization%20of%20CUDA%20operations%2C%20and%20holds%20promise%20to%20substantially%20promote%20GPU%0Aefficiency%20and%20alleviate%20the%20rising%20pressure%20on%20GPU%20computing%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCUDA-L1%253A%2520Improving%2520CUDA%2520Optimization%2520via%2520Contrastive%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DXiaoya%2520Li%2520and%2520Xiaofei%2520Sun%2520and%2520Albert%2520Wang%2520and%2520Jiwei%2520Li%2520and%2520Chris%2520Shum%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520in%2520demand%2520for%2520GPU%2520computing%2520resources%252C%2520driven%2520by%2520the%250Arapid%2520advancement%2520of%2520Large%2520Language%2520Models%252C%2520has%2520created%2520an%2520urgent%2520need%2520for%250Aautomated%2520CUDA%2520optimization%2520strategies.%2520While%2520recent%2520advances%2520in%2520LLMs%2520show%250Apromise%2520for%2520code%2520generation%252C%2520current%2520SOTA%2520models%2520%2528e.g.%2520R1%252C%2520o1%2529%2520achieve%2520low%250Asuccess%2520rates%2520in%2520improving%2520CUDA%2520speed.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CUDA-L1%252C%2520an%250Aautomated%2520reinforcement%2520learning%2520framework%2520for%2520CUDA%2520optimization.%250A%2520%2520CUDA-L1%2520achieves%2520performance%2520improvements%2520on%2520the%2520CUDA%2520optimization%2520task%253A%250Atrained%2520on%2520NVIDIA%2520A100%252C%2520it%2520delivers%2520an%2520average%2520speedup%2520of%2520x17.7%2520across%2520all%2520250%250ACUDA%2520kernels%2520of%2520KernelBench%252C%2520with%2520peak%2520speedups%2520reaching%2520x449.%2520Furthermore%252C%2520the%250Amodel%2520also%2520demonstrates%2520excellent%2520portability%2520across%2520GPU%2520architectures%252C%250Aachieving%2520average%2520speedups%2520of%2520x17.8%2520on%2520H100%252C%2520x19.0%2520on%2520RTX%25203090%252C%2520x16.5%2520on%2520L40%252C%250Ax14.7%2520on%2520H800%252C%2520and%2520x13.9%2520on%2520H20%2520despite%2520being%2520optimized%2520specifically%2520for%2520A100.%250ABeyond%2520these%2520benchmark%2520results%252C%2520CUDA-L1%2520demonstrates%2520several%2520remarkable%250Aproperties%253A%25201%2529%2520Discovers%2520a%2520variety%2520of%2520CUDA%2520optimization%2520techniques%2520and%2520learns%250Ato%2520combine%2520them%2520strategically%2520to%2520achieve%2520optimal%2520performance%253B%25202%2529%2520Uncovers%250Afundamental%2520principles%2520of%2520CUDA%2520optimization%253B%25203%2529%2520Identifies%2520non-obvious%250Aperformance%2520bottlenecks%2520and%2520rejects%2520seemingly%2520beneficial%2520optimizations%2520that%250Aharm%2520performance.%250A%2520%2520The%2520capabilities%2520of%2520CUDA-L1%2520demonstrate%2520that%2520reinforcement%2520learning%2520can%250Atransform%2520an%2520initially%2520poor-performing%2520LLM%2520into%2520an%2520effective%2520CUDA%2520optimizer%250Athrough%2520speedup-based%2520reward%2520signals%2520alone%252C%2520without%2520human%2520expertise%2520or%2520domain%250Aknowledge.%2520More%2520importantly%252C%2520the%2520trained%2520RL%2520model%2520extend%2520the%2520acquired%2520reasoning%250Aabilities%2520to%2520new%2520kernels.%2520This%2520paradigm%2520opens%2520possibilities%2520for%2520automated%250Aoptimization%2520of%2520CUDA%2520operations%252C%2520and%2520holds%2520promise%2520to%2520substantially%2520promote%2520GPU%250Aefficiency%2520and%2520alleviate%2520the%2520rising%2520pressure%2520on%2520GPU%2520computing%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CUDA-L1%3A%20Improving%20CUDA%20Optimization%20via%20Contrastive%20Reinforcement%0A%20%20Learning&entry.906535625=Xiaoya%20Li%20and%20Xiaofei%20Sun%20and%20Albert%20Wang%20and%20Jiwei%20Li%20and%20Chris%20Shum&entry.1292438233=%20%20The%20exponential%20growth%20in%20demand%20for%20GPU%20computing%20resources%2C%20driven%20by%20the%0Arapid%20advancement%20of%20Large%20Language%20Models%2C%20has%20created%20an%20urgent%20need%20for%0Aautomated%20CUDA%20optimization%20strategies.%20While%20recent%20advances%20in%20LLMs%20show%0Apromise%20for%20code%20generation%2C%20current%20SOTA%20models%20%28e.g.%20R1%2C%20o1%29%20achieve%20low%0Asuccess%20rates%20in%20improving%20CUDA%20speed.%20In%20this%20paper%2C%20we%20introduce%20CUDA-L1%2C%20an%0Aautomated%20reinforcement%20learning%20framework%20for%20CUDA%20optimization.%0A%20%20CUDA-L1%20achieves%20performance%20improvements%20on%20the%20CUDA%20optimization%20task%3A%0Atrained%20on%20NVIDIA%20A100%2C%20it%20delivers%20an%20average%20speedup%20of%20x17.7%20across%20all%20250%0ACUDA%20kernels%20of%20KernelBench%2C%20with%20peak%20speedups%20reaching%20x449.%20Furthermore%2C%20the%0Amodel%20also%20demonstrates%20excellent%20portability%20across%20GPU%20architectures%2C%0Aachieving%20average%20speedups%20of%20x17.8%20on%20H100%2C%20x19.0%20on%20RTX%203090%2C%20x16.5%20on%20L40%2C%0Ax14.7%20on%20H800%2C%20and%20x13.9%20on%20H20%20despite%20being%20optimized%20specifically%20for%20A100.%0ABeyond%20these%20benchmark%20results%2C%20CUDA-L1%20demonstrates%20several%20remarkable%0Aproperties%3A%201%29%20Discovers%20a%20variety%20of%20CUDA%20optimization%20techniques%20and%20learns%0Ato%20combine%20them%20strategically%20to%20achieve%20optimal%20performance%3B%202%29%20Uncovers%0Afundamental%20principles%20of%20CUDA%20optimization%3B%203%29%20Identifies%20non-obvious%0Aperformance%20bottlenecks%20and%20rejects%20seemingly%20beneficial%20optimizations%20that%0Aharm%20performance.%0A%20%20The%20capabilities%20of%20CUDA-L1%20demonstrate%20that%20reinforcement%20learning%20can%0Atransform%20an%20initially%20poor-performing%20LLM%20into%20an%20effective%20CUDA%20optimizer%0Athrough%20speedup-based%20reward%20signals%20alone%2C%20without%20human%20expertise%20or%20domain%0Aknowledge.%20More%20importantly%2C%20the%20trained%20RL%20model%20extend%20the%20acquired%20reasoning%0Aabilities%20to%20new%20kernels.%20This%20paradigm%20opens%20possibilities%20for%20automated%0Aoptimization%20of%20CUDA%20operations%2C%20and%20holds%20promise%20to%20substantially%20promote%20GPU%0Aefficiency%20and%20alleviate%20the%20rising%20pressure%20on%20GPU%20computing%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14111v1&entry.124074799=Read"},
{"title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph\n  is What We Need", "author": "Bhishma Dedhia and Yuval Kansal and Niraj K. Jha", "abstract": "  Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.\n", "link": "http://arxiv.org/abs/2507.13966v1", "date": "2025-07-18", "relevancy": 1.9906, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5101}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bottom-up%20Domain-specific%20Superintelligence%3A%20A%20Reliable%20Knowledge%20Graph%0A%20%20is%20What%20We%20Need&body=Title%3A%20Bottom-up%20Domain-specific%20Superintelligence%3A%20A%20Reliable%20Knowledge%20Graph%0A%20%20is%20What%20We%20Need%0AAuthor%3A%20Bhishma%20Dedhia%20and%20Yuval%20Kansal%20and%20Niraj%20K.%20Jha%0AAbstract%3A%20%20%20Language%20models%20traditionally%20used%20for%20cross-domain%20generalization%20have%0Arecently%20demonstrated%20task-specific%20reasoning.%20However%2C%20their%20top-down%20training%0Aapproach%20on%20general%20corpora%20is%20insufficient%20for%20acquiring%20abstractions%20needed%0Afor%20deep%20domain%20expertise.%20This%20may%20require%20a%20bottom-up%20approach%20that%20acquires%0Aexpertise%20by%20learning%20to%20compose%20simple%20domain%20concepts%20into%20more%20complex%20ones.%0AA%20knowledge%20graph%20%28KG%29%20provides%20this%20compositional%20structure%2C%20where%20domain%0Aprimitives%20are%20represented%20as%20head-relation-tail%20edges%20and%20their%20paths%20encode%0Ahigher-level%20concepts.%20We%20present%20a%20task%20generation%20pipeline%20that%20synthesizes%0Atasks%20directly%20from%20KG%20primitives%2C%20enabling%20models%20to%20acquire%20and%20compose%20them%0Afor%20reasoning.%20We%20fine-tune%20language%20models%20on%20the%20resultant%20KG-grounded%0Acurriculum%20to%20demonstrate%20domain-specific%20superintelligence.%20While%20broadly%0Aapplicable%2C%20we%20validate%20our%20approach%20in%20medicine%2C%20where%20reliable%20KGs%20exist.%0AUsing%20a%20medical%20KG%2C%20we%20curate%2024%2C000%20reasoning%20tasks%20paired%20with%20thinking%0Atraces%20derived%20from%20diverse%20medical%20primitives.%20We%20fine-tune%20the%20QwQ-32B%20model%0Aon%20this%20curriculum%20to%20obtain%20QwQ-Med-3%20that%20takes%20a%20step%20towards%20medical%0Asuperintelligence.%20We%20also%20introduce%20ICD-Bench%2C%20an%20evaluation%20suite%20to%20quantify%0Areasoning%20abilities%20across%2015%20medical%20domains.%20Our%20experiments%20demonstrate%20that%0AQwQ-Med-3%20significantly%20outperforms%20state-of-the-art%20reasoning%20models%20on%0AICD-Bench%20categories.%20Further%20analysis%20reveals%20that%20QwQ-Med-3%20utilizes%20acquired%0Aprimitives%20to%20widen%20the%20performance%20gap%20on%20the%20hardest%20tasks%20of%20ICD-Bench.%0AFinally%2C%20evaluation%20on%20medical%20question-answer%20benchmarks%20shows%20that%20QwQ-Med-3%0Atransfers%20acquired%20expertise%20to%20enhance%20the%20base%20model%27s%20performance.%20While%20the%0Aindustry%27s%20approach%20to%20artificial%20general%20intelligence%20%28AGI%29%20emphasizes%20broad%0Aexpertise%2C%20we%20envision%20a%20future%20in%20which%20AGI%20emerges%20from%20the%20composable%0Ainteraction%20of%20efficient%20domain-specific%20superintelligent%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBottom-up%2520Domain-specific%2520Superintelligence%253A%2520A%2520Reliable%2520Knowledge%2520Graph%250A%2520%2520is%2520What%2520We%2520Need%26entry.906535625%3DBhishma%2520Dedhia%2520and%2520Yuval%2520Kansal%2520and%2520Niraj%2520K.%2520Jha%26entry.1292438233%3D%2520%2520Language%2520models%2520traditionally%2520used%2520for%2520cross-domain%2520generalization%2520have%250Arecently%2520demonstrated%2520task-specific%2520reasoning.%2520However%252C%2520their%2520top-down%2520training%250Aapproach%2520on%2520general%2520corpora%2520is%2520insufficient%2520for%2520acquiring%2520abstractions%2520needed%250Afor%2520deep%2520domain%2520expertise.%2520This%2520may%2520require%2520a%2520bottom-up%2520approach%2520that%2520acquires%250Aexpertise%2520by%2520learning%2520to%2520compose%2520simple%2520domain%2520concepts%2520into%2520more%2520complex%2520ones.%250AA%2520knowledge%2520graph%2520%2528KG%2529%2520provides%2520this%2520compositional%2520structure%252C%2520where%2520domain%250Aprimitives%2520are%2520represented%2520as%2520head-relation-tail%2520edges%2520and%2520their%2520paths%2520encode%250Ahigher-level%2520concepts.%2520We%2520present%2520a%2520task%2520generation%2520pipeline%2520that%2520synthesizes%250Atasks%2520directly%2520from%2520KG%2520primitives%252C%2520enabling%2520models%2520to%2520acquire%2520and%2520compose%2520them%250Afor%2520reasoning.%2520We%2520fine-tune%2520language%2520models%2520on%2520the%2520resultant%2520KG-grounded%250Acurriculum%2520to%2520demonstrate%2520domain-specific%2520superintelligence.%2520While%2520broadly%250Aapplicable%252C%2520we%2520validate%2520our%2520approach%2520in%2520medicine%252C%2520where%2520reliable%2520KGs%2520exist.%250AUsing%2520a%2520medical%2520KG%252C%2520we%2520curate%252024%252C000%2520reasoning%2520tasks%2520paired%2520with%2520thinking%250Atraces%2520derived%2520from%2520diverse%2520medical%2520primitives.%2520We%2520fine-tune%2520the%2520QwQ-32B%2520model%250Aon%2520this%2520curriculum%2520to%2520obtain%2520QwQ-Med-3%2520that%2520takes%2520a%2520step%2520towards%2520medical%250Asuperintelligence.%2520We%2520also%2520introduce%2520ICD-Bench%252C%2520an%2520evaluation%2520suite%2520to%2520quantify%250Areasoning%2520abilities%2520across%252015%2520medical%2520domains.%2520Our%2520experiments%2520demonstrate%2520that%250AQwQ-Med-3%2520significantly%2520outperforms%2520state-of-the-art%2520reasoning%2520models%2520on%250AICD-Bench%2520categories.%2520Further%2520analysis%2520reveals%2520that%2520QwQ-Med-3%2520utilizes%2520acquired%250Aprimitives%2520to%2520widen%2520the%2520performance%2520gap%2520on%2520the%2520hardest%2520tasks%2520of%2520ICD-Bench.%250AFinally%252C%2520evaluation%2520on%2520medical%2520question-answer%2520benchmarks%2520shows%2520that%2520QwQ-Med-3%250Atransfers%2520acquired%2520expertise%2520to%2520enhance%2520the%2520base%2520model%2527s%2520performance.%2520While%2520the%250Aindustry%2527s%2520approach%2520to%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%2520emphasizes%2520broad%250Aexpertise%252C%2520we%2520envision%2520a%2520future%2520in%2520which%2520AGI%2520emerges%2520from%2520the%2520composable%250Ainteraction%2520of%2520efficient%2520domain-specific%2520superintelligent%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bottom-up%20Domain-specific%20Superintelligence%3A%20A%20Reliable%20Knowledge%20Graph%0A%20%20is%20What%20We%20Need&entry.906535625=Bhishma%20Dedhia%20and%20Yuval%20Kansal%20and%20Niraj%20K.%20Jha&entry.1292438233=%20%20Language%20models%20traditionally%20used%20for%20cross-domain%20generalization%20have%0Arecently%20demonstrated%20task-specific%20reasoning.%20However%2C%20their%20top-down%20training%0Aapproach%20on%20general%20corpora%20is%20insufficient%20for%20acquiring%20abstractions%20needed%0Afor%20deep%20domain%20expertise.%20This%20may%20require%20a%20bottom-up%20approach%20that%20acquires%0Aexpertise%20by%20learning%20to%20compose%20simple%20domain%20concepts%20into%20more%20complex%20ones.%0AA%20knowledge%20graph%20%28KG%29%20provides%20this%20compositional%20structure%2C%20where%20domain%0Aprimitives%20are%20represented%20as%20head-relation-tail%20edges%20and%20their%20paths%20encode%0Ahigher-level%20concepts.%20We%20present%20a%20task%20generation%20pipeline%20that%20synthesizes%0Atasks%20directly%20from%20KG%20primitives%2C%20enabling%20models%20to%20acquire%20and%20compose%20them%0Afor%20reasoning.%20We%20fine-tune%20language%20models%20on%20the%20resultant%20KG-grounded%0Acurriculum%20to%20demonstrate%20domain-specific%20superintelligence.%20While%20broadly%0Aapplicable%2C%20we%20validate%20our%20approach%20in%20medicine%2C%20where%20reliable%20KGs%20exist.%0AUsing%20a%20medical%20KG%2C%20we%20curate%2024%2C000%20reasoning%20tasks%20paired%20with%20thinking%0Atraces%20derived%20from%20diverse%20medical%20primitives.%20We%20fine-tune%20the%20QwQ-32B%20model%0Aon%20this%20curriculum%20to%20obtain%20QwQ-Med-3%20that%20takes%20a%20step%20towards%20medical%0Asuperintelligence.%20We%20also%20introduce%20ICD-Bench%2C%20an%20evaluation%20suite%20to%20quantify%0Areasoning%20abilities%20across%2015%20medical%20domains.%20Our%20experiments%20demonstrate%20that%0AQwQ-Med-3%20significantly%20outperforms%20state-of-the-art%20reasoning%20models%20on%0AICD-Bench%20categories.%20Further%20analysis%20reveals%20that%20QwQ-Med-3%20utilizes%20acquired%0Aprimitives%20to%20widen%20the%20performance%20gap%20on%20the%20hardest%20tasks%20of%20ICD-Bench.%0AFinally%2C%20evaluation%20on%20medical%20question-answer%20benchmarks%20shows%20that%20QwQ-Med-3%0Atransfers%20acquired%20expertise%20to%20enhance%20the%20base%20model%27s%20performance.%20While%20the%0Aindustry%27s%20approach%20to%20artificial%20general%20intelligence%20%28AGI%29%20emphasizes%20broad%0Aexpertise%2C%20we%20envision%20a%20future%20in%20which%20AGI%20emerges%20from%20the%20composable%0Ainteraction%20of%20efficient%20domain-specific%20superintelligent%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13966v1&entry.124074799=Read"},
{"title": "Explainable AI in Genomics: Transcription Factor Binding Site Prediction\n  with Mixture of Experts", "author": "Aakash Tripathi and Ian E. Nielsen and Muhammad Umer and Ravi P. Ramachandran and Ghulam Rasool", "abstract": "  Transcription Factor Binding Site (TFBS) prediction is crucial for\nunderstanding gene regulation and various biological processes. This study\nintroduces a novel Mixture of Experts (MoE) approach for TFBS prediction,\nintegrating multiple pre-trained Convolutional Neural Network (CNN) models,\neach specializing in different TFBS patterns. We evaluate the performance of\nour MoE model against individual expert models on both in-distribution and\nout-of-distribution (OOD) datasets, using six randomly selected transcription\nfactors (TFs) for OOD testing. Our results demonstrate that the MoE model\nachieves competitive or superior performance across diverse TF binding sites,\nparticularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)\nstatistical test confirms the significance of these performance differences.\nAdditionally, we introduce ShiftSmooth, a novel attribution mapping technique\nthat provides more robust model interpretability by considering small shifts in\ninput sequences. Through comprehensive explainability analysis, we show that\nShiftSmooth offers superior attribution for motif discovery and localization\ncompared to traditional Vanilla Gradient methods. Our work presents an\nefficient, generalizable, and interpretable solution for TFBS prediction,\npotentially enabling new discoveries in genome biology and advancing our\nunderstanding of transcriptional regulation.\n", "link": "http://arxiv.org/abs/2507.09754v2", "date": "2025-07-18", "relevancy": 1.9856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4962}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20in%20Genomics%3A%20Transcription%20Factor%20Binding%20Site%20Prediction%0A%20%20with%20Mixture%20of%20Experts&body=Title%3A%20Explainable%20AI%20in%20Genomics%3A%20Transcription%20Factor%20Binding%20Site%20Prediction%0A%20%20with%20Mixture%20of%20Experts%0AAuthor%3A%20Aakash%20Tripathi%20and%20Ian%20E.%20Nielsen%20and%20Muhammad%20Umer%20and%20Ravi%20P.%20Ramachandran%20and%20Ghulam%20Rasool%0AAbstract%3A%20%20%20Transcription%20Factor%20Binding%20Site%20%28TFBS%29%20prediction%20is%20crucial%20for%0Aunderstanding%20gene%20regulation%20and%20various%20biological%20processes.%20This%20study%0Aintroduces%20a%20novel%20Mixture%20of%20Experts%20%28MoE%29%20approach%20for%20TFBS%20prediction%2C%0Aintegrating%20multiple%20pre-trained%20Convolutional%20Neural%20Network%20%28CNN%29%20models%2C%0Aeach%20specializing%20in%20different%20TFBS%20patterns.%20We%20evaluate%20the%20performance%20of%0Aour%20MoE%20model%20against%20individual%20expert%20models%20on%20both%20in-distribution%20and%0Aout-of-distribution%20%28OOD%29%20datasets%2C%20using%20six%20randomly%20selected%20transcription%0Afactors%20%28TFs%29%20for%20OOD%20testing.%20Our%20results%20demonstrate%20that%20the%20MoE%20model%0Aachieves%20competitive%20or%20superior%20performance%20across%20diverse%20TF%20binding%20sites%2C%0Aparticularly%20excelling%20in%20OOD%20scenarios.%20The%20Analysis%20of%20Variance%20%28ANOVA%29%0Astatistical%20test%20confirms%20the%20significance%20of%20these%20performance%20differences.%0AAdditionally%2C%20we%20introduce%20ShiftSmooth%2C%20a%20novel%20attribution%20mapping%20technique%0Athat%20provides%20more%20robust%20model%20interpretability%20by%20considering%20small%20shifts%20in%0Ainput%20sequences.%20Through%20comprehensive%20explainability%20analysis%2C%20we%20show%20that%0AShiftSmooth%20offers%20superior%20attribution%20for%20motif%20discovery%20and%20localization%0Acompared%20to%20traditional%20Vanilla%20Gradient%20methods.%20Our%20work%20presents%20an%0Aefficient%2C%20generalizable%2C%20and%20interpretable%20solution%20for%20TFBS%20prediction%2C%0Apotentially%20enabling%20new%20discoveries%20in%20genome%20biology%20and%20advancing%20our%0Aunderstanding%20of%20transcriptional%20regulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520in%2520Genomics%253A%2520Transcription%2520Factor%2520Binding%2520Site%2520Prediction%250A%2520%2520with%2520Mixture%2520of%2520Experts%26entry.906535625%3DAakash%2520Tripathi%2520and%2520Ian%2520E.%2520Nielsen%2520and%2520Muhammad%2520Umer%2520and%2520Ravi%2520P.%2520Ramachandran%2520and%2520Ghulam%2520Rasool%26entry.1292438233%3D%2520%2520Transcription%2520Factor%2520Binding%2520Site%2520%2528TFBS%2529%2520prediction%2520is%2520crucial%2520for%250Aunderstanding%2520gene%2520regulation%2520and%2520various%2520biological%2520processes.%2520This%2520study%250Aintroduces%2520a%2520novel%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520approach%2520for%2520TFBS%2520prediction%252C%250Aintegrating%2520multiple%2520pre-trained%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2520models%252C%250Aeach%2520specializing%2520in%2520different%2520TFBS%2520patterns.%2520We%2520evaluate%2520the%2520performance%2520of%250Aour%2520MoE%2520model%2520against%2520individual%2520expert%2520models%2520on%2520both%2520in-distribution%2520and%250Aout-of-distribution%2520%2528OOD%2529%2520datasets%252C%2520using%2520six%2520randomly%2520selected%2520transcription%250Afactors%2520%2528TFs%2529%2520for%2520OOD%2520testing.%2520Our%2520results%2520demonstrate%2520that%2520the%2520MoE%2520model%250Aachieves%2520competitive%2520or%2520superior%2520performance%2520across%2520diverse%2520TF%2520binding%2520sites%252C%250Aparticularly%2520excelling%2520in%2520OOD%2520scenarios.%2520The%2520Analysis%2520of%2520Variance%2520%2528ANOVA%2529%250Astatistical%2520test%2520confirms%2520the%2520significance%2520of%2520these%2520performance%2520differences.%250AAdditionally%252C%2520we%2520introduce%2520ShiftSmooth%252C%2520a%2520novel%2520attribution%2520mapping%2520technique%250Athat%2520provides%2520more%2520robust%2520model%2520interpretability%2520by%2520considering%2520small%2520shifts%2520in%250Ainput%2520sequences.%2520Through%2520comprehensive%2520explainability%2520analysis%252C%2520we%2520show%2520that%250AShiftSmooth%2520offers%2520superior%2520attribution%2520for%2520motif%2520discovery%2520and%2520localization%250Acompared%2520to%2520traditional%2520Vanilla%2520Gradient%2520methods.%2520Our%2520work%2520presents%2520an%250Aefficient%252C%2520generalizable%252C%2520and%2520interpretable%2520solution%2520for%2520TFBS%2520prediction%252C%250Apotentially%2520enabling%2520new%2520discoveries%2520in%2520genome%2520biology%2520and%2520advancing%2520our%250Aunderstanding%2520of%2520transcriptional%2520regulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20in%20Genomics%3A%20Transcription%20Factor%20Binding%20Site%20Prediction%0A%20%20with%20Mixture%20of%20Experts&entry.906535625=Aakash%20Tripathi%20and%20Ian%20E.%20Nielsen%20and%20Muhammad%20Umer%20and%20Ravi%20P.%20Ramachandran%20and%20Ghulam%20Rasool&entry.1292438233=%20%20Transcription%20Factor%20Binding%20Site%20%28TFBS%29%20prediction%20is%20crucial%20for%0Aunderstanding%20gene%20regulation%20and%20various%20biological%20processes.%20This%20study%0Aintroduces%20a%20novel%20Mixture%20of%20Experts%20%28MoE%29%20approach%20for%20TFBS%20prediction%2C%0Aintegrating%20multiple%20pre-trained%20Convolutional%20Neural%20Network%20%28CNN%29%20models%2C%0Aeach%20specializing%20in%20different%20TFBS%20patterns.%20We%20evaluate%20the%20performance%20of%0Aour%20MoE%20model%20against%20individual%20expert%20models%20on%20both%20in-distribution%20and%0Aout-of-distribution%20%28OOD%29%20datasets%2C%20using%20six%20randomly%20selected%20transcription%0Afactors%20%28TFs%29%20for%20OOD%20testing.%20Our%20results%20demonstrate%20that%20the%20MoE%20model%0Aachieves%20competitive%20or%20superior%20performance%20across%20diverse%20TF%20binding%20sites%2C%0Aparticularly%20excelling%20in%20OOD%20scenarios.%20The%20Analysis%20of%20Variance%20%28ANOVA%29%0Astatistical%20test%20confirms%20the%20significance%20of%20these%20performance%20differences.%0AAdditionally%2C%20we%20introduce%20ShiftSmooth%2C%20a%20novel%20attribution%20mapping%20technique%0Athat%20provides%20more%20robust%20model%20interpretability%20by%20considering%20small%20shifts%20in%0Ainput%20sequences.%20Through%20comprehensive%20explainability%20analysis%2C%20we%20show%20that%0AShiftSmooth%20offers%20superior%20attribution%20for%20motif%20discovery%20and%20localization%0Acompared%20to%20traditional%20Vanilla%20Gradient%20methods.%20Our%20work%20presents%20an%0Aefficient%2C%20generalizable%2C%20and%20interpretable%20solution%20for%20TFBS%20prediction%2C%0Apotentially%20enabling%20new%20discoveries%20in%20genome%20biology%20and%20advancing%20our%0Aunderstanding%20of%20transcriptional%20regulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09754v2&entry.124074799=Read"},
{"title": "On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A\n  Bayesian Neural Network Approach", "author": "Tim Rensmeyer and Denis Kramer and Oliver Niggemann", "abstract": "  Due to the computational complexity of evaluating interatomic forces from\nfirst principles, the creation of interatomic machine learning force fields has\nbecome a highly active field of research. However, the generation of training\ndatasets of sufficient size and sample diversity itself comes with a\ncomputational burden that can make this approach impractical for modeling rare\nevents or systems with a large configuration space. Fine-tuning foundation\nmodels that have been pre-trained on large-scale material or molecular\ndatabases offers a promising opportunity to reduce the amount of training data\nnecessary to reach a desired level of accuracy. However, even if this approach\nrequires less training data overall, creating a suitable training dataset can\nstill be a very challenging problem, especially for systems with rare events\nand for end-users who don't have an extensive background in machine learning.\nIn on-the-fly learning, the creation of a training dataset can be largely\nautomated by using model uncertainty during the simulation to decide if the\nmodel is accurate enough or if a structure should be recalculated with\nclassical methods and used to update the model. A key challenge for applying\nthis form of active learning to the fine-tuning of foundation models is how to\nassess the uncertainty of those models during the fine-tuning process, even\nthough most foundation models lack any form of uncertainty quantification. In\nthis paper, we overcome this challenge by introducing a fine-tuning approach\nbased on Bayesian neural network methods and a subsequent on-the-fly workflow\nthat automatically fine-tunes the model while maintaining a pre-specified\naccuracy and can detect rare events such as transition states and sample them\nat an increased rate relative to their occurrence.\n", "link": "http://arxiv.org/abs/2507.13805v1", "date": "2025-07-18", "relevancy": 1.9786, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5129}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5123}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-the-Fly%20Fine-Tuning%20of%20Foundational%20Neural%20Network%20Potentials%3A%20A%0A%20%20Bayesian%20Neural%20Network%20Approach&body=Title%3A%20On-the-Fly%20Fine-Tuning%20of%20Foundational%20Neural%20Network%20Potentials%3A%20A%0A%20%20Bayesian%20Neural%20Network%20Approach%0AAuthor%3A%20Tim%20Rensmeyer%20and%20Denis%20Kramer%20and%20Oliver%20Niggemann%0AAbstract%3A%20%20%20Due%20to%20the%20computational%20complexity%20of%20evaluating%20interatomic%20forces%20from%0Afirst%20principles%2C%20the%20creation%20of%20interatomic%20machine%20learning%20force%20fields%20has%0Abecome%20a%20highly%20active%20field%20of%20research.%20However%2C%20the%20generation%20of%20training%0Adatasets%20of%20sufficient%20size%20and%20sample%20diversity%20itself%20comes%20with%20a%0Acomputational%20burden%20that%20can%20make%20this%20approach%20impractical%20for%20modeling%20rare%0Aevents%20or%20systems%20with%20a%20large%20configuration%20space.%20Fine-tuning%20foundation%0Amodels%20that%20have%20been%20pre-trained%20on%20large-scale%20material%20or%20molecular%0Adatabases%20offers%20a%20promising%20opportunity%20to%20reduce%20the%20amount%20of%20training%20data%0Anecessary%20to%20reach%20a%20desired%20level%20of%20accuracy.%20However%2C%20even%20if%20this%20approach%0Arequires%20less%20training%20data%20overall%2C%20creating%20a%20suitable%20training%20dataset%20can%0Astill%20be%20a%20very%20challenging%20problem%2C%20especially%20for%20systems%20with%20rare%20events%0Aand%20for%20end-users%20who%20don%27t%20have%20an%20extensive%20background%20in%20machine%20learning.%0AIn%20on-the-fly%20learning%2C%20the%20creation%20of%20a%20training%20dataset%20can%20be%20largely%0Aautomated%20by%20using%20model%20uncertainty%20during%20the%20simulation%20to%20decide%20if%20the%0Amodel%20is%20accurate%20enough%20or%20if%20a%20structure%20should%20be%20recalculated%20with%0Aclassical%20methods%20and%20used%20to%20update%20the%20model.%20A%20key%20challenge%20for%20applying%0Athis%20form%20of%20active%20learning%20to%20the%20fine-tuning%20of%20foundation%20models%20is%20how%20to%0Aassess%20the%20uncertainty%20of%20those%20models%20during%20the%20fine-tuning%20process%2C%20even%0Athough%20most%20foundation%20models%20lack%20any%20form%20of%20uncertainty%20quantification.%20In%0Athis%20paper%2C%20we%20overcome%20this%20challenge%20by%20introducing%20a%20fine-tuning%20approach%0Abased%20on%20Bayesian%20neural%20network%20methods%20and%20a%20subsequent%20on-the-fly%20workflow%0Athat%20automatically%20fine-tunes%20the%20model%20while%20maintaining%20a%20pre-specified%0Aaccuracy%20and%20can%20detect%20rare%20events%20such%20as%20transition%20states%20and%20sample%20them%0Aat%20an%20increased%20rate%20relative%20to%20their%20occurrence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-the-Fly%2520Fine-Tuning%2520of%2520Foundational%2520Neural%2520Network%2520Potentials%253A%2520A%250A%2520%2520Bayesian%2520Neural%2520Network%2520Approach%26entry.906535625%3DTim%2520Rensmeyer%2520and%2520Denis%2520Kramer%2520and%2520Oliver%2520Niggemann%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520computational%2520complexity%2520of%2520evaluating%2520interatomic%2520forces%2520from%250Afirst%2520principles%252C%2520the%2520creation%2520of%2520interatomic%2520machine%2520learning%2520force%2520fields%2520has%250Abecome%2520a%2520highly%2520active%2520field%2520of%2520research.%2520However%252C%2520the%2520generation%2520of%2520training%250Adatasets%2520of%2520sufficient%2520size%2520and%2520sample%2520diversity%2520itself%2520comes%2520with%2520a%250Acomputational%2520burden%2520that%2520can%2520make%2520this%2520approach%2520impractical%2520for%2520modeling%2520rare%250Aevents%2520or%2520systems%2520with%2520a%2520large%2520configuration%2520space.%2520Fine-tuning%2520foundation%250Amodels%2520that%2520have%2520been%2520pre-trained%2520on%2520large-scale%2520material%2520or%2520molecular%250Adatabases%2520offers%2520a%2520promising%2520opportunity%2520to%2520reduce%2520the%2520amount%2520of%2520training%2520data%250Anecessary%2520to%2520reach%2520a%2520desired%2520level%2520of%2520accuracy.%2520However%252C%2520even%2520if%2520this%2520approach%250Arequires%2520less%2520training%2520data%2520overall%252C%2520creating%2520a%2520suitable%2520training%2520dataset%2520can%250Astill%2520be%2520a%2520very%2520challenging%2520problem%252C%2520especially%2520for%2520systems%2520with%2520rare%2520events%250Aand%2520for%2520end-users%2520who%2520don%2527t%2520have%2520an%2520extensive%2520background%2520in%2520machine%2520learning.%250AIn%2520on-the-fly%2520learning%252C%2520the%2520creation%2520of%2520a%2520training%2520dataset%2520can%2520be%2520largely%250Aautomated%2520by%2520using%2520model%2520uncertainty%2520during%2520the%2520simulation%2520to%2520decide%2520if%2520the%250Amodel%2520is%2520accurate%2520enough%2520or%2520if%2520a%2520structure%2520should%2520be%2520recalculated%2520with%250Aclassical%2520methods%2520and%2520used%2520to%2520update%2520the%2520model.%2520A%2520key%2520challenge%2520for%2520applying%250Athis%2520form%2520of%2520active%2520learning%2520to%2520the%2520fine-tuning%2520of%2520foundation%2520models%2520is%2520how%2520to%250Aassess%2520the%2520uncertainty%2520of%2520those%2520models%2520during%2520the%2520fine-tuning%2520process%252C%2520even%250Athough%2520most%2520foundation%2520models%2520lack%2520any%2520form%2520of%2520uncertainty%2520quantification.%2520In%250Athis%2520paper%252C%2520we%2520overcome%2520this%2520challenge%2520by%2520introducing%2520a%2520fine-tuning%2520approach%250Abased%2520on%2520Bayesian%2520neural%2520network%2520methods%2520and%2520a%2520subsequent%2520on-the-fly%2520workflow%250Athat%2520automatically%2520fine-tunes%2520the%2520model%2520while%2520maintaining%2520a%2520pre-specified%250Aaccuracy%2520and%2520can%2520detect%2520rare%2520events%2520such%2520as%2520transition%2520states%2520and%2520sample%2520them%250Aat%2520an%2520increased%2520rate%2520relative%2520to%2520their%2520occurrence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-the-Fly%20Fine-Tuning%20of%20Foundational%20Neural%20Network%20Potentials%3A%20A%0A%20%20Bayesian%20Neural%20Network%20Approach&entry.906535625=Tim%20Rensmeyer%20and%20Denis%20Kramer%20and%20Oliver%20Niggemann&entry.1292438233=%20%20Due%20to%20the%20computational%20complexity%20of%20evaluating%20interatomic%20forces%20from%0Afirst%20principles%2C%20the%20creation%20of%20interatomic%20machine%20learning%20force%20fields%20has%0Abecome%20a%20highly%20active%20field%20of%20research.%20However%2C%20the%20generation%20of%20training%0Adatasets%20of%20sufficient%20size%20and%20sample%20diversity%20itself%20comes%20with%20a%0Acomputational%20burden%20that%20can%20make%20this%20approach%20impractical%20for%20modeling%20rare%0Aevents%20or%20systems%20with%20a%20large%20configuration%20space.%20Fine-tuning%20foundation%0Amodels%20that%20have%20been%20pre-trained%20on%20large-scale%20material%20or%20molecular%0Adatabases%20offers%20a%20promising%20opportunity%20to%20reduce%20the%20amount%20of%20training%20data%0Anecessary%20to%20reach%20a%20desired%20level%20of%20accuracy.%20However%2C%20even%20if%20this%20approach%0Arequires%20less%20training%20data%20overall%2C%20creating%20a%20suitable%20training%20dataset%20can%0Astill%20be%20a%20very%20challenging%20problem%2C%20especially%20for%20systems%20with%20rare%20events%0Aand%20for%20end-users%20who%20don%27t%20have%20an%20extensive%20background%20in%20machine%20learning.%0AIn%20on-the-fly%20learning%2C%20the%20creation%20of%20a%20training%20dataset%20can%20be%20largely%0Aautomated%20by%20using%20model%20uncertainty%20during%20the%20simulation%20to%20decide%20if%20the%0Amodel%20is%20accurate%20enough%20or%20if%20a%20structure%20should%20be%20recalculated%20with%0Aclassical%20methods%20and%20used%20to%20update%20the%20model.%20A%20key%20challenge%20for%20applying%0Athis%20form%20of%20active%20learning%20to%20the%20fine-tuning%20of%20foundation%20models%20is%20how%20to%0Aassess%20the%20uncertainty%20of%20those%20models%20during%20the%20fine-tuning%20process%2C%20even%0Athough%20most%20foundation%20models%20lack%20any%20form%20of%20uncertainty%20quantification.%20In%0Athis%20paper%2C%20we%20overcome%20this%20challenge%20by%20introducing%20a%20fine-tuning%20approach%0Abased%20on%20Bayesian%20neural%20network%20methods%20and%20a%20subsequent%20on-the-fly%20workflow%0Athat%20automatically%20fine-tunes%20the%20model%20while%20maintaining%20a%20pre-specified%0Aaccuracy%20and%20can%20detect%20rare%20events%20such%20as%20transition%20states%20and%20sample%20them%0Aat%20an%20increased%20rate%20relative%20to%20their%20occurrence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13805v1&entry.124074799=Read"},
{"title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs", "author": "Tamim Al Mahmud and Najeeb Jebreel and Josep Domingo-Ferrer and David Sanchez", "abstract": "  Large language models (LLMs) have recently revolutionized language processing\ntasks but have also brought ethical and legal issues. LLMs have a tendency to\nmemorize potentially private or copyrighted information present in the training\ndata, which might then be delivered to end users at inference time. When this\nhappens, a naive solution is to retrain the model from scratch after excluding\nthe undesired data. Although this guarantees that the target data have been\nforgotten, it is also prohibitively expensive for LLMs. Approximate unlearning\noffers a more efficient alternative, as it consists of ex post modifications of\nthe trained model itself to prevent undesirable results, but it lacks\nforgetting guarantees because it relies solely on empirical evidence. In this\nwork, we present DP2Unlearning, a novel LLM unlearning framework that offers\nformal forgetting guarantees at a significantly lower cost than retraining from\nscratch on the data to be retained. DP2Unlearning involves training LLMs on\ntextual data protected using {\\epsilon}-differential privacy (DP), which later\nenables efficient unlearning with the guarantees against disclosure associated\nwith the chosen {\\epsilon}. Our experiments demonstrate that DP2Unlearning\nachieves similar model performance post-unlearning, compared to an LLM\nretraining from scratch on retained data -- the gold standard exact unlearning\n-- but at approximately half the unlearning cost. In addition, with a\nreasonable computational cost, it outperforms approximate unlearning methods at\nboth preserving the utility of the model post-unlearning and effectively\nforgetting the targeted information.\n", "link": "http://arxiv.org/abs/2504.13774v2", "date": "2025-07-18", "relevancy": 1.9739, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4891}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP2Unlearning%3A%20An%20Efficient%20and%20Guaranteed%20Unlearning%20Framework%20for%20LLMs&body=Title%3A%20DP2Unlearning%3A%20An%20Efficient%20and%20Guaranteed%20Unlearning%20Framework%20for%20LLMs%0AAuthor%3A%20Tamim%20Al%20Mahmud%20and%20Najeeb%20Jebreel%20and%20Josep%20Domingo-Ferrer%20and%20David%20Sanchez%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20revolutionized%20language%20processing%0Atasks%20but%20have%20also%20brought%20ethical%20and%20legal%20issues.%20LLMs%20have%20a%20tendency%20to%0Amemorize%20potentially%20private%20or%20copyrighted%20information%20present%20in%20the%20training%0Adata%2C%20which%20might%20then%20be%20delivered%20to%20end%20users%20at%20inference%20time.%20When%20this%0Ahappens%2C%20a%20naive%20solution%20is%20to%20retrain%20the%20model%20from%20scratch%20after%20excluding%0Athe%20undesired%20data.%20Although%20this%20guarantees%20that%20the%20target%20data%20have%20been%0Aforgotten%2C%20it%20is%20also%20prohibitively%20expensive%20for%20LLMs.%20Approximate%20unlearning%0Aoffers%20a%20more%20efficient%20alternative%2C%20as%20it%20consists%20of%20ex%20post%20modifications%20of%0Athe%20trained%20model%20itself%20to%20prevent%20undesirable%20results%2C%20but%20it%20lacks%0Aforgetting%20guarantees%20because%20it%20relies%20solely%20on%20empirical%20evidence.%20In%20this%0Awork%2C%20we%20present%20DP2Unlearning%2C%20a%20novel%20LLM%20unlearning%20framework%20that%20offers%0Aformal%20forgetting%20guarantees%20at%20a%20significantly%20lower%20cost%20than%20retraining%20from%0Ascratch%20on%20the%20data%20to%20be%20retained.%20DP2Unlearning%20involves%20training%20LLMs%20on%0Atextual%20data%20protected%20using%20%7B%5Cepsilon%7D-differential%20privacy%20%28DP%29%2C%20which%20later%0Aenables%20efficient%20unlearning%20with%20the%20guarantees%20against%20disclosure%20associated%0Awith%20the%20chosen%20%7B%5Cepsilon%7D.%20Our%20experiments%20demonstrate%20that%20DP2Unlearning%0Aachieves%20similar%20model%20performance%20post-unlearning%2C%20compared%20to%20an%20LLM%0Aretraining%20from%20scratch%20on%20retained%20data%20--%20the%20gold%20standard%20exact%20unlearning%0A--%20but%20at%20approximately%20half%20the%20unlearning%20cost.%20In%20addition%2C%20with%20a%0Areasonable%20computational%20cost%2C%20it%20outperforms%20approximate%20unlearning%20methods%20at%0Aboth%20preserving%20the%20utility%20of%20the%20model%20post-unlearning%20and%20effectively%0Aforgetting%20the%20targeted%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP2Unlearning%253A%2520An%2520Efficient%2520and%2520Guaranteed%2520Unlearning%2520Framework%2520for%2520LLMs%26entry.906535625%3DTamim%2520Al%2520Mahmud%2520and%2520Najeeb%2520Jebreel%2520and%2520Josep%2520Domingo-Ferrer%2520and%2520David%2520Sanchez%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%2520revolutionized%2520language%2520processing%250Atasks%2520but%2520have%2520also%2520brought%2520ethical%2520and%2520legal%2520issues.%2520LLMs%2520have%2520a%2520tendency%2520to%250Amemorize%2520potentially%2520private%2520or%2520copyrighted%2520information%2520present%2520in%2520the%2520training%250Adata%252C%2520which%2520might%2520then%2520be%2520delivered%2520to%2520end%2520users%2520at%2520inference%2520time.%2520When%2520this%250Ahappens%252C%2520a%2520naive%2520solution%2520is%2520to%2520retrain%2520the%2520model%2520from%2520scratch%2520after%2520excluding%250Athe%2520undesired%2520data.%2520Although%2520this%2520guarantees%2520that%2520the%2520target%2520data%2520have%2520been%250Aforgotten%252C%2520it%2520is%2520also%2520prohibitively%2520expensive%2520for%2520LLMs.%2520Approximate%2520unlearning%250Aoffers%2520a%2520more%2520efficient%2520alternative%252C%2520as%2520it%2520consists%2520of%2520ex%2520post%2520modifications%2520of%250Athe%2520trained%2520model%2520itself%2520to%2520prevent%2520undesirable%2520results%252C%2520but%2520it%2520lacks%250Aforgetting%2520guarantees%2520because%2520it%2520relies%2520solely%2520on%2520empirical%2520evidence.%2520In%2520this%250Awork%252C%2520we%2520present%2520DP2Unlearning%252C%2520a%2520novel%2520LLM%2520unlearning%2520framework%2520that%2520offers%250Aformal%2520forgetting%2520guarantees%2520at%2520a%2520significantly%2520lower%2520cost%2520than%2520retraining%2520from%250Ascratch%2520on%2520the%2520data%2520to%2520be%2520retained.%2520DP2Unlearning%2520involves%2520training%2520LLMs%2520on%250Atextual%2520data%2520protected%2520using%2520%257B%255Cepsilon%257D-differential%2520privacy%2520%2528DP%2529%252C%2520which%2520later%250Aenables%2520efficient%2520unlearning%2520with%2520the%2520guarantees%2520against%2520disclosure%2520associated%250Awith%2520the%2520chosen%2520%257B%255Cepsilon%257D.%2520Our%2520experiments%2520demonstrate%2520that%2520DP2Unlearning%250Aachieves%2520similar%2520model%2520performance%2520post-unlearning%252C%2520compared%2520to%2520an%2520LLM%250Aretraining%2520from%2520scratch%2520on%2520retained%2520data%2520--%2520the%2520gold%2520standard%2520exact%2520unlearning%250A--%2520but%2520at%2520approximately%2520half%2520the%2520unlearning%2520cost.%2520In%2520addition%252C%2520with%2520a%250Areasonable%2520computational%2520cost%252C%2520it%2520outperforms%2520approximate%2520unlearning%2520methods%2520at%250Aboth%2520preserving%2520the%2520utility%2520of%2520the%2520model%2520post-unlearning%2520and%2520effectively%250Aforgetting%2520the%2520targeted%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP2Unlearning%3A%20An%20Efficient%20and%20Guaranteed%20Unlearning%20Framework%20for%20LLMs&entry.906535625=Tamim%20Al%20Mahmud%20and%20Najeeb%20Jebreel%20and%20Josep%20Domingo-Ferrer%20and%20David%20Sanchez&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20recently%20revolutionized%20language%20processing%0Atasks%20but%20have%20also%20brought%20ethical%20and%20legal%20issues.%20LLMs%20have%20a%20tendency%20to%0Amemorize%20potentially%20private%20or%20copyrighted%20information%20present%20in%20the%20training%0Adata%2C%20which%20might%20then%20be%20delivered%20to%20end%20users%20at%20inference%20time.%20When%20this%0Ahappens%2C%20a%20naive%20solution%20is%20to%20retrain%20the%20model%20from%20scratch%20after%20excluding%0Athe%20undesired%20data.%20Although%20this%20guarantees%20that%20the%20target%20data%20have%20been%0Aforgotten%2C%20it%20is%20also%20prohibitively%20expensive%20for%20LLMs.%20Approximate%20unlearning%0Aoffers%20a%20more%20efficient%20alternative%2C%20as%20it%20consists%20of%20ex%20post%20modifications%20of%0Athe%20trained%20model%20itself%20to%20prevent%20undesirable%20results%2C%20but%20it%20lacks%0Aforgetting%20guarantees%20because%20it%20relies%20solely%20on%20empirical%20evidence.%20In%20this%0Awork%2C%20we%20present%20DP2Unlearning%2C%20a%20novel%20LLM%20unlearning%20framework%20that%20offers%0Aformal%20forgetting%20guarantees%20at%20a%20significantly%20lower%20cost%20than%20retraining%20from%0Ascratch%20on%20the%20data%20to%20be%20retained.%20DP2Unlearning%20involves%20training%20LLMs%20on%0Atextual%20data%20protected%20using%20%7B%5Cepsilon%7D-differential%20privacy%20%28DP%29%2C%20which%20later%0Aenables%20efficient%20unlearning%20with%20the%20guarantees%20against%20disclosure%20associated%0Awith%20the%20chosen%20%7B%5Cepsilon%7D.%20Our%20experiments%20demonstrate%20that%20DP2Unlearning%0Aachieves%20similar%20model%20performance%20post-unlearning%2C%20compared%20to%20an%20LLM%0Aretraining%20from%20scratch%20on%20retained%20data%20--%20the%20gold%20standard%20exact%20unlearning%0A--%20but%20at%20approximately%20half%20the%20unlearning%20cost.%20In%20addition%2C%20with%20a%0Areasonable%20computational%20cost%2C%20it%20outperforms%20approximate%20unlearning%20methods%20at%0Aboth%20preserving%20the%20utility%20of%20the%20model%20post-unlearning%20and%20effectively%0Aforgetting%20the%20targeted%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13774v2&entry.124074799=Read"},
{"title": "Towards Regulated Deep Learning", "author": "Andr\u00e9s Garc\u00eda-Camino", "abstract": "  Regulation of Multi-Agent Systems (MAS) and Declarative Electronic\nInstitutions (DEIs) was a multidisciplinary research topic of the past decade\ninvolving (Physical and Software) Agents and Law since the beginning, but\nrecently evolved towards News-claimed Robot Lawyer since 2016. One of these\nfirst proposals of restricting the behaviour of Software Agents was Electronic\nInstitutions. However, with the recent reformulation of Artificial Neural\nNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal\nissues regarding the use of DL has raised concerns in the Artificial\nIntelligence (AI) Community. Now that the Regulation of MAS is almost correctly\naddressed, we propose the Regulation of Artificial Neural Networks as\nAgent-based Training of a special type of regulated Artificial Neural Network\nthat we call Institutional Neural Network (INN).The main purpose of this paper\nis to bring attention to Artificial Teaching (AT) and to give a tentative\nanswer showing a proof-of-concept implementation of Regulated Deep Learning\n(RDL). This paper introduces the former concept and provide $I^*$, a language\npreviously used to model declaratively and extend Electronic Institutions, as a\nmeans to regulate the execution of Artificial Neural Networks and their\ninteractions with Artificial Teachers (ATs)\n", "link": "http://arxiv.org/abs/1912.13122v8", "date": "2025-07-18", "relevancy": 1.9675, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4947}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Regulated%20Deep%20Learning&body=Title%3A%20Towards%20Regulated%20Deep%20Learning%0AAuthor%3A%20Andr%C3%A9s%20Garc%C3%ADa-Camino%0AAbstract%3A%20%20%20Regulation%20of%20Multi-Agent%20Systems%20%28MAS%29%20and%20Declarative%20Electronic%0AInstitutions%20%28DEIs%29%20was%20a%20multidisciplinary%20research%20topic%20of%20the%20past%20decade%0Ainvolving%20%28Physical%20and%20Software%29%20Agents%20and%20Law%20since%20the%20beginning%2C%20but%0Arecently%20evolved%20towards%20News-claimed%20Robot%20Lawyer%20since%202016.%20One%20of%20these%0Afirst%20proposals%20of%20restricting%20the%20behaviour%20of%20Software%20Agents%20was%20Electronic%0AInstitutions.%20However%2C%20with%20the%20recent%20reformulation%20of%20Artificial%20Neural%0ANetworks%20%28ANNs%29%20as%20Deep%20Learning%20%28DL%29%2C%20Security%2C%20Privacy%2CEthical%20and%20Legal%0Aissues%20regarding%20the%20use%20of%20DL%20has%20raised%20concerns%20in%20the%20Artificial%0AIntelligence%20%28AI%29%20Community.%20Now%20that%20the%20Regulation%20of%20MAS%20is%20almost%20correctly%0Aaddressed%2C%20we%20propose%20the%20Regulation%20of%20Artificial%20Neural%20Networks%20as%0AAgent-based%20Training%20of%20a%20special%20type%20of%20regulated%20Artificial%20Neural%20Network%0Athat%20we%20call%20Institutional%20Neural%20Network%20%28INN%29.The%20main%20purpose%20of%20this%20paper%0Ais%20to%20bring%20attention%20to%20Artificial%20Teaching%20%28AT%29%20and%20to%20give%20a%20tentative%0Aanswer%20showing%20a%20proof-of-concept%20implementation%20of%20Regulated%20Deep%20Learning%0A%28RDL%29.%20This%20paper%20introduces%20the%20former%20concept%20and%20provide%20%24I%5E%2A%24%2C%20a%20language%0Apreviously%20used%20to%20model%20declaratively%20and%20extend%20Electronic%20Institutions%2C%20as%20a%0Ameans%20to%20regulate%20the%20execution%20of%20Artificial%20Neural%20Networks%20and%20their%0Ainteractions%20with%20Artificial%20Teachers%20%28ATs%29%0A%0ALink%3A%20http%3A//arxiv.org/abs/1912.13122v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Regulated%2520Deep%2520Learning%26entry.906535625%3DAndr%25C3%25A9s%2520Garc%25C3%25ADa-Camino%26entry.1292438233%3D%2520%2520Regulation%2520of%2520Multi-Agent%2520Systems%2520%2528MAS%2529%2520and%2520Declarative%2520Electronic%250AInstitutions%2520%2528DEIs%2529%2520was%2520a%2520multidisciplinary%2520research%2520topic%2520of%2520the%2520past%2520decade%250Ainvolving%2520%2528Physical%2520and%2520Software%2529%2520Agents%2520and%2520Law%2520since%2520the%2520beginning%252C%2520but%250Arecently%2520evolved%2520towards%2520News-claimed%2520Robot%2520Lawyer%2520since%25202016.%2520One%2520of%2520these%250Afirst%2520proposals%2520of%2520restricting%2520the%2520behaviour%2520of%2520Software%2520Agents%2520was%2520Electronic%250AInstitutions.%2520However%252C%2520with%2520the%2520recent%2520reformulation%2520of%2520Artificial%2520Neural%250ANetworks%2520%2528ANNs%2529%2520as%2520Deep%2520Learning%2520%2528DL%2529%252C%2520Security%252C%2520Privacy%252CEthical%2520and%2520Legal%250Aissues%2520regarding%2520the%2520use%2520of%2520DL%2520has%2520raised%2520concerns%2520in%2520the%2520Artificial%250AIntelligence%2520%2528AI%2529%2520Community.%2520Now%2520that%2520the%2520Regulation%2520of%2520MAS%2520is%2520almost%2520correctly%250Aaddressed%252C%2520we%2520propose%2520the%2520Regulation%2520of%2520Artificial%2520Neural%2520Networks%2520as%250AAgent-based%2520Training%2520of%2520a%2520special%2520type%2520of%2520regulated%2520Artificial%2520Neural%2520Network%250Athat%2520we%2520call%2520Institutional%2520Neural%2520Network%2520%2528INN%2529.The%2520main%2520purpose%2520of%2520this%2520paper%250Ais%2520to%2520bring%2520attention%2520to%2520Artificial%2520Teaching%2520%2528AT%2529%2520and%2520to%2520give%2520a%2520tentative%250Aanswer%2520showing%2520a%2520proof-of-concept%2520implementation%2520of%2520Regulated%2520Deep%2520Learning%250A%2528RDL%2529.%2520This%2520paper%2520introduces%2520the%2520former%2520concept%2520and%2520provide%2520%2524I%255E%252A%2524%252C%2520a%2520language%250Apreviously%2520used%2520to%2520model%2520declaratively%2520and%2520extend%2520Electronic%2520Institutions%252C%2520as%2520a%250Ameans%2520to%2520regulate%2520the%2520execution%2520of%2520Artificial%2520Neural%2520Networks%2520and%2520their%250Ainteractions%2520with%2520Artificial%2520Teachers%2520%2528ATs%2529%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1912.13122v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Regulated%20Deep%20Learning&entry.906535625=Andr%C3%A9s%20Garc%C3%ADa-Camino&entry.1292438233=%20%20Regulation%20of%20Multi-Agent%20Systems%20%28MAS%29%20and%20Declarative%20Electronic%0AInstitutions%20%28DEIs%29%20was%20a%20multidisciplinary%20research%20topic%20of%20the%20past%20decade%0Ainvolving%20%28Physical%20and%20Software%29%20Agents%20and%20Law%20since%20the%20beginning%2C%20but%0Arecently%20evolved%20towards%20News-claimed%20Robot%20Lawyer%20since%202016.%20One%20of%20these%0Afirst%20proposals%20of%20restricting%20the%20behaviour%20of%20Software%20Agents%20was%20Electronic%0AInstitutions.%20However%2C%20with%20the%20recent%20reformulation%20of%20Artificial%20Neural%0ANetworks%20%28ANNs%29%20as%20Deep%20Learning%20%28DL%29%2C%20Security%2C%20Privacy%2CEthical%20and%20Legal%0Aissues%20regarding%20the%20use%20of%20DL%20has%20raised%20concerns%20in%20the%20Artificial%0AIntelligence%20%28AI%29%20Community.%20Now%20that%20the%20Regulation%20of%20MAS%20is%20almost%20correctly%0Aaddressed%2C%20we%20propose%20the%20Regulation%20of%20Artificial%20Neural%20Networks%20as%0AAgent-based%20Training%20of%20a%20special%20type%20of%20regulated%20Artificial%20Neural%20Network%0Athat%20we%20call%20Institutional%20Neural%20Network%20%28INN%29.The%20main%20purpose%20of%20this%20paper%0Ais%20to%20bring%20attention%20to%20Artificial%20Teaching%20%28AT%29%20and%20to%20give%20a%20tentative%0Aanswer%20showing%20a%20proof-of-concept%20implementation%20of%20Regulated%20Deep%20Learning%0A%28RDL%29.%20This%20paper%20introduces%20the%20former%20concept%20and%20provide%20%24I%5E%2A%24%2C%20a%20language%0Apreviously%20used%20to%20model%20declaratively%20and%20extend%20Electronic%20Institutions%2C%20as%20a%0Ameans%20to%20regulate%20the%20execution%20of%20Artificial%20Neural%20Networks%20and%20their%0Ainteractions%20with%20Artificial%20Teachers%20%28ATs%29%0A&entry.1838667208=http%3A//arxiv.org/abs/1912.13122v8&entry.124074799=Read"},
{"title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for\n  Temporal Link Prediction", "author": "Haoyang Li and Yuming Xu and Yiming Li and Hanmo Liu and Darian Li and Chen Jason Zhang and Lei Chen and Qing Li", "abstract": "  Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.\n", "link": "http://arxiv.org/abs/2507.13825v1", "date": "2025-07-18", "relevancy": 1.9675, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.523}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4713}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Speed%20meets%20Accuracy%3A%20an%20Efficient%20and%20Effective%20Graph%20Model%20for%0A%20%20Temporal%20Link%20Prediction&body=Title%3A%20When%20Speed%20meets%20Accuracy%3A%20an%20Efficient%20and%20Effective%20Graph%20Model%20for%0A%20%20Temporal%20Link%20Prediction%0AAuthor%3A%20Haoyang%20Li%20and%20Yuming%20Xu%20and%20Yiming%20Li%20and%20Hanmo%20Liu%20and%20Darian%20Li%20and%20Chen%20Jason%20Zhang%20and%20Lei%20Chen%20and%20Qing%20Li%0AAbstract%3A%20%20%20Temporal%20link%20prediction%20in%20dynamic%20graphs%20is%20a%20critical%20task%20with%0Aapplications%20in%20diverse%20domains%20such%20as%20social%20networks%2C%20recommendation%0Asystems%2C%20and%20e-commerce%20platforms.%20While%20existing%20Temporal%20Graph%20Neural%0ANetworks%20%28T-GNNs%29%20have%20achieved%20notable%20success%20by%20leveraging%20complex%0Aarchitectures%20to%20model%20temporal%20and%20structural%20dependencies%2C%20they%20often%20suffer%0Afrom%20scalability%20and%20efficiency%20challenges%20due%20to%20high%20computational%20overhead.%0AIn%20this%20paper%2C%20we%20propose%20EAGLE%2C%20a%20lightweight%20framework%20that%20integrates%0Ashort-term%20temporal%20recency%20and%20long-term%20global%20structural%20patterns.%20EAGLE%0Aconsists%20of%20a%20time-aware%20module%20that%20aggregates%20information%20from%20a%20node%27s%20most%0Arecent%20neighbors%20to%20reflect%20its%20immediate%20preferences%2C%20and%20a%20structure-aware%0Amodule%20that%20leverages%20temporal%20personalized%20PageRank%20to%20capture%20the%20influence%0Aof%20globally%20important%20nodes.%20To%20balance%20these%20attributes%2C%20EAGLE%20employs%20an%0Aadaptive%20weighting%20mechanism%20to%20dynamically%20adjust%20their%20contributions%20based%20on%0Adata%20characteristics.%20Also%2C%20EAGLE%20eliminates%20the%20need%20for%20complex%20multi-hop%0Amessage%20passing%20or%20memory-intensive%20mechanisms%2C%20enabling%20significant%0Aimprovements%20in%20efficiency.%20Extensive%20experiments%20on%20seven%20real-world%20temporal%0Agraphs%20demonstrate%20that%20EAGLE%20consistently%20achieves%20superior%20performance%0Aagainst%20state-of-the-art%20T-GNNs%20in%20both%20effectiveness%20and%20efficiency%2C%0Adelivering%20more%20than%20a%2050x%20speedup%20over%20effective%20transformer-based%20T-GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13825v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Speed%2520meets%2520Accuracy%253A%2520an%2520Efficient%2520and%2520Effective%2520Graph%2520Model%2520for%250A%2520%2520Temporal%2520Link%2520Prediction%26entry.906535625%3DHaoyang%2520Li%2520and%2520Yuming%2520Xu%2520and%2520Yiming%2520Li%2520and%2520Hanmo%2520Liu%2520and%2520Darian%2520Li%2520and%2520Chen%2520Jason%2520Zhang%2520and%2520Lei%2520Chen%2520and%2520Qing%2520Li%26entry.1292438233%3D%2520%2520Temporal%2520link%2520prediction%2520in%2520dynamic%2520graphs%2520is%2520a%2520critical%2520task%2520with%250Aapplications%2520in%2520diverse%2520domains%2520such%2520as%2520social%2520networks%252C%2520recommendation%250Asystems%252C%2520and%2520e-commerce%2520platforms.%2520While%2520existing%2520Temporal%2520Graph%2520Neural%250ANetworks%2520%2528T-GNNs%2529%2520have%2520achieved%2520notable%2520success%2520by%2520leveraging%2520complex%250Aarchitectures%2520to%2520model%2520temporal%2520and%2520structural%2520dependencies%252C%2520they%2520often%2520suffer%250Afrom%2520scalability%2520and%2520efficiency%2520challenges%2520due%2520to%2520high%2520computational%2520overhead.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520EAGLE%252C%2520a%2520lightweight%2520framework%2520that%2520integrates%250Ashort-term%2520temporal%2520recency%2520and%2520long-term%2520global%2520structural%2520patterns.%2520EAGLE%250Aconsists%2520of%2520a%2520time-aware%2520module%2520that%2520aggregates%2520information%2520from%2520a%2520node%2527s%2520most%250Arecent%2520neighbors%2520to%2520reflect%2520its%2520immediate%2520preferences%252C%2520and%2520a%2520structure-aware%250Amodule%2520that%2520leverages%2520temporal%2520personalized%2520PageRank%2520to%2520capture%2520the%2520influence%250Aof%2520globally%2520important%2520nodes.%2520To%2520balance%2520these%2520attributes%252C%2520EAGLE%2520employs%2520an%250Aadaptive%2520weighting%2520mechanism%2520to%2520dynamically%2520adjust%2520their%2520contributions%2520based%2520on%250Adata%2520characteristics.%2520Also%252C%2520EAGLE%2520eliminates%2520the%2520need%2520for%2520complex%2520multi-hop%250Amessage%2520passing%2520or%2520memory-intensive%2520mechanisms%252C%2520enabling%2520significant%250Aimprovements%2520in%2520efficiency.%2520Extensive%2520experiments%2520on%2520seven%2520real-world%2520temporal%250Agraphs%2520demonstrate%2520that%2520EAGLE%2520consistently%2520achieves%2520superior%2520performance%250Aagainst%2520state-of-the-art%2520T-GNNs%2520in%2520both%2520effectiveness%2520and%2520efficiency%252C%250Adelivering%2520more%2520than%2520a%252050x%2520speedup%2520over%2520effective%2520transformer-based%2520T-GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13825v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Speed%20meets%20Accuracy%3A%20an%20Efficient%20and%20Effective%20Graph%20Model%20for%0A%20%20Temporal%20Link%20Prediction&entry.906535625=Haoyang%20Li%20and%20Yuming%20Xu%20and%20Yiming%20Li%20and%20Hanmo%20Liu%20and%20Darian%20Li%20and%20Chen%20Jason%20Zhang%20and%20Lei%20Chen%20and%20Qing%20Li&entry.1292438233=%20%20Temporal%20link%20prediction%20in%20dynamic%20graphs%20is%20a%20critical%20task%20with%0Aapplications%20in%20diverse%20domains%20such%20as%20social%20networks%2C%20recommendation%0Asystems%2C%20and%20e-commerce%20platforms.%20While%20existing%20Temporal%20Graph%20Neural%0ANetworks%20%28T-GNNs%29%20have%20achieved%20notable%20success%20by%20leveraging%20complex%0Aarchitectures%20to%20model%20temporal%20and%20structural%20dependencies%2C%20they%20often%20suffer%0Afrom%20scalability%20and%20efficiency%20challenges%20due%20to%20high%20computational%20overhead.%0AIn%20this%20paper%2C%20we%20propose%20EAGLE%2C%20a%20lightweight%20framework%20that%20integrates%0Ashort-term%20temporal%20recency%20and%20long-term%20global%20structural%20patterns.%20EAGLE%0Aconsists%20of%20a%20time-aware%20module%20that%20aggregates%20information%20from%20a%20node%27s%20most%0Arecent%20neighbors%20to%20reflect%20its%20immediate%20preferences%2C%20and%20a%20structure-aware%0Amodule%20that%20leverages%20temporal%20personalized%20PageRank%20to%20capture%20the%20influence%0Aof%20globally%20important%20nodes.%20To%20balance%20these%20attributes%2C%20EAGLE%20employs%20an%0Aadaptive%20weighting%20mechanism%20to%20dynamically%20adjust%20their%20contributions%20based%20on%0Adata%20characteristics.%20Also%2C%20EAGLE%20eliminates%20the%20need%20for%20complex%20multi-hop%0Amessage%20passing%20or%20memory-intensive%20mechanisms%2C%20enabling%20significant%0Aimprovements%20in%20efficiency.%20Extensive%20experiments%20on%20seven%20real-world%20temporal%0Agraphs%20demonstrate%20that%20EAGLE%20consistently%20achieves%20superior%20performance%0Aagainst%20state-of-the-art%20T-GNNs%20in%20both%20effectiveness%20and%20efficiency%2C%0Adelivering%20more%20than%20a%2050x%20speedup%20over%20effective%20transformer-based%20T-GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13825v1&entry.124074799=Read"},
{"title": "Self-supervised learning on gene expression data", "author": "Kevin Dradjat and Massinissa Hamidi and Pierre Bartet and Blaise Hanczar", "abstract": "  Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.\n", "link": "http://arxiv.org/abs/2507.13912v1", "date": "2025-07-18", "relevancy": 1.9653, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5371}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4654}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20learning%20on%20gene%20expression%20data&body=Title%3A%20Self-supervised%20learning%20on%20gene%20expression%20data%0AAuthor%3A%20Kevin%20Dradjat%20and%20Massinissa%20Hamidi%20and%20Pierre%20Bartet%20and%20Blaise%20Hanczar%0AAbstract%3A%20%20%20Predicting%20phenotypes%20from%20gene%20expression%20data%20is%20a%20crucial%20task%20in%0Abiomedical%20research%2C%20enabling%20insights%20into%20disease%20mechanisms%2C%20drug%20responses%2C%0Aand%20personalized%20medicine.%20Traditional%20machine%20learning%20and%20deep%20learning%20rely%0Aon%20supervised%20learning%2C%20which%20requires%20large%20quantities%20of%20labeled%20data%20that%0Aare%20costly%20and%20time-consuming%20to%20obtain%20in%20the%20case%20of%20gene%20expression%20data.%0ASelf-supervised%20learning%20has%20recently%20emerged%20as%20a%20promising%20approach%20to%0Aovercome%20these%20limitations%20by%20extracting%20information%20directly%20from%20the%0Astructure%20of%20unlabeled%20data.%20In%20this%20study%2C%20we%20investigate%20the%20application%20of%0Astate-of-the-art%20self-supervised%20learning%20methods%20to%20bulk%20gene%20expression%20data%0Afor%20phenotype%20prediction.%20We%20selected%20three%20self-supervised%20methods%2C%20based%20on%0Adifferent%20approaches%2C%20to%20assess%20their%20ability%20to%20exploit%20the%20inherent%20structure%0Aof%20the%20data%20and%20to%20generate%20qualitative%20representations%20which%20can%20be%20used%20for%0Adownstream%20predictive%20tasks.%20By%20using%20several%20publicly%20available%20gene%0Aexpression%20datasets%2C%20we%20demonstrate%20how%20the%20selected%20methods%20can%20effectively%0Acapture%20complex%20information%20and%20improve%20phenotype%20prediction%20accuracy.%20The%0Aresults%20obtained%20show%20that%20self-supervised%20learning%20methods%20can%20outperform%0Atraditional%20supervised%20models%20besides%20offering%20significant%20advantage%20by%0Areducing%20the%20dependency%20on%20annotated%20data.%20We%20provide%20a%20comprehensive%20analysis%0Aof%20the%20performance%20of%20each%20method%20by%20highlighting%20their%20strengths%20and%0Alimitations.%20We%20also%20provide%20recommendations%20for%20using%20these%20methods%20depending%0Aon%20the%20case%20under%20study.%20Finally%2C%20we%20outline%20future%20research%20directions%20to%0Aenhance%20the%20application%20of%20self-supervised%20learning%20in%20the%20field%20of%20gene%0Aexpression%20data%20analysis.%20This%20study%20is%20the%20first%20work%20that%20deals%20with%20bulk%0ARNA-Seq%20data%20and%20self-supervised%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520learning%2520on%2520gene%2520expression%2520data%26entry.906535625%3DKevin%2520Dradjat%2520and%2520Massinissa%2520Hamidi%2520and%2520Pierre%2520Bartet%2520and%2520Blaise%2520Hanczar%26entry.1292438233%3D%2520%2520Predicting%2520phenotypes%2520from%2520gene%2520expression%2520data%2520is%2520a%2520crucial%2520task%2520in%250Abiomedical%2520research%252C%2520enabling%2520insights%2520into%2520disease%2520mechanisms%252C%2520drug%2520responses%252C%250Aand%2520personalized%2520medicine.%2520Traditional%2520machine%2520learning%2520and%2520deep%2520learning%2520rely%250Aon%2520supervised%2520learning%252C%2520which%2520requires%2520large%2520quantities%2520of%2520labeled%2520data%2520that%250Aare%2520costly%2520and%2520time-consuming%2520to%2520obtain%2520in%2520the%2520case%2520of%2520gene%2520expression%2520data.%250ASelf-supervised%2520learning%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520approach%2520to%250Aovercome%2520these%2520limitations%2520by%2520extracting%2520information%2520directly%2520from%2520the%250Astructure%2520of%2520unlabeled%2520data.%2520In%2520this%2520study%252C%2520we%2520investigate%2520the%2520application%2520of%250Astate-of-the-art%2520self-supervised%2520learning%2520methods%2520to%2520bulk%2520gene%2520expression%2520data%250Afor%2520phenotype%2520prediction.%2520We%2520selected%2520three%2520self-supervised%2520methods%252C%2520based%2520on%250Adifferent%2520approaches%252C%2520to%2520assess%2520their%2520ability%2520to%2520exploit%2520the%2520inherent%2520structure%250Aof%2520the%2520data%2520and%2520to%2520generate%2520qualitative%2520representations%2520which%2520can%2520be%2520used%2520for%250Adownstream%2520predictive%2520tasks.%2520By%2520using%2520several%2520publicly%2520available%2520gene%250Aexpression%2520datasets%252C%2520we%2520demonstrate%2520how%2520the%2520selected%2520methods%2520can%2520effectively%250Acapture%2520complex%2520information%2520and%2520improve%2520phenotype%2520prediction%2520accuracy.%2520The%250Aresults%2520obtained%2520show%2520that%2520self-supervised%2520learning%2520methods%2520can%2520outperform%250Atraditional%2520supervised%2520models%2520besides%2520offering%2520significant%2520advantage%2520by%250Areducing%2520the%2520dependency%2520on%2520annotated%2520data.%2520We%2520provide%2520a%2520comprehensive%2520analysis%250Aof%2520the%2520performance%2520of%2520each%2520method%2520by%2520highlighting%2520their%2520strengths%2520and%250Alimitations.%2520We%2520also%2520provide%2520recommendations%2520for%2520using%2520these%2520methods%2520depending%250Aon%2520the%2520case%2520under%2520study.%2520Finally%252C%2520we%2520outline%2520future%2520research%2520directions%2520to%250Aenhance%2520the%2520application%2520of%2520self-supervised%2520learning%2520in%2520the%2520field%2520of%2520gene%250Aexpression%2520data%2520analysis.%2520This%2520study%2520is%2520the%2520first%2520work%2520that%2520deals%2520with%2520bulk%250ARNA-Seq%2520data%2520and%2520self-supervised%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20learning%20on%20gene%20expression%20data&entry.906535625=Kevin%20Dradjat%20and%20Massinissa%20Hamidi%20and%20Pierre%20Bartet%20and%20Blaise%20Hanczar&entry.1292438233=%20%20Predicting%20phenotypes%20from%20gene%20expression%20data%20is%20a%20crucial%20task%20in%0Abiomedical%20research%2C%20enabling%20insights%20into%20disease%20mechanisms%2C%20drug%20responses%2C%0Aand%20personalized%20medicine.%20Traditional%20machine%20learning%20and%20deep%20learning%20rely%0Aon%20supervised%20learning%2C%20which%20requires%20large%20quantities%20of%20labeled%20data%20that%0Aare%20costly%20and%20time-consuming%20to%20obtain%20in%20the%20case%20of%20gene%20expression%20data.%0ASelf-supervised%20learning%20has%20recently%20emerged%20as%20a%20promising%20approach%20to%0Aovercome%20these%20limitations%20by%20extracting%20information%20directly%20from%20the%0Astructure%20of%20unlabeled%20data.%20In%20this%20study%2C%20we%20investigate%20the%20application%20of%0Astate-of-the-art%20self-supervised%20learning%20methods%20to%20bulk%20gene%20expression%20data%0Afor%20phenotype%20prediction.%20We%20selected%20three%20self-supervised%20methods%2C%20based%20on%0Adifferent%20approaches%2C%20to%20assess%20their%20ability%20to%20exploit%20the%20inherent%20structure%0Aof%20the%20data%20and%20to%20generate%20qualitative%20representations%20which%20can%20be%20used%20for%0Adownstream%20predictive%20tasks.%20By%20using%20several%20publicly%20available%20gene%0Aexpression%20datasets%2C%20we%20demonstrate%20how%20the%20selected%20methods%20can%20effectively%0Acapture%20complex%20information%20and%20improve%20phenotype%20prediction%20accuracy.%20The%0Aresults%20obtained%20show%20that%20self-supervised%20learning%20methods%20can%20outperform%0Atraditional%20supervised%20models%20besides%20offering%20significant%20advantage%20by%0Areducing%20the%20dependency%20on%20annotated%20data.%20We%20provide%20a%20comprehensive%20analysis%0Aof%20the%20performance%20of%20each%20method%20by%20highlighting%20their%20strengths%20and%0Alimitations.%20We%20also%20provide%20recommendations%20for%20using%20these%20methods%20depending%0Aon%20the%20case%20under%20study.%20Finally%2C%20we%20outline%20future%20research%20directions%20to%0Aenhance%20the%20application%20of%20self-supervised%20learning%20in%20the%20field%20of%20gene%0Aexpression%20data%20analysis.%20This%20study%20is%20the%20first%20work%20that%20deals%20with%20bulk%0ARNA-Seq%20data%20and%20self-supervised%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13912v1&entry.124074799=Read"},
{"title": "$\u03b5$-rank and the Staircase Phenomenon: New Insights into Neural\n  Network Training Dynamics", "author": "Jiang Yang and Yuxiang Zhao and Quanhui Zhu", "abstract": "  Understanding the training dynamics of deep neural networks (DNNs),\nparticularly how they evolve low-dimensional features from high-dimensional\ndata, remains a central challenge in deep learning theory. In this work, we\nintroduce the concept of $\\epsilon$-rank, a novel metric quantifying the\neffective feature of neuron functions in the terminal hidden layer. Through\nextensive experiments across diverse tasks, we observe a universal staircase\nphenomenon: during training process implemented by the standard stochastic\ngradient descent methods, the decline of the loss function is accompanied by an\nincrease in the $\\epsilon$-rank and exhibits a staircase pattern.\nTheoretically, we rigorously prove a negative correlation between the loss\nlower bound and $\\epsilon$-rank, demonstrating that a high $\\epsilon$-rank is\nessential for significant loss reduction. Moreover, numerical evidences show\nthat within the same deep neural network, the $\\epsilon$-rank of the subsequent\nhidden layer is higher than that of the previous hidden layer. Based on these\nobservations, to eliminate the staircase phenomenon, we propose a novel\npre-training strategy on the initial hidden layer that elevates the\n$\\epsilon$-rank of the terminal hidden layer. Numerical experiments validate\nits effectiveness in reducing training time and improving accuracy across\nvarious tasks. Therefore, the newly introduced concept of $\\epsilon$-rank is a\ncomputable quantity that serves as an intrinsic effective metric characteristic\nfor deep neural networks, providing a novel perspective for understanding the\ntraining dynamics of neural networks and offering a theoretical foundation for\ndesigning efficient training strategies in practical applications.\n", "link": "http://arxiv.org/abs/2412.05144v3", "date": "2025-07-18", "relevancy": 1.9594, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5273}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4674}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CE%B5%24-rank%20and%20the%20Staircase%20Phenomenon%3A%20New%20Insights%20into%20Neural%0A%20%20Network%20Training%20Dynamics&body=Title%3A%20%24%CE%B5%24-rank%20and%20the%20Staircase%20Phenomenon%3A%20New%20Insights%20into%20Neural%0A%20%20Network%20Training%20Dynamics%0AAuthor%3A%20Jiang%20Yang%20and%20Yuxiang%20Zhao%20and%20Quanhui%20Zhu%0AAbstract%3A%20%20%20Understanding%20the%20training%20dynamics%20of%20deep%20neural%20networks%20%28DNNs%29%2C%0Aparticularly%20how%20they%20evolve%20low-dimensional%20features%20from%20high-dimensional%0Adata%2C%20remains%20a%20central%20challenge%20in%20deep%20learning%20theory.%20In%20this%20work%2C%20we%0Aintroduce%20the%20concept%20of%20%24%5Cepsilon%24-rank%2C%20a%20novel%20metric%20quantifying%20the%0Aeffective%20feature%20of%20neuron%20functions%20in%20the%20terminal%20hidden%20layer.%20Through%0Aextensive%20experiments%20across%20diverse%20tasks%2C%20we%20observe%20a%20universal%20staircase%0Aphenomenon%3A%20during%20training%20process%20implemented%20by%20the%20standard%20stochastic%0Agradient%20descent%20methods%2C%20the%20decline%20of%20the%20loss%20function%20is%20accompanied%20by%20an%0Aincrease%20in%20the%20%24%5Cepsilon%24-rank%20and%20exhibits%20a%20staircase%20pattern.%0ATheoretically%2C%20we%20rigorously%20prove%20a%20negative%20correlation%20between%20the%20loss%0Alower%20bound%20and%20%24%5Cepsilon%24-rank%2C%20demonstrating%20that%20a%20high%20%24%5Cepsilon%24-rank%20is%0Aessential%20for%20significant%20loss%20reduction.%20Moreover%2C%20numerical%20evidences%20show%0Athat%20within%20the%20same%20deep%20neural%20network%2C%20the%20%24%5Cepsilon%24-rank%20of%20the%20subsequent%0Ahidden%20layer%20is%20higher%20than%20that%20of%20the%20previous%20hidden%20layer.%20Based%20on%20these%0Aobservations%2C%20to%20eliminate%20the%20staircase%20phenomenon%2C%20we%20propose%20a%20novel%0Apre-training%20strategy%20on%20the%20initial%20hidden%20layer%20that%20elevates%20the%0A%24%5Cepsilon%24-rank%20of%20the%20terminal%20hidden%20layer.%20Numerical%20experiments%20validate%0Aits%20effectiveness%20in%20reducing%20training%20time%20and%20improving%20accuracy%20across%0Avarious%20tasks.%20Therefore%2C%20the%20newly%20introduced%20concept%20of%20%24%5Cepsilon%24-rank%20is%20a%0Acomputable%20quantity%20that%20serves%20as%20an%20intrinsic%20effective%20metric%20characteristic%0Afor%20deep%20neural%20networks%2C%20providing%20a%20novel%20perspective%20for%20understanding%20the%0Atraining%20dynamics%20of%20neural%20networks%20and%20offering%20a%20theoretical%20foundation%20for%0Adesigning%20efficient%20training%20strategies%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05144v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CE%25B5%2524-rank%2520and%2520the%2520Staircase%2520Phenomenon%253A%2520New%2520Insights%2520into%2520Neural%250A%2520%2520Network%2520Training%2520Dynamics%26entry.906535625%3DJiang%2520Yang%2520and%2520Yuxiang%2520Zhao%2520and%2520Quanhui%2520Zhu%26entry.1292438233%3D%2520%2520Understanding%2520the%2520training%2520dynamics%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%250Aparticularly%2520how%2520they%2520evolve%2520low-dimensional%2520features%2520from%2520high-dimensional%250Adata%252C%2520remains%2520a%2520central%2520challenge%2520in%2520deep%2520learning%2520theory.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520concept%2520of%2520%2524%255Cepsilon%2524-rank%252C%2520a%2520novel%2520metric%2520quantifying%2520the%250Aeffective%2520feature%2520of%2520neuron%2520functions%2520in%2520the%2520terminal%2520hidden%2520layer.%2520Through%250Aextensive%2520experiments%2520across%2520diverse%2520tasks%252C%2520we%2520observe%2520a%2520universal%2520staircase%250Aphenomenon%253A%2520during%2520training%2520process%2520implemented%2520by%2520the%2520standard%2520stochastic%250Agradient%2520descent%2520methods%252C%2520the%2520decline%2520of%2520the%2520loss%2520function%2520is%2520accompanied%2520by%2520an%250Aincrease%2520in%2520the%2520%2524%255Cepsilon%2524-rank%2520and%2520exhibits%2520a%2520staircase%2520pattern.%250ATheoretically%252C%2520we%2520rigorously%2520prove%2520a%2520negative%2520correlation%2520between%2520the%2520loss%250Alower%2520bound%2520and%2520%2524%255Cepsilon%2524-rank%252C%2520demonstrating%2520that%2520a%2520high%2520%2524%255Cepsilon%2524-rank%2520is%250Aessential%2520for%2520significant%2520loss%2520reduction.%2520Moreover%252C%2520numerical%2520evidences%2520show%250Athat%2520within%2520the%2520same%2520deep%2520neural%2520network%252C%2520the%2520%2524%255Cepsilon%2524-rank%2520of%2520the%2520subsequent%250Ahidden%2520layer%2520is%2520higher%2520than%2520that%2520of%2520the%2520previous%2520hidden%2520layer.%2520Based%2520on%2520these%250Aobservations%252C%2520to%2520eliminate%2520the%2520staircase%2520phenomenon%252C%2520we%2520propose%2520a%2520novel%250Apre-training%2520strategy%2520on%2520the%2520initial%2520hidden%2520layer%2520that%2520elevates%2520the%250A%2524%255Cepsilon%2524-rank%2520of%2520the%2520terminal%2520hidden%2520layer.%2520Numerical%2520experiments%2520validate%250Aits%2520effectiveness%2520in%2520reducing%2520training%2520time%2520and%2520improving%2520accuracy%2520across%250Avarious%2520tasks.%2520Therefore%252C%2520the%2520newly%2520introduced%2520concept%2520of%2520%2524%255Cepsilon%2524-rank%2520is%2520a%250Acomputable%2520quantity%2520that%2520serves%2520as%2520an%2520intrinsic%2520effective%2520metric%2520characteristic%250Afor%2520deep%2520neural%2520networks%252C%2520providing%2520a%2520novel%2520perspective%2520for%2520understanding%2520the%250Atraining%2520dynamics%2520of%2520neural%2520networks%2520and%2520offering%2520a%2520theoretical%2520foundation%2520for%250Adesigning%2520efficient%2520training%2520strategies%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05144v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CE%B5%24-rank%20and%20the%20Staircase%20Phenomenon%3A%20New%20Insights%20into%20Neural%0A%20%20Network%20Training%20Dynamics&entry.906535625=Jiang%20Yang%20and%20Yuxiang%20Zhao%20and%20Quanhui%20Zhu&entry.1292438233=%20%20Understanding%20the%20training%20dynamics%20of%20deep%20neural%20networks%20%28DNNs%29%2C%0Aparticularly%20how%20they%20evolve%20low-dimensional%20features%20from%20high-dimensional%0Adata%2C%20remains%20a%20central%20challenge%20in%20deep%20learning%20theory.%20In%20this%20work%2C%20we%0Aintroduce%20the%20concept%20of%20%24%5Cepsilon%24-rank%2C%20a%20novel%20metric%20quantifying%20the%0Aeffective%20feature%20of%20neuron%20functions%20in%20the%20terminal%20hidden%20layer.%20Through%0Aextensive%20experiments%20across%20diverse%20tasks%2C%20we%20observe%20a%20universal%20staircase%0Aphenomenon%3A%20during%20training%20process%20implemented%20by%20the%20standard%20stochastic%0Agradient%20descent%20methods%2C%20the%20decline%20of%20the%20loss%20function%20is%20accompanied%20by%20an%0Aincrease%20in%20the%20%24%5Cepsilon%24-rank%20and%20exhibits%20a%20staircase%20pattern.%0ATheoretically%2C%20we%20rigorously%20prove%20a%20negative%20correlation%20between%20the%20loss%0Alower%20bound%20and%20%24%5Cepsilon%24-rank%2C%20demonstrating%20that%20a%20high%20%24%5Cepsilon%24-rank%20is%0Aessential%20for%20significant%20loss%20reduction.%20Moreover%2C%20numerical%20evidences%20show%0Athat%20within%20the%20same%20deep%20neural%20network%2C%20the%20%24%5Cepsilon%24-rank%20of%20the%20subsequent%0Ahidden%20layer%20is%20higher%20than%20that%20of%20the%20previous%20hidden%20layer.%20Based%20on%20these%0Aobservations%2C%20to%20eliminate%20the%20staircase%20phenomenon%2C%20we%20propose%20a%20novel%0Apre-training%20strategy%20on%20the%20initial%20hidden%20layer%20that%20elevates%20the%0A%24%5Cepsilon%24-rank%20of%20the%20terminal%20hidden%20layer.%20Numerical%20experiments%20validate%0Aits%20effectiveness%20in%20reducing%20training%20time%20and%20improving%20accuracy%20across%0Avarious%20tasks.%20Therefore%2C%20the%20newly%20introduced%20concept%20of%20%24%5Cepsilon%24-rank%20is%20a%0Acomputable%20quantity%20that%20serves%20as%20an%20intrinsic%20effective%20metric%20characteristic%0Afor%20deep%20neural%20networks%2C%20providing%20a%20novel%20perspective%20for%20understanding%20the%0Atraining%20dynamics%20of%20neural%20networks%20and%20offering%20a%20theoretical%20foundation%20for%0Adesigning%20efficient%20training%20strategies%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05144v3&entry.124074799=Read"},
{"title": "XpertAI: uncovering regression model strategies for sub-manifolds", "author": "Simon Letzgus and Klaus-Robert M\u00fcller and Gr\u00e9goire Montavon", "abstract": "  In recent years, Explainable AI (XAI) methods have facilitated profound\nvalidation and knowledge extraction from ML models. While extensively studied\nfor classification, few XAI solutions have addressed the challenges specific to\nregression models. In regression, explanations need to be precisely formulated\nto address specific user queries (e.g.\\ distinguishing between `Why is the\noutput above 0?' and `Why is the output above 50?'). They should furthermore\nreflect the model's behavior on the relevant data sub-manifold. In this paper,\nwe introduce XpertAI, a framework that disentangles the prediction strategy\ninto multiple range-specific sub-strategies and allows the formulation of\nprecise queries about the model (the `explanandum') as a linear combination of\nthose sub-strategies. XpertAI is formulated generally to work alongside popular\nXAI attribution techniques, based on occlusion, gradient integration, or\nreverse propagation. Qualitative and quantitative results, demonstrate the\nbenefits of our approach.\n", "link": "http://arxiv.org/abs/2403.07486v4", "date": "2025-07-18", "relevancy": 1.9441, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XpertAI%3A%20uncovering%20regression%20model%20strategies%20for%20sub-manifolds&body=Title%3A%20XpertAI%3A%20uncovering%20regression%20model%20strategies%20for%20sub-manifolds%0AAuthor%3A%20Simon%20Letzgus%20and%20Klaus-Robert%20M%C3%BCller%20and%20Gr%C3%A9goire%20Montavon%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Explainable%20AI%20%28XAI%29%20methods%20have%20facilitated%20profound%0Avalidation%20and%20knowledge%20extraction%20from%20ML%20models.%20While%20extensively%20studied%0Afor%20classification%2C%20few%20XAI%20solutions%20have%20addressed%20the%20challenges%20specific%20to%0Aregression%20models.%20In%20regression%2C%20explanations%20need%20to%20be%20precisely%20formulated%0Ato%20address%20specific%20user%20queries%20%28e.g.%5C%20distinguishing%20between%20%60Why%20is%20the%0Aoutput%20above%200%3F%27%20and%20%60Why%20is%20the%20output%20above%2050%3F%27%29.%20They%20should%20furthermore%0Areflect%20the%20model%27s%20behavior%20on%20the%20relevant%20data%20sub-manifold.%20In%20this%20paper%2C%0Awe%20introduce%20XpertAI%2C%20a%20framework%20that%20disentangles%20the%20prediction%20strategy%0Ainto%20multiple%20range-specific%20sub-strategies%20and%20allows%20the%20formulation%20of%0Aprecise%20queries%20about%20the%20model%20%28the%20%60explanandum%27%29%20as%20a%20linear%20combination%20of%0Athose%20sub-strategies.%20XpertAI%20is%20formulated%20generally%20to%20work%20alongside%20popular%0AXAI%20attribution%20techniques%2C%20based%20on%20occlusion%2C%20gradient%20integration%2C%20or%0Areverse%20propagation.%20Qualitative%20and%20quantitative%20results%2C%20demonstrate%20the%0Abenefits%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07486v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXpertAI%253A%2520uncovering%2520regression%2520model%2520strategies%2520for%2520sub-manifolds%26entry.906535625%3DSimon%2520Letzgus%2520and%2520Klaus-Robert%2520M%25C3%25BCller%2520and%2520Gr%25C3%25A9goire%2520Montavon%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Explainable%2520AI%2520%2528XAI%2529%2520methods%2520have%2520facilitated%2520profound%250Avalidation%2520and%2520knowledge%2520extraction%2520from%2520ML%2520models.%2520While%2520extensively%2520studied%250Afor%2520classification%252C%2520few%2520XAI%2520solutions%2520have%2520addressed%2520the%2520challenges%2520specific%2520to%250Aregression%2520models.%2520In%2520regression%252C%2520explanations%2520need%2520to%2520be%2520precisely%2520formulated%250Ato%2520address%2520specific%2520user%2520queries%2520%2528e.g.%255C%2520distinguishing%2520between%2520%2560Why%2520is%2520the%250Aoutput%2520above%25200%253F%2527%2520and%2520%2560Why%2520is%2520the%2520output%2520above%252050%253F%2527%2529.%2520They%2520should%2520furthermore%250Areflect%2520the%2520model%2527s%2520behavior%2520on%2520the%2520relevant%2520data%2520sub-manifold.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520XpertAI%252C%2520a%2520framework%2520that%2520disentangles%2520the%2520prediction%2520strategy%250Ainto%2520multiple%2520range-specific%2520sub-strategies%2520and%2520allows%2520the%2520formulation%2520of%250Aprecise%2520queries%2520about%2520the%2520model%2520%2528the%2520%2560explanandum%2527%2529%2520as%2520a%2520linear%2520combination%2520of%250Athose%2520sub-strategies.%2520XpertAI%2520is%2520formulated%2520generally%2520to%2520work%2520alongside%2520popular%250AXAI%2520attribution%2520techniques%252C%2520based%2520on%2520occlusion%252C%2520gradient%2520integration%252C%2520or%250Areverse%2520propagation.%2520Qualitative%2520and%2520quantitative%2520results%252C%2520demonstrate%2520the%250Abenefits%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07486v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XpertAI%3A%20uncovering%20regression%20model%20strategies%20for%20sub-manifolds&entry.906535625=Simon%20Letzgus%20and%20Klaus-Robert%20M%C3%BCller%20and%20Gr%C3%A9goire%20Montavon&entry.1292438233=%20%20In%20recent%20years%2C%20Explainable%20AI%20%28XAI%29%20methods%20have%20facilitated%20profound%0Avalidation%20and%20knowledge%20extraction%20from%20ML%20models.%20While%20extensively%20studied%0Afor%20classification%2C%20few%20XAI%20solutions%20have%20addressed%20the%20challenges%20specific%20to%0Aregression%20models.%20In%20regression%2C%20explanations%20need%20to%20be%20precisely%20formulated%0Ato%20address%20specific%20user%20queries%20%28e.g.%5C%20distinguishing%20between%20%60Why%20is%20the%0Aoutput%20above%200%3F%27%20and%20%60Why%20is%20the%20output%20above%2050%3F%27%29.%20They%20should%20furthermore%0Areflect%20the%20model%27s%20behavior%20on%20the%20relevant%20data%20sub-manifold.%20In%20this%20paper%2C%0Awe%20introduce%20XpertAI%2C%20a%20framework%20that%20disentangles%20the%20prediction%20strategy%0Ainto%20multiple%20range-specific%20sub-strategies%20and%20allows%20the%20formulation%20of%0Aprecise%20queries%20about%20the%20model%20%28the%20%60explanandum%27%29%20as%20a%20linear%20combination%20of%0Athose%20sub-strategies.%20XpertAI%20is%20formulated%20generally%20to%20work%20alongside%20popular%0AXAI%20attribution%20techniques%2C%20based%20on%20occlusion%2C%20gradient%20integration%2C%20or%0Areverse%20propagation.%20Qualitative%20and%20quantitative%20results%2C%20demonstrate%20the%0Abenefits%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07486v4&entry.124074799=Read"},
{"title": "Bridging Local and Global Knowledge via Transformer in Board Games", "author": "Yan-Ru Ju and Tai-Lin Wu and Chung-Chin Shih and Ti-Rong Wu", "abstract": "  Although AlphaZero has achieved superhuman performance in board games, recent\nstudies reveal its limitations in handling scenarios requiring a comprehensive\nunderstanding of the entire board, such as recognizing long-sequence patterns\nin Go. To address this challenge, we propose ResTNet, a network that\ninterleaves residual and Transformer blocks to bridge local and global\nknowledge. ResTNet improves playing strength across multiple board games,\nincreasing win rate from 54.6% to 60.8% in 9x9 Go, 53.6% to 60.9% in 19x19 Go,\nand 50.4% to 58.0% in 19x19 Hex. In addition, ResTNet effectively processes\nglobal information and tackles two long-sequence patterns in 19x19 Go,\nincluding circular pattern and ladder pattern. It reduces the mean square error\nfor circular pattern recognition from 2.58 to 1.07 and lowers the attack\nprobability against an adversary program from 70.44% to 23.91%. ResTNet also\nimproves ladder pattern recognition accuracy from 59.15% to 80.01%. By\nvisualizing attention maps, we demonstrate that ResTNet captures critical game\nconcepts in both Go and Hex, offering insights into AlphaZero's decision-making\nprocess. Overall, ResTNet shows a promising approach to integrating local and\nglobal knowledge, paving the way for more effective AlphaZero-based algorithms\nin board games. Our code is available at\nhttps://rlg.iis.sinica.edu.tw/papers/restnet.\n", "link": "http://arxiv.org/abs/2410.05347v2", "date": "2025-07-18", "relevancy": 1.932, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5127}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Local%20and%20Global%20Knowledge%20via%20Transformer%20in%20Board%20Games&body=Title%3A%20Bridging%20Local%20and%20Global%20Knowledge%20via%20Transformer%20in%20Board%20Games%0AAuthor%3A%20Yan-Ru%20Ju%20and%20Tai-Lin%20Wu%20and%20Chung-Chin%20Shih%20and%20Ti-Rong%20Wu%0AAbstract%3A%20%20%20Although%20AlphaZero%20has%20achieved%20superhuman%20performance%20in%20board%20games%2C%20recent%0Astudies%20reveal%20its%20limitations%20in%20handling%20scenarios%20requiring%20a%20comprehensive%0Aunderstanding%20of%20the%20entire%20board%2C%20such%20as%20recognizing%20long-sequence%20patterns%0Ain%20Go.%20To%20address%20this%20challenge%2C%20we%20propose%20ResTNet%2C%20a%20network%20that%0Ainterleaves%20residual%20and%20Transformer%20blocks%20to%20bridge%20local%20and%20global%0Aknowledge.%20ResTNet%20improves%20playing%20strength%20across%20multiple%20board%20games%2C%0Aincreasing%20win%20rate%20from%2054.6%25%20to%2060.8%25%20in%209x9%20Go%2C%2053.6%25%20to%2060.9%25%20in%2019x19%20Go%2C%0Aand%2050.4%25%20to%2058.0%25%20in%2019x19%20Hex.%20In%20addition%2C%20ResTNet%20effectively%20processes%0Aglobal%20information%20and%20tackles%20two%20long-sequence%20patterns%20in%2019x19%20Go%2C%0Aincluding%20circular%20pattern%20and%20ladder%20pattern.%20It%20reduces%20the%20mean%20square%20error%0Afor%20circular%20pattern%20recognition%20from%202.58%20to%201.07%20and%20lowers%20the%20attack%0Aprobability%20against%20an%20adversary%20program%20from%2070.44%25%20to%2023.91%25.%20ResTNet%20also%0Aimproves%20ladder%20pattern%20recognition%20accuracy%20from%2059.15%25%20to%2080.01%25.%20By%0Avisualizing%20attention%20maps%2C%20we%20demonstrate%20that%20ResTNet%20captures%20critical%20game%0Aconcepts%20in%20both%20Go%20and%20Hex%2C%20offering%20insights%20into%20AlphaZero%27s%20decision-making%0Aprocess.%20Overall%2C%20ResTNet%20shows%20a%20promising%20approach%20to%20integrating%20local%20and%0Aglobal%20knowledge%2C%20paving%20the%20way%20for%20more%20effective%20AlphaZero-based%20algorithms%0Ain%20board%20games.%20Our%20code%20is%20available%20at%0Ahttps%3A//rlg.iis.sinica.edu.tw/papers/restnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05347v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Local%2520and%2520Global%2520Knowledge%2520via%2520Transformer%2520in%2520Board%2520Games%26entry.906535625%3DYan-Ru%2520Ju%2520and%2520Tai-Lin%2520Wu%2520and%2520Chung-Chin%2520Shih%2520and%2520Ti-Rong%2520Wu%26entry.1292438233%3D%2520%2520Although%2520AlphaZero%2520has%2520achieved%2520superhuman%2520performance%2520in%2520board%2520games%252C%2520recent%250Astudies%2520reveal%2520its%2520limitations%2520in%2520handling%2520scenarios%2520requiring%2520a%2520comprehensive%250Aunderstanding%2520of%2520the%2520entire%2520board%252C%2520such%2520as%2520recognizing%2520long-sequence%2520patterns%250Ain%2520Go.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520ResTNet%252C%2520a%2520network%2520that%250Ainterleaves%2520residual%2520and%2520Transformer%2520blocks%2520to%2520bridge%2520local%2520and%2520global%250Aknowledge.%2520ResTNet%2520improves%2520playing%2520strength%2520across%2520multiple%2520board%2520games%252C%250Aincreasing%2520win%2520rate%2520from%252054.6%2525%2520to%252060.8%2525%2520in%25209x9%2520Go%252C%252053.6%2525%2520to%252060.9%2525%2520in%252019x19%2520Go%252C%250Aand%252050.4%2525%2520to%252058.0%2525%2520in%252019x19%2520Hex.%2520In%2520addition%252C%2520ResTNet%2520effectively%2520processes%250Aglobal%2520information%2520and%2520tackles%2520two%2520long-sequence%2520patterns%2520in%252019x19%2520Go%252C%250Aincluding%2520circular%2520pattern%2520and%2520ladder%2520pattern.%2520It%2520reduces%2520the%2520mean%2520square%2520error%250Afor%2520circular%2520pattern%2520recognition%2520from%25202.58%2520to%25201.07%2520and%2520lowers%2520the%2520attack%250Aprobability%2520against%2520an%2520adversary%2520program%2520from%252070.44%2525%2520to%252023.91%2525.%2520ResTNet%2520also%250Aimproves%2520ladder%2520pattern%2520recognition%2520accuracy%2520from%252059.15%2525%2520to%252080.01%2525.%2520By%250Avisualizing%2520attention%2520maps%252C%2520we%2520demonstrate%2520that%2520ResTNet%2520captures%2520critical%2520game%250Aconcepts%2520in%2520both%2520Go%2520and%2520Hex%252C%2520offering%2520insights%2520into%2520AlphaZero%2527s%2520decision-making%250Aprocess.%2520Overall%252C%2520ResTNet%2520shows%2520a%2520promising%2520approach%2520to%2520integrating%2520local%2520and%250Aglobal%2520knowledge%252C%2520paving%2520the%2520way%2520for%2520more%2520effective%2520AlphaZero-based%2520algorithms%250Ain%2520board%2520games.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//rlg.iis.sinica.edu.tw/papers/restnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05347v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Local%20and%20Global%20Knowledge%20via%20Transformer%20in%20Board%20Games&entry.906535625=Yan-Ru%20Ju%20and%20Tai-Lin%20Wu%20and%20Chung-Chin%20Shih%20and%20Ti-Rong%20Wu&entry.1292438233=%20%20Although%20AlphaZero%20has%20achieved%20superhuman%20performance%20in%20board%20games%2C%20recent%0Astudies%20reveal%20its%20limitations%20in%20handling%20scenarios%20requiring%20a%20comprehensive%0Aunderstanding%20of%20the%20entire%20board%2C%20such%20as%20recognizing%20long-sequence%20patterns%0Ain%20Go.%20To%20address%20this%20challenge%2C%20we%20propose%20ResTNet%2C%20a%20network%20that%0Ainterleaves%20residual%20and%20Transformer%20blocks%20to%20bridge%20local%20and%20global%0Aknowledge.%20ResTNet%20improves%20playing%20strength%20across%20multiple%20board%20games%2C%0Aincreasing%20win%20rate%20from%2054.6%25%20to%2060.8%25%20in%209x9%20Go%2C%2053.6%25%20to%2060.9%25%20in%2019x19%20Go%2C%0Aand%2050.4%25%20to%2058.0%25%20in%2019x19%20Hex.%20In%20addition%2C%20ResTNet%20effectively%20processes%0Aglobal%20information%20and%20tackles%20two%20long-sequence%20patterns%20in%2019x19%20Go%2C%0Aincluding%20circular%20pattern%20and%20ladder%20pattern.%20It%20reduces%20the%20mean%20square%20error%0Afor%20circular%20pattern%20recognition%20from%202.58%20to%201.07%20and%20lowers%20the%20attack%0Aprobability%20against%20an%20adversary%20program%20from%2070.44%25%20to%2023.91%25.%20ResTNet%20also%0Aimproves%20ladder%20pattern%20recognition%20accuracy%20from%2059.15%25%20to%2080.01%25.%20By%0Avisualizing%20attention%20maps%2C%20we%20demonstrate%20that%20ResTNet%20captures%20critical%20game%0Aconcepts%20in%20both%20Go%20and%20Hex%2C%20offering%20insights%20into%20AlphaZero%27s%20decision-making%0Aprocess.%20Overall%2C%20ResTNet%20shows%20a%20promising%20approach%20to%20integrating%20local%20and%0Aglobal%20knowledge%2C%20paving%20the%20way%20for%20more%20effective%20AlphaZero-based%20algorithms%0Ain%20board%20games.%20Our%20code%20is%20available%20at%0Ahttps%3A//rlg.iis.sinica.edu.tw/papers/restnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05347v2&entry.124074799=Read"},
{"title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts\n  (PLABA) track", "author": "Brian Ondov and William Xia and Kush Attal and Ishita Unde and Jerry He and Hoa Dang and Ian Soboroff and Dina Demner-Fushman", "abstract": "  Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.\n", "link": "http://arxiv.org/abs/2507.14096v1", "date": "2025-07-18", "relevancy": 1.9142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4698}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lessons%20from%20the%20TREC%20Plain%20Language%20Adaptation%20of%20Biomedical%20Abstracts%0A%20%20%28PLABA%29%20track&body=Title%3A%20Lessons%20from%20the%20TREC%20Plain%20Language%20Adaptation%20of%20Biomedical%20Abstracts%0A%20%20%28PLABA%29%20track%0AAuthor%3A%20Brian%20Ondov%20and%20William%20Xia%20and%20Kush%20Attal%20and%20Ishita%20Unde%20and%20Jerry%20He%20and%20Hoa%20Dang%20and%20Ian%20Soboroff%20and%20Dina%20Demner-Fushman%0AAbstract%3A%20%20%20Objective%3A%20Recent%20advances%20in%20language%20models%20have%20shown%20potential%20to%20adapt%0Aprofessional-facing%20biomedical%20literature%20to%20plain%20language%2C%20making%20it%0Aaccessible%20to%20patients%20and%20caregivers.%20However%2C%20their%20unpredictability%2C%0Acombined%20with%20the%20high%20potential%20for%20harm%20in%20this%20domain%2C%20means%20rigorous%0Aevaluation%20is%20necessary.%20Our%20goals%20with%20this%20track%20were%20to%20stimulate%20research%0Aand%20to%20provide%20high-quality%20evaluation%20of%20the%20most%20promising%20systems.%0A%20%20Methods%3A%20We%20hosted%20the%20Plain%20Language%20Adaptation%20of%20Biomedical%20Abstracts%0A%28PLABA%29%20track%20at%20the%202023%20and%202024%20Text%20Retrieval%20Conferences.%20Tasks%20included%0Acomplete%2C%20sentence-level%2C%20rewriting%20of%20abstracts%20%28Task%201%29%20as%20well%20as%0Aidentifying%20and%20replacing%20difficult%20terms%20%28Task%202%29.%20For%20automatic%20evaluation%20of%0ATask%201%2C%20we%20developed%20a%20four-fold%20set%20of%20professionally-written%20references.%0ASubmissions%20for%20both%20Tasks%201%20and%202%20were%20provided%20extensive%20manual%20evaluation%0Afrom%20biomedical%20experts.%0A%20%20Results%3A%20Twelve%20teams%20spanning%20twelve%20countries%20participated%20in%20the%20track%2C%0Awith%20models%20from%20multilayer%20perceptrons%20to%20large%20pretrained%20transformers.%20In%0Amanual%20judgments%20of%20Task%201%2C%20top-performing%20models%20rivaled%20human%20levels%20of%0Afactual%20accuracy%20and%20completeness%2C%20but%20not%20simplicity%20or%20brevity.%20Automatic%2C%0Areference-based%20metrics%20generally%20did%20not%20correlate%20well%20with%20manual%20judgments.%0AIn%20Task%202%2C%20systems%20struggled%20with%20identifying%20difficult%20terms%20and%20classifying%0Ahow%20to%20replace%20them.%20When%20generating%20replacements%2C%20however%2C%20LLM-based%20systems%0Adid%20well%20in%20manually%20judged%20accuracy%2C%20completeness%2C%20and%20simplicity%2C%20though%20not%0Ain%20brevity.%0A%20%20Conclusion%3A%20The%20PLABA%20track%20showed%20promise%20for%20using%20Large%20Language%20Models%20to%0Aadapt%20biomedical%20literature%20for%20the%20general%20public%2C%20while%20also%20highlighting%0Atheir%20deficiencies%20and%20the%20need%20for%20improved%20automatic%20benchmarking%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLessons%2520from%2520the%2520TREC%2520Plain%2520Language%2520Adaptation%2520of%2520Biomedical%2520Abstracts%250A%2520%2520%2528PLABA%2529%2520track%26entry.906535625%3DBrian%2520Ondov%2520and%2520William%2520Xia%2520and%2520Kush%2520Attal%2520and%2520Ishita%2520Unde%2520and%2520Jerry%2520He%2520and%2520Hoa%2520Dang%2520and%2520Ian%2520Soboroff%2520and%2520Dina%2520Demner-Fushman%26entry.1292438233%3D%2520%2520Objective%253A%2520Recent%2520advances%2520in%2520language%2520models%2520have%2520shown%2520potential%2520to%2520adapt%250Aprofessional-facing%2520biomedical%2520literature%2520to%2520plain%2520language%252C%2520making%2520it%250Aaccessible%2520to%2520patients%2520and%2520caregivers.%2520However%252C%2520their%2520unpredictability%252C%250Acombined%2520with%2520the%2520high%2520potential%2520for%2520harm%2520in%2520this%2520domain%252C%2520means%2520rigorous%250Aevaluation%2520is%2520necessary.%2520Our%2520goals%2520with%2520this%2520track%2520were%2520to%2520stimulate%2520research%250Aand%2520to%2520provide%2520high-quality%2520evaluation%2520of%2520the%2520most%2520promising%2520systems.%250A%2520%2520Methods%253A%2520We%2520hosted%2520the%2520Plain%2520Language%2520Adaptation%2520of%2520Biomedical%2520Abstracts%250A%2528PLABA%2529%2520track%2520at%2520the%25202023%2520and%25202024%2520Text%2520Retrieval%2520Conferences.%2520Tasks%2520included%250Acomplete%252C%2520sentence-level%252C%2520rewriting%2520of%2520abstracts%2520%2528Task%25201%2529%2520as%2520well%2520as%250Aidentifying%2520and%2520replacing%2520difficult%2520terms%2520%2528Task%25202%2529.%2520For%2520automatic%2520evaluation%2520of%250ATask%25201%252C%2520we%2520developed%2520a%2520four-fold%2520set%2520of%2520professionally-written%2520references.%250ASubmissions%2520for%2520both%2520Tasks%25201%2520and%25202%2520were%2520provided%2520extensive%2520manual%2520evaluation%250Afrom%2520biomedical%2520experts.%250A%2520%2520Results%253A%2520Twelve%2520teams%2520spanning%2520twelve%2520countries%2520participated%2520in%2520the%2520track%252C%250Awith%2520models%2520from%2520multilayer%2520perceptrons%2520to%2520large%2520pretrained%2520transformers.%2520In%250Amanual%2520judgments%2520of%2520Task%25201%252C%2520top-performing%2520models%2520rivaled%2520human%2520levels%2520of%250Afactual%2520accuracy%2520and%2520completeness%252C%2520but%2520not%2520simplicity%2520or%2520brevity.%2520Automatic%252C%250Areference-based%2520metrics%2520generally%2520did%2520not%2520correlate%2520well%2520with%2520manual%2520judgments.%250AIn%2520Task%25202%252C%2520systems%2520struggled%2520with%2520identifying%2520difficult%2520terms%2520and%2520classifying%250Ahow%2520to%2520replace%2520them.%2520When%2520generating%2520replacements%252C%2520however%252C%2520LLM-based%2520systems%250Adid%2520well%2520in%2520manually%2520judged%2520accuracy%252C%2520completeness%252C%2520and%2520simplicity%252C%2520though%2520not%250Ain%2520brevity.%250A%2520%2520Conclusion%253A%2520The%2520PLABA%2520track%2520showed%2520promise%2520for%2520using%2520Large%2520Language%2520Models%2520to%250Aadapt%2520biomedical%2520literature%2520for%2520the%2520general%2520public%252C%2520while%2520also%2520highlighting%250Atheir%2520deficiencies%2520and%2520the%2520need%2520for%2520improved%2520automatic%2520benchmarking%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lessons%20from%20the%20TREC%20Plain%20Language%20Adaptation%20of%20Biomedical%20Abstracts%0A%20%20%28PLABA%29%20track&entry.906535625=Brian%20Ondov%20and%20William%20Xia%20and%20Kush%20Attal%20and%20Ishita%20Unde%20and%20Jerry%20He%20and%20Hoa%20Dang%20and%20Ian%20Soboroff%20and%20Dina%20Demner-Fushman&entry.1292438233=%20%20Objective%3A%20Recent%20advances%20in%20language%20models%20have%20shown%20potential%20to%20adapt%0Aprofessional-facing%20biomedical%20literature%20to%20plain%20language%2C%20making%20it%0Aaccessible%20to%20patients%20and%20caregivers.%20However%2C%20their%20unpredictability%2C%0Acombined%20with%20the%20high%20potential%20for%20harm%20in%20this%20domain%2C%20means%20rigorous%0Aevaluation%20is%20necessary.%20Our%20goals%20with%20this%20track%20were%20to%20stimulate%20research%0Aand%20to%20provide%20high-quality%20evaluation%20of%20the%20most%20promising%20systems.%0A%20%20Methods%3A%20We%20hosted%20the%20Plain%20Language%20Adaptation%20of%20Biomedical%20Abstracts%0A%28PLABA%29%20track%20at%20the%202023%20and%202024%20Text%20Retrieval%20Conferences.%20Tasks%20included%0Acomplete%2C%20sentence-level%2C%20rewriting%20of%20abstracts%20%28Task%201%29%20as%20well%20as%0Aidentifying%20and%20replacing%20difficult%20terms%20%28Task%202%29.%20For%20automatic%20evaluation%20of%0ATask%201%2C%20we%20developed%20a%20four-fold%20set%20of%20professionally-written%20references.%0ASubmissions%20for%20both%20Tasks%201%20and%202%20were%20provided%20extensive%20manual%20evaluation%0Afrom%20biomedical%20experts.%0A%20%20Results%3A%20Twelve%20teams%20spanning%20twelve%20countries%20participated%20in%20the%20track%2C%0Awith%20models%20from%20multilayer%20perceptrons%20to%20large%20pretrained%20transformers.%20In%0Amanual%20judgments%20of%20Task%201%2C%20top-performing%20models%20rivaled%20human%20levels%20of%0Afactual%20accuracy%20and%20completeness%2C%20but%20not%20simplicity%20or%20brevity.%20Automatic%2C%0Areference-based%20metrics%20generally%20did%20not%20correlate%20well%20with%20manual%20judgments.%0AIn%20Task%202%2C%20systems%20struggled%20with%20identifying%20difficult%20terms%20and%20classifying%0Ahow%20to%20replace%20them.%20When%20generating%20replacements%2C%20however%2C%20LLM-based%20systems%0Adid%20well%20in%20manually%20judged%20accuracy%2C%20completeness%2C%20and%20simplicity%2C%20though%20not%0Ain%20brevity.%0A%20%20Conclusion%3A%20The%20PLABA%20track%20showed%20promise%20for%20using%20Large%20Language%20Models%20to%0Aadapt%20biomedical%20literature%20for%20the%20general%20public%2C%20while%20also%20highlighting%0Atheir%20deficiencies%20and%20the%20need%20for%20improved%20automatic%20benchmarking%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14096v1&entry.124074799=Read"},
{"title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data\n  Memorization and Knowledge Injection", "author": "Aleksandr Gashkov and Aleksandr Perevalov and Maria Eltsova and Andreas Both", "abstract": "  Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.\n", "link": "http://arxiv.org/abs/2507.13859v1", "date": "2025-07-18", "relevancy": 1.9064, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4978}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPARQL%20Query%20Generation%20with%20LLMs%3A%20Measuring%20the%20Impact%20of%20Training%20Data%0A%20%20Memorization%20and%20Knowledge%20Injection&body=Title%3A%20SPARQL%20Query%20Generation%20with%20LLMs%3A%20Measuring%20the%20Impact%20of%20Training%20Data%0A%20%20Memorization%20and%20Knowledge%20Injection%0AAuthor%3A%20Aleksandr%20Gashkov%20and%20Aleksandr%20Perevalov%20and%20Maria%20Eltsova%20and%20Andreas%20Both%0AAbstract%3A%20%20%20Nowadays%2C%20the%20importance%20of%20software%20with%20natural-language%20user%20interfaces%0Acannot%20be%20underestimated.%20In%20particular%2C%20in%20Question%20Answering%20%28QA%29%20systems%2C%0Agenerating%20a%20SPARQL%20query%20for%20a%20given%20natural-language%20question%20%28often%20named%0AQuery%20Building%29%20from%20the%20information%20retrieved%20from%20the%20same%20question%20is%20the%0Acentral%20task%20of%20QA%20systems%20working%20over%20Knowledge%20Graphs%20%28KGQA%29.%20Due%20to%20the%0Arise%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20they%20are%20considered%20a%20well-suited%20method%0Ato%20increase%20the%20quality%20of%20the%20question-answering%20functionality%2C%20as%20there%20is%0Astill%20a%20lot%20of%20room%20for%20improvement%2C%20aiming%20for%20enhanced%20quality%20and%0Atrustworthiness.%20However%2C%20LLMs%20are%20trained%20on%20web%20data%2C%20where%20researchers%20have%0Ano%20control%20over%20whether%20the%20benchmark%20or%20the%20knowledge%20graph%20was%20already%0Aincluded%20in%20the%20training%20data.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20that%0Aevaluates%20the%20quality%20of%20LLMs%20by%20generating%20a%20SPARQL%20query%20from%20a%0Anatural-language%20question%20under%20various%20conditions%3A%20%281%29%20zero-shot%20SPARQL%0Ageneration%2C%20%282%29%20with%20knowledge%20injection%2C%20and%20%283%29%20with%20%22anonymized%22%20knowledge%0Ainjection.%20This%20enables%20us%2C%20for%20the%20first%20time%2C%20to%20estimate%20the%20influence%20of%0Athe%20training%20data%20on%20the%20QA%20quality%20improved%20by%20LLMs.%20Ultimately%2C%20this%20will%0Ahelp%20to%20identify%20how%20portable%20a%20method%20is%20or%20whether%20good%20results%20might%20mostly%0Abe%20achieved%20because%20a%20benchmark%20was%20already%20included%20in%20the%20training%20data%20%28cf.%0ALLM%20memorization%29.%20The%20developed%20method%20is%20portable%2C%20robust%2C%20and%20supports%20any%0Aknowledge%20graph%3B%20therefore%2C%20it%20could%20be%20easily%20applied%20to%20any%20KGQA%20or%20LLM%2C%0As.t.%2C%20generating%20consistent%20insights%20into%20the%20actual%20LLM%20capabilities%20is%0Apossible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPARQL%2520Query%2520Generation%2520with%2520LLMs%253A%2520Measuring%2520the%2520Impact%2520of%2520Training%2520Data%250A%2520%2520Memorization%2520and%2520Knowledge%2520Injection%26entry.906535625%3DAleksandr%2520Gashkov%2520and%2520Aleksandr%2520Perevalov%2520and%2520Maria%2520Eltsova%2520and%2520Andreas%2520Both%26entry.1292438233%3D%2520%2520Nowadays%252C%2520the%2520importance%2520of%2520software%2520with%2520natural-language%2520user%2520interfaces%250Acannot%2520be%2520underestimated.%2520In%2520particular%252C%2520in%2520Question%2520Answering%2520%2528QA%2529%2520systems%252C%250Agenerating%2520a%2520SPARQL%2520query%2520for%2520a%2520given%2520natural-language%2520question%2520%2528often%2520named%250AQuery%2520Building%2529%2520from%2520the%2520information%2520retrieved%2520from%2520the%2520same%2520question%2520is%2520the%250Acentral%2520task%2520of%2520QA%2520systems%2520working%2520over%2520Knowledge%2520Graphs%2520%2528KGQA%2529.%2520Due%2520to%2520the%250Arise%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520they%2520are%2520considered%2520a%2520well-suited%2520method%250Ato%2520increase%2520the%2520quality%2520of%2520the%2520question-answering%2520functionality%252C%2520as%2520there%2520is%250Astill%2520a%2520lot%2520of%2520room%2520for%2520improvement%252C%2520aiming%2520for%2520enhanced%2520quality%2520and%250Atrustworthiness.%2520However%252C%2520LLMs%2520are%2520trained%2520on%2520web%2520data%252C%2520where%2520researchers%2520have%250Ano%2520control%2520over%2520whether%2520the%2520benchmark%2520or%2520the%2520knowledge%2520graph%2520was%2520already%250Aincluded%2520in%2520the%2520training%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%250Aevaluates%2520the%2520quality%2520of%2520LLMs%2520by%2520generating%2520a%2520SPARQL%2520query%2520from%2520a%250Anatural-language%2520question%2520under%2520various%2520conditions%253A%2520%25281%2529%2520zero-shot%2520SPARQL%250Ageneration%252C%2520%25282%2529%2520with%2520knowledge%2520injection%252C%2520and%2520%25283%2529%2520with%2520%2522anonymized%2522%2520knowledge%250Ainjection.%2520This%2520enables%2520us%252C%2520for%2520the%2520first%2520time%252C%2520to%2520estimate%2520the%2520influence%2520of%250Athe%2520training%2520data%2520on%2520the%2520QA%2520quality%2520improved%2520by%2520LLMs.%2520Ultimately%252C%2520this%2520will%250Ahelp%2520to%2520identify%2520how%2520portable%2520a%2520method%2520is%2520or%2520whether%2520good%2520results%2520might%2520mostly%250Abe%2520achieved%2520because%2520a%2520benchmark%2520was%2520already%2520included%2520in%2520the%2520training%2520data%2520%2528cf.%250ALLM%2520memorization%2529.%2520The%2520developed%2520method%2520is%2520portable%252C%2520robust%252C%2520and%2520supports%2520any%250Aknowledge%2520graph%253B%2520therefore%252C%2520it%2520could%2520be%2520easily%2520applied%2520to%2520any%2520KGQA%2520or%2520LLM%252C%250As.t.%252C%2520generating%2520consistent%2520insights%2520into%2520the%2520actual%2520LLM%2520capabilities%2520is%250Apossible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPARQL%20Query%20Generation%20with%20LLMs%3A%20Measuring%20the%20Impact%20of%20Training%20Data%0A%20%20Memorization%20and%20Knowledge%20Injection&entry.906535625=Aleksandr%20Gashkov%20and%20Aleksandr%20Perevalov%20and%20Maria%20Eltsova%20and%20Andreas%20Both&entry.1292438233=%20%20Nowadays%2C%20the%20importance%20of%20software%20with%20natural-language%20user%20interfaces%0Acannot%20be%20underestimated.%20In%20particular%2C%20in%20Question%20Answering%20%28QA%29%20systems%2C%0Agenerating%20a%20SPARQL%20query%20for%20a%20given%20natural-language%20question%20%28often%20named%0AQuery%20Building%29%20from%20the%20information%20retrieved%20from%20the%20same%20question%20is%20the%0Acentral%20task%20of%20QA%20systems%20working%20over%20Knowledge%20Graphs%20%28KGQA%29.%20Due%20to%20the%0Arise%20of%20Large%20Language%20Models%20%28LLMs%29%2C%20they%20are%20considered%20a%20well-suited%20method%0Ato%20increase%20the%20quality%20of%20the%20question-answering%20functionality%2C%20as%20there%20is%0Astill%20a%20lot%20of%20room%20for%20improvement%2C%20aiming%20for%20enhanced%20quality%20and%0Atrustworthiness.%20However%2C%20LLMs%20are%20trained%20on%20web%20data%2C%20where%20researchers%20have%0Ano%20control%20over%20whether%20the%20benchmark%20or%20the%20knowledge%20graph%20was%20already%0Aincluded%20in%20the%20training%20data.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20method%20that%0Aevaluates%20the%20quality%20of%20LLMs%20by%20generating%20a%20SPARQL%20query%20from%20a%0Anatural-language%20question%20under%20various%20conditions%3A%20%281%29%20zero-shot%20SPARQL%0Ageneration%2C%20%282%29%20with%20knowledge%20injection%2C%20and%20%283%29%20with%20%22anonymized%22%20knowledge%0Ainjection.%20This%20enables%20us%2C%20for%20the%20first%20time%2C%20to%20estimate%20the%20influence%20of%0Athe%20training%20data%20on%20the%20QA%20quality%20improved%20by%20LLMs.%20Ultimately%2C%20this%20will%0Ahelp%20to%20identify%20how%20portable%20a%20method%20is%20or%20whether%20good%20results%20might%20mostly%0Abe%20achieved%20because%20a%20benchmark%20was%20already%20included%20in%20the%20training%20data%20%28cf.%0ALLM%20memorization%29.%20The%20developed%20method%20is%20portable%2C%20robust%2C%20and%20supports%20any%0Aknowledge%20graph%3B%20therefore%2C%20it%20could%20be%20easily%20applied%20to%20any%20KGQA%20or%20LLM%2C%0As.t.%2C%20generating%20consistent%20insights%20into%20the%20actual%20LLM%20capabilities%20is%0Apossible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13859v1&entry.124074799=Read"},
{"title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical\n  Perspective", "author": "Pankaj Yadav and Vivek Vijay", "abstract": "  Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.\n", "link": "http://arxiv.org/abs/2507.14121v1", "date": "2025-07-18", "relevancy": 1.9039, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4793}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4781}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kolmogorov%20Arnold%20Networks%20%28KANs%29%20for%20Imbalanced%20Data%20--%20An%20Empirical%0A%20%20Perspective&body=Title%3A%20Kolmogorov%20Arnold%20Networks%20%28KANs%29%20for%20Imbalanced%20Data%20--%20An%20Empirical%0A%20%20Perspective%0AAuthor%3A%20Pankaj%20Yadav%20and%20Vivek%20Vijay%0AAbstract%3A%20%20%20Kolmogorov%20Arnold%20Networks%20%28KANs%29%20are%20recent%20architectural%20advancement%20in%0Aneural%20computation%20that%20offer%20a%20mathematically%20grounded%20alternative%20to%20standard%0Aneural%20networks.%20This%20study%20presents%20an%20empirical%20evaluation%20of%20KANs%20in%20context%0Aof%20class%20imbalanced%20classification%2C%20using%20ten%20benchmark%20datasets.%20We%20observe%0Athat%20KANs%20can%20inherently%20perform%20well%20on%20raw%20imbalanced%20data%20more%20effectively%0Athan%20Multi-Layer%20Perceptrons%20%28MLPs%29%20without%20any%20resampling%20strategy.%20However%2C%0Aconventional%20imbalance%20strategies%20fundamentally%20conflict%20with%20KANs%20mathematical%0Astructure%20as%20resampling%20and%20focal%20loss%20implementations%20significantly%20degrade%0AKANs%20performance%2C%20while%20marginally%20benefiting%20MLPs.%20Crucially%2C%20KANs%20suffer%20from%0Aprohibitive%20computational%20costs%20without%20proportional%20performance%20gains.%0AStatistical%20validation%20confirms%20that%20MLPs%20with%20imbalance%20techniques%20achieve%0Aequivalence%20with%20KANs%20%28%7Cd%7C%20%3C%200.08%20across%20metrics%29%20at%20minimal%20resource%20costs.%0AThese%20findings%20reveal%20that%20KANs%20represent%20a%20specialized%20solution%20for%20raw%0Aimbalanced%20data%20where%20resources%20permit.%20But%20their%20severe%20performance-resource%0Atradeoffs%20and%20incompatibility%20with%20standard%20resampling%20techniques%20currently%0Alimits%20practical%20deployment.%20We%20identify%20critical%20research%20priorities%20as%0Adeveloping%20KAN%20specific%20architectural%20modifications%20for%20imbalance%20learning%2C%0Aoptimizing%20computational%20efficiency%2C%20and%20theoretical%20reconciling%20their%20conflict%0Awith%20data%20augmentation.%20This%20work%20establishes%20foundational%20insights%20for%20next%0Ageneration%20KAN%20architectures%20in%20imbalanced%20classification%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKolmogorov%2520Arnold%2520Networks%2520%2528KANs%2529%2520for%2520Imbalanced%2520Data%2520--%2520An%2520Empirical%250A%2520%2520Perspective%26entry.906535625%3DPankaj%2520Yadav%2520and%2520Vivek%2520Vijay%26entry.1292438233%3D%2520%2520Kolmogorov%2520Arnold%2520Networks%2520%2528KANs%2529%2520are%2520recent%2520architectural%2520advancement%2520in%250Aneural%2520computation%2520that%2520offer%2520a%2520mathematically%2520grounded%2520alternative%2520to%2520standard%250Aneural%2520networks.%2520This%2520study%2520presents%2520an%2520empirical%2520evaluation%2520of%2520KANs%2520in%2520context%250Aof%2520class%2520imbalanced%2520classification%252C%2520using%2520ten%2520benchmark%2520datasets.%2520We%2520observe%250Athat%2520KANs%2520can%2520inherently%2520perform%2520well%2520on%2520raw%2520imbalanced%2520data%2520more%2520effectively%250Athan%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520without%2520any%2520resampling%2520strategy.%2520However%252C%250Aconventional%2520imbalance%2520strategies%2520fundamentally%2520conflict%2520with%2520KANs%2520mathematical%250Astructure%2520as%2520resampling%2520and%2520focal%2520loss%2520implementations%2520significantly%2520degrade%250AKANs%2520performance%252C%2520while%2520marginally%2520benefiting%2520MLPs.%2520Crucially%252C%2520KANs%2520suffer%2520from%250Aprohibitive%2520computational%2520costs%2520without%2520proportional%2520performance%2520gains.%250AStatistical%2520validation%2520confirms%2520that%2520MLPs%2520with%2520imbalance%2520techniques%2520achieve%250Aequivalence%2520with%2520KANs%2520%2528%257Cd%257C%2520%253C%25200.08%2520across%2520metrics%2529%2520at%2520minimal%2520resource%2520costs.%250AThese%2520findings%2520reveal%2520that%2520KANs%2520represent%2520a%2520specialized%2520solution%2520for%2520raw%250Aimbalanced%2520data%2520where%2520resources%2520permit.%2520But%2520their%2520severe%2520performance-resource%250Atradeoffs%2520and%2520incompatibility%2520with%2520standard%2520resampling%2520techniques%2520currently%250Alimits%2520practical%2520deployment.%2520We%2520identify%2520critical%2520research%2520priorities%2520as%250Adeveloping%2520KAN%2520specific%2520architectural%2520modifications%2520for%2520imbalance%2520learning%252C%250Aoptimizing%2520computational%2520efficiency%252C%2520and%2520theoretical%2520reconciling%2520their%2520conflict%250Awith%2520data%2520augmentation.%2520This%2520work%2520establishes%2520foundational%2520insights%2520for%2520next%250Ageneration%2520KAN%2520architectures%2520in%2520imbalanced%2520classification%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kolmogorov%20Arnold%20Networks%20%28KANs%29%20for%20Imbalanced%20Data%20--%20An%20Empirical%0A%20%20Perspective&entry.906535625=Pankaj%20Yadav%20and%20Vivek%20Vijay&entry.1292438233=%20%20Kolmogorov%20Arnold%20Networks%20%28KANs%29%20are%20recent%20architectural%20advancement%20in%0Aneural%20computation%20that%20offer%20a%20mathematically%20grounded%20alternative%20to%20standard%0Aneural%20networks.%20This%20study%20presents%20an%20empirical%20evaluation%20of%20KANs%20in%20context%0Aof%20class%20imbalanced%20classification%2C%20using%20ten%20benchmark%20datasets.%20We%20observe%0Athat%20KANs%20can%20inherently%20perform%20well%20on%20raw%20imbalanced%20data%20more%20effectively%0Athan%20Multi-Layer%20Perceptrons%20%28MLPs%29%20without%20any%20resampling%20strategy.%20However%2C%0Aconventional%20imbalance%20strategies%20fundamentally%20conflict%20with%20KANs%20mathematical%0Astructure%20as%20resampling%20and%20focal%20loss%20implementations%20significantly%20degrade%0AKANs%20performance%2C%20while%20marginally%20benefiting%20MLPs.%20Crucially%2C%20KANs%20suffer%20from%0Aprohibitive%20computational%20costs%20without%20proportional%20performance%20gains.%0AStatistical%20validation%20confirms%20that%20MLPs%20with%20imbalance%20techniques%20achieve%0Aequivalence%20with%20KANs%20%28%7Cd%7C%20%3C%200.08%20across%20metrics%29%20at%20minimal%20resource%20costs.%0AThese%20findings%20reveal%20that%20KANs%20represent%20a%20specialized%20solution%20for%20raw%0Aimbalanced%20data%20where%20resources%20permit.%20But%20their%20severe%20performance-resource%0Atradeoffs%20and%20incompatibility%20with%20standard%20resampling%20techniques%20currently%0Alimits%20practical%20deployment.%20We%20identify%20critical%20research%20priorities%20as%0Adeveloping%20KAN%20specific%20architectural%20modifications%20for%20imbalance%20learning%2C%0Aoptimizing%20computational%20efficiency%2C%20and%20theoretical%20reconciling%20their%20conflict%0Awith%20data%20augmentation.%20This%20work%20establishes%20foundational%20insights%20for%20next%0Ageneration%20KAN%20architectures%20in%20imbalanced%20classification%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14121v1&entry.124074799=Read"},
{"title": "Glucose-ML: A collection of longitudinal diabetes datasets for\n  development of robust AI solutions", "author": "Temiloluwa Prioleau and Baiying Lu and Yanjun Cui", "abstract": "  Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.\n", "link": "http://arxiv.org/abs/2507.14077v1", "date": "2025-07-18", "relevancy": 1.3308, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4498}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4433}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glucose-ML%3A%20A%20collection%20of%20longitudinal%20diabetes%20datasets%20for%0A%20%20development%20of%20robust%20AI%20solutions&body=Title%3A%20Glucose-ML%3A%20A%20collection%20of%20longitudinal%20diabetes%20datasets%20for%0A%20%20development%20of%20robust%20AI%20solutions%0AAuthor%3A%20Temiloluwa%20Prioleau%20and%20Baiying%20Lu%20and%20Yanjun%20Cui%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20algorithms%20are%20a%20critical%20part%20of%0Astate-of-the-art%20digital%20health%20technology%20for%20diabetes%20management.%20Yet%2C%20access%0Ato%20large%20high-quality%20datasets%20is%20creating%20barriers%20that%20impede%20development%20of%0Arobust%20AI%20solutions.%20To%20accelerate%20development%20of%20transparent%2C%20reproducible%2C%0Aand%20robust%20AI%20solutions%2C%20we%20present%20Glucose-ML%2C%20a%20collection%20of%2010%20publicly%0Aavailable%20diabetes%20datasets%2C%20released%20within%20the%20last%207%20years%20%28i.e.%2C%202018%20-%0A2025%29.%20The%20Glucose-ML%20collection%20comprises%20over%20300%2C000%20days%20of%20continuous%0Aglucose%20monitor%20%28CGM%29%20data%20with%20a%20total%20of%2038%20million%20glucose%20samples%20collected%0Afrom%202500%2B%20people%20across%204%20countries.%20Participants%20include%20persons%20living%20with%0Atype%201%20diabetes%2C%20type%202%20diabetes%2C%20prediabetes%2C%20and%20no%20diabetes.%20To%20support%0Aresearchers%20and%20innovators%20with%20using%20this%20rich%20collection%20of%20diabetes%0Adatasets%2C%20we%20present%20a%20comparative%20analysis%20to%20guide%20algorithm%20developers%20with%0Adata%20selection.%20Additionally%2C%20we%20conduct%20a%20case%20study%20for%20the%20task%20of%20blood%0Aglucose%20prediction%20-%20one%20of%20the%20most%20common%20AI%20tasks%20within%20the%20field.%20Through%0Athis%20case%20study%2C%20we%20provide%20a%20benchmark%20for%20short-term%20blood%20glucose%20prediction%0Aacross%20all%2010%20publicly%20available%20diabetes%20datasets%20within%20the%20Glucose-ML%0Acollection.%20We%20show%20that%20the%20same%20algorithm%20can%20have%20significantly%20different%0Aprediction%20results%20when%20developed/evaluated%20with%20different%20datasets.%20Findings%0Afrom%20this%20study%20are%20then%20used%20to%20inform%20recommendations%20for%20developing%20robust%0AAI%20solutions%20within%20the%20diabetes%20or%20broader%20health%20domain.%20We%20provide%20direct%0Alinks%20to%20each%20longitudinal%20diabetes%20dataset%20in%20the%20Glucose-ML%20collection%20and%0Aopenly%20provide%20our%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlucose-ML%253A%2520A%2520collection%2520of%2520longitudinal%2520diabetes%2520datasets%2520for%250A%2520%2520development%2520of%2520robust%2520AI%2520solutions%26entry.906535625%3DTemiloluwa%2520Prioleau%2520and%2520Baiying%2520Lu%2520and%2520Yanjun%2520Cui%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520algorithms%2520are%2520a%2520critical%2520part%2520of%250Astate-of-the-art%2520digital%2520health%2520technology%2520for%2520diabetes%2520management.%2520Yet%252C%2520access%250Ato%2520large%2520high-quality%2520datasets%2520is%2520creating%2520barriers%2520that%2520impede%2520development%2520of%250Arobust%2520AI%2520solutions.%2520To%2520accelerate%2520development%2520of%2520transparent%252C%2520reproducible%252C%250Aand%2520robust%2520AI%2520solutions%252C%2520we%2520present%2520Glucose-ML%252C%2520a%2520collection%2520of%252010%2520publicly%250Aavailable%2520diabetes%2520datasets%252C%2520released%2520within%2520the%2520last%25207%2520years%2520%2528i.e.%252C%25202018%2520-%250A2025%2529.%2520The%2520Glucose-ML%2520collection%2520comprises%2520over%2520300%252C000%2520days%2520of%2520continuous%250Aglucose%2520monitor%2520%2528CGM%2529%2520data%2520with%2520a%2520total%2520of%252038%2520million%2520glucose%2520samples%2520collected%250Afrom%25202500%252B%2520people%2520across%25204%2520countries.%2520Participants%2520include%2520persons%2520living%2520with%250Atype%25201%2520diabetes%252C%2520type%25202%2520diabetes%252C%2520prediabetes%252C%2520and%2520no%2520diabetes.%2520To%2520support%250Aresearchers%2520and%2520innovators%2520with%2520using%2520this%2520rich%2520collection%2520of%2520diabetes%250Adatasets%252C%2520we%2520present%2520a%2520comparative%2520analysis%2520to%2520guide%2520algorithm%2520developers%2520with%250Adata%2520selection.%2520Additionally%252C%2520we%2520conduct%2520a%2520case%2520study%2520for%2520the%2520task%2520of%2520blood%250Aglucose%2520prediction%2520-%2520one%2520of%2520the%2520most%2520common%2520AI%2520tasks%2520within%2520the%2520field.%2520Through%250Athis%2520case%2520study%252C%2520we%2520provide%2520a%2520benchmark%2520for%2520short-term%2520blood%2520glucose%2520prediction%250Aacross%2520all%252010%2520publicly%2520available%2520diabetes%2520datasets%2520within%2520the%2520Glucose-ML%250Acollection.%2520We%2520show%2520that%2520the%2520same%2520algorithm%2520can%2520have%2520significantly%2520different%250Aprediction%2520results%2520when%2520developed/evaluated%2520with%2520different%2520datasets.%2520Findings%250Afrom%2520this%2520study%2520are%2520then%2520used%2520to%2520inform%2520recommendations%2520for%2520developing%2520robust%250AAI%2520solutions%2520within%2520the%2520diabetes%2520or%2520broader%2520health%2520domain.%2520We%2520provide%2520direct%250Alinks%2520to%2520each%2520longitudinal%2520diabetes%2520dataset%2520in%2520the%2520Glucose-ML%2520collection%2520and%250Aopenly%2520provide%2520our%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glucose-ML%3A%20A%20collection%20of%20longitudinal%20diabetes%20datasets%20for%0A%20%20development%20of%20robust%20AI%20solutions&entry.906535625=Temiloluwa%20Prioleau%20and%20Baiying%20Lu%20and%20Yanjun%20Cui&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20algorithms%20are%20a%20critical%20part%20of%0Astate-of-the-art%20digital%20health%20technology%20for%20diabetes%20management.%20Yet%2C%20access%0Ato%20large%20high-quality%20datasets%20is%20creating%20barriers%20that%20impede%20development%20of%0Arobust%20AI%20solutions.%20To%20accelerate%20development%20of%20transparent%2C%20reproducible%2C%0Aand%20robust%20AI%20solutions%2C%20we%20present%20Glucose-ML%2C%20a%20collection%20of%2010%20publicly%0Aavailable%20diabetes%20datasets%2C%20released%20within%20the%20last%207%20years%20%28i.e.%2C%202018%20-%0A2025%29.%20The%20Glucose-ML%20collection%20comprises%20over%20300%2C000%20days%20of%20continuous%0Aglucose%20monitor%20%28CGM%29%20data%20with%20a%20total%20of%2038%20million%20glucose%20samples%20collected%0Afrom%202500%2B%20people%20across%204%20countries.%20Participants%20include%20persons%20living%20with%0Atype%201%20diabetes%2C%20type%202%20diabetes%2C%20prediabetes%2C%20and%20no%20diabetes.%20To%20support%0Aresearchers%20and%20innovators%20with%20using%20this%20rich%20collection%20of%20diabetes%0Adatasets%2C%20we%20present%20a%20comparative%20analysis%20to%20guide%20algorithm%20developers%20with%0Adata%20selection.%20Additionally%2C%20we%20conduct%20a%20case%20study%20for%20the%20task%20of%20blood%0Aglucose%20prediction%20-%20one%20of%20the%20most%20common%20AI%20tasks%20within%20the%20field.%20Through%0Athis%20case%20study%2C%20we%20provide%20a%20benchmark%20for%20short-term%20blood%20glucose%20prediction%0Aacross%20all%2010%20publicly%20available%20diabetes%20datasets%20within%20the%20Glucose-ML%0Acollection.%20We%20show%20that%20the%20same%20algorithm%20can%20have%20significantly%20different%0Aprediction%20results%20when%20developed/evaluated%20with%20different%20datasets.%20Findings%0Afrom%20this%20study%20are%20then%20used%20to%20inform%20recommendations%20for%20developing%20robust%0AAI%20solutions%20within%20the%20diabetes%20or%20broader%20health%20domain.%20We%20provide%20direct%0Alinks%20to%20each%20longitudinal%20diabetes%20dataset%20in%20the%20Glucose-ML%20collection%20and%0Aopenly%20provide%20our%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14077v1&entry.124074799=Read"},
{"title": "An Adversarial-Driven Experimental Study on Deep Learning for RF\n  Fingerprinting", "author": "Xinyu Cao and Bimal Adhikari and Shangqing Zhao and Jingxian Wu and Yanjun Pan", "abstract": "  Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.\n", "link": "http://arxiv.org/abs/2507.14109v1", "date": "2025-07-18", "relevancy": 1.3879, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4744}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4599}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adversarial-Driven%20Experimental%20Study%20on%20Deep%20Learning%20for%20RF%0A%20%20Fingerprinting&body=Title%3A%20An%20Adversarial-Driven%20Experimental%20Study%20on%20Deep%20Learning%20for%20RF%0A%20%20Fingerprinting%0AAuthor%3A%20Xinyu%20Cao%20and%20Bimal%20Adhikari%20and%20Shangqing%20Zhao%20and%20Jingxian%20Wu%20and%20Yanjun%20Pan%0AAbstract%3A%20%20%20Radio%20frequency%20%28RF%29%20fingerprinting%2C%20which%20extracts%20unique%20hardware%0Aimperfections%20of%20radio%20devices%2C%20has%20emerged%20as%20a%20promising%20physical-layer%0Adevice%20identification%20mechanism%20in%20zero%20trust%20architectures%20and%20beyond%205G%0Anetworks.%20In%20particular%2C%20deep%20learning%20%28DL%29%20methods%20have%20demonstrated%0Astate-of-the-art%20performance%20in%20this%20domain.%20However%2C%20existing%20approaches%20have%0Aprimarily%20focused%20on%20enhancing%20system%20robustness%20against%20temporal%20and%20spatial%0Avariations%20in%20wireless%20environments%2C%20while%20the%20security%20vulnerabilities%20of%0Athese%20DL-based%20approaches%20have%20often%20been%20overlooked.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20the%20security%20risks%20of%20DL-based%20RF%20fingerprinting%0Asystems%20through%20an%20adversarial-driven%20experimental%20analysis.%20We%20observe%20a%0Aconsistent%20misclassification%20behavior%20for%20DL%20models%20under%20domain%20shifts%2C%20where%0Aa%20device%20is%20frequently%20misclassified%20as%20another%20specific%20one.%20Our%20analysis%0Abased%20on%20extensive%20real-world%20experiments%20demonstrates%20that%20this%20behavior%20can%0Abe%20exploited%20as%20an%20effective%20backdoor%20to%20enable%20external%20attackers%20to%20intrude%0Ainto%20the%20system.%20Furthermore%2C%20we%20show%20that%20training%20DL%20models%20on%20raw%20received%0Asignals%20causes%20the%20models%20to%20entangle%20RF%20fingerprints%20with%20environmental%20and%0Asignal-pattern%20features%2C%20creating%20additional%20attack%20vectors%20that%20cannot%20be%0Amitigated%20solely%20through%20post-processing%20security%20methods%20such%20as%20confidence%0Athresholds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adversarial-Driven%2520Experimental%2520Study%2520on%2520Deep%2520Learning%2520for%2520RF%250A%2520%2520Fingerprinting%26entry.906535625%3DXinyu%2520Cao%2520and%2520Bimal%2520Adhikari%2520and%2520Shangqing%2520Zhao%2520and%2520Jingxian%2520Wu%2520and%2520Yanjun%2520Pan%26entry.1292438233%3D%2520%2520Radio%2520frequency%2520%2528RF%2529%2520fingerprinting%252C%2520which%2520extracts%2520unique%2520hardware%250Aimperfections%2520of%2520radio%2520devices%252C%2520has%2520emerged%2520as%2520a%2520promising%2520physical-layer%250Adevice%2520identification%2520mechanism%2520in%2520zero%2520trust%2520architectures%2520and%2520beyond%25205G%250Anetworks.%2520In%2520particular%252C%2520deep%2520learning%2520%2528DL%2529%2520methods%2520have%2520demonstrated%250Astate-of-the-art%2520performance%2520in%2520this%2520domain.%2520However%252C%2520existing%2520approaches%2520have%250Aprimarily%2520focused%2520on%2520enhancing%2520system%2520robustness%2520against%2520temporal%2520and%2520spatial%250Avariations%2520in%2520wireless%2520environments%252C%2520while%2520the%2520security%2520vulnerabilities%2520of%250Athese%2520DL-based%2520approaches%2520have%2520often%2520been%2520overlooked.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520investigate%2520the%2520security%2520risks%2520of%2520DL-based%2520RF%2520fingerprinting%250Asystems%2520through%2520an%2520adversarial-driven%2520experimental%2520analysis.%2520We%2520observe%2520a%250Aconsistent%2520misclassification%2520behavior%2520for%2520DL%2520models%2520under%2520domain%2520shifts%252C%2520where%250Aa%2520device%2520is%2520frequently%2520misclassified%2520as%2520another%2520specific%2520one.%2520Our%2520analysis%250Abased%2520on%2520extensive%2520real-world%2520experiments%2520demonstrates%2520that%2520this%2520behavior%2520can%250Abe%2520exploited%2520as%2520an%2520effective%2520backdoor%2520to%2520enable%2520external%2520attackers%2520to%2520intrude%250Ainto%2520the%2520system.%2520Furthermore%252C%2520we%2520show%2520that%2520training%2520DL%2520models%2520on%2520raw%2520received%250Asignals%2520causes%2520the%2520models%2520to%2520entangle%2520RF%2520fingerprints%2520with%2520environmental%2520and%250Asignal-pattern%2520features%252C%2520creating%2520additional%2520attack%2520vectors%2520that%2520cannot%2520be%250Amitigated%2520solely%2520through%2520post-processing%2520security%2520methods%2520such%2520as%2520confidence%250Athresholds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adversarial-Driven%20Experimental%20Study%20on%20Deep%20Learning%20for%20RF%0A%20%20Fingerprinting&entry.906535625=Xinyu%20Cao%20and%20Bimal%20Adhikari%20and%20Shangqing%20Zhao%20and%20Jingxian%20Wu%20and%20Yanjun%20Pan&entry.1292438233=%20%20Radio%20frequency%20%28RF%29%20fingerprinting%2C%20which%20extracts%20unique%20hardware%0Aimperfections%20of%20radio%20devices%2C%20has%20emerged%20as%20a%20promising%20physical-layer%0Adevice%20identification%20mechanism%20in%20zero%20trust%20architectures%20and%20beyond%205G%0Anetworks.%20In%20particular%2C%20deep%20learning%20%28DL%29%20methods%20have%20demonstrated%0Astate-of-the-art%20performance%20in%20this%20domain.%20However%2C%20existing%20approaches%20have%0Aprimarily%20focused%20on%20enhancing%20system%20robustness%20against%20temporal%20and%20spatial%0Avariations%20in%20wireless%20environments%2C%20while%20the%20security%20vulnerabilities%20of%0Athese%20DL-based%20approaches%20have%20often%20been%20overlooked.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20the%20security%20risks%20of%20DL-based%20RF%20fingerprinting%0Asystems%20through%20an%20adversarial-driven%20experimental%20analysis.%20We%20observe%20a%0Aconsistent%20misclassification%20behavior%20for%20DL%20models%20under%20domain%20shifts%2C%20where%0Aa%20device%20is%20frequently%20misclassified%20as%20another%20specific%20one.%20Our%20analysis%0Abased%20on%20extensive%20real-world%20experiments%20demonstrates%20that%20this%20behavior%20can%0Abe%20exploited%20as%20an%20effective%20backdoor%20to%20enable%20external%20attackers%20to%20intrude%0Ainto%20the%20system.%20Furthermore%2C%20we%20show%20that%20training%20DL%20models%20on%20raw%20received%0Asignals%20causes%20the%20models%20to%20entangle%20RF%20fingerprints%20with%20environmental%20and%0Asignal-pattern%20features%2C%20creating%20additional%20attack%20vectors%20that%20cannot%20be%0Amitigated%20solely%20through%20post-processing%20security%20methods%20such%20as%20confidence%0Athresholds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14109v1&entry.124074799=Read"},
{"title": "Towards Constraint Temporal Answer Set Programming", "author": "Pedro Cabalar and Mart\u00edn Di\u00e9guez and Fran\u00e7ois Olivier and Torsten Schaub and Igor St\u00e9phan", "abstract": "  Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.\n", "link": "http://arxiv.org/abs/2507.13958v1", "date": "2025-07-18", "relevancy": 1.2423, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.424}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Constraint%20Temporal%20Answer%20Set%20Programming&body=Title%3A%20Towards%20Constraint%20Temporal%20Answer%20Set%20Programming%0AAuthor%3A%20Pedro%20Cabalar%20and%20Mart%C3%ADn%20Di%C3%A9guez%20and%20Fran%C3%A7ois%20Olivier%20and%20Torsten%20Schaub%20and%20Igor%20St%C3%A9phan%0AAbstract%3A%20%20%20Reasoning%20about%20dynamic%20systems%20with%20a%20fine-grained%20temporal%20and%20numeric%0Aresolution%20presents%20significant%20challenges%20for%20logic-based%20approaches%20like%0AAnswer%20Set%20Programming%20%28ASP%29.%20To%20address%20this%2C%20we%20introduce%20and%20elaborate%20upon%0Aa%20novel%20temporal%20and%20constraint-based%20extension%20of%20the%20logic%20of%20Here-and-There%0Aand%20its%20nonmonotonic%20equilibrium%20extension%2C%20representing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20approach%20to%20nonmonotonic%20temporal%20reasoning%20with%0Aconstraints%20specifically%20tailored%20for%20ASP.%20This%20expressive%20system%20is%20achieved%0Aby%20a%20synergistic%20combination%20of%20two%20foundational%20ASP%20extensions%3A%20the%0Alinear-time%20logic%20of%20Here-and-There%2C%20providing%20robust%20nonmonotonic%20temporal%0Areasoning%20capabilities%2C%20and%20the%20logic%20of%20Here-and-There%20with%20constraints%2C%0Aenabling%20the%20direct%20integration%20and%20manipulation%20of%20numeric%20constraints%2C%20among%0Aothers.%20This%20work%20establishes%20the%20foundational%20logical%20framework%20for%20tackling%0Acomplex%20dynamic%20systems%20with%20high%20resolution%20within%20the%20ASP%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Constraint%2520Temporal%2520Answer%2520Set%2520Programming%26entry.906535625%3DPedro%2520Cabalar%2520and%2520Mart%25C3%25ADn%2520Di%25C3%25A9guez%2520and%2520Fran%25C3%25A7ois%2520Olivier%2520and%2520Torsten%2520Schaub%2520and%2520Igor%2520St%25C3%25A9phan%26entry.1292438233%3D%2520%2520Reasoning%2520about%2520dynamic%2520systems%2520with%2520a%2520fine-grained%2520temporal%2520and%2520numeric%250Aresolution%2520presents%2520significant%2520challenges%2520for%2520logic-based%2520approaches%2520like%250AAnswer%2520Set%2520Programming%2520%2528ASP%2529.%2520To%2520address%2520this%252C%2520we%2520introduce%2520and%2520elaborate%2520upon%250Aa%2520novel%2520temporal%2520and%2520constraint-based%2520extension%2520of%2520the%2520logic%2520of%2520Here-and-There%250Aand%2520its%2520nonmonotonic%2520equilibrium%2520extension%252C%2520representing%252C%2520to%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520the%2520first%2520approach%2520to%2520nonmonotonic%2520temporal%2520reasoning%2520with%250Aconstraints%2520specifically%2520tailored%2520for%2520ASP.%2520This%2520expressive%2520system%2520is%2520achieved%250Aby%2520a%2520synergistic%2520combination%2520of%2520two%2520foundational%2520ASP%2520extensions%253A%2520the%250Alinear-time%2520logic%2520of%2520Here-and-There%252C%2520providing%2520robust%2520nonmonotonic%2520temporal%250Areasoning%2520capabilities%252C%2520and%2520the%2520logic%2520of%2520Here-and-There%2520with%2520constraints%252C%250Aenabling%2520the%2520direct%2520integration%2520and%2520manipulation%2520of%2520numeric%2520constraints%252C%2520among%250Aothers.%2520This%2520work%2520establishes%2520the%2520foundational%2520logical%2520framework%2520for%2520tackling%250Acomplex%2520dynamic%2520systems%2520with%2520high%2520resolution%2520within%2520the%2520ASP%2520paradigm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Constraint%20Temporal%20Answer%20Set%20Programming&entry.906535625=Pedro%20Cabalar%20and%20Mart%C3%ADn%20Di%C3%A9guez%20and%20Fran%C3%A7ois%20Olivier%20and%20Torsten%20Schaub%20and%20Igor%20St%C3%A9phan&entry.1292438233=%20%20Reasoning%20about%20dynamic%20systems%20with%20a%20fine-grained%20temporal%20and%20numeric%0Aresolution%20presents%20significant%20challenges%20for%20logic-based%20approaches%20like%0AAnswer%20Set%20Programming%20%28ASP%29.%20To%20address%20this%2C%20we%20introduce%20and%20elaborate%20upon%0Aa%20novel%20temporal%20and%20constraint-based%20extension%20of%20the%20logic%20of%20Here-and-There%0Aand%20its%20nonmonotonic%20equilibrium%20extension%2C%20representing%2C%20to%20the%20best%20of%20our%0Aknowledge%2C%20the%20first%20approach%20to%20nonmonotonic%20temporal%20reasoning%20with%0Aconstraints%20specifically%20tailored%20for%20ASP.%20This%20expressive%20system%20is%20achieved%0Aby%20a%20synergistic%20combination%20of%20two%20foundational%20ASP%20extensions%3A%20the%0Alinear-time%20logic%20of%20Here-and-There%2C%20providing%20robust%20nonmonotonic%20temporal%0Areasoning%20capabilities%2C%20and%20the%20logic%20of%20Here-and-There%20with%20constraints%2C%0Aenabling%20the%20direct%20integration%20and%20manipulation%20of%20numeric%20constraints%2C%20among%0Aothers.%20This%20work%20establishes%20the%20foundational%20logical%20framework%20for%20tackling%0Acomplex%20dynamic%20systems%20with%20high%20resolution%20within%20the%20ASP%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13958v1&entry.124074799=Read"},
{"title": "Computer-Vision-Enabled Worker Video Analysis for Motion Amount\n  Quantification", "author": "Hari Iyer and Neel Macwan and Shenghan Guo and Heejin Jeong", "abstract": "  The performance of physical workers is significantly influenced by the extent\nof their motions. However, monitoring and assessing these motions remains a\nchallenge. Recent advancements have enabled in-situ video analysis for\nreal-time observation of worker behaviors. This paper introduces a novel\nframework for tracking and quantifying upper and lower limb motions, issuing\nalerts when critical thresholds are reached. Using joint position data from\nposture estimation, the framework employs Hotelling's $T^2$ statistic to\nquantify and monitor motion amounts. A significant positive correlation was\nnoted between motion warnings and the overall NASA Task Load Index (TLX)\nworkload rating (\\textit{r} = 0.218, \\textit{p} = 0.0024). A supervised Random\nForest model trained on the collected motion data was benchmarked against\nmultiple datasets including UCF Sports Action and UCF50, and was found to\neffectively generalize across environments, identifying ergonomic risk patterns\nwith accuracies up to 94\\%.\n", "link": "http://arxiv.org/abs/2405.13999v3", "date": "2025-07-18", "relevancy": 1.5546, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5449}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5169}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computer-Vision-Enabled%20Worker%20Video%20Analysis%20for%20Motion%20Amount%0A%20%20Quantification&body=Title%3A%20Computer-Vision-Enabled%20Worker%20Video%20Analysis%20for%20Motion%20Amount%0A%20%20Quantification%0AAuthor%3A%20Hari%20Iyer%20and%20Neel%20Macwan%20and%20Shenghan%20Guo%20and%20Heejin%20Jeong%0AAbstract%3A%20%20%20The%20performance%20of%20physical%20workers%20is%20significantly%20influenced%20by%20the%20extent%0Aof%20their%20motions.%20However%2C%20monitoring%20and%20assessing%20these%20motions%20remains%20a%0Achallenge.%20Recent%20advancements%20have%20enabled%20in-situ%20video%20analysis%20for%0Areal-time%20observation%20of%20worker%20behaviors.%20This%20paper%20introduces%20a%20novel%0Aframework%20for%20tracking%20and%20quantifying%20upper%20and%20lower%20limb%20motions%2C%20issuing%0Aalerts%20when%20critical%20thresholds%20are%20reached.%20Using%20joint%20position%20data%20from%0Aposture%20estimation%2C%20the%20framework%20employs%20Hotelling%27s%20%24T%5E2%24%20statistic%20to%0Aquantify%20and%20monitor%20motion%20amounts.%20A%20significant%20positive%20correlation%20was%0Anoted%20between%20motion%20warnings%20and%20the%20overall%20NASA%20Task%20Load%20Index%20%28TLX%29%0Aworkload%20rating%20%28%5Ctextit%7Br%7D%20%3D%200.218%2C%20%5Ctextit%7Bp%7D%20%3D%200.0024%29.%20A%20supervised%20Random%0AForest%20model%20trained%20on%20the%20collected%20motion%20data%20was%20benchmarked%20against%0Amultiple%20datasets%20including%20UCF%20Sports%20Action%20and%20UCF50%2C%20and%20was%20found%20to%0Aeffectively%20generalize%20across%20environments%2C%20identifying%20ergonomic%20risk%20patterns%0Awith%20accuracies%20up%20to%2094%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13999v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputer-Vision-Enabled%2520Worker%2520Video%2520Analysis%2520for%2520Motion%2520Amount%250A%2520%2520Quantification%26entry.906535625%3DHari%2520Iyer%2520and%2520Neel%2520Macwan%2520and%2520Shenghan%2520Guo%2520and%2520Heejin%2520Jeong%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520physical%2520workers%2520is%2520significantly%2520influenced%2520by%2520the%2520extent%250Aof%2520their%2520motions.%2520However%252C%2520monitoring%2520and%2520assessing%2520these%2520motions%2520remains%2520a%250Achallenge.%2520Recent%2520advancements%2520have%2520enabled%2520in-situ%2520video%2520analysis%2520for%250Areal-time%2520observation%2520of%2520worker%2520behaviors.%2520This%2520paper%2520introduces%2520a%2520novel%250Aframework%2520for%2520tracking%2520and%2520quantifying%2520upper%2520and%2520lower%2520limb%2520motions%252C%2520issuing%250Aalerts%2520when%2520critical%2520thresholds%2520are%2520reached.%2520Using%2520joint%2520position%2520data%2520from%250Aposture%2520estimation%252C%2520the%2520framework%2520employs%2520Hotelling%2527s%2520%2524T%255E2%2524%2520statistic%2520to%250Aquantify%2520and%2520monitor%2520motion%2520amounts.%2520A%2520significant%2520positive%2520correlation%2520was%250Anoted%2520between%2520motion%2520warnings%2520and%2520the%2520overall%2520NASA%2520Task%2520Load%2520Index%2520%2528TLX%2529%250Aworkload%2520rating%2520%2528%255Ctextit%257Br%257D%2520%253D%25200.218%252C%2520%255Ctextit%257Bp%257D%2520%253D%25200.0024%2529.%2520A%2520supervised%2520Random%250AForest%2520model%2520trained%2520on%2520the%2520collected%2520motion%2520data%2520was%2520benchmarked%2520against%250Amultiple%2520datasets%2520including%2520UCF%2520Sports%2520Action%2520and%2520UCF50%252C%2520and%2520was%2520found%2520to%250Aeffectively%2520generalize%2520across%2520environments%252C%2520identifying%2520ergonomic%2520risk%2520patterns%250Awith%2520accuracies%2520up%2520to%252094%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13999v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computer-Vision-Enabled%20Worker%20Video%20Analysis%20for%20Motion%20Amount%0A%20%20Quantification&entry.906535625=Hari%20Iyer%20and%20Neel%20Macwan%20and%20Shenghan%20Guo%20and%20Heejin%20Jeong&entry.1292438233=%20%20The%20performance%20of%20physical%20workers%20is%20significantly%20influenced%20by%20the%20extent%0Aof%20their%20motions.%20However%2C%20monitoring%20and%20assessing%20these%20motions%20remains%20a%0Achallenge.%20Recent%20advancements%20have%20enabled%20in-situ%20video%20analysis%20for%0Areal-time%20observation%20of%20worker%20behaviors.%20This%20paper%20introduces%20a%20novel%0Aframework%20for%20tracking%20and%20quantifying%20upper%20and%20lower%20limb%20motions%2C%20issuing%0Aalerts%20when%20critical%20thresholds%20are%20reached.%20Using%20joint%20position%20data%20from%0Aposture%20estimation%2C%20the%20framework%20employs%20Hotelling%27s%20%24T%5E2%24%20statistic%20to%0Aquantify%20and%20monitor%20motion%20amounts.%20A%20significant%20positive%20correlation%20was%0Anoted%20between%20motion%20warnings%20and%20the%20overall%20NASA%20Task%20Load%20Index%20%28TLX%29%0Aworkload%20rating%20%28%5Ctextit%7Br%7D%20%3D%200.218%2C%20%5Ctextit%7Bp%7D%20%3D%200.0024%29.%20A%20supervised%20Random%0AForest%20model%20trained%20on%20the%20collected%20motion%20data%20was%20benchmarked%20against%0Amultiple%20datasets%20including%20UCF%20Sports%20Action%20and%20UCF50%2C%20and%20was%20found%20to%0Aeffectively%20generalize%20across%20environments%2C%20identifying%20ergonomic%20risk%20patterns%0Awith%20accuracies%20up%20to%2094%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13999v3&entry.124074799=Read"},
{"title": "Uncertainty-Aware Explanations Through Probabilistic Self-Explainable\n  Neural Networks", "author": "Jon Vadillo and Roberto Santana and Jose A. Lozano and Marta Kwiatkowska", "abstract": "  The lack of transparency of Deep Neural Networks continues to be a limitation\nthat severely undermines their reliability and usage in high-stakes\napplications. Promising approaches to overcome such limitations are\nPrototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions\nrely on the similarity between the input at hand and a set of prototypical\nrepresentations of the output classes, offering therefore a deep, yet\ntransparent-by-design, architecture. In this paper, we introduce a\nprobabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point\nestimates for the prototypes with probability distributions over their values.\nThis provides not only a more flexible framework for an end-to-end learning of\nprototypes, but can also capture the explanatory uncertainty of the model,\nwhich is a missing feature in previous approaches. In addition, since the\nprototypes determine both the explanation and the prediction, Prob-PSENNs allow\nus to detect when the model is making uninformed or uncertain predictions, and\nto obtain valid explanations for them. Our experiments demonstrate that\nProb-PSENNs provide more meaningful and robust explanations than their\nnon-probabilistic counterparts, while remaining competitive in terms of\npredictive performance, thus enhancing the explainability and reliability of\nthe models.\n", "link": "http://arxiv.org/abs/2403.13740v3", "date": "2025-07-18", "relevancy": 1.5967, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5301}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Explanations%20Through%20Probabilistic%20Self-Explainable%0A%20%20Neural%20Networks&body=Title%3A%20Uncertainty-Aware%20Explanations%20Through%20Probabilistic%20Self-Explainable%0A%20%20Neural%20Networks%0AAuthor%3A%20Jon%20Vadillo%20and%20Roberto%20Santana%20and%20Jose%20A.%20Lozano%20and%20Marta%20Kwiatkowska%0AAbstract%3A%20%20%20The%20lack%20of%20transparency%20of%20Deep%20Neural%20Networks%20continues%20to%20be%20a%20limitation%0Athat%20severely%20undermines%20their%20reliability%20and%20usage%20in%20high-stakes%0Aapplications.%20Promising%20approaches%20to%20overcome%20such%20limitations%20are%0APrototype-Based%20Self-Explainable%20Neural%20Networks%20%28PSENNs%29%2C%20whose%20predictions%0Arely%20on%20the%20similarity%20between%20the%20input%20at%20hand%20and%20a%20set%20of%20prototypical%0Arepresentations%20of%20the%20output%20classes%2C%20offering%20therefore%20a%20deep%2C%20yet%0Atransparent-by-design%2C%20architecture.%20In%20this%20paper%2C%20we%20introduce%20a%0Aprobabilistic%20reformulation%20of%20PSENNs%2C%20called%20Prob-PSENN%2C%20which%20replaces%20point%0Aestimates%20for%20the%20prototypes%20with%20probability%20distributions%20over%20their%20values.%0AThis%20provides%20not%20only%20a%20more%20flexible%20framework%20for%20an%20end-to-end%20learning%20of%0Aprototypes%2C%20but%20can%20also%20capture%20the%20explanatory%20uncertainty%20of%20the%20model%2C%0Awhich%20is%20a%20missing%20feature%20in%20previous%20approaches.%20In%20addition%2C%20since%20the%0Aprototypes%20determine%20both%20the%20explanation%20and%20the%20prediction%2C%20Prob-PSENNs%20allow%0Aus%20to%20detect%20when%20the%20model%20is%20making%20uninformed%20or%20uncertain%20predictions%2C%20and%0Ato%20obtain%20valid%20explanations%20for%20them.%20Our%20experiments%20demonstrate%20that%0AProb-PSENNs%20provide%20more%20meaningful%20and%20robust%20explanations%20than%20their%0Anon-probabilistic%20counterparts%2C%20while%20remaining%20competitive%20in%20terms%20of%0Apredictive%20performance%2C%20thus%20enhancing%20the%20explainability%20and%20reliability%20of%0Athe%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13740v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Explanations%2520Through%2520Probabilistic%2520Self-Explainable%250A%2520%2520Neural%2520Networks%26entry.906535625%3DJon%2520Vadillo%2520and%2520Roberto%2520Santana%2520and%2520Jose%2520A.%2520Lozano%2520and%2520Marta%2520Kwiatkowska%26entry.1292438233%3D%2520%2520The%2520lack%2520of%2520transparency%2520of%2520Deep%2520Neural%2520Networks%2520continues%2520to%2520be%2520a%2520limitation%250Athat%2520severely%2520undermines%2520their%2520reliability%2520and%2520usage%2520in%2520high-stakes%250Aapplications.%2520Promising%2520approaches%2520to%2520overcome%2520such%2520limitations%2520are%250APrototype-Based%2520Self-Explainable%2520Neural%2520Networks%2520%2528PSENNs%2529%252C%2520whose%2520predictions%250Arely%2520on%2520the%2520similarity%2520between%2520the%2520input%2520at%2520hand%2520and%2520a%2520set%2520of%2520prototypical%250Arepresentations%2520of%2520the%2520output%2520classes%252C%2520offering%2520therefore%2520a%2520deep%252C%2520yet%250Atransparent-by-design%252C%2520architecture.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Aprobabilistic%2520reformulation%2520of%2520PSENNs%252C%2520called%2520Prob-PSENN%252C%2520which%2520replaces%2520point%250Aestimates%2520for%2520the%2520prototypes%2520with%2520probability%2520distributions%2520over%2520their%2520values.%250AThis%2520provides%2520not%2520only%2520a%2520more%2520flexible%2520framework%2520for%2520an%2520end-to-end%2520learning%2520of%250Aprototypes%252C%2520but%2520can%2520also%2520capture%2520the%2520explanatory%2520uncertainty%2520of%2520the%2520model%252C%250Awhich%2520is%2520a%2520missing%2520feature%2520in%2520previous%2520approaches.%2520In%2520addition%252C%2520since%2520the%250Aprototypes%2520determine%2520both%2520the%2520explanation%2520and%2520the%2520prediction%252C%2520Prob-PSENNs%2520allow%250Aus%2520to%2520detect%2520when%2520the%2520model%2520is%2520making%2520uninformed%2520or%2520uncertain%2520predictions%252C%2520and%250Ato%2520obtain%2520valid%2520explanations%2520for%2520them.%2520Our%2520experiments%2520demonstrate%2520that%250AProb-PSENNs%2520provide%2520more%2520meaningful%2520and%2520robust%2520explanations%2520than%2520their%250Anon-probabilistic%2520counterparts%252C%2520while%2520remaining%2520competitive%2520in%2520terms%2520of%250Apredictive%2520performance%252C%2520thus%2520enhancing%2520the%2520explainability%2520and%2520reliability%2520of%250Athe%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13740v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Explanations%20Through%20Probabilistic%20Self-Explainable%0A%20%20Neural%20Networks&entry.906535625=Jon%20Vadillo%20and%20Roberto%20Santana%20and%20Jose%20A.%20Lozano%20and%20Marta%20Kwiatkowska&entry.1292438233=%20%20The%20lack%20of%20transparency%20of%20Deep%20Neural%20Networks%20continues%20to%20be%20a%20limitation%0Athat%20severely%20undermines%20their%20reliability%20and%20usage%20in%20high-stakes%0Aapplications.%20Promising%20approaches%20to%20overcome%20such%20limitations%20are%0APrototype-Based%20Self-Explainable%20Neural%20Networks%20%28PSENNs%29%2C%20whose%20predictions%0Arely%20on%20the%20similarity%20between%20the%20input%20at%20hand%20and%20a%20set%20of%20prototypical%0Arepresentations%20of%20the%20output%20classes%2C%20offering%20therefore%20a%20deep%2C%20yet%0Atransparent-by-design%2C%20architecture.%20In%20this%20paper%2C%20we%20introduce%20a%0Aprobabilistic%20reformulation%20of%20PSENNs%2C%20called%20Prob-PSENN%2C%20which%20replaces%20point%0Aestimates%20for%20the%20prototypes%20with%20probability%20distributions%20over%20their%20values.%0AThis%20provides%20not%20only%20a%20more%20flexible%20framework%20for%20an%20end-to-end%20learning%20of%0Aprototypes%2C%20but%20can%20also%20capture%20the%20explanatory%20uncertainty%20of%20the%20model%2C%0Awhich%20is%20a%20missing%20feature%20in%20previous%20approaches.%20In%20addition%2C%20since%20the%0Aprototypes%20determine%20both%20the%20explanation%20and%20the%20prediction%2C%20Prob-PSENNs%20allow%0Aus%20to%20detect%20when%20the%20model%20is%20making%20uninformed%20or%20uncertain%20predictions%2C%20and%0Ato%20obtain%20valid%20explanations%20for%20them.%20Our%20experiments%20demonstrate%20that%0AProb-PSENNs%20provide%20more%20meaningful%20and%20robust%20explanations%20than%20their%0Anon-probabilistic%20counterparts%2C%20while%20remaining%20competitive%20in%20terms%20of%0Apredictive%20performance%2C%20thus%20enhancing%20the%20explainability%20and%20reliability%20of%0Athe%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13740v3&entry.124074799=Read"},
{"title": "Using LLMs to identify features of personal and professional skills in\n  an open-response situational judgment test", "author": "Cole Walsh and Rodica Ivan and Muhammad Zafar Iqbal and Colleen Robb", "abstract": "  Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.\n", "link": "http://arxiv.org/abs/2507.13881v1", "date": "2025-07-18", "relevancy": 0.9255, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4773}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4598}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20LLMs%20to%20identify%20features%20of%20personal%20and%20professional%20skills%20in%0A%20%20an%20open-response%20situational%20judgment%20test&body=Title%3A%20Using%20LLMs%20to%20identify%20features%20of%20personal%20and%20professional%20skills%20in%0A%20%20an%20open-response%20situational%20judgment%20test%0AAuthor%3A%20Cole%20Walsh%20and%20Rodica%20Ivan%20and%20Muhammad%20Zafar%20Iqbal%20and%20Colleen%20Robb%0AAbstract%3A%20%20%20Academic%20programs%20are%20increasingly%20recognizing%20the%20importance%20of%20personal%20and%0Aprofessional%20skills%20and%20their%20critical%20role%20alongside%20technical%20expertise%20in%0Apreparing%20students%20for%20future%20success%20in%20diverse%20career%20paths.%20With%20this%0Agrowing%20demand%20comes%20the%20need%20for%20scalable%20systems%20to%20measure%2C%20evaluate%2C%20and%0Adevelop%20these%20skills.%20Situational%20Judgment%20Tests%20%28SJTs%29%20offer%20one%20potential%0Aavenue%20for%20measuring%20these%20skills%20in%20a%20standardized%20and%20reliable%20way%2C%20but%0Aopen-response%20SJTs%20have%20traditionally%20relied%20on%20trained%20human%20raters%20for%0Aevaluation%2C%20presenting%20operational%20challenges%20to%20delivering%20SJTs%20at%20scale.%20Past%0Aattempts%20at%20developing%20NLP-based%20scoring%20systems%20for%20SJTs%20have%20fallen%20short%20due%0Ato%20issues%20with%20construct%20validity%20of%20these%20systems.%20In%20this%20article%2C%20we%20explore%0Aa%20novel%20approach%20to%20extracting%20construct-relevant%20features%20from%20SJT%20responses%0Ausing%20large%20language%20models%20%28LLMs%29.%20We%20use%20the%20Casper%20SJT%20to%20demonstrate%20the%0Aefficacy%20of%20this%20approach.%20This%20study%20sets%20the%20foundation%20for%20future%0Adevelopments%20in%20automated%20scoring%20for%20personal%20and%20professional%20skills.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520LLMs%2520to%2520identify%2520features%2520of%2520personal%2520and%2520professional%2520skills%2520in%250A%2520%2520an%2520open-response%2520situational%2520judgment%2520test%26entry.906535625%3DCole%2520Walsh%2520and%2520Rodica%2520Ivan%2520and%2520Muhammad%2520Zafar%2520Iqbal%2520and%2520Colleen%2520Robb%26entry.1292438233%3D%2520%2520Academic%2520programs%2520are%2520increasingly%2520recognizing%2520the%2520importance%2520of%2520personal%2520and%250Aprofessional%2520skills%2520and%2520their%2520critical%2520role%2520alongside%2520technical%2520expertise%2520in%250Apreparing%2520students%2520for%2520future%2520success%2520in%2520diverse%2520career%2520paths.%2520With%2520this%250Agrowing%2520demand%2520comes%2520the%2520need%2520for%2520scalable%2520systems%2520to%2520measure%252C%2520evaluate%252C%2520and%250Adevelop%2520these%2520skills.%2520Situational%2520Judgment%2520Tests%2520%2528SJTs%2529%2520offer%2520one%2520potential%250Aavenue%2520for%2520measuring%2520these%2520skills%2520in%2520a%2520standardized%2520and%2520reliable%2520way%252C%2520but%250Aopen-response%2520SJTs%2520have%2520traditionally%2520relied%2520on%2520trained%2520human%2520raters%2520for%250Aevaluation%252C%2520presenting%2520operational%2520challenges%2520to%2520delivering%2520SJTs%2520at%2520scale.%2520Past%250Aattempts%2520at%2520developing%2520NLP-based%2520scoring%2520systems%2520for%2520SJTs%2520have%2520fallen%2520short%2520due%250Ato%2520issues%2520with%2520construct%2520validity%2520of%2520these%2520systems.%2520In%2520this%2520article%252C%2520we%2520explore%250Aa%2520novel%2520approach%2520to%2520extracting%2520construct-relevant%2520features%2520from%2520SJT%2520responses%250Ausing%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520use%2520the%2520Casper%2520SJT%2520to%2520demonstrate%2520the%250Aefficacy%2520of%2520this%2520approach.%2520This%2520study%2520sets%2520the%2520foundation%2520for%2520future%250Adevelopments%2520in%2520automated%2520scoring%2520for%2520personal%2520and%2520professional%2520skills.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20LLMs%20to%20identify%20features%20of%20personal%20and%20professional%20skills%20in%0A%20%20an%20open-response%20situational%20judgment%20test&entry.906535625=Cole%20Walsh%20and%20Rodica%20Ivan%20and%20Muhammad%20Zafar%20Iqbal%20and%20Colleen%20Robb&entry.1292438233=%20%20Academic%20programs%20are%20increasingly%20recognizing%20the%20importance%20of%20personal%20and%0Aprofessional%20skills%20and%20their%20critical%20role%20alongside%20technical%20expertise%20in%0Apreparing%20students%20for%20future%20success%20in%20diverse%20career%20paths.%20With%20this%0Agrowing%20demand%20comes%20the%20need%20for%20scalable%20systems%20to%20measure%2C%20evaluate%2C%20and%0Adevelop%20these%20skills.%20Situational%20Judgment%20Tests%20%28SJTs%29%20offer%20one%20potential%0Aavenue%20for%20measuring%20these%20skills%20in%20a%20standardized%20and%20reliable%20way%2C%20but%0Aopen-response%20SJTs%20have%20traditionally%20relied%20on%20trained%20human%20raters%20for%0Aevaluation%2C%20presenting%20operational%20challenges%20to%20delivering%20SJTs%20at%20scale.%20Past%0Aattempts%20at%20developing%20NLP-based%20scoring%20systems%20for%20SJTs%20have%20fallen%20short%20due%0Ato%20issues%20with%20construct%20validity%20of%20these%20systems.%20In%20this%20article%2C%20we%20explore%0Aa%20novel%20approach%20to%20extracting%20construct-relevant%20features%20from%20SJT%20responses%0Ausing%20large%20language%20models%20%28LLMs%29.%20We%20use%20the%20Casper%20SJT%20to%20demonstrate%20the%0Aefficacy%20of%20this%20approach.%20This%20study%20sets%20the%20foundation%20for%20future%0Adevelopments%20in%20automated%20scoring%20for%20personal%20and%20professional%20skills.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13881v1&entry.124074799=Read"},
{"title": "Design Analysis of an Innovative Parallel Robot for Minimally Invasive\n  Pancreatic Surgery", "author": "Doina Pisla and Alexandru Pusca and Andrei Caprariu and Adrian Pisla and Bogdan Gherman and Calin Vaida and Damien Chablat", "abstract": "  This paper focuses on the design of a parallel robot designed for robotic\nassisted minimally invasive pancreatic surgery. Two alternative architectures,\ncalled ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are\nproposed. Their kinematic schemes are presented, and the conceptual 3D CAD\nmodels are illustrated. Based on these, two Finite Element Method (FEM)\nsimulations were performed to determine which architecture has the higher\nstiffness. A workspace quantitative analysis is performed to further assess the\nusability of the two proposed parallel architectures related to the medical\ntasks. The obtained results are used to select the architecture which fit the\nrequired design criteria and will be used to develop the experimental model of\nthe surgical robot.\n", "link": "http://arxiv.org/abs/2507.13787v1", "date": "2025-07-18", "relevancy": 1.4838, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5032}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4991}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4687}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%20Analysis%20of%20an%20Innovative%20Parallel%20Robot%20for%20Minimally%20Invasive%0A%20%20Pancreatic%20Surgery&body=Title%3A%20Design%20Analysis%20of%20an%20Innovative%20Parallel%20Robot%20for%20Minimally%20Invasive%0A%20%20Pancreatic%20Surgery%0AAuthor%3A%20Doina%20Pisla%20and%20Alexandru%20Pusca%20and%20Andrei%20Caprariu%20and%20Adrian%20Pisla%20and%20Bogdan%20Gherman%20and%20Calin%20Vaida%20and%20Damien%20Chablat%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20the%20design%20of%20a%20parallel%20robot%20designed%20for%20robotic%0Aassisted%20minimally%20invasive%20pancreatic%20surgery.%20Two%20alternative%20architectures%2C%0Acalled%20ATHENA-1%20and%20ATHENA-2%2C%20each%20with%204%20degrees%20of%20freedom%20%28DOF%29%20are%0Aproposed.%20Their%20kinematic%20schemes%20are%20presented%2C%20and%20the%20conceptual%203D%20CAD%0Amodels%20are%20illustrated.%20Based%20on%20these%2C%20two%20Finite%20Element%20Method%20%28FEM%29%0Asimulations%20were%20performed%20to%20determine%20which%20architecture%20has%20the%20higher%0Astiffness.%20A%20workspace%20quantitative%20analysis%20is%20performed%20to%20further%20assess%20the%0Ausability%20of%20the%20two%20proposed%20parallel%20architectures%20related%20to%20the%20medical%0Atasks.%20The%20obtained%20results%20are%20used%20to%20select%20the%20architecture%20which%20fit%20the%0Arequired%20design%20criteria%20and%20will%20be%20used%20to%20develop%20the%20experimental%20model%20of%0Athe%20surgical%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%2520Analysis%2520of%2520an%2520Innovative%2520Parallel%2520Robot%2520for%2520Minimally%2520Invasive%250A%2520%2520Pancreatic%2520Surgery%26entry.906535625%3DDoina%2520Pisla%2520and%2520Alexandru%2520Pusca%2520and%2520Andrei%2520Caprariu%2520and%2520Adrian%2520Pisla%2520and%2520Bogdan%2520Gherman%2520and%2520Calin%2520Vaida%2520and%2520Damien%2520Chablat%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520the%2520design%2520of%2520a%2520parallel%2520robot%2520designed%2520for%2520robotic%250Aassisted%2520minimally%2520invasive%2520pancreatic%2520surgery.%2520Two%2520alternative%2520architectures%252C%250Acalled%2520ATHENA-1%2520and%2520ATHENA-2%252C%2520each%2520with%25204%2520degrees%2520of%2520freedom%2520%2528DOF%2529%2520are%250Aproposed.%2520Their%2520kinematic%2520schemes%2520are%2520presented%252C%2520and%2520the%2520conceptual%25203D%2520CAD%250Amodels%2520are%2520illustrated.%2520Based%2520on%2520these%252C%2520two%2520Finite%2520Element%2520Method%2520%2528FEM%2529%250Asimulations%2520were%2520performed%2520to%2520determine%2520which%2520architecture%2520has%2520the%2520higher%250Astiffness.%2520A%2520workspace%2520quantitative%2520analysis%2520is%2520performed%2520to%2520further%2520assess%2520the%250Ausability%2520of%2520the%2520two%2520proposed%2520parallel%2520architectures%2520related%2520to%2520the%2520medical%250Atasks.%2520The%2520obtained%2520results%2520are%2520used%2520to%2520select%2520the%2520architecture%2520which%2520fit%2520the%250Arequired%2520design%2520criteria%2520and%2520will%2520be%2520used%2520to%2520develop%2520the%2520experimental%2520model%2520of%250Athe%2520surgical%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20Analysis%20of%20an%20Innovative%20Parallel%20Robot%20for%20Minimally%20Invasive%0A%20%20Pancreatic%20Surgery&entry.906535625=Doina%20Pisla%20and%20Alexandru%20Pusca%20and%20Andrei%20Caprariu%20and%20Adrian%20Pisla%20and%20Bogdan%20Gherman%20and%20Calin%20Vaida%20and%20Damien%20Chablat&entry.1292438233=%20%20This%20paper%20focuses%20on%20the%20design%20of%20a%20parallel%20robot%20designed%20for%20robotic%0Aassisted%20minimally%20invasive%20pancreatic%20surgery.%20Two%20alternative%20architectures%2C%0Acalled%20ATHENA-1%20and%20ATHENA-2%2C%20each%20with%204%20degrees%20of%20freedom%20%28DOF%29%20are%0Aproposed.%20Their%20kinematic%20schemes%20are%20presented%2C%20and%20the%20conceptual%203D%20CAD%0Amodels%20are%20illustrated.%20Based%20on%20these%2C%20two%20Finite%20Element%20Method%20%28FEM%29%0Asimulations%20were%20performed%20to%20determine%20which%20architecture%20has%20the%20higher%0Astiffness.%20A%20workspace%20quantitative%20analysis%20is%20performed%20to%20further%20assess%20the%0Ausability%20of%20the%20two%20proposed%20parallel%20architectures%20related%20to%20the%20medical%0Atasks.%20The%20obtained%20results%20are%20used%20to%20select%20the%20architecture%20which%20fit%20the%0Arequired%20design%20criteria%20and%20will%20be%20used%20to%20develop%20the%20experimental%20model%20of%0Athe%20surgical%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13787v1&entry.124074799=Read"},
{"title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph", "author": "Aditi Anand and Suman Banerjee and Dildar Ali", "abstract": "  In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.\n", "link": "http://arxiv.org/abs/2507.13834v1", "date": "2025-07-18", "relevancy": 1.4184, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.471}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Submodular%20Policy%20Optimization%20via%20Pruned%20Submodularity%20Graph&body=Title%3A%20Scalable%20Submodular%20Policy%20Optimization%20via%20Pruned%20Submodularity%20Graph%0AAuthor%3A%20Aditi%20Anand%20and%20Suman%20Banerjee%20and%20Dildar%20Ali%0AAbstract%3A%20%20%20In%20Reinforcement%20Learning%20%28abbreviated%20as%20RL%29%2C%20an%20agent%20interacts%20with%20the%0Aenvironment%20via%20a%20set%20of%20possible%20actions%2C%20and%20a%20reward%20is%20generated%20from%20some%0Aunknown%20distribution.%20The%20task%20here%20is%20to%20find%20an%20optimal%20set%20of%20actions%20such%0Athat%20the%20reward%20after%20a%20certain%20time%20step%20gets%20maximized.%20In%20a%20traditional%0Asetup%2C%20the%20reward%20function%20in%20an%20RL%20Problem%20is%20considered%20additive.%20However%2C%20in%0Areality%2C%20there%20exist%20many%20problems%2C%20including%20path%20planning%2C%20coverage%20control%2C%0Aetc.%2C%20the%20reward%20function%20follows%20the%20diminishing%20return%2C%20which%20can%20be%20modeled%0Aas%20a%20submodular%20function.%20In%20this%20paper%2C%20we%20study%20a%20variant%20of%20the%20RL%20Problem%0Awhere%20the%20reward%20function%20is%20submodular%2C%20and%20our%20objective%20is%20to%20find%20an%0Aoptimal%20policy%20such%20that%20this%20reward%20function%20gets%20maximized.%20We%20have%20proposed%0Aa%20pruned%20submodularity%20graph-based%20approach%20that%20provides%20a%20provably%0Aapproximate%20solution%20in%20a%20feasible%20computation%20time.%20The%20proposed%20approach%20has%0Abeen%20analyzed%20to%20understand%20its%20time%20and%20space%20requirements%20as%20well%20as%20a%0Aperformance%20guarantee.%20We%20have%20experimented%20with%20a%20benchmark%20agent-environment%0Asetup%2C%20which%20has%20been%20used%20for%20similar%20previous%20studies%2C%20and%20the%20results%20are%0Areported.%20From%20the%20results%2C%20we%20observe%20that%20the%20policy%20obtained%20by%20our%20proposed%0Aapproach%20leads%20to%20more%20reward%20than%20the%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Submodular%2520Policy%2520Optimization%2520via%2520Pruned%2520Submodularity%2520Graph%26entry.906535625%3DAditi%2520Anand%2520and%2520Suman%2520Banerjee%2520and%2520Dildar%2520Ali%26entry.1292438233%3D%2520%2520In%2520Reinforcement%2520Learning%2520%2528abbreviated%2520as%2520RL%2529%252C%2520an%2520agent%2520interacts%2520with%2520the%250Aenvironment%2520via%2520a%2520set%2520of%2520possible%2520actions%252C%2520and%2520a%2520reward%2520is%2520generated%2520from%2520some%250Aunknown%2520distribution.%2520The%2520task%2520here%2520is%2520to%2520find%2520an%2520optimal%2520set%2520of%2520actions%2520such%250Athat%2520the%2520reward%2520after%2520a%2520certain%2520time%2520step%2520gets%2520maximized.%2520In%2520a%2520traditional%250Asetup%252C%2520the%2520reward%2520function%2520in%2520an%2520RL%2520Problem%2520is%2520considered%2520additive.%2520However%252C%2520in%250Areality%252C%2520there%2520exist%2520many%2520problems%252C%2520including%2520path%2520planning%252C%2520coverage%2520control%252C%250Aetc.%252C%2520the%2520reward%2520function%2520follows%2520the%2520diminishing%2520return%252C%2520which%2520can%2520be%2520modeled%250Aas%2520a%2520submodular%2520function.%2520In%2520this%2520paper%252C%2520we%2520study%2520a%2520variant%2520of%2520the%2520RL%2520Problem%250Awhere%2520the%2520reward%2520function%2520is%2520submodular%252C%2520and%2520our%2520objective%2520is%2520to%2520find%2520an%250Aoptimal%2520policy%2520such%2520that%2520this%2520reward%2520function%2520gets%2520maximized.%2520We%2520have%2520proposed%250Aa%2520pruned%2520submodularity%2520graph-based%2520approach%2520that%2520provides%2520a%2520provably%250Aapproximate%2520solution%2520in%2520a%2520feasible%2520computation%2520time.%2520The%2520proposed%2520approach%2520has%250Abeen%2520analyzed%2520to%2520understand%2520its%2520time%2520and%2520space%2520requirements%2520as%2520well%2520as%2520a%250Aperformance%2520guarantee.%2520We%2520have%2520experimented%2520with%2520a%2520benchmark%2520agent-environment%250Asetup%252C%2520which%2520has%2520been%2520used%2520for%2520similar%2520previous%2520studies%252C%2520and%2520the%2520results%2520are%250Areported.%2520From%2520the%2520results%252C%2520we%2520observe%2520that%2520the%2520policy%2520obtained%2520by%2520our%2520proposed%250Aapproach%2520leads%2520to%2520more%2520reward%2520than%2520the%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Submodular%20Policy%20Optimization%20via%20Pruned%20Submodularity%20Graph&entry.906535625=Aditi%20Anand%20and%20Suman%20Banerjee%20and%20Dildar%20Ali&entry.1292438233=%20%20In%20Reinforcement%20Learning%20%28abbreviated%20as%20RL%29%2C%20an%20agent%20interacts%20with%20the%0Aenvironment%20via%20a%20set%20of%20possible%20actions%2C%20and%20a%20reward%20is%20generated%20from%20some%0Aunknown%20distribution.%20The%20task%20here%20is%20to%20find%20an%20optimal%20set%20of%20actions%20such%0Athat%20the%20reward%20after%20a%20certain%20time%20step%20gets%20maximized.%20In%20a%20traditional%0Asetup%2C%20the%20reward%20function%20in%20an%20RL%20Problem%20is%20considered%20additive.%20However%2C%20in%0Areality%2C%20there%20exist%20many%20problems%2C%20including%20path%20planning%2C%20coverage%20control%2C%0Aetc.%2C%20the%20reward%20function%20follows%20the%20diminishing%20return%2C%20which%20can%20be%20modeled%0Aas%20a%20submodular%20function.%20In%20this%20paper%2C%20we%20study%20a%20variant%20of%20the%20RL%20Problem%0Awhere%20the%20reward%20function%20is%20submodular%2C%20and%20our%20objective%20is%20to%20find%20an%0Aoptimal%20policy%20such%20that%20this%20reward%20function%20gets%20maximized.%20We%20have%20proposed%0Aa%20pruned%20submodularity%20graph-based%20approach%20that%20provides%20a%20provably%0Aapproximate%20solution%20in%20a%20feasible%20computation%20time.%20The%20proposed%20approach%20has%0Abeen%20analyzed%20to%20understand%20its%20time%20and%20space%20requirements%20as%20well%20as%20a%0Aperformance%20guarantee.%20We%20have%20experimented%20with%20a%20benchmark%20agent-environment%0Asetup%2C%20which%20has%20been%20used%20for%20similar%20previous%20studies%2C%20and%20the%20results%20are%0Areported.%20From%20the%20results%2C%20we%20observe%20that%20the%20policy%20obtained%20by%20our%20proposed%0Aapproach%20leads%20to%20more%20reward%20than%20the%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13834v1&entry.124074799=Read"},
{"title": "Edge Intelligence with Spiking Neural Networks", "author": "Shuiguang Deng and Di Yu and Changze Lv and Xin Du and Linshan Jiang and Xiaofan Zhao and Wentao Tong and Xiaoqing Zheng and Weijia Fang and Peng Zhao and Gang Pan and Schahram Dustdar and Albert Y. Zomaya", "abstract": "  The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.\n", "link": "http://arxiv.org/abs/2507.14069v1", "date": "2025-07-18", "relevancy": 1.4456, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5163}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20Intelligence%20with%20Spiking%20Neural%20Networks&body=Title%3A%20Edge%20Intelligence%20with%20Spiking%20Neural%20Networks%0AAuthor%3A%20Shuiguang%20Deng%20and%20Di%20Yu%20and%20Changze%20Lv%20and%20Xin%20Du%20and%20Linshan%20Jiang%20and%20Xiaofan%20Zhao%20and%20Wentao%20Tong%20and%20Xiaoqing%20Zheng%20and%20Weijia%20Fang%20and%20Peng%20Zhao%20and%20Gang%20Pan%20and%20Schahram%20Dustdar%20and%20Albert%20Y.%20Zomaya%0AAbstract%3A%20%20%20The%20convergence%20of%20artificial%20intelligence%20and%20edge%20computing%20has%20spurred%0Agrowing%20interest%20in%20enabling%20intelligent%20services%20directly%20on%0Aresource-constrained%20devices.%20While%20traditional%20deep%20learning%20models%20require%0Asignificant%20computational%20resources%20and%20centralized%20data%20management%2C%20the%0Aresulting%20latency%2C%20bandwidth%20consumption%2C%20and%20privacy%20concerns%20have%20exposed%0Acritical%20limitations%20in%20cloud-centric%20paradigms.%20Brain-inspired%20computing%2C%0Aparticularly%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20offers%20a%20promising%20alternative%20by%0Aemulating%20biological%20neuronal%20dynamics%20to%20achieve%20low-power%2C%20event-driven%0Acomputation.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20Edge%20Intelligence%0Abased%20on%20SNNs%20%28EdgeSNNs%29%2C%20examining%20their%20potential%20to%20address%20the%20challenges%0Aof%20on-device%20learning%2C%20inference%2C%20and%20security%20in%20edge%20scenarios.%20We%20present%20a%0Asystematic%20taxonomy%20of%20EdgeSNN%20foundations%2C%20encompassing%20neuron%20models%2C%0Alearning%20algorithms%2C%20and%20supporting%20hardware%20platforms.%20Three%20representative%0Apractical%20considerations%20of%20EdgeSNN%20are%20discussed%20in%20depth%3A%20on-device%20inference%0Ausing%20lightweight%20SNN%20models%2C%20resource-aware%20training%20and%20updating%20under%0Anon-stationary%20data%20conditions%2C%20and%20secure%20and%20privacy-preserving%20issues.%0AFurthermore%2C%20we%20highlight%20the%20limitations%20of%20evaluating%20EdgeSNNs%20on%0Aconventional%20hardware%20and%20introduce%20a%20dual-track%20benchmarking%20strategy%20to%0Asupport%20fair%20comparisons%20and%20hardware-aware%20optimization.%20Through%20this%20study%2C%0Awe%20aim%20to%20bridge%20the%20gap%20between%20brain-inspired%20learning%20and%20practical%20edge%0Adeployment%2C%20offering%20insights%20into%20current%20advancements%2C%20open%20challenges%2C%20and%0Afuture%20research%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Adedicated%20and%20comprehensive%20survey%20on%20EdgeSNNs%2C%20providing%20an%20essential%0Areference%20for%20researchers%20and%20practitioners%20working%20at%20the%20intersection%20of%0Aneuromorphic%20computing%20and%20edge%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520Intelligence%2520with%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DShuiguang%2520Deng%2520and%2520Di%2520Yu%2520and%2520Changze%2520Lv%2520and%2520Xin%2520Du%2520and%2520Linshan%2520Jiang%2520and%2520Xiaofan%2520Zhao%2520and%2520Wentao%2520Tong%2520and%2520Xiaoqing%2520Zheng%2520and%2520Weijia%2520Fang%2520and%2520Peng%2520Zhao%2520and%2520Gang%2520Pan%2520and%2520Schahram%2520Dustdar%2520and%2520Albert%2520Y.%2520Zomaya%26entry.1292438233%3D%2520%2520The%2520convergence%2520of%2520artificial%2520intelligence%2520and%2520edge%2520computing%2520has%2520spurred%250Agrowing%2520interest%2520in%2520enabling%2520intelligent%2520services%2520directly%2520on%250Aresource-constrained%2520devices.%2520While%2520traditional%2520deep%2520learning%2520models%2520require%250Asignificant%2520computational%2520resources%2520and%2520centralized%2520data%2520management%252C%2520the%250Aresulting%2520latency%252C%2520bandwidth%2520consumption%252C%2520and%2520privacy%2520concerns%2520have%2520exposed%250Acritical%2520limitations%2520in%2520cloud-centric%2520paradigms.%2520Brain-inspired%2520computing%252C%250Aparticularly%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%252C%2520offers%2520a%2520promising%2520alternative%2520by%250Aemulating%2520biological%2520neuronal%2520dynamics%2520to%2520achieve%2520low-power%252C%2520event-driven%250Acomputation.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520overview%2520of%2520Edge%2520Intelligence%250Abased%2520on%2520SNNs%2520%2528EdgeSNNs%2529%252C%2520examining%2520their%2520potential%2520to%2520address%2520the%2520challenges%250Aof%2520on-device%2520learning%252C%2520inference%252C%2520and%2520security%2520in%2520edge%2520scenarios.%2520We%2520present%2520a%250Asystematic%2520taxonomy%2520of%2520EdgeSNN%2520foundations%252C%2520encompassing%2520neuron%2520models%252C%250Alearning%2520algorithms%252C%2520and%2520supporting%2520hardware%2520platforms.%2520Three%2520representative%250Apractical%2520considerations%2520of%2520EdgeSNN%2520are%2520discussed%2520in%2520depth%253A%2520on-device%2520inference%250Ausing%2520lightweight%2520SNN%2520models%252C%2520resource-aware%2520training%2520and%2520updating%2520under%250Anon-stationary%2520data%2520conditions%252C%2520and%2520secure%2520and%2520privacy-preserving%2520issues.%250AFurthermore%252C%2520we%2520highlight%2520the%2520limitations%2520of%2520evaluating%2520EdgeSNNs%2520on%250Aconventional%2520hardware%2520and%2520introduce%2520a%2520dual-track%2520benchmarking%2520strategy%2520to%250Asupport%2520fair%2520comparisons%2520and%2520hardware-aware%2520optimization.%2520Through%2520this%2520study%252C%250Awe%2520aim%2520to%2520bridge%2520the%2520gap%2520between%2520brain-inspired%2520learning%2520and%2520practical%2520edge%250Adeployment%252C%2520offering%2520insights%2520into%2520current%2520advancements%252C%2520open%2520challenges%252C%2520and%250Afuture%2520research%2520directions.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Adedicated%2520and%2520comprehensive%2520survey%2520on%2520EdgeSNNs%252C%2520providing%2520an%2520essential%250Areference%2520for%2520researchers%2520and%2520practitioners%2520working%2520at%2520the%2520intersection%2520of%250Aneuromorphic%2520computing%2520and%2520edge%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Intelligence%20with%20Spiking%20Neural%20Networks&entry.906535625=Shuiguang%20Deng%20and%20Di%20Yu%20and%20Changze%20Lv%20and%20Xin%20Du%20and%20Linshan%20Jiang%20and%20Xiaofan%20Zhao%20and%20Wentao%20Tong%20and%20Xiaoqing%20Zheng%20and%20Weijia%20Fang%20and%20Peng%20Zhao%20and%20Gang%20Pan%20and%20Schahram%20Dustdar%20and%20Albert%20Y.%20Zomaya&entry.1292438233=%20%20The%20convergence%20of%20artificial%20intelligence%20and%20edge%20computing%20has%20spurred%0Agrowing%20interest%20in%20enabling%20intelligent%20services%20directly%20on%0Aresource-constrained%20devices.%20While%20traditional%20deep%20learning%20models%20require%0Asignificant%20computational%20resources%20and%20centralized%20data%20management%2C%20the%0Aresulting%20latency%2C%20bandwidth%20consumption%2C%20and%20privacy%20concerns%20have%20exposed%0Acritical%20limitations%20in%20cloud-centric%20paradigms.%20Brain-inspired%20computing%2C%0Aparticularly%20Spiking%20Neural%20Networks%20%28SNNs%29%2C%20offers%20a%20promising%20alternative%20by%0Aemulating%20biological%20neuronal%20dynamics%20to%20achieve%20low-power%2C%20event-driven%0Acomputation.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20Edge%20Intelligence%0Abased%20on%20SNNs%20%28EdgeSNNs%29%2C%20examining%20their%20potential%20to%20address%20the%20challenges%0Aof%20on-device%20learning%2C%20inference%2C%20and%20security%20in%20edge%20scenarios.%20We%20present%20a%0Asystematic%20taxonomy%20of%20EdgeSNN%20foundations%2C%20encompassing%20neuron%20models%2C%0Alearning%20algorithms%2C%20and%20supporting%20hardware%20platforms.%20Three%20representative%0Apractical%20considerations%20of%20EdgeSNN%20are%20discussed%20in%20depth%3A%20on-device%20inference%0Ausing%20lightweight%20SNN%20models%2C%20resource-aware%20training%20and%20updating%20under%0Anon-stationary%20data%20conditions%2C%20and%20secure%20and%20privacy-preserving%20issues.%0AFurthermore%2C%20we%20highlight%20the%20limitations%20of%20evaluating%20EdgeSNNs%20on%0Aconventional%20hardware%20and%20introduce%20a%20dual-track%20benchmarking%20strategy%20to%0Asupport%20fair%20comparisons%20and%20hardware-aware%20optimization.%20Through%20this%20study%2C%0Awe%20aim%20to%20bridge%20the%20gap%20between%20brain-inspired%20learning%20and%20practical%20edge%0Adeployment%2C%20offering%20insights%20into%20current%20advancements%2C%20open%20challenges%2C%20and%0Afuture%20research%20directions.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Adedicated%20and%20comprehensive%20survey%20on%20EdgeSNNs%2C%20providing%20an%20essential%0Areference%20for%20researchers%20and%20practitioners%20working%20at%20the%20intersection%20of%0Aneuromorphic%20computing%20and%20edge%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14069v1&entry.124074799=Read"},
{"title": "A Minimalist Controller for Autonomously Self-Aggregating Robotic\n  Swarms: Enabling Compact Formations in Multitasking Scenarios", "author": "Maria Eduarda Silva de Macedo and Ana Paula Chiarelli de Souza and Roberto Silvio Ubertino Rosso Jr. and Yuri Kaszubowski Lopes", "abstract": "  The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.\n", "link": "http://arxiv.org/abs/2507.13969v1", "date": "2025-07-18", "relevancy": 1.5151, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5642}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4961}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Minimalist%20Controller%20for%20Autonomously%20Self-Aggregating%20Robotic%0A%20%20Swarms%3A%20Enabling%20Compact%20Formations%20in%20Multitasking%20Scenarios&body=Title%3A%20A%20Minimalist%20Controller%20for%20Autonomously%20Self-Aggregating%20Robotic%0A%20%20Swarms%3A%20Enabling%20Compact%20Formations%20in%20Multitasking%20Scenarios%0AAuthor%3A%20Maria%20Eduarda%20Silva%20de%20Macedo%20and%20Ana%20Paula%20Chiarelli%20de%20Souza%20and%20Roberto%20Silvio%20Ubertino%20Rosso%20Jr.%20and%20Yuri%20Kaszubowski%20Lopes%0AAbstract%3A%20%20%20The%20deployment%20of%20simple%20emergent%20behaviors%20in%20swarm%20robotics%20has%20been%0Awell-rehearsed%20in%20the%20literature.%20A%20recent%20study%20has%20shown%20how%20self-aggregation%0Ais%20possible%20in%20a%20multitask%20approach%20--%20where%20multiple%20self-aggregation%20task%0Ainstances%20occur%20concurrently%20in%20the%20same%20environment.%20The%20multitask%20approach%0Aposes%20new%20challenges%2C%20in%20special%2C%20how%20the%20dynamic%20of%20each%20group%20impacts%20the%0Aperformance%20of%20others.%20So%20far%2C%20the%20multitask%20self-aggregation%20of%20groups%20of%0Arobots%20suffers%20from%20generating%20a%20circular%20formation%20--%20that%20is%20not%20fully%0Acompact%20--%20or%20is%20not%20fully%20autonomous.%20In%20this%20paper%2C%20we%20present%20a%20multitask%0Aself-aggregation%20where%20groups%20of%20homogeneous%20robots%20sort%20themselves%20into%0Adifferent%20compact%20clusters%2C%20relying%20solely%20on%20a%20line-of-sight%20sensor.%20Our%0Amultitask%20self-aggregation%20behavior%20was%20able%20to%20scale%20well%20and%20achieve%20a%0Acompact%20formation.%20We%20report%20scalability%20results%20from%20a%20series%20of%20simulation%0Atrials%20with%20different%20configurations%20in%20the%20number%20of%20groups%20and%20the%20number%20of%0Arobots%20per%20group.%20We%20were%20able%20to%20improve%20the%20multitask%20self-aggregation%0Abehavior%20performance%20in%20terms%20of%20the%20compactness%20of%20the%20clusters%2C%20keeping%20the%0Aproportion%20of%20clustered%20robots%20found%20in%20other%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Minimalist%2520Controller%2520for%2520Autonomously%2520Self-Aggregating%2520Robotic%250A%2520%2520Swarms%253A%2520Enabling%2520Compact%2520Formations%2520in%2520Multitasking%2520Scenarios%26entry.906535625%3DMaria%2520Eduarda%2520Silva%2520de%2520Macedo%2520and%2520Ana%2520Paula%2520Chiarelli%2520de%2520Souza%2520and%2520Roberto%2520Silvio%2520Ubertino%2520Rosso%2520Jr.%2520and%2520Yuri%2520Kaszubowski%2520Lopes%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520simple%2520emergent%2520behaviors%2520in%2520swarm%2520robotics%2520has%2520been%250Awell-rehearsed%2520in%2520the%2520literature.%2520A%2520recent%2520study%2520has%2520shown%2520how%2520self-aggregation%250Ais%2520possible%2520in%2520a%2520multitask%2520approach%2520--%2520where%2520multiple%2520self-aggregation%2520task%250Ainstances%2520occur%2520concurrently%2520in%2520the%2520same%2520environment.%2520The%2520multitask%2520approach%250Aposes%2520new%2520challenges%252C%2520in%2520special%252C%2520how%2520the%2520dynamic%2520of%2520each%2520group%2520impacts%2520the%250Aperformance%2520of%2520others.%2520So%2520far%252C%2520the%2520multitask%2520self-aggregation%2520of%2520groups%2520of%250Arobots%2520suffers%2520from%2520generating%2520a%2520circular%2520formation%2520--%2520that%2520is%2520not%2520fully%250Acompact%2520--%2520or%2520is%2520not%2520fully%2520autonomous.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520multitask%250Aself-aggregation%2520where%2520groups%2520of%2520homogeneous%2520robots%2520sort%2520themselves%2520into%250Adifferent%2520compact%2520clusters%252C%2520relying%2520solely%2520on%2520a%2520line-of-sight%2520sensor.%2520Our%250Amultitask%2520self-aggregation%2520behavior%2520was%2520able%2520to%2520scale%2520well%2520and%2520achieve%2520a%250Acompact%2520formation.%2520We%2520report%2520scalability%2520results%2520from%2520a%2520series%2520of%2520simulation%250Atrials%2520with%2520different%2520configurations%2520in%2520the%2520number%2520of%2520groups%2520and%2520the%2520number%2520of%250Arobots%2520per%2520group.%2520We%2520were%2520able%2520to%2520improve%2520the%2520multitask%2520self-aggregation%250Abehavior%2520performance%2520in%2520terms%2520of%2520the%2520compactness%2520of%2520the%2520clusters%252C%2520keeping%2520the%250Aproportion%2520of%2520clustered%2520robots%2520found%2520in%2520other%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Minimalist%20Controller%20for%20Autonomously%20Self-Aggregating%20Robotic%0A%20%20Swarms%3A%20Enabling%20Compact%20Formations%20in%20Multitasking%20Scenarios&entry.906535625=Maria%20Eduarda%20Silva%20de%20Macedo%20and%20Ana%20Paula%20Chiarelli%20de%20Souza%20and%20Roberto%20Silvio%20Ubertino%20Rosso%20Jr.%20and%20Yuri%20Kaszubowski%20Lopes&entry.1292438233=%20%20The%20deployment%20of%20simple%20emergent%20behaviors%20in%20swarm%20robotics%20has%20been%0Awell-rehearsed%20in%20the%20literature.%20A%20recent%20study%20has%20shown%20how%20self-aggregation%0Ais%20possible%20in%20a%20multitask%20approach%20--%20where%20multiple%20self-aggregation%20task%0Ainstances%20occur%20concurrently%20in%20the%20same%20environment.%20The%20multitask%20approach%0Aposes%20new%20challenges%2C%20in%20special%2C%20how%20the%20dynamic%20of%20each%20group%20impacts%20the%0Aperformance%20of%20others.%20So%20far%2C%20the%20multitask%20self-aggregation%20of%20groups%20of%0Arobots%20suffers%20from%20generating%20a%20circular%20formation%20--%20that%20is%20not%20fully%0Acompact%20--%20or%20is%20not%20fully%20autonomous.%20In%20this%20paper%2C%20we%20present%20a%20multitask%0Aself-aggregation%20where%20groups%20of%20homogeneous%20robots%20sort%20themselves%20into%0Adifferent%20compact%20clusters%2C%20relying%20solely%20on%20a%20line-of-sight%20sensor.%20Our%0Amultitask%20self-aggregation%20behavior%20was%20able%20to%20scale%20well%20and%20achieve%20a%0Acompact%20formation.%20We%20report%20scalability%20results%20from%20a%20series%20of%20simulation%0Atrials%20with%20different%20configurations%20in%20the%20number%20of%20groups%20and%20the%20number%20of%0Arobots%20per%20group.%20We%20were%20able%20to%20improve%20the%20multitask%20self-aggregation%0Abehavior%20performance%20in%20terms%20of%20the%20compactness%20of%20the%20clusters%2C%20keeping%20the%0Aproportion%20of%20clustered%20robots%20found%20in%20other%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13969v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


