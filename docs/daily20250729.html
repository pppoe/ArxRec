<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250728.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "$S^3$LAM: Surfel Splatting SLAM for Geometrically Accurate Tracking and\n  Mapping", "author": "Ruoyu Fan and Yuhui Wen and Jiajia Dai and Tao Zhang and Long Zeng and Yong-jin Liu", "abstract": "  We propose $S^3$LAM, a novel RGB-D SLAM system that leverages 2D surfel\nsplatting to achieve highly accurate geometric representations for simultaneous\ntracking and mapping. Unlike existing 3DGS-based SLAM approaches that rely on\n3D Gaussian ellipsoids, we utilize 2D Gaussian surfels as primitives for more\nefficient scene representation. By focusing on the surfaces of objects in the\nscene, this design enables $S^3$LAM to reconstruct high-quality geometry,\nbenefiting both mapping and tracking. To address inherent SLAM challenges\nincluding real-time optimization under limited viewpoints, we introduce a novel\nadaptive surface rendering strategy that improves mapping accuracy while\nmaintaining computational efficiency. We further derive camera pose Jacobians\ndirectly from 2D surfel splatting formulation, highlighting the importance of\nour geometrically accurate representation that improves tracking convergence.\nExtensive experiments on both synthetic and real-world datasets validate that\n$S^3$LAM achieves state-of-the-art performance. Code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2507.20854v1", "date": "2025-07-28", "relevancy": 3.2822, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7573}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6105}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24S%5E3%24LAM%3A%20Surfel%20Splatting%20SLAM%20for%20Geometrically%20Accurate%20Tracking%20and%0A%20%20Mapping&body=Title%3A%20%24S%5E3%24LAM%3A%20Surfel%20Splatting%20SLAM%20for%20Geometrically%20Accurate%20Tracking%20and%0A%20%20Mapping%0AAuthor%3A%20Ruoyu%20Fan%20and%20Yuhui%20Wen%20and%20Jiajia%20Dai%20and%20Tao%20Zhang%20and%20Long%20Zeng%20and%20Yong-jin%20Liu%0AAbstract%3A%20%20%20We%20propose%20%24S%5E3%24LAM%2C%20a%20novel%20RGB-D%20SLAM%20system%20that%20leverages%202D%20surfel%0Asplatting%20to%20achieve%20highly%20accurate%20geometric%20representations%20for%20simultaneous%0Atracking%20and%20mapping.%20Unlike%20existing%203DGS-based%20SLAM%20approaches%20that%20rely%20on%0A3D%20Gaussian%20ellipsoids%2C%20we%20utilize%202D%20Gaussian%20surfels%20as%20primitives%20for%20more%0Aefficient%20scene%20representation.%20By%20focusing%20on%20the%20surfaces%20of%20objects%20in%20the%0Ascene%2C%20this%20design%20enables%20%24S%5E3%24LAM%20to%20reconstruct%20high-quality%20geometry%2C%0Abenefiting%20both%20mapping%20and%20tracking.%20To%20address%20inherent%20SLAM%20challenges%0Aincluding%20real-time%20optimization%20under%20limited%20viewpoints%2C%20we%20introduce%20a%20novel%0Aadaptive%20surface%20rendering%20strategy%20that%20improves%20mapping%20accuracy%20while%0Amaintaining%20computational%20efficiency.%20We%20further%20derive%20camera%20pose%20Jacobians%0Adirectly%20from%202D%20surfel%20splatting%20formulation%2C%20highlighting%20the%20importance%20of%0Aour%20geometrically%20accurate%20representation%20that%20improves%20tracking%20convergence.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20validate%20that%0A%24S%5E3%24LAM%20achieves%20state-of-the-art%20performance.%20Code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524S%255E3%2524LAM%253A%2520Surfel%2520Splatting%2520SLAM%2520for%2520Geometrically%2520Accurate%2520Tracking%2520and%250A%2520%2520Mapping%26entry.906535625%3DRuoyu%2520Fan%2520and%2520Yuhui%2520Wen%2520and%2520Jiajia%2520Dai%2520and%2520Tao%2520Zhang%2520and%2520Long%2520Zeng%2520and%2520Yong-jin%2520Liu%26entry.1292438233%3D%2520%2520We%2520propose%2520%2524S%255E3%2524LAM%252C%2520a%2520novel%2520RGB-D%2520SLAM%2520system%2520that%2520leverages%25202D%2520surfel%250Asplatting%2520to%2520achieve%2520highly%2520accurate%2520geometric%2520representations%2520for%2520simultaneous%250Atracking%2520and%2520mapping.%2520Unlike%2520existing%25203DGS-based%2520SLAM%2520approaches%2520that%2520rely%2520on%250A3D%2520Gaussian%2520ellipsoids%252C%2520we%2520utilize%25202D%2520Gaussian%2520surfels%2520as%2520primitives%2520for%2520more%250Aefficient%2520scene%2520representation.%2520By%2520focusing%2520on%2520the%2520surfaces%2520of%2520objects%2520in%2520the%250Ascene%252C%2520this%2520design%2520enables%2520%2524S%255E3%2524LAM%2520to%2520reconstruct%2520high-quality%2520geometry%252C%250Abenefiting%2520both%2520mapping%2520and%2520tracking.%2520To%2520address%2520inherent%2520SLAM%2520challenges%250Aincluding%2520real-time%2520optimization%2520under%2520limited%2520viewpoints%252C%2520we%2520introduce%2520a%2520novel%250Aadaptive%2520surface%2520rendering%2520strategy%2520that%2520improves%2520mapping%2520accuracy%2520while%250Amaintaining%2520computational%2520efficiency.%2520We%2520further%2520derive%2520camera%2520pose%2520Jacobians%250Adirectly%2520from%25202D%2520surfel%2520splatting%2520formulation%252C%2520highlighting%2520the%2520importance%2520of%250Aour%2520geometrically%2520accurate%2520representation%2520that%2520improves%2520tracking%2520convergence.%250AExtensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520validate%2520that%250A%2524S%255E3%2524LAM%2520achieves%2520state-of-the-art%2520performance.%2520Code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24S%5E3%24LAM%3A%20Surfel%20Splatting%20SLAM%20for%20Geometrically%20Accurate%20Tracking%20and%0A%20%20Mapping&entry.906535625=Ruoyu%20Fan%20and%20Yuhui%20Wen%20and%20Jiajia%20Dai%20and%20Tao%20Zhang%20and%20Long%20Zeng%20and%20Yong-jin%20Liu&entry.1292438233=%20%20We%20propose%20%24S%5E3%24LAM%2C%20a%20novel%20RGB-D%20SLAM%20system%20that%20leverages%202D%20surfel%0Asplatting%20to%20achieve%20highly%20accurate%20geometric%20representations%20for%20simultaneous%0Atracking%20and%20mapping.%20Unlike%20existing%203DGS-based%20SLAM%20approaches%20that%20rely%20on%0A3D%20Gaussian%20ellipsoids%2C%20we%20utilize%202D%20Gaussian%20surfels%20as%20primitives%20for%20more%0Aefficient%20scene%20representation.%20By%20focusing%20on%20the%20surfaces%20of%20objects%20in%20the%0Ascene%2C%20this%20design%20enables%20%24S%5E3%24LAM%20to%20reconstruct%20high-quality%20geometry%2C%0Abenefiting%20both%20mapping%20and%20tracking.%20To%20address%20inherent%20SLAM%20challenges%0Aincluding%20real-time%20optimization%20under%20limited%20viewpoints%2C%20we%20introduce%20a%20novel%0Aadaptive%20surface%20rendering%20strategy%20that%20improves%20mapping%20accuracy%20while%0Amaintaining%20computational%20efficiency.%20We%20further%20derive%20camera%20pose%20Jacobians%0Adirectly%20from%202D%20surfel%20splatting%20formulation%2C%20highlighting%20the%20importance%20of%0Aour%20geometrically%20accurate%20representation%20that%20improves%20tracking%20convergence.%0AExtensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20validate%20that%0A%24S%5E3%24LAM%20achieves%20state-of-the-art%20performance.%20Code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20854v1&entry.124074799=Read"},
{"title": "SparseLoc: Sparse Open-Set Landmark-based Global Localization for\n  Autonomous Navigation", "author": "Pranjal Paul and Vineeth Bhat and Tejas Salian and Mohammad Omama and Krishna Murthy Jatavallabhula and Naveen Arulselvan and K. Madhava Krishna", "abstract": "  Global localization is a critical problem in autonomous navigation, enabling\nprecise positioning without reliance on GPS. Modern global localization\ntechniques often depend on dense LiDAR maps, which, while precise, require\nextensive storage and computational resources. Recent approaches have explored\nalternative methods, such as sparse maps and learned features, but they suffer\nfrom poor robustness and generalization. We propose SparseLoc, a global\nlocalization framework that leverages vision-language foundation models to\ngenerate sparse, semantic-topometric maps in a zero-shot manner. It combines\nthis map representation with a Monte Carlo localization scheme enhanced by a\nnovel late optimization strategy, ensuring improved pose estimation. By\nconstructing compact yet highly discriminative maps and refining localization\nthrough a carefully designed optimization schedule, SparseLoc overcomes the\nlimitations of existing techniques, offering a more efficient and robust\nsolution for global localization. Our system achieves over a 5X improvement in\nlocalization accuracy compared to existing sparse mapping techniques. Despite\nutilizing only 1/500th of the points of dense mapping methods, it achieves\ncomparable performance, maintaining an average global localization error below\n5m and 2 degrees on KITTI sequences.\n", "link": "http://arxiv.org/abs/2503.23465v2", "date": "2025-07-28", "relevancy": 3.1069, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6693}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6166}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseLoc%3A%20Sparse%20Open-Set%20Landmark-based%20Global%20Localization%20for%0A%20%20Autonomous%20Navigation&body=Title%3A%20SparseLoc%3A%20Sparse%20Open-Set%20Landmark-based%20Global%20Localization%20for%0A%20%20Autonomous%20Navigation%0AAuthor%3A%20Pranjal%20Paul%20and%20Vineeth%20Bhat%20and%20Tejas%20Salian%20and%20Mohammad%20Omama%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Naveen%20Arulselvan%20and%20K.%20Madhava%20Krishna%0AAbstract%3A%20%20%20Global%20localization%20is%20a%20critical%20problem%20in%20autonomous%20navigation%2C%20enabling%0Aprecise%20positioning%20without%20reliance%20on%20GPS.%20Modern%20global%20localization%0Atechniques%20often%20depend%20on%20dense%20LiDAR%20maps%2C%20which%2C%20while%20precise%2C%20require%0Aextensive%20storage%20and%20computational%20resources.%20Recent%20approaches%20have%20explored%0Aalternative%20methods%2C%20such%20as%20sparse%20maps%20and%20learned%20features%2C%20but%20they%20suffer%0Afrom%20poor%20robustness%20and%20generalization.%20We%20propose%20SparseLoc%2C%20a%20global%0Alocalization%20framework%20that%20leverages%20vision-language%20foundation%20models%20to%0Agenerate%20sparse%2C%20semantic-topometric%20maps%20in%20a%20zero-shot%20manner.%20It%20combines%0Athis%20map%20representation%20with%20a%20Monte%20Carlo%20localization%20scheme%20enhanced%20by%20a%0Anovel%20late%20optimization%20strategy%2C%20ensuring%20improved%20pose%20estimation.%20By%0Aconstructing%20compact%20yet%20highly%20discriminative%20maps%20and%20refining%20localization%0Athrough%20a%20carefully%20designed%20optimization%20schedule%2C%20SparseLoc%20overcomes%20the%0Alimitations%20of%20existing%20techniques%2C%20offering%20a%20more%20efficient%20and%20robust%0Asolution%20for%20global%20localization.%20Our%20system%20achieves%20over%20a%205X%20improvement%20in%0Alocalization%20accuracy%20compared%20to%20existing%20sparse%20mapping%20techniques.%20Despite%0Autilizing%20only%201/500th%20of%20the%20points%20of%20dense%20mapping%20methods%2C%20it%20achieves%0Acomparable%20performance%2C%20maintaining%20an%20average%20global%20localization%20error%20below%0A5m%20and%202%20degrees%20on%20KITTI%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23465v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseLoc%253A%2520Sparse%2520Open-Set%2520Landmark-based%2520Global%2520Localization%2520for%250A%2520%2520Autonomous%2520Navigation%26entry.906535625%3DPranjal%2520Paul%2520and%2520Vineeth%2520Bhat%2520and%2520Tejas%2520Salian%2520and%2520Mohammad%2520Omama%2520and%2520Krishna%2520Murthy%2520Jatavallabhula%2520and%2520Naveen%2520Arulselvan%2520and%2520K.%2520Madhava%2520Krishna%26entry.1292438233%3D%2520%2520Global%2520localization%2520is%2520a%2520critical%2520problem%2520in%2520autonomous%2520navigation%252C%2520enabling%250Aprecise%2520positioning%2520without%2520reliance%2520on%2520GPS.%2520Modern%2520global%2520localization%250Atechniques%2520often%2520depend%2520on%2520dense%2520LiDAR%2520maps%252C%2520which%252C%2520while%2520precise%252C%2520require%250Aextensive%2520storage%2520and%2520computational%2520resources.%2520Recent%2520approaches%2520have%2520explored%250Aalternative%2520methods%252C%2520such%2520as%2520sparse%2520maps%2520and%2520learned%2520features%252C%2520but%2520they%2520suffer%250Afrom%2520poor%2520robustness%2520and%2520generalization.%2520We%2520propose%2520SparseLoc%252C%2520a%2520global%250Alocalization%2520framework%2520that%2520leverages%2520vision-language%2520foundation%2520models%2520to%250Agenerate%2520sparse%252C%2520semantic-topometric%2520maps%2520in%2520a%2520zero-shot%2520manner.%2520It%2520combines%250Athis%2520map%2520representation%2520with%2520a%2520Monte%2520Carlo%2520localization%2520scheme%2520enhanced%2520by%2520a%250Anovel%2520late%2520optimization%2520strategy%252C%2520ensuring%2520improved%2520pose%2520estimation.%2520By%250Aconstructing%2520compact%2520yet%2520highly%2520discriminative%2520maps%2520and%2520refining%2520localization%250Athrough%2520a%2520carefully%2520designed%2520optimization%2520schedule%252C%2520SparseLoc%2520overcomes%2520the%250Alimitations%2520of%2520existing%2520techniques%252C%2520offering%2520a%2520more%2520efficient%2520and%2520robust%250Asolution%2520for%2520global%2520localization.%2520Our%2520system%2520achieves%2520over%2520a%25205X%2520improvement%2520in%250Alocalization%2520accuracy%2520compared%2520to%2520existing%2520sparse%2520mapping%2520techniques.%2520Despite%250Autilizing%2520only%25201/500th%2520of%2520the%2520points%2520of%2520dense%2520mapping%2520methods%252C%2520it%2520achieves%250Acomparable%2520performance%252C%2520maintaining%2520an%2520average%2520global%2520localization%2520error%2520below%250A5m%2520and%25202%2520degrees%2520on%2520KITTI%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23465v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseLoc%3A%20Sparse%20Open-Set%20Landmark-based%20Global%20Localization%20for%0A%20%20Autonomous%20Navigation&entry.906535625=Pranjal%20Paul%20and%20Vineeth%20Bhat%20and%20Tejas%20Salian%20and%20Mohammad%20Omama%20and%20Krishna%20Murthy%20Jatavallabhula%20and%20Naveen%20Arulselvan%20and%20K.%20Madhava%20Krishna&entry.1292438233=%20%20Global%20localization%20is%20a%20critical%20problem%20in%20autonomous%20navigation%2C%20enabling%0Aprecise%20positioning%20without%20reliance%20on%20GPS.%20Modern%20global%20localization%0Atechniques%20often%20depend%20on%20dense%20LiDAR%20maps%2C%20which%2C%20while%20precise%2C%20require%0Aextensive%20storage%20and%20computational%20resources.%20Recent%20approaches%20have%20explored%0Aalternative%20methods%2C%20such%20as%20sparse%20maps%20and%20learned%20features%2C%20but%20they%20suffer%0Afrom%20poor%20robustness%20and%20generalization.%20We%20propose%20SparseLoc%2C%20a%20global%0Alocalization%20framework%20that%20leverages%20vision-language%20foundation%20models%20to%0Agenerate%20sparse%2C%20semantic-topometric%20maps%20in%20a%20zero-shot%20manner.%20It%20combines%0Athis%20map%20representation%20with%20a%20Monte%20Carlo%20localization%20scheme%20enhanced%20by%20a%0Anovel%20late%20optimization%20strategy%2C%20ensuring%20improved%20pose%20estimation.%20By%0Aconstructing%20compact%20yet%20highly%20discriminative%20maps%20and%20refining%20localization%0Athrough%20a%20carefully%20designed%20optimization%20schedule%2C%20SparseLoc%20overcomes%20the%0Alimitations%20of%20existing%20techniques%2C%20offering%20a%20more%20efficient%20and%20robust%0Asolution%20for%20global%20localization.%20Our%20system%20achieves%20over%20a%205X%20improvement%20in%0Alocalization%20accuracy%20compared%20to%20existing%20sparse%20mapping%20techniques.%20Despite%0Autilizing%20only%201/500th%20of%20the%20points%20of%20dense%20mapping%20methods%2C%20it%20achieves%0Acomparable%20performance%2C%20maintaining%20an%20average%20global%20localization%20error%20below%0A5m%20and%202%20degrees%20on%20KITTI%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23465v2&entry.124074799=Read"},
{"title": "Reconstructing 4D Spatial Intelligence: A Survey", "author": "Yukang Cao and Jiahao Lu and Zhisheng Huang and Zhuowei Shen and Chengfeng Zhao and Fangzhou Hong and Zhaoxi Chen and Xin Li and Wenping Wang and Yuan Liu and Ziwei Liu", "abstract": "  Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.\n", "link": "http://arxiv.org/abs/2507.21045v1", "date": "2025-07-28", "relevancy": 3.0163, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6077}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reconstructing%204D%20Spatial%20Intelligence%3A%20A%20Survey&body=Title%3A%20Reconstructing%204D%20Spatial%20Intelligence%3A%20A%20Survey%0AAuthor%3A%20Yukang%20Cao%20and%20Jiahao%20Lu%20and%20Zhisheng%20Huang%20and%20Zhuowei%20Shen%20and%20Chengfeng%20Zhao%20and%20Fangzhou%20Hong%20and%20Zhaoxi%20Chen%20and%20Xin%20Li%20and%20Wenping%20Wang%20and%20Yuan%20Liu%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Reconstructing%204D%20spatial%20intelligence%20from%20visual%20observations%20has%20long%20been%0Aa%20central%20yet%20challenging%20task%20in%20computer%20vision%2C%20with%20broad%20real-world%0Aapplications.%20These%20range%20from%20entertainment%20domains%20like%20movies%2C%20where%20the%0Afocus%20is%20often%20on%20reconstructing%20fundamental%20visual%20elements%2C%20to%20embodied%20AI%2C%0Awhich%20emphasizes%20interaction%20modeling%20and%20physical%20realism.%20Fueled%20by%20rapid%0Aadvances%20in%203D%20representations%20and%20deep%20learning%20architectures%2C%20the%20field%20has%0Aevolved%20quickly%2C%20outpacing%20the%20scope%20of%20previous%20surveys.%20Additionally%2C%0Aexisting%20surveys%20rarely%20offer%20a%20comprehensive%20analysis%20of%20the%20hierarchical%0Astructure%20of%204D%20scene%20reconstruction.%20To%20address%20this%20gap%2C%20we%20present%20a%20new%0Aperspective%20that%20organizes%20existing%20methods%20into%20five%20progressive%20levels%20of%204D%0Aspatial%20intelligence%3A%20%281%29%20Level%201%20--%20reconstruction%20of%20low-level%203D%20attributes%0A%28e.g.%2C%20depth%2C%20pose%2C%20and%20point%20maps%29%3B%20%282%29%20Level%202%20--%20reconstruction%20of%203D%20scene%0Acomponents%20%28e.g.%2C%20objects%2C%20humans%2C%20structures%29%3B%20%283%29%20Level%203%20--%20reconstruction%0Aof%204D%20dynamic%20scenes%3B%20%284%29%20Level%204%20--%20modeling%20of%20interactions%20among%20scene%0Acomponents%3B%20and%20%285%29%20Level%205%20--%20incorporation%20of%20physical%20laws%20and%20constraints.%0AWe%20conclude%20the%20survey%20by%20discussing%20the%20key%20challenges%20at%20each%20level%20and%0Ahighlighting%20promising%20directions%20for%20advancing%20toward%20even%20richer%20levels%20of%204D%0Aspatial%20intelligence.%20To%20track%20ongoing%20developments%2C%20we%20maintain%20an%20up-to-date%0Aproject%20page%3A%20https%3A//github.com/yukangcao/Awesome-4D-Spatial-Intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReconstructing%25204D%2520Spatial%2520Intelligence%253A%2520A%2520Survey%26entry.906535625%3DYukang%2520Cao%2520and%2520Jiahao%2520Lu%2520and%2520Zhisheng%2520Huang%2520and%2520Zhuowei%2520Shen%2520and%2520Chengfeng%2520Zhao%2520and%2520Fangzhou%2520Hong%2520and%2520Zhaoxi%2520Chen%2520and%2520Xin%2520Li%2520and%2520Wenping%2520Wang%2520and%2520Yuan%2520Liu%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Reconstructing%25204D%2520spatial%2520intelligence%2520from%2520visual%2520observations%2520has%2520long%2520been%250Aa%2520central%2520yet%2520challenging%2520task%2520in%2520computer%2520vision%252C%2520with%2520broad%2520real-world%250Aapplications.%2520These%2520range%2520from%2520entertainment%2520domains%2520like%2520movies%252C%2520where%2520the%250Afocus%2520is%2520often%2520on%2520reconstructing%2520fundamental%2520visual%2520elements%252C%2520to%2520embodied%2520AI%252C%250Awhich%2520emphasizes%2520interaction%2520modeling%2520and%2520physical%2520realism.%2520Fueled%2520by%2520rapid%250Aadvances%2520in%25203D%2520representations%2520and%2520deep%2520learning%2520architectures%252C%2520the%2520field%2520has%250Aevolved%2520quickly%252C%2520outpacing%2520the%2520scope%2520of%2520previous%2520surveys.%2520Additionally%252C%250Aexisting%2520surveys%2520rarely%2520offer%2520a%2520comprehensive%2520analysis%2520of%2520the%2520hierarchical%250Astructure%2520of%25204D%2520scene%2520reconstruction.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520a%2520new%250Aperspective%2520that%2520organizes%2520existing%2520methods%2520into%2520five%2520progressive%2520levels%2520of%25204D%250Aspatial%2520intelligence%253A%2520%25281%2529%2520Level%25201%2520--%2520reconstruction%2520of%2520low-level%25203D%2520attributes%250A%2528e.g.%252C%2520depth%252C%2520pose%252C%2520and%2520point%2520maps%2529%253B%2520%25282%2529%2520Level%25202%2520--%2520reconstruction%2520of%25203D%2520scene%250Acomponents%2520%2528e.g.%252C%2520objects%252C%2520humans%252C%2520structures%2529%253B%2520%25283%2529%2520Level%25203%2520--%2520reconstruction%250Aof%25204D%2520dynamic%2520scenes%253B%2520%25284%2529%2520Level%25204%2520--%2520modeling%2520of%2520interactions%2520among%2520scene%250Acomponents%253B%2520and%2520%25285%2529%2520Level%25205%2520--%2520incorporation%2520of%2520physical%2520laws%2520and%2520constraints.%250AWe%2520conclude%2520the%2520survey%2520by%2520discussing%2520the%2520key%2520challenges%2520at%2520each%2520level%2520and%250Ahighlighting%2520promising%2520directions%2520for%2520advancing%2520toward%2520even%2520richer%2520levels%2520of%25204D%250Aspatial%2520intelligence.%2520To%2520track%2520ongoing%2520developments%252C%2520we%2520maintain%2520an%2520up-to-date%250Aproject%2520page%253A%2520https%253A//github.com/yukangcao/Awesome-4D-Spatial-Intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reconstructing%204D%20Spatial%20Intelligence%3A%20A%20Survey&entry.906535625=Yukang%20Cao%20and%20Jiahao%20Lu%20and%20Zhisheng%20Huang%20and%20Zhuowei%20Shen%20and%20Chengfeng%20Zhao%20and%20Fangzhou%20Hong%20and%20Zhaoxi%20Chen%20and%20Xin%20Li%20and%20Wenping%20Wang%20and%20Yuan%20Liu%20and%20Ziwei%20Liu&entry.1292438233=%20%20Reconstructing%204D%20spatial%20intelligence%20from%20visual%20observations%20has%20long%20been%0Aa%20central%20yet%20challenging%20task%20in%20computer%20vision%2C%20with%20broad%20real-world%0Aapplications.%20These%20range%20from%20entertainment%20domains%20like%20movies%2C%20where%20the%0Afocus%20is%20often%20on%20reconstructing%20fundamental%20visual%20elements%2C%20to%20embodied%20AI%2C%0Awhich%20emphasizes%20interaction%20modeling%20and%20physical%20realism.%20Fueled%20by%20rapid%0Aadvances%20in%203D%20representations%20and%20deep%20learning%20architectures%2C%20the%20field%20has%0Aevolved%20quickly%2C%20outpacing%20the%20scope%20of%20previous%20surveys.%20Additionally%2C%0Aexisting%20surveys%20rarely%20offer%20a%20comprehensive%20analysis%20of%20the%20hierarchical%0Astructure%20of%204D%20scene%20reconstruction.%20To%20address%20this%20gap%2C%20we%20present%20a%20new%0Aperspective%20that%20organizes%20existing%20methods%20into%20five%20progressive%20levels%20of%204D%0Aspatial%20intelligence%3A%20%281%29%20Level%201%20--%20reconstruction%20of%20low-level%203D%20attributes%0A%28e.g.%2C%20depth%2C%20pose%2C%20and%20point%20maps%29%3B%20%282%29%20Level%202%20--%20reconstruction%20of%203D%20scene%0Acomponents%20%28e.g.%2C%20objects%2C%20humans%2C%20structures%29%3B%20%283%29%20Level%203%20--%20reconstruction%0Aof%204D%20dynamic%20scenes%3B%20%284%29%20Level%204%20--%20modeling%20of%20interactions%20among%20scene%0Acomponents%3B%20and%20%285%29%20Level%205%20--%20incorporation%20of%20physical%20laws%20and%20constraints.%0AWe%20conclude%20the%20survey%20by%20discussing%20the%20key%20challenges%20at%20each%20level%20and%0Ahighlighting%20promising%20directions%20for%20advancing%20toward%20even%20richer%20levels%20of%204D%0Aspatial%20intelligence.%20To%20track%20ongoing%20developments%2C%20we%20maintain%20an%20up-to-date%0Aproject%20page%3A%20https%3A//github.com/yukangcao/Awesome-4D-Spatial-Intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21045v1&entry.124074799=Read"},
{"title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single\n  Image", "author": "Hyeongjin Nam and Donghwan Kim and Gyeongsik Moon and Kyoung Mu Lee", "abstract": "  The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/.\n", "link": "http://arxiv.org/abs/2507.17332v3", "date": "2025-07-28", "relevancy": 3.0016, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6134}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6089}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PARTE%3A%20Part-Guided%20Texturing%20for%203D%20Human%20Reconstruction%20from%20a%20Single%0A%20%20Image&body=Title%3A%20PARTE%3A%20Part-Guided%20Texturing%20for%203D%20Human%20Reconstruction%20from%20a%20Single%0A%20%20Image%0AAuthor%3A%20Hyeongjin%20Nam%20and%20Donghwan%20Kim%20and%20Gyeongsik%20Moon%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20%20%20The%20misaligned%20human%20texture%20across%20different%20human%20parts%20is%20one%20of%20the%20main%0Alimitations%20of%20existing%203D%20human%20reconstruction%20methods.%20Each%20human%20part%2C%20such%0Aas%20a%20jacket%20or%20pants%2C%20should%20maintain%20a%20distinct%20texture%20without%20blending%20into%0Aothers.%20The%20structural%20coherence%20of%20human%20parts%20serves%20as%20a%20crucial%20cue%20to%0Ainfer%20human%20textures%20in%20the%20invisible%20regions%20of%20a%20single%20image.%20However%2C%20most%0Aexisting%203D%20human%20reconstruction%20methods%20do%20not%20explicitly%20exploit%20such%20part%0Asegmentation%20priors%2C%20leading%20to%20misaligned%20textures%20in%20their%20reconstructions.%0AIn%20this%20regard%2C%20we%20present%20PARTE%2C%20which%20utilizes%203D%20human%20part%20information%20as%20a%0Akey%20guide%20to%20reconstruct%203D%20human%20textures.%20Our%20framework%20comprises%20two%20core%0Acomponents.%20First%2C%20to%20infer%203D%20human%20part%20information%20from%20a%20single%20image%2C%20we%0Apropose%20a%203D%20part%20segmentation%20module%20%28PartSegmenter%29%20that%20initially%0Areconstructs%20a%20textureless%20human%20surface%20and%20predicts%20human%20part%20labels%20based%0Aon%20the%20textureless%20surface.%20Second%2C%20to%20incorporate%20part%20information%20into%0Atexture%20reconstruction%2C%20we%20introduce%20a%20part-guided%20texturing%20module%0A%28PartTexturer%29%2C%20which%20acquires%20prior%20knowledge%20from%20a%20pre-trained%20image%0Ageneration%20network%20on%20texture%20alignment%20of%20human%20parts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20quality%20in%203D%20human%0Areconstruction.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//hygenie1228.github.io/PARTE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17332v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPARTE%253A%2520Part-Guided%2520Texturing%2520for%25203D%2520Human%2520Reconstruction%2520from%2520a%2520Single%250A%2520%2520Image%26entry.906535625%3DHyeongjin%2520Nam%2520and%2520Donghwan%2520Kim%2520and%2520Gyeongsik%2520Moon%2520and%2520Kyoung%2520Mu%2520Lee%26entry.1292438233%3D%2520%2520The%2520misaligned%2520human%2520texture%2520across%2520different%2520human%2520parts%2520is%2520one%2520of%2520the%2520main%250Alimitations%2520of%2520existing%25203D%2520human%2520reconstruction%2520methods.%2520Each%2520human%2520part%252C%2520such%250Aas%2520a%2520jacket%2520or%2520pants%252C%2520should%2520maintain%2520a%2520distinct%2520texture%2520without%2520blending%2520into%250Aothers.%2520The%2520structural%2520coherence%2520of%2520human%2520parts%2520serves%2520as%2520a%2520crucial%2520cue%2520to%250Ainfer%2520human%2520textures%2520in%2520the%2520invisible%2520regions%2520of%2520a%2520single%2520image.%2520However%252C%2520most%250Aexisting%25203D%2520human%2520reconstruction%2520methods%2520do%2520not%2520explicitly%2520exploit%2520such%2520part%250Asegmentation%2520priors%252C%2520leading%2520to%2520misaligned%2520textures%2520in%2520their%2520reconstructions.%250AIn%2520this%2520regard%252C%2520we%2520present%2520PARTE%252C%2520which%2520utilizes%25203D%2520human%2520part%2520information%2520as%2520a%250Akey%2520guide%2520to%2520reconstruct%25203D%2520human%2520textures.%2520Our%2520framework%2520comprises%2520two%2520core%250Acomponents.%2520First%252C%2520to%2520infer%25203D%2520human%2520part%2520information%2520from%2520a%2520single%2520image%252C%2520we%250Apropose%2520a%25203D%2520part%2520segmentation%2520module%2520%2528PartSegmenter%2529%2520that%2520initially%250Areconstructs%2520a%2520textureless%2520human%2520surface%2520and%2520predicts%2520human%2520part%2520labels%2520based%250Aon%2520the%2520textureless%2520surface.%2520Second%252C%2520to%2520incorporate%2520part%2520information%2520into%250Atexture%2520reconstruction%252C%2520we%2520introduce%2520a%2520part-guided%2520texturing%2520module%250A%2528PartTexturer%2529%252C%2520which%2520acquires%2520prior%2520knowledge%2520from%2520a%2520pre-trained%2520image%250Ageneration%2520network%2520on%2520texture%2520alignment%2520of%2520human%2520parts.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520state-of-the-art%2520quality%2520in%25203D%2520human%250Areconstruction.%2520The%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//hygenie1228.github.io/PARTE/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17332v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARTE%3A%20Part-Guided%20Texturing%20for%203D%20Human%20Reconstruction%20from%20a%20Single%0A%20%20Image&entry.906535625=Hyeongjin%20Nam%20and%20Donghwan%20Kim%20and%20Gyeongsik%20Moon%20and%20Kyoung%20Mu%20Lee&entry.1292438233=%20%20The%20misaligned%20human%20texture%20across%20different%20human%20parts%20is%20one%20of%20the%20main%0Alimitations%20of%20existing%203D%20human%20reconstruction%20methods.%20Each%20human%20part%2C%20such%0Aas%20a%20jacket%20or%20pants%2C%20should%20maintain%20a%20distinct%20texture%20without%20blending%20into%0Aothers.%20The%20structural%20coherence%20of%20human%20parts%20serves%20as%20a%20crucial%20cue%20to%0Ainfer%20human%20textures%20in%20the%20invisible%20regions%20of%20a%20single%20image.%20However%2C%20most%0Aexisting%203D%20human%20reconstruction%20methods%20do%20not%20explicitly%20exploit%20such%20part%0Asegmentation%20priors%2C%20leading%20to%20misaligned%20textures%20in%20their%20reconstructions.%0AIn%20this%20regard%2C%20we%20present%20PARTE%2C%20which%20utilizes%203D%20human%20part%20information%20as%20a%0Akey%20guide%20to%20reconstruct%203D%20human%20textures.%20Our%20framework%20comprises%20two%20core%0Acomponents.%20First%2C%20to%20infer%203D%20human%20part%20information%20from%20a%20single%20image%2C%20we%0Apropose%20a%203D%20part%20segmentation%20module%20%28PartSegmenter%29%20that%20initially%0Areconstructs%20a%20textureless%20human%20surface%20and%20predicts%20human%20part%20labels%20based%0Aon%20the%20textureless%20surface.%20Second%2C%20to%20incorporate%20part%20information%20into%0Atexture%20reconstruction%2C%20we%20introduce%20a%20part-guided%20texturing%20module%0A%28PartTexturer%29%2C%20which%20acquires%20prior%20knowledge%20from%20a%20pre-trained%20image%0Ageneration%20network%20on%20texture%20alignment%20of%20human%20parts.%20Extensive%20experiments%0Ademonstrate%20that%20our%20framework%20achieves%20state-of-the-art%20quality%20in%203D%20human%0Areconstruction.%20The%20project%20page%20is%20available%20at%0Ahttps%3A//hygenie1228.github.io/PARTE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17332v3&entry.124074799=Read"},
{"title": "REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm\n  using Consistency Evaluation", "author": "D\u00e9bora N. P. Oliveira and Joshua Knights and Sebasti\u00e1n Barbas Laina and Simon Boche and Wolfram Burgard and Stefan Leutenegger", "abstract": "  Loop closures are essential for correcting odometry drift and creating\nconsistent maps, especially in the context of large-scale navigation. Current\nmethods using dense point clouds for accurate place recognition do not scale\nwell due to computationally expensive scan-to-scan comparisons. Alternative\nobject-centric approaches are more efficient but often struggle with\nsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novel\napproach that addresses these challenges of scalability and perspective\ndifference in re-localization by using LiDAR-based submaps. We introduce\nrotation-invariant features for each labeled object and enhance them with\nneighborhood context through a graph neural network. To identify potential\nrevisits, we employ a scalable bag-of-words approach, pooling one learned\nglobal feature per submap. Additionally, we define a revisit with geometrical\nconsistency cues rather than embedding distance, allowing us to recognize\nfar-away loop closures. Our evaluations demonstrate that REGRACE achieves\nsimilar results compared to state-of-the-art place recognition and registration\nbaselines while being twice as fast. Code and models are publicly available.\n", "link": "http://arxiv.org/abs/2503.03599v2", "date": "2025-07-28", "relevancy": 2.9949, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6199}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REGRACE%3A%20A%20Robust%20and%20Efficient%20Graph-based%20Re-localization%20Algorithm%0A%20%20using%20Consistency%20Evaluation&body=Title%3A%20REGRACE%3A%20A%20Robust%20and%20Efficient%20Graph-based%20Re-localization%20Algorithm%0A%20%20using%20Consistency%20Evaluation%0AAuthor%3A%20D%C3%A9bora%20N.%20P.%20Oliveira%20and%20Joshua%20Knights%20and%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Wolfram%20Burgard%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20Loop%20closures%20are%20essential%20for%20correcting%20odometry%20drift%20and%20creating%0Aconsistent%20maps%2C%20especially%20in%20the%20context%20of%20large-scale%20navigation.%20Current%0Amethods%20using%20dense%20point%20clouds%20for%20accurate%20place%20recognition%20do%20not%20scale%0Awell%20due%20to%20computationally%20expensive%20scan-to-scan%20comparisons.%20Alternative%0Aobject-centric%20approaches%20are%20more%20efficient%20but%20often%20struggle%20with%0Asensitivity%20to%20viewpoint%20variation.%20In%20this%20work%2C%20we%20introduce%20REGRACE%2C%20a%20novel%0Aapproach%20that%20addresses%20these%20challenges%20of%20scalability%20and%20perspective%0Adifference%20in%20re-localization%20by%20using%20LiDAR-based%20submaps.%20We%20introduce%0Arotation-invariant%20features%20for%20each%20labeled%20object%20and%20enhance%20them%20with%0Aneighborhood%20context%20through%20a%20graph%20neural%20network.%20To%20identify%20potential%0Arevisits%2C%20we%20employ%20a%20scalable%20bag-of-words%20approach%2C%20pooling%20one%20learned%0Aglobal%20feature%20per%20submap.%20Additionally%2C%20we%20define%20a%20revisit%20with%20geometrical%0Aconsistency%20cues%20rather%20than%20embedding%20distance%2C%20allowing%20us%20to%20recognize%0Afar-away%20loop%20closures.%20Our%20evaluations%20demonstrate%20that%20REGRACE%20achieves%0Asimilar%20results%20compared%20to%20state-of-the-art%20place%20recognition%20and%20registration%0Abaselines%20while%20being%20twice%20as%20fast.%20Code%20and%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREGRACE%253A%2520A%2520Robust%2520and%2520Efficient%2520Graph-based%2520Re-localization%2520Algorithm%250A%2520%2520using%2520Consistency%2520Evaluation%26entry.906535625%3DD%25C3%25A9bora%2520N.%2520P.%2520Oliveira%2520and%2520Joshua%2520Knights%2520and%2520Sebasti%25C3%25A1n%2520Barbas%2520Laina%2520and%2520Simon%2520Boche%2520and%2520Wolfram%2520Burgard%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3D%2520%2520Loop%2520closures%2520are%2520essential%2520for%2520correcting%2520odometry%2520drift%2520and%2520creating%250Aconsistent%2520maps%252C%2520especially%2520in%2520the%2520context%2520of%2520large-scale%2520navigation.%2520Current%250Amethods%2520using%2520dense%2520point%2520clouds%2520for%2520accurate%2520place%2520recognition%2520do%2520not%2520scale%250Awell%2520due%2520to%2520computationally%2520expensive%2520scan-to-scan%2520comparisons.%2520Alternative%250Aobject-centric%2520approaches%2520are%2520more%2520efficient%2520but%2520often%2520struggle%2520with%250Asensitivity%2520to%2520viewpoint%2520variation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520REGRACE%252C%2520a%2520novel%250Aapproach%2520that%2520addresses%2520these%2520challenges%2520of%2520scalability%2520and%2520perspective%250Adifference%2520in%2520re-localization%2520by%2520using%2520LiDAR-based%2520submaps.%2520We%2520introduce%250Arotation-invariant%2520features%2520for%2520each%2520labeled%2520object%2520and%2520enhance%2520them%2520with%250Aneighborhood%2520context%2520through%2520a%2520graph%2520neural%2520network.%2520To%2520identify%2520potential%250Arevisits%252C%2520we%2520employ%2520a%2520scalable%2520bag-of-words%2520approach%252C%2520pooling%2520one%2520learned%250Aglobal%2520feature%2520per%2520submap.%2520Additionally%252C%2520we%2520define%2520a%2520revisit%2520with%2520geometrical%250Aconsistency%2520cues%2520rather%2520than%2520embedding%2520distance%252C%2520allowing%2520us%2520to%2520recognize%250Afar-away%2520loop%2520closures.%2520Our%2520evaluations%2520demonstrate%2520that%2520REGRACE%2520achieves%250Asimilar%2520results%2520compared%2520to%2520state-of-the-art%2520place%2520recognition%2520and%2520registration%250Abaselines%2520while%2520being%2520twice%2520as%2520fast.%2520Code%2520and%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REGRACE%3A%20A%20Robust%20and%20Efficient%20Graph-based%20Re-localization%20Algorithm%0A%20%20using%20Consistency%20Evaluation&entry.906535625=D%C3%A9bora%20N.%20P.%20Oliveira%20and%20Joshua%20Knights%20and%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Simon%20Boche%20and%20Wolfram%20Burgard%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20Loop%20closures%20are%20essential%20for%20correcting%20odometry%20drift%20and%20creating%0Aconsistent%20maps%2C%20especially%20in%20the%20context%20of%20large-scale%20navigation.%20Current%0Amethods%20using%20dense%20point%20clouds%20for%20accurate%20place%20recognition%20do%20not%20scale%0Awell%20due%20to%20computationally%20expensive%20scan-to-scan%20comparisons.%20Alternative%0Aobject-centric%20approaches%20are%20more%20efficient%20but%20often%20struggle%20with%0Asensitivity%20to%20viewpoint%20variation.%20In%20this%20work%2C%20we%20introduce%20REGRACE%2C%20a%20novel%0Aapproach%20that%20addresses%20these%20challenges%20of%20scalability%20and%20perspective%0Adifference%20in%20re-localization%20by%20using%20LiDAR-based%20submaps.%20We%20introduce%0Arotation-invariant%20features%20for%20each%20labeled%20object%20and%20enhance%20them%20with%0Aneighborhood%20context%20through%20a%20graph%20neural%20network.%20To%20identify%20potential%0Arevisits%2C%20we%20employ%20a%20scalable%20bag-of-words%20approach%2C%20pooling%20one%20learned%0Aglobal%20feature%20per%20submap.%20Additionally%2C%20we%20define%20a%20revisit%20with%20geometrical%0Aconsistency%20cues%20rather%20than%20embedding%20distance%2C%20allowing%20us%20to%20recognize%0Afar-away%20loop%20closures.%20Our%20evaluations%20demonstrate%20that%20REGRACE%20achieves%0Asimilar%20results%20compared%20to%20state-of-the-art%20place%20recognition%20and%20registration%0Abaselines%20while%20being%20twice%20as%20fast.%20Code%20and%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03599v2&entry.124074799=Read"},
{"title": "KASportsFormer: Kinematic Anatomy Enhanced Transformer for 3D Human Pose\n  Estimation on Short Sports Scene Video", "author": "Zhuoer Yin and Calvin Yeung and Tomohiro Suzuki and Ryota Tanaka and Keisuke Fujii", "abstract": "  Recent transformer based approaches have demonstrated impressive performance\nin solving real-world 3D human pose estimation problems. Albeit these\napproaches achieve fruitful results on benchmark datasets, they tend to fall\nshort of sports scenarios where human movements are more complicated than daily\nlife actions, as being hindered by motion blur, occlusions, and domain shifts.\nMoreover, due to the fact that critical motions in a sports game often finish\nin moments of time (e.g., shooting), the ability to focus on momentary actions\nis becoming a crucial factor in sports analysis, where current methods appear\nto struggle with instantaneous scenarios. To overcome these limitations, we\nintroduce KASportsFormer, a novel transformer based 3D pose estimation\nframework for sports that incorporates a kinematic anatomy-informed feature\nrepresentation and integration module. In which the inherent kinematic motion\ninformation is extracted with the Bone Extractor (BoneExt) and Limb Fuser\n(LimbFus) modules and encoded in a multimodal manner. This improved the\ncapability of comprehending sports poses in short videos. We evaluate our\nmethod through two representative sports scene datasets: SportsPose and\nWorldPose. Experimental results show that our proposed method achieves\nstate-of-the-art results with MPJPE errors of 58.0mm and 34.3mm, respectively.\nOur code and models are available at: https://github.com/jw0r1n/KASportsFormer\n", "link": "http://arxiv.org/abs/2507.20763v1", "date": "2025-07-28", "relevancy": 2.9801, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6218}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6083}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KASportsFormer%3A%20Kinematic%20Anatomy%20Enhanced%20Transformer%20for%203D%20Human%20Pose%0A%20%20Estimation%20on%20Short%20Sports%20Scene%20Video&body=Title%3A%20KASportsFormer%3A%20Kinematic%20Anatomy%20Enhanced%20Transformer%20for%203D%20Human%20Pose%0A%20%20Estimation%20on%20Short%20Sports%20Scene%20Video%0AAuthor%3A%20Zhuoer%20Yin%20and%20Calvin%20Yeung%20and%20Tomohiro%20Suzuki%20and%20Ryota%20Tanaka%20and%20Keisuke%20Fujii%0AAbstract%3A%20%20%20Recent%20transformer%20based%20approaches%20have%20demonstrated%20impressive%20performance%0Ain%20solving%20real-world%203D%20human%20pose%20estimation%20problems.%20Albeit%20these%0Aapproaches%20achieve%20fruitful%20results%20on%20benchmark%20datasets%2C%20they%20tend%20to%20fall%0Ashort%20of%20sports%20scenarios%20where%20human%20movements%20are%20more%20complicated%20than%20daily%0Alife%20actions%2C%20as%20being%20hindered%20by%20motion%20blur%2C%20occlusions%2C%20and%20domain%20shifts.%0AMoreover%2C%20due%20to%20the%20fact%20that%20critical%20motions%20in%20a%20sports%20game%20often%20finish%0Ain%20moments%20of%20time%20%28e.g.%2C%20shooting%29%2C%20the%20ability%20to%20focus%20on%20momentary%20actions%0Ais%20becoming%20a%20crucial%20factor%20in%20sports%20analysis%2C%20where%20current%20methods%20appear%0Ato%20struggle%20with%20instantaneous%20scenarios.%20To%20overcome%20these%20limitations%2C%20we%0Aintroduce%20KASportsFormer%2C%20a%20novel%20transformer%20based%203D%20pose%20estimation%0Aframework%20for%20sports%20that%20incorporates%20a%20kinematic%20anatomy-informed%20feature%0Arepresentation%20and%20integration%20module.%20In%20which%20the%20inherent%20kinematic%20motion%0Ainformation%20is%20extracted%20with%20the%20Bone%20Extractor%20%28BoneExt%29%20and%20Limb%20Fuser%0A%28LimbFus%29%20modules%20and%20encoded%20in%20a%20multimodal%20manner.%20This%20improved%20the%0Acapability%20of%20comprehending%20sports%20poses%20in%20short%20videos.%20We%20evaluate%20our%0Amethod%20through%20two%20representative%20sports%20scene%20datasets%3A%20SportsPose%20and%0AWorldPose.%20Experimental%20results%20show%20that%20our%20proposed%20method%20achieves%0Astate-of-the-art%20results%20with%20MPJPE%20errors%20of%2058.0mm%20and%2034.3mm%2C%20respectively.%0AOur%20code%20and%20models%20are%20available%20at%3A%20https%3A//github.com/jw0r1n/KASportsFormer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKASportsFormer%253A%2520Kinematic%2520Anatomy%2520Enhanced%2520Transformer%2520for%25203D%2520Human%2520Pose%250A%2520%2520Estimation%2520on%2520Short%2520Sports%2520Scene%2520Video%26entry.906535625%3DZhuoer%2520Yin%2520and%2520Calvin%2520Yeung%2520and%2520Tomohiro%2520Suzuki%2520and%2520Ryota%2520Tanaka%2520and%2520Keisuke%2520Fujii%26entry.1292438233%3D%2520%2520Recent%2520transformer%2520based%2520approaches%2520have%2520demonstrated%2520impressive%2520performance%250Ain%2520solving%2520real-world%25203D%2520human%2520pose%2520estimation%2520problems.%2520Albeit%2520these%250Aapproaches%2520achieve%2520fruitful%2520results%2520on%2520benchmark%2520datasets%252C%2520they%2520tend%2520to%2520fall%250Ashort%2520of%2520sports%2520scenarios%2520where%2520human%2520movements%2520are%2520more%2520complicated%2520than%2520daily%250Alife%2520actions%252C%2520as%2520being%2520hindered%2520by%2520motion%2520blur%252C%2520occlusions%252C%2520and%2520domain%2520shifts.%250AMoreover%252C%2520due%2520to%2520the%2520fact%2520that%2520critical%2520motions%2520in%2520a%2520sports%2520game%2520often%2520finish%250Ain%2520moments%2520of%2520time%2520%2528e.g.%252C%2520shooting%2529%252C%2520the%2520ability%2520to%2520focus%2520on%2520momentary%2520actions%250Ais%2520becoming%2520a%2520crucial%2520factor%2520in%2520sports%2520analysis%252C%2520where%2520current%2520methods%2520appear%250Ato%2520struggle%2520with%2520instantaneous%2520scenarios.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Aintroduce%2520KASportsFormer%252C%2520a%2520novel%2520transformer%2520based%25203D%2520pose%2520estimation%250Aframework%2520for%2520sports%2520that%2520incorporates%2520a%2520kinematic%2520anatomy-informed%2520feature%250Arepresentation%2520and%2520integration%2520module.%2520In%2520which%2520the%2520inherent%2520kinematic%2520motion%250Ainformation%2520is%2520extracted%2520with%2520the%2520Bone%2520Extractor%2520%2528BoneExt%2529%2520and%2520Limb%2520Fuser%250A%2528LimbFus%2529%2520modules%2520and%2520encoded%2520in%2520a%2520multimodal%2520manner.%2520This%2520improved%2520the%250Acapability%2520of%2520comprehending%2520sports%2520poses%2520in%2520short%2520videos.%2520We%2520evaluate%2520our%250Amethod%2520through%2520two%2520representative%2520sports%2520scene%2520datasets%253A%2520SportsPose%2520and%250AWorldPose.%2520Experimental%2520results%2520show%2520that%2520our%2520proposed%2520method%2520achieves%250Astate-of-the-art%2520results%2520with%2520MPJPE%2520errors%2520of%252058.0mm%2520and%252034.3mm%252C%2520respectively.%250AOur%2520code%2520and%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/jw0r1n/KASportsFormer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KASportsFormer%3A%20Kinematic%20Anatomy%20Enhanced%20Transformer%20for%203D%20Human%20Pose%0A%20%20Estimation%20on%20Short%20Sports%20Scene%20Video&entry.906535625=Zhuoer%20Yin%20and%20Calvin%20Yeung%20and%20Tomohiro%20Suzuki%20and%20Ryota%20Tanaka%20and%20Keisuke%20Fujii&entry.1292438233=%20%20Recent%20transformer%20based%20approaches%20have%20demonstrated%20impressive%20performance%0Ain%20solving%20real-world%203D%20human%20pose%20estimation%20problems.%20Albeit%20these%0Aapproaches%20achieve%20fruitful%20results%20on%20benchmark%20datasets%2C%20they%20tend%20to%20fall%0Ashort%20of%20sports%20scenarios%20where%20human%20movements%20are%20more%20complicated%20than%20daily%0Alife%20actions%2C%20as%20being%20hindered%20by%20motion%20blur%2C%20occlusions%2C%20and%20domain%20shifts.%0AMoreover%2C%20due%20to%20the%20fact%20that%20critical%20motions%20in%20a%20sports%20game%20often%20finish%0Ain%20moments%20of%20time%20%28e.g.%2C%20shooting%29%2C%20the%20ability%20to%20focus%20on%20momentary%20actions%0Ais%20becoming%20a%20crucial%20factor%20in%20sports%20analysis%2C%20where%20current%20methods%20appear%0Ato%20struggle%20with%20instantaneous%20scenarios.%20To%20overcome%20these%20limitations%2C%20we%0Aintroduce%20KASportsFormer%2C%20a%20novel%20transformer%20based%203D%20pose%20estimation%0Aframework%20for%20sports%20that%20incorporates%20a%20kinematic%20anatomy-informed%20feature%0Arepresentation%20and%20integration%20module.%20In%20which%20the%20inherent%20kinematic%20motion%0Ainformation%20is%20extracted%20with%20the%20Bone%20Extractor%20%28BoneExt%29%20and%20Limb%20Fuser%0A%28LimbFus%29%20modules%20and%20encoded%20in%20a%20multimodal%20manner.%20This%20improved%20the%0Acapability%20of%20comprehending%20sports%20poses%20in%20short%20videos.%20We%20evaluate%20our%0Amethod%20through%20two%20representative%20sports%20scene%20datasets%3A%20SportsPose%20and%0AWorldPose.%20Experimental%20results%20show%20that%20our%20proposed%20method%20achieves%0Astate-of-the-art%20results%20with%20MPJPE%20errors%20of%2058.0mm%20and%2034.3mm%2C%20respectively.%0AOur%20code%20and%20models%20are%20available%20at%3A%20https%3A//github.com/jw0r1n/KASportsFormer%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20763v1&entry.124074799=Read"},
{"title": "Beyond Line-of-Sight: Cooperative Localization Using Vision and V2X\n  Communication", "author": "Annika Wong and Zhiqi Tang and Frank J. Jiang and Karl H. Johansson and Jonas M\u00e5rtensson", "abstract": "  Accurate and robust localization is critical for the safe operation of\nConnected and Automated Vehicles (CAVs), especially in complex urban\nenvironments where Global Navigation Satellite System (GNSS) signals are\nunreliable. This paper presents a novel vision-based cooperative localization\nalgorithm that leverages onboard cameras and Vehicle-to-Everything (V2X)\ncommunication to enable CAVs to estimate their poses, even in occlusion-heavy\nscenarios such as busy intersections. In particular, we propose a novel\ndecentralized observer for a group of connected agents that includes landmark\nagents (static or moving) in the environment with known positions and vehicle\nagents that need to estimate their poses (both positions and orientations).\nAssuming that (i) there are at least three landmark agents in the environment,\n(ii) each vehicle agent can measure its own angular and translational\nvelocities as well as relative bearings to at least three neighboring landmarks\nor vehicles, and (iii) neighboring vehicles can communicate their pose\nestimates, each vehicle can estimate its own pose using the proposed\ndecentralized observer. We prove that the origin of the estimation error is\nlocally exponentially stable under the proposed observer, provided that the\nminimal observability conditions are satisfied. Moreover, we evaluate the\nproposed approach through experiments with real 1/10th-scale connected vehicles\nand large-scale simulations, demonstrating its scalability and validating the\ntheoretical guarantees in practical scenarios.\n", "link": "http://arxiv.org/abs/2507.20772v1", "date": "2025-07-28", "relevancy": 2.977, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6209}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6024}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Line-of-Sight%3A%20Cooperative%20Localization%20Using%20Vision%20and%20V2X%0A%20%20Communication&body=Title%3A%20Beyond%20Line-of-Sight%3A%20Cooperative%20Localization%20Using%20Vision%20and%20V2X%0A%20%20Communication%0AAuthor%3A%20Annika%20Wong%20and%20Zhiqi%20Tang%20and%20Frank%20J.%20Jiang%20and%20Karl%20H.%20Johansson%20and%20Jonas%20M%C3%A5rtensson%0AAbstract%3A%20%20%20Accurate%20and%20robust%20localization%20is%20critical%20for%20the%20safe%20operation%20of%0AConnected%20and%20Automated%20Vehicles%20%28CAVs%29%2C%20especially%20in%20complex%20urban%0Aenvironments%20where%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20signals%20are%0Aunreliable.%20This%20paper%20presents%20a%20novel%20vision-based%20cooperative%20localization%0Aalgorithm%20that%20leverages%20onboard%20cameras%20and%20Vehicle-to-Everything%20%28V2X%29%0Acommunication%20to%20enable%20CAVs%20to%20estimate%20their%20poses%2C%20even%20in%20occlusion-heavy%0Ascenarios%20such%20as%20busy%20intersections.%20In%20particular%2C%20we%20propose%20a%20novel%0Adecentralized%20observer%20for%20a%20group%20of%20connected%20agents%20that%20includes%20landmark%0Aagents%20%28static%20or%20moving%29%20in%20the%20environment%20with%20known%20positions%20and%20vehicle%0Aagents%20that%20need%20to%20estimate%20their%20poses%20%28both%20positions%20and%20orientations%29.%0AAssuming%20that%20%28i%29%20there%20are%20at%20least%20three%20landmark%20agents%20in%20the%20environment%2C%0A%28ii%29%20each%20vehicle%20agent%20can%20measure%20its%20own%20angular%20and%20translational%0Avelocities%20as%20well%20as%20relative%20bearings%20to%20at%20least%20three%20neighboring%20landmarks%0Aor%20vehicles%2C%20and%20%28iii%29%20neighboring%20vehicles%20can%20communicate%20their%20pose%0Aestimates%2C%20each%20vehicle%20can%20estimate%20its%20own%20pose%20using%20the%20proposed%0Adecentralized%20observer.%20We%20prove%20that%20the%20origin%20of%20the%20estimation%20error%20is%0Alocally%20exponentially%20stable%20under%20the%20proposed%20observer%2C%20provided%20that%20the%0Aminimal%20observability%20conditions%20are%20satisfied.%20Moreover%2C%20we%20evaluate%20the%0Aproposed%20approach%20through%20experiments%20with%20real%201/10th-scale%20connected%20vehicles%0Aand%20large-scale%20simulations%2C%20demonstrating%20its%20scalability%20and%20validating%20the%0Atheoretical%20guarantees%20in%20practical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Line-of-Sight%253A%2520Cooperative%2520Localization%2520Using%2520Vision%2520and%2520V2X%250A%2520%2520Communication%26entry.906535625%3DAnnika%2520Wong%2520and%2520Zhiqi%2520Tang%2520and%2520Frank%2520J.%2520Jiang%2520and%2520Karl%2520H.%2520Johansson%2520and%2520Jonas%2520M%25C3%25A5rtensson%26entry.1292438233%3D%2520%2520Accurate%2520and%2520robust%2520localization%2520is%2520critical%2520for%2520the%2520safe%2520operation%2520of%250AConnected%2520and%2520Automated%2520Vehicles%2520%2528CAVs%2529%252C%2520especially%2520in%2520complex%2520urban%250Aenvironments%2520where%2520Global%2520Navigation%2520Satellite%2520System%2520%2528GNSS%2529%2520signals%2520are%250Aunreliable.%2520This%2520paper%2520presents%2520a%2520novel%2520vision-based%2520cooperative%2520localization%250Aalgorithm%2520that%2520leverages%2520onboard%2520cameras%2520and%2520Vehicle-to-Everything%2520%2528V2X%2529%250Acommunication%2520to%2520enable%2520CAVs%2520to%2520estimate%2520their%2520poses%252C%2520even%2520in%2520occlusion-heavy%250Ascenarios%2520such%2520as%2520busy%2520intersections.%2520In%2520particular%252C%2520we%2520propose%2520a%2520novel%250Adecentralized%2520observer%2520for%2520a%2520group%2520of%2520connected%2520agents%2520that%2520includes%2520landmark%250Aagents%2520%2528static%2520or%2520moving%2529%2520in%2520the%2520environment%2520with%2520known%2520positions%2520and%2520vehicle%250Aagents%2520that%2520need%2520to%2520estimate%2520their%2520poses%2520%2528both%2520positions%2520and%2520orientations%2529.%250AAssuming%2520that%2520%2528i%2529%2520there%2520are%2520at%2520least%2520three%2520landmark%2520agents%2520in%2520the%2520environment%252C%250A%2528ii%2529%2520each%2520vehicle%2520agent%2520can%2520measure%2520its%2520own%2520angular%2520and%2520translational%250Avelocities%2520as%2520well%2520as%2520relative%2520bearings%2520to%2520at%2520least%2520three%2520neighboring%2520landmarks%250Aor%2520vehicles%252C%2520and%2520%2528iii%2529%2520neighboring%2520vehicles%2520can%2520communicate%2520their%2520pose%250Aestimates%252C%2520each%2520vehicle%2520can%2520estimate%2520its%2520own%2520pose%2520using%2520the%2520proposed%250Adecentralized%2520observer.%2520We%2520prove%2520that%2520the%2520origin%2520of%2520the%2520estimation%2520error%2520is%250Alocally%2520exponentially%2520stable%2520under%2520the%2520proposed%2520observer%252C%2520provided%2520that%2520the%250Aminimal%2520observability%2520conditions%2520are%2520satisfied.%2520Moreover%252C%2520we%2520evaluate%2520the%250Aproposed%2520approach%2520through%2520experiments%2520with%2520real%25201/10th-scale%2520connected%2520vehicles%250Aand%2520large-scale%2520simulations%252C%2520demonstrating%2520its%2520scalability%2520and%2520validating%2520the%250Atheoretical%2520guarantees%2520in%2520practical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Line-of-Sight%3A%20Cooperative%20Localization%20Using%20Vision%20and%20V2X%0A%20%20Communication&entry.906535625=Annika%20Wong%20and%20Zhiqi%20Tang%20and%20Frank%20J.%20Jiang%20and%20Karl%20H.%20Johansson%20and%20Jonas%20M%C3%A5rtensson&entry.1292438233=%20%20Accurate%20and%20robust%20localization%20is%20critical%20for%20the%20safe%20operation%20of%0AConnected%20and%20Automated%20Vehicles%20%28CAVs%29%2C%20especially%20in%20complex%20urban%0Aenvironments%20where%20Global%20Navigation%20Satellite%20System%20%28GNSS%29%20signals%20are%0Aunreliable.%20This%20paper%20presents%20a%20novel%20vision-based%20cooperative%20localization%0Aalgorithm%20that%20leverages%20onboard%20cameras%20and%20Vehicle-to-Everything%20%28V2X%29%0Acommunication%20to%20enable%20CAVs%20to%20estimate%20their%20poses%2C%20even%20in%20occlusion-heavy%0Ascenarios%20such%20as%20busy%20intersections.%20In%20particular%2C%20we%20propose%20a%20novel%0Adecentralized%20observer%20for%20a%20group%20of%20connected%20agents%20that%20includes%20landmark%0Aagents%20%28static%20or%20moving%29%20in%20the%20environment%20with%20known%20positions%20and%20vehicle%0Aagents%20that%20need%20to%20estimate%20their%20poses%20%28both%20positions%20and%20orientations%29.%0AAssuming%20that%20%28i%29%20there%20are%20at%20least%20three%20landmark%20agents%20in%20the%20environment%2C%0A%28ii%29%20each%20vehicle%20agent%20can%20measure%20its%20own%20angular%20and%20translational%0Avelocities%20as%20well%20as%20relative%20bearings%20to%20at%20least%20three%20neighboring%20landmarks%0Aor%20vehicles%2C%20and%20%28iii%29%20neighboring%20vehicles%20can%20communicate%20their%20pose%0Aestimates%2C%20each%20vehicle%20can%20estimate%20its%20own%20pose%20using%20the%20proposed%0Adecentralized%20observer.%20We%20prove%20that%20the%20origin%20of%20the%20estimation%20error%20is%0Alocally%20exponentially%20stable%20under%20the%20proposed%20observer%2C%20provided%20that%20the%0Aminimal%20observability%20conditions%20are%20satisfied.%20Moreover%2C%20we%20evaluate%20the%0Aproposed%20approach%20through%20experiments%20with%20real%201/10th-scale%20connected%20vehicles%0Aand%20large-scale%20simulations%2C%20demonstrating%20its%20scalability%20and%20validating%20the%0Atheoretical%20guarantees%20in%20practical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20772v1&entry.124074799=Read"},
{"title": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision\n  Language Models", "author": "Yuchen Liu and Yaoming Wang and Bowen Shi and Xiaopeng Zhang and Wenrui Dai and Chenglin Li and Hongkai Xiong and Qi Tian", "abstract": "  Vision encoders serve as the cornerstone of multimodal understanding.\nSingle-encoder architectures like CLIP exhibit inherent constraints in\ngeneralizing across diverse multimodal tasks, while recent multi-encoder fusion\nmethods introduce prohibitive computational overhead to achieve superior\nperformance using complementary visual representations from multiple vision\nencoders. To address this, we propose a progressive pruning framework, namely\nMulti-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant\nvisual tokens across the encoding, fusion, and decoding stages for\nmulti-encoder MLLMs. For multi-vision encoding, we discard redundant tokens\nwithin each encoder via a rank guided collaborative token assignment strategy.\nSubsequently, for multi-vision fusion, we combine the visual features from\ndifferent encoders while reducing cross-encoder redundancy with cooperative\npruning. Finally, we propose an adaptive token pruning method in the LLM\ndecoding stage to further discard irrelevant tokens based on the text prompts\nwith dynamically adjusting pruning ratios for specific task demands. To our\nbest knowledge, this is the first successful attempt that achieves an efficient\nmulti-encoder based vision language model with multi-stage pruning strategies.\nExtensive experiments on 11 benchmarks demonstrate the effectiveness of our\nproposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR\nreduces 76% visual tokens with only 0.3% performance drop in average. The code\nis available at https://github.com/YuchenLiu98/METEOR.\n", "link": "http://arxiv.org/abs/2507.20842v1", "date": "2025-07-28", "relevancy": 2.8904, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5868}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20METEOR%3A%20Multi-Encoder%20Collaborative%20Token%20Pruning%20for%20Efficient%20Vision%0A%20%20Language%20Models&body=Title%3A%20METEOR%3A%20Multi-Encoder%20Collaborative%20Token%20Pruning%20for%20Efficient%20Vision%0A%20%20Language%20Models%0AAuthor%3A%20Yuchen%20Liu%20and%20Yaoming%20Wang%20and%20Bowen%20Shi%20and%20Xiaopeng%20Zhang%20and%20Wenrui%20Dai%20and%20Chenglin%20Li%20and%20Hongkai%20Xiong%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Vision%20encoders%20serve%20as%20the%20cornerstone%20of%20multimodal%20understanding.%0ASingle-encoder%20architectures%20like%20CLIP%20exhibit%20inherent%20constraints%20in%0Ageneralizing%20across%20diverse%20multimodal%20tasks%2C%20while%20recent%20multi-encoder%20fusion%0Amethods%20introduce%20prohibitive%20computational%20overhead%20to%20achieve%20superior%0Aperformance%20using%20complementary%20visual%20representations%20from%20multiple%20vision%0Aencoders.%20To%20address%20this%2C%20we%20propose%20a%20progressive%20pruning%20framework%2C%20namely%0AMulti-Encoder%20collaboraTivE%20tOken%20pRuning%20%28METEOR%29%2C%20that%20eliminates%20redundant%0Avisual%20tokens%20across%20the%20encoding%2C%20fusion%2C%20and%20decoding%20stages%20for%0Amulti-encoder%20MLLMs.%20For%20multi-vision%20encoding%2C%20we%20discard%20redundant%20tokens%0Awithin%20each%20encoder%20via%20a%20rank%20guided%20collaborative%20token%20assignment%20strategy.%0ASubsequently%2C%20for%20multi-vision%20fusion%2C%20we%20combine%20the%20visual%20features%20from%0Adifferent%20encoders%20while%20reducing%20cross-encoder%20redundancy%20with%20cooperative%0Apruning.%20Finally%2C%20we%20propose%20an%20adaptive%20token%20pruning%20method%20in%20the%20LLM%0Adecoding%20stage%20to%20further%20discard%20irrelevant%20tokens%20based%20on%20the%20text%20prompts%0Awith%20dynamically%20adjusting%20pruning%20ratios%20for%20specific%20task%20demands.%20To%20our%0Abest%20knowledge%2C%20this%20is%20the%20first%20successful%20attempt%20that%20achieves%20an%20efficient%0Amulti-encoder%20based%20vision%20language%20model%20with%20multi-stage%20pruning%20strategies.%0AExtensive%20experiments%20on%2011%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20approach.%20Compared%20with%20EAGLE%2C%20a%20typical%20multi-encoder%20MLLMs%2C%20METEOR%0Areduces%2076%25%20visual%20tokens%20with%20only%200.3%25%20performance%20drop%20in%20average.%20The%20code%0Ais%20available%20at%20https%3A//github.com/YuchenLiu98/METEOR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMETEOR%253A%2520Multi-Encoder%2520Collaborative%2520Token%2520Pruning%2520for%2520Efficient%2520Vision%250A%2520%2520Language%2520Models%26entry.906535625%3DYuchen%2520Liu%2520and%2520Yaoming%2520Wang%2520and%2520Bowen%2520Shi%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wenrui%2520Dai%2520and%2520Chenglin%2520Li%2520and%2520Hongkai%2520Xiong%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Vision%2520encoders%2520serve%2520as%2520the%2520cornerstone%2520of%2520multimodal%2520understanding.%250ASingle-encoder%2520architectures%2520like%2520CLIP%2520exhibit%2520inherent%2520constraints%2520in%250Ageneralizing%2520across%2520diverse%2520multimodal%2520tasks%252C%2520while%2520recent%2520multi-encoder%2520fusion%250Amethods%2520introduce%2520prohibitive%2520computational%2520overhead%2520to%2520achieve%2520superior%250Aperformance%2520using%2520complementary%2520visual%2520representations%2520from%2520multiple%2520vision%250Aencoders.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520progressive%2520pruning%2520framework%252C%2520namely%250AMulti-Encoder%2520collaboraTivE%2520tOken%2520pRuning%2520%2528METEOR%2529%252C%2520that%2520eliminates%2520redundant%250Avisual%2520tokens%2520across%2520the%2520encoding%252C%2520fusion%252C%2520and%2520decoding%2520stages%2520for%250Amulti-encoder%2520MLLMs.%2520For%2520multi-vision%2520encoding%252C%2520we%2520discard%2520redundant%2520tokens%250Awithin%2520each%2520encoder%2520via%2520a%2520rank%2520guided%2520collaborative%2520token%2520assignment%2520strategy.%250ASubsequently%252C%2520for%2520multi-vision%2520fusion%252C%2520we%2520combine%2520the%2520visual%2520features%2520from%250Adifferent%2520encoders%2520while%2520reducing%2520cross-encoder%2520redundancy%2520with%2520cooperative%250Apruning.%2520Finally%252C%2520we%2520propose%2520an%2520adaptive%2520token%2520pruning%2520method%2520in%2520the%2520LLM%250Adecoding%2520stage%2520to%2520further%2520discard%2520irrelevant%2520tokens%2520based%2520on%2520the%2520text%2520prompts%250Awith%2520dynamically%2520adjusting%2520pruning%2520ratios%2520for%2520specific%2520task%2520demands.%2520To%2520our%250Abest%2520knowledge%252C%2520this%2520is%2520the%2520first%2520successful%2520attempt%2520that%2520achieves%2520an%2520efficient%250Amulti-encoder%2520based%2520vision%2520language%2520model%2520with%2520multi-stage%2520pruning%2520strategies.%250AExtensive%2520experiments%2520on%252011%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520approach.%2520Compared%2520with%2520EAGLE%252C%2520a%2520typical%2520multi-encoder%2520MLLMs%252C%2520METEOR%250Areduces%252076%2525%2520visual%2520tokens%2520with%2520only%25200.3%2525%2520performance%2520drop%2520in%2520average.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/YuchenLiu98/METEOR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=METEOR%3A%20Multi-Encoder%20Collaborative%20Token%20Pruning%20for%20Efficient%20Vision%0A%20%20Language%20Models&entry.906535625=Yuchen%20Liu%20and%20Yaoming%20Wang%20and%20Bowen%20Shi%20and%20Xiaopeng%20Zhang%20and%20Wenrui%20Dai%20and%20Chenglin%20Li%20and%20Hongkai%20Xiong%20and%20Qi%20Tian&entry.1292438233=%20%20Vision%20encoders%20serve%20as%20the%20cornerstone%20of%20multimodal%20understanding.%0ASingle-encoder%20architectures%20like%20CLIP%20exhibit%20inherent%20constraints%20in%0Ageneralizing%20across%20diverse%20multimodal%20tasks%2C%20while%20recent%20multi-encoder%20fusion%0Amethods%20introduce%20prohibitive%20computational%20overhead%20to%20achieve%20superior%0Aperformance%20using%20complementary%20visual%20representations%20from%20multiple%20vision%0Aencoders.%20To%20address%20this%2C%20we%20propose%20a%20progressive%20pruning%20framework%2C%20namely%0AMulti-Encoder%20collaboraTivE%20tOken%20pRuning%20%28METEOR%29%2C%20that%20eliminates%20redundant%0Avisual%20tokens%20across%20the%20encoding%2C%20fusion%2C%20and%20decoding%20stages%20for%0Amulti-encoder%20MLLMs.%20For%20multi-vision%20encoding%2C%20we%20discard%20redundant%20tokens%0Awithin%20each%20encoder%20via%20a%20rank%20guided%20collaborative%20token%20assignment%20strategy.%0ASubsequently%2C%20for%20multi-vision%20fusion%2C%20we%20combine%20the%20visual%20features%20from%0Adifferent%20encoders%20while%20reducing%20cross-encoder%20redundancy%20with%20cooperative%0Apruning.%20Finally%2C%20we%20propose%20an%20adaptive%20token%20pruning%20method%20in%20the%20LLM%0Adecoding%20stage%20to%20further%20discard%20irrelevant%20tokens%20based%20on%20the%20text%20prompts%0Awith%20dynamically%20adjusting%20pruning%20ratios%20for%20specific%20task%20demands.%20To%20our%0Abest%20knowledge%2C%20this%20is%20the%20first%20successful%20attempt%20that%20achieves%20an%20efficient%0Amulti-encoder%20based%20vision%20language%20model%20with%20multi-stage%20pruning%20strategies.%0AExtensive%20experiments%20on%2011%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%0Aproposed%20approach.%20Compared%20with%20EAGLE%2C%20a%20typical%20multi-encoder%20MLLMs%2C%20METEOR%0Areduces%2076%25%20visual%20tokens%20with%20only%200.3%25%20performance%20drop%20in%20average.%20The%20code%0Ais%20available%20at%20https%3A//github.com/YuchenLiu98/METEOR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20842v1&entry.124074799=Read"},
{"title": "Aether: Geometric-Aware Unified World Modeling", "author": " Aether Team and Haoyi Zhu and Yifan Wang and Jianjun Zhou and Wenzheng Chang and Yang Zhou and Zizun Li and Junyi Chen and Chunhua Shen and Jiangmiao Pang and Tong He", "abstract": "  The integration of geometric reconstruction and generative modeling remains a\ncritical challenge in developing AI systems capable of human-like spatial\nreasoning. This paper proposes Aether, a unified framework that enables\ngeometry-aware reasoning in world models by jointly optimizing three core\ncapabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video\nprediction, and (3) goal-conditioned visual planning. Through task-interleaved\nfeature learning, Aether achieves synergistic knowledge sharing across\nreconstruction, prediction, and planning objectives. Building upon video\ngeneration models, our framework demonstrates zero-shot synthetic-to-real\ngeneralization despite never observing real-world data during training.\nFurthermore, our approach achieves zero-shot generalization in both action\nfollowing and reconstruction tasks, thanks to its intrinsic geometric modeling.\nNotably, even without real-world data, its reconstruction performance is\ncomparable with or even better than that of domain-specific models.\nAdditionally, Aether employs camera trajectories as geometry-informed action\nspaces, enabling effective action-conditioned prediction and visual planning.\nWe hope our work inspires the community to explore new frontiers in\nphysically-reasonable world modeling and its applications.\n", "link": "http://arxiv.org/abs/2503.18945v3", "date": "2025-07-28", "relevancy": 2.8769, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6017}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5683}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aether%3A%20Geometric-Aware%20Unified%20World%20Modeling&body=Title%3A%20Aether%3A%20Geometric-Aware%20Unified%20World%20Modeling%0AAuthor%3A%20%20Aether%20Team%20and%20Haoyi%20Zhu%20and%20Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Zizun%20Li%20and%20Junyi%20Chen%20and%20Chunhua%20Shen%20and%20Jiangmiao%20Pang%20and%20Tong%20He%0AAbstract%3A%20%20%20The%20integration%20of%20geometric%20reconstruction%20and%20generative%20modeling%20remains%20a%0Acritical%20challenge%20in%20developing%20AI%20systems%20capable%20of%20human-like%20spatial%0Areasoning.%20This%20paper%20proposes%20Aether%2C%20a%20unified%20framework%20that%20enables%0Ageometry-aware%20reasoning%20in%20world%20models%20by%20jointly%20optimizing%20three%20core%0Acapabilities%3A%20%281%29%204D%20dynamic%20reconstruction%2C%20%282%29%20action-conditioned%20video%0Aprediction%2C%20and%20%283%29%20goal-conditioned%20visual%20planning.%20Through%20task-interleaved%0Afeature%20learning%2C%20Aether%20achieves%20synergistic%20knowledge%20sharing%20across%0Areconstruction%2C%20prediction%2C%20and%20planning%20objectives.%20Building%20upon%20video%0Ageneration%20models%2C%20our%20framework%20demonstrates%20zero-shot%20synthetic-to-real%0Ageneralization%20despite%20never%20observing%20real-world%20data%20during%20training.%0AFurthermore%2C%20our%20approach%20achieves%20zero-shot%20generalization%20in%20both%20action%0Afollowing%20and%20reconstruction%20tasks%2C%20thanks%20to%20its%20intrinsic%20geometric%20modeling.%0ANotably%2C%20even%20without%20real-world%20data%2C%20its%20reconstruction%20performance%20is%0Acomparable%20with%20or%20even%20better%20than%20that%20of%20domain-specific%20models.%0AAdditionally%2C%20Aether%20employs%20camera%20trajectories%20as%20geometry-informed%20action%0Aspaces%2C%20enabling%20effective%20action-conditioned%20prediction%20and%20visual%20planning.%0AWe%20hope%20our%20work%20inspires%20the%20community%20to%20explore%20new%20frontiers%20in%0Aphysically-reasonable%20world%20modeling%20and%20its%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAether%253A%2520Geometric-Aware%2520Unified%2520World%2520Modeling%26entry.906535625%3D%2520Aether%2520Team%2520and%2520Haoyi%2520Zhu%2520and%2520Yifan%2520Wang%2520and%2520Jianjun%2520Zhou%2520and%2520Wenzheng%2520Chang%2520and%2520Yang%2520Zhou%2520and%2520Zizun%2520Li%2520and%2520Junyi%2520Chen%2520and%2520Chunhua%2520Shen%2520and%2520Jiangmiao%2520Pang%2520and%2520Tong%2520He%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520geometric%2520reconstruction%2520and%2520generative%2520modeling%2520remains%2520a%250Acritical%2520challenge%2520in%2520developing%2520AI%2520systems%2520capable%2520of%2520human-like%2520spatial%250Areasoning.%2520This%2520paper%2520proposes%2520Aether%252C%2520a%2520unified%2520framework%2520that%2520enables%250Ageometry-aware%2520reasoning%2520in%2520world%2520models%2520by%2520jointly%2520optimizing%2520three%2520core%250Acapabilities%253A%2520%25281%2529%25204D%2520dynamic%2520reconstruction%252C%2520%25282%2529%2520action-conditioned%2520video%250Aprediction%252C%2520and%2520%25283%2529%2520goal-conditioned%2520visual%2520planning.%2520Through%2520task-interleaved%250Afeature%2520learning%252C%2520Aether%2520achieves%2520synergistic%2520knowledge%2520sharing%2520across%250Areconstruction%252C%2520prediction%252C%2520and%2520planning%2520objectives.%2520Building%2520upon%2520video%250Ageneration%2520models%252C%2520our%2520framework%2520demonstrates%2520zero-shot%2520synthetic-to-real%250Ageneralization%2520despite%2520never%2520observing%2520real-world%2520data%2520during%2520training.%250AFurthermore%252C%2520our%2520approach%2520achieves%2520zero-shot%2520generalization%2520in%2520both%2520action%250Afollowing%2520and%2520reconstruction%2520tasks%252C%2520thanks%2520to%2520its%2520intrinsic%2520geometric%2520modeling.%250ANotably%252C%2520even%2520without%2520real-world%2520data%252C%2520its%2520reconstruction%2520performance%2520is%250Acomparable%2520with%2520or%2520even%2520better%2520than%2520that%2520of%2520domain-specific%2520models.%250AAdditionally%252C%2520Aether%2520employs%2520camera%2520trajectories%2520as%2520geometry-informed%2520action%250Aspaces%252C%2520enabling%2520effective%2520action-conditioned%2520prediction%2520and%2520visual%2520planning.%250AWe%2520hope%2520our%2520work%2520inspires%2520the%2520community%2520to%2520explore%2520new%2520frontiers%2520in%250Aphysically-reasonable%2520world%2520modeling%2520and%2520its%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aether%3A%20Geometric-Aware%20Unified%20World%20Modeling&entry.906535625=%20Aether%20Team%20and%20Haoyi%20Zhu%20and%20Yifan%20Wang%20and%20Jianjun%20Zhou%20and%20Wenzheng%20Chang%20and%20Yang%20Zhou%20and%20Zizun%20Li%20and%20Junyi%20Chen%20and%20Chunhua%20Shen%20and%20Jiangmiao%20Pang%20and%20Tong%20He&entry.1292438233=%20%20The%20integration%20of%20geometric%20reconstruction%20and%20generative%20modeling%20remains%20a%0Acritical%20challenge%20in%20developing%20AI%20systems%20capable%20of%20human-like%20spatial%0Areasoning.%20This%20paper%20proposes%20Aether%2C%20a%20unified%20framework%20that%20enables%0Ageometry-aware%20reasoning%20in%20world%20models%20by%20jointly%20optimizing%20three%20core%0Acapabilities%3A%20%281%29%204D%20dynamic%20reconstruction%2C%20%282%29%20action-conditioned%20video%0Aprediction%2C%20and%20%283%29%20goal-conditioned%20visual%20planning.%20Through%20task-interleaved%0Afeature%20learning%2C%20Aether%20achieves%20synergistic%20knowledge%20sharing%20across%0Areconstruction%2C%20prediction%2C%20and%20planning%20objectives.%20Building%20upon%20video%0Ageneration%20models%2C%20our%20framework%20demonstrates%20zero-shot%20synthetic-to-real%0Ageneralization%20despite%20never%20observing%20real-world%20data%20during%20training.%0AFurthermore%2C%20our%20approach%20achieves%20zero-shot%20generalization%20in%20both%20action%0Afollowing%20and%20reconstruction%20tasks%2C%20thanks%20to%20its%20intrinsic%20geometric%20modeling.%0ANotably%2C%20even%20without%20real-world%20data%2C%20its%20reconstruction%20performance%20is%0Acomparable%20with%20or%20even%20better%20than%20that%20of%20domain-specific%20models.%0AAdditionally%2C%20Aether%20employs%20camera%20trajectories%20as%20geometry-informed%20action%0Aspaces%2C%20enabling%20effective%20action-conditioned%20prediction%20and%20visual%20planning.%0AWe%20hope%20our%20work%20inspires%20the%20community%20to%20explore%20new%20frontiers%20in%0Aphysically-reasonable%20world%20modeling%20and%20its%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18945v3&entry.124074799=Read"},
{"title": "Implicit Counterfactual Learning for Audio-Visual Segmentation", "author": "Mingfeng Zha and Tianyu Li and Guoqing Wang and Peng Wang and Yangyang Wu and Yang Yang and Heng Tao Shen", "abstract": "  Audio-visual segmentation (AVS) aims to segment objects in videos based on\naudio cues. Existing AVS methods are primarily designed to enhance interaction\nefficiency but pay limited attention to modality representation discrepancies\nand imbalances. To overcome this, we propose the implicit counterfactual\nframework (ICF) to achieve unbiased cross-modal understanding. Due to the lack\nof semantics, heterogeneous representations may lead to erroneous matches,\nespecially in complex scenes with ambiguous visual content or interference from\nmultiple audio sources. We introduce the multi-granularity implicit text (MIT)\ninvolving video-, segment- and frame-level as the bridge to establish the\nmodality-shared space, reducing modality gaps and providing prior guidance.\nVisual content carries more information and typically dominates, thereby\nmarginalizing audio features in the decision-making. To mitigate knowledge\npreference, we propose the semantic counterfactual (SC) to learn orthogonal\nrepresentations in the latent space, generating diverse counterfactual samples,\nthus avoiding biases introduced by complex functional designs and explicit\nmodifications of text structures or attributes. We further formulate the\ncollaborative distribution-aware contrastive learning (CDCL), incorporating\nfactual-counterfactual and inter-modality contrasts to align representations,\npromoting cohesion and decoupling. Extensive experiments on three public\ndatasets validate that the proposed method achieves state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2507.20740v1", "date": "2025-07-28", "relevancy": 2.8621, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Counterfactual%20Learning%20for%20Audio-Visual%20Segmentation&body=Title%3A%20Implicit%20Counterfactual%20Learning%20for%20Audio-Visual%20Segmentation%0AAuthor%3A%20Mingfeng%20Zha%20and%20Tianyu%20Li%20and%20Guoqing%20Wang%20and%20Peng%20Wang%20and%20Yangyang%20Wu%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen%0AAbstract%3A%20%20%20Audio-visual%20segmentation%20%28AVS%29%20aims%20to%20segment%20objects%20in%20videos%20based%20on%0Aaudio%20cues.%20Existing%20AVS%20methods%20are%20primarily%20designed%20to%20enhance%20interaction%0Aefficiency%20but%20pay%20limited%20attention%20to%20modality%20representation%20discrepancies%0Aand%20imbalances.%20To%20overcome%20this%2C%20we%20propose%20the%20implicit%20counterfactual%0Aframework%20%28ICF%29%20to%20achieve%20unbiased%20cross-modal%20understanding.%20Due%20to%20the%20lack%0Aof%20semantics%2C%20heterogeneous%20representations%20may%20lead%20to%20erroneous%20matches%2C%0Aespecially%20in%20complex%20scenes%20with%20ambiguous%20visual%20content%20or%20interference%20from%0Amultiple%20audio%20sources.%20We%20introduce%20the%20multi-granularity%20implicit%20text%20%28MIT%29%0Ainvolving%20video-%2C%20segment-%20and%20frame-level%20as%20the%20bridge%20to%20establish%20the%0Amodality-shared%20space%2C%20reducing%20modality%20gaps%20and%20providing%20prior%20guidance.%0AVisual%20content%20carries%20more%20information%20and%20typically%20dominates%2C%20thereby%0Amarginalizing%20audio%20features%20in%20the%20decision-making.%20To%20mitigate%20knowledge%0Apreference%2C%20we%20propose%20the%20semantic%20counterfactual%20%28SC%29%20to%20learn%20orthogonal%0Arepresentations%20in%20the%20latent%20space%2C%20generating%20diverse%20counterfactual%20samples%2C%0Athus%20avoiding%20biases%20introduced%20by%20complex%20functional%20designs%20and%20explicit%0Amodifications%20of%20text%20structures%20or%20attributes.%20We%20further%20formulate%20the%0Acollaborative%20distribution-aware%20contrastive%20learning%20%28CDCL%29%2C%20incorporating%0Afactual-counterfactual%20and%20inter-modality%20contrasts%20to%20align%20representations%2C%0Apromoting%20cohesion%20and%20decoupling.%20Extensive%20experiments%20on%20three%20public%0Adatasets%20validate%20that%20the%20proposed%20method%20achieves%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Counterfactual%2520Learning%2520for%2520Audio-Visual%2520Segmentation%26entry.906535625%3DMingfeng%2520Zha%2520and%2520Tianyu%2520Li%2520and%2520Guoqing%2520Wang%2520and%2520Peng%2520Wang%2520and%2520Yangyang%2520Wu%2520and%2520Yang%2520Yang%2520and%2520Heng%2520Tao%2520Shen%26entry.1292438233%3D%2520%2520Audio-visual%2520segmentation%2520%2528AVS%2529%2520aims%2520to%2520segment%2520objects%2520in%2520videos%2520based%2520on%250Aaudio%2520cues.%2520Existing%2520AVS%2520methods%2520are%2520primarily%2520designed%2520to%2520enhance%2520interaction%250Aefficiency%2520but%2520pay%2520limited%2520attention%2520to%2520modality%2520representation%2520discrepancies%250Aand%2520imbalances.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520the%2520implicit%2520counterfactual%250Aframework%2520%2528ICF%2529%2520to%2520achieve%2520unbiased%2520cross-modal%2520understanding.%2520Due%2520to%2520the%2520lack%250Aof%2520semantics%252C%2520heterogeneous%2520representations%2520may%2520lead%2520to%2520erroneous%2520matches%252C%250Aespecially%2520in%2520complex%2520scenes%2520with%2520ambiguous%2520visual%2520content%2520or%2520interference%2520from%250Amultiple%2520audio%2520sources.%2520We%2520introduce%2520the%2520multi-granularity%2520implicit%2520text%2520%2528MIT%2529%250Ainvolving%2520video-%252C%2520segment-%2520and%2520frame-level%2520as%2520the%2520bridge%2520to%2520establish%2520the%250Amodality-shared%2520space%252C%2520reducing%2520modality%2520gaps%2520and%2520providing%2520prior%2520guidance.%250AVisual%2520content%2520carries%2520more%2520information%2520and%2520typically%2520dominates%252C%2520thereby%250Amarginalizing%2520audio%2520features%2520in%2520the%2520decision-making.%2520To%2520mitigate%2520knowledge%250Apreference%252C%2520we%2520propose%2520the%2520semantic%2520counterfactual%2520%2528SC%2529%2520to%2520learn%2520orthogonal%250Arepresentations%2520in%2520the%2520latent%2520space%252C%2520generating%2520diverse%2520counterfactual%2520samples%252C%250Athus%2520avoiding%2520biases%2520introduced%2520by%2520complex%2520functional%2520designs%2520and%2520explicit%250Amodifications%2520of%2520text%2520structures%2520or%2520attributes.%2520We%2520further%2520formulate%2520the%250Acollaborative%2520distribution-aware%2520contrastive%2520learning%2520%2528CDCL%2529%252C%2520incorporating%250Afactual-counterfactual%2520and%2520inter-modality%2520contrasts%2520to%2520align%2520representations%252C%250Apromoting%2520cohesion%2520and%2520decoupling.%2520Extensive%2520experiments%2520on%2520three%2520public%250Adatasets%2520validate%2520that%2520the%2520proposed%2520method%2520achieves%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Counterfactual%20Learning%20for%20Audio-Visual%20Segmentation&entry.906535625=Mingfeng%20Zha%20and%20Tianyu%20Li%20and%20Guoqing%20Wang%20and%20Peng%20Wang%20and%20Yangyang%20Wu%20and%20Yang%20Yang%20and%20Heng%20Tao%20Shen&entry.1292438233=%20%20Audio-visual%20segmentation%20%28AVS%29%20aims%20to%20segment%20objects%20in%20videos%20based%20on%0Aaudio%20cues.%20Existing%20AVS%20methods%20are%20primarily%20designed%20to%20enhance%20interaction%0Aefficiency%20but%20pay%20limited%20attention%20to%20modality%20representation%20discrepancies%0Aand%20imbalances.%20To%20overcome%20this%2C%20we%20propose%20the%20implicit%20counterfactual%0Aframework%20%28ICF%29%20to%20achieve%20unbiased%20cross-modal%20understanding.%20Due%20to%20the%20lack%0Aof%20semantics%2C%20heterogeneous%20representations%20may%20lead%20to%20erroneous%20matches%2C%0Aespecially%20in%20complex%20scenes%20with%20ambiguous%20visual%20content%20or%20interference%20from%0Amultiple%20audio%20sources.%20We%20introduce%20the%20multi-granularity%20implicit%20text%20%28MIT%29%0Ainvolving%20video-%2C%20segment-%20and%20frame-level%20as%20the%20bridge%20to%20establish%20the%0Amodality-shared%20space%2C%20reducing%20modality%20gaps%20and%20providing%20prior%20guidance.%0AVisual%20content%20carries%20more%20information%20and%20typically%20dominates%2C%20thereby%0Amarginalizing%20audio%20features%20in%20the%20decision-making.%20To%20mitigate%20knowledge%0Apreference%2C%20we%20propose%20the%20semantic%20counterfactual%20%28SC%29%20to%20learn%20orthogonal%0Arepresentations%20in%20the%20latent%20space%2C%20generating%20diverse%20counterfactual%20samples%2C%0Athus%20avoiding%20biases%20introduced%20by%20complex%20functional%20designs%20and%20explicit%0Amodifications%20of%20text%20structures%20or%20attributes.%20We%20further%20formulate%20the%0Acollaborative%20distribution-aware%20contrastive%20learning%20%28CDCL%29%2C%20incorporating%0Afactual-counterfactual%20and%20inter-modality%20contrasts%20to%20align%20representations%2C%0Apromoting%20cohesion%20and%20decoupling.%20Extensive%20experiments%20on%20three%20public%0Adatasets%20validate%20that%20the%20proposed%20method%20achieves%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20740v1&entry.124074799=Read"},
{"title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning,\n  Rendering, and Visual Feedback", "author": "Yang Chen and Yufan Shen and Wenxuan Huang and Shen Zhou and Qunshu Lin and Xinyu Cai and Zhi Yu and Botian Shi and Yu Qiao", "abstract": "  Multimodal Large Language Models (MLLMs) have exhibited impressive\nperformance across various visual tasks. Subsequent investigations into\nenhancing their visual reasoning abilities have significantly expanded their\nperformance envelope. However, a critical bottleneck in the advancement of\nMLLMs toward deep visual reasoning is their heavy reliance on curated\nimage-text supervision. To solve this problem, we introduce a novel framework\ntermed ``Reasoning-Rendering-Visual-Feedback'' (RRVF), which enables MLLMs to\nlearn complex visual reasoning from only raw images. This framework builds on\nthe ``Asymmetry of Verification'' principle to train MLLMs, i.e., verifying the\nrendered output against a source image is easier than generating it. We\ndemonstrate that this relative ease provides an ideal reward signal for\noptimization via Reinforcement Learning (RL) training, reducing the reliance on\nthe image-text supervision. Guided by the above principle, RRVF implements a\nclosed-loop iterative process encompassing reasoning, rendering, and visual\nfeedback components, enabling the model to perform self-correction through\nmulti-turn interactions and tool invocation, while this pipeline can be\noptimized by the GRPO algorithm in an end-to-end manner. Extensive experiments\non image-to-code generation for data charts and web interfaces show that RRVF\nsubstantially outperforms existing open-source MLLMs and surpasses supervised\nfine-tuning baselines. Our findings demonstrate that systems driven by purely\nvisual feedback present a viable path toward more robust and generalizable\nreasoning models without requiring explicit supervision. Code will be available\nat https://github.com/L-O-I/RRVF.\n", "link": "http://arxiv.org/abs/2507.20766v1", "date": "2025-07-28", "relevancy": 2.8455, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback&body=Title%3A%20Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback%0AAuthor%3A%20Yang%20Chen%20and%20Yufan%20Shen%20and%20Wenxuan%20Huang%20and%20Shen%20Zhou%20and%20Qunshu%20Lin%20and%20Xinyu%20Cai%20and%20Zhi%20Yu%20and%20Botian%20Shi%20and%20Yu%20Qiao%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20exhibited%20impressive%0Aperformance%20across%20various%20visual%20tasks.%20Subsequent%20investigations%20into%0Aenhancing%20their%20visual%20reasoning%20abilities%20have%20significantly%20expanded%20their%0Aperformance%20envelope.%20However%2C%20a%20critical%20bottleneck%20in%20the%20advancement%20of%0AMLLMs%20toward%20deep%20visual%20reasoning%20is%20their%20heavy%20reliance%20on%20curated%0Aimage-text%20supervision.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%20framework%0Atermed%20%60%60Reasoning-Rendering-Visual-Feedback%27%27%20%28RRVF%29%2C%20which%20enables%20MLLMs%20to%0Alearn%20complex%20visual%20reasoning%20from%20only%20raw%20images.%20This%20framework%20builds%20on%0Athe%20%60%60Asymmetry%20of%20Verification%27%27%20principle%20to%20train%20MLLMs%2C%20i.e.%2C%20verifying%20the%0Arendered%20output%20against%20a%20source%20image%20is%20easier%20than%20generating%20it.%20We%0Ademonstrate%20that%20this%20relative%20ease%20provides%20an%20ideal%20reward%20signal%20for%0Aoptimization%20via%20Reinforcement%20Learning%20%28RL%29%20training%2C%20reducing%20the%20reliance%20on%0Athe%20image-text%20supervision.%20Guided%20by%20the%20above%20principle%2C%20RRVF%20implements%20a%0Aclosed-loop%20iterative%20process%20encompassing%20reasoning%2C%20rendering%2C%20and%20visual%0Afeedback%20components%2C%20enabling%20the%20model%20to%20perform%20self-correction%20through%0Amulti-turn%20interactions%20and%20tool%20invocation%2C%20while%20this%20pipeline%20can%20be%0Aoptimized%20by%20the%20GRPO%20algorithm%20in%20an%20end-to-end%20manner.%20Extensive%20experiments%0Aon%20image-to-code%20generation%20for%20data%20charts%20and%20web%20interfaces%20show%20that%20RRVF%0Asubstantially%20outperforms%20existing%20open-source%20MLLMs%20and%20surpasses%20supervised%0Afine-tuning%20baselines.%20Our%20findings%20demonstrate%20that%20systems%20driven%20by%20purely%0Avisual%20feedback%20present%20a%20viable%20path%20toward%20more%20robust%20and%20generalizable%0Areasoning%20models%20without%20requiring%20explicit%20supervision.%20Code%20will%20be%20available%0Aat%20https%3A//github.com/L-O-I/RRVF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Only%2520with%2520Images%253A%2520Visual%2520Reinforcement%2520Learning%2520with%2520Reasoning%252C%250A%2520%2520Rendering%252C%2520and%2520Visual%2520Feedback%26entry.906535625%3DYang%2520Chen%2520and%2520Yufan%2520Shen%2520and%2520Wenxuan%2520Huang%2520and%2520Shen%2520Zhou%2520and%2520Qunshu%2520Lin%2520and%2520Xinyu%2520Cai%2520and%2520Zhi%2520Yu%2520and%2520Botian%2520Shi%2520and%2520Yu%2520Qiao%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520exhibited%2520impressive%250Aperformance%2520across%2520various%2520visual%2520tasks.%2520Subsequent%2520investigations%2520into%250Aenhancing%2520their%2520visual%2520reasoning%2520abilities%2520have%2520significantly%2520expanded%2520their%250Aperformance%2520envelope.%2520However%252C%2520a%2520critical%2520bottleneck%2520in%2520the%2520advancement%2520of%250AMLLMs%2520toward%2520deep%2520visual%2520reasoning%2520is%2520their%2520heavy%2520reliance%2520on%2520curated%250Aimage-text%2520supervision.%2520To%2520solve%2520this%2520problem%252C%2520we%2520introduce%2520a%2520novel%2520framework%250Atermed%2520%2560%2560Reasoning-Rendering-Visual-Feedback%2527%2527%2520%2528RRVF%2529%252C%2520which%2520enables%2520MLLMs%2520to%250Alearn%2520complex%2520visual%2520reasoning%2520from%2520only%2520raw%2520images.%2520This%2520framework%2520builds%2520on%250Athe%2520%2560%2560Asymmetry%2520of%2520Verification%2527%2527%2520principle%2520to%2520train%2520MLLMs%252C%2520i.e.%252C%2520verifying%2520the%250Arendered%2520output%2520against%2520a%2520source%2520image%2520is%2520easier%2520than%2520generating%2520it.%2520We%250Ademonstrate%2520that%2520this%2520relative%2520ease%2520provides%2520an%2520ideal%2520reward%2520signal%2520for%250Aoptimization%2520via%2520Reinforcement%2520Learning%2520%2528RL%2529%2520training%252C%2520reducing%2520the%2520reliance%2520on%250Athe%2520image-text%2520supervision.%2520Guided%2520by%2520the%2520above%2520principle%252C%2520RRVF%2520implements%2520a%250Aclosed-loop%2520iterative%2520process%2520encompassing%2520reasoning%252C%2520rendering%252C%2520and%2520visual%250Afeedback%2520components%252C%2520enabling%2520the%2520model%2520to%2520perform%2520self-correction%2520through%250Amulti-turn%2520interactions%2520and%2520tool%2520invocation%252C%2520while%2520this%2520pipeline%2520can%2520be%250Aoptimized%2520by%2520the%2520GRPO%2520algorithm%2520in%2520an%2520end-to-end%2520manner.%2520Extensive%2520experiments%250Aon%2520image-to-code%2520generation%2520for%2520data%2520charts%2520and%2520web%2520interfaces%2520show%2520that%2520RRVF%250Asubstantially%2520outperforms%2520existing%2520open-source%2520MLLMs%2520and%2520surpasses%2520supervised%250Afine-tuning%2520baselines.%2520Our%2520findings%2520demonstrate%2520that%2520systems%2520driven%2520by%2520purely%250Avisual%2520feedback%2520present%2520a%2520viable%2520path%2520toward%2520more%2520robust%2520and%2520generalizable%250Areasoning%2520models%2520without%2520requiring%2520explicit%2520supervision.%2520Code%2520will%2520be%2520available%250Aat%2520https%253A//github.com/L-O-I/RRVF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Only%20with%20Images%3A%20Visual%20Reinforcement%20Learning%20with%20Reasoning%2C%0A%20%20Rendering%2C%20and%20Visual%20Feedback&entry.906535625=Yang%20Chen%20and%20Yufan%20Shen%20and%20Wenxuan%20Huang%20and%20Shen%20Zhou%20and%20Qunshu%20Lin%20and%20Xinyu%20Cai%20and%20Zhi%20Yu%20and%20Botian%20Shi%20and%20Yu%20Qiao&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20exhibited%20impressive%0Aperformance%20across%20various%20visual%20tasks.%20Subsequent%20investigations%20into%0Aenhancing%20their%20visual%20reasoning%20abilities%20have%20significantly%20expanded%20their%0Aperformance%20envelope.%20However%2C%20a%20critical%20bottleneck%20in%20the%20advancement%20of%0AMLLMs%20toward%20deep%20visual%20reasoning%20is%20their%20heavy%20reliance%20on%20curated%0Aimage-text%20supervision.%20To%20solve%20this%20problem%2C%20we%20introduce%20a%20novel%20framework%0Atermed%20%60%60Reasoning-Rendering-Visual-Feedback%27%27%20%28RRVF%29%2C%20which%20enables%20MLLMs%20to%0Alearn%20complex%20visual%20reasoning%20from%20only%20raw%20images.%20This%20framework%20builds%20on%0Athe%20%60%60Asymmetry%20of%20Verification%27%27%20principle%20to%20train%20MLLMs%2C%20i.e.%2C%20verifying%20the%0Arendered%20output%20against%20a%20source%20image%20is%20easier%20than%20generating%20it.%20We%0Ademonstrate%20that%20this%20relative%20ease%20provides%20an%20ideal%20reward%20signal%20for%0Aoptimization%20via%20Reinforcement%20Learning%20%28RL%29%20training%2C%20reducing%20the%20reliance%20on%0Athe%20image-text%20supervision.%20Guided%20by%20the%20above%20principle%2C%20RRVF%20implements%20a%0Aclosed-loop%20iterative%20process%20encompassing%20reasoning%2C%20rendering%2C%20and%20visual%0Afeedback%20components%2C%20enabling%20the%20model%20to%20perform%20self-correction%20through%0Amulti-turn%20interactions%20and%20tool%20invocation%2C%20while%20this%20pipeline%20can%20be%0Aoptimized%20by%20the%20GRPO%20algorithm%20in%20an%20end-to-end%20manner.%20Extensive%20experiments%0Aon%20image-to-code%20generation%20for%20data%20charts%20and%20web%20interfaces%20show%20that%20RRVF%0Asubstantially%20outperforms%20existing%20open-source%20MLLMs%20and%20surpasses%20supervised%0Afine-tuning%20baselines.%20Our%20findings%20demonstrate%20that%20systems%20driven%20by%20purely%0Avisual%20feedback%20present%20a%20viable%20path%20toward%20more%20robust%20and%20generalizable%0Areasoning%20models%20without%20requiring%20explicit%20supervision.%20Code%20will%20be%20available%0Aat%20https%3A//github.com/L-O-I/RRVF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20766v1&entry.124074799=Read"},
{"title": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\n  Attention-Guided Refinement", "author": "Zhecheng Li and Guoxian Song and Yiwei Wang and Zhen Xiong and Junsong Yuan and Yujun Cai", "abstract": "  Img2LaTeX is a practically significant task that involves converting\nmathematical expressions or tabular data from images into LaTeX code. In recent\nyears, vision-language models (VLMs) have demonstrated strong performance\nacross a variety of visual understanding tasks, owing to their generalization\ncapabilities. While some studies have explored the use of VLMs for the\nImg2LaTeX task, their performance often falls short of expectations.\nEmpirically, VLMs sometimes struggle with fine-grained visual elements, leading\nto inaccurate LaTeX predictions. To address this challenge, we propose\n$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention\nlocalization and iterative refinement within a visual reasoning framework,\nenabling VLMs to perform self-correction and progressively improve prediction\nquality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging\nexamples designed to rigorously evaluate the capabilities of VLMs within this\ntask domain. Extensive experimental results demonstrate that: (1) $A^2R^2$\nsignificantly improves model performance across six evaluation metrics spanning\nboth textual and visual levels, consistently outperforming other baseline\nmethods; (2) Increasing the number of inference rounds yields notable\nperformance gains, underscoring the potential of $A^2R^2$ in test-time scaling\nscenarios; (3) Ablation studies and human evaluations validate the practical\neffectiveness of our approach, as well as the strong synergy among its core\ncomponents during inference.\n", "link": "http://arxiv.org/abs/2507.20890v1", "date": "2025-07-28", "relevancy": 2.8325, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.591}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24A%5E2R%5E2%24%3A%20Advancing%20Img2LaTeX%20Conversion%20via%20Visual%20Reasoning%20with%0A%20%20Attention-Guided%20Refinement&body=Title%3A%20%24A%5E2R%5E2%24%3A%20Advancing%20Img2LaTeX%20Conversion%20via%20Visual%20Reasoning%20with%0A%20%20Attention-Guided%20Refinement%0AAuthor%3A%20Zhecheng%20Li%20and%20Guoxian%20Song%20and%20Yiwei%20Wang%20and%20Zhen%20Xiong%20and%20Junsong%20Yuan%20and%20Yujun%20Cai%0AAbstract%3A%20%20%20Img2LaTeX%20is%20a%20practically%20significant%20task%20that%20involves%20converting%0Amathematical%20expressions%20or%20tabular%20data%20from%20images%20into%20LaTeX%20code.%20In%20recent%0Ayears%2C%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20strong%20performance%0Aacross%20a%20variety%20of%20visual%20understanding%20tasks%2C%20owing%20to%20their%20generalization%0Acapabilities.%20While%20some%20studies%20have%20explored%20the%20use%20of%20VLMs%20for%20the%0AImg2LaTeX%20task%2C%20their%20performance%20often%20falls%20short%20of%20expectations.%0AEmpirically%2C%20VLMs%20sometimes%20struggle%20with%20fine-grained%20visual%20elements%2C%20leading%0Ato%20inaccurate%20LaTeX%20predictions.%20To%20address%20this%20challenge%2C%20we%20propose%0A%24A%5E2R%5E2%24%3A%20Advancing%20Img2LaTeX%20Conversion%20via%20Visual%20Reasoning%20with%0AAttention-Guided%20Refinement%2C%20a%20framework%20that%20effectively%20integrates%20attention%0Alocalization%20and%20iterative%20refinement%20within%20a%20visual%20reasoning%20framework%2C%0Aenabling%20VLMs%20to%20perform%20self-correction%20and%20progressively%20improve%20prediction%0Aquality.%20For%20effective%20evaluation%2C%20we%20introduce%20a%20new%20dataset%2C%0AImg2LaTex-Hard-1K%2C%20consisting%20of%201%2C100%20carefully%20curated%20and%20challenging%0Aexamples%20designed%20to%20rigorously%20evaluate%20the%20capabilities%20of%20VLMs%20within%20this%0Atask%20domain.%20Extensive%20experimental%20results%20demonstrate%20that%3A%20%281%29%20%24A%5E2R%5E2%24%0Asignificantly%20improves%20model%20performance%20across%20six%20evaluation%20metrics%20spanning%0Aboth%20textual%20and%20visual%20levels%2C%20consistently%20outperforming%20other%20baseline%0Amethods%3B%20%282%29%20Increasing%20the%20number%20of%20inference%20rounds%20yields%20notable%0Aperformance%20gains%2C%20underscoring%20the%20potential%20of%20%24A%5E2R%5E2%24%20in%20test-time%20scaling%0Ascenarios%3B%20%283%29%20Ablation%20studies%20and%20human%20evaluations%20validate%20the%20practical%0Aeffectiveness%20of%20our%20approach%2C%20as%20well%20as%20the%20strong%20synergy%20among%20its%20core%0Acomponents%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524A%255E2R%255E2%2524%253A%2520Advancing%2520Img2LaTeX%2520Conversion%2520via%2520Visual%2520Reasoning%2520with%250A%2520%2520Attention-Guided%2520Refinement%26entry.906535625%3DZhecheng%2520Li%2520and%2520Guoxian%2520Song%2520and%2520Yiwei%2520Wang%2520and%2520Zhen%2520Xiong%2520and%2520Junsong%2520Yuan%2520and%2520Yujun%2520Cai%26entry.1292438233%3D%2520%2520Img2LaTeX%2520is%2520a%2520practically%2520significant%2520task%2520that%2520involves%2520converting%250Amathematical%2520expressions%2520or%2520tabular%2520data%2520from%2520images%2520into%2520LaTeX%2520code.%2520In%2520recent%250Ayears%252C%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520strong%2520performance%250Aacross%2520a%2520variety%2520of%2520visual%2520understanding%2520tasks%252C%2520owing%2520to%2520their%2520generalization%250Acapabilities.%2520While%2520some%2520studies%2520have%2520explored%2520the%2520use%2520of%2520VLMs%2520for%2520the%250AImg2LaTeX%2520task%252C%2520their%2520performance%2520often%2520falls%2520short%2520of%2520expectations.%250AEmpirically%252C%2520VLMs%2520sometimes%2520struggle%2520with%2520fine-grained%2520visual%2520elements%252C%2520leading%250Ato%2520inaccurate%2520LaTeX%2520predictions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250A%2524A%255E2R%255E2%2524%253A%2520Advancing%2520Img2LaTeX%2520Conversion%2520via%2520Visual%2520Reasoning%2520with%250AAttention-Guided%2520Refinement%252C%2520a%2520framework%2520that%2520effectively%2520integrates%2520attention%250Alocalization%2520and%2520iterative%2520refinement%2520within%2520a%2520visual%2520reasoning%2520framework%252C%250Aenabling%2520VLMs%2520to%2520perform%2520self-correction%2520and%2520progressively%2520improve%2520prediction%250Aquality.%2520For%2520effective%2520evaluation%252C%2520we%2520introduce%2520a%2520new%2520dataset%252C%250AImg2LaTex-Hard-1K%252C%2520consisting%2520of%25201%252C100%2520carefully%2520curated%2520and%2520challenging%250Aexamples%2520designed%2520to%2520rigorously%2520evaluate%2520the%2520capabilities%2520of%2520VLMs%2520within%2520this%250Atask%2520domain.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%253A%2520%25281%2529%2520%2524A%255E2R%255E2%2524%250Asignificantly%2520improves%2520model%2520performance%2520across%2520six%2520evaluation%2520metrics%2520spanning%250Aboth%2520textual%2520and%2520visual%2520levels%252C%2520consistently%2520outperforming%2520other%2520baseline%250Amethods%253B%2520%25282%2529%2520Increasing%2520the%2520number%2520of%2520inference%2520rounds%2520yields%2520notable%250Aperformance%2520gains%252C%2520underscoring%2520the%2520potential%2520of%2520%2524A%255E2R%255E2%2524%2520in%2520test-time%2520scaling%250Ascenarios%253B%2520%25283%2529%2520Ablation%2520studies%2520and%2520human%2520evaluations%2520validate%2520the%2520practical%250Aeffectiveness%2520of%2520our%2520approach%252C%2520as%2520well%2520as%2520the%2520strong%2520synergy%2520among%2520its%2520core%250Acomponents%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24A%5E2R%5E2%24%3A%20Advancing%20Img2LaTeX%20Conversion%20via%20Visual%20Reasoning%20with%0A%20%20Attention-Guided%20Refinement&entry.906535625=Zhecheng%20Li%20and%20Guoxian%20Song%20and%20Yiwei%20Wang%20and%20Zhen%20Xiong%20and%20Junsong%20Yuan%20and%20Yujun%20Cai&entry.1292438233=%20%20Img2LaTeX%20is%20a%20practically%20significant%20task%20that%20involves%20converting%0Amathematical%20expressions%20or%20tabular%20data%20from%20images%20into%20LaTeX%20code.%20In%20recent%0Ayears%2C%20vision-language%20models%20%28VLMs%29%20have%20demonstrated%20strong%20performance%0Aacross%20a%20variety%20of%20visual%20understanding%20tasks%2C%20owing%20to%20their%20generalization%0Acapabilities.%20While%20some%20studies%20have%20explored%20the%20use%20of%20VLMs%20for%20the%0AImg2LaTeX%20task%2C%20their%20performance%20often%20falls%20short%20of%20expectations.%0AEmpirically%2C%20VLMs%20sometimes%20struggle%20with%20fine-grained%20visual%20elements%2C%20leading%0Ato%20inaccurate%20LaTeX%20predictions.%20To%20address%20this%20challenge%2C%20we%20propose%0A%24A%5E2R%5E2%24%3A%20Advancing%20Img2LaTeX%20Conversion%20via%20Visual%20Reasoning%20with%0AAttention-Guided%20Refinement%2C%20a%20framework%20that%20effectively%20integrates%20attention%0Alocalization%20and%20iterative%20refinement%20within%20a%20visual%20reasoning%20framework%2C%0Aenabling%20VLMs%20to%20perform%20self-correction%20and%20progressively%20improve%20prediction%0Aquality.%20For%20effective%20evaluation%2C%20we%20introduce%20a%20new%20dataset%2C%0AImg2LaTex-Hard-1K%2C%20consisting%20of%201%2C100%20carefully%20curated%20and%20challenging%0Aexamples%20designed%20to%20rigorously%20evaluate%20the%20capabilities%20of%20VLMs%20within%20this%0Atask%20domain.%20Extensive%20experimental%20results%20demonstrate%20that%3A%20%281%29%20%24A%5E2R%5E2%24%0Asignificantly%20improves%20model%20performance%20across%20six%20evaluation%20metrics%20spanning%0Aboth%20textual%20and%20visual%20levels%2C%20consistently%20outperforming%20other%20baseline%0Amethods%3B%20%282%29%20Increasing%20the%20number%20of%20inference%20rounds%20yields%20notable%0Aperformance%20gains%2C%20underscoring%20the%20potential%20of%20%24A%5E2R%5E2%24%20in%20test-time%20scaling%0Ascenarios%3B%20%283%29%20Ablation%20studies%20and%20human%20evaluations%20validate%20the%20practical%0Aeffectiveness%20of%20our%20approach%2C%20as%20well%20as%20the%20strong%20synergy%20among%20its%20core%0Acomponents%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20890v1&entry.124074799=Read"},
{"title": "\"Principal Components\" Enable A New Language of Images", "author": "Xin Wen and Bingchen Zhao and Ismail Elezi and Jiankang Deng and Xiaojuan Qi", "abstract": "  We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space--a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, autoregressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.\n", "link": "http://arxiv.org/abs/2503.08685v2", "date": "2025-07-28", "relevancy": 2.8133, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Principal%20Components%22%20Enable%20A%20New%20Language%20of%20Images&body=Title%3A%20%22Principal%20Components%22%20Enable%20A%20New%20Language%20of%20Images%0AAuthor%3A%20Xin%20Wen%20and%20Bingchen%20Zhao%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20visual%20tokenization%20framework%20that%20embeds%20a%20provable%0APCA-like%20structure%20into%20the%20latent%20token%20space.%20While%20existing%20visual%0Atokenizers%20primarily%20optimize%20for%20reconstruction%20fidelity%2C%20they%20often%20neglect%0Athe%20structural%20properties%20of%20the%20latent%20space--a%20critical%20factor%20for%20both%0Ainterpretability%20and%20downstream%20tasks.%20Our%20method%20generates%20a%201D%20causal%20token%0Asequence%20for%20images%2C%20where%20each%20successive%20token%20contributes%20non-overlapping%0Ainformation%20with%20mathematically%20guaranteed%20decreasing%20explained%20variance%2C%0Aanalogous%20to%20principal%20component%20analysis.%20This%20structural%20constraint%20ensures%0Athe%20tokenizer%20extracts%20the%20most%20salient%20visual%20features%20first%2C%20with%20each%0Asubsequent%20token%20adding%20diminishing%20yet%20complementary%20information.%0AAdditionally%2C%20we%20identified%20and%20resolved%20a%20semantic-spectrum%20coupling%20effect%0Athat%20causes%20the%20unwanted%20entanglement%20of%20high-level%20semantic%20content%20and%0Alow-level%20spectral%20details%20in%20the%20tokens%20by%20leveraging%20a%20diffusion%20decoder.%0AExperiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%0Areconstruction%20performance%20and%20enables%20better%20interpretability%20to%20align%20with%0Athe%20human%20vision%20system.%20Moreover%2C%20autoregressive%20models%20trained%20on%20our%20token%0Asequences%20achieve%20performance%20comparable%20to%20current%20state-of-the-art%20methods%0Awhile%20requiring%20fewer%20tokens%20for%20training%20and%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08685v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Principal%2520Components%2522%2520Enable%2520A%2520New%2520Language%2520of%2520Images%26entry.906535625%3DXin%2520Wen%2520and%2520Bingchen%2520Zhao%2520and%2520Ismail%2520Elezi%2520and%2520Jiankang%2520Deng%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520visual%2520tokenization%2520framework%2520that%2520embeds%2520a%2520provable%250APCA-like%2520structure%2520into%2520the%2520latent%2520token%2520space.%2520While%2520existing%2520visual%250Atokenizers%2520primarily%2520optimize%2520for%2520reconstruction%2520fidelity%252C%2520they%2520often%2520neglect%250Athe%2520structural%2520properties%2520of%2520the%2520latent%2520space--a%2520critical%2520factor%2520for%2520both%250Ainterpretability%2520and%2520downstream%2520tasks.%2520Our%2520method%2520generates%2520a%25201D%2520causal%2520token%250Asequence%2520for%2520images%252C%2520where%2520each%2520successive%2520token%2520contributes%2520non-overlapping%250Ainformation%2520with%2520mathematically%2520guaranteed%2520decreasing%2520explained%2520variance%252C%250Aanalogous%2520to%2520principal%2520component%2520analysis.%2520This%2520structural%2520constraint%2520ensures%250Athe%2520tokenizer%2520extracts%2520the%2520most%2520salient%2520visual%2520features%2520first%252C%2520with%2520each%250Asubsequent%2520token%2520adding%2520diminishing%2520yet%2520complementary%2520information.%250AAdditionally%252C%2520we%2520identified%2520and%2520resolved%2520a%2520semantic-spectrum%2520coupling%2520effect%250Athat%2520causes%2520the%2520unwanted%2520entanglement%2520of%2520high-level%2520semantic%2520content%2520and%250Alow-level%2520spectral%2520details%2520in%2520the%2520tokens%2520by%2520leveraging%2520a%2520diffusion%2520decoder.%250AExperiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%250Areconstruction%2520performance%2520and%2520enables%2520better%2520interpretability%2520to%2520align%2520with%250Athe%2520human%2520vision%2520system.%2520Moreover%252C%2520autoregressive%2520models%2520trained%2520on%2520our%2520token%250Asequences%2520achieve%2520performance%2520comparable%2520to%2520current%2520state-of-the-art%2520methods%250Awhile%2520requiring%2520fewer%2520tokens%2520for%2520training%2520and%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08685v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Principal%20Components%22%20Enable%20A%20New%20Language%20of%20Images&entry.906535625=Xin%20Wen%20and%20Bingchen%20Zhao%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20We%20introduce%20a%20novel%20visual%20tokenization%20framework%20that%20embeds%20a%20provable%0APCA-like%20structure%20into%20the%20latent%20token%20space.%20While%20existing%20visual%0Atokenizers%20primarily%20optimize%20for%20reconstruction%20fidelity%2C%20they%20often%20neglect%0Athe%20structural%20properties%20of%20the%20latent%20space--a%20critical%20factor%20for%20both%0Ainterpretability%20and%20downstream%20tasks.%20Our%20method%20generates%20a%201D%20causal%20token%0Asequence%20for%20images%2C%20where%20each%20successive%20token%20contributes%20non-overlapping%0Ainformation%20with%20mathematically%20guaranteed%20decreasing%20explained%20variance%2C%0Aanalogous%20to%20principal%20component%20analysis.%20This%20structural%20constraint%20ensures%0Athe%20tokenizer%20extracts%20the%20most%20salient%20visual%20features%20first%2C%20with%20each%0Asubsequent%20token%20adding%20diminishing%20yet%20complementary%20information.%0AAdditionally%2C%20we%20identified%20and%20resolved%20a%20semantic-spectrum%20coupling%20effect%0Athat%20causes%20the%20unwanted%20entanglement%20of%20high-level%20semantic%20content%20and%0Alow-level%20spectral%20details%20in%20the%20tokens%20by%20leveraging%20a%20diffusion%20decoder.%0AExperiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%0Areconstruction%20performance%20and%20enables%20better%20interpretability%20to%20align%20with%0Athe%20human%20vision%20system.%20Moreover%2C%20autoregressive%20models%20trained%20on%20our%20token%0Asequences%20achieve%20performance%20comparable%20to%20current%20state-of-the-art%20methods%0Awhile%20requiring%20fewer%20tokens%20for%20training%20and%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08685v2&entry.124074799=Read"},
{"title": "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with\n  Weak Supervision", "author": "Xiao Fang and Minhyek Jeon and Zheyang Qin and Stanislav Panev and Celso de Melo and Shuowen Hu and Shayok Chakraborty and Fernando De la Torre", "abstract": "  Detecting vehicles in aerial imagery is a critical task with applications in\ntraffic monitoring, urban planning, and defense intelligence. Deep learning\nmethods have provided state-of-the-art (SOTA) results for this application.\nHowever, a significant challenge arises when models trained on data from one\ngeographic region fail to generalize effectively to other areas. Variability in\nfactors such as environmental conditions, urban layouts, road networks, vehicle\ntypes, and image acquisition parameters (e.g., resolution, lighting, and angle)\nleads to domain shifts that degrade model performance. This paper proposes a\nnovel method that uses generative AI to synthesize high-quality aerial images\nand their labels, improving detector training through data augmentation. Our\nkey contribution is the development of a multi-stage, multi-modal knowledge\ntransfer framework utilizing fine-tuned latent diffusion models (LDMs) to\nmitigate the distribution gap between the source and target environments.\nExtensive experiments across diverse aerial imagery domains show consistent\nperformance improvements in AP50 over supervised learning on source domain\ndata, weakly supervised adaptation methods, unsupervised domain adaptation\nmethods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than\n50%, respectively. Furthermore, we introduce two newly annotated aerial\ndatasets from New Zealand and Utah to support further research in this field.\nProject page is available at: https://humansensinglab.github.io/AGenDA\n", "link": "http://arxiv.org/abs/2507.20976v1", "date": "2025-07-28", "relevancy": 2.8048, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5569}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapting%20Vehicle%20Detectors%20for%20Aerial%20Imagery%20to%20Unseen%20Domains%20with%0A%20%20Weak%20Supervision&body=Title%3A%20Adapting%20Vehicle%20Detectors%20for%20Aerial%20Imagery%20to%20Unseen%20Domains%20with%0A%20%20Weak%20Supervision%0AAuthor%3A%20Xiao%20Fang%20and%20Minhyek%20Jeon%20and%20Zheyang%20Qin%20and%20Stanislav%20Panev%20and%20Celso%20de%20Melo%20and%20Shuowen%20Hu%20and%20Shayok%20Chakraborty%20and%20Fernando%20De%20la%20Torre%0AAbstract%3A%20%20%20Detecting%20vehicles%20in%20aerial%20imagery%20is%20a%20critical%20task%20with%20applications%20in%0Atraffic%20monitoring%2C%20urban%20planning%2C%20and%20defense%20intelligence.%20Deep%20learning%0Amethods%20have%20provided%20state-of-the-art%20%28SOTA%29%20results%20for%20this%20application.%0AHowever%2C%20a%20significant%20challenge%20arises%20when%20models%20trained%20on%20data%20from%20one%0Ageographic%20region%20fail%20to%20generalize%20effectively%20to%20other%20areas.%20Variability%20in%0Afactors%20such%20as%20environmental%20conditions%2C%20urban%20layouts%2C%20road%20networks%2C%20vehicle%0Atypes%2C%20and%20image%20acquisition%20parameters%20%28e.g.%2C%20resolution%2C%20lighting%2C%20and%20angle%29%0Aleads%20to%20domain%20shifts%20that%20degrade%20model%20performance.%20This%20paper%20proposes%20a%0Anovel%20method%20that%20uses%20generative%20AI%20to%20synthesize%20high-quality%20aerial%20images%0Aand%20their%20labels%2C%20improving%20detector%20training%20through%20data%20augmentation.%20Our%0Akey%20contribution%20is%20the%20development%20of%20a%20multi-stage%2C%20multi-modal%20knowledge%0Atransfer%20framework%20utilizing%20fine-tuned%20latent%20diffusion%20models%20%28LDMs%29%20to%0Amitigate%20the%20distribution%20gap%20between%20the%20source%20and%20target%20environments.%0AExtensive%20experiments%20across%20diverse%20aerial%20imagery%20domains%20show%20consistent%0Aperformance%20improvements%20in%20AP50%20over%20supervised%20learning%20on%20source%20domain%0Adata%2C%20weakly%20supervised%20adaptation%20methods%2C%20unsupervised%20domain%20adaptation%0Amethods%2C%20and%20open-set%20object%20detectors%20by%204-23%25%2C%206-10%25%2C%207-40%25%2C%20and%20more%20than%0A50%25%2C%20respectively.%20Furthermore%2C%20we%20introduce%20two%20newly%20annotated%20aerial%0Adatasets%20from%20New%20Zealand%20and%20Utah%20to%20support%20further%20research%20in%20this%20field.%0AProject%20page%20is%20available%20at%3A%20https%3A//humansensinglab.github.io/AGenDA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapting%2520Vehicle%2520Detectors%2520for%2520Aerial%2520Imagery%2520to%2520Unseen%2520Domains%2520with%250A%2520%2520Weak%2520Supervision%26entry.906535625%3DXiao%2520Fang%2520and%2520Minhyek%2520Jeon%2520and%2520Zheyang%2520Qin%2520and%2520Stanislav%2520Panev%2520and%2520Celso%2520de%2520Melo%2520and%2520Shuowen%2520Hu%2520and%2520Shayok%2520Chakraborty%2520and%2520Fernando%2520De%2520la%2520Torre%26entry.1292438233%3D%2520%2520Detecting%2520vehicles%2520in%2520aerial%2520imagery%2520is%2520a%2520critical%2520task%2520with%2520applications%2520in%250Atraffic%2520monitoring%252C%2520urban%2520planning%252C%2520and%2520defense%2520intelligence.%2520Deep%2520learning%250Amethods%2520have%2520provided%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520for%2520this%2520application.%250AHowever%252C%2520a%2520significant%2520challenge%2520arises%2520when%2520models%2520trained%2520on%2520data%2520from%2520one%250Ageographic%2520region%2520fail%2520to%2520generalize%2520effectively%2520to%2520other%2520areas.%2520Variability%2520in%250Afactors%2520such%2520as%2520environmental%2520conditions%252C%2520urban%2520layouts%252C%2520road%2520networks%252C%2520vehicle%250Atypes%252C%2520and%2520image%2520acquisition%2520parameters%2520%2528e.g.%252C%2520resolution%252C%2520lighting%252C%2520and%2520angle%2529%250Aleads%2520to%2520domain%2520shifts%2520that%2520degrade%2520model%2520performance.%2520This%2520paper%2520proposes%2520a%250Anovel%2520method%2520that%2520uses%2520generative%2520AI%2520to%2520synthesize%2520high-quality%2520aerial%2520images%250Aand%2520their%2520labels%252C%2520improving%2520detector%2520training%2520through%2520data%2520augmentation.%2520Our%250Akey%2520contribution%2520is%2520the%2520development%2520of%2520a%2520multi-stage%252C%2520multi-modal%2520knowledge%250Atransfer%2520framework%2520utilizing%2520fine-tuned%2520latent%2520diffusion%2520models%2520%2528LDMs%2529%2520to%250Amitigate%2520the%2520distribution%2520gap%2520between%2520the%2520source%2520and%2520target%2520environments.%250AExtensive%2520experiments%2520across%2520diverse%2520aerial%2520imagery%2520domains%2520show%2520consistent%250Aperformance%2520improvements%2520in%2520AP50%2520over%2520supervised%2520learning%2520on%2520source%2520domain%250Adata%252C%2520weakly%2520supervised%2520adaptation%2520methods%252C%2520unsupervised%2520domain%2520adaptation%250Amethods%252C%2520and%2520open-set%2520object%2520detectors%2520by%25204-23%2525%252C%25206-10%2525%252C%25207-40%2525%252C%2520and%2520more%2520than%250A50%2525%252C%2520respectively.%2520Furthermore%252C%2520we%2520introduce%2520two%2520newly%2520annotated%2520aerial%250Adatasets%2520from%2520New%2520Zealand%2520and%2520Utah%2520to%2520support%2520further%2520research%2520in%2520this%2520field.%250AProject%2520page%2520is%2520available%2520at%253A%2520https%253A//humansensinglab.github.io/AGenDA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapting%20Vehicle%20Detectors%20for%20Aerial%20Imagery%20to%20Unseen%20Domains%20with%0A%20%20Weak%20Supervision&entry.906535625=Xiao%20Fang%20and%20Minhyek%20Jeon%20and%20Zheyang%20Qin%20and%20Stanislav%20Panev%20and%20Celso%20de%20Melo%20and%20Shuowen%20Hu%20and%20Shayok%20Chakraborty%20and%20Fernando%20De%20la%20Torre&entry.1292438233=%20%20Detecting%20vehicles%20in%20aerial%20imagery%20is%20a%20critical%20task%20with%20applications%20in%0Atraffic%20monitoring%2C%20urban%20planning%2C%20and%20defense%20intelligence.%20Deep%20learning%0Amethods%20have%20provided%20state-of-the-art%20%28SOTA%29%20results%20for%20this%20application.%0AHowever%2C%20a%20significant%20challenge%20arises%20when%20models%20trained%20on%20data%20from%20one%0Ageographic%20region%20fail%20to%20generalize%20effectively%20to%20other%20areas.%20Variability%20in%0Afactors%20such%20as%20environmental%20conditions%2C%20urban%20layouts%2C%20road%20networks%2C%20vehicle%0Atypes%2C%20and%20image%20acquisition%20parameters%20%28e.g.%2C%20resolution%2C%20lighting%2C%20and%20angle%29%0Aleads%20to%20domain%20shifts%20that%20degrade%20model%20performance.%20This%20paper%20proposes%20a%0Anovel%20method%20that%20uses%20generative%20AI%20to%20synthesize%20high-quality%20aerial%20images%0Aand%20their%20labels%2C%20improving%20detector%20training%20through%20data%20augmentation.%20Our%0Akey%20contribution%20is%20the%20development%20of%20a%20multi-stage%2C%20multi-modal%20knowledge%0Atransfer%20framework%20utilizing%20fine-tuned%20latent%20diffusion%20models%20%28LDMs%29%20to%0Amitigate%20the%20distribution%20gap%20between%20the%20source%20and%20target%20environments.%0AExtensive%20experiments%20across%20diverse%20aerial%20imagery%20domains%20show%20consistent%0Aperformance%20improvements%20in%20AP50%20over%20supervised%20learning%20on%20source%20domain%0Adata%2C%20weakly%20supervised%20adaptation%20methods%2C%20unsupervised%20domain%20adaptation%0Amethods%2C%20and%20open-set%20object%20detectors%20by%204-23%25%2C%206-10%25%2C%207-40%25%2C%20and%20more%20than%0A50%25%2C%20respectively.%20Furthermore%2C%20we%20introduce%20two%20newly%20annotated%20aerial%0Adatasets%20from%20New%20Zealand%20and%20Utah%20to%20support%20further%20research%20in%20this%20field.%0AProject%20page%20is%20available%20at%3A%20https%3A//humansensinglab.github.io/AGenDA%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20976v1&entry.124074799=Read"},
{"title": "JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech\n  Generation Version 1", "author": "Xinhan Di and Kristin Qi and Pengqian Yu", "abstract": "  Recent advances in diffusion-based video generation have enabled\nphoto-realistic short clips, but current methods still struggle to achieve\nmulti-modal consistency when jointly generating whole-body motion and natural\nspeech. Current approaches lack comprehensive evaluation frameworks that assess\nboth visual and audio quality, and there are insufficient benchmarks for\nregion-specific performance analysis. To address these gaps, we introduce the\nJoint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1),\ncomprising a large-scale multi-modal dataset with 10,000 unique identities\nacross 2 million video samples, and an evaluation protocol for assessing joint\naudio-video generation of whole-body animatable avatars. Our evaluation of SOTA\nmodels reveals consistent performance disparities between face/hand-centric and\nwhole-body performance, which incidates essential areas for future research.\nThe dataset and evaluation tools are publicly available at\nhttps://github.com/deepreasonings/WholeBodyBenchmark.\n", "link": "http://arxiv.org/abs/2507.20987v1", "date": "2025-07-28", "relevancy": 2.774, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5605}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5562}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JWB-DH-V1%3A%20Benchmark%20for%20Joint%20Whole-Body%20Talking%20Avatar%20and%20Speech%0A%20%20Generation%20Version%201&body=Title%3A%20JWB-DH-V1%3A%20Benchmark%20for%20Joint%20Whole-Body%20Talking%20Avatar%20and%20Speech%0A%20%20Generation%20Version%201%0AAuthor%3A%20Xinhan%20Di%20and%20Kristin%20Qi%20and%20Pengqian%20Yu%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion-based%20video%20generation%20have%20enabled%0Aphoto-realistic%20short%20clips%2C%20but%20current%20methods%20still%20struggle%20to%20achieve%0Amulti-modal%20consistency%20when%20jointly%20generating%20whole-body%20motion%20and%20natural%0Aspeech.%20Current%20approaches%20lack%20comprehensive%20evaluation%20frameworks%20that%20assess%0Aboth%20visual%20and%20audio%20quality%2C%20and%20there%20are%20insufficient%20benchmarks%20for%0Aregion-specific%20performance%20analysis.%20To%20address%20these%20gaps%2C%20we%20introduce%20the%0AJoint%20Whole-Body%20Talking%20Avatar%20and%20Speech%20Generation%20Version%20I%28JWB-DH-V1%29%2C%0Acomprising%20a%20large-scale%20multi-modal%20dataset%20with%2010%2C000%20unique%20identities%0Aacross%202%20million%20video%20samples%2C%20and%20an%20evaluation%20protocol%20for%20assessing%20joint%0Aaudio-video%20generation%20of%20whole-body%20animatable%20avatars.%20Our%20evaluation%20of%20SOTA%0Amodels%20reveals%20consistent%20performance%20disparities%20between%20face/hand-centric%20and%0Awhole-body%20performance%2C%20which%20incidates%20essential%20areas%20for%20future%20research.%0AThe%20dataset%20and%20evaluation%20tools%20are%20publicly%20available%20at%0Ahttps%3A//github.com/deepreasonings/WholeBodyBenchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJWB-DH-V1%253A%2520Benchmark%2520for%2520Joint%2520Whole-Body%2520Talking%2520Avatar%2520and%2520Speech%250A%2520%2520Generation%2520Version%25201%26entry.906535625%3DXinhan%2520Di%2520and%2520Kristin%2520Qi%2520and%2520Pengqian%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion-based%2520video%2520generation%2520have%2520enabled%250Aphoto-realistic%2520short%2520clips%252C%2520but%2520current%2520methods%2520still%2520struggle%2520to%2520achieve%250Amulti-modal%2520consistency%2520when%2520jointly%2520generating%2520whole-body%2520motion%2520and%2520natural%250Aspeech.%2520Current%2520approaches%2520lack%2520comprehensive%2520evaluation%2520frameworks%2520that%2520assess%250Aboth%2520visual%2520and%2520audio%2520quality%252C%2520and%2520there%2520are%2520insufficient%2520benchmarks%2520for%250Aregion-specific%2520performance%2520analysis.%2520To%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520the%250AJoint%2520Whole-Body%2520Talking%2520Avatar%2520and%2520Speech%2520Generation%2520Version%2520I%2528JWB-DH-V1%2529%252C%250Acomprising%2520a%2520large-scale%2520multi-modal%2520dataset%2520with%252010%252C000%2520unique%2520identities%250Aacross%25202%2520million%2520video%2520samples%252C%2520and%2520an%2520evaluation%2520protocol%2520for%2520assessing%2520joint%250Aaudio-video%2520generation%2520of%2520whole-body%2520animatable%2520avatars.%2520Our%2520evaluation%2520of%2520SOTA%250Amodels%2520reveals%2520consistent%2520performance%2520disparities%2520between%2520face/hand-centric%2520and%250Awhole-body%2520performance%252C%2520which%2520incidates%2520essential%2520areas%2520for%2520future%2520research.%250AThe%2520dataset%2520and%2520evaluation%2520tools%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/deepreasonings/WholeBodyBenchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JWB-DH-V1%3A%20Benchmark%20for%20Joint%20Whole-Body%20Talking%20Avatar%20and%20Speech%0A%20%20Generation%20Version%201&entry.906535625=Xinhan%20Di%20and%20Kristin%20Qi%20and%20Pengqian%20Yu&entry.1292438233=%20%20Recent%20advances%20in%20diffusion-based%20video%20generation%20have%20enabled%0Aphoto-realistic%20short%20clips%2C%20but%20current%20methods%20still%20struggle%20to%20achieve%0Amulti-modal%20consistency%20when%20jointly%20generating%20whole-body%20motion%20and%20natural%0Aspeech.%20Current%20approaches%20lack%20comprehensive%20evaluation%20frameworks%20that%20assess%0Aboth%20visual%20and%20audio%20quality%2C%20and%20there%20are%20insufficient%20benchmarks%20for%0Aregion-specific%20performance%20analysis.%20To%20address%20these%20gaps%2C%20we%20introduce%20the%0AJoint%20Whole-Body%20Talking%20Avatar%20and%20Speech%20Generation%20Version%20I%28JWB-DH-V1%29%2C%0Acomprising%20a%20large-scale%20multi-modal%20dataset%20with%2010%2C000%20unique%20identities%0Aacross%202%20million%20video%20samples%2C%20and%20an%20evaluation%20protocol%20for%20assessing%20joint%0Aaudio-video%20generation%20of%20whole-body%20animatable%20avatars.%20Our%20evaluation%20of%20SOTA%0Amodels%20reveals%20consistent%20performance%20disparities%20between%20face/hand-centric%20and%0Awhole-body%20performance%2C%20which%20incidates%20essential%20areas%20for%20future%20research.%0AThe%20dataset%20and%20evaluation%20tools%20are%20publicly%20available%20at%0Ahttps%3A//github.com/deepreasonings/WholeBodyBenchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20987v1&entry.124074799=Read"},
{"title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety\n  to Vision in LVLM", "author": "Shen Li and Liuyi Yao and Wujia Niu and Lan Zhang and Yaliang Li", "abstract": "  Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality.\n", "link": "http://arxiv.org/abs/2507.20994v1", "date": "2025-07-28", "relevancy": 2.7524, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Security%20Tensors%20as%20a%20Cross-Modal%20Bridge%3A%20Extending%20Text-Aligned%20Safety%0A%20%20to%20Vision%20in%20LVLM&body=Title%3A%20Security%20Tensors%20as%20a%20Cross-Modal%20Bridge%3A%20Extending%20Text-Aligned%20Safety%0A%20%20to%20Vision%20in%20LVLM%0AAuthor%3A%20Shen%20Li%20and%20Liuyi%20Yao%20and%20Wujia%20Niu%20and%20Lan%20Zhang%20and%20Yaliang%20Li%0AAbstract%3A%20%20%20Large%20visual-language%20models%20%28LVLMs%29%20integrate%20aligned%20large%20language%20models%0A%28LLMs%29%20with%20visual%20modules%20to%20process%20multimodal%20inputs.%20However%2C%20the%20safety%0Amechanisms%20developed%20for%20text-based%20LLMs%20do%20not%20naturally%20extend%20to%20visual%0Amodalities%2C%20leaving%20LVLMs%20vulnerable%20to%20harmful%20image%20inputs.%20To%20address%20this%0Across-modal%20safety%20gap%2C%20we%20introduce%20security%20tensors%20-%20trainable%20input%20vectors%0Aapplied%20during%20inference%20through%20either%20the%20textual%20or%20visual%20modality.%20These%0Atensors%20transfer%20textual%20safety%20alignment%20to%20visual%20processing%20without%0Amodifying%20the%20model%27s%20parameters.%20They%20are%20optimized%20using%20a%20curated%20dataset%0Acontaining%20%28i%29%20malicious%20image-text%20pairs%20requiring%20rejection%2C%20%28ii%29%20contrastive%0Abenign%20pairs%20with%20text%20structurally%20similar%20to%20malicious%20queries%2C%20with%20the%0Apurpose%20of%20being%20contrastive%20examples%20to%20guide%20visual%20reliance%2C%20and%20%28iii%29%0Ageneral%20benign%20samples%20preserving%20model%20functionality.%20Experimental%20results%0Ademonstrate%20that%20both%20textual%20and%20visual%20security%20tensors%20significantly%20enhance%0ALVLMs%27%20ability%20to%20reject%20diverse%20harmful%20visual%20inputs%20while%20maintaining%0Anear-identical%20performance%20on%20benign%20tasks.%20Further%20internal%20analysis%20towards%0Ahidden-layer%20representations%20reveals%20that%20security%20tensors%20successfully%0Aactivate%20the%20language%20module%27s%20textual%20%22safety%20layers%22%20in%20visual%20inputs%2C%0Athereby%20effectively%20extending%20text-based%20safety%20to%20the%20visual%20modality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecurity%2520Tensors%2520as%2520a%2520Cross-Modal%2520Bridge%253A%2520Extending%2520Text-Aligned%2520Safety%250A%2520%2520to%2520Vision%2520in%2520LVLM%26entry.906535625%3DShen%2520Li%2520and%2520Liuyi%2520Yao%2520and%2520Wujia%2520Niu%2520and%2520Lan%2520Zhang%2520and%2520Yaliang%2520Li%26entry.1292438233%3D%2520%2520Large%2520visual-language%2520models%2520%2528LVLMs%2529%2520integrate%2520aligned%2520large%2520language%2520models%250A%2528LLMs%2529%2520with%2520visual%2520modules%2520to%2520process%2520multimodal%2520inputs.%2520However%252C%2520the%2520safety%250Amechanisms%2520developed%2520for%2520text-based%2520LLMs%2520do%2520not%2520naturally%2520extend%2520to%2520visual%250Amodalities%252C%2520leaving%2520LVLMs%2520vulnerable%2520to%2520harmful%2520image%2520inputs.%2520To%2520address%2520this%250Across-modal%2520safety%2520gap%252C%2520we%2520introduce%2520security%2520tensors%2520-%2520trainable%2520input%2520vectors%250Aapplied%2520during%2520inference%2520through%2520either%2520the%2520textual%2520or%2520visual%2520modality.%2520These%250Atensors%2520transfer%2520textual%2520safety%2520alignment%2520to%2520visual%2520processing%2520without%250Amodifying%2520the%2520model%2527s%2520parameters.%2520They%2520are%2520optimized%2520using%2520a%2520curated%2520dataset%250Acontaining%2520%2528i%2529%2520malicious%2520image-text%2520pairs%2520requiring%2520rejection%252C%2520%2528ii%2529%2520contrastive%250Abenign%2520pairs%2520with%2520text%2520structurally%2520similar%2520to%2520malicious%2520queries%252C%2520with%2520the%250Apurpose%2520of%2520being%2520contrastive%2520examples%2520to%2520guide%2520visual%2520reliance%252C%2520and%2520%2528iii%2529%250Ageneral%2520benign%2520samples%2520preserving%2520model%2520functionality.%2520Experimental%2520results%250Ademonstrate%2520that%2520both%2520textual%2520and%2520visual%2520security%2520tensors%2520significantly%2520enhance%250ALVLMs%2527%2520ability%2520to%2520reject%2520diverse%2520harmful%2520visual%2520inputs%2520while%2520maintaining%250Anear-identical%2520performance%2520on%2520benign%2520tasks.%2520Further%2520internal%2520analysis%2520towards%250Ahidden-layer%2520representations%2520reveals%2520that%2520security%2520tensors%2520successfully%250Aactivate%2520the%2520language%2520module%2527s%2520textual%2520%2522safety%2520layers%2522%2520in%2520visual%2520inputs%252C%250Athereby%2520effectively%2520extending%2520text-based%2520safety%2520to%2520the%2520visual%2520modality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Security%20Tensors%20as%20a%20Cross-Modal%20Bridge%3A%20Extending%20Text-Aligned%20Safety%0A%20%20to%20Vision%20in%20LVLM&entry.906535625=Shen%20Li%20and%20Liuyi%20Yao%20and%20Wujia%20Niu%20and%20Lan%20Zhang%20and%20Yaliang%20Li&entry.1292438233=%20%20Large%20visual-language%20models%20%28LVLMs%29%20integrate%20aligned%20large%20language%20models%0A%28LLMs%29%20with%20visual%20modules%20to%20process%20multimodal%20inputs.%20However%2C%20the%20safety%0Amechanisms%20developed%20for%20text-based%20LLMs%20do%20not%20naturally%20extend%20to%20visual%0Amodalities%2C%20leaving%20LVLMs%20vulnerable%20to%20harmful%20image%20inputs.%20To%20address%20this%0Across-modal%20safety%20gap%2C%20we%20introduce%20security%20tensors%20-%20trainable%20input%20vectors%0Aapplied%20during%20inference%20through%20either%20the%20textual%20or%20visual%20modality.%20These%0Atensors%20transfer%20textual%20safety%20alignment%20to%20visual%20processing%20without%0Amodifying%20the%20model%27s%20parameters.%20They%20are%20optimized%20using%20a%20curated%20dataset%0Acontaining%20%28i%29%20malicious%20image-text%20pairs%20requiring%20rejection%2C%20%28ii%29%20contrastive%0Abenign%20pairs%20with%20text%20structurally%20similar%20to%20malicious%20queries%2C%20with%20the%0Apurpose%20of%20being%20contrastive%20examples%20to%20guide%20visual%20reliance%2C%20and%20%28iii%29%0Ageneral%20benign%20samples%20preserving%20model%20functionality.%20Experimental%20results%0Ademonstrate%20that%20both%20textual%20and%20visual%20security%20tensors%20significantly%20enhance%0ALVLMs%27%20ability%20to%20reject%20diverse%20harmful%20visual%20inputs%20while%20maintaining%0Anear-identical%20performance%20on%20benign%20tasks.%20Further%20internal%20analysis%20towards%0Ahidden-layer%20representations%20reveals%20that%20security%20tensors%20successfully%0Aactivate%20the%20language%20module%27s%20textual%20%22safety%20layers%22%20in%20visual%20inputs%2C%0Athereby%20effectively%20extending%20text-based%20safety%20to%20the%20visual%20modality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20994v1&entry.124074799=Read"},
{"title": "RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image\n  Segmentation", "author": "Kai Ye and YingShi Luan and Zhudi Chen and Guangyue Meng and Pingyang Dai and Liujuan Cao", "abstract": "  Referring Image Segmentation (RIS), which aims to segment specific objects\nbased on natural language descriptions, plays an essential role in\nvision-language understanding. Despite its progress in remote sensing\napplications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored.\nExisting datasets and methods are typically designed for high-altitude and\nstatic-view imagery. They struggle to handle the unique characteristics of LAD\nviews, such as diverse viewpoints and high object density. To fill this gap, we\npresent RIS-LAD, the first fine-grained RIS benchmark tailored for LAD\nscenarios. This dataset comprises 13,871 carefully annotated image-text-mask\ntriplets collected from realistic drone footage, with a focus on small,\ncluttered, and multi-viewpoint scenes. It highlights new challenges absent in\nprevious benchmarks, such as category drift caused by tiny objects and object\ndrift under crowded same-class objects. To tackle these issues, we propose the\nSemantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly\ninjecting all linguistic features, SAARN decomposes and routes semantic\ninformation to different stages of the network. Specifically, the\nCategory-Dominated Linguistic Enhancement (CDLE) aligns visual features with\nobject categories during early encoding, while the Adaptive Reasoning Fusion\nModule (ARFM) dynamically selects semantic cues across scales to improve\nreasoning in complex scenes. The experimental evaluation reveals that RIS-LAD\npresents substantial challenges to state-of-the-art RIS algorithms, and also\ndemonstrates the effectiveness of our proposed model in addressing these\nchallenges. The dataset and code will be publicly released soon at:\nhttps://github.com/AHideoKuzeA/RIS-LAD/.\n", "link": "http://arxiv.org/abs/2507.20920v1", "date": "2025-07-28", "relevancy": 2.7209, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIS-LAD%3A%20A%20Benchmark%20and%20Model%20for%20Referring%20Low-Altitude%20Drone%20Image%0A%20%20Segmentation&body=Title%3A%20RIS-LAD%3A%20A%20Benchmark%20and%20Model%20for%20Referring%20Low-Altitude%20Drone%20Image%0A%20%20Segmentation%0AAuthor%3A%20Kai%20Ye%20and%20YingShi%20Luan%20and%20Zhudi%20Chen%20and%20Guangyue%20Meng%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao%0AAbstract%3A%20%20%20Referring%20Image%20Segmentation%20%28RIS%29%2C%20which%20aims%20to%20segment%20specific%20objects%0Abased%20on%20natural%20language%20descriptions%2C%20plays%20an%20essential%20role%20in%0Avision-language%20understanding.%20Despite%20its%20progress%20in%20remote%20sensing%0Aapplications%2C%20RIS%20in%20Low-Altitude%20Drone%20%28LAD%29%20scenarios%20remains%20underexplored.%0AExisting%20datasets%20and%20methods%20are%20typically%20designed%20for%20high-altitude%20and%0Astatic-view%20imagery.%20They%20struggle%20to%20handle%20the%20unique%20characteristics%20of%20LAD%0Aviews%2C%20such%20as%20diverse%20viewpoints%20and%20high%20object%20density.%20To%20fill%20this%20gap%2C%20we%0Apresent%20RIS-LAD%2C%20the%20first%20fine-grained%20RIS%20benchmark%20tailored%20for%20LAD%0Ascenarios.%20This%20dataset%20comprises%2013%2C871%20carefully%20annotated%20image-text-mask%0Atriplets%20collected%20from%20realistic%20drone%20footage%2C%20with%20a%20focus%20on%20small%2C%0Acluttered%2C%20and%20multi-viewpoint%20scenes.%20It%20highlights%20new%20challenges%20absent%20in%0Aprevious%20benchmarks%2C%20such%20as%20category%20drift%20caused%20by%20tiny%20objects%20and%20object%0Adrift%20under%20crowded%20same-class%20objects.%20To%20tackle%20these%20issues%2C%20we%20propose%20the%0ASemantic-Aware%20Adaptive%20Reasoning%20Network%20%28SAARN%29.%20Rather%20than%20uniformly%0Ainjecting%20all%20linguistic%20features%2C%20SAARN%20decomposes%20and%20routes%20semantic%0Ainformation%20to%20different%20stages%20of%20the%20network.%20Specifically%2C%20the%0ACategory-Dominated%20Linguistic%20Enhancement%20%28CDLE%29%20aligns%20visual%20features%20with%0Aobject%20categories%20during%20early%20encoding%2C%20while%20the%20Adaptive%20Reasoning%20Fusion%0AModule%20%28ARFM%29%20dynamically%20selects%20semantic%20cues%20across%20scales%20to%20improve%0Areasoning%20in%20complex%20scenes.%20The%20experimental%20evaluation%20reveals%20that%20RIS-LAD%0Apresents%20substantial%20challenges%20to%20state-of-the-art%20RIS%20algorithms%2C%20and%20also%0Ademonstrates%20the%20effectiveness%20of%20our%20proposed%20model%20in%20addressing%20these%0Achallenges.%20The%20dataset%20and%20code%20will%20be%20publicly%20released%20soon%20at%3A%0Ahttps%3A//github.com/AHideoKuzeA/RIS-LAD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIS-LAD%253A%2520A%2520Benchmark%2520and%2520Model%2520for%2520Referring%2520Low-Altitude%2520Drone%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DKai%2520Ye%2520and%2520YingShi%2520Luan%2520and%2520Zhudi%2520Chen%2520and%2520Guangyue%2520Meng%2520and%2520Pingyang%2520Dai%2520and%2520Liujuan%2520Cao%26entry.1292438233%3D%2520%2520Referring%2520Image%2520Segmentation%2520%2528RIS%2529%252C%2520which%2520aims%2520to%2520segment%2520specific%2520objects%250Abased%2520on%2520natural%2520language%2520descriptions%252C%2520plays%2520an%2520essential%2520role%2520in%250Avision-language%2520understanding.%2520Despite%2520its%2520progress%2520in%2520remote%2520sensing%250Aapplications%252C%2520RIS%2520in%2520Low-Altitude%2520Drone%2520%2528LAD%2529%2520scenarios%2520remains%2520underexplored.%250AExisting%2520datasets%2520and%2520methods%2520are%2520typically%2520designed%2520for%2520high-altitude%2520and%250Astatic-view%2520imagery.%2520They%2520struggle%2520to%2520handle%2520the%2520unique%2520characteristics%2520of%2520LAD%250Aviews%252C%2520such%2520as%2520diverse%2520viewpoints%2520and%2520high%2520object%2520density.%2520To%2520fill%2520this%2520gap%252C%2520we%250Apresent%2520RIS-LAD%252C%2520the%2520first%2520fine-grained%2520RIS%2520benchmark%2520tailored%2520for%2520LAD%250Ascenarios.%2520This%2520dataset%2520comprises%252013%252C871%2520carefully%2520annotated%2520image-text-mask%250Atriplets%2520collected%2520from%2520realistic%2520drone%2520footage%252C%2520with%2520a%2520focus%2520on%2520small%252C%250Acluttered%252C%2520and%2520multi-viewpoint%2520scenes.%2520It%2520highlights%2520new%2520challenges%2520absent%2520in%250Aprevious%2520benchmarks%252C%2520such%2520as%2520category%2520drift%2520caused%2520by%2520tiny%2520objects%2520and%2520object%250Adrift%2520under%2520crowded%2520same-class%2520objects.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520the%250ASemantic-Aware%2520Adaptive%2520Reasoning%2520Network%2520%2528SAARN%2529.%2520Rather%2520than%2520uniformly%250Ainjecting%2520all%2520linguistic%2520features%252C%2520SAARN%2520decomposes%2520and%2520routes%2520semantic%250Ainformation%2520to%2520different%2520stages%2520of%2520the%2520network.%2520Specifically%252C%2520the%250ACategory-Dominated%2520Linguistic%2520Enhancement%2520%2528CDLE%2529%2520aligns%2520visual%2520features%2520with%250Aobject%2520categories%2520during%2520early%2520encoding%252C%2520while%2520the%2520Adaptive%2520Reasoning%2520Fusion%250AModule%2520%2528ARFM%2529%2520dynamically%2520selects%2520semantic%2520cues%2520across%2520scales%2520to%2520improve%250Areasoning%2520in%2520complex%2520scenes.%2520The%2520experimental%2520evaluation%2520reveals%2520that%2520RIS-LAD%250Apresents%2520substantial%2520challenges%2520to%2520state-of-the-art%2520RIS%2520algorithms%252C%2520and%2520also%250Ademonstrates%2520the%2520effectiveness%2520of%2520our%2520proposed%2520model%2520in%2520addressing%2520these%250Achallenges.%2520The%2520dataset%2520and%2520code%2520will%2520be%2520publicly%2520released%2520soon%2520at%253A%250Ahttps%253A//github.com/AHideoKuzeA/RIS-LAD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIS-LAD%3A%20A%20Benchmark%20and%20Model%20for%20Referring%20Low-Altitude%20Drone%20Image%0A%20%20Segmentation&entry.906535625=Kai%20Ye%20and%20YingShi%20Luan%20and%20Zhudi%20Chen%20and%20Guangyue%20Meng%20and%20Pingyang%20Dai%20and%20Liujuan%20Cao&entry.1292438233=%20%20Referring%20Image%20Segmentation%20%28RIS%29%2C%20which%20aims%20to%20segment%20specific%20objects%0Abased%20on%20natural%20language%20descriptions%2C%20plays%20an%20essential%20role%20in%0Avision-language%20understanding.%20Despite%20its%20progress%20in%20remote%20sensing%0Aapplications%2C%20RIS%20in%20Low-Altitude%20Drone%20%28LAD%29%20scenarios%20remains%20underexplored.%0AExisting%20datasets%20and%20methods%20are%20typically%20designed%20for%20high-altitude%20and%0Astatic-view%20imagery.%20They%20struggle%20to%20handle%20the%20unique%20characteristics%20of%20LAD%0Aviews%2C%20such%20as%20diverse%20viewpoints%20and%20high%20object%20density.%20To%20fill%20this%20gap%2C%20we%0Apresent%20RIS-LAD%2C%20the%20first%20fine-grained%20RIS%20benchmark%20tailored%20for%20LAD%0Ascenarios.%20This%20dataset%20comprises%2013%2C871%20carefully%20annotated%20image-text-mask%0Atriplets%20collected%20from%20realistic%20drone%20footage%2C%20with%20a%20focus%20on%20small%2C%0Acluttered%2C%20and%20multi-viewpoint%20scenes.%20It%20highlights%20new%20challenges%20absent%20in%0Aprevious%20benchmarks%2C%20such%20as%20category%20drift%20caused%20by%20tiny%20objects%20and%20object%0Adrift%20under%20crowded%20same-class%20objects.%20To%20tackle%20these%20issues%2C%20we%20propose%20the%0ASemantic-Aware%20Adaptive%20Reasoning%20Network%20%28SAARN%29.%20Rather%20than%20uniformly%0Ainjecting%20all%20linguistic%20features%2C%20SAARN%20decomposes%20and%20routes%20semantic%0Ainformation%20to%20different%20stages%20of%20the%20network.%20Specifically%2C%20the%0ACategory-Dominated%20Linguistic%20Enhancement%20%28CDLE%29%20aligns%20visual%20features%20with%0Aobject%20categories%20during%20early%20encoding%2C%20while%20the%20Adaptive%20Reasoning%20Fusion%0AModule%20%28ARFM%29%20dynamically%20selects%20semantic%20cues%20across%20scales%20to%20improve%0Areasoning%20in%20complex%20scenes.%20The%20experimental%20evaluation%20reveals%20that%20RIS-LAD%0Apresents%20substantial%20challenges%20to%20state-of-the-art%20RIS%20algorithms%2C%20and%20also%0Ademonstrates%20the%20effectiveness%20of%20our%20proposed%20model%20in%20addressing%20these%0Achallenges.%20The%20dataset%20and%20code%20will%20be%20publicly%20released%20soon%20at%3A%0Ahttps%3A//github.com/AHideoKuzeA/RIS-LAD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20920v1&entry.124074799=Read"},
{"title": "The Importance of Facial Features in Vision-based Sign Language\n  Recognition: Eyes, Mouth or Full Face?", "author": "Dinh Nam Pham and Eleftherios Avramidis", "abstract": "  Non-manual facial features play a crucial role in sign language\ncommunication, yet their importance in automatic sign language recognition\n(ASLR) remains underexplored. While prior studies have shown that incorporating\nfacial features can improve recognition, related work often relies on\nhand-crafted feature extraction and fails to go beyond the comparison of manual\nfeatures versus the combination of manual and facial features. In this work, we\nsystematically investigate the contribution of distinct facial regionseyes,\nmouth, and full faceusing two different deep learning models (a CNN-based model\nand a transformer-based model) trained on an SLR dataset of isolated signs with\nrandomly selected classes. Through quantitative performance and qualitative\nsaliency map evaluation, we reveal that the mouth is the most important\nnon-manual facial feature, significantly improving accuracy. Our findings\nhighlight the necessity of incorporating facial features in ASLR.\n", "link": "http://arxiv.org/abs/2507.20884v1", "date": "2025-07-28", "relevancy": 2.719, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Importance%20of%20Facial%20Features%20in%20Vision-based%20Sign%20Language%0A%20%20Recognition%3A%20Eyes%2C%20Mouth%20or%20Full%20Face%3F&body=Title%3A%20The%20Importance%20of%20Facial%20Features%20in%20Vision-based%20Sign%20Language%0A%20%20Recognition%3A%20Eyes%2C%20Mouth%20or%20Full%20Face%3F%0AAuthor%3A%20Dinh%20Nam%20Pham%20and%20Eleftherios%20Avramidis%0AAbstract%3A%20%20%20Non-manual%20facial%20features%20play%20a%20crucial%20role%20in%20sign%20language%0Acommunication%2C%20yet%20their%20importance%20in%20automatic%20sign%20language%20recognition%0A%28ASLR%29%20remains%20underexplored.%20While%20prior%20studies%20have%20shown%20that%20incorporating%0Afacial%20features%20can%20improve%20recognition%2C%20related%20work%20often%20relies%20on%0Ahand-crafted%20feature%20extraction%20and%20fails%20to%20go%20beyond%20the%20comparison%20of%20manual%0Afeatures%20versus%20the%20combination%20of%20manual%20and%20facial%20features.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20the%20contribution%20of%20distinct%20facial%20regionseyes%2C%0Amouth%2C%20and%20full%20faceusing%20two%20different%20deep%20learning%20models%20%28a%20CNN-based%20model%0Aand%20a%20transformer-based%20model%29%20trained%20on%20an%20SLR%20dataset%20of%20isolated%20signs%20with%0Arandomly%20selected%20classes.%20Through%20quantitative%20performance%20and%20qualitative%0Asaliency%20map%20evaluation%2C%20we%20reveal%20that%20the%20mouth%20is%20the%20most%20important%0Anon-manual%20facial%20feature%2C%20significantly%20improving%20accuracy.%20Our%20findings%0Ahighlight%20the%20necessity%20of%20incorporating%20facial%20features%20in%20ASLR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Importance%2520of%2520Facial%2520Features%2520in%2520Vision-based%2520Sign%2520Language%250A%2520%2520Recognition%253A%2520Eyes%252C%2520Mouth%2520or%2520Full%2520Face%253F%26entry.906535625%3DDinh%2520Nam%2520Pham%2520and%2520Eleftherios%2520Avramidis%26entry.1292438233%3D%2520%2520Non-manual%2520facial%2520features%2520play%2520a%2520crucial%2520role%2520in%2520sign%2520language%250Acommunication%252C%2520yet%2520their%2520importance%2520in%2520automatic%2520sign%2520language%2520recognition%250A%2528ASLR%2529%2520remains%2520underexplored.%2520While%2520prior%2520studies%2520have%2520shown%2520that%2520incorporating%250Afacial%2520features%2520can%2520improve%2520recognition%252C%2520related%2520work%2520often%2520relies%2520on%250Ahand-crafted%2520feature%2520extraction%2520and%2520fails%2520to%2520go%2520beyond%2520the%2520comparison%2520of%2520manual%250Afeatures%2520versus%2520the%2520combination%2520of%2520manual%2520and%2520facial%2520features.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520investigate%2520the%2520contribution%2520of%2520distinct%2520facial%2520regionseyes%252C%250Amouth%252C%2520and%2520full%2520faceusing%2520two%2520different%2520deep%2520learning%2520models%2520%2528a%2520CNN-based%2520model%250Aand%2520a%2520transformer-based%2520model%2529%2520trained%2520on%2520an%2520SLR%2520dataset%2520of%2520isolated%2520signs%2520with%250Arandomly%2520selected%2520classes.%2520Through%2520quantitative%2520performance%2520and%2520qualitative%250Asaliency%2520map%2520evaluation%252C%2520we%2520reveal%2520that%2520the%2520mouth%2520is%2520the%2520most%2520important%250Anon-manual%2520facial%2520feature%252C%2520significantly%2520improving%2520accuracy.%2520Our%2520findings%250Ahighlight%2520the%2520necessity%2520of%2520incorporating%2520facial%2520features%2520in%2520ASLR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Importance%20of%20Facial%20Features%20in%20Vision-based%20Sign%20Language%0A%20%20Recognition%3A%20Eyes%2C%20Mouth%20or%20Full%20Face%3F&entry.906535625=Dinh%20Nam%20Pham%20and%20Eleftherios%20Avramidis&entry.1292438233=%20%20Non-manual%20facial%20features%20play%20a%20crucial%20role%20in%20sign%20language%0Acommunication%2C%20yet%20their%20importance%20in%20automatic%20sign%20language%20recognition%0A%28ASLR%29%20remains%20underexplored.%20While%20prior%20studies%20have%20shown%20that%20incorporating%0Afacial%20features%20can%20improve%20recognition%2C%20related%20work%20often%20relies%20on%0Ahand-crafted%20feature%20extraction%20and%20fails%20to%20go%20beyond%20the%20comparison%20of%20manual%0Afeatures%20versus%20the%20combination%20of%20manual%20and%20facial%20features.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20the%20contribution%20of%20distinct%20facial%20regionseyes%2C%0Amouth%2C%20and%20full%20faceusing%20two%20different%20deep%20learning%20models%20%28a%20CNN-based%20model%0Aand%20a%20transformer-based%20model%29%20trained%20on%20an%20SLR%20dataset%20of%20isolated%20signs%20with%0Arandomly%20selected%20classes.%20Through%20quantitative%20performance%20and%20qualitative%0Asaliency%20map%20evaluation%2C%20we%20reveal%20that%20the%20mouth%20is%20the%20most%20important%0Anon-manual%20facial%20feature%2C%20significantly%20improving%20accuracy.%20Our%20findings%0Ahighlight%20the%20necessity%20of%20incorporating%20facial%20features%20in%20ASLR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20884v1&entry.124074799=Read"},
{"title": "Learning Transferable Facial Emotion Representations from Large-Scale\n  Semantically Rich Captions", "author": "Licai Sun and Xingxun Jiang and Haoyu Chen and Yante Li and Zheng Lian and Biu Liu and Yuan Zong and Wenming Zheng and Jukka M. Lepp\u00e4nen and Guoying Zhao", "abstract": "  Current facial emotion recognition systems are predominately trained to\npredict a fixed set of predefined categories or abstract dimensional values.\nThis constrained form of supervision hinders generalization and applicability,\nas it reduces the rich and nuanced spectrum of emotions into oversimplified\nlabels or scales. In contrast, natural language provides a more flexible,\nexpressive, and interpretable way to represent emotions, offering a much\nbroader source of supervision. Yet, leveraging semantically rich natural\nlanguage captions as supervisory signals for facial emotion representation\nlearning remains relatively underexplored, primarily due to two key challenges:\n1) the lack of large-scale caption datasets with rich emotional semantics, and\n2) the absence of effective frameworks tailored to harness such rich\nsupervision. To this end, we introduce EmoCap100K, a large-scale facial emotion\ncaption dataset comprising over 100,000 samples, featuring rich and structured\nsemantic descriptions that capture both global affective states and\nfine-grained local facial behaviors. Building upon this dataset, we further\npropose EmoCapCLIP, which incorporates a joint global-local contrastive\nlearning framework enhanced by a cross-modal guided positive mining module.\nThis design facilitates the comprehensive exploitation of multi-level caption\ninformation while accommodating semantic similarities between closely related\nexpressions. Extensive evaluations on over 20 benchmarks covering five tasks\ndemonstrate the superior performance of our method, highlighting the promise of\nlearning facial emotion representations from large-scale semantically rich\ncaptions. The code and data will be available at\nhttps://github.com/sunlicai/EmoCapCLIP.\n", "link": "http://arxiv.org/abs/2507.21015v1", "date": "2025-07-28", "relevancy": 2.7071, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5768}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Transferable%20Facial%20Emotion%20Representations%20from%20Large-Scale%0A%20%20Semantically%20Rich%20Captions&body=Title%3A%20Learning%20Transferable%20Facial%20Emotion%20Representations%20from%20Large-Scale%0A%20%20Semantically%20Rich%20Captions%0AAuthor%3A%20Licai%20Sun%20and%20Xingxun%20Jiang%20and%20Haoyu%20Chen%20and%20Yante%20Li%20and%20Zheng%20Lian%20and%20Biu%20Liu%20and%20Yuan%20Zong%20and%20Wenming%20Zheng%20and%20Jukka%20M.%20Lepp%C3%A4nen%20and%20Guoying%20Zhao%0AAbstract%3A%20%20%20Current%20facial%20emotion%20recognition%20systems%20are%20predominately%20trained%20to%0Apredict%20a%20fixed%20set%20of%20predefined%20categories%20or%20abstract%20dimensional%20values.%0AThis%20constrained%20form%20of%20supervision%20hinders%20generalization%20and%20applicability%2C%0Aas%20it%20reduces%20the%20rich%20and%20nuanced%20spectrum%20of%20emotions%20into%20oversimplified%0Alabels%20or%20scales.%20In%20contrast%2C%20natural%20language%20provides%20a%20more%20flexible%2C%0Aexpressive%2C%20and%20interpretable%20way%20to%20represent%20emotions%2C%20offering%20a%20much%0Abroader%20source%20of%20supervision.%20Yet%2C%20leveraging%20semantically%20rich%20natural%0Alanguage%20captions%20as%20supervisory%20signals%20for%20facial%20emotion%20representation%0Alearning%20remains%20relatively%20underexplored%2C%20primarily%20due%20to%20two%20key%20challenges%3A%0A1%29%20the%20lack%20of%20large-scale%20caption%20datasets%20with%20rich%20emotional%20semantics%2C%20and%0A2%29%20the%20absence%20of%20effective%20frameworks%20tailored%20to%20harness%20such%20rich%0Asupervision.%20To%20this%20end%2C%20we%20introduce%20EmoCap100K%2C%20a%20large-scale%20facial%20emotion%0Acaption%20dataset%20comprising%20over%20100%2C000%20samples%2C%20featuring%20rich%20and%20structured%0Asemantic%20descriptions%20that%20capture%20both%20global%20affective%20states%20and%0Afine-grained%20local%20facial%20behaviors.%20Building%20upon%20this%20dataset%2C%20we%20further%0Apropose%20EmoCapCLIP%2C%20which%20incorporates%20a%20joint%20global-local%20contrastive%0Alearning%20framework%20enhanced%20by%20a%20cross-modal%20guided%20positive%20mining%20module.%0AThis%20design%20facilitates%20the%20comprehensive%20exploitation%20of%20multi-level%20caption%0Ainformation%20while%20accommodating%20semantic%20similarities%20between%20closely%20related%0Aexpressions.%20Extensive%20evaluations%20on%20over%2020%20benchmarks%20covering%20five%20tasks%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%2C%20highlighting%20the%20promise%20of%0Alearning%20facial%20emotion%20representations%20from%20large-scale%20semantically%20rich%0Acaptions.%20The%20code%20and%20data%20will%20be%20available%20at%0Ahttps%3A//github.com/sunlicai/EmoCapCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21015v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Transferable%2520Facial%2520Emotion%2520Representations%2520from%2520Large-Scale%250A%2520%2520Semantically%2520Rich%2520Captions%26entry.906535625%3DLicai%2520Sun%2520and%2520Xingxun%2520Jiang%2520and%2520Haoyu%2520Chen%2520and%2520Yante%2520Li%2520and%2520Zheng%2520Lian%2520and%2520Biu%2520Liu%2520and%2520Yuan%2520Zong%2520and%2520Wenming%2520Zheng%2520and%2520Jukka%2520M.%2520Lepp%25C3%25A4nen%2520and%2520Guoying%2520Zhao%26entry.1292438233%3D%2520%2520Current%2520facial%2520emotion%2520recognition%2520systems%2520are%2520predominately%2520trained%2520to%250Apredict%2520a%2520fixed%2520set%2520of%2520predefined%2520categories%2520or%2520abstract%2520dimensional%2520values.%250AThis%2520constrained%2520form%2520of%2520supervision%2520hinders%2520generalization%2520and%2520applicability%252C%250Aas%2520it%2520reduces%2520the%2520rich%2520and%2520nuanced%2520spectrum%2520of%2520emotions%2520into%2520oversimplified%250Alabels%2520or%2520scales.%2520In%2520contrast%252C%2520natural%2520language%2520provides%2520a%2520more%2520flexible%252C%250Aexpressive%252C%2520and%2520interpretable%2520way%2520to%2520represent%2520emotions%252C%2520offering%2520a%2520much%250Abroader%2520source%2520of%2520supervision.%2520Yet%252C%2520leveraging%2520semantically%2520rich%2520natural%250Alanguage%2520captions%2520as%2520supervisory%2520signals%2520for%2520facial%2520emotion%2520representation%250Alearning%2520remains%2520relatively%2520underexplored%252C%2520primarily%2520due%2520to%2520two%2520key%2520challenges%253A%250A1%2529%2520the%2520lack%2520of%2520large-scale%2520caption%2520datasets%2520with%2520rich%2520emotional%2520semantics%252C%2520and%250A2%2529%2520the%2520absence%2520of%2520effective%2520frameworks%2520tailored%2520to%2520harness%2520such%2520rich%250Asupervision.%2520To%2520this%2520end%252C%2520we%2520introduce%2520EmoCap100K%252C%2520a%2520large-scale%2520facial%2520emotion%250Acaption%2520dataset%2520comprising%2520over%2520100%252C000%2520samples%252C%2520featuring%2520rich%2520and%2520structured%250Asemantic%2520descriptions%2520that%2520capture%2520both%2520global%2520affective%2520states%2520and%250Afine-grained%2520local%2520facial%2520behaviors.%2520Building%2520upon%2520this%2520dataset%252C%2520we%2520further%250Apropose%2520EmoCapCLIP%252C%2520which%2520incorporates%2520a%2520joint%2520global-local%2520contrastive%250Alearning%2520framework%2520enhanced%2520by%2520a%2520cross-modal%2520guided%2520positive%2520mining%2520module.%250AThis%2520design%2520facilitates%2520the%2520comprehensive%2520exploitation%2520of%2520multi-level%2520caption%250Ainformation%2520while%2520accommodating%2520semantic%2520similarities%2520between%2520closely%2520related%250Aexpressions.%2520Extensive%2520evaluations%2520on%2520over%252020%2520benchmarks%2520covering%2520five%2520tasks%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520method%252C%2520highlighting%2520the%2520promise%2520of%250Alearning%2520facial%2520emotion%2520representations%2520from%2520large-scale%2520semantically%2520rich%250Acaptions.%2520The%2520code%2520and%2520data%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/sunlicai/EmoCapCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21015v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Transferable%20Facial%20Emotion%20Representations%20from%20Large-Scale%0A%20%20Semantically%20Rich%20Captions&entry.906535625=Licai%20Sun%20and%20Xingxun%20Jiang%20and%20Haoyu%20Chen%20and%20Yante%20Li%20and%20Zheng%20Lian%20and%20Biu%20Liu%20and%20Yuan%20Zong%20and%20Wenming%20Zheng%20and%20Jukka%20M.%20Lepp%C3%A4nen%20and%20Guoying%20Zhao&entry.1292438233=%20%20Current%20facial%20emotion%20recognition%20systems%20are%20predominately%20trained%20to%0Apredict%20a%20fixed%20set%20of%20predefined%20categories%20or%20abstract%20dimensional%20values.%0AThis%20constrained%20form%20of%20supervision%20hinders%20generalization%20and%20applicability%2C%0Aas%20it%20reduces%20the%20rich%20and%20nuanced%20spectrum%20of%20emotions%20into%20oversimplified%0Alabels%20or%20scales.%20In%20contrast%2C%20natural%20language%20provides%20a%20more%20flexible%2C%0Aexpressive%2C%20and%20interpretable%20way%20to%20represent%20emotions%2C%20offering%20a%20much%0Abroader%20source%20of%20supervision.%20Yet%2C%20leveraging%20semantically%20rich%20natural%0Alanguage%20captions%20as%20supervisory%20signals%20for%20facial%20emotion%20representation%0Alearning%20remains%20relatively%20underexplored%2C%20primarily%20due%20to%20two%20key%20challenges%3A%0A1%29%20the%20lack%20of%20large-scale%20caption%20datasets%20with%20rich%20emotional%20semantics%2C%20and%0A2%29%20the%20absence%20of%20effective%20frameworks%20tailored%20to%20harness%20such%20rich%0Asupervision.%20To%20this%20end%2C%20we%20introduce%20EmoCap100K%2C%20a%20large-scale%20facial%20emotion%0Acaption%20dataset%20comprising%20over%20100%2C000%20samples%2C%20featuring%20rich%20and%20structured%0Asemantic%20descriptions%20that%20capture%20both%20global%20affective%20states%20and%0Afine-grained%20local%20facial%20behaviors.%20Building%20upon%20this%20dataset%2C%20we%20further%0Apropose%20EmoCapCLIP%2C%20which%20incorporates%20a%20joint%20global-local%20contrastive%0Alearning%20framework%20enhanced%20by%20a%20cross-modal%20guided%20positive%20mining%20module.%0AThis%20design%20facilitates%20the%20comprehensive%20exploitation%20of%20multi-level%20caption%0Ainformation%20while%20accommodating%20semantic%20similarities%20between%20closely%20related%0Aexpressions.%20Extensive%20evaluations%20on%20over%2020%20benchmarks%20covering%20five%20tasks%0Ademonstrate%20the%20superior%20performance%20of%20our%20method%2C%20highlighting%20the%20promise%20of%0Alearning%20facial%20emotion%20representations%20from%20large-scale%20semantically%20rich%0Acaptions.%20The%20code%20and%20data%20will%20be%20available%20at%0Ahttps%3A//github.com/sunlicai/EmoCapCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21015v1&entry.124074799=Read"},
{"title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "author": "Fei Tang and Zhangxuan Gu and Zhengxi Lu and Xuyang Liu and Shuheng Shen and Changhua Meng and Wen Wang and Wenqi Zhang and Yongliang Shen and Weiming Lu and Jun Xiao and Yueting Zhuang", "abstract": "  Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.\n", "link": "http://arxiv.org/abs/2507.15846v3", "date": "2025-07-28", "relevancy": 2.7009, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5456}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding&body=Title%3A%20GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding%0AAuthor%3A%20Fei%20Tang%20and%20Zhangxuan%20Gu%20and%20Zhengxi%20Lu%20and%20Xuyang%20Liu%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Wen%20Wang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20maps%20natural%20language%20instructions%0Ato%20precise%20interface%20locations%20for%20autonomous%20interaction.%20Current%0Areinforcement%20learning%20approaches%20use%20binary%20rewards%20that%20treat%20elements%20as%0Ahit-or-miss%20targets%2C%20creating%20sparse%20signals%20that%20ignore%20the%20continuous%20nature%0Aof%20spatial%20interactions.%20Motivated%20by%20human%20clicking%20behavior%20that%20naturally%0Aforms%20Gaussian%20distributions%20centered%20on%20target%20elements%2C%20we%20introduce%20GUI%0AGaussian%20Grounding%20Rewards%20%28GUI-G%24%5E2%24%29%2C%20a%20principled%20reward%20framework%20that%0Amodels%20GUI%20elements%20as%20continuous%20Gaussian%20distributions%20across%20the%20interface%0Aplane.%20GUI-G%24%5E2%24%20incorporates%20two%20synergistic%20mechanisms%3A%20Gaussian%20point%0Arewards%20model%20precise%20localization%20through%20exponentially%20decaying%20distributions%0Acentered%20on%20element%20centroids%2C%20while%20coverage%20rewards%20assess%20spatial%20alignment%0Aby%20measuring%20the%20overlap%20between%20predicted%20Gaussian%20distributions%20and%20target%0Aregions.%20To%20handle%20diverse%20element%20scales%2C%20we%20develop%20an%20adaptive%20variance%0Amechanism%20that%20calibrates%20reward%20distributions%20based%20on%20element%20dimensions.%0AThis%20framework%20transforms%20GUI%20grounding%20from%20sparse%20binary%20classification%20to%0Adense%20continuous%20optimization%2C%20where%20Gaussian%20distributions%20generate%20rich%0Agradient%20signals%20that%20guide%20models%20toward%20optimal%20interaction%20positions.%0AExtensive%20experiments%20across%20ScreenSpot%2C%20ScreenSpot-v2%2C%20and%20ScreenSpot-Pro%0Abenchmarks%20demonstrate%20that%20GUI-G%24%5E2%24%2C%20substantially%20outperforms%0Astate-of-the-art%20method%20UI-TARS-72B%2C%20with%20the%20most%20significant%20improvement%20of%0A24.7%25%20on%20ScreenSpot-Pro.%20Our%20analysis%20reveals%20that%20continuous%20modeling%20provides%0Asuperior%20robustness%20to%20interface%20variations%20and%20enhanced%20generalization%20to%0Aunseen%20layouts%2C%20establishing%20a%20new%20paradigm%20for%20spatial%20reasoning%20in%20GUI%0Ainteraction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15846v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-G%2524%255E2%2524%253A%2520Gaussian%2520Reward%2520Modeling%2520for%2520GUI%2520Grounding%26entry.906535625%3DFei%2520Tang%2520and%2520Zhangxuan%2520Gu%2520and%2520Zhengxi%2520Lu%2520and%2520Xuyang%2520Liu%2520and%2520Shuheng%2520Shen%2520and%2520Changhua%2520Meng%2520and%2520Wen%2520Wang%2520and%2520Wenqi%2520Zhang%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520grounding%2520maps%2520natural%2520language%2520instructions%250Ato%2520precise%2520interface%2520locations%2520for%2520autonomous%2520interaction.%2520Current%250Areinforcement%2520learning%2520approaches%2520use%2520binary%2520rewards%2520that%2520treat%2520elements%2520as%250Ahit-or-miss%2520targets%252C%2520creating%2520sparse%2520signals%2520that%2520ignore%2520the%2520continuous%2520nature%250Aof%2520spatial%2520interactions.%2520Motivated%2520by%2520human%2520clicking%2520behavior%2520that%2520naturally%250Aforms%2520Gaussian%2520distributions%2520centered%2520on%2520target%2520elements%252C%2520we%2520introduce%2520GUI%250AGaussian%2520Grounding%2520Rewards%2520%2528GUI-G%2524%255E2%2524%2529%252C%2520a%2520principled%2520reward%2520framework%2520that%250Amodels%2520GUI%2520elements%2520as%2520continuous%2520Gaussian%2520distributions%2520across%2520the%2520interface%250Aplane.%2520GUI-G%2524%255E2%2524%2520incorporates%2520two%2520synergistic%2520mechanisms%253A%2520Gaussian%2520point%250Arewards%2520model%2520precise%2520localization%2520through%2520exponentially%2520decaying%2520distributions%250Acentered%2520on%2520element%2520centroids%252C%2520while%2520coverage%2520rewards%2520assess%2520spatial%2520alignment%250Aby%2520measuring%2520the%2520overlap%2520between%2520predicted%2520Gaussian%2520distributions%2520and%2520target%250Aregions.%2520To%2520handle%2520diverse%2520element%2520scales%252C%2520we%2520develop%2520an%2520adaptive%2520variance%250Amechanism%2520that%2520calibrates%2520reward%2520distributions%2520based%2520on%2520element%2520dimensions.%250AThis%2520framework%2520transforms%2520GUI%2520grounding%2520from%2520sparse%2520binary%2520classification%2520to%250Adense%2520continuous%2520optimization%252C%2520where%2520Gaussian%2520distributions%2520generate%2520rich%250Agradient%2520signals%2520that%2520guide%2520models%2520toward%2520optimal%2520interaction%2520positions.%250AExtensive%2520experiments%2520across%2520ScreenSpot%252C%2520ScreenSpot-v2%252C%2520and%2520ScreenSpot-Pro%250Abenchmarks%2520demonstrate%2520that%2520GUI-G%2524%255E2%2524%252C%2520substantially%2520outperforms%250Astate-of-the-art%2520method%2520UI-TARS-72B%252C%2520with%2520the%2520most%2520significant%2520improvement%2520of%250A24.7%2525%2520on%2520ScreenSpot-Pro.%2520Our%2520analysis%2520reveals%2520that%2520continuous%2520modeling%2520provides%250Asuperior%2520robustness%2520to%2520interface%2520variations%2520and%2520enhanced%2520generalization%2520to%250Aunseen%2520layouts%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520spatial%2520reasoning%2520in%2520GUI%250Ainteraction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15846v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding&entry.906535625=Fei%20Tang%20and%20Zhangxuan%20Gu%20and%20Zhengxi%20Lu%20and%20Xuyang%20Liu%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Wen%20Wang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20maps%20natural%20language%20instructions%0Ato%20precise%20interface%20locations%20for%20autonomous%20interaction.%20Current%0Areinforcement%20learning%20approaches%20use%20binary%20rewards%20that%20treat%20elements%20as%0Ahit-or-miss%20targets%2C%20creating%20sparse%20signals%20that%20ignore%20the%20continuous%20nature%0Aof%20spatial%20interactions.%20Motivated%20by%20human%20clicking%20behavior%20that%20naturally%0Aforms%20Gaussian%20distributions%20centered%20on%20target%20elements%2C%20we%20introduce%20GUI%0AGaussian%20Grounding%20Rewards%20%28GUI-G%24%5E2%24%29%2C%20a%20principled%20reward%20framework%20that%0Amodels%20GUI%20elements%20as%20continuous%20Gaussian%20distributions%20across%20the%20interface%0Aplane.%20GUI-G%24%5E2%24%20incorporates%20two%20synergistic%20mechanisms%3A%20Gaussian%20point%0Arewards%20model%20precise%20localization%20through%20exponentially%20decaying%20distributions%0Acentered%20on%20element%20centroids%2C%20while%20coverage%20rewards%20assess%20spatial%20alignment%0Aby%20measuring%20the%20overlap%20between%20predicted%20Gaussian%20distributions%20and%20target%0Aregions.%20To%20handle%20diverse%20element%20scales%2C%20we%20develop%20an%20adaptive%20variance%0Amechanism%20that%20calibrates%20reward%20distributions%20based%20on%20element%20dimensions.%0AThis%20framework%20transforms%20GUI%20grounding%20from%20sparse%20binary%20classification%20to%0Adense%20continuous%20optimization%2C%20where%20Gaussian%20distributions%20generate%20rich%0Agradient%20signals%20that%20guide%20models%20toward%20optimal%20interaction%20positions.%0AExtensive%20experiments%20across%20ScreenSpot%2C%20ScreenSpot-v2%2C%20and%20ScreenSpot-Pro%0Abenchmarks%20demonstrate%20that%20GUI-G%24%5E2%24%2C%20substantially%20outperforms%0Astate-of-the-art%20method%20UI-TARS-72B%2C%20with%20the%20most%20significant%20improvement%20of%0A24.7%25%20on%20ScreenSpot-Pro.%20Our%20analysis%20reveals%20that%20continuous%20modeling%20provides%0Asuperior%20robustness%20to%20interface%20variations%20and%20enhanced%20generalization%20to%0Aunseen%20layouts%2C%20establishing%20a%20new%20paradigm%20for%20spatial%20reasoning%20in%20GUI%0Ainteraction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15846v3&entry.124074799=Read"},
{"title": "ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive\n  Histopathology Image Reconstruction", "author": "Juming Xiong and Ruining Deng and Jialin Yue and Siqi Lu and Junlin Guo and Marilyn Lionts and Tianyuan Yao and Can Cui and Junchao Zhu and Chongyu Qu and Mengmeng Yin and Haichun Yang and Yuankai Huo", "abstract": "  Histological analysis plays a crucial role in understanding tissue structure\nand pathology. While recent advancements in registration methods have improved\n2D histological analysis, they often struggle to preserve critical 3D spatial\nrelationships, limiting their utility in both clinical and research\napplications. Specifically, constructing accurate 3D models from 2D slices\nremains challenging due to tissue deformation, sectioning artifacts,\nvariability in imaging techniques, and inconsistent illumination. Deep\nlearning-based registration methods have demonstrated improved performance but\nsuffer from limited generalizability and require large-scale training data. In\ncontrast, non-deep-learning approaches offer better generalizability but often\ncompromise on accuracy. In this study, we introduced ZeroReg3D, a novel\nzero-shot registration pipeline tailored for accurate 3D reconstruction from\nserial histological sections. By combining zero-shot deep learning-based\nkeypoint matching with optimization-based affine and non-rigid registration\ntechniques, ZeroReg3D effectively addresses critical challenges such as tissue\ndeformation, sectioning artifacts, staining variability, and inconsistent\nillumination without requiring retraining or fine-tuning. The code has been\nmade publicly available at https://github.com/hrlblab/ZeroReg3D\n", "link": "http://arxiv.org/abs/2506.21923v2", "date": "2025-07-28", "relevancy": 2.6952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5497}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroReg3D%3A%20A%20Zero-shot%20Registration%20Pipeline%20for%203D%20Consecutive%0A%20%20Histopathology%20Image%20Reconstruction&body=Title%3A%20ZeroReg3D%3A%20A%20Zero-shot%20Registration%20Pipeline%20for%203D%20Consecutive%0A%20%20Histopathology%20Image%20Reconstruction%0AAuthor%3A%20Juming%20Xiong%20and%20Ruining%20Deng%20and%20Jialin%20Yue%20and%20Siqi%20Lu%20and%20Junlin%20Guo%20and%20Marilyn%20Lionts%20and%20Tianyuan%20Yao%20and%20Can%20Cui%20and%20Junchao%20Zhu%20and%20Chongyu%20Qu%20and%20Mengmeng%20Yin%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%0AAbstract%3A%20%20%20Histological%20analysis%20plays%20a%20crucial%20role%20in%20understanding%20tissue%20structure%0Aand%20pathology.%20While%20recent%20advancements%20in%20registration%20methods%20have%20improved%0A2D%20histological%20analysis%2C%20they%20often%20struggle%20to%20preserve%20critical%203D%20spatial%0Arelationships%2C%20limiting%20their%20utility%20in%20both%20clinical%20and%20research%0Aapplications.%20Specifically%2C%20constructing%20accurate%203D%20models%20from%202D%20slices%0Aremains%20challenging%20due%20to%20tissue%20deformation%2C%20sectioning%20artifacts%2C%0Avariability%20in%20imaging%20techniques%2C%20and%20inconsistent%20illumination.%20Deep%0Alearning-based%20registration%20methods%20have%20demonstrated%20improved%20performance%20but%0Asuffer%20from%20limited%20generalizability%20and%20require%20large-scale%20training%20data.%20In%0Acontrast%2C%20non-deep-learning%20approaches%20offer%20better%20generalizability%20but%20often%0Acompromise%20on%20accuracy.%20In%20this%20study%2C%20we%20introduced%20ZeroReg3D%2C%20a%20novel%0Azero-shot%20registration%20pipeline%20tailored%20for%20accurate%203D%20reconstruction%20from%0Aserial%20histological%20sections.%20By%20combining%20zero-shot%20deep%20learning-based%0Akeypoint%20matching%20with%20optimization-based%20affine%20and%20non-rigid%20registration%0Atechniques%2C%20ZeroReg3D%20effectively%20addresses%20critical%20challenges%20such%20as%20tissue%0Adeformation%2C%20sectioning%20artifacts%2C%20staining%20variability%2C%20and%20inconsistent%0Aillumination%20without%20requiring%20retraining%20or%20fine-tuning.%20The%20code%20has%20been%0Amade%20publicly%20available%20at%20https%3A//github.com/hrlblab/ZeroReg3D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21923v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroReg3D%253A%2520A%2520Zero-shot%2520Registration%2520Pipeline%2520for%25203D%2520Consecutive%250A%2520%2520Histopathology%2520Image%2520Reconstruction%26entry.906535625%3DJuming%2520Xiong%2520and%2520Ruining%2520Deng%2520and%2520Jialin%2520Yue%2520and%2520Siqi%2520Lu%2520and%2520Junlin%2520Guo%2520and%2520Marilyn%2520Lionts%2520and%2520Tianyuan%2520Yao%2520and%2520Can%2520Cui%2520and%2520Junchao%2520Zhu%2520and%2520Chongyu%2520Qu%2520and%2520Mengmeng%2520Yin%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%26entry.1292438233%3D%2520%2520Histological%2520analysis%2520plays%2520a%2520crucial%2520role%2520in%2520understanding%2520tissue%2520structure%250Aand%2520pathology.%2520While%2520recent%2520advancements%2520in%2520registration%2520methods%2520have%2520improved%250A2D%2520histological%2520analysis%252C%2520they%2520often%2520struggle%2520to%2520preserve%2520critical%25203D%2520spatial%250Arelationships%252C%2520limiting%2520their%2520utility%2520in%2520both%2520clinical%2520and%2520research%250Aapplications.%2520Specifically%252C%2520constructing%2520accurate%25203D%2520models%2520from%25202D%2520slices%250Aremains%2520challenging%2520due%2520to%2520tissue%2520deformation%252C%2520sectioning%2520artifacts%252C%250Avariability%2520in%2520imaging%2520techniques%252C%2520and%2520inconsistent%2520illumination.%2520Deep%250Alearning-based%2520registration%2520methods%2520have%2520demonstrated%2520improved%2520performance%2520but%250Asuffer%2520from%2520limited%2520generalizability%2520and%2520require%2520large-scale%2520training%2520data.%2520In%250Acontrast%252C%2520non-deep-learning%2520approaches%2520offer%2520better%2520generalizability%2520but%2520often%250Acompromise%2520on%2520accuracy.%2520In%2520this%2520study%252C%2520we%2520introduced%2520ZeroReg3D%252C%2520a%2520novel%250Azero-shot%2520registration%2520pipeline%2520tailored%2520for%2520accurate%25203D%2520reconstruction%2520from%250Aserial%2520histological%2520sections.%2520By%2520combining%2520zero-shot%2520deep%2520learning-based%250Akeypoint%2520matching%2520with%2520optimization-based%2520affine%2520and%2520non-rigid%2520registration%250Atechniques%252C%2520ZeroReg3D%2520effectively%2520addresses%2520critical%2520challenges%2520such%2520as%2520tissue%250Adeformation%252C%2520sectioning%2520artifacts%252C%2520staining%2520variability%252C%2520and%2520inconsistent%250Aillumination%2520without%2520requiring%2520retraining%2520or%2520fine-tuning.%2520The%2520code%2520has%2520been%250Amade%2520publicly%2520available%2520at%2520https%253A//github.com/hrlblab/ZeroReg3D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21923v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroReg3D%3A%20A%20Zero-shot%20Registration%20Pipeline%20for%203D%20Consecutive%0A%20%20Histopathology%20Image%20Reconstruction&entry.906535625=Juming%20Xiong%20and%20Ruining%20Deng%20and%20Jialin%20Yue%20and%20Siqi%20Lu%20and%20Junlin%20Guo%20and%20Marilyn%20Lionts%20and%20Tianyuan%20Yao%20and%20Can%20Cui%20and%20Junchao%20Zhu%20and%20Chongyu%20Qu%20and%20Mengmeng%20Yin%20and%20Haichun%20Yang%20and%20Yuankai%20Huo&entry.1292438233=%20%20Histological%20analysis%20plays%20a%20crucial%20role%20in%20understanding%20tissue%20structure%0Aand%20pathology.%20While%20recent%20advancements%20in%20registration%20methods%20have%20improved%0A2D%20histological%20analysis%2C%20they%20often%20struggle%20to%20preserve%20critical%203D%20spatial%0Arelationships%2C%20limiting%20their%20utility%20in%20both%20clinical%20and%20research%0Aapplications.%20Specifically%2C%20constructing%20accurate%203D%20models%20from%202D%20slices%0Aremains%20challenging%20due%20to%20tissue%20deformation%2C%20sectioning%20artifacts%2C%0Avariability%20in%20imaging%20techniques%2C%20and%20inconsistent%20illumination.%20Deep%0Alearning-based%20registration%20methods%20have%20demonstrated%20improved%20performance%20but%0Asuffer%20from%20limited%20generalizability%20and%20require%20large-scale%20training%20data.%20In%0Acontrast%2C%20non-deep-learning%20approaches%20offer%20better%20generalizability%20but%20often%0Acompromise%20on%20accuracy.%20In%20this%20study%2C%20we%20introduced%20ZeroReg3D%2C%20a%20novel%0Azero-shot%20registration%20pipeline%20tailored%20for%20accurate%203D%20reconstruction%20from%0Aserial%20histological%20sections.%20By%20combining%20zero-shot%20deep%20learning-based%0Akeypoint%20matching%20with%20optimization-based%20affine%20and%20non-rigid%20registration%0Atechniques%2C%20ZeroReg3D%20effectively%20addresses%20critical%20challenges%20such%20as%20tissue%0Adeformation%2C%20sectioning%20artifacts%2C%20staining%20variability%2C%20and%20inconsistent%0Aillumination%20without%20requiring%20retraining%20or%20fine-tuning.%20The%20code%20has%20been%0Amade%20publicly%20available%20at%20https%3A//github.com/hrlblab/ZeroReg3D%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21923v2&entry.124074799=Read"},
{"title": "Regularizing Subspace Redundancy of Low-Rank Adaptation", "author": "Yue Zhu and Haiwen Diao and Shang Gao and Jiazuo Yu and Jiawen Zhu and Yunzhi Zhuge and Shuai Hao and Xu Jia and Lu Zhang and Ying Zhang and Huchuan Lu", "abstract": "  Low-Rank Adaptation (LoRA) and its variants have delivered strong capability\nin Parameter-Efficient Transfer Learning (PETL) by minimizing trainable\nparameters and benefiting from reparameterization. However, their projection\nmatrices remain unrestricted during training, causing high representation\nredundancy and diminishing the effectiveness of feature adaptation in the\nresulting subspaces. While existing methods mitigate this by manually adjusting\nthe rank or implicitly applying channel-wise masks, they lack flexibility and\ngeneralize poorly across various datasets and architectures. Hence, we propose\nReSoRA, a method that explicitly models redundancy between mapping subspaces\nand adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.\nSpecifically, it theoretically decomposes the low-rank submatrices into\nmultiple equivalent subspaces and systematically applies de-redundancy\nconstraints to the feature distributions across different projections.\nExtensive experiments validate that our proposed method consistently\nfacilitates existing state-of-the-art PETL methods across various backbones and\ndatasets in vision-language retrieval and standard visual classification\nbenchmarks. Besides, as a training supervision, ReSoRA can be seamlessly\nintegrated into existing approaches in a plug-and-play manner, with no\nadditional inference costs. Code is publicly available at:\nhttps://github.com/Lucenova/ReSoRA.\n", "link": "http://arxiv.org/abs/2507.20745v1", "date": "2025-07-28", "relevancy": 2.6351, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5419}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5278}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularizing%20Subspace%20Redundancy%20of%20Low-Rank%20Adaptation&body=Title%3A%20Regularizing%20Subspace%20Redundancy%20of%20Low-Rank%20Adaptation%0AAuthor%3A%20Yue%20Zhu%20and%20Haiwen%20Diao%20and%20Shang%20Gao%20and%20Jiazuo%20Yu%20and%20Jiawen%20Zhu%20and%20Yunzhi%20Zhuge%20and%20Shuai%20Hao%20and%20Xu%20Jia%20and%20Lu%20Zhang%20and%20Ying%20Zhang%20and%20Huchuan%20Lu%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20its%20variants%20have%20delivered%20strong%20capability%0Ain%20Parameter-Efficient%20Transfer%20Learning%20%28PETL%29%20by%20minimizing%20trainable%0Aparameters%20and%20benefiting%20from%20reparameterization.%20However%2C%20their%20projection%0Amatrices%20remain%20unrestricted%20during%20training%2C%20causing%20high%20representation%0Aredundancy%20and%20diminishing%20the%20effectiveness%20of%20feature%20adaptation%20in%20the%0Aresulting%20subspaces.%20While%20existing%20methods%20mitigate%20this%20by%20manually%20adjusting%0Athe%20rank%20or%20implicitly%20applying%20channel-wise%20masks%2C%20they%20lack%20flexibility%20and%0Ageneralize%20poorly%20across%20various%20datasets%20and%20architectures.%20Hence%2C%20we%20propose%0AReSoRA%2C%20a%20method%20that%20explicitly%20models%20redundancy%20between%20mapping%20subspaces%0Aand%20adaptively%20Regularizes%20Subspace%20redundancy%20of%20Low-Rank%20Adaptation.%0ASpecifically%2C%20it%20theoretically%20decomposes%20the%20low-rank%20submatrices%20into%0Amultiple%20equivalent%20subspaces%20and%20systematically%20applies%20de-redundancy%0Aconstraints%20to%20the%20feature%20distributions%20across%20different%20projections.%0AExtensive%20experiments%20validate%20that%20our%20proposed%20method%20consistently%0Afacilitates%20existing%20state-of-the-art%20PETL%20methods%20across%20various%20backbones%20and%0Adatasets%20in%20vision-language%20retrieval%20and%20standard%20visual%20classification%0Abenchmarks.%20Besides%2C%20as%20a%20training%20supervision%2C%20ReSoRA%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20approaches%20in%20a%20plug-and-play%20manner%2C%20with%20no%0Aadditional%20inference%20costs.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Lucenova/ReSoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularizing%2520Subspace%2520Redundancy%2520of%2520Low-Rank%2520Adaptation%26entry.906535625%3DYue%2520Zhu%2520and%2520Haiwen%2520Diao%2520and%2520Shang%2520Gao%2520and%2520Jiazuo%2520Yu%2520and%2520Jiawen%2520Zhu%2520and%2520Yunzhi%2520Zhuge%2520and%2520Shuai%2520Hao%2520and%2520Xu%2520Jia%2520and%2520Lu%2520Zhang%2520and%2520Ying%2520Zhang%2520and%2520Huchuan%2520Lu%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520and%2520its%2520variants%2520have%2520delivered%2520strong%2520capability%250Ain%2520Parameter-Efficient%2520Transfer%2520Learning%2520%2528PETL%2529%2520by%2520minimizing%2520trainable%250Aparameters%2520and%2520benefiting%2520from%2520reparameterization.%2520However%252C%2520their%2520projection%250Amatrices%2520remain%2520unrestricted%2520during%2520training%252C%2520causing%2520high%2520representation%250Aredundancy%2520and%2520diminishing%2520the%2520effectiveness%2520of%2520feature%2520adaptation%2520in%2520the%250Aresulting%2520subspaces.%2520While%2520existing%2520methods%2520mitigate%2520this%2520by%2520manually%2520adjusting%250Athe%2520rank%2520or%2520implicitly%2520applying%2520channel-wise%2520masks%252C%2520they%2520lack%2520flexibility%2520and%250Ageneralize%2520poorly%2520across%2520various%2520datasets%2520and%2520architectures.%2520Hence%252C%2520we%2520propose%250AReSoRA%252C%2520a%2520method%2520that%2520explicitly%2520models%2520redundancy%2520between%2520mapping%2520subspaces%250Aand%2520adaptively%2520Regularizes%2520Subspace%2520redundancy%2520of%2520Low-Rank%2520Adaptation.%250ASpecifically%252C%2520it%2520theoretically%2520decomposes%2520the%2520low-rank%2520submatrices%2520into%250Amultiple%2520equivalent%2520subspaces%2520and%2520systematically%2520applies%2520de-redundancy%250Aconstraints%2520to%2520the%2520feature%2520distributions%2520across%2520different%2520projections.%250AExtensive%2520experiments%2520validate%2520that%2520our%2520proposed%2520method%2520consistently%250Afacilitates%2520existing%2520state-of-the-art%2520PETL%2520methods%2520across%2520various%2520backbones%2520and%250Adatasets%2520in%2520vision-language%2520retrieval%2520and%2520standard%2520visual%2520classification%250Abenchmarks.%2520Besides%252C%2520as%2520a%2520training%2520supervision%252C%2520ReSoRA%2520can%2520be%2520seamlessly%250Aintegrated%2520into%2520existing%2520approaches%2520in%2520a%2520plug-and-play%2520manner%252C%2520with%2520no%250Aadditional%2520inference%2520costs.%2520Code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/Lucenova/ReSoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularizing%20Subspace%20Redundancy%20of%20Low-Rank%20Adaptation&entry.906535625=Yue%20Zhu%20and%20Haiwen%20Diao%20and%20Shang%20Gao%20and%20Jiazuo%20Yu%20and%20Jiawen%20Zhu%20and%20Yunzhi%20Zhuge%20and%20Shuai%20Hao%20and%20Xu%20Jia%20and%20Lu%20Zhang%20and%20Ying%20Zhang%20and%20Huchuan%20Lu&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20and%20its%20variants%20have%20delivered%20strong%20capability%0Ain%20Parameter-Efficient%20Transfer%20Learning%20%28PETL%29%20by%20minimizing%20trainable%0Aparameters%20and%20benefiting%20from%20reparameterization.%20However%2C%20their%20projection%0Amatrices%20remain%20unrestricted%20during%20training%2C%20causing%20high%20representation%0Aredundancy%20and%20diminishing%20the%20effectiveness%20of%20feature%20adaptation%20in%20the%0Aresulting%20subspaces.%20While%20existing%20methods%20mitigate%20this%20by%20manually%20adjusting%0Athe%20rank%20or%20implicitly%20applying%20channel-wise%20masks%2C%20they%20lack%20flexibility%20and%0Ageneralize%20poorly%20across%20various%20datasets%20and%20architectures.%20Hence%2C%20we%20propose%0AReSoRA%2C%20a%20method%20that%20explicitly%20models%20redundancy%20between%20mapping%20subspaces%0Aand%20adaptively%20Regularizes%20Subspace%20redundancy%20of%20Low-Rank%20Adaptation.%0ASpecifically%2C%20it%20theoretically%20decomposes%20the%20low-rank%20submatrices%20into%0Amultiple%20equivalent%20subspaces%20and%20systematically%20applies%20de-redundancy%0Aconstraints%20to%20the%20feature%20distributions%20across%20different%20projections.%0AExtensive%20experiments%20validate%20that%20our%20proposed%20method%20consistently%0Afacilitates%20existing%20state-of-the-art%20PETL%20methods%20across%20various%20backbones%20and%0Adatasets%20in%20vision-language%20retrieval%20and%20standard%20visual%20classification%0Abenchmarks.%20Besides%2C%20as%20a%20training%20supervision%2C%20ReSoRA%20can%20be%20seamlessly%0Aintegrated%20into%20existing%20approaches%20in%20a%20plug-and-play%20manner%2C%20with%20no%0Aadditional%20inference%20costs.%20Code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/Lucenova/ReSoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20745v1&entry.124074799=Read"},
{"title": "ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided\n  Image Editing Evaluation", "author": "Sherry X. Chen and Yi Wei and Luowei Zhou and Suren Kumar", "abstract": "  Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%). Our code and models are available at\nhttps://github.com/SherryXTChen/ADIEE.git.\n", "link": "http://arxiv.org/abs/2507.07317v2", "date": "2025-07-28", "relevancy": 2.6329, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5321}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADIEE%3A%20Automatic%20Dataset%20Creation%20and%20Scorer%20for%20Instruction-Guided%0A%20%20Image%20Editing%20Evaluation&body=Title%3A%20ADIEE%3A%20Automatic%20Dataset%20Creation%20and%20Scorer%20for%20Instruction-Guided%0A%20%20Image%20Editing%20Evaluation%0AAuthor%3A%20Sherry%20X.%20Chen%20and%20Yi%20Wei%20and%20Luowei%20Zhou%20and%20Suren%20Kumar%0AAbstract%3A%20%20%20Recent%20advances%20in%20instruction-guided%20image%20editing%20underscore%20the%20need%20for%0Aeffective%20automated%20evaluation.%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20been%0Aexplored%20as%20judges%2C%20open-source%20models%20struggle%20with%20alignment%2C%20and%20proprietary%0Amodels%20lack%20transparency%20and%20cost%20efficiency.%20Additionally%2C%20no%20public%20training%0Adatasets%20exist%20to%20fine-tune%20open-source%20VLMs%2C%20only%20small%20benchmarks%20with%0Adiverse%20evaluation%20schemes.%20To%20address%20this%2C%20we%20introduce%20ADIEE%2C%20an%20automated%0Adataset%20creation%20approach%20which%20is%20then%20used%20to%20train%20a%20scoring%20model%20for%0Ainstruction-guided%20image%20editing%20evaluation.%20We%20generate%20a%20large-scale%20dataset%0Awith%20over%20100K%20samples%20and%20use%20it%20to%20fine-tune%20a%20LLaVA-NeXT-8B%20model%20modified%0Ato%20decode%20a%20numeric%20score%20from%20a%20custom%20token.%20The%20resulting%20scorer%20outperforms%0Aall%20open-source%20VLMs%20and%20Gemini-Pro%201.5%20across%20all%20benchmarks%2C%20achieving%20a%0A0.0696%20%28%2B17.24%25%29%20gain%20in%20score%20correlation%20with%20human%20ratings%20on%20AURORA-Bench%2C%0Aand%20improving%20pair-wise%20comparison%20accuracy%20by%204.03%25%20%28%2B7.21%25%29%20on%20GenAI-Bench%0Aand%204.75%25%20%28%2B9.35%25%29%20on%20AURORA-Bench%2C%20respectively%2C%20compared%20to%20the%0Astate-of-the-art.%20The%20scorer%20can%20act%20as%20a%20reward%20model%2C%20enabling%20automated%20best%0Aedit%20selection%20and%20model%20fine-tuning.%20Notably%2C%20the%20proposed%20scorer%20can%20boost%0AMagicBrush%20model%27s%20average%20evaluation%20score%20on%20ImagenHub%20from%205.90%20to%206.43%0A%28%2B8.98%25%29.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/SherryXTChen/ADIEE.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07317v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADIEE%253A%2520Automatic%2520Dataset%2520Creation%2520and%2520Scorer%2520for%2520Instruction-Guided%250A%2520%2520Image%2520Editing%2520Evaluation%26entry.906535625%3DSherry%2520X.%2520Chen%2520and%2520Yi%2520Wei%2520and%2520Luowei%2520Zhou%2520and%2520Suren%2520Kumar%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520instruction-guided%2520image%2520editing%2520underscore%2520the%2520need%2520for%250Aeffective%2520automated%2520evaluation.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520been%250Aexplored%2520as%2520judges%252C%2520open-source%2520models%2520struggle%2520with%2520alignment%252C%2520and%2520proprietary%250Amodels%2520lack%2520transparency%2520and%2520cost%2520efficiency.%2520Additionally%252C%2520no%2520public%2520training%250Adatasets%2520exist%2520to%2520fine-tune%2520open-source%2520VLMs%252C%2520only%2520small%2520benchmarks%2520with%250Adiverse%2520evaluation%2520schemes.%2520To%2520address%2520this%252C%2520we%2520introduce%2520ADIEE%252C%2520an%2520automated%250Adataset%2520creation%2520approach%2520which%2520is%2520then%2520used%2520to%2520train%2520a%2520scoring%2520model%2520for%250Ainstruction-guided%2520image%2520editing%2520evaluation.%2520We%2520generate%2520a%2520large-scale%2520dataset%250Awith%2520over%2520100K%2520samples%2520and%2520use%2520it%2520to%2520fine-tune%2520a%2520LLaVA-NeXT-8B%2520model%2520modified%250Ato%2520decode%2520a%2520numeric%2520score%2520from%2520a%2520custom%2520token.%2520The%2520resulting%2520scorer%2520outperforms%250Aall%2520open-source%2520VLMs%2520and%2520Gemini-Pro%25201.5%2520across%2520all%2520benchmarks%252C%2520achieving%2520a%250A0.0696%2520%2528%252B17.24%2525%2529%2520gain%2520in%2520score%2520correlation%2520with%2520human%2520ratings%2520on%2520AURORA-Bench%252C%250Aand%2520improving%2520pair-wise%2520comparison%2520accuracy%2520by%25204.03%2525%2520%2528%252B7.21%2525%2529%2520on%2520GenAI-Bench%250Aand%25204.75%2525%2520%2528%252B9.35%2525%2529%2520on%2520AURORA-Bench%252C%2520respectively%252C%2520compared%2520to%2520the%250Astate-of-the-art.%2520The%2520scorer%2520can%2520act%2520as%2520a%2520reward%2520model%252C%2520enabling%2520automated%2520best%250Aedit%2520selection%2520and%2520model%2520fine-tuning.%2520Notably%252C%2520the%2520proposed%2520scorer%2520can%2520boost%250AMagicBrush%2520model%2527s%2520average%2520evaluation%2520score%2520on%2520ImagenHub%2520from%25205.90%2520to%25206.43%250A%2528%252B8.98%2525%2529.%2520Our%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/SherryXTChen/ADIEE.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07317v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADIEE%3A%20Automatic%20Dataset%20Creation%20and%20Scorer%20for%20Instruction-Guided%0A%20%20Image%20Editing%20Evaluation&entry.906535625=Sherry%20X.%20Chen%20and%20Yi%20Wei%20and%20Luowei%20Zhou%20and%20Suren%20Kumar&entry.1292438233=%20%20Recent%20advances%20in%20instruction-guided%20image%20editing%20underscore%20the%20need%20for%0Aeffective%20automated%20evaluation.%20While%20Vision-Language%20Models%20%28VLMs%29%20have%20been%0Aexplored%20as%20judges%2C%20open-source%20models%20struggle%20with%20alignment%2C%20and%20proprietary%0Amodels%20lack%20transparency%20and%20cost%20efficiency.%20Additionally%2C%20no%20public%20training%0Adatasets%20exist%20to%20fine-tune%20open-source%20VLMs%2C%20only%20small%20benchmarks%20with%0Adiverse%20evaluation%20schemes.%20To%20address%20this%2C%20we%20introduce%20ADIEE%2C%20an%20automated%0Adataset%20creation%20approach%20which%20is%20then%20used%20to%20train%20a%20scoring%20model%20for%0Ainstruction-guided%20image%20editing%20evaluation.%20We%20generate%20a%20large-scale%20dataset%0Awith%20over%20100K%20samples%20and%20use%20it%20to%20fine-tune%20a%20LLaVA-NeXT-8B%20model%20modified%0Ato%20decode%20a%20numeric%20score%20from%20a%20custom%20token.%20The%20resulting%20scorer%20outperforms%0Aall%20open-source%20VLMs%20and%20Gemini-Pro%201.5%20across%20all%20benchmarks%2C%20achieving%20a%0A0.0696%20%28%2B17.24%25%29%20gain%20in%20score%20correlation%20with%20human%20ratings%20on%20AURORA-Bench%2C%0Aand%20improving%20pair-wise%20comparison%20accuracy%20by%204.03%25%20%28%2B7.21%25%29%20on%20GenAI-Bench%0Aand%204.75%25%20%28%2B9.35%25%29%20on%20AURORA-Bench%2C%20respectively%2C%20compared%20to%20the%0Astate-of-the-art.%20The%20scorer%20can%20act%20as%20a%20reward%20model%2C%20enabling%20automated%20best%0Aedit%20selection%20and%20model%20fine-tuning.%20Notably%2C%20the%20proposed%20scorer%20can%20boost%0AMagicBrush%20model%27s%20average%20evaluation%20score%20on%20ImagenHub%20from%205.90%20to%206.43%0A%28%2B8.98%25%29.%20Our%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/SherryXTChen/ADIEE.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07317v2&entry.124074799=Read"},
{"title": "Investigating Structural Pruning and Recovery Techniques for Compressing\n  Multimodal Large Language Models: An Empirical Study", "author": "Yiran Huang and Lukas Thede and Massimiliano Mancini and Wenjia Xu and Zeynep Akata", "abstract": "  While Multimodal Large Language Models (MLLMs) demonstrate impressive\ncapabilities, their substantial computational and memory requirements pose\nsignificant barriers to practical deployment. Current parameter reduction\ntechniques primarily involve training MLLMs from Small Language Models (SLMs),\nbut these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs\nthrough structural pruning combined with efficient recovery training.\nSpecifically, we investigate two structural pruning paradigms--layerwise and\nwidthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the\nfeasibility of conducting recovery training with only a small fraction of the\navailable data. Our results show that widthwise pruning generally maintains\nbetter performance in low-resource scenarios with limited computational\nresources or insufficient finetuning data. As for the recovery training,\nfinetuning only the multimodal projector is sufficient at small compression\nlevels (< 20%). Furthermore, a combination of supervised finetuning and\nhidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the\noriginal training data, while retaining over 95% of the original performance.\nThrough empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and\nBunny-v1.0-3B, this study offers actionable insights for practitioners aiming\nto compress MLLMs effectively without extensive computation resources or\nsufficient data.\n", "link": "http://arxiv.org/abs/2507.20749v1", "date": "2025-07-28", "relevancy": 2.6188, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5226}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Structural%20Pruning%20and%20Recovery%20Techniques%20for%20Compressing%0A%20%20Multimodal%20Large%20Language%20Models%3A%20An%20Empirical%20Study&body=Title%3A%20Investigating%20Structural%20Pruning%20and%20Recovery%20Techniques%20for%20Compressing%0A%20%20Multimodal%20Large%20Language%20Models%3A%20An%20Empirical%20Study%0AAuthor%3A%20Yiran%20Huang%20and%20Lukas%20Thede%20and%20Massimiliano%20Mancini%20and%20Wenjia%20Xu%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20impressive%0Acapabilities%2C%20their%20substantial%20computational%20and%20memory%20requirements%20pose%0Asignificant%20barriers%20to%20practical%20deployment.%20Current%20parameter%20reduction%0Atechniques%20primarily%20involve%20training%20MLLMs%20from%20Small%20Language%20Models%20%28SLMs%29%2C%0Abut%20these%20methods%20offer%20limited%20flexibility%20and%20remain%20computationally%0Aintensive.%20To%20address%20this%20gap%2C%20we%20propose%20to%20directly%20compress%20existing%20MLLMs%0Athrough%20structural%20pruning%20combined%20with%20efficient%20recovery%20training.%0ASpecifically%2C%20we%20investigate%20two%20structural%20pruning%20paradigms--layerwise%20and%0Awidthwise%20pruning--applied%20to%20the%20language%20model%20backbone%20of%20MLLMs%2C%20alongside%0Asupervised%20finetuning%20and%20knowledge%20distillation.%20Additionally%2C%20we%20assess%20the%0Afeasibility%20of%20conducting%20recovery%20training%20with%20only%20a%20small%20fraction%20of%20the%0Aavailable%20data.%20Our%20results%20show%20that%20widthwise%20pruning%20generally%20maintains%0Abetter%20performance%20in%20low-resource%20scenarios%20with%20limited%20computational%0Aresources%20or%20insufficient%20finetuning%20data.%20As%20for%20the%20recovery%20training%2C%0Afinetuning%20only%20the%20multimodal%20projector%20is%20sufficient%20at%20small%20compression%0Alevels%20%28%3C%2020%25%29.%20Furthermore%2C%20a%20combination%20of%20supervised%20finetuning%20and%0Ahidden-state%20distillation%20yields%20optimal%20recovery%20across%20various%20pruning%0Alevels.%20Notably%2C%20effective%20recovery%20can%20be%20achieved%20with%20as%20little%20as%205%25%20of%20the%0Aoriginal%20training%20data%2C%20while%20retaining%20over%2095%25%20of%20the%20original%20performance.%0AThrough%20empirical%20study%20on%20two%20representative%20MLLMs%2C%20i.e.%2C%20LLaVA-v1.5-7B%20and%0ABunny-v1.0-3B%2C%20this%20study%20offers%20actionable%20insights%20for%20practitioners%20aiming%0Ato%20compress%20MLLMs%20effectively%20without%20extensive%20computation%20resources%20or%0Asufficient%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Structural%2520Pruning%2520and%2520Recovery%2520Techniques%2520for%2520Compressing%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%253A%2520An%2520Empirical%2520Study%26entry.906535625%3DYiran%2520Huang%2520and%2520Lukas%2520Thede%2520and%2520Massimiliano%2520Mancini%2520and%2520Wenjia%2520Xu%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520While%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrate%2520impressive%250Acapabilities%252C%2520their%2520substantial%2520computational%2520and%2520memory%2520requirements%2520pose%250Asignificant%2520barriers%2520to%2520practical%2520deployment.%2520Current%2520parameter%2520reduction%250Atechniques%2520primarily%2520involve%2520training%2520MLLMs%2520from%2520Small%2520Language%2520Models%2520%2528SLMs%2529%252C%250Abut%2520these%2520methods%2520offer%2520limited%2520flexibility%2520and%2520remain%2520computationally%250Aintensive.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520to%2520directly%2520compress%2520existing%2520MLLMs%250Athrough%2520structural%2520pruning%2520combined%2520with%2520efficient%2520recovery%2520training.%250ASpecifically%252C%2520we%2520investigate%2520two%2520structural%2520pruning%2520paradigms--layerwise%2520and%250Awidthwise%2520pruning--applied%2520to%2520the%2520language%2520model%2520backbone%2520of%2520MLLMs%252C%2520alongside%250Asupervised%2520finetuning%2520and%2520knowledge%2520distillation.%2520Additionally%252C%2520we%2520assess%2520the%250Afeasibility%2520of%2520conducting%2520recovery%2520training%2520with%2520only%2520a%2520small%2520fraction%2520of%2520the%250Aavailable%2520data.%2520Our%2520results%2520show%2520that%2520widthwise%2520pruning%2520generally%2520maintains%250Abetter%2520performance%2520in%2520low-resource%2520scenarios%2520with%2520limited%2520computational%250Aresources%2520or%2520insufficient%2520finetuning%2520data.%2520As%2520for%2520the%2520recovery%2520training%252C%250Afinetuning%2520only%2520the%2520multimodal%2520projector%2520is%2520sufficient%2520at%2520small%2520compression%250Alevels%2520%2528%253C%252020%2525%2529.%2520Furthermore%252C%2520a%2520combination%2520of%2520supervised%2520finetuning%2520and%250Ahidden-state%2520distillation%2520yields%2520optimal%2520recovery%2520across%2520various%2520pruning%250Alevels.%2520Notably%252C%2520effective%2520recovery%2520can%2520be%2520achieved%2520with%2520as%2520little%2520as%25205%2525%2520of%2520the%250Aoriginal%2520training%2520data%252C%2520while%2520retaining%2520over%252095%2525%2520of%2520the%2520original%2520performance.%250AThrough%2520empirical%2520study%2520on%2520two%2520representative%2520MLLMs%252C%2520i.e.%252C%2520LLaVA-v1.5-7B%2520and%250ABunny-v1.0-3B%252C%2520this%2520study%2520offers%2520actionable%2520insights%2520for%2520practitioners%2520aiming%250Ato%2520compress%2520MLLMs%2520effectively%2520without%2520extensive%2520computation%2520resources%2520or%250Asufficient%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Structural%20Pruning%20and%20Recovery%20Techniques%20for%20Compressing%0A%20%20Multimodal%20Large%20Language%20Models%3A%20An%20Empirical%20Study&entry.906535625=Yiran%20Huang%20and%20Lukas%20Thede%20and%20Massimiliano%20Mancini%20and%20Wenjia%20Xu%20and%20Zeynep%20Akata&entry.1292438233=%20%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20impressive%0Acapabilities%2C%20their%20substantial%20computational%20and%20memory%20requirements%20pose%0Asignificant%20barriers%20to%20practical%20deployment.%20Current%20parameter%20reduction%0Atechniques%20primarily%20involve%20training%20MLLMs%20from%20Small%20Language%20Models%20%28SLMs%29%2C%0Abut%20these%20methods%20offer%20limited%20flexibility%20and%20remain%20computationally%0Aintensive.%20To%20address%20this%20gap%2C%20we%20propose%20to%20directly%20compress%20existing%20MLLMs%0Athrough%20structural%20pruning%20combined%20with%20efficient%20recovery%20training.%0ASpecifically%2C%20we%20investigate%20two%20structural%20pruning%20paradigms--layerwise%20and%0Awidthwise%20pruning--applied%20to%20the%20language%20model%20backbone%20of%20MLLMs%2C%20alongside%0Asupervised%20finetuning%20and%20knowledge%20distillation.%20Additionally%2C%20we%20assess%20the%0Afeasibility%20of%20conducting%20recovery%20training%20with%20only%20a%20small%20fraction%20of%20the%0Aavailable%20data.%20Our%20results%20show%20that%20widthwise%20pruning%20generally%20maintains%0Abetter%20performance%20in%20low-resource%20scenarios%20with%20limited%20computational%0Aresources%20or%20insufficient%20finetuning%20data.%20As%20for%20the%20recovery%20training%2C%0Afinetuning%20only%20the%20multimodal%20projector%20is%20sufficient%20at%20small%20compression%0Alevels%20%28%3C%2020%25%29.%20Furthermore%2C%20a%20combination%20of%20supervised%20finetuning%20and%0Ahidden-state%20distillation%20yields%20optimal%20recovery%20across%20various%20pruning%0Alevels.%20Notably%2C%20effective%20recovery%20can%20be%20achieved%20with%20as%20little%20as%205%25%20of%20the%0Aoriginal%20training%20data%2C%20while%20retaining%20over%2095%25%20of%20the%20original%20performance.%0AThrough%20empirical%20study%20on%20two%20representative%20MLLMs%2C%20i.e.%2C%20LLaVA-v1.5-7B%20and%0ABunny-v1.0-3B%2C%20this%20study%20offers%20actionable%20insights%20for%20practitioners%20aiming%0Ato%20compress%20MLLMs%20effectively%20without%20extensive%20computation%20resources%20or%0Asufficient%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20749v1&entry.124074799=Read"},
{"title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for\n  3D Scene Understanding", "author": "Chenlu Zhan and Yufei Zhang and Gaoang Wang and Hongwei Wang", "abstract": "  Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.\n", "link": "http://arxiv.org/abs/2506.13629v2", "date": "2025-07-28", "relevancy": 2.5468, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6458}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeQ-Graph%3A%20Free-form%20Querying%20with%20Semantic%20Consistent%20Scene%20Graph%20for%0A%20%203D%20Scene%20Understanding&body=Title%3A%20FreeQ-Graph%3A%20Free-form%20Querying%20with%20Semantic%20Consistent%20Scene%20Graph%20for%0A%20%203D%20Scene%20Understanding%0AAuthor%3A%20Chenlu%20Zhan%20and%20Yufei%20Zhang%20and%20Gaoang%20Wang%20and%20Hongwei%20Wang%0AAbstract%3A%20%20%20Semantic%20querying%20in%20complex%203D%20scenes%20through%20free-form%20language%20presents%20a%0Asignificant%20challenge.%20Existing%203D%20scene%20understanding%20methods%20use%20large-scale%0Atraining%20data%20and%20CLIP%20to%20align%20text%20queries%20with%203D%20semantic%20features.%0AHowever%2C%20their%20reliance%20on%20predefined%20vocabulary%20priors%20from%20training%20data%0Ahinders%20free-form%20semantic%20querying.%20Besides%2C%20recent%20advanced%20methods%20rely%20on%0ALLMs%20for%20scene%20understanding%20but%20lack%20comprehensive%203D%20scene-level%20information%0Aand%20often%20overlook%20the%20potential%20inconsistencies%20in%20LLM-generated%20outputs.%20In%0Aour%20paper%2C%20we%20propose%20FreeQ-Graph%2C%20which%20enables%20Free-form%20Querying%20with%20a%0Asemantic%20consistent%20scene%20Graph%20for%203D%20scene%20understanding.%20The%20core%20idea%20is%20to%0Aencode%20free-form%20queries%20from%20a%20complete%20and%20accurate%203D%20scene%20graph%20without%0Apredefined%20vocabularies%2C%20and%20to%20align%20them%20with%203D%20consistent%20semantic%20labels%2C%0Awhich%20accomplished%20through%20three%20key%20steps.%20We%20initiate%20by%20constructing%20a%0Acomplete%20and%20accurate%203D%20scene%20graph%20that%20maps%20free-form%20objects%20and%20their%0Arelations%20through%20LLM%20and%20LVLM%20guidance%2C%20entirely%20free%20from%20training%20data%20or%0Apredefined%20priors.%20Most%20importantly%2C%20we%20align%20graph%20nodes%20with%20accurate%0Asemantic%20labels%20by%20leveraging%203D%20semantic%20aligned%20features%20from%20merged%0Asuperpoints%2C%20enhancing%203D%20semantic%20consistency.%20To%20enable%20free-form%20semantic%0Aquerying%2C%20we%20then%20design%20an%20LLM-based%20reasoning%20algorithm%20that%20combines%0Ascene-level%20and%20object-level%20information%20to%20intricate%20reasoning.%20We%20conducted%0Aextensive%20experiments%20on%203D%20semantic%20grounding%2C%20segmentation%2C%20and%20complex%0Aquerying%20tasks%2C%20while%20also%20validating%20the%20accuracy%20of%20graph%20generation.%0AExperiments%20on%206%20datasets%20show%20that%20our%20model%20excels%20in%20both%20complex%20free-form%0Asemantic%20queries%20and%20intricate%20relational%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.13629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeQ-Graph%253A%2520Free-form%2520Querying%2520with%2520Semantic%2520Consistent%2520Scene%2520Graph%2520for%250A%2520%25203D%2520Scene%2520Understanding%26entry.906535625%3DChenlu%2520Zhan%2520and%2520Yufei%2520Zhang%2520and%2520Gaoang%2520Wang%2520and%2520Hongwei%2520Wang%26entry.1292438233%3D%2520%2520Semantic%2520querying%2520in%2520complex%25203D%2520scenes%2520through%2520free-form%2520language%2520presents%2520a%250Asignificant%2520challenge.%2520Existing%25203D%2520scene%2520understanding%2520methods%2520use%2520large-scale%250Atraining%2520data%2520and%2520CLIP%2520to%2520align%2520text%2520queries%2520with%25203D%2520semantic%2520features.%250AHowever%252C%2520their%2520reliance%2520on%2520predefined%2520vocabulary%2520priors%2520from%2520training%2520data%250Ahinders%2520free-form%2520semantic%2520querying.%2520Besides%252C%2520recent%2520advanced%2520methods%2520rely%2520on%250ALLMs%2520for%2520scene%2520understanding%2520but%2520lack%2520comprehensive%25203D%2520scene-level%2520information%250Aand%2520often%2520overlook%2520the%2520potential%2520inconsistencies%2520in%2520LLM-generated%2520outputs.%2520In%250Aour%2520paper%252C%2520we%2520propose%2520FreeQ-Graph%252C%2520which%2520enables%2520Free-form%2520Querying%2520with%2520a%250Asemantic%2520consistent%2520scene%2520Graph%2520for%25203D%2520scene%2520understanding.%2520The%2520core%2520idea%2520is%2520to%250Aencode%2520free-form%2520queries%2520from%2520a%2520complete%2520and%2520accurate%25203D%2520scene%2520graph%2520without%250Apredefined%2520vocabularies%252C%2520and%2520to%2520align%2520them%2520with%25203D%2520consistent%2520semantic%2520labels%252C%250Awhich%2520accomplished%2520through%2520three%2520key%2520steps.%2520We%2520initiate%2520by%2520constructing%2520a%250Acomplete%2520and%2520accurate%25203D%2520scene%2520graph%2520that%2520maps%2520free-form%2520objects%2520and%2520their%250Arelations%2520through%2520LLM%2520and%2520LVLM%2520guidance%252C%2520entirely%2520free%2520from%2520training%2520data%2520or%250Apredefined%2520priors.%2520Most%2520importantly%252C%2520we%2520align%2520graph%2520nodes%2520with%2520accurate%250Asemantic%2520labels%2520by%2520leveraging%25203D%2520semantic%2520aligned%2520features%2520from%2520merged%250Asuperpoints%252C%2520enhancing%25203D%2520semantic%2520consistency.%2520To%2520enable%2520free-form%2520semantic%250Aquerying%252C%2520we%2520then%2520design%2520an%2520LLM-based%2520reasoning%2520algorithm%2520that%2520combines%250Ascene-level%2520and%2520object-level%2520information%2520to%2520intricate%2520reasoning.%2520We%2520conducted%250Aextensive%2520experiments%2520on%25203D%2520semantic%2520grounding%252C%2520segmentation%252C%2520and%2520complex%250Aquerying%2520tasks%252C%2520while%2520also%2520validating%2520the%2520accuracy%2520of%2520graph%2520generation.%250AExperiments%2520on%25206%2520datasets%2520show%2520that%2520our%2520model%2520excels%2520in%2520both%2520complex%2520free-form%250Asemantic%2520queries%2520and%2520intricate%2520relational%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeQ-Graph%3A%20Free-form%20Querying%20with%20Semantic%20Consistent%20Scene%20Graph%20for%0A%20%203D%20Scene%20Understanding&entry.906535625=Chenlu%20Zhan%20and%20Yufei%20Zhang%20and%20Gaoang%20Wang%20and%20Hongwei%20Wang&entry.1292438233=%20%20Semantic%20querying%20in%20complex%203D%20scenes%20through%20free-form%20language%20presents%20a%0Asignificant%20challenge.%20Existing%203D%20scene%20understanding%20methods%20use%20large-scale%0Atraining%20data%20and%20CLIP%20to%20align%20text%20queries%20with%203D%20semantic%20features.%0AHowever%2C%20their%20reliance%20on%20predefined%20vocabulary%20priors%20from%20training%20data%0Ahinders%20free-form%20semantic%20querying.%20Besides%2C%20recent%20advanced%20methods%20rely%20on%0ALLMs%20for%20scene%20understanding%20but%20lack%20comprehensive%203D%20scene-level%20information%0Aand%20often%20overlook%20the%20potential%20inconsistencies%20in%20LLM-generated%20outputs.%20In%0Aour%20paper%2C%20we%20propose%20FreeQ-Graph%2C%20which%20enables%20Free-form%20Querying%20with%20a%0Asemantic%20consistent%20scene%20Graph%20for%203D%20scene%20understanding.%20The%20core%20idea%20is%20to%0Aencode%20free-form%20queries%20from%20a%20complete%20and%20accurate%203D%20scene%20graph%20without%0Apredefined%20vocabularies%2C%20and%20to%20align%20them%20with%203D%20consistent%20semantic%20labels%2C%0Awhich%20accomplished%20through%20three%20key%20steps.%20We%20initiate%20by%20constructing%20a%0Acomplete%20and%20accurate%203D%20scene%20graph%20that%20maps%20free-form%20objects%20and%20their%0Arelations%20through%20LLM%20and%20LVLM%20guidance%2C%20entirely%20free%20from%20training%20data%20or%0Apredefined%20priors.%20Most%20importantly%2C%20we%20align%20graph%20nodes%20with%20accurate%0Asemantic%20labels%20by%20leveraging%203D%20semantic%20aligned%20features%20from%20merged%0Asuperpoints%2C%20enhancing%203D%20semantic%20consistency.%20To%20enable%20free-form%20semantic%0Aquerying%2C%20we%20then%20design%20an%20LLM-based%20reasoning%20algorithm%20that%20combines%0Ascene-level%20and%20object-level%20information%20to%20intricate%20reasoning.%20We%20conducted%0Aextensive%20experiments%20on%203D%20semantic%20grounding%2C%20segmentation%2C%20and%20complex%0Aquerying%20tasks%2C%20while%20also%20validating%20the%20accuracy%20of%20graph%20generation.%0AExperiments%20on%206%20datasets%20show%20that%20our%20model%20excels%20in%20both%20complex%20free-form%0Asemantic%20queries%20and%20intricate%20relational%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.13629v2&entry.124074799=Read"},
{"title": "LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale\n  Multi-view Clustering", "author": "Shide Du and Chunming Wu and Zihan Fang and Wendi Zhao and Yilin Wu and Changwei Wang and Shiping Wang", "abstract": "  Deep anchor-based multi-view clustering methods enhance the scalability of\nneural networks by utilizing representative anchors to reduce the computational\ncomplexity of large-scale clustering. Despite their scalability advantages,\nexisting approaches often incorporate anchor structures in a heuristic or\ntask-agnostic manner, either through post-hoc graph construction or as\nauxiliary components for message passing. Such designs overlook the core\nstructural demands of anchor-based clustering, neglecting key optimization\nprinciples. To bridge this gap, we revisit the underlying optimization problem\nof large-scale anchor-based multi-view clustering and unfold its iterative\nsolution into a novel deep network architecture, termed LargeMvC-Net. The\nproposed model decomposes the anchor-based clustering process into three\nmodules: RepresentModule, NoiseModule, and AnchorModule, corresponding to\nrepresentation learning, noise suppression, and anchor indicator estimation.\nEach module is derived by unfolding a step of the original optimization\nprocedure into a dedicated network component, providing structural clarity and\noptimization traceability. In addition, an unsupervised reconstruction loss\naligns each view with the anchor-induced latent space, encouraging consistent\nclustering structures across views. Extensive experiments on several\nlarge-scale multi-view benchmarks show that LargeMvC-Net consistently\noutperforms state-of-the-art methods in terms of both effectiveness and\nscalability.\n", "link": "http://arxiv.org/abs/2507.20980v1", "date": "2025-07-28", "relevancy": 2.5336, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5083}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LargeMvC-Net%3A%20Anchor-based%20Deep%20Unfolding%20Network%20for%20Large-scale%0A%20%20Multi-view%20Clustering&body=Title%3A%20LargeMvC-Net%3A%20Anchor-based%20Deep%20Unfolding%20Network%20for%20Large-scale%0A%20%20Multi-view%20Clustering%0AAuthor%3A%20Shide%20Du%20and%20Chunming%20Wu%20and%20Zihan%20Fang%20and%20Wendi%20Zhao%20and%20Yilin%20Wu%20and%20Changwei%20Wang%20and%20Shiping%20Wang%0AAbstract%3A%20%20%20Deep%20anchor-based%20multi-view%20clustering%20methods%20enhance%20the%20scalability%20of%0Aneural%20networks%20by%20utilizing%20representative%20anchors%20to%20reduce%20the%20computational%0Acomplexity%20of%20large-scale%20clustering.%20Despite%20their%20scalability%20advantages%2C%0Aexisting%20approaches%20often%20incorporate%20anchor%20structures%20in%20a%20heuristic%20or%0Atask-agnostic%20manner%2C%20either%20through%20post-hoc%20graph%20construction%20or%20as%0Aauxiliary%20components%20for%20message%20passing.%20Such%20designs%20overlook%20the%20core%0Astructural%20demands%20of%20anchor-based%20clustering%2C%20neglecting%20key%20optimization%0Aprinciples.%20To%20bridge%20this%20gap%2C%20we%20revisit%20the%20underlying%20optimization%20problem%0Aof%20large-scale%20anchor-based%20multi-view%20clustering%20and%20unfold%20its%20iterative%0Asolution%20into%20a%20novel%20deep%20network%20architecture%2C%20termed%20LargeMvC-Net.%20The%0Aproposed%20model%20decomposes%20the%20anchor-based%20clustering%20process%20into%20three%0Amodules%3A%20RepresentModule%2C%20NoiseModule%2C%20and%20AnchorModule%2C%20corresponding%20to%0Arepresentation%20learning%2C%20noise%20suppression%2C%20and%20anchor%20indicator%20estimation.%0AEach%20module%20is%20derived%20by%20unfolding%20a%20step%20of%20the%20original%20optimization%0Aprocedure%20into%20a%20dedicated%20network%20component%2C%20providing%20structural%20clarity%20and%0Aoptimization%20traceability.%20In%20addition%2C%20an%20unsupervised%20reconstruction%20loss%0Aaligns%20each%20view%20with%20the%20anchor-induced%20latent%20space%2C%20encouraging%20consistent%0Aclustering%20structures%20across%20views.%20Extensive%20experiments%20on%20several%0Alarge-scale%20multi-view%20benchmarks%20show%20that%20LargeMvC-Net%20consistently%0Aoutperforms%20state-of-the-art%20methods%20in%20terms%20of%20both%20effectiveness%20and%0Ascalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLargeMvC-Net%253A%2520Anchor-based%2520Deep%2520Unfolding%2520Network%2520for%2520Large-scale%250A%2520%2520Multi-view%2520Clustering%26entry.906535625%3DShide%2520Du%2520and%2520Chunming%2520Wu%2520and%2520Zihan%2520Fang%2520and%2520Wendi%2520Zhao%2520and%2520Yilin%2520Wu%2520and%2520Changwei%2520Wang%2520and%2520Shiping%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520anchor-based%2520multi-view%2520clustering%2520methods%2520enhance%2520the%2520scalability%2520of%250Aneural%2520networks%2520by%2520utilizing%2520representative%2520anchors%2520to%2520reduce%2520the%2520computational%250Acomplexity%2520of%2520large-scale%2520clustering.%2520Despite%2520their%2520scalability%2520advantages%252C%250Aexisting%2520approaches%2520often%2520incorporate%2520anchor%2520structures%2520in%2520a%2520heuristic%2520or%250Atask-agnostic%2520manner%252C%2520either%2520through%2520post-hoc%2520graph%2520construction%2520or%2520as%250Aauxiliary%2520components%2520for%2520message%2520passing.%2520Such%2520designs%2520overlook%2520the%2520core%250Astructural%2520demands%2520of%2520anchor-based%2520clustering%252C%2520neglecting%2520key%2520optimization%250Aprinciples.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520revisit%2520the%2520underlying%2520optimization%2520problem%250Aof%2520large-scale%2520anchor-based%2520multi-view%2520clustering%2520and%2520unfold%2520its%2520iterative%250Asolution%2520into%2520a%2520novel%2520deep%2520network%2520architecture%252C%2520termed%2520LargeMvC-Net.%2520The%250Aproposed%2520model%2520decomposes%2520the%2520anchor-based%2520clustering%2520process%2520into%2520three%250Amodules%253A%2520RepresentModule%252C%2520NoiseModule%252C%2520and%2520AnchorModule%252C%2520corresponding%2520to%250Arepresentation%2520learning%252C%2520noise%2520suppression%252C%2520and%2520anchor%2520indicator%2520estimation.%250AEach%2520module%2520is%2520derived%2520by%2520unfolding%2520a%2520step%2520of%2520the%2520original%2520optimization%250Aprocedure%2520into%2520a%2520dedicated%2520network%2520component%252C%2520providing%2520structural%2520clarity%2520and%250Aoptimization%2520traceability.%2520In%2520addition%252C%2520an%2520unsupervised%2520reconstruction%2520loss%250Aaligns%2520each%2520view%2520with%2520the%2520anchor-induced%2520latent%2520space%252C%2520encouraging%2520consistent%250Aclustering%2520structures%2520across%2520views.%2520Extensive%2520experiments%2520on%2520several%250Alarge-scale%2520multi-view%2520benchmarks%2520show%2520that%2520LargeMvC-Net%2520consistently%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520effectiveness%2520and%250Ascalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LargeMvC-Net%3A%20Anchor-based%20Deep%20Unfolding%20Network%20for%20Large-scale%0A%20%20Multi-view%20Clustering&entry.906535625=Shide%20Du%20and%20Chunming%20Wu%20and%20Zihan%20Fang%20and%20Wendi%20Zhao%20and%20Yilin%20Wu%20and%20Changwei%20Wang%20and%20Shiping%20Wang&entry.1292438233=%20%20Deep%20anchor-based%20multi-view%20clustering%20methods%20enhance%20the%20scalability%20of%0Aneural%20networks%20by%20utilizing%20representative%20anchors%20to%20reduce%20the%20computational%0Acomplexity%20of%20large-scale%20clustering.%20Despite%20their%20scalability%20advantages%2C%0Aexisting%20approaches%20often%20incorporate%20anchor%20structures%20in%20a%20heuristic%20or%0Atask-agnostic%20manner%2C%20either%20through%20post-hoc%20graph%20construction%20or%20as%0Aauxiliary%20components%20for%20message%20passing.%20Such%20designs%20overlook%20the%20core%0Astructural%20demands%20of%20anchor-based%20clustering%2C%20neglecting%20key%20optimization%0Aprinciples.%20To%20bridge%20this%20gap%2C%20we%20revisit%20the%20underlying%20optimization%20problem%0Aof%20large-scale%20anchor-based%20multi-view%20clustering%20and%20unfold%20its%20iterative%0Asolution%20into%20a%20novel%20deep%20network%20architecture%2C%20termed%20LargeMvC-Net.%20The%0Aproposed%20model%20decomposes%20the%20anchor-based%20clustering%20process%20into%20three%0Amodules%3A%20RepresentModule%2C%20NoiseModule%2C%20and%20AnchorModule%2C%20corresponding%20to%0Arepresentation%20learning%2C%20noise%20suppression%2C%20and%20anchor%20indicator%20estimation.%0AEach%20module%20is%20derived%20by%20unfolding%20a%20step%20of%20the%20original%20optimization%0Aprocedure%20into%20a%20dedicated%20network%20component%2C%20providing%20structural%20clarity%20and%0Aoptimization%20traceability.%20In%20addition%2C%20an%20unsupervised%20reconstruction%20loss%0Aaligns%20each%20view%20with%20the%20anchor-induced%20latent%20space%2C%20encouraging%20consistent%0Aclustering%20structures%20across%20views.%20Extensive%20experiments%20on%20several%0Alarge-scale%20multi-view%20benchmarks%20show%20that%20LargeMvC-Net%20consistently%0Aoutperforms%20state-of-the-art%20methods%20in%20terms%20of%20both%20effectiveness%20and%0Ascalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20980v1&entry.124074799=Read"},
{"title": "Understanding Bias in Perceiving Dimensionality Reduction Projections", "author": "Seoyoung Doh and Hyeon Jeon and Sungbok Shin and Ghulam Jilani Quadri and Nam Wook Kim and Jinwook Seo", "abstract": "  Selecting the dimensionality reduction technique that faithfully represents\nthe structure is essential for reliable visual communication and analytics. In\nreality, however, practitioners favor projections for other attractions, such\nas aesthetics and visual saliency, over the projection's structural\nfaithfulness, a bias we define as visual interestingness. In this research, we\nconduct a user study that (1) verifies the existence of such bias and (2)\nexplains why the bias exists. Our study suggests that visual interestingness\nbiases practitioners' preferences when selecting projections for analysis, and\nthis bias intensifies with color-encoded labels and shorter exposure time.\nBased on our findings, we discuss strategies to mitigate bias in perceiving and\ninterpreting DR projections.\n", "link": "http://arxiv.org/abs/2507.20805v1", "date": "2025-07-28", "relevancy": 2.5284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Bias%20in%20Perceiving%20Dimensionality%20Reduction%20Projections&body=Title%3A%20Understanding%20Bias%20in%20Perceiving%20Dimensionality%20Reduction%20Projections%0AAuthor%3A%20Seoyoung%20Doh%20and%20Hyeon%20Jeon%20and%20Sungbok%20Shin%20and%20Ghulam%20Jilani%20Quadri%20and%20Nam%20Wook%20Kim%20and%20Jinwook%20Seo%0AAbstract%3A%20%20%20Selecting%20the%20dimensionality%20reduction%20technique%20that%20faithfully%20represents%0Athe%20structure%20is%20essential%20for%20reliable%20visual%20communication%20and%20analytics.%20In%0Areality%2C%20however%2C%20practitioners%20favor%20projections%20for%20other%20attractions%2C%20such%0Aas%20aesthetics%20and%20visual%20saliency%2C%20over%20the%20projection%27s%20structural%0Afaithfulness%2C%20a%20bias%20we%20define%20as%20visual%20interestingness.%20In%20this%20research%2C%20we%0Aconduct%20a%20user%20study%20that%20%281%29%20verifies%20the%20existence%20of%20such%20bias%20and%20%282%29%0Aexplains%20why%20the%20bias%20exists.%20Our%20study%20suggests%20that%20visual%20interestingness%0Abiases%20practitioners%27%20preferences%20when%20selecting%20projections%20for%20analysis%2C%20and%0Athis%20bias%20intensifies%20with%20color-encoded%20labels%20and%20shorter%20exposure%20time.%0ABased%20on%20our%20findings%2C%20we%20discuss%20strategies%20to%20mitigate%20bias%20in%20perceiving%20and%0Ainterpreting%20DR%20projections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20805v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Bias%2520in%2520Perceiving%2520Dimensionality%2520Reduction%2520Projections%26entry.906535625%3DSeoyoung%2520Doh%2520and%2520Hyeon%2520Jeon%2520and%2520Sungbok%2520Shin%2520and%2520Ghulam%2520Jilani%2520Quadri%2520and%2520Nam%2520Wook%2520Kim%2520and%2520Jinwook%2520Seo%26entry.1292438233%3D%2520%2520Selecting%2520the%2520dimensionality%2520reduction%2520technique%2520that%2520faithfully%2520represents%250Athe%2520structure%2520is%2520essential%2520for%2520reliable%2520visual%2520communication%2520and%2520analytics.%2520In%250Areality%252C%2520however%252C%2520practitioners%2520favor%2520projections%2520for%2520other%2520attractions%252C%2520such%250Aas%2520aesthetics%2520and%2520visual%2520saliency%252C%2520over%2520the%2520projection%2527s%2520structural%250Afaithfulness%252C%2520a%2520bias%2520we%2520define%2520as%2520visual%2520interestingness.%2520In%2520this%2520research%252C%2520we%250Aconduct%2520a%2520user%2520study%2520that%2520%25281%2529%2520verifies%2520the%2520existence%2520of%2520such%2520bias%2520and%2520%25282%2529%250Aexplains%2520why%2520the%2520bias%2520exists.%2520Our%2520study%2520suggests%2520that%2520visual%2520interestingness%250Abiases%2520practitioners%2527%2520preferences%2520when%2520selecting%2520projections%2520for%2520analysis%252C%2520and%250Athis%2520bias%2520intensifies%2520with%2520color-encoded%2520labels%2520and%2520shorter%2520exposure%2520time.%250ABased%2520on%2520our%2520findings%252C%2520we%2520discuss%2520strategies%2520to%2520mitigate%2520bias%2520in%2520perceiving%2520and%250Ainterpreting%2520DR%2520projections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20805v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Bias%20in%20Perceiving%20Dimensionality%20Reduction%20Projections&entry.906535625=Seoyoung%20Doh%20and%20Hyeon%20Jeon%20and%20Sungbok%20Shin%20and%20Ghulam%20Jilani%20Quadri%20and%20Nam%20Wook%20Kim%20and%20Jinwook%20Seo&entry.1292438233=%20%20Selecting%20the%20dimensionality%20reduction%20technique%20that%20faithfully%20represents%0Athe%20structure%20is%20essential%20for%20reliable%20visual%20communication%20and%20analytics.%20In%0Areality%2C%20however%2C%20practitioners%20favor%20projections%20for%20other%20attractions%2C%20such%0Aas%20aesthetics%20and%20visual%20saliency%2C%20over%20the%20projection%27s%20structural%0Afaithfulness%2C%20a%20bias%20we%20define%20as%20visual%20interestingness.%20In%20this%20research%2C%20we%0Aconduct%20a%20user%20study%20that%20%281%29%20verifies%20the%20existence%20of%20such%20bias%20and%20%282%29%0Aexplains%20why%20the%20bias%20exists.%20Our%20study%20suggests%20that%20visual%20interestingness%0Abiases%20practitioners%27%20preferences%20when%20selecting%20projections%20for%20analysis%2C%20and%0Athis%20bias%20intensifies%20with%20color-encoded%20labels%20and%20shorter%20exposure%20time.%0ABased%20on%20our%20findings%2C%20we%20discuss%20strategies%20to%20mitigate%20bias%20in%20perceiving%20and%0Ainterpreting%20DR%20projections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20805v1&entry.124074799=Read"},
{"title": "\\textit{FedABC}: Attention-Based Client Selection for Federated Learning\n  with Long-Term View", "author": "Wenxuan Ye and Xueli An and Junfan Wang and Xueqiang Yan and Georg Carle", "abstract": "  Native AI support is a key objective in the evolution of 6G networks, with\nFederated Learning (FL) emerging as a promising paradigm. FL allows\ndecentralized clients to collaboratively train an AI model without directly\nsharing their data, preserving privacy. Clients train local models on private\ndata and share model updates, which a central server aggregates to refine the\nglobal model and redistribute it for the next iteration. However, client data\nheterogeneity slows convergence and reduces model accuracy, and frequent client\nparticipation imposes communication and computational burdens. To address these\nchallenges, we propose \\textit{FedABC}, an innovative client selection\nalgorithm designed to take a long-term view in managing data heterogeneity and\noptimizing client participation. Inspired by attention mechanisms,\n\\textit{FedABC} prioritizes informative clients by evaluating both model\nsimilarity and each model's unique contributions to the global model. Moreover,\nconsidering the evolving demands of the global model, we formulate an\noptimization problem to guide \\textit{FedABC} throughout the training process.\nFollowing the ``later-is-better\" principle, \\textit{FedABC} adaptively adjusts\nthe client selection threshold, encouraging greater participation in later\ntraining stages. Extensive simulations on CIFAR-10 demonstrate that\n\\textit{FedABC} significantly outperforms existing approaches in model accuracy\nand client participation efficiency, achieving comparable performance with 32\\%\nfewer clients than the classical FL algorithm \\textit{FedAvg}, and 3.5\\% higher\naccuracy with 2\\% fewer clients than the state-of-the-art. This work marks a\nstep toward deploying FL in heterogeneous, resource-constrained environments,\nthereby supporting native AI capabilities in 6G networks.\n", "link": "http://arxiv.org/abs/2507.20871v1", "date": "2025-07-28", "relevancy": 2.5182, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.517}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5044}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%5Ctextit%7BFedABC%7D%3A%20Attention-Based%20Client%20Selection%20for%20Federated%20Learning%0A%20%20with%20Long-Term%20View&body=Title%3A%20%5Ctextit%7BFedABC%7D%3A%20Attention-Based%20Client%20Selection%20for%20Federated%20Learning%0A%20%20with%20Long-Term%20View%0AAuthor%3A%20Wenxuan%20Ye%20and%20Xueli%20An%20and%20Junfan%20Wang%20and%20Xueqiang%20Yan%20and%20Georg%20Carle%0AAbstract%3A%20%20%20Native%20AI%20support%20is%20a%20key%20objective%20in%20the%20evolution%20of%206G%20networks%2C%20with%0AFederated%20Learning%20%28FL%29%20emerging%20as%20a%20promising%20paradigm.%20FL%20allows%0Adecentralized%20clients%20to%20collaboratively%20train%20an%20AI%20model%20without%20directly%0Asharing%20their%20data%2C%20preserving%20privacy.%20Clients%20train%20local%20models%20on%20private%0Adata%20and%20share%20model%20updates%2C%20which%20a%20central%20server%20aggregates%20to%20refine%20the%0Aglobal%20model%20and%20redistribute%20it%20for%20the%20next%20iteration.%20However%2C%20client%20data%0Aheterogeneity%20slows%20convergence%20and%20reduces%20model%20accuracy%2C%20and%20frequent%20client%0Aparticipation%20imposes%20communication%20and%20computational%20burdens.%20To%20address%20these%0Achallenges%2C%20we%20propose%20%5Ctextit%7BFedABC%7D%2C%20an%20innovative%20client%20selection%0Aalgorithm%20designed%20to%20take%20a%20long-term%20view%20in%20managing%20data%20heterogeneity%20and%0Aoptimizing%20client%20participation.%20Inspired%20by%20attention%20mechanisms%2C%0A%5Ctextit%7BFedABC%7D%20prioritizes%20informative%20clients%20by%20evaluating%20both%20model%0Asimilarity%20and%20each%20model%27s%20unique%20contributions%20to%20the%20global%20model.%20Moreover%2C%0Aconsidering%20the%20evolving%20demands%20of%20the%20global%20model%2C%20we%20formulate%20an%0Aoptimization%20problem%20to%20guide%20%5Ctextit%7BFedABC%7D%20throughout%20the%20training%20process.%0AFollowing%20the%20%60%60later-is-better%22%20principle%2C%20%5Ctextit%7BFedABC%7D%20adaptively%20adjusts%0Athe%20client%20selection%20threshold%2C%20encouraging%20greater%20participation%20in%20later%0Atraining%20stages.%20Extensive%20simulations%20on%20CIFAR-10%20demonstrate%20that%0A%5Ctextit%7BFedABC%7D%20significantly%20outperforms%20existing%20approaches%20in%20model%20accuracy%0Aand%20client%20participation%20efficiency%2C%20achieving%20comparable%20performance%20with%2032%5C%25%0Afewer%20clients%20than%20the%20classical%20FL%20algorithm%20%5Ctextit%7BFedAvg%7D%2C%20and%203.5%5C%25%20higher%0Aaccuracy%20with%202%5C%25%20fewer%20clients%20than%20the%20state-of-the-art.%20This%20work%20marks%20a%0Astep%20toward%20deploying%20FL%20in%20heterogeneous%2C%20resource-constrained%20environments%2C%0Athereby%20supporting%20native%20AI%20capabilities%20in%206G%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20871v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%255Ctextit%257BFedABC%257D%253A%2520Attention-Based%2520Client%2520Selection%2520for%2520Federated%2520Learning%250A%2520%2520with%2520Long-Term%2520View%26entry.906535625%3DWenxuan%2520Ye%2520and%2520Xueli%2520An%2520and%2520Junfan%2520Wang%2520and%2520Xueqiang%2520Yan%2520and%2520Georg%2520Carle%26entry.1292438233%3D%2520%2520Native%2520AI%2520support%2520is%2520a%2520key%2520objective%2520in%2520the%2520evolution%2520of%25206G%2520networks%252C%2520with%250AFederated%2520Learning%2520%2528FL%2529%2520emerging%2520as%2520a%2520promising%2520paradigm.%2520FL%2520allows%250Adecentralized%2520clients%2520to%2520collaboratively%2520train%2520an%2520AI%2520model%2520without%2520directly%250Asharing%2520their%2520data%252C%2520preserving%2520privacy.%2520Clients%2520train%2520local%2520models%2520on%2520private%250Adata%2520and%2520share%2520model%2520updates%252C%2520which%2520a%2520central%2520server%2520aggregates%2520to%2520refine%2520the%250Aglobal%2520model%2520and%2520redistribute%2520it%2520for%2520the%2520next%2520iteration.%2520However%252C%2520client%2520data%250Aheterogeneity%2520slows%2520convergence%2520and%2520reduces%2520model%2520accuracy%252C%2520and%2520frequent%2520client%250Aparticipation%2520imposes%2520communication%2520and%2520computational%2520burdens.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520%255Ctextit%257BFedABC%257D%252C%2520an%2520innovative%2520client%2520selection%250Aalgorithm%2520designed%2520to%2520take%2520a%2520long-term%2520view%2520in%2520managing%2520data%2520heterogeneity%2520and%250Aoptimizing%2520client%2520participation.%2520Inspired%2520by%2520attention%2520mechanisms%252C%250A%255Ctextit%257BFedABC%257D%2520prioritizes%2520informative%2520clients%2520by%2520evaluating%2520both%2520model%250Asimilarity%2520and%2520each%2520model%2527s%2520unique%2520contributions%2520to%2520the%2520global%2520model.%2520Moreover%252C%250Aconsidering%2520the%2520evolving%2520demands%2520of%2520the%2520global%2520model%252C%2520we%2520formulate%2520an%250Aoptimization%2520problem%2520to%2520guide%2520%255Ctextit%257BFedABC%257D%2520throughout%2520the%2520training%2520process.%250AFollowing%2520the%2520%2560%2560later-is-better%2522%2520principle%252C%2520%255Ctextit%257BFedABC%257D%2520adaptively%2520adjusts%250Athe%2520client%2520selection%2520threshold%252C%2520encouraging%2520greater%2520participation%2520in%2520later%250Atraining%2520stages.%2520Extensive%2520simulations%2520on%2520CIFAR-10%2520demonstrate%2520that%250A%255Ctextit%257BFedABC%257D%2520significantly%2520outperforms%2520existing%2520approaches%2520in%2520model%2520accuracy%250Aand%2520client%2520participation%2520efficiency%252C%2520achieving%2520comparable%2520performance%2520with%252032%255C%2525%250Afewer%2520clients%2520than%2520the%2520classical%2520FL%2520algorithm%2520%255Ctextit%257BFedAvg%257D%252C%2520and%25203.5%255C%2525%2520higher%250Aaccuracy%2520with%25202%255C%2525%2520fewer%2520clients%2520than%2520the%2520state-of-the-art.%2520This%2520work%2520marks%2520a%250Astep%2520toward%2520deploying%2520FL%2520in%2520heterogeneous%252C%2520resource-constrained%2520environments%252C%250Athereby%2520supporting%2520native%2520AI%2520capabilities%2520in%25206G%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20871v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%5Ctextit%7BFedABC%7D%3A%20Attention-Based%20Client%20Selection%20for%20Federated%20Learning%0A%20%20with%20Long-Term%20View&entry.906535625=Wenxuan%20Ye%20and%20Xueli%20An%20and%20Junfan%20Wang%20and%20Xueqiang%20Yan%20and%20Georg%20Carle&entry.1292438233=%20%20Native%20AI%20support%20is%20a%20key%20objective%20in%20the%20evolution%20of%206G%20networks%2C%20with%0AFederated%20Learning%20%28FL%29%20emerging%20as%20a%20promising%20paradigm.%20FL%20allows%0Adecentralized%20clients%20to%20collaboratively%20train%20an%20AI%20model%20without%20directly%0Asharing%20their%20data%2C%20preserving%20privacy.%20Clients%20train%20local%20models%20on%20private%0Adata%20and%20share%20model%20updates%2C%20which%20a%20central%20server%20aggregates%20to%20refine%20the%0Aglobal%20model%20and%20redistribute%20it%20for%20the%20next%20iteration.%20However%2C%20client%20data%0Aheterogeneity%20slows%20convergence%20and%20reduces%20model%20accuracy%2C%20and%20frequent%20client%0Aparticipation%20imposes%20communication%20and%20computational%20burdens.%20To%20address%20these%0Achallenges%2C%20we%20propose%20%5Ctextit%7BFedABC%7D%2C%20an%20innovative%20client%20selection%0Aalgorithm%20designed%20to%20take%20a%20long-term%20view%20in%20managing%20data%20heterogeneity%20and%0Aoptimizing%20client%20participation.%20Inspired%20by%20attention%20mechanisms%2C%0A%5Ctextit%7BFedABC%7D%20prioritizes%20informative%20clients%20by%20evaluating%20both%20model%0Asimilarity%20and%20each%20model%27s%20unique%20contributions%20to%20the%20global%20model.%20Moreover%2C%0Aconsidering%20the%20evolving%20demands%20of%20the%20global%20model%2C%20we%20formulate%20an%0Aoptimization%20problem%20to%20guide%20%5Ctextit%7BFedABC%7D%20throughout%20the%20training%20process.%0AFollowing%20the%20%60%60later-is-better%22%20principle%2C%20%5Ctextit%7BFedABC%7D%20adaptively%20adjusts%0Athe%20client%20selection%20threshold%2C%20encouraging%20greater%20participation%20in%20later%0Atraining%20stages.%20Extensive%20simulations%20on%20CIFAR-10%20demonstrate%20that%0A%5Ctextit%7BFedABC%7D%20significantly%20outperforms%20existing%20approaches%20in%20model%20accuracy%0Aand%20client%20participation%20efficiency%2C%20achieving%20comparable%20performance%20with%2032%5C%25%0Afewer%20clients%20than%20the%20classical%20FL%20algorithm%20%5Ctextit%7BFedAvg%7D%2C%20and%203.5%5C%25%20higher%0Aaccuracy%20with%202%5C%25%20fewer%20clients%20than%20the%20state-of-the-art.%20This%20work%20marks%20a%0Astep%20toward%20deploying%20FL%20in%20heterogeneous%2C%20resource-constrained%20environments%2C%0Athereby%20supporting%20native%20AI%20capabilities%20in%206G%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20871v1&entry.124074799=Read"},
{"title": "A Survey of Deep Learning for Geometry Problem Solving", "author": "Jianzhe Ma and Wenxuan Wang and Qin Jin", "abstract": "  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n", "link": "http://arxiv.org/abs/2507.11936v4", "date": "2025-07-28", "relevancy": 2.4959, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Deep%20Learning%20for%20Geometry%20Problem%20Solving&body=Title%3A%20A%20Survey%20of%20Deep%20Learning%20for%20Geometry%20Problem%20Solving%0AAuthor%3A%20Jianzhe%20Ma%20and%20Wenxuan%20Wang%20and%20Qin%20Jin%0AAbstract%3A%20%20%20Geometry%20problem%20solving%20is%20a%20key%20area%20of%20mathematical%20reasoning%2C%20which%20is%0Awidely%20involved%20in%20many%20important%20fields%20such%20as%20education%2C%20mathematical%0Aability%20assessment%20of%20artificial%20intelligence%2C%20and%20multimodal%20ability%0Aassessment.%20In%20recent%20years%2C%20the%20rapid%20development%20of%20deep%20learning%20technology%2C%0Aespecially%20the%20rise%20of%20multimodal%20large%20language%20models%2C%20has%20triggered%20a%0Awidespread%20research%20boom.%20This%20paper%20provides%20a%20survey%20of%20the%20applications%20of%0Adeep%20learning%20in%20geometry%20problem%20solving%2C%20including%20%28i%29%20a%20comprehensive%0Asummary%20of%20the%20relevant%20tasks%20in%20geometry%20problem%20solving%3B%20%28ii%29%20a%20thorough%0Areview%20of%20related%20deep%20learning%20methods%3B%20%28iii%29%20a%20detailed%20analysis%20of%0Aevaluation%20metrics%20and%20methods%3B%20and%20%28iv%29%20a%20critical%20discussion%20of%20the%20current%0Achallenges%20and%20future%20directions%20that%20can%20be%20explored.%20Our%20goal%20is%20to%20provide%20a%0Acomprehensive%20and%20practical%20reference%20of%20deep%20learning%20for%20geometry%20problem%0Asolving%20to%20promote%20further%20developments%20in%20this%20field.%20We%20create%20a%20continuously%0Aupdated%20list%20of%20papers%20on%20GitHub%3A%20https%3A//github.com/majianz/dl4gps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11936v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Deep%2520Learning%2520for%2520Geometry%2520Problem%2520Solving%26entry.906535625%3DJianzhe%2520Ma%2520and%2520Wenxuan%2520Wang%2520and%2520Qin%2520Jin%26entry.1292438233%3D%2520%2520Geometry%2520problem%2520solving%2520is%2520a%2520key%2520area%2520of%2520mathematical%2520reasoning%252C%2520which%2520is%250Awidely%2520involved%2520in%2520many%2520important%2520fields%2520such%2520as%2520education%252C%2520mathematical%250Aability%2520assessment%2520of%2520artificial%2520intelligence%252C%2520and%2520multimodal%2520ability%250Aassessment.%2520In%2520recent%2520years%252C%2520the%2520rapid%2520development%2520of%2520deep%2520learning%2520technology%252C%250Aespecially%2520the%2520rise%2520of%2520multimodal%2520large%2520language%2520models%252C%2520has%2520triggered%2520a%250Awidespread%2520research%2520boom.%2520This%2520paper%2520provides%2520a%2520survey%2520of%2520the%2520applications%2520of%250Adeep%2520learning%2520in%2520geometry%2520problem%2520solving%252C%2520including%2520%2528i%2529%2520a%2520comprehensive%250Asummary%2520of%2520the%2520relevant%2520tasks%2520in%2520geometry%2520problem%2520solving%253B%2520%2528ii%2529%2520a%2520thorough%250Areview%2520of%2520related%2520deep%2520learning%2520methods%253B%2520%2528iii%2529%2520a%2520detailed%2520analysis%2520of%250Aevaluation%2520metrics%2520and%2520methods%253B%2520and%2520%2528iv%2529%2520a%2520critical%2520discussion%2520of%2520the%2520current%250Achallenges%2520and%2520future%2520directions%2520that%2520can%2520be%2520explored.%2520Our%2520goal%2520is%2520to%2520provide%2520a%250Acomprehensive%2520and%2520practical%2520reference%2520of%2520deep%2520learning%2520for%2520geometry%2520problem%250Asolving%2520to%2520promote%2520further%2520developments%2520in%2520this%2520field.%2520We%2520create%2520a%2520continuously%250Aupdated%2520list%2520of%2520papers%2520on%2520GitHub%253A%2520https%253A//github.com/majianz/dl4gps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11936v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Deep%20Learning%20for%20Geometry%20Problem%20Solving&entry.906535625=Jianzhe%20Ma%20and%20Wenxuan%20Wang%20and%20Qin%20Jin&entry.1292438233=%20%20Geometry%20problem%20solving%20is%20a%20key%20area%20of%20mathematical%20reasoning%2C%20which%20is%0Awidely%20involved%20in%20many%20important%20fields%20such%20as%20education%2C%20mathematical%0Aability%20assessment%20of%20artificial%20intelligence%2C%20and%20multimodal%20ability%0Aassessment.%20In%20recent%20years%2C%20the%20rapid%20development%20of%20deep%20learning%20technology%2C%0Aespecially%20the%20rise%20of%20multimodal%20large%20language%20models%2C%20has%20triggered%20a%0Awidespread%20research%20boom.%20This%20paper%20provides%20a%20survey%20of%20the%20applications%20of%0Adeep%20learning%20in%20geometry%20problem%20solving%2C%20including%20%28i%29%20a%20comprehensive%0Asummary%20of%20the%20relevant%20tasks%20in%20geometry%20problem%20solving%3B%20%28ii%29%20a%20thorough%0Areview%20of%20related%20deep%20learning%20methods%3B%20%28iii%29%20a%20detailed%20analysis%20of%0Aevaluation%20metrics%20and%20methods%3B%20and%20%28iv%29%20a%20critical%20discussion%20of%20the%20current%0Achallenges%20and%20future%20directions%20that%20can%20be%20explored.%20Our%20goal%20is%20to%20provide%20a%0Acomprehensive%20and%20practical%20reference%20of%20deep%20learning%20for%20geometry%20problem%0Asolving%20to%20promote%20further%20developments%20in%20this%20field.%20We%20create%20a%20continuously%0Aupdated%20list%20of%20papers%20on%20GitHub%3A%20https%3A//github.com/majianz/dl4gps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11936v4&entry.124074799=Read"},
{"title": "Mask-Free Audio-driven Talking Face Generation for Enhanced Visual\n  Quality and Identity Preservation", "author": "Dogucan Yaman and Fevziye Irem Eyiokur and Leonard B\u00e4rmann and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "  Audio-Driven Talking Face Generation aims at generating realistic videos of\ntalking faces, focusing on accurate audio-lip synchronization without\ndeteriorating any identity-related visual details. Recent state-of-the-art\nmethods are based on inpainting, meaning that the lower half of the input face\nis masked, and the model fills the masked region by generating lips aligned\nwith the given audio. Hence, to preserve identity-related visual details from\nthe lower half, these approaches additionally require an unmasked identity\nreference image randomly selected from the same video. However, this common\nmasking strategy suffers from (1) information loss in the input faces,\nsignificantly affecting the networks' ability to preserve visual quality and\nidentity details, (2) variation between identity reference and input image\ndegrading reconstruction performance, and (3) the identity reference negatively\nimpacting the model, causing unintended copying of elements unaligned with the\naudio. To address these issues, we propose a mask-free talking face generation\napproach while maintaining the 2D-based face editing task. Instead of masking\nthe lower half, we transform the input images to have closed mouths, using a\ntwo-step landmark-based approach trained in an unpaired manner. Subsequently,\nwe provide these edited but unmasked faces to a lip adaptation model alongside\nthe audio to generate appropriate lip movements. Thus, our approach needs\nneither masked input images nor identity reference images. We conduct\nexperiments on the benchmark LRS2 and HDTF datasets and perform various\nablation studies to validate our contributions.\n", "link": "http://arxiv.org/abs/2507.20953v1", "date": "2025-07-28", "relevancy": 2.475, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6506}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5995}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mask-Free%20Audio-driven%20Talking%20Face%20Generation%20for%20Enhanced%20Visual%0A%20%20Quality%20and%20Identity%20Preservation&body=Title%3A%20Mask-Free%20Audio-driven%20Talking%20Face%20Generation%20for%20Enhanced%20Visual%0A%20%20Quality%20and%20Identity%20Preservation%0AAuthor%3A%20Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Leonard%20B%C3%A4rmann%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20%20%20Audio-Driven%20Talking%20Face%20Generation%20aims%20at%20generating%20realistic%20videos%20of%0Atalking%20faces%2C%20focusing%20on%20accurate%20audio-lip%20synchronization%20without%0Adeteriorating%20any%20identity-related%20visual%20details.%20Recent%20state-of-the-art%0Amethods%20are%20based%20on%20inpainting%2C%20meaning%20that%20the%20lower%20half%20of%20the%20input%20face%0Ais%20masked%2C%20and%20the%20model%20fills%20the%20masked%20region%20by%20generating%20lips%20aligned%0Awith%20the%20given%20audio.%20Hence%2C%20to%20preserve%20identity-related%20visual%20details%20from%0Athe%20lower%20half%2C%20these%20approaches%20additionally%20require%20an%20unmasked%20identity%0Areference%20image%20randomly%20selected%20from%20the%20same%20video.%20However%2C%20this%20common%0Amasking%20strategy%20suffers%20from%20%281%29%20information%20loss%20in%20the%20input%20faces%2C%0Asignificantly%20affecting%20the%20networks%27%20ability%20to%20preserve%20visual%20quality%20and%0Aidentity%20details%2C%20%282%29%20variation%20between%20identity%20reference%20and%20input%20image%0Adegrading%20reconstruction%20performance%2C%20and%20%283%29%20the%20identity%20reference%20negatively%0Aimpacting%20the%20model%2C%20causing%20unintended%20copying%20of%20elements%20unaligned%20with%20the%0Aaudio.%20To%20address%20these%20issues%2C%20we%20propose%20a%20mask-free%20talking%20face%20generation%0Aapproach%20while%20maintaining%20the%202D-based%20face%20editing%20task.%20Instead%20of%20masking%0Athe%20lower%20half%2C%20we%20transform%20the%20input%20images%20to%20have%20closed%20mouths%2C%20using%20a%0Atwo-step%20landmark-based%20approach%20trained%20in%20an%20unpaired%20manner.%20Subsequently%2C%0Awe%20provide%20these%20edited%20but%20unmasked%20faces%20to%20a%20lip%20adaptation%20model%20alongside%0Athe%20audio%20to%20generate%20appropriate%20lip%20movements.%20Thus%2C%20our%20approach%20needs%0Aneither%20masked%20input%20images%20nor%20identity%20reference%20images.%20We%20conduct%0Aexperiments%20on%20the%20benchmark%20LRS2%20and%20HDTF%20datasets%20and%20perform%20various%0Aablation%20studies%20to%20validate%20our%20contributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMask-Free%2520Audio-driven%2520Talking%2520Face%2520Generation%2520for%2520Enhanced%2520Visual%250A%2520%2520Quality%2520and%2520Identity%2520Preservation%26entry.906535625%3DDogucan%2520Yaman%2520and%2520Fevziye%2520Irem%2520Eyiokur%2520and%2520Leonard%2520B%25C3%25A4rmann%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3D%2520%2520Audio-Driven%2520Talking%2520Face%2520Generation%2520aims%2520at%2520generating%2520realistic%2520videos%2520of%250Atalking%2520faces%252C%2520focusing%2520on%2520accurate%2520audio-lip%2520synchronization%2520without%250Adeteriorating%2520any%2520identity-related%2520visual%2520details.%2520Recent%2520state-of-the-art%250Amethods%2520are%2520based%2520on%2520inpainting%252C%2520meaning%2520that%2520the%2520lower%2520half%2520of%2520the%2520input%2520face%250Ais%2520masked%252C%2520and%2520the%2520model%2520fills%2520the%2520masked%2520region%2520by%2520generating%2520lips%2520aligned%250Awith%2520the%2520given%2520audio.%2520Hence%252C%2520to%2520preserve%2520identity-related%2520visual%2520details%2520from%250Athe%2520lower%2520half%252C%2520these%2520approaches%2520additionally%2520require%2520an%2520unmasked%2520identity%250Areference%2520image%2520randomly%2520selected%2520from%2520the%2520same%2520video.%2520However%252C%2520this%2520common%250Amasking%2520strategy%2520suffers%2520from%2520%25281%2529%2520information%2520loss%2520in%2520the%2520input%2520faces%252C%250Asignificantly%2520affecting%2520the%2520networks%2527%2520ability%2520to%2520preserve%2520visual%2520quality%2520and%250Aidentity%2520details%252C%2520%25282%2529%2520variation%2520between%2520identity%2520reference%2520and%2520input%2520image%250Adegrading%2520reconstruction%2520performance%252C%2520and%2520%25283%2529%2520the%2520identity%2520reference%2520negatively%250Aimpacting%2520the%2520model%252C%2520causing%2520unintended%2520copying%2520of%2520elements%2520unaligned%2520with%2520the%250Aaudio.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520mask-free%2520talking%2520face%2520generation%250Aapproach%2520while%2520maintaining%2520the%25202D-based%2520face%2520editing%2520task.%2520Instead%2520of%2520masking%250Athe%2520lower%2520half%252C%2520we%2520transform%2520the%2520input%2520images%2520to%2520have%2520closed%2520mouths%252C%2520using%2520a%250Atwo-step%2520landmark-based%2520approach%2520trained%2520in%2520an%2520unpaired%2520manner.%2520Subsequently%252C%250Awe%2520provide%2520these%2520edited%2520but%2520unmasked%2520faces%2520to%2520a%2520lip%2520adaptation%2520model%2520alongside%250Athe%2520audio%2520to%2520generate%2520appropriate%2520lip%2520movements.%2520Thus%252C%2520our%2520approach%2520needs%250Aneither%2520masked%2520input%2520images%2520nor%2520identity%2520reference%2520images.%2520We%2520conduct%250Aexperiments%2520on%2520the%2520benchmark%2520LRS2%2520and%2520HDTF%2520datasets%2520and%2520perform%2520various%250Aablation%2520studies%2520to%2520validate%2520our%2520contributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mask-Free%20Audio-driven%20Talking%20Face%20Generation%20for%20Enhanced%20Visual%0A%20%20Quality%20and%20Identity%20Preservation&entry.906535625=Dogucan%20Yaman%20and%20Fevziye%20Irem%20Eyiokur%20and%20Leonard%20B%C3%A4rmann%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=%20%20Audio-Driven%20Talking%20Face%20Generation%20aims%20at%20generating%20realistic%20videos%20of%0Atalking%20faces%2C%20focusing%20on%20accurate%20audio-lip%20synchronization%20without%0Adeteriorating%20any%20identity-related%20visual%20details.%20Recent%20state-of-the-art%0Amethods%20are%20based%20on%20inpainting%2C%20meaning%20that%20the%20lower%20half%20of%20the%20input%20face%0Ais%20masked%2C%20and%20the%20model%20fills%20the%20masked%20region%20by%20generating%20lips%20aligned%0Awith%20the%20given%20audio.%20Hence%2C%20to%20preserve%20identity-related%20visual%20details%20from%0Athe%20lower%20half%2C%20these%20approaches%20additionally%20require%20an%20unmasked%20identity%0Areference%20image%20randomly%20selected%20from%20the%20same%20video.%20However%2C%20this%20common%0Amasking%20strategy%20suffers%20from%20%281%29%20information%20loss%20in%20the%20input%20faces%2C%0Asignificantly%20affecting%20the%20networks%27%20ability%20to%20preserve%20visual%20quality%20and%0Aidentity%20details%2C%20%282%29%20variation%20between%20identity%20reference%20and%20input%20image%0Adegrading%20reconstruction%20performance%2C%20and%20%283%29%20the%20identity%20reference%20negatively%0Aimpacting%20the%20model%2C%20causing%20unintended%20copying%20of%20elements%20unaligned%20with%20the%0Aaudio.%20To%20address%20these%20issues%2C%20we%20propose%20a%20mask-free%20talking%20face%20generation%0Aapproach%20while%20maintaining%20the%202D-based%20face%20editing%20task.%20Instead%20of%20masking%0Athe%20lower%20half%2C%20we%20transform%20the%20input%20images%20to%20have%20closed%20mouths%2C%20using%20a%0Atwo-step%20landmark-based%20approach%20trained%20in%20an%20unpaired%20manner.%20Subsequently%2C%0Awe%20provide%20these%20edited%20but%20unmasked%20faces%20to%20a%20lip%20adaptation%20model%20alongside%0Athe%20audio%20to%20generate%20appropriate%20lip%20movements.%20Thus%2C%20our%20approach%20needs%0Aneither%20masked%20input%20images%20nor%20identity%20reference%20images.%20We%20conduct%0Aexperiments%20on%20the%20benchmark%20LRS2%20and%20HDTF%20datasets%20and%20perform%20various%0Aablation%20studies%20to%20validate%20our%20contributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20953v1&entry.124074799=Read"},
{"title": "PixelNav: Towards Model-based Vision-Only Navigation with Topological\n  Graphs", "author": "Sergey Bakulin and Timur Akhtyamov and Denis Fatykhov and German Devchich and Gonzalo Ferrer", "abstract": "  This work proposes a novel hybrid approach for vision-only navigation of\nmobile robots, which combines advances of both deep learning approaches and\nclassical model-based planning algorithms. Today, purely data-driven end-to-end\nmodels are dominant solutions to this problem. Despite advantages such as\nflexibility and adaptability, the requirement of a large amount of training\ndata and limited interpretability are the main bottlenecks for their practical\napplications. To address these limitations, we propose a hierarchical system\nthat utilizes recent advances in model predictive control, traversability\nestimation, visual place recognition, and pose estimation, employing\ntopological graphs as a representation of the target environment. Using such a\ncombination, we provide a scalable system with a higher level of\ninterpretability compared to end-to-end approaches. Extensive real-world\nexperiments show the efficiency of the proposed method.\n", "link": "http://arxiv.org/abs/2507.20892v1", "date": "2025-07-28", "relevancy": 2.4532, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6364}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6074}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixelNav%3A%20Towards%20Model-based%20Vision-Only%20Navigation%20with%20Topological%0A%20%20Graphs&body=Title%3A%20PixelNav%3A%20Towards%20Model-based%20Vision-Only%20Navigation%20with%20Topological%0A%20%20Graphs%0AAuthor%3A%20Sergey%20Bakulin%20and%20Timur%20Akhtyamov%20and%20Denis%20Fatykhov%20and%20German%20Devchich%20and%20Gonzalo%20Ferrer%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20novel%20hybrid%20approach%20for%20vision-only%20navigation%20of%0Amobile%20robots%2C%20which%20combines%20advances%20of%20both%20deep%20learning%20approaches%20and%0Aclassical%20model-based%20planning%20algorithms.%20Today%2C%20purely%20data-driven%20end-to-end%0Amodels%20are%20dominant%20solutions%20to%20this%20problem.%20Despite%20advantages%20such%20as%0Aflexibility%20and%20adaptability%2C%20the%20requirement%20of%20a%20large%20amount%20of%20training%0Adata%20and%20limited%20interpretability%20are%20the%20main%20bottlenecks%20for%20their%20practical%0Aapplications.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20hierarchical%20system%0Athat%20utilizes%20recent%20advances%20in%20model%20predictive%20control%2C%20traversability%0Aestimation%2C%20visual%20place%20recognition%2C%20and%20pose%20estimation%2C%20employing%0Atopological%20graphs%20as%20a%20representation%20of%20the%20target%20environment.%20Using%20such%20a%0Acombination%2C%20we%20provide%20a%20scalable%20system%20with%20a%20higher%20level%20of%0Ainterpretability%20compared%20to%20end-to-end%20approaches.%20Extensive%20real-world%0Aexperiments%20show%20the%20efficiency%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixelNav%253A%2520Towards%2520Model-based%2520Vision-Only%2520Navigation%2520with%2520Topological%250A%2520%2520Graphs%26entry.906535625%3DSergey%2520Bakulin%2520and%2520Timur%2520Akhtyamov%2520and%2520Denis%2520Fatykhov%2520and%2520German%2520Devchich%2520and%2520Gonzalo%2520Ferrer%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520novel%2520hybrid%2520approach%2520for%2520vision-only%2520navigation%2520of%250Amobile%2520robots%252C%2520which%2520combines%2520advances%2520of%2520both%2520deep%2520learning%2520approaches%2520and%250Aclassical%2520model-based%2520planning%2520algorithms.%2520Today%252C%2520purely%2520data-driven%2520end-to-end%250Amodels%2520are%2520dominant%2520solutions%2520to%2520this%2520problem.%2520Despite%2520advantages%2520such%2520as%250Aflexibility%2520and%2520adaptability%252C%2520the%2520requirement%2520of%2520a%2520large%2520amount%2520of%2520training%250Adata%2520and%2520limited%2520interpretability%2520are%2520the%2520main%2520bottlenecks%2520for%2520their%2520practical%250Aapplications.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520hierarchical%2520system%250Athat%2520utilizes%2520recent%2520advances%2520in%2520model%2520predictive%2520control%252C%2520traversability%250Aestimation%252C%2520visual%2520place%2520recognition%252C%2520and%2520pose%2520estimation%252C%2520employing%250Atopological%2520graphs%2520as%2520a%2520representation%2520of%2520the%2520target%2520environment.%2520Using%2520such%2520a%250Acombination%252C%2520we%2520provide%2520a%2520scalable%2520system%2520with%2520a%2520higher%2520level%2520of%250Ainterpretability%2520compared%2520to%2520end-to-end%2520approaches.%2520Extensive%2520real-world%250Aexperiments%2520show%2520the%2520efficiency%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixelNav%3A%20Towards%20Model-based%20Vision-Only%20Navigation%20with%20Topological%0A%20%20Graphs&entry.906535625=Sergey%20Bakulin%20and%20Timur%20Akhtyamov%20and%20Denis%20Fatykhov%20and%20German%20Devchich%20and%20Gonzalo%20Ferrer&entry.1292438233=%20%20This%20work%20proposes%20a%20novel%20hybrid%20approach%20for%20vision-only%20navigation%20of%0Amobile%20robots%2C%20which%20combines%20advances%20of%20both%20deep%20learning%20approaches%20and%0Aclassical%20model-based%20planning%20algorithms.%20Today%2C%20purely%20data-driven%20end-to-end%0Amodels%20are%20dominant%20solutions%20to%20this%20problem.%20Despite%20advantages%20such%20as%0Aflexibility%20and%20adaptability%2C%20the%20requirement%20of%20a%20large%20amount%20of%20training%0Adata%20and%20limited%20interpretability%20are%20the%20main%20bottlenecks%20for%20their%20practical%0Aapplications.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20hierarchical%20system%0Athat%20utilizes%20recent%20advances%20in%20model%20predictive%20control%2C%20traversability%0Aestimation%2C%20visual%20place%20recognition%2C%20and%20pose%20estimation%2C%20employing%0Atopological%20graphs%20as%20a%20representation%20of%20the%20target%20environment.%20Using%20such%20a%0Acombination%2C%20we%20provide%20a%20scalable%20system%20with%20a%20higher%20level%20of%0Ainterpretability%20compared%20to%20end-to-end%20approaches.%20Extensive%20real-world%0Aexperiments%20show%20the%20efficiency%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20892v1&entry.124074799=Read"},
{"title": "Online hierarchical partitioning of the output space in extreme\n  multi-label data stream", "author": "Lara Neves and Afonso Louren\u00e7o and Alberto Cano and Goreti Marreiros", "abstract": "  Mining data streams with multi-label outputs poses significant challenges due\nto evolving distributions, high-dimensional label spaces, sparse label\noccurrences, and complex label dependencies. Moreover, concept drift affects\nnot only input distributions but also label correlations and imbalance ratios\nover time, complicating model adaptation. To address these challenges,\nstructured learners are categorized into local and global methods. Local\nmethods break down the task into simpler components, while global methods adapt\nthe algorithm to the full output space, potentially yielding better predictions\nby exploiting label correlations. This work introduces iHOMER (Incremental\nHierarchy Of Multi-label Classifiers), an online multi-label learning framework\nthat incrementally partitions the label space into disjoint, correlated\nclusters without relying on predefined hierarchies. iHOMER leverages online\ndivisive-agglomerative clustering based on \\textit{Jaccard} similarity and a\nglobal tree-based learner driven by a multivariate \\textit{Bernoulli} process\nto guide instance partitioning. To address non-stationarity, it integrates\ndrift detection mechanisms at both global and local levels, enabling dynamic\nrestructuring of label partitions and subtrees. Experiments across 23\nreal-world datasets show iHOMER outperforms 5 state-of-the-art global\nbaselines, such as MLHAT, MLHT of Pruned Sets and iSOUPT, by 23\\%, and 12 local\nbaselines, such as binary relevance transformations of kNN, EFDT, ARF, and\nADWIN bagging/boosting ensembles, by 32\\%, establishing its robustness for\nonline multi-label classification.\n", "link": "http://arxiv.org/abs/2507.20894v1", "date": "2025-07-28", "relevancy": 2.4507, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4993}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20hierarchical%20partitioning%20of%20the%20output%20space%20in%20extreme%0A%20%20multi-label%20data%20stream&body=Title%3A%20Online%20hierarchical%20partitioning%20of%20the%20output%20space%20in%20extreme%0A%20%20multi-label%20data%20stream%0AAuthor%3A%20Lara%20Neves%20and%20Afonso%20Louren%C3%A7o%20and%20Alberto%20Cano%20and%20Goreti%20Marreiros%0AAbstract%3A%20%20%20Mining%20data%20streams%20with%20multi-label%20outputs%20poses%20significant%20challenges%20due%0Ato%20evolving%20distributions%2C%20high-dimensional%20label%20spaces%2C%20sparse%20label%0Aoccurrences%2C%20and%20complex%20label%20dependencies.%20Moreover%2C%20concept%20drift%20affects%0Anot%20only%20input%20distributions%20but%20also%20label%20correlations%20and%20imbalance%20ratios%0Aover%20time%2C%20complicating%20model%20adaptation.%20To%20address%20these%20challenges%2C%0Astructured%20learners%20are%20categorized%20into%20local%20and%20global%20methods.%20Local%0Amethods%20break%20down%20the%20task%20into%20simpler%20components%2C%20while%20global%20methods%20adapt%0Athe%20algorithm%20to%20the%20full%20output%20space%2C%20potentially%20yielding%20better%20predictions%0Aby%20exploiting%20label%20correlations.%20This%20work%20introduces%20iHOMER%20%28Incremental%0AHierarchy%20Of%20Multi-label%20Classifiers%29%2C%20an%20online%20multi-label%20learning%20framework%0Athat%20incrementally%20partitions%20the%20label%20space%20into%20disjoint%2C%20correlated%0Aclusters%20without%20relying%20on%20predefined%20hierarchies.%20iHOMER%20leverages%20online%0Adivisive-agglomerative%20clustering%20based%20on%20%5Ctextit%7BJaccard%7D%20similarity%20and%20a%0Aglobal%20tree-based%20learner%20driven%20by%20a%20multivariate%20%5Ctextit%7BBernoulli%7D%20process%0Ato%20guide%20instance%20partitioning.%20To%20address%20non-stationarity%2C%20it%20integrates%0Adrift%20detection%20mechanisms%20at%20both%20global%20and%20local%20levels%2C%20enabling%20dynamic%0Arestructuring%20of%20label%20partitions%20and%20subtrees.%20Experiments%20across%2023%0Areal-world%20datasets%20show%20iHOMER%20outperforms%205%20state-of-the-art%20global%0Abaselines%2C%20such%20as%20MLHAT%2C%20MLHT%20of%20Pruned%20Sets%20and%20iSOUPT%2C%20by%2023%5C%25%2C%20and%2012%20local%0Abaselines%2C%20such%20as%20binary%20relevance%20transformations%20of%20kNN%2C%20EFDT%2C%20ARF%2C%20and%0AADWIN%20bagging/boosting%20ensembles%2C%20by%2032%5C%25%2C%20establishing%20its%20robustness%20for%0Aonline%20multi-label%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520hierarchical%2520partitioning%2520of%2520the%2520output%2520space%2520in%2520extreme%250A%2520%2520multi-label%2520data%2520stream%26entry.906535625%3DLara%2520Neves%2520and%2520Afonso%2520Louren%25C3%25A7o%2520and%2520Alberto%2520Cano%2520and%2520Goreti%2520Marreiros%26entry.1292438233%3D%2520%2520Mining%2520data%2520streams%2520with%2520multi-label%2520outputs%2520poses%2520significant%2520challenges%2520due%250Ato%2520evolving%2520distributions%252C%2520high-dimensional%2520label%2520spaces%252C%2520sparse%2520label%250Aoccurrences%252C%2520and%2520complex%2520label%2520dependencies.%2520Moreover%252C%2520concept%2520drift%2520affects%250Anot%2520only%2520input%2520distributions%2520but%2520also%2520label%2520correlations%2520and%2520imbalance%2520ratios%250Aover%2520time%252C%2520complicating%2520model%2520adaptation.%2520To%2520address%2520these%2520challenges%252C%250Astructured%2520learners%2520are%2520categorized%2520into%2520local%2520and%2520global%2520methods.%2520Local%250Amethods%2520break%2520down%2520the%2520task%2520into%2520simpler%2520components%252C%2520while%2520global%2520methods%2520adapt%250Athe%2520algorithm%2520to%2520the%2520full%2520output%2520space%252C%2520potentially%2520yielding%2520better%2520predictions%250Aby%2520exploiting%2520label%2520correlations.%2520This%2520work%2520introduces%2520iHOMER%2520%2528Incremental%250AHierarchy%2520Of%2520Multi-label%2520Classifiers%2529%252C%2520an%2520online%2520multi-label%2520learning%2520framework%250Athat%2520incrementally%2520partitions%2520the%2520label%2520space%2520into%2520disjoint%252C%2520correlated%250Aclusters%2520without%2520relying%2520on%2520predefined%2520hierarchies.%2520iHOMER%2520leverages%2520online%250Adivisive-agglomerative%2520clustering%2520based%2520on%2520%255Ctextit%257BJaccard%257D%2520similarity%2520and%2520a%250Aglobal%2520tree-based%2520learner%2520driven%2520by%2520a%2520multivariate%2520%255Ctextit%257BBernoulli%257D%2520process%250Ato%2520guide%2520instance%2520partitioning.%2520To%2520address%2520non-stationarity%252C%2520it%2520integrates%250Adrift%2520detection%2520mechanisms%2520at%2520both%2520global%2520and%2520local%2520levels%252C%2520enabling%2520dynamic%250Arestructuring%2520of%2520label%2520partitions%2520and%2520subtrees.%2520Experiments%2520across%252023%250Areal-world%2520datasets%2520show%2520iHOMER%2520outperforms%25205%2520state-of-the-art%2520global%250Abaselines%252C%2520such%2520as%2520MLHAT%252C%2520MLHT%2520of%2520Pruned%2520Sets%2520and%2520iSOUPT%252C%2520by%252023%255C%2525%252C%2520and%252012%2520local%250Abaselines%252C%2520such%2520as%2520binary%2520relevance%2520transformations%2520of%2520kNN%252C%2520EFDT%252C%2520ARF%252C%2520and%250AADWIN%2520bagging/boosting%2520ensembles%252C%2520by%252032%255C%2525%252C%2520establishing%2520its%2520robustness%2520for%250Aonline%2520multi-label%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20hierarchical%20partitioning%20of%20the%20output%20space%20in%20extreme%0A%20%20multi-label%20data%20stream&entry.906535625=Lara%20Neves%20and%20Afonso%20Louren%C3%A7o%20and%20Alberto%20Cano%20and%20Goreti%20Marreiros&entry.1292438233=%20%20Mining%20data%20streams%20with%20multi-label%20outputs%20poses%20significant%20challenges%20due%0Ato%20evolving%20distributions%2C%20high-dimensional%20label%20spaces%2C%20sparse%20label%0Aoccurrences%2C%20and%20complex%20label%20dependencies.%20Moreover%2C%20concept%20drift%20affects%0Anot%20only%20input%20distributions%20but%20also%20label%20correlations%20and%20imbalance%20ratios%0Aover%20time%2C%20complicating%20model%20adaptation.%20To%20address%20these%20challenges%2C%0Astructured%20learners%20are%20categorized%20into%20local%20and%20global%20methods.%20Local%0Amethods%20break%20down%20the%20task%20into%20simpler%20components%2C%20while%20global%20methods%20adapt%0Athe%20algorithm%20to%20the%20full%20output%20space%2C%20potentially%20yielding%20better%20predictions%0Aby%20exploiting%20label%20correlations.%20This%20work%20introduces%20iHOMER%20%28Incremental%0AHierarchy%20Of%20Multi-label%20Classifiers%29%2C%20an%20online%20multi-label%20learning%20framework%0Athat%20incrementally%20partitions%20the%20label%20space%20into%20disjoint%2C%20correlated%0Aclusters%20without%20relying%20on%20predefined%20hierarchies.%20iHOMER%20leverages%20online%0Adivisive-agglomerative%20clustering%20based%20on%20%5Ctextit%7BJaccard%7D%20similarity%20and%20a%0Aglobal%20tree-based%20learner%20driven%20by%20a%20multivariate%20%5Ctextit%7BBernoulli%7D%20process%0Ato%20guide%20instance%20partitioning.%20To%20address%20non-stationarity%2C%20it%20integrates%0Adrift%20detection%20mechanisms%20at%20both%20global%20and%20local%20levels%2C%20enabling%20dynamic%0Arestructuring%20of%20label%20partitions%20and%20subtrees.%20Experiments%20across%2023%0Areal-world%20datasets%20show%20iHOMER%20outperforms%205%20state-of-the-art%20global%0Abaselines%2C%20such%20as%20MLHAT%2C%20MLHT%20of%20Pruned%20Sets%20and%20iSOUPT%2C%20by%2023%5C%25%2C%20and%2012%20local%0Abaselines%2C%20such%20as%20binary%20relevance%20transformations%20of%20kNN%2C%20EFDT%2C%20ARF%2C%20and%0AADWIN%20bagging/boosting%20ensembles%2C%20by%2032%5C%25%2C%20establishing%20its%20robustness%20for%0Aonline%20multi-label%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20894v1&entry.124074799=Read"},
{"title": "SEAL: Searching Expandable Architectures for Incremental Learning", "author": "Matteo Gambella and Manuel Roveri", "abstract": "  Incremental learning is a machine learning paradigm where a model learns from\na sequential stream of tasks. This setting poses a key challenge: balancing\nplasticity (learning new tasks) and stability (preserving past knowledge).\nNeural Architecture Search (NAS), a branch of AutoML, automates the design of\nthe architecture of Deep Neural Networks and has shown success in static\nsettings. However, existing NAS-based approaches to incremental learning often\nrely on expanding the model at every task, making them impractical in\nresource-constrained environments. In this work, we introduce SEAL, a NAS-based\nframework tailored for data-incremental learning, a scenario where disjoint\ndata samples arrive sequentially and are not stored for future access. SEAL\nadapts the model structure dynamically by expanding it only when necessary,\nbased on a capacity estimation metric. Stability is preserved through\ncross-distillation training after each expansion step. The NAS component\njointly searches for both the architecture and the optimal expansion policy.\nExperiments across multiple benchmarks demonstrate that SEAL effectively\nreduces forgetting and enhances accuracy while maintaining a lower model size\ncompared to prior methods. These results highlight the promise of combining NAS\nand selective expansion for efficient, adaptive learning in incremental\nscenarios.\n", "link": "http://arxiv.org/abs/2505.10457v2", "date": "2025-07-28", "relevancy": 2.435, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4901}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAL%3A%20Searching%20Expandable%20Architectures%20for%20Incremental%20Learning&body=Title%3A%20SEAL%3A%20Searching%20Expandable%20Architectures%20for%20Incremental%20Learning%0AAuthor%3A%20Matteo%20Gambella%20and%20Manuel%20Roveri%0AAbstract%3A%20%20%20Incremental%20learning%20is%20a%20machine%20learning%20paradigm%20where%20a%20model%20learns%20from%0Aa%20sequential%20stream%20of%20tasks.%20This%20setting%20poses%20a%20key%20challenge%3A%20balancing%0Aplasticity%20%28learning%20new%20tasks%29%20and%20stability%20%28preserving%20past%20knowledge%29.%0ANeural%20Architecture%20Search%20%28NAS%29%2C%20a%20branch%20of%20AutoML%2C%20automates%20the%20design%20of%0Athe%20architecture%20of%20Deep%20Neural%20Networks%20and%20has%20shown%20success%20in%20static%0Asettings.%20However%2C%20existing%20NAS-based%20approaches%20to%20incremental%20learning%20often%0Arely%20on%20expanding%20the%20model%20at%20every%20task%2C%20making%20them%20impractical%20in%0Aresource-constrained%20environments.%20In%20this%20work%2C%20we%20introduce%20SEAL%2C%20a%20NAS-based%0Aframework%20tailored%20for%20data-incremental%20learning%2C%20a%20scenario%20where%20disjoint%0Adata%20samples%20arrive%20sequentially%20and%20are%20not%20stored%20for%20future%20access.%20SEAL%0Aadapts%20the%20model%20structure%20dynamically%20by%20expanding%20it%20only%20when%20necessary%2C%0Abased%20on%20a%20capacity%20estimation%20metric.%20Stability%20is%20preserved%20through%0Across-distillation%20training%20after%20each%20expansion%20step.%20The%20NAS%20component%0Ajointly%20searches%20for%20both%20the%20architecture%20and%20the%20optimal%20expansion%20policy.%0AExperiments%20across%20multiple%20benchmarks%20demonstrate%20that%20SEAL%20effectively%0Areduces%20forgetting%20and%20enhances%20accuracy%20while%20maintaining%20a%20lower%20model%20size%0Acompared%20to%20prior%20methods.%20These%20results%20highlight%20the%20promise%20of%20combining%20NAS%0Aand%20selective%20expansion%20for%20efficient%2C%20adaptive%20learning%20in%20incremental%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.10457v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAL%253A%2520Searching%2520Expandable%2520Architectures%2520for%2520Incremental%2520Learning%26entry.906535625%3DMatteo%2520Gambella%2520and%2520Manuel%2520Roveri%26entry.1292438233%3D%2520%2520Incremental%2520learning%2520is%2520a%2520machine%2520learning%2520paradigm%2520where%2520a%2520model%2520learns%2520from%250Aa%2520sequential%2520stream%2520of%2520tasks.%2520This%2520setting%2520poses%2520a%2520key%2520challenge%253A%2520balancing%250Aplasticity%2520%2528learning%2520new%2520tasks%2529%2520and%2520stability%2520%2528preserving%2520past%2520knowledge%2529.%250ANeural%2520Architecture%2520Search%2520%2528NAS%2529%252C%2520a%2520branch%2520of%2520AutoML%252C%2520automates%2520the%2520design%2520of%250Athe%2520architecture%2520of%2520Deep%2520Neural%2520Networks%2520and%2520has%2520shown%2520success%2520in%2520static%250Asettings.%2520However%252C%2520existing%2520NAS-based%2520approaches%2520to%2520incremental%2520learning%2520often%250Arely%2520on%2520expanding%2520the%2520model%2520at%2520every%2520task%252C%2520making%2520them%2520impractical%2520in%250Aresource-constrained%2520environments.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SEAL%252C%2520a%2520NAS-based%250Aframework%2520tailored%2520for%2520data-incremental%2520learning%252C%2520a%2520scenario%2520where%2520disjoint%250Adata%2520samples%2520arrive%2520sequentially%2520and%2520are%2520not%2520stored%2520for%2520future%2520access.%2520SEAL%250Aadapts%2520the%2520model%2520structure%2520dynamically%2520by%2520expanding%2520it%2520only%2520when%2520necessary%252C%250Abased%2520on%2520a%2520capacity%2520estimation%2520metric.%2520Stability%2520is%2520preserved%2520through%250Across-distillation%2520training%2520after%2520each%2520expansion%2520step.%2520The%2520NAS%2520component%250Ajointly%2520searches%2520for%2520both%2520the%2520architecture%2520and%2520the%2520optimal%2520expansion%2520policy.%250AExperiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520SEAL%2520effectively%250Areduces%2520forgetting%2520and%2520enhances%2520accuracy%2520while%2520maintaining%2520a%2520lower%2520model%2520size%250Acompared%2520to%2520prior%2520methods.%2520These%2520results%2520highlight%2520the%2520promise%2520of%2520combining%2520NAS%250Aand%2520selective%2520expansion%2520for%2520efficient%252C%2520adaptive%2520learning%2520in%2520incremental%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10457v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAL%3A%20Searching%20Expandable%20Architectures%20for%20Incremental%20Learning&entry.906535625=Matteo%20Gambella%20and%20Manuel%20Roveri&entry.1292438233=%20%20Incremental%20learning%20is%20a%20machine%20learning%20paradigm%20where%20a%20model%20learns%20from%0Aa%20sequential%20stream%20of%20tasks.%20This%20setting%20poses%20a%20key%20challenge%3A%20balancing%0Aplasticity%20%28learning%20new%20tasks%29%20and%20stability%20%28preserving%20past%20knowledge%29.%0ANeural%20Architecture%20Search%20%28NAS%29%2C%20a%20branch%20of%20AutoML%2C%20automates%20the%20design%20of%0Athe%20architecture%20of%20Deep%20Neural%20Networks%20and%20has%20shown%20success%20in%20static%0Asettings.%20However%2C%20existing%20NAS-based%20approaches%20to%20incremental%20learning%20often%0Arely%20on%20expanding%20the%20model%20at%20every%20task%2C%20making%20them%20impractical%20in%0Aresource-constrained%20environments.%20In%20this%20work%2C%20we%20introduce%20SEAL%2C%20a%20NAS-based%0Aframework%20tailored%20for%20data-incremental%20learning%2C%20a%20scenario%20where%20disjoint%0Adata%20samples%20arrive%20sequentially%20and%20are%20not%20stored%20for%20future%20access.%20SEAL%0Aadapts%20the%20model%20structure%20dynamically%20by%20expanding%20it%20only%20when%20necessary%2C%0Abased%20on%20a%20capacity%20estimation%20metric.%20Stability%20is%20preserved%20through%0Across-distillation%20training%20after%20each%20expansion%20step.%20The%20NAS%20component%0Ajointly%20searches%20for%20both%20the%20architecture%20and%20the%20optimal%20expansion%20policy.%0AExperiments%20across%20multiple%20benchmarks%20demonstrate%20that%20SEAL%20effectively%0Areduces%20forgetting%20and%20enhances%20accuracy%20while%20maintaining%20a%20lower%20model%20size%0Acompared%20to%20prior%20methods.%20These%20results%20highlight%20the%20promise%20of%20combining%20NAS%0Aand%20selective%20expansion%20for%20efficient%2C%20adaptive%20learning%20in%20incremental%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.10457v2&entry.124074799=Read"},
{"title": "Novel Pivoted Cholesky Decompositions for Efficient Gaussian Process\n  Inference", "author": "Filip de Roos and Fabio Muratore", "abstract": "  The Cholesky decomposition is a fundamental tool for solving linear systems\nwith symmetric and positive definite matrices which are ubiquitous in linear\nalgebra, optimization, and machine learning. Its numerical stability can be\nimproved by introducing a pivoting strategy that iteratively permutes the rows\nand columns of the matrix. The order of pivoting indices determines how\naccurately the intermediate decomposition can reconstruct the original matrix,\nthus is decisive for the algorithm's efficiency in the case of early\ntermination. Standard implementations select the next pivot from the largest\nvalue on the diagonal. In the case of Bayesian nonparametric inference, this\nstrategy corresponds to greedy entropy maximization, which is often used in\nactive learning and design of experiments. We explore this connection in detail\nand deduce novel pivoting strategies for the Cholesky decomposition. The\nresulting algorithms are more efficient at reducing the uncertainty over a data\nset, can be updated to include information about observations, and additionally\nbenefit from a tailored implementation. We benchmark the effectiveness of the\nnew selection strategies on two tasks important to Gaussian processes: sparse\nregression and inference based on preconditioned iterative solvers. Our results\nshow that the proposed selection strategies are either on par or, in most\ncases, outperform traditional baselines while requiring a negligible amount of\nadditional computation.\n", "link": "http://arxiv.org/abs/2507.20678v1", "date": "2025-07-28", "relevancy": 2.432, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5122}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4742}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Pivoted%20Cholesky%20Decompositions%20for%20Efficient%20Gaussian%20Process%0A%20%20Inference&body=Title%3A%20Novel%20Pivoted%20Cholesky%20Decompositions%20for%20Efficient%20Gaussian%20Process%0A%20%20Inference%0AAuthor%3A%20Filip%20de%20Roos%20and%20Fabio%20Muratore%0AAbstract%3A%20%20%20The%20Cholesky%20decomposition%20is%20a%20fundamental%20tool%20for%20solving%20linear%20systems%0Awith%20symmetric%20and%20positive%20definite%20matrices%20which%20are%20ubiquitous%20in%20linear%0Aalgebra%2C%20optimization%2C%20and%20machine%20learning.%20Its%20numerical%20stability%20can%20be%0Aimproved%20by%20introducing%20a%20pivoting%20strategy%20that%20iteratively%20permutes%20the%20rows%0Aand%20columns%20of%20the%20matrix.%20The%20order%20of%20pivoting%20indices%20determines%20how%0Aaccurately%20the%20intermediate%20decomposition%20can%20reconstruct%20the%20original%20matrix%2C%0Athus%20is%20decisive%20for%20the%20algorithm%27s%20efficiency%20in%20the%20case%20of%20early%0Atermination.%20Standard%20implementations%20select%20the%20next%20pivot%20from%20the%20largest%0Avalue%20on%20the%20diagonal.%20In%20the%20case%20of%20Bayesian%20nonparametric%20inference%2C%20this%0Astrategy%20corresponds%20to%20greedy%20entropy%20maximization%2C%20which%20is%20often%20used%20in%0Aactive%20learning%20and%20design%20of%20experiments.%20We%20explore%20this%20connection%20in%20detail%0Aand%20deduce%20novel%20pivoting%20strategies%20for%20the%20Cholesky%20decomposition.%20The%0Aresulting%20algorithms%20are%20more%20efficient%20at%20reducing%20the%20uncertainty%20over%20a%20data%0Aset%2C%20can%20be%20updated%20to%20include%20information%20about%20observations%2C%20and%20additionally%0Abenefit%20from%20a%20tailored%20implementation.%20We%20benchmark%20the%20effectiveness%20of%20the%0Anew%20selection%20strategies%20on%20two%20tasks%20important%20to%20Gaussian%20processes%3A%20sparse%0Aregression%20and%20inference%20based%20on%20preconditioned%20iterative%20solvers.%20Our%20results%0Ashow%20that%20the%20proposed%20selection%20strategies%20are%20either%20on%20par%20or%2C%20in%20most%0Acases%2C%20outperform%20traditional%20baselines%20while%20requiring%20a%20negligible%20amount%20of%0Aadditional%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Pivoted%2520Cholesky%2520Decompositions%2520for%2520Efficient%2520Gaussian%2520Process%250A%2520%2520Inference%26entry.906535625%3DFilip%2520de%2520Roos%2520and%2520Fabio%2520Muratore%26entry.1292438233%3D%2520%2520The%2520Cholesky%2520decomposition%2520is%2520a%2520fundamental%2520tool%2520for%2520solving%2520linear%2520systems%250Awith%2520symmetric%2520and%2520positive%2520definite%2520matrices%2520which%2520are%2520ubiquitous%2520in%2520linear%250Aalgebra%252C%2520optimization%252C%2520and%2520machine%2520learning.%2520Its%2520numerical%2520stability%2520can%2520be%250Aimproved%2520by%2520introducing%2520a%2520pivoting%2520strategy%2520that%2520iteratively%2520permutes%2520the%2520rows%250Aand%2520columns%2520of%2520the%2520matrix.%2520The%2520order%2520of%2520pivoting%2520indices%2520determines%2520how%250Aaccurately%2520the%2520intermediate%2520decomposition%2520can%2520reconstruct%2520the%2520original%2520matrix%252C%250Athus%2520is%2520decisive%2520for%2520the%2520algorithm%2527s%2520efficiency%2520in%2520the%2520case%2520of%2520early%250Atermination.%2520Standard%2520implementations%2520select%2520the%2520next%2520pivot%2520from%2520the%2520largest%250Avalue%2520on%2520the%2520diagonal.%2520In%2520the%2520case%2520of%2520Bayesian%2520nonparametric%2520inference%252C%2520this%250Astrategy%2520corresponds%2520to%2520greedy%2520entropy%2520maximization%252C%2520which%2520is%2520often%2520used%2520in%250Aactive%2520learning%2520and%2520design%2520of%2520experiments.%2520We%2520explore%2520this%2520connection%2520in%2520detail%250Aand%2520deduce%2520novel%2520pivoting%2520strategies%2520for%2520the%2520Cholesky%2520decomposition.%2520The%250Aresulting%2520algorithms%2520are%2520more%2520efficient%2520at%2520reducing%2520the%2520uncertainty%2520over%2520a%2520data%250Aset%252C%2520can%2520be%2520updated%2520to%2520include%2520information%2520about%2520observations%252C%2520and%2520additionally%250Abenefit%2520from%2520a%2520tailored%2520implementation.%2520We%2520benchmark%2520the%2520effectiveness%2520of%2520the%250Anew%2520selection%2520strategies%2520on%2520two%2520tasks%2520important%2520to%2520Gaussian%2520processes%253A%2520sparse%250Aregression%2520and%2520inference%2520based%2520on%2520preconditioned%2520iterative%2520solvers.%2520Our%2520results%250Ashow%2520that%2520the%2520proposed%2520selection%2520strategies%2520are%2520either%2520on%2520par%2520or%252C%2520in%2520most%250Acases%252C%2520outperform%2520traditional%2520baselines%2520while%2520requiring%2520a%2520negligible%2520amount%2520of%250Aadditional%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Pivoted%20Cholesky%20Decompositions%20for%20Efficient%20Gaussian%20Process%0A%20%20Inference&entry.906535625=Filip%20de%20Roos%20and%20Fabio%20Muratore&entry.1292438233=%20%20The%20Cholesky%20decomposition%20is%20a%20fundamental%20tool%20for%20solving%20linear%20systems%0Awith%20symmetric%20and%20positive%20definite%20matrices%20which%20are%20ubiquitous%20in%20linear%0Aalgebra%2C%20optimization%2C%20and%20machine%20learning.%20Its%20numerical%20stability%20can%20be%0Aimproved%20by%20introducing%20a%20pivoting%20strategy%20that%20iteratively%20permutes%20the%20rows%0Aand%20columns%20of%20the%20matrix.%20The%20order%20of%20pivoting%20indices%20determines%20how%0Aaccurately%20the%20intermediate%20decomposition%20can%20reconstruct%20the%20original%20matrix%2C%0Athus%20is%20decisive%20for%20the%20algorithm%27s%20efficiency%20in%20the%20case%20of%20early%0Atermination.%20Standard%20implementations%20select%20the%20next%20pivot%20from%20the%20largest%0Avalue%20on%20the%20diagonal.%20In%20the%20case%20of%20Bayesian%20nonparametric%20inference%2C%20this%0Astrategy%20corresponds%20to%20greedy%20entropy%20maximization%2C%20which%20is%20often%20used%20in%0Aactive%20learning%20and%20design%20of%20experiments.%20We%20explore%20this%20connection%20in%20detail%0Aand%20deduce%20novel%20pivoting%20strategies%20for%20the%20Cholesky%20decomposition.%20The%0Aresulting%20algorithms%20are%20more%20efficient%20at%20reducing%20the%20uncertainty%20over%20a%20data%0Aset%2C%20can%20be%20updated%20to%20include%20information%20about%20observations%2C%20and%20additionally%0Abenefit%20from%20a%20tailored%20implementation.%20We%20benchmark%20the%20effectiveness%20of%20the%0Anew%20selection%20strategies%20on%20two%20tasks%20important%20to%20Gaussian%20processes%3A%20sparse%0Aregression%20and%20inference%20based%20on%20preconditioned%20iterative%20solvers.%20Our%20results%0Ashow%20that%20the%20proposed%20selection%20strategies%20are%20either%20on%20par%20or%2C%20in%20most%0Acases%2C%20outperform%20traditional%20baselines%20while%20requiring%20a%20negligible%20amount%20of%0Aadditional%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20678v1&entry.124074799=Read"},
{"title": "SCORPION: Addressing Scanner-Induced Variability in Histopathology", "author": "Jeongun Ryu and Heon Song and Seungeun Lee and Soo Ick Cho and Jiwon Shin and Kyunghyun Paeng and S\u00e9rgio Pereira", "abstract": "  Ensuring reliable model performance across diverse domains is a critical\nchallenge in computational pathology. A particular source of variability in\nWhole-Slide Images is introduced by differences in digital scanners, thus\ncalling for better scanner generalization. This is critical for the real-world\nadoption of computational pathology, where the scanning devices may differ per\ninstitution or hospital, and the model should not be dependent on\nscanner-induced details, which can ultimately affect the patient's diagnosis\nand treatment planning. However, past efforts have primarily focused on\nstandard domain generalization settings, evaluating on unseen scanners during\ntraining, without directly evaluating consistency across scanners for the same\ntissue. To overcome this limitation, we introduce SCORPION, a new dataset\nexplicitly designed to evaluate model reliability under scanner variability.\nSCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding\n2,400 spatially aligned patches. This scanner-paired design allows for the\nisolation of scanner-induced variability, enabling a rigorous evaluation of\nmodel consistency while controlling for differences in tissue composition.\nFurthermore, we propose SimCons, a flexible framework that combines\naugmentation-based domain generalization techniques with a consistency loss to\nexplicitly address scanner generalization. We empirically show that SimCons\nimproves model consistency on varying scanners without compromising\ntask-specific performance. By releasing the SCORPION dataset and proposing\nSimCons, we provide the research community with a crucial resource for\nevaluating and improving model consistency across diverse scanners, setting a\nnew standard for reliability testing.\n", "link": "http://arxiv.org/abs/2507.20907v1", "date": "2025-07-28", "relevancy": 2.4292, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4893}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4893}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCORPION%3A%20Addressing%20Scanner-Induced%20Variability%20in%20Histopathology&body=Title%3A%20SCORPION%3A%20Addressing%20Scanner-Induced%20Variability%20in%20Histopathology%0AAuthor%3A%20Jeongun%20Ryu%20and%20Heon%20Song%20and%20Seungeun%20Lee%20and%20Soo%20Ick%20Cho%20and%20Jiwon%20Shin%20and%20Kyunghyun%20Paeng%20and%20S%C3%A9rgio%20Pereira%0AAbstract%3A%20%20%20Ensuring%20reliable%20model%20performance%20across%20diverse%20domains%20is%20a%20critical%0Achallenge%20in%20computational%20pathology.%20A%20particular%20source%20of%20variability%20in%0AWhole-Slide%20Images%20is%20introduced%20by%20differences%20in%20digital%20scanners%2C%20thus%0Acalling%20for%20better%20scanner%20generalization.%20This%20is%20critical%20for%20the%20real-world%0Aadoption%20of%20computational%20pathology%2C%20where%20the%20scanning%20devices%20may%20differ%20per%0Ainstitution%20or%20hospital%2C%20and%20the%20model%20should%20not%20be%20dependent%20on%0Ascanner-induced%20details%2C%20which%20can%20ultimately%20affect%20the%20patient%27s%20diagnosis%0Aand%20treatment%20planning.%20However%2C%20past%20efforts%20have%20primarily%20focused%20on%0Astandard%20domain%20generalization%20settings%2C%20evaluating%20on%20unseen%20scanners%20during%0Atraining%2C%20without%20directly%20evaluating%20consistency%20across%20scanners%20for%20the%20same%0Atissue.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20SCORPION%2C%20a%20new%20dataset%0Aexplicitly%20designed%20to%20evaluate%20model%20reliability%20under%20scanner%20variability.%0ASCORPION%20includes%20480%20tissue%20samples%2C%20each%20scanned%20with%205%20scanners%2C%20yielding%0A2%2C400%20spatially%20aligned%20patches.%20This%20scanner-paired%20design%20allows%20for%20the%0Aisolation%20of%20scanner-induced%20variability%2C%20enabling%20a%20rigorous%20evaluation%20of%0Amodel%20consistency%20while%20controlling%20for%20differences%20in%20tissue%20composition.%0AFurthermore%2C%20we%20propose%20SimCons%2C%20a%20flexible%20framework%20that%20combines%0Aaugmentation-based%20domain%20generalization%20techniques%20with%20a%20consistency%20loss%20to%0Aexplicitly%20address%20scanner%20generalization.%20We%20empirically%20show%20that%20SimCons%0Aimproves%20model%20consistency%20on%20varying%20scanners%20without%20compromising%0Atask-specific%20performance.%20By%20releasing%20the%20SCORPION%20dataset%20and%20proposing%0ASimCons%2C%20we%20provide%20the%20research%20community%20with%20a%20crucial%20resource%20for%0Aevaluating%20and%20improving%20model%20consistency%20across%20diverse%20scanners%2C%20setting%20a%0Anew%20standard%20for%20reliability%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCORPION%253A%2520Addressing%2520Scanner-Induced%2520Variability%2520in%2520Histopathology%26entry.906535625%3DJeongun%2520Ryu%2520and%2520Heon%2520Song%2520and%2520Seungeun%2520Lee%2520and%2520Soo%2520Ick%2520Cho%2520and%2520Jiwon%2520Shin%2520and%2520Kyunghyun%2520Paeng%2520and%2520S%25C3%25A9rgio%2520Pereira%26entry.1292438233%3D%2520%2520Ensuring%2520reliable%2520model%2520performance%2520across%2520diverse%2520domains%2520is%2520a%2520critical%250Achallenge%2520in%2520computational%2520pathology.%2520A%2520particular%2520source%2520of%2520variability%2520in%250AWhole-Slide%2520Images%2520is%2520introduced%2520by%2520differences%2520in%2520digital%2520scanners%252C%2520thus%250Acalling%2520for%2520better%2520scanner%2520generalization.%2520This%2520is%2520critical%2520for%2520the%2520real-world%250Aadoption%2520of%2520computational%2520pathology%252C%2520where%2520the%2520scanning%2520devices%2520may%2520differ%2520per%250Ainstitution%2520or%2520hospital%252C%2520and%2520the%2520model%2520should%2520not%2520be%2520dependent%2520on%250Ascanner-induced%2520details%252C%2520which%2520can%2520ultimately%2520affect%2520the%2520patient%2527s%2520diagnosis%250Aand%2520treatment%2520planning.%2520However%252C%2520past%2520efforts%2520have%2520primarily%2520focused%2520on%250Astandard%2520domain%2520generalization%2520settings%252C%2520evaluating%2520on%2520unseen%2520scanners%2520during%250Atraining%252C%2520without%2520directly%2520evaluating%2520consistency%2520across%2520scanners%2520for%2520the%2520same%250Atissue.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520SCORPION%252C%2520a%2520new%2520dataset%250Aexplicitly%2520designed%2520to%2520evaluate%2520model%2520reliability%2520under%2520scanner%2520variability.%250ASCORPION%2520includes%2520480%2520tissue%2520samples%252C%2520each%2520scanned%2520with%25205%2520scanners%252C%2520yielding%250A2%252C400%2520spatially%2520aligned%2520patches.%2520This%2520scanner-paired%2520design%2520allows%2520for%2520the%250Aisolation%2520of%2520scanner-induced%2520variability%252C%2520enabling%2520a%2520rigorous%2520evaluation%2520of%250Amodel%2520consistency%2520while%2520controlling%2520for%2520differences%2520in%2520tissue%2520composition.%250AFurthermore%252C%2520we%2520propose%2520SimCons%252C%2520a%2520flexible%2520framework%2520that%2520combines%250Aaugmentation-based%2520domain%2520generalization%2520techniques%2520with%2520a%2520consistency%2520loss%2520to%250Aexplicitly%2520address%2520scanner%2520generalization.%2520We%2520empirically%2520show%2520that%2520SimCons%250Aimproves%2520model%2520consistency%2520on%2520varying%2520scanners%2520without%2520compromising%250Atask-specific%2520performance.%2520By%2520releasing%2520the%2520SCORPION%2520dataset%2520and%2520proposing%250ASimCons%252C%2520we%2520provide%2520the%2520research%2520community%2520with%2520a%2520crucial%2520resource%2520for%250Aevaluating%2520and%2520improving%2520model%2520consistency%2520across%2520diverse%2520scanners%252C%2520setting%2520a%250Anew%2520standard%2520for%2520reliability%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCORPION%3A%20Addressing%20Scanner-Induced%20Variability%20in%20Histopathology&entry.906535625=Jeongun%20Ryu%20and%20Heon%20Song%20and%20Seungeun%20Lee%20and%20Soo%20Ick%20Cho%20and%20Jiwon%20Shin%20and%20Kyunghyun%20Paeng%20and%20S%C3%A9rgio%20Pereira&entry.1292438233=%20%20Ensuring%20reliable%20model%20performance%20across%20diverse%20domains%20is%20a%20critical%0Achallenge%20in%20computational%20pathology.%20A%20particular%20source%20of%20variability%20in%0AWhole-Slide%20Images%20is%20introduced%20by%20differences%20in%20digital%20scanners%2C%20thus%0Acalling%20for%20better%20scanner%20generalization.%20This%20is%20critical%20for%20the%20real-world%0Aadoption%20of%20computational%20pathology%2C%20where%20the%20scanning%20devices%20may%20differ%20per%0Ainstitution%20or%20hospital%2C%20and%20the%20model%20should%20not%20be%20dependent%20on%0Ascanner-induced%20details%2C%20which%20can%20ultimately%20affect%20the%20patient%27s%20diagnosis%0Aand%20treatment%20planning.%20However%2C%20past%20efforts%20have%20primarily%20focused%20on%0Astandard%20domain%20generalization%20settings%2C%20evaluating%20on%20unseen%20scanners%20during%0Atraining%2C%20without%20directly%20evaluating%20consistency%20across%20scanners%20for%20the%20same%0Atissue.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20SCORPION%2C%20a%20new%20dataset%0Aexplicitly%20designed%20to%20evaluate%20model%20reliability%20under%20scanner%20variability.%0ASCORPION%20includes%20480%20tissue%20samples%2C%20each%20scanned%20with%205%20scanners%2C%20yielding%0A2%2C400%20spatially%20aligned%20patches.%20This%20scanner-paired%20design%20allows%20for%20the%0Aisolation%20of%20scanner-induced%20variability%2C%20enabling%20a%20rigorous%20evaluation%20of%0Amodel%20consistency%20while%20controlling%20for%20differences%20in%20tissue%20composition.%0AFurthermore%2C%20we%20propose%20SimCons%2C%20a%20flexible%20framework%20that%20combines%0Aaugmentation-based%20domain%20generalization%20techniques%20with%20a%20consistency%20loss%20to%0Aexplicitly%20address%20scanner%20generalization.%20We%20empirically%20show%20that%20SimCons%0Aimproves%20model%20consistency%20on%20varying%20scanners%20without%20compromising%0Atask-specific%20performance.%20By%20releasing%20the%20SCORPION%20dataset%20and%20proposing%0ASimCons%2C%20we%20provide%20the%20research%20community%20with%20a%20crucial%20resource%20for%0Aevaluating%20and%20improving%20model%20consistency%20across%20diverse%20scanners%2C%20setting%20a%0Anew%20standard%20for%20reliability%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20907v1&entry.124074799=Read"},
{"title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech\n  Concept Bottleneck Models", "author": "Roberto Labadie-Tamayo and Adrian Jaques B\u00f6ck and Djordje Slijep\u010devi\u0107 and Xihui Chen and Andreas Babic and Matthias Zeppelzauer", "abstract": "  Sexism has become widespread on social media and in online conversation. To\nhelp address this issue, the fifth Sexism Identification in Social Networks\n(EXIST) challenge is initiated at CLEF 2025. Among this year's international\nbenchmarks, we concentrate on solving the first task aiming to identify and\nclassify sexism in social media textual posts. In this paper, we describe our\nsolutions and report results for three subtasks: Subtask 1.1 - Sexism\nIdentification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask\n1.3 - Sexism Categorization in Tweets. We implement three models to address\neach subtask which constitute three individual runs: Speech Concept Bottleneck\nModel (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a\nfine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to encode input texts into a human-interpretable representation of\nadjectives, then used to train a lightweight classifier for downstream tasks.\nSCBMT extends SCBM by fusing adjective-based representation with contextual\nembeddings from transformers to balance interpretability and classification\nperformance. Beyond competitive results, these two models offer fine-grained\nexplanations at both instance (local) and class (global) levels. We also\ninvestigate how additional metadata, e.g., annotators' demographic profiles,\ncan be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data\naugmented with prior datasets, ranks 6th for English and Spanish and 4th for\nEnglish in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and\nSpanish and 6th for Spanish.\n", "link": "http://arxiv.org/abs/2507.20924v1", "date": "2025-07-28", "relevancy": 2.4195, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FHSTP%40EXIST%202025%20Benchmark%3A%20Sexism%20Detection%20with%20Transparent%20Speech%0A%20%20Concept%20Bottleneck%20Models&body=Title%3A%20FHSTP%40EXIST%202025%20Benchmark%3A%20Sexism%20Detection%20with%20Transparent%20Speech%0A%20%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Roberto%20Labadie-Tamayo%20and%20Adrian%20Jaques%20B%C3%B6ck%20and%20Djordje%20Slijep%C4%8Devi%C4%87%20and%20Xihui%20Chen%20and%20Andreas%20Babic%20and%20Matthias%20Zeppelzauer%0AAbstract%3A%20%20%20Sexism%20has%20become%20widespread%20on%20social%20media%20and%20in%20online%20conversation.%20To%0Ahelp%20address%20this%20issue%2C%20the%20fifth%20Sexism%20Identification%20in%20Social%20Networks%0A%28EXIST%29%20challenge%20is%20initiated%20at%20CLEF%202025.%20Among%20this%20year%27s%20international%0Abenchmarks%2C%20we%20concentrate%20on%20solving%20the%20first%20task%20aiming%20to%20identify%20and%0Aclassify%20sexism%20in%20social%20media%20textual%20posts.%20In%20this%20paper%2C%20we%20describe%20our%0Asolutions%20and%20report%20results%20for%20three%20subtasks%3A%20Subtask%201.1%20-%20Sexism%0AIdentification%20in%20Tweets%2C%20Subtask%201.2%20-%20Source%20Intention%20in%20Tweets%2C%20and%20Subtask%0A1.3%20-%20Sexism%20Categorization%20in%20Tweets.%20We%20implement%20three%20models%20to%20address%0Aeach%20subtask%20which%20constitute%20three%20individual%20runs%3A%20Speech%20Concept%20Bottleneck%0AModel%20%28SCBM%29%2C%20Speech%20Concept%20Bottleneck%20Model%20with%20Transformer%20%28SCBMT%29%2C%20and%20a%0Afine-tuned%20XLM-RoBERTa%20transformer%20model.%20SCBM%20uses%20descriptive%20adjectives%20as%0Ahuman-interpretable%20bottleneck%20concepts.%20SCBM%20leverages%20large%20language%20models%0A%28LLMs%29%20to%20encode%20input%20texts%20into%20a%20human-interpretable%20representation%20of%0Aadjectives%2C%20then%20used%20to%20train%20a%20lightweight%20classifier%20for%20downstream%20tasks.%0ASCBMT%20extends%20SCBM%20by%20fusing%20adjective-based%20representation%20with%20contextual%0Aembeddings%20from%20transformers%20to%20balance%20interpretability%20and%20classification%0Aperformance.%20Beyond%20competitive%20results%2C%20these%20two%20models%20offer%20fine-grained%0Aexplanations%20at%20both%20instance%20%28local%29%20and%20class%20%28global%29%20levels.%20We%20also%0Ainvestigate%20how%20additional%20metadata%2C%20e.g.%2C%20annotators%27%20demographic%20profiles%2C%0Acan%20be%20leveraged.%20For%20Subtask%201.1%2C%20XLM-RoBERTa%2C%20fine-tuned%20on%20provided%20data%0Aaugmented%20with%20prior%20datasets%2C%20ranks%206th%20for%20English%20and%20Spanish%20and%204th%20for%0AEnglish%20in%20the%20Soft-Soft%20evaluation.%20Our%20SCBMT%20achieves%207th%20for%20English%20and%0ASpanish%20and%206th%20for%20Spanish.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFHSTP%2540EXIST%25202025%2520Benchmark%253A%2520Sexism%2520Detection%2520with%2520Transparent%2520Speech%250A%2520%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DRoberto%2520Labadie-Tamayo%2520and%2520Adrian%2520Jaques%2520B%25C3%25B6ck%2520and%2520Djordje%2520Slijep%25C4%258Devi%25C4%2587%2520and%2520Xihui%2520Chen%2520and%2520Andreas%2520Babic%2520and%2520Matthias%2520Zeppelzauer%26entry.1292438233%3D%2520%2520Sexism%2520has%2520become%2520widespread%2520on%2520social%2520media%2520and%2520in%2520online%2520conversation.%2520To%250Ahelp%2520address%2520this%2520issue%252C%2520the%2520fifth%2520Sexism%2520Identification%2520in%2520Social%2520Networks%250A%2528EXIST%2529%2520challenge%2520is%2520initiated%2520at%2520CLEF%25202025.%2520Among%2520this%2520year%2527s%2520international%250Abenchmarks%252C%2520we%2520concentrate%2520on%2520solving%2520the%2520first%2520task%2520aiming%2520to%2520identify%2520and%250Aclassify%2520sexism%2520in%2520social%2520media%2520textual%2520posts.%2520In%2520this%2520paper%252C%2520we%2520describe%2520our%250Asolutions%2520and%2520report%2520results%2520for%2520three%2520subtasks%253A%2520Subtask%25201.1%2520-%2520Sexism%250AIdentification%2520in%2520Tweets%252C%2520Subtask%25201.2%2520-%2520Source%2520Intention%2520in%2520Tweets%252C%2520and%2520Subtask%250A1.3%2520-%2520Sexism%2520Categorization%2520in%2520Tweets.%2520We%2520implement%2520three%2520models%2520to%2520address%250Aeach%2520subtask%2520which%2520constitute%2520three%2520individual%2520runs%253A%2520Speech%2520Concept%2520Bottleneck%250AModel%2520%2528SCBM%2529%252C%2520Speech%2520Concept%2520Bottleneck%2520Model%2520with%2520Transformer%2520%2528SCBMT%2529%252C%2520and%2520a%250Afine-tuned%2520XLM-RoBERTa%2520transformer%2520model.%2520SCBM%2520uses%2520descriptive%2520adjectives%2520as%250Ahuman-interpretable%2520bottleneck%2520concepts.%2520SCBM%2520leverages%2520large%2520language%2520models%250A%2528LLMs%2529%2520to%2520encode%2520input%2520texts%2520into%2520a%2520human-interpretable%2520representation%2520of%250Aadjectives%252C%2520then%2520used%2520to%2520train%2520a%2520lightweight%2520classifier%2520for%2520downstream%2520tasks.%250ASCBMT%2520extends%2520SCBM%2520by%2520fusing%2520adjective-based%2520representation%2520with%2520contextual%250Aembeddings%2520from%2520transformers%2520to%2520balance%2520interpretability%2520and%2520classification%250Aperformance.%2520Beyond%2520competitive%2520results%252C%2520these%2520two%2520models%2520offer%2520fine-grained%250Aexplanations%2520at%2520both%2520instance%2520%2528local%2529%2520and%2520class%2520%2528global%2529%2520levels.%2520We%2520also%250Ainvestigate%2520how%2520additional%2520metadata%252C%2520e.g.%252C%2520annotators%2527%2520demographic%2520profiles%252C%250Acan%2520be%2520leveraged.%2520For%2520Subtask%25201.1%252C%2520XLM-RoBERTa%252C%2520fine-tuned%2520on%2520provided%2520data%250Aaugmented%2520with%2520prior%2520datasets%252C%2520ranks%25206th%2520for%2520English%2520and%2520Spanish%2520and%25204th%2520for%250AEnglish%2520in%2520the%2520Soft-Soft%2520evaluation.%2520Our%2520SCBMT%2520achieves%25207th%2520for%2520English%2520and%250ASpanish%2520and%25206th%2520for%2520Spanish.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FHSTP%40EXIST%202025%20Benchmark%3A%20Sexism%20Detection%20with%20Transparent%20Speech%0A%20%20Concept%20Bottleneck%20Models&entry.906535625=Roberto%20Labadie-Tamayo%20and%20Adrian%20Jaques%20B%C3%B6ck%20and%20Djordje%20Slijep%C4%8Devi%C4%87%20and%20Xihui%20Chen%20and%20Andreas%20Babic%20and%20Matthias%20Zeppelzauer&entry.1292438233=%20%20Sexism%20has%20become%20widespread%20on%20social%20media%20and%20in%20online%20conversation.%20To%0Ahelp%20address%20this%20issue%2C%20the%20fifth%20Sexism%20Identification%20in%20Social%20Networks%0A%28EXIST%29%20challenge%20is%20initiated%20at%20CLEF%202025.%20Among%20this%20year%27s%20international%0Abenchmarks%2C%20we%20concentrate%20on%20solving%20the%20first%20task%20aiming%20to%20identify%20and%0Aclassify%20sexism%20in%20social%20media%20textual%20posts.%20In%20this%20paper%2C%20we%20describe%20our%0Asolutions%20and%20report%20results%20for%20three%20subtasks%3A%20Subtask%201.1%20-%20Sexism%0AIdentification%20in%20Tweets%2C%20Subtask%201.2%20-%20Source%20Intention%20in%20Tweets%2C%20and%20Subtask%0A1.3%20-%20Sexism%20Categorization%20in%20Tweets.%20We%20implement%20three%20models%20to%20address%0Aeach%20subtask%20which%20constitute%20three%20individual%20runs%3A%20Speech%20Concept%20Bottleneck%0AModel%20%28SCBM%29%2C%20Speech%20Concept%20Bottleneck%20Model%20with%20Transformer%20%28SCBMT%29%2C%20and%20a%0Afine-tuned%20XLM-RoBERTa%20transformer%20model.%20SCBM%20uses%20descriptive%20adjectives%20as%0Ahuman-interpretable%20bottleneck%20concepts.%20SCBM%20leverages%20large%20language%20models%0A%28LLMs%29%20to%20encode%20input%20texts%20into%20a%20human-interpretable%20representation%20of%0Aadjectives%2C%20then%20used%20to%20train%20a%20lightweight%20classifier%20for%20downstream%20tasks.%0ASCBMT%20extends%20SCBM%20by%20fusing%20adjective-based%20representation%20with%20contextual%0Aembeddings%20from%20transformers%20to%20balance%20interpretability%20and%20classification%0Aperformance.%20Beyond%20competitive%20results%2C%20these%20two%20models%20offer%20fine-grained%0Aexplanations%20at%20both%20instance%20%28local%29%20and%20class%20%28global%29%20levels.%20We%20also%0Ainvestigate%20how%20additional%20metadata%2C%20e.g.%2C%20annotators%27%20demographic%20profiles%2C%0Acan%20be%20leveraged.%20For%20Subtask%201.1%2C%20XLM-RoBERTa%2C%20fine-tuned%20on%20provided%20data%0Aaugmented%20with%20prior%20datasets%2C%20ranks%206th%20for%20English%20and%20Spanish%20and%204th%20for%0AEnglish%20in%20the%20Soft-Soft%20evaluation.%20Our%20SCBMT%20achieves%207th%20for%20English%20and%0ASpanish%20and%206th%20for%20Spanish.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20924v1&entry.124074799=Read"},
{"title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding", "author": "Rongchang Xie and Chen Du and Ping Song and Chang Liu", "abstract": "  We introduce MUSE-VL, a Unified Vision-Language Model through Semantic\ndiscrete Encoding for multimodal understanding and generation. Recently, the\nresearch community has begun exploring unified models for visual generation and\nunderstanding. However, existing vision tokenizers (e.g., VQGAN) only consider\nlow-level information, which makes it difficult to align with language tokens.\nThis results in high training complexity and necessitates a large amount of\ntraining data to achieve optimal performance. Additionally, their performance\nis still far from dedicated understanding models. This paper proposes Semantic\nDiscrete Encoding (SDE), which effectively aligns the information of visual\ntokens and language tokens by adding semantic constraints to the visual\ntokenizer. This greatly reduces the amount of training data and improves the\nperformance of the unified model. With the same LLM size, our method improved\nthe understanding performance by 4.8% compared to the previous SOTA Emu3 and\nsurpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model\nalso surpasses the existing unified models on visual generation benchmarks.\n", "link": "http://arxiv.org/abs/2411.17762v4", "date": "2025-07-28", "relevancy": 2.4049, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.612}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUSE-VL%3A%20Modeling%20Unified%20VLM%20through%20Semantic%20Discrete%20Encoding&body=Title%3A%20MUSE-VL%3A%20Modeling%20Unified%20VLM%20through%20Semantic%20Discrete%20Encoding%0AAuthor%3A%20Rongchang%20Xie%20and%20Chen%20Du%20and%20Ping%20Song%20and%20Chang%20Liu%0AAbstract%3A%20%20%20We%20introduce%20MUSE-VL%2C%20a%20Unified%20Vision-Language%20Model%20through%20Semantic%0Adiscrete%20Encoding%20for%20multimodal%20understanding%20and%20generation.%20Recently%2C%20the%0Aresearch%20community%20has%20begun%20exploring%20unified%20models%20for%20visual%20generation%20and%0Aunderstanding.%20However%2C%20existing%20vision%20tokenizers%20%28e.g.%2C%20VQGAN%29%20only%20consider%0Alow-level%20information%2C%20which%20makes%20it%20difficult%20to%20align%20with%20language%20tokens.%0AThis%20results%20in%20high%20training%20complexity%20and%20necessitates%20a%20large%20amount%20of%0Atraining%20data%20to%20achieve%20optimal%20performance.%20Additionally%2C%20their%20performance%0Ais%20still%20far%20from%20dedicated%20understanding%20models.%20This%20paper%20proposes%20Semantic%0ADiscrete%20Encoding%20%28SDE%29%2C%20which%20effectively%20aligns%20the%20information%20of%20visual%0Atokens%20and%20language%20tokens%20by%20adding%20semantic%20constraints%20to%20the%20visual%0Atokenizer.%20This%20greatly%20reduces%20the%20amount%20of%20training%20data%20and%20improves%20the%0Aperformance%20of%20the%20unified%20model.%20With%20the%20same%20LLM%20size%2C%20our%20method%20improved%0Athe%20understanding%20performance%20by%204.8%25%20compared%20to%20the%20previous%20SOTA%20Emu3%20and%0Asurpassed%20the%20dedicated%20understanding%20model%20LLaVA-NeXT%2034B%20by%203.7%25.%20Our%20model%0Aalso%20surpasses%20the%20existing%20unified%20models%20on%20visual%20generation%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17762v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUSE-VL%253A%2520Modeling%2520Unified%2520VLM%2520through%2520Semantic%2520Discrete%2520Encoding%26entry.906535625%3DRongchang%2520Xie%2520and%2520Chen%2520Du%2520and%2520Ping%2520Song%2520and%2520Chang%2520Liu%26entry.1292438233%3D%2520%2520We%2520introduce%2520MUSE-VL%252C%2520a%2520Unified%2520Vision-Language%2520Model%2520through%2520Semantic%250Adiscrete%2520Encoding%2520for%2520multimodal%2520understanding%2520and%2520generation.%2520Recently%252C%2520the%250Aresearch%2520community%2520has%2520begun%2520exploring%2520unified%2520models%2520for%2520visual%2520generation%2520and%250Aunderstanding.%2520However%252C%2520existing%2520vision%2520tokenizers%2520%2528e.g.%252C%2520VQGAN%2529%2520only%2520consider%250Alow-level%2520information%252C%2520which%2520makes%2520it%2520difficult%2520to%2520align%2520with%2520language%2520tokens.%250AThis%2520results%2520in%2520high%2520training%2520complexity%2520and%2520necessitates%2520a%2520large%2520amount%2520of%250Atraining%2520data%2520to%2520achieve%2520optimal%2520performance.%2520Additionally%252C%2520their%2520performance%250Ais%2520still%2520far%2520from%2520dedicated%2520understanding%2520models.%2520This%2520paper%2520proposes%2520Semantic%250ADiscrete%2520Encoding%2520%2528SDE%2529%252C%2520which%2520effectively%2520aligns%2520the%2520information%2520of%2520visual%250Atokens%2520and%2520language%2520tokens%2520by%2520adding%2520semantic%2520constraints%2520to%2520the%2520visual%250Atokenizer.%2520This%2520greatly%2520reduces%2520the%2520amount%2520of%2520training%2520data%2520and%2520improves%2520the%250Aperformance%2520of%2520the%2520unified%2520model.%2520With%2520the%2520same%2520LLM%2520size%252C%2520our%2520method%2520improved%250Athe%2520understanding%2520performance%2520by%25204.8%2525%2520compared%2520to%2520the%2520previous%2520SOTA%2520Emu3%2520and%250Asurpassed%2520the%2520dedicated%2520understanding%2520model%2520LLaVA-NeXT%252034B%2520by%25203.7%2525.%2520Our%2520model%250Aalso%2520surpasses%2520the%2520existing%2520unified%2520models%2520on%2520visual%2520generation%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17762v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUSE-VL%3A%20Modeling%20Unified%20VLM%20through%20Semantic%20Discrete%20Encoding&entry.906535625=Rongchang%20Xie%20and%20Chen%20Du%20and%20Ping%20Song%20and%20Chang%20Liu&entry.1292438233=%20%20We%20introduce%20MUSE-VL%2C%20a%20Unified%20Vision-Language%20Model%20through%20Semantic%0Adiscrete%20Encoding%20for%20multimodal%20understanding%20and%20generation.%20Recently%2C%20the%0Aresearch%20community%20has%20begun%20exploring%20unified%20models%20for%20visual%20generation%20and%0Aunderstanding.%20However%2C%20existing%20vision%20tokenizers%20%28e.g.%2C%20VQGAN%29%20only%20consider%0Alow-level%20information%2C%20which%20makes%20it%20difficult%20to%20align%20with%20language%20tokens.%0AThis%20results%20in%20high%20training%20complexity%20and%20necessitates%20a%20large%20amount%20of%0Atraining%20data%20to%20achieve%20optimal%20performance.%20Additionally%2C%20their%20performance%0Ais%20still%20far%20from%20dedicated%20understanding%20models.%20This%20paper%20proposes%20Semantic%0ADiscrete%20Encoding%20%28SDE%29%2C%20which%20effectively%20aligns%20the%20information%20of%20visual%0Atokens%20and%20language%20tokens%20by%20adding%20semantic%20constraints%20to%20the%20visual%0Atokenizer.%20This%20greatly%20reduces%20the%20amount%20of%20training%20data%20and%20improves%20the%0Aperformance%20of%20the%20unified%20model.%20With%20the%20same%20LLM%20size%2C%20our%20method%20improved%0Athe%20understanding%20performance%20by%204.8%25%20compared%20to%20the%20previous%20SOTA%20Emu3%20and%0Asurpassed%20the%20dedicated%20understanding%20model%20LLaVA-NeXT%2034B%20by%203.7%25.%20Our%20model%0Aalso%20surpasses%20the%20existing%20unified%20models%20on%20visual%20generation%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17762v4&entry.124074799=Read"},
{"title": "GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic\n  Occupancy Prediction", "author": "Tianhao Li and Yang Li and Mengtian Li and Yisheng Deng and Weifeng Ge", "abstract": "  Accurately perceiving dynamic environments is a fundamental task for\nautonomous driving and robotic systems. Existing methods inadequately utilize\ntemporal information, relying mainly on local temporal interactions between\nadjacent frames and failing to leverage global sequence information\neffectively. To address this limitation, we investigate how to effectively\naggregate global temporal features from temporal sequences, aiming to achieve\noccupancy representations that efficiently utilize global temporal information\nfrom historical observations. For this purpose, we propose a global temporal\naggregation denoising network named GTAD, introducing a global temporal\ninformation aggregation framework as a new paradigm for holistic 3D scene\nunderstanding. Our method employs an in-model latent denoising network to\naggregate local temporal features from the current moment and global temporal\nfeatures from historical sequences. This approach enables the effective\nperception of both fine-grained temporal information from adjacent frames and\nglobal temporal patterns from historical observations. As a result, it provides\na more coherent and comprehensive understanding of the environment. Extensive\nexperiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studies\ndemonstrate the superiority of our method.\n", "link": "http://arxiv.org/abs/2507.20963v1", "date": "2025-07-28", "relevancy": 2.4043, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6366}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6038}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTAD%3A%20Global%20Temporal%20Aggregation%20Denoising%20Learning%20for%203D%20Semantic%0A%20%20Occupancy%20Prediction&body=Title%3A%20GTAD%3A%20Global%20Temporal%20Aggregation%20Denoising%20Learning%20for%203D%20Semantic%0A%20%20Occupancy%20Prediction%0AAuthor%3A%20Tianhao%20Li%20and%20Yang%20Li%20and%20Mengtian%20Li%20and%20Yisheng%20Deng%20and%20Weifeng%20Ge%0AAbstract%3A%20%20%20Accurately%20perceiving%20dynamic%20environments%20is%20a%20fundamental%20task%20for%0Aautonomous%20driving%20and%20robotic%20systems.%20Existing%20methods%20inadequately%20utilize%0Atemporal%20information%2C%20relying%20mainly%20on%20local%20temporal%20interactions%20between%0Aadjacent%20frames%20and%20failing%20to%20leverage%20global%20sequence%20information%0Aeffectively.%20To%20address%20this%20limitation%2C%20we%20investigate%20how%20to%20effectively%0Aaggregate%20global%20temporal%20features%20from%20temporal%20sequences%2C%20aiming%20to%20achieve%0Aoccupancy%20representations%20that%20efficiently%20utilize%20global%20temporal%20information%0Afrom%20historical%20observations.%20For%20this%20purpose%2C%20we%20propose%20a%20global%20temporal%0Aaggregation%20denoising%20network%20named%20GTAD%2C%20introducing%20a%20global%20temporal%0Ainformation%20aggregation%20framework%20as%20a%20new%20paradigm%20for%20holistic%203D%20scene%0Aunderstanding.%20Our%20method%20employs%20an%20in-model%20latent%20denoising%20network%20to%0Aaggregate%20local%20temporal%20features%20from%20the%20current%20moment%20and%20global%20temporal%0Afeatures%20from%20historical%20sequences.%20This%20approach%20enables%20the%20effective%0Aperception%20of%20both%20fine-grained%20temporal%20information%20from%20adjacent%20frames%20and%0Aglobal%20temporal%20patterns%20from%20historical%20observations.%20As%20a%20result%2C%20it%20provides%0Aa%20more%20coherent%20and%20comprehensive%20understanding%20of%20the%20environment.%20Extensive%0Aexperiments%20on%20the%20nuScenes%20and%20Occ3D-nuScenes%20benchmark%20and%20ablation%20studies%0Ademonstrate%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTAD%253A%2520Global%2520Temporal%2520Aggregation%2520Denoising%2520Learning%2520for%25203D%2520Semantic%250A%2520%2520Occupancy%2520Prediction%26entry.906535625%3DTianhao%2520Li%2520and%2520Yang%2520Li%2520and%2520Mengtian%2520Li%2520and%2520Yisheng%2520Deng%2520and%2520Weifeng%2520Ge%26entry.1292438233%3D%2520%2520Accurately%2520perceiving%2520dynamic%2520environments%2520is%2520a%2520fundamental%2520task%2520for%250Aautonomous%2520driving%2520and%2520robotic%2520systems.%2520Existing%2520methods%2520inadequately%2520utilize%250Atemporal%2520information%252C%2520relying%2520mainly%2520on%2520local%2520temporal%2520interactions%2520between%250Aadjacent%2520frames%2520and%2520failing%2520to%2520leverage%2520global%2520sequence%2520information%250Aeffectively.%2520To%2520address%2520this%2520limitation%252C%2520we%2520investigate%2520how%2520to%2520effectively%250Aaggregate%2520global%2520temporal%2520features%2520from%2520temporal%2520sequences%252C%2520aiming%2520to%2520achieve%250Aoccupancy%2520representations%2520that%2520efficiently%2520utilize%2520global%2520temporal%2520information%250Afrom%2520historical%2520observations.%2520For%2520this%2520purpose%252C%2520we%2520propose%2520a%2520global%2520temporal%250Aaggregation%2520denoising%2520network%2520named%2520GTAD%252C%2520introducing%2520a%2520global%2520temporal%250Ainformation%2520aggregation%2520framework%2520as%2520a%2520new%2520paradigm%2520for%2520holistic%25203D%2520scene%250Aunderstanding.%2520Our%2520method%2520employs%2520an%2520in-model%2520latent%2520denoising%2520network%2520to%250Aaggregate%2520local%2520temporal%2520features%2520from%2520the%2520current%2520moment%2520and%2520global%2520temporal%250Afeatures%2520from%2520historical%2520sequences.%2520This%2520approach%2520enables%2520the%2520effective%250Aperception%2520of%2520both%2520fine-grained%2520temporal%2520information%2520from%2520adjacent%2520frames%2520and%250Aglobal%2520temporal%2520patterns%2520from%2520historical%2520observations.%2520As%2520a%2520result%252C%2520it%2520provides%250Aa%2520more%2520coherent%2520and%2520comprehensive%2520understanding%2520of%2520the%2520environment.%2520Extensive%250Aexperiments%2520on%2520the%2520nuScenes%2520and%2520Occ3D-nuScenes%2520benchmark%2520and%2520ablation%2520studies%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTAD%3A%20Global%20Temporal%20Aggregation%20Denoising%20Learning%20for%203D%20Semantic%0A%20%20Occupancy%20Prediction&entry.906535625=Tianhao%20Li%20and%20Yang%20Li%20and%20Mengtian%20Li%20and%20Yisheng%20Deng%20and%20Weifeng%20Ge&entry.1292438233=%20%20Accurately%20perceiving%20dynamic%20environments%20is%20a%20fundamental%20task%20for%0Aautonomous%20driving%20and%20robotic%20systems.%20Existing%20methods%20inadequately%20utilize%0Atemporal%20information%2C%20relying%20mainly%20on%20local%20temporal%20interactions%20between%0Aadjacent%20frames%20and%20failing%20to%20leverage%20global%20sequence%20information%0Aeffectively.%20To%20address%20this%20limitation%2C%20we%20investigate%20how%20to%20effectively%0Aaggregate%20global%20temporal%20features%20from%20temporal%20sequences%2C%20aiming%20to%20achieve%0Aoccupancy%20representations%20that%20efficiently%20utilize%20global%20temporal%20information%0Afrom%20historical%20observations.%20For%20this%20purpose%2C%20we%20propose%20a%20global%20temporal%0Aaggregation%20denoising%20network%20named%20GTAD%2C%20introducing%20a%20global%20temporal%0Ainformation%20aggregation%20framework%20as%20a%20new%20paradigm%20for%20holistic%203D%20scene%0Aunderstanding.%20Our%20method%20employs%20an%20in-model%20latent%20denoising%20network%20to%0Aaggregate%20local%20temporal%20features%20from%20the%20current%20moment%20and%20global%20temporal%0Afeatures%20from%20historical%20sequences.%20This%20approach%20enables%20the%20effective%0Aperception%20of%20both%20fine-grained%20temporal%20information%20from%20adjacent%20frames%20and%0Aglobal%20temporal%20patterns%20from%20historical%20observations.%20As%20a%20result%2C%20it%20provides%0Aa%20more%20coherent%20and%20comprehensive%20understanding%20of%20the%20environment.%20Extensive%0Aexperiments%20on%20the%20nuScenes%20and%20Occ3D-nuScenes%20benchmark%20and%20ablation%20studies%0Ademonstrate%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20963v1&entry.124074799=Read"},
{"title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings", "author": "Tong Liu and Xiao Yu and Wenxuan Zhou and Jindong Gu and Volker Tresp", "abstract": "  Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.\n", "link": "http://arxiv.org/abs/2501.06645v3", "date": "2025-07-28", "relevancy": 2.3885, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4843}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocalPO%3A%20Enhancing%20Preference%20Optimizing%20by%20Focusing%20on%20Correct%0A%20%20Preference%20Rankings&body=Title%3A%20FocalPO%3A%20Enhancing%20Preference%20Optimizing%20by%20Focusing%20on%20Correct%0A%20%20Preference%20Rankings%0AAuthor%3A%20Tong%20Liu%20and%20Xiao%20Yu%20and%20Wenxuan%20Zhou%20and%20Jindong%20Gu%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20Efficient%20preference%20optimization%20algorithms%20such%20as%20Direct%20Preference%0AOptimization%20%28DPO%29%20have%20become%20a%20popular%20approach%20in%20aligning%20large%20language%0Amodels%20%28LLMs%29%20with%20human%20preferences.%20These%20algorithms%20implicitly%20treat%20the%20LLM%0Aas%20a%20reward%20model%2C%20and%20focus%20on%20training%20it%20to%20correct%20misranked%20preference%0Apairs.%20However%2C%20recent%20work~%5Ccitep%7Bchen2024preference%7D%20empirically%20finds%20that%0ADPO%20training%20%5Ctextit%7Brarely%20improves%20these%20misranked%20preference%20pairs%7D%2C%20despite%0Aits%20gradient%20emphasizing%20on%20these%20cases.%20We%20introduce%20FocalPO%2C%20a%20DPO%20variant%0Athat%20instead%20%5Ctextit%7Bdown-weighs%7D%20misranked%20preference%20pairs%20and%20prioritizes%0Aenhancing%20the%20model%27s%20understanding%20of%20pairs%20that%20it%20can%20already%20rank%0Acorrectly.%20Inspired%20by%20Focal%20Loss%20used%20in%20vision%20tasks%2C%20FocalPO%20achieves%20this%0Aby%20adding%20a%20modulating%20factor%20to%20dynamically%20scale%20DPO%20loss.%20Our%20experiment%0Ademonstrates%20that%20FocalPO%20surpasses%20DPO%20and%20its%20variants%20on%20popular%20benchmarks%0Alike%20Alpaca%20Eval%202.0%20using%20Mistral-Base-7B%20and%20Llama-3-Instruct-8B%2C%20with%20the%0Aintroduced%20hyperparameter%20fixed.%20Additionally%2C%20we%20empirically%20reveals%20how%0AFocalPO%20affects%20training%20on%20correct%20and%20incorrect%20sample%20groups%2C%20further%0Aunderscoring%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06645v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocalPO%253A%2520Enhancing%2520Preference%2520Optimizing%2520by%2520Focusing%2520on%2520Correct%250A%2520%2520Preference%2520Rankings%26entry.906535625%3DTong%2520Liu%2520and%2520Xiao%2520Yu%2520and%2520Wenxuan%2520Zhou%2520and%2520Jindong%2520Gu%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520Efficient%2520preference%2520optimization%2520algorithms%2520such%2520as%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%2520have%2520become%2520a%2520popular%2520approach%2520in%2520aligning%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520with%2520human%2520preferences.%2520These%2520algorithms%2520implicitly%2520treat%2520the%2520LLM%250Aas%2520a%2520reward%2520model%252C%2520and%2520focus%2520on%2520training%2520it%2520to%2520correct%2520misranked%2520preference%250Apairs.%2520However%252C%2520recent%2520work~%255Ccitep%257Bchen2024preference%257D%2520empirically%2520finds%2520that%250ADPO%2520training%2520%255Ctextit%257Brarely%2520improves%2520these%2520misranked%2520preference%2520pairs%257D%252C%2520despite%250Aits%2520gradient%2520emphasizing%2520on%2520these%2520cases.%2520We%2520introduce%2520FocalPO%252C%2520a%2520DPO%2520variant%250Athat%2520instead%2520%255Ctextit%257Bdown-weighs%257D%2520misranked%2520preference%2520pairs%2520and%2520prioritizes%250Aenhancing%2520the%2520model%2527s%2520understanding%2520of%2520pairs%2520that%2520it%2520can%2520already%2520rank%250Acorrectly.%2520Inspired%2520by%2520Focal%2520Loss%2520used%2520in%2520vision%2520tasks%252C%2520FocalPO%2520achieves%2520this%250Aby%2520adding%2520a%2520modulating%2520factor%2520to%2520dynamically%2520scale%2520DPO%2520loss.%2520Our%2520experiment%250Ademonstrates%2520that%2520FocalPO%2520surpasses%2520DPO%2520and%2520its%2520variants%2520on%2520popular%2520benchmarks%250Alike%2520Alpaca%2520Eval%25202.0%2520using%2520Mistral-Base-7B%2520and%2520Llama-3-Instruct-8B%252C%2520with%2520the%250Aintroduced%2520hyperparameter%2520fixed.%2520Additionally%252C%2520we%2520empirically%2520reveals%2520how%250AFocalPO%2520affects%2520training%2520on%2520correct%2520and%2520incorrect%2520sample%2520groups%252C%2520further%250Aunderscoring%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06645v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocalPO%3A%20Enhancing%20Preference%20Optimizing%20by%20Focusing%20on%20Correct%0A%20%20Preference%20Rankings&entry.906535625=Tong%20Liu%20and%20Xiao%20Yu%20and%20Wenxuan%20Zhou%20and%20Jindong%20Gu%20and%20Volker%20Tresp&entry.1292438233=%20%20Efficient%20preference%20optimization%20algorithms%20such%20as%20Direct%20Preference%0AOptimization%20%28DPO%29%20have%20become%20a%20popular%20approach%20in%20aligning%20large%20language%0Amodels%20%28LLMs%29%20with%20human%20preferences.%20These%20algorithms%20implicitly%20treat%20the%20LLM%0Aas%20a%20reward%20model%2C%20and%20focus%20on%20training%20it%20to%20correct%20misranked%20preference%0Apairs.%20However%2C%20recent%20work~%5Ccitep%7Bchen2024preference%7D%20empirically%20finds%20that%0ADPO%20training%20%5Ctextit%7Brarely%20improves%20these%20misranked%20preference%20pairs%7D%2C%20despite%0Aits%20gradient%20emphasizing%20on%20these%20cases.%20We%20introduce%20FocalPO%2C%20a%20DPO%20variant%0Athat%20instead%20%5Ctextit%7Bdown-weighs%7D%20misranked%20preference%20pairs%20and%20prioritizes%0Aenhancing%20the%20model%27s%20understanding%20of%20pairs%20that%20it%20can%20already%20rank%0Acorrectly.%20Inspired%20by%20Focal%20Loss%20used%20in%20vision%20tasks%2C%20FocalPO%20achieves%20this%0Aby%20adding%20a%20modulating%20factor%20to%20dynamically%20scale%20DPO%20loss.%20Our%20experiment%0Ademonstrates%20that%20FocalPO%20surpasses%20DPO%20and%20its%20variants%20on%20popular%20benchmarks%0Alike%20Alpaca%20Eval%202.0%20using%20Mistral-Base-7B%20and%20Llama-3-Instruct-8B%2C%20with%20the%0Aintroduced%20hyperparameter%20fixed.%20Additionally%2C%20we%20empirically%20reveals%20how%0AFocalPO%20affects%20training%20on%20correct%20and%20incorrect%20sample%20groups%2C%20further%0Aunderscoring%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06645v3&entry.124074799=Read"},
{"title": "Zero-Shot Learning with Subsequence Reordering Pretraining for\n  Compound-Protein Interaction", "author": "Hongzhi Zhang and Zhonglie Liu and Kun Meng and Jiameng Chen and Jia Wu and Bo Du and Di Lin and Yan Che and Wenbin Hu", "abstract": "  Given the vastness of chemical space and the ongoing emergence of previously\nuncharacterized proteins, zero-shot compound-protein interaction (CPI)\nprediction better reflects the practical challenges and requirements of\nreal-world drug development. Although existing methods perform adequately\nduring certain CPI tasks, they still face the following challenges: (1)\nRepresentation learning from local or complete protein sequences often\noverlooks the complex interdependencies between subsequences, which are\nessential for predicting spatial structures and binding properties. (2)\nDependence on large-scale or scarce multimodal protein datasets demands\nsignificant training data and computational resources, limiting scalability and\nefficiency. To address these challenges, we propose a novel approach that\npretrains protein representations for CPI prediction tasks using subsequence\nreordering, explicitly capturing the dependencies between protein subsequences.\nFurthermore, we apply length-variable protein augmentation to ensure excellent\npretraining performance on small training datasets. To evaluate the model's\neffectiveness and zero-shot learning ability, we combine it with various\nbaseline methods. The results demonstrate that our approach can improve the\nbaseline model's performance on the CPI task, especially in the challenging\nzero-shot scenario. Compared to existing pre-training models, our model\ndemonstrates superior performance, particularly in data-scarce scenarios where\ntraining samples are limited. Our implementation is available at\nhttps://github.com/Hoch-Zhang/PSRP-CPI.\n", "link": "http://arxiv.org/abs/2507.20925v1", "date": "2025-07-28", "relevancy": 2.3874, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4681}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Learning%20with%20Subsequence%20Reordering%20Pretraining%20for%0A%20%20Compound-Protein%20Interaction&body=Title%3A%20Zero-Shot%20Learning%20with%20Subsequence%20Reordering%20Pretraining%20for%0A%20%20Compound-Protein%20Interaction%0AAuthor%3A%20Hongzhi%20Zhang%20and%20Zhonglie%20Liu%20and%20Kun%20Meng%20and%20Jiameng%20Chen%20and%20Jia%20Wu%20and%20Bo%20Du%20and%20Di%20Lin%20and%20Yan%20Che%20and%20Wenbin%20Hu%0AAbstract%3A%20%20%20Given%20the%20vastness%20of%20chemical%20space%20and%20the%20ongoing%20emergence%20of%20previously%0Auncharacterized%20proteins%2C%20zero-shot%20compound-protein%20interaction%20%28CPI%29%0Aprediction%20better%20reflects%20the%20practical%20challenges%20and%20requirements%20of%0Areal-world%20drug%20development.%20Although%20existing%20methods%20perform%20adequately%0Aduring%20certain%20CPI%20tasks%2C%20they%20still%20face%20the%20following%20challenges%3A%20%281%29%0ARepresentation%20learning%20from%20local%20or%20complete%20protein%20sequences%20often%0Aoverlooks%20the%20complex%20interdependencies%20between%20subsequences%2C%20which%20are%0Aessential%20for%20predicting%20spatial%20structures%20and%20binding%20properties.%20%282%29%0ADependence%20on%20large-scale%20or%20scarce%20multimodal%20protein%20datasets%20demands%0Asignificant%20training%20data%20and%20computational%20resources%2C%20limiting%20scalability%20and%0Aefficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%0Apretrains%20protein%20representations%20for%20CPI%20prediction%20tasks%20using%20subsequence%0Areordering%2C%20explicitly%20capturing%20the%20dependencies%20between%20protein%20subsequences.%0AFurthermore%2C%20we%20apply%20length-variable%20protein%20augmentation%20to%20ensure%20excellent%0Apretraining%20performance%20on%20small%20training%20datasets.%20To%20evaluate%20the%20model%27s%0Aeffectiveness%20and%20zero-shot%20learning%20ability%2C%20we%20combine%20it%20with%20various%0Abaseline%20methods.%20The%20results%20demonstrate%20that%20our%20approach%20can%20improve%20the%0Abaseline%20model%27s%20performance%20on%20the%20CPI%20task%2C%20especially%20in%20the%20challenging%0Azero-shot%20scenario.%20Compared%20to%20existing%20pre-training%20models%2C%20our%20model%0Ademonstrates%20superior%20performance%2C%20particularly%20in%20data-scarce%20scenarios%20where%0Atraining%20samples%20are%20limited.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/Hoch-Zhang/PSRP-CPI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Learning%2520with%2520Subsequence%2520Reordering%2520Pretraining%2520for%250A%2520%2520Compound-Protein%2520Interaction%26entry.906535625%3DHongzhi%2520Zhang%2520and%2520Zhonglie%2520Liu%2520and%2520Kun%2520Meng%2520and%2520Jiameng%2520Chen%2520and%2520Jia%2520Wu%2520and%2520Bo%2520Du%2520and%2520Di%2520Lin%2520and%2520Yan%2520Che%2520and%2520Wenbin%2520Hu%26entry.1292438233%3D%2520%2520Given%2520the%2520vastness%2520of%2520chemical%2520space%2520and%2520the%2520ongoing%2520emergence%2520of%2520previously%250Auncharacterized%2520proteins%252C%2520zero-shot%2520compound-protein%2520interaction%2520%2528CPI%2529%250Aprediction%2520better%2520reflects%2520the%2520practical%2520challenges%2520and%2520requirements%2520of%250Areal-world%2520drug%2520development.%2520Although%2520existing%2520methods%2520perform%2520adequately%250Aduring%2520certain%2520CPI%2520tasks%252C%2520they%2520still%2520face%2520the%2520following%2520challenges%253A%2520%25281%2529%250ARepresentation%2520learning%2520from%2520local%2520or%2520complete%2520protein%2520sequences%2520often%250Aoverlooks%2520the%2520complex%2520interdependencies%2520between%2520subsequences%252C%2520which%2520are%250Aessential%2520for%2520predicting%2520spatial%2520structures%2520and%2520binding%2520properties.%2520%25282%2529%250ADependence%2520on%2520large-scale%2520or%2520scarce%2520multimodal%2520protein%2520datasets%2520demands%250Asignificant%2520training%2520data%2520and%2520computational%2520resources%252C%2520limiting%2520scalability%2520and%250Aefficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%250Apretrains%2520protein%2520representations%2520for%2520CPI%2520prediction%2520tasks%2520using%2520subsequence%250Areordering%252C%2520explicitly%2520capturing%2520the%2520dependencies%2520between%2520protein%2520subsequences.%250AFurthermore%252C%2520we%2520apply%2520length-variable%2520protein%2520augmentation%2520to%2520ensure%2520excellent%250Apretraining%2520performance%2520on%2520small%2520training%2520datasets.%2520To%2520evaluate%2520the%2520model%2527s%250Aeffectiveness%2520and%2520zero-shot%2520learning%2520ability%252C%2520we%2520combine%2520it%2520with%2520various%250Abaseline%2520methods.%2520The%2520results%2520demonstrate%2520that%2520our%2520approach%2520can%2520improve%2520the%250Abaseline%2520model%2527s%2520performance%2520on%2520the%2520CPI%2520task%252C%2520especially%2520in%2520the%2520challenging%250Azero-shot%2520scenario.%2520Compared%2520to%2520existing%2520pre-training%2520models%252C%2520our%2520model%250Ademonstrates%2520superior%2520performance%252C%2520particularly%2520in%2520data-scarce%2520scenarios%2520where%250Atraining%2520samples%2520are%2520limited.%2520Our%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/Hoch-Zhang/PSRP-CPI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Learning%20with%20Subsequence%20Reordering%20Pretraining%20for%0A%20%20Compound-Protein%20Interaction&entry.906535625=Hongzhi%20Zhang%20and%20Zhonglie%20Liu%20and%20Kun%20Meng%20and%20Jiameng%20Chen%20and%20Jia%20Wu%20and%20Bo%20Du%20and%20Di%20Lin%20and%20Yan%20Che%20and%20Wenbin%20Hu&entry.1292438233=%20%20Given%20the%20vastness%20of%20chemical%20space%20and%20the%20ongoing%20emergence%20of%20previously%0Auncharacterized%20proteins%2C%20zero-shot%20compound-protein%20interaction%20%28CPI%29%0Aprediction%20better%20reflects%20the%20practical%20challenges%20and%20requirements%20of%0Areal-world%20drug%20development.%20Although%20existing%20methods%20perform%20adequately%0Aduring%20certain%20CPI%20tasks%2C%20they%20still%20face%20the%20following%20challenges%3A%20%281%29%0ARepresentation%20learning%20from%20local%20or%20complete%20protein%20sequences%20often%0Aoverlooks%20the%20complex%20interdependencies%20between%20subsequences%2C%20which%20are%0Aessential%20for%20predicting%20spatial%20structures%20and%20binding%20properties.%20%282%29%0ADependence%20on%20large-scale%20or%20scarce%20multimodal%20protein%20datasets%20demands%0Asignificant%20training%20data%20and%20computational%20resources%2C%20limiting%20scalability%20and%0Aefficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20approach%20that%0Apretrains%20protein%20representations%20for%20CPI%20prediction%20tasks%20using%20subsequence%0Areordering%2C%20explicitly%20capturing%20the%20dependencies%20between%20protein%20subsequences.%0AFurthermore%2C%20we%20apply%20length-variable%20protein%20augmentation%20to%20ensure%20excellent%0Apretraining%20performance%20on%20small%20training%20datasets.%20To%20evaluate%20the%20model%27s%0Aeffectiveness%20and%20zero-shot%20learning%20ability%2C%20we%20combine%20it%20with%20various%0Abaseline%20methods.%20The%20results%20demonstrate%20that%20our%20approach%20can%20improve%20the%0Abaseline%20model%27s%20performance%20on%20the%20CPI%20task%2C%20especially%20in%20the%20challenging%0Azero-shot%20scenario.%20Compared%20to%20existing%20pre-training%20models%2C%20our%20model%0Ademonstrates%20superior%20performance%2C%20particularly%20in%20data-scarce%20scenarios%20where%0Atraining%20samples%20are%20limited.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/Hoch-Zhang/PSRP-CPI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20925v1&entry.124074799=Read"},
{"title": "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation", "author": "Yihong Cao and Jiaming Zhang and Xu Zheng and Hao Shi and Kunyu Peng and Hang Liu and Kailun Yang and Hui Zhang", "abstract": "  Panoramic image processing is essential for omni-context perception, yet\nfaces constraints like distortions, perspective occlusions, and limited\nannotations. Previous unsupervised domain adaptation methods transfer knowledge\nfrom labeled pinhole data to unlabeled panoramic images, but they require\naccess to source pinhole data. To address these, we introduce a more practical\ntask, i.e., Source-Free Occlusion-Aware Seamless Segmentation (SFOASS), and\npropose its first solution, called UNconstrained Learning Omni-Context\nKnowledge (UNLOCK). Specifically, UNLOCK includes two key modules: Omni\nPseudo-Labeling Learning and Amodal-Driven Context Learning. While adapting\nwithout relying on source data or target labels, this framework enhances models\nto achieve segmentation with 360{\\deg} viewpoint coverage and occlusion-aware\nreasoning. Furthermore, we benchmark the proposed SFOASS task through both\nreal-to-real and synthetic-to-real adaptation settings. Experimental results\nshow that our source-free method achieves performance comparable to\nsource-dependent methods, yielding state-of-the-art scores of 10.9 in mAAP and\n11.6 in mAP, along with an absolute improvement of +4.3 in mAPQ over the\nsource-only method. All data and code will be made publicly available at\nhttps://github.com/yihong-97/UNLOCK.\n", "link": "http://arxiv.org/abs/2506.21198v2", "date": "2025-07-28", "relevancy": 2.3698, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6189}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5818}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlocking%20Constraints%3A%20Source-Free%20Occlusion-Aware%20Seamless%20Segmentation&body=Title%3A%20Unlocking%20Constraints%3A%20Source-Free%20Occlusion-Aware%20Seamless%20Segmentation%0AAuthor%3A%20Yihong%20Cao%20and%20Jiaming%20Zhang%20and%20Xu%20Zheng%20and%20Hao%20Shi%20and%20Kunyu%20Peng%20and%20Hang%20Liu%20and%20Kailun%20Yang%20and%20Hui%20Zhang%0AAbstract%3A%20%20%20Panoramic%20image%20processing%20is%20essential%20for%20omni-context%20perception%2C%20yet%0Afaces%20constraints%20like%20distortions%2C%20perspective%20occlusions%2C%20and%20limited%0Aannotations.%20Previous%20unsupervised%20domain%20adaptation%20methods%20transfer%20knowledge%0Afrom%20labeled%20pinhole%20data%20to%20unlabeled%20panoramic%20images%2C%20but%20they%20require%0Aaccess%20to%20source%20pinhole%20data.%20To%20address%20these%2C%20we%20introduce%20a%20more%20practical%0Atask%2C%20i.e.%2C%20Source-Free%20Occlusion-Aware%20Seamless%20Segmentation%20%28SFOASS%29%2C%20and%0Apropose%20its%20first%20solution%2C%20called%20UNconstrained%20Learning%20Omni-Context%0AKnowledge%20%28UNLOCK%29.%20Specifically%2C%20UNLOCK%20includes%20two%20key%20modules%3A%20Omni%0APseudo-Labeling%20Learning%20and%20Amodal-Driven%20Context%20Learning.%20While%20adapting%0Awithout%20relying%20on%20source%20data%20or%20target%20labels%2C%20this%20framework%20enhances%20models%0Ato%20achieve%20segmentation%20with%20360%7B%5Cdeg%7D%20viewpoint%20coverage%20and%20occlusion-aware%0Areasoning.%20Furthermore%2C%20we%20benchmark%20the%20proposed%20SFOASS%20task%20through%20both%0Areal-to-real%20and%20synthetic-to-real%20adaptation%20settings.%20Experimental%20results%0Ashow%20that%20our%20source-free%20method%20achieves%20performance%20comparable%20to%0Asource-dependent%20methods%2C%20yielding%20state-of-the-art%20scores%20of%2010.9%20in%20mAAP%20and%0A11.6%20in%20mAP%2C%20along%20with%20an%20absolute%20improvement%20of%20%2B4.3%20in%20mAPQ%20over%20the%0Asource-only%20method.%20All%20data%20and%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/yihong-97/UNLOCK.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlocking%2520Constraints%253A%2520Source-Free%2520Occlusion-Aware%2520Seamless%2520Segmentation%26entry.906535625%3DYihong%2520Cao%2520and%2520Jiaming%2520Zhang%2520and%2520Xu%2520Zheng%2520and%2520Hao%2520Shi%2520and%2520Kunyu%2520Peng%2520and%2520Hang%2520Liu%2520and%2520Kailun%2520Yang%2520and%2520Hui%2520Zhang%26entry.1292438233%3D%2520%2520Panoramic%2520image%2520processing%2520is%2520essential%2520for%2520omni-context%2520perception%252C%2520yet%250Afaces%2520constraints%2520like%2520distortions%252C%2520perspective%2520occlusions%252C%2520and%2520limited%250Aannotations.%2520Previous%2520unsupervised%2520domain%2520adaptation%2520methods%2520transfer%2520knowledge%250Afrom%2520labeled%2520pinhole%2520data%2520to%2520unlabeled%2520panoramic%2520images%252C%2520but%2520they%2520require%250Aaccess%2520to%2520source%2520pinhole%2520data.%2520To%2520address%2520these%252C%2520we%2520introduce%2520a%2520more%2520practical%250Atask%252C%2520i.e.%252C%2520Source-Free%2520Occlusion-Aware%2520Seamless%2520Segmentation%2520%2528SFOASS%2529%252C%2520and%250Apropose%2520its%2520first%2520solution%252C%2520called%2520UNconstrained%2520Learning%2520Omni-Context%250AKnowledge%2520%2528UNLOCK%2529.%2520Specifically%252C%2520UNLOCK%2520includes%2520two%2520key%2520modules%253A%2520Omni%250APseudo-Labeling%2520Learning%2520and%2520Amodal-Driven%2520Context%2520Learning.%2520While%2520adapting%250Awithout%2520relying%2520on%2520source%2520data%2520or%2520target%2520labels%252C%2520this%2520framework%2520enhances%2520models%250Ato%2520achieve%2520segmentation%2520with%2520360%257B%255Cdeg%257D%2520viewpoint%2520coverage%2520and%2520occlusion-aware%250Areasoning.%2520Furthermore%252C%2520we%2520benchmark%2520the%2520proposed%2520SFOASS%2520task%2520through%2520both%250Areal-to-real%2520and%2520synthetic-to-real%2520adaptation%2520settings.%2520Experimental%2520results%250Ashow%2520that%2520our%2520source-free%2520method%2520achieves%2520performance%2520comparable%2520to%250Asource-dependent%2520methods%252C%2520yielding%2520state-of-the-art%2520scores%2520of%252010.9%2520in%2520mAAP%2520and%250A11.6%2520in%2520mAP%252C%2520along%2520with%2520an%2520absolute%2520improvement%2520of%2520%252B4.3%2520in%2520mAPQ%2520over%2520the%250Asource-only%2520method.%2520All%2520data%2520and%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/yihong-97/UNLOCK.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlocking%20Constraints%3A%20Source-Free%20Occlusion-Aware%20Seamless%20Segmentation&entry.906535625=Yihong%20Cao%20and%20Jiaming%20Zhang%20and%20Xu%20Zheng%20and%20Hao%20Shi%20and%20Kunyu%20Peng%20and%20Hang%20Liu%20and%20Kailun%20Yang%20and%20Hui%20Zhang&entry.1292438233=%20%20Panoramic%20image%20processing%20is%20essential%20for%20omni-context%20perception%2C%20yet%0Afaces%20constraints%20like%20distortions%2C%20perspective%20occlusions%2C%20and%20limited%0Aannotations.%20Previous%20unsupervised%20domain%20adaptation%20methods%20transfer%20knowledge%0Afrom%20labeled%20pinhole%20data%20to%20unlabeled%20panoramic%20images%2C%20but%20they%20require%0Aaccess%20to%20source%20pinhole%20data.%20To%20address%20these%2C%20we%20introduce%20a%20more%20practical%0Atask%2C%20i.e.%2C%20Source-Free%20Occlusion-Aware%20Seamless%20Segmentation%20%28SFOASS%29%2C%20and%0Apropose%20its%20first%20solution%2C%20called%20UNconstrained%20Learning%20Omni-Context%0AKnowledge%20%28UNLOCK%29.%20Specifically%2C%20UNLOCK%20includes%20two%20key%20modules%3A%20Omni%0APseudo-Labeling%20Learning%20and%20Amodal-Driven%20Context%20Learning.%20While%20adapting%0Awithout%20relying%20on%20source%20data%20or%20target%20labels%2C%20this%20framework%20enhances%20models%0Ato%20achieve%20segmentation%20with%20360%7B%5Cdeg%7D%20viewpoint%20coverage%20and%20occlusion-aware%0Areasoning.%20Furthermore%2C%20we%20benchmark%20the%20proposed%20SFOASS%20task%20through%20both%0Areal-to-real%20and%20synthetic-to-real%20adaptation%20settings.%20Experimental%20results%0Ashow%20that%20our%20source-free%20method%20achieves%20performance%20comparable%20to%0Asource-dependent%20methods%2C%20yielding%20state-of-the-art%20scores%20of%2010.9%20in%20mAAP%20and%0A11.6%20in%20mAP%2C%20along%20with%20an%20absolute%20improvement%20of%20%2B4.3%20in%20mAPQ%20over%20the%0Asource-only%20method.%20All%20data%20and%20code%20will%20be%20made%20publicly%20available%20at%0Ahttps%3A//github.com/yihong-97/UNLOCK.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21198v2&entry.124074799=Read"},
{"title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis", "author": "Hoyoung Lee and Junhyuk Seo and Suhwan Park and Junhyeong Lee and Wonbin Ahn and Chanyeol Choi and Alejandro Lopez-Lira and Yongjae Lee", "abstract": "  In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract models' latent preferences and measure\ntheir persistence. Focusing on sector, size, and momentum, our analysis reveals\ndistinct, model-specific tendencies. In particular, we observe a consistent\npreference for large-cap stocks and contrarian strategies across most models.\nThese preferences often harden into confirmation bias, with models clinging to\ninitial judgments despite counter-evidence.\n", "link": "http://arxiv.org/abs/2507.20957v1", "date": "2025-07-28", "relevancy": 2.3467, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis&body=Title%3A%20Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis%0AAuthor%3A%20Hoyoung%20Lee%20and%20Junhyuk%20Seo%20and%20Suhwan%20Park%20and%20Junhyeong%20Lee%20and%20Wonbin%20Ahn%20and%20Chanyeol%20Choi%20and%20Alejandro%20Lopez-Lira%20and%20Yongjae%20Lee%0AAbstract%3A%20%20%20In%20finance%2C%20Large%20Language%20Models%20%28LLMs%29%20face%20frequent%20knowledge%20conflicts%0Adue%20to%20discrepancies%20between%20pre-trained%20parametric%20knowledge%20and%20real-time%0Amarket%20data.%20These%20conflicts%20become%20particularly%20problematic%20when%20LLMs%20are%0Adeployed%20in%20real-world%20investment%20services%2C%20where%20misalignment%20between%20a%0Amodel%27s%20embedded%20preferences%20and%20those%20of%20the%20financial%20institution%20can%20lead%20to%0Aunreliable%20recommendations.%20Yet%20little%20research%20has%20examined%20what%20investment%0Aviews%20LLMs%20actually%20hold.%20We%20propose%20an%20experimental%20framework%20to%20investigate%0Asuch%20conflicts%2C%20offering%20the%20first%20quantitative%20analysis%20of%20confirmation%20bias%0Ain%20LLM-based%20investment%20analysis.%20Using%20hypothetical%20scenarios%20with%20balanced%0Aand%20imbalanced%20arguments%2C%20we%20extract%20models%27%20latent%20preferences%20and%20measure%0Atheir%20persistence.%20Focusing%20on%20sector%2C%20size%2C%20and%20momentum%2C%20our%20analysis%20reveals%0Adistinct%2C%20model-specific%20tendencies.%20In%20particular%2C%20we%20observe%20a%20consistent%0Apreference%20for%20large-cap%20stocks%20and%20contrarian%20strategies%20across%20most%20models.%0AThese%20preferences%20often%20harden%20into%20confirmation%20bias%2C%20with%20models%20clinging%20to%0Ainitial%20judgments%20despite%20counter-evidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYour%2520AI%252C%2520Not%2520Your%2520View%253A%2520The%2520Bias%2520of%2520LLMs%2520in%2520Investment%2520Analysis%26entry.906535625%3DHoyoung%2520Lee%2520and%2520Junhyuk%2520Seo%2520and%2520Suhwan%2520Park%2520and%2520Junhyeong%2520Lee%2520and%2520Wonbin%2520Ahn%2520and%2520Chanyeol%2520Choi%2520and%2520Alejandro%2520Lopez-Lira%2520and%2520Yongjae%2520Lee%26entry.1292438233%3D%2520%2520In%2520finance%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520frequent%2520knowledge%2520conflicts%250Adue%2520to%2520discrepancies%2520between%2520pre-trained%2520parametric%2520knowledge%2520and%2520real-time%250Amarket%2520data.%2520These%2520conflicts%2520become%2520particularly%2520problematic%2520when%2520LLMs%2520are%250Adeployed%2520in%2520real-world%2520investment%2520services%252C%2520where%2520misalignment%2520between%2520a%250Amodel%2527s%2520embedded%2520preferences%2520and%2520those%2520of%2520the%2520financial%2520institution%2520can%2520lead%2520to%250Aunreliable%2520recommendations.%2520Yet%2520little%2520research%2520has%2520examined%2520what%2520investment%250Aviews%2520LLMs%2520actually%2520hold.%2520We%2520propose%2520an%2520experimental%2520framework%2520to%2520investigate%250Asuch%2520conflicts%252C%2520offering%2520the%2520first%2520quantitative%2520analysis%2520of%2520confirmation%2520bias%250Ain%2520LLM-based%2520investment%2520analysis.%2520Using%2520hypothetical%2520scenarios%2520with%2520balanced%250Aand%2520imbalanced%2520arguments%252C%2520we%2520extract%2520models%2527%2520latent%2520preferences%2520and%2520measure%250Atheir%2520persistence.%2520Focusing%2520on%2520sector%252C%2520size%252C%2520and%2520momentum%252C%2520our%2520analysis%2520reveals%250Adistinct%252C%2520model-specific%2520tendencies.%2520In%2520particular%252C%2520we%2520observe%2520a%2520consistent%250Apreference%2520for%2520large-cap%2520stocks%2520and%2520contrarian%2520strategies%2520across%2520most%2520models.%250AThese%2520preferences%2520often%2520harden%2520into%2520confirmation%2520bias%252C%2520with%2520models%2520clinging%2520to%250Ainitial%2520judgments%2520despite%2520counter-evidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Your%20AI%2C%20Not%20Your%20View%3A%20The%20Bias%20of%20LLMs%20in%20Investment%20Analysis&entry.906535625=Hoyoung%20Lee%20and%20Junhyuk%20Seo%20and%20Suhwan%20Park%20and%20Junhyeong%20Lee%20and%20Wonbin%20Ahn%20and%20Chanyeol%20Choi%20and%20Alejandro%20Lopez-Lira%20and%20Yongjae%20Lee&entry.1292438233=%20%20In%20finance%2C%20Large%20Language%20Models%20%28LLMs%29%20face%20frequent%20knowledge%20conflicts%0Adue%20to%20discrepancies%20between%20pre-trained%20parametric%20knowledge%20and%20real-time%0Amarket%20data.%20These%20conflicts%20become%20particularly%20problematic%20when%20LLMs%20are%0Adeployed%20in%20real-world%20investment%20services%2C%20where%20misalignment%20between%20a%0Amodel%27s%20embedded%20preferences%20and%20those%20of%20the%20financial%20institution%20can%20lead%20to%0Aunreliable%20recommendations.%20Yet%20little%20research%20has%20examined%20what%20investment%0Aviews%20LLMs%20actually%20hold.%20We%20propose%20an%20experimental%20framework%20to%20investigate%0Asuch%20conflicts%2C%20offering%20the%20first%20quantitative%20analysis%20of%20confirmation%20bias%0Ain%20LLM-based%20investment%20analysis.%20Using%20hypothetical%20scenarios%20with%20balanced%0Aand%20imbalanced%20arguments%2C%20we%20extract%20models%27%20latent%20preferences%20and%20measure%0Atheir%20persistence.%20Focusing%20on%20sector%2C%20size%2C%20and%20momentum%2C%20our%20analysis%20reveals%0Adistinct%2C%20model-specific%20tendencies.%20In%20particular%2C%20we%20observe%20a%20consistent%0Apreference%20for%20large-cap%20stocks%20and%20contrarian%20strategies%20across%20most%20models.%0AThese%20preferences%20often%20harden%20into%20confirmation%20bias%2C%20with%20models%20clinging%20to%0Ainitial%20judgments%20despite%20counter-evidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20957v1&entry.124074799=Read"},
{"title": "HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings\n  Transformation for Face Forgery Detection", "author": "Jialei Cui and Jianwei Du and Yanzhe Li and Lei Gao and Hui Jiang and Chenfu Bao", "abstract": "  The rapid evolution of face manipulation techniques poses a critical\nchallenge for face forgery detection: cross-domain generalization. Conventional\nmethods, which rely on simple classification objectives, often fail to learn\ndomain-invariant representations. We propose HAMLET-FFD, a cognitively inspired\nHierarchical Adaptive Multi-modal Learning framework that tackles this\nchallenge via bidirectional cross-modal reasoning. Building on contrastive\nvision-language models such as CLIP, HAMLET-FFD introduces a knowledge\nrefinement loop that iteratively assesses authenticity by integrating visual\nevidence with conceptual cues, emulating expert forensic analysis. A key\ninnovation is a bidirectional fusion mechanism in which textual authenticity\nembeddings guide the aggregation of hierarchical visual features, while\nmodulated visual features refine text embeddings to generate image-adaptive\nprompts. This closed-loop process progressively aligns visual observations with\nsemantic priors to enhance authenticity assessment. By design, HAMLET-FFD\nfreezes all pretrained parameters, serving as an external plugin that preserves\nCLIP's original capabilities. Extensive experiments demonstrate its superior\ngeneralization to unseen manipulations across multiple benchmarks, and visual\nanalyses reveal a division of labor among embeddings, with distinct\nrepresentations specializing in fine-grained artifact recognition.\n", "link": "http://arxiv.org/abs/2507.20913v1", "date": "2025-07-28", "relevancy": 2.3298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6269}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5597}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAMLET-FFD%3A%20Hierarchical%20Adaptive%20Multi-modal%20Learning%20Embeddings%0A%20%20Transformation%20for%20Face%20Forgery%20Detection&body=Title%3A%20HAMLET-FFD%3A%20Hierarchical%20Adaptive%20Multi-modal%20Learning%20Embeddings%0A%20%20Transformation%20for%20Face%20Forgery%20Detection%0AAuthor%3A%20Jialei%20Cui%20and%20Jianwei%20Du%20and%20Yanzhe%20Li%20and%20Lei%20Gao%20and%20Hui%20Jiang%20and%20Chenfu%20Bao%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20face%20manipulation%20techniques%20poses%20a%20critical%0Achallenge%20for%20face%20forgery%20detection%3A%20cross-domain%20generalization.%20Conventional%0Amethods%2C%20which%20rely%20on%20simple%20classification%20objectives%2C%20often%20fail%20to%20learn%0Adomain-invariant%20representations.%20We%20propose%20HAMLET-FFD%2C%20a%20cognitively%20inspired%0AHierarchical%20Adaptive%20Multi-modal%20Learning%20framework%20that%20tackles%20this%0Achallenge%20via%20bidirectional%20cross-modal%20reasoning.%20Building%20on%20contrastive%0Avision-language%20models%20such%20as%20CLIP%2C%20HAMLET-FFD%20introduces%20a%20knowledge%0Arefinement%20loop%20that%20iteratively%20assesses%20authenticity%20by%20integrating%20visual%0Aevidence%20with%20conceptual%20cues%2C%20emulating%20expert%20forensic%20analysis.%20A%20key%0Ainnovation%20is%20a%20bidirectional%20fusion%20mechanism%20in%20which%20textual%20authenticity%0Aembeddings%20guide%20the%20aggregation%20of%20hierarchical%20visual%20features%2C%20while%0Amodulated%20visual%20features%20refine%20text%20embeddings%20to%20generate%20image-adaptive%0Aprompts.%20This%20closed-loop%20process%20progressively%20aligns%20visual%20observations%20with%0Asemantic%20priors%20to%20enhance%20authenticity%20assessment.%20By%20design%2C%20HAMLET-FFD%0Afreezes%20all%20pretrained%20parameters%2C%20serving%20as%20an%20external%20plugin%20that%20preserves%0ACLIP%27s%20original%20capabilities.%20Extensive%20experiments%20demonstrate%20its%20superior%0Ageneralization%20to%20unseen%20manipulations%20across%20multiple%20benchmarks%2C%20and%20visual%0Aanalyses%20reveal%20a%20division%20of%20labor%20among%20embeddings%2C%20with%20distinct%0Arepresentations%20specializing%20in%20fine-grained%20artifact%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAMLET-FFD%253A%2520Hierarchical%2520Adaptive%2520Multi-modal%2520Learning%2520Embeddings%250A%2520%2520Transformation%2520for%2520Face%2520Forgery%2520Detection%26entry.906535625%3DJialei%2520Cui%2520and%2520Jianwei%2520Du%2520and%2520Yanzhe%2520Li%2520and%2520Lei%2520Gao%2520and%2520Hui%2520Jiang%2520and%2520Chenfu%2520Bao%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520face%2520manipulation%2520techniques%2520poses%2520a%2520critical%250Achallenge%2520for%2520face%2520forgery%2520detection%253A%2520cross-domain%2520generalization.%2520Conventional%250Amethods%252C%2520which%2520rely%2520on%2520simple%2520classification%2520objectives%252C%2520often%2520fail%2520to%2520learn%250Adomain-invariant%2520representations.%2520We%2520propose%2520HAMLET-FFD%252C%2520a%2520cognitively%2520inspired%250AHierarchical%2520Adaptive%2520Multi-modal%2520Learning%2520framework%2520that%2520tackles%2520this%250Achallenge%2520via%2520bidirectional%2520cross-modal%2520reasoning.%2520Building%2520on%2520contrastive%250Avision-language%2520models%2520such%2520as%2520CLIP%252C%2520HAMLET-FFD%2520introduces%2520a%2520knowledge%250Arefinement%2520loop%2520that%2520iteratively%2520assesses%2520authenticity%2520by%2520integrating%2520visual%250Aevidence%2520with%2520conceptual%2520cues%252C%2520emulating%2520expert%2520forensic%2520analysis.%2520A%2520key%250Ainnovation%2520is%2520a%2520bidirectional%2520fusion%2520mechanism%2520in%2520which%2520textual%2520authenticity%250Aembeddings%2520guide%2520the%2520aggregation%2520of%2520hierarchical%2520visual%2520features%252C%2520while%250Amodulated%2520visual%2520features%2520refine%2520text%2520embeddings%2520to%2520generate%2520image-adaptive%250Aprompts.%2520This%2520closed-loop%2520process%2520progressively%2520aligns%2520visual%2520observations%2520with%250Asemantic%2520priors%2520to%2520enhance%2520authenticity%2520assessment.%2520By%2520design%252C%2520HAMLET-FFD%250Afreezes%2520all%2520pretrained%2520parameters%252C%2520serving%2520as%2520an%2520external%2520plugin%2520that%2520preserves%250ACLIP%2527s%2520original%2520capabilities.%2520Extensive%2520experiments%2520demonstrate%2520its%2520superior%250Ageneralization%2520to%2520unseen%2520manipulations%2520across%2520multiple%2520benchmarks%252C%2520and%2520visual%250Aanalyses%2520reveal%2520a%2520division%2520of%2520labor%2520among%2520embeddings%252C%2520with%2520distinct%250Arepresentations%2520specializing%2520in%2520fine-grained%2520artifact%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAMLET-FFD%3A%20Hierarchical%20Adaptive%20Multi-modal%20Learning%20Embeddings%0A%20%20Transformation%20for%20Face%20Forgery%20Detection&entry.906535625=Jialei%20Cui%20and%20Jianwei%20Du%20and%20Yanzhe%20Li%20and%20Lei%20Gao%20and%20Hui%20Jiang%20and%20Chenfu%20Bao&entry.1292438233=%20%20The%20rapid%20evolution%20of%20face%20manipulation%20techniques%20poses%20a%20critical%0Achallenge%20for%20face%20forgery%20detection%3A%20cross-domain%20generalization.%20Conventional%0Amethods%2C%20which%20rely%20on%20simple%20classification%20objectives%2C%20often%20fail%20to%20learn%0Adomain-invariant%20representations.%20We%20propose%20HAMLET-FFD%2C%20a%20cognitively%20inspired%0AHierarchical%20Adaptive%20Multi-modal%20Learning%20framework%20that%20tackles%20this%0Achallenge%20via%20bidirectional%20cross-modal%20reasoning.%20Building%20on%20contrastive%0Avision-language%20models%20such%20as%20CLIP%2C%20HAMLET-FFD%20introduces%20a%20knowledge%0Arefinement%20loop%20that%20iteratively%20assesses%20authenticity%20by%20integrating%20visual%0Aevidence%20with%20conceptual%20cues%2C%20emulating%20expert%20forensic%20analysis.%20A%20key%0Ainnovation%20is%20a%20bidirectional%20fusion%20mechanism%20in%20which%20textual%20authenticity%0Aembeddings%20guide%20the%20aggregation%20of%20hierarchical%20visual%20features%2C%20while%0Amodulated%20visual%20features%20refine%20text%20embeddings%20to%20generate%20image-adaptive%0Aprompts.%20This%20closed-loop%20process%20progressively%20aligns%20visual%20observations%20with%0Asemantic%20priors%20to%20enhance%20authenticity%20assessment.%20By%20design%2C%20HAMLET-FFD%0Afreezes%20all%20pretrained%20parameters%2C%20serving%20as%20an%20external%20plugin%20that%20preserves%0ACLIP%27s%20original%20capabilities.%20Extensive%20experiments%20demonstrate%20its%20superior%0Ageneralization%20to%20unseen%20manipulations%20across%20multiple%20benchmarks%2C%20and%20visual%0Aanalyses%20reveal%20a%20division%20of%20labor%20among%20embeddings%2C%20with%20distinct%0Arepresentations%20specializing%20in%20fine-grained%20artifact%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20913v1&entry.124074799=Read"},
{"title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World\n  Shorts", "author": "Yuying Ge and Yixiao Ge and Chen Li and Teng Wang and Junfu Pu and Yizhuo Li and Lu Qiu and Jin Ma and Lisheng Duan and Xinyu Zuo and Jinwen Luo and Weibo Gu and Zexuan Li and Xiaojing Zhang and Yangyu Tao and Han Hu and Di Wang and Ying Shan", "abstract": "  Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.\n", "link": "http://arxiv.org/abs/2507.20939v1", "date": "2025-07-28", "relevancy": 2.3289, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.589}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARC-Hunyuan-Video-7B%3A%20Structured%20Video%20Comprehension%20of%20Real-World%0A%20%20Shorts&body=Title%3A%20ARC-Hunyuan-Video-7B%3A%20Structured%20Video%20Comprehension%20of%20Real-World%0A%20%20Shorts%0AAuthor%3A%20Yuying%20Ge%20and%20Yixiao%20Ge%20and%20Chen%20Li%20and%20Teng%20Wang%20and%20Junfu%20Pu%20and%20Yizhuo%20Li%20and%20Lu%20Qiu%20and%20Jin%20Ma%20and%20Lisheng%20Duan%20and%20Xinyu%20Zuo%20and%20Jinwen%20Luo%20and%20Weibo%20Gu%20and%20Zexuan%20Li%20and%20Xiaojing%20Zhang%20and%20Yangyu%20Tao%20and%20Han%20Hu%20and%20Di%20Wang%20and%20Ying%20Shan%0AAbstract%3A%20%20%20Real-world%20user-generated%20short%20videos%2C%20especially%20those%20distributed%20on%0Aplatforms%20such%20as%20WeChat%20Channel%20and%20TikTok%2C%20dominate%20the%20mobile%20internet.%0AHowever%2C%20current%20large%20multimodal%20models%20lack%20essential%20temporally-structured%2C%0Adetailed%2C%20and%20in-depth%20video%20comprehension%20capabilities%2C%20which%20are%20the%0Acornerstone%20of%20effective%20video%20search%20and%20recommendation%2C%20as%20well%20as%20emerging%0Avideo%20applications.%20Understanding%20real-world%20shorts%20is%20actually%20challenging%20due%0Ato%20their%20complex%20visual%20elements%2C%20high%20information%20density%20in%20both%20visuals%20and%0Aaudio%2C%20and%20fast%20pacing%20that%20focuses%20on%20emotional%20expression%20and%20viewpoint%0Adelivery.%20This%20requires%20advanced%20reasoning%20to%20effectively%20integrate%20multimodal%0Ainformation%2C%20including%20visual%2C%20audio%2C%20and%20text.%20In%20this%20work%2C%20we%20introduce%0AARC-Hunyuan-Video%2C%20a%20multimodal%20model%20that%20processes%20visual%2C%20audio%2C%20and%20textual%0Asignals%20from%20raw%20video%20inputs%20end-to-end%20for%20structured%20comprehension.%20The%0Amodel%20is%20capable%20of%20multi-granularity%20timestamped%20video%20captioning%20and%0Asummarization%2C%20open-ended%20video%20question%20answering%2C%20temporal%20video%20grounding%2C%0Aand%20video%20reasoning.%20Leveraging%20high-quality%20data%20from%20an%20automated%20annotation%0Apipeline%2C%20our%20compact%207B-parameter%20model%20is%20trained%20through%20a%20comprehensive%0Aregimen%3A%20pre-training%2C%20instruction%20fine-tuning%2C%20cold%20start%2C%20reinforcement%0Alearning%20%28RL%29%20post-training%2C%20and%20final%20instruction%20fine-tuning.%20Quantitative%0Aevaluations%20on%20our%20introduced%20benchmark%20ShortVid-Bench%20and%20qualitative%0Acomparisons%20demonstrate%20its%20strong%20performance%20in%20real-world%20video%0Acomprehension%2C%20and%20it%20supports%20zero-shot%20or%20fine-tuning%20with%20a%20few%20samples%20for%0Adiverse%20downstream%20applications.%20The%20real-world%20production%20deployment%20of%20our%0Amodel%20has%20yielded%20tangible%20and%20measurable%20improvements%20in%20user%20engagement%20and%0Asatisfaction%2C%20a%20success%20supported%20by%20its%20remarkable%20efficiency%2C%20with%20stress%0Atests%20indicating%20an%20inference%20time%20of%20just%2010%20seconds%20for%20a%20one-minute%20video%20on%0AH20%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARC-Hunyuan-Video-7B%253A%2520Structured%2520Video%2520Comprehension%2520of%2520Real-World%250A%2520%2520Shorts%26entry.906535625%3DYuying%2520Ge%2520and%2520Yixiao%2520Ge%2520and%2520Chen%2520Li%2520and%2520Teng%2520Wang%2520and%2520Junfu%2520Pu%2520and%2520Yizhuo%2520Li%2520and%2520Lu%2520Qiu%2520and%2520Jin%2520Ma%2520and%2520Lisheng%2520Duan%2520and%2520Xinyu%2520Zuo%2520and%2520Jinwen%2520Luo%2520and%2520Weibo%2520Gu%2520and%2520Zexuan%2520Li%2520and%2520Xiaojing%2520Zhang%2520and%2520Yangyu%2520Tao%2520and%2520Han%2520Hu%2520and%2520Di%2520Wang%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520Real-world%2520user-generated%2520short%2520videos%252C%2520especially%2520those%2520distributed%2520on%250Aplatforms%2520such%2520as%2520WeChat%2520Channel%2520and%2520TikTok%252C%2520dominate%2520the%2520mobile%2520internet.%250AHowever%252C%2520current%2520large%2520multimodal%2520models%2520lack%2520essential%2520temporally-structured%252C%250Adetailed%252C%2520and%2520in-depth%2520video%2520comprehension%2520capabilities%252C%2520which%2520are%2520the%250Acornerstone%2520of%2520effective%2520video%2520search%2520and%2520recommendation%252C%2520as%2520well%2520as%2520emerging%250Avideo%2520applications.%2520Understanding%2520real-world%2520shorts%2520is%2520actually%2520challenging%2520due%250Ato%2520their%2520complex%2520visual%2520elements%252C%2520high%2520information%2520density%2520in%2520both%2520visuals%2520and%250Aaudio%252C%2520and%2520fast%2520pacing%2520that%2520focuses%2520on%2520emotional%2520expression%2520and%2520viewpoint%250Adelivery.%2520This%2520requires%2520advanced%2520reasoning%2520to%2520effectively%2520integrate%2520multimodal%250Ainformation%252C%2520including%2520visual%252C%2520audio%252C%2520and%2520text.%2520In%2520this%2520work%252C%2520we%2520introduce%250AARC-Hunyuan-Video%252C%2520a%2520multimodal%2520model%2520that%2520processes%2520visual%252C%2520audio%252C%2520and%2520textual%250Asignals%2520from%2520raw%2520video%2520inputs%2520end-to-end%2520for%2520structured%2520comprehension.%2520The%250Amodel%2520is%2520capable%2520of%2520multi-granularity%2520timestamped%2520video%2520captioning%2520and%250Asummarization%252C%2520open-ended%2520video%2520question%2520answering%252C%2520temporal%2520video%2520grounding%252C%250Aand%2520video%2520reasoning.%2520Leveraging%2520high-quality%2520data%2520from%2520an%2520automated%2520annotation%250Apipeline%252C%2520our%2520compact%25207B-parameter%2520model%2520is%2520trained%2520through%2520a%2520comprehensive%250Aregimen%253A%2520pre-training%252C%2520instruction%2520fine-tuning%252C%2520cold%2520start%252C%2520reinforcement%250Alearning%2520%2528RL%2529%2520post-training%252C%2520and%2520final%2520instruction%2520fine-tuning.%2520Quantitative%250Aevaluations%2520on%2520our%2520introduced%2520benchmark%2520ShortVid-Bench%2520and%2520qualitative%250Acomparisons%2520demonstrate%2520its%2520strong%2520performance%2520in%2520real-world%2520video%250Acomprehension%252C%2520and%2520it%2520supports%2520zero-shot%2520or%2520fine-tuning%2520with%2520a%2520few%2520samples%2520for%250Adiverse%2520downstream%2520applications.%2520The%2520real-world%2520production%2520deployment%2520of%2520our%250Amodel%2520has%2520yielded%2520tangible%2520and%2520measurable%2520improvements%2520in%2520user%2520engagement%2520and%250Asatisfaction%252C%2520a%2520success%2520supported%2520by%2520its%2520remarkable%2520efficiency%252C%2520with%2520stress%250Atests%2520indicating%2520an%2520inference%2520time%2520of%2520just%252010%2520seconds%2520for%2520a%2520one-minute%2520video%2520on%250AH20%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARC-Hunyuan-Video-7B%3A%20Structured%20Video%20Comprehension%20of%20Real-World%0A%20%20Shorts&entry.906535625=Yuying%20Ge%20and%20Yixiao%20Ge%20and%20Chen%20Li%20and%20Teng%20Wang%20and%20Junfu%20Pu%20and%20Yizhuo%20Li%20and%20Lu%20Qiu%20and%20Jin%20Ma%20and%20Lisheng%20Duan%20and%20Xinyu%20Zuo%20and%20Jinwen%20Luo%20and%20Weibo%20Gu%20and%20Zexuan%20Li%20and%20Xiaojing%20Zhang%20and%20Yangyu%20Tao%20and%20Han%20Hu%20and%20Di%20Wang%20and%20Ying%20Shan&entry.1292438233=%20%20Real-world%20user-generated%20short%20videos%2C%20especially%20those%20distributed%20on%0Aplatforms%20such%20as%20WeChat%20Channel%20and%20TikTok%2C%20dominate%20the%20mobile%20internet.%0AHowever%2C%20current%20large%20multimodal%20models%20lack%20essential%20temporally-structured%2C%0Adetailed%2C%20and%20in-depth%20video%20comprehension%20capabilities%2C%20which%20are%20the%0Acornerstone%20of%20effective%20video%20search%20and%20recommendation%2C%20as%20well%20as%20emerging%0Avideo%20applications.%20Understanding%20real-world%20shorts%20is%20actually%20challenging%20due%0Ato%20their%20complex%20visual%20elements%2C%20high%20information%20density%20in%20both%20visuals%20and%0Aaudio%2C%20and%20fast%20pacing%20that%20focuses%20on%20emotional%20expression%20and%20viewpoint%0Adelivery.%20This%20requires%20advanced%20reasoning%20to%20effectively%20integrate%20multimodal%0Ainformation%2C%20including%20visual%2C%20audio%2C%20and%20text.%20In%20this%20work%2C%20we%20introduce%0AARC-Hunyuan-Video%2C%20a%20multimodal%20model%20that%20processes%20visual%2C%20audio%2C%20and%20textual%0Asignals%20from%20raw%20video%20inputs%20end-to-end%20for%20structured%20comprehension.%20The%0Amodel%20is%20capable%20of%20multi-granularity%20timestamped%20video%20captioning%20and%0Asummarization%2C%20open-ended%20video%20question%20answering%2C%20temporal%20video%20grounding%2C%0Aand%20video%20reasoning.%20Leveraging%20high-quality%20data%20from%20an%20automated%20annotation%0Apipeline%2C%20our%20compact%207B-parameter%20model%20is%20trained%20through%20a%20comprehensive%0Aregimen%3A%20pre-training%2C%20instruction%20fine-tuning%2C%20cold%20start%2C%20reinforcement%0Alearning%20%28RL%29%20post-training%2C%20and%20final%20instruction%20fine-tuning.%20Quantitative%0Aevaluations%20on%20our%20introduced%20benchmark%20ShortVid-Bench%20and%20qualitative%0Acomparisons%20demonstrate%20its%20strong%20performance%20in%20real-world%20video%0Acomprehension%2C%20and%20it%20supports%20zero-shot%20or%20fine-tuning%20with%20a%20few%20samples%20for%0Adiverse%20downstream%20applications.%20The%20real-world%20production%20deployment%20of%20our%0Amodel%20has%20yielded%20tangible%20and%20measurable%20improvements%20in%20user%20engagement%20and%0Asatisfaction%2C%20a%20success%20supported%20by%20its%20remarkable%20efficiency%2C%20with%20stress%0Atests%20indicating%20an%20inference%20time%20of%20just%2010%20seconds%20for%20a%20one-minute%20video%20on%0AH20%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20939v1&entry.124074799=Read"},
{"title": "Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the\n  Inductive Setting", "author": "Alexey Kravets and Da Chen and Vinay P. Namboodiri", "abstract": "  CLIP is a foundational model with transferable classification performance in\nthe few-shot setting. Several methods have shown improved performance of CLIP\nusing few-shot examples. However, so far, all these techniques have been\nbenchmarked using standard few-shot datasets. We argue that this mode of\nevaluation does not provide a true indication of the inductive generalization\nability using few-shot examples. As most datasets have been seen by the CLIP\nmodel, the resultant setting can be termed as partially transductive. To solve\nthis, we propose a pipeline that uses an unlearning technique to obtain true\ninductive baselines. In this new inductive setting, the methods show a\nsignificant drop in performance (-55% on average among 13 baselines with\nmultiple datasets). We validate the unlearning technique using oracle\nbaselines. An improved few-shot classification technique is proposed that\nconsistently obtains state-of-the-art performance over 13 other recent baseline\nmethods on a comprehensive analysis with 5880 experiments - varying the\ndatasets, differing number of few-shot examples, unlearning setting, and with\ndifferent seeds. Thus, we identify the issue with the evaluation of CLIP-based\nfew-shot classification, provide a solution using unlearning, propose new\nbenchmarks, and provide an improved method.\n", "link": "http://arxiv.org/abs/2507.20834v1", "date": "2025-07-28", "relevancy": 2.3214, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4727}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Few%20Shot%20CLIP%20Benchmarks%3A%20A%20Critical%20Analysis%20in%20the%0A%20%20Inductive%20Setting&body=Title%3A%20Rethinking%20Few%20Shot%20CLIP%20Benchmarks%3A%20A%20Critical%20Analysis%20in%20the%0A%20%20Inductive%20Setting%0AAuthor%3A%20Alexey%20Kravets%20and%20Da%20Chen%20and%20Vinay%20P.%20Namboodiri%0AAbstract%3A%20%20%20CLIP%20is%20a%20foundational%20model%20with%20transferable%20classification%20performance%20in%0Athe%20few-shot%20setting.%20Several%20methods%20have%20shown%20improved%20performance%20of%20CLIP%0Ausing%20few-shot%20examples.%20However%2C%20so%20far%2C%20all%20these%20techniques%20have%20been%0Abenchmarked%20using%20standard%20few-shot%20datasets.%20We%20argue%20that%20this%20mode%20of%0Aevaluation%20does%20not%20provide%20a%20true%20indication%20of%20the%20inductive%20generalization%0Aability%20using%20few-shot%20examples.%20As%20most%20datasets%20have%20been%20seen%20by%20the%20CLIP%0Amodel%2C%20the%20resultant%20setting%20can%20be%20termed%20as%20partially%20transductive.%20To%20solve%0Athis%2C%20we%20propose%20a%20pipeline%20that%20uses%20an%20unlearning%20technique%20to%20obtain%20true%0Ainductive%20baselines.%20In%20this%20new%20inductive%20setting%2C%20the%20methods%20show%20a%0Asignificant%20drop%20in%20performance%20%28-55%25%20on%20average%20among%2013%20baselines%20with%0Amultiple%20datasets%29.%20We%20validate%20the%20unlearning%20technique%20using%20oracle%0Abaselines.%20An%20improved%20few-shot%20classification%20technique%20is%20proposed%20that%0Aconsistently%20obtains%20state-of-the-art%20performance%20over%2013%20other%20recent%20baseline%0Amethods%20on%20a%20comprehensive%20analysis%20with%205880%20experiments%20-%20varying%20the%0Adatasets%2C%20differing%20number%20of%20few-shot%20examples%2C%20unlearning%20setting%2C%20and%20with%0Adifferent%20seeds.%20Thus%2C%20we%20identify%20the%20issue%20with%20the%20evaluation%20of%20CLIP-based%0Afew-shot%20classification%2C%20provide%20a%20solution%20using%20unlearning%2C%20propose%20new%0Abenchmarks%2C%20and%20provide%20an%20improved%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Few%2520Shot%2520CLIP%2520Benchmarks%253A%2520A%2520Critical%2520Analysis%2520in%2520the%250A%2520%2520Inductive%2520Setting%26entry.906535625%3DAlexey%2520Kravets%2520and%2520Da%2520Chen%2520and%2520Vinay%2520P.%2520Namboodiri%26entry.1292438233%3D%2520%2520CLIP%2520is%2520a%2520foundational%2520model%2520with%2520transferable%2520classification%2520performance%2520in%250Athe%2520few-shot%2520setting.%2520Several%2520methods%2520have%2520shown%2520improved%2520performance%2520of%2520CLIP%250Ausing%2520few-shot%2520examples.%2520However%252C%2520so%2520far%252C%2520all%2520these%2520techniques%2520have%2520been%250Abenchmarked%2520using%2520standard%2520few-shot%2520datasets.%2520We%2520argue%2520that%2520this%2520mode%2520of%250Aevaluation%2520does%2520not%2520provide%2520a%2520true%2520indication%2520of%2520the%2520inductive%2520generalization%250Aability%2520using%2520few-shot%2520examples.%2520As%2520most%2520datasets%2520have%2520been%2520seen%2520by%2520the%2520CLIP%250Amodel%252C%2520the%2520resultant%2520setting%2520can%2520be%2520termed%2520as%2520partially%2520transductive.%2520To%2520solve%250Athis%252C%2520we%2520propose%2520a%2520pipeline%2520that%2520uses%2520an%2520unlearning%2520technique%2520to%2520obtain%2520true%250Ainductive%2520baselines.%2520In%2520this%2520new%2520inductive%2520setting%252C%2520the%2520methods%2520show%2520a%250Asignificant%2520drop%2520in%2520performance%2520%2528-55%2525%2520on%2520average%2520among%252013%2520baselines%2520with%250Amultiple%2520datasets%2529.%2520We%2520validate%2520the%2520unlearning%2520technique%2520using%2520oracle%250Abaselines.%2520An%2520improved%2520few-shot%2520classification%2520technique%2520is%2520proposed%2520that%250Aconsistently%2520obtains%2520state-of-the-art%2520performance%2520over%252013%2520other%2520recent%2520baseline%250Amethods%2520on%2520a%2520comprehensive%2520analysis%2520with%25205880%2520experiments%2520-%2520varying%2520the%250Adatasets%252C%2520differing%2520number%2520of%2520few-shot%2520examples%252C%2520unlearning%2520setting%252C%2520and%2520with%250Adifferent%2520seeds.%2520Thus%252C%2520we%2520identify%2520the%2520issue%2520with%2520the%2520evaluation%2520of%2520CLIP-based%250Afew-shot%2520classification%252C%2520provide%2520a%2520solution%2520using%2520unlearning%252C%2520propose%2520new%250Abenchmarks%252C%2520and%2520provide%2520an%2520improved%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Few%20Shot%20CLIP%20Benchmarks%3A%20A%20Critical%20Analysis%20in%20the%0A%20%20Inductive%20Setting&entry.906535625=Alexey%20Kravets%20and%20Da%20Chen%20and%20Vinay%20P.%20Namboodiri&entry.1292438233=%20%20CLIP%20is%20a%20foundational%20model%20with%20transferable%20classification%20performance%20in%0Athe%20few-shot%20setting.%20Several%20methods%20have%20shown%20improved%20performance%20of%20CLIP%0Ausing%20few-shot%20examples.%20However%2C%20so%20far%2C%20all%20these%20techniques%20have%20been%0Abenchmarked%20using%20standard%20few-shot%20datasets.%20We%20argue%20that%20this%20mode%20of%0Aevaluation%20does%20not%20provide%20a%20true%20indication%20of%20the%20inductive%20generalization%0Aability%20using%20few-shot%20examples.%20As%20most%20datasets%20have%20been%20seen%20by%20the%20CLIP%0Amodel%2C%20the%20resultant%20setting%20can%20be%20termed%20as%20partially%20transductive.%20To%20solve%0Athis%2C%20we%20propose%20a%20pipeline%20that%20uses%20an%20unlearning%20technique%20to%20obtain%20true%0Ainductive%20baselines.%20In%20this%20new%20inductive%20setting%2C%20the%20methods%20show%20a%0Asignificant%20drop%20in%20performance%20%28-55%25%20on%20average%20among%2013%20baselines%20with%0Amultiple%20datasets%29.%20We%20validate%20the%20unlearning%20technique%20using%20oracle%0Abaselines.%20An%20improved%20few-shot%20classification%20technique%20is%20proposed%20that%0Aconsistently%20obtains%20state-of-the-art%20performance%20over%2013%20other%20recent%20baseline%0Amethods%20on%20a%20comprehensive%20analysis%20with%205880%20experiments%20-%20varying%20the%0Adatasets%2C%20differing%20number%20of%20few-shot%20examples%2C%20unlearning%20setting%2C%20and%20with%0Adifferent%20seeds.%20Thus%2C%20we%20identify%20the%20issue%20with%20the%20evaluation%20of%20CLIP-based%0Afew-shot%20classification%2C%20provide%20a%20solution%20using%20unlearning%2C%20propose%20new%0Abenchmarks%2C%20and%20provide%20an%20improved%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20834v1&entry.124074799=Read"},
{"title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in\n  Pedestrian-Rich Environments", "author": "Meiting Dang and Yanping Wu and Yafei Wang and Dezong Zhao and David Flynn and Chongfeng Wei", "abstract": "  Recent advances in autonomous vehicle (AV) behavior planning have shown\nimpressive social interaction capabilities when interacting with other road\nusers. However, achieving human-like prediction and decision-making in\ninteractions with vulnerable road users remains a key challenge in complex\nmulti-agent interactive environments. Existing research focuses primarily on\ncrowd navigation for small mobile robots, which cannot be directly applied to\nAVs due to inherent differences in their decision-making strategies and dynamic\nboundaries. Moreover, pedestrians in these multi-agent simulations follow fixed\nbehavior patterns that cannot dynamically respond to AV actions. To overcome\nthese limitations, this paper proposes a novel framework for modeling\ninteractions between the AV and multiple pedestrians. In this framework, a\ncognitive process modeling approach inspired by the Free Energy Principle is\nintegrated into both the AV and pedestrian models to simulate more realistic\ninteraction dynamics. Specifically, the proposed pedestrian Cognitive-Risk\nSocial Force Model adjusts goal-directed and repulsive forces using a fused\nmeasure of cognitive uncertainty and physical risk to produce human-like\ntrajectories. Meanwhile, the AV leverages this fused risk to construct a\ndynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a\nSoft Actor-Critic architecture, allowing it to make more reasonable and\ninformed decisions. Simulation results indicate that our proposed framework\neffectively improves safety, efficiency, and smoothness of AV navigation\ncompared to the state-of-the-art method.\n", "link": "http://arxiv.org/abs/2507.20850v1", "date": "2025-07-28", "relevancy": 2.3172, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6093}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5925}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20Energy-Inspired%20Cognitive%20Risk%20Integration%20for%20AV%20Navigation%20in%0A%20%20Pedestrian-Rich%20Environments&body=Title%3A%20Free%20Energy-Inspired%20Cognitive%20Risk%20Integration%20for%20AV%20Navigation%20in%0A%20%20Pedestrian-Rich%20Environments%0AAuthor%3A%20Meiting%20Dang%20and%20Yanping%20Wu%20and%20Yafei%20Wang%20and%20Dezong%20Zhao%20and%20David%20Flynn%20and%20Chongfeng%20Wei%0AAbstract%3A%20%20%20Recent%20advances%20in%20autonomous%20vehicle%20%28AV%29%20behavior%20planning%20have%20shown%0Aimpressive%20social%20interaction%20capabilities%20when%20interacting%20with%20other%20road%0Ausers.%20However%2C%20achieving%20human-like%20prediction%20and%20decision-making%20in%0Ainteractions%20with%20vulnerable%20road%20users%20remains%20a%20key%20challenge%20in%20complex%0Amulti-agent%20interactive%20environments.%20Existing%20research%20focuses%20primarily%20on%0Acrowd%20navigation%20for%20small%20mobile%20robots%2C%20which%20cannot%20be%20directly%20applied%20to%0AAVs%20due%20to%20inherent%20differences%20in%20their%20decision-making%20strategies%20and%20dynamic%0Aboundaries.%20Moreover%2C%20pedestrians%20in%20these%20multi-agent%20simulations%20follow%20fixed%0Abehavior%20patterns%20that%20cannot%20dynamically%20respond%20to%20AV%20actions.%20To%20overcome%0Athese%20limitations%2C%20this%20paper%20proposes%20a%20novel%20framework%20for%20modeling%0Ainteractions%20between%20the%20AV%20and%20multiple%20pedestrians.%20In%20this%20framework%2C%20a%0Acognitive%20process%20modeling%20approach%20inspired%20by%20the%20Free%20Energy%20Principle%20is%0Aintegrated%20into%20both%20the%20AV%20and%20pedestrian%20models%20to%20simulate%20more%20realistic%0Ainteraction%20dynamics.%20Specifically%2C%20the%20proposed%20pedestrian%20Cognitive-Risk%0ASocial%20Force%20Model%20adjusts%20goal-directed%20and%20repulsive%20forces%20using%20a%20fused%0Ameasure%20of%20cognitive%20uncertainty%20and%20physical%20risk%20to%20produce%20human-like%0Atrajectories.%20Meanwhile%2C%20the%20AV%20leverages%20this%20fused%20risk%20to%20construct%20a%0Adynamic%2C%20risk-aware%20adjacency%20matrix%20for%20a%20Graph%20Convolutional%20Network%20within%20a%0ASoft%20Actor-Critic%20architecture%2C%20allowing%20it%20to%20make%20more%20reasonable%20and%0Ainformed%20decisions.%20Simulation%20results%20indicate%20that%20our%20proposed%20framework%0Aeffectively%20improves%20safety%2C%20efficiency%2C%20and%20smoothness%20of%20AV%20navigation%0Acompared%20to%20the%20state-of-the-art%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520Energy-Inspired%2520Cognitive%2520Risk%2520Integration%2520for%2520AV%2520Navigation%2520in%250A%2520%2520Pedestrian-Rich%2520Environments%26entry.906535625%3DMeiting%2520Dang%2520and%2520Yanping%2520Wu%2520and%2520Yafei%2520Wang%2520and%2520Dezong%2520Zhao%2520and%2520David%2520Flynn%2520and%2520Chongfeng%2520Wei%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520autonomous%2520vehicle%2520%2528AV%2529%2520behavior%2520planning%2520have%2520shown%250Aimpressive%2520social%2520interaction%2520capabilities%2520when%2520interacting%2520with%2520other%2520road%250Ausers.%2520However%252C%2520achieving%2520human-like%2520prediction%2520and%2520decision-making%2520in%250Ainteractions%2520with%2520vulnerable%2520road%2520users%2520remains%2520a%2520key%2520challenge%2520in%2520complex%250Amulti-agent%2520interactive%2520environments.%2520Existing%2520research%2520focuses%2520primarily%2520on%250Acrowd%2520navigation%2520for%2520small%2520mobile%2520robots%252C%2520which%2520cannot%2520be%2520directly%2520applied%2520to%250AAVs%2520due%2520to%2520inherent%2520differences%2520in%2520their%2520decision-making%2520strategies%2520and%2520dynamic%250Aboundaries.%2520Moreover%252C%2520pedestrians%2520in%2520these%2520multi-agent%2520simulations%2520follow%2520fixed%250Abehavior%2520patterns%2520that%2520cannot%2520dynamically%2520respond%2520to%2520AV%2520actions.%2520To%2520overcome%250Athese%2520limitations%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520framework%2520for%2520modeling%250Ainteractions%2520between%2520the%2520AV%2520and%2520multiple%2520pedestrians.%2520In%2520this%2520framework%252C%2520a%250Acognitive%2520process%2520modeling%2520approach%2520inspired%2520by%2520the%2520Free%2520Energy%2520Principle%2520is%250Aintegrated%2520into%2520both%2520the%2520AV%2520and%2520pedestrian%2520models%2520to%2520simulate%2520more%2520realistic%250Ainteraction%2520dynamics.%2520Specifically%252C%2520the%2520proposed%2520pedestrian%2520Cognitive-Risk%250ASocial%2520Force%2520Model%2520adjusts%2520goal-directed%2520and%2520repulsive%2520forces%2520using%2520a%2520fused%250Ameasure%2520of%2520cognitive%2520uncertainty%2520and%2520physical%2520risk%2520to%2520produce%2520human-like%250Atrajectories.%2520Meanwhile%252C%2520the%2520AV%2520leverages%2520this%2520fused%2520risk%2520to%2520construct%2520a%250Adynamic%252C%2520risk-aware%2520adjacency%2520matrix%2520for%2520a%2520Graph%2520Convolutional%2520Network%2520within%2520a%250ASoft%2520Actor-Critic%2520architecture%252C%2520allowing%2520it%2520to%2520make%2520more%2520reasonable%2520and%250Ainformed%2520decisions.%2520Simulation%2520results%2520indicate%2520that%2520our%2520proposed%2520framework%250Aeffectively%2520improves%2520safety%252C%2520efficiency%252C%2520and%2520smoothness%2520of%2520AV%2520navigation%250Acompared%2520to%2520the%2520state-of-the-art%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20Energy-Inspired%20Cognitive%20Risk%20Integration%20for%20AV%20Navigation%20in%0A%20%20Pedestrian-Rich%20Environments&entry.906535625=Meiting%20Dang%20and%20Yanping%20Wu%20and%20Yafei%20Wang%20and%20Dezong%20Zhao%20and%20David%20Flynn%20and%20Chongfeng%20Wei&entry.1292438233=%20%20Recent%20advances%20in%20autonomous%20vehicle%20%28AV%29%20behavior%20planning%20have%20shown%0Aimpressive%20social%20interaction%20capabilities%20when%20interacting%20with%20other%20road%0Ausers.%20However%2C%20achieving%20human-like%20prediction%20and%20decision-making%20in%0Ainteractions%20with%20vulnerable%20road%20users%20remains%20a%20key%20challenge%20in%20complex%0Amulti-agent%20interactive%20environments.%20Existing%20research%20focuses%20primarily%20on%0Acrowd%20navigation%20for%20small%20mobile%20robots%2C%20which%20cannot%20be%20directly%20applied%20to%0AAVs%20due%20to%20inherent%20differences%20in%20their%20decision-making%20strategies%20and%20dynamic%0Aboundaries.%20Moreover%2C%20pedestrians%20in%20these%20multi-agent%20simulations%20follow%20fixed%0Abehavior%20patterns%20that%20cannot%20dynamically%20respond%20to%20AV%20actions.%20To%20overcome%0Athese%20limitations%2C%20this%20paper%20proposes%20a%20novel%20framework%20for%20modeling%0Ainteractions%20between%20the%20AV%20and%20multiple%20pedestrians.%20In%20this%20framework%2C%20a%0Acognitive%20process%20modeling%20approach%20inspired%20by%20the%20Free%20Energy%20Principle%20is%0Aintegrated%20into%20both%20the%20AV%20and%20pedestrian%20models%20to%20simulate%20more%20realistic%0Ainteraction%20dynamics.%20Specifically%2C%20the%20proposed%20pedestrian%20Cognitive-Risk%0ASocial%20Force%20Model%20adjusts%20goal-directed%20and%20repulsive%20forces%20using%20a%20fused%0Ameasure%20of%20cognitive%20uncertainty%20and%20physical%20risk%20to%20produce%20human-like%0Atrajectories.%20Meanwhile%2C%20the%20AV%20leverages%20this%20fused%20risk%20to%20construct%20a%0Adynamic%2C%20risk-aware%20adjacency%20matrix%20for%20a%20Graph%20Convolutional%20Network%20within%20a%0ASoft%20Actor-Critic%20architecture%2C%20allowing%20it%20to%20make%20more%20reasonable%20and%0Ainformed%20decisions.%20Simulation%20results%20indicate%20that%20our%20proposed%20framework%0Aeffectively%20improves%20safety%2C%20efficiency%2C%20and%20smoothness%20of%20AV%20navigation%0Acompared%20to%20the%20state-of-the-art%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20850v1&entry.124074799=Read"},
{"title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for\n  Multi-Platform and Multi-Modal Reasoning", "author": "Huiyang Hu and Peijin Wang and Yingchao Feng and Kaiwen Wei and Wenxin Yin and Wenhui Diao and Mengyu Wang and Hanbo Bi and Kaiyue Kang and Tong Ling and Kun Fu and Xian Sun", "abstract": "  Remote sensing (RS) images from multiple modalities and platforms exhibit\ndiverse details due to differences in sensor characteristics and imaging\nperspectives. Existing vision-language research in RS largely relies on\nrelatively homogeneous data sources. Moreover, they still remain limited to\nconventional visual perception tasks such as classification or captioning. As a\nresult, these methods fail to serve as a unified and standalone framework\ncapable of effectively handling RS imagery from diverse sources in real-world\napplications. To address these issues, we propose RingMo-Agent, a model\ndesigned to handle multi-modal and multi-platform data that performs perception\nand reasoning tasks based on user textual instructions. Compared with existing\nmodels, RingMo-Agent 1) is supported by a large-scale vision-language dataset\nnamed RS-VL3M, comprising over 3 million image-text pairs, spanning optical,\nSAR, and infrared (IR) modalities collected from both satellite and UAV\nplatforms, covering perception and challenging reasoning tasks; 2) learns\nmodality adaptive representations by incorporating separated embedding layers\nto construct isolated features for heterogeneous modalities and reduce\ncross-modal interference; 3) unifies task modeling by introducing task-specific\ntokens and employing a token-based high-dimensional hidden state decoding\nmechanism designed for long-horizon spatial tasks. Extensive experiments on\nvarious RS vision-language tasks demonstrate that RingMo-Agent not only proves\neffective in both visual understanding and sophisticated analytical tasks, but\nalso exhibits strong generalizability across different platforms and sensing\nmodalities.\n", "link": "http://arxiv.org/abs/2507.20776v1", "date": "2025-07-28", "relevancy": 2.3058, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6125}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RingMo-Agent%3A%20A%20Unified%20Remote%20Sensing%20Foundation%20Model%20for%0A%20%20Multi-Platform%20and%20Multi-Modal%20Reasoning&body=Title%3A%20RingMo-Agent%3A%20A%20Unified%20Remote%20Sensing%20Foundation%20Model%20for%0A%20%20Multi-Platform%20and%20Multi-Modal%20Reasoning%0AAuthor%3A%20Huiyang%20Hu%20and%20Peijin%20Wang%20and%20Yingchao%20Feng%20and%20Kaiwen%20Wei%20and%20Wenxin%20Yin%20and%20Wenhui%20Diao%20and%20Mengyu%20Wang%20and%20Hanbo%20Bi%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Kun%20Fu%20and%20Xian%20Sun%0AAbstract%3A%20%20%20Remote%20sensing%20%28RS%29%20images%20from%20multiple%20modalities%20and%20platforms%20exhibit%0Adiverse%20details%20due%20to%20differences%20in%20sensor%20characteristics%20and%20imaging%0Aperspectives.%20Existing%20vision-language%20research%20in%20RS%20largely%20relies%20on%0Arelatively%20homogeneous%20data%20sources.%20Moreover%2C%20they%20still%20remain%20limited%20to%0Aconventional%20visual%20perception%20tasks%20such%20as%20classification%20or%20captioning.%20As%20a%0Aresult%2C%20these%20methods%20fail%20to%20serve%20as%20a%20unified%20and%20standalone%20framework%0Acapable%20of%20effectively%20handling%20RS%20imagery%20from%20diverse%20sources%20in%20real-world%0Aapplications.%20To%20address%20these%20issues%2C%20we%20propose%20RingMo-Agent%2C%20a%20model%0Adesigned%20to%20handle%20multi-modal%20and%20multi-platform%20data%20that%20performs%20perception%0Aand%20reasoning%20tasks%20based%20on%20user%20textual%20instructions.%20Compared%20with%20existing%0Amodels%2C%20RingMo-Agent%201%29%20is%20supported%20by%20a%20large-scale%20vision-language%20dataset%0Anamed%20RS-VL3M%2C%20comprising%20over%203%20million%20image-text%20pairs%2C%20spanning%20optical%2C%0ASAR%2C%20and%20infrared%20%28IR%29%20modalities%20collected%20from%20both%20satellite%20and%20UAV%0Aplatforms%2C%20covering%20perception%20and%20challenging%20reasoning%20tasks%3B%202%29%20learns%0Amodality%20adaptive%20representations%20by%20incorporating%20separated%20embedding%20layers%0Ato%20construct%20isolated%20features%20for%20heterogeneous%20modalities%20and%20reduce%0Across-modal%20interference%3B%203%29%20unifies%20task%20modeling%20by%20introducing%20task-specific%0Atokens%20and%20employing%20a%20token-based%20high-dimensional%20hidden%20state%20decoding%0Amechanism%20designed%20for%20long-horizon%20spatial%20tasks.%20Extensive%20experiments%20on%0Avarious%20RS%20vision-language%20tasks%20demonstrate%20that%20RingMo-Agent%20not%20only%20proves%0Aeffective%20in%20both%20visual%20understanding%20and%20sophisticated%20analytical%20tasks%2C%20but%0Aalso%20exhibits%20strong%20generalizability%20across%20different%20platforms%20and%20sensing%0Amodalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRingMo-Agent%253A%2520A%2520Unified%2520Remote%2520Sensing%2520Foundation%2520Model%2520for%250A%2520%2520Multi-Platform%2520and%2520Multi-Modal%2520Reasoning%26entry.906535625%3DHuiyang%2520Hu%2520and%2520Peijin%2520Wang%2520and%2520Yingchao%2520Feng%2520and%2520Kaiwen%2520Wei%2520and%2520Wenxin%2520Yin%2520and%2520Wenhui%2520Diao%2520and%2520Mengyu%2520Wang%2520and%2520Hanbo%2520Bi%2520and%2520Kaiyue%2520Kang%2520and%2520Tong%2520Ling%2520and%2520Kun%2520Fu%2520and%2520Xian%2520Sun%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520%2528RS%2529%2520images%2520from%2520multiple%2520modalities%2520and%2520platforms%2520exhibit%250Adiverse%2520details%2520due%2520to%2520differences%2520in%2520sensor%2520characteristics%2520and%2520imaging%250Aperspectives.%2520Existing%2520vision-language%2520research%2520in%2520RS%2520largely%2520relies%2520on%250Arelatively%2520homogeneous%2520data%2520sources.%2520Moreover%252C%2520they%2520still%2520remain%2520limited%2520to%250Aconventional%2520visual%2520perception%2520tasks%2520such%2520as%2520classification%2520or%2520captioning.%2520As%2520a%250Aresult%252C%2520these%2520methods%2520fail%2520to%2520serve%2520as%2520a%2520unified%2520and%2520standalone%2520framework%250Acapable%2520of%2520effectively%2520handling%2520RS%2520imagery%2520from%2520diverse%2520sources%2520in%2520real-world%250Aapplications.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520RingMo-Agent%252C%2520a%2520model%250Adesigned%2520to%2520handle%2520multi-modal%2520and%2520multi-platform%2520data%2520that%2520performs%2520perception%250Aand%2520reasoning%2520tasks%2520based%2520on%2520user%2520textual%2520instructions.%2520Compared%2520with%2520existing%250Amodels%252C%2520RingMo-Agent%25201%2529%2520is%2520supported%2520by%2520a%2520large-scale%2520vision-language%2520dataset%250Anamed%2520RS-VL3M%252C%2520comprising%2520over%25203%2520million%2520image-text%2520pairs%252C%2520spanning%2520optical%252C%250ASAR%252C%2520and%2520infrared%2520%2528IR%2529%2520modalities%2520collected%2520from%2520both%2520satellite%2520and%2520UAV%250Aplatforms%252C%2520covering%2520perception%2520and%2520challenging%2520reasoning%2520tasks%253B%25202%2529%2520learns%250Amodality%2520adaptive%2520representations%2520by%2520incorporating%2520separated%2520embedding%2520layers%250Ato%2520construct%2520isolated%2520features%2520for%2520heterogeneous%2520modalities%2520and%2520reduce%250Across-modal%2520interference%253B%25203%2529%2520unifies%2520task%2520modeling%2520by%2520introducing%2520task-specific%250Atokens%2520and%2520employing%2520a%2520token-based%2520high-dimensional%2520hidden%2520state%2520decoding%250Amechanism%2520designed%2520for%2520long-horizon%2520spatial%2520tasks.%2520Extensive%2520experiments%2520on%250Avarious%2520RS%2520vision-language%2520tasks%2520demonstrate%2520that%2520RingMo-Agent%2520not%2520only%2520proves%250Aeffective%2520in%2520both%2520visual%2520understanding%2520and%2520sophisticated%2520analytical%2520tasks%252C%2520but%250Aalso%2520exhibits%2520strong%2520generalizability%2520across%2520different%2520platforms%2520and%2520sensing%250Amodalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingMo-Agent%3A%20A%20Unified%20Remote%20Sensing%20Foundation%20Model%20for%0A%20%20Multi-Platform%20and%20Multi-Modal%20Reasoning&entry.906535625=Huiyang%20Hu%20and%20Peijin%20Wang%20and%20Yingchao%20Feng%20and%20Kaiwen%20Wei%20and%20Wenxin%20Yin%20and%20Wenhui%20Diao%20and%20Mengyu%20Wang%20and%20Hanbo%20Bi%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Kun%20Fu%20and%20Xian%20Sun&entry.1292438233=%20%20Remote%20sensing%20%28RS%29%20images%20from%20multiple%20modalities%20and%20platforms%20exhibit%0Adiverse%20details%20due%20to%20differences%20in%20sensor%20characteristics%20and%20imaging%0Aperspectives.%20Existing%20vision-language%20research%20in%20RS%20largely%20relies%20on%0Arelatively%20homogeneous%20data%20sources.%20Moreover%2C%20they%20still%20remain%20limited%20to%0Aconventional%20visual%20perception%20tasks%20such%20as%20classification%20or%20captioning.%20As%20a%0Aresult%2C%20these%20methods%20fail%20to%20serve%20as%20a%20unified%20and%20standalone%20framework%0Acapable%20of%20effectively%20handling%20RS%20imagery%20from%20diverse%20sources%20in%20real-world%0Aapplications.%20To%20address%20these%20issues%2C%20we%20propose%20RingMo-Agent%2C%20a%20model%0Adesigned%20to%20handle%20multi-modal%20and%20multi-platform%20data%20that%20performs%20perception%0Aand%20reasoning%20tasks%20based%20on%20user%20textual%20instructions.%20Compared%20with%20existing%0Amodels%2C%20RingMo-Agent%201%29%20is%20supported%20by%20a%20large-scale%20vision-language%20dataset%0Anamed%20RS-VL3M%2C%20comprising%20over%203%20million%20image-text%20pairs%2C%20spanning%20optical%2C%0ASAR%2C%20and%20infrared%20%28IR%29%20modalities%20collected%20from%20both%20satellite%20and%20UAV%0Aplatforms%2C%20covering%20perception%20and%20challenging%20reasoning%20tasks%3B%202%29%20learns%0Amodality%20adaptive%20representations%20by%20incorporating%20separated%20embedding%20layers%0Ato%20construct%20isolated%20features%20for%20heterogeneous%20modalities%20and%20reduce%0Across-modal%20interference%3B%203%29%20unifies%20task%20modeling%20by%20introducing%20task-specific%0Atokens%20and%20employing%20a%20token-based%20high-dimensional%20hidden%20state%20decoding%0Amechanism%20designed%20for%20long-horizon%20spatial%20tasks.%20Extensive%20experiments%20on%0Avarious%20RS%20vision-language%20tasks%20demonstrate%20that%20RingMo-Agent%20not%20only%20proves%0Aeffective%20in%20both%20visual%20understanding%20and%20sophisticated%20analytical%20tasks%2C%20but%0Aalso%20exhibits%20strong%20generalizability%20across%20different%20platforms%20and%20sensing%0Amodalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20776v1&entry.124074799=Read"},
{"title": "DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid\n  Thinking and Active Perception", "author": "Weicheng Zheng and Xiaofei Mao and Nanfei Ye and Pengxiang Li and Kun Zhan and Xianpeng Lang and Hang Zhao", "abstract": "  Vision-Language Models (VLMs) are advancing autonomous driving, yet their\npotential is constrained by myopic decision-making and passive perception,\nlimiting reliability in complex environments. We introduce DriveAgent-R1 to\ntackle these challenges in long-horizon, high-level behavioral decision-making.\nDriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that\nadaptively switches between efficient text-based and in-depth tool-based\nreasoning, and an Active Perception mechanism with a vision toolkit to\nproactively resolve uncertainties, thereby balancing decision-making efficiency\nand reliability. The agent is trained using a novel, three-stage progressive\nreinforcement learning strategy designed to master these hybrid capabilities.\nExtensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art\nperformance, outperforming even leading proprietary large multimodal models,\nsuch as Claude Sonnet 4. Ablation studies validate our approach and confirm\nthat the agent's decisions are robustly grounded in actively perceived visual\nevidence, paving a path toward safer and more intelligent autonomous systems.\n", "link": "http://arxiv.org/abs/2507.20879v1", "date": "2025-07-28", "relevancy": 2.2999, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveAgent-R1%3A%20Advancing%20VLM-based%20Autonomous%20Driving%20with%20Hybrid%0A%20%20Thinking%20and%20Active%20Perception&body=Title%3A%20DriveAgent-R1%3A%20Advancing%20VLM-based%20Autonomous%20Driving%20with%20Hybrid%0A%20%20Thinking%20and%20Active%20Perception%0AAuthor%3A%20Weicheng%20Zheng%20and%20Xiaofei%20Mao%20and%20Nanfei%20Ye%20and%20Pengxiang%20Li%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20are%20advancing%20autonomous%20driving%2C%20yet%20their%0Apotential%20is%20constrained%20by%20myopic%20decision-making%20and%20passive%20perception%2C%0Alimiting%20reliability%20in%20complex%20environments.%20We%20introduce%20DriveAgent-R1%20to%0Atackle%20these%20challenges%20in%20long-horizon%2C%20high-level%20behavioral%20decision-making.%0ADriveAgent-R1%20features%20two%20core%20innovations%3A%20a%20Hybrid-Thinking%20framework%20that%0Aadaptively%20switches%20between%20efficient%20text-based%20and%20in-depth%20tool-based%0Areasoning%2C%20and%20an%20Active%20Perception%20mechanism%20with%20a%20vision%20toolkit%20to%0Aproactively%20resolve%20uncertainties%2C%20thereby%20balancing%20decision-making%20efficiency%0Aand%20reliability.%20The%20agent%20is%20trained%20using%20a%20novel%2C%20three-stage%20progressive%0Areinforcement%20learning%20strategy%20designed%20to%20master%20these%20hybrid%20capabilities.%0AExtensive%20experiments%20demonstrate%20that%20DriveAgent-R1%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20even%20leading%20proprietary%20large%20multimodal%20models%2C%0Asuch%20as%20Claude%20Sonnet%204.%20Ablation%20studies%20validate%20our%20approach%20and%20confirm%0Athat%20the%20agent%27s%20decisions%20are%20robustly%20grounded%20in%20actively%20perceived%20visual%0Aevidence%2C%20paving%20a%20path%20toward%20safer%20and%20more%20intelligent%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveAgent-R1%253A%2520Advancing%2520VLM-based%2520Autonomous%2520Driving%2520with%2520Hybrid%250A%2520%2520Thinking%2520and%2520Active%2520Perception%26entry.906535625%3DWeicheng%2520Zheng%2520and%2520Xiaofei%2520Mao%2520and%2520Nanfei%2520Ye%2520and%2520Pengxiang%2520Li%2520and%2520Kun%2520Zhan%2520and%2520Xianpeng%2520Lang%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520advancing%2520autonomous%2520driving%252C%2520yet%2520their%250Apotential%2520is%2520constrained%2520by%2520myopic%2520decision-making%2520and%2520passive%2520perception%252C%250Alimiting%2520reliability%2520in%2520complex%2520environments.%2520We%2520introduce%2520DriveAgent-R1%2520to%250Atackle%2520these%2520challenges%2520in%2520long-horizon%252C%2520high-level%2520behavioral%2520decision-making.%250ADriveAgent-R1%2520features%2520two%2520core%2520innovations%253A%2520a%2520Hybrid-Thinking%2520framework%2520that%250Aadaptively%2520switches%2520between%2520efficient%2520text-based%2520and%2520in-depth%2520tool-based%250Areasoning%252C%2520and%2520an%2520Active%2520Perception%2520mechanism%2520with%2520a%2520vision%2520toolkit%2520to%250Aproactively%2520resolve%2520uncertainties%252C%2520thereby%2520balancing%2520decision-making%2520efficiency%250Aand%2520reliability.%2520The%2520agent%2520is%2520trained%2520using%2520a%2520novel%252C%2520three-stage%2520progressive%250Areinforcement%2520learning%2520strategy%2520designed%2520to%2520master%2520these%2520hybrid%2520capabilities.%250AExtensive%2520experiments%2520demonstrate%2520that%2520DriveAgent-R1%2520achieves%2520state-of-the-art%250Aperformance%252C%2520outperforming%2520even%2520leading%2520proprietary%2520large%2520multimodal%2520models%252C%250Asuch%2520as%2520Claude%2520Sonnet%25204.%2520Ablation%2520studies%2520validate%2520our%2520approach%2520and%2520confirm%250Athat%2520the%2520agent%2527s%2520decisions%2520are%2520robustly%2520grounded%2520in%2520actively%2520perceived%2520visual%250Aevidence%252C%2520paving%2520a%2520path%2520toward%2520safer%2520and%2520more%2520intelligent%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveAgent-R1%3A%20Advancing%20VLM-based%20Autonomous%20Driving%20with%20Hybrid%0A%20%20Thinking%20and%20Active%20Perception&entry.906535625=Weicheng%20Zheng%20and%20Xiaofei%20Mao%20and%20Nanfei%20Ye%20and%20Pengxiang%20Li%20and%20Kun%20Zhan%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20are%20advancing%20autonomous%20driving%2C%20yet%20their%0Apotential%20is%20constrained%20by%20myopic%20decision-making%20and%20passive%20perception%2C%0Alimiting%20reliability%20in%20complex%20environments.%20We%20introduce%20DriveAgent-R1%20to%0Atackle%20these%20challenges%20in%20long-horizon%2C%20high-level%20behavioral%20decision-making.%0ADriveAgent-R1%20features%20two%20core%20innovations%3A%20a%20Hybrid-Thinking%20framework%20that%0Aadaptively%20switches%20between%20efficient%20text-based%20and%20in-depth%20tool-based%0Areasoning%2C%20and%20an%20Active%20Perception%20mechanism%20with%20a%20vision%20toolkit%20to%0Aproactively%20resolve%20uncertainties%2C%20thereby%20balancing%20decision-making%20efficiency%0Aand%20reliability.%20The%20agent%20is%20trained%20using%20a%20novel%2C%20three-stage%20progressive%0Areinforcement%20learning%20strategy%20designed%20to%20master%20these%20hybrid%20capabilities.%0AExtensive%20experiments%20demonstrate%20that%20DriveAgent-R1%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20even%20leading%20proprietary%20large%20multimodal%20models%2C%0Asuch%20as%20Claude%20Sonnet%204.%20Ablation%20studies%20validate%20our%20approach%20and%20confirm%0Athat%20the%20agent%27s%20decisions%20are%20robustly%20grounded%20in%20actively%20perceived%20visual%0Aevidence%2C%20paving%20a%20path%20toward%20safer%20and%20more%20intelligent%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20879v1&entry.124074799=Read"},
{"title": "A Multimodal Architecture for Endpoint Position Prediction in Team-based\n  Multiplayer Games", "author": "Jonas Peche and Aliaksei Tsishurou and Alexander Zap and Guenter Wallner", "abstract": "  Understanding and predicting player movement in multiplayer games is crucial\nfor achieving use cases such as player-mimicking bot navigation, preemptive bot\ncontrol, strategy recommendation, and real-time player behavior analytics.\nHowever, the complex environments allow for a high degree of navigational\nfreedom, and the interactions and team-play between players require models that\nmake effective use of the available heterogeneous input data. This paper\npresents a multimodal architecture for predicting future player locations on a\ndynamic time horizon, using a U-Net-based approach for calculating endpoint\nlocation probability heatmaps, conditioned using a multimodal feature encoder.\nThe application of a multi-head attention mechanism for different groups of\nfeatures allows for communication between agents. In doing so, the architecture\nmakes efficient use of the multimodal game state including image inputs,\nnumerical and categorical features, as well as dynamic game data. Consequently,\nthe presented technique lays the foundation for various downstream tasks that\nrely on future player positions such as the creation of player-predictive bot\nbehavior or player anomaly detection.\n", "link": "http://arxiv.org/abs/2507.20670v1", "date": "2025-07-28", "relevancy": 2.2971, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6573}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5649}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Architecture%20for%20Endpoint%20Position%20Prediction%20in%20Team-based%0A%20%20Multiplayer%20Games&body=Title%3A%20A%20Multimodal%20Architecture%20for%20Endpoint%20Position%20Prediction%20in%20Team-based%0A%20%20Multiplayer%20Games%0AAuthor%3A%20Jonas%20Peche%20and%20Aliaksei%20Tsishurou%20and%20Alexander%20Zap%20and%20Guenter%20Wallner%0AAbstract%3A%20%20%20Understanding%20and%20predicting%20player%20movement%20in%20multiplayer%20games%20is%20crucial%0Afor%20achieving%20use%20cases%20such%20as%20player-mimicking%20bot%20navigation%2C%20preemptive%20bot%0Acontrol%2C%20strategy%20recommendation%2C%20and%20real-time%20player%20behavior%20analytics.%0AHowever%2C%20the%20complex%20environments%20allow%20for%20a%20high%20degree%20of%20navigational%0Afreedom%2C%20and%20the%20interactions%20and%20team-play%20between%20players%20require%20models%20that%0Amake%20effective%20use%20of%20the%20available%20heterogeneous%20input%20data.%20This%20paper%0Apresents%20a%20multimodal%20architecture%20for%20predicting%20future%20player%20locations%20on%20a%0Adynamic%20time%20horizon%2C%20using%20a%20U-Net-based%20approach%20for%20calculating%20endpoint%0Alocation%20probability%20heatmaps%2C%20conditioned%20using%20a%20multimodal%20feature%20encoder.%0AThe%20application%20of%20a%20multi-head%20attention%20mechanism%20for%20different%20groups%20of%0Afeatures%20allows%20for%20communication%20between%20agents.%20In%20doing%20so%2C%20the%20architecture%0Amakes%20efficient%20use%20of%20the%20multimodal%20game%20state%20including%20image%20inputs%2C%0Anumerical%20and%20categorical%20features%2C%20as%20well%20as%20dynamic%20game%20data.%20Consequently%2C%0Athe%20presented%20technique%20lays%20the%20foundation%20for%20various%20downstream%20tasks%20that%0Arely%20on%20future%20player%20positions%20such%20as%20the%20creation%20of%20player-predictive%20bot%0Abehavior%20or%20player%20anomaly%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20670v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Architecture%2520for%2520Endpoint%2520Position%2520Prediction%2520in%2520Team-based%250A%2520%2520Multiplayer%2520Games%26entry.906535625%3DJonas%2520Peche%2520and%2520Aliaksei%2520Tsishurou%2520and%2520Alexander%2520Zap%2520and%2520Guenter%2520Wallner%26entry.1292438233%3D%2520%2520Understanding%2520and%2520predicting%2520player%2520movement%2520in%2520multiplayer%2520games%2520is%2520crucial%250Afor%2520achieving%2520use%2520cases%2520such%2520as%2520player-mimicking%2520bot%2520navigation%252C%2520preemptive%2520bot%250Acontrol%252C%2520strategy%2520recommendation%252C%2520and%2520real-time%2520player%2520behavior%2520analytics.%250AHowever%252C%2520the%2520complex%2520environments%2520allow%2520for%2520a%2520high%2520degree%2520of%2520navigational%250Afreedom%252C%2520and%2520the%2520interactions%2520and%2520team-play%2520between%2520players%2520require%2520models%2520that%250Amake%2520effective%2520use%2520of%2520the%2520available%2520heterogeneous%2520input%2520data.%2520This%2520paper%250Apresents%2520a%2520multimodal%2520architecture%2520for%2520predicting%2520future%2520player%2520locations%2520on%2520a%250Adynamic%2520time%2520horizon%252C%2520using%2520a%2520U-Net-based%2520approach%2520for%2520calculating%2520endpoint%250Alocation%2520probability%2520heatmaps%252C%2520conditioned%2520using%2520a%2520multimodal%2520feature%2520encoder.%250AThe%2520application%2520of%2520a%2520multi-head%2520attention%2520mechanism%2520for%2520different%2520groups%2520of%250Afeatures%2520allows%2520for%2520communication%2520between%2520agents.%2520In%2520doing%2520so%252C%2520the%2520architecture%250Amakes%2520efficient%2520use%2520of%2520the%2520multimodal%2520game%2520state%2520including%2520image%2520inputs%252C%250Anumerical%2520and%2520categorical%2520features%252C%2520as%2520well%2520as%2520dynamic%2520game%2520data.%2520Consequently%252C%250Athe%2520presented%2520technique%2520lays%2520the%2520foundation%2520for%2520various%2520downstream%2520tasks%2520that%250Arely%2520on%2520future%2520player%2520positions%2520such%2520as%2520the%2520creation%2520of%2520player-predictive%2520bot%250Abehavior%2520or%2520player%2520anomaly%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20670v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Architecture%20for%20Endpoint%20Position%20Prediction%20in%20Team-based%0A%20%20Multiplayer%20Games&entry.906535625=Jonas%20Peche%20and%20Aliaksei%20Tsishurou%20and%20Alexander%20Zap%20and%20Guenter%20Wallner&entry.1292438233=%20%20Understanding%20and%20predicting%20player%20movement%20in%20multiplayer%20games%20is%20crucial%0Afor%20achieving%20use%20cases%20such%20as%20player-mimicking%20bot%20navigation%2C%20preemptive%20bot%0Acontrol%2C%20strategy%20recommendation%2C%20and%20real-time%20player%20behavior%20analytics.%0AHowever%2C%20the%20complex%20environments%20allow%20for%20a%20high%20degree%20of%20navigational%0Afreedom%2C%20and%20the%20interactions%20and%20team-play%20between%20players%20require%20models%20that%0Amake%20effective%20use%20of%20the%20available%20heterogeneous%20input%20data.%20This%20paper%0Apresents%20a%20multimodal%20architecture%20for%20predicting%20future%20player%20locations%20on%20a%0Adynamic%20time%20horizon%2C%20using%20a%20U-Net-based%20approach%20for%20calculating%20endpoint%0Alocation%20probability%20heatmaps%2C%20conditioned%20using%20a%20multimodal%20feature%20encoder.%0AThe%20application%20of%20a%20multi-head%20attention%20mechanism%20for%20different%20groups%20of%0Afeatures%20allows%20for%20communication%20between%20agents.%20In%20doing%20so%2C%20the%20architecture%0Amakes%20efficient%20use%20of%20the%20multimodal%20game%20state%20including%20image%20inputs%2C%0Anumerical%20and%20categorical%20features%2C%20as%20well%20as%20dynamic%20game%20data.%20Consequently%2C%0Athe%20presented%20technique%20lays%20the%20foundation%20for%20various%20downstream%20tasks%20that%0Arely%20on%20future%20player%20positions%20such%20as%20the%20creation%20of%20player-predictive%20bot%0Abehavior%20or%20player%20anomaly%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20670v1&entry.124074799=Read"},
{"title": "Coherent Online Road Topology Estimation and Reasoning with\n  Standard-Definition Maps", "author": "Khanh Son Pham and Christian Witte and Jens Behley and Johannes Betz and Cyrill Stachniss", "abstract": "  Most autonomous cars rely on the availability of high-definition (HD) maps.\nCurrent research aims to address this constraint by directly predicting HD map\nelements from onboard sensors and reasoning about the relationships between the\npredicted map and traffic elements. Despite recent advancements, the coherent\nonline construction of HD maps remains a challenging endeavor, as it\nnecessitates modeling the high complexity of road topologies in a unified and\nconsistent manner. To address this challenge, we propose a coherent approach to\npredict lane segments and their corresponding topology, as well as road\nboundaries, all by leveraging prior map information represented by commonly\navailable standard-definition (SD) maps. We propose a network architecture,\nwhich leverages hybrid lane segment encodings comprising prior information and\ndenoising techniques to enhance training stability and performance.\nFurthermore, we facilitate past frames for temporal consistency. Our\nexperimental evaluation demonstrates that our approach outperforms previous\nmethods by a large margin, highlighting the benefits of our modeling scheme.\n", "link": "http://arxiv.org/abs/2507.01397v2", "date": "2025-07-28", "relevancy": 2.2864, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.575}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coherent%20Online%20Road%20Topology%20Estimation%20and%20Reasoning%20with%0A%20%20Standard-Definition%20Maps&body=Title%3A%20Coherent%20Online%20Road%20Topology%20Estimation%20and%20Reasoning%20with%0A%20%20Standard-Definition%20Maps%0AAuthor%3A%20Khanh%20Son%20Pham%20and%20Christian%20Witte%20and%20Jens%20Behley%20and%20Johannes%20Betz%20and%20Cyrill%20Stachniss%0AAbstract%3A%20%20%20Most%20autonomous%20cars%20rely%20on%20the%20availability%20of%20high-definition%20%28HD%29%20maps.%0ACurrent%20research%20aims%20to%20address%20this%20constraint%20by%20directly%20predicting%20HD%20map%0Aelements%20from%20onboard%20sensors%20and%20reasoning%20about%20the%20relationships%20between%20the%0Apredicted%20map%20and%20traffic%20elements.%20Despite%20recent%20advancements%2C%20the%20coherent%0Aonline%20construction%20of%20HD%20maps%20remains%20a%20challenging%20endeavor%2C%20as%20it%0Anecessitates%20modeling%20the%20high%20complexity%20of%20road%20topologies%20in%20a%20unified%20and%0Aconsistent%20manner.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20coherent%20approach%20to%0Apredict%20lane%20segments%20and%20their%20corresponding%20topology%2C%20as%20well%20as%20road%0Aboundaries%2C%20all%20by%20leveraging%20prior%20map%20information%20represented%20by%20commonly%0Aavailable%20standard-definition%20%28SD%29%20maps.%20We%20propose%20a%20network%20architecture%2C%0Awhich%20leverages%20hybrid%20lane%20segment%20encodings%20comprising%20prior%20information%20and%0Adenoising%20techniques%20to%20enhance%20training%20stability%20and%20performance.%0AFurthermore%2C%20we%20facilitate%20past%20frames%20for%20temporal%20consistency.%20Our%0Aexperimental%20evaluation%20demonstrates%20that%20our%20approach%20outperforms%20previous%0Amethods%20by%20a%20large%20margin%2C%20highlighting%20the%20benefits%20of%20our%20modeling%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoherent%2520Online%2520Road%2520Topology%2520Estimation%2520and%2520Reasoning%2520with%250A%2520%2520Standard-Definition%2520Maps%26entry.906535625%3DKhanh%2520Son%2520Pham%2520and%2520Christian%2520Witte%2520and%2520Jens%2520Behley%2520and%2520Johannes%2520Betz%2520and%2520Cyrill%2520Stachniss%26entry.1292438233%3D%2520%2520Most%2520autonomous%2520cars%2520rely%2520on%2520the%2520availability%2520of%2520high-definition%2520%2528HD%2529%2520maps.%250ACurrent%2520research%2520aims%2520to%2520address%2520this%2520constraint%2520by%2520directly%2520predicting%2520HD%2520map%250Aelements%2520from%2520onboard%2520sensors%2520and%2520reasoning%2520about%2520the%2520relationships%2520between%2520the%250Apredicted%2520map%2520and%2520traffic%2520elements.%2520Despite%2520recent%2520advancements%252C%2520the%2520coherent%250Aonline%2520construction%2520of%2520HD%2520maps%2520remains%2520a%2520challenging%2520endeavor%252C%2520as%2520it%250Anecessitates%2520modeling%2520the%2520high%2520complexity%2520of%2520road%2520topologies%2520in%2520a%2520unified%2520and%250Aconsistent%2520manner.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520coherent%2520approach%2520to%250Apredict%2520lane%2520segments%2520and%2520their%2520corresponding%2520topology%252C%2520as%2520well%2520as%2520road%250Aboundaries%252C%2520all%2520by%2520leveraging%2520prior%2520map%2520information%2520represented%2520by%2520commonly%250Aavailable%2520standard-definition%2520%2528SD%2529%2520maps.%2520We%2520propose%2520a%2520network%2520architecture%252C%250Awhich%2520leverages%2520hybrid%2520lane%2520segment%2520encodings%2520comprising%2520prior%2520information%2520and%250Adenoising%2520techniques%2520to%2520enhance%2520training%2520stability%2520and%2520performance.%250AFurthermore%252C%2520we%2520facilitate%2520past%2520frames%2520for%2520temporal%2520consistency.%2520Our%250Aexperimental%2520evaluation%2520demonstrates%2520that%2520our%2520approach%2520outperforms%2520previous%250Amethods%2520by%2520a%2520large%2520margin%252C%2520highlighting%2520the%2520benefits%2520of%2520our%2520modeling%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coherent%20Online%20Road%20Topology%20Estimation%20and%20Reasoning%20with%0A%20%20Standard-Definition%20Maps&entry.906535625=Khanh%20Son%20Pham%20and%20Christian%20Witte%20and%20Jens%20Behley%20and%20Johannes%20Betz%20and%20Cyrill%20Stachniss&entry.1292438233=%20%20Most%20autonomous%20cars%20rely%20on%20the%20availability%20of%20high-definition%20%28HD%29%20maps.%0ACurrent%20research%20aims%20to%20address%20this%20constraint%20by%20directly%20predicting%20HD%20map%0Aelements%20from%20onboard%20sensors%20and%20reasoning%20about%20the%20relationships%20between%20the%0Apredicted%20map%20and%20traffic%20elements.%20Despite%20recent%20advancements%2C%20the%20coherent%0Aonline%20construction%20of%20HD%20maps%20remains%20a%20challenging%20endeavor%2C%20as%20it%0Anecessitates%20modeling%20the%20high%20complexity%20of%20road%20topologies%20in%20a%20unified%20and%0Aconsistent%20manner.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20coherent%20approach%20to%0Apredict%20lane%20segments%20and%20their%20corresponding%20topology%2C%20as%20well%20as%20road%0Aboundaries%2C%20all%20by%20leveraging%20prior%20map%20information%20represented%20by%20commonly%0Aavailable%20standard-definition%20%28SD%29%20maps.%20We%20propose%20a%20network%20architecture%2C%0Awhich%20leverages%20hybrid%20lane%20segment%20encodings%20comprising%20prior%20information%20and%0Adenoising%20techniques%20to%20enhance%20training%20stability%20and%20performance.%0AFurthermore%2C%20we%20facilitate%20past%20frames%20for%20temporal%20consistency.%20Our%0Aexperimental%20evaluation%20demonstrates%20that%20our%20approach%20outperforms%20previous%0Amethods%20by%20a%20large%20margin%2C%20highlighting%20the%20benefits%20of%20our%20modeling%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01397v2&entry.124074799=Read"},
{"title": "Rethinking Multi-Modal Object Detection from the Perspective of\n  Mono-Modality Feature Learning", "author": "Tianyi Zhao and Boyang Liu and Yanglei Gao and Yiming Sun and Maoxun Yuan and Xingxing Wei", "abstract": "  Multi-Modal Object Detection (MMOD), due to its stronger adaptability to\nvarious complex environments, has been widely applied in various applications.\nExtensive research is dedicated to the RGB-IR object detection, primarily\nfocusing on how to integrate complementary features from RGB-IR modalities.\nHowever, they neglect the mono-modality insufficient learning problem, which\narises from decreased feature extraction capability in multi-modal joint\nlearning. This leads to a prevalent but unreasonable phenomenon\\textemdash\nFusion Degradation, which hinders the performance improvement of the MMOD\nmodel. Motivated by this, in this paper, we introduce linear probing evaluation\nto the multi-modal detectors and rethink the multi-modal object detection task\nfrom the mono-modality learning perspective. Therefore, we construct a novel\nframework called M$^2$D-LIF, which consists of the Mono-Modality Distillation\n(M$^2$D) method and the Local Illumination-aware Fusion (LIF) module. The\nM$^2$D-LIF framework facilitates the sufficient learning of mono-modality\nduring multi-modal joint training and explores a lightweight yet effective\nfeature fusion manner to achieve superior object detection performance.\nExtensive experiments conducted on three MMOD datasets demonstrate that our\nM$^2$D-LIF effectively mitigates the Fusion Degradation phenomenon and\noutperforms the previous SOTA detectors. The codes are available at\nhttps://github.com/Zhao-Tian-yi/M2D-LIF.\n", "link": "http://arxiv.org/abs/2503.11780v2", "date": "2025-07-28", "relevancy": 2.2861, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5835}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5678}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Multi-Modal%20Object%20Detection%20from%20the%20Perspective%20of%0A%20%20Mono-Modality%20Feature%20Learning&body=Title%3A%20Rethinking%20Multi-Modal%20Object%20Detection%20from%20the%20Perspective%20of%0A%20%20Mono-Modality%20Feature%20Learning%0AAuthor%3A%20Tianyi%20Zhao%20and%20Boyang%20Liu%20and%20Yanglei%20Gao%20and%20Yiming%20Sun%20and%20Maoxun%20Yuan%20and%20Xingxing%20Wei%0AAbstract%3A%20%20%20Multi-Modal%20Object%20Detection%20%28MMOD%29%2C%20due%20to%20its%20stronger%20adaptability%20to%0Avarious%20complex%20environments%2C%20has%20been%20widely%20applied%20in%20various%20applications.%0AExtensive%20research%20is%20dedicated%20to%20the%20RGB-IR%20object%20detection%2C%20primarily%0Afocusing%20on%20how%20to%20integrate%20complementary%20features%20from%20RGB-IR%20modalities.%0AHowever%2C%20they%20neglect%20the%20mono-modality%20insufficient%20learning%20problem%2C%20which%0Aarises%20from%20decreased%20feature%20extraction%20capability%20in%20multi-modal%20joint%0Alearning.%20This%20leads%20to%20a%20prevalent%20but%20unreasonable%20phenomenon%5Ctextemdash%0AFusion%20Degradation%2C%20which%20hinders%20the%20performance%20improvement%20of%20the%20MMOD%0Amodel.%20Motivated%20by%20this%2C%20in%20this%20paper%2C%20we%20introduce%20linear%20probing%20evaluation%0Ato%20the%20multi-modal%20detectors%20and%20rethink%20the%20multi-modal%20object%20detection%20task%0Afrom%20the%20mono-modality%20learning%20perspective.%20Therefore%2C%20we%20construct%20a%20novel%0Aframework%20called%20M%24%5E2%24D-LIF%2C%20which%20consists%20of%20the%20Mono-Modality%20Distillation%0A%28M%24%5E2%24D%29%20method%20and%20the%20Local%20Illumination-aware%20Fusion%20%28LIF%29%20module.%20The%0AM%24%5E2%24D-LIF%20framework%20facilitates%20the%20sufficient%20learning%20of%20mono-modality%0Aduring%20multi-modal%20joint%20training%20and%20explores%20a%20lightweight%20yet%20effective%0Afeature%20fusion%20manner%20to%20achieve%20superior%20object%20detection%20performance.%0AExtensive%20experiments%20conducted%20on%20three%20MMOD%20datasets%20demonstrate%20that%20our%0AM%24%5E2%24D-LIF%20effectively%20mitigates%20the%20Fusion%20Degradation%20phenomenon%20and%0Aoutperforms%20the%20previous%20SOTA%20detectors.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Zhao-Tian-yi/M2D-LIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.11780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Multi-Modal%2520Object%2520Detection%2520from%2520the%2520Perspective%2520of%250A%2520%2520Mono-Modality%2520Feature%2520Learning%26entry.906535625%3DTianyi%2520Zhao%2520and%2520Boyang%2520Liu%2520and%2520Yanglei%2520Gao%2520and%2520Yiming%2520Sun%2520and%2520Maoxun%2520Yuan%2520and%2520Xingxing%2520Wei%26entry.1292438233%3D%2520%2520Multi-Modal%2520Object%2520Detection%2520%2528MMOD%2529%252C%2520due%2520to%2520its%2520stronger%2520adaptability%2520to%250Avarious%2520complex%2520environments%252C%2520has%2520been%2520widely%2520applied%2520in%2520various%2520applications.%250AExtensive%2520research%2520is%2520dedicated%2520to%2520the%2520RGB-IR%2520object%2520detection%252C%2520primarily%250Afocusing%2520on%2520how%2520to%2520integrate%2520complementary%2520features%2520from%2520RGB-IR%2520modalities.%250AHowever%252C%2520they%2520neglect%2520the%2520mono-modality%2520insufficient%2520learning%2520problem%252C%2520which%250Aarises%2520from%2520decreased%2520feature%2520extraction%2520capability%2520in%2520multi-modal%2520joint%250Alearning.%2520This%2520leads%2520to%2520a%2520prevalent%2520but%2520unreasonable%2520phenomenon%255Ctextemdash%250AFusion%2520Degradation%252C%2520which%2520hinders%2520the%2520performance%2520improvement%2520of%2520the%2520MMOD%250Amodel.%2520Motivated%2520by%2520this%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520linear%2520probing%2520evaluation%250Ato%2520the%2520multi-modal%2520detectors%2520and%2520rethink%2520the%2520multi-modal%2520object%2520detection%2520task%250Afrom%2520the%2520mono-modality%2520learning%2520perspective.%2520Therefore%252C%2520we%2520construct%2520a%2520novel%250Aframework%2520called%2520M%2524%255E2%2524D-LIF%252C%2520which%2520consists%2520of%2520the%2520Mono-Modality%2520Distillation%250A%2528M%2524%255E2%2524D%2529%2520method%2520and%2520the%2520Local%2520Illumination-aware%2520Fusion%2520%2528LIF%2529%2520module.%2520The%250AM%2524%255E2%2524D-LIF%2520framework%2520facilitates%2520the%2520sufficient%2520learning%2520of%2520mono-modality%250Aduring%2520multi-modal%2520joint%2520training%2520and%2520explores%2520a%2520lightweight%2520yet%2520effective%250Afeature%2520fusion%2520manner%2520to%2520achieve%2520superior%2520object%2520detection%2520performance.%250AExtensive%2520experiments%2520conducted%2520on%2520three%2520MMOD%2520datasets%2520demonstrate%2520that%2520our%250AM%2524%255E2%2524D-LIF%2520effectively%2520mitigates%2520the%2520Fusion%2520Degradation%2520phenomenon%2520and%250Aoutperforms%2520the%2520previous%2520SOTA%2520detectors.%2520The%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/Zhao-Tian-yi/M2D-LIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.11780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Multi-Modal%20Object%20Detection%20from%20the%20Perspective%20of%0A%20%20Mono-Modality%20Feature%20Learning&entry.906535625=Tianyi%20Zhao%20and%20Boyang%20Liu%20and%20Yanglei%20Gao%20and%20Yiming%20Sun%20and%20Maoxun%20Yuan%20and%20Xingxing%20Wei&entry.1292438233=%20%20Multi-Modal%20Object%20Detection%20%28MMOD%29%2C%20due%20to%20its%20stronger%20adaptability%20to%0Avarious%20complex%20environments%2C%20has%20been%20widely%20applied%20in%20various%20applications.%0AExtensive%20research%20is%20dedicated%20to%20the%20RGB-IR%20object%20detection%2C%20primarily%0Afocusing%20on%20how%20to%20integrate%20complementary%20features%20from%20RGB-IR%20modalities.%0AHowever%2C%20they%20neglect%20the%20mono-modality%20insufficient%20learning%20problem%2C%20which%0Aarises%20from%20decreased%20feature%20extraction%20capability%20in%20multi-modal%20joint%0Alearning.%20This%20leads%20to%20a%20prevalent%20but%20unreasonable%20phenomenon%5Ctextemdash%0AFusion%20Degradation%2C%20which%20hinders%20the%20performance%20improvement%20of%20the%20MMOD%0Amodel.%20Motivated%20by%20this%2C%20in%20this%20paper%2C%20we%20introduce%20linear%20probing%20evaluation%0Ato%20the%20multi-modal%20detectors%20and%20rethink%20the%20multi-modal%20object%20detection%20task%0Afrom%20the%20mono-modality%20learning%20perspective.%20Therefore%2C%20we%20construct%20a%20novel%0Aframework%20called%20M%24%5E2%24D-LIF%2C%20which%20consists%20of%20the%20Mono-Modality%20Distillation%0A%28M%24%5E2%24D%29%20method%20and%20the%20Local%20Illumination-aware%20Fusion%20%28LIF%29%20module.%20The%0AM%24%5E2%24D-LIF%20framework%20facilitates%20the%20sufficient%20learning%20of%20mono-modality%0Aduring%20multi-modal%20joint%20training%20and%20explores%20a%20lightweight%20yet%20effective%0Afeature%20fusion%20manner%20to%20achieve%20superior%20object%20detection%20performance.%0AExtensive%20experiments%20conducted%20on%20three%20MMOD%20datasets%20demonstrate%20that%20our%0AM%24%5E2%24D-LIF%20effectively%20mitigates%20the%20Fusion%20Degradation%20phenomenon%20and%0Aoutperforms%20the%20previous%20SOTA%20detectors.%20The%20codes%20are%20available%20at%0Ahttps%3A//github.com/Zhao-Tian-yi/M2D-LIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.11780v2&entry.124074799=Read"},
{"title": "ViewActive: Active viewpoint optimization from a single image", "author": "Jiayi Wu and Xiaomin Lin and Botao He and Cornelia Fermuller and Yiannis Aloimonos", "abstract": "  When observing objects, humans benefit from their spatial visualization and\nmental rotation ability to envision potential optimal viewpoints based on the\ncurrent observation. This capability is crucial for enabling robots to achieve\nefficient and robust scene perception during operation, as optimal viewpoints\nprovide essential and informative features for accurately representing scenes\nin 2D images, thereby enhancing downstream tasks.\n  To endow robots with this human-like active viewpoint optimization\ncapability, we propose ViewActive, a modernized machine learning approach\ndrawing inspiration from aspect graph, which provides viewpoint optimization\nguidance based solely on the current 2D image input. Specifically, we introduce\nthe 3D Viewpoint Quality Field (VQF), a compact and consistent representation\nof viewpoint quality distribution similar to an aspect graph, composed of three\ngeneral-purpose viewpoint quality metrics: self-occlusion ratio,\noccupancy-aware surface normal entropy, and visual entropy. We utilize\npre-trained image encoders to extract robust visual and semantic features,\nwhich are then decoded into the 3D VQF, allowing our model to generalize\neffectively across diverse objects, including unseen categories. The\nlightweight ViewActive network (72 FPS on a single GPU) significantly enhances\nthe performance of state-of-the-art object recognition pipelines and can be\nintegrated into real-time motion planning for robotic applications. Our code\nand dataset are available here: https://github.com/jiayi-wu-umd/ViewActive.\n", "link": "http://arxiv.org/abs/2409.09997v5", "date": "2025-07-28", "relevancy": 2.2856, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6028}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5651}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViewActive%3A%20Active%20viewpoint%20optimization%20from%20a%20single%20image&body=Title%3A%20ViewActive%3A%20Active%20viewpoint%20optimization%20from%20a%20single%20image%0AAuthor%3A%20Jiayi%20Wu%20and%20Xiaomin%20Lin%20and%20Botao%20He%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20When%20observing%20objects%2C%20humans%20benefit%20from%20their%20spatial%20visualization%20and%0Amental%20rotation%20ability%20to%20envision%20potential%20optimal%20viewpoints%20based%20on%20the%0Acurrent%20observation.%20This%20capability%20is%20crucial%20for%20enabling%20robots%20to%20achieve%0Aefficient%20and%20robust%20scene%20perception%20during%20operation%2C%20as%20optimal%20viewpoints%0Aprovide%20essential%20and%20informative%20features%20for%20accurately%20representing%20scenes%0Ain%202D%20images%2C%20thereby%20enhancing%20downstream%20tasks.%0A%20%20To%20endow%20robots%20with%20this%20human-like%20active%20viewpoint%20optimization%0Acapability%2C%20we%20propose%20ViewActive%2C%20a%20modernized%20machine%20learning%20approach%0Adrawing%20inspiration%20from%20aspect%20graph%2C%20which%20provides%20viewpoint%20optimization%0Aguidance%20based%20solely%20on%20the%20current%202D%20image%20input.%20Specifically%2C%20we%20introduce%0Athe%203D%20Viewpoint%20Quality%20Field%20%28VQF%29%2C%20a%20compact%20and%20consistent%20representation%0Aof%20viewpoint%20quality%20distribution%20similar%20to%20an%20aspect%20graph%2C%20composed%20of%20three%0Ageneral-purpose%20viewpoint%20quality%20metrics%3A%20self-occlusion%20ratio%2C%0Aoccupancy-aware%20surface%20normal%20entropy%2C%20and%20visual%20entropy.%20We%20utilize%0Apre-trained%20image%20encoders%20to%20extract%20robust%20visual%20and%20semantic%20features%2C%0Awhich%20are%20then%20decoded%20into%20the%203D%20VQF%2C%20allowing%20our%20model%20to%20generalize%0Aeffectively%20across%20diverse%20objects%2C%20including%20unseen%20categories.%20The%0Alightweight%20ViewActive%20network%20%2872%20FPS%20on%20a%20single%20GPU%29%20significantly%20enhances%0Athe%20performance%20of%20state-of-the-art%20object%20recognition%20pipelines%20and%20can%20be%0Aintegrated%20into%20real-time%20motion%20planning%20for%20robotic%20applications.%20Our%20code%0Aand%20dataset%20are%20available%20here%3A%20https%3A//github.com/jiayi-wu-umd/ViewActive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09997v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViewActive%253A%2520Active%2520viewpoint%2520optimization%2520from%2520a%2520single%2520image%26entry.906535625%3DJiayi%2520Wu%2520and%2520Xiaomin%2520Lin%2520and%2520Botao%2520He%2520and%2520Cornelia%2520Fermuller%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3D%2520%2520When%2520observing%2520objects%252C%2520humans%2520benefit%2520from%2520their%2520spatial%2520visualization%2520and%250Amental%2520rotation%2520ability%2520to%2520envision%2520potential%2520optimal%2520viewpoints%2520based%2520on%2520the%250Acurrent%2520observation.%2520This%2520capability%2520is%2520crucial%2520for%2520enabling%2520robots%2520to%2520achieve%250Aefficient%2520and%2520robust%2520scene%2520perception%2520during%2520operation%252C%2520as%2520optimal%2520viewpoints%250Aprovide%2520essential%2520and%2520informative%2520features%2520for%2520accurately%2520representing%2520scenes%250Ain%25202D%2520images%252C%2520thereby%2520enhancing%2520downstream%2520tasks.%250A%2520%2520To%2520endow%2520robots%2520with%2520this%2520human-like%2520active%2520viewpoint%2520optimization%250Acapability%252C%2520we%2520propose%2520ViewActive%252C%2520a%2520modernized%2520machine%2520learning%2520approach%250Adrawing%2520inspiration%2520from%2520aspect%2520graph%252C%2520which%2520provides%2520viewpoint%2520optimization%250Aguidance%2520based%2520solely%2520on%2520the%2520current%25202D%2520image%2520input.%2520Specifically%252C%2520we%2520introduce%250Athe%25203D%2520Viewpoint%2520Quality%2520Field%2520%2528VQF%2529%252C%2520a%2520compact%2520and%2520consistent%2520representation%250Aof%2520viewpoint%2520quality%2520distribution%2520similar%2520to%2520an%2520aspect%2520graph%252C%2520composed%2520of%2520three%250Ageneral-purpose%2520viewpoint%2520quality%2520metrics%253A%2520self-occlusion%2520ratio%252C%250Aoccupancy-aware%2520surface%2520normal%2520entropy%252C%2520and%2520visual%2520entropy.%2520We%2520utilize%250Apre-trained%2520image%2520encoders%2520to%2520extract%2520robust%2520visual%2520and%2520semantic%2520features%252C%250Awhich%2520are%2520then%2520decoded%2520into%2520the%25203D%2520VQF%252C%2520allowing%2520our%2520model%2520to%2520generalize%250Aeffectively%2520across%2520diverse%2520objects%252C%2520including%2520unseen%2520categories.%2520The%250Alightweight%2520ViewActive%2520network%2520%252872%2520FPS%2520on%2520a%2520single%2520GPU%2529%2520significantly%2520enhances%250Athe%2520performance%2520of%2520state-of-the-art%2520object%2520recognition%2520pipelines%2520and%2520can%2520be%250Aintegrated%2520into%2520real-time%2520motion%2520planning%2520for%2520robotic%2520applications.%2520Our%2520code%250Aand%2520dataset%2520are%2520available%2520here%253A%2520https%253A//github.com/jiayi-wu-umd/ViewActive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09997v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViewActive%3A%20Active%20viewpoint%20optimization%20from%20a%20single%20image&entry.906535625=Jiayi%20Wu%20and%20Xiaomin%20Lin%20and%20Botao%20He%20and%20Cornelia%20Fermuller%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20When%20observing%20objects%2C%20humans%20benefit%20from%20their%20spatial%20visualization%20and%0Amental%20rotation%20ability%20to%20envision%20potential%20optimal%20viewpoints%20based%20on%20the%0Acurrent%20observation.%20This%20capability%20is%20crucial%20for%20enabling%20robots%20to%20achieve%0Aefficient%20and%20robust%20scene%20perception%20during%20operation%2C%20as%20optimal%20viewpoints%0Aprovide%20essential%20and%20informative%20features%20for%20accurately%20representing%20scenes%0Ain%202D%20images%2C%20thereby%20enhancing%20downstream%20tasks.%0A%20%20To%20endow%20robots%20with%20this%20human-like%20active%20viewpoint%20optimization%0Acapability%2C%20we%20propose%20ViewActive%2C%20a%20modernized%20machine%20learning%20approach%0Adrawing%20inspiration%20from%20aspect%20graph%2C%20which%20provides%20viewpoint%20optimization%0Aguidance%20based%20solely%20on%20the%20current%202D%20image%20input.%20Specifically%2C%20we%20introduce%0Athe%203D%20Viewpoint%20Quality%20Field%20%28VQF%29%2C%20a%20compact%20and%20consistent%20representation%0Aof%20viewpoint%20quality%20distribution%20similar%20to%20an%20aspect%20graph%2C%20composed%20of%20three%0Ageneral-purpose%20viewpoint%20quality%20metrics%3A%20self-occlusion%20ratio%2C%0Aoccupancy-aware%20surface%20normal%20entropy%2C%20and%20visual%20entropy.%20We%20utilize%0Apre-trained%20image%20encoders%20to%20extract%20robust%20visual%20and%20semantic%20features%2C%0Awhich%20are%20then%20decoded%20into%20the%203D%20VQF%2C%20allowing%20our%20model%20to%20generalize%0Aeffectively%20across%20diverse%20objects%2C%20including%20unseen%20categories.%20The%0Alightweight%20ViewActive%20network%20%2872%20FPS%20on%20a%20single%20GPU%29%20significantly%20enhances%0Athe%20performance%20of%20state-of-the-art%20object%20recognition%20pipelines%20and%20can%20be%0Aintegrated%20into%20real-time%20motion%20planning%20for%20robotic%20applications.%20Our%20code%0Aand%20dataset%20are%20available%20here%3A%20https%3A//github.com/jiayi-wu-umd/ViewActive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09997v5&entry.124074799=Read"},
{"title": "DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability", "author": "Zhengtong Xu and Zichen Miao and Qiang Qiu and Zhe Zhang and Yu She", "abstract": "  Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. DiffOG refines\naction trajectories to be smoother and more constraint-compliant while\nmaintaining alignment with the original demonstration distribution, thus\navoiding degradation in policy performance. We evaluated DiffOG across 11\nsimulated tasks and 2 real-world tasks. The results demonstrate that DiffOG\nsignificantly enhances the trajectory quality of visuomotor policies while\nhaving minimal impact on policy performance, outperforming trajectory\nprocessing baselines such as greedy constraint clipping and penalty-based\ntrajectory optimization. Furthermore, DiffOG achieves superior performance\ncompared to existing constrained visuomotor policy. For more details, please\nvisit the project website: https://zhengtongxu.github.io/diffog-website/.\n", "link": "http://arxiv.org/abs/2504.13807v4", "date": "2025-07-28", "relevancy": 2.2777, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5882}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffOG%3A%20Differentiable%20Policy%20Trajectory%20Optimization%20with%0A%20%20Generalizability&body=Title%3A%20DiffOG%3A%20Differentiable%20Policy%20Trajectory%20Optimization%20with%0A%20%20Generalizability%0AAuthor%3A%20Zhengtong%20Xu%20and%20Zichen%20Miao%20and%20Qiang%20Qiu%20and%20Zhe%20Zhang%20and%20Yu%20She%0AAbstract%3A%20%20%20Imitation%20learning-based%20visuomotor%20policies%20excel%20at%20manipulation%20tasks%20but%0Aoften%20produce%20suboptimal%20action%20trajectories%20compared%20to%20model-based%20methods.%0ADirectly%20mapping%20camera%20data%20to%20actions%20via%20neural%20networks%20can%20result%20in%20jerky%0Amotions%20and%20difficulties%20in%20meeting%20critical%20constraints%2C%20compromising%20safety%0Aand%20robustness%20in%20real-world%20deployment.%20For%20tasks%20that%20require%20high%20robustness%0Aor%20strict%20adherence%20to%20constraints%2C%20ensuring%20trajectory%20quality%20is%20crucial.%0AHowever%2C%20the%20lack%20of%20interpretability%20in%20neural%20networks%20makes%20it%20challenging%0Ato%20generate%20constraint-compliant%20actions%20in%20a%20controlled%20manner.%20This%20paper%0Aintroduces%20differentiable%20policy%20trajectory%20optimization%20with%20generalizability%0A%28DiffOG%29%2C%20a%20learning-based%20trajectory%20optimization%20framework%20designed%20to%0Aenhance%20visuomotor%20policies.%20By%20leveraging%20the%20proposed%20differentiable%0Aformulation%20of%20trajectory%20optimization%20with%20transformer%2C%20DiffOG%20seamlessly%0Aintegrates%20policies%20with%20a%20generalizable%20optimization%20layer.%20DiffOG%20refines%0Aaction%20trajectories%20to%20be%20smoother%20and%20more%20constraint-compliant%20while%0Amaintaining%20alignment%20with%20the%20original%20demonstration%20distribution%2C%20thus%0Aavoiding%20degradation%20in%20policy%20performance.%20We%20evaluated%20DiffOG%20across%2011%0Asimulated%20tasks%20and%202%20real-world%20tasks.%20The%20results%20demonstrate%20that%20DiffOG%0Asignificantly%20enhances%20the%20trajectory%20quality%20of%20visuomotor%20policies%20while%0Ahaving%20minimal%20impact%20on%20policy%20performance%2C%20outperforming%20trajectory%0Aprocessing%20baselines%20such%20as%20greedy%20constraint%20clipping%20and%20penalty-based%0Atrajectory%20optimization.%20Furthermore%2C%20DiffOG%20achieves%20superior%20performance%0Acompared%20to%20existing%20constrained%20visuomotor%20policy.%20For%20more%20details%2C%20please%0Avisit%20the%20project%20website%3A%20https%3A//zhengtongxu.github.io/diffog-website/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13807v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffOG%253A%2520Differentiable%2520Policy%2520Trajectory%2520Optimization%2520with%250A%2520%2520Generalizability%26entry.906535625%3DZhengtong%2520Xu%2520and%2520Zichen%2520Miao%2520and%2520Qiang%2520Qiu%2520and%2520Zhe%2520Zhang%2520and%2520Yu%2520She%26entry.1292438233%3D%2520%2520Imitation%2520learning-based%2520visuomotor%2520policies%2520excel%2520at%2520manipulation%2520tasks%2520but%250Aoften%2520produce%2520suboptimal%2520action%2520trajectories%2520compared%2520to%2520model-based%2520methods.%250ADirectly%2520mapping%2520camera%2520data%2520to%2520actions%2520via%2520neural%2520networks%2520can%2520result%2520in%2520jerky%250Amotions%2520and%2520difficulties%2520in%2520meeting%2520critical%2520constraints%252C%2520compromising%2520safety%250Aand%2520robustness%2520in%2520real-world%2520deployment.%2520For%2520tasks%2520that%2520require%2520high%2520robustness%250Aor%2520strict%2520adherence%2520to%2520constraints%252C%2520ensuring%2520trajectory%2520quality%2520is%2520crucial.%250AHowever%252C%2520the%2520lack%2520of%2520interpretability%2520in%2520neural%2520networks%2520makes%2520it%2520challenging%250Ato%2520generate%2520constraint-compliant%2520actions%2520in%2520a%2520controlled%2520manner.%2520This%2520paper%250Aintroduces%2520differentiable%2520policy%2520trajectory%2520optimization%2520with%2520generalizability%250A%2528DiffOG%2529%252C%2520a%2520learning-based%2520trajectory%2520optimization%2520framework%2520designed%2520to%250Aenhance%2520visuomotor%2520policies.%2520By%2520leveraging%2520the%2520proposed%2520differentiable%250Aformulation%2520of%2520trajectory%2520optimization%2520with%2520transformer%252C%2520DiffOG%2520seamlessly%250Aintegrates%2520policies%2520with%2520a%2520generalizable%2520optimization%2520layer.%2520DiffOG%2520refines%250Aaction%2520trajectories%2520to%2520be%2520smoother%2520and%2520more%2520constraint-compliant%2520while%250Amaintaining%2520alignment%2520with%2520the%2520original%2520demonstration%2520distribution%252C%2520thus%250Aavoiding%2520degradation%2520in%2520policy%2520performance.%2520We%2520evaluated%2520DiffOG%2520across%252011%250Asimulated%2520tasks%2520and%25202%2520real-world%2520tasks.%2520The%2520results%2520demonstrate%2520that%2520DiffOG%250Asignificantly%2520enhances%2520the%2520trajectory%2520quality%2520of%2520visuomotor%2520policies%2520while%250Ahaving%2520minimal%2520impact%2520on%2520policy%2520performance%252C%2520outperforming%2520trajectory%250Aprocessing%2520baselines%2520such%2520as%2520greedy%2520constraint%2520clipping%2520and%2520penalty-based%250Atrajectory%2520optimization.%2520Furthermore%252C%2520DiffOG%2520achieves%2520superior%2520performance%250Acompared%2520to%2520existing%2520constrained%2520visuomotor%2520policy.%2520For%2520more%2520details%252C%2520please%250Avisit%2520the%2520project%2520website%253A%2520https%253A//zhengtongxu.github.io/diffog-website/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13807v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffOG%3A%20Differentiable%20Policy%20Trajectory%20Optimization%20with%0A%20%20Generalizability&entry.906535625=Zhengtong%20Xu%20and%20Zichen%20Miao%20and%20Qiang%20Qiu%20and%20Zhe%20Zhang%20and%20Yu%20She&entry.1292438233=%20%20Imitation%20learning-based%20visuomotor%20policies%20excel%20at%20manipulation%20tasks%20but%0Aoften%20produce%20suboptimal%20action%20trajectories%20compared%20to%20model-based%20methods.%0ADirectly%20mapping%20camera%20data%20to%20actions%20via%20neural%20networks%20can%20result%20in%20jerky%0Amotions%20and%20difficulties%20in%20meeting%20critical%20constraints%2C%20compromising%20safety%0Aand%20robustness%20in%20real-world%20deployment.%20For%20tasks%20that%20require%20high%20robustness%0Aor%20strict%20adherence%20to%20constraints%2C%20ensuring%20trajectory%20quality%20is%20crucial.%0AHowever%2C%20the%20lack%20of%20interpretability%20in%20neural%20networks%20makes%20it%20challenging%0Ato%20generate%20constraint-compliant%20actions%20in%20a%20controlled%20manner.%20This%20paper%0Aintroduces%20differentiable%20policy%20trajectory%20optimization%20with%20generalizability%0A%28DiffOG%29%2C%20a%20learning-based%20trajectory%20optimization%20framework%20designed%20to%0Aenhance%20visuomotor%20policies.%20By%20leveraging%20the%20proposed%20differentiable%0Aformulation%20of%20trajectory%20optimization%20with%20transformer%2C%20DiffOG%20seamlessly%0Aintegrates%20policies%20with%20a%20generalizable%20optimization%20layer.%20DiffOG%20refines%0Aaction%20trajectories%20to%20be%20smoother%20and%20more%20constraint-compliant%20while%0Amaintaining%20alignment%20with%20the%20original%20demonstration%20distribution%2C%20thus%0Aavoiding%20degradation%20in%20policy%20performance.%20We%20evaluated%20DiffOG%20across%2011%0Asimulated%20tasks%20and%202%20real-world%20tasks.%20The%20results%20demonstrate%20that%20DiffOG%0Asignificantly%20enhances%20the%20trajectory%20quality%20of%20visuomotor%20policies%20while%0Ahaving%20minimal%20impact%20on%20policy%20performance%2C%20outperforming%20trajectory%0Aprocessing%20baselines%20such%20as%20greedy%20constraint%20clipping%20and%20penalty-based%0Atrajectory%20optimization.%20Furthermore%2C%20DiffOG%20achieves%20superior%20performance%0Acompared%20to%20existing%20constrained%20visuomotor%20policy.%20For%20more%20details%2C%20please%0Avisit%20the%20project%20website%3A%20https%3A//zhengtongxu.github.io/diffog-website/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13807v4&entry.124074799=Read"},
{"title": "When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New\n  Framework for Cross-Subject Motor Imagery Decoding", "author": "Jinzhou Wu and Baoping Tang and Qikang Li and Yi Wang and Cheng Li and Shujian Yu", "abstract": "  Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key\nnon-invasive brain-computer interface (BCI) paradigm for controlling external\nsystems, has been significantly advanced by deep learning. However, MI-EEG\ndecoding remains challenging due to substantial inter-subject variability and\nlimited labeled target data, which necessitate costly calibration for new\nusers. Many existing multi-source domain adaptation (MSDA) methods\nindiscriminately incorporate all available source domains, disregarding the\nlarge inter-subject differences in EEG signals, which leads to negative\ntransfer and excessive computational costs. Moreover, while many approaches\nfocus on feature distribution alignment, they often neglect the explicit\ndependence between features and decision-level outputs, limiting their ability\nto preserve discriminative structures. To address these gaps, we propose a\nnovel MSDA framework that leverages a pretrained large Brain Foundation Model\n(BFM) for dynamic and informed source subject selection, ensuring only relevant\nsources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS)\nand Conditional CS (CCS) divergences to jointly perform feature-level and\ndecision-level alignment, enhancing domain invariance while maintaining class\ndiscriminability. Extensive evaluations on two benchmark MI-EEG datasets\ndemonstrate that our framework outperforms a broad range of state-of-the-art\nbaselines. Additional experiments with a large source pool validate the\nscalability and efficiency of BFM-guided selection, which significantly reduces\ntraining time without sacrificing performance.\n", "link": "http://arxiv.org/abs/2507.21037v1", "date": "2025-07-28", "relevancy": 2.2702, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5698}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5698}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Brain%20Foundation%20Model%20Meets%20Cauchy-Schwarz%20Divergence%3A%20A%20New%0A%20%20Framework%20for%20Cross-Subject%20Motor%20Imagery%20Decoding&body=Title%3A%20When%20Brain%20Foundation%20Model%20Meets%20Cauchy-Schwarz%20Divergence%3A%20A%20New%0A%20%20Framework%20for%20Cross-Subject%20Motor%20Imagery%20Decoding%0AAuthor%3A%20Jinzhou%20Wu%20and%20Baoping%20Tang%20and%20Qikang%20Li%20and%20Yi%20Wang%20and%20Cheng%20Li%20and%20Shujian%20Yu%0AAbstract%3A%20%20%20Decoding%20motor%20imagery%20%28MI%29%20electroencephalogram%20%28EEG%29%20signals%2C%20a%20key%0Anon-invasive%20brain-computer%20interface%20%28BCI%29%20paradigm%20for%20controlling%20external%0Asystems%2C%20has%20been%20significantly%20advanced%20by%20deep%20learning.%20However%2C%20MI-EEG%0Adecoding%20remains%20challenging%20due%20to%20substantial%20inter-subject%20variability%20and%0Alimited%20labeled%20target%20data%2C%20which%20necessitate%20costly%20calibration%20for%20new%0Ausers.%20Many%20existing%20multi-source%20domain%20adaptation%20%28MSDA%29%20methods%0Aindiscriminately%20incorporate%20all%20available%20source%20domains%2C%20disregarding%20the%0Alarge%20inter-subject%20differences%20in%20EEG%20signals%2C%20which%20leads%20to%20negative%0Atransfer%20and%20excessive%20computational%20costs.%20Moreover%2C%20while%20many%20approaches%0Afocus%20on%20feature%20distribution%20alignment%2C%20they%20often%20neglect%20the%20explicit%0Adependence%20between%20features%20and%20decision-level%20outputs%2C%20limiting%20their%20ability%0Ato%20preserve%20discriminative%20structures.%20To%20address%20these%20gaps%2C%20we%20propose%20a%0Anovel%20MSDA%20framework%20that%20leverages%20a%20pretrained%20large%20Brain%20Foundation%20Model%0A%28BFM%29%20for%20dynamic%20and%20informed%20source%20subject%20selection%2C%20ensuring%20only%20relevant%0Asources%20contribute%20to%20adaptation.%20Furthermore%2C%20we%20employ%20Cauchy-Schwarz%20%28CS%29%0Aand%20Conditional%20CS%20%28CCS%29%20divergences%20to%20jointly%20perform%20feature-level%20and%0Adecision-level%20alignment%2C%20enhancing%20domain%20invariance%20while%20maintaining%20class%0Adiscriminability.%20Extensive%20evaluations%20on%20two%20benchmark%20MI-EEG%20datasets%0Ademonstrate%20that%20our%20framework%20outperforms%20a%20broad%20range%20of%20state-of-the-art%0Abaselines.%20Additional%20experiments%20with%20a%20large%20source%20pool%20validate%20the%0Ascalability%20and%20efficiency%20of%20BFM-guided%20selection%2C%20which%20significantly%20reduces%0Atraining%20time%20without%20sacrificing%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Brain%2520Foundation%2520Model%2520Meets%2520Cauchy-Schwarz%2520Divergence%253A%2520A%2520New%250A%2520%2520Framework%2520for%2520Cross-Subject%2520Motor%2520Imagery%2520Decoding%26entry.906535625%3DJinzhou%2520Wu%2520and%2520Baoping%2520Tang%2520and%2520Qikang%2520Li%2520and%2520Yi%2520Wang%2520and%2520Cheng%2520Li%2520and%2520Shujian%2520Yu%26entry.1292438233%3D%2520%2520Decoding%2520motor%2520imagery%2520%2528MI%2529%2520electroencephalogram%2520%2528EEG%2529%2520signals%252C%2520a%2520key%250Anon-invasive%2520brain-computer%2520interface%2520%2528BCI%2529%2520paradigm%2520for%2520controlling%2520external%250Asystems%252C%2520has%2520been%2520significantly%2520advanced%2520by%2520deep%2520learning.%2520However%252C%2520MI-EEG%250Adecoding%2520remains%2520challenging%2520due%2520to%2520substantial%2520inter-subject%2520variability%2520and%250Alimited%2520labeled%2520target%2520data%252C%2520which%2520necessitate%2520costly%2520calibration%2520for%2520new%250Ausers.%2520Many%2520existing%2520multi-source%2520domain%2520adaptation%2520%2528MSDA%2529%2520methods%250Aindiscriminately%2520incorporate%2520all%2520available%2520source%2520domains%252C%2520disregarding%2520the%250Alarge%2520inter-subject%2520differences%2520in%2520EEG%2520signals%252C%2520which%2520leads%2520to%2520negative%250Atransfer%2520and%2520excessive%2520computational%2520costs.%2520Moreover%252C%2520while%2520many%2520approaches%250Afocus%2520on%2520feature%2520distribution%2520alignment%252C%2520they%2520often%2520neglect%2520the%2520explicit%250Adependence%2520between%2520features%2520and%2520decision-level%2520outputs%252C%2520limiting%2520their%2520ability%250Ato%2520preserve%2520discriminative%2520structures.%2520To%2520address%2520these%2520gaps%252C%2520we%2520propose%2520a%250Anovel%2520MSDA%2520framework%2520that%2520leverages%2520a%2520pretrained%2520large%2520Brain%2520Foundation%2520Model%250A%2528BFM%2529%2520for%2520dynamic%2520and%2520informed%2520source%2520subject%2520selection%252C%2520ensuring%2520only%2520relevant%250Asources%2520contribute%2520to%2520adaptation.%2520Furthermore%252C%2520we%2520employ%2520Cauchy-Schwarz%2520%2528CS%2529%250Aand%2520Conditional%2520CS%2520%2528CCS%2529%2520divergences%2520to%2520jointly%2520perform%2520feature-level%2520and%250Adecision-level%2520alignment%252C%2520enhancing%2520domain%2520invariance%2520while%2520maintaining%2520class%250Adiscriminability.%2520Extensive%2520evaluations%2520on%2520two%2520benchmark%2520MI-EEG%2520datasets%250Ademonstrate%2520that%2520our%2520framework%2520outperforms%2520a%2520broad%2520range%2520of%2520state-of-the-art%250Abaselines.%2520Additional%2520experiments%2520with%2520a%2520large%2520source%2520pool%2520validate%2520the%250Ascalability%2520and%2520efficiency%2520of%2520BFM-guided%2520selection%252C%2520which%2520significantly%2520reduces%250Atraining%2520time%2520without%2520sacrificing%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Brain%20Foundation%20Model%20Meets%20Cauchy-Schwarz%20Divergence%3A%20A%20New%0A%20%20Framework%20for%20Cross-Subject%20Motor%20Imagery%20Decoding&entry.906535625=Jinzhou%20Wu%20and%20Baoping%20Tang%20and%20Qikang%20Li%20and%20Yi%20Wang%20and%20Cheng%20Li%20and%20Shujian%20Yu&entry.1292438233=%20%20Decoding%20motor%20imagery%20%28MI%29%20electroencephalogram%20%28EEG%29%20signals%2C%20a%20key%0Anon-invasive%20brain-computer%20interface%20%28BCI%29%20paradigm%20for%20controlling%20external%0Asystems%2C%20has%20been%20significantly%20advanced%20by%20deep%20learning.%20However%2C%20MI-EEG%0Adecoding%20remains%20challenging%20due%20to%20substantial%20inter-subject%20variability%20and%0Alimited%20labeled%20target%20data%2C%20which%20necessitate%20costly%20calibration%20for%20new%0Ausers.%20Many%20existing%20multi-source%20domain%20adaptation%20%28MSDA%29%20methods%0Aindiscriminately%20incorporate%20all%20available%20source%20domains%2C%20disregarding%20the%0Alarge%20inter-subject%20differences%20in%20EEG%20signals%2C%20which%20leads%20to%20negative%0Atransfer%20and%20excessive%20computational%20costs.%20Moreover%2C%20while%20many%20approaches%0Afocus%20on%20feature%20distribution%20alignment%2C%20they%20often%20neglect%20the%20explicit%0Adependence%20between%20features%20and%20decision-level%20outputs%2C%20limiting%20their%20ability%0Ato%20preserve%20discriminative%20structures.%20To%20address%20these%20gaps%2C%20we%20propose%20a%0Anovel%20MSDA%20framework%20that%20leverages%20a%20pretrained%20large%20Brain%20Foundation%20Model%0A%28BFM%29%20for%20dynamic%20and%20informed%20source%20subject%20selection%2C%20ensuring%20only%20relevant%0Asources%20contribute%20to%20adaptation.%20Furthermore%2C%20we%20employ%20Cauchy-Schwarz%20%28CS%29%0Aand%20Conditional%20CS%20%28CCS%29%20divergences%20to%20jointly%20perform%20feature-level%20and%0Adecision-level%20alignment%2C%20enhancing%20domain%20invariance%20while%20maintaining%20class%0Adiscriminability.%20Extensive%20evaluations%20on%20two%20benchmark%20MI-EEG%20datasets%0Ademonstrate%20that%20our%20framework%20outperforms%20a%20broad%20range%20of%20state-of-the-art%0Abaselines.%20Additional%20experiments%20with%20a%20large%20source%20pool%20validate%20the%0Ascalability%20and%20efficiency%20of%20BFM-guided%20selection%2C%20which%20significantly%20reduces%0Atraining%20time%20without%20sacrificing%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21037v1&entry.124074799=Read"},
{"title": "LLM2TEA: An Agentic AI Designer for Discovery with Generative\n  Evolutionary Multitasking", "author": "Melvin Wong and Jiao Liu and Thiago Rios and Stefan Menzel and Yew Soon Ong", "abstract": "  This paper presents LLM2TEA, a Large Language Model (LLM) driven MultiTask\nEvolutionary Algorithm, representing the first agentic AI designer of its kind\noperating with generative evolutionary multitasking (GEM). LLM2TEA enables the\ncrossbreeding of solutions from multiple domains, fostering novel solutions\nthat transcend disciplinary boundaries. Of particular interest is the ability\nto discover designs that are both novel and conforming to real-world physical\nspecifications. LLM2TEA comprises an LLM to generate genotype samples from text\nprompts describing target objects, a text-to-3D generative model to produce\ncorresponding phenotypes, a classifier to interpret its semantic\nrepresentations, and a computational simulator to assess its physical\nproperties. Novel LLM-based multitask evolutionary operators are introduced to\nguide the search towards high-performing, practically viable designs.\nExperimental results in conceptual design optimization validate the\neffectiveness of LLM2TEA, showing 97% to 174% improvements in the diversity of\nnovel designs over the current text-to-3D baseline. Moreover, over 73% of the\ngenerated designs outperform the top 1% of designs produced by the text-to-3D\nbaseline in terms of physical performance. The designs produced by LLM2TEA are\nnot only aesthetically creative but also functional in real-world contexts.\nSeveral of these designs have been successfully 3D printed, demonstrating the\nability of our approach to transform AI-generated outputs into tangible,\nphysical designs. These designs underscore the potential of LLM2TEA as a\npowerful tool for complex design optimization and discovery, capable of\nproducing novel and physically viable designs.\n", "link": "http://arxiv.org/abs/2406.14917v3", "date": "2025-07-28", "relevancy": 2.2633, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5918}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5544}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM2TEA%3A%20An%20Agentic%20AI%20Designer%20for%20Discovery%20with%20Generative%0A%20%20Evolutionary%20Multitasking&body=Title%3A%20LLM2TEA%3A%20An%20Agentic%20AI%20Designer%20for%20Discovery%20with%20Generative%0A%20%20Evolutionary%20Multitasking%0AAuthor%3A%20Melvin%20Wong%20and%20Jiao%20Liu%20and%20Thiago%20Rios%20and%20Stefan%20Menzel%20and%20Yew%20Soon%20Ong%0AAbstract%3A%20%20%20This%20paper%20presents%20LLM2TEA%2C%20a%20Large%20Language%20Model%20%28LLM%29%20driven%20MultiTask%0AEvolutionary%20Algorithm%2C%20representing%20the%20first%20agentic%20AI%20designer%20of%20its%20kind%0Aoperating%20with%20generative%20evolutionary%20multitasking%20%28GEM%29.%20LLM2TEA%20enables%20the%0Acrossbreeding%20of%20solutions%20from%20multiple%20domains%2C%20fostering%20novel%20solutions%0Athat%20transcend%20disciplinary%20boundaries.%20Of%20particular%20interest%20is%20the%20ability%0Ato%20discover%20designs%20that%20are%20both%20novel%20and%20conforming%20to%20real-world%20physical%0Aspecifications.%20LLM2TEA%20comprises%20an%20LLM%20to%20generate%20genotype%20samples%20from%20text%0Aprompts%20describing%20target%20objects%2C%20a%20text-to-3D%20generative%20model%20to%20produce%0Acorresponding%20phenotypes%2C%20a%20classifier%20to%20interpret%20its%20semantic%0Arepresentations%2C%20and%20a%20computational%20simulator%20to%20assess%20its%20physical%0Aproperties.%20Novel%20LLM-based%20multitask%20evolutionary%20operators%20are%20introduced%20to%0Aguide%20the%20search%20towards%20high-performing%2C%20practically%20viable%20designs.%0AExperimental%20results%20in%20conceptual%20design%20optimization%20validate%20the%0Aeffectiveness%20of%20LLM2TEA%2C%20showing%2097%25%20to%20174%25%20improvements%20in%20the%20diversity%20of%0Anovel%20designs%20over%20the%20current%20text-to-3D%20baseline.%20Moreover%2C%20over%2073%25%20of%20the%0Agenerated%20designs%20outperform%20the%20top%201%25%20of%20designs%20produced%20by%20the%20text-to-3D%0Abaseline%20in%20terms%20of%20physical%20performance.%20The%20designs%20produced%20by%20LLM2TEA%20are%0Anot%20only%20aesthetically%20creative%20but%20also%20functional%20in%20real-world%20contexts.%0ASeveral%20of%20these%20designs%20have%20been%20successfully%203D%20printed%2C%20demonstrating%20the%0Aability%20of%20our%20approach%20to%20transform%20AI-generated%20outputs%20into%20tangible%2C%0Aphysical%20designs.%20These%20designs%20underscore%20the%20potential%20of%20LLM2TEA%20as%20a%0Apowerful%20tool%20for%20complex%20design%20optimization%20and%20discovery%2C%20capable%20of%0Aproducing%20novel%20and%20physically%20viable%20designs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14917v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM2TEA%253A%2520An%2520Agentic%2520AI%2520Designer%2520for%2520Discovery%2520with%2520Generative%250A%2520%2520Evolutionary%2520Multitasking%26entry.906535625%3DMelvin%2520Wong%2520and%2520Jiao%2520Liu%2520and%2520Thiago%2520Rios%2520and%2520Stefan%2520Menzel%2520and%2520Yew%2520Soon%2520Ong%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520LLM2TEA%252C%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520driven%2520MultiTask%250AEvolutionary%2520Algorithm%252C%2520representing%2520the%2520first%2520agentic%2520AI%2520designer%2520of%2520its%2520kind%250Aoperating%2520with%2520generative%2520evolutionary%2520multitasking%2520%2528GEM%2529.%2520LLM2TEA%2520enables%2520the%250Acrossbreeding%2520of%2520solutions%2520from%2520multiple%2520domains%252C%2520fostering%2520novel%2520solutions%250Athat%2520transcend%2520disciplinary%2520boundaries.%2520Of%2520particular%2520interest%2520is%2520the%2520ability%250Ato%2520discover%2520designs%2520that%2520are%2520both%2520novel%2520and%2520conforming%2520to%2520real-world%2520physical%250Aspecifications.%2520LLM2TEA%2520comprises%2520an%2520LLM%2520to%2520generate%2520genotype%2520samples%2520from%2520text%250Aprompts%2520describing%2520target%2520objects%252C%2520a%2520text-to-3D%2520generative%2520model%2520to%2520produce%250Acorresponding%2520phenotypes%252C%2520a%2520classifier%2520to%2520interpret%2520its%2520semantic%250Arepresentations%252C%2520and%2520a%2520computational%2520simulator%2520to%2520assess%2520its%2520physical%250Aproperties.%2520Novel%2520LLM-based%2520multitask%2520evolutionary%2520operators%2520are%2520introduced%2520to%250Aguide%2520the%2520search%2520towards%2520high-performing%252C%2520practically%2520viable%2520designs.%250AExperimental%2520results%2520in%2520conceptual%2520design%2520optimization%2520validate%2520the%250Aeffectiveness%2520of%2520LLM2TEA%252C%2520showing%252097%2525%2520to%2520174%2525%2520improvements%2520in%2520the%2520diversity%2520of%250Anovel%2520designs%2520over%2520the%2520current%2520text-to-3D%2520baseline.%2520Moreover%252C%2520over%252073%2525%2520of%2520the%250Agenerated%2520designs%2520outperform%2520the%2520top%25201%2525%2520of%2520designs%2520produced%2520by%2520the%2520text-to-3D%250Abaseline%2520in%2520terms%2520of%2520physical%2520performance.%2520The%2520designs%2520produced%2520by%2520LLM2TEA%2520are%250Anot%2520only%2520aesthetically%2520creative%2520but%2520also%2520functional%2520in%2520real-world%2520contexts.%250ASeveral%2520of%2520these%2520designs%2520have%2520been%2520successfully%25203D%2520printed%252C%2520demonstrating%2520the%250Aability%2520of%2520our%2520approach%2520to%2520transform%2520AI-generated%2520outputs%2520into%2520tangible%252C%250Aphysical%2520designs.%2520These%2520designs%2520underscore%2520the%2520potential%2520of%2520LLM2TEA%2520as%2520a%250Apowerful%2520tool%2520for%2520complex%2520design%2520optimization%2520and%2520discovery%252C%2520capable%2520of%250Aproducing%2520novel%2520and%2520physically%2520viable%2520designs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14917v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM2TEA%3A%20An%20Agentic%20AI%20Designer%20for%20Discovery%20with%20Generative%0A%20%20Evolutionary%20Multitasking&entry.906535625=Melvin%20Wong%20and%20Jiao%20Liu%20and%20Thiago%20Rios%20and%20Stefan%20Menzel%20and%20Yew%20Soon%20Ong&entry.1292438233=%20%20This%20paper%20presents%20LLM2TEA%2C%20a%20Large%20Language%20Model%20%28LLM%29%20driven%20MultiTask%0AEvolutionary%20Algorithm%2C%20representing%20the%20first%20agentic%20AI%20designer%20of%20its%20kind%0Aoperating%20with%20generative%20evolutionary%20multitasking%20%28GEM%29.%20LLM2TEA%20enables%20the%0Acrossbreeding%20of%20solutions%20from%20multiple%20domains%2C%20fostering%20novel%20solutions%0Athat%20transcend%20disciplinary%20boundaries.%20Of%20particular%20interest%20is%20the%20ability%0Ato%20discover%20designs%20that%20are%20both%20novel%20and%20conforming%20to%20real-world%20physical%0Aspecifications.%20LLM2TEA%20comprises%20an%20LLM%20to%20generate%20genotype%20samples%20from%20text%0Aprompts%20describing%20target%20objects%2C%20a%20text-to-3D%20generative%20model%20to%20produce%0Acorresponding%20phenotypes%2C%20a%20classifier%20to%20interpret%20its%20semantic%0Arepresentations%2C%20and%20a%20computational%20simulator%20to%20assess%20its%20physical%0Aproperties.%20Novel%20LLM-based%20multitask%20evolutionary%20operators%20are%20introduced%20to%0Aguide%20the%20search%20towards%20high-performing%2C%20practically%20viable%20designs.%0AExperimental%20results%20in%20conceptual%20design%20optimization%20validate%20the%0Aeffectiveness%20of%20LLM2TEA%2C%20showing%2097%25%20to%20174%25%20improvements%20in%20the%20diversity%20of%0Anovel%20designs%20over%20the%20current%20text-to-3D%20baseline.%20Moreover%2C%20over%2073%25%20of%20the%0Agenerated%20designs%20outperform%20the%20top%201%25%20of%20designs%20produced%20by%20the%20text-to-3D%0Abaseline%20in%20terms%20of%20physical%20performance.%20The%20designs%20produced%20by%20LLM2TEA%20are%0Anot%20only%20aesthetically%20creative%20but%20also%20functional%20in%20real-world%20contexts.%0ASeveral%20of%20these%20designs%20have%20been%20successfully%203D%20printed%2C%20demonstrating%20the%0Aability%20of%20our%20approach%20to%20transform%20AI-generated%20outputs%20into%20tangible%2C%0Aphysical%20designs.%20These%20designs%20underscore%20the%20potential%20of%20LLM2TEA%20as%20a%0Apowerful%20tool%20for%20complex%20design%20optimization%20and%20discovery%2C%20capable%20of%0Aproducing%20novel%20and%20physically%20viable%20designs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14917v3&entry.124074799=Read"},
{"title": "Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime", "author": "Amit Attia and Matan Schliserman and Uri Sherman and Tomer Koren", "abstract": "  We study population convergence guarantees of stochastic gradient descent\n(SGD) for smooth convex objectives in the interpolation regime, where the noise\nat optimum is zero or near zero. The behavior of the last iterate of SGD in\nthis setting -- particularly with large (constant) stepsizes -- has received\ngrowing attention in recent years due to implications for the training of\nover-parameterized models, as well as to analyzing forgetting in continual\nlearning and to understanding the convergence of the randomized Kaczmarz method\nfor solving linear systems. We establish that after $T$ steps of SGD on\n$\\beta$-smooth convex loss functions with stepsize $0 < \\eta < 2/\\beta$, the\nlast iterate exhibits expected excess risk $\\widetilde{O}(\\frac{1}{\\eta\n(2-\\beta \\eta) T^{1-\\beta\\eta/2}} + \\frac{\\eta}{(2-\\beta\\eta)^2}\nT^{\\beta\\eta/2} \\sigma_\\star^2)$, where $\\sigma_\\star^2$ denotes the variance\nof the stochastic gradients at the optimum. In particular, for a well-tuned\nstepsize we obtain a near optimal $\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt{T})$\nrate for the last iterate, extending the results of Varre et al. (2021) beyond\nleast squares regression; and when $\\sigma_\\star=0$ we obtain a rate of\n$\\smash{O(1/\\sqrt T)}$ with $\\eta=1/\\beta$, improving upon the best-known\n$\\smash{O(T^{-1/4})}$ rate recently established by Evron et al. (2025) in the\nspecial case of realizable linear regression.\n", "link": "http://arxiv.org/abs/2507.11274v2", "date": "2025-07-28", "relevancy": 2.2583, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4535}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4528}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Last-Iterate%20Convergence%20of%20SGD%20in%20the%20Smooth%20Interpolation%20Regime&body=Title%3A%20Fast%20Last-Iterate%20Convergence%20of%20SGD%20in%20the%20Smooth%20Interpolation%20Regime%0AAuthor%3A%20Amit%20Attia%20and%20Matan%20Schliserman%20and%20Uri%20Sherman%20and%20Tomer%20Koren%0AAbstract%3A%20%20%20We%20study%20population%20convergence%20guarantees%20of%20stochastic%20gradient%20descent%0A%28SGD%29%20for%20smooth%20convex%20objectives%20in%20the%20interpolation%20regime%2C%20where%20the%20noise%0Aat%20optimum%20is%20zero%20or%20near%20zero.%20The%20behavior%20of%20the%20last%20iterate%20of%20SGD%20in%0Athis%20setting%20--%20particularly%20with%20large%20%28constant%29%20stepsizes%20--%20has%20received%0Agrowing%20attention%20in%20recent%20years%20due%20to%20implications%20for%20the%20training%20of%0Aover-parameterized%20models%2C%20as%20well%20as%20to%20analyzing%20forgetting%20in%20continual%0Alearning%20and%20to%20understanding%20the%20convergence%20of%20the%20randomized%20Kaczmarz%20method%0Afor%20solving%20linear%20systems.%20We%20establish%20that%20after%20%24T%24%20steps%20of%20SGD%20on%0A%24%5Cbeta%24-smooth%20convex%20loss%20functions%20with%20stepsize%20%240%20%3C%20%5Ceta%20%3C%202/%5Cbeta%24%2C%20the%0Alast%20iterate%20exhibits%20expected%20excess%20risk%20%24%5Cwidetilde%7BO%7D%28%5Cfrac%7B1%7D%7B%5Ceta%0A%282-%5Cbeta%20%5Ceta%29%20T%5E%7B1-%5Cbeta%5Ceta/2%7D%7D%20%2B%20%5Cfrac%7B%5Ceta%7D%7B%282-%5Cbeta%5Ceta%29%5E2%7D%0AT%5E%7B%5Cbeta%5Ceta/2%7D%20%5Csigma_%5Cstar%5E2%29%24%2C%20where%20%24%5Csigma_%5Cstar%5E2%24%20denotes%20the%20variance%0Aof%20the%20stochastic%20gradients%20at%20the%20optimum.%20In%20particular%2C%20for%20a%20well-tuned%0Astepsize%20we%20obtain%20a%20near%20optimal%20%24%5Cwidetilde%7BO%7D%281/T%20%2B%20%5Csigma_%5Cstar/%5Csqrt%7BT%7D%29%24%0Arate%20for%20the%20last%20iterate%2C%20extending%20the%20results%20of%20Varre%20et%20al.%20%282021%29%20beyond%0Aleast%20squares%20regression%3B%20and%20when%20%24%5Csigma_%5Cstar%3D0%24%20we%20obtain%20a%20rate%20of%0A%24%5Csmash%7BO%281/%5Csqrt%20T%29%7D%24%20with%20%24%5Ceta%3D1/%5Cbeta%24%2C%20improving%20upon%20the%20best-known%0A%24%5Csmash%7BO%28T%5E%7B-1/4%7D%29%7D%24%20rate%20recently%20established%20by%20Evron%20et%20al.%20%282025%29%20in%20the%0Aspecial%20case%20of%20realizable%20linear%20regression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Last-Iterate%2520Convergence%2520of%2520SGD%2520in%2520the%2520Smooth%2520Interpolation%2520Regime%26entry.906535625%3DAmit%2520Attia%2520and%2520Matan%2520Schliserman%2520and%2520Uri%2520Sherman%2520and%2520Tomer%2520Koren%26entry.1292438233%3D%2520%2520We%2520study%2520population%2520convergence%2520guarantees%2520of%2520stochastic%2520gradient%2520descent%250A%2528SGD%2529%2520for%2520smooth%2520convex%2520objectives%2520in%2520the%2520interpolation%2520regime%252C%2520where%2520the%2520noise%250Aat%2520optimum%2520is%2520zero%2520or%2520near%2520zero.%2520The%2520behavior%2520of%2520the%2520last%2520iterate%2520of%2520SGD%2520in%250Athis%2520setting%2520--%2520particularly%2520with%2520large%2520%2528constant%2529%2520stepsizes%2520--%2520has%2520received%250Agrowing%2520attention%2520in%2520recent%2520years%2520due%2520to%2520implications%2520for%2520the%2520training%2520of%250Aover-parameterized%2520models%252C%2520as%2520well%2520as%2520to%2520analyzing%2520forgetting%2520in%2520continual%250Alearning%2520and%2520to%2520understanding%2520the%2520convergence%2520of%2520the%2520randomized%2520Kaczmarz%2520method%250Afor%2520solving%2520linear%2520systems.%2520We%2520establish%2520that%2520after%2520%2524T%2524%2520steps%2520of%2520SGD%2520on%250A%2524%255Cbeta%2524-smooth%2520convex%2520loss%2520functions%2520with%2520stepsize%2520%25240%2520%253C%2520%255Ceta%2520%253C%25202/%255Cbeta%2524%252C%2520the%250Alast%2520iterate%2520exhibits%2520expected%2520excess%2520risk%2520%2524%255Cwidetilde%257BO%257D%2528%255Cfrac%257B1%257D%257B%255Ceta%250A%25282-%255Cbeta%2520%255Ceta%2529%2520T%255E%257B1-%255Cbeta%255Ceta/2%257D%257D%2520%252B%2520%255Cfrac%257B%255Ceta%257D%257B%25282-%255Cbeta%255Ceta%2529%255E2%257D%250AT%255E%257B%255Cbeta%255Ceta/2%257D%2520%255Csigma_%255Cstar%255E2%2529%2524%252C%2520where%2520%2524%255Csigma_%255Cstar%255E2%2524%2520denotes%2520the%2520variance%250Aof%2520the%2520stochastic%2520gradients%2520at%2520the%2520optimum.%2520In%2520particular%252C%2520for%2520a%2520well-tuned%250Astepsize%2520we%2520obtain%2520a%2520near%2520optimal%2520%2524%255Cwidetilde%257BO%257D%25281/T%2520%252B%2520%255Csigma_%255Cstar/%255Csqrt%257BT%257D%2529%2524%250Arate%2520for%2520the%2520last%2520iterate%252C%2520extending%2520the%2520results%2520of%2520Varre%2520et%2520al.%2520%25282021%2529%2520beyond%250Aleast%2520squares%2520regression%253B%2520and%2520when%2520%2524%255Csigma_%255Cstar%253D0%2524%2520we%2520obtain%2520a%2520rate%2520of%250A%2524%255Csmash%257BO%25281/%255Csqrt%2520T%2529%257D%2524%2520with%2520%2524%255Ceta%253D1/%255Cbeta%2524%252C%2520improving%2520upon%2520the%2520best-known%250A%2524%255Csmash%257BO%2528T%255E%257B-1/4%257D%2529%257D%2524%2520rate%2520recently%2520established%2520by%2520Evron%2520et%2520al.%2520%25282025%2529%2520in%2520the%250Aspecial%2520case%2520of%2520realizable%2520linear%2520regression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Last-Iterate%20Convergence%20of%20SGD%20in%20the%20Smooth%20Interpolation%20Regime&entry.906535625=Amit%20Attia%20and%20Matan%20Schliserman%20and%20Uri%20Sherman%20and%20Tomer%20Koren&entry.1292438233=%20%20We%20study%20population%20convergence%20guarantees%20of%20stochastic%20gradient%20descent%0A%28SGD%29%20for%20smooth%20convex%20objectives%20in%20the%20interpolation%20regime%2C%20where%20the%20noise%0Aat%20optimum%20is%20zero%20or%20near%20zero.%20The%20behavior%20of%20the%20last%20iterate%20of%20SGD%20in%0Athis%20setting%20--%20particularly%20with%20large%20%28constant%29%20stepsizes%20--%20has%20received%0Agrowing%20attention%20in%20recent%20years%20due%20to%20implications%20for%20the%20training%20of%0Aover-parameterized%20models%2C%20as%20well%20as%20to%20analyzing%20forgetting%20in%20continual%0Alearning%20and%20to%20understanding%20the%20convergence%20of%20the%20randomized%20Kaczmarz%20method%0Afor%20solving%20linear%20systems.%20We%20establish%20that%20after%20%24T%24%20steps%20of%20SGD%20on%0A%24%5Cbeta%24-smooth%20convex%20loss%20functions%20with%20stepsize%20%240%20%3C%20%5Ceta%20%3C%202/%5Cbeta%24%2C%20the%0Alast%20iterate%20exhibits%20expected%20excess%20risk%20%24%5Cwidetilde%7BO%7D%28%5Cfrac%7B1%7D%7B%5Ceta%0A%282-%5Cbeta%20%5Ceta%29%20T%5E%7B1-%5Cbeta%5Ceta/2%7D%7D%20%2B%20%5Cfrac%7B%5Ceta%7D%7B%282-%5Cbeta%5Ceta%29%5E2%7D%0AT%5E%7B%5Cbeta%5Ceta/2%7D%20%5Csigma_%5Cstar%5E2%29%24%2C%20where%20%24%5Csigma_%5Cstar%5E2%24%20denotes%20the%20variance%0Aof%20the%20stochastic%20gradients%20at%20the%20optimum.%20In%20particular%2C%20for%20a%20well-tuned%0Astepsize%20we%20obtain%20a%20near%20optimal%20%24%5Cwidetilde%7BO%7D%281/T%20%2B%20%5Csigma_%5Cstar/%5Csqrt%7BT%7D%29%24%0Arate%20for%20the%20last%20iterate%2C%20extending%20the%20results%20of%20Varre%20et%20al.%20%282021%29%20beyond%0Aleast%20squares%20regression%3B%20and%20when%20%24%5Csigma_%5Cstar%3D0%24%20we%20obtain%20a%20rate%20of%0A%24%5Csmash%7BO%281/%5Csqrt%20T%29%7D%24%20with%20%24%5Ceta%3D1/%5Cbeta%24%2C%20improving%20upon%20the%20best-known%0A%24%5Csmash%7BO%28T%5E%7B-1/4%7D%29%7D%24%20rate%20recently%20established%20by%20Evron%20et%20al.%20%282025%29%20in%20the%0Aspecial%20case%20of%20realizable%20linear%20regression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11274v2&entry.124074799=Read"},
{"title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints", "author": "Francesco Corti and Balz Maag and Joachim Schauer and Ulrich Pferschy and Olga Saukh", "abstract": "  Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.\n", "link": "http://arxiv.org/abs/2311.13349v3", "date": "2025-07-28", "relevancy": 2.2509, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.585}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.582}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20REDS%3A%20Resource-Efficient%20Deep%20Subnetworks%20for%20Dynamic%20Resource%0A%20%20Constraints&body=Title%3A%20REDS%3A%20Resource-Efficient%20Deep%20Subnetworks%20for%20Dynamic%20Resource%0A%20%20Constraints%0AAuthor%3A%20Francesco%20Corti%20and%20Balz%20Maag%20and%20Joachim%20Schauer%20and%20Ulrich%20Pferschy%20and%20Olga%20Saukh%0AAbstract%3A%20%20%20Deep%20learning%20models%20deployed%20on%20edge%20devices%20frequently%20encounter%20resource%0Avariability%2C%20which%20arises%20from%20fluctuating%20energy%20levels%2C%20timing%20constraints%2C%0Aor%20prioritization%20of%20other%20critical%20tasks%20within%20the%20system.%20State-of-the-art%0Amachine%20learning%20pipelines%20generate%20resource-agnostic%20models%20that%20are%20not%0Acapable%20to%20adapt%20at%20runtime.%20In%20this%20work%2C%20we%20introduce%20Resource-Efficient%20Deep%0ASubnetworks%20%28REDS%29%20to%20tackle%20model%20adaptation%20to%20variable%20resources.%20In%0Acontrast%20to%20the%20state-of-the-art%2C%20REDS%20leverages%20structured%20sparsity%0Aconstructively%20by%20exploiting%20permutation%20invariance%20of%20neurons%2C%20which%20allows%0Afor%20hardware-specific%20optimizations.%20Specifically%2C%20REDS%20achieves%20computational%0Aefficiency%20by%20%281%29%20skipping%20sequential%20computational%20blocks%20identified%20by%20a%0Anovel%20iterative%20knapsack%20optimizer%2C%20and%20%282%29%20taking%20advantage%20of%20data%20cache%20by%0Are-arranging%20the%20order%20of%20operations%20in%20REDS%20computational%20graph.%20REDS%20supports%0Aconventional%20deep%20networks%20frequently%20deployed%20on%20the%20edge%20and%20provides%0Acomputational%20benefits%20even%20for%20small%20and%20simple%20networks.%20We%20evaluate%20REDS%20on%0Aeight%20benchmark%20architectures%20trained%20on%20the%20Visual%20Wake%20Words%2C%20Google%20Speech%0ACommands%2C%20Fashion-MNIST%2C%20CIFAR-10%20and%20ImageNet-1K%20datasets%2C%20and%20test%20on%20four%0Aoff-the-shelf%20mobile%20and%20embedded%20hardware%20platforms.%20We%20provide%20a%20theoretical%0Aresult%20and%20empirical%20evidence%20demonstrating%20REDS%27%20outstanding%20performance%20in%0Aterms%20of%20submodels%27%20test%20set%20accuracy%2C%20and%20demonstrate%20an%20adaptation%20time%20in%0Aresponse%20to%20dynamic%20resource%20constraints%20of%20under%2040%24%5Cmu%24s%2C%20utilizing%20a%0Afully-connected%20network%20on%20Arduino%20Nano%2033%20BLE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.13349v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DREDS%253A%2520Resource-Efficient%2520Deep%2520Subnetworks%2520for%2520Dynamic%2520Resource%250A%2520%2520Constraints%26entry.906535625%3DFrancesco%2520Corti%2520and%2520Balz%2520Maag%2520and%2520Joachim%2520Schauer%2520and%2520Ulrich%2520Pferschy%2520and%2520Olga%2520Saukh%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520deployed%2520on%2520edge%2520devices%2520frequently%2520encounter%2520resource%250Avariability%252C%2520which%2520arises%2520from%2520fluctuating%2520energy%2520levels%252C%2520timing%2520constraints%252C%250Aor%2520prioritization%2520of%2520other%2520critical%2520tasks%2520within%2520the%2520system.%2520State-of-the-art%250Amachine%2520learning%2520pipelines%2520generate%2520resource-agnostic%2520models%2520that%2520are%2520not%250Acapable%2520to%2520adapt%2520at%2520runtime.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Resource-Efficient%2520Deep%250ASubnetworks%2520%2528REDS%2529%2520to%2520tackle%2520model%2520adaptation%2520to%2520variable%2520resources.%2520In%250Acontrast%2520to%2520the%2520state-of-the-art%252C%2520REDS%2520leverages%2520structured%2520sparsity%250Aconstructively%2520by%2520exploiting%2520permutation%2520invariance%2520of%2520neurons%252C%2520which%2520allows%250Afor%2520hardware-specific%2520optimizations.%2520Specifically%252C%2520REDS%2520achieves%2520computational%250Aefficiency%2520by%2520%25281%2529%2520skipping%2520sequential%2520computational%2520blocks%2520identified%2520by%2520a%250Anovel%2520iterative%2520knapsack%2520optimizer%252C%2520and%2520%25282%2529%2520taking%2520advantage%2520of%2520data%2520cache%2520by%250Are-arranging%2520the%2520order%2520of%2520operations%2520in%2520REDS%2520computational%2520graph.%2520REDS%2520supports%250Aconventional%2520deep%2520networks%2520frequently%2520deployed%2520on%2520the%2520edge%2520and%2520provides%250Acomputational%2520benefits%2520even%2520for%2520small%2520and%2520simple%2520networks.%2520We%2520evaluate%2520REDS%2520on%250Aeight%2520benchmark%2520architectures%2520trained%2520on%2520the%2520Visual%2520Wake%2520Words%252C%2520Google%2520Speech%250ACommands%252C%2520Fashion-MNIST%252C%2520CIFAR-10%2520and%2520ImageNet-1K%2520datasets%252C%2520and%2520test%2520on%2520four%250Aoff-the-shelf%2520mobile%2520and%2520embedded%2520hardware%2520platforms.%2520We%2520provide%2520a%2520theoretical%250Aresult%2520and%2520empirical%2520evidence%2520demonstrating%2520REDS%2527%2520outstanding%2520performance%2520in%250Aterms%2520of%2520submodels%2527%2520test%2520set%2520accuracy%252C%2520and%2520demonstrate%2520an%2520adaptation%2520time%2520in%250Aresponse%2520to%2520dynamic%2520resource%2520constraints%2520of%2520under%252040%2524%255Cmu%2524s%252C%2520utilizing%2520a%250Afully-connected%2520network%2520on%2520Arduino%2520Nano%252033%2520BLE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.13349v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=REDS%3A%20Resource-Efficient%20Deep%20Subnetworks%20for%20Dynamic%20Resource%0A%20%20Constraints&entry.906535625=Francesco%20Corti%20and%20Balz%20Maag%20and%20Joachim%20Schauer%20and%20Ulrich%20Pferschy%20and%20Olga%20Saukh&entry.1292438233=%20%20Deep%20learning%20models%20deployed%20on%20edge%20devices%20frequently%20encounter%20resource%0Avariability%2C%20which%20arises%20from%20fluctuating%20energy%20levels%2C%20timing%20constraints%2C%0Aor%20prioritization%20of%20other%20critical%20tasks%20within%20the%20system.%20State-of-the-art%0Amachine%20learning%20pipelines%20generate%20resource-agnostic%20models%20that%20are%20not%0Acapable%20to%20adapt%20at%20runtime.%20In%20this%20work%2C%20we%20introduce%20Resource-Efficient%20Deep%0ASubnetworks%20%28REDS%29%20to%20tackle%20model%20adaptation%20to%20variable%20resources.%20In%0Acontrast%20to%20the%20state-of-the-art%2C%20REDS%20leverages%20structured%20sparsity%0Aconstructively%20by%20exploiting%20permutation%20invariance%20of%20neurons%2C%20which%20allows%0Afor%20hardware-specific%20optimizations.%20Specifically%2C%20REDS%20achieves%20computational%0Aefficiency%20by%20%281%29%20skipping%20sequential%20computational%20blocks%20identified%20by%20a%0Anovel%20iterative%20knapsack%20optimizer%2C%20and%20%282%29%20taking%20advantage%20of%20data%20cache%20by%0Are-arranging%20the%20order%20of%20operations%20in%20REDS%20computational%20graph.%20REDS%20supports%0Aconventional%20deep%20networks%20frequently%20deployed%20on%20the%20edge%20and%20provides%0Acomputational%20benefits%20even%20for%20small%20and%20simple%20networks.%20We%20evaluate%20REDS%20on%0Aeight%20benchmark%20architectures%20trained%20on%20the%20Visual%20Wake%20Words%2C%20Google%20Speech%0ACommands%2C%20Fashion-MNIST%2C%20CIFAR-10%20and%20ImageNet-1K%20datasets%2C%20and%20test%20on%20four%0Aoff-the-shelf%20mobile%20and%20embedded%20hardware%20platforms.%20We%20provide%20a%20theoretical%0Aresult%20and%20empirical%20evidence%20demonstrating%20REDS%27%20outstanding%20performance%20in%0Aterms%20of%20submodels%27%20test%20set%20accuracy%2C%20and%20demonstrate%20an%20adaptation%20time%20in%0Aresponse%20to%20dynamic%20resource%20constraints%20of%20under%2040%24%5Cmu%24s%2C%20utilizing%20a%0Afully-connected%20network%20on%20Arduino%20Nano%2033%20BLE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.13349v3&entry.124074799=Read"},
{"title": "Uncertainty-driven Embedding Convolution", "author": "Sungjun Lim and Kangjun Noh and Youngjun Choi and Heeyoung Lee and Kyungwoo Song", "abstract": "  Text embeddings are essential components in modern NLP pipelines. While\nnumerous embedding models have been proposed, their performance varies across\ndomains, and no single model consistently excels across all tasks. This\nvariability motivates the use of ensemble techniques to combine complementary\nstrengths. However, most existing ensemble methods operate on deterministic\nembeddings and fail to account for model-specific uncertainty, limiting their\nrobustness and reliability in downstream applications. To address these\nlimitations, we propose Uncertainty-driven Embedding Convolution (UEC). UEC\nfirst transforms deterministic embeddings into probabilistic ones in a post-hoc\nmanner. It then computes adaptive ensemble weights based on embedding\nuncertainty, grounded in a Bayes-optimal solution under a surrogate loss.\nAdditionally, UEC introduces an uncertainty-aware similarity function that\ndirectly incorporates uncertainty into similarity scoring. Extensive\nexperiments on retrieval, classification, and semantic similarity benchmarks\ndemonstrate that UEC consistently improves both performance and robustness by\nleveraging principled uncertainty modeling.\n", "link": "http://arxiv.org/abs/2507.20718v1", "date": "2025-07-28", "relevancy": 2.244, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6276}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5783}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-driven%20Embedding%20Convolution&body=Title%3A%20Uncertainty-driven%20Embedding%20Convolution%0AAuthor%3A%20Sungjun%20Lim%20and%20Kangjun%20Noh%20and%20Youngjun%20Choi%20and%20Heeyoung%20Lee%20and%20Kyungwoo%20Song%0AAbstract%3A%20%20%20Text%20embeddings%20are%20essential%20components%20in%20modern%20NLP%20pipelines.%20While%0Anumerous%20embedding%20models%20have%20been%20proposed%2C%20their%20performance%20varies%20across%0Adomains%2C%20and%20no%20single%20model%20consistently%20excels%20across%20all%20tasks.%20This%0Avariability%20motivates%20the%20use%20of%20ensemble%20techniques%20to%20combine%20complementary%0Astrengths.%20However%2C%20most%20existing%20ensemble%20methods%20operate%20on%20deterministic%0Aembeddings%20and%20fail%20to%20account%20for%20model-specific%20uncertainty%2C%20limiting%20their%0Arobustness%20and%20reliability%20in%20downstream%20applications.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Uncertainty-driven%20Embedding%20Convolution%20%28UEC%29.%20UEC%0Afirst%20transforms%20deterministic%20embeddings%20into%20probabilistic%20ones%20in%20a%20post-hoc%0Amanner.%20It%20then%20computes%20adaptive%20ensemble%20weights%20based%20on%20embedding%0Auncertainty%2C%20grounded%20in%20a%20Bayes-optimal%20solution%20under%20a%20surrogate%20loss.%0AAdditionally%2C%20UEC%20introduces%20an%20uncertainty-aware%20similarity%20function%20that%0Adirectly%20incorporates%20uncertainty%20into%20similarity%20scoring.%20Extensive%0Aexperiments%20on%20retrieval%2C%20classification%2C%20and%20semantic%20similarity%20benchmarks%0Ademonstrate%20that%20UEC%20consistently%20improves%20both%20performance%20and%20robustness%20by%0Aleveraging%20principled%20uncertainty%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-driven%2520Embedding%2520Convolution%26entry.906535625%3DSungjun%2520Lim%2520and%2520Kangjun%2520Noh%2520and%2520Youngjun%2520Choi%2520and%2520Heeyoung%2520Lee%2520and%2520Kyungwoo%2520Song%26entry.1292438233%3D%2520%2520Text%2520embeddings%2520are%2520essential%2520components%2520in%2520modern%2520NLP%2520pipelines.%2520While%250Anumerous%2520embedding%2520models%2520have%2520been%2520proposed%252C%2520their%2520performance%2520varies%2520across%250Adomains%252C%2520and%2520no%2520single%2520model%2520consistently%2520excels%2520across%2520all%2520tasks.%2520This%250Avariability%2520motivates%2520the%2520use%2520of%2520ensemble%2520techniques%2520to%2520combine%2520complementary%250Astrengths.%2520However%252C%2520most%2520existing%2520ensemble%2520methods%2520operate%2520on%2520deterministic%250Aembeddings%2520and%2520fail%2520to%2520account%2520for%2520model-specific%2520uncertainty%252C%2520limiting%2520their%250Arobustness%2520and%2520reliability%2520in%2520downstream%2520applications.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520Uncertainty-driven%2520Embedding%2520Convolution%2520%2528UEC%2529.%2520UEC%250Afirst%2520transforms%2520deterministic%2520embeddings%2520into%2520probabilistic%2520ones%2520in%2520a%2520post-hoc%250Amanner.%2520It%2520then%2520computes%2520adaptive%2520ensemble%2520weights%2520based%2520on%2520embedding%250Auncertainty%252C%2520grounded%2520in%2520a%2520Bayes-optimal%2520solution%2520under%2520a%2520surrogate%2520loss.%250AAdditionally%252C%2520UEC%2520introduces%2520an%2520uncertainty-aware%2520similarity%2520function%2520that%250Adirectly%2520incorporates%2520uncertainty%2520into%2520similarity%2520scoring.%2520Extensive%250Aexperiments%2520on%2520retrieval%252C%2520classification%252C%2520and%2520semantic%2520similarity%2520benchmarks%250Ademonstrate%2520that%2520UEC%2520consistently%2520improves%2520both%2520performance%2520and%2520robustness%2520by%250Aleveraging%2520principled%2520uncertainty%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-driven%20Embedding%20Convolution&entry.906535625=Sungjun%20Lim%20and%20Kangjun%20Noh%20and%20Youngjun%20Choi%20and%20Heeyoung%20Lee%20and%20Kyungwoo%20Song&entry.1292438233=%20%20Text%20embeddings%20are%20essential%20components%20in%20modern%20NLP%20pipelines.%20While%0Anumerous%20embedding%20models%20have%20been%20proposed%2C%20their%20performance%20varies%20across%0Adomains%2C%20and%20no%20single%20model%20consistently%20excels%20across%20all%20tasks.%20This%0Avariability%20motivates%20the%20use%20of%20ensemble%20techniques%20to%20combine%20complementary%0Astrengths.%20However%2C%20most%20existing%20ensemble%20methods%20operate%20on%20deterministic%0Aembeddings%20and%20fail%20to%20account%20for%20model-specific%20uncertainty%2C%20limiting%20their%0Arobustness%20and%20reliability%20in%20downstream%20applications.%20To%20address%20these%0Alimitations%2C%20we%20propose%20Uncertainty-driven%20Embedding%20Convolution%20%28UEC%29.%20UEC%0Afirst%20transforms%20deterministic%20embeddings%20into%20probabilistic%20ones%20in%20a%20post-hoc%0Amanner.%20It%20then%20computes%20adaptive%20ensemble%20weights%20based%20on%20embedding%0Auncertainty%2C%20grounded%20in%20a%20Bayes-optimal%20solution%20under%20a%20surrogate%20loss.%0AAdditionally%2C%20UEC%20introduces%20an%20uncertainty-aware%20similarity%20function%20that%0Adirectly%20incorporates%20uncertainty%20into%20similarity%20scoring.%20Extensive%0Aexperiments%20on%20retrieval%2C%20classification%2C%20and%20semantic%20similarity%20benchmarks%0Ademonstrate%20that%20UEC%20consistently%20improves%20both%20performance%20and%20robustness%20by%0Aleveraging%20principled%20uncertainty%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20718v1&entry.124074799=Read"},
{"title": "Visual Enumeration Remains Challenging for Multimodal Generative AI", "author": "Alberto Testolin and Kuinan Hou and Marco Zorzi", "abstract": "  Many animal species can approximately judge the number of objects in a visual\nscene at a single glance, and humans can further determine the exact\ncardinality of a set by deploying systematic counting procedures. In contrast,\nit has been observed that even state-of-the-art AI systems have very limited\nenumeration skills. In this work, we propose two benchmark tasks inspired by\ncognitive science that allow to precisely evaluate the visual enumeration\ncapabilities of multimodal foundation models, thereby providing an objective\nmeasure of their number sense and counting level. We consider popular visual\nquestion answering models (BLIP, LLaVA and ViLT) as well as advanced\nimage-to-text (Gemini, GPT and Qwen) and text-to-image (DALL-E, FLUX and Stable\nDiffusion) AI systems. Our analyses show that even the most advanced models\ncannot reliably name the number of objects in simple visual stimuli or generate\nimages containing a target number of items, as indexed by their low accuracy in\nboth types of tasks. Especially for numbers outside the subitizing range, their\nresponses are often far from the target numerosity, and, in stark contrast with\nhuman behavior, in many cases the distribution of errors depends on the object\ncategory. We also observe some striking mistakes with small numbers. Our\nfindings demonstrate that developing an intuitive visual understanding of\nnumber remains challenging for AI models and that merely increasing model size\nmight not be a viable strategy to promote the emergence of systematic counting\nskills. We release the full code of our benchmark to facilitate the evaluation\nof enumeration skills in future AI systems.\n", "link": "http://arxiv.org/abs/2402.03328v3", "date": "2025-07-28", "relevancy": 2.2312, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5672}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Enumeration%20Remains%20Challenging%20for%20Multimodal%20Generative%20AI&body=Title%3A%20Visual%20Enumeration%20Remains%20Challenging%20for%20Multimodal%20Generative%20AI%0AAuthor%3A%20Alberto%20Testolin%20and%20Kuinan%20Hou%20and%20Marco%20Zorzi%0AAbstract%3A%20%20%20Many%20animal%20species%20can%20approximately%20judge%20the%20number%20of%20objects%20in%20a%20visual%0Ascene%20at%20a%20single%20glance%2C%20and%20humans%20can%20further%20determine%20the%20exact%0Acardinality%20of%20a%20set%20by%20deploying%20systematic%20counting%20procedures.%20In%20contrast%2C%0Ait%20has%20been%20observed%20that%20even%20state-of-the-art%20AI%20systems%20have%20very%20limited%0Aenumeration%20skills.%20In%20this%20work%2C%20we%20propose%20two%20benchmark%20tasks%20inspired%20by%0Acognitive%20science%20that%20allow%20to%20precisely%20evaluate%20the%20visual%20enumeration%0Acapabilities%20of%20multimodal%20foundation%20models%2C%20thereby%20providing%20an%20objective%0Ameasure%20of%20their%20number%20sense%20and%20counting%20level.%20We%20consider%20popular%20visual%0Aquestion%20answering%20models%20%28BLIP%2C%20LLaVA%20and%20ViLT%29%20as%20well%20as%20advanced%0Aimage-to-text%20%28Gemini%2C%20GPT%20and%20Qwen%29%20and%20text-to-image%20%28DALL-E%2C%20FLUX%20and%20Stable%0ADiffusion%29%20AI%20systems.%20Our%20analyses%20show%20that%20even%20the%20most%20advanced%20models%0Acannot%20reliably%20name%20the%20number%20of%20objects%20in%20simple%20visual%20stimuli%20or%20generate%0Aimages%20containing%20a%20target%20number%20of%20items%2C%20as%20indexed%20by%20their%20low%20accuracy%20in%0Aboth%20types%20of%20tasks.%20Especially%20for%20numbers%20outside%20the%20subitizing%20range%2C%20their%0Aresponses%20are%20often%20far%20from%20the%20target%20numerosity%2C%20and%2C%20in%20stark%20contrast%20with%0Ahuman%20behavior%2C%20in%20many%20cases%20the%20distribution%20of%20errors%20depends%20on%20the%20object%0Acategory.%20We%20also%20observe%20some%20striking%20mistakes%20with%20small%20numbers.%20Our%0Afindings%20demonstrate%20that%20developing%20an%20intuitive%20visual%20understanding%20of%0Anumber%20remains%20challenging%20for%20AI%20models%20and%20that%20merely%20increasing%20model%20size%0Amight%20not%20be%20a%20viable%20strategy%20to%20promote%20the%20emergence%20of%20systematic%20counting%0Askills.%20We%20release%20the%20full%20code%20of%20our%20benchmark%20to%20facilitate%20the%20evaluation%0Aof%20enumeration%20skills%20in%20future%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03328v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Enumeration%2520Remains%2520Challenging%2520for%2520Multimodal%2520Generative%2520AI%26entry.906535625%3DAlberto%2520Testolin%2520and%2520Kuinan%2520Hou%2520and%2520Marco%2520Zorzi%26entry.1292438233%3D%2520%2520Many%2520animal%2520species%2520can%2520approximately%2520judge%2520the%2520number%2520of%2520objects%2520in%2520a%2520visual%250Ascene%2520at%2520a%2520single%2520glance%252C%2520and%2520humans%2520can%2520further%2520determine%2520the%2520exact%250Acardinality%2520of%2520a%2520set%2520by%2520deploying%2520systematic%2520counting%2520procedures.%2520In%2520contrast%252C%250Ait%2520has%2520been%2520observed%2520that%2520even%2520state-of-the-art%2520AI%2520systems%2520have%2520very%2520limited%250Aenumeration%2520skills.%2520In%2520this%2520work%252C%2520we%2520propose%2520two%2520benchmark%2520tasks%2520inspired%2520by%250Acognitive%2520science%2520that%2520allow%2520to%2520precisely%2520evaluate%2520the%2520visual%2520enumeration%250Acapabilities%2520of%2520multimodal%2520foundation%2520models%252C%2520thereby%2520providing%2520an%2520objective%250Ameasure%2520of%2520their%2520number%2520sense%2520and%2520counting%2520level.%2520We%2520consider%2520popular%2520visual%250Aquestion%2520answering%2520models%2520%2528BLIP%252C%2520LLaVA%2520and%2520ViLT%2529%2520as%2520well%2520as%2520advanced%250Aimage-to-text%2520%2528Gemini%252C%2520GPT%2520and%2520Qwen%2529%2520and%2520text-to-image%2520%2528DALL-E%252C%2520FLUX%2520and%2520Stable%250ADiffusion%2529%2520AI%2520systems.%2520Our%2520analyses%2520show%2520that%2520even%2520the%2520most%2520advanced%2520models%250Acannot%2520reliably%2520name%2520the%2520number%2520of%2520objects%2520in%2520simple%2520visual%2520stimuli%2520or%2520generate%250Aimages%2520containing%2520a%2520target%2520number%2520of%2520items%252C%2520as%2520indexed%2520by%2520their%2520low%2520accuracy%2520in%250Aboth%2520types%2520of%2520tasks.%2520Especially%2520for%2520numbers%2520outside%2520the%2520subitizing%2520range%252C%2520their%250Aresponses%2520are%2520often%2520far%2520from%2520the%2520target%2520numerosity%252C%2520and%252C%2520in%2520stark%2520contrast%2520with%250Ahuman%2520behavior%252C%2520in%2520many%2520cases%2520the%2520distribution%2520of%2520errors%2520depends%2520on%2520the%2520object%250Acategory.%2520We%2520also%2520observe%2520some%2520striking%2520mistakes%2520with%2520small%2520numbers.%2520Our%250Afindings%2520demonstrate%2520that%2520developing%2520an%2520intuitive%2520visual%2520understanding%2520of%250Anumber%2520remains%2520challenging%2520for%2520AI%2520models%2520and%2520that%2520merely%2520increasing%2520model%2520size%250Amight%2520not%2520be%2520a%2520viable%2520strategy%2520to%2520promote%2520the%2520emergence%2520of%2520systematic%2520counting%250Askills.%2520We%2520release%2520the%2520full%2520code%2520of%2520our%2520benchmark%2520to%2520facilitate%2520the%2520evaluation%250Aof%2520enumeration%2520skills%2520in%2520future%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03328v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Enumeration%20Remains%20Challenging%20for%20Multimodal%20Generative%20AI&entry.906535625=Alberto%20Testolin%20and%20Kuinan%20Hou%20and%20Marco%20Zorzi&entry.1292438233=%20%20Many%20animal%20species%20can%20approximately%20judge%20the%20number%20of%20objects%20in%20a%20visual%0Ascene%20at%20a%20single%20glance%2C%20and%20humans%20can%20further%20determine%20the%20exact%0Acardinality%20of%20a%20set%20by%20deploying%20systematic%20counting%20procedures.%20In%20contrast%2C%0Ait%20has%20been%20observed%20that%20even%20state-of-the-art%20AI%20systems%20have%20very%20limited%0Aenumeration%20skills.%20In%20this%20work%2C%20we%20propose%20two%20benchmark%20tasks%20inspired%20by%0Acognitive%20science%20that%20allow%20to%20precisely%20evaluate%20the%20visual%20enumeration%0Acapabilities%20of%20multimodal%20foundation%20models%2C%20thereby%20providing%20an%20objective%0Ameasure%20of%20their%20number%20sense%20and%20counting%20level.%20We%20consider%20popular%20visual%0Aquestion%20answering%20models%20%28BLIP%2C%20LLaVA%20and%20ViLT%29%20as%20well%20as%20advanced%0Aimage-to-text%20%28Gemini%2C%20GPT%20and%20Qwen%29%20and%20text-to-image%20%28DALL-E%2C%20FLUX%20and%20Stable%0ADiffusion%29%20AI%20systems.%20Our%20analyses%20show%20that%20even%20the%20most%20advanced%20models%0Acannot%20reliably%20name%20the%20number%20of%20objects%20in%20simple%20visual%20stimuli%20or%20generate%0Aimages%20containing%20a%20target%20number%20of%20items%2C%20as%20indexed%20by%20their%20low%20accuracy%20in%0Aboth%20types%20of%20tasks.%20Especially%20for%20numbers%20outside%20the%20subitizing%20range%2C%20their%0Aresponses%20are%20often%20far%20from%20the%20target%20numerosity%2C%20and%2C%20in%20stark%20contrast%20with%0Ahuman%20behavior%2C%20in%20many%20cases%20the%20distribution%20of%20errors%20depends%20on%20the%20object%0Acategory.%20We%20also%20observe%20some%20striking%20mistakes%20with%20small%20numbers.%20Our%0Afindings%20demonstrate%20that%20developing%20an%20intuitive%20visual%20understanding%20of%0Anumber%20remains%20challenging%20for%20AI%20models%20and%20that%20merely%20increasing%20model%20size%0Amight%20not%20be%20a%20viable%20strategy%20to%20promote%20the%20emergence%20of%20systematic%20counting%0Askills.%20We%20release%20the%20full%20code%20of%20our%20benchmark%20to%20facilitate%20the%20evaluation%0Aof%20enumeration%20skills%20in%20future%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03328v3&entry.124074799=Read"},
{"title": "ATR-UMMIM: A Benchmark Dataset for UAV-Based Multimodal Image\n  Registration under Complex Imaging Conditions", "author": "Kangcheng Bin and Chen Chen and Ting Hu and Jiahao Qi and Ping Zhong", "abstract": "  Multimodal fusion has become a key enabler for UAV-based object detection, as\neach modality provides complementary cues for robust feature extraction.\nHowever, due to significant differences in resolution, field of view, and\nsensing characteristics across modalities, accurate registration is a\nprerequisite before fusion. Despite its importance, there is currently no\npublicly available benchmark specifically designed for multimodal registration\nin UAV-based aerial scenarios, which severely limits the development and\nevaluation of advanced registration methods under real-world conditions. To\nbridge this gap, we present ATR-UMMIM, the first benchmark dataset specifically\ntailored for multimodal image registration in UAV-based applications. This\ndataset includes 7,969 triplets of raw visible, infrared, and precisely\nregistered visible images captured covers diverse scenarios including flight\naltitudes from 80m to 300m, camera angles from 0{\\deg} to 75{\\deg}, and\nall-day, all-year temporal variations under rich weather and illumination\nconditions. To ensure high registration quality, we design a semi-automated\nannotation pipeline to introduce reliable pixel-level ground truth to each\ntriplet. In addition, each triplet is annotated with six imaging condition\nattributes, enabling benchmarking of registration robustness under real-world\ndeployment settings. To further support downstream tasks, we provide\nobject-level annotations on all registered images, covering 11 object\ncategories with 77,753 visible and 78,409 infrared bounding boxes. We believe\nATR-UMMIM will serve as a foundational benchmark for advancing multimodal\nregistration, fusion, and perception in real-world UAV scenarios. The datatset\ncan be download from https://github.com/supercpy/ATR-UMMIM\n", "link": "http://arxiv.org/abs/2507.20764v1", "date": "2025-07-28", "relevancy": 2.2235, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5824}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ATR-UMMIM%3A%20A%20Benchmark%20Dataset%20for%20UAV-Based%20Multimodal%20Image%0A%20%20Registration%20under%20Complex%20Imaging%20Conditions&body=Title%3A%20ATR-UMMIM%3A%20A%20Benchmark%20Dataset%20for%20UAV-Based%20Multimodal%20Image%0A%20%20Registration%20under%20Complex%20Imaging%20Conditions%0AAuthor%3A%20Kangcheng%20Bin%20and%20Chen%20Chen%20and%20Ting%20Hu%20and%20Jiahao%20Qi%20and%20Ping%20Zhong%0AAbstract%3A%20%20%20Multimodal%20fusion%20has%20become%20a%20key%20enabler%20for%20UAV-based%20object%20detection%2C%20as%0Aeach%20modality%20provides%20complementary%20cues%20for%20robust%20feature%20extraction.%0AHowever%2C%20due%20to%20significant%20differences%20in%20resolution%2C%20field%20of%20view%2C%20and%0Asensing%20characteristics%20across%20modalities%2C%20accurate%20registration%20is%20a%0Aprerequisite%20before%20fusion.%20Despite%20its%20importance%2C%20there%20is%20currently%20no%0Apublicly%20available%20benchmark%20specifically%20designed%20for%20multimodal%20registration%0Ain%20UAV-based%20aerial%20scenarios%2C%20which%20severely%20limits%20the%20development%20and%0Aevaluation%20of%20advanced%20registration%20methods%20under%20real-world%20conditions.%20To%0Abridge%20this%20gap%2C%20we%20present%20ATR-UMMIM%2C%20the%20first%20benchmark%20dataset%20specifically%0Atailored%20for%20multimodal%20image%20registration%20in%20UAV-based%20applications.%20This%0Adataset%20includes%207%2C969%20triplets%20of%20raw%20visible%2C%20infrared%2C%20and%20precisely%0Aregistered%20visible%20images%20captured%20covers%20diverse%20scenarios%20including%20flight%0Aaltitudes%20from%2080m%20to%20300m%2C%20camera%20angles%20from%200%7B%5Cdeg%7D%20to%2075%7B%5Cdeg%7D%2C%20and%0Aall-day%2C%20all-year%20temporal%20variations%20under%20rich%20weather%20and%20illumination%0Aconditions.%20To%20ensure%20high%20registration%20quality%2C%20we%20design%20a%20semi-automated%0Aannotation%20pipeline%20to%20introduce%20reliable%20pixel-level%20ground%20truth%20to%20each%0Atriplet.%20In%20addition%2C%20each%20triplet%20is%20annotated%20with%20six%20imaging%20condition%0Aattributes%2C%20enabling%20benchmarking%20of%20registration%20robustness%20under%20real-world%0Adeployment%20settings.%20To%20further%20support%20downstream%20tasks%2C%20we%20provide%0Aobject-level%20annotations%20on%20all%20registered%20images%2C%20covering%2011%20object%0Acategories%20with%2077%2C753%20visible%20and%2078%2C409%20infrared%20bounding%20boxes.%20We%20believe%0AATR-UMMIM%20will%20serve%20as%20a%20foundational%20benchmark%20for%20advancing%20multimodal%0Aregistration%2C%20fusion%2C%20and%20perception%20in%20real-world%20UAV%20scenarios.%20The%20datatset%0Acan%20be%20download%20from%20https%3A//github.com/supercpy/ATR-UMMIM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DATR-UMMIM%253A%2520A%2520Benchmark%2520Dataset%2520for%2520UAV-Based%2520Multimodal%2520Image%250A%2520%2520Registration%2520under%2520Complex%2520Imaging%2520Conditions%26entry.906535625%3DKangcheng%2520Bin%2520and%2520Chen%2520Chen%2520and%2520Ting%2520Hu%2520and%2520Jiahao%2520Qi%2520and%2520Ping%2520Zhong%26entry.1292438233%3D%2520%2520Multimodal%2520fusion%2520has%2520become%2520a%2520key%2520enabler%2520for%2520UAV-based%2520object%2520detection%252C%2520as%250Aeach%2520modality%2520provides%2520complementary%2520cues%2520for%2520robust%2520feature%2520extraction.%250AHowever%252C%2520due%2520to%2520significant%2520differences%2520in%2520resolution%252C%2520field%2520of%2520view%252C%2520and%250Asensing%2520characteristics%2520across%2520modalities%252C%2520accurate%2520registration%2520is%2520a%250Aprerequisite%2520before%2520fusion.%2520Despite%2520its%2520importance%252C%2520there%2520is%2520currently%2520no%250Apublicly%2520available%2520benchmark%2520specifically%2520designed%2520for%2520multimodal%2520registration%250Ain%2520UAV-based%2520aerial%2520scenarios%252C%2520which%2520severely%2520limits%2520the%2520development%2520and%250Aevaluation%2520of%2520advanced%2520registration%2520methods%2520under%2520real-world%2520conditions.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520present%2520ATR-UMMIM%252C%2520the%2520first%2520benchmark%2520dataset%2520specifically%250Atailored%2520for%2520multimodal%2520image%2520registration%2520in%2520UAV-based%2520applications.%2520This%250Adataset%2520includes%25207%252C969%2520triplets%2520of%2520raw%2520visible%252C%2520infrared%252C%2520and%2520precisely%250Aregistered%2520visible%2520images%2520captured%2520covers%2520diverse%2520scenarios%2520including%2520flight%250Aaltitudes%2520from%252080m%2520to%2520300m%252C%2520camera%2520angles%2520from%25200%257B%255Cdeg%257D%2520to%252075%257B%255Cdeg%257D%252C%2520and%250Aall-day%252C%2520all-year%2520temporal%2520variations%2520under%2520rich%2520weather%2520and%2520illumination%250Aconditions.%2520To%2520ensure%2520high%2520registration%2520quality%252C%2520we%2520design%2520a%2520semi-automated%250Aannotation%2520pipeline%2520to%2520introduce%2520reliable%2520pixel-level%2520ground%2520truth%2520to%2520each%250Atriplet.%2520In%2520addition%252C%2520each%2520triplet%2520is%2520annotated%2520with%2520six%2520imaging%2520condition%250Aattributes%252C%2520enabling%2520benchmarking%2520of%2520registration%2520robustness%2520under%2520real-world%250Adeployment%2520settings.%2520To%2520further%2520support%2520downstream%2520tasks%252C%2520we%2520provide%250Aobject-level%2520annotations%2520on%2520all%2520registered%2520images%252C%2520covering%252011%2520object%250Acategories%2520with%252077%252C753%2520visible%2520and%252078%252C409%2520infrared%2520bounding%2520boxes.%2520We%2520believe%250AATR-UMMIM%2520will%2520serve%2520as%2520a%2520foundational%2520benchmark%2520for%2520advancing%2520multimodal%250Aregistration%252C%2520fusion%252C%2520and%2520perception%2520in%2520real-world%2520UAV%2520scenarios.%2520The%2520datatset%250Acan%2520be%2520download%2520from%2520https%253A//github.com/supercpy/ATR-UMMIM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATR-UMMIM%3A%20A%20Benchmark%20Dataset%20for%20UAV-Based%20Multimodal%20Image%0A%20%20Registration%20under%20Complex%20Imaging%20Conditions&entry.906535625=Kangcheng%20Bin%20and%20Chen%20Chen%20and%20Ting%20Hu%20and%20Jiahao%20Qi%20and%20Ping%20Zhong&entry.1292438233=%20%20Multimodal%20fusion%20has%20become%20a%20key%20enabler%20for%20UAV-based%20object%20detection%2C%20as%0Aeach%20modality%20provides%20complementary%20cues%20for%20robust%20feature%20extraction.%0AHowever%2C%20due%20to%20significant%20differences%20in%20resolution%2C%20field%20of%20view%2C%20and%0Asensing%20characteristics%20across%20modalities%2C%20accurate%20registration%20is%20a%0Aprerequisite%20before%20fusion.%20Despite%20its%20importance%2C%20there%20is%20currently%20no%0Apublicly%20available%20benchmark%20specifically%20designed%20for%20multimodal%20registration%0Ain%20UAV-based%20aerial%20scenarios%2C%20which%20severely%20limits%20the%20development%20and%0Aevaluation%20of%20advanced%20registration%20methods%20under%20real-world%20conditions.%20To%0Abridge%20this%20gap%2C%20we%20present%20ATR-UMMIM%2C%20the%20first%20benchmark%20dataset%20specifically%0Atailored%20for%20multimodal%20image%20registration%20in%20UAV-based%20applications.%20This%0Adataset%20includes%207%2C969%20triplets%20of%20raw%20visible%2C%20infrared%2C%20and%20precisely%0Aregistered%20visible%20images%20captured%20covers%20diverse%20scenarios%20including%20flight%0Aaltitudes%20from%2080m%20to%20300m%2C%20camera%20angles%20from%200%7B%5Cdeg%7D%20to%2075%7B%5Cdeg%7D%2C%20and%0Aall-day%2C%20all-year%20temporal%20variations%20under%20rich%20weather%20and%20illumination%0Aconditions.%20To%20ensure%20high%20registration%20quality%2C%20we%20design%20a%20semi-automated%0Aannotation%20pipeline%20to%20introduce%20reliable%20pixel-level%20ground%20truth%20to%20each%0Atriplet.%20In%20addition%2C%20each%20triplet%20is%20annotated%20with%20six%20imaging%20condition%0Aattributes%2C%20enabling%20benchmarking%20of%20registration%20robustness%20under%20real-world%0Adeployment%20settings.%20To%20further%20support%20downstream%20tasks%2C%20we%20provide%0Aobject-level%20annotations%20on%20all%20registered%20images%2C%20covering%2011%20object%0Acategories%20with%2077%2C753%20visible%20and%2078%2C409%20infrared%20bounding%20boxes.%20We%20believe%0AATR-UMMIM%20will%20serve%20as%20a%20foundational%20benchmark%20for%20advancing%20multimodal%0Aregistration%2C%20fusion%2C%20and%20perception%20in%20real-world%20UAV%20scenarios.%20The%20datatset%0Acan%20be%20download%20from%20https%3A//github.com/supercpy/ATR-UMMIM%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20764v1&entry.124074799=Read"},
{"title": "A Large Language Model-Supported Threat Modeling Framework for\n  Transportation Cyber-Physical Systems", "author": "M Sabbir Salek and Mashrur Chowdhury and Muhaimin Bin Munir and Yuchen Cai and Mohammad Imtiaz Hasan and Jean-Michel Tine and Latifur Khan and Mizanur Rahman", "abstract": "  Existing threat modeling frameworks related to transportation cyber-physical\nsystems (CPS) are often narrow in scope, labor-intensive, and require\nsubstantial cybersecurity expertise. To this end, we introduce the\nTransportation Cybersecurity and Resiliency Threat Modeling Framework\n(TraCR-TMF), a large language model (LLM)-based threat modeling framework for\ntransportation CPS that requires limited cybersecurity expert intervention.\nTraCR-TMF identifies threats, potential attack techniques, and relevant\ncountermeasures for transportation CPS. Three LLM-based approaches support\nthese identifications: (i) a retrieval-augmented generation approach requiring\nno cybersecurity expert intervention, (ii) an in-context learning approach with\nlow expert intervention, and (iii) a supervised fine-tuning approach with\nmoderate expert intervention. TraCR-TMF offers LLM-based attack path\nidentification for critical assets based on vulnerabilities across\ntransportation CPS entities. Additionally, it incorporates the Common\nVulnerability Scoring System (CVSS) scores of known exploited vulnerabilities\nto prioritize threat mitigations. The framework was evaluated through two\ncases. First, the framework identified relevant attack techniques for various\ntransportation CPS applications, 73% of which were validated by cybersecurity\nexperts as correct. Second, the framework was used to identify attack paths for\na target asset in a real-world cyberattack incident. TraCR-TMF successfully\npredicted exploitations, like lateral movement of adversaries, data\nexfiltration, and data encryption for ransomware, as reported in the incident.\nThese findings show the efficacy of TraCR-TMF in transportation CPS threat\nmodeling, while reducing the need for extensive involvement of cybersecurity\nexperts. To facilitate real-world adoptions, all our codes are shared via an\nopen-source repository.\n", "link": "http://arxiv.org/abs/2506.00831v2", "date": "2025-07-28", "relevancy": 2.2115, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4578}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large%20Language%20Model-Supported%20Threat%20Modeling%20Framework%20for%0A%20%20Transportation%20Cyber-Physical%20Systems&body=Title%3A%20A%20Large%20Language%20Model-Supported%20Threat%20Modeling%20Framework%20for%0A%20%20Transportation%20Cyber-Physical%20Systems%0AAuthor%3A%20M%20Sabbir%20Salek%20and%20Mashrur%20Chowdhury%20and%20Muhaimin%20Bin%20Munir%20and%20Yuchen%20Cai%20and%20Mohammad%20Imtiaz%20Hasan%20and%20Jean-Michel%20Tine%20and%20Latifur%20Khan%20and%20Mizanur%20Rahman%0AAbstract%3A%20%20%20Existing%20threat%20modeling%20frameworks%20related%20to%20transportation%20cyber-physical%0Asystems%20%28CPS%29%20are%20often%20narrow%20in%20scope%2C%20labor-intensive%2C%20and%20require%0Asubstantial%20cybersecurity%20expertise.%20To%20this%20end%2C%20we%20introduce%20the%0ATransportation%20Cybersecurity%20and%20Resiliency%20Threat%20Modeling%20Framework%0A%28TraCR-TMF%29%2C%20a%20large%20language%20model%20%28LLM%29-based%20threat%20modeling%20framework%20for%0Atransportation%20CPS%20that%20requires%20limited%20cybersecurity%20expert%20intervention.%0ATraCR-TMF%20identifies%20threats%2C%20potential%20attack%20techniques%2C%20and%20relevant%0Acountermeasures%20for%20transportation%20CPS.%20Three%20LLM-based%20approaches%20support%0Athese%20identifications%3A%20%28i%29%20a%20retrieval-augmented%20generation%20approach%20requiring%0Ano%20cybersecurity%20expert%20intervention%2C%20%28ii%29%20an%20in-context%20learning%20approach%20with%0Alow%20expert%20intervention%2C%20and%20%28iii%29%20a%20supervised%20fine-tuning%20approach%20with%0Amoderate%20expert%20intervention.%20TraCR-TMF%20offers%20LLM-based%20attack%20path%0Aidentification%20for%20critical%20assets%20based%20on%20vulnerabilities%20across%0Atransportation%20CPS%20entities.%20Additionally%2C%20it%20incorporates%20the%20Common%0AVulnerability%20Scoring%20System%20%28CVSS%29%20scores%20of%20known%20exploited%20vulnerabilities%0Ato%20prioritize%20threat%20mitigations.%20The%20framework%20was%20evaluated%20through%20two%0Acases.%20First%2C%20the%20framework%20identified%20relevant%20attack%20techniques%20for%20various%0Atransportation%20CPS%20applications%2C%2073%25%20of%20which%20were%20validated%20by%20cybersecurity%0Aexperts%20as%20correct.%20Second%2C%20the%20framework%20was%20used%20to%20identify%20attack%20paths%20for%0Aa%20target%20asset%20in%20a%20real-world%20cyberattack%20incident.%20TraCR-TMF%20successfully%0Apredicted%20exploitations%2C%20like%20lateral%20movement%20of%20adversaries%2C%20data%0Aexfiltration%2C%20and%20data%20encryption%20for%20ransomware%2C%20as%20reported%20in%20the%20incident.%0AThese%20findings%20show%20the%20efficacy%20of%20TraCR-TMF%20in%20transportation%20CPS%20threat%0Amodeling%2C%20while%20reducing%20the%20need%20for%20extensive%20involvement%20of%20cybersecurity%0Aexperts.%20To%20facilitate%20real-world%20adoptions%2C%20all%20our%20codes%20are%20shared%20via%20an%0Aopen-source%20repository.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large%2520Language%2520Model-Supported%2520Threat%2520Modeling%2520Framework%2520for%250A%2520%2520Transportation%2520Cyber-Physical%2520Systems%26entry.906535625%3DM%2520Sabbir%2520Salek%2520and%2520Mashrur%2520Chowdhury%2520and%2520Muhaimin%2520Bin%2520Munir%2520and%2520Yuchen%2520Cai%2520and%2520Mohammad%2520Imtiaz%2520Hasan%2520and%2520Jean-Michel%2520Tine%2520and%2520Latifur%2520Khan%2520and%2520Mizanur%2520Rahman%26entry.1292438233%3D%2520%2520Existing%2520threat%2520modeling%2520frameworks%2520related%2520to%2520transportation%2520cyber-physical%250Asystems%2520%2528CPS%2529%2520are%2520often%2520narrow%2520in%2520scope%252C%2520labor-intensive%252C%2520and%2520require%250Asubstantial%2520cybersecurity%2520expertise.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%250ATransportation%2520Cybersecurity%2520and%2520Resiliency%2520Threat%2520Modeling%2520Framework%250A%2528TraCR-TMF%2529%252C%2520a%2520large%2520language%2520model%2520%2528LLM%2529-based%2520threat%2520modeling%2520framework%2520for%250Atransportation%2520CPS%2520that%2520requires%2520limited%2520cybersecurity%2520expert%2520intervention.%250ATraCR-TMF%2520identifies%2520threats%252C%2520potential%2520attack%2520techniques%252C%2520and%2520relevant%250Acountermeasures%2520for%2520transportation%2520CPS.%2520Three%2520LLM-based%2520approaches%2520support%250Athese%2520identifications%253A%2520%2528i%2529%2520a%2520retrieval-augmented%2520generation%2520approach%2520requiring%250Ano%2520cybersecurity%2520expert%2520intervention%252C%2520%2528ii%2529%2520an%2520in-context%2520learning%2520approach%2520with%250Alow%2520expert%2520intervention%252C%2520and%2520%2528iii%2529%2520a%2520supervised%2520fine-tuning%2520approach%2520with%250Amoderate%2520expert%2520intervention.%2520TraCR-TMF%2520offers%2520LLM-based%2520attack%2520path%250Aidentification%2520for%2520critical%2520assets%2520based%2520on%2520vulnerabilities%2520across%250Atransportation%2520CPS%2520entities.%2520Additionally%252C%2520it%2520incorporates%2520the%2520Common%250AVulnerability%2520Scoring%2520System%2520%2528CVSS%2529%2520scores%2520of%2520known%2520exploited%2520vulnerabilities%250Ato%2520prioritize%2520threat%2520mitigations.%2520The%2520framework%2520was%2520evaluated%2520through%2520two%250Acases.%2520First%252C%2520the%2520framework%2520identified%2520relevant%2520attack%2520techniques%2520for%2520various%250Atransportation%2520CPS%2520applications%252C%252073%2525%2520of%2520which%2520were%2520validated%2520by%2520cybersecurity%250Aexperts%2520as%2520correct.%2520Second%252C%2520the%2520framework%2520was%2520used%2520to%2520identify%2520attack%2520paths%2520for%250Aa%2520target%2520asset%2520in%2520a%2520real-world%2520cyberattack%2520incident.%2520TraCR-TMF%2520successfully%250Apredicted%2520exploitations%252C%2520like%2520lateral%2520movement%2520of%2520adversaries%252C%2520data%250Aexfiltration%252C%2520and%2520data%2520encryption%2520for%2520ransomware%252C%2520as%2520reported%2520in%2520the%2520incident.%250AThese%2520findings%2520show%2520the%2520efficacy%2520of%2520TraCR-TMF%2520in%2520transportation%2520CPS%2520threat%250Amodeling%252C%2520while%2520reducing%2520the%2520need%2520for%2520extensive%2520involvement%2520of%2520cybersecurity%250Aexperts.%2520To%2520facilitate%2520real-world%2520adoptions%252C%2520all%2520our%2520codes%2520are%2520shared%2520via%2520an%250Aopen-source%2520repository.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large%20Language%20Model-Supported%20Threat%20Modeling%20Framework%20for%0A%20%20Transportation%20Cyber-Physical%20Systems&entry.906535625=M%20Sabbir%20Salek%20and%20Mashrur%20Chowdhury%20and%20Muhaimin%20Bin%20Munir%20and%20Yuchen%20Cai%20and%20Mohammad%20Imtiaz%20Hasan%20and%20Jean-Michel%20Tine%20and%20Latifur%20Khan%20and%20Mizanur%20Rahman&entry.1292438233=%20%20Existing%20threat%20modeling%20frameworks%20related%20to%20transportation%20cyber-physical%0Asystems%20%28CPS%29%20are%20often%20narrow%20in%20scope%2C%20labor-intensive%2C%20and%20require%0Asubstantial%20cybersecurity%20expertise.%20To%20this%20end%2C%20we%20introduce%20the%0ATransportation%20Cybersecurity%20and%20Resiliency%20Threat%20Modeling%20Framework%0A%28TraCR-TMF%29%2C%20a%20large%20language%20model%20%28LLM%29-based%20threat%20modeling%20framework%20for%0Atransportation%20CPS%20that%20requires%20limited%20cybersecurity%20expert%20intervention.%0ATraCR-TMF%20identifies%20threats%2C%20potential%20attack%20techniques%2C%20and%20relevant%0Acountermeasures%20for%20transportation%20CPS.%20Three%20LLM-based%20approaches%20support%0Athese%20identifications%3A%20%28i%29%20a%20retrieval-augmented%20generation%20approach%20requiring%0Ano%20cybersecurity%20expert%20intervention%2C%20%28ii%29%20an%20in-context%20learning%20approach%20with%0Alow%20expert%20intervention%2C%20and%20%28iii%29%20a%20supervised%20fine-tuning%20approach%20with%0Amoderate%20expert%20intervention.%20TraCR-TMF%20offers%20LLM-based%20attack%20path%0Aidentification%20for%20critical%20assets%20based%20on%20vulnerabilities%20across%0Atransportation%20CPS%20entities.%20Additionally%2C%20it%20incorporates%20the%20Common%0AVulnerability%20Scoring%20System%20%28CVSS%29%20scores%20of%20known%20exploited%20vulnerabilities%0Ato%20prioritize%20threat%20mitigations.%20The%20framework%20was%20evaluated%20through%20two%0Acases.%20First%2C%20the%20framework%20identified%20relevant%20attack%20techniques%20for%20various%0Atransportation%20CPS%20applications%2C%2073%25%20of%20which%20were%20validated%20by%20cybersecurity%0Aexperts%20as%20correct.%20Second%2C%20the%20framework%20was%20used%20to%20identify%20attack%20paths%20for%0Aa%20target%20asset%20in%20a%20real-world%20cyberattack%20incident.%20TraCR-TMF%20successfully%0Apredicted%20exploitations%2C%20like%20lateral%20movement%20of%20adversaries%2C%20data%0Aexfiltration%2C%20and%20data%20encryption%20for%20ransomware%2C%20as%20reported%20in%20the%20incident.%0AThese%20findings%20show%20the%20efficacy%20of%20TraCR-TMF%20in%20transportation%20CPS%20threat%0Amodeling%2C%20while%20reducing%20the%20need%20for%20extensive%20involvement%20of%20cybersecurity%0Aexperts.%20To%20facilitate%20real-world%20adoptions%2C%20all%20our%20codes%20are%20shared%20via%20an%0Aopen-source%20repository.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00831v2&entry.124074799=Read"},
{"title": "Model-Agnostic Gender Bias Control for Text-to-Image Generation via\n  Sparse Autoencoder", "author": "Chao Wu and Zhenyi Wang and Kangxian Xie and Naresh Kumar Devulapally and Vishnu Suresh Lokhande and Mingchen Gao", "abstract": "  Text-to-image (T2I) diffusion models often exhibit gender bias, particularly\nby generating stereotypical associations between professions and gendered\nsubjects. This paper presents SAE Debias, a lightweight and model-agnostic\nframework for mitigating such bias in T2I generation. Unlike prior approaches\nthat rely on CLIP-based filtering or prompt engineering, which often require\nmodel-specific adjustments and offer limited control, SAE Debias operates\ndirectly within the feature space without retraining or architectural\nmodifications. By leveraging a k-sparse autoencoder pre-trained on a gender\nbias dataset, the method identifies gender-relevant directions within the\nsparse latent space, capturing professional stereotypes. Specifically, a biased\ndirection per profession is constructed from sparse latents and suppressed\nduring inference to steer generations toward more gender-balanced outputs.\nTrained only once, the sparse autoencoder provides a reusable debiasing\ndirection, offering effective control and interpretable insight into biased\nsubspaces. Extensive evaluations across multiple T2I models, including Stable\nDiffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially\nreduces gender bias while preserving generation quality. To the best of our\nknowledge, this is the first work to apply sparse autoencoders for identifying\nand intervening in gender bias within T2I models. These findings contribute\ntoward building socially responsible generative AI, providing an interpretable\nand model-agnostic tool to support fairness in text-to-image generation.\n", "link": "http://arxiv.org/abs/2507.20973v1", "date": "2025-07-28", "relevancy": 2.1972, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5597}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.544}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Agnostic%20Gender%20Bias%20Control%20for%20Text-to-Image%20Generation%20via%0A%20%20Sparse%20Autoencoder&body=Title%3A%20Model-Agnostic%20Gender%20Bias%20Control%20for%20Text-to-Image%20Generation%20via%0A%20%20Sparse%20Autoencoder%0AAuthor%3A%20Chao%20Wu%20and%20Zhenyi%20Wang%20and%20Kangxian%20Xie%20and%20Naresh%20Kumar%20Devulapally%20and%20Vishnu%20Suresh%20Lokhande%20and%20Mingchen%20Gao%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20often%20exhibit%20gender%20bias%2C%20particularly%0Aby%20generating%20stereotypical%20associations%20between%20professions%20and%20gendered%0Asubjects.%20This%20paper%20presents%20SAE%20Debias%2C%20a%20lightweight%20and%20model-agnostic%0Aframework%20for%20mitigating%20such%20bias%20in%20T2I%20generation.%20Unlike%20prior%20approaches%0Athat%20rely%20on%20CLIP-based%20filtering%20or%20prompt%20engineering%2C%20which%20often%20require%0Amodel-specific%20adjustments%20and%20offer%20limited%20control%2C%20SAE%20Debias%20operates%0Adirectly%20within%20the%20feature%20space%20without%20retraining%20or%20architectural%0Amodifications.%20By%20leveraging%20a%20k-sparse%20autoencoder%20pre-trained%20on%20a%20gender%0Abias%20dataset%2C%20the%20method%20identifies%20gender-relevant%20directions%20within%20the%0Asparse%20latent%20space%2C%20capturing%20professional%20stereotypes.%20Specifically%2C%20a%20biased%0Adirection%20per%20profession%20is%20constructed%20from%20sparse%20latents%20and%20suppressed%0Aduring%20inference%20to%20steer%20generations%20toward%20more%20gender-balanced%20outputs.%0ATrained%20only%20once%2C%20the%20sparse%20autoencoder%20provides%20a%20reusable%20debiasing%0Adirection%2C%20offering%20effective%20control%20and%20interpretable%20insight%20into%20biased%0Asubspaces.%20Extensive%20evaluations%20across%20multiple%20T2I%20models%2C%20including%20Stable%0ADiffusion%201.4%2C%201.5%2C%202.1%2C%20and%20SDXL%2C%20demonstrate%20that%20SAE%20Debias%20substantially%0Areduces%20gender%20bias%20while%20preserving%20generation%20quality.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20sparse%20autoencoders%20for%20identifying%0Aand%20intervening%20in%20gender%20bias%20within%20T2I%20models.%20These%20findings%20contribute%0Atoward%20building%20socially%20responsible%20generative%20AI%2C%20providing%20an%20interpretable%0Aand%20model-agnostic%20tool%20to%20support%20fairness%20in%20text-to-image%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Agnostic%2520Gender%2520Bias%2520Control%2520for%2520Text-to-Image%2520Generation%2520via%250A%2520%2520Sparse%2520Autoencoder%26entry.906535625%3DChao%2520Wu%2520and%2520Zhenyi%2520Wang%2520and%2520Kangxian%2520Xie%2520and%2520Naresh%2520Kumar%2520Devulapally%2520and%2520Vishnu%2520Suresh%2520Lokhande%2520and%2520Mingchen%2520Gao%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520often%2520exhibit%2520gender%2520bias%252C%2520particularly%250Aby%2520generating%2520stereotypical%2520associations%2520between%2520professions%2520and%2520gendered%250Asubjects.%2520This%2520paper%2520presents%2520SAE%2520Debias%252C%2520a%2520lightweight%2520and%2520model-agnostic%250Aframework%2520for%2520mitigating%2520such%2520bias%2520in%2520T2I%2520generation.%2520Unlike%2520prior%2520approaches%250Athat%2520rely%2520on%2520CLIP-based%2520filtering%2520or%2520prompt%2520engineering%252C%2520which%2520often%2520require%250Amodel-specific%2520adjustments%2520and%2520offer%2520limited%2520control%252C%2520SAE%2520Debias%2520operates%250Adirectly%2520within%2520the%2520feature%2520space%2520without%2520retraining%2520or%2520architectural%250Amodifications.%2520By%2520leveraging%2520a%2520k-sparse%2520autoencoder%2520pre-trained%2520on%2520a%2520gender%250Abias%2520dataset%252C%2520the%2520method%2520identifies%2520gender-relevant%2520directions%2520within%2520the%250Asparse%2520latent%2520space%252C%2520capturing%2520professional%2520stereotypes.%2520Specifically%252C%2520a%2520biased%250Adirection%2520per%2520profession%2520is%2520constructed%2520from%2520sparse%2520latents%2520and%2520suppressed%250Aduring%2520inference%2520to%2520steer%2520generations%2520toward%2520more%2520gender-balanced%2520outputs.%250ATrained%2520only%2520once%252C%2520the%2520sparse%2520autoencoder%2520provides%2520a%2520reusable%2520debiasing%250Adirection%252C%2520offering%2520effective%2520control%2520and%2520interpretable%2520insight%2520into%2520biased%250Asubspaces.%2520Extensive%2520evaluations%2520across%2520multiple%2520T2I%2520models%252C%2520including%2520Stable%250ADiffusion%25201.4%252C%25201.5%252C%25202.1%252C%2520and%2520SDXL%252C%2520demonstrate%2520that%2520SAE%2520Debias%2520substantially%250Areduces%2520gender%2520bias%2520while%2520preserving%2520generation%2520quality.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520work%2520to%2520apply%2520sparse%2520autoencoders%2520for%2520identifying%250Aand%2520intervening%2520in%2520gender%2520bias%2520within%2520T2I%2520models.%2520These%2520findings%2520contribute%250Atoward%2520building%2520socially%2520responsible%2520generative%2520AI%252C%2520providing%2520an%2520interpretable%250Aand%2520model-agnostic%2520tool%2520to%2520support%2520fairness%2520in%2520text-to-image%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Agnostic%20Gender%20Bias%20Control%20for%20Text-to-Image%20Generation%20via%0A%20%20Sparse%20Autoencoder&entry.906535625=Chao%20Wu%20and%20Zhenyi%20Wang%20and%20Kangxian%20Xie%20and%20Naresh%20Kumar%20Devulapally%20and%20Vishnu%20Suresh%20Lokhande%20and%20Mingchen%20Gao&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20often%20exhibit%20gender%20bias%2C%20particularly%0Aby%20generating%20stereotypical%20associations%20between%20professions%20and%20gendered%0Asubjects.%20This%20paper%20presents%20SAE%20Debias%2C%20a%20lightweight%20and%20model-agnostic%0Aframework%20for%20mitigating%20such%20bias%20in%20T2I%20generation.%20Unlike%20prior%20approaches%0Athat%20rely%20on%20CLIP-based%20filtering%20or%20prompt%20engineering%2C%20which%20often%20require%0Amodel-specific%20adjustments%20and%20offer%20limited%20control%2C%20SAE%20Debias%20operates%0Adirectly%20within%20the%20feature%20space%20without%20retraining%20or%20architectural%0Amodifications.%20By%20leveraging%20a%20k-sparse%20autoencoder%20pre-trained%20on%20a%20gender%0Abias%20dataset%2C%20the%20method%20identifies%20gender-relevant%20directions%20within%20the%0Asparse%20latent%20space%2C%20capturing%20professional%20stereotypes.%20Specifically%2C%20a%20biased%0Adirection%20per%20profession%20is%20constructed%20from%20sparse%20latents%20and%20suppressed%0Aduring%20inference%20to%20steer%20generations%20toward%20more%20gender-balanced%20outputs.%0ATrained%20only%20once%2C%20the%20sparse%20autoencoder%20provides%20a%20reusable%20debiasing%0Adirection%2C%20offering%20effective%20control%20and%20interpretable%20insight%20into%20biased%0Asubspaces.%20Extensive%20evaluations%20across%20multiple%20T2I%20models%2C%20including%20Stable%0ADiffusion%201.4%2C%201.5%2C%202.1%2C%20and%20SDXL%2C%20demonstrate%20that%20SAE%20Debias%20substantially%0Areduces%20gender%20bias%20while%20preserving%20generation%20quality.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20work%20to%20apply%20sparse%20autoencoders%20for%20identifying%0Aand%20intervening%20in%20gender%20bias%20within%20T2I%20models.%20These%20findings%20contribute%0Atoward%20building%20socially%20responsible%20generative%20AI%2C%20providing%20an%20interpretable%0Aand%20model-agnostic%20tool%20to%20support%20fairness%20in%20text-to-image%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20973v1&entry.124074799=Read"},
{"title": "ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for\n  Visual Question Answering", "author": "Duong T. Tran and Trung-Kien Tran and Manfred Hauswirth and Danh Le Phuoc", "abstract": "  In this paper, we propose a new dataset, ReasonVQA, for the Visual Question\nAnswering (VQA) task. Our dataset is automatically integrated with structured\nencyclopedic knowledge and constructed using a low-cost framework, which is\ncapable of generating complex, multi-hop questions. We evaluated\nstate-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate\nthat ReasonVQA poses significant challenges to these models, highlighting its\npotential for benchmarking and advancing the field of VQA. Additionally, our\ndataset can be easily scaled with respect to input images; the current version\nsurpasses the largest existing datasets requiring external knowledge by more\nthan an order of magnitude.\n", "link": "http://arxiv.org/abs/2507.16403v2", "date": "2025-07-28", "relevancy": 2.1941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5562}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasonVQA%3A%20A%20Multi-hop%20Reasoning%20Benchmark%20with%20Structural%20Knowledge%20for%0A%20%20Visual%20Question%20Answering&body=Title%3A%20ReasonVQA%3A%20A%20Multi-hop%20Reasoning%20Benchmark%20with%20Structural%20Knowledge%20for%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Duong%20T.%20Tran%20and%20Trung-Kien%20Tran%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20dataset%2C%20ReasonVQA%2C%20for%20the%20Visual%20Question%0AAnswering%20%28VQA%29%20task.%20Our%20dataset%20is%20automatically%20integrated%20with%20structured%0Aencyclopedic%20knowledge%20and%20constructed%20using%20a%20low-cost%20framework%2C%20which%20is%0Acapable%20of%20generating%20complex%2C%20multi-hop%20questions.%20We%20evaluated%0Astate-of-the-art%20VQA%20models%20on%20ReasonVQA%2C%20and%20the%20empirical%20results%20demonstrate%0Athat%20ReasonVQA%20poses%20significant%20challenges%20to%20these%20models%2C%20highlighting%20its%0Apotential%20for%20benchmarking%20and%20advancing%20the%20field%20of%20VQA.%20Additionally%2C%20our%0Adataset%20can%20be%20easily%20scaled%20with%20respect%20to%20input%20images%3B%20the%20current%20version%0Asurpasses%20the%20largest%20existing%20datasets%20requiring%20external%20knowledge%20by%20more%0Athan%20an%20order%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasonVQA%253A%2520A%2520Multi-hop%2520Reasoning%2520Benchmark%2520with%2520Structural%2520Knowledge%2520for%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DDuong%2520T.%2520Tran%2520and%2520Trung-Kien%2520Tran%2520and%2520Manfred%2520Hauswirth%2520and%2520Danh%2520Le%2520Phuoc%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520dataset%252C%2520ReasonVQA%252C%2520for%2520the%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520task.%2520Our%2520dataset%2520is%2520automatically%2520integrated%2520with%2520structured%250Aencyclopedic%2520knowledge%2520and%2520constructed%2520using%2520a%2520low-cost%2520framework%252C%2520which%2520is%250Acapable%2520of%2520generating%2520complex%252C%2520multi-hop%2520questions.%2520We%2520evaluated%250Astate-of-the-art%2520VQA%2520models%2520on%2520ReasonVQA%252C%2520and%2520the%2520empirical%2520results%2520demonstrate%250Athat%2520ReasonVQA%2520poses%2520significant%2520challenges%2520to%2520these%2520models%252C%2520highlighting%2520its%250Apotential%2520for%2520benchmarking%2520and%2520advancing%2520the%2520field%2520of%2520VQA.%2520Additionally%252C%2520our%250Adataset%2520can%2520be%2520easily%2520scaled%2520with%2520respect%2520to%2520input%2520images%253B%2520the%2520current%2520version%250Asurpasses%2520the%2520largest%2520existing%2520datasets%2520requiring%2520external%2520knowledge%2520by%2520more%250Athan%2520an%2520order%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasonVQA%3A%20A%20Multi-hop%20Reasoning%20Benchmark%20with%20Structural%20Knowledge%20for%0A%20%20Visual%20Question%20Answering&entry.906535625=Duong%20T.%20Tran%20and%20Trung-Kien%20Tran%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20dataset%2C%20ReasonVQA%2C%20for%20the%20Visual%20Question%0AAnswering%20%28VQA%29%20task.%20Our%20dataset%20is%20automatically%20integrated%20with%20structured%0Aencyclopedic%20knowledge%20and%20constructed%20using%20a%20low-cost%20framework%2C%20which%20is%0Acapable%20of%20generating%20complex%2C%20multi-hop%20questions.%20We%20evaluated%0Astate-of-the-art%20VQA%20models%20on%20ReasonVQA%2C%20and%20the%20empirical%20results%20demonstrate%0Athat%20ReasonVQA%20poses%20significant%20challenges%20to%20these%20models%2C%20highlighting%20its%0Apotential%20for%20benchmarking%20and%20advancing%20the%20field%20of%20VQA.%20Additionally%2C%20our%0Adataset%20can%20be%20easily%20scaled%20with%20respect%20to%20input%20images%3B%20the%20current%20version%0Asurpasses%20the%20largest%20existing%20datasets%20requiring%20external%20knowledge%20by%20more%0Athan%20an%20order%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16403v2&entry.124074799=Read"},
{"title": "Endoscopic Depth Estimation Based on Deep Learning: A Survey", "author": "Ke Niu and Zeyun Liu and Xue Feng and Heng Li and Kaize Shi", "abstract": "  Endoscopic depth estimation is a critical technology for improving the safety\nand precision of minimally invasive surgery. It has attracted considerable\nattention from researchers in medical imaging, computer vision, and robotics.\nOver the past decade, a large number of methods have been developed. Despite\nthe existence of several related surveys, a comprehensive overview focusing on\nrecent deep learning-based techniques is still limited. This paper endeavors to\nbridge this gap by systematically reviewing the state-of-the-art literature.\nSpecifically, we provide a thorough survey of the field from three key\nperspectives: data, methods, and applications, covering a range of methods\nincluding both monocular and stereo approaches. We describe common performance\nevaluation metrics and summarize publicly available datasets. Furthermore, this\nreview analyzes the specific challenges of endoscopic scenes and categorizes\nrepresentative techniques based on their supervision strategies and network\narchitectures. The application of endoscopic depth estimation in the important\narea of robot-assisted surgery is also reviewed. Finally, we outline potential\ndirections for future research, such as domain adaptation, real-time\nimplementation, and enhanced model generalization, thereby providing a valuable\nstarting point for researchers to engage with and advance the field.\n", "link": "http://arxiv.org/abs/2507.20881v1", "date": "2025-07-28", "relevancy": 2.1792, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Endoscopic%20Depth%20Estimation%20Based%20on%20Deep%20Learning%3A%20A%20Survey&body=Title%3A%20Endoscopic%20Depth%20Estimation%20Based%20on%20Deep%20Learning%3A%20A%20Survey%0AAuthor%3A%20Ke%20Niu%20and%20Zeyun%20Liu%20and%20Xue%20Feng%20and%20Heng%20Li%20and%20Kaize%20Shi%0AAbstract%3A%20%20%20Endoscopic%20depth%20estimation%20is%20a%20critical%20technology%20for%20improving%20the%20safety%0Aand%20precision%20of%20minimally%20invasive%20surgery.%20It%20has%20attracted%20considerable%0Aattention%20from%20researchers%20in%20medical%20imaging%2C%20computer%20vision%2C%20and%20robotics.%0AOver%20the%20past%20decade%2C%20a%20large%20number%20of%20methods%20have%20been%20developed.%20Despite%0Athe%20existence%20of%20several%20related%20surveys%2C%20a%20comprehensive%20overview%20focusing%20on%0Arecent%20deep%20learning-based%20techniques%20is%20still%20limited.%20This%20paper%20endeavors%20to%0Abridge%20this%20gap%20by%20systematically%20reviewing%20the%20state-of-the-art%20literature.%0ASpecifically%2C%20we%20provide%20a%20thorough%20survey%20of%20the%20field%20from%20three%20key%0Aperspectives%3A%20data%2C%20methods%2C%20and%20applications%2C%20covering%20a%20range%20of%20methods%0Aincluding%20both%20monocular%20and%20stereo%20approaches.%20We%20describe%20common%20performance%0Aevaluation%20metrics%20and%20summarize%20publicly%20available%20datasets.%20Furthermore%2C%20this%0Areview%20analyzes%20the%20specific%20challenges%20of%20endoscopic%20scenes%20and%20categorizes%0Arepresentative%20techniques%20based%20on%20their%20supervision%20strategies%20and%20network%0Aarchitectures.%20The%20application%20of%20endoscopic%20depth%20estimation%20in%20the%20important%0Aarea%20of%20robot-assisted%20surgery%20is%20also%20reviewed.%20Finally%2C%20we%20outline%20potential%0Adirections%20for%20future%20research%2C%20such%20as%20domain%20adaptation%2C%20real-time%0Aimplementation%2C%20and%20enhanced%20model%20generalization%2C%20thereby%20providing%20a%20valuable%0Astarting%20point%20for%20researchers%20to%20engage%20with%20and%20advance%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEndoscopic%2520Depth%2520Estimation%2520Based%2520on%2520Deep%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DKe%2520Niu%2520and%2520Zeyun%2520Liu%2520and%2520Xue%2520Feng%2520and%2520Heng%2520Li%2520and%2520Kaize%2520Shi%26entry.1292438233%3D%2520%2520Endoscopic%2520depth%2520estimation%2520is%2520a%2520critical%2520technology%2520for%2520improving%2520the%2520safety%250Aand%2520precision%2520of%2520minimally%2520invasive%2520surgery.%2520It%2520has%2520attracted%2520considerable%250Aattention%2520from%2520researchers%2520in%2520medical%2520imaging%252C%2520computer%2520vision%252C%2520and%2520robotics.%250AOver%2520the%2520past%2520decade%252C%2520a%2520large%2520number%2520of%2520methods%2520have%2520been%2520developed.%2520Despite%250Athe%2520existence%2520of%2520several%2520related%2520surveys%252C%2520a%2520comprehensive%2520overview%2520focusing%2520on%250Arecent%2520deep%2520learning-based%2520techniques%2520is%2520still%2520limited.%2520This%2520paper%2520endeavors%2520to%250Abridge%2520this%2520gap%2520by%2520systematically%2520reviewing%2520the%2520state-of-the-art%2520literature.%250ASpecifically%252C%2520we%2520provide%2520a%2520thorough%2520survey%2520of%2520the%2520field%2520from%2520three%2520key%250Aperspectives%253A%2520data%252C%2520methods%252C%2520and%2520applications%252C%2520covering%2520a%2520range%2520of%2520methods%250Aincluding%2520both%2520monocular%2520and%2520stereo%2520approaches.%2520We%2520describe%2520common%2520performance%250Aevaluation%2520metrics%2520and%2520summarize%2520publicly%2520available%2520datasets.%2520Furthermore%252C%2520this%250Areview%2520analyzes%2520the%2520specific%2520challenges%2520of%2520endoscopic%2520scenes%2520and%2520categorizes%250Arepresentative%2520techniques%2520based%2520on%2520their%2520supervision%2520strategies%2520and%2520network%250Aarchitectures.%2520The%2520application%2520of%2520endoscopic%2520depth%2520estimation%2520in%2520the%2520important%250Aarea%2520of%2520robot-assisted%2520surgery%2520is%2520also%2520reviewed.%2520Finally%252C%2520we%2520outline%2520potential%250Adirections%2520for%2520future%2520research%252C%2520such%2520as%2520domain%2520adaptation%252C%2520real-time%250Aimplementation%252C%2520and%2520enhanced%2520model%2520generalization%252C%2520thereby%2520providing%2520a%2520valuable%250Astarting%2520point%2520for%2520researchers%2520to%2520engage%2520with%2520and%2520advance%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Endoscopic%20Depth%20Estimation%20Based%20on%20Deep%20Learning%3A%20A%20Survey&entry.906535625=Ke%20Niu%20and%20Zeyun%20Liu%20and%20Xue%20Feng%20and%20Heng%20Li%20and%20Kaize%20Shi&entry.1292438233=%20%20Endoscopic%20depth%20estimation%20is%20a%20critical%20technology%20for%20improving%20the%20safety%0Aand%20precision%20of%20minimally%20invasive%20surgery.%20It%20has%20attracted%20considerable%0Aattention%20from%20researchers%20in%20medical%20imaging%2C%20computer%20vision%2C%20and%20robotics.%0AOver%20the%20past%20decade%2C%20a%20large%20number%20of%20methods%20have%20been%20developed.%20Despite%0Athe%20existence%20of%20several%20related%20surveys%2C%20a%20comprehensive%20overview%20focusing%20on%0Arecent%20deep%20learning-based%20techniques%20is%20still%20limited.%20This%20paper%20endeavors%20to%0Abridge%20this%20gap%20by%20systematically%20reviewing%20the%20state-of-the-art%20literature.%0ASpecifically%2C%20we%20provide%20a%20thorough%20survey%20of%20the%20field%20from%20three%20key%0Aperspectives%3A%20data%2C%20methods%2C%20and%20applications%2C%20covering%20a%20range%20of%20methods%0Aincluding%20both%20monocular%20and%20stereo%20approaches.%20We%20describe%20common%20performance%0Aevaluation%20metrics%20and%20summarize%20publicly%20available%20datasets.%20Furthermore%2C%20this%0Areview%20analyzes%20the%20specific%20challenges%20of%20endoscopic%20scenes%20and%20categorizes%0Arepresentative%20techniques%20based%20on%20their%20supervision%20strategies%20and%20network%0Aarchitectures.%20The%20application%20of%20endoscopic%20depth%20estimation%20in%20the%20important%0Aarea%20of%20robot-assisted%20surgery%20is%20also%20reviewed.%20Finally%2C%20we%20outline%20potential%0Adirections%20for%20future%20research%2C%20such%20as%20domain%20adaptation%2C%20real-time%0Aimplementation%2C%20and%20enhanced%20model%20generalization%2C%20thereby%20providing%20a%20valuable%0Astarting%20point%20for%20researchers%20to%20engage%20with%20and%20advance%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20881v1&entry.124074799=Read"},
{"title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind", "author": "Zheng Zhang and Nuoqian Xiao and Qi Chai and Deheng Ye and Hao Wang", "abstract": "  Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.\n", "link": "http://arxiv.org/abs/2504.18039v3", "date": "2025-07-28", "relevancy": 2.1765, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiMind%3A%20Enhancing%20Werewolf%20Agents%20with%20Multimodal%20Reasoning%20and%0A%20%20Theory%20of%20Mind&body=Title%3A%20MultiMind%3A%20Enhancing%20Werewolf%20Agents%20with%20Multimodal%20Reasoning%20and%0A%20%20Theory%20of%20Mind%0AAuthor%3A%20Zheng%20Zhang%20and%20Nuoqian%20Xiao%20and%20Qi%20Chai%20and%20Deheng%20Ye%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20demonstrated%20impressive%20capabilities%0Ain%20social%20deduction%20games%20%28SDGs%29%20like%20Werewolf%2C%20where%20strategic%20reasoning%20and%0Asocial%20deception%20are%20essential.%20However%2C%20current%20approaches%20remain%20limited%20to%0Atextual%20information%2C%20ignoring%20crucial%20multimodal%20cues%20such%20as%20facial%0Aexpressions%20and%20tone%20of%20voice%20that%20humans%20naturally%20use%20to%20communicate.%0AMoreover%2C%20existing%20SDG%20agents%20primarily%20focus%20on%20inferring%20other%20players%27%0Aidentities%20without%20modeling%20how%20others%20perceive%20themselves%20or%20fellow%20players.%0ATo%20address%20these%20limitations%2C%20we%20use%20One%20Night%20Ultimate%20Werewolf%20%28ONUW%29%20as%20a%0Atestbed%20and%20present%20MultiMind%2C%20the%20first%20framework%20integrating%20multimodal%0Ainformation%20into%20SDG%20agents.%20MultiMind%20processes%20facial%20expressions%20and%20vocal%0Atones%20alongside%20verbal%20content%2C%20while%20employing%20a%20Theory%20of%20Mind%20%28ToM%29%20model%20to%0Arepresent%20each%20player%27s%20suspicion%20levels%20toward%20others.%20By%20combining%20this%20ToM%0Amodel%20with%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20our%20agent%20identifies%20communication%0Astrategies%20that%20minimize%20suspicion%20directed%20at%20itself.%20Through%20comprehensive%0Aevaluation%20in%20both%20agent-versus-agent%20simulations%20and%20studies%20with%20human%0Aplayers%2C%20we%20demonstrate%20MultiMind%27s%20superior%20performance%20in%20gameplay.%20Our%20work%0Apresents%20a%20significant%20advancement%20toward%20LLM%20agents%20capable%20of%20human-like%0Asocial%20reasoning%20across%20multimodal%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18039v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiMind%253A%2520Enhancing%2520Werewolf%2520Agents%2520with%2520Multimodal%2520Reasoning%2520and%250A%2520%2520Theory%2520of%2520Mind%26entry.906535625%3DZheng%2520Zhang%2520and%2520Nuoqian%2520Xiao%2520and%2520Qi%2520Chai%2520and%2520Deheng%2520Ye%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520have%2520demonstrated%2520impressive%2520capabilities%250Ain%2520social%2520deduction%2520games%2520%2528SDGs%2529%2520like%2520Werewolf%252C%2520where%2520strategic%2520reasoning%2520and%250Asocial%2520deception%2520are%2520essential.%2520However%252C%2520current%2520approaches%2520remain%2520limited%2520to%250Atextual%2520information%252C%2520ignoring%2520crucial%2520multimodal%2520cues%2520such%2520as%2520facial%250Aexpressions%2520and%2520tone%2520of%2520voice%2520that%2520humans%2520naturally%2520use%2520to%2520communicate.%250AMoreover%252C%2520existing%2520SDG%2520agents%2520primarily%2520focus%2520on%2520inferring%2520other%2520players%2527%250Aidentities%2520without%2520modeling%2520how%2520others%2520perceive%2520themselves%2520or%2520fellow%2520players.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520use%2520One%2520Night%2520Ultimate%2520Werewolf%2520%2528ONUW%2529%2520as%2520a%250Atestbed%2520and%2520present%2520MultiMind%252C%2520the%2520first%2520framework%2520integrating%2520multimodal%250Ainformation%2520into%2520SDG%2520agents.%2520MultiMind%2520processes%2520facial%2520expressions%2520and%2520vocal%250Atones%2520alongside%2520verbal%2520content%252C%2520while%2520employing%2520a%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520model%2520to%250Arepresent%2520each%2520player%2527s%2520suspicion%2520levels%2520toward%2520others.%2520By%2520combining%2520this%2520ToM%250Amodel%2520with%2520Monte%2520Carlo%2520Tree%2520Search%2520%2528MCTS%2529%252C%2520our%2520agent%2520identifies%2520communication%250Astrategies%2520that%2520minimize%2520suspicion%2520directed%2520at%2520itself.%2520Through%2520comprehensive%250Aevaluation%2520in%2520both%2520agent-versus-agent%2520simulations%2520and%2520studies%2520with%2520human%250Aplayers%252C%2520we%2520demonstrate%2520MultiMind%2527s%2520superior%2520performance%2520in%2520gameplay.%2520Our%2520work%250Apresents%2520a%2520significant%2520advancement%2520toward%2520LLM%2520agents%2520capable%2520of%2520human-like%250Asocial%2520reasoning%2520across%2520multimodal%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18039v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiMind%3A%20Enhancing%20Werewolf%20Agents%20with%20Multimodal%20Reasoning%20and%0A%20%20Theory%20of%20Mind&entry.906535625=Zheng%20Zhang%20and%20Nuoqian%20Xiao%20and%20Qi%20Chai%20and%20Deheng%20Ye%20and%20Hao%20Wang&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20demonstrated%20impressive%20capabilities%0Ain%20social%20deduction%20games%20%28SDGs%29%20like%20Werewolf%2C%20where%20strategic%20reasoning%20and%0Asocial%20deception%20are%20essential.%20However%2C%20current%20approaches%20remain%20limited%20to%0Atextual%20information%2C%20ignoring%20crucial%20multimodal%20cues%20such%20as%20facial%0Aexpressions%20and%20tone%20of%20voice%20that%20humans%20naturally%20use%20to%20communicate.%0AMoreover%2C%20existing%20SDG%20agents%20primarily%20focus%20on%20inferring%20other%20players%27%0Aidentities%20without%20modeling%20how%20others%20perceive%20themselves%20or%20fellow%20players.%0ATo%20address%20these%20limitations%2C%20we%20use%20One%20Night%20Ultimate%20Werewolf%20%28ONUW%29%20as%20a%0Atestbed%20and%20present%20MultiMind%2C%20the%20first%20framework%20integrating%20multimodal%0Ainformation%20into%20SDG%20agents.%20MultiMind%20processes%20facial%20expressions%20and%20vocal%0Atones%20alongside%20verbal%20content%2C%20while%20employing%20a%20Theory%20of%20Mind%20%28ToM%29%20model%20to%0Arepresent%20each%20player%27s%20suspicion%20levels%20toward%20others.%20By%20combining%20this%20ToM%0Amodel%20with%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%2C%20our%20agent%20identifies%20communication%0Astrategies%20that%20minimize%20suspicion%20directed%20at%20itself.%20Through%20comprehensive%0Aevaluation%20in%20both%20agent-versus-agent%20simulations%20and%20studies%20with%20human%0Aplayers%2C%20we%20demonstrate%20MultiMind%27s%20superior%20performance%20in%20gameplay.%20Our%20work%0Apresents%20a%20significant%20advancement%20toward%20LLM%20agents%20capable%20of%20human-like%0Asocial%20reasoning%20across%20multimodal%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18039v3&entry.124074799=Read"},
{"title": "Benchmarking Open-ended Audio Dialogue Understanding for Large\n  Audio-Language Models", "author": "Kuofeng Gao and Shu-Tao Xia and Ke Xu and Philip Torr and Jindong Gu", "abstract": "  Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked\naudio dialogue capabilities, enabling direct spoken exchanges with humans. The\npotential of LALMs broadens their applicability across a wide range of\npractical scenarios supported by audio dialogues. However, given these\nadvancements, a comprehensive benchmark to evaluate the performance of LALMs in\nthe open-ended audio dialogue understanding remains absent currently. To\naddress this gap, we propose an Audio Dialogue Understanding Benchmark\n(ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended\naudio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9\nmultilingual languages, and 4 categories of ambiguity handling. Notably, we\nfirstly propose the evaluation of ambiguity handling in audio dialogues that\nexpresses different intentions beyond the same literal meaning of sentences,\ne.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes\nover 20,000 open-ended audio dialogues for the assessment of LALMs. Through\nextensive experiments on 16 LALMs, our analysis reveals that existing LALMs\nstruggle with mathematical symbols and formulas, understanding human behavior\nsuch as roleplay, comprehending multiple languages, and handling audio dialogue\nambiguities from different phonetic elements, such as intonations, pause\npositions, and homophones. The benchmark is available at\nhttps://adu-bench.github.io/.\n", "link": "http://arxiv.org/abs/2412.05167v2", "date": "2025-07-28", "relevancy": 2.1728, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5476}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Open-ended%20Audio%20Dialogue%20Understanding%20for%20Large%0A%20%20Audio-Language%20Models&body=Title%3A%20Benchmarking%20Open-ended%20Audio%20Dialogue%20Understanding%20for%20Large%0A%20%20Audio-Language%20Models%0AAuthor%3A%20Kuofeng%20Gao%20and%20Shu-Tao%20Xia%20and%20Ke%20Xu%20and%20Philip%20Torr%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Large%20Audio-Language%20Models%20%28LALMs%29%2C%20such%20as%20GPT-4o%2C%20have%20recently%20unlocked%0Aaudio%20dialogue%20capabilities%2C%20enabling%20direct%20spoken%20exchanges%20with%20humans.%20The%0Apotential%20of%20LALMs%20broadens%20their%20applicability%20across%20a%20wide%20range%20of%0Apractical%20scenarios%20supported%20by%20audio%20dialogues.%20However%2C%20given%20these%0Aadvancements%2C%20a%20comprehensive%20benchmark%20to%20evaluate%20the%20performance%20of%20LALMs%20in%0Athe%20open-ended%20audio%20dialogue%20understanding%20remains%20absent%20currently.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20an%20Audio%20Dialogue%20Understanding%20Benchmark%0A%28ADU-Bench%29%2C%20which%20consists%20of%204%20benchmark%20datasets.%20They%20assess%20the%20open-ended%0Aaudio%20dialogue%20ability%20for%20LALMs%20in%203%20general%20scenarios%2C%2012%20skills%2C%209%0Amultilingual%20languages%2C%20and%204%20categories%20of%20ambiguity%20handling.%20Notably%2C%20we%0Afirstly%20propose%20the%20evaluation%20of%20ambiguity%20handling%20in%20audio%20dialogues%20that%0Aexpresses%20different%20intentions%20beyond%20the%20same%20literal%20meaning%20of%20sentences%2C%0Ae.g.%2C%20%22Really%21%3F%22%20with%20different%20intonations.%20In%20summary%2C%20ADU-Bench%20includes%0Aover%2020%2C000%20open-ended%20audio%20dialogues%20for%20the%20assessment%20of%20LALMs.%20Through%0Aextensive%20experiments%20on%2016%20LALMs%2C%20our%20analysis%20reveals%20that%20existing%20LALMs%0Astruggle%20with%20mathematical%20symbols%20and%20formulas%2C%20understanding%20human%20behavior%0Asuch%20as%20roleplay%2C%20comprehending%20multiple%20languages%2C%20and%20handling%20audio%20dialogue%0Aambiguities%20from%20different%20phonetic%20elements%2C%20such%20as%20intonations%2C%20pause%0Apositions%2C%20and%20homophones.%20The%20benchmark%20is%20available%20at%0Ahttps%3A//adu-bench.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05167v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Open-ended%2520Audio%2520Dialogue%2520Understanding%2520for%2520Large%250A%2520%2520Audio-Language%2520Models%26entry.906535625%3DKuofeng%2520Gao%2520and%2520Shu-Tao%2520Xia%2520and%2520Ke%2520Xu%2520and%2520Philip%2520Torr%2520and%2520Jindong%2520Gu%26entry.1292438233%3D%2520%2520Large%2520Audio-Language%2520Models%2520%2528LALMs%2529%252C%2520such%2520as%2520GPT-4o%252C%2520have%2520recently%2520unlocked%250Aaudio%2520dialogue%2520capabilities%252C%2520enabling%2520direct%2520spoken%2520exchanges%2520with%2520humans.%2520The%250Apotential%2520of%2520LALMs%2520broadens%2520their%2520applicability%2520across%2520a%2520wide%2520range%2520of%250Apractical%2520scenarios%2520supported%2520by%2520audio%2520dialogues.%2520However%252C%2520given%2520these%250Aadvancements%252C%2520a%2520comprehensive%2520benchmark%2520to%2520evaluate%2520the%2520performance%2520of%2520LALMs%2520in%250Athe%2520open-ended%2520audio%2520dialogue%2520understanding%2520remains%2520absent%2520currently.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520an%2520Audio%2520Dialogue%2520Understanding%2520Benchmark%250A%2528ADU-Bench%2529%252C%2520which%2520consists%2520of%25204%2520benchmark%2520datasets.%2520They%2520assess%2520the%2520open-ended%250Aaudio%2520dialogue%2520ability%2520for%2520LALMs%2520in%25203%2520general%2520scenarios%252C%252012%2520skills%252C%25209%250Amultilingual%2520languages%252C%2520and%25204%2520categories%2520of%2520ambiguity%2520handling.%2520Notably%252C%2520we%250Afirstly%2520propose%2520the%2520evaluation%2520of%2520ambiguity%2520handling%2520in%2520audio%2520dialogues%2520that%250Aexpresses%2520different%2520intentions%2520beyond%2520the%2520same%2520literal%2520meaning%2520of%2520sentences%252C%250Ae.g.%252C%2520%2522Really%2521%253F%2522%2520with%2520different%2520intonations.%2520In%2520summary%252C%2520ADU-Bench%2520includes%250Aover%252020%252C000%2520open-ended%2520audio%2520dialogues%2520for%2520the%2520assessment%2520of%2520LALMs.%2520Through%250Aextensive%2520experiments%2520on%252016%2520LALMs%252C%2520our%2520analysis%2520reveals%2520that%2520existing%2520LALMs%250Astruggle%2520with%2520mathematical%2520symbols%2520and%2520formulas%252C%2520understanding%2520human%2520behavior%250Asuch%2520as%2520roleplay%252C%2520comprehending%2520multiple%2520languages%252C%2520and%2520handling%2520audio%2520dialogue%250Aambiguities%2520from%2520different%2520phonetic%2520elements%252C%2520such%2520as%2520intonations%252C%2520pause%250Apositions%252C%2520and%2520homophones.%2520The%2520benchmark%2520is%2520available%2520at%250Ahttps%253A//adu-bench.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05167v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Open-ended%20Audio%20Dialogue%20Understanding%20for%20Large%0A%20%20Audio-Language%20Models&entry.906535625=Kuofeng%20Gao%20and%20Shu-Tao%20Xia%20and%20Ke%20Xu%20and%20Philip%20Torr%20and%20Jindong%20Gu&entry.1292438233=%20%20Large%20Audio-Language%20Models%20%28LALMs%29%2C%20such%20as%20GPT-4o%2C%20have%20recently%20unlocked%0Aaudio%20dialogue%20capabilities%2C%20enabling%20direct%20spoken%20exchanges%20with%20humans.%20The%0Apotential%20of%20LALMs%20broadens%20their%20applicability%20across%20a%20wide%20range%20of%0Apractical%20scenarios%20supported%20by%20audio%20dialogues.%20However%2C%20given%20these%0Aadvancements%2C%20a%20comprehensive%20benchmark%20to%20evaluate%20the%20performance%20of%20LALMs%20in%0Athe%20open-ended%20audio%20dialogue%20understanding%20remains%20absent%20currently.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20an%20Audio%20Dialogue%20Understanding%20Benchmark%0A%28ADU-Bench%29%2C%20which%20consists%20of%204%20benchmark%20datasets.%20They%20assess%20the%20open-ended%0Aaudio%20dialogue%20ability%20for%20LALMs%20in%203%20general%20scenarios%2C%2012%20skills%2C%209%0Amultilingual%20languages%2C%20and%204%20categories%20of%20ambiguity%20handling.%20Notably%2C%20we%0Afirstly%20propose%20the%20evaluation%20of%20ambiguity%20handling%20in%20audio%20dialogues%20that%0Aexpresses%20different%20intentions%20beyond%20the%20same%20literal%20meaning%20of%20sentences%2C%0Ae.g.%2C%20%22Really%21%3F%22%20with%20different%20intonations.%20In%20summary%2C%20ADU-Bench%20includes%0Aover%2020%2C000%20open-ended%20audio%20dialogues%20for%20the%20assessment%20of%20LALMs.%20Through%0Aextensive%20experiments%20on%2016%20LALMs%2C%20our%20analysis%20reveals%20that%20existing%20LALMs%0Astruggle%20with%20mathematical%20symbols%20and%20formulas%2C%20understanding%20human%20behavior%0Asuch%20as%20roleplay%2C%20comprehending%20multiple%20languages%2C%20and%20handling%20audio%20dialogue%0Aambiguities%20from%20different%20phonetic%20elements%2C%20such%20as%20intonations%2C%20pause%0Apositions%2C%20and%20homophones.%20The%20benchmark%20is%20available%20at%0Ahttps%3A//adu-bench.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05167v2&entry.124074799=Read"},
{"title": "Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry", "author": "Matan Kichler and Shai Bagon and Mark Sheinin", "abstract": "  Computer vision seeks to infer a wide range of information about objects and\nevents. However, vision systems based on conventional imaging are limited to\nextracting information only from the visible surfaces of scene objects. For\ninstance, a vision system can detect and identify a Coke can in the scene, but\nit cannot determine whether the can is full or empty. In this paper, we aim to\nexpand the scope of computer vision to include the novel task of inferring the\nhidden liquid levels of opaque containers by sensing the tiny vibrations on\ntheir surfaces. Our method provides a first-of-a-kind way to inspect the fill\nlevel of multiple sealed containers remotely, at once, without needing physical\nmanipulation and manual weighing. First, we propose a novel speckle-based\nvibration sensing system for simultaneously capturing scene vibrations on a 2D\ngrid of points. We use our system to efficiently and remotely capture a dataset\nof vibration responses for a variety of everyday liquid containers. Then, we\ndevelop a transformer-based approach for analyzing the captured vibrations and\nclassifying the container type and its hidden liquid level at the time of\nmeasurement. Our architecture is invariant to the vibration source, yielding\ncorrect liquid level estimates for controlled and ambient scene sound sources.\nMoreover, our model generalizes to unseen container instances within known\nclasses (e.g., training on five Coke cans of a six-pack, testing on a sixth)\nand fluid levels. We demonstrate our method by recovering liquid levels from\nvarious everyday containers.\n", "link": "http://arxiv.org/abs/2507.20757v1", "date": "2025-07-28", "relevancy": 2.1686, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20See%20Inside%20Opaque%20Liquid%20Containers%20using%20Speckle%20Vibrometry&body=Title%3A%20Learning%20to%20See%20Inside%20Opaque%20Liquid%20Containers%20using%20Speckle%20Vibrometry%0AAuthor%3A%20Matan%20Kichler%20and%20Shai%20Bagon%20and%20Mark%20Sheinin%0AAbstract%3A%20%20%20Computer%20vision%20seeks%20to%20infer%20a%20wide%20range%20of%20information%20about%20objects%20and%0Aevents.%20However%2C%20vision%20systems%20based%20on%20conventional%20imaging%20are%20limited%20to%0Aextracting%20information%20only%20from%20the%20visible%20surfaces%20of%20scene%20objects.%20For%0Ainstance%2C%20a%20vision%20system%20can%20detect%20and%20identify%20a%20Coke%20can%20in%20the%20scene%2C%20but%0Ait%20cannot%20determine%20whether%20the%20can%20is%20full%20or%20empty.%20In%20this%20paper%2C%20we%20aim%20to%0Aexpand%20the%20scope%20of%20computer%20vision%20to%20include%20the%20novel%20task%20of%20inferring%20the%0Ahidden%20liquid%20levels%20of%20opaque%20containers%20by%20sensing%20the%20tiny%20vibrations%20on%0Atheir%20surfaces.%20Our%20method%20provides%20a%20first-of-a-kind%20way%20to%20inspect%20the%20fill%0Alevel%20of%20multiple%20sealed%20containers%20remotely%2C%20at%20once%2C%20without%20needing%20physical%0Amanipulation%20and%20manual%20weighing.%20First%2C%20we%20propose%20a%20novel%20speckle-based%0Avibration%20sensing%20system%20for%20simultaneously%20capturing%20scene%20vibrations%20on%20a%202D%0Agrid%20of%20points.%20We%20use%20our%20system%20to%20efficiently%20and%20remotely%20capture%20a%20dataset%0Aof%20vibration%20responses%20for%20a%20variety%20of%20everyday%20liquid%20containers.%20Then%2C%20we%0Adevelop%20a%20transformer-based%20approach%20for%20analyzing%20the%20captured%20vibrations%20and%0Aclassifying%20the%20container%20type%20and%20its%20hidden%20liquid%20level%20at%20the%20time%20of%0Ameasurement.%20Our%20architecture%20is%20invariant%20to%20the%20vibration%20source%2C%20yielding%0Acorrect%20liquid%20level%20estimates%20for%20controlled%20and%20ambient%20scene%20sound%20sources.%0AMoreover%2C%20our%20model%20generalizes%20to%20unseen%20container%20instances%20within%20known%0Aclasses%20%28e.g.%2C%20training%20on%20five%20Coke%20cans%20of%20a%20six-pack%2C%20testing%20on%20a%20sixth%29%0Aand%20fluid%20levels.%20We%20demonstrate%20our%20method%20by%20recovering%20liquid%20levels%20from%0Avarious%20everyday%20containers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520See%2520Inside%2520Opaque%2520Liquid%2520Containers%2520using%2520Speckle%2520Vibrometry%26entry.906535625%3DMatan%2520Kichler%2520and%2520Shai%2520Bagon%2520and%2520Mark%2520Sheinin%26entry.1292438233%3D%2520%2520Computer%2520vision%2520seeks%2520to%2520infer%2520a%2520wide%2520range%2520of%2520information%2520about%2520objects%2520and%250Aevents.%2520However%252C%2520vision%2520systems%2520based%2520on%2520conventional%2520imaging%2520are%2520limited%2520to%250Aextracting%2520information%2520only%2520from%2520the%2520visible%2520surfaces%2520of%2520scene%2520objects.%2520For%250Ainstance%252C%2520a%2520vision%2520system%2520can%2520detect%2520and%2520identify%2520a%2520Coke%2520can%2520in%2520the%2520scene%252C%2520but%250Ait%2520cannot%2520determine%2520whether%2520the%2520can%2520is%2520full%2520or%2520empty.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%250Aexpand%2520the%2520scope%2520of%2520computer%2520vision%2520to%2520include%2520the%2520novel%2520task%2520of%2520inferring%2520the%250Ahidden%2520liquid%2520levels%2520of%2520opaque%2520containers%2520by%2520sensing%2520the%2520tiny%2520vibrations%2520on%250Atheir%2520surfaces.%2520Our%2520method%2520provides%2520a%2520first-of-a-kind%2520way%2520to%2520inspect%2520the%2520fill%250Alevel%2520of%2520multiple%2520sealed%2520containers%2520remotely%252C%2520at%2520once%252C%2520without%2520needing%2520physical%250Amanipulation%2520and%2520manual%2520weighing.%2520First%252C%2520we%2520propose%2520a%2520novel%2520speckle-based%250Avibration%2520sensing%2520system%2520for%2520simultaneously%2520capturing%2520scene%2520vibrations%2520on%2520a%25202D%250Agrid%2520of%2520points.%2520We%2520use%2520our%2520system%2520to%2520efficiently%2520and%2520remotely%2520capture%2520a%2520dataset%250Aof%2520vibration%2520responses%2520for%2520a%2520variety%2520of%2520everyday%2520liquid%2520containers.%2520Then%252C%2520we%250Adevelop%2520a%2520transformer-based%2520approach%2520for%2520analyzing%2520the%2520captured%2520vibrations%2520and%250Aclassifying%2520the%2520container%2520type%2520and%2520its%2520hidden%2520liquid%2520level%2520at%2520the%2520time%2520of%250Ameasurement.%2520Our%2520architecture%2520is%2520invariant%2520to%2520the%2520vibration%2520source%252C%2520yielding%250Acorrect%2520liquid%2520level%2520estimates%2520for%2520controlled%2520and%2520ambient%2520scene%2520sound%2520sources.%250AMoreover%252C%2520our%2520model%2520generalizes%2520to%2520unseen%2520container%2520instances%2520within%2520known%250Aclasses%2520%2528e.g.%252C%2520training%2520on%2520five%2520Coke%2520cans%2520of%2520a%2520six-pack%252C%2520testing%2520on%2520a%2520sixth%2529%250Aand%2520fluid%2520levels.%2520We%2520demonstrate%2520our%2520method%2520by%2520recovering%2520liquid%2520levels%2520from%250Avarious%2520everyday%2520containers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20See%20Inside%20Opaque%20Liquid%20Containers%20using%20Speckle%20Vibrometry&entry.906535625=Matan%20Kichler%20and%20Shai%20Bagon%20and%20Mark%20Sheinin&entry.1292438233=%20%20Computer%20vision%20seeks%20to%20infer%20a%20wide%20range%20of%20information%20about%20objects%20and%0Aevents.%20However%2C%20vision%20systems%20based%20on%20conventional%20imaging%20are%20limited%20to%0Aextracting%20information%20only%20from%20the%20visible%20surfaces%20of%20scene%20objects.%20For%0Ainstance%2C%20a%20vision%20system%20can%20detect%20and%20identify%20a%20Coke%20can%20in%20the%20scene%2C%20but%0Ait%20cannot%20determine%20whether%20the%20can%20is%20full%20or%20empty.%20In%20this%20paper%2C%20we%20aim%20to%0Aexpand%20the%20scope%20of%20computer%20vision%20to%20include%20the%20novel%20task%20of%20inferring%20the%0Ahidden%20liquid%20levels%20of%20opaque%20containers%20by%20sensing%20the%20tiny%20vibrations%20on%0Atheir%20surfaces.%20Our%20method%20provides%20a%20first-of-a-kind%20way%20to%20inspect%20the%20fill%0Alevel%20of%20multiple%20sealed%20containers%20remotely%2C%20at%20once%2C%20without%20needing%20physical%0Amanipulation%20and%20manual%20weighing.%20First%2C%20we%20propose%20a%20novel%20speckle-based%0Avibration%20sensing%20system%20for%20simultaneously%20capturing%20scene%20vibrations%20on%20a%202D%0Agrid%20of%20points.%20We%20use%20our%20system%20to%20efficiently%20and%20remotely%20capture%20a%20dataset%0Aof%20vibration%20responses%20for%20a%20variety%20of%20everyday%20liquid%20containers.%20Then%2C%20we%0Adevelop%20a%20transformer-based%20approach%20for%20analyzing%20the%20captured%20vibrations%20and%0Aclassifying%20the%20container%20type%20and%20its%20hidden%20liquid%20level%20at%20the%20time%20of%0Ameasurement.%20Our%20architecture%20is%20invariant%20to%20the%20vibration%20source%2C%20yielding%0Acorrect%20liquid%20level%20estimates%20for%20controlled%20and%20ambient%20scene%20sound%20sources.%0AMoreover%2C%20our%20model%20generalizes%20to%20unseen%20container%20instances%20within%20known%0Aclasses%20%28e.g.%2C%20training%20on%20five%20Coke%20cans%20of%20a%20six-pack%2C%20testing%20on%20a%20sixth%29%0Aand%20fluid%20levels.%20We%20demonstrate%20our%20method%20by%20recovering%20liquid%20levels%20from%0Avarious%20everyday%20containers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20757v1&entry.124074799=Read"},
{"title": "First Hallucination Tokens Are Different from Conditional Ones", "author": "Jakob Snel and Seong Joon Oh", "abstract": "  Hallucination, the generation of untruthful content, is one of the major\nconcerns regarding foundational models. Detecting hallucinations at the token\nlevel is vital for real-time filtering and targeted correction, yet the\nvariation of hallucination signals within token sequences is not fully\nunderstood. Leveraging the RAGTruth corpus with token-level annotations and\nreproduced logits, we analyse how these signals depend on a token's position\nwithin hallucinated spans, contributing to an improved understanding of\ntoken-level hallucination. Our results show that the first hallucinated token\ncarries a stronger signal and is more detectable than conditional tokens. We\nrelease our analysis framework, along with code for logit reproduction and\nmetric computation at https://github.com/jakobsnl/RAGTruth_Xtended.\n", "link": "http://arxiv.org/abs/2507.20836v1", "date": "2025-07-28", "relevancy": 2.1538, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4449}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20First%20Hallucination%20Tokens%20Are%20Different%20from%20Conditional%20Ones&body=Title%3A%20First%20Hallucination%20Tokens%20Are%20Different%20from%20Conditional%20Ones%0AAuthor%3A%20Jakob%20Snel%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20Hallucination%2C%20the%20generation%20of%20untruthful%20content%2C%20is%20one%20of%20the%20major%0Aconcerns%20regarding%20foundational%20models.%20Detecting%20hallucinations%20at%20the%20token%0Alevel%20is%20vital%20for%20real-time%20filtering%20and%20targeted%20correction%2C%20yet%20the%0Avariation%20of%20hallucination%20signals%20within%20token%20sequences%20is%20not%20fully%0Aunderstood.%20Leveraging%20the%20RAGTruth%20corpus%20with%20token-level%20annotations%20and%0Areproduced%20logits%2C%20we%20analyse%20how%20these%20signals%20depend%20on%20a%20token%27s%20position%0Awithin%20hallucinated%20spans%2C%20contributing%20to%20an%20improved%20understanding%20of%0Atoken-level%20hallucination.%20Our%20results%20show%20that%20the%20first%20hallucinated%20token%0Acarries%20a%20stronger%20signal%20and%20is%20more%20detectable%20than%20conditional%20tokens.%20We%0Arelease%20our%20analysis%20framework%2C%20along%20with%20code%20for%20logit%20reproduction%20and%0Ametric%20computation%20at%20https%3A//github.com/jakobsnl/RAGTruth_Xtended.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFirst%2520Hallucination%2520Tokens%2520Are%2520Different%2520from%2520Conditional%2520Ones%26entry.906535625%3DJakob%2520Snel%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520Hallucination%252C%2520the%2520generation%2520of%2520untruthful%2520content%252C%2520is%2520one%2520of%2520the%2520major%250Aconcerns%2520regarding%2520foundational%2520models.%2520Detecting%2520hallucinations%2520at%2520the%2520token%250Alevel%2520is%2520vital%2520for%2520real-time%2520filtering%2520and%2520targeted%2520correction%252C%2520yet%2520the%250Avariation%2520of%2520hallucination%2520signals%2520within%2520token%2520sequences%2520is%2520not%2520fully%250Aunderstood.%2520Leveraging%2520the%2520RAGTruth%2520corpus%2520with%2520token-level%2520annotations%2520and%250Areproduced%2520logits%252C%2520we%2520analyse%2520how%2520these%2520signals%2520depend%2520on%2520a%2520token%2527s%2520position%250Awithin%2520hallucinated%2520spans%252C%2520contributing%2520to%2520an%2520improved%2520understanding%2520of%250Atoken-level%2520hallucination.%2520Our%2520results%2520show%2520that%2520the%2520first%2520hallucinated%2520token%250Acarries%2520a%2520stronger%2520signal%2520and%2520is%2520more%2520detectable%2520than%2520conditional%2520tokens.%2520We%250Arelease%2520our%2520analysis%2520framework%252C%2520along%2520with%2520code%2520for%2520logit%2520reproduction%2520and%250Ametric%2520computation%2520at%2520https%253A//github.com/jakobsnl/RAGTruth_Xtended.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=First%20Hallucination%20Tokens%20Are%20Different%20from%20Conditional%20Ones&entry.906535625=Jakob%20Snel%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20Hallucination%2C%20the%20generation%20of%20untruthful%20content%2C%20is%20one%20of%20the%20major%0Aconcerns%20regarding%20foundational%20models.%20Detecting%20hallucinations%20at%20the%20token%0Alevel%20is%20vital%20for%20real-time%20filtering%20and%20targeted%20correction%2C%20yet%20the%0Avariation%20of%20hallucination%20signals%20within%20token%20sequences%20is%20not%20fully%0Aunderstood.%20Leveraging%20the%20RAGTruth%20corpus%20with%20token-level%20annotations%20and%0Areproduced%20logits%2C%20we%20analyse%20how%20these%20signals%20depend%20on%20a%20token%27s%20position%0Awithin%20hallucinated%20spans%2C%20contributing%20to%20an%20improved%20understanding%20of%0Atoken-level%20hallucination.%20Our%20results%20show%20that%20the%20first%20hallucinated%20token%0Acarries%20a%20stronger%20signal%20and%20is%20more%20detectable%20than%20conditional%20tokens.%20We%0Arelease%20our%20analysis%20framework%2C%20along%20with%20code%20for%20logit%20reproduction%20and%0Ametric%20computation%20at%20https%3A//github.com/jakobsnl/RAGTruth_Xtended.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20836v1&entry.124074799=Read"},
{"title": "Improving Open-world Continual Learning under the Constraints of Scarce\n  Labeled Data", "author": "Yujie Li and Xiangkun Wang and Xin Yang and Marcello Bonsangue and Junbo Zhang and Tianrui Li", "abstract": "  Open-world continual learning (OWCL) adapts to sequential tasks with open\nsamples, learning knowledge incrementally while preventing forgetting. However,\nexisting OWCL still requires a large amount of labeled data for training, which\nis often impractical in real-world applications. Given that new\ncategories/entities typically come with limited annotations and are in small\nquantities, a more realistic situation is OWCL with scarce labeled data, i.e.,\nfew-shot training samples. Hence, this paper investigates the problem of\nopen-world few-shot continual learning (OFCL), challenging in (i) learning\nunbounded tasks without forgetting previous knowledge and avoiding overfitting,\n(ii) constructing compact decision boundaries for open detection with limited\nlabeled data, and (iii) transferring knowledge about knowns and unknowns and\neven update the unknowns to knowns once the labels of open samples are learned.\nIn response, we propose a novel OFCL framework that integrates three key\ncomponents: (1) an instance-wise token augmentation (ITA) that represents and\nenriches sample representations with additional knowledge, (2) a margin-based\nopen boundary (MOB) that supports open detection with new tasks emerge over\ntime, and (3) an adaptive knowledge space (AKS) that endows unknowns with\nknowledge for the updating from unknowns to knowns. Finally, extensive\nexperiments show that the proposed OFCL framework outperforms all baselines\nremarkably with practical importance and reproducibility. The source code is\nreleased at https://github.com/liyj1201/OFCL.\n", "link": "http://arxiv.org/abs/2502.20974v2", "date": "2025-07-28", "relevancy": 2.1532, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5422}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5387}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Open-world%20Continual%20Learning%20under%20the%20Constraints%20of%20Scarce%0A%20%20Labeled%20Data&body=Title%3A%20Improving%20Open-world%20Continual%20Learning%20under%20the%20Constraints%20of%20Scarce%0A%20%20Labeled%20Data%0AAuthor%3A%20Yujie%20Li%20and%20Xiangkun%20Wang%20and%20Xin%20Yang%20and%20Marcello%20Bonsangue%20and%20Junbo%20Zhang%20and%20Tianrui%20Li%0AAbstract%3A%20%20%20Open-world%20continual%20learning%20%28OWCL%29%20adapts%20to%20sequential%20tasks%20with%20open%0Asamples%2C%20learning%20knowledge%20incrementally%20while%20preventing%20forgetting.%20However%2C%0Aexisting%20OWCL%20still%20requires%20a%20large%20amount%20of%20labeled%20data%20for%20training%2C%20which%0Ais%20often%20impractical%20in%20real-world%20applications.%20Given%20that%20new%0Acategories/entities%20typically%20come%20with%20limited%20annotations%20and%20are%20in%20small%0Aquantities%2C%20a%20more%20realistic%20situation%20is%20OWCL%20with%20scarce%20labeled%20data%2C%20i.e.%2C%0Afew-shot%20training%20samples.%20Hence%2C%20this%20paper%20investigates%20the%20problem%20of%0Aopen-world%20few-shot%20continual%20learning%20%28OFCL%29%2C%20challenging%20in%20%28i%29%20learning%0Aunbounded%20tasks%20without%20forgetting%20previous%20knowledge%20and%20avoiding%20overfitting%2C%0A%28ii%29%20constructing%20compact%20decision%20boundaries%20for%20open%20detection%20with%20limited%0Alabeled%20data%2C%20and%20%28iii%29%20transferring%20knowledge%20about%20knowns%20and%20unknowns%20and%0Aeven%20update%20the%20unknowns%20to%20knowns%20once%20the%20labels%20of%20open%20samples%20are%20learned.%0AIn%20response%2C%20we%20propose%20a%20novel%20OFCL%20framework%20that%20integrates%20three%20key%0Acomponents%3A%20%281%29%20an%20instance-wise%20token%20augmentation%20%28ITA%29%20that%20represents%20and%0Aenriches%20sample%20representations%20with%20additional%20knowledge%2C%20%282%29%20a%20margin-based%0Aopen%20boundary%20%28MOB%29%20that%20supports%20open%20detection%20with%20new%20tasks%20emerge%20over%0Atime%2C%20and%20%283%29%20an%20adaptive%20knowledge%20space%20%28AKS%29%20that%20endows%20unknowns%20with%0Aknowledge%20for%20the%20updating%20from%20unknowns%20to%20knowns.%20Finally%2C%20extensive%0Aexperiments%20show%20that%20the%20proposed%20OFCL%20framework%20outperforms%20all%20baselines%0Aremarkably%20with%20practical%20importance%20and%20reproducibility.%20The%20source%20code%20is%0Areleased%20at%20https%3A//github.com/liyj1201/OFCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20974v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Open-world%2520Continual%2520Learning%2520under%2520the%2520Constraints%2520of%2520Scarce%250A%2520%2520Labeled%2520Data%26entry.906535625%3DYujie%2520Li%2520and%2520Xiangkun%2520Wang%2520and%2520Xin%2520Yang%2520and%2520Marcello%2520Bonsangue%2520and%2520Junbo%2520Zhang%2520and%2520Tianrui%2520Li%26entry.1292438233%3D%2520%2520Open-world%2520continual%2520learning%2520%2528OWCL%2529%2520adapts%2520to%2520sequential%2520tasks%2520with%2520open%250Asamples%252C%2520learning%2520knowledge%2520incrementally%2520while%2520preventing%2520forgetting.%2520However%252C%250Aexisting%2520OWCL%2520still%2520requires%2520a%2520large%2520amount%2520of%2520labeled%2520data%2520for%2520training%252C%2520which%250Ais%2520often%2520impractical%2520in%2520real-world%2520applications.%2520Given%2520that%2520new%250Acategories/entities%2520typically%2520come%2520with%2520limited%2520annotations%2520and%2520are%2520in%2520small%250Aquantities%252C%2520a%2520more%2520realistic%2520situation%2520is%2520OWCL%2520with%2520scarce%2520labeled%2520data%252C%2520i.e.%252C%250Afew-shot%2520training%2520samples.%2520Hence%252C%2520this%2520paper%2520investigates%2520the%2520problem%2520of%250Aopen-world%2520few-shot%2520continual%2520learning%2520%2528OFCL%2529%252C%2520challenging%2520in%2520%2528i%2529%2520learning%250Aunbounded%2520tasks%2520without%2520forgetting%2520previous%2520knowledge%2520and%2520avoiding%2520overfitting%252C%250A%2528ii%2529%2520constructing%2520compact%2520decision%2520boundaries%2520for%2520open%2520detection%2520with%2520limited%250Alabeled%2520data%252C%2520and%2520%2528iii%2529%2520transferring%2520knowledge%2520about%2520knowns%2520and%2520unknowns%2520and%250Aeven%2520update%2520the%2520unknowns%2520to%2520knowns%2520once%2520the%2520labels%2520of%2520open%2520samples%2520are%2520learned.%250AIn%2520response%252C%2520we%2520propose%2520a%2520novel%2520OFCL%2520framework%2520that%2520integrates%2520three%2520key%250Acomponents%253A%2520%25281%2529%2520an%2520instance-wise%2520token%2520augmentation%2520%2528ITA%2529%2520that%2520represents%2520and%250Aenriches%2520sample%2520representations%2520with%2520additional%2520knowledge%252C%2520%25282%2529%2520a%2520margin-based%250Aopen%2520boundary%2520%2528MOB%2529%2520that%2520supports%2520open%2520detection%2520with%2520new%2520tasks%2520emerge%2520over%250Atime%252C%2520and%2520%25283%2529%2520an%2520adaptive%2520knowledge%2520space%2520%2528AKS%2529%2520that%2520endows%2520unknowns%2520with%250Aknowledge%2520for%2520the%2520updating%2520from%2520unknowns%2520to%2520knowns.%2520Finally%252C%2520extensive%250Aexperiments%2520show%2520that%2520the%2520proposed%2520OFCL%2520framework%2520outperforms%2520all%2520baselines%250Aremarkably%2520with%2520practical%2520importance%2520and%2520reproducibility.%2520The%2520source%2520code%2520is%250Areleased%2520at%2520https%253A//github.com/liyj1201/OFCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20974v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Open-world%20Continual%20Learning%20under%20the%20Constraints%20of%20Scarce%0A%20%20Labeled%20Data&entry.906535625=Yujie%20Li%20and%20Xiangkun%20Wang%20and%20Xin%20Yang%20and%20Marcello%20Bonsangue%20and%20Junbo%20Zhang%20and%20Tianrui%20Li&entry.1292438233=%20%20Open-world%20continual%20learning%20%28OWCL%29%20adapts%20to%20sequential%20tasks%20with%20open%0Asamples%2C%20learning%20knowledge%20incrementally%20while%20preventing%20forgetting.%20However%2C%0Aexisting%20OWCL%20still%20requires%20a%20large%20amount%20of%20labeled%20data%20for%20training%2C%20which%0Ais%20often%20impractical%20in%20real-world%20applications.%20Given%20that%20new%0Acategories/entities%20typically%20come%20with%20limited%20annotations%20and%20are%20in%20small%0Aquantities%2C%20a%20more%20realistic%20situation%20is%20OWCL%20with%20scarce%20labeled%20data%2C%20i.e.%2C%0Afew-shot%20training%20samples.%20Hence%2C%20this%20paper%20investigates%20the%20problem%20of%0Aopen-world%20few-shot%20continual%20learning%20%28OFCL%29%2C%20challenging%20in%20%28i%29%20learning%0Aunbounded%20tasks%20without%20forgetting%20previous%20knowledge%20and%20avoiding%20overfitting%2C%0A%28ii%29%20constructing%20compact%20decision%20boundaries%20for%20open%20detection%20with%20limited%0Alabeled%20data%2C%20and%20%28iii%29%20transferring%20knowledge%20about%20knowns%20and%20unknowns%20and%0Aeven%20update%20the%20unknowns%20to%20knowns%20once%20the%20labels%20of%20open%20samples%20are%20learned.%0AIn%20response%2C%20we%20propose%20a%20novel%20OFCL%20framework%20that%20integrates%20three%20key%0Acomponents%3A%20%281%29%20an%20instance-wise%20token%20augmentation%20%28ITA%29%20that%20represents%20and%0Aenriches%20sample%20representations%20with%20additional%20knowledge%2C%20%282%29%20a%20margin-based%0Aopen%20boundary%20%28MOB%29%20that%20supports%20open%20detection%20with%20new%20tasks%20emerge%20over%0Atime%2C%20and%20%283%29%20an%20adaptive%20knowledge%20space%20%28AKS%29%20that%20endows%20unknowns%20with%0Aknowledge%20for%20the%20updating%20from%20unknowns%20to%20knowns.%20Finally%2C%20extensive%0Aexperiments%20show%20that%20the%20proposed%20OFCL%20framework%20outperforms%20all%20baselines%0Aremarkably%20with%20practical%20importance%20and%20reproducibility.%20The%20source%20code%20is%0Areleased%20at%20https%3A//github.com/liyj1201/OFCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20974v2&entry.124074799=Read"},
{"title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal\n  Knowledge Graphs", "author": "Xueyao Wan and Hang Yu", "abstract": "  Retrieval-Augmented Generation (RAG) enhances language model generation by\nretrieving relevant information from external knowledge bases. However,\nconventional RAG methods face the issue of missing multimodal information.\nMultimodal RAG methods address this by fusing images and text through mapping\nthem into a shared embedding space, but they fail to capture the structure of\nknowledge and logical chains between modalities. Moreover, they also require\nlarge-scale training for specific tasks, resulting in limited generalizing\nability. To address these limitations, we propose MMGraphRAG, which refines\nvisual content through scene graphs and constructs a multimodal knowledge graph\n(MMKG) in conjunction with text-based KG. It employs spectral clustering to\nachieve cross-modal entity linking and retrieves context along reasoning paths\nto guide the generative process. Experimental results show that MMGraphRAG\nachieves state-of-the-art performance on the DocBench and MMLongBench datasets,\ndemonstrating strong domain adaptability and clear reasoning paths.\n", "link": "http://arxiv.org/abs/2507.20804v1", "date": "2025-07-28", "relevancy": 2.151, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.537}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMGraphRAG%3A%20Bridging%20Vision%20and%20Language%20with%20Interpretable%20Multimodal%0A%20%20Knowledge%20Graphs&body=Title%3A%20MMGraphRAG%3A%20Bridging%20Vision%20and%20Language%20with%20Interpretable%20Multimodal%0A%20%20Knowledge%20Graphs%0AAuthor%3A%20Xueyao%20Wan%20and%20Hang%20Yu%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20enhances%20language%20model%20generation%20by%0Aretrieving%20relevant%20information%20from%20external%20knowledge%20bases.%20However%2C%0Aconventional%20RAG%20methods%20face%20the%20issue%20of%20missing%20multimodal%20information.%0AMultimodal%20RAG%20methods%20address%20this%20by%20fusing%20images%20and%20text%20through%20mapping%0Athem%20into%20a%20shared%20embedding%20space%2C%20but%20they%20fail%20to%20capture%20the%20structure%20of%0Aknowledge%20and%20logical%20chains%20between%20modalities.%20Moreover%2C%20they%20also%20require%0Alarge-scale%20training%20for%20specific%20tasks%2C%20resulting%20in%20limited%20generalizing%0Aability.%20To%20address%20these%20limitations%2C%20we%20propose%20MMGraphRAG%2C%20which%20refines%0Avisual%20content%20through%20scene%20graphs%20and%20constructs%20a%20multimodal%20knowledge%20graph%0A%28MMKG%29%20in%20conjunction%20with%20text-based%20KG.%20It%20employs%20spectral%20clustering%20to%0Aachieve%20cross-modal%20entity%20linking%20and%20retrieves%20context%20along%20reasoning%20paths%0Ato%20guide%20the%20generative%20process.%20Experimental%20results%20show%20that%20MMGraphRAG%0Aachieves%20state-of-the-art%20performance%20on%20the%20DocBench%20and%20MMLongBench%20datasets%2C%0Ademonstrating%20strong%20domain%20adaptability%20and%20clear%20reasoning%20paths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20804v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMGraphRAG%253A%2520Bridging%2520Vision%2520and%2520Language%2520with%2520Interpretable%2520Multimodal%250A%2520%2520Knowledge%2520Graphs%26entry.906535625%3DXueyao%2520Wan%2520and%2520Hang%2520Yu%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520enhances%2520language%2520model%2520generation%2520by%250Aretrieving%2520relevant%2520information%2520from%2520external%2520knowledge%2520bases.%2520However%252C%250Aconventional%2520RAG%2520methods%2520face%2520the%2520issue%2520of%2520missing%2520multimodal%2520information.%250AMultimodal%2520RAG%2520methods%2520address%2520this%2520by%2520fusing%2520images%2520and%2520text%2520through%2520mapping%250Athem%2520into%2520a%2520shared%2520embedding%2520space%252C%2520but%2520they%2520fail%2520to%2520capture%2520the%2520structure%2520of%250Aknowledge%2520and%2520logical%2520chains%2520between%2520modalities.%2520Moreover%252C%2520they%2520also%2520require%250Alarge-scale%2520training%2520for%2520specific%2520tasks%252C%2520resulting%2520in%2520limited%2520generalizing%250Aability.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MMGraphRAG%252C%2520which%2520refines%250Avisual%2520content%2520through%2520scene%2520graphs%2520and%2520constructs%2520a%2520multimodal%2520knowledge%2520graph%250A%2528MMKG%2529%2520in%2520conjunction%2520with%2520text-based%2520KG.%2520It%2520employs%2520spectral%2520clustering%2520to%250Aachieve%2520cross-modal%2520entity%2520linking%2520and%2520retrieves%2520context%2520along%2520reasoning%2520paths%250Ato%2520guide%2520the%2520generative%2520process.%2520Experimental%2520results%2520show%2520that%2520MMGraphRAG%250Aachieves%2520state-of-the-art%2520performance%2520on%2520the%2520DocBench%2520and%2520MMLongBench%2520datasets%252C%250Ademonstrating%2520strong%2520domain%2520adaptability%2520and%2520clear%2520reasoning%2520paths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20804v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMGraphRAG%3A%20Bridging%20Vision%20and%20Language%20with%20Interpretable%20Multimodal%0A%20%20Knowledge%20Graphs&entry.906535625=Xueyao%20Wan%20and%20Hang%20Yu&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20enhances%20language%20model%20generation%20by%0Aretrieving%20relevant%20information%20from%20external%20knowledge%20bases.%20However%2C%0Aconventional%20RAG%20methods%20face%20the%20issue%20of%20missing%20multimodal%20information.%0AMultimodal%20RAG%20methods%20address%20this%20by%20fusing%20images%20and%20text%20through%20mapping%0Athem%20into%20a%20shared%20embedding%20space%2C%20but%20they%20fail%20to%20capture%20the%20structure%20of%0Aknowledge%20and%20logical%20chains%20between%20modalities.%20Moreover%2C%20they%20also%20require%0Alarge-scale%20training%20for%20specific%20tasks%2C%20resulting%20in%20limited%20generalizing%0Aability.%20To%20address%20these%20limitations%2C%20we%20propose%20MMGraphRAG%2C%20which%20refines%0Avisual%20content%20through%20scene%20graphs%20and%20constructs%20a%20multimodal%20knowledge%20graph%0A%28MMKG%29%20in%20conjunction%20with%20text-based%20KG.%20It%20employs%20spectral%20clustering%20to%0Aachieve%20cross-modal%20entity%20linking%20and%20retrieves%20context%20along%20reasoning%20paths%0Ato%20guide%20the%20generative%20process.%20Experimental%20results%20show%20that%20MMGraphRAG%0Aachieves%20state-of-the-art%20performance%20on%20the%20DocBench%20and%20MMLongBench%20datasets%2C%0Ademonstrating%20strong%20domain%20adaptability%20and%20clear%20reasoning%20paths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20804v1&entry.124074799=Read"},
{"title": "Continual Low-Rank Scaled Dot-product Attention", "author": "Gin\u00e9s Carreto Pic\u00f3n and Illia Oleksiienko and Lukas Hedegaard and Arian Bakhtiarnia and Alexandros Iosifidis", "abstract": "  Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of Transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.\n", "link": "http://arxiv.org/abs/2412.03214v4", "date": "2025-07-28", "relevancy": 2.132, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5881}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5637}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Low-Rank%20Scaled%20Dot-product%20Attention&body=Title%3A%20Continual%20Low-Rank%20Scaled%20Dot-product%20Attention%0AAuthor%3A%20Gin%C3%A9s%20Carreto%20Pic%C3%B3n%20and%20Illia%20Oleksiienko%20and%20Lukas%20Hedegaard%20and%20Arian%20Bakhtiarnia%20and%20Alexandros%20Iosifidis%0AAbstract%3A%20%20%20Transformers%20are%20widely%20used%20for%20their%20ability%20to%20capture%20data%20relations%20in%0Asequence%20processing%2C%20with%20great%20success%20for%20a%20wide%20range%20of%20static%20tasks.%0AHowever%2C%20the%20computational%20and%20memory%20footprint%20of%20their%20main%20component%2C%20i.e.%2C%0Athe%20Scaled%20Dot-product%20Attention%2C%20is%20commonly%20overlooked.%20This%20makes%20their%0Aadoption%20in%20applications%20involving%20stream%20data%20processing%20with%20constraints%20in%0Aresponse%20latency%2C%20computational%20and%20memory%20resources%20infeasible.%20Some%20works%0Ahave%20proposed%20methods%20to%20lower%20the%20computational%20cost%20of%20Transformers%2C%20i.e.%0Alow-rank%20approximations%2C%20sparsity%20in%20attention%2C%20and%20efficient%20formulations%20for%0AContinual%20Inference.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20formulation%20of%20the%0AScaled%20Dot-product%20Attention%20based%20on%20the%20Nystr%5C%22om%20approximation%20that%20is%0Asuitable%20for%20Continual%20Inference.%20In%20experiments%20on%20Online%20Audio%20Classification%0Aand%20Online%20Action%20Detection%20tasks%2C%20the%20proposed%20Continual%20Scaled%20Dot-product%0AAttention%20can%20lower%20the%20number%20of%20operations%20by%20up%20to%20three%20orders%20of%20magnitude%0Acompared%20to%20the%20original%20Transformers%20while%20retaining%20the%20predictive%0Aperformance%20of%20competing%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03214v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Low-Rank%2520Scaled%2520Dot-product%2520Attention%26entry.906535625%3DGin%25C3%25A9s%2520Carreto%2520Pic%25C3%25B3n%2520and%2520Illia%2520Oleksiienko%2520and%2520Lukas%2520Hedegaard%2520and%2520Arian%2520Bakhtiarnia%2520and%2520Alexandros%2520Iosifidis%26entry.1292438233%3D%2520%2520Transformers%2520are%2520widely%2520used%2520for%2520their%2520ability%2520to%2520capture%2520data%2520relations%2520in%250Asequence%2520processing%252C%2520with%2520great%2520success%2520for%2520a%2520wide%2520range%2520of%2520static%2520tasks.%250AHowever%252C%2520the%2520computational%2520and%2520memory%2520footprint%2520of%2520their%2520main%2520component%252C%2520i.e.%252C%250Athe%2520Scaled%2520Dot-product%2520Attention%252C%2520is%2520commonly%2520overlooked.%2520This%2520makes%2520their%250Aadoption%2520in%2520applications%2520involving%2520stream%2520data%2520processing%2520with%2520constraints%2520in%250Aresponse%2520latency%252C%2520computational%2520and%2520memory%2520resources%2520infeasible.%2520Some%2520works%250Ahave%2520proposed%2520methods%2520to%2520lower%2520the%2520computational%2520cost%2520of%2520Transformers%252C%2520i.e.%250Alow-rank%2520approximations%252C%2520sparsity%2520in%2520attention%252C%2520and%2520efficient%2520formulations%2520for%250AContinual%2520Inference.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520formulation%2520of%2520the%250AScaled%2520Dot-product%2520Attention%2520based%2520on%2520the%2520Nystr%255C%2522om%2520approximation%2520that%2520is%250Asuitable%2520for%2520Continual%2520Inference.%2520In%2520experiments%2520on%2520Online%2520Audio%2520Classification%250Aand%2520Online%2520Action%2520Detection%2520tasks%252C%2520the%2520proposed%2520Continual%2520Scaled%2520Dot-product%250AAttention%2520can%2520lower%2520the%2520number%2520of%2520operations%2520by%2520up%2520to%2520three%2520orders%2520of%2520magnitude%250Acompared%2520to%2520the%2520original%2520Transformers%2520while%2520retaining%2520the%2520predictive%250Aperformance%2520of%2520competing%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03214v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Low-Rank%20Scaled%20Dot-product%20Attention&entry.906535625=Gin%C3%A9s%20Carreto%20Pic%C3%B3n%20and%20Illia%20Oleksiienko%20and%20Lukas%20Hedegaard%20and%20Arian%20Bakhtiarnia%20and%20Alexandros%20Iosifidis&entry.1292438233=%20%20Transformers%20are%20widely%20used%20for%20their%20ability%20to%20capture%20data%20relations%20in%0Asequence%20processing%2C%20with%20great%20success%20for%20a%20wide%20range%20of%20static%20tasks.%0AHowever%2C%20the%20computational%20and%20memory%20footprint%20of%20their%20main%20component%2C%20i.e.%2C%0Athe%20Scaled%20Dot-product%20Attention%2C%20is%20commonly%20overlooked.%20This%20makes%20their%0Aadoption%20in%20applications%20involving%20stream%20data%20processing%20with%20constraints%20in%0Aresponse%20latency%2C%20computational%20and%20memory%20resources%20infeasible.%20Some%20works%0Ahave%20proposed%20methods%20to%20lower%20the%20computational%20cost%20of%20Transformers%2C%20i.e.%0Alow-rank%20approximations%2C%20sparsity%20in%20attention%2C%20and%20efficient%20formulations%20for%0AContinual%20Inference.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20formulation%20of%20the%0AScaled%20Dot-product%20Attention%20based%20on%20the%20Nystr%5C%22om%20approximation%20that%20is%0Asuitable%20for%20Continual%20Inference.%20In%20experiments%20on%20Online%20Audio%20Classification%0Aand%20Online%20Action%20Detection%20tasks%2C%20the%20proposed%20Continual%20Scaled%20Dot-product%0AAttention%20can%20lower%20the%20number%20of%20operations%20by%20up%20to%20three%20orders%20of%20magnitude%0Acompared%20to%20the%20original%20Transformers%20while%20retaining%20the%20predictive%0Aperformance%20of%20competing%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03214v4&entry.124074799=Read"},
{"title": "PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge\n  Attributes", "author": "Tianhao Wang and Simon Klancher and Kunal Mukherjee and Josh Wiedemeier and Feng Chen and Murat Kantarcioglu and Kangkook Jee", "abstract": "  The rise of graph-structured data has driven interest in graph learning and\nsynthetic data generation. While successful in text and image domains,\nsynthetic graph generation remains challenging -- especially for real-world\ngraphs with complex, heterogeneous schemas. Existing research has focused\nmostly on homogeneous structures with simple attributes, limiting their\nusefulness and relevance for application domains requiring semantic fidelity.\n  In this research, we introduce ProvCreator, a synthetic graph framework\ndesigned for complex heterogeneous graphs with high-dimensional node and edge\nattributes. ProvCreator formulates graph synthesis as a sequence generation\ntask, enabling the use of transformer-based large language models. It features\na versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph\nstructure and attributes, 2. efficiently compresses large graphs for contextual\nmodeling, and 3. supports end-to-end, learnable graph generation.\n  To validate our research, we evaluate ProvCreator on two challenging domains:\nsystem provenance graphs in cybersecurity and knowledge graphs from\nIntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate\ndependencies between structure and semantics, enabling the generation of\nrealistic and privacy-aware synthetic datasets.\n", "link": "http://arxiv.org/abs/2507.20967v1", "date": "2025-07-28", "relevancy": 2.1266, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5365}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROVCREATOR%3A%20Synthesizing%20Complex%20Heterogenous%20Graphs%20with%20Node%20and%20Edge%0A%20%20Attributes&body=Title%3A%20PROVCREATOR%3A%20Synthesizing%20Complex%20Heterogenous%20Graphs%20with%20Node%20and%20Edge%0A%20%20Attributes%0AAuthor%3A%20Tianhao%20Wang%20and%20Simon%20Klancher%20and%20Kunal%20Mukherjee%20and%20Josh%20Wiedemeier%20and%20Feng%20Chen%20and%20Murat%20Kantarcioglu%20and%20Kangkook%20Jee%0AAbstract%3A%20%20%20The%20rise%20of%20graph-structured%20data%20has%20driven%20interest%20in%20graph%20learning%20and%0Asynthetic%20data%20generation.%20While%20successful%20in%20text%20and%20image%20domains%2C%0Asynthetic%20graph%20generation%20remains%20challenging%20--%20especially%20for%20real-world%0Agraphs%20with%20complex%2C%20heterogeneous%20schemas.%20Existing%20research%20has%20focused%0Amostly%20on%20homogeneous%20structures%20with%20simple%20attributes%2C%20limiting%20their%0Ausefulness%20and%20relevance%20for%20application%20domains%20requiring%20semantic%20fidelity.%0A%20%20In%20this%20research%2C%20we%20introduce%20ProvCreator%2C%20a%20synthetic%20graph%20framework%0Adesigned%20for%20complex%20heterogeneous%20graphs%20with%20high-dimensional%20node%20and%20edge%0Aattributes.%20ProvCreator%20formulates%20graph%20synthesis%20as%20a%20sequence%20generation%0Atask%2C%20enabling%20the%20use%20of%20transformer-based%20large%20language%20models.%20It%20features%0Aa%20versatile%20graph-to-sequence%20encoder-decoder%20that%201.%20losslessly%20encodes%20graph%0Astructure%20and%20attributes%2C%202.%20efficiently%20compresses%20large%20graphs%20for%20contextual%0Amodeling%2C%20and%203.%20supports%20end-to-end%2C%20learnable%20graph%20generation.%0A%20%20To%20validate%20our%20research%2C%20we%20evaluate%20ProvCreator%20on%20two%20challenging%20domains%3A%0Asystem%20provenance%20graphs%20in%20cybersecurity%20and%20knowledge%20graphs%20from%0AIntelliGraph%20Benchmark%20Dataset.%20In%20both%20cases%2C%20ProvCreator%20captures%20intricate%0Adependencies%20between%20structure%20and%20semantics%2C%20enabling%20the%20generation%20of%0Arealistic%20and%20privacy-aware%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROVCREATOR%253A%2520Synthesizing%2520Complex%2520Heterogenous%2520Graphs%2520with%2520Node%2520and%2520Edge%250A%2520%2520Attributes%26entry.906535625%3DTianhao%2520Wang%2520and%2520Simon%2520Klancher%2520and%2520Kunal%2520Mukherjee%2520and%2520Josh%2520Wiedemeier%2520and%2520Feng%2520Chen%2520and%2520Murat%2520Kantarcioglu%2520and%2520Kangkook%2520Jee%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520graph-structured%2520data%2520has%2520driven%2520interest%2520in%2520graph%2520learning%2520and%250Asynthetic%2520data%2520generation.%2520While%2520successful%2520in%2520text%2520and%2520image%2520domains%252C%250Asynthetic%2520graph%2520generation%2520remains%2520challenging%2520--%2520especially%2520for%2520real-world%250Agraphs%2520with%2520complex%252C%2520heterogeneous%2520schemas.%2520Existing%2520research%2520has%2520focused%250Amostly%2520on%2520homogeneous%2520structures%2520with%2520simple%2520attributes%252C%2520limiting%2520their%250Ausefulness%2520and%2520relevance%2520for%2520application%2520domains%2520requiring%2520semantic%2520fidelity.%250A%2520%2520In%2520this%2520research%252C%2520we%2520introduce%2520ProvCreator%252C%2520a%2520synthetic%2520graph%2520framework%250Adesigned%2520for%2520complex%2520heterogeneous%2520graphs%2520with%2520high-dimensional%2520node%2520and%2520edge%250Aattributes.%2520ProvCreator%2520formulates%2520graph%2520synthesis%2520as%2520a%2520sequence%2520generation%250Atask%252C%2520enabling%2520the%2520use%2520of%2520transformer-based%2520large%2520language%2520models.%2520It%2520features%250Aa%2520versatile%2520graph-to-sequence%2520encoder-decoder%2520that%25201.%2520losslessly%2520encodes%2520graph%250Astructure%2520and%2520attributes%252C%25202.%2520efficiently%2520compresses%2520large%2520graphs%2520for%2520contextual%250Amodeling%252C%2520and%25203.%2520supports%2520end-to-end%252C%2520learnable%2520graph%2520generation.%250A%2520%2520To%2520validate%2520our%2520research%252C%2520we%2520evaluate%2520ProvCreator%2520on%2520two%2520challenging%2520domains%253A%250Asystem%2520provenance%2520graphs%2520in%2520cybersecurity%2520and%2520knowledge%2520graphs%2520from%250AIntelliGraph%2520Benchmark%2520Dataset.%2520In%2520both%2520cases%252C%2520ProvCreator%2520captures%2520intricate%250Adependencies%2520between%2520structure%2520and%2520semantics%252C%2520enabling%2520the%2520generation%2520of%250Arealistic%2520and%2520privacy-aware%2520synthetic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROVCREATOR%3A%20Synthesizing%20Complex%20Heterogenous%20Graphs%20with%20Node%20and%20Edge%0A%20%20Attributes&entry.906535625=Tianhao%20Wang%20and%20Simon%20Klancher%20and%20Kunal%20Mukherjee%20and%20Josh%20Wiedemeier%20and%20Feng%20Chen%20and%20Murat%20Kantarcioglu%20and%20Kangkook%20Jee&entry.1292438233=%20%20The%20rise%20of%20graph-structured%20data%20has%20driven%20interest%20in%20graph%20learning%20and%0Asynthetic%20data%20generation.%20While%20successful%20in%20text%20and%20image%20domains%2C%0Asynthetic%20graph%20generation%20remains%20challenging%20--%20especially%20for%20real-world%0Agraphs%20with%20complex%2C%20heterogeneous%20schemas.%20Existing%20research%20has%20focused%0Amostly%20on%20homogeneous%20structures%20with%20simple%20attributes%2C%20limiting%20their%0Ausefulness%20and%20relevance%20for%20application%20domains%20requiring%20semantic%20fidelity.%0A%20%20In%20this%20research%2C%20we%20introduce%20ProvCreator%2C%20a%20synthetic%20graph%20framework%0Adesigned%20for%20complex%20heterogeneous%20graphs%20with%20high-dimensional%20node%20and%20edge%0Aattributes.%20ProvCreator%20formulates%20graph%20synthesis%20as%20a%20sequence%20generation%0Atask%2C%20enabling%20the%20use%20of%20transformer-based%20large%20language%20models.%20It%20features%0Aa%20versatile%20graph-to-sequence%20encoder-decoder%20that%201.%20losslessly%20encodes%20graph%0Astructure%20and%20attributes%2C%202.%20efficiently%20compresses%20large%20graphs%20for%20contextual%0Amodeling%2C%20and%203.%20supports%20end-to-end%2C%20learnable%20graph%20generation.%0A%20%20To%20validate%20our%20research%2C%20we%20evaluate%20ProvCreator%20on%20two%20challenging%20domains%3A%0Asystem%20provenance%20graphs%20in%20cybersecurity%20and%20knowledge%20graphs%20from%0AIntelliGraph%20Benchmark%20Dataset.%20In%20both%20cases%2C%20ProvCreator%20captures%20intricate%0Adependencies%20between%20structure%20and%20semantics%2C%20enabling%20the%20generation%20of%0Arealistic%20and%20privacy-aware%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20967v1&entry.124074799=Read"},
{"title": "Crop Pest Classification Using Deep Learning Techniques: A Review", "author": "Muhammad Hassam Ejaz and Muhammad Bilal and Usman Habib", "abstract": "  Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.\n", "link": "http://arxiv.org/abs/2507.01494v2", "date": "2025-07-28", "relevancy": 2.1242, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4295}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4293}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crop%20Pest%20Classification%20Using%20Deep%20Learning%20Techniques%3A%20A%20Review&body=Title%3A%20Crop%20Pest%20Classification%20Using%20Deep%20Learning%20Techniques%3A%20A%20Review%0AAuthor%3A%20Muhammad%20Hassam%20Ejaz%20and%20Muhammad%20Bilal%20and%20Usman%20Habib%0AAbstract%3A%20%20%20Insect%20pests%20continue%20to%20bring%20a%20serious%20threat%20to%20crop%20yields%20around%20the%0Aworld%2C%20and%20traditional%20methods%20for%20monitoring%20them%20are%20often%20slow%2C%20manual%2C%20and%0Adifficult%20to%20scale.%20In%20recent%20years%2C%20deep%20learning%20has%20emerged%20as%20a%20powerful%0Asolution%2C%20with%20techniques%20like%20convolutional%20neural%20networks%20%28CNNs%29%2C%20vision%0Atransformers%20%28ViTs%29%2C%20and%20hybrid%20models%20gaining%20popularity%20for%20automating%20pest%0Adetection.%20This%20review%20looks%20at%2037%20carefully%20selected%20studies%20published%20between%0A2018%20and%202025%2C%20all%20focused%20on%20AI-based%20pest%20classification.%20The%20selected%0Aresearch%20is%20organized%20by%20crop%20type%2C%20pest%20species%2C%20model%20architecture%2C%20dataset%0Ausage%2C%20and%20key%20technical%20challenges.%20The%20early%20studies%20relied%20heavily%20on%20CNNs%0Abut%20latest%20work%20is%20shifting%20toward%20hybrid%20and%20transformer-based%20models%20that%0Adeliver%20higher%20accuracy%20and%20better%20contextual%20understanding.%20Still%2C%20challenges%0Alike%20imbalanced%20datasets%2C%20difficulty%20in%20detecting%20small%20pests%2C%20limited%0Ageneralizability%2C%20and%20deployment%20on%20edge%20devices%20remain%20significant%20hurdles.%0AOverall%2C%20this%20review%20offers%20a%20structured%20overview%20of%20the%20field%2C%20highlights%0Auseful%20datasets%2C%20and%20outlines%20the%20key%20challenges%20and%20future%20directions%20for%0AAI-based%20pest%20monitoring%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrop%2520Pest%2520Classification%2520Using%2520Deep%2520Learning%2520Techniques%253A%2520A%2520Review%26entry.906535625%3DMuhammad%2520Hassam%2520Ejaz%2520and%2520Muhammad%2520Bilal%2520and%2520Usman%2520Habib%26entry.1292438233%3D%2520%2520Insect%2520pests%2520continue%2520to%2520bring%2520a%2520serious%2520threat%2520to%2520crop%2520yields%2520around%2520the%250Aworld%252C%2520and%2520traditional%2520methods%2520for%2520monitoring%2520them%2520are%2520often%2520slow%252C%2520manual%252C%2520and%250Adifficult%2520to%2520scale.%2520In%2520recent%2520years%252C%2520deep%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%250Asolution%252C%2520with%2520techniques%2520like%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%252C%2520vision%250Atransformers%2520%2528ViTs%2529%252C%2520and%2520hybrid%2520models%2520gaining%2520popularity%2520for%2520automating%2520pest%250Adetection.%2520This%2520review%2520looks%2520at%252037%2520carefully%2520selected%2520studies%2520published%2520between%250A2018%2520and%25202025%252C%2520all%2520focused%2520on%2520AI-based%2520pest%2520classification.%2520The%2520selected%250Aresearch%2520is%2520organized%2520by%2520crop%2520type%252C%2520pest%2520species%252C%2520model%2520architecture%252C%2520dataset%250Ausage%252C%2520and%2520key%2520technical%2520challenges.%2520The%2520early%2520studies%2520relied%2520heavily%2520on%2520CNNs%250Abut%2520latest%2520work%2520is%2520shifting%2520toward%2520hybrid%2520and%2520transformer-based%2520models%2520that%250Adeliver%2520higher%2520accuracy%2520and%2520better%2520contextual%2520understanding.%2520Still%252C%2520challenges%250Alike%2520imbalanced%2520datasets%252C%2520difficulty%2520in%2520detecting%2520small%2520pests%252C%2520limited%250Ageneralizability%252C%2520and%2520deployment%2520on%2520edge%2520devices%2520remain%2520significant%2520hurdles.%250AOverall%252C%2520this%2520review%2520offers%2520a%2520structured%2520overview%2520of%2520the%2520field%252C%2520highlights%250Auseful%2520datasets%252C%2520and%2520outlines%2520the%2520key%2520challenges%2520and%2520future%2520directions%2520for%250AAI-based%2520pest%2520monitoring%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crop%20Pest%20Classification%20Using%20Deep%20Learning%20Techniques%3A%20A%20Review&entry.906535625=Muhammad%20Hassam%20Ejaz%20and%20Muhammad%20Bilal%20and%20Usman%20Habib&entry.1292438233=%20%20Insect%20pests%20continue%20to%20bring%20a%20serious%20threat%20to%20crop%20yields%20around%20the%0Aworld%2C%20and%20traditional%20methods%20for%20monitoring%20them%20are%20often%20slow%2C%20manual%2C%20and%0Adifficult%20to%20scale.%20In%20recent%20years%2C%20deep%20learning%20has%20emerged%20as%20a%20powerful%0Asolution%2C%20with%20techniques%20like%20convolutional%20neural%20networks%20%28CNNs%29%2C%20vision%0Atransformers%20%28ViTs%29%2C%20and%20hybrid%20models%20gaining%20popularity%20for%20automating%20pest%0Adetection.%20This%20review%20looks%20at%2037%20carefully%20selected%20studies%20published%20between%0A2018%20and%202025%2C%20all%20focused%20on%20AI-based%20pest%20classification.%20The%20selected%0Aresearch%20is%20organized%20by%20crop%20type%2C%20pest%20species%2C%20model%20architecture%2C%20dataset%0Ausage%2C%20and%20key%20technical%20challenges.%20The%20early%20studies%20relied%20heavily%20on%20CNNs%0Abut%20latest%20work%20is%20shifting%20toward%20hybrid%20and%20transformer-based%20models%20that%0Adeliver%20higher%20accuracy%20and%20better%20contextual%20understanding.%20Still%2C%20challenges%0Alike%20imbalanced%20datasets%2C%20difficulty%20in%20detecting%20small%20pests%2C%20limited%0Ageneralizability%2C%20and%20deployment%20on%20edge%20devices%20remain%20significant%20hurdles.%0AOverall%2C%20this%20review%20offers%20a%20structured%20overview%20of%20the%20field%2C%20highlights%0Auseful%20datasets%2C%20and%20outlines%20the%20key%20challenges%20and%20future%20directions%20for%0AAI-based%20pest%20monitoring%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01494v2&entry.124074799=Read"},
{"title": "Modular Delta Merging with Orthogonal Constraints: A Scalable Framework\n  for Continual and Reversible Model Composition", "author": "Haris Khan and Shumaila Asif and Sadia Asif", "abstract": "  In real-world machine learning deployments, models must be continually\nupdated, composed, and when required, selectively undone. However, existing\napproaches to model merging and continual learning often suffer from task\ninterference, catastrophic forgetting, or lack of reversibility. We propose\nModular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework\nthat enables scalable, interference-free, and reversible composition of\nfine-tuned models. Each task-specific model is encoded as a delta from a shared\nbase and projected into an orthogonal subspace to eliminate conflict. These\nprojected deltas are then merged via gradient-based optimization to form a\nunified model that retains performance across tasks. Our approach supports\ncontinual integration of new models, structured unmerging for compliance such\nas GDPR requirements, and model stability via elastic weight consolidation and\nsynthetic replay. Extensive experiments on vision and natural language\nprocessing benchmarks demonstrate that MDM-OC outperforms prior baselines in\naccuracy, backward transfer, and unmerge fidelity, while remaining\nmemory-efficient and computationally tractable. This framework offers a\nprincipled solution for modular and compliant AI system design.\n", "link": "http://arxiv.org/abs/2507.20997v1", "date": "2025-07-28", "relevancy": 2.1223, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5726}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Delta%20Merging%20with%20Orthogonal%20Constraints%3A%20A%20Scalable%20Framework%0A%20%20for%20Continual%20and%20Reversible%20Model%20Composition&body=Title%3A%20Modular%20Delta%20Merging%20with%20Orthogonal%20Constraints%3A%20A%20Scalable%20Framework%0A%20%20for%20Continual%20and%20Reversible%20Model%20Composition%0AAuthor%3A%20Haris%20Khan%20and%20Shumaila%20Asif%20and%20Sadia%20Asif%0AAbstract%3A%20%20%20In%20real-world%20machine%20learning%20deployments%2C%20models%20must%20be%20continually%0Aupdated%2C%20composed%2C%20and%20when%20required%2C%20selectively%20undone.%20However%2C%20existing%0Aapproaches%20to%20model%20merging%20and%20continual%20learning%20often%20suffer%20from%20task%0Ainterference%2C%20catastrophic%20forgetting%2C%20or%20lack%20of%20reversibility.%20We%20propose%0AModular%20Delta%20Merging%20with%20Orthogonal%20Constraints%20%28MDM-OC%29%2C%20a%20novel%20framework%0Athat%20enables%20scalable%2C%20interference-free%2C%20and%20reversible%20composition%20of%0Afine-tuned%20models.%20Each%20task-specific%20model%20is%20encoded%20as%20a%20delta%20from%20a%20shared%0Abase%20and%20projected%20into%20an%20orthogonal%20subspace%20to%20eliminate%20conflict.%20These%0Aprojected%20deltas%20are%20then%20merged%20via%20gradient-based%20optimization%20to%20form%20a%0Aunified%20model%20that%20retains%20performance%20across%20tasks.%20Our%20approach%20supports%0Acontinual%20integration%20of%20new%20models%2C%20structured%20unmerging%20for%20compliance%20such%0Aas%20GDPR%20requirements%2C%20and%20model%20stability%20via%20elastic%20weight%20consolidation%20and%0Asynthetic%20replay.%20Extensive%20experiments%20on%20vision%20and%20natural%20language%0Aprocessing%20benchmarks%20demonstrate%20that%20MDM-OC%20outperforms%20prior%20baselines%20in%0Aaccuracy%2C%20backward%20transfer%2C%20and%20unmerge%20fidelity%2C%20while%20remaining%0Amemory-efficient%20and%20computationally%20tractable.%20This%20framework%20offers%20a%0Aprincipled%20solution%20for%20modular%20and%20compliant%20AI%20system%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Delta%2520Merging%2520with%2520Orthogonal%2520Constraints%253A%2520A%2520Scalable%2520Framework%250A%2520%2520for%2520Continual%2520and%2520Reversible%2520Model%2520Composition%26entry.906535625%3DHaris%2520Khan%2520and%2520Shumaila%2520Asif%2520and%2520Sadia%2520Asif%26entry.1292438233%3D%2520%2520In%2520real-world%2520machine%2520learning%2520deployments%252C%2520models%2520must%2520be%2520continually%250Aupdated%252C%2520composed%252C%2520and%2520when%2520required%252C%2520selectively%2520undone.%2520However%252C%2520existing%250Aapproaches%2520to%2520model%2520merging%2520and%2520continual%2520learning%2520often%2520suffer%2520from%2520task%250Ainterference%252C%2520catastrophic%2520forgetting%252C%2520or%2520lack%2520of%2520reversibility.%2520We%2520propose%250AModular%2520Delta%2520Merging%2520with%2520Orthogonal%2520Constraints%2520%2528MDM-OC%2529%252C%2520a%2520novel%2520framework%250Athat%2520enables%2520scalable%252C%2520interference-free%252C%2520and%2520reversible%2520composition%2520of%250Afine-tuned%2520models.%2520Each%2520task-specific%2520model%2520is%2520encoded%2520as%2520a%2520delta%2520from%2520a%2520shared%250Abase%2520and%2520projected%2520into%2520an%2520orthogonal%2520subspace%2520to%2520eliminate%2520conflict.%2520These%250Aprojected%2520deltas%2520are%2520then%2520merged%2520via%2520gradient-based%2520optimization%2520to%2520form%2520a%250Aunified%2520model%2520that%2520retains%2520performance%2520across%2520tasks.%2520Our%2520approach%2520supports%250Acontinual%2520integration%2520of%2520new%2520models%252C%2520structured%2520unmerging%2520for%2520compliance%2520such%250Aas%2520GDPR%2520requirements%252C%2520and%2520model%2520stability%2520via%2520elastic%2520weight%2520consolidation%2520and%250Asynthetic%2520replay.%2520Extensive%2520experiments%2520on%2520vision%2520and%2520natural%2520language%250Aprocessing%2520benchmarks%2520demonstrate%2520that%2520MDM-OC%2520outperforms%2520prior%2520baselines%2520in%250Aaccuracy%252C%2520backward%2520transfer%252C%2520and%2520unmerge%2520fidelity%252C%2520while%2520remaining%250Amemory-efficient%2520and%2520computationally%2520tractable.%2520This%2520framework%2520offers%2520a%250Aprincipled%2520solution%2520for%2520modular%2520and%2520compliant%2520AI%2520system%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Delta%20Merging%20with%20Orthogonal%20Constraints%3A%20A%20Scalable%20Framework%0A%20%20for%20Continual%20and%20Reversible%20Model%20Composition&entry.906535625=Haris%20Khan%20and%20Shumaila%20Asif%20and%20Sadia%20Asif&entry.1292438233=%20%20In%20real-world%20machine%20learning%20deployments%2C%20models%20must%20be%20continually%0Aupdated%2C%20composed%2C%20and%20when%20required%2C%20selectively%20undone.%20However%2C%20existing%0Aapproaches%20to%20model%20merging%20and%20continual%20learning%20often%20suffer%20from%20task%0Ainterference%2C%20catastrophic%20forgetting%2C%20or%20lack%20of%20reversibility.%20We%20propose%0AModular%20Delta%20Merging%20with%20Orthogonal%20Constraints%20%28MDM-OC%29%2C%20a%20novel%20framework%0Athat%20enables%20scalable%2C%20interference-free%2C%20and%20reversible%20composition%20of%0Afine-tuned%20models.%20Each%20task-specific%20model%20is%20encoded%20as%20a%20delta%20from%20a%20shared%0Abase%20and%20projected%20into%20an%20orthogonal%20subspace%20to%20eliminate%20conflict.%20These%0Aprojected%20deltas%20are%20then%20merged%20via%20gradient-based%20optimization%20to%20form%20a%0Aunified%20model%20that%20retains%20performance%20across%20tasks.%20Our%20approach%20supports%0Acontinual%20integration%20of%20new%20models%2C%20structured%20unmerging%20for%20compliance%20such%0Aas%20GDPR%20requirements%2C%20and%20model%20stability%20via%20elastic%20weight%20consolidation%20and%0Asynthetic%20replay.%20Extensive%20experiments%20on%20vision%20and%20natural%20language%0Aprocessing%20benchmarks%20demonstrate%20that%20MDM-OC%20outperforms%20prior%20baselines%20in%0Aaccuracy%2C%20backward%20transfer%2C%20and%20unmerge%20fidelity%2C%20while%20remaining%0Amemory-efficient%20and%20computationally%20tractable.%20This%20framework%20offers%20a%0Aprincipled%20solution%20for%20modular%20and%20compliant%20AI%20system%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20997v1&entry.124074799=Read"},
{"title": "Exploring text-to-image generation for historical document image\n  retrieval", "author": "Melissa Cote and Alexandra Branzan Albu", "abstract": "  Attribute-based document image retrieval (ABDIR) was recently proposed as an\nalternative to query-by-example (QBE) searches, the dominant document image\nretrieval (DIR) paradigm. One drawback of QBE searches is that they require\nsample query documents on hand that may not be available. ABDIR aims to offer\nusers a flexible way to retrieve document images based on memorable visual\nfeatures of document contents, describing document images with combinations of\nvisual attributes determined via convolutional neural network (CNN)-based\nbinary classifiers. We present an exploratory study of the use of generative AI\nto bridge the gap between QBE and ABDIR, focusing on historical documents as a\nuse case for their diversity and uniqueness in visual features. We hypothesize\nthat text-to-image (T2I) generation can be leveraged to create query document\nimages using text prompts based on ABDIR-like attributes. We propose T2I-QBE,\nwhich uses Leonardo.Ai as the T2I generator with prompts that include a rough\ndescription of the desired document type and a list of the desired ABDIR-style\nattributes. This creates query images that are then used within the traditional\nQBE paradigm, which compares CNN-extracted query features to those of the\ndocument images in the dataset to retrieve the most relevant documents.\nExperiments on the HisIR19 dataset of historical documents confirm our\nhypothesis and suggest that T2I-QBE is a viable option for historical document\nimage retrieval. To the authors' knowledge, this is the first attempt at\nutilizing T2I generation for DIR.\n", "link": "http://arxiv.org/abs/2507.20934v1", "date": "2025-07-28", "relevancy": 2.1167, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20text-to-image%20generation%20for%20historical%20document%20image%0A%20%20retrieval&body=Title%3A%20Exploring%20text-to-image%20generation%20for%20historical%20document%20image%0A%20%20retrieval%0AAuthor%3A%20Melissa%20Cote%20and%20Alexandra%20Branzan%20Albu%0AAbstract%3A%20%20%20Attribute-based%20document%20image%20retrieval%20%28ABDIR%29%20was%20recently%20proposed%20as%20an%0Aalternative%20to%20query-by-example%20%28QBE%29%20searches%2C%20the%20dominant%20document%20image%0Aretrieval%20%28DIR%29%20paradigm.%20One%20drawback%20of%20QBE%20searches%20is%20that%20they%20require%0Asample%20query%20documents%20on%20hand%20that%20may%20not%20be%20available.%20ABDIR%20aims%20to%20offer%0Ausers%20a%20flexible%20way%20to%20retrieve%20document%20images%20based%20on%20memorable%20visual%0Afeatures%20of%20document%20contents%2C%20describing%20document%20images%20with%20combinations%20of%0Avisual%20attributes%20determined%20via%20convolutional%20neural%20network%20%28CNN%29-based%0Abinary%20classifiers.%20We%20present%20an%20exploratory%20study%20of%20the%20use%20of%20generative%20AI%0Ato%20bridge%20the%20gap%20between%20QBE%20and%20ABDIR%2C%20focusing%20on%20historical%20documents%20as%20a%0Ause%20case%20for%20their%20diversity%20and%20uniqueness%20in%20visual%20features.%20We%20hypothesize%0Athat%20text-to-image%20%28T2I%29%20generation%20can%20be%20leveraged%20to%20create%20query%20document%0Aimages%20using%20text%20prompts%20based%20on%20ABDIR-like%20attributes.%20We%20propose%20T2I-QBE%2C%0Awhich%20uses%20Leonardo.Ai%20as%20the%20T2I%20generator%20with%20prompts%20that%20include%20a%20rough%0Adescription%20of%20the%20desired%20document%20type%20and%20a%20list%20of%20the%20desired%20ABDIR-style%0Aattributes.%20This%20creates%20query%20images%20that%20are%20then%20used%20within%20the%20traditional%0AQBE%20paradigm%2C%20which%20compares%20CNN-extracted%20query%20features%20to%20those%20of%20the%0Adocument%20images%20in%20the%20dataset%20to%20retrieve%20the%20most%20relevant%20documents.%0AExperiments%20on%20the%20HisIR19%20dataset%20of%20historical%20documents%20confirm%20our%0Ahypothesis%20and%20suggest%20that%20T2I-QBE%20is%20a%20viable%20option%20for%20historical%20document%0Aimage%20retrieval.%20To%20the%20authors%27%20knowledge%2C%20this%20is%20the%20first%20attempt%20at%0Autilizing%20T2I%20generation%20for%20DIR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520text-to-image%2520generation%2520for%2520historical%2520document%2520image%250A%2520%2520retrieval%26entry.906535625%3DMelissa%2520Cote%2520and%2520Alexandra%2520Branzan%2520Albu%26entry.1292438233%3D%2520%2520Attribute-based%2520document%2520image%2520retrieval%2520%2528ABDIR%2529%2520was%2520recently%2520proposed%2520as%2520an%250Aalternative%2520to%2520query-by-example%2520%2528QBE%2529%2520searches%252C%2520the%2520dominant%2520document%2520image%250Aretrieval%2520%2528DIR%2529%2520paradigm.%2520One%2520drawback%2520of%2520QBE%2520searches%2520is%2520that%2520they%2520require%250Asample%2520query%2520documents%2520on%2520hand%2520that%2520may%2520not%2520be%2520available.%2520ABDIR%2520aims%2520to%2520offer%250Ausers%2520a%2520flexible%2520way%2520to%2520retrieve%2520document%2520images%2520based%2520on%2520memorable%2520visual%250Afeatures%2520of%2520document%2520contents%252C%2520describing%2520document%2520images%2520with%2520combinations%2520of%250Avisual%2520attributes%2520determined%2520via%2520convolutional%2520neural%2520network%2520%2528CNN%2529-based%250Abinary%2520classifiers.%2520We%2520present%2520an%2520exploratory%2520study%2520of%2520the%2520use%2520of%2520generative%2520AI%250Ato%2520bridge%2520the%2520gap%2520between%2520QBE%2520and%2520ABDIR%252C%2520focusing%2520on%2520historical%2520documents%2520as%2520a%250Ause%2520case%2520for%2520their%2520diversity%2520and%2520uniqueness%2520in%2520visual%2520features.%2520We%2520hypothesize%250Athat%2520text-to-image%2520%2528T2I%2529%2520generation%2520can%2520be%2520leveraged%2520to%2520create%2520query%2520document%250Aimages%2520using%2520text%2520prompts%2520based%2520on%2520ABDIR-like%2520attributes.%2520We%2520propose%2520T2I-QBE%252C%250Awhich%2520uses%2520Leonardo.Ai%2520as%2520the%2520T2I%2520generator%2520with%2520prompts%2520that%2520include%2520a%2520rough%250Adescription%2520of%2520the%2520desired%2520document%2520type%2520and%2520a%2520list%2520of%2520the%2520desired%2520ABDIR-style%250Aattributes.%2520This%2520creates%2520query%2520images%2520that%2520are%2520then%2520used%2520within%2520the%2520traditional%250AQBE%2520paradigm%252C%2520which%2520compares%2520CNN-extracted%2520query%2520features%2520to%2520those%2520of%2520the%250Adocument%2520images%2520in%2520the%2520dataset%2520to%2520retrieve%2520the%2520most%2520relevant%2520documents.%250AExperiments%2520on%2520the%2520HisIR19%2520dataset%2520of%2520historical%2520documents%2520confirm%2520our%250Ahypothesis%2520and%2520suggest%2520that%2520T2I-QBE%2520is%2520a%2520viable%2520option%2520for%2520historical%2520document%250Aimage%2520retrieval.%2520To%2520the%2520authors%2527%2520knowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520at%250Autilizing%2520T2I%2520generation%2520for%2520DIR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20text-to-image%20generation%20for%20historical%20document%20image%0A%20%20retrieval&entry.906535625=Melissa%20Cote%20and%20Alexandra%20Branzan%20Albu&entry.1292438233=%20%20Attribute-based%20document%20image%20retrieval%20%28ABDIR%29%20was%20recently%20proposed%20as%20an%0Aalternative%20to%20query-by-example%20%28QBE%29%20searches%2C%20the%20dominant%20document%20image%0Aretrieval%20%28DIR%29%20paradigm.%20One%20drawback%20of%20QBE%20searches%20is%20that%20they%20require%0Asample%20query%20documents%20on%20hand%20that%20may%20not%20be%20available.%20ABDIR%20aims%20to%20offer%0Ausers%20a%20flexible%20way%20to%20retrieve%20document%20images%20based%20on%20memorable%20visual%0Afeatures%20of%20document%20contents%2C%20describing%20document%20images%20with%20combinations%20of%0Avisual%20attributes%20determined%20via%20convolutional%20neural%20network%20%28CNN%29-based%0Abinary%20classifiers.%20We%20present%20an%20exploratory%20study%20of%20the%20use%20of%20generative%20AI%0Ato%20bridge%20the%20gap%20between%20QBE%20and%20ABDIR%2C%20focusing%20on%20historical%20documents%20as%20a%0Ause%20case%20for%20their%20diversity%20and%20uniqueness%20in%20visual%20features.%20We%20hypothesize%0Athat%20text-to-image%20%28T2I%29%20generation%20can%20be%20leveraged%20to%20create%20query%20document%0Aimages%20using%20text%20prompts%20based%20on%20ABDIR-like%20attributes.%20We%20propose%20T2I-QBE%2C%0Awhich%20uses%20Leonardo.Ai%20as%20the%20T2I%20generator%20with%20prompts%20that%20include%20a%20rough%0Adescription%20of%20the%20desired%20document%20type%20and%20a%20list%20of%20the%20desired%20ABDIR-style%0Aattributes.%20This%20creates%20query%20images%20that%20are%20then%20used%20within%20the%20traditional%0AQBE%20paradigm%2C%20which%20compares%20CNN-extracted%20query%20features%20to%20those%20of%20the%0Adocument%20images%20in%20the%20dataset%20to%20retrieve%20the%20most%20relevant%20documents.%0AExperiments%20on%20the%20HisIR19%20dataset%20of%20historical%20documents%20confirm%20our%0Ahypothesis%20and%20suggest%20that%20T2I-QBE%20is%20a%20viable%20option%20for%20historical%20document%0Aimage%20retrieval.%20To%20the%20authors%27%20knowledge%2C%20this%20is%20the%20first%20attempt%20at%0Autilizing%20T2I%20generation%20for%20DIR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20934v1&entry.124074799=Read"},
{"title": "Edge-guided Low-light Image Enhancement with Inertial Bregman\n  Alternating Linearized Minimization", "author": "Chaoyan Huang and Zhongming Wu and Tieyong Zeng", "abstract": "  Prior-based methods for low-light image enhancement often face challenges in\nextracting available prior information from dim images. To overcome this\nlimitation, we introduce a simple yet effective Retinex model with the proposed\nedge extraction prior. More specifically, we design an edge extraction network\nto capture the fine edge features from the low-light image directly. Building\nupon the Retinex theory, we decompose the low-light image into its illumination\nand reflectance components and introduce an edge-guided Retinex model for\nenhancing low-light images. To solve the proposed model, we propose a novel\ninertial Bregman alternating linearized minimization algorithm. This algorithm\naddresses the optimization problem associated with the edge-guided Retinex\nmodel, enabling effective enhancement of low-light images. Through rigorous\ntheoretical analysis, we establish the convergence properties of the algorithm.\nBesides, we prove that the proposed algorithm converges to a stationary point\nof the problem through nonconvex optimization theory. Furthermore, extensive\nexperiments are conducted on multiple real-world low-light image datasets to\ndemonstrate the efficiency and superiority of the proposed scheme.\n", "link": "http://arxiv.org/abs/2403.01142v2", "date": "2025-07-28", "relevancy": 2.11, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5437}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5322}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-guided%20Low-light%20Image%20Enhancement%20with%20Inertial%20Bregman%0A%20%20Alternating%20Linearized%20Minimization&body=Title%3A%20Edge-guided%20Low-light%20Image%20Enhancement%20with%20Inertial%20Bregman%0A%20%20Alternating%20Linearized%20Minimization%0AAuthor%3A%20Chaoyan%20Huang%20and%20Zhongming%20Wu%20and%20Tieyong%20Zeng%0AAbstract%3A%20%20%20Prior-based%20methods%20for%20low-light%20image%20enhancement%20often%20face%20challenges%20in%0Aextracting%20available%20prior%20information%20from%20dim%20images.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20a%20simple%20yet%20effective%20Retinex%20model%20with%20the%20proposed%0Aedge%20extraction%20prior.%20More%20specifically%2C%20we%20design%20an%20edge%20extraction%20network%0Ato%20capture%20the%20fine%20edge%20features%20from%20the%20low-light%20image%20directly.%20Building%0Aupon%20the%20Retinex%20theory%2C%20we%20decompose%20the%20low-light%20image%20into%20its%20illumination%0Aand%20reflectance%20components%20and%20introduce%20an%20edge-guided%20Retinex%20model%20for%0Aenhancing%20low-light%20images.%20To%20solve%20the%20proposed%20model%2C%20we%20propose%20a%20novel%0Ainertial%20Bregman%20alternating%20linearized%20minimization%20algorithm.%20This%20algorithm%0Aaddresses%20the%20optimization%20problem%20associated%20with%20the%20edge-guided%20Retinex%0Amodel%2C%20enabling%20effective%20enhancement%20of%20low-light%20images.%20Through%20rigorous%0Atheoretical%20analysis%2C%20we%20establish%20the%20convergence%20properties%20of%20the%20algorithm.%0ABesides%2C%20we%20prove%20that%20the%20proposed%20algorithm%20converges%20to%20a%20stationary%20point%0Aof%20the%20problem%20through%20nonconvex%20optimization%20theory.%20Furthermore%2C%20extensive%0Aexperiments%20are%20conducted%20on%20multiple%20real-world%20low-light%20image%20datasets%20to%0Ademonstrate%20the%20efficiency%20and%20superiority%20of%20the%20proposed%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-guided%2520Low-light%2520Image%2520Enhancement%2520with%2520Inertial%2520Bregman%250A%2520%2520Alternating%2520Linearized%2520Minimization%26entry.906535625%3DChaoyan%2520Huang%2520and%2520Zhongming%2520Wu%2520and%2520Tieyong%2520Zeng%26entry.1292438233%3D%2520%2520Prior-based%2520methods%2520for%2520low-light%2520image%2520enhancement%2520often%2520face%2520challenges%2520in%250Aextracting%2520available%2520prior%2520information%2520from%2520dim%2520images.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520introduce%2520a%2520simple%2520yet%2520effective%2520Retinex%2520model%2520with%2520the%2520proposed%250Aedge%2520extraction%2520prior.%2520More%2520specifically%252C%2520we%2520design%2520an%2520edge%2520extraction%2520network%250Ato%2520capture%2520the%2520fine%2520edge%2520features%2520from%2520the%2520low-light%2520image%2520directly.%2520Building%250Aupon%2520the%2520Retinex%2520theory%252C%2520we%2520decompose%2520the%2520low-light%2520image%2520into%2520its%2520illumination%250Aand%2520reflectance%2520components%2520and%2520introduce%2520an%2520edge-guided%2520Retinex%2520model%2520for%250Aenhancing%2520low-light%2520images.%2520To%2520solve%2520the%2520proposed%2520model%252C%2520we%2520propose%2520a%2520novel%250Ainertial%2520Bregman%2520alternating%2520linearized%2520minimization%2520algorithm.%2520This%2520algorithm%250Aaddresses%2520the%2520optimization%2520problem%2520associated%2520with%2520the%2520edge-guided%2520Retinex%250Amodel%252C%2520enabling%2520effective%2520enhancement%2520of%2520low-light%2520images.%2520Through%2520rigorous%250Atheoretical%2520analysis%252C%2520we%2520establish%2520the%2520convergence%2520properties%2520of%2520the%2520algorithm.%250ABesides%252C%2520we%2520prove%2520that%2520the%2520proposed%2520algorithm%2520converges%2520to%2520a%2520stationary%2520point%250Aof%2520the%2520problem%2520through%2520nonconvex%2520optimization%2520theory.%2520Furthermore%252C%2520extensive%250Aexperiments%2520are%2520conducted%2520on%2520multiple%2520real-world%2520low-light%2520image%2520datasets%2520to%250Ademonstrate%2520the%2520efficiency%2520and%2520superiority%2520of%2520the%2520proposed%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-guided%20Low-light%20Image%20Enhancement%20with%20Inertial%20Bregman%0A%20%20Alternating%20Linearized%20Minimization&entry.906535625=Chaoyan%20Huang%20and%20Zhongming%20Wu%20and%20Tieyong%20Zeng&entry.1292438233=%20%20Prior-based%20methods%20for%20low-light%20image%20enhancement%20often%20face%20challenges%20in%0Aextracting%20available%20prior%20information%20from%20dim%20images.%20To%20overcome%20this%0Alimitation%2C%20we%20introduce%20a%20simple%20yet%20effective%20Retinex%20model%20with%20the%20proposed%0Aedge%20extraction%20prior.%20More%20specifically%2C%20we%20design%20an%20edge%20extraction%20network%0Ato%20capture%20the%20fine%20edge%20features%20from%20the%20low-light%20image%20directly.%20Building%0Aupon%20the%20Retinex%20theory%2C%20we%20decompose%20the%20low-light%20image%20into%20its%20illumination%0Aand%20reflectance%20components%20and%20introduce%20an%20edge-guided%20Retinex%20model%20for%0Aenhancing%20low-light%20images.%20To%20solve%20the%20proposed%20model%2C%20we%20propose%20a%20novel%0Ainertial%20Bregman%20alternating%20linearized%20minimization%20algorithm.%20This%20algorithm%0Aaddresses%20the%20optimization%20problem%20associated%20with%20the%20edge-guided%20Retinex%0Amodel%2C%20enabling%20effective%20enhancement%20of%20low-light%20images.%20Through%20rigorous%0Atheoretical%20analysis%2C%20we%20establish%20the%20convergence%20properties%20of%20the%20algorithm.%0ABesides%2C%20we%20prove%20that%20the%20proposed%20algorithm%20converges%20to%20a%20stationary%20point%0Aof%20the%20problem%20through%20nonconvex%20optimization%20theory.%20Furthermore%2C%20extensive%0Aexperiments%20are%20conducted%20on%20multiple%20real-world%20low-light%20image%20datasets%20to%0Ademonstrate%20the%20efficiency%20and%20superiority%20of%20the%20proposed%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01142v2&entry.124074799=Read"},
{"title": "Improving Adversarial Robustness Through Adaptive Learning-Driven\n  Multi-Teacher Knowledge Distillation", "author": "Hayat Ullah and Syed Muhammad Talha Zaidi and Arslan Munir", "abstract": "  Convolutional neural networks (CNNs) excel in computer vision but are\nsusceptible to adversarial attacks, crafted perturbations designed to mislead\npredictions. Despite advances in adversarial training, a gap persists between\nmodel accuracy and robustness. To mitigate this issue, in this paper, we\npresent a multi-teacher adversarial robustness distillation using an adaptive\nlearning strategy. Specifically, our proposed method first trained multiple\nclones of a baseline CNN model using an adversarial training strategy on a pool\nof perturbed data acquired through different adversarial attacks. Once trained,\nthese adversarially trained models are used as teacher models to supervise the\nlearning of a student model on clean data using multi-teacher knowledge\ndistillation. To ensure an effective robustness distillation, we design an\nadaptive learning strategy that controls the knowledge contribution of each\nmodel by assigning weights as per their prediction precision. Distilling\nknowledge from adversarially pre-trained teacher models not only enhances the\nlearning capabilities of the student model but also empowers it with the\ncapacity to withstand different adversarial attacks, despite having no exposure\nto adversarial data. To verify our claims, we extensively evaluated our\nproposed method on MNIST-Digits and Fashion-MNIST datasets across diverse\nexperimental settings. The obtained results exhibit the efficacy of our\nmulti-teacher adversarial distillation and adaptive learning strategy,\nenhancing CNNs' adversarial robustness against various adversarial attacks.\n", "link": "http://arxiv.org/abs/2507.20996v1", "date": "2025-07-28", "relevancy": 2.0857, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5318}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5158}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Adversarial%20Robustness%20Through%20Adaptive%20Learning-Driven%0A%20%20Multi-Teacher%20Knowledge%20Distillation&body=Title%3A%20Improving%20Adversarial%20Robustness%20Through%20Adaptive%20Learning-Driven%0A%20%20Multi-Teacher%20Knowledge%20Distillation%0AAuthor%3A%20Hayat%20Ullah%20and%20Syed%20Muhammad%20Talha%20Zaidi%20and%20Arslan%20Munir%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20excel%20in%20computer%20vision%20but%20are%0Asusceptible%20to%20adversarial%20attacks%2C%20crafted%20perturbations%20designed%20to%20mislead%0Apredictions.%20Despite%20advances%20in%20adversarial%20training%2C%20a%20gap%20persists%20between%0Amodel%20accuracy%20and%20robustness.%20To%20mitigate%20this%20issue%2C%20in%20this%20paper%2C%20we%0Apresent%20a%20multi-teacher%20adversarial%20robustness%20distillation%20using%20an%20adaptive%0Alearning%20strategy.%20Specifically%2C%20our%20proposed%20method%20first%20trained%20multiple%0Aclones%20of%20a%20baseline%20CNN%20model%20using%20an%20adversarial%20training%20strategy%20on%20a%20pool%0Aof%20perturbed%20data%20acquired%20through%20different%20adversarial%20attacks.%20Once%20trained%2C%0Athese%20adversarially%20trained%20models%20are%20used%20as%20teacher%20models%20to%20supervise%20the%0Alearning%20of%20a%20student%20model%20on%20clean%20data%20using%20multi-teacher%20knowledge%0Adistillation.%20To%20ensure%20an%20effective%20robustness%20distillation%2C%20we%20design%20an%0Aadaptive%20learning%20strategy%20that%20controls%20the%20knowledge%20contribution%20of%20each%0Amodel%20by%20assigning%20weights%20as%20per%20their%20prediction%20precision.%20Distilling%0Aknowledge%20from%20adversarially%20pre-trained%20teacher%20models%20not%20only%20enhances%20the%0Alearning%20capabilities%20of%20the%20student%20model%20but%20also%20empowers%20it%20with%20the%0Acapacity%20to%20withstand%20different%20adversarial%20attacks%2C%20despite%20having%20no%20exposure%0Ato%20adversarial%20data.%20To%20verify%20our%20claims%2C%20we%20extensively%20evaluated%20our%0Aproposed%20method%20on%20MNIST-Digits%20and%20Fashion-MNIST%20datasets%20across%20diverse%0Aexperimental%20settings.%20The%20obtained%20results%20exhibit%20the%20efficacy%20of%20our%0Amulti-teacher%20adversarial%20distillation%20and%20adaptive%20learning%20strategy%2C%0Aenhancing%20CNNs%27%20adversarial%20robustness%20against%20various%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Adversarial%2520Robustness%2520Through%2520Adaptive%2520Learning-Driven%250A%2520%2520Multi-Teacher%2520Knowledge%2520Distillation%26entry.906535625%3DHayat%2520Ullah%2520and%2520Syed%2520Muhammad%2520Talha%2520Zaidi%2520and%2520Arslan%2520Munir%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520excel%2520in%2520computer%2520vision%2520but%2520are%250Asusceptible%2520to%2520adversarial%2520attacks%252C%2520crafted%2520perturbations%2520designed%2520to%2520mislead%250Apredictions.%2520Despite%2520advances%2520in%2520adversarial%2520training%252C%2520a%2520gap%2520persists%2520between%250Amodel%2520accuracy%2520and%2520robustness.%2520To%2520mitigate%2520this%2520issue%252C%2520in%2520this%2520paper%252C%2520we%250Apresent%2520a%2520multi-teacher%2520adversarial%2520robustness%2520distillation%2520using%2520an%2520adaptive%250Alearning%2520strategy.%2520Specifically%252C%2520our%2520proposed%2520method%2520first%2520trained%2520multiple%250Aclones%2520of%2520a%2520baseline%2520CNN%2520model%2520using%2520an%2520adversarial%2520training%2520strategy%2520on%2520a%2520pool%250Aof%2520perturbed%2520data%2520acquired%2520through%2520different%2520adversarial%2520attacks.%2520Once%2520trained%252C%250Athese%2520adversarially%2520trained%2520models%2520are%2520used%2520as%2520teacher%2520models%2520to%2520supervise%2520the%250Alearning%2520of%2520a%2520student%2520model%2520on%2520clean%2520data%2520using%2520multi-teacher%2520knowledge%250Adistillation.%2520To%2520ensure%2520an%2520effective%2520robustness%2520distillation%252C%2520we%2520design%2520an%250Aadaptive%2520learning%2520strategy%2520that%2520controls%2520the%2520knowledge%2520contribution%2520of%2520each%250Amodel%2520by%2520assigning%2520weights%2520as%2520per%2520their%2520prediction%2520precision.%2520Distilling%250Aknowledge%2520from%2520adversarially%2520pre-trained%2520teacher%2520models%2520not%2520only%2520enhances%2520the%250Alearning%2520capabilities%2520of%2520the%2520student%2520model%2520but%2520also%2520empowers%2520it%2520with%2520the%250Acapacity%2520to%2520withstand%2520different%2520adversarial%2520attacks%252C%2520despite%2520having%2520no%2520exposure%250Ato%2520adversarial%2520data.%2520To%2520verify%2520our%2520claims%252C%2520we%2520extensively%2520evaluated%2520our%250Aproposed%2520method%2520on%2520MNIST-Digits%2520and%2520Fashion-MNIST%2520datasets%2520across%2520diverse%250Aexperimental%2520settings.%2520The%2520obtained%2520results%2520exhibit%2520the%2520efficacy%2520of%2520our%250Amulti-teacher%2520adversarial%2520distillation%2520and%2520adaptive%2520learning%2520strategy%252C%250Aenhancing%2520CNNs%2527%2520adversarial%2520robustness%2520against%2520various%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Adversarial%20Robustness%20Through%20Adaptive%20Learning-Driven%0A%20%20Multi-Teacher%20Knowledge%20Distillation&entry.906535625=Hayat%20Ullah%20and%20Syed%20Muhammad%20Talha%20Zaidi%20and%20Arslan%20Munir&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20excel%20in%20computer%20vision%20but%20are%0Asusceptible%20to%20adversarial%20attacks%2C%20crafted%20perturbations%20designed%20to%20mislead%0Apredictions.%20Despite%20advances%20in%20adversarial%20training%2C%20a%20gap%20persists%20between%0Amodel%20accuracy%20and%20robustness.%20To%20mitigate%20this%20issue%2C%20in%20this%20paper%2C%20we%0Apresent%20a%20multi-teacher%20adversarial%20robustness%20distillation%20using%20an%20adaptive%0Alearning%20strategy.%20Specifically%2C%20our%20proposed%20method%20first%20trained%20multiple%0Aclones%20of%20a%20baseline%20CNN%20model%20using%20an%20adversarial%20training%20strategy%20on%20a%20pool%0Aof%20perturbed%20data%20acquired%20through%20different%20adversarial%20attacks.%20Once%20trained%2C%0Athese%20adversarially%20trained%20models%20are%20used%20as%20teacher%20models%20to%20supervise%20the%0Alearning%20of%20a%20student%20model%20on%20clean%20data%20using%20multi-teacher%20knowledge%0Adistillation.%20To%20ensure%20an%20effective%20robustness%20distillation%2C%20we%20design%20an%0Aadaptive%20learning%20strategy%20that%20controls%20the%20knowledge%20contribution%20of%20each%0Amodel%20by%20assigning%20weights%20as%20per%20their%20prediction%20precision.%20Distilling%0Aknowledge%20from%20adversarially%20pre-trained%20teacher%20models%20not%20only%20enhances%20the%0Alearning%20capabilities%20of%20the%20student%20model%20but%20also%20empowers%20it%20with%20the%0Acapacity%20to%20withstand%20different%20adversarial%20attacks%2C%20despite%20having%20no%20exposure%0Ato%20adversarial%20data.%20To%20verify%20our%20claims%2C%20we%20extensively%20evaluated%20our%0Aproposed%20method%20on%20MNIST-Digits%20and%20Fashion-MNIST%20datasets%20across%20diverse%0Aexperimental%20settings.%20The%20obtained%20results%20exhibit%20the%20efficacy%20of%20our%0Amulti-teacher%20adversarial%20distillation%20and%20adaptive%20learning%20strategy%2C%0Aenhancing%20CNNs%27%20adversarial%20robustness%20against%20various%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20996v1&entry.124074799=Read"},
{"title": "SCANet: Split Coordinate Attention Network for Building Footprint\n  Extraction", "author": "Chunshi Wang and Bin Zhao and Shuxue Ding", "abstract": "  Building footprint extraction holds immense significance in remote sensing\nimage analysis and has great value in urban planning, land use, environmental\nprotection and disaster assessment. Despite the progress made by conventional\nand deep learning approaches in this field, they continue to encounter\nsignificant challenges. This paper introduces a novel plug-and-play attention\nmodule, Split Coordinate Attention (SCA), which ingeniously captures spatially\nremote interactions by employing two spatial range of pooling kernels,\nstrategically encoding each channel along x and y planes, and separately\nperforms a series of split operations for each feature group, thus enabling\nmore efficient semantic feature extraction. By inserting into a 2D CNN to form\nan effective SCANet, our SCANet outperforms recent SOTA methods on the public\nWuhan University (WHU) Building Dataset and Massachusetts Building Dataset in\nterms of various metrics. Particularly SCANet achieves the best IoU, 91.61% and\n75.49% for the two datasets. Our code is available at\nhttps://github.com/AiEson/SCANet\n", "link": "http://arxiv.org/abs/2507.20809v1", "date": "2025-07-28", "relevancy": 2.0852, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5311}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5146}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCANet%3A%20Split%20Coordinate%20Attention%20Network%20for%20Building%20Footprint%0A%20%20Extraction&body=Title%3A%20SCANet%3A%20Split%20Coordinate%20Attention%20Network%20for%20Building%20Footprint%0A%20%20Extraction%0AAuthor%3A%20Chunshi%20Wang%20and%20Bin%20Zhao%20and%20Shuxue%20Ding%0AAbstract%3A%20%20%20Building%20footprint%20extraction%20holds%20immense%20significance%20in%20remote%20sensing%0Aimage%20analysis%20and%20has%20great%20value%20in%20urban%20planning%2C%20land%20use%2C%20environmental%0Aprotection%20and%20disaster%20assessment.%20Despite%20the%20progress%20made%20by%20conventional%0Aand%20deep%20learning%20approaches%20in%20this%20field%2C%20they%20continue%20to%20encounter%0Asignificant%20challenges.%20This%20paper%20introduces%20a%20novel%20plug-and-play%20attention%0Amodule%2C%20Split%20Coordinate%20Attention%20%28SCA%29%2C%20which%20ingeniously%20captures%20spatially%0Aremote%20interactions%20by%20employing%20two%20spatial%20range%20of%20pooling%20kernels%2C%0Astrategically%20encoding%20each%20channel%20along%20x%20and%20y%20planes%2C%20and%20separately%0Aperforms%20a%20series%20of%20split%20operations%20for%20each%20feature%20group%2C%20thus%20enabling%0Amore%20efficient%20semantic%20feature%20extraction.%20By%20inserting%20into%20a%202D%20CNN%20to%20form%0Aan%20effective%20SCANet%2C%20our%20SCANet%20outperforms%20recent%20SOTA%20methods%20on%20the%20public%0AWuhan%20University%20%28WHU%29%20Building%20Dataset%20and%20Massachusetts%20Building%20Dataset%20in%0Aterms%20of%20various%20metrics.%20Particularly%20SCANet%20achieves%20the%20best%20IoU%2C%2091.61%25%20and%0A75.49%25%20for%20the%20two%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AiEson/SCANet%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCANet%253A%2520Split%2520Coordinate%2520Attention%2520Network%2520for%2520Building%2520Footprint%250A%2520%2520Extraction%26entry.906535625%3DChunshi%2520Wang%2520and%2520Bin%2520Zhao%2520and%2520Shuxue%2520Ding%26entry.1292438233%3D%2520%2520Building%2520footprint%2520extraction%2520holds%2520immense%2520significance%2520in%2520remote%2520sensing%250Aimage%2520analysis%2520and%2520has%2520great%2520value%2520in%2520urban%2520planning%252C%2520land%2520use%252C%2520environmental%250Aprotection%2520and%2520disaster%2520assessment.%2520Despite%2520the%2520progress%2520made%2520by%2520conventional%250Aand%2520deep%2520learning%2520approaches%2520in%2520this%2520field%252C%2520they%2520continue%2520to%2520encounter%250Asignificant%2520challenges.%2520This%2520paper%2520introduces%2520a%2520novel%2520plug-and-play%2520attention%250Amodule%252C%2520Split%2520Coordinate%2520Attention%2520%2528SCA%2529%252C%2520which%2520ingeniously%2520captures%2520spatially%250Aremote%2520interactions%2520by%2520employing%2520two%2520spatial%2520range%2520of%2520pooling%2520kernels%252C%250Astrategically%2520encoding%2520each%2520channel%2520along%2520x%2520and%2520y%2520planes%252C%2520and%2520separately%250Aperforms%2520a%2520series%2520of%2520split%2520operations%2520for%2520each%2520feature%2520group%252C%2520thus%2520enabling%250Amore%2520efficient%2520semantic%2520feature%2520extraction.%2520By%2520inserting%2520into%2520a%25202D%2520CNN%2520to%2520form%250Aan%2520effective%2520SCANet%252C%2520our%2520SCANet%2520outperforms%2520recent%2520SOTA%2520methods%2520on%2520the%2520public%250AWuhan%2520University%2520%2528WHU%2529%2520Building%2520Dataset%2520and%2520Massachusetts%2520Building%2520Dataset%2520in%250Aterms%2520of%2520various%2520metrics.%2520Particularly%2520SCANet%2520achieves%2520the%2520best%2520IoU%252C%252091.61%2525%2520and%250A75.49%2525%2520for%2520the%2520two%2520datasets.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/AiEson/SCANet%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCANet%3A%20Split%20Coordinate%20Attention%20Network%20for%20Building%20Footprint%0A%20%20Extraction&entry.906535625=Chunshi%20Wang%20and%20Bin%20Zhao%20and%20Shuxue%20Ding&entry.1292438233=%20%20Building%20footprint%20extraction%20holds%20immense%20significance%20in%20remote%20sensing%0Aimage%20analysis%20and%20has%20great%20value%20in%20urban%20planning%2C%20land%20use%2C%20environmental%0Aprotection%20and%20disaster%20assessment.%20Despite%20the%20progress%20made%20by%20conventional%0Aand%20deep%20learning%20approaches%20in%20this%20field%2C%20they%20continue%20to%20encounter%0Asignificant%20challenges.%20This%20paper%20introduces%20a%20novel%20plug-and-play%20attention%0Amodule%2C%20Split%20Coordinate%20Attention%20%28SCA%29%2C%20which%20ingeniously%20captures%20spatially%0Aremote%20interactions%20by%20employing%20two%20spatial%20range%20of%20pooling%20kernels%2C%0Astrategically%20encoding%20each%20channel%20along%20x%20and%20y%20planes%2C%20and%20separately%0Aperforms%20a%20series%20of%20split%20operations%20for%20each%20feature%20group%2C%20thus%20enabling%0Amore%20efficient%20semantic%20feature%20extraction.%20By%20inserting%20into%20a%202D%20CNN%20to%20form%0Aan%20effective%20SCANet%2C%20our%20SCANet%20outperforms%20recent%20SOTA%20methods%20on%20the%20public%0AWuhan%20University%20%28WHU%29%20Building%20Dataset%20and%20Massachusetts%20Building%20Dataset%20in%0Aterms%20of%20various%20metrics.%20Particularly%20SCANet%20achieves%20the%20best%20IoU%2C%2091.61%25%20and%0A75.49%25%20for%20the%20two%20datasets.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/AiEson/SCANet%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20809v1&entry.124074799=Read"},
{"title": "Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment:\n  A Benchmark", "author": "Ali Ismail-Fawaz and Maxime Devanne and Stefano Berretti and Jonathan Weber and Germain Forestier", "abstract": "  Automated assessment of human motion plays a vital role in rehabilitation,\nenabling objective evaluation of patient performance and progress. Unlike\ngeneral human activity recognition, rehabilitation motion assessment focuses on\nanalyzing the quality of movement within the same action class, requiring the\ndetection of subtle deviations from ideal motion. Recent advances in deep\nlearning and video-based skeleton extraction have opened new possibilities for\naccessible, scalable motion assessment using affordable devices such as\nsmartphones or webcams. However, the field lacks standardized benchmarks,\nconsistent evaluation protocols, and reproducible methodologies, limiting\nprogress and comparability across studies. In this work, we address these gaps\nby (i) aggregating existing rehabilitation datasets into a unified archive\ncalled Rehab-Pile, (ii) proposing a general benchmarking framework for\nevaluating deep learning methods in this domain, and (iii) conducting extensive\nbenchmarking of multiple architectures across classification and regression\ntasks. All datasets and implementations are released to the community to\nsupport transparency and reproducibility. This paper aims to establish a solid\nfoundation for future research in automated rehabilitation assessment and\nfoster the development of reliable, accessible, and personalized rehabilitation\nsolutions. The datasets, source-code and results of this article are all\npublicly available.\n", "link": "http://arxiv.org/abs/2507.21018v1", "date": "2025-07-28", "relevancy": 2.0837, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5413}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5253}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Skeleton%20Based%20Human%20Motion%20Rehabilitation%20Assessment%3A%0A%20%20A%20Benchmark&body=Title%3A%20Deep%20Learning%20for%20Skeleton%20Based%20Human%20Motion%20Rehabilitation%20Assessment%3A%0A%20%20A%20Benchmark%0AAuthor%3A%20Ali%20Ismail-Fawaz%20and%20Maxime%20Devanne%20and%20Stefano%20Berretti%20and%20Jonathan%20Weber%20and%20Germain%20Forestier%0AAbstract%3A%20%20%20Automated%20assessment%20of%20human%20motion%20plays%20a%20vital%20role%20in%20rehabilitation%2C%0Aenabling%20objective%20evaluation%20of%20patient%20performance%20and%20progress.%20Unlike%0Ageneral%20human%20activity%20recognition%2C%20rehabilitation%20motion%20assessment%20focuses%20on%0Aanalyzing%20the%20quality%20of%20movement%20within%20the%20same%20action%20class%2C%20requiring%20the%0Adetection%20of%20subtle%20deviations%20from%20ideal%20motion.%20Recent%20advances%20in%20deep%0Alearning%20and%20video-based%20skeleton%20extraction%20have%20opened%20new%20possibilities%20for%0Aaccessible%2C%20scalable%20motion%20assessment%20using%20affordable%20devices%20such%20as%0Asmartphones%20or%20webcams.%20However%2C%20the%20field%20lacks%20standardized%20benchmarks%2C%0Aconsistent%20evaluation%20protocols%2C%20and%20reproducible%20methodologies%2C%20limiting%0Aprogress%20and%20comparability%20across%20studies.%20In%20this%20work%2C%20we%20address%20these%20gaps%0Aby%20%28i%29%20aggregating%20existing%20rehabilitation%20datasets%20into%20a%20unified%20archive%0Acalled%20Rehab-Pile%2C%20%28ii%29%20proposing%20a%20general%20benchmarking%20framework%20for%0Aevaluating%20deep%20learning%20methods%20in%20this%20domain%2C%20and%20%28iii%29%20conducting%20extensive%0Abenchmarking%20of%20multiple%20architectures%20across%20classification%20and%20regression%0Atasks.%20All%20datasets%20and%20implementations%20are%20released%20to%20the%20community%20to%0Asupport%20transparency%20and%20reproducibility.%20This%20paper%20aims%20to%20establish%20a%20solid%0Afoundation%20for%20future%20research%20in%20automated%20rehabilitation%20assessment%20and%0Afoster%20the%20development%20of%20reliable%2C%20accessible%2C%20and%20personalized%20rehabilitation%0Asolutions.%20The%20datasets%2C%20source-code%20and%20results%20of%20this%20article%20are%20all%0Apublicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Skeleton%2520Based%2520Human%2520Motion%2520Rehabilitation%2520Assessment%253A%250A%2520%2520A%2520Benchmark%26entry.906535625%3DAli%2520Ismail-Fawaz%2520and%2520Maxime%2520Devanne%2520and%2520Stefano%2520Berretti%2520and%2520Jonathan%2520Weber%2520and%2520Germain%2520Forestier%26entry.1292438233%3D%2520%2520Automated%2520assessment%2520of%2520human%2520motion%2520plays%2520a%2520vital%2520role%2520in%2520rehabilitation%252C%250Aenabling%2520objective%2520evaluation%2520of%2520patient%2520performance%2520and%2520progress.%2520Unlike%250Ageneral%2520human%2520activity%2520recognition%252C%2520rehabilitation%2520motion%2520assessment%2520focuses%2520on%250Aanalyzing%2520the%2520quality%2520of%2520movement%2520within%2520the%2520same%2520action%2520class%252C%2520requiring%2520the%250Adetection%2520of%2520subtle%2520deviations%2520from%2520ideal%2520motion.%2520Recent%2520advances%2520in%2520deep%250Alearning%2520and%2520video-based%2520skeleton%2520extraction%2520have%2520opened%2520new%2520possibilities%2520for%250Aaccessible%252C%2520scalable%2520motion%2520assessment%2520using%2520affordable%2520devices%2520such%2520as%250Asmartphones%2520or%2520webcams.%2520However%252C%2520the%2520field%2520lacks%2520standardized%2520benchmarks%252C%250Aconsistent%2520evaluation%2520protocols%252C%2520and%2520reproducible%2520methodologies%252C%2520limiting%250Aprogress%2520and%2520comparability%2520across%2520studies.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520gaps%250Aby%2520%2528i%2529%2520aggregating%2520existing%2520rehabilitation%2520datasets%2520into%2520a%2520unified%2520archive%250Acalled%2520Rehab-Pile%252C%2520%2528ii%2529%2520proposing%2520a%2520general%2520benchmarking%2520framework%2520for%250Aevaluating%2520deep%2520learning%2520methods%2520in%2520this%2520domain%252C%2520and%2520%2528iii%2529%2520conducting%2520extensive%250Abenchmarking%2520of%2520multiple%2520architectures%2520across%2520classification%2520and%2520regression%250Atasks.%2520All%2520datasets%2520and%2520implementations%2520are%2520released%2520to%2520the%2520community%2520to%250Asupport%2520transparency%2520and%2520reproducibility.%2520This%2520paper%2520aims%2520to%2520establish%2520a%2520solid%250Afoundation%2520for%2520future%2520research%2520in%2520automated%2520rehabilitation%2520assessment%2520and%250Afoster%2520the%2520development%2520of%2520reliable%252C%2520accessible%252C%2520and%2520personalized%2520rehabilitation%250Asolutions.%2520The%2520datasets%252C%2520source-code%2520and%2520results%2520of%2520this%2520article%2520are%2520all%250Apublicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Skeleton%20Based%20Human%20Motion%20Rehabilitation%20Assessment%3A%0A%20%20A%20Benchmark&entry.906535625=Ali%20Ismail-Fawaz%20and%20Maxime%20Devanne%20and%20Stefano%20Berretti%20and%20Jonathan%20Weber%20and%20Germain%20Forestier&entry.1292438233=%20%20Automated%20assessment%20of%20human%20motion%20plays%20a%20vital%20role%20in%20rehabilitation%2C%0Aenabling%20objective%20evaluation%20of%20patient%20performance%20and%20progress.%20Unlike%0Ageneral%20human%20activity%20recognition%2C%20rehabilitation%20motion%20assessment%20focuses%20on%0Aanalyzing%20the%20quality%20of%20movement%20within%20the%20same%20action%20class%2C%20requiring%20the%0Adetection%20of%20subtle%20deviations%20from%20ideal%20motion.%20Recent%20advances%20in%20deep%0Alearning%20and%20video-based%20skeleton%20extraction%20have%20opened%20new%20possibilities%20for%0Aaccessible%2C%20scalable%20motion%20assessment%20using%20affordable%20devices%20such%20as%0Asmartphones%20or%20webcams.%20However%2C%20the%20field%20lacks%20standardized%20benchmarks%2C%0Aconsistent%20evaluation%20protocols%2C%20and%20reproducible%20methodologies%2C%20limiting%0Aprogress%20and%20comparability%20across%20studies.%20In%20this%20work%2C%20we%20address%20these%20gaps%0Aby%20%28i%29%20aggregating%20existing%20rehabilitation%20datasets%20into%20a%20unified%20archive%0Acalled%20Rehab-Pile%2C%20%28ii%29%20proposing%20a%20general%20benchmarking%20framework%20for%0Aevaluating%20deep%20learning%20methods%20in%20this%20domain%2C%20and%20%28iii%29%20conducting%20extensive%0Abenchmarking%20of%20multiple%20architectures%20across%20classification%20and%20regression%0Atasks.%20All%20datasets%20and%20implementations%20are%20released%20to%20the%20community%20to%0Asupport%20transparency%20and%20reproducibility.%20This%20paper%20aims%20to%20establish%20a%20solid%0Afoundation%20for%20future%20research%20in%20automated%20rehabilitation%20assessment%20and%0Afoster%20the%20development%20of%20reliable%2C%20accessible%2C%20and%20personalized%20rehabilitation%0Asolutions.%20The%20datasets%2C%20source-code%20and%20results%20of%20this%20article%20are%20all%0Apublicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21018v1&entry.124074799=Read"},
{"title": "Locally Adaptive Conformal Inference for Operator Models", "author": "Trevor Harris and Yan Liu", "abstract": "  Operator models are regression algorithms for functional data and have become\na key tool for emulating large-scale dynamical systems. Recent advances in deep\nneural operators have dramatically improved the accuracy and scalability of\noperator modeling, but lack an inherent notion of predictive uncertainty. We\nintroduce Local Spectral Conformal Inference (LSCI), a new framework for\nlocally adaptive, distribution-free uncertainty quantification for neural\noperator models. LSCI uses projection-based depth scoring and localized\nconformal inference to generate function-valued prediction sets with\nstatistical guarantees. We prove approximate finite-sample marginal coverage\nunder local exchangeability, and demonstrate significant gains in adaptivity\nand coverage across synthetic and real-world operator learning tasks.\n", "link": "http://arxiv.org/abs/2507.20975v1", "date": "2025-07-28", "relevancy": 2.0835, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5446}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Adaptive%20Conformal%20Inference%20for%20Operator%20Models&body=Title%3A%20Locally%20Adaptive%20Conformal%20Inference%20for%20Operator%20Models%0AAuthor%3A%20Trevor%20Harris%20and%20Yan%20Liu%0AAbstract%3A%20%20%20Operator%20models%20are%20regression%20algorithms%20for%20functional%20data%20and%20have%20become%0Aa%20key%20tool%20for%20emulating%20large-scale%20dynamical%20systems.%20Recent%20advances%20in%20deep%0Aneural%20operators%20have%20dramatically%20improved%20the%20accuracy%20and%20scalability%20of%0Aoperator%20modeling%2C%20but%20lack%20an%20inherent%20notion%20of%20predictive%20uncertainty.%20We%0Aintroduce%20Local%20Spectral%20Conformal%20Inference%20%28LSCI%29%2C%20a%20new%20framework%20for%0Alocally%20adaptive%2C%20distribution-free%20uncertainty%20quantification%20for%20neural%0Aoperator%20models.%20LSCI%20uses%20projection-based%20depth%20scoring%20and%20localized%0Aconformal%20inference%20to%20generate%20function-valued%20prediction%20sets%20with%0Astatistical%20guarantees.%20We%20prove%20approximate%20finite-sample%20marginal%20coverage%0Aunder%20local%20exchangeability%2C%20and%20demonstrate%20significant%20gains%20in%20adaptivity%0Aand%20coverage%20across%20synthetic%20and%20real-world%20operator%20learning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Adaptive%2520Conformal%2520Inference%2520for%2520Operator%2520Models%26entry.906535625%3DTrevor%2520Harris%2520and%2520Yan%2520Liu%26entry.1292438233%3D%2520%2520Operator%2520models%2520are%2520regression%2520algorithms%2520for%2520functional%2520data%2520and%2520have%2520become%250Aa%2520key%2520tool%2520for%2520emulating%2520large-scale%2520dynamical%2520systems.%2520Recent%2520advances%2520in%2520deep%250Aneural%2520operators%2520have%2520dramatically%2520improved%2520the%2520accuracy%2520and%2520scalability%2520of%250Aoperator%2520modeling%252C%2520but%2520lack%2520an%2520inherent%2520notion%2520of%2520predictive%2520uncertainty.%2520We%250Aintroduce%2520Local%2520Spectral%2520Conformal%2520Inference%2520%2528LSCI%2529%252C%2520a%2520new%2520framework%2520for%250Alocally%2520adaptive%252C%2520distribution-free%2520uncertainty%2520quantification%2520for%2520neural%250Aoperator%2520models.%2520LSCI%2520uses%2520projection-based%2520depth%2520scoring%2520and%2520localized%250Aconformal%2520inference%2520to%2520generate%2520function-valued%2520prediction%2520sets%2520with%250Astatistical%2520guarantees.%2520We%2520prove%2520approximate%2520finite-sample%2520marginal%2520coverage%250Aunder%2520local%2520exchangeability%252C%2520and%2520demonstrate%2520significant%2520gains%2520in%2520adaptivity%250Aand%2520coverage%2520across%2520synthetic%2520and%2520real-world%2520operator%2520learning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Adaptive%20Conformal%20Inference%20for%20Operator%20Models&entry.906535625=Trevor%20Harris%20and%20Yan%20Liu&entry.1292438233=%20%20Operator%20models%20are%20regression%20algorithms%20for%20functional%20data%20and%20have%20become%0Aa%20key%20tool%20for%20emulating%20large-scale%20dynamical%20systems.%20Recent%20advances%20in%20deep%0Aneural%20operators%20have%20dramatically%20improved%20the%20accuracy%20and%20scalability%20of%0Aoperator%20modeling%2C%20but%20lack%20an%20inherent%20notion%20of%20predictive%20uncertainty.%20We%0Aintroduce%20Local%20Spectral%20Conformal%20Inference%20%28LSCI%29%2C%20a%20new%20framework%20for%0Alocally%20adaptive%2C%20distribution-free%20uncertainty%20quantification%20for%20neural%0Aoperator%20models.%20LSCI%20uses%20projection-based%20depth%20scoring%20and%20localized%0Aconformal%20inference%20to%20generate%20function-valued%20prediction%20sets%20with%0Astatistical%20guarantees.%20We%20prove%20approximate%20finite-sample%20marginal%20coverage%0Aunder%20local%20exchangeability%2C%20and%20demonstrate%20significant%20gains%20in%20adaptivity%0Aand%20coverage%20across%20synthetic%20and%20real-world%20operator%20learning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20975v1&entry.124074799=Read"},
{"title": "LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient\n  LLM Fine-Tuning", "author": "Yining Huang and Bin Li and Keke Tang and Meilian Chen", "abstract": "  Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit\nsubstantially from chain-of-thought (CoT) reasoning, yet pushing their\nperformance typically requires vast data, large model sizes, and full-parameter\nfine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost,\nmost existing approaches primarily address domain adaptation or layer-wise\nallocation rather than explicitly tailoring data and parameters to different\nresponse demands. Inspired by \"Thinking, Fast and Slow,\" which characterizes\ntwo distinct modes of thought-System 1 (fast, intuitive, often automatic) and\nSystem 2 (slower, more deliberative and analytic)-we draw an analogy that\ndifferent \"subregions\" of an LLM's parameters might similarly specialize for\ntasks that demand quick, intuitive responses versus those requiring multi-step\nlogical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework\nthat partitions both data and parameters by System 1 or System 2 demands, using\nfewer yet more focused parameters for each task. Specifically, we classify task\ndata via multi-model role-playing and voting, and partition parameters based on\nimportance scoring, then adopt a two-stage fine-tuning strategy of training\nSystem 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and\nintuition and refine System 2 tasks with reinforcement learning (RL) to\nreinforce deeper logical deliberation next. Extensive experiments show that the\ntwo-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while\nmatching or surpassing SOTA PEFT baselines.\n", "link": "http://arxiv.org/abs/2507.20999v1", "date": "2025-07-28", "relevancy": 2.0829, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-PAR%3A%20A%20Flexible%20Dual-System%20LoRA%20Partitioning%20Approach%20to%20Efficient%0A%20%20LLM%20Fine-Tuning&body=Title%3A%20LoRA-PAR%3A%20A%20Flexible%20Dual-System%20LoRA%20Partitioning%20Approach%20to%20Efficient%0A%20%20LLM%20Fine-Tuning%0AAuthor%3A%20Yining%20Huang%20and%20Bin%20Li%20and%20Keke%20Tang%20and%20Meilian%20Chen%0AAbstract%3A%20%20%20Large-scale%20generative%20models%20like%20DeepSeek-R1%20and%20OpenAI-O1%20benefit%0Asubstantially%20from%20chain-of-thought%20%28CoT%29%20reasoning%2C%20yet%20pushing%20their%0Aperformance%20typically%20requires%20vast%20data%2C%20large%20model%20sizes%2C%20and%20full-parameter%0Afine-tuning.%20While%20parameter-efficient%20fine-tuning%20%28PEFT%29%20helps%20reduce%20cost%2C%0Amost%20existing%20approaches%20primarily%20address%20domain%20adaptation%20or%20layer-wise%0Aallocation%20rather%20than%20explicitly%20tailoring%20data%20and%20parameters%20to%20different%0Aresponse%20demands.%20Inspired%20by%20%22Thinking%2C%20Fast%20and%20Slow%2C%22%20which%20characterizes%0Atwo%20distinct%20modes%20of%20thought-System%201%20%28fast%2C%20intuitive%2C%20often%20automatic%29%20and%0ASystem%202%20%28slower%2C%20more%20deliberative%20and%20analytic%29-we%20draw%20an%20analogy%20that%0Adifferent%20%22subregions%22%20of%20an%20LLM%27s%20parameters%20might%20similarly%20specialize%20for%0Atasks%20that%20demand%20quick%2C%20intuitive%20responses%20versus%20those%20requiring%20multi-step%0Alogical%20reasoning.%20Therefore%2C%20we%20propose%20LoRA-PAR%2C%20a%20dual-system%20LoRA%20framework%0Athat%20partitions%20both%20data%20and%20parameters%20by%20System%201%20or%20System%202%20demands%2C%20using%0Afewer%20yet%20more%20focused%20parameters%20for%20each%20task.%20Specifically%2C%20we%20classify%20task%0Adata%20via%20multi-model%20role-playing%20and%20voting%2C%20and%20partition%20parameters%20based%20on%0Aimportance%20scoring%2C%20then%20adopt%20a%20two-stage%20fine-tuning%20strategy%20of%20training%0ASystem%201%20tasks%20with%20supervised%20fine-tuning%20%28SFT%29%20to%20enhance%20knowledge%20and%0Aintuition%20and%20refine%20System%202%20tasks%20with%20reinforcement%20learning%20%28RL%29%20to%0Areinforce%20deeper%20logical%20deliberation%20next.%20Extensive%20experiments%20show%20that%20the%0Atwo-stage%20fine-tuning%20strategy%2C%20SFT%20and%20RL%2C%20lowers%20active%20parameter%20usage%20while%0Amatching%20or%20surpassing%20SOTA%20PEFT%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-PAR%253A%2520A%2520Flexible%2520Dual-System%2520LoRA%2520Partitioning%2520Approach%2520to%2520Efficient%250A%2520%2520LLM%2520Fine-Tuning%26entry.906535625%3DYining%2520Huang%2520and%2520Bin%2520Li%2520and%2520Keke%2520Tang%2520and%2520Meilian%2520Chen%26entry.1292438233%3D%2520%2520Large-scale%2520generative%2520models%2520like%2520DeepSeek-R1%2520and%2520OpenAI-O1%2520benefit%250Asubstantially%2520from%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%252C%2520yet%2520pushing%2520their%250Aperformance%2520typically%2520requires%2520vast%2520data%252C%2520large%2520model%2520sizes%252C%2520and%2520full-parameter%250Afine-tuning.%2520While%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520helps%2520reduce%2520cost%252C%250Amost%2520existing%2520approaches%2520primarily%2520address%2520domain%2520adaptation%2520or%2520layer-wise%250Aallocation%2520rather%2520than%2520explicitly%2520tailoring%2520data%2520and%2520parameters%2520to%2520different%250Aresponse%2520demands.%2520Inspired%2520by%2520%2522Thinking%252C%2520Fast%2520and%2520Slow%252C%2522%2520which%2520characterizes%250Atwo%2520distinct%2520modes%2520of%2520thought-System%25201%2520%2528fast%252C%2520intuitive%252C%2520often%2520automatic%2529%2520and%250ASystem%25202%2520%2528slower%252C%2520more%2520deliberative%2520and%2520analytic%2529-we%2520draw%2520an%2520analogy%2520that%250Adifferent%2520%2522subregions%2522%2520of%2520an%2520LLM%2527s%2520parameters%2520might%2520similarly%2520specialize%2520for%250Atasks%2520that%2520demand%2520quick%252C%2520intuitive%2520responses%2520versus%2520those%2520requiring%2520multi-step%250Alogical%2520reasoning.%2520Therefore%252C%2520we%2520propose%2520LoRA-PAR%252C%2520a%2520dual-system%2520LoRA%2520framework%250Athat%2520partitions%2520both%2520data%2520and%2520parameters%2520by%2520System%25201%2520or%2520System%25202%2520demands%252C%2520using%250Afewer%2520yet%2520more%2520focused%2520parameters%2520for%2520each%2520task.%2520Specifically%252C%2520we%2520classify%2520task%250Adata%2520via%2520multi-model%2520role-playing%2520and%2520voting%252C%2520and%2520partition%2520parameters%2520based%2520on%250Aimportance%2520scoring%252C%2520then%2520adopt%2520a%2520two-stage%2520fine-tuning%2520strategy%2520of%2520training%250ASystem%25201%2520tasks%2520with%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520enhance%2520knowledge%2520and%250Aintuition%2520and%2520refine%2520System%25202%2520tasks%2520with%2520reinforcement%2520learning%2520%2528RL%2529%2520to%250Areinforce%2520deeper%2520logical%2520deliberation%2520next.%2520Extensive%2520experiments%2520show%2520that%2520the%250Atwo-stage%2520fine-tuning%2520strategy%252C%2520SFT%2520and%2520RL%252C%2520lowers%2520active%2520parameter%2520usage%2520while%250Amatching%2520or%2520surpassing%2520SOTA%2520PEFT%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-PAR%3A%20A%20Flexible%20Dual-System%20LoRA%20Partitioning%20Approach%20to%20Efficient%0A%20%20LLM%20Fine-Tuning&entry.906535625=Yining%20Huang%20and%20Bin%20Li%20and%20Keke%20Tang%20and%20Meilian%20Chen&entry.1292438233=%20%20Large-scale%20generative%20models%20like%20DeepSeek-R1%20and%20OpenAI-O1%20benefit%0Asubstantially%20from%20chain-of-thought%20%28CoT%29%20reasoning%2C%20yet%20pushing%20their%0Aperformance%20typically%20requires%20vast%20data%2C%20large%20model%20sizes%2C%20and%20full-parameter%0Afine-tuning.%20While%20parameter-efficient%20fine-tuning%20%28PEFT%29%20helps%20reduce%20cost%2C%0Amost%20existing%20approaches%20primarily%20address%20domain%20adaptation%20or%20layer-wise%0Aallocation%20rather%20than%20explicitly%20tailoring%20data%20and%20parameters%20to%20different%0Aresponse%20demands.%20Inspired%20by%20%22Thinking%2C%20Fast%20and%20Slow%2C%22%20which%20characterizes%0Atwo%20distinct%20modes%20of%20thought-System%201%20%28fast%2C%20intuitive%2C%20often%20automatic%29%20and%0ASystem%202%20%28slower%2C%20more%20deliberative%20and%20analytic%29-we%20draw%20an%20analogy%20that%0Adifferent%20%22subregions%22%20of%20an%20LLM%27s%20parameters%20might%20similarly%20specialize%20for%0Atasks%20that%20demand%20quick%2C%20intuitive%20responses%20versus%20those%20requiring%20multi-step%0Alogical%20reasoning.%20Therefore%2C%20we%20propose%20LoRA-PAR%2C%20a%20dual-system%20LoRA%20framework%0Athat%20partitions%20both%20data%20and%20parameters%20by%20System%201%20or%20System%202%20demands%2C%20using%0Afewer%20yet%20more%20focused%20parameters%20for%20each%20task.%20Specifically%2C%20we%20classify%20task%0Adata%20via%20multi-model%20role-playing%20and%20voting%2C%20and%20partition%20parameters%20based%20on%0Aimportance%20scoring%2C%20then%20adopt%20a%20two-stage%20fine-tuning%20strategy%20of%20training%0ASystem%201%20tasks%20with%20supervised%20fine-tuning%20%28SFT%29%20to%20enhance%20knowledge%20and%0Aintuition%20and%20refine%20System%202%20tasks%20with%20reinforcement%20learning%20%28RL%29%20to%0Areinforce%20deeper%20logical%20deliberation%20next.%20Extensive%20experiments%20show%20that%20the%0Atwo-stage%20fine-tuning%20strategy%2C%20SFT%20and%20RL%2C%20lowers%20active%20parameter%20usage%20while%0Amatching%20or%20surpassing%20SOTA%20PEFT%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20999v1&entry.124074799=Read"},
{"title": "Predicting Cognition from fMRI:A Comparative Study of Graph,\n  Transformer, and Kernel Models Across Task and Rest Conditions", "author": "Jagruti Patel and Mikkel Sch\u00f6ttner and Thomas A. W. Bolton and Patric Hagmann", "abstract": "  Predicting cognition from neuroimaging data in healthy individuals offers\ninsights into the neural mechanisms underlying cognitive abilities, with\npotential applications in precision medicine and early detection of\nneurological and psychiatric conditions. This study systematically benchmarked\nclassical machine learning (Kernel Ridge Regression (KRR)) and advanced deep\nlearning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN))\nfor cognitive prediction using Resting-state (RS), Working Memory, and Language\ntask fMRI data from the Human Connectome Project Young Adult dataset.\n  Our results, based on R2 scores, Pearson correlation coefficient, and mean\nabsolute error, revealed that task-based fMRI, eliciting neural responses\ndirectly tied to cognition, outperformed RS fMRI in predicting cognitive\nbehavior. Among the methods compared, a GNN combining structural connectivity\n(SC) and functional connectivity (FC) consistently achieved the highest\nperformance across all fMRI modalities; however, its advantage over KRR using\nFC alone was not statistically significant. The TGNN, designed to model\ntemporal dynamics with SC as a prior, performed competitively with FC-based\napproaches for task-fMRI but struggled with RS data, where its performance\naligned with the lower-performing GNN that directly used fMRI time-series data\nas node features. These findings emphasize the importance of selecting\nappropriate model architectures and feature representations to fully leverage\nthe spatial and temporal richness of neuroimaging data.\n  This study highlights the potential of multimodal graph-aware DL models to\ncombine SC and FC for cognitive prediction, as well as the promise of\nTransformer-based approaches for capturing temporal dynamics. By providing a\ncomprehensive comparison of models, this work serves as a guide for advancing\nbrain-behavior modeling using fMRI, SC and DL.\n", "link": "http://arxiv.org/abs/2507.21016v1", "date": "2025-07-28", "relevancy": 2.0811, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Cognition%20from%20fMRI%3AA%20Comparative%20Study%20of%20Graph%2C%0A%20%20Transformer%2C%20and%20Kernel%20Models%20Across%20Task%20and%20Rest%20Conditions&body=Title%3A%20Predicting%20Cognition%20from%20fMRI%3AA%20Comparative%20Study%20of%20Graph%2C%0A%20%20Transformer%2C%20and%20Kernel%20Models%20Across%20Task%20and%20Rest%20Conditions%0AAuthor%3A%20Jagruti%20Patel%20and%20Mikkel%20Sch%C3%B6ttner%20and%20Thomas%20A.%20W.%20Bolton%20and%20Patric%20Hagmann%0AAbstract%3A%20%20%20Predicting%20cognition%20from%20neuroimaging%20data%20in%20healthy%20individuals%20offers%0Ainsights%20into%20the%20neural%20mechanisms%20underlying%20cognitive%20abilities%2C%20with%0Apotential%20applications%20in%20precision%20medicine%20and%20early%20detection%20of%0Aneurological%20and%20psychiatric%20conditions.%20This%20study%20systematically%20benchmarked%0Aclassical%20machine%20learning%20%28Kernel%20Ridge%20Regression%20%28KRR%29%29%20and%20advanced%20deep%0Alearning%20%28DL%29%20models%20%28Graph%20Neural%20Networks%20%28GNN%29%20and%20Transformer-GNN%20%28TGNN%29%29%0Afor%20cognitive%20prediction%20using%20Resting-state%20%28RS%29%2C%20Working%20Memory%2C%20and%20Language%0Atask%20fMRI%20data%20from%20the%20Human%20Connectome%20Project%20Young%20Adult%20dataset.%0A%20%20Our%20results%2C%20based%20on%20R2%20scores%2C%20Pearson%20correlation%20coefficient%2C%20and%20mean%0Aabsolute%20error%2C%20revealed%20that%20task-based%20fMRI%2C%20eliciting%20neural%20responses%0Adirectly%20tied%20to%20cognition%2C%20outperformed%20RS%20fMRI%20in%20predicting%20cognitive%0Abehavior.%20Among%20the%20methods%20compared%2C%20a%20GNN%20combining%20structural%20connectivity%0A%28SC%29%20and%20functional%20connectivity%20%28FC%29%20consistently%20achieved%20the%20highest%0Aperformance%20across%20all%20fMRI%20modalities%3B%20however%2C%20its%20advantage%20over%20KRR%20using%0AFC%20alone%20was%20not%20statistically%20significant.%20The%20TGNN%2C%20designed%20to%20model%0Atemporal%20dynamics%20with%20SC%20as%20a%20prior%2C%20performed%20competitively%20with%20FC-based%0Aapproaches%20for%20task-fMRI%20but%20struggled%20with%20RS%20data%2C%20where%20its%20performance%0Aaligned%20with%20the%20lower-performing%20GNN%20that%20directly%20used%20fMRI%20time-series%20data%0Aas%20node%20features.%20These%20findings%20emphasize%20the%20importance%20of%20selecting%0Aappropriate%20model%20architectures%20and%20feature%20representations%20to%20fully%20leverage%0Athe%20spatial%20and%20temporal%20richness%20of%20neuroimaging%20data.%0A%20%20This%20study%20highlights%20the%20potential%20of%20multimodal%20graph-aware%20DL%20models%20to%0Acombine%20SC%20and%20FC%20for%20cognitive%20prediction%2C%20as%20well%20as%20the%20promise%20of%0ATransformer-based%20approaches%20for%20capturing%20temporal%20dynamics.%20By%20providing%20a%0Acomprehensive%20comparison%20of%20models%2C%20this%20work%20serves%20as%20a%20guide%20for%20advancing%0Abrain-behavior%20modeling%20using%20fMRI%2C%20SC%20and%20DL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Cognition%2520from%2520fMRI%253AA%2520Comparative%2520Study%2520of%2520Graph%252C%250A%2520%2520Transformer%252C%2520and%2520Kernel%2520Models%2520Across%2520Task%2520and%2520Rest%2520Conditions%26entry.906535625%3DJagruti%2520Patel%2520and%2520Mikkel%2520Sch%25C3%25B6ttner%2520and%2520Thomas%2520A.%2520W.%2520Bolton%2520and%2520Patric%2520Hagmann%26entry.1292438233%3D%2520%2520Predicting%2520cognition%2520from%2520neuroimaging%2520data%2520in%2520healthy%2520individuals%2520offers%250Ainsights%2520into%2520the%2520neural%2520mechanisms%2520underlying%2520cognitive%2520abilities%252C%2520with%250Apotential%2520applications%2520in%2520precision%2520medicine%2520and%2520early%2520detection%2520of%250Aneurological%2520and%2520psychiatric%2520conditions.%2520This%2520study%2520systematically%2520benchmarked%250Aclassical%2520machine%2520learning%2520%2528Kernel%2520Ridge%2520Regression%2520%2528KRR%2529%2529%2520and%2520advanced%2520deep%250Alearning%2520%2528DL%2529%2520models%2520%2528Graph%2520Neural%2520Networks%2520%2528GNN%2529%2520and%2520Transformer-GNN%2520%2528TGNN%2529%2529%250Afor%2520cognitive%2520prediction%2520using%2520Resting-state%2520%2528RS%2529%252C%2520Working%2520Memory%252C%2520and%2520Language%250Atask%2520fMRI%2520data%2520from%2520the%2520Human%2520Connectome%2520Project%2520Young%2520Adult%2520dataset.%250A%2520%2520Our%2520results%252C%2520based%2520on%2520R2%2520scores%252C%2520Pearson%2520correlation%2520coefficient%252C%2520and%2520mean%250Aabsolute%2520error%252C%2520revealed%2520that%2520task-based%2520fMRI%252C%2520eliciting%2520neural%2520responses%250Adirectly%2520tied%2520to%2520cognition%252C%2520outperformed%2520RS%2520fMRI%2520in%2520predicting%2520cognitive%250Abehavior.%2520Among%2520the%2520methods%2520compared%252C%2520a%2520GNN%2520combining%2520structural%2520connectivity%250A%2528SC%2529%2520and%2520functional%2520connectivity%2520%2528FC%2529%2520consistently%2520achieved%2520the%2520highest%250Aperformance%2520across%2520all%2520fMRI%2520modalities%253B%2520however%252C%2520its%2520advantage%2520over%2520KRR%2520using%250AFC%2520alone%2520was%2520not%2520statistically%2520significant.%2520The%2520TGNN%252C%2520designed%2520to%2520model%250Atemporal%2520dynamics%2520with%2520SC%2520as%2520a%2520prior%252C%2520performed%2520competitively%2520with%2520FC-based%250Aapproaches%2520for%2520task-fMRI%2520but%2520struggled%2520with%2520RS%2520data%252C%2520where%2520its%2520performance%250Aaligned%2520with%2520the%2520lower-performing%2520GNN%2520that%2520directly%2520used%2520fMRI%2520time-series%2520data%250Aas%2520node%2520features.%2520These%2520findings%2520emphasize%2520the%2520importance%2520of%2520selecting%250Aappropriate%2520model%2520architectures%2520and%2520feature%2520representations%2520to%2520fully%2520leverage%250Athe%2520spatial%2520and%2520temporal%2520richness%2520of%2520neuroimaging%2520data.%250A%2520%2520This%2520study%2520highlights%2520the%2520potential%2520of%2520multimodal%2520graph-aware%2520DL%2520models%2520to%250Acombine%2520SC%2520and%2520FC%2520for%2520cognitive%2520prediction%252C%2520as%2520well%2520as%2520the%2520promise%2520of%250ATransformer-based%2520approaches%2520for%2520capturing%2520temporal%2520dynamics.%2520By%2520providing%2520a%250Acomprehensive%2520comparison%2520of%2520models%252C%2520this%2520work%2520serves%2520as%2520a%2520guide%2520for%2520advancing%250Abrain-behavior%2520modeling%2520using%2520fMRI%252C%2520SC%2520and%2520DL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Cognition%20from%20fMRI%3AA%20Comparative%20Study%20of%20Graph%2C%0A%20%20Transformer%2C%20and%20Kernel%20Models%20Across%20Task%20and%20Rest%20Conditions&entry.906535625=Jagruti%20Patel%20and%20Mikkel%20Sch%C3%B6ttner%20and%20Thomas%20A.%20W.%20Bolton%20and%20Patric%20Hagmann&entry.1292438233=%20%20Predicting%20cognition%20from%20neuroimaging%20data%20in%20healthy%20individuals%20offers%0Ainsights%20into%20the%20neural%20mechanisms%20underlying%20cognitive%20abilities%2C%20with%0Apotential%20applications%20in%20precision%20medicine%20and%20early%20detection%20of%0Aneurological%20and%20psychiatric%20conditions.%20This%20study%20systematically%20benchmarked%0Aclassical%20machine%20learning%20%28Kernel%20Ridge%20Regression%20%28KRR%29%29%20and%20advanced%20deep%0Alearning%20%28DL%29%20models%20%28Graph%20Neural%20Networks%20%28GNN%29%20and%20Transformer-GNN%20%28TGNN%29%29%0Afor%20cognitive%20prediction%20using%20Resting-state%20%28RS%29%2C%20Working%20Memory%2C%20and%20Language%0Atask%20fMRI%20data%20from%20the%20Human%20Connectome%20Project%20Young%20Adult%20dataset.%0A%20%20Our%20results%2C%20based%20on%20R2%20scores%2C%20Pearson%20correlation%20coefficient%2C%20and%20mean%0Aabsolute%20error%2C%20revealed%20that%20task-based%20fMRI%2C%20eliciting%20neural%20responses%0Adirectly%20tied%20to%20cognition%2C%20outperformed%20RS%20fMRI%20in%20predicting%20cognitive%0Abehavior.%20Among%20the%20methods%20compared%2C%20a%20GNN%20combining%20structural%20connectivity%0A%28SC%29%20and%20functional%20connectivity%20%28FC%29%20consistently%20achieved%20the%20highest%0Aperformance%20across%20all%20fMRI%20modalities%3B%20however%2C%20its%20advantage%20over%20KRR%20using%0AFC%20alone%20was%20not%20statistically%20significant.%20The%20TGNN%2C%20designed%20to%20model%0Atemporal%20dynamics%20with%20SC%20as%20a%20prior%2C%20performed%20competitively%20with%20FC-based%0Aapproaches%20for%20task-fMRI%20but%20struggled%20with%20RS%20data%2C%20where%20its%20performance%0Aaligned%20with%20the%20lower-performing%20GNN%20that%20directly%20used%20fMRI%20time-series%20data%0Aas%20node%20features.%20These%20findings%20emphasize%20the%20importance%20of%20selecting%0Aappropriate%20model%20architectures%20and%20feature%20representations%20to%20fully%20leverage%0Athe%20spatial%20and%20temporal%20richness%20of%20neuroimaging%20data.%0A%20%20This%20study%20highlights%20the%20potential%20of%20multimodal%20graph-aware%20DL%20models%20to%0Acombine%20SC%20and%20FC%20for%20cognitive%20prediction%2C%20as%20well%20as%20the%20promise%20of%0ATransformer-based%20approaches%20for%20capturing%20temporal%20dynamics.%20By%20providing%20a%0Acomprehensive%20comparison%20of%20models%2C%20this%20work%20serves%20as%20a%20guide%20for%20advancing%0Abrain-behavior%20modeling%20using%20fMRI%2C%20SC%20and%20DL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21016v1&entry.124074799=Read"},
{"title": "On the similarity of bandwidth-tuned quantum kernels and classical\n  kernels", "author": "Roberto Fl\u00f3rez-Ablan and Marco Roth and Jan Schnabel", "abstract": "  Quantum kernels (QK) are widely used in quantum machine learning\napplications; yet, their potential to surpass classical machine learning\nmethods on classical datasets remains uncertain. This limitation can be\nattributed to the exponential concentration phenomenon, which can impair\ngeneralization. A common strategy to alleviate this is bandwidth tuning, which\ninvolves rescaling data points in the quantum model to improve generalization.\nIn this work, we numerically demonstrate that optimal bandwidth tuning results\nin QKs that closely resemble radial basis function (RBF) kernels, leading to a\nlack of quantum advantage over classical methods. Moreover, we reveal that the\nsize of optimal bandwidth tuning parameters further simplifies QKs, causing\nthem to behave like polynomial kernels, corresponding to a low-order Taylor\napproximation of a RBF kernel. We thoroughly investigate this for fidelity\nquantum kernels and projected quantum kernels using various data encoding\ncircuits across several classification datasets. We provide numerical evidence\nand derive a simple analytical model that elucidates how bandwidth tuning\ninfluences key quantities in classification tasks. Overall, our findings shed\nlight on the mechanisms that render QK methods classically tractable.\n", "link": "http://arxiv.org/abs/2503.05602v3", "date": "2025-07-28", "relevancy": 2.0721, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4072}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20similarity%20of%20bandwidth-tuned%20quantum%20kernels%20and%20classical%0A%20%20kernels&body=Title%3A%20On%20the%20similarity%20of%20bandwidth-tuned%20quantum%20kernels%20and%20classical%0A%20%20kernels%0AAuthor%3A%20Roberto%20Fl%C3%B3rez-Ablan%20and%20Marco%20Roth%20and%20Jan%20Schnabel%0AAbstract%3A%20%20%20Quantum%20kernels%20%28QK%29%20are%20widely%20used%20in%20quantum%20machine%20learning%0Aapplications%3B%20yet%2C%20their%20potential%20to%20surpass%20classical%20machine%20learning%0Amethods%20on%20classical%20datasets%20remains%20uncertain.%20This%20limitation%20can%20be%0Aattributed%20to%20the%20exponential%20concentration%20phenomenon%2C%20which%20can%20impair%0Ageneralization.%20A%20common%20strategy%20to%20alleviate%20this%20is%20bandwidth%20tuning%2C%20which%0Ainvolves%20rescaling%20data%20points%20in%20the%20quantum%20model%20to%20improve%20generalization.%0AIn%20this%20work%2C%20we%20numerically%20demonstrate%20that%20optimal%20bandwidth%20tuning%20results%0Ain%20QKs%20that%20closely%20resemble%20radial%20basis%20function%20%28RBF%29%20kernels%2C%20leading%20to%20a%0Alack%20of%20quantum%20advantage%20over%20classical%20methods.%20Moreover%2C%20we%20reveal%20that%20the%0Asize%20of%20optimal%20bandwidth%20tuning%20parameters%20further%20simplifies%20QKs%2C%20causing%0Athem%20to%20behave%20like%20polynomial%20kernels%2C%20corresponding%20to%20a%20low-order%20Taylor%0Aapproximation%20of%20a%20RBF%20kernel.%20We%20thoroughly%20investigate%20this%20for%20fidelity%0Aquantum%20kernels%20and%20projected%20quantum%20kernels%20using%20various%20data%20encoding%0Acircuits%20across%20several%20classification%20datasets.%20We%20provide%20numerical%20evidence%0Aand%20derive%20a%20simple%20analytical%20model%20that%20elucidates%20how%20bandwidth%20tuning%0Ainfluences%20key%20quantities%20in%20classification%20tasks.%20Overall%2C%20our%20findings%20shed%0Alight%20on%20the%20mechanisms%20that%20render%20QK%20methods%20classically%20tractable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05602v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520similarity%2520of%2520bandwidth-tuned%2520quantum%2520kernels%2520and%2520classical%250A%2520%2520kernels%26entry.906535625%3DRoberto%2520Fl%25C3%25B3rez-Ablan%2520and%2520Marco%2520Roth%2520and%2520Jan%2520Schnabel%26entry.1292438233%3D%2520%2520Quantum%2520kernels%2520%2528QK%2529%2520are%2520widely%2520used%2520in%2520quantum%2520machine%2520learning%250Aapplications%253B%2520yet%252C%2520their%2520potential%2520to%2520surpass%2520classical%2520machine%2520learning%250Amethods%2520on%2520classical%2520datasets%2520remains%2520uncertain.%2520This%2520limitation%2520can%2520be%250Aattributed%2520to%2520the%2520exponential%2520concentration%2520phenomenon%252C%2520which%2520can%2520impair%250Ageneralization.%2520A%2520common%2520strategy%2520to%2520alleviate%2520this%2520is%2520bandwidth%2520tuning%252C%2520which%250Ainvolves%2520rescaling%2520data%2520points%2520in%2520the%2520quantum%2520model%2520to%2520improve%2520generalization.%250AIn%2520this%2520work%252C%2520we%2520numerically%2520demonstrate%2520that%2520optimal%2520bandwidth%2520tuning%2520results%250Ain%2520QKs%2520that%2520closely%2520resemble%2520radial%2520basis%2520function%2520%2528RBF%2529%2520kernels%252C%2520leading%2520to%2520a%250Alack%2520of%2520quantum%2520advantage%2520over%2520classical%2520methods.%2520Moreover%252C%2520we%2520reveal%2520that%2520the%250Asize%2520of%2520optimal%2520bandwidth%2520tuning%2520parameters%2520further%2520simplifies%2520QKs%252C%2520causing%250Athem%2520to%2520behave%2520like%2520polynomial%2520kernels%252C%2520corresponding%2520to%2520a%2520low-order%2520Taylor%250Aapproximation%2520of%2520a%2520RBF%2520kernel.%2520We%2520thoroughly%2520investigate%2520this%2520for%2520fidelity%250Aquantum%2520kernels%2520and%2520projected%2520quantum%2520kernels%2520using%2520various%2520data%2520encoding%250Acircuits%2520across%2520several%2520classification%2520datasets.%2520We%2520provide%2520numerical%2520evidence%250Aand%2520derive%2520a%2520simple%2520analytical%2520model%2520that%2520elucidates%2520how%2520bandwidth%2520tuning%250Ainfluences%2520key%2520quantities%2520in%2520classification%2520tasks.%2520Overall%252C%2520our%2520findings%2520shed%250Alight%2520on%2520the%2520mechanisms%2520that%2520render%2520QK%2520methods%2520classically%2520tractable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05602v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20similarity%20of%20bandwidth-tuned%20quantum%20kernels%20and%20classical%0A%20%20kernels&entry.906535625=Roberto%20Fl%C3%B3rez-Ablan%20and%20Marco%20Roth%20and%20Jan%20Schnabel&entry.1292438233=%20%20Quantum%20kernels%20%28QK%29%20are%20widely%20used%20in%20quantum%20machine%20learning%0Aapplications%3B%20yet%2C%20their%20potential%20to%20surpass%20classical%20machine%20learning%0Amethods%20on%20classical%20datasets%20remains%20uncertain.%20This%20limitation%20can%20be%0Aattributed%20to%20the%20exponential%20concentration%20phenomenon%2C%20which%20can%20impair%0Ageneralization.%20A%20common%20strategy%20to%20alleviate%20this%20is%20bandwidth%20tuning%2C%20which%0Ainvolves%20rescaling%20data%20points%20in%20the%20quantum%20model%20to%20improve%20generalization.%0AIn%20this%20work%2C%20we%20numerically%20demonstrate%20that%20optimal%20bandwidth%20tuning%20results%0Ain%20QKs%20that%20closely%20resemble%20radial%20basis%20function%20%28RBF%29%20kernels%2C%20leading%20to%20a%0Alack%20of%20quantum%20advantage%20over%20classical%20methods.%20Moreover%2C%20we%20reveal%20that%20the%0Asize%20of%20optimal%20bandwidth%20tuning%20parameters%20further%20simplifies%20QKs%2C%20causing%0Athem%20to%20behave%20like%20polynomial%20kernels%2C%20corresponding%20to%20a%20low-order%20Taylor%0Aapproximation%20of%20a%20RBF%20kernel.%20We%20thoroughly%20investigate%20this%20for%20fidelity%0Aquantum%20kernels%20and%20projected%20quantum%20kernels%20using%20various%20data%20encoding%0Acircuits%20across%20several%20classification%20datasets.%20We%20provide%20numerical%20evidence%0Aand%20derive%20a%20simple%20analytical%20model%20that%20elucidates%20how%20bandwidth%20tuning%0Ainfluences%20key%20quantities%20in%20classification%20tasks.%20Overall%2C%20our%20findings%20shed%0Alight%20on%20the%20mechanisms%20that%20render%20QK%20methods%20classically%20tractable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05602v3&entry.124074799=Read"},
{"title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence", "author": "Huan-ang Gao and Jiayi Geng and Wenyue Hua and Mengkang Hu and Xinzhe Juan and Hongzhang Liu and Shilong Liu and Jiahao Qiu and Xuan Qi and Yiran Wu and Hongru Wang and Han Xiao and Yuhang Zhou and Shaokun Zhang and Jiayi Zhang and Jinyu Xiang and Yixiong Fang and Qiwen Zhao and Dongrui Liu and Qihan Ren and Cheng Qian and Zhenghailong Wang and Minda Hu and Huazheng Wang and Qingyun Wu and Heng Ji and Mengdi Wang", "abstract": "  Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.\n", "link": "http://arxiv.org/abs/2507.21046v1", "date": "2025-07-28", "relevancy": 2.0711, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.534}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5196}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence&body=Title%3A%20A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence%0AAuthor%3A%20Huan-ang%20Gao%20and%20Jiayi%20Geng%20and%20Wenyue%20Hua%20and%20Mengkang%20Hu%20and%20Xinzhe%20Juan%20and%20Hongzhang%20Liu%20and%20Shilong%20Liu%20and%20Jiahao%20Qiu%20and%20Xuan%20Qi%20and%20Yiran%20Wu%20and%20Hongru%20Wang%20and%20Han%20Xiao%20and%20Yuhang%20Zhou%20and%20Shaokun%20Zhang%20and%20Jiayi%20Zhang%20and%20Jinyu%20Xiang%20and%20Yixiong%20Fang%20and%20Qiwen%20Zhao%20and%20Dongrui%20Liu%20and%20Qihan%20Ren%20and%20Cheng%20Qian%20and%20Zhenghailong%20Wang%20and%20Minda%20Hu%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%20and%20Heng%20Ji%20and%20Mengdi%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20but%20remain%0Afundamentally%20static%2C%20unable%20to%20adapt%20their%20internal%20parameters%20to%20novel%20tasks%2C%0Aevolving%20knowledge%20domains%2C%20or%20dynamic%20interaction%20contexts.%20As%20LLMs%20are%0Aincreasingly%20deployed%20in%20open-ended%2C%20interactive%20environments%2C%20this%20static%0Anature%20has%20become%20a%20critical%20bottleneck%2C%20necessitating%20agents%20that%20can%0Aadaptively%20reason%2C%20act%2C%20and%20evolve%20in%20real%20time.%20This%20paradigm%20shift%20--%20from%0Ascaling%20static%20models%20to%20developing%20self-evolving%20agents%20--%20has%20sparked%20growing%0Ainterest%20in%20architectures%20and%20methods%20enabling%20continual%20learning%20and%0Aadaptation%20from%20data%2C%20interactions%2C%20and%20experiences.%20This%20survey%20provides%20the%0Afirst%20systematic%20and%20comprehensive%20review%20of%20self-evolving%20agents%2C%20organized%0Aaround%20three%20foundational%20dimensions%20--%20what%20to%20evolve%2C%20when%20to%20evolve%2C%20and%20how%0Ato%20evolve.%20We%20examine%20evolutionary%20mechanisms%20across%20agent%20components%20%28e.g.%2C%0Amodels%2C%20memory%2C%20tools%2C%20architecture%29%2C%20categorize%20adaptation%20methods%20by%20stages%0A%28e.g.%2C%20intra-test-time%2C%20inter-test-time%29%2C%20and%20analyze%20the%20algorithmic%20and%0Aarchitectural%20designs%20that%20guide%20evolutionary%20adaptation%20%28e.g.%2C%20scalar%20rewards%2C%0Atextual%20feedback%2C%20single-agent%20and%20multi-agent%20systems%29.%20Additionally%2C%20we%0Aanalyze%20evaluation%20metrics%20and%20benchmarks%20tailored%20for%20self-evolving%20agents%2C%0Ahighlight%20applications%20in%20domains%20such%20as%20coding%2C%20education%2C%20and%20healthcare%2C%0Aand%20identify%20critical%20challenges%20and%20research%20directions%20in%20safety%2C%0Ascalability%2C%20and%20co-evolutionary%20dynamics.%20By%20providing%20a%20structured%20framework%0Afor%20understanding%20and%20designing%20self-evolving%20agents%2C%20this%20survey%20establishes%20a%0Aroadmap%20for%20advancing%20adaptive%20agentic%20systems%20in%20both%20research%20and%20real-world%0Adeployments%2C%20ultimately%20shedding%20lights%20to%20pave%20the%20way%20for%20the%20realization%20of%0AArtificial%20Super%20Intelligence%20%28ASI%29%2C%20where%20agents%20evolve%20autonomously%2C%0Aperforming%20at%20or%20beyond%20human-level%20intelligence%20across%20a%20wide%20array%20of%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Self-Evolving%2520Agents%253A%2520On%2520Path%2520to%2520Artificial%2520Super%250A%2520%2520Intelligence%26entry.906535625%3DHuan-ang%2520Gao%2520and%2520Jiayi%2520Geng%2520and%2520Wenyue%2520Hua%2520and%2520Mengkang%2520Hu%2520and%2520Xinzhe%2520Juan%2520and%2520Hongzhang%2520Liu%2520and%2520Shilong%2520Liu%2520and%2520Jiahao%2520Qiu%2520and%2520Xuan%2520Qi%2520and%2520Yiran%2520Wu%2520and%2520Hongru%2520Wang%2520and%2520Han%2520Xiao%2520and%2520Yuhang%2520Zhou%2520and%2520Shaokun%2520Zhang%2520and%2520Jiayi%2520Zhang%2520and%2520Jinyu%2520Xiang%2520and%2520Yixiong%2520Fang%2520and%2520Qiwen%2520Zhao%2520and%2520Dongrui%2520Liu%2520and%2520Qihan%2520Ren%2520and%2520Cheng%2520Qian%2520and%2520Zhenghailong%2520Wang%2520and%2520Minda%2520Hu%2520and%2520Huazheng%2520Wang%2520and%2520Qingyun%2520Wu%2520and%2520Heng%2520Ji%2520and%2520Mengdi%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520but%2520remain%250Afundamentally%2520static%252C%2520unable%2520to%2520adapt%2520their%2520internal%2520parameters%2520to%2520novel%2520tasks%252C%250Aevolving%2520knowledge%2520domains%252C%2520or%2520dynamic%2520interaction%2520contexts.%2520As%2520LLMs%2520are%250Aincreasingly%2520deployed%2520in%2520open-ended%252C%2520interactive%2520environments%252C%2520this%2520static%250Anature%2520has%2520become%2520a%2520critical%2520bottleneck%252C%2520necessitating%2520agents%2520that%2520can%250Aadaptively%2520reason%252C%2520act%252C%2520and%2520evolve%2520in%2520real%2520time.%2520This%2520paradigm%2520shift%2520--%2520from%250Ascaling%2520static%2520models%2520to%2520developing%2520self-evolving%2520agents%2520--%2520has%2520sparked%2520growing%250Ainterest%2520in%2520architectures%2520and%2520methods%2520enabling%2520continual%2520learning%2520and%250Aadaptation%2520from%2520data%252C%2520interactions%252C%2520and%2520experiences.%2520This%2520survey%2520provides%2520the%250Afirst%2520systematic%2520and%2520comprehensive%2520review%2520of%2520self-evolving%2520agents%252C%2520organized%250Aaround%2520three%2520foundational%2520dimensions%2520--%2520what%2520to%2520evolve%252C%2520when%2520to%2520evolve%252C%2520and%2520how%250Ato%2520evolve.%2520We%2520examine%2520evolutionary%2520mechanisms%2520across%2520agent%2520components%2520%2528e.g.%252C%250Amodels%252C%2520memory%252C%2520tools%252C%2520architecture%2529%252C%2520categorize%2520adaptation%2520methods%2520by%2520stages%250A%2528e.g.%252C%2520intra-test-time%252C%2520inter-test-time%2529%252C%2520and%2520analyze%2520the%2520algorithmic%2520and%250Aarchitectural%2520designs%2520that%2520guide%2520evolutionary%2520adaptation%2520%2528e.g.%252C%2520scalar%2520rewards%252C%250Atextual%2520feedback%252C%2520single-agent%2520and%2520multi-agent%2520systems%2529.%2520Additionally%252C%2520we%250Aanalyze%2520evaluation%2520metrics%2520and%2520benchmarks%2520tailored%2520for%2520self-evolving%2520agents%252C%250Ahighlight%2520applications%2520in%2520domains%2520such%2520as%2520coding%252C%2520education%252C%2520and%2520healthcare%252C%250Aand%2520identify%2520critical%2520challenges%2520and%2520research%2520directions%2520in%2520safety%252C%250Ascalability%252C%2520and%2520co-evolutionary%2520dynamics.%2520By%2520providing%2520a%2520structured%2520framework%250Afor%2520understanding%2520and%2520designing%2520self-evolving%2520agents%252C%2520this%2520survey%2520establishes%2520a%250Aroadmap%2520for%2520advancing%2520adaptive%2520agentic%2520systems%2520in%2520both%2520research%2520and%2520real-world%250Adeployments%252C%2520ultimately%2520shedding%2520lights%2520to%2520pave%2520the%2520way%2520for%2520the%2520realization%2520of%250AArtificial%2520Super%2520Intelligence%2520%2528ASI%2529%252C%2520where%2520agents%2520evolve%2520autonomously%252C%250Aperforming%2520at%2520or%2520beyond%2520human-level%2520intelligence%2520across%2520a%2520wide%2520array%2520of%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Self-Evolving%20Agents%3A%20On%20Path%20to%20Artificial%20Super%0A%20%20Intelligence&entry.906535625=Huan-ang%20Gao%20and%20Jiayi%20Geng%20and%20Wenyue%20Hua%20and%20Mengkang%20Hu%20and%20Xinzhe%20Juan%20and%20Hongzhang%20Liu%20and%20Shilong%20Liu%20and%20Jiahao%20Qiu%20and%20Xuan%20Qi%20and%20Yiran%20Wu%20and%20Hongru%20Wang%20and%20Han%20Xiao%20and%20Yuhang%20Zhou%20and%20Shaokun%20Zhang%20and%20Jiayi%20Zhang%20and%20Jinyu%20Xiang%20and%20Yixiong%20Fang%20and%20Qiwen%20Zhao%20and%20Dongrui%20Liu%20and%20Qihan%20Ren%20and%20Cheng%20Qian%20and%20Zhenghailong%20Wang%20and%20Minda%20Hu%20and%20Huazheng%20Wang%20and%20Qingyun%20Wu%20and%20Heng%20Ji%20and%20Mengdi%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20strong%20capabilities%20but%20remain%0Afundamentally%20static%2C%20unable%20to%20adapt%20their%20internal%20parameters%20to%20novel%20tasks%2C%0Aevolving%20knowledge%20domains%2C%20or%20dynamic%20interaction%20contexts.%20As%20LLMs%20are%0Aincreasingly%20deployed%20in%20open-ended%2C%20interactive%20environments%2C%20this%20static%0Anature%20has%20become%20a%20critical%20bottleneck%2C%20necessitating%20agents%20that%20can%0Aadaptively%20reason%2C%20act%2C%20and%20evolve%20in%20real%20time.%20This%20paradigm%20shift%20--%20from%0Ascaling%20static%20models%20to%20developing%20self-evolving%20agents%20--%20has%20sparked%20growing%0Ainterest%20in%20architectures%20and%20methods%20enabling%20continual%20learning%20and%0Aadaptation%20from%20data%2C%20interactions%2C%20and%20experiences.%20This%20survey%20provides%20the%0Afirst%20systematic%20and%20comprehensive%20review%20of%20self-evolving%20agents%2C%20organized%0Aaround%20three%20foundational%20dimensions%20--%20what%20to%20evolve%2C%20when%20to%20evolve%2C%20and%20how%0Ato%20evolve.%20We%20examine%20evolutionary%20mechanisms%20across%20agent%20components%20%28e.g.%2C%0Amodels%2C%20memory%2C%20tools%2C%20architecture%29%2C%20categorize%20adaptation%20methods%20by%20stages%0A%28e.g.%2C%20intra-test-time%2C%20inter-test-time%29%2C%20and%20analyze%20the%20algorithmic%20and%0Aarchitectural%20designs%20that%20guide%20evolutionary%20adaptation%20%28e.g.%2C%20scalar%20rewards%2C%0Atextual%20feedback%2C%20single-agent%20and%20multi-agent%20systems%29.%20Additionally%2C%20we%0Aanalyze%20evaluation%20metrics%20and%20benchmarks%20tailored%20for%20self-evolving%20agents%2C%0Ahighlight%20applications%20in%20domains%20such%20as%20coding%2C%20education%2C%20and%20healthcare%2C%0Aand%20identify%20critical%20challenges%20and%20research%20directions%20in%20safety%2C%0Ascalability%2C%20and%20co-evolutionary%20dynamics.%20By%20providing%20a%20structured%20framework%0Afor%20understanding%20and%20designing%20self-evolving%20agents%2C%20this%20survey%20establishes%20a%0Aroadmap%20for%20advancing%20adaptive%20agentic%20systems%20in%20both%20research%20and%20real-world%0Adeployments%2C%20ultimately%20shedding%20lights%20to%20pave%20the%20way%20for%20the%20realization%20of%0AArtificial%20Super%20Intelligence%20%28ASI%29%2C%20where%20agents%20evolve%20autonomously%2C%0Aperforming%20at%20or%20beyond%20human-level%20intelligence%20across%20a%20wide%20array%20of%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21046v1&entry.124074799=Read"},
{"title": "Modeling User Behavior from Adaptive Surveys with Supplemental Context", "author": "Aman Shukla and Daniel Patrick Scantlebury and Rishabh Kumar", "abstract": "  Modeling user behavior is critical across many industries where understanding\npreferences, intent, or decisions informs personalization, targeting, and\nstrategic outcomes. Surveys have long served as a classical mechanism for\ncollecting such behavioral data due to their interpretability, structure, and\nease of deployment. However, surveys alone are inherently limited by user\nfatigue, incomplete responses, and practical constraints on their length making\nthem insufficient for capturing user behavior. In this work, we present LANTERN\n(Late-Attentive Network for Enriched Response Modeling), a modular architecture\nfor modeling user behavior by fusing adaptive survey responses with\nsupplemental contextual signals. We demonstrate the architectural value of\nmaintaining survey primacy through selective gating, residual connections and\nlate fusion via cross-attention, treating survey data as the primary signal\nwhile incorporating external modalities only when relevant. LANTERN outperforms\nstrong survey-only baselines in multi-label prediction of survey responses. We\nfurther investigate threshold sensitivity and the benefits of selective\nmodality reliance through ablation and rare/frequent attribute analysis.\nLANTERN's modularity supports scalable integration of new encoders and evolving\ndatasets. This work provides a practical and extensible blueprint for behavior\nmodeling in survey-centric applications.\n", "link": "http://arxiv.org/abs/2507.20919v1", "date": "2025-07-28", "relevancy": 2.0597, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5174}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20User%20Behavior%20from%20Adaptive%20Surveys%20with%20Supplemental%20Context&body=Title%3A%20Modeling%20User%20Behavior%20from%20Adaptive%20Surveys%20with%20Supplemental%20Context%0AAuthor%3A%20Aman%20Shukla%20and%20Daniel%20Patrick%20Scantlebury%20and%20Rishabh%20Kumar%0AAbstract%3A%20%20%20Modeling%20user%20behavior%20is%20critical%20across%20many%20industries%20where%20understanding%0Apreferences%2C%20intent%2C%20or%20decisions%20informs%20personalization%2C%20targeting%2C%20and%0Astrategic%20outcomes.%20Surveys%20have%20long%20served%20as%20a%20classical%20mechanism%20for%0Acollecting%20such%20behavioral%20data%20due%20to%20their%20interpretability%2C%20structure%2C%20and%0Aease%20of%20deployment.%20However%2C%20surveys%20alone%20are%20inherently%20limited%20by%20user%0Afatigue%2C%20incomplete%20responses%2C%20and%20practical%20constraints%20on%20their%20length%20making%0Athem%20insufficient%20for%20capturing%20user%20behavior.%20In%20this%20work%2C%20we%20present%20LANTERN%0A%28Late-Attentive%20Network%20for%20Enriched%20Response%20Modeling%29%2C%20a%20modular%20architecture%0Afor%20modeling%20user%20behavior%20by%20fusing%20adaptive%20survey%20responses%20with%0Asupplemental%20contextual%20signals.%20We%20demonstrate%20the%20architectural%20value%20of%0Amaintaining%20survey%20primacy%20through%20selective%20gating%2C%20residual%20connections%20and%0Alate%20fusion%20via%20cross-attention%2C%20treating%20survey%20data%20as%20the%20primary%20signal%0Awhile%20incorporating%20external%20modalities%20only%20when%20relevant.%20LANTERN%20outperforms%0Astrong%20survey-only%20baselines%20in%20multi-label%20prediction%20of%20survey%20responses.%20We%0Afurther%20investigate%20threshold%20sensitivity%20and%20the%20benefits%20of%20selective%0Amodality%20reliance%20through%20ablation%20and%20rare/frequent%20attribute%20analysis.%0ALANTERN%27s%20modularity%20supports%20scalable%20integration%20of%20new%20encoders%20and%20evolving%0Adatasets.%20This%20work%20provides%20a%20practical%20and%20extensible%20blueprint%20for%20behavior%0Amodeling%20in%20survey-centric%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520User%2520Behavior%2520from%2520Adaptive%2520Surveys%2520with%2520Supplemental%2520Context%26entry.906535625%3DAman%2520Shukla%2520and%2520Daniel%2520Patrick%2520Scantlebury%2520and%2520Rishabh%2520Kumar%26entry.1292438233%3D%2520%2520Modeling%2520user%2520behavior%2520is%2520critical%2520across%2520many%2520industries%2520where%2520understanding%250Apreferences%252C%2520intent%252C%2520or%2520decisions%2520informs%2520personalization%252C%2520targeting%252C%2520and%250Astrategic%2520outcomes.%2520Surveys%2520have%2520long%2520served%2520as%2520a%2520classical%2520mechanism%2520for%250Acollecting%2520such%2520behavioral%2520data%2520due%2520to%2520their%2520interpretability%252C%2520structure%252C%2520and%250Aease%2520of%2520deployment.%2520However%252C%2520surveys%2520alone%2520are%2520inherently%2520limited%2520by%2520user%250Afatigue%252C%2520incomplete%2520responses%252C%2520and%2520practical%2520constraints%2520on%2520their%2520length%2520making%250Athem%2520insufficient%2520for%2520capturing%2520user%2520behavior.%2520In%2520this%2520work%252C%2520we%2520present%2520LANTERN%250A%2528Late-Attentive%2520Network%2520for%2520Enriched%2520Response%2520Modeling%2529%252C%2520a%2520modular%2520architecture%250Afor%2520modeling%2520user%2520behavior%2520by%2520fusing%2520adaptive%2520survey%2520responses%2520with%250Asupplemental%2520contextual%2520signals.%2520We%2520demonstrate%2520the%2520architectural%2520value%2520of%250Amaintaining%2520survey%2520primacy%2520through%2520selective%2520gating%252C%2520residual%2520connections%2520and%250Alate%2520fusion%2520via%2520cross-attention%252C%2520treating%2520survey%2520data%2520as%2520the%2520primary%2520signal%250Awhile%2520incorporating%2520external%2520modalities%2520only%2520when%2520relevant.%2520LANTERN%2520outperforms%250Astrong%2520survey-only%2520baselines%2520in%2520multi-label%2520prediction%2520of%2520survey%2520responses.%2520We%250Afurther%2520investigate%2520threshold%2520sensitivity%2520and%2520the%2520benefits%2520of%2520selective%250Amodality%2520reliance%2520through%2520ablation%2520and%2520rare/frequent%2520attribute%2520analysis.%250ALANTERN%2527s%2520modularity%2520supports%2520scalable%2520integration%2520of%2520new%2520encoders%2520and%2520evolving%250Adatasets.%2520This%2520work%2520provides%2520a%2520practical%2520and%2520extensible%2520blueprint%2520for%2520behavior%250Amodeling%2520in%2520survey-centric%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20User%20Behavior%20from%20Adaptive%20Surveys%20with%20Supplemental%20Context&entry.906535625=Aman%20Shukla%20and%20Daniel%20Patrick%20Scantlebury%20and%20Rishabh%20Kumar&entry.1292438233=%20%20Modeling%20user%20behavior%20is%20critical%20across%20many%20industries%20where%20understanding%0Apreferences%2C%20intent%2C%20or%20decisions%20informs%20personalization%2C%20targeting%2C%20and%0Astrategic%20outcomes.%20Surveys%20have%20long%20served%20as%20a%20classical%20mechanism%20for%0Acollecting%20such%20behavioral%20data%20due%20to%20their%20interpretability%2C%20structure%2C%20and%0Aease%20of%20deployment.%20However%2C%20surveys%20alone%20are%20inherently%20limited%20by%20user%0Afatigue%2C%20incomplete%20responses%2C%20and%20practical%20constraints%20on%20their%20length%20making%0Athem%20insufficient%20for%20capturing%20user%20behavior.%20In%20this%20work%2C%20we%20present%20LANTERN%0A%28Late-Attentive%20Network%20for%20Enriched%20Response%20Modeling%29%2C%20a%20modular%20architecture%0Afor%20modeling%20user%20behavior%20by%20fusing%20adaptive%20survey%20responses%20with%0Asupplemental%20contextual%20signals.%20We%20demonstrate%20the%20architectural%20value%20of%0Amaintaining%20survey%20primacy%20through%20selective%20gating%2C%20residual%20connections%20and%0Alate%20fusion%20via%20cross-attention%2C%20treating%20survey%20data%20as%20the%20primary%20signal%0Awhile%20incorporating%20external%20modalities%20only%20when%20relevant.%20LANTERN%20outperforms%0Astrong%20survey-only%20baselines%20in%20multi-label%20prediction%20of%20survey%20responses.%20We%0Afurther%20investigate%20threshold%20sensitivity%20and%20the%20benefits%20of%20selective%0Amodality%20reliance%20through%20ablation%20and%20rare/frequent%20attribute%20analysis.%0ALANTERN%27s%20modularity%20supports%20scalable%20integration%20of%20new%20encoders%20and%20evolving%0Adatasets.%20This%20work%20provides%20a%20practical%20and%20extensible%20blueprint%20for%20behavior%0Amodeling%20in%20survey-centric%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20919v1&entry.124074799=Read"},
{"title": "Dissecting Persona-Driven Reasoning in Language Models via Activation\n  Patching", "author": "Ansh Poonia and Maeghal Jain", "abstract": "  Large language models (LLMs) exhibit remarkable versatility in adopting\ndiverse personas. In this study, we examine how assigning a persona influences\na model's reasoning on an objective task. Using activation patching, we take a\nfirst step toward understanding how key components of the model encode\npersona-specific information. Our findings reveal that the early Multi-Layer\nPerceptron (MLP) layers attend not only to the syntactic structure of the input\nbut also process its semantic content. These layers transform persona tokens\ninto richer representations, which are then used by the middle Multi-Head\nAttention (MHA) layers to shape the model's output. Additionally, we identify\nspecific attention heads that disproportionately attend to racial and\ncolor-based identities.\n", "link": "http://arxiv.org/abs/2507.20936v1", "date": "2025-07-28", "relevancy": 2.0531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Persona-Driven%20Reasoning%20in%20Language%20Models%20via%20Activation%0A%20%20Patching&body=Title%3A%20Dissecting%20Persona-Driven%20Reasoning%20in%20Language%20Models%20via%20Activation%0A%20%20Patching%0AAuthor%3A%20Ansh%20Poonia%20and%20Maeghal%20Jain%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20versatility%20in%20adopting%0Adiverse%20personas.%20In%20this%20study%2C%20we%20examine%20how%20assigning%20a%20persona%20influences%0Aa%20model%27s%20reasoning%20on%20an%20objective%20task.%20Using%20activation%20patching%2C%20we%20take%20a%0Afirst%20step%20toward%20understanding%20how%20key%20components%20of%20the%20model%20encode%0Apersona-specific%20information.%20Our%20findings%20reveal%20that%20the%20early%20Multi-Layer%0APerceptron%20%28MLP%29%20layers%20attend%20not%20only%20to%20the%20syntactic%20structure%20of%20the%20input%0Abut%20also%20process%20its%20semantic%20content.%20These%20layers%20transform%20persona%20tokens%0Ainto%20richer%20representations%2C%20which%20are%20then%20used%20by%20the%20middle%20Multi-Head%0AAttention%20%28MHA%29%20layers%20to%20shape%20the%20model%27s%20output.%20Additionally%2C%20we%20identify%0Aspecific%20attention%20heads%20that%20disproportionately%20attend%20to%20racial%20and%0Acolor-based%20identities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20936v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Persona-Driven%2520Reasoning%2520in%2520Language%2520Models%2520via%2520Activation%250A%2520%2520Patching%26entry.906535625%3DAnsh%2520Poonia%2520and%2520Maeghal%2520Jain%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520remarkable%2520versatility%2520in%2520adopting%250Adiverse%2520personas.%2520In%2520this%2520study%252C%2520we%2520examine%2520how%2520assigning%2520a%2520persona%2520influences%250Aa%2520model%2527s%2520reasoning%2520on%2520an%2520objective%2520task.%2520Using%2520activation%2520patching%252C%2520we%2520take%2520a%250Afirst%2520step%2520toward%2520understanding%2520how%2520key%2520components%2520of%2520the%2520model%2520encode%250Apersona-specific%2520information.%2520Our%2520findings%2520reveal%2520that%2520the%2520early%2520Multi-Layer%250APerceptron%2520%2528MLP%2529%2520layers%2520attend%2520not%2520only%2520to%2520the%2520syntactic%2520structure%2520of%2520the%2520input%250Abut%2520also%2520process%2520its%2520semantic%2520content.%2520These%2520layers%2520transform%2520persona%2520tokens%250Ainto%2520richer%2520representations%252C%2520which%2520are%2520then%2520used%2520by%2520the%2520middle%2520Multi-Head%250AAttention%2520%2528MHA%2529%2520layers%2520to%2520shape%2520the%2520model%2527s%2520output.%2520Additionally%252C%2520we%2520identify%250Aspecific%2520attention%2520heads%2520that%2520disproportionately%2520attend%2520to%2520racial%2520and%250Acolor-based%2520identities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20936v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Persona-Driven%20Reasoning%20in%20Language%20Models%20via%20Activation%0A%20%20Patching&entry.906535625=Ansh%20Poonia%20and%20Maeghal%20Jain&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20remarkable%20versatility%20in%20adopting%0Adiverse%20personas.%20In%20this%20study%2C%20we%20examine%20how%20assigning%20a%20persona%20influences%0Aa%20model%27s%20reasoning%20on%20an%20objective%20task.%20Using%20activation%20patching%2C%20we%20take%20a%0Afirst%20step%20toward%20understanding%20how%20key%20components%20of%20the%20model%20encode%0Apersona-specific%20information.%20Our%20findings%20reveal%20that%20the%20early%20Multi-Layer%0APerceptron%20%28MLP%29%20layers%20attend%20not%20only%20to%20the%20syntactic%20structure%20of%20the%20input%0Abut%20also%20process%20its%20semantic%20content.%20These%20layers%20transform%20persona%20tokens%0Ainto%20richer%20representations%2C%20which%20are%20then%20used%20by%20the%20middle%20Multi-Head%0AAttention%20%28MHA%29%20layers%20to%20shape%20the%20model%27s%20output.%20Additionally%2C%20we%20identify%0Aspecific%20attention%20heads%20that%20disproportionately%20attend%20to%20racial%20and%0Acolor-based%20identities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20936v1&entry.124074799=Read"},
{"title": "Video Forgery Detection for Surveillance Cameras: A Review", "author": "Noor B. Tayfor and Tarik A. Rashid and Shko M. Qader and Bryar A. Hassan and Mohammed H. Abdalla and Jafar Majidpour and Aram M. Ahmed and Hussein M. Ali and Aso M. Aladdin and Abdulhady A. Abdullah and Ahmed S. Shamsaldin and Haval M. Sidqi and Abdulrahman Salih and Zaher M. Yaseen and Azad A. Ameen and Janmenjoy Nayak and Mahmood Yashar Hamza", "abstract": "  The widespread availability of video recording through smartphones and\ndigital devices has made video-based evidence more accessible than ever.\nSurveillance footage plays a crucial role in security, law enforcement, and\njudicial processes. However, with the rise of advanced video editing tools,\ntampering with digital recordings has become increasingly easy, raising\nconcerns about their authenticity. Ensuring the integrity of surveillance\nvideos is essential, as manipulated footage can lead to misinformation and\nundermine judicial decisions. This paper provides a comprehensive review of\nexisting forensic techniques used to detect video forgery, focusing on their\neffectiveness in verifying the authenticity of surveillance recordings. Various\nmethods, including compression-based analysis, frame duplication detection, and\nmachine learning-based approaches, are explored. The findings highlight the\ngrowing necessity for more robust forensic techniques to counteract evolving\nforgery methods. Strengthening video forensic capabilities will ensure that\nsurveillance recordings remain credible and admissible as legal evidence.\n", "link": "http://arxiv.org/abs/2505.03832v2", "date": "2025-07-28", "relevancy": 2.053, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5189}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5105}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Forgery%20Detection%20for%20Surveillance%20Cameras%3A%20A%20Review&body=Title%3A%20Video%20Forgery%20Detection%20for%20Surveillance%20Cameras%3A%20A%20Review%0AAuthor%3A%20Noor%20B.%20Tayfor%20and%20Tarik%20A.%20Rashid%20and%20Shko%20M.%20Qader%20and%20Bryar%20A.%20Hassan%20and%20Mohammed%20H.%20Abdalla%20and%20Jafar%20Majidpour%20and%20Aram%20M.%20Ahmed%20and%20Hussein%20M.%20Ali%20and%20Aso%20M.%20Aladdin%20and%20Abdulhady%20A.%20Abdullah%20and%20Ahmed%20S.%20Shamsaldin%20and%20Haval%20M.%20Sidqi%20and%20Abdulrahman%20Salih%20and%20Zaher%20M.%20Yaseen%20and%20Azad%20A.%20Ameen%20and%20Janmenjoy%20Nayak%20and%20Mahmood%20Yashar%20Hamza%0AAbstract%3A%20%20%20The%20widespread%20availability%20of%20video%20recording%20through%20smartphones%20and%0Adigital%20devices%20has%20made%20video-based%20evidence%20more%20accessible%20than%20ever.%0ASurveillance%20footage%20plays%20a%20crucial%20role%20in%20security%2C%20law%20enforcement%2C%20and%0Ajudicial%20processes.%20However%2C%20with%20the%20rise%20of%20advanced%20video%20editing%20tools%2C%0Atampering%20with%20digital%20recordings%20has%20become%20increasingly%20easy%2C%20raising%0Aconcerns%20about%20their%20authenticity.%20Ensuring%20the%20integrity%20of%20surveillance%0Avideos%20is%20essential%2C%20as%20manipulated%20footage%20can%20lead%20to%20misinformation%20and%0Aundermine%20judicial%20decisions.%20This%20paper%20provides%20a%20comprehensive%20review%20of%0Aexisting%20forensic%20techniques%20used%20to%20detect%20video%20forgery%2C%20focusing%20on%20their%0Aeffectiveness%20in%20verifying%20the%20authenticity%20of%20surveillance%20recordings.%20Various%0Amethods%2C%20including%20compression-based%20analysis%2C%20frame%20duplication%20detection%2C%20and%0Amachine%20learning-based%20approaches%2C%20are%20explored.%20The%20findings%20highlight%20the%0Agrowing%20necessity%20for%20more%20robust%20forensic%20techniques%20to%20counteract%20evolving%0Aforgery%20methods.%20Strengthening%20video%20forensic%20capabilities%20will%20ensure%20that%0Asurveillance%20recordings%20remain%20credible%20and%20admissible%20as%20legal%20evidence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03832v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Forgery%2520Detection%2520for%2520Surveillance%2520Cameras%253A%2520A%2520Review%26entry.906535625%3DNoor%2520B.%2520Tayfor%2520and%2520Tarik%2520A.%2520Rashid%2520and%2520Shko%2520M.%2520Qader%2520and%2520Bryar%2520A.%2520Hassan%2520and%2520Mohammed%2520H.%2520Abdalla%2520and%2520Jafar%2520Majidpour%2520and%2520Aram%2520M.%2520Ahmed%2520and%2520Hussein%2520M.%2520Ali%2520and%2520Aso%2520M.%2520Aladdin%2520and%2520Abdulhady%2520A.%2520Abdullah%2520and%2520Ahmed%2520S.%2520Shamsaldin%2520and%2520Haval%2520M.%2520Sidqi%2520and%2520Abdulrahman%2520Salih%2520and%2520Zaher%2520M.%2520Yaseen%2520and%2520Azad%2520A.%2520Ameen%2520and%2520Janmenjoy%2520Nayak%2520and%2520Mahmood%2520Yashar%2520Hamza%26entry.1292438233%3D%2520%2520The%2520widespread%2520availability%2520of%2520video%2520recording%2520through%2520smartphones%2520and%250Adigital%2520devices%2520has%2520made%2520video-based%2520evidence%2520more%2520accessible%2520than%2520ever.%250ASurveillance%2520footage%2520plays%2520a%2520crucial%2520role%2520in%2520security%252C%2520law%2520enforcement%252C%2520and%250Ajudicial%2520processes.%2520However%252C%2520with%2520the%2520rise%2520of%2520advanced%2520video%2520editing%2520tools%252C%250Atampering%2520with%2520digital%2520recordings%2520has%2520become%2520increasingly%2520easy%252C%2520raising%250Aconcerns%2520about%2520their%2520authenticity.%2520Ensuring%2520the%2520integrity%2520of%2520surveillance%250Avideos%2520is%2520essential%252C%2520as%2520manipulated%2520footage%2520can%2520lead%2520to%2520misinformation%2520and%250Aundermine%2520judicial%2520decisions.%2520This%2520paper%2520provides%2520a%2520comprehensive%2520review%2520of%250Aexisting%2520forensic%2520techniques%2520used%2520to%2520detect%2520video%2520forgery%252C%2520focusing%2520on%2520their%250Aeffectiveness%2520in%2520verifying%2520the%2520authenticity%2520of%2520surveillance%2520recordings.%2520Various%250Amethods%252C%2520including%2520compression-based%2520analysis%252C%2520frame%2520duplication%2520detection%252C%2520and%250Amachine%2520learning-based%2520approaches%252C%2520are%2520explored.%2520The%2520findings%2520highlight%2520the%250Agrowing%2520necessity%2520for%2520more%2520robust%2520forensic%2520techniques%2520to%2520counteract%2520evolving%250Aforgery%2520methods.%2520Strengthening%2520video%2520forensic%2520capabilities%2520will%2520ensure%2520that%250Asurveillance%2520recordings%2520remain%2520credible%2520and%2520admissible%2520as%2520legal%2520evidence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03832v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Forgery%20Detection%20for%20Surveillance%20Cameras%3A%20A%20Review&entry.906535625=Noor%20B.%20Tayfor%20and%20Tarik%20A.%20Rashid%20and%20Shko%20M.%20Qader%20and%20Bryar%20A.%20Hassan%20and%20Mohammed%20H.%20Abdalla%20and%20Jafar%20Majidpour%20and%20Aram%20M.%20Ahmed%20and%20Hussein%20M.%20Ali%20and%20Aso%20M.%20Aladdin%20and%20Abdulhady%20A.%20Abdullah%20and%20Ahmed%20S.%20Shamsaldin%20and%20Haval%20M.%20Sidqi%20and%20Abdulrahman%20Salih%20and%20Zaher%20M.%20Yaseen%20and%20Azad%20A.%20Ameen%20and%20Janmenjoy%20Nayak%20and%20Mahmood%20Yashar%20Hamza&entry.1292438233=%20%20The%20widespread%20availability%20of%20video%20recording%20through%20smartphones%20and%0Adigital%20devices%20has%20made%20video-based%20evidence%20more%20accessible%20than%20ever.%0ASurveillance%20footage%20plays%20a%20crucial%20role%20in%20security%2C%20law%20enforcement%2C%20and%0Ajudicial%20processes.%20However%2C%20with%20the%20rise%20of%20advanced%20video%20editing%20tools%2C%0Atampering%20with%20digital%20recordings%20has%20become%20increasingly%20easy%2C%20raising%0Aconcerns%20about%20their%20authenticity.%20Ensuring%20the%20integrity%20of%20surveillance%0Avideos%20is%20essential%2C%20as%20manipulated%20footage%20can%20lead%20to%20misinformation%20and%0Aundermine%20judicial%20decisions.%20This%20paper%20provides%20a%20comprehensive%20review%20of%0Aexisting%20forensic%20techniques%20used%20to%20detect%20video%20forgery%2C%20focusing%20on%20their%0Aeffectiveness%20in%20verifying%20the%20authenticity%20of%20surveillance%20recordings.%20Various%0Amethods%2C%20including%20compression-based%20analysis%2C%20frame%20duplication%20detection%2C%20and%0Amachine%20learning-based%20approaches%2C%20are%20explored.%20The%20findings%20highlight%20the%0Agrowing%20necessity%20for%20more%20robust%20forensic%20techniques%20to%20counteract%20evolving%0Aforgery%20methods.%20Strengthening%20video%20forensic%20capabilities%20will%20ensure%20that%0Asurveillance%20recordings%20remain%20credible%20and%20admissible%20as%20legal%20evidence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03832v2&entry.124074799=Read"},
{"title": "FantasyID: A dataset for detecting digital manipulations of ID-documents", "author": "Pavel Korshunov and Amir Mohammadi and Vidit Vidit and Christophe Ecabert and S\u00e9bastien Marcel", "abstract": "  Advancements in image generation led to the availability of easy-to-use tools\nfor malicious actors to create forged images. These tools pose a serious threat\nto the widespread Know Your Customer (KYC) applications, requiring robust\nsystems for detection of the forged Identity Documents (IDs). To facilitate the\ndevelopment of the detection algorithms, in this paper, we propose a novel\npublicly available (including commercial use) dataset, FantasyID, which mimics\nreal-world IDs but without tampering with legal documents and, compared to\nprevious public datasets, it does not contain generated faces or specimen\nwatermarks. FantasyID contains ID cards with diverse design styles, languages,\nand faces of real people. To simulate a realistic KYC scenario, the cards from\nFantasyID were printed and captured with three different devices, constituting\nthe bonafide class. We have emulated digital forgery/injection attacks that\ncould be performed by a malicious actor to tamper the IDs using the existing\ngenerative tools. The current state-of-the-art forgery detection algorithms,\nsuch as TruFor, MMFusion, UniFD, and FatFormer, are challenged by FantasyID\ndataset. It especially evident, in the evaluation conditions close to\npractical, with the operational threshold set on validation set so that false\npositive rate is at 10%, leading to false negative rates close to 50% across\nthe board on the test set. The evaluation experiments demonstrate that\nFantasyID dataset is complex enough to be used as an evaluation benchmark for\ndetection algorithms.\n", "link": "http://arxiv.org/abs/2507.20808v1", "date": "2025-07-28", "relevancy": 2.0518, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5454}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FantasyID%3A%20A%20dataset%20for%20detecting%20digital%20manipulations%20of%20ID-documents&body=Title%3A%20FantasyID%3A%20A%20dataset%20for%20detecting%20digital%20manipulations%20of%20ID-documents%0AAuthor%3A%20Pavel%20Korshunov%20and%20Amir%20Mohammadi%20and%20Vidit%20Vidit%20and%20Christophe%20Ecabert%20and%20S%C3%A9bastien%20Marcel%0AAbstract%3A%20%20%20Advancements%20in%20image%20generation%20led%20to%20the%20availability%20of%20easy-to-use%20tools%0Afor%20malicious%20actors%20to%20create%20forged%20images.%20These%20tools%20pose%20a%20serious%20threat%0Ato%20the%20widespread%20Know%20Your%20Customer%20%28KYC%29%20applications%2C%20requiring%20robust%0Asystems%20for%20detection%20of%20the%20forged%20Identity%20Documents%20%28IDs%29.%20To%20facilitate%20the%0Adevelopment%20of%20the%20detection%20algorithms%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Apublicly%20available%20%28including%20commercial%20use%29%20dataset%2C%20FantasyID%2C%20which%20mimics%0Areal-world%20IDs%20but%20without%20tampering%20with%20legal%20documents%20and%2C%20compared%20to%0Aprevious%20public%20datasets%2C%20it%20does%20not%20contain%20generated%20faces%20or%20specimen%0Awatermarks.%20FantasyID%20contains%20ID%20cards%20with%20diverse%20design%20styles%2C%20languages%2C%0Aand%20faces%20of%20real%20people.%20To%20simulate%20a%20realistic%20KYC%20scenario%2C%20the%20cards%20from%0AFantasyID%20were%20printed%20and%20captured%20with%20three%20different%20devices%2C%20constituting%0Athe%20bonafide%20class.%20We%20have%20emulated%20digital%20forgery/injection%20attacks%20that%0Acould%20be%20performed%20by%20a%20malicious%20actor%20to%20tamper%20the%20IDs%20using%20the%20existing%0Agenerative%20tools.%20The%20current%20state-of-the-art%20forgery%20detection%20algorithms%2C%0Asuch%20as%20TruFor%2C%20MMFusion%2C%20UniFD%2C%20and%20FatFormer%2C%20are%20challenged%20by%20FantasyID%0Adataset.%20It%20especially%20evident%2C%20in%20the%20evaluation%20conditions%20close%20to%0Apractical%2C%20with%20the%20operational%20threshold%20set%20on%20validation%20set%20so%20that%20false%0Apositive%20rate%20is%20at%2010%25%2C%20leading%20to%20false%20negative%20rates%20close%20to%2050%25%20across%0Athe%20board%20on%20the%20test%20set.%20The%20evaluation%20experiments%20demonstrate%20that%0AFantasyID%20dataset%20is%20complex%20enough%20to%20be%20used%20as%20an%20evaluation%20benchmark%20for%0Adetection%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFantasyID%253A%2520A%2520dataset%2520for%2520detecting%2520digital%2520manipulations%2520of%2520ID-documents%26entry.906535625%3DPavel%2520Korshunov%2520and%2520Amir%2520Mohammadi%2520and%2520Vidit%2520Vidit%2520and%2520Christophe%2520Ecabert%2520and%2520S%25C3%25A9bastien%2520Marcel%26entry.1292438233%3D%2520%2520Advancements%2520in%2520image%2520generation%2520led%2520to%2520the%2520availability%2520of%2520easy-to-use%2520tools%250Afor%2520malicious%2520actors%2520to%2520create%2520forged%2520images.%2520These%2520tools%2520pose%2520a%2520serious%2520threat%250Ato%2520the%2520widespread%2520Know%2520Your%2520Customer%2520%2528KYC%2529%2520applications%252C%2520requiring%2520robust%250Asystems%2520for%2520detection%2520of%2520the%2520forged%2520Identity%2520Documents%2520%2528IDs%2529.%2520To%2520facilitate%2520the%250Adevelopment%2520of%2520the%2520detection%2520algorithms%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Apublicly%2520available%2520%2528including%2520commercial%2520use%2529%2520dataset%252C%2520FantasyID%252C%2520which%2520mimics%250Areal-world%2520IDs%2520but%2520without%2520tampering%2520with%2520legal%2520documents%2520and%252C%2520compared%2520to%250Aprevious%2520public%2520datasets%252C%2520it%2520does%2520not%2520contain%2520generated%2520faces%2520or%2520specimen%250Awatermarks.%2520FantasyID%2520contains%2520ID%2520cards%2520with%2520diverse%2520design%2520styles%252C%2520languages%252C%250Aand%2520faces%2520of%2520real%2520people.%2520To%2520simulate%2520a%2520realistic%2520KYC%2520scenario%252C%2520the%2520cards%2520from%250AFantasyID%2520were%2520printed%2520and%2520captured%2520with%2520three%2520different%2520devices%252C%2520constituting%250Athe%2520bonafide%2520class.%2520We%2520have%2520emulated%2520digital%2520forgery/injection%2520attacks%2520that%250Acould%2520be%2520performed%2520by%2520a%2520malicious%2520actor%2520to%2520tamper%2520the%2520IDs%2520using%2520the%2520existing%250Agenerative%2520tools.%2520The%2520current%2520state-of-the-art%2520forgery%2520detection%2520algorithms%252C%250Asuch%2520as%2520TruFor%252C%2520MMFusion%252C%2520UniFD%252C%2520and%2520FatFormer%252C%2520are%2520challenged%2520by%2520FantasyID%250Adataset.%2520It%2520especially%2520evident%252C%2520in%2520the%2520evaluation%2520conditions%2520close%2520to%250Apractical%252C%2520with%2520the%2520operational%2520threshold%2520set%2520on%2520validation%2520set%2520so%2520that%2520false%250Apositive%2520rate%2520is%2520at%252010%2525%252C%2520leading%2520to%2520false%2520negative%2520rates%2520close%2520to%252050%2525%2520across%250Athe%2520board%2520on%2520the%2520test%2520set.%2520The%2520evaluation%2520experiments%2520demonstrate%2520that%250AFantasyID%2520dataset%2520is%2520complex%2520enough%2520to%2520be%2520used%2520as%2520an%2520evaluation%2520benchmark%2520for%250Adetection%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FantasyID%3A%20A%20dataset%20for%20detecting%20digital%20manipulations%20of%20ID-documents&entry.906535625=Pavel%20Korshunov%20and%20Amir%20Mohammadi%20and%20Vidit%20Vidit%20and%20Christophe%20Ecabert%20and%20S%C3%A9bastien%20Marcel&entry.1292438233=%20%20Advancements%20in%20image%20generation%20led%20to%20the%20availability%20of%20easy-to-use%20tools%0Afor%20malicious%20actors%20to%20create%20forged%20images.%20These%20tools%20pose%20a%20serious%20threat%0Ato%20the%20widespread%20Know%20Your%20Customer%20%28KYC%29%20applications%2C%20requiring%20robust%0Asystems%20for%20detection%20of%20the%20forged%20Identity%20Documents%20%28IDs%29.%20To%20facilitate%20the%0Adevelopment%20of%20the%20detection%20algorithms%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Apublicly%20available%20%28including%20commercial%20use%29%20dataset%2C%20FantasyID%2C%20which%20mimics%0Areal-world%20IDs%20but%20without%20tampering%20with%20legal%20documents%20and%2C%20compared%20to%0Aprevious%20public%20datasets%2C%20it%20does%20not%20contain%20generated%20faces%20or%20specimen%0Awatermarks.%20FantasyID%20contains%20ID%20cards%20with%20diverse%20design%20styles%2C%20languages%2C%0Aand%20faces%20of%20real%20people.%20To%20simulate%20a%20realistic%20KYC%20scenario%2C%20the%20cards%20from%0AFantasyID%20were%20printed%20and%20captured%20with%20three%20different%20devices%2C%20constituting%0Athe%20bonafide%20class.%20We%20have%20emulated%20digital%20forgery/injection%20attacks%20that%0Acould%20be%20performed%20by%20a%20malicious%20actor%20to%20tamper%20the%20IDs%20using%20the%20existing%0Agenerative%20tools.%20The%20current%20state-of-the-art%20forgery%20detection%20algorithms%2C%0Asuch%20as%20TruFor%2C%20MMFusion%2C%20UniFD%2C%20and%20FatFormer%2C%20are%20challenged%20by%20FantasyID%0Adataset.%20It%20especially%20evident%2C%20in%20the%20evaluation%20conditions%20close%20to%0Apractical%2C%20with%20the%20operational%20threshold%20set%20on%20validation%20set%20so%20that%20false%0Apositive%20rate%20is%20at%2010%25%2C%20leading%20to%20false%20negative%20rates%20close%20to%2050%25%20across%0Athe%20board%20on%20the%20test%20set.%20The%20evaluation%20experiments%20demonstrate%20that%0AFantasyID%20dataset%20is%20complex%20enough%20to%20be%20used%20as%20an%20evaluation%20benchmark%20for%0Adetection%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20808v1&entry.124074799=Read"},
{"title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of\n  Instruction-Tuned Large Language Models", "author": "Max Peeperkorn and Tom Kouwenhoven and Dan Brown and Anna Jordanous", "abstract": "  Instruction-tuning large language models (LLMs) reduces the diversity of\ntheir outputs, which has implications for many tasks, particularly for creative\ntasks. This paper investigates the ``diversity gap'' for a writing prompt\nnarrative generation task. This gap emerges as measured by current diversity\nmetrics for various open-weight and open-source LLMs. The results show\nsignificant decreases in diversity due to instruction-tuning. We explore the\ndiversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to\nfurther understand how output diversity is affected. The results indicate that\nDPO has the most substantial impact on diversity. Motivated by these findings,\nwe present a new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases diversity and\neven maintains or improves quality.\n", "link": "http://arxiv.org/abs/2507.20956v1", "date": "2025-07-28", "relevancy": 2.0512, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20Gap%3A%20Conformative%20Decoding%20to%20Improve%20Output%20Diversity%20of%0A%20%20Instruction-Tuned%20Large%20Language%20Models&body=Title%3A%20Mind%20the%20Gap%3A%20Conformative%20Decoding%20to%20Improve%20Output%20Diversity%20of%0A%20%20Instruction-Tuned%20Large%20Language%20Models%0AAuthor%3A%20Max%20Peeperkorn%20and%20Tom%20Kouwenhoven%20and%20Dan%20Brown%20and%20Anna%20Jordanous%0AAbstract%3A%20%20%20Instruction-tuning%20large%20language%20models%20%28LLMs%29%20reduces%20the%20diversity%20of%0Atheir%20outputs%2C%20which%20has%20implications%20for%20many%20tasks%2C%20particularly%20for%20creative%0Atasks.%20This%20paper%20investigates%20the%20%60%60diversity%20gap%27%27%20for%20a%20writing%20prompt%0Anarrative%20generation%20task.%20This%20gap%20emerges%20as%20measured%20by%20current%20diversity%0Ametrics%20for%20various%20open-weight%20and%20open-source%20LLMs.%20The%20results%20show%0Asignificant%20decreases%20in%20diversity%20due%20to%20instruction-tuning.%20We%20explore%20the%0Adiversity%20loss%20at%20each%20fine-tuning%20stage%20for%20the%20OLMo%20and%20OLMo%202%20models%20to%0Afurther%20understand%20how%20output%20diversity%20is%20affected.%20The%20results%20indicate%20that%0ADPO%20has%20the%20most%20substantial%20impact%20on%20diversity.%20Motivated%20by%20these%20findings%2C%0Awe%20present%20a%20new%20decoding%20strategy%2C%20conformative%20decoding%2C%20which%20guides%20an%0Ainstruct%20model%20using%20its%20more%20diverse%20base%20model%20to%20reintroduce%20output%0Adiversity.%20We%20show%20that%20conformative%20decoding%20typically%20increases%20diversity%20and%0Aeven%20maintains%20or%20improves%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520Gap%253A%2520Conformative%2520Decoding%2520to%2520Improve%2520Output%2520Diversity%2520of%250A%2520%2520Instruction-Tuned%2520Large%2520Language%2520Models%26entry.906535625%3DMax%2520Peeperkorn%2520and%2520Tom%2520Kouwenhoven%2520and%2520Dan%2520Brown%2520and%2520Anna%2520Jordanous%26entry.1292438233%3D%2520%2520Instruction-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520reduces%2520the%2520diversity%2520of%250Atheir%2520outputs%252C%2520which%2520has%2520implications%2520for%2520many%2520tasks%252C%2520particularly%2520for%2520creative%250Atasks.%2520This%2520paper%2520investigates%2520the%2520%2560%2560diversity%2520gap%2527%2527%2520for%2520a%2520writing%2520prompt%250Anarrative%2520generation%2520task.%2520This%2520gap%2520emerges%2520as%2520measured%2520by%2520current%2520diversity%250Ametrics%2520for%2520various%2520open-weight%2520and%2520open-source%2520LLMs.%2520The%2520results%2520show%250Asignificant%2520decreases%2520in%2520diversity%2520due%2520to%2520instruction-tuning.%2520We%2520explore%2520the%250Adiversity%2520loss%2520at%2520each%2520fine-tuning%2520stage%2520for%2520the%2520OLMo%2520and%2520OLMo%25202%2520models%2520to%250Afurther%2520understand%2520how%2520output%2520diversity%2520is%2520affected.%2520The%2520results%2520indicate%2520that%250ADPO%2520has%2520the%2520most%2520substantial%2520impact%2520on%2520diversity.%2520Motivated%2520by%2520these%2520findings%252C%250Awe%2520present%2520a%2520new%2520decoding%2520strategy%252C%2520conformative%2520decoding%252C%2520which%2520guides%2520an%250Ainstruct%2520model%2520using%2520its%2520more%2520diverse%2520base%2520model%2520to%2520reintroduce%2520output%250Adiversity.%2520We%2520show%2520that%2520conformative%2520decoding%2520typically%2520increases%2520diversity%2520and%250Aeven%2520maintains%2520or%2520improves%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20Gap%3A%20Conformative%20Decoding%20to%20Improve%20Output%20Diversity%20of%0A%20%20Instruction-Tuned%20Large%20Language%20Models&entry.906535625=Max%20Peeperkorn%20and%20Tom%20Kouwenhoven%20and%20Dan%20Brown%20and%20Anna%20Jordanous&entry.1292438233=%20%20Instruction-tuning%20large%20language%20models%20%28LLMs%29%20reduces%20the%20diversity%20of%0Atheir%20outputs%2C%20which%20has%20implications%20for%20many%20tasks%2C%20particularly%20for%20creative%0Atasks.%20This%20paper%20investigates%20the%20%60%60diversity%20gap%27%27%20for%20a%20writing%20prompt%0Anarrative%20generation%20task.%20This%20gap%20emerges%20as%20measured%20by%20current%20diversity%0Ametrics%20for%20various%20open-weight%20and%20open-source%20LLMs.%20The%20results%20show%0Asignificant%20decreases%20in%20diversity%20due%20to%20instruction-tuning.%20We%20explore%20the%0Adiversity%20loss%20at%20each%20fine-tuning%20stage%20for%20the%20OLMo%20and%20OLMo%202%20models%20to%0Afurther%20understand%20how%20output%20diversity%20is%20affected.%20The%20results%20indicate%20that%0ADPO%20has%20the%20most%20substantial%20impact%20on%20diversity.%20Motivated%20by%20these%20findings%2C%0Awe%20present%20a%20new%20decoding%20strategy%2C%20conformative%20decoding%2C%20which%20guides%20an%0Ainstruct%20model%20using%20its%20more%20diverse%20base%20model%20to%20reintroduce%20output%0Adiversity.%20We%20show%20that%20conformative%20decoding%20typically%20increases%20diversity%20and%0Aeven%20maintains%20or%20improves%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20956v1&entry.124074799=Read"},
{"title": "Investigation of Accuracy and Bias in Face Recognition Trained with\n  Synthetic Data", "author": "Pavel Korshunov and Ketan Kotwal and Christophe Ecabert and Vidit Vidit and Amir Mohammadi and Sebastien Marcel", "abstract": "  Synthetic data has emerged as a promising alternative for training face\nrecognition (FR) models, offering advantages in scalability, privacy\ncompliance, and potential for bias mitigation. However, critical questions\nremain on whether both high accuracy and fairness can be achieved with\nsynthetic data. In this work, we evaluate the impact of synthetic data on bias\nand performance of FR systems. We generate balanced face dataset, FairFaceGen,\nusing two state of the art text-to-image generators, Flux.1-dev and Stable\nDiffusion v3.5 (SD35), and combine them with several identity augmentation\nmethods, including Arc2Face and four IP-Adapters. By maintaining equal identity\ncount across synthetic and real datasets, we ensure fair comparisons when\nevaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging\nIJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our\nresults demonstrate that although synthetic data still lags behind the real\ndatasets in the generalization on IJB-B/C, demographically balanced synthetic\ndatasets, especially those generated with SD35, show potential for bias\nmitigation. We also observe that the number and quality of intra-class\naugmentations significantly affect FR accuracy and fairness. These findings\nprovide practical guidelines for constructing fairer FR systems using synthetic\ndata.\n", "link": "http://arxiv.org/abs/2507.20782v1", "date": "2025-07-28", "relevancy": 2.0487, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5149}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5122}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigation%20of%20Accuracy%20and%20Bias%20in%20Face%20Recognition%20Trained%20with%0A%20%20Synthetic%20Data&body=Title%3A%20Investigation%20of%20Accuracy%20and%20Bias%20in%20Face%20Recognition%20Trained%20with%0A%20%20Synthetic%20Data%0AAuthor%3A%20Pavel%20Korshunov%20and%20Ketan%20Kotwal%20and%20Christophe%20Ecabert%20and%20Vidit%20Vidit%20and%20Amir%20Mohammadi%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20Synthetic%20data%20has%20emerged%20as%20a%20promising%20alternative%20for%20training%20face%0Arecognition%20%28FR%29%20models%2C%20offering%20advantages%20in%20scalability%2C%20privacy%0Acompliance%2C%20and%20potential%20for%20bias%20mitigation.%20However%2C%20critical%20questions%0Aremain%20on%20whether%20both%20high%20accuracy%20and%20fairness%20can%20be%20achieved%20with%0Asynthetic%20data.%20In%20this%20work%2C%20we%20evaluate%20the%20impact%20of%20synthetic%20data%20on%20bias%0Aand%20performance%20of%20FR%20systems.%20We%20generate%20balanced%20face%20dataset%2C%20FairFaceGen%2C%0Ausing%20two%20state%20of%20the%20art%20text-to-image%20generators%2C%20Flux.1-dev%20and%20Stable%0ADiffusion%20v3.5%20%28SD35%29%2C%20and%20combine%20them%20with%20several%20identity%20augmentation%0Amethods%2C%20including%20Arc2Face%20and%20four%20IP-Adapters.%20By%20maintaining%20equal%20identity%0Acount%20across%20synthetic%20and%20real%20datasets%2C%20we%20ensure%20fair%20comparisons%20when%0Aevaluating%20FR%20performance%20on%20standard%20%28LFW%2C%20AgeDB-30%2C%20etc.%29%20and%20challenging%0AIJB-B/C%20benchmarks%20and%20FR%20bias%20on%20Racial%20Faces%20in-the-Wild%20%28RFW%29%20dataset.%20Our%0Aresults%20demonstrate%20that%20although%20synthetic%20data%20still%20lags%20behind%20the%20real%0Adatasets%20in%20the%20generalization%20on%20IJB-B/C%2C%20demographically%20balanced%20synthetic%0Adatasets%2C%20especially%20those%20generated%20with%20SD35%2C%20show%20potential%20for%20bias%0Amitigation.%20We%20also%20observe%20that%20the%20number%20and%20quality%20of%20intra-class%0Aaugmentations%20significantly%20affect%20FR%20accuracy%20and%20fairness.%20These%20findings%0Aprovide%20practical%20guidelines%20for%20constructing%20fairer%20FR%20systems%20using%20synthetic%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigation%2520of%2520Accuracy%2520and%2520Bias%2520in%2520Face%2520Recognition%2520Trained%2520with%250A%2520%2520Synthetic%2520Data%26entry.906535625%3DPavel%2520Korshunov%2520and%2520Ketan%2520Kotwal%2520and%2520Christophe%2520Ecabert%2520and%2520Vidit%2520Vidit%2520and%2520Amir%2520Mohammadi%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520Synthetic%2520data%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520for%2520training%2520face%250Arecognition%2520%2528FR%2529%2520models%252C%2520offering%2520advantages%2520in%2520scalability%252C%2520privacy%250Acompliance%252C%2520and%2520potential%2520for%2520bias%2520mitigation.%2520However%252C%2520critical%2520questions%250Aremain%2520on%2520whether%2520both%2520high%2520accuracy%2520and%2520fairness%2520can%2520be%2520achieved%2520with%250Asynthetic%2520data.%2520In%2520this%2520work%252C%2520we%2520evaluate%2520the%2520impact%2520of%2520synthetic%2520data%2520on%2520bias%250Aand%2520performance%2520of%2520FR%2520systems.%2520We%2520generate%2520balanced%2520face%2520dataset%252C%2520FairFaceGen%252C%250Ausing%2520two%2520state%2520of%2520the%2520art%2520text-to-image%2520generators%252C%2520Flux.1-dev%2520and%2520Stable%250ADiffusion%2520v3.5%2520%2528SD35%2529%252C%2520and%2520combine%2520them%2520with%2520several%2520identity%2520augmentation%250Amethods%252C%2520including%2520Arc2Face%2520and%2520four%2520IP-Adapters.%2520By%2520maintaining%2520equal%2520identity%250Acount%2520across%2520synthetic%2520and%2520real%2520datasets%252C%2520we%2520ensure%2520fair%2520comparisons%2520when%250Aevaluating%2520FR%2520performance%2520on%2520standard%2520%2528LFW%252C%2520AgeDB-30%252C%2520etc.%2529%2520and%2520challenging%250AIJB-B/C%2520benchmarks%2520and%2520FR%2520bias%2520on%2520Racial%2520Faces%2520in-the-Wild%2520%2528RFW%2529%2520dataset.%2520Our%250Aresults%2520demonstrate%2520that%2520although%2520synthetic%2520data%2520still%2520lags%2520behind%2520the%2520real%250Adatasets%2520in%2520the%2520generalization%2520on%2520IJB-B/C%252C%2520demographically%2520balanced%2520synthetic%250Adatasets%252C%2520especially%2520those%2520generated%2520with%2520SD35%252C%2520show%2520potential%2520for%2520bias%250Amitigation.%2520We%2520also%2520observe%2520that%2520the%2520number%2520and%2520quality%2520of%2520intra-class%250Aaugmentations%2520significantly%2520affect%2520FR%2520accuracy%2520and%2520fairness.%2520These%2520findings%250Aprovide%2520practical%2520guidelines%2520for%2520constructing%2520fairer%2520FR%2520systems%2520using%2520synthetic%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigation%20of%20Accuracy%20and%20Bias%20in%20Face%20Recognition%20Trained%20with%0A%20%20Synthetic%20Data&entry.906535625=Pavel%20Korshunov%20and%20Ketan%20Kotwal%20and%20Christophe%20Ecabert%20and%20Vidit%20Vidit%20and%20Amir%20Mohammadi%20and%20Sebastien%20Marcel&entry.1292438233=%20%20Synthetic%20data%20has%20emerged%20as%20a%20promising%20alternative%20for%20training%20face%0Arecognition%20%28FR%29%20models%2C%20offering%20advantages%20in%20scalability%2C%20privacy%0Acompliance%2C%20and%20potential%20for%20bias%20mitigation.%20However%2C%20critical%20questions%0Aremain%20on%20whether%20both%20high%20accuracy%20and%20fairness%20can%20be%20achieved%20with%0Asynthetic%20data.%20In%20this%20work%2C%20we%20evaluate%20the%20impact%20of%20synthetic%20data%20on%20bias%0Aand%20performance%20of%20FR%20systems.%20We%20generate%20balanced%20face%20dataset%2C%20FairFaceGen%2C%0Ausing%20two%20state%20of%20the%20art%20text-to-image%20generators%2C%20Flux.1-dev%20and%20Stable%0ADiffusion%20v3.5%20%28SD35%29%2C%20and%20combine%20them%20with%20several%20identity%20augmentation%0Amethods%2C%20including%20Arc2Face%20and%20four%20IP-Adapters.%20By%20maintaining%20equal%20identity%0Acount%20across%20synthetic%20and%20real%20datasets%2C%20we%20ensure%20fair%20comparisons%20when%0Aevaluating%20FR%20performance%20on%20standard%20%28LFW%2C%20AgeDB-30%2C%20etc.%29%20and%20challenging%0AIJB-B/C%20benchmarks%20and%20FR%20bias%20on%20Racial%20Faces%20in-the-Wild%20%28RFW%29%20dataset.%20Our%0Aresults%20demonstrate%20that%20although%20synthetic%20data%20still%20lags%20behind%20the%20real%0Adatasets%20in%20the%20generalization%20on%20IJB-B/C%2C%20demographically%20balanced%20synthetic%0Adatasets%2C%20especially%20those%20generated%20with%20SD35%2C%20show%20potential%20for%20bias%0Amitigation.%20We%20also%20observe%20that%20the%20number%20and%20quality%20of%20intra-class%0Aaugmentations%20significantly%20affect%20FR%20accuracy%20and%20fairness.%20These%20findings%0Aprovide%20practical%20guidelines%20for%20constructing%20fairer%20FR%20systems%20using%20synthetic%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20782v1&entry.124074799=Read"},
{"title": "Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural\n  Network", "author": "Davide Piccinini and Diego Valsesia and Enrico Magli", "abstract": "  Hyperspectral imagers on satellites obtain the fine spectral signatures\nessential for distinguishing one material from another at the expense of\nlimited spatial resolution. Enhancing the latter is thus a desirable\npreprocessing step in order to further improve the detection capabilities\noffered by hyperspectral images on downstream tasks. At the same time, there is\na growing interest towards deploying inference methods directly onboard of\nsatellites, which calls for lightweight image super-resolution methods that can\nbe run on the payload in real time. In this paper, we present a novel neural\nnetwork design, called Deep Pushbroom Super-Resolution (DPSR) that matches the\npushbroom acquisition of hyperspectral sensors by processing an image line by\nline in the along-track direction with a causal memory mechanism to exploit\npreviously acquired lines. This design greatly limits memory requirements and\ncomputational complexity, achieving onboard real-time performance, i.e., the\nability to super-resolve a line in the time it takes to acquire the next one,\non low-power hardware. Experiments show that the quality of the super-resolved\nimages is competitive or even outperforms state-of-the-art methods that are\nsignificantly more complex.\n", "link": "http://arxiv.org/abs/2507.20765v1", "date": "2025-07-28", "relevancy": 2.0436, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5415}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5105}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Onboard%20Hyperspectral%20Super-Resolution%20with%20Deep%20Pushbroom%20Neural%0A%20%20Network&body=Title%3A%20Onboard%20Hyperspectral%20Super-Resolution%20with%20Deep%20Pushbroom%20Neural%0A%20%20Network%0AAuthor%3A%20Davide%20Piccinini%20and%20Diego%20Valsesia%20and%20Enrico%20Magli%0AAbstract%3A%20%20%20Hyperspectral%20imagers%20on%20satellites%20obtain%20the%20fine%20spectral%20signatures%0Aessential%20for%20distinguishing%20one%20material%20from%20another%20at%20the%20expense%20of%0Alimited%20spatial%20resolution.%20Enhancing%20the%20latter%20is%20thus%20a%20desirable%0Apreprocessing%20step%20in%20order%20to%20further%20improve%20the%20detection%20capabilities%0Aoffered%20by%20hyperspectral%20images%20on%20downstream%20tasks.%20At%20the%20same%20time%2C%20there%20is%0Aa%20growing%20interest%20towards%20deploying%20inference%20methods%20directly%20onboard%20of%0Asatellites%2C%20which%20calls%20for%20lightweight%20image%20super-resolution%20methods%20that%20can%0Abe%20run%20on%20the%20payload%20in%20real%20time.%20In%20this%20paper%2C%20we%20present%20a%20novel%20neural%0Anetwork%20design%2C%20called%20Deep%20Pushbroom%20Super-Resolution%20%28DPSR%29%20that%20matches%20the%0Apushbroom%20acquisition%20of%20hyperspectral%20sensors%20by%20processing%20an%20image%20line%20by%0Aline%20in%20the%20along-track%20direction%20with%20a%20causal%20memory%20mechanism%20to%20exploit%0Apreviously%20acquired%20lines.%20This%20design%20greatly%20limits%20memory%20requirements%20and%0Acomputational%20complexity%2C%20achieving%20onboard%20real-time%20performance%2C%20i.e.%2C%20the%0Aability%20to%20super-resolve%20a%20line%20in%20the%20time%20it%20takes%20to%20acquire%20the%20next%20one%2C%0Aon%20low-power%20hardware.%20Experiments%20show%20that%20the%20quality%20of%20the%20super-resolved%0Aimages%20is%20competitive%20or%20even%20outperforms%20state-of-the-art%20methods%20that%20are%0Asignificantly%20more%20complex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnboard%2520Hyperspectral%2520Super-Resolution%2520with%2520Deep%2520Pushbroom%2520Neural%250A%2520%2520Network%26entry.906535625%3DDavide%2520Piccinini%2520and%2520Diego%2520Valsesia%2520and%2520Enrico%2520Magli%26entry.1292438233%3D%2520%2520Hyperspectral%2520imagers%2520on%2520satellites%2520obtain%2520the%2520fine%2520spectral%2520signatures%250Aessential%2520for%2520distinguishing%2520one%2520material%2520from%2520another%2520at%2520the%2520expense%2520of%250Alimited%2520spatial%2520resolution.%2520Enhancing%2520the%2520latter%2520is%2520thus%2520a%2520desirable%250Apreprocessing%2520step%2520in%2520order%2520to%2520further%2520improve%2520the%2520detection%2520capabilities%250Aoffered%2520by%2520hyperspectral%2520images%2520on%2520downstream%2520tasks.%2520At%2520the%2520same%2520time%252C%2520there%2520is%250Aa%2520growing%2520interest%2520towards%2520deploying%2520inference%2520methods%2520directly%2520onboard%2520of%250Asatellites%252C%2520which%2520calls%2520for%2520lightweight%2520image%2520super-resolution%2520methods%2520that%2520can%250Abe%2520run%2520on%2520the%2520payload%2520in%2520real%2520time.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520neural%250Anetwork%2520design%252C%2520called%2520Deep%2520Pushbroom%2520Super-Resolution%2520%2528DPSR%2529%2520that%2520matches%2520the%250Apushbroom%2520acquisition%2520of%2520hyperspectral%2520sensors%2520by%2520processing%2520an%2520image%2520line%2520by%250Aline%2520in%2520the%2520along-track%2520direction%2520with%2520a%2520causal%2520memory%2520mechanism%2520to%2520exploit%250Apreviously%2520acquired%2520lines.%2520This%2520design%2520greatly%2520limits%2520memory%2520requirements%2520and%250Acomputational%2520complexity%252C%2520achieving%2520onboard%2520real-time%2520performance%252C%2520i.e.%252C%2520the%250Aability%2520to%2520super-resolve%2520a%2520line%2520in%2520the%2520time%2520it%2520takes%2520to%2520acquire%2520the%2520next%2520one%252C%250Aon%2520low-power%2520hardware.%2520Experiments%2520show%2520that%2520the%2520quality%2520of%2520the%2520super-resolved%250Aimages%2520is%2520competitive%2520or%2520even%2520outperforms%2520state-of-the-art%2520methods%2520that%2520are%250Asignificantly%2520more%2520complex.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Onboard%20Hyperspectral%20Super-Resolution%20with%20Deep%20Pushbroom%20Neural%0A%20%20Network&entry.906535625=Davide%20Piccinini%20and%20Diego%20Valsesia%20and%20Enrico%20Magli&entry.1292438233=%20%20Hyperspectral%20imagers%20on%20satellites%20obtain%20the%20fine%20spectral%20signatures%0Aessential%20for%20distinguishing%20one%20material%20from%20another%20at%20the%20expense%20of%0Alimited%20spatial%20resolution.%20Enhancing%20the%20latter%20is%20thus%20a%20desirable%0Apreprocessing%20step%20in%20order%20to%20further%20improve%20the%20detection%20capabilities%0Aoffered%20by%20hyperspectral%20images%20on%20downstream%20tasks.%20At%20the%20same%20time%2C%20there%20is%0Aa%20growing%20interest%20towards%20deploying%20inference%20methods%20directly%20onboard%20of%0Asatellites%2C%20which%20calls%20for%20lightweight%20image%20super-resolution%20methods%20that%20can%0Abe%20run%20on%20the%20payload%20in%20real%20time.%20In%20this%20paper%2C%20we%20present%20a%20novel%20neural%0Anetwork%20design%2C%20called%20Deep%20Pushbroom%20Super-Resolution%20%28DPSR%29%20that%20matches%20the%0Apushbroom%20acquisition%20of%20hyperspectral%20sensors%20by%20processing%20an%20image%20line%20by%0Aline%20in%20the%20along-track%20direction%20with%20a%20causal%20memory%20mechanism%20to%20exploit%0Apreviously%20acquired%20lines.%20This%20design%20greatly%20limits%20memory%20requirements%20and%0Acomputational%20complexity%2C%20achieving%20onboard%20real-time%20performance%2C%20i.e.%2C%20the%0Aability%20to%20super-resolve%20a%20line%20in%20the%20time%20it%20takes%20to%20acquire%20the%20next%20one%2C%0Aon%20low-power%20hardware.%20Experiments%20show%20that%20the%20quality%20of%20the%20super-resolved%0Aimages%20is%20competitive%20or%20even%20outperforms%20state-of-the-art%20methods%20that%20are%0Asignificantly%20more%20complex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20765v1&entry.124074799=Read"},
{"title": "Are ECGs enough? Deep learning classification of pulmonary embolism\n  using electrocardiograms", "author": "Joao D. S. Marques and Arlindo L. Oliveira", "abstract": "  Pulmonary embolism is a leading cause of out of hospital cardiac arrest that\nrequires fast diagnosis. While computed tomography pulmonary angiography is the\nstandard diagnostic tool, it is not always accessible. Electrocardiography is\nan essential tool for diagnosing multiple cardiac anomalies, as it is\naffordable, fast and available in many settings. However, the availability of\npublic ECG datasets, specially for PE, is limited and, in practice, these\ndatasets tend to be small, making it essential to optimize learning strategies.\nIn this study, we investigate the performance of multiple neural networks in\norder to assess the impact of various approaches. Moreover, we check whether\nthese practices enhance model generalization when transfer learning is used to\ntranslate information learned in larger ECG datasets, such as PTB-XL, CPSC18\nand MedalCare-XL, to a smaller, more challenging dataset for PE. By leveraging\ntransfer learning, we analyze the extent to which we can improve learning\nefficiency and predictive performance on limited data. Code available at\nhttps://github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers .\n", "link": "http://arxiv.org/abs/2503.08960v2", "date": "2025-07-28", "relevancy": 1.8507, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4748}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4674}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20ECGs%20enough%3F%20Deep%20learning%20classification%20of%20pulmonary%20embolism%0A%20%20using%20electrocardiograms&body=Title%3A%20Are%20ECGs%20enough%3F%20Deep%20learning%20classification%20of%20pulmonary%20embolism%0A%20%20using%20electrocardiograms%0AAuthor%3A%20Joao%20D.%20S.%20Marques%20and%20Arlindo%20L.%20Oliveira%0AAbstract%3A%20%20%20Pulmonary%20embolism%20is%20a%20leading%20cause%20of%20out%20of%20hospital%20cardiac%20arrest%20that%0Arequires%20fast%20diagnosis.%20While%20computed%20tomography%20pulmonary%20angiography%20is%20the%0Astandard%20diagnostic%20tool%2C%20it%20is%20not%20always%20accessible.%20Electrocardiography%20is%0Aan%20essential%20tool%20for%20diagnosing%20multiple%20cardiac%20anomalies%2C%20as%20it%20is%0Aaffordable%2C%20fast%20and%20available%20in%20many%20settings.%20However%2C%20the%20availability%20of%0Apublic%20ECG%20datasets%2C%20specially%20for%20PE%2C%20is%20limited%20and%2C%20in%20practice%2C%20these%0Adatasets%20tend%20to%20be%20small%2C%20making%20it%20essential%20to%20optimize%20learning%20strategies.%0AIn%20this%20study%2C%20we%20investigate%20the%20performance%20of%20multiple%20neural%20networks%20in%0Aorder%20to%20assess%20the%20impact%20of%20various%20approaches.%20Moreover%2C%20we%20check%20whether%0Athese%20practices%20enhance%20model%20generalization%20when%20transfer%20learning%20is%20used%20to%0Atranslate%20information%20learned%20in%20larger%20ECG%20datasets%2C%20such%20as%20PTB-XL%2C%20CPSC18%0Aand%20MedalCare-XL%2C%20to%20a%20smaller%2C%20more%20challenging%20dataset%20for%20PE.%20By%20leveraging%0Atransfer%20learning%2C%20we%20analyze%20the%20extent%20to%20which%20we%20can%20improve%20learning%0Aefficiency%20and%20predictive%20performance%20on%20limited%20data.%20Code%20available%20at%0Ahttps%3A//github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08960v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520ECGs%2520enough%253F%2520Deep%2520learning%2520classification%2520of%2520pulmonary%2520embolism%250A%2520%2520using%2520electrocardiograms%26entry.906535625%3DJoao%2520D.%2520S.%2520Marques%2520and%2520Arlindo%2520L.%2520Oliveira%26entry.1292438233%3D%2520%2520Pulmonary%2520embolism%2520is%2520a%2520leading%2520cause%2520of%2520out%2520of%2520hospital%2520cardiac%2520arrest%2520that%250Arequires%2520fast%2520diagnosis.%2520While%2520computed%2520tomography%2520pulmonary%2520angiography%2520is%2520the%250Astandard%2520diagnostic%2520tool%252C%2520it%2520is%2520not%2520always%2520accessible.%2520Electrocardiography%2520is%250Aan%2520essential%2520tool%2520for%2520diagnosing%2520multiple%2520cardiac%2520anomalies%252C%2520as%2520it%2520is%250Aaffordable%252C%2520fast%2520and%2520available%2520in%2520many%2520settings.%2520However%252C%2520the%2520availability%2520of%250Apublic%2520ECG%2520datasets%252C%2520specially%2520for%2520PE%252C%2520is%2520limited%2520and%252C%2520in%2520practice%252C%2520these%250Adatasets%2520tend%2520to%2520be%2520small%252C%2520making%2520it%2520essential%2520to%2520optimize%2520learning%2520strategies.%250AIn%2520this%2520study%252C%2520we%2520investigate%2520the%2520performance%2520of%2520multiple%2520neural%2520networks%2520in%250Aorder%2520to%2520assess%2520the%2520impact%2520of%2520various%2520approaches.%2520Moreover%252C%2520we%2520check%2520whether%250Athese%2520practices%2520enhance%2520model%2520generalization%2520when%2520transfer%2520learning%2520is%2520used%2520to%250Atranslate%2520information%2520learned%2520in%2520larger%2520ECG%2520datasets%252C%2520such%2520as%2520PTB-XL%252C%2520CPSC18%250Aand%2520MedalCare-XL%252C%2520to%2520a%2520smaller%252C%2520more%2520challenging%2520dataset%2520for%2520PE.%2520By%2520leveraging%250Atransfer%2520learning%252C%2520we%2520analyze%2520the%2520extent%2520to%2520which%2520we%2520can%2520improve%2520learning%250Aefficiency%2520and%2520predictive%2520performance%2520on%2520limited%2520data.%2520Code%2520available%2520at%250Ahttps%253A//github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08960v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20ECGs%20enough%3F%20Deep%20learning%20classification%20of%20pulmonary%20embolism%0A%20%20using%20electrocardiograms&entry.906535625=Joao%20D.%20S.%20Marques%20and%20Arlindo%20L.%20Oliveira&entry.1292438233=%20%20Pulmonary%20embolism%20is%20a%20leading%20cause%20of%20out%20of%20hospital%20cardiac%20arrest%20that%0Arequires%20fast%20diagnosis.%20While%20computed%20tomography%20pulmonary%20angiography%20is%20the%0Astandard%20diagnostic%20tool%2C%20it%20is%20not%20always%20accessible.%20Electrocardiography%20is%0Aan%20essential%20tool%20for%20diagnosing%20multiple%20cardiac%20anomalies%2C%20as%20it%20is%0Aaffordable%2C%20fast%20and%20available%20in%20many%20settings.%20However%2C%20the%20availability%20of%0Apublic%20ECG%20datasets%2C%20specially%20for%20PE%2C%20is%20limited%20and%2C%20in%20practice%2C%20these%0Adatasets%20tend%20to%20be%20small%2C%20making%20it%20essential%20to%20optimize%20learning%20strategies.%0AIn%20this%20study%2C%20we%20investigate%20the%20performance%20of%20multiple%20neural%20networks%20in%0Aorder%20to%20assess%20the%20impact%20of%20various%20approaches.%20Moreover%2C%20we%20check%20whether%0Athese%20practices%20enhance%20model%20generalization%20when%20transfer%20learning%20is%20used%20to%0Atranslate%20information%20learned%20in%20larger%20ECG%20datasets%2C%20such%20as%20PTB-XL%2C%20CPSC18%0Aand%20MedalCare-XL%2C%20to%20a%20smaller%2C%20more%20challenging%20dataset%20for%20PE.%20By%20leveraging%0Atransfer%20learning%2C%20we%20analyze%20the%20extent%20to%20which%20we%20can%20improve%20learning%0Aefficiency%20and%20predictive%20performance%20on%20limited%20data.%20Code%20available%20at%0Ahttps%3A//github.com/joaodsmarques/Are-ECGs-enough-Deep-Learning-Classifiers%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08960v2&entry.124074799=Read"},
{"title": "Core Safety Values for Provably Corrigible Agents", "author": "Aran Nayebi", "abstract": "  We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems.\n", "link": "http://arxiv.org/abs/2507.20964v1", "date": "2025-07-28", "relevancy": 1.4415, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4819}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Core%20Safety%20Values%20for%20Provably%20Corrigible%20Agents&body=Title%3A%20Core%20Safety%20Values%20for%20Provably%20Corrigible%20Agents%0AAuthor%3A%20Aran%20Nayebi%0AAbstract%3A%20%20%20We%20introduce%20the%20first%20implementable%20framework%20for%20corrigibility%2C%20with%0Aprovable%20guarantees%20in%20multi-step%2C%20partially%20observed%20environments.%20Our%0Aframework%20replaces%20a%20single%20opaque%20reward%20with%20five%20%2Astructurally%20separate%2A%0Autility%20heads%20--%20deference%2C%20switch-access%20preservation%2C%20truthfulness%2C%0Alow-impact%20behavior%20via%20a%20belief-based%20extension%20of%20Attainable%20Utility%0APreservation%2C%20and%20bounded%20task%20reward%20--%20combined%20lexicographically%20by%20strict%0Aweight%20gaps.%20Theorem%201%20proves%20exact%20single-round%20corrigibility%20in%20the%20partially%0Aobservable%20off-switch%20game%3B%20Theorem%203%20extends%20the%20guarantee%20to%20multi-step%2C%0Aself-spawning%20agents%2C%20showing%20that%20even%20if%20each%20head%20is%20%5Cemph%7Blearned%7D%20to%0Amean-squared%20error%20%24%5Cvarepsilon%24%20and%20the%20planner%20is%20%24%5Cvarepsilon%24-sub-optimal%2C%0Athe%20probability%20of%20violating%20%5Cemph%7Bany%7D%20safety%20property%20is%20bounded%20while%20still%0Aensuring%20net%20human%20benefit.%20In%20contrast%20to%20Constitutional%20AI%20or%20RLHF/RLAIF%2C%0Awhich%20merge%20all%20norms%20into%20one%20learned%20scalar%2C%20our%20separation%20makes%20obedience%0Aand%20impact-limits%20dominate%20even%20when%20incentives%20conflict.%20For%20open-ended%0Asettings%20where%20adversaries%20can%20modify%20the%20agent%2C%20we%20prove%20that%20deciding%20whether%0Aan%20arbitrary%20post-hack%20agent%20will%20ever%20violate%20corrigibility%20is%20undecidable%20by%0Areduction%20to%20the%20halting%20problem%2C%20then%20carve%20out%20a%20finite-horizon%20%60%60decidable%0Aisland%27%27%20where%20safety%20can%20be%20certified%20in%20randomized%20polynomial%20time%20and%0Averified%20with%20privacy-preserving%2C%20constant-round%20zero-knowledge%20proofs.%0AConsequently%2C%20the%20remaining%20challenge%20is%20the%20ordinary%20ML%20task%20of%20data%20coverage%0Aand%20generalization%3A%20reward-hacking%20risk%20is%20pushed%20into%20evaluation%20quality%0Arather%20than%20hidden%20incentive%20leak-through%2C%20giving%20clearer%20implementation%0Aguidance%20for%20today%27s%20LLM%20assistants%20and%20future%20autonomous%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCore%2520Safety%2520Values%2520for%2520Provably%2520Corrigible%2520Agents%26entry.906535625%3DAran%2520Nayebi%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520first%2520implementable%2520framework%2520for%2520corrigibility%252C%2520with%250Aprovable%2520guarantees%2520in%2520multi-step%252C%2520partially%2520observed%2520environments.%2520Our%250Aframework%2520replaces%2520a%2520single%2520opaque%2520reward%2520with%2520five%2520%252Astructurally%2520separate%252A%250Autility%2520heads%2520--%2520deference%252C%2520switch-access%2520preservation%252C%2520truthfulness%252C%250Alow-impact%2520behavior%2520via%2520a%2520belief-based%2520extension%2520of%2520Attainable%2520Utility%250APreservation%252C%2520and%2520bounded%2520task%2520reward%2520--%2520combined%2520lexicographically%2520by%2520strict%250Aweight%2520gaps.%2520Theorem%25201%2520proves%2520exact%2520single-round%2520corrigibility%2520in%2520the%2520partially%250Aobservable%2520off-switch%2520game%253B%2520Theorem%25203%2520extends%2520the%2520guarantee%2520to%2520multi-step%252C%250Aself-spawning%2520agents%252C%2520showing%2520that%2520even%2520if%2520each%2520head%2520is%2520%255Cemph%257Blearned%257D%2520to%250Amean-squared%2520error%2520%2524%255Cvarepsilon%2524%2520and%2520the%2520planner%2520is%2520%2524%255Cvarepsilon%2524-sub-optimal%252C%250Athe%2520probability%2520of%2520violating%2520%255Cemph%257Bany%257D%2520safety%2520property%2520is%2520bounded%2520while%2520still%250Aensuring%2520net%2520human%2520benefit.%2520In%2520contrast%2520to%2520Constitutional%2520AI%2520or%2520RLHF/RLAIF%252C%250Awhich%2520merge%2520all%2520norms%2520into%2520one%2520learned%2520scalar%252C%2520our%2520separation%2520makes%2520obedience%250Aand%2520impact-limits%2520dominate%2520even%2520when%2520incentives%2520conflict.%2520For%2520open-ended%250Asettings%2520where%2520adversaries%2520can%2520modify%2520the%2520agent%252C%2520we%2520prove%2520that%2520deciding%2520whether%250Aan%2520arbitrary%2520post-hack%2520agent%2520will%2520ever%2520violate%2520corrigibility%2520is%2520undecidable%2520by%250Areduction%2520to%2520the%2520halting%2520problem%252C%2520then%2520carve%2520out%2520a%2520finite-horizon%2520%2560%2560decidable%250Aisland%2527%2527%2520where%2520safety%2520can%2520be%2520certified%2520in%2520randomized%2520polynomial%2520time%2520and%250Averified%2520with%2520privacy-preserving%252C%2520constant-round%2520zero-knowledge%2520proofs.%250AConsequently%252C%2520the%2520remaining%2520challenge%2520is%2520the%2520ordinary%2520ML%2520task%2520of%2520data%2520coverage%250Aand%2520generalization%253A%2520reward-hacking%2520risk%2520is%2520pushed%2520into%2520evaluation%2520quality%250Arather%2520than%2520hidden%2520incentive%2520leak-through%252C%2520giving%2520clearer%2520implementation%250Aguidance%2520for%2520today%2527s%2520LLM%2520assistants%2520and%2520future%2520autonomous%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Core%20Safety%20Values%20for%20Provably%20Corrigible%20Agents&entry.906535625=Aran%20Nayebi&entry.1292438233=%20%20We%20introduce%20the%20first%20implementable%20framework%20for%20corrigibility%2C%20with%0Aprovable%20guarantees%20in%20multi-step%2C%20partially%20observed%20environments.%20Our%0Aframework%20replaces%20a%20single%20opaque%20reward%20with%20five%20%2Astructurally%20separate%2A%0Autility%20heads%20--%20deference%2C%20switch-access%20preservation%2C%20truthfulness%2C%0Alow-impact%20behavior%20via%20a%20belief-based%20extension%20of%20Attainable%20Utility%0APreservation%2C%20and%20bounded%20task%20reward%20--%20combined%20lexicographically%20by%20strict%0Aweight%20gaps.%20Theorem%201%20proves%20exact%20single-round%20corrigibility%20in%20the%20partially%0Aobservable%20off-switch%20game%3B%20Theorem%203%20extends%20the%20guarantee%20to%20multi-step%2C%0Aself-spawning%20agents%2C%20showing%20that%20even%20if%20each%20head%20is%20%5Cemph%7Blearned%7D%20to%0Amean-squared%20error%20%24%5Cvarepsilon%24%20and%20the%20planner%20is%20%24%5Cvarepsilon%24-sub-optimal%2C%0Athe%20probability%20of%20violating%20%5Cemph%7Bany%7D%20safety%20property%20is%20bounded%20while%20still%0Aensuring%20net%20human%20benefit.%20In%20contrast%20to%20Constitutional%20AI%20or%20RLHF/RLAIF%2C%0Awhich%20merge%20all%20norms%20into%20one%20learned%20scalar%2C%20our%20separation%20makes%20obedience%0Aand%20impact-limits%20dominate%20even%20when%20incentives%20conflict.%20For%20open-ended%0Asettings%20where%20adversaries%20can%20modify%20the%20agent%2C%20we%20prove%20that%20deciding%20whether%0Aan%20arbitrary%20post-hack%20agent%20will%20ever%20violate%20corrigibility%20is%20undecidable%20by%0Areduction%20to%20the%20halting%20problem%2C%20then%20carve%20out%20a%20finite-horizon%20%60%60decidable%0Aisland%27%27%20where%20safety%20can%20be%20certified%20in%20randomized%20polynomial%20time%20and%0Averified%20with%20privacy-preserving%2C%20constant-round%20zero-knowledge%20proofs.%0AConsequently%2C%20the%20remaining%20challenge%20is%20the%20ordinary%20ML%20task%20of%20data%20coverage%0Aand%20generalization%3A%20reward-hacking%20risk%20is%20pushed%20into%20evaluation%20quality%0Arather%20than%20hidden%20incentive%20leak-through%2C%20giving%20clearer%20implementation%0Aguidance%20for%20today%27s%20LLM%20assistants%20and%20future%20autonomous%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20964v1&entry.124074799=Read"},
{"title": "AIComposer: Any Style and Content Image Composition via Feature\n  Integration", "author": "Haowen Li and Zhenfeng Fan and Zhang Wen and Zhengzhou Zhu and Yunjin Li", "abstract": "  Image composition has advanced significantly with large-scale pre-trained T2I\ndiffusion models. Despite progress in same-domain composition, cross-domain\ncomposition remains under-explored. The main challenges are the stochastic\nnature of diffusion models and the style gap between input images, leading to\nfailures and artifacts. Additionally, heavy reliance on text prompts limits\npractical applications. This paper presents the first cross-domain image\ncomposition method that does not require text prompts, allowing natural\nstylization and seamless compositions. Our method is efficient and robust,\npreserving the diffusion prior, as it involves minor steps for backward\ninversion and forward denoising without training the diffuser. Our method also\nuses a simple multilayer perceptron network to integrate CLIP features from\nforeground and background, manipulating diffusion with a local cross-attention\nstrategy. It effectively preserves foreground content while enabling stable\nstylization without a pre-stylization network. Finally, we create a benchmark\ndataset with diverse contents and styles for fair evaluation, addressing the\nlack of testing datasets for cross-domain image composition. Our method\noutperforms state-of-the-art techniques in both qualitative and quantitative\nevaluations, significantly improving the LPIPS score by 30.5% and the CSD\nmetric by 18.1%. We believe our method will advance future research and\napplications. Code and benchmark at https://github.com/sherlhw/AIComposer.\n", "link": "http://arxiv.org/abs/2507.20721v1", "date": "2025-07-28", "relevancy": 1.7464, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6296}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.57}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIComposer%3A%20Any%20Style%20and%20Content%20Image%20Composition%20via%20Feature%0A%20%20Integration&body=Title%3A%20AIComposer%3A%20Any%20Style%20and%20Content%20Image%20Composition%20via%20Feature%0A%20%20Integration%0AAuthor%3A%20Haowen%20Li%20and%20Zhenfeng%20Fan%20and%20Zhang%20Wen%20and%20Zhengzhou%20Zhu%20and%20Yunjin%20Li%0AAbstract%3A%20%20%20Image%20composition%20has%20advanced%20significantly%20with%20large-scale%20pre-trained%20T2I%0Adiffusion%20models.%20Despite%20progress%20in%20same-domain%20composition%2C%20cross-domain%0Acomposition%20remains%20under-explored.%20The%20main%20challenges%20are%20the%20stochastic%0Anature%20of%20diffusion%20models%20and%20the%20style%20gap%20between%20input%20images%2C%20leading%20to%0Afailures%20and%20artifacts.%20Additionally%2C%20heavy%20reliance%20on%20text%20prompts%20limits%0Apractical%20applications.%20This%20paper%20presents%20the%20first%20cross-domain%20image%0Acomposition%20method%20that%20does%20not%20require%20text%20prompts%2C%20allowing%20natural%0Astylization%20and%20seamless%20compositions.%20Our%20method%20is%20efficient%20and%20robust%2C%0Apreserving%20the%20diffusion%20prior%2C%20as%20it%20involves%20minor%20steps%20for%20backward%0Ainversion%20and%20forward%20denoising%20without%20training%20the%20diffuser.%20Our%20method%20also%0Auses%20a%20simple%20multilayer%20perceptron%20network%20to%20integrate%20CLIP%20features%20from%0Aforeground%20and%20background%2C%20manipulating%20diffusion%20with%20a%20local%20cross-attention%0Astrategy.%20It%20effectively%20preserves%20foreground%20content%20while%20enabling%20stable%0Astylization%20without%20a%20pre-stylization%20network.%20Finally%2C%20we%20create%20a%20benchmark%0Adataset%20with%20diverse%20contents%20and%20styles%20for%20fair%20evaluation%2C%20addressing%20the%0Alack%20of%20testing%20datasets%20for%20cross-domain%20image%20composition.%20Our%20method%0Aoutperforms%20state-of-the-art%20techniques%20in%20both%20qualitative%20and%20quantitative%0Aevaluations%2C%20significantly%20improving%20the%20LPIPS%20score%20by%2030.5%25%20and%20the%20CSD%0Ametric%20by%2018.1%25.%20We%20believe%20our%20method%20will%20advance%20future%20research%20and%0Aapplications.%20Code%20and%20benchmark%20at%20https%3A//github.com/sherlhw/AIComposer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIComposer%253A%2520Any%2520Style%2520and%2520Content%2520Image%2520Composition%2520via%2520Feature%250A%2520%2520Integration%26entry.906535625%3DHaowen%2520Li%2520and%2520Zhenfeng%2520Fan%2520and%2520Zhang%2520Wen%2520and%2520Zhengzhou%2520Zhu%2520and%2520Yunjin%2520Li%26entry.1292438233%3D%2520%2520Image%2520composition%2520has%2520advanced%2520significantly%2520with%2520large-scale%2520pre-trained%2520T2I%250Adiffusion%2520models.%2520Despite%2520progress%2520in%2520same-domain%2520composition%252C%2520cross-domain%250Acomposition%2520remains%2520under-explored.%2520The%2520main%2520challenges%2520are%2520the%2520stochastic%250Anature%2520of%2520diffusion%2520models%2520and%2520the%2520style%2520gap%2520between%2520input%2520images%252C%2520leading%2520to%250Afailures%2520and%2520artifacts.%2520Additionally%252C%2520heavy%2520reliance%2520on%2520text%2520prompts%2520limits%250Apractical%2520applications.%2520This%2520paper%2520presents%2520the%2520first%2520cross-domain%2520image%250Acomposition%2520method%2520that%2520does%2520not%2520require%2520text%2520prompts%252C%2520allowing%2520natural%250Astylization%2520and%2520seamless%2520compositions.%2520Our%2520method%2520is%2520efficient%2520and%2520robust%252C%250Apreserving%2520the%2520diffusion%2520prior%252C%2520as%2520it%2520involves%2520minor%2520steps%2520for%2520backward%250Ainversion%2520and%2520forward%2520denoising%2520without%2520training%2520the%2520diffuser.%2520Our%2520method%2520also%250Auses%2520a%2520simple%2520multilayer%2520perceptron%2520network%2520to%2520integrate%2520CLIP%2520features%2520from%250Aforeground%2520and%2520background%252C%2520manipulating%2520diffusion%2520with%2520a%2520local%2520cross-attention%250Astrategy.%2520It%2520effectively%2520preserves%2520foreground%2520content%2520while%2520enabling%2520stable%250Astylization%2520without%2520a%2520pre-stylization%2520network.%2520Finally%252C%2520we%2520create%2520a%2520benchmark%250Adataset%2520with%2520diverse%2520contents%2520and%2520styles%2520for%2520fair%2520evaluation%252C%2520addressing%2520the%250Alack%2520of%2520testing%2520datasets%2520for%2520cross-domain%2520image%2520composition.%2520Our%2520method%250Aoutperforms%2520state-of-the-art%2520techniques%2520in%2520both%2520qualitative%2520and%2520quantitative%250Aevaluations%252C%2520significantly%2520improving%2520the%2520LPIPS%2520score%2520by%252030.5%2525%2520and%2520the%2520CSD%250Ametric%2520by%252018.1%2525.%2520We%2520believe%2520our%2520method%2520will%2520advance%2520future%2520research%2520and%250Aapplications.%2520Code%2520and%2520benchmark%2520at%2520https%253A//github.com/sherlhw/AIComposer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIComposer%3A%20Any%20Style%20and%20Content%20Image%20Composition%20via%20Feature%0A%20%20Integration&entry.906535625=Haowen%20Li%20and%20Zhenfeng%20Fan%20and%20Zhang%20Wen%20and%20Zhengzhou%20Zhu%20and%20Yunjin%20Li&entry.1292438233=%20%20Image%20composition%20has%20advanced%20significantly%20with%20large-scale%20pre-trained%20T2I%0Adiffusion%20models.%20Despite%20progress%20in%20same-domain%20composition%2C%20cross-domain%0Acomposition%20remains%20under-explored.%20The%20main%20challenges%20are%20the%20stochastic%0Anature%20of%20diffusion%20models%20and%20the%20style%20gap%20between%20input%20images%2C%20leading%20to%0Afailures%20and%20artifacts.%20Additionally%2C%20heavy%20reliance%20on%20text%20prompts%20limits%0Apractical%20applications.%20This%20paper%20presents%20the%20first%20cross-domain%20image%0Acomposition%20method%20that%20does%20not%20require%20text%20prompts%2C%20allowing%20natural%0Astylization%20and%20seamless%20compositions.%20Our%20method%20is%20efficient%20and%20robust%2C%0Apreserving%20the%20diffusion%20prior%2C%20as%20it%20involves%20minor%20steps%20for%20backward%0Ainversion%20and%20forward%20denoising%20without%20training%20the%20diffuser.%20Our%20method%20also%0Auses%20a%20simple%20multilayer%20perceptron%20network%20to%20integrate%20CLIP%20features%20from%0Aforeground%20and%20background%2C%20manipulating%20diffusion%20with%20a%20local%20cross-attention%0Astrategy.%20It%20effectively%20preserves%20foreground%20content%20while%20enabling%20stable%0Astylization%20without%20a%20pre-stylization%20network.%20Finally%2C%20we%20create%20a%20benchmark%0Adataset%20with%20diverse%20contents%20and%20styles%20for%20fair%20evaluation%2C%20addressing%20the%0Alack%20of%20testing%20datasets%20for%20cross-domain%20image%20composition.%20Our%20method%0Aoutperforms%20state-of-the-art%20techniques%20in%20both%20qualitative%20and%20quantitative%0Aevaluations%2C%20significantly%20improving%20the%20LPIPS%20score%20by%2030.5%25%20and%20the%20CSD%0Ametric%20by%2018.1%25.%20We%20believe%20our%20method%20will%20advance%20future%20research%20and%0Aapplications.%20Code%20and%20benchmark%20at%20https%3A//github.com/sherlhw/AIComposer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20721v1&entry.124074799=Read"},
{"title": "Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in\n  Visual Language Models", "author": "Gabriel Downer and Sean Craven and Damian Ruck and Jake Thomas", "abstract": "  The increasing integration of Visual Language Models (VLMs) into AI systems\nnecessitates robust model alignment, especially when handling multimodal\ncontent that combines text and images. Existing evaluation datasets heavily\nlean towards text-only prompts, leaving visual vulnerabilities under evaluated.\nTo address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline\nthat adapts text-only datasets into multimodal formats, specifically designed\nto evaluate the resilience of VLMs against typographic prompt injection\nattacks. The Text2VLM pipeline identifies harmful content in the original text\nand converts it into a typographic image, creating a multimodal prompt for\nVLMs. Also, our evaluation of open-source VLMs highlights their increased\nsusceptibility to prompt injection when visual inputs are introduced, revealing\ncritical weaknesses in the current models' alignment. This is in addition to a\nsignificant performance gap compared to closed-source frontier models. We\nvalidate Text2VLM through human evaluations, ensuring the alignment of\nextracted salient concepts; text summarization and output classification align\nwith human expectations. Text2VLM provides a scalable tool for comprehensive\nsafety assessment, contributing to the development of more robust safety\nmechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,\nText2VLM plays a role in advancing the safe deployment of VLMs in diverse,\nreal-world applications.\n", "link": "http://arxiv.org/abs/2507.20704v1", "date": "2025-07-28", "relevancy": 1.7115, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6045}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5326}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2VLM%3A%20Adapting%20Text-Only%20Datasets%20to%20Evaluate%20Alignment%20Training%20in%0A%20%20Visual%20Language%20Models&body=Title%3A%20Text2VLM%3A%20Adapting%20Text-Only%20Datasets%20to%20Evaluate%20Alignment%20Training%20in%0A%20%20Visual%20Language%20Models%0AAuthor%3A%20Gabriel%20Downer%20and%20Sean%20Craven%20and%20Damian%20Ruck%20and%20Jake%20Thomas%0AAbstract%3A%20%20%20The%20increasing%20integration%20of%20Visual%20Language%20Models%20%28VLMs%29%20into%20AI%20systems%0Anecessitates%20robust%20model%20alignment%2C%20especially%20when%20handling%20multimodal%0Acontent%20that%20combines%20text%20and%20images.%20Existing%20evaluation%20datasets%20heavily%0Alean%20towards%20text-only%20prompts%2C%20leaving%20visual%20vulnerabilities%20under%20evaluated.%0ATo%20address%20this%20gap%2C%20we%20propose%20%5Ctextbf%7BText2VLM%7D%2C%20a%20novel%20multi-stage%20pipeline%0Athat%20adapts%20text-only%20datasets%20into%20multimodal%20formats%2C%20specifically%20designed%0Ato%20evaluate%20the%20resilience%20of%20VLMs%20against%20typographic%20prompt%20injection%0Aattacks.%20The%20Text2VLM%20pipeline%20identifies%20harmful%20content%20in%20the%20original%20text%0Aand%20converts%20it%20into%20a%20typographic%20image%2C%20creating%20a%20multimodal%20prompt%20for%0AVLMs.%20Also%2C%20our%20evaluation%20of%20open-source%20VLMs%20highlights%20their%20increased%0Asusceptibility%20to%20prompt%20injection%20when%20visual%20inputs%20are%20introduced%2C%20revealing%0Acritical%20weaknesses%20in%20the%20current%20models%27%20alignment.%20This%20is%20in%20addition%20to%20a%0Asignificant%20performance%20gap%20compared%20to%20closed-source%20frontier%20models.%20We%0Avalidate%20Text2VLM%20through%20human%20evaluations%2C%20ensuring%20the%20alignment%20of%0Aextracted%20salient%20concepts%3B%20text%20summarization%20and%20output%20classification%20align%0Awith%20human%20expectations.%20Text2VLM%20provides%20a%20scalable%20tool%20for%20comprehensive%0Asafety%20assessment%2C%20contributing%20to%20the%20development%20of%20more%20robust%20safety%0Amechanisms%20for%20VLMs.%20By%20enhancing%20the%20evaluation%20of%20multimodal%20vulnerabilities%2C%0AText2VLM%20plays%20a%20role%20in%20advancing%20the%20safe%20deployment%20of%20VLMs%20in%20diverse%2C%0Areal-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2VLM%253A%2520Adapting%2520Text-Only%2520Datasets%2520to%2520Evaluate%2520Alignment%2520Training%2520in%250A%2520%2520Visual%2520Language%2520Models%26entry.906535625%3DGabriel%2520Downer%2520and%2520Sean%2520Craven%2520and%2520Damian%2520Ruck%2520and%2520Jake%2520Thomas%26entry.1292438233%3D%2520%2520The%2520increasing%2520integration%2520of%2520Visual%2520Language%2520Models%2520%2528VLMs%2529%2520into%2520AI%2520systems%250Anecessitates%2520robust%2520model%2520alignment%252C%2520especially%2520when%2520handling%2520multimodal%250Acontent%2520that%2520combines%2520text%2520and%2520images.%2520Existing%2520evaluation%2520datasets%2520heavily%250Alean%2520towards%2520text-only%2520prompts%252C%2520leaving%2520visual%2520vulnerabilities%2520under%2520evaluated.%250ATo%2520address%2520this%2520gap%252C%2520we%2520propose%2520%255Ctextbf%257BText2VLM%257D%252C%2520a%2520novel%2520multi-stage%2520pipeline%250Athat%2520adapts%2520text-only%2520datasets%2520into%2520multimodal%2520formats%252C%2520specifically%2520designed%250Ato%2520evaluate%2520the%2520resilience%2520of%2520VLMs%2520against%2520typographic%2520prompt%2520injection%250Aattacks.%2520The%2520Text2VLM%2520pipeline%2520identifies%2520harmful%2520content%2520in%2520the%2520original%2520text%250Aand%2520converts%2520it%2520into%2520a%2520typographic%2520image%252C%2520creating%2520a%2520multimodal%2520prompt%2520for%250AVLMs.%2520Also%252C%2520our%2520evaluation%2520of%2520open-source%2520VLMs%2520highlights%2520their%2520increased%250Asusceptibility%2520to%2520prompt%2520injection%2520when%2520visual%2520inputs%2520are%2520introduced%252C%2520revealing%250Acritical%2520weaknesses%2520in%2520the%2520current%2520models%2527%2520alignment.%2520This%2520is%2520in%2520addition%2520to%2520a%250Asignificant%2520performance%2520gap%2520compared%2520to%2520closed-source%2520frontier%2520models.%2520We%250Avalidate%2520Text2VLM%2520through%2520human%2520evaluations%252C%2520ensuring%2520the%2520alignment%2520of%250Aextracted%2520salient%2520concepts%253B%2520text%2520summarization%2520and%2520output%2520classification%2520align%250Awith%2520human%2520expectations.%2520Text2VLM%2520provides%2520a%2520scalable%2520tool%2520for%2520comprehensive%250Asafety%2520assessment%252C%2520contributing%2520to%2520the%2520development%2520of%2520more%2520robust%2520safety%250Amechanisms%2520for%2520VLMs.%2520By%2520enhancing%2520the%2520evaluation%2520of%2520multimodal%2520vulnerabilities%252C%250AText2VLM%2520plays%2520a%2520role%2520in%2520advancing%2520the%2520safe%2520deployment%2520of%2520VLMs%2520in%2520diverse%252C%250Areal-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2VLM%3A%20Adapting%20Text-Only%20Datasets%20to%20Evaluate%20Alignment%20Training%20in%0A%20%20Visual%20Language%20Models&entry.906535625=Gabriel%20Downer%20and%20Sean%20Craven%20and%20Damian%20Ruck%20and%20Jake%20Thomas&entry.1292438233=%20%20The%20increasing%20integration%20of%20Visual%20Language%20Models%20%28VLMs%29%20into%20AI%20systems%0Anecessitates%20robust%20model%20alignment%2C%20especially%20when%20handling%20multimodal%0Acontent%20that%20combines%20text%20and%20images.%20Existing%20evaluation%20datasets%20heavily%0Alean%20towards%20text-only%20prompts%2C%20leaving%20visual%20vulnerabilities%20under%20evaluated.%0ATo%20address%20this%20gap%2C%20we%20propose%20%5Ctextbf%7BText2VLM%7D%2C%20a%20novel%20multi-stage%20pipeline%0Athat%20adapts%20text-only%20datasets%20into%20multimodal%20formats%2C%20specifically%20designed%0Ato%20evaluate%20the%20resilience%20of%20VLMs%20against%20typographic%20prompt%20injection%0Aattacks.%20The%20Text2VLM%20pipeline%20identifies%20harmful%20content%20in%20the%20original%20text%0Aand%20converts%20it%20into%20a%20typographic%20image%2C%20creating%20a%20multimodal%20prompt%20for%0AVLMs.%20Also%2C%20our%20evaluation%20of%20open-source%20VLMs%20highlights%20their%20increased%0Asusceptibility%20to%20prompt%20injection%20when%20visual%20inputs%20are%20introduced%2C%20revealing%0Acritical%20weaknesses%20in%20the%20current%20models%27%20alignment.%20This%20is%20in%20addition%20to%20a%0Asignificant%20performance%20gap%20compared%20to%20closed-source%20frontier%20models.%20We%0Avalidate%20Text2VLM%20through%20human%20evaluations%2C%20ensuring%20the%20alignment%20of%0Aextracted%20salient%20concepts%3B%20text%20summarization%20and%20output%20classification%20align%0Awith%20human%20expectations.%20Text2VLM%20provides%20a%20scalable%20tool%20for%20comprehensive%0Asafety%20assessment%2C%20contributing%20to%20the%20development%20of%20more%20robust%20safety%0Amechanisms%20for%20VLMs.%20By%20enhancing%20the%20evaluation%20of%20multimodal%20vulnerabilities%2C%0AText2VLM%20plays%20a%20role%20in%20advancing%20the%20safe%20deployment%20of%20VLMs%20in%20diverse%2C%0Areal-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20704v1&entry.124074799=Read"},
{"title": "Finite-Time Analysis of Discrete-Time Stochastic Interpolants", "author": "Yuhao Liu and Yu Chen and Rui Hu and Longbo Huang", "abstract": "  The stochastic interpolant framework offers a powerful approach for\nconstructing generative models based on ordinary differential equations (ODEs)\nor stochastic differential equations (SDEs) to transform arbitrary data\ndistributions. However, prior analyses of this framework have primarily focused\non the continuous-time setting, assuming a perfect solution of the underlying\nequations. In this work, we present the first discrete-time analysis of the\nstochastic interpolant framework, where we introduce an innovative\ndiscrete-time sampler and derive a finite-time upper bound on its distribution\nestimation error. Our result provides a novel quantification of how different\nfactors, including the distance between source and target distributions and\nestimation accuracy, affect the convergence rate and also offers a new\nprincipled way to design efficient schedules for convergence acceleration.\nFinally, numerical experiments are conducted on the discrete-time sampler to\ncorroborate our theoretical findings.\n", "link": "http://arxiv.org/abs/2502.09130v2", "date": "2025-07-28", "relevancy": 1.3871, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4683}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4648}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite-Time%20Analysis%20of%20Discrete-Time%20Stochastic%20Interpolants&body=Title%3A%20Finite-Time%20Analysis%20of%20Discrete-Time%20Stochastic%20Interpolants%0AAuthor%3A%20Yuhao%20Liu%20and%20Yu%20Chen%20and%20Rui%20Hu%20and%20Longbo%20Huang%0AAbstract%3A%20%20%20The%20stochastic%20interpolant%20framework%20offers%20a%20powerful%20approach%20for%0Aconstructing%20generative%20models%20based%20on%20ordinary%20differential%20equations%20%28ODEs%29%0Aor%20stochastic%20differential%20equations%20%28SDEs%29%20to%20transform%20arbitrary%20data%0Adistributions.%20However%2C%20prior%20analyses%20of%20this%20framework%20have%20primarily%20focused%0Aon%20the%20continuous-time%20setting%2C%20assuming%20a%20perfect%20solution%20of%20the%20underlying%0Aequations.%20In%20this%20work%2C%20we%20present%20the%20first%20discrete-time%20analysis%20of%20the%0Astochastic%20interpolant%20framework%2C%20where%20we%20introduce%20an%20innovative%0Adiscrete-time%20sampler%20and%20derive%20a%20finite-time%20upper%20bound%20on%20its%20distribution%0Aestimation%20error.%20Our%20result%20provides%20a%20novel%20quantification%20of%20how%20different%0Afactors%2C%20including%20the%20distance%20between%20source%20and%20target%20distributions%20and%0Aestimation%20accuracy%2C%20affect%20the%20convergence%20rate%20and%20also%20offers%20a%20new%0Aprincipled%20way%20to%20design%20efficient%20schedules%20for%20convergence%20acceleration.%0AFinally%2C%20numerical%20experiments%20are%20conducted%20on%20the%20discrete-time%20sampler%20to%0Acorroborate%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.09130v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite-Time%2520Analysis%2520of%2520Discrete-Time%2520Stochastic%2520Interpolants%26entry.906535625%3DYuhao%2520Liu%2520and%2520Yu%2520Chen%2520and%2520Rui%2520Hu%2520and%2520Longbo%2520Huang%26entry.1292438233%3D%2520%2520The%2520stochastic%2520interpolant%2520framework%2520offers%2520a%2520powerful%2520approach%2520for%250Aconstructing%2520generative%2520models%2520based%2520on%2520ordinary%2520differential%2520equations%2520%2528ODEs%2529%250Aor%2520stochastic%2520differential%2520equations%2520%2528SDEs%2529%2520to%2520transform%2520arbitrary%2520data%250Adistributions.%2520However%252C%2520prior%2520analyses%2520of%2520this%2520framework%2520have%2520primarily%2520focused%250Aon%2520the%2520continuous-time%2520setting%252C%2520assuming%2520a%2520perfect%2520solution%2520of%2520the%2520underlying%250Aequations.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520discrete-time%2520analysis%2520of%2520the%250Astochastic%2520interpolant%2520framework%252C%2520where%2520we%2520introduce%2520an%2520innovative%250Adiscrete-time%2520sampler%2520and%2520derive%2520a%2520finite-time%2520upper%2520bound%2520on%2520its%2520distribution%250Aestimation%2520error.%2520Our%2520result%2520provides%2520a%2520novel%2520quantification%2520of%2520how%2520different%250Afactors%252C%2520including%2520the%2520distance%2520between%2520source%2520and%2520target%2520distributions%2520and%250Aestimation%2520accuracy%252C%2520affect%2520the%2520convergence%2520rate%2520and%2520also%2520offers%2520a%2520new%250Aprincipled%2520way%2520to%2520design%2520efficient%2520schedules%2520for%2520convergence%2520acceleration.%250AFinally%252C%2520numerical%2520experiments%2520are%2520conducted%2520on%2520the%2520discrete-time%2520sampler%2520to%250Acorroborate%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09130v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite-Time%20Analysis%20of%20Discrete-Time%20Stochastic%20Interpolants&entry.906535625=Yuhao%20Liu%20and%20Yu%20Chen%20and%20Rui%20Hu%20and%20Longbo%20Huang&entry.1292438233=%20%20The%20stochastic%20interpolant%20framework%20offers%20a%20powerful%20approach%20for%0Aconstructing%20generative%20models%20based%20on%20ordinary%20differential%20equations%20%28ODEs%29%0Aor%20stochastic%20differential%20equations%20%28SDEs%29%20to%20transform%20arbitrary%20data%0Adistributions.%20However%2C%20prior%20analyses%20of%20this%20framework%20have%20primarily%20focused%0Aon%20the%20continuous-time%20setting%2C%20assuming%20a%20perfect%20solution%20of%20the%20underlying%0Aequations.%20In%20this%20work%2C%20we%20present%20the%20first%20discrete-time%20analysis%20of%20the%0Astochastic%20interpolant%20framework%2C%20where%20we%20introduce%20an%20innovative%0Adiscrete-time%20sampler%20and%20derive%20a%20finite-time%20upper%20bound%20on%20its%20distribution%0Aestimation%20error.%20Our%20result%20provides%20a%20novel%20quantification%20of%20how%20different%0Afactors%2C%20including%20the%20distance%20between%20source%20and%20target%20distributions%20and%0Aestimation%20accuracy%2C%20affect%20the%20convergence%20rate%20and%20also%20offers%20a%20new%0Aprincipled%20way%20to%20design%20efficient%20schedules%20for%20convergence%20acceleration.%0AFinally%2C%20numerical%20experiments%20are%20conducted%20on%20the%20discrete-time%20sampler%20to%0Acorroborate%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.09130v2&entry.124074799=Read"},
{"title": "Partially Observable Monte-Carlo Graph Search", "author": "Yang You and Vincent Thomas and Alex Schutz and Robert Skilton and Nick Hawes and Olivier Buffet", "abstract": "  Currently, large partially observable Markov decision processes (POMDPs) are\noften solved by sampling-based online methods which interleave planning and\nexecution phases. However, a pre-computed offline policy is more desirable in\nPOMDP applications with time or energy constraints. But previous offline\nalgorithms are not able to scale up to large POMDPs. In this article, we\npropose a new sampling-based algorithm, the partially observable Monte-Carlo\ngraph search (POMCGS) to solve large POMDPs offline. Different from many online\nPOMDP methods, which progressively develop a tree while performing\n(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to\nconstruct a policy graph, so that computations can be drastically reduced, and\nusers can analyze and validate the policy prior to embedding and executing it.\nMoreover, POMCGS, together with action progressive widening and observation\nclustering methods provided in this article, is able to address certain\ncontinuous POMDPs. Through experiments, we demonstrate that POMCGS can generate\npolicies on the most challenging POMDPs, which cannot be computed by previous\noffline algorithms, and these policies' values are competitive compared with\nthe state-of-the-art online POMDP algorithms.\n", "link": "http://arxiv.org/abs/2507.20951v1", "date": "2025-07-28", "relevancy": 0.9698, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5148}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4842}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Partially%20Observable%20Monte-Carlo%20Graph%20Search&body=Title%3A%20Partially%20Observable%20Monte-Carlo%20Graph%20Search%0AAuthor%3A%20Yang%20You%20and%20Vincent%20Thomas%20and%20Alex%20Schutz%20and%20Robert%20Skilton%20and%20Nick%20Hawes%20and%20Olivier%20Buffet%0AAbstract%3A%20%20%20Currently%2C%20large%20partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20are%0Aoften%20solved%20by%20sampling-based%20online%20methods%20which%20interleave%20planning%20and%0Aexecution%20phases.%20However%2C%20a%20pre-computed%20offline%20policy%20is%20more%20desirable%20in%0APOMDP%20applications%20with%20time%20or%20energy%20constraints.%20But%20previous%20offline%0Aalgorithms%20are%20not%20able%20to%20scale%20up%20to%20large%20POMDPs.%20In%20this%20article%2C%20we%0Apropose%20a%20new%20sampling-based%20algorithm%2C%20the%20partially%20observable%20Monte-Carlo%0Agraph%20search%20%28POMCGS%29%20to%20solve%20large%20POMDPs%20offline.%20Different%20from%20many%20online%0APOMDP%20methods%2C%20which%20progressively%20develop%20a%20tree%20while%20performing%0A%28Monte-Carlo%29%20simulations%2C%20POMCGS%20folds%20this%20search%20tree%20on%20the%20fly%20to%0Aconstruct%20a%20policy%20graph%2C%20so%20that%20computations%20can%20be%20drastically%20reduced%2C%20and%0Ausers%20can%20analyze%20and%20validate%20the%20policy%20prior%20to%20embedding%20and%20executing%20it.%0AMoreover%2C%20POMCGS%2C%20together%20with%20action%20progressive%20widening%20and%20observation%0Aclustering%20methods%20provided%20in%20this%20article%2C%20is%20able%20to%20address%20certain%0Acontinuous%20POMDPs.%20Through%20experiments%2C%20we%20demonstrate%20that%20POMCGS%20can%20generate%0Apolicies%20on%20the%20most%20challenging%20POMDPs%2C%20which%20cannot%20be%20computed%20by%20previous%0Aoffline%20algorithms%2C%20and%20these%20policies%27%20values%20are%20competitive%20compared%20with%0Athe%20state-of-the-art%20online%20POMDP%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartially%2520Observable%2520Monte-Carlo%2520Graph%2520Search%26entry.906535625%3DYang%2520You%2520and%2520Vincent%2520Thomas%2520and%2520Alex%2520Schutz%2520and%2520Robert%2520Skilton%2520and%2520Nick%2520Hawes%2520and%2520Olivier%2520Buffet%26entry.1292438233%3D%2520%2520Currently%252C%2520large%2520partially%2520observable%2520Markov%2520decision%2520processes%2520%2528POMDPs%2529%2520are%250Aoften%2520solved%2520by%2520sampling-based%2520online%2520methods%2520which%2520interleave%2520planning%2520and%250Aexecution%2520phases.%2520However%252C%2520a%2520pre-computed%2520offline%2520policy%2520is%2520more%2520desirable%2520in%250APOMDP%2520applications%2520with%2520time%2520or%2520energy%2520constraints.%2520But%2520previous%2520offline%250Aalgorithms%2520are%2520not%2520able%2520to%2520scale%2520up%2520to%2520large%2520POMDPs.%2520In%2520this%2520article%252C%2520we%250Apropose%2520a%2520new%2520sampling-based%2520algorithm%252C%2520the%2520partially%2520observable%2520Monte-Carlo%250Agraph%2520search%2520%2528POMCGS%2529%2520to%2520solve%2520large%2520POMDPs%2520offline.%2520Different%2520from%2520many%2520online%250APOMDP%2520methods%252C%2520which%2520progressively%2520develop%2520a%2520tree%2520while%2520performing%250A%2528Monte-Carlo%2529%2520simulations%252C%2520POMCGS%2520folds%2520this%2520search%2520tree%2520on%2520the%2520fly%2520to%250Aconstruct%2520a%2520policy%2520graph%252C%2520so%2520that%2520computations%2520can%2520be%2520drastically%2520reduced%252C%2520and%250Ausers%2520can%2520analyze%2520and%2520validate%2520the%2520policy%2520prior%2520to%2520embedding%2520and%2520executing%2520it.%250AMoreover%252C%2520POMCGS%252C%2520together%2520with%2520action%2520progressive%2520widening%2520and%2520observation%250Aclustering%2520methods%2520provided%2520in%2520this%2520article%252C%2520is%2520able%2520to%2520address%2520certain%250Acontinuous%2520POMDPs.%2520Through%2520experiments%252C%2520we%2520demonstrate%2520that%2520POMCGS%2520can%2520generate%250Apolicies%2520on%2520the%2520most%2520challenging%2520POMDPs%252C%2520which%2520cannot%2520be%2520computed%2520by%2520previous%250Aoffline%2520algorithms%252C%2520and%2520these%2520policies%2527%2520values%2520are%2520competitive%2520compared%2520with%250Athe%2520state-of-the-art%2520online%2520POMDP%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Partially%20Observable%20Monte-Carlo%20Graph%20Search&entry.906535625=Yang%20You%20and%20Vincent%20Thomas%20and%20Alex%20Schutz%20and%20Robert%20Skilton%20and%20Nick%20Hawes%20and%20Olivier%20Buffet&entry.1292438233=%20%20Currently%2C%20large%20partially%20observable%20Markov%20decision%20processes%20%28POMDPs%29%20are%0Aoften%20solved%20by%20sampling-based%20online%20methods%20which%20interleave%20planning%20and%0Aexecution%20phases.%20However%2C%20a%20pre-computed%20offline%20policy%20is%20more%20desirable%20in%0APOMDP%20applications%20with%20time%20or%20energy%20constraints.%20But%20previous%20offline%0Aalgorithms%20are%20not%20able%20to%20scale%20up%20to%20large%20POMDPs.%20In%20this%20article%2C%20we%0Apropose%20a%20new%20sampling-based%20algorithm%2C%20the%20partially%20observable%20Monte-Carlo%0Agraph%20search%20%28POMCGS%29%20to%20solve%20large%20POMDPs%20offline.%20Different%20from%20many%20online%0APOMDP%20methods%2C%20which%20progressively%20develop%20a%20tree%20while%20performing%0A%28Monte-Carlo%29%20simulations%2C%20POMCGS%20folds%20this%20search%20tree%20on%20the%20fly%20to%0Aconstruct%20a%20policy%20graph%2C%20so%20that%20computations%20can%20be%20drastically%20reduced%2C%20and%0Ausers%20can%20analyze%20and%20validate%20the%20policy%20prior%20to%20embedding%20and%20executing%20it.%0AMoreover%2C%20POMCGS%2C%20together%20with%20action%20progressive%20widening%20and%20observation%0Aclustering%20methods%20provided%20in%20this%20article%2C%20is%20able%20to%20address%20certain%0Acontinuous%20POMDPs.%20Through%20experiments%2C%20we%20demonstrate%20that%20POMCGS%20can%20generate%0Apolicies%20on%20the%20most%20challenging%20POMDPs%2C%20which%20cannot%20be%20computed%20by%20previous%0Aoffline%20algorithms%2C%20and%20these%20policies%27%20values%20are%20competitive%20compared%20with%0Athe%20state-of-the-art%20online%20POMDP%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20951v1&entry.124074799=Read"},
{"title": "On the Robustness of Global Feature Effect Explanations", "author": "Hubert Baniecki and Giuseppe Casalicchio and Bernd Bischl and Przemyslaw Biecek", "abstract": "  We study the robustness of global post-hoc explanations for predictive models\ntrained on tabular data. Effects of predictor features in black-box supervised\nlearning are an essential diagnostic tool for model debugging and scientific\ndiscovery in applied sciences. However, how vulnerable they are to data and\nmodel perturbations remains an open research question. We introduce several\ntheoretical bounds for evaluating the robustness of partial dependence plots\nand accumulated local effects. Our experimental results with synthetic and\nreal-world datasets quantify the gap between the best and worst-case scenarios\nof (mis)interpreting machine learning predictions globally.\n", "link": "http://arxiv.org/abs/2406.09069v2", "date": "2025-07-28", "relevancy": 1.7417, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Global%20Feature%20Effect%20Explanations&body=Title%3A%20On%20the%20Robustness%20of%20Global%20Feature%20Effect%20Explanations%0AAuthor%3A%20Hubert%20Baniecki%20and%20Giuseppe%20Casalicchio%20and%20Bernd%20Bischl%20and%20Przemyslaw%20Biecek%0AAbstract%3A%20%20%20We%20study%20the%20robustness%20of%20global%20post-hoc%20explanations%20for%20predictive%20models%0Atrained%20on%20tabular%20data.%20Effects%20of%20predictor%20features%20in%20black-box%20supervised%0Alearning%20are%20an%20essential%20diagnostic%20tool%20for%20model%20debugging%20and%20scientific%0Adiscovery%20in%20applied%20sciences.%20However%2C%20how%20vulnerable%20they%20are%20to%20data%20and%0Amodel%20perturbations%20remains%20an%20open%20research%20question.%20We%20introduce%20several%0Atheoretical%20bounds%20for%20evaluating%20the%20robustness%20of%20partial%20dependence%20plots%0Aand%20accumulated%20local%20effects.%20Our%20experimental%20results%20with%20synthetic%20and%0Areal-world%20datasets%20quantify%20the%20gap%20between%20the%20best%20and%20worst-case%20scenarios%0Aof%20%28mis%29interpreting%20machine%20learning%20predictions%20globally.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520of%2520Global%2520Feature%2520Effect%2520Explanations%26entry.906535625%3DHubert%2520Baniecki%2520and%2520Giuseppe%2520Casalicchio%2520and%2520Bernd%2520Bischl%2520and%2520Przemyslaw%2520Biecek%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520robustness%2520of%2520global%2520post-hoc%2520explanations%2520for%2520predictive%2520models%250Atrained%2520on%2520tabular%2520data.%2520Effects%2520of%2520predictor%2520features%2520in%2520black-box%2520supervised%250Alearning%2520are%2520an%2520essential%2520diagnostic%2520tool%2520for%2520model%2520debugging%2520and%2520scientific%250Adiscovery%2520in%2520applied%2520sciences.%2520However%252C%2520how%2520vulnerable%2520they%2520are%2520to%2520data%2520and%250Amodel%2520perturbations%2520remains%2520an%2520open%2520research%2520question.%2520We%2520introduce%2520several%250Atheoretical%2520bounds%2520for%2520evaluating%2520the%2520robustness%2520of%2520partial%2520dependence%2520plots%250Aand%2520accumulated%2520local%2520effects.%2520Our%2520experimental%2520results%2520with%2520synthetic%2520and%250Areal-world%2520datasets%2520quantify%2520the%2520gap%2520between%2520the%2520best%2520and%2520worst-case%2520scenarios%250Aof%2520%2528mis%2529interpreting%2520machine%2520learning%2520predictions%2520globally.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Global%20Feature%20Effect%20Explanations&entry.906535625=Hubert%20Baniecki%20and%20Giuseppe%20Casalicchio%20and%20Bernd%20Bischl%20and%20Przemyslaw%20Biecek&entry.1292438233=%20%20We%20study%20the%20robustness%20of%20global%20post-hoc%20explanations%20for%20predictive%20models%0Atrained%20on%20tabular%20data.%20Effects%20of%20predictor%20features%20in%20black-box%20supervised%0Alearning%20are%20an%20essential%20diagnostic%20tool%20for%20model%20debugging%20and%20scientific%0Adiscovery%20in%20applied%20sciences.%20However%2C%20how%20vulnerable%20they%20are%20to%20data%20and%0Amodel%20perturbations%20remains%20an%20open%20research%20question.%20We%20introduce%20several%0Atheoretical%20bounds%20for%20evaluating%20the%20robustness%20of%20partial%20dependence%20plots%0Aand%20accumulated%20local%20effects.%20Our%20experimental%20results%20with%20synthetic%20and%0Areal-world%20datasets%20quantify%20the%20gap%20between%20the%20best%20and%20worst-case%20scenarios%0Aof%20%28mis%29interpreting%20machine%20learning%20predictions%20globally.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09069v2&entry.124074799=Read"},
{"title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract\n  Generated Comments", "author": "Fatou Ndiaye Mbodji", "abstract": "  Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods.\n", "link": "http://arxiv.org/abs/2507.20774v1", "date": "2025-07-28", "relevancy": 1.1923, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4187}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3922}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20evalSmarT%3A%20An%20LLM-Based%20Framework%20for%20Evaluating%20Smart%20Contract%0A%20%20Generated%20Comments&body=Title%3A%20evalSmarT%3A%20An%20LLM-Based%20Framework%20for%20Evaluating%20Smart%20Contract%0A%20%20Generated%20Comments%0AAuthor%3A%20Fatou%20Ndiaye%20Mbodji%0AAbstract%3A%20%20%20Smart%20contract%20comment%20generation%20has%20gained%20traction%20as%20a%20means%20to%20improve%0Acode%20comprehension%20and%20maintainability%20in%20blockchain%20systems.%20However%2C%0Aevaluating%20the%20quality%20of%20generated%20comments%20remains%20a%20challenge.%20Traditional%0Ametrics%20such%20as%20BLEU%20and%20ROUGE%20fail%20to%20capture%20domain-specific%20nuances%2C%20while%0Ahuman%20evaluation%20is%20costly%20and%20unscalable.%20In%20this%20paper%2C%20we%20present%0A%5Ctexttt%7BevalSmarT%7D%2C%20a%20modular%20and%20extensible%20framework%20that%20leverages%20large%0Alanguage%20models%20%28LLMs%29%20as%20evaluators.%20The%20system%20supports%20over%20400%20evaluator%0Aconfigurations%20by%20combining%20approximately%2040%20LLMs%20with%2010%20prompting%20strategies.%0AWe%20demonstrate%20its%20application%20in%20benchmarking%20comment%20generation%20tools%20and%0Aselecting%20the%20most%20informative%20outputs.%20Our%20results%20show%20that%20prompt%20design%0Asignificantly%20impacts%20alignment%20with%20human%20judgment%2C%20and%20that%20LLM-based%0Aevaluation%20offers%20a%20scalable%20and%20semantically%20rich%20alternative%20to%20existing%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DevalSmarT%253A%2520An%2520LLM-Based%2520Framework%2520for%2520Evaluating%2520Smart%2520Contract%250A%2520%2520Generated%2520Comments%26entry.906535625%3DFatou%2520Ndiaye%2520Mbodji%26entry.1292438233%3D%2520%2520Smart%2520contract%2520comment%2520generation%2520has%2520gained%2520traction%2520as%2520a%2520means%2520to%2520improve%250Acode%2520comprehension%2520and%2520maintainability%2520in%2520blockchain%2520systems.%2520However%252C%250Aevaluating%2520the%2520quality%2520of%2520generated%2520comments%2520remains%2520a%2520challenge.%2520Traditional%250Ametrics%2520such%2520as%2520BLEU%2520and%2520ROUGE%2520fail%2520to%2520capture%2520domain-specific%2520nuances%252C%2520while%250Ahuman%2520evaluation%2520is%2520costly%2520and%2520unscalable.%2520In%2520this%2520paper%252C%2520we%2520present%250A%255Ctexttt%257BevalSmarT%257D%252C%2520a%2520modular%2520and%2520extensible%2520framework%2520that%2520leverages%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520as%2520evaluators.%2520The%2520system%2520supports%2520over%2520400%2520evaluator%250Aconfigurations%2520by%2520combining%2520approximately%252040%2520LLMs%2520with%252010%2520prompting%2520strategies.%250AWe%2520demonstrate%2520its%2520application%2520in%2520benchmarking%2520comment%2520generation%2520tools%2520and%250Aselecting%2520the%2520most%2520informative%2520outputs.%2520Our%2520results%2520show%2520that%2520prompt%2520design%250Asignificantly%2520impacts%2520alignment%2520with%2520human%2520judgment%252C%2520and%2520that%2520LLM-based%250Aevaluation%2520offers%2520a%2520scalable%2520and%2520semantically%2520rich%2520alternative%2520to%2520existing%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=evalSmarT%3A%20An%20LLM-Based%20Framework%20for%20Evaluating%20Smart%20Contract%0A%20%20Generated%20Comments&entry.906535625=Fatou%20Ndiaye%20Mbodji&entry.1292438233=%20%20Smart%20contract%20comment%20generation%20has%20gained%20traction%20as%20a%20means%20to%20improve%0Acode%20comprehension%20and%20maintainability%20in%20blockchain%20systems.%20However%2C%0Aevaluating%20the%20quality%20of%20generated%20comments%20remains%20a%20challenge.%20Traditional%0Ametrics%20such%20as%20BLEU%20and%20ROUGE%20fail%20to%20capture%20domain-specific%20nuances%2C%20while%0Ahuman%20evaluation%20is%20costly%20and%20unscalable.%20In%20this%20paper%2C%20we%20present%0A%5Ctexttt%7BevalSmarT%7D%2C%20a%20modular%20and%20extensible%20framework%20that%20leverages%20large%0Alanguage%20models%20%28LLMs%29%20as%20evaluators.%20The%20system%20supports%20over%20400%20evaluator%0Aconfigurations%20by%20combining%20approximately%2040%20LLMs%20with%2010%20prompting%20strategies.%0AWe%20demonstrate%20its%20application%20in%20benchmarking%20comment%20generation%20tools%20and%0Aselecting%20the%20most%20informative%20outputs.%20Our%20results%20show%20that%20prompt%20design%0Asignificantly%20impacts%20alignment%20with%20human%20judgment%2C%20and%20that%20LLM-based%0Aevaluation%20offers%20a%20scalable%20and%20semantically%20rich%20alternative%20to%20existing%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20774v1&entry.124074799=Read"},
{"title": "FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with\n  Accurate Quantization", "author": "Aotao Wang and Haikuo Shao and Shaobo Ma and Zhongfeng Wang", "abstract": "  State Space Models (SSMs), like recent Mamba2, have achieved remarkable\nperformance and received extensive attention. However, deploying Mamba2 on\nresource-constrained edge devices encounters many problems: severe outliers\nwithin the linear layer challenging the quantization, diverse and irregular\nelement-wise tensor operations, and hardware-unfriendly nonlinear functions in\nthe SSM block. To address these issues, this paper presents FastMamba, a\ndedicated accelerator on FPGA with hardware-algorithm co-design to promote the\ndeployment efficiency of Mamba2. Specifically, we successfully achieve 8-bit\nquantization for linear layers through Hadamard transformation to eliminate\noutliers. Moreover, a hardware-friendly and fine-grained power-of-two\nquantization framework is presented for the SSM block and convolution layer,\nand a first-order linear approximation is developed to optimize the nonlinear\nfunctions. Based on the accurate algorithm quantization, we propose an\naccelerator that integrates parallel vector processing units, pipelined\nexecution dataflow, and an efficient SSM Nonlinear Approximation Unit, which\nenhances computational efficiency and reduces hardware complexity. Finally, we\nevaluate FastMamba on Xilinx VC709 FPGA. For the input prefill task on\nMamba2-130M, FastMamba achieves 68.80\\times and 8.90\\times speedup over Intel\nXeon 4210R CPU and NVIDIA RTX 3090 GPU, respectively. In the output decode\nexperiment with Mamba2-2.7B, FastMamba attains 6\\times higher energy efficiency\nthan RTX 3090 GPU.\n", "link": "http://arxiv.org/abs/2505.18975v4", "date": "2025-07-28", "relevancy": 1.8017, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4976}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4493}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastMamba%3A%20A%20High-Speed%20and%20Efficient%20Mamba%20Accelerator%20on%20FPGA%20with%0A%20%20Accurate%20Quantization&body=Title%3A%20FastMamba%3A%20A%20High-Speed%20and%20Efficient%20Mamba%20Accelerator%20on%20FPGA%20with%0A%20%20Accurate%20Quantization%0AAuthor%3A%20Aotao%20Wang%20and%20Haikuo%20Shao%20and%20Shaobo%20Ma%20and%20Zhongfeng%20Wang%0AAbstract%3A%20%20%20State%20Space%20Models%20%28SSMs%29%2C%20like%20recent%20Mamba2%2C%20have%20achieved%20remarkable%0Aperformance%20and%20received%20extensive%20attention.%20However%2C%20deploying%20Mamba2%20on%0Aresource-constrained%20edge%20devices%20encounters%20many%20problems%3A%20severe%20outliers%0Awithin%20the%20linear%20layer%20challenging%20the%20quantization%2C%20diverse%20and%20irregular%0Aelement-wise%20tensor%20operations%2C%20and%20hardware-unfriendly%20nonlinear%20functions%20in%0Athe%20SSM%20block.%20To%20address%20these%20issues%2C%20this%20paper%20presents%20FastMamba%2C%20a%0Adedicated%20accelerator%20on%20FPGA%20with%20hardware-algorithm%20co-design%20to%20promote%20the%0Adeployment%20efficiency%20of%20Mamba2.%20Specifically%2C%20we%20successfully%20achieve%208-bit%0Aquantization%20for%20linear%20layers%20through%20Hadamard%20transformation%20to%20eliminate%0Aoutliers.%20Moreover%2C%20a%20hardware-friendly%20and%20fine-grained%20power-of-two%0Aquantization%20framework%20is%20presented%20for%20the%20SSM%20block%20and%20convolution%20layer%2C%0Aand%20a%20first-order%20linear%20approximation%20is%20developed%20to%20optimize%20the%20nonlinear%0Afunctions.%20Based%20on%20the%20accurate%20algorithm%20quantization%2C%20we%20propose%20an%0Aaccelerator%20that%20integrates%20parallel%20vector%20processing%20units%2C%20pipelined%0Aexecution%20dataflow%2C%20and%20an%20efficient%20SSM%20Nonlinear%20Approximation%20Unit%2C%20which%0Aenhances%20computational%20efficiency%20and%20reduces%20hardware%20complexity.%20Finally%2C%20we%0Aevaluate%20FastMamba%20on%20Xilinx%20VC709%20FPGA.%20For%20the%20input%20prefill%20task%20on%0AMamba2-130M%2C%20FastMamba%20achieves%2068.80%5Ctimes%20and%208.90%5Ctimes%20speedup%20over%20Intel%0AXeon%204210R%20CPU%20and%20NVIDIA%20RTX%203090%20GPU%2C%20respectively.%20In%20the%20output%20decode%0Aexperiment%20with%20Mamba2-2.7B%2C%20FastMamba%20attains%206%5Ctimes%20higher%20energy%20efficiency%0Athan%20RTX%203090%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18975v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastMamba%253A%2520A%2520High-Speed%2520and%2520Efficient%2520Mamba%2520Accelerator%2520on%2520FPGA%2520with%250A%2520%2520Accurate%2520Quantization%26entry.906535625%3DAotao%2520Wang%2520and%2520Haikuo%2520Shao%2520and%2520Shaobo%2520Ma%2520and%2520Zhongfeng%2520Wang%26entry.1292438233%3D%2520%2520State%2520Space%2520Models%2520%2528SSMs%2529%252C%2520like%2520recent%2520Mamba2%252C%2520have%2520achieved%2520remarkable%250Aperformance%2520and%2520received%2520extensive%2520attention.%2520However%252C%2520deploying%2520Mamba2%2520on%250Aresource-constrained%2520edge%2520devices%2520encounters%2520many%2520problems%253A%2520severe%2520outliers%250Awithin%2520the%2520linear%2520layer%2520challenging%2520the%2520quantization%252C%2520diverse%2520and%2520irregular%250Aelement-wise%2520tensor%2520operations%252C%2520and%2520hardware-unfriendly%2520nonlinear%2520functions%2520in%250Athe%2520SSM%2520block.%2520To%2520address%2520these%2520issues%252C%2520this%2520paper%2520presents%2520FastMamba%252C%2520a%250Adedicated%2520accelerator%2520on%2520FPGA%2520with%2520hardware-algorithm%2520co-design%2520to%2520promote%2520the%250Adeployment%2520efficiency%2520of%2520Mamba2.%2520Specifically%252C%2520we%2520successfully%2520achieve%25208-bit%250Aquantization%2520for%2520linear%2520layers%2520through%2520Hadamard%2520transformation%2520to%2520eliminate%250Aoutliers.%2520Moreover%252C%2520a%2520hardware-friendly%2520and%2520fine-grained%2520power-of-two%250Aquantization%2520framework%2520is%2520presented%2520for%2520the%2520SSM%2520block%2520and%2520convolution%2520layer%252C%250Aand%2520a%2520first-order%2520linear%2520approximation%2520is%2520developed%2520to%2520optimize%2520the%2520nonlinear%250Afunctions.%2520Based%2520on%2520the%2520accurate%2520algorithm%2520quantization%252C%2520we%2520propose%2520an%250Aaccelerator%2520that%2520integrates%2520parallel%2520vector%2520processing%2520units%252C%2520pipelined%250Aexecution%2520dataflow%252C%2520and%2520an%2520efficient%2520SSM%2520Nonlinear%2520Approximation%2520Unit%252C%2520which%250Aenhances%2520computational%2520efficiency%2520and%2520reduces%2520hardware%2520complexity.%2520Finally%252C%2520we%250Aevaluate%2520FastMamba%2520on%2520Xilinx%2520VC709%2520FPGA.%2520For%2520the%2520input%2520prefill%2520task%2520on%250AMamba2-130M%252C%2520FastMamba%2520achieves%252068.80%255Ctimes%2520and%25208.90%255Ctimes%2520speedup%2520over%2520Intel%250AXeon%25204210R%2520CPU%2520and%2520NVIDIA%2520RTX%25203090%2520GPU%252C%2520respectively.%2520In%2520the%2520output%2520decode%250Aexperiment%2520with%2520Mamba2-2.7B%252C%2520FastMamba%2520attains%25206%255Ctimes%2520higher%2520energy%2520efficiency%250Athan%2520RTX%25203090%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18975v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastMamba%3A%20A%20High-Speed%20and%20Efficient%20Mamba%20Accelerator%20on%20FPGA%20with%0A%20%20Accurate%20Quantization&entry.906535625=Aotao%20Wang%20and%20Haikuo%20Shao%20and%20Shaobo%20Ma%20and%20Zhongfeng%20Wang&entry.1292438233=%20%20State%20Space%20Models%20%28SSMs%29%2C%20like%20recent%20Mamba2%2C%20have%20achieved%20remarkable%0Aperformance%20and%20received%20extensive%20attention.%20However%2C%20deploying%20Mamba2%20on%0Aresource-constrained%20edge%20devices%20encounters%20many%20problems%3A%20severe%20outliers%0Awithin%20the%20linear%20layer%20challenging%20the%20quantization%2C%20diverse%20and%20irregular%0Aelement-wise%20tensor%20operations%2C%20and%20hardware-unfriendly%20nonlinear%20functions%20in%0Athe%20SSM%20block.%20To%20address%20these%20issues%2C%20this%20paper%20presents%20FastMamba%2C%20a%0Adedicated%20accelerator%20on%20FPGA%20with%20hardware-algorithm%20co-design%20to%20promote%20the%0Adeployment%20efficiency%20of%20Mamba2.%20Specifically%2C%20we%20successfully%20achieve%208-bit%0Aquantization%20for%20linear%20layers%20through%20Hadamard%20transformation%20to%20eliminate%0Aoutliers.%20Moreover%2C%20a%20hardware-friendly%20and%20fine-grained%20power-of-two%0Aquantization%20framework%20is%20presented%20for%20the%20SSM%20block%20and%20convolution%20layer%2C%0Aand%20a%20first-order%20linear%20approximation%20is%20developed%20to%20optimize%20the%20nonlinear%0Afunctions.%20Based%20on%20the%20accurate%20algorithm%20quantization%2C%20we%20propose%20an%0Aaccelerator%20that%20integrates%20parallel%20vector%20processing%20units%2C%20pipelined%0Aexecution%20dataflow%2C%20and%20an%20efficient%20SSM%20Nonlinear%20Approximation%20Unit%2C%20which%0Aenhances%20computational%20efficiency%20and%20reduces%20hardware%20complexity.%20Finally%2C%20we%0Aevaluate%20FastMamba%20on%20Xilinx%20VC709%20FPGA.%20For%20the%20input%20prefill%20task%20on%0AMamba2-130M%2C%20FastMamba%20achieves%2068.80%5Ctimes%20and%208.90%5Ctimes%20speedup%20over%20Intel%0AXeon%204210R%20CPU%20and%20NVIDIA%20RTX%203090%20GPU%2C%20respectively.%20In%20the%20output%20decode%0Aexperiment%20with%20Mamba2-2.7B%2C%20FastMamba%20attains%206%5Ctimes%20higher%20energy%20efficiency%0Athan%20RTX%203090%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18975v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


