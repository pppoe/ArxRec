<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250806.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline", "author": "Linqing Zhao and Xiuwei Xu and Yirui Wang and Hao Wang and Wenzhao Zheng and Yansong Tang and Haibin Yan and Jiwen Lu", "abstract": "  Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.\n", "link": "http://arxiv.org/abs/2508.04597v1", "date": "2025-08-06", "relevancy": 3.5078, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.813}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6505}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo%20Depth%20Meets%20Gaussian%3A%20A%20Feed-forward%20RGB%20SLAM%20Baseline&body=Title%3A%20Pseudo%20Depth%20Meets%20Gaussian%3A%20A%20Feed-forward%20RGB%20SLAM%20Baseline%0AAuthor%3A%20Linqing%20Zhao%20and%20Xiuwei%20Xu%20and%20Yirui%20Wang%20and%20Hao%20Wang%20and%20Wenzhao%20Zheng%20and%20Yansong%20Tang%20and%20Haibin%20Yan%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Incrementally%20recovering%20real-sized%203D%20geometry%20from%20a%20pose-free%20RGB%20stream%0Ais%20a%20challenging%20task%20in%203D%20reconstruction%2C%20requiring%20minimal%20assumptions%20on%0Ainput%20data.%20Existing%20methods%20can%20be%20broadly%20categorized%20into%20end-to-end%20and%0Avisual%20SLAM-based%20approaches%2C%20both%20of%20which%20either%20struggle%20with%20long%20sequences%0Aor%20depend%20on%20slow%20test-time%20optimization%20and%20depth%20sensors.%20To%20address%20this%2C%20we%0Afirst%20integrate%20a%20depth%20estimator%20into%20an%20RGB-D%20SLAM%20system%2C%20but%20this%20approach%0Ais%20hindered%20by%20inaccurate%20geometric%20details%20in%20predicted%20depth.%20Through%20further%0Ainvestigation%2C%20we%20find%20that%203D%20Gaussian%20mapping%20can%20effectively%20solve%20this%0Aproblem.%20Building%20on%20this%2C%20we%20propose%20an%20online%203D%20reconstruction%20method%20using%0A3D%20Gaussian-based%20SLAM%2C%20combined%20with%20a%20feed-forward%20recurrent%20prediction%0Amodule%20to%20directly%20infer%20camera%20pose%20from%20optical%20flow.%20This%20approach%20replaces%0Aslow%20test-time%20optimization%20with%20fast%20network%20inference%2C%20significantly%0Aimproving%20tracking%20speed.%20Additionally%2C%20we%20introduce%20a%20local%20graph%20rendering%0Atechnique%20to%20enhance%20robustness%20in%20feed-forward%20pose%20prediction.%20Experimental%0Aresults%20on%20the%20Replica%20and%20TUM-RGBD%20datasets%2C%20along%20with%20a%20real-world%0Adeployment%20demonstration%2C%20show%20that%20our%20method%20achieves%20performance%20on%20par%20with%0Athe%20state-of-the-art%20SplaTAM%2C%20while%20reducing%20tracking%20time%20by%20more%20than%2090%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo%2520Depth%2520Meets%2520Gaussian%253A%2520A%2520Feed-forward%2520RGB%2520SLAM%2520Baseline%26entry.906535625%3DLinqing%2520Zhao%2520and%2520Xiuwei%2520Xu%2520and%2520Yirui%2520Wang%2520and%2520Hao%2520Wang%2520and%2520Wenzhao%2520Zheng%2520and%2520Yansong%2520Tang%2520and%2520Haibin%2520Yan%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Incrementally%2520recovering%2520real-sized%25203D%2520geometry%2520from%2520a%2520pose-free%2520RGB%2520stream%250Ais%2520a%2520challenging%2520task%2520in%25203D%2520reconstruction%252C%2520requiring%2520minimal%2520assumptions%2520on%250Ainput%2520data.%2520Existing%2520methods%2520can%2520be%2520broadly%2520categorized%2520into%2520end-to-end%2520and%250Avisual%2520SLAM-based%2520approaches%252C%2520both%2520of%2520which%2520either%2520struggle%2520with%2520long%2520sequences%250Aor%2520depend%2520on%2520slow%2520test-time%2520optimization%2520and%2520depth%2520sensors.%2520To%2520address%2520this%252C%2520we%250Afirst%2520integrate%2520a%2520depth%2520estimator%2520into%2520an%2520RGB-D%2520SLAM%2520system%252C%2520but%2520this%2520approach%250Ais%2520hindered%2520by%2520inaccurate%2520geometric%2520details%2520in%2520predicted%2520depth.%2520Through%2520further%250Ainvestigation%252C%2520we%2520find%2520that%25203D%2520Gaussian%2520mapping%2520can%2520effectively%2520solve%2520this%250Aproblem.%2520Building%2520on%2520this%252C%2520we%2520propose%2520an%2520online%25203D%2520reconstruction%2520method%2520using%250A3D%2520Gaussian-based%2520SLAM%252C%2520combined%2520with%2520a%2520feed-forward%2520recurrent%2520prediction%250Amodule%2520to%2520directly%2520infer%2520camera%2520pose%2520from%2520optical%2520flow.%2520This%2520approach%2520replaces%250Aslow%2520test-time%2520optimization%2520with%2520fast%2520network%2520inference%252C%2520significantly%250Aimproving%2520tracking%2520speed.%2520Additionally%252C%2520we%2520introduce%2520a%2520local%2520graph%2520rendering%250Atechnique%2520to%2520enhance%2520robustness%2520in%2520feed-forward%2520pose%2520prediction.%2520Experimental%250Aresults%2520on%2520the%2520Replica%2520and%2520TUM-RGBD%2520datasets%252C%2520along%2520with%2520a%2520real-world%250Adeployment%2520demonstration%252C%2520show%2520that%2520our%2520method%2520achieves%2520performance%2520on%2520par%2520with%250Athe%2520state-of-the-art%2520SplaTAM%252C%2520while%2520reducing%2520tracking%2520time%2520by%2520more%2520than%252090%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo%20Depth%20Meets%20Gaussian%3A%20A%20Feed-forward%20RGB%20SLAM%20Baseline&entry.906535625=Linqing%20Zhao%20and%20Xiuwei%20Xu%20and%20Yirui%20Wang%20and%20Hao%20Wang%20and%20Wenzhao%20Zheng%20and%20Yansong%20Tang%20and%20Haibin%20Yan%20and%20Jiwen%20Lu&entry.1292438233=%20%20Incrementally%20recovering%20real-sized%203D%20geometry%20from%20a%20pose-free%20RGB%20stream%0Ais%20a%20challenging%20task%20in%203D%20reconstruction%2C%20requiring%20minimal%20assumptions%20on%0Ainput%20data.%20Existing%20methods%20can%20be%20broadly%20categorized%20into%20end-to-end%20and%0Avisual%20SLAM-based%20approaches%2C%20both%20of%20which%20either%20struggle%20with%20long%20sequences%0Aor%20depend%20on%20slow%20test-time%20optimization%20and%20depth%20sensors.%20To%20address%20this%2C%20we%0Afirst%20integrate%20a%20depth%20estimator%20into%20an%20RGB-D%20SLAM%20system%2C%20but%20this%20approach%0Ais%20hindered%20by%20inaccurate%20geometric%20details%20in%20predicted%20depth.%20Through%20further%0Ainvestigation%2C%20we%20find%20that%203D%20Gaussian%20mapping%20can%20effectively%20solve%20this%0Aproblem.%20Building%20on%20this%2C%20we%20propose%20an%20online%203D%20reconstruction%20method%20using%0A3D%20Gaussian-based%20SLAM%2C%20combined%20with%20a%20feed-forward%20recurrent%20prediction%0Amodule%20to%20directly%20infer%20camera%20pose%20from%20optical%20flow.%20This%20approach%20replaces%0Aslow%20test-time%20optimization%20with%20fast%20network%20inference%2C%20significantly%0Aimproving%20tracking%20speed.%20Additionally%2C%20we%20introduce%20a%20local%20graph%20rendering%0Atechnique%20to%20enhance%20robustness%20in%20feed-forward%20pose%20prediction.%20Experimental%0Aresults%20on%20the%20Replica%20and%20TUM-RGBD%20datasets%2C%20along%20with%20a%20real-world%0Adeployment%20demonstration%2C%20show%20that%20our%20method%20achieves%20performance%20on%20par%20with%0Athe%20state-of-the-art%20SplaTAM%2C%20while%20reducing%20tracking%20time%20by%20more%20than%2090%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04597v1&entry.124074799=Read"},
{"title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction", "author": "Yaopeng Lou and Liao Shen and Tianqi Liu and Jiaqi Li and Zihao Huang and Huiqiang Sun and Zhiguo Cao", "abstract": "  We present Multi-Baseline Gaussian Splatting (MuRF), a generalized\nfeed-forward approach for novel view synthesis that effectively handles diverse\nbaseline settings, including sparse input views with both small and large\nbaselines. Specifically, we integrate features from Multi-View Stereo (MVS) and\nMonocular Depth Estimation (MDE) to enhance feature representations for\ngeneralizable reconstruction. Next, We propose a projection-and-sampling\nmechanism for deep depth fusion, which constructs a fine probability volume to\nguide the regression of the feature map. Furthermore, We introduce a\nreference-view loss to improve geometry and optimization efficiency. We\nleverage 3D Gaussian representations to accelerate training and inference time\nwhile enhancing rendering quality. MuRF achieves state-of-the-art performance\nacross multiple baseline settings and diverse scenarios ranging from simple\nobjects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also\ndemonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360\ndatasets.\n", "link": "http://arxiv.org/abs/2508.04297v1", "date": "2025-08-06", "relevancy": 3.459, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7247}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6782}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuGS%3A%20Multi-Baseline%20Generalizable%20Gaussian%20Splatting%20Reconstruction&body=Title%3A%20MuGS%3A%20Multi-Baseline%20Generalizable%20Gaussian%20Splatting%20Reconstruction%0AAuthor%3A%20Yaopeng%20Lou%20and%20Liao%20Shen%20and%20Tianqi%20Liu%20and%20Jiaqi%20Li%20and%20Zihao%20Huang%20and%20Huiqiang%20Sun%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20We%20present%20Multi-Baseline%20Gaussian%20Splatting%20%28MuRF%29%2C%20a%20generalized%0Afeed-forward%20approach%20for%20novel%20view%20synthesis%20that%20effectively%20handles%20diverse%0Abaseline%20settings%2C%20including%20sparse%20input%20views%20with%20both%20small%20and%20large%0Abaselines.%20Specifically%2C%20we%20integrate%20features%20from%20Multi-View%20Stereo%20%28MVS%29%20and%0AMonocular%20Depth%20Estimation%20%28MDE%29%20to%20enhance%20feature%20representations%20for%0Ageneralizable%20reconstruction.%20Next%2C%20We%20propose%20a%20projection-and-sampling%0Amechanism%20for%20deep%20depth%20fusion%2C%20which%20constructs%20a%20fine%20probability%20volume%20to%0Aguide%20the%20regression%20of%20the%20feature%20map.%20Furthermore%2C%20We%20introduce%20a%0Areference-view%20loss%20to%20improve%20geometry%20and%20optimization%20efficiency.%20We%0Aleverage%203D%20Gaussian%20representations%20to%20accelerate%20training%20and%20inference%20time%0Awhile%20enhancing%20rendering%20quality.%20MuRF%20achieves%20state-of-the-art%20performance%0Aacross%20multiple%20baseline%20settings%20and%20diverse%20scenarios%20ranging%20from%20simple%0Aobjects%20%28DTU%29%20to%20complex%20indoor%20and%20outdoor%20scenes%20%28RealEstate10K%29.%20We%20also%0Ademonstrate%20promising%20zero-shot%20performance%20on%20the%20LLFF%20and%20Mip-NeRF%20360%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuGS%253A%2520Multi-Baseline%2520Generalizable%2520Gaussian%2520Splatting%2520Reconstruction%26entry.906535625%3DYaopeng%2520Lou%2520and%2520Liao%2520Shen%2520and%2520Tianqi%2520Liu%2520and%2520Jiaqi%2520Li%2520and%2520Zihao%2520Huang%2520and%2520Huiqiang%2520Sun%2520and%2520Zhiguo%2520Cao%26entry.1292438233%3D%2520%2520We%2520present%2520Multi-Baseline%2520Gaussian%2520Splatting%2520%2528MuRF%2529%252C%2520a%2520generalized%250Afeed-forward%2520approach%2520for%2520novel%2520view%2520synthesis%2520that%2520effectively%2520handles%2520diverse%250Abaseline%2520settings%252C%2520including%2520sparse%2520input%2520views%2520with%2520both%2520small%2520and%2520large%250Abaselines.%2520Specifically%252C%2520we%2520integrate%2520features%2520from%2520Multi-View%2520Stereo%2520%2528MVS%2529%2520and%250AMonocular%2520Depth%2520Estimation%2520%2528MDE%2529%2520to%2520enhance%2520feature%2520representations%2520for%250Ageneralizable%2520reconstruction.%2520Next%252C%2520We%2520propose%2520a%2520projection-and-sampling%250Amechanism%2520for%2520deep%2520depth%2520fusion%252C%2520which%2520constructs%2520a%2520fine%2520probability%2520volume%2520to%250Aguide%2520the%2520regression%2520of%2520the%2520feature%2520map.%2520Furthermore%252C%2520We%2520introduce%2520a%250Areference-view%2520loss%2520to%2520improve%2520geometry%2520and%2520optimization%2520efficiency.%2520We%250Aleverage%25203D%2520Gaussian%2520representations%2520to%2520accelerate%2520training%2520and%2520inference%2520time%250Awhile%2520enhancing%2520rendering%2520quality.%2520MuRF%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520multiple%2520baseline%2520settings%2520and%2520diverse%2520scenarios%2520ranging%2520from%2520simple%250Aobjects%2520%2528DTU%2529%2520to%2520complex%2520indoor%2520and%2520outdoor%2520scenes%2520%2528RealEstate10K%2529.%2520We%2520also%250Ademonstrate%2520promising%2520zero-shot%2520performance%2520on%2520the%2520LLFF%2520and%2520Mip-NeRF%2520360%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuGS%3A%20Multi-Baseline%20Generalizable%20Gaussian%20Splatting%20Reconstruction&entry.906535625=Yaopeng%20Lou%20and%20Liao%20Shen%20and%20Tianqi%20Liu%20and%20Jiaqi%20Li%20and%20Zihao%20Huang%20and%20Huiqiang%20Sun%20and%20Zhiguo%20Cao&entry.1292438233=%20%20We%20present%20Multi-Baseline%20Gaussian%20Splatting%20%28MuRF%29%2C%20a%20generalized%0Afeed-forward%20approach%20for%20novel%20view%20synthesis%20that%20effectively%20handles%20diverse%0Abaseline%20settings%2C%20including%20sparse%20input%20views%20with%20both%20small%20and%20large%0Abaselines.%20Specifically%2C%20we%20integrate%20features%20from%20Multi-View%20Stereo%20%28MVS%29%20and%0AMonocular%20Depth%20Estimation%20%28MDE%29%20to%20enhance%20feature%20representations%20for%0Ageneralizable%20reconstruction.%20Next%2C%20We%20propose%20a%20projection-and-sampling%0Amechanism%20for%20deep%20depth%20fusion%2C%20which%20constructs%20a%20fine%20probability%20volume%20to%0Aguide%20the%20regression%20of%20the%20feature%20map.%20Furthermore%2C%20We%20introduce%20a%0Areference-view%20loss%20to%20improve%20geometry%20and%20optimization%20efficiency.%20We%0Aleverage%203D%20Gaussian%20representations%20to%20accelerate%20training%20and%20inference%20time%0Awhile%20enhancing%20rendering%20quality.%20MuRF%20achieves%20state-of-the-art%20performance%0Aacross%20multiple%20baseline%20settings%20and%20diverse%20scenarios%20ranging%20from%20simple%0Aobjects%20%28DTU%29%20to%20complex%20indoor%20and%20outdoor%20scenes%20%28RealEstate10K%29.%20We%20also%0Ademonstrate%20promising%20zero-shot%20performance%20on%20the%20LLFF%20and%20Mip-NeRF%20360%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04297v1&entry.124074799=Read"},
{"title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to\n  Unfavorable Views", "author": "Yuki Fujimura and Takahiro Kushida and Kazuya Kitano and Takuya Funatomi and Yasuhiro Mukaigawa", "abstract": "  This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS)\nframework designed to handle unfavorable input views. A common rendering setup\nfor training feed-forward approaches places a 3D object at the world origin and\nrenders it from cameras pointed toward the origin -- i.e., from favorable\nviews, limiting the applicability of these models to real-world scenarios\ninvolving varying and unknown camera poses. To overcome this limitation, we\nintroduce a novel adaptation framework that enables pretrained pose-free\nfeed-forward 3DGS models to handle unfavorable views. We leverage priors\nlearned from favorable images by feeding recentered images into a pretrained\nmodel augmented with low-rank adaptation (LoRA) layers. We further propose a\nGaussian adapter module to enhance the geometric consistency of the Gaussians\nderived from the recentered inputs, along with a Gaussian alignment method to\nrender accurate target views for training. Additionally, we introduce a new\ntraining strategy that utilizes an off-the-shelf dataset composed solely of\nfavorable images. Experimental results on both synthetic images from the Google\nScanned Objects dataset and real images from the OmniObject3D dataset validate\nthe effectiveness of our method in handling unfavorable input views.\n", "link": "http://arxiv.org/abs/2507.22342v2", "date": "2025-08-06", "relevancy": 3.4144, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7193}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6944}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UFV-Splatter%3A%20Pose-Free%20Feed-Forward%203D%20Gaussian%20Splatting%20Adapted%20to%0A%20%20Unfavorable%20Views&body=Title%3A%20UFV-Splatter%3A%20Pose-Free%20Feed-Forward%203D%20Gaussian%20Splatting%20Adapted%20to%0A%20%20Unfavorable%20Views%0AAuthor%3A%20Yuki%20Fujimura%20and%20Takahiro%20Kushida%20and%20Kazuya%20Kitano%20and%20Takuya%20Funatomi%20and%20Yasuhiro%20Mukaigawa%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20pose-free%2C%20feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%0Aframework%20designed%20to%20handle%20unfavorable%20input%20views.%20A%20common%20rendering%20setup%0Afor%20training%20feed-forward%20approaches%20places%20a%203D%20object%20at%20the%20world%20origin%20and%0Arenders%20it%20from%20cameras%20pointed%20toward%20the%20origin%20--%20i.e.%2C%20from%20favorable%0Aviews%2C%20limiting%20the%20applicability%20of%20these%20models%20to%20real-world%20scenarios%0Ainvolving%20varying%20and%20unknown%20camera%20poses.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20a%20novel%20adaptation%20framework%20that%20enables%20pretrained%20pose-free%0Afeed-forward%203DGS%20models%20to%20handle%20unfavorable%20views.%20We%20leverage%20priors%0Alearned%20from%20favorable%20images%20by%20feeding%20recentered%20images%20into%20a%20pretrained%0Amodel%20augmented%20with%20low-rank%20adaptation%20%28LoRA%29%20layers.%20We%20further%20propose%20a%0AGaussian%20adapter%20module%20to%20enhance%20the%20geometric%20consistency%20of%20the%20Gaussians%0Aderived%20from%20the%20recentered%20inputs%2C%20along%20with%20a%20Gaussian%20alignment%20method%20to%0Arender%20accurate%20target%20views%20for%20training.%20Additionally%2C%20we%20introduce%20a%20new%0Atraining%20strategy%20that%20utilizes%20an%20off-the-shelf%20dataset%20composed%20solely%20of%0Afavorable%20images.%20Experimental%20results%20on%20both%20synthetic%20images%20from%20the%20Google%0AScanned%20Objects%20dataset%20and%20real%20images%20from%20the%20OmniObject3D%20dataset%20validate%0Athe%20effectiveness%20of%20our%20method%20in%20handling%20unfavorable%20input%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22342v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUFV-Splatter%253A%2520Pose-Free%2520Feed-Forward%25203D%2520Gaussian%2520Splatting%2520Adapted%2520to%250A%2520%2520Unfavorable%2520Views%26entry.906535625%3DYuki%2520Fujimura%2520and%2520Takahiro%2520Kushida%2520and%2520Kazuya%2520Kitano%2520and%2520Takuya%2520Funatomi%2520and%2520Yasuhiro%2520Mukaigawa%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520pose-free%252C%2520feed-forward%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Aframework%2520designed%2520to%2520handle%2520unfavorable%2520input%2520views.%2520A%2520common%2520rendering%2520setup%250Afor%2520training%2520feed-forward%2520approaches%2520places%2520a%25203D%2520object%2520at%2520the%2520world%2520origin%2520and%250Arenders%2520it%2520from%2520cameras%2520pointed%2520toward%2520the%2520origin%2520--%2520i.e.%252C%2520from%2520favorable%250Aviews%252C%2520limiting%2520the%2520applicability%2520of%2520these%2520models%2520to%2520real-world%2520scenarios%250Ainvolving%2520varying%2520and%2520unknown%2520camera%2520poses.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aintroduce%2520a%2520novel%2520adaptation%2520framework%2520that%2520enables%2520pretrained%2520pose-free%250Afeed-forward%25203DGS%2520models%2520to%2520handle%2520unfavorable%2520views.%2520We%2520leverage%2520priors%250Alearned%2520from%2520favorable%2520images%2520by%2520feeding%2520recentered%2520images%2520into%2520a%2520pretrained%250Amodel%2520augmented%2520with%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520layers.%2520We%2520further%2520propose%2520a%250AGaussian%2520adapter%2520module%2520to%2520enhance%2520the%2520geometric%2520consistency%2520of%2520the%2520Gaussians%250Aderived%2520from%2520the%2520recentered%2520inputs%252C%2520along%2520with%2520a%2520Gaussian%2520alignment%2520method%2520to%250Arender%2520accurate%2520target%2520views%2520for%2520training.%2520Additionally%252C%2520we%2520introduce%2520a%2520new%250Atraining%2520strategy%2520that%2520utilizes%2520an%2520off-the-shelf%2520dataset%2520composed%2520solely%2520of%250Afavorable%2520images.%2520Experimental%2520results%2520on%2520both%2520synthetic%2520images%2520from%2520the%2520Google%250AScanned%2520Objects%2520dataset%2520and%2520real%2520images%2520from%2520the%2520OmniObject3D%2520dataset%2520validate%250Athe%2520effectiveness%2520of%2520our%2520method%2520in%2520handling%2520unfavorable%2520input%2520views.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22342v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UFV-Splatter%3A%20Pose-Free%20Feed-Forward%203D%20Gaussian%20Splatting%20Adapted%20to%0A%20%20Unfavorable%20Views&entry.906535625=Yuki%20Fujimura%20and%20Takahiro%20Kushida%20and%20Kazuya%20Kitano%20and%20Takuya%20Funatomi%20and%20Yasuhiro%20Mukaigawa&entry.1292438233=%20%20This%20paper%20presents%20a%20pose-free%2C%20feed-forward%203D%20Gaussian%20Splatting%20%283DGS%29%0Aframework%20designed%20to%20handle%20unfavorable%20input%20views.%20A%20common%20rendering%20setup%0Afor%20training%20feed-forward%20approaches%20places%20a%203D%20object%20at%20the%20world%20origin%20and%0Arenders%20it%20from%20cameras%20pointed%20toward%20the%20origin%20--%20i.e.%2C%20from%20favorable%0Aviews%2C%20limiting%20the%20applicability%20of%20these%20models%20to%20real-world%20scenarios%0Ainvolving%20varying%20and%20unknown%20camera%20poses.%20To%20overcome%20this%20limitation%2C%20we%0Aintroduce%20a%20novel%20adaptation%20framework%20that%20enables%20pretrained%20pose-free%0Afeed-forward%203DGS%20models%20to%20handle%20unfavorable%20views.%20We%20leverage%20priors%0Alearned%20from%20favorable%20images%20by%20feeding%20recentered%20images%20into%20a%20pretrained%0Amodel%20augmented%20with%20low-rank%20adaptation%20%28LoRA%29%20layers.%20We%20further%20propose%20a%0AGaussian%20adapter%20module%20to%20enhance%20the%20geometric%20consistency%20of%20the%20Gaussians%0Aderived%20from%20the%20recentered%20inputs%2C%20along%20with%20a%20Gaussian%20alignment%20method%20to%0Arender%20accurate%20target%20views%20for%20training.%20Additionally%2C%20we%20introduce%20a%20new%0Atraining%20strategy%20that%20utilizes%20an%20off-the-shelf%20dataset%20composed%20solely%20of%0Afavorable%20images.%20Experimental%20results%20on%20both%20synthetic%20images%20from%20the%20Google%0AScanned%20Objects%20dataset%20and%20real%20images%20from%20the%20OmniObject3D%20dataset%20validate%0Athe%20effectiveness%20of%20our%20method%20in%20handling%20unfavorable%20input%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22342v2&entry.124074799=Read"},
{"title": "4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D\n  Content Generation", "author": "Shuzhou Yang and Xiaodong Cun and Xiaoyu Li and Yaowei Li and Jian Zhang", "abstract": "  Given the high complexity of directly generating high-dimensional data such\nas 4D, we present 4DVD, a cascaded video diffusion model that generates 4D\ncontent in a decoupled manner. Unlike previous multi-view video methods that\ndirectly model 3D space and temporal features simultaneously with stacked cross\nview/temporal attention modules, 4DVD decouples this into two subtasks: coarse\nmulti-view layout generation and structure-aware conditional generation, and\neffectively unifies them. Specifically, given a monocular video, 4DVD first\npredicts the dense view content of its layout with superior cross-view and\ntemporal consistency. Based on the produced layout priors, a structure-aware\nspatio-temporal generation branch is developed, combining these coarse\nstructural priors with the exquisite appearance content of input monocular\nvideo to generate final high-quality dense-view videos. Benefit from this,\nexplicit 4D representation~(such as 4D Gaussian) can be optimized accurately,\nenabling wider practical application. To train 4DVD, we collect a dynamic 3D\nobject dataset, called D-Objaverse, from the Objaverse benchmark and render 16\nvideos with 21 frames for each object. Extensive experiments demonstrate our\nstate-of-the-art performance on both novel view synthesis and 4D generation.\nOur project page is https://4dvd.github.io/\n", "link": "http://arxiv.org/abs/2508.04467v1", "date": "2025-08-06", "relevancy": 3.4075, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7026}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7026}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DVD%3A%20Cascaded%20Dense-view%20Video%20Diffusion%20Model%20for%20High-quality%204D%0A%20%20Content%20Generation&body=Title%3A%204DVD%3A%20Cascaded%20Dense-view%20Video%20Diffusion%20Model%20for%20High-quality%204D%0A%20%20Content%20Generation%0AAuthor%3A%20Shuzhou%20Yang%20and%20Xiaodong%20Cun%20and%20Xiaoyu%20Li%20and%20Yaowei%20Li%20and%20Jian%20Zhang%0AAbstract%3A%20%20%20Given%20the%20high%20complexity%20of%20directly%20generating%20high-dimensional%20data%20such%0Aas%204D%2C%20we%20present%204DVD%2C%20a%20cascaded%20video%20diffusion%20model%20that%20generates%204D%0Acontent%20in%20a%20decoupled%20manner.%20Unlike%20previous%20multi-view%20video%20methods%20that%0Adirectly%20model%203D%20space%20and%20temporal%20features%20simultaneously%20with%20stacked%20cross%0Aview/temporal%20attention%20modules%2C%204DVD%20decouples%20this%20into%20two%20subtasks%3A%20coarse%0Amulti-view%20layout%20generation%20and%20structure-aware%20conditional%20generation%2C%20and%0Aeffectively%20unifies%20them.%20Specifically%2C%20given%20a%20monocular%20video%2C%204DVD%20first%0Apredicts%20the%20dense%20view%20content%20of%20its%20layout%20with%20superior%20cross-view%20and%0Atemporal%20consistency.%20Based%20on%20the%20produced%20layout%20priors%2C%20a%20structure-aware%0Aspatio-temporal%20generation%20branch%20is%20developed%2C%20combining%20these%20coarse%0Astructural%20priors%20with%20the%20exquisite%20appearance%20content%20of%20input%20monocular%0Avideo%20to%20generate%20final%20high-quality%20dense-view%20videos.%20Benefit%20from%20this%2C%0Aexplicit%204D%20representation~%28such%20as%204D%20Gaussian%29%20can%20be%20optimized%20accurately%2C%0Aenabling%20wider%20practical%20application.%20To%20train%204DVD%2C%20we%20collect%20a%20dynamic%203D%0Aobject%20dataset%2C%20called%20D-Objaverse%2C%20from%20the%20Objaverse%20benchmark%20and%20render%2016%0Avideos%20with%2021%20frames%20for%20each%20object.%20Extensive%20experiments%20demonstrate%20our%0Astate-of-the-art%20performance%20on%20both%20novel%20view%20synthesis%20and%204D%20generation.%0AOur%20project%20page%20is%20https%3A//4dvd.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DVD%253A%2520Cascaded%2520Dense-view%2520Video%2520Diffusion%2520Model%2520for%2520High-quality%25204D%250A%2520%2520Content%2520Generation%26entry.906535625%3DShuzhou%2520Yang%2520and%2520Xiaodong%2520Cun%2520and%2520Xiaoyu%2520Li%2520and%2520Yaowei%2520Li%2520and%2520Jian%2520Zhang%26entry.1292438233%3D%2520%2520Given%2520the%2520high%2520complexity%2520of%2520directly%2520generating%2520high-dimensional%2520data%2520such%250Aas%25204D%252C%2520we%2520present%25204DVD%252C%2520a%2520cascaded%2520video%2520diffusion%2520model%2520that%2520generates%25204D%250Acontent%2520in%2520a%2520decoupled%2520manner.%2520Unlike%2520previous%2520multi-view%2520video%2520methods%2520that%250Adirectly%2520model%25203D%2520space%2520and%2520temporal%2520features%2520simultaneously%2520with%2520stacked%2520cross%250Aview/temporal%2520attention%2520modules%252C%25204DVD%2520decouples%2520this%2520into%2520two%2520subtasks%253A%2520coarse%250Amulti-view%2520layout%2520generation%2520and%2520structure-aware%2520conditional%2520generation%252C%2520and%250Aeffectively%2520unifies%2520them.%2520Specifically%252C%2520given%2520a%2520monocular%2520video%252C%25204DVD%2520first%250Apredicts%2520the%2520dense%2520view%2520content%2520of%2520its%2520layout%2520with%2520superior%2520cross-view%2520and%250Atemporal%2520consistency.%2520Based%2520on%2520the%2520produced%2520layout%2520priors%252C%2520a%2520structure-aware%250Aspatio-temporal%2520generation%2520branch%2520is%2520developed%252C%2520combining%2520these%2520coarse%250Astructural%2520priors%2520with%2520the%2520exquisite%2520appearance%2520content%2520of%2520input%2520monocular%250Avideo%2520to%2520generate%2520final%2520high-quality%2520dense-view%2520videos.%2520Benefit%2520from%2520this%252C%250Aexplicit%25204D%2520representation~%2528such%2520as%25204D%2520Gaussian%2529%2520can%2520be%2520optimized%2520accurately%252C%250Aenabling%2520wider%2520practical%2520application.%2520To%2520train%25204DVD%252C%2520we%2520collect%2520a%2520dynamic%25203D%250Aobject%2520dataset%252C%2520called%2520D-Objaverse%252C%2520from%2520the%2520Objaverse%2520benchmark%2520and%2520render%252016%250Avideos%2520with%252021%2520frames%2520for%2520each%2520object.%2520Extensive%2520experiments%2520demonstrate%2520our%250Astate-of-the-art%2520performance%2520on%2520both%2520novel%2520view%2520synthesis%2520and%25204D%2520generation.%250AOur%2520project%2520page%2520is%2520https%253A//4dvd.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DVD%3A%20Cascaded%20Dense-view%20Video%20Diffusion%20Model%20for%20High-quality%204D%0A%20%20Content%20Generation&entry.906535625=Shuzhou%20Yang%20and%20Xiaodong%20Cun%20and%20Xiaoyu%20Li%20and%20Yaowei%20Li%20and%20Jian%20Zhang&entry.1292438233=%20%20Given%20the%20high%20complexity%20of%20directly%20generating%20high-dimensional%20data%20such%0Aas%204D%2C%20we%20present%204DVD%2C%20a%20cascaded%20video%20diffusion%20model%20that%20generates%204D%0Acontent%20in%20a%20decoupled%20manner.%20Unlike%20previous%20multi-view%20video%20methods%20that%0Adirectly%20model%203D%20space%20and%20temporal%20features%20simultaneously%20with%20stacked%20cross%0Aview/temporal%20attention%20modules%2C%204DVD%20decouples%20this%20into%20two%20subtasks%3A%20coarse%0Amulti-view%20layout%20generation%20and%20structure-aware%20conditional%20generation%2C%20and%0Aeffectively%20unifies%20them.%20Specifically%2C%20given%20a%20monocular%20video%2C%204DVD%20first%0Apredicts%20the%20dense%20view%20content%20of%20its%20layout%20with%20superior%20cross-view%20and%0Atemporal%20consistency.%20Based%20on%20the%20produced%20layout%20priors%2C%20a%20structure-aware%0Aspatio-temporal%20generation%20branch%20is%20developed%2C%20combining%20these%20coarse%0Astructural%20priors%20with%20the%20exquisite%20appearance%20content%20of%20input%20monocular%0Avideo%20to%20generate%20final%20high-quality%20dense-view%20videos.%20Benefit%20from%20this%2C%0Aexplicit%204D%20representation~%28such%20as%204D%20Gaussian%29%20can%20be%20optimized%20accurately%2C%0Aenabling%20wider%20practical%20application.%20To%20train%204DVD%2C%20we%20collect%20a%20dynamic%203D%0Aobject%20dataset%2C%20called%20D-Objaverse%2C%20from%20the%20Objaverse%20benchmark%20and%20render%2016%0Avideos%20with%2021%20frames%20for%20each%20object.%20Extensive%20experiments%20demonstrate%20our%0Astate-of-the-art%20performance%20on%20both%20novel%20view%20synthesis%20and%204D%20generation.%0AOur%20project%20page%20is%20https%3A//4dvd.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04467v1&entry.124074799=Read"},
{"title": "Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds", "author": "Haodong Zhu and Changbai Li and Yangyang Ren and Zichao Feng and Xuhui Liu and Hanlin Chen and Xiantong Zhen and Baochang Zhang", "abstract": "  Current multi-view 3D reconstruction methods rely on accurate camera\ncalibration and pose estimation, requiring complex and time-intensive\npre-processing that hinders their practical deployment. To address this\nchallenge, we introduce Surf3R, an end-to-end feedforward approach that\nreconstructs 3D surfaces from sparse views without estimating camera poses and\ncompletes an entire scene in under 10 seconds. Our method employs a\nmulti-branch and multi-view decoding architecture in which multiple reference\nviews jointly guide the reconstruction process. Through the proposed\nbranch-wise processing, cross-view attention, and inter-branch fusion, the\nmodel effectively captures complementary geometric cues without requiring\ncamera calibration. Moreover, we introduce a D-Normal regularizer based on an\nexplicit 3D Gaussian representation for surface reconstruction. It couples\nsurface normals with other geometric parameters to jointly optimize the 3D\ngeometry, significantly improving 3D consistency and surface detail accuracy.\nExperimental results demonstrate that Surf3R achieves state-of-the-art\nperformance on multiple surface reconstruction metrics on ScanNet++ and Replica\ndatasets, exhibiting excellent generalization and efficiency.\n", "link": "http://arxiv.org/abs/2508.04508v1", "date": "2025-08-06", "relevancy": 3.3253, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6901}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6525}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Surf3R%3A%20Rapid%20Surface%20Reconstruction%20from%20Sparse%20RGB%20Views%20in%20Seconds&body=Title%3A%20Surf3R%3A%20Rapid%20Surface%20Reconstruction%20from%20Sparse%20RGB%20Views%20in%20Seconds%0AAuthor%3A%20Haodong%20Zhu%20and%20Changbai%20Li%20and%20Yangyang%20Ren%20and%20Zichao%20Feng%20and%20Xuhui%20Liu%20and%20Hanlin%20Chen%20and%20Xiantong%20Zhen%20and%20Baochang%20Zhang%0AAbstract%3A%20%20%20Current%20multi-view%203D%20reconstruction%20methods%20rely%20on%20accurate%20camera%0Acalibration%20and%20pose%20estimation%2C%20requiring%20complex%20and%20time-intensive%0Apre-processing%20that%20hinders%20their%20practical%20deployment.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20Surf3R%2C%20an%20end-to-end%20feedforward%20approach%20that%0Areconstructs%203D%20surfaces%20from%20sparse%20views%20without%20estimating%20camera%20poses%20and%0Acompletes%20an%20entire%20scene%20in%20under%2010%20seconds.%20Our%20method%20employs%20a%0Amulti-branch%20and%20multi-view%20decoding%20architecture%20in%20which%20multiple%20reference%0Aviews%20jointly%20guide%20the%20reconstruction%20process.%20Through%20the%20proposed%0Abranch-wise%20processing%2C%20cross-view%20attention%2C%20and%20inter-branch%20fusion%2C%20the%0Amodel%20effectively%20captures%20complementary%20geometric%20cues%20without%20requiring%0Acamera%20calibration.%20Moreover%2C%20we%20introduce%20a%20D-Normal%20regularizer%20based%20on%20an%0Aexplicit%203D%20Gaussian%20representation%20for%20surface%20reconstruction.%20It%20couples%0Asurface%20normals%20with%20other%20geometric%20parameters%20to%20jointly%20optimize%20the%203D%0Ageometry%2C%20significantly%20improving%203D%20consistency%20and%20surface%20detail%20accuracy.%0AExperimental%20results%20demonstrate%20that%20Surf3R%20achieves%20state-of-the-art%0Aperformance%20on%20multiple%20surface%20reconstruction%20metrics%20on%20ScanNet%2B%2B%20and%20Replica%0Adatasets%2C%20exhibiting%20excellent%20generalization%20and%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurf3R%253A%2520Rapid%2520Surface%2520Reconstruction%2520from%2520Sparse%2520RGB%2520Views%2520in%2520Seconds%26entry.906535625%3DHaodong%2520Zhu%2520and%2520Changbai%2520Li%2520and%2520Yangyang%2520Ren%2520and%2520Zichao%2520Feng%2520and%2520Xuhui%2520Liu%2520and%2520Hanlin%2520Chen%2520and%2520Xiantong%2520Zhen%2520and%2520Baochang%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520multi-view%25203D%2520reconstruction%2520methods%2520rely%2520on%2520accurate%2520camera%250Acalibration%2520and%2520pose%2520estimation%252C%2520requiring%2520complex%2520and%2520time-intensive%250Apre-processing%2520that%2520hinders%2520their%2520practical%2520deployment.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520introduce%2520Surf3R%252C%2520an%2520end-to-end%2520feedforward%2520approach%2520that%250Areconstructs%25203D%2520surfaces%2520from%2520sparse%2520views%2520without%2520estimating%2520camera%2520poses%2520and%250Acompletes%2520an%2520entire%2520scene%2520in%2520under%252010%2520seconds.%2520Our%2520method%2520employs%2520a%250Amulti-branch%2520and%2520multi-view%2520decoding%2520architecture%2520in%2520which%2520multiple%2520reference%250Aviews%2520jointly%2520guide%2520the%2520reconstruction%2520process.%2520Through%2520the%2520proposed%250Abranch-wise%2520processing%252C%2520cross-view%2520attention%252C%2520and%2520inter-branch%2520fusion%252C%2520the%250Amodel%2520effectively%2520captures%2520complementary%2520geometric%2520cues%2520without%2520requiring%250Acamera%2520calibration.%2520Moreover%252C%2520we%2520introduce%2520a%2520D-Normal%2520regularizer%2520based%2520on%2520an%250Aexplicit%25203D%2520Gaussian%2520representation%2520for%2520surface%2520reconstruction.%2520It%2520couples%250Asurface%2520normals%2520with%2520other%2520geometric%2520parameters%2520to%2520jointly%2520optimize%2520the%25203D%250Ageometry%252C%2520significantly%2520improving%25203D%2520consistency%2520and%2520surface%2520detail%2520accuracy.%250AExperimental%2520results%2520demonstrate%2520that%2520Surf3R%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520multiple%2520surface%2520reconstruction%2520metrics%2520on%2520ScanNet%252B%252B%2520and%2520Replica%250Adatasets%252C%2520exhibiting%2520excellent%2520generalization%2520and%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Surf3R%3A%20Rapid%20Surface%20Reconstruction%20from%20Sparse%20RGB%20Views%20in%20Seconds&entry.906535625=Haodong%20Zhu%20and%20Changbai%20Li%20and%20Yangyang%20Ren%20and%20Zichao%20Feng%20and%20Xuhui%20Liu%20and%20Hanlin%20Chen%20and%20Xiantong%20Zhen%20and%20Baochang%20Zhang&entry.1292438233=%20%20Current%20multi-view%203D%20reconstruction%20methods%20rely%20on%20accurate%20camera%0Acalibration%20and%20pose%20estimation%2C%20requiring%20complex%20and%20time-intensive%0Apre-processing%20that%20hinders%20their%20practical%20deployment.%20To%20address%20this%0Achallenge%2C%20we%20introduce%20Surf3R%2C%20an%20end-to-end%20feedforward%20approach%20that%0Areconstructs%203D%20surfaces%20from%20sparse%20views%20without%20estimating%20camera%20poses%20and%0Acompletes%20an%20entire%20scene%20in%20under%2010%20seconds.%20Our%20method%20employs%20a%0Amulti-branch%20and%20multi-view%20decoding%20architecture%20in%20which%20multiple%20reference%0Aviews%20jointly%20guide%20the%20reconstruction%20process.%20Through%20the%20proposed%0Abranch-wise%20processing%2C%20cross-view%20attention%2C%20and%20inter-branch%20fusion%2C%20the%0Amodel%20effectively%20captures%20complementary%20geometric%20cues%20without%20requiring%0Acamera%20calibration.%20Moreover%2C%20we%20introduce%20a%20D-Normal%20regularizer%20based%20on%20an%0Aexplicit%203D%20Gaussian%20representation%20for%20surface%20reconstruction.%20It%20couples%0Asurface%20normals%20with%20other%20geometric%20parameters%20to%20jointly%20optimize%20the%203D%0Ageometry%2C%20significantly%20improving%203D%20consistency%20and%20surface%20detail%20accuracy.%0AExperimental%20results%20demonstrate%20that%20Surf3R%20achieves%20state-of-the-art%0Aperformance%20on%20multiple%20surface%20reconstruction%20metrics%20on%20ScanNet%2B%2B%20and%20Replica%0Adatasets%2C%20exhibiting%20excellent%20generalization%20and%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04508v1&entry.124074799=Read"},
{"title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT\n  Reconstruction", "author": "Yikuang Yuluo and Yue Ma and Kuan Shen and Tongtong Jin and Wang Liao and Yangpu Ma and Fuquan Wang", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT\nreconstruction. However, existing methods rely on the average gradient\nmagnitude of points within the view, often leading to severe needle-like\nartifacts under sparse-view conditions. To address this challenge, we propose\nGR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses\nneedle-like artifacts and improves reconstruction accuracy under sparse-view\nconditions. Our framework introduces two key innovations: (1) a Denoised Point\nCloud Initialization Strategy that reduces initialization errors and\naccelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that\nrefines gradient computation using graph-based density differences, improving\nsplitting accuracy and density representation. Experiments on X-3D and\nreal-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR\nimprovements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These\nresults highlight the applicability of GR-Gaussian for accurate CT\nreconstruction under challenging sparse-view conditions.\n", "link": "http://arxiv.org/abs/2508.02408v2", "date": "2025-08-06", "relevancy": 3.3132, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7125}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6653}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction&body=Title%3A%20GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction%0AAuthor%3A%20Yikuang%20Yuluo%20and%20Yue%20Ma%20and%20Kuan%20Shen%20and%20Tongtong%20Jin%20and%20Wang%20Liao%20and%20Yangpu%20Ma%20and%20Fuquan%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20approach%20for%20CT%0Areconstruction.%20However%2C%20existing%20methods%20rely%20on%20the%20average%20gradient%0Amagnitude%20of%20points%20within%20the%20view%2C%20often%20leading%20to%20severe%20needle-like%0Aartifacts%20under%20sparse-view%20conditions.%20To%20address%20this%20challenge%2C%20we%20propose%0AGR-Gaussian%2C%20a%20graph-based%203D%20Gaussian%20Splatting%20framework%20that%20suppresses%0Aneedle-like%20artifacts%20and%20improves%20reconstruction%20accuracy%20under%20sparse-view%0Aconditions.%20Our%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20Denoised%20Point%0ACloud%20Initialization%20Strategy%20that%20reduces%20initialization%20errors%20and%0Aaccelerates%20convergence%3B%20and%20%282%29%20a%20Pixel-Graph-Aware%20Gradient%20Strategy%20that%0Arefines%20gradient%20computation%20using%20graph-based%20density%20differences%2C%20improving%0Asplitting%20accuracy%20and%20density%20representation.%20Experiments%20on%20X-3D%20and%0Areal-world%20datasets%20validate%20the%20effectiveness%20of%20GR-Gaussian%2C%20achieving%20PSNR%0Aimprovements%20of%200.67%20dB%20and%200.92%20dB%2C%20and%20SSIM%20gains%20of%200.011%20and%200.021.%20These%0Aresults%20highlight%20the%20applicability%20of%20GR-Gaussian%20for%20accurate%20CT%0Areconstruction%20under%20challenging%20sparse-view%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGR-Gaussian%253A%2520Graph-Based%2520Radiative%2520Gaussian%2520Splatting%2520for%2520Sparse-View%2520CT%250A%2520%2520Reconstruction%26entry.906535625%3DYikuang%2520Yuluo%2520and%2520Yue%2520Ma%2520and%2520Kuan%2520Shen%2520and%2520Tongtong%2520Jin%2520and%2520Wang%2520Liao%2520and%2520Yangpu%2520Ma%2520and%2520Fuquan%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520CT%250Areconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520the%2520average%2520gradient%250Amagnitude%2520of%2520points%2520within%2520the%2520view%252C%2520often%2520leading%2520to%2520severe%2520needle-like%250Aartifacts%2520under%2520sparse-view%2520conditions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250AGR-Gaussian%252C%2520a%2520graph-based%25203D%2520Gaussian%2520Splatting%2520framework%2520that%2520suppresses%250Aneedle-like%2520artifacts%2520and%2520improves%2520reconstruction%2520accuracy%2520under%2520sparse-view%250Aconditions.%2520Our%2520framework%2520introduces%2520two%2520key%2520innovations%253A%2520%25281%2529%2520a%2520Denoised%2520Point%250ACloud%2520Initialization%2520Strategy%2520that%2520reduces%2520initialization%2520errors%2520and%250Aaccelerates%2520convergence%253B%2520and%2520%25282%2529%2520a%2520Pixel-Graph-Aware%2520Gradient%2520Strategy%2520that%250Arefines%2520gradient%2520computation%2520using%2520graph-based%2520density%2520differences%252C%2520improving%250Asplitting%2520accuracy%2520and%2520density%2520representation.%2520Experiments%2520on%2520X-3D%2520and%250Areal-world%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520GR-Gaussian%252C%2520achieving%2520PSNR%250Aimprovements%2520of%25200.67%2520dB%2520and%25200.92%2520dB%252C%2520and%2520SSIM%2520gains%2520of%25200.011%2520and%25200.021.%2520These%250Aresults%2520highlight%2520the%2520applicability%2520of%2520GR-Gaussian%2520for%2520accurate%2520CT%250Areconstruction%2520under%2520challenging%2520sparse-view%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GR-Gaussian%3A%20Graph-Based%20Radiative%20Gaussian%20Splatting%20for%20Sparse-View%20CT%0A%20%20Reconstruction&entry.906535625=Yikuang%20Yuluo%20and%20Yue%20Ma%20and%20Kuan%20Shen%20and%20Tongtong%20Jin%20and%20Wang%20Liao%20and%20Yangpu%20Ma%20and%20Fuquan%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20approach%20for%20CT%0Areconstruction.%20However%2C%20existing%20methods%20rely%20on%20the%20average%20gradient%0Amagnitude%20of%20points%20within%20the%20view%2C%20often%20leading%20to%20severe%20needle-like%0Aartifacts%20under%20sparse-view%20conditions.%20To%20address%20this%20challenge%2C%20we%20propose%0AGR-Gaussian%2C%20a%20graph-based%203D%20Gaussian%20Splatting%20framework%20that%20suppresses%0Aneedle-like%20artifacts%20and%20improves%20reconstruction%20accuracy%20under%20sparse-view%0Aconditions.%20Our%20framework%20introduces%20two%20key%20innovations%3A%20%281%29%20a%20Denoised%20Point%0ACloud%20Initialization%20Strategy%20that%20reduces%20initialization%20errors%20and%0Aaccelerates%20convergence%3B%20and%20%282%29%20a%20Pixel-Graph-Aware%20Gradient%20Strategy%20that%0Arefines%20gradient%20computation%20using%20graph-based%20density%20differences%2C%20improving%0Asplitting%20accuracy%20and%20density%20representation.%20Experiments%20on%20X-3D%20and%0Areal-world%20datasets%20validate%20the%20effectiveness%20of%20GR-Gaussian%2C%20achieving%20PSNR%0Aimprovements%20of%200.67%20dB%20and%200.92%20dB%2C%20and%20SSIM%20gains%20of%200.011%20and%200.021.%20These%0Aresults%20highlight%20the%20applicability%20of%20GR-Gaussian%20for%20accurate%20CT%0Areconstruction%20under%20challenging%20sparse-view%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02408v2&entry.124074799=Read"},
{"title": "OpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene\n  Understanding", "author": "Youjun Zhao and Jiaying Lin and Shuquan Ye and Qianshi Pang and Rynson W. H. Lau", "abstract": "  Open-vocabulary 3D scene understanding (OV-3D) aims to localize and classify\nnovel objects beyond the closed set of object classes. However, existing\napproaches and benchmarks primarily focus on the open vocabulary problem within\nthe context of object classes, which is insufficient in providing a holistic\nevaluation to what extent a model understands the 3D scene. In this paper, we\nintroduce a more challenging task called Generalized Open-Vocabulary 3D Scene\nUnderstanding (GOV-3D) to explore the open vocabulary problem beyond object\nclasses. It encompasses an open and diverse set of generalized knowledge,\nexpressed as linguistic queries of fine-grained and object-specific attributes.\nTo this end, we contribute a new benchmark named \\textit{OpenScan}, which\nconsists of 3D object attributes across eight representative linguistic\naspects, including affordance, property, and material. We further evaluate\nstate-of-the-art OV-3D methods on our OpenScan benchmark and discover that\nthese methods struggle to comprehend the abstract vocabularies of the GOV-3D\ntask, a challenge that cannot be addressed simply by scaling up object classes\nduring training. We highlight the limitations of existing methodologies and\nexplore promising directions to overcome the identified shortcomings.\n", "link": "http://arxiv.org/abs/2408.11030v3", "date": "2025-08-06", "relevancy": 3.2144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6915}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenScan%3A%20A%20Benchmark%20for%20Generalized%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding&body=Title%3A%20OpenScan%3A%20A%20Benchmark%20for%20Generalized%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding%0AAuthor%3A%20Youjun%20Zhao%20and%20Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Qianshi%20Pang%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Open-vocabulary%203D%20scene%20understanding%20%28OV-3D%29%20aims%20to%20localize%20and%20classify%0Anovel%20objects%20beyond%20the%20closed%20set%20of%20object%20classes.%20However%2C%20existing%0Aapproaches%20and%20benchmarks%20primarily%20focus%20on%20the%20open%20vocabulary%20problem%20within%0Athe%20context%20of%20object%20classes%2C%20which%20is%20insufficient%20in%20providing%20a%20holistic%0Aevaluation%20to%20what%20extent%20a%20model%20understands%20the%203D%20scene.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20more%20challenging%20task%20called%20Generalized%20Open-Vocabulary%203D%20Scene%0AUnderstanding%20%28GOV-3D%29%20to%20explore%20the%20open%20vocabulary%20problem%20beyond%20object%0Aclasses.%20It%20encompasses%20an%20open%20and%20diverse%20set%20of%20generalized%20knowledge%2C%0Aexpressed%20as%20linguistic%20queries%20of%20fine-grained%20and%20object-specific%20attributes.%0ATo%20this%20end%2C%20we%20contribute%20a%20new%20benchmark%20named%20%5Ctextit%7BOpenScan%7D%2C%20which%0Aconsists%20of%203D%20object%20attributes%20across%20eight%20representative%20linguistic%0Aaspects%2C%20including%20affordance%2C%20property%2C%20and%20material.%20We%20further%20evaluate%0Astate-of-the-art%20OV-3D%20methods%20on%20our%20OpenScan%20benchmark%20and%20discover%20that%0Athese%20methods%20struggle%20to%20comprehend%20the%20abstract%20vocabularies%20of%20the%20GOV-3D%0Atask%2C%20a%20challenge%20that%20cannot%20be%20addressed%20simply%20by%20scaling%20up%20object%20classes%0Aduring%20training.%20We%20highlight%20the%20limitations%20of%20existing%20methodologies%20and%0Aexplore%20promising%20directions%20to%20overcome%20the%20identified%20shortcomings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11030v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenScan%253A%2520A%2520Benchmark%2520for%2520Generalized%2520Open-Vocabulary%25203D%2520Scene%250A%2520%2520Understanding%26entry.906535625%3DYoujun%2520Zhao%2520and%2520Jiaying%2520Lin%2520and%2520Shuquan%2520Ye%2520and%2520Qianshi%2520Pang%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520scene%2520understanding%2520%2528OV-3D%2529%2520aims%2520to%2520localize%2520and%2520classify%250Anovel%2520objects%2520beyond%2520the%2520closed%2520set%2520of%2520object%2520classes.%2520However%252C%2520existing%250Aapproaches%2520and%2520benchmarks%2520primarily%2520focus%2520on%2520the%2520open%2520vocabulary%2520problem%2520within%250Athe%2520context%2520of%2520object%2520classes%252C%2520which%2520is%2520insufficient%2520in%2520providing%2520a%2520holistic%250Aevaluation%2520to%2520what%2520extent%2520a%2520model%2520understands%2520the%25203D%2520scene.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520more%2520challenging%2520task%2520called%2520Generalized%2520Open-Vocabulary%25203D%2520Scene%250AUnderstanding%2520%2528GOV-3D%2529%2520to%2520explore%2520the%2520open%2520vocabulary%2520problem%2520beyond%2520object%250Aclasses.%2520It%2520encompasses%2520an%2520open%2520and%2520diverse%2520set%2520of%2520generalized%2520knowledge%252C%250Aexpressed%2520as%2520linguistic%2520queries%2520of%2520fine-grained%2520and%2520object-specific%2520attributes.%250ATo%2520this%2520end%252C%2520we%2520contribute%2520a%2520new%2520benchmark%2520named%2520%255Ctextit%257BOpenScan%257D%252C%2520which%250Aconsists%2520of%25203D%2520object%2520attributes%2520across%2520eight%2520representative%2520linguistic%250Aaspects%252C%2520including%2520affordance%252C%2520property%252C%2520and%2520material.%2520We%2520further%2520evaluate%250Astate-of-the-art%2520OV-3D%2520methods%2520on%2520our%2520OpenScan%2520benchmark%2520and%2520discover%2520that%250Athese%2520methods%2520struggle%2520to%2520comprehend%2520the%2520abstract%2520vocabularies%2520of%2520the%2520GOV-3D%250Atask%252C%2520a%2520challenge%2520that%2520cannot%2520be%2520addressed%2520simply%2520by%2520scaling%2520up%2520object%2520classes%250Aduring%2520training.%2520We%2520highlight%2520the%2520limitations%2520of%2520existing%2520methodologies%2520and%250Aexplore%2520promising%2520directions%2520to%2520overcome%2520the%2520identified%2520shortcomings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11030v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenScan%3A%20A%20Benchmark%20for%20Generalized%20Open-Vocabulary%203D%20Scene%0A%20%20Understanding&entry.906535625=Youjun%20Zhao%20and%20Jiaying%20Lin%20and%20Shuquan%20Ye%20and%20Qianshi%20Pang%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Open-vocabulary%203D%20scene%20understanding%20%28OV-3D%29%20aims%20to%20localize%20and%20classify%0Anovel%20objects%20beyond%20the%20closed%20set%20of%20object%20classes.%20However%2C%20existing%0Aapproaches%20and%20benchmarks%20primarily%20focus%20on%20the%20open%20vocabulary%20problem%20within%0Athe%20context%20of%20object%20classes%2C%20which%20is%20insufficient%20in%20providing%20a%20holistic%0Aevaluation%20to%20what%20extent%20a%20model%20understands%20the%203D%20scene.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20more%20challenging%20task%20called%20Generalized%20Open-Vocabulary%203D%20Scene%0AUnderstanding%20%28GOV-3D%29%20to%20explore%20the%20open%20vocabulary%20problem%20beyond%20object%0Aclasses.%20It%20encompasses%20an%20open%20and%20diverse%20set%20of%20generalized%20knowledge%2C%0Aexpressed%20as%20linguistic%20queries%20of%20fine-grained%20and%20object-specific%20attributes.%0ATo%20this%20end%2C%20we%20contribute%20a%20new%20benchmark%20named%20%5Ctextit%7BOpenScan%7D%2C%20which%0Aconsists%20of%203D%20object%20attributes%20across%20eight%20representative%20linguistic%0Aaspects%2C%20including%20affordance%2C%20property%2C%20and%20material.%20We%20further%20evaluate%0Astate-of-the-art%20OV-3D%20methods%20on%20our%20OpenScan%20benchmark%20and%20discover%20that%0Athese%20methods%20struggle%20to%20comprehend%20the%20abstract%20vocabularies%20of%20the%20GOV-3D%0Atask%2C%20a%20challenge%20that%20cannot%20be%20addressed%20simply%20by%20scaling%20up%20object%20classes%0Aduring%20training.%20We%20highlight%20the%20limitations%20of%20existing%20methodologies%20and%0Aexplore%20promising%20directions%20to%20overcome%20the%20identified%20shortcomings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11030v3&entry.124074799=Read"},
{"title": "MienCap: Realtime Performance-Based Facial Animation with Live Mood\n  Dynamics", "author": "Ye Pan and Ruisi Zhang and Jingying Wang and Nengfu Chen and Yilin Qiu and Yu Ding and Kenny Mitchell", "abstract": "  Our purpose is to improve performance-based animation which can drive\nbelievable 3D stylized characters that are truly perceptual. By combining\ntraditional blendshape animation techniques with multiple machine learning\nmodels, we present both non-real time and real time solutions which drive\ncharacter expressions in a geometrically consistent and perceptually valid way.\nFor the non-real time system, we propose a 3D emotion transfer network makes\nuse of a 2D human image to generate a stylized 3D rig parameters. For the real\ntime system, we propose a blendshape adaption network which generates the\ncharacter rig parameter motions with geometric consistency and temporally\nstability. We demonstrate the effectiveness of our system by comparing to a\ncommercial product Faceware. Results reveal that ratings of the recognition,\nintensity, and attractiveness of expressions depicted for animated characters\nvia our systems are statistically higher than Faceware. Our results may be\nimplemented into the animation pipeline, and provide animators with a system\nfor creating the expressions they wish to use more quickly and accurately.\n", "link": "http://arxiv.org/abs/2508.04687v1", "date": "2025-08-06", "relevancy": 3.19, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6576}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6287}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MienCap%3A%20Realtime%20Performance-Based%20Facial%20Animation%20with%20Live%20Mood%0A%20%20Dynamics&body=Title%3A%20MienCap%3A%20Realtime%20Performance-Based%20Facial%20Animation%20with%20Live%20Mood%0A%20%20Dynamics%0AAuthor%3A%20Ye%20Pan%20and%20Ruisi%20Zhang%20and%20Jingying%20Wang%20and%20Nengfu%20Chen%20and%20Yilin%20Qiu%20and%20Yu%20Ding%20and%20Kenny%20Mitchell%0AAbstract%3A%20%20%20Our%20purpose%20is%20to%20improve%20performance-based%20animation%20which%20can%20drive%0Abelievable%203D%20stylized%20characters%20that%20are%20truly%20perceptual.%20By%20combining%0Atraditional%20blendshape%20animation%20techniques%20with%20multiple%20machine%20learning%0Amodels%2C%20we%20present%20both%20non-real%20time%20and%20real%20time%20solutions%20which%20drive%0Acharacter%20expressions%20in%20a%20geometrically%20consistent%20and%20perceptually%20valid%20way.%0AFor%20the%20non-real%20time%20system%2C%20we%20propose%20a%203D%20emotion%20transfer%20network%20makes%0Ause%20of%20a%202D%20human%20image%20to%20generate%20a%20stylized%203D%20rig%20parameters.%20For%20the%20real%0Atime%20system%2C%20we%20propose%20a%20blendshape%20adaption%20network%20which%20generates%20the%0Acharacter%20rig%20parameter%20motions%20with%20geometric%20consistency%20and%20temporally%0Astability.%20We%20demonstrate%20the%20effectiveness%20of%20our%20system%20by%20comparing%20to%20a%0Acommercial%20product%20Faceware.%20Results%20reveal%20that%20ratings%20of%20the%20recognition%2C%0Aintensity%2C%20and%20attractiveness%20of%20expressions%20depicted%20for%20animated%20characters%0Avia%20our%20systems%20are%20statistically%20higher%20than%20Faceware.%20Our%20results%20may%20be%0Aimplemented%20into%20the%20animation%20pipeline%2C%20and%20provide%20animators%20with%20a%20system%0Afor%20creating%20the%20expressions%20they%20wish%20to%20use%20more%20quickly%20and%20accurately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMienCap%253A%2520Realtime%2520Performance-Based%2520Facial%2520Animation%2520with%2520Live%2520Mood%250A%2520%2520Dynamics%26entry.906535625%3DYe%2520Pan%2520and%2520Ruisi%2520Zhang%2520and%2520Jingying%2520Wang%2520and%2520Nengfu%2520Chen%2520and%2520Yilin%2520Qiu%2520and%2520Yu%2520Ding%2520and%2520Kenny%2520Mitchell%26entry.1292438233%3D%2520%2520Our%2520purpose%2520is%2520to%2520improve%2520performance-based%2520animation%2520which%2520can%2520drive%250Abelievable%25203D%2520stylized%2520characters%2520that%2520are%2520truly%2520perceptual.%2520By%2520combining%250Atraditional%2520blendshape%2520animation%2520techniques%2520with%2520multiple%2520machine%2520learning%250Amodels%252C%2520we%2520present%2520both%2520non-real%2520time%2520and%2520real%2520time%2520solutions%2520which%2520drive%250Acharacter%2520expressions%2520in%2520a%2520geometrically%2520consistent%2520and%2520perceptually%2520valid%2520way.%250AFor%2520the%2520non-real%2520time%2520system%252C%2520we%2520propose%2520a%25203D%2520emotion%2520transfer%2520network%2520makes%250Ause%2520of%2520a%25202D%2520human%2520image%2520to%2520generate%2520a%2520stylized%25203D%2520rig%2520parameters.%2520For%2520the%2520real%250Atime%2520system%252C%2520we%2520propose%2520a%2520blendshape%2520adaption%2520network%2520which%2520generates%2520the%250Acharacter%2520rig%2520parameter%2520motions%2520with%2520geometric%2520consistency%2520and%2520temporally%250Astability.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520system%2520by%2520comparing%2520to%2520a%250Acommercial%2520product%2520Faceware.%2520Results%2520reveal%2520that%2520ratings%2520of%2520the%2520recognition%252C%250Aintensity%252C%2520and%2520attractiveness%2520of%2520expressions%2520depicted%2520for%2520animated%2520characters%250Avia%2520our%2520systems%2520are%2520statistically%2520higher%2520than%2520Faceware.%2520Our%2520results%2520may%2520be%250Aimplemented%2520into%2520the%2520animation%2520pipeline%252C%2520and%2520provide%2520animators%2520with%2520a%2520system%250Afor%2520creating%2520the%2520expressions%2520they%2520wish%2520to%2520use%2520more%2520quickly%2520and%2520accurately.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MienCap%3A%20Realtime%20Performance-Based%20Facial%20Animation%20with%20Live%20Mood%0A%20%20Dynamics&entry.906535625=Ye%20Pan%20and%20Ruisi%20Zhang%20and%20Jingying%20Wang%20and%20Nengfu%20Chen%20and%20Yilin%20Qiu%20and%20Yu%20Ding%20and%20Kenny%20Mitchell&entry.1292438233=%20%20Our%20purpose%20is%20to%20improve%20performance-based%20animation%20which%20can%20drive%0Abelievable%203D%20stylized%20characters%20that%20are%20truly%20perceptual.%20By%20combining%0Atraditional%20blendshape%20animation%20techniques%20with%20multiple%20machine%20learning%0Amodels%2C%20we%20present%20both%20non-real%20time%20and%20real%20time%20solutions%20which%20drive%0Acharacter%20expressions%20in%20a%20geometrically%20consistent%20and%20perceptually%20valid%20way.%0AFor%20the%20non-real%20time%20system%2C%20we%20propose%20a%203D%20emotion%20transfer%20network%20makes%0Ause%20of%20a%202D%20human%20image%20to%20generate%20a%20stylized%203D%20rig%20parameters.%20For%20the%20real%0Atime%20system%2C%20we%20propose%20a%20blendshape%20adaption%20network%20which%20generates%20the%0Acharacter%20rig%20parameter%20motions%20with%20geometric%20consistency%20and%20temporally%0Astability.%20We%20demonstrate%20the%20effectiveness%20of%20our%20system%20by%20comparing%20to%20a%0Acommercial%20product%20Faceware.%20Results%20reveal%20that%20ratings%20of%20the%20recognition%2C%0Aintensity%2C%20and%20attractiveness%20of%20expressions%20depicted%20for%20animated%20characters%0Avia%20our%20systems%20are%20statistically%20higher%20than%20Faceware.%20Our%20results%20may%20be%0Aimplemented%20into%20the%20animation%20pipeline%2C%20and%20provide%20animators%20with%20a%20system%0Afor%20creating%20the%20expressions%20they%20wish%20to%20use%20more%20quickly%20and%20accurately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04687v1&entry.124074799=Read"},
{"title": "EncQA: Benchmarking Vision-Language Models on Visual Encodings for\n  Charts", "author": "Kushin Mukherjee and Donghao Ren and Dominik Moritz and Yannick Assogba", "abstract": "  Multimodal vision-language models (VLMs) continue to achieve ever-improving\nscores on chart understanding benchmarks. Yet, we find that this progress does\nnot fully capture the breadth of visual reasoning capabilities essential for\ninterpreting charts. We introduce EncQA, a novel benchmark informed by the\nvisualization literature, designed to provide systematic coverage of visual\nencodings and analytic tasks that are crucial for chart understanding. EncQA\nprovides 2,076 synthetic question-answer pairs, enabling balanced coverage of\nsix visual encoding channels (position, length, area, color quantitative, color\nnominal, and shape) and eight tasks (find extrema, retrieve value, find\nanomaly, filter values, compute derived value exact, compute derived value\nrelative, correlate values, and correlate values relative). Our evaluation of 9\nstate-of-the-art VLMs reveals that performance varies significantly across\nencodings within the same task, as well as across tasks. Contrary to\nexpectations, we observe that performance does not improve with model size for\nmany task-encoding pairs. Our results suggest that advancing chart\nunderstanding requires targeted strategies addressing specific visual reasoning\ngaps, rather than solely scaling up model or dataset size.\n", "link": "http://arxiv.org/abs/2508.04650v1", "date": "2025-08-06", "relevancy": 3.074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6652}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6652}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EncQA%3A%20Benchmarking%20Vision-Language%20Models%20on%20Visual%20Encodings%20for%0A%20%20Charts&body=Title%3A%20EncQA%3A%20Benchmarking%20Vision-Language%20Models%20on%20Visual%20Encodings%20for%0A%20%20Charts%0AAuthor%3A%20Kushin%20Mukherjee%20and%20Donghao%20Ren%20and%20Dominik%20Moritz%20and%20Yannick%20Assogba%0AAbstract%3A%20%20%20Multimodal%20vision-language%20models%20%28VLMs%29%20continue%20to%20achieve%20ever-improving%0Ascores%20on%20chart%20understanding%20benchmarks.%20Yet%2C%20we%20find%20that%20this%20progress%20does%0Anot%20fully%20capture%20the%20breadth%20of%20visual%20reasoning%20capabilities%20essential%20for%0Ainterpreting%20charts.%20We%20introduce%20EncQA%2C%20a%20novel%20benchmark%20informed%20by%20the%0Avisualization%20literature%2C%20designed%20to%20provide%20systematic%20coverage%20of%20visual%0Aencodings%20and%20analytic%20tasks%20that%20are%20crucial%20for%20chart%20understanding.%20EncQA%0Aprovides%202%2C076%20synthetic%20question-answer%20pairs%2C%20enabling%20balanced%20coverage%20of%0Asix%20visual%20encoding%20channels%20%28position%2C%20length%2C%20area%2C%20color%20quantitative%2C%20color%0Anominal%2C%20and%20shape%29%20and%20eight%20tasks%20%28find%20extrema%2C%20retrieve%20value%2C%20find%0Aanomaly%2C%20filter%20values%2C%20compute%20derived%20value%20exact%2C%20compute%20derived%20value%0Arelative%2C%20correlate%20values%2C%20and%20correlate%20values%20relative%29.%20Our%20evaluation%20of%209%0Astate-of-the-art%20VLMs%20reveals%20that%20performance%20varies%20significantly%20across%0Aencodings%20within%20the%20same%20task%2C%20as%20well%20as%20across%20tasks.%20Contrary%20to%0Aexpectations%2C%20we%20observe%20that%20performance%20does%20not%20improve%20with%20model%20size%20for%0Amany%20task-encoding%20pairs.%20Our%20results%20suggest%20that%20advancing%20chart%0Aunderstanding%20requires%20targeted%20strategies%20addressing%20specific%20visual%20reasoning%0Agaps%2C%20rather%20than%20solely%20scaling%20up%20model%20or%20dataset%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncQA%253A%2520Benchmarking%2520Vision-Language%2520Models%2520on%2520Visual%2520Encodings%2520for%250A%2520%2520Charts%26entry.906535625%3DKushin%2520Mukherjee%2520and%2520Donghao%2520Ren%2520and%2520Dominik%2520Moritz%2520and%2520Yannick%2520Assogba%26entry.1292438233%3D%2520%2520Multimodal%2520vision-language%2520models%2520%2528VLMs%2529%2520continue%2520to%2520achieve%2520ever-improving%250Ascores%2520on%2520chart%2520understanding%2520benchmarks.%2520Yet%252C%2520we%2520find%2520that%2520this%2520progress%2520does%250Anot%2520fully%2520capture%2520the%2520breadth%2520of%2520visual%2520reasoning%2520capabilities%2520essential%2520for%250Ainterpreting%2520charts.%2520We%2520introduce%2520EncQA%252C%2520a%2520novel%2520benchmark%2520informed%2520by%2520the%250Avisualization%2520literature%252C%2520designed%2520to%2520provide%2520systematic%2520coverage%2520of%2520visual%250Aencodings%2520and%2520analytic%2520tasks%2520that%2520are%2520crucial%2520for%2520chart%2520understanding.%2520EncQA%250Aprovides%25202%252C076%2520synthetic%2520question-answer%2520pairs%252C%2520enabling%2520balanced%2520coverage%2520of%250Asix%2520visual%2520encoding%2520channels%2520%2528position%252C%2520length%252C%2520area%252C%2520color%2520quantitative%252C%2520color%250Anominal%252C%2520and%2520shape%2529%2520and%2520eight%2520tasks%2520%2528find%2520extrema%252C%2520retrieve%2520value%252C%2520find%250Aanomaly%252C%2520filter%2520values%252C%2520compute%2520derived%2520value%2520exact%252C%2520compute%2520derived%2520value%250Arelative%252C%2520correlate%2520values%252C%2520and%2520correlate%2520values%2520relative%2529.%2520Our%2520evaluation%2520of%25209%250Astate-of-the-art%2520VLMs%2520reveals%2520that%2520performance%2520varies%2520significantly%2520across%250Aencodings%2520within%2520the%2520same%2520task%252C%2520as%2520well%2520as%2520across%2520tasks.%2520Contrary%2520to%250Aexpectations%252C%2520we%2520observe%2520that%2520performance%2520does%2520not%2520improve%2520with%2520model%2520size%2520for%250Amany%2520task-encoding%2520pairs.%2520Our%2520results%2520suggest%2520that%2520advancing%2520chart%250Aunderstanding%2520requires%2520targeted%2520strategies%2520addressing%2520specific%2520visual%2520reasoning%250Agaps%252C%2520rather%2520than%2520solely%2520scaling%2520up%2520model%2520or%2520dataset%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EncQA%3A%20Benchmarking%20Vision-Language%20Models%20on%20Visual%20Encodings%20for%0A%20%20Charts&entry.906535625=Kushin%20Mukherjee%20and%20Donghao%20Ren%20and%20Dominik%20Moritz%20and%20Yannick%20Assogba&entry.1292438233=%20%20Multimodal%20vision-language%20models%20%28VLMs%29%20continue%20to%20achieve%20ever-improving%0Ascores%20on%20chart%20understanding%20benchmarks.%20Yet%2C%20we%20find%20that%20this%20progress%20does%0Anot%20fully%20capture%20the%20breadth%20of%20visual%20reasoning%20capabilities%20essential%20for%0Ainterpreting%20charts.%20We%20introduce%20EncQA%2C%20a%20novel%20benchmark%20informed%20by%20the%0Avisualization%20literature%2C%20designed%20to%20provide%20systematic%20coverage%20of%20visual%0Aencodings%20and%20analytic%20tasks%20that%20are%20crucial%20for%20chart%20understanding.%20EncQA%0Aprovides%202%2C076%20synthetic%20question-answer%20pairs%2C%20enabling%20balanced%20coverage%20of%0Asix%20visual%20encoding%20channels%20%28position%2C%20length%2C%20area%2C%20color%20quantitative%2C%20color%0Anominal%2C%20and%20shape%29%20and%20eight%20tasks%20%28find%20extrema%2C%20retrieve%20value%2C%20find%0Aanomaly%2C%20filter%20values%2C%20compute%20derived%20value%20exact%2C%20compute%20derived%20value%0Arelative%2C%20correlate%20values%2C%20and%20correlate%20values%20relative%29.%20Our%20evaluation%20of%209%0Astate-of-the-art%20VLMs%20reveals%20that%20performance%20varies%20significantly%20across%0Aencodings%20within%20the%20same%20task%2C%20as%20well%20as%20across%20tasks.%20Contrary%20to%0Aexpectations%2C%20we%20observe%20that%20performance%20does%20not%20improve%20with%20model%20size%20for%0Amany%20task-encoding%20pairs.%20Our%20results%20suggest%20that%20advancing%20chart%0Aunderstanding%20requires%20targeted%20strategies%20addressing%20specific%20visual%20reasoning%0Agaps%2C%20rather%20than%20solely%20scaling%20up%20model%20or%20dataset%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04650v1&entry.124074799=Read"},
{"title": "EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal\n  and Multi-Task Human Animation", "author": "Rang Meng and Yan Wang and Weipeng Wu and Ruobing Zheng and Yuming Li and Chenguang Ma", "abstract": "  Recent work on human animation usually incorporates large-scale video models,\nthereby achieving more vivid performance. However, the practical use of such\nmethods is hindered by the slow inference speed and high computational demands.\nMoreover, traditional work typically employs separate models for each animation\ntask, increasing costs in multi-task scenarios and worsening the dilemma. To\naddress these limitations, we introduce EchoMimicV3, an efficient framework\nthat unifies multi-task and multi-modal human animation. At the core of\nEchoMimicV3 lies a threefold design: a Soup-of-Tasks paradigm, a Soup-of-Modals\nparadigm, and a novel training and inference strategy. The Soup-of-Tasks\nleverages multi-task mask inputs and a counter-intuitive task allocation\nstrategy to achieve multi-task gains without multi-model pains. Meanwhile, the\nSoup-of-Modals introduces a Coupled-Decoupled Multi-Modal Cross Attention\nmodule to inject multi-modal conditions, complemented by a Multi-Modal Timestep\nPhase-aware Dynamical Allocation mechanism to modulate multi-modal mixtures.\nBesides, we propose Negative Direct Preference Optimization, Phase-aware\nNegative Classifier-Free Guidance (CFG), and Long Video CFG, which ensure\nstable training and inference. Extensive experiments and analyses demonstrate\nthat EchoMimicV3, with a minimal model size of 1.3 billion parameters, achieves\ncompetitive performance in both quantitative and qualitative evaluations. We\nare committed to open-sourcing our code for community use.\n", "link": "http://arxiv.org/abs/2507.03905v3", "date": "2025-08-06", "relevancy": 2.9885, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6287}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5995}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoMimicV3%3A%201.3B%20Parameters%20are%20All%20You%20Need%20for%20Unified%20Multi-Modal%0A%20%20and%20Multi-Task%20Human%20Animation&body=Title%3A%20EchoMimicV3%3A%201.3B%20Parameters%20are%20All%20You%20Need%20for%20Unified%20Multi-Modal%0A%20%20and%20Multi-Task%20Human%20Animation%0AAuthor%3A%20Rang%20Meng%20and%20Yan%20Wang%20and%20Weipeng%20Wu%20and%20Ruobing%20Zheng%20and%20Yuming%20Li%20and%20Chenguang%20Ma%0AAbstract%3A%20%20%20Recent%20work%20on%20human%20animation%20usually%20incorporates%20large-scale%20video%20models%2C%0Athereby%20achieving%20more%20vivid%20performance.%20However%2C%20the%20practical%20use%20of%20such%0Amethods%20is%20hindered%20by%20the%20slow%20inference%20speed%20and%20high%20computational%20demands.%0AMoreover%2C%20traditional%20work%20typically%20employs%20separate%20models%20for%20each%20animation%0Atask%2C%20increasing%20costs%20in%20multi-task%20scenarios%20and%20worsening%20the%20dilemma.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20EchoMimicV3%2C%20an%20efficient%20framework%0Athat%20unifies%20multi-task%20and%20multi-modal%20human%20animation.%20At%20the%20core%20of%0AEchoMimicV3%20lies%20a%20threefold%20design%3A%20a%20Soup-of-Tasks%20paradigm%2C%20a%20Soup-of-Modals%0Aparadigm%2C%20and%20a%20novel%20training%20and%20inference%20strategy.%20The%20Soup-of-Tasks%0Aleverages%20multi-task%20mask%20inputs%20and%20a%20counter-intuitive%20task%20allocation%0Astrategy%20to%20achieve%20multi-task%20gains%20without%20multi-model%20pains.%20Meanwhile%2C%20the%0ASoup-of-Modals%20introduces%20a%20Coupled-Decoupled%20Multi-Modal%20Cross%20Attention%0Amodule%20to%20inject%20multi-modal%20conditions%2C%20complemented%20by%20a%20Multi-Modal%20Timestep%0APhase-aware%20Dynamical%20Allocation%20mechanism%20to%20modulate%20multi-modal%20mixtures.%0ABesides%2C%20we%20propose%20Negative%20Direct%20Preference%20Optimization%2C%20Phase-aware%0ANegative%20Classifier-Free%20Guidance%20%28CFG%29%2C%20and%20Long%20Video%20CFG%2C%20which%20ensure%0Astable%20training%20and%20inference.%20Extensive%20experiments%20and%20analyses%20demonstrate%0Athat%20EchoMimicV3%2C%20with%20a%20minimal%20model%20size%20of%201.3%20billion%20parameters%2C%20achieves%0Acompetitive%20performance%20in%20both%20quantitative%20and%20qualitative%20evaluations.%20We%0Aare%20committed%20to%20open-sourcing%20our%20code%20for%20community%20use.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03905v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoMimicV3%253A%25201.3B%2520Parameters%2520are%2520All%2520You%2520Need%2520for%2520Unified%2520Multi-Modal%250A%2520%2520and%2520Multi-Task%2520Human%2520Animation%26entry.906535625%3DRang%2520Meng%2520and%2520Yan%2520Wang%2520and%2520Weipeng%2520Wu%2520and%2520Ruobing%2520Zheng%2520and%2520Yuming%2520Li%2520and%2520Chenguang%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520work%2520on%2520human%2520animation%2520usually%2520incorporates%2520large-scale%2520video%2520models%252C%250Athereby%2520achieving%2520more%2520vivid%2520performance.%2520However%252C%2520the%2520practical%2520use%2520of%2520such%250Amethods%2520is%2520hindered%2520by%2520the%2520slow%2520inference%2520speed%2520and%2520high%2520computational%2520demands.%250AMoreover%252C%2520traditional%2520work%2520typically%2520employs%2520separate%2520models%2520for%2520each%2520animation%250Atask%252C%2520increasing%2520costs%2520in%2520multi-task%2520scenarios%2520and%2520worsening%2520the%2520dilemma.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520introduce%2520EchoMimicV3%252C%2520an%2520efficient%2520framework%250Athat%2520unifies%2520multi-task%2520and%2520multi-modal%2520human%2520animation.%2520At%2520the%2520core%2520of%250AEchoMimicV3%2520lies%2520a%2520threefold%2520design%253A%2520a%2520Soup-of-Tasks%2520paradigm%252C%2520a%2520Soup-of-Modals%250Aparadigm%252C%2520and%2520a%2520novel%2520training%2520and%2520inference%2520strategy.%2520The%2520Soup-of-Tasks%250Aleverages%2520multi-task%2520mask%2520inputs%2520and%2520a%2520counter-intuitive%2520task%2520allocation%250Astrategy%2520to%2520achieve%2520multi-task%2520gains%2520without%2520multi-model%2520pains.%2520Meanwhile%252C%2520the%250ASoup-of-Modals%2520introduces%2520a%2520Coupled-Decoupled%2520Multi-Modal%2520Cross%2520Attention%250Amodule%2520to%2520inject%2520multi-modal%2520conditions%252C%2520complemented%2520by%2520a%2520Multi-Modal%2520Timestep%250APhase-aware%2520Dynamical%2520Allocation%2520mechanism%2520to%2520modulate%2520multi-modal%2520mixtures.%250ABesides%252C%2520we%2520propose%2520Negative%2520Direct%2520Preference%2520Optimization%252C%2520Phase-aware%250ANegative%2520Classifier-Free%2520Guidance%2520%2528CFG%2529%252C%2520and%2520Long%2520Video%2520CFG%252C%2520which%2520ensure%250Astable%2520training%2520and%2520inference.%2520Extensive%2520experiments%2520and%2520analyses%2520demonstrate%250Athat%2520EchoMimicV3%252C%2520with%2520a%2520minimal%2520model%2520size%2520of%25201.3%2520billion%2520parameters%252C%2520achieves%250Acompetitive%2520performance%2520in%2520both%2520quantitative%2520and%2520qualitative%2520evaluations.%2520We%250Aare%2520committed%2520to%2520open-sourcing%2520our%2520code%2520for%2520community%2520use.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03905v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoMimicV3%3A%201.3B%20Parameters%20are%20All%20You%20Need%20for%20Unified%20Multi-Modal%0A%20%20and%20Multi-Task%20Human%20Animation&entry.906535625=Rang%20Meng%20and%20Yan%20Wang%20and%20Weipeng%20Wu%20and%20Ruobing%20Zheng%20and%20Yuming%20Li%20and%20Chenguang%20Ma&entry.1292438233=%20%20Recent%20work%20on%20human%20animation%20usually%20incorporates%20large-scale%20video%20models%2C%0Athereby%20achieving%20more%20vivid%20performance.%20However%2C%20the%20practical%20use%20of%20such%0Amethods%20is%20hindered%20by%20the%20slow%20inference%20speed%20and%20high%20computational%20demands.%0AMoreover%2C%20traditional%20work%20typically%20employs%20separate%20models%20for%20each%20animation%0Atask%2C%20increasing%20costs%20in%20multi-task%20scenarios%20and%20worsening%20the%20dilemma.%20To%0Aaddress%20these%20limitations%2C%20we%20introduce%20EchoMimicV3%2C%20an%20efficient%20framework%0Athat%20unifies%20multi-task%20and%20multi-modal%20human%20animation.%20At%20the%20core%20of%0AEchoMimicV3%20lies%20a%20threefold%20design%3A%20a%20Soup-of-Tasks%20paradigm%2C%20a%20Soup-of-Modals%0Aparadigm%2C%20and%20a%20novel%20training%20and%20inference%20strategy.%20The%20Soup-of-Tasks%0Aleverages%20multi-task%20mask%20inputs%20and%20a%20counter-intuitive%20task%20allocation%0Astrategy%20to%20achieve%20multi-task%20gains%20without%20multi-model%20pains.%20Meanwhile%2C%20the%0ASoup-of-Modals%20introduces%20a%20Coupled-Decoupled%20Multi-Modal%20Cross%20Attention%0Amodule%20to%20inject%20multi-modal%20conditions%2C%20complemented%20by%20a%20Multi-Modal%20Timestep%0APhase-aware%20Dynamical%20Allocation%20mechanism%20to%20modulate%20multi-modal%20mixtures.%0ABesides%2C%20we%20propose%20Negative%20Direct%20Preference%20Optimization%2C%20Phase-aware%0ANegative%20Classifier-Free%20Guidance%20%28CFG%29%2C%20and%20Long%20Video%20CFG%2C%20which%20ensure%0Astable%20training%20and%20inference.%20Extensive%20experiments%20and%20analyses%20demonstrate%0Athat%20EchoMimicV3%2C%20with%20a%20minimal%20model%20size%20of%201.3%20billion%20parameters%2C%20achieves%0Acompetitive%20performance%20in%20both%20quantitative%20and%20qualitative%20evaluations.%20We%0Aare%20committed%20to%20open-sourcing%20our%20code%20for%20community%20use.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03905v3&entry.124074799=Read"},
{"title": "OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment", "author": "Tongfan Guan and Jiaxin Guo and Chen Wang and Yun-Hui Liu", "abstract": "  Monocular and stereo depth estimation offer complementary strengths:\nmonocular methods capture rich contextual priors but lack geometric precision,\nwhile stereo approaches leverage epipolar geometry yet struggle with\nambiguities such as reflective or textureless surfaces. Despite post-hoc\nsynergies, these paradigms remain largely disjoint in practice. We introduce\nOmniDepth, a unified framework that bridges both through iterative\nbidirectional alignment of their latent representations. At its core, a novel\ncross-attentive alignment mechanism dynamically synchronizes monocular\ncontextual cues with stereo hypothesis representations during stereo reasoning.\nThis mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by\ninjecting monocular structure priors while refining monocular depth with stereo\ngeometry within a single network. Extensive experiments demonstrate\nstate-of-the-art results: \\textbf{OmniDepth reduces zero-shot generalization\nerror by $\\!>\\!40\\%$ on Middlebury and ETH3D}, while addressing longstanding\nfailures on transparent and reflective surfaces. By harmonizing multi-view\ngeometry with monocular context, OmniDepth enables robust 3D perception that\ntranscends modality-specific limitations. Codes available at\nhttps://github.com/aeolusguan/OmniDepth.\n", "link": "http://arxiv.org/abs/2508.04611v1", "date": "2025-08-06", "relevancy": 2.9848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5996}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5917}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniDepth%3A%20Bridging%20Monocular%20and%20Stereo%20Reasoning%20with%20Latent%20Alignment&body=Title%3A%20OmniDepth%3A%20Bridging%20Monocular%20and%20Stereo%20Reasoning%20with%20Latent%20Alignment%0AAuthor%3A%20Tongfan%20Guan%20and%20Jiaxin%20Guo%20and%20Chen%20Wang%20and%20Yun-Hui%20Liu%0AAbstract%3A%20%20%20Monocular%20and%20stereo%20depth%20estimation%20offer%20complementary%20strengths%3A%0Amonocular%20methods%20capture%20rich%20contextual%20priors%20but%20lack%20geometric%20precision%2C%0Awhile%20stereo%20approaches%20leverage%20epipolar%20geometry%20yet%20struggle%20with%0Aambiguities%20such%20as%20reflective%20or%20textureless%20surfaces.%20Despite%20post-hoc%0Asynergies%2C%20these%20paradigms%20remain%20largely%20disjoint%20in%20practice.%20We%20introduce%0AOmniDepth%2C%20a%20unified%20framework%20that%20bridges%20both%20through%20iterative%0Abidirectional%20alignment%20of%20their%20latent%20representations.%20At%20its%20core%2C%20a%20novel%0Across-attentive%20alignment%20mechanism%20dynamically%20synchronizes%20monocular%0Acontextual%20cues%20with%20stereo%20hypothesis%20representations%20during%20stereo%20reasoning.%0AThis%20mutual%20alignment%20resolves%20stereo%20ambiguities%20%28e.g.%2C%20specular%20surfaces%29%20by%0Ainjecting%20monocular%20structure%20priors%20while%20refining%20monocular%20depth%20with%20stereo%0Ageometry%20within%20a%20single%20network.%20Extensive%20experiments%20demonstrate%0Astate-of-the-art%20results%3A%20%5Ctextbf%7BOmniDepth%20reduces%20zero-shot%20generalization%0Aerror%20by%20%24%5C%21%3E%5C%2140%5C%25%24%20on%20Middlebury%20and%20ETH3D%7D%2C%20while%20addressing%20longstanding%0Afailures%20on%20transparent%20and%20reflective%20surfaces.%20By%20harmonizing%20multi-view%0Ageometry%20with%20monocular%20context%2C%20OmniDepth%20enables%20robust%203D%20perception%20that%0Atranscends%20modality-specific%20limitations.%20Codes%20available%20at%0Ahttps%3A//github.com/aeolusguan/OmniDepth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniDepth%253A%2520Bridging%2520Monocular%2520and%2520Stereo%2520Reasoning%2520with%2520Latent%2520Alignment%26entry.906535625%3DTongfan%2520Guan%2520and%2520Jiaxin%2520Guo%2520and%2520Chen%2520Wang%2520and%2520Yun-Hui%2520Liu%26entry.1292438233%3D%2520%2520Monocular%2520and%2520stereo%2520depth%2520estimation%2520offer%2520complementary%2520strengths%253A%250Amonocular%2520methods%2520capture%2520rich%2520contextual%2520priors%2520but%2520lack%2520geometric%2520precision%252C%250Awhile%2520stereo%2520approaches%2520leverage%2520epipolar%2520geometry%2520yet%2520struggle%2520with%250Aambiguities%2520such%2520as%2520reflective%2520or%2520textureless%2520surfaces.%2520Despite%2520post-hoc%250Asynergies%252C%2520these%2520paradigms%2520remain%2520largely%2520disjoint%2520in%2520practice.%2520We%2520introduce%250AOmniDepth%252C%2520a%2520unified%2520framework%2520that%2520bridges%2520both%2520through%2520iterative%250Abidirectional%2520alignment%2520of%2520their%2520latent%2520representations.%2520At%2520its%2520core%252C%2520a%2520novel%250Across-attentive%2520alignment%2520mechanism%2520dynamically%2520synchronizes%2520monocular%250Acontextual%2520cues%2520with%2520stereo%2520hypothesis%2520representations%2520during%2520stereo%2520reasoning.%250AThis%2520mutual%2520alignment%2520resolves%2520stereo%2520ambiguities%2520%2528e.g.%252C%2520specular%2520surfaces%2529%2520by%250Ainjecting%2520monocular%2520structure%2520priors%2520while%2520refining%2520monocular%2520depth%2520with%2520stereo%250Ageometry%2520within%2520a%2520single%2520network.%2520Extensive%2520experiments%2520demonstrate%250Astate-of-the-art%2520results%253A%2520%255Ctextbf%257BOmniDepth%2520reduces%2520zero-shot%2520generalization%250Aerror%2520by%2520%2524%255C%2521%253E%255C%252140%255C%2525%2524%2520on%2520Middlebury%2520and%2520ETH3D%257D%252C%2520while%2520addressing%2520longstanding%250Afailures%2520on%2520transparent%2520and%2520reflective%2520surfaces.%2520By%2520harmonizing%2520multi-view%250Ageometry%2520with%2520monocular%2520context%252C%2520OmniDepth%2520enables%2520robust%25203D%2520perception%2520that%250Atranscends%2520modality-specific%2520limitations.%2520Codes%2520available%2520at%250Ahttps%253A//github.com/aeolusguan/OmniDepth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDepth%3A%20Bridging%20Monocular%20and%20Stereo%20Reasoning%20with%20Latent%20Alignment&entry.906535625=Tongfan%20Guan%20and%20Jiaxin%20Guo%20and%20Chen%20Wang%20and%20Yun-Hui%20Liu&entry.1292438233=%20%20Monocular%20and%20stereo%20depth%20estimation%20offer%20complementary%20strengths%3A%0Amonocular%20methods%20capture%20rich%20contextual%20priors%20but%20lack%20geometric%20precision%2C%0Awhile%20stereo%20approaches%20leverage%20epipolar%20geometry%20yet%20struggle%20with%0Aambiguities%20such%20as%20reflective%20or%20textureless%20surfaces.%20Despite%20post-hoc%0Asynergies%2C%20these%20paradigms%20remain%20largely%20disjoint%20in%20practice.%20We%20introduce%0AOmniDepth%2C%20a%20unified%20framework%20that%20bridges%20both%20through%20iterative%0Abidirectional%20alignment%20of%20their%20latent%20representations.%20At%20its%20core%2C%20a%20novel%0Across-attentive%20alignment%20mechanism%20dynamically%20synchronizes%20monocular%0Acontextual%20cues%20with%20stereo%20hypothesis%20representations%20during%20stereo%20reasoning.%0AThis%20mutual%20alignment%20resolves%20stereo%20ambiguities%20%28e.g.%2C%20specular%20surfaces%29%20by%0Ainjecting%20monocular%20structure%20priors%20while%20refining%20monocular%20depth%20with%20stereo%0Ageometry%20within%20a%20single%20network.%20Extensive%20experiments%20demonstrate%0Astate-of-the-art%20results%3A%20%5Ctextbf%7BOmniDepth%20reduces%20zero-shot%20generalization%0Aerror%20by%20%24%5C%21%3E%5C%2140%5C%25%24%20on%20Middlebury%20and%20ETH3D%7D%2C%20while%20addressing%20longstanding%0Afailures%20on%20transparent%20and%20reflective%20surfaces.%20By%20harmonizing%20multi-view%0Ageometry%20with%20monocular%20context%2C%20OmniDepth%20enables%20robust%203D%20perception%20that%0Atranscends%20modality-specific%20limitations.%20Codes%20available%20at%0Ahttps%3A//github.com/aeolusguan/OmniDepth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04611v1&entry.124074799=Read"},
{"title": "MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars\n  from Monocular Videos", "author": "Daisheng Jin and Ying He", "abstract": "  Reconstructing realistic 3D human avatars from monocular videos is a\nchallenging task due to the limited geometric information and complex non-rigid\nmotion involved. We present MonoCloth, a new method for reconstructing and\nanimating clothed human avatars from monocular videos. To overcome the\nlimitations of monocular input, we introduce a part-based decomposition\nstrategy that separates the avatar into body, face, hands, and clothing. This\ndesign reflects the varying levels of reconstruction difficulty and deformation\ncomplexity across these components. Specifically, we focus on detailed geometry\nrecovery for the face and hands. For clothing, we propose a dedicated cloth\nsimulation module that captures garment deformation using temporal motion cues\nand geometric constraints. Experimental results demonstrate that MonoCloth\nimproves both visual reconstruction quality and animation realism compared to\nexisting methods. Furthermore, thanks to its part-based design, MonoCloth also\nsupports additional tasks such as clothing transfer, underscoring its\nversatility and practical utility.\n", "link": "http://arxiv.org/abs/2508.04505v1", "date": "2025-08-06", "relevancy": 2.9773, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6363}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5751}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoCloth%3A%20Reconstruction%20and%20Animation%20of%20Cloth-Decoupled%20Human%20Avatars%0A%20%20from%20Monocular%20Videos&body=Title%3A%20MonoCloth%3A%20Reconstruction%20and%20Animation%20of%20Cloth-Decoupled%20Human%20Avatars%0A%20%20from%20Monocular%20Videos%0AAuthor%3A%20Daisheng%20Jin%20and%20Ying%20He%0AAbstract%3A%20%20%20Reconstructing%20realistic%203D%20human%20avatars%20from%20monocular%20videos%20is%20a%0Achallenging%20task%20due%20to%20the%20limited%20geometric%20information%20and%20complex%20non-rigid%0Amotion%20involved.%20We%20present%20MonoCloth%2C%20a%20new%20method%20for%20reconstructing%20and%0Aanimating%20clothed%20human%20avatars%20from%20monocular%20videos.%20To%20overcome%20the%0Alimitations%20of%20monocular%20input%2C%20we%20introduce%20a%20part-based%20decomposition%0Astrategy%20that%20separates%20the%20avatar%20into%20body%2C%20face%2C%20hands%2C%20and%20clothing.%20This%0Adesign%20reflects%20the%20varying%20levels%20of%20reconstruction%20difficulty%20and%20deformation%0Acomplexity%20across%20these%20components.%20Specifically%2C%20we%20focus%20on%20detailed%20geometry%0Arecovery%20for%20the%20face%20and%20hands.%20For%20clothing%2C%20we%20propose%20a%20dedicated%20cloth%0Asimulation%20module%20that%20captures%20garment%20deformation%20using%20temporal%20motion%20cues%0Aand%20geometric%20constraints.%20Experimental%20results%20demonstrate%20that%20MonoCloth%0Aimproves%20both%20visual%20reconstruction%20quality%20and%20animation%20realism%20compared%20to%0Aexisting%20methods.%20Furthermore%2C%20thanks%20to%20its%20part-based%20design%2C%20MonoCloth%20also%0Asupports%20additional%20tasks%20such%20as%20clothing%20transfer%2C%20underscoring%20its%0Aversatility%20and%20practical%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoCloth%253A%2520Reconstruction%2520and%2520Animation%2520of%2520Cloth-Decoupled%2520Human%2520Avatars%250A%2520%2520from%2520Monocular%2520Videos%26entry.906535625%3DDaisheng%2520Jin%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520Reconstructing%2520realistic%25203D%2520human%2520avatars%2520from%2520monocular%2520videos%2520is%2520a%250Achallenging%2520task%2520due%2520to%2520the%2520limited%2520geometric%2520information%2520and%2520complex%2520non-rigid%250Amotion%2520involved.%2520We%2520present%2520MonoCloth%252C%2520a%2520new%2520method%2520for%2520reconstructing%2520and%250Aanimating%2520clothed%2520human%2520avatars%2520from%2520monocular%2520videos.%2520To%2520overcome%2520the%250Alimitations%2520of%2520monocular%2520input%252C%2520we%2520introduce%2520a%2520part-based%2520decomposition%250Astrategy%2520that%2520separates%2520the%2520avatar%2520into%2520body%252C%2520face%252C%2520hands%252C%2520and%2520clothing.%2520This%250Adesign%2520reflects%2520the%2520varying%2520levels%2520of%2520reconstruction%2520difficulty%2520and%2520deformation%250Acomplexity%2520across%2520these%2520components.%2520Specifically%252C%2520we%2520focus%2520on%2520detailed%2520geometry%250Arecovery%2520for%2520the%2520face%2520and%2520hands.%2520For%2520clothing%252C%2520we%2520propose%2520a%2520dedicated%2520cloth%250Asimulation%2520module%2520that%2520captures%2520garment%2520deformation%2520using%2520temporal%2520motion%2520cues%250Aand%2520geometric%2520constraints.%2520Experimental%2520results%2520demonstrate%2520that%2520MonoCloth%250Aimproves%2520both%2520visual%2520reconstruction%2520quality%2520and%2520animation%2520realism%2520compared%2520to%250Aexisting%2520methods.%2520Furthermore%252C%2520thanks%2520to%2520its%2520part-based%2520design%252C%2520MonoCloth%2520also%250Asupports%2520additional%2520tasks%2520such%2520as%2520clothing%2520transfer%252C%2520underscoring%2520its%250Aversatility%2520and%2520practical%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoCloth%3A%20Reconstruction%20and%20Animation%20of%20Cloth-Decoupled%20Human%20Avatars%0A%20%20from%20Monocular%20Videos&entry.906535625=Daisheng%20Jin%20and%20Ying%20He&entry.1292438233=%20%20Reconstructing%20realistic%203D%20human%20avatars%20from%20monocular%20videos%20is%20a%0Achallenging%20task%20due%20to%20the%20limited%20geometric%20information%20and%20complex%20non-rigid%0Amotion%20involved.%20We%20present%20MonoCloth%2C%20a%20new%20method%20for%20reconstructing%20and%0Aanimating%20clothed%20human%20avatars%20from%20monocular%20videos.%20To%20overcome%20the%0Alimitations%20of%20monocular%20input%2C%20we%20introduce%20a%20part-based%20decomposition%0Astrategy%20that%20separates%20the%20avatar%20into%20body%2C%20face%2C%20hands%2C%20and%20clothing.%20This%0Adesign%20reflects%20the%20varying%20levels%20of%20reconstruction%20difficulty%20and%20deformation%0Acomplexity%20across%20these%20components.%20Specifically%2C%20we%20focus%20on%20detailed%20geometry%0Arecovery%20for%20the%20face%20and%20hands.%20For%20clothing%2C%20we%20propose%20a%20dedicated%20cloth%0Asimulation%20module%20that%20captures%20garment%20deformation%20using%20temporal%20motion%20cues%0Aand%20geometric%20constraints.%20Experimental%20results%20demonstrate%20that%20MonoCloth%0Aimproves%20both%20visual%20reconstruction%20quality%20and%20animation%20realism%20compared%20to%0Aexisting%20methods.%20Furthermore%2C%20thanks%20to%20its%20part-based%20design%2C%20MonoCloth%20also%0Asupports%20additional%20tasks%20such%20as%20clothing%20transfer%2C%20underscoring%20its%0Aversatility%20and%20practical%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04505v1&entry.124074799=Read"},
{"title": "Knowledge to Sight: Reasoning over Visual Attributes via Knowledge\n  Decomposition for Abnormality Grounding", "author": "Jun Li and Che Liu and Wenjia Bai and Mingxuan Liu and Rossella Arcucci and Cosmin I. Bercea and Julia A. Schnabel", "abstract": "  In this work, we address the problem of grounding abnormalities in medical\nimages, where the goal is to localize clinical findings based on textual\ndescriptions. While generalist Vision-Language Models (VLMs) excel in natural\ngrounding tasks, they often struggle in the medical domain due to rare,\ncompositional, and domain-specific terms that are poorly aligned with visual\npatterns. Specialized medical VLMs address this challenge via large-scale\ndomain pretraining, but at the cost of substantial annotation and computational\nresources. To overcome these limitations, we propose \\textbf{Knowledge to Sight\n(K2Sight)}, a framework that introduces structured semantic supervision by\ndecomposing clinical concepts into interpretable visual attributes, such as\nshape, density, and anatomical location. These attributes are distilled from\ndomain ontologies and encoded into concise instruction-style prompts, which\nguide region-text alignment during training. Unlike conventional report-level\nsupervision, our approach explicitly bridges domain knowledge and spatial\nstructure, enabling data-efficient training of compact models. We train compact\nmodels with 0.23B and 2B parameters using only 1.5\\% of the data required by\nstate-of-the-art medical VLMs. Despite their small size and limited training\ndata, these models achieve performance on par with or better than 7B+ medical\nVLMs, with up to 9.82\\% improvement in $mAP_{50}$. Code and models:\n\\href{https://lijunrio.github.io/K2Sight/}{\\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.\n", "link": "http://arxiv.org/abs/2508.04572v1", "date": "2025-08-06", "relevancy": 2.9722, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20to%20Sight%3A%20Reasoning%20over%20Visual%20Attributes%20via%20Knowledge%0A%20%20Decomposition%20for%20Abnormality%20Grounding&body=Title%3A%20Knowledge%20to%20Sight%3A%20Reasoning%20over%20Visual%20Attributes%20via%20Knowledge%0A%20%20Decomposition%20for%20Abnormality%20Grounding%0AAuthor%3A%20Jun%20Li%20and%20Che%20Liu%20and%20Wenjia%20Bai%20and%20Mingxuan%20Liu%20and%20Rossella%20Arcucci%20and%20Cosmin%20I.%20Bercea%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20grounding%20abnormalities%20in%20medical%0Aimages%2C%20where%20the%20goal%20is%20to%20localize%20clinical%20findings%20based%20on%20textual%0Adescriptions.%20While%20generalist%20Vision-Language%20Models%20%28VLMs%29%20excel%20in%20natural%0Agrounding%20tasks%2C%20they%20often%20struggle%20in%20the%20medical%20domain%20due%20to%20rare%2C%0Acompositional%2C%20and%20domain-specific%20terms%20that%20are%20poorly%20aligned%20with%20visual%0Apatterns.%20Specialized%20medical%20VLMs%20address%20this%20challenge%20via%20large-scale%0Adomain%20pretraining%2C%20but%20at%20the%20cost%20of%20substantial%20annotation%20and%20computational%0Aresources.%20To%20overcome%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BKnowledge%20to%20Sight%0A%28K2Sight%29%7D%2C%20a%20framework%20that%20introduces%20structured%20semantic%20supervision%20by%0Adecomposing%20clinical%20concepts%20into%20interpretable%20visual%20attributes%2C%20such%20as%0Ashape%2C%20density%2C%20and%20anatomical%20location.%20These%20attributes%20are%20distilled%20from%0Adomain%20ontologies%20and%20encoded%20into%20concise%20instruction-style%20prompts%2C%20which%0Aguide%20region-text%20alignment%20during%20training.%20Unlike%20conventional%20report-level%0Asupervision%2C%20our%20approach%20explicitly%20bridges%20domain%20knowledge%20and%20spatial%0Astructure%2C%20enabling%20data-efficient%20training%20of%20compact%20models.%20We%20train%20compact%0Amodels%20with%200.23B%20and%202B%20parameters%20using%20only%201.5%5C%25%20of%20the%20data%20required%20by%0Astate-of-the-art%20medical%20VLMs.%20Despite%20their%20small%20size%20and%20limited%20training%0Adata%2C%20these%20models%20achieve%20performance%20on%20par%20with%20or%20better%20than%207B%2B%20medical%0AVLMs%2C%20with%20up%20to%209.82%5C%25%20improvement%20in%20%24mAP_%7B50%7D%24.%20Code%20and%20models%3A%0A%5Chref%7Bhttps%3A//lijunrio.github.io/K2Sight/%7D%7B%5Ctextcolor%7BSOTAPink%7D%7Bhttps%3A//lijunrio.github.io/K2Sight/%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520to%2520Sight%253A%2520Reasoning%2520over%2520Visual%2520Attributes%2520via%2520Knowledge%250A%2520%2520Decomposition%2520for%2520Abnormality%2520Grounding%26entry.906535625%3DJun%2520Li%2520and%2520Che%2520Liu%2520and%2520Wenjia%2520Bai%2520and%2520Mingxuan%2520Liu%2520and%2520Rossella%2520Arcucci%2520and%2520Cosmin%2520I.%2520Bercea%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520problem%2520of%2520grounding%2520abnormalities%2520in%2520medical%250Aimages%252C%2520where%2520the%2520goal%2520is%2520to%2520localize%2520clinical%2520findings%2520based%2520on%2520textual%250Adescriptions.%2520While%2520generalist%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520excel%2520in%2520natural%250Agrounding%2520tasks%252C%2520they%2520often%2520struggle%2520in%2520the%2520medical%2520domain%2520due%2520to%2520rare%252C%250Acompositional%252C%2520and%2520domain-specific%2520terms%2520that%2520are%2520poorly%2520aligned%2520with%2520visual%250Apatterns.%2520Specialized%2520medical%2520VLMs%2520address%2520this%2520challenge%2520via%2520large-scale%250Adomain%2520pretraining%252C%2520but%2520at%2520the%2520cost%2520of%2520substantial%2520annotation%2520and%2520computational%250Aresources.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520%255Ctextbf%257BKnowledge%2520to%2520Sight%250A%2528K2Sight%2529%257D%252C%2520a%2520framework%2520that%2520introduces%2520structured%2520semantic%2520supervision%2520by%250Adecomposing%2520clinical%2520concepts%2520into%2520interpretable%2520visual%2520attributes%252C%2520such%2520as%250Ashape%252C%2520density%252C%2520and%2520anatomical%2520location.%2520These%2520attributes%2520are%2520distilled%2520from%250Adomain%2520ontologies%2520and%2520encoded%2520into%2520concise%2520instruction-style%2520prompts%252C%2520which%250Aguide%2520region-text%2520alignment%2520during%2520training.%2520Unlike%2520conventional%2520report-level%250Asupervision%252C%2520our%2520approach%2520explicitly%2520bridges%2520domain%2520knowledge%2520and%2520spatial%250Astructure%252C%2520enabling%2520data-efficient%2520training%2520of%2520compact%2520models.%2520We%2520train%2520compact%250Amodels%2520with%25200.23B%2520and%25202B%2520parameters%2520using%2520only%25201.5%255C%2525%2520of%2520the%2520data%2520required%2520by%250Astate-of-the-art%2520medical%2520VLMs.%2520Despite%2520their%2520small%2520size%2520and%2520limited%2520training%250Adata%252C%2520these%2520models%2520achieve%2520performance%2520on%2520par%2520with%2520or%2520better%2520than%25207B%252B%2520medical%250AVLMs%252C%2520with%2520up%2520to%25209.82%255C%2525%2520improvement%2520in%2520%2524mAP_%257B50%257D%2524.%2520Code%2520and%2520models%253A%250A%255Chref%257Bhttps%253A//lijunrio.github.io/K2Sight/%257D%257B%255Ctextcolor%257BSOTAPink%257D%257Bhttps%253A//lijunrio.github.io/K2Sight/%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20to%20Sight%3A%20Reasoning%20over%20Visual%20Attributes%20via%20Knowledge%0A%20%20Decomposition%20for%20Abnormality%20Grounding&entry.906535625=Jun%20Li%20and%20Che%20Liu%20and%20Wenjia%20Bai%20and%20Mingxuan%20Liu%20and%20Rossella%20Arcucci%20and%20Cosmin%20I.%20Bercea%20and%20Julia%20A.%20Schnabel&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20grounding%20abnormalities%20in%20medical%0Aimages%2C%20where%20the%20goal%20is%20to%20localize%20clinical%20findings%20based%20on%20textual%0Adescriptions.%20While%20generalist%20Vision-Language%20Models%20%28VLMs%29%20excel%20in%20natural%0Agrounding%20tasks%2C%20they%20often%20struggle%20in%20the%20medical%20domain%20due%20to%20rare%2C%0Acompositional%2C%20and%20domain-specific%20terms%20that%20are%20poorly%20aligned%20with%20visual%0Apatterns.%20Specialized%20medical%20VLMs%20address%20this%20challenge%20via%20large-scale%0Adomain%20pretraining%2C%20but%20at%20the%20cost%20of%20substantial%20annotation%20and%20computational%0Aresources.%20To%20overcome%20these%20limitations%2C%20we%20propose%20%5Ctextbf%7BKnowledge%20to%20Sight%0A%28K2Sight%29%7D%2C%20a%20framework%20that%20introduces%20structured%20semantic%20supervision%20by%0Adecomposing%20clinical%20concepts%20into%20interpretable%20visual%20attributes%2C%20such%20as%0Ashape%2C%20density%2C%20and%20anatomical%20location.%20These%20attributes%20are%20distilled%20from%0Adomain%20ontologies%20and%20encoded%20into%20concise%20instruction-style%20prompts%2C%20which%0Aguide%20region-text%20alignment%20during%20training.%20Unlike%20conventional%20report-level%0Asupervision%2C%20our%20approach%20explicitly%20bridges%20domain%20knowledge%20and%20spatial%0Astructure%2C%20enabling%20data-efficient%20training%20of%20compact%20models.%20We%20train%20compact%0Amodels%20with%200.23B%20and%202B%20parameters%20using%20only%201.5%5C%25%20of%20the%20data%20required%20by%0Astate-of-the-art%20medical%20VLMs.%20Despite%20their%20small%20size%20and%20limited%20training%0Adata%2C%20these%20models%20achieve%20performance%20on%20par%20with%20or%20better%20than%207B%2B%20medical%0AVLMs%2C%20with%20up%20to%209.82%5C%25%20improvement%20in%20%24mAP_%7B50%7D%24.%20Code%20and%20models%3A%0A%5Chref%7Bhttps%3A//lijunrio.github.io/K2Sight/%7D%7B%5Ctextcolor%7BSOTAPink%7D%7Bhttps%3A//lijunrio.github.io/K2Sight/%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04572v1&entry.124074799=Read"},
{"title": "X-SAM: From Segment Anything to Any Segmentation", "author": "Hao Wang and Limeng Qiao and Zequn Jie and Zhijian Huang and Chengjian Feng and Qingfang Zheng and Lin Ma and Xiangyuan Lan and Xiaodan Liang", "abstract": "  Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.\n", "link": "http://arxiv.org/abs/2508.04655v1", "date": "2025-08-06", "relevancy": 2.9525, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-SAM%3A%20From%20Segment%20Anything%20to%20Any%20Segmentation&body=Title%3A%20X-SAM%3A%20From%20Segment%20Anything%20to%20Any%20Segmentation%0AAuthor%3A%20Hao%20Wang%20and%20Limeng%20Qiao%20and%20Zequn%20Jie%20and%20Zhijian%20Huang%20and%20Chengjian%20Feng%20and%20Qingfang%20Zheng%20and%20Lin%20Ma%20and%20Xiangyuan%20Lan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20in%20broad%0Aknowledge%20representation%2C%20yet%20they%20are%20inherently%20deficient%20in%20pixel-level%0Aperceptual%20understanding.%20Although%20the%20Segment%20Anything%20Model%20%28SAM%29%20represents%0Aa%20significant%20advancement%20in%20visual-prompt-driven%20image%20segmentation%2C%20it%0Aexhibits%20notable%20limitations%20in%20multi-mask%20prediction%20and%20category-specific%0Asegmentation%20tasks%2C%20and%20it%20cannot%20integrate%20all%20segmentation%20tasks%20within%20a%0Aunified%20model%20architecture.%20To%20address%20these%20limitations%2C%20we%20present%20X-SAM%2C%20a%0Astreamlined%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20framework%20that%20extends%20the%0Asegmentation%20paradigm%20from%20%5Ctextit%7Bsegment%20anything%7D%20to%20%5Ctextit%7Bany%0Asegmentation%7D.%20Specifically%2C%20we%20introduce%20a%20novel%20unified%20framework%20that%0Aenables%20more%20advanced%20pixel-level%20perceptual%20comprehension%20for%20MLLMs.%0AFurthermore%2C%20we%20propose%20a%20new%20segmentation%20task%2C%20termed%20Visual%20GrounDed%20%28VGD%29%0Asegmentation%2C%20which%20segments%20all%20instance%20objects%20with%20interactive%20visual%0Aprompts%20and%20empowers%20MLLMs%20with%20visual%20grounded%2C%20pixel-wise%20interpretative%0Acapabilities.%20To%20enable%20effective%20training%20on%20diverse%20data%20sources%2C%20we%20present%0Aa%20unified%20training%20strategy%20that%20supports%20co-training%20across%20multiple%20datasets.%0AExperimental%20results%20demonstrate%20that%20X-SAM%20achieves%20state-of-the-art%0Aperformance%20on%20a%20wide%20range%20of%20image%20segmentation%20benchmarks%2C%20highlighting%20its%0Aefficiency%20for%20multimodal%2C%20pixel-level%20visual%20understanding.%20Code%20is%20available%0Aat%20https%3A//github.com/wanghao9610/X-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-SAM%253A%2520From%2520Segment%2520Anything%2520to%2520Any%2520Segmentation%26entry.906535625%3DHao%2520Wang%2520and%2520Limeng%2520Qiao%2520and%2520Zequn%2520Jie%2520and%2520Zhijian%2520Huang%2520and%2520Chengjian%2520Feng%2520and%2520Qingfang%2520Zheng%2520and%2520Lin%2520Ma%2520and%2520Xiangyuan%2520Lan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520strong%2520capabilities%2520in%2520broad%250Aknowledge%2520representation%252C%2520yet%2520they%2520are%2520inherently%2520deficient%2520in%2520pixel-level%250Aperceptual%2520understanding.%2520Although%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520represents%250Aa%2520significant%2520advancement%2520in%2520visual-prompt-driven%2520image%2520segmentation%252C%2520it%250Aexhibits%2520notable%2520limitations%2520in%2520multi-mask%2520prediction%2520and%2520category-specific%250Asegmentation%2520tasks%252C%2520and%2520it%2520cannot%2520integrate%2520all%2520segmentation%2520tasks%2520within%2520a%250Aunified%2520model%2520architecture.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520X-SAM%252C%2520a%250Astreamlined%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520framework%2520that%2520extends%2520the%250Asegmentation%2520paradigm%2520from%2520%255Ctextit%257Bsegment%2520anything%257D%2520to%2520%255Ctextit%257Bany%250Asegmentation%257D.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520unified%2520framework%2520that%250Aenables%2520more%2520advanced%2520pixel-level%2520perceptual%2520comprehension%2520for%2520MLLMs.%250AFurthermore%252C%2520we%2520propose%2520a%2520new%2520segmentation%2520task%252C%2520termed%2520Visual%2520GrounDed%2520%2528VGD%2529%250Asegmentation%252C%2520which%2520segments%2520all%2520instance%2520objects%2520with%2520interactive%2520visual%250Aprompts%2520and%2520empowers%2520MLLMs%2520with%2520visual%2520grounded%252C%2520pixel-wise%2520interpretative%250Acapabilities.%2520To%2520enable%2520effective%2520training%2520on%2520diverse%2520data%2520sources%252C%2520we%2520present%250Aa%2520unified%2520training%2520strategy%2520that%2520supports%2520co-training%2520across%2520multiple%2520datasets.%250AExperimental%2520results%2520demonstrate%2520that%2520X-SAM%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520a%2520wide%2520range%2520of%2520image%2520segmentation%2520benchmarks%252C%2520highlighting%2520its%250Aefficiency%2520for%2520multimodal%252C%2520pixel-level%2520visual%2520understanding.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/wanghao9610/X-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-SAM%3A%20From%20Segment%20Anything%20to%20Any%20Segmentation&entry.906535625=Hao%20Wang%20and%20Limeng%20Qiao%20and%20Zequn%20Jie%20and%20Zhijian%20Huang%20and%20Chengjian%20Feng%20and%20Qingfang%20Zheng%20and%20Lin%20Ma%20and%20Xiangyuan%20Lan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20in%20broad%0Aknowledge%20representation%2C%20yet%20they%20are%20inherently%20deficient%20in%20pixel-level%0Aperceptual%20understanding.%20Although%20the%20Segment%20Anything%20Model%20%28SAM%29%20represents%0Aa%20significant%20advancement%20in%20visual-prompt-driven%20image%20segmentation%2C%20it%0Aexhibits%20notable%20limitations%20in%20multi-mask%20prediction%20and%20category-specific%0Asegmentation%20tasks%2C%20and%20it%20cannot%20integrate%20all%20segmentation%20tasks%20within%20a%0Aunified%20model%20architecture.%20To%20address%20these%20limitations%2C%20we%20present%20X-SAM%2C%20a%0Astreamlined%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20framework%20that%20extends%20the%0Asegmentation%20paradigm%20from%20%5Ctextit%7Bsegment%20anything%7D%20to%20%5Ctextit%7Bany%0Asegmentation%7D.%20Specifically%2C%20we%20introduce%20a%20novel%20unified%20framework%20that%0Aenables%20more%20advanced%20pixel-level%20perceptual%20comprehension%20for%20MLLMs.%0AFurthermore%2C%20we%20propose%20a%20new%20segmentation%20task%2C%20termed%20Visual%20GrounDed%20%28VGD%29%0Asegmentation%2C%20which%20segments%20all%20instance%20objects%20with%20interactive%20visual%0Aprompts%20and%20empowers%20MLLMs%20with%20visual%20grounded%2C%20pixel-wise%20interpretative%0Acapabilities.%20To%20enable%20effective%20training%20on%20diverse%20data%20sources%2C%20we%20present%0Aa%20unified%20training%20strategy%20that%20supports%20co-training%20across%20multiple%20datasets.%0AExperimental%20results%20demonstrate%20that%20X-SAM%20achieves%20state-of-the-art%0Aperformance%20on%20a%20wide%20range%20of%20image%20segmentation%20benchmarks%2C%20highlighting%20its%0Aefficiency%20for%20multimodal%2C%20pixel-level%20visual%20understanding.%20Code%20is%20available%0Aat%20https%3A//github.com/wanghao9610/X-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04655v1&entry.124074799=Read"},
{"title": "PixCuboid: Room Layout Estimation from Multi-view Featuremetric\n  Alignment", "author": "Gustav Hanning and Kalle \u00c5str\u00f6m and Viktor Larsson", "abstract": "  Coarse room layout estimation provides important geometric cues for many\ndownstream tasks. Current state-of-the-art methods are predominantly based on\nsingle views and often assume panoramic images. We introduce PixCuboid, an\noptimization-based approach for cuboid-shaped room layout estimation, which is\nbased on multi-view alignment of dense deep features. By training with the\noptimization end-to-end, we learn feature maps that yield large convergence\nbasins and smooth loss landscapes in the alignment. This allows us to\ninitialize the room layout using simple heuristics.\n  For the evaluation we propose two new benchmarks based on ScanNet++ and\n2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough\nexperiments we validate our approach and significantly outperform the\ncompetition. Finally, while our network is trained with single cuboids, the\nflexibility of the optimization-based approach allow us to easily extend to\nmulti-room estimation, e.g. larger apartments or offices. Code and model\nweights are available at https://github.com/ghanning/PixCuboid.\n", "link": "http://arxiv.org/abs/2508.04659v1", "date": "2025-08-06", "relevancy": 2.8884, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6012}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5867}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixCuboid%3A%20Room%20Layout%20Estimation%20from%20Multi-view%20Featuremetric%0A%20%20Alignment&body=Title%3A%20PixCuboid%3A%20Room%20Layout%20Estimation%20from%20Multi-view%20Featuremetric%0A%20%20Alignment%0AAuthor%3A%20Gustav%20Hanning%20and%20Kalle%20%C3%85str%C3%B6m%20and%20Viktor%20Larsson%0AAbstract%3A%20%20%20Coarse%20room%20layout%20estimation%20provides%20important%20geometric%20cues%20for%20many%0Adownstream%20tasks.%20Current%20state-of-the-art%20methods%20are%20predominantly%20based%20on%0Asingle%20views%20and%20often%20assume%20panoramic%20images.%20We%20introduce%20PixCuboid%2C%20an%0Aoptimization-based%20approach%20for%20cuboid-shaped%20room%20layout%20estimation%2C%20which%20is%0Abased%20on%20multi-view%20alignment%20of%20dense%20deep%20features.%20By%20training%20with%20the%0Aoptimization%20end-to-end%2C%20we%20learn%20feature%20maps%20that%20yield%20large%20convergence%0Abasins%20and%20smooth%20loss%20landscapes%20in%20the%20alignment.%20This%20allows%20us%20to%0Ainitialize%20the%20room%20layout%20using%20simple%20heuristics.%0A%20%20For%20the%20evaluation%20we%20propose%20two%20new%20benchmarks%20based%20on%20ScanNet%2B%2B%20and%0A2D-3D-Semantics%2C%20with%20manually%20verified%20ground%20truth%203D%20cuboids.%20In%20thorough%0Aexperiments%20we%20validate%20our%20approach%20and%20significantly%20outperform%20the%0Acompetition.%20Finally%2C%20while%20our%20network%20is%20trained%20with%20single%20cuboids%2C%20the%0Aflexibility%20of%20the%20optimization-based%20approach%20allow%20us%20to%20easily%20extend%20to%0Amulti-room%20estimation%2C%20e.g.%20larger%20apartments%20or%20offices.%20Code%20and%20model%0Aweights%20are%20available%20at%20https%3A//github.com/ghanning/PixCuboid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixCuboid%253A%2520Room%2520Layout%2520Estimation%2520from%2520Multi-view%2520Featuremetric%250A%2520%2520Alignment%26entry.906535625%3DGustav%2520Hanning%2520and%2520Kalle%2520%25C3%2585str%25C3%25B6m%2520and%2520Viktor%2520Larsson%26entry.1292438233%3D%2520%2520Coarse%2520room%2520layout%2520estimation%2520provides%2520important%2520geometric%2520cues%2520for%2520many%250Adownstream%2520tasks.%2520Current%2520state-of-the-art%2520methods%2520are%2520predominantly%2520based%2520on%250Asingle%2520views%2520and%2520often%2520assume%2520panoramic%2520images.%2520We%2520introduce%2520PixCuboid%252C%2520an%250Aoptimization-based%2520approach%2520for%2520cuboid-shaped%2520room%2520layout%2520estimation%252C%2520which%2520is%250Abased%2520on%2520multi-view%2520alignment%2520of%2520dense%2520deep%2520features.%2520By%2520training%2520with%2520the%250Aoptimization%2520end-to-end%252C%2520we%2520learn%2520feature%2520maps%2520that%2520yield%2520large%2520convergence%250Abasins%2520and%2520smooth%2520loss%2520landscapes%2520in%2520the%2520alignment.%2520This%2520allows%2520us%2520to%250Ainitialize%2520the%2520room%2520layout%2520using%2520simple%2520heuristics.%250A%2520%2520For%2520the%2520evaluation%2520we%2520propose%2520two%2520new%2520benchmarks%2520based%2520on%2520ScanNet%252B%252B%2520and%250A2D-3D-Semantics%252C%2520with%2520manually%2520verified%2520ground%2520truth%25203D%2520cuboids.%2520In%2520thorough%250Aexperiments%2520we%2520validate%2520our%2520approach%2520and%2520significantly%2520outperform%2520the%250Acompetition.%2520Finally%252C%2520while%2520our%2520network%2520is%2520trained%2520with%2520single%2520cuboids%252C%2520the%250Aflexibility%2520of%2520the%2520optimization-based%2520approach%2520allow%2520us%2520to%2520easily%2520extend%2520to%250Amulti-room%2520estimation%252C%2520e.g.%2520larger%2520apartments%2520or%2520offices.%2520Code%2520and%2520model%250Aweights%2520are%2520available%2520at%2520https%253A//github.com/ghanning/PixCuboid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixCuboid%3A%20Room%20Layout%20Estimation%20from%20Multi-view%20Featuremetric%0A%20%20Alignment&entry.906535625=Gustav%20Hanning%20and%20Kalle%20%C3%85str%C3%B6m%20and%20Viktor%20Larsson&entry.1292438233=%20%20Coarse%20room%20layout%20estimation%20provides%20important%20geometric%20cues%20for%20many%0Adownstream%20tasks.%20Current%20state-of-the-art%20methods%20are%20predominantly%20based%20on%0Asingle%20views%20and%20often%20assume%20panoramic%20images.%20We%20introduce%20PixCuboid%2C%20an%0Aoptimization-based%20approach%20for%20cuboid-shaped%20room%20layout%20estimation%2C%20which%20is%0Abased%20on%20multi-view%20alignment%20of%20dense%20deep%20features.%20By%20training%20with%20the%0Aoptimization%20end-to-end%2C%20we%20learn%20feature%20maps%20that%20yield%20large%20convergence%0Abasins%20and%20smooth%20loss%20landscapes%20in%20the%20alignment.%20This%20allows%20us%20to%0Ainitialize%20the%20room%20layout%20using%20simple%20heuristics.%0A%20%20For%20the%20evaluation%20we%20propose%20two%20new%20benchmarks%20based%20on%20ScanNet%2B%2B%20and%0A2D-3D-Semantics%2C%20with%20manually%20verified%20ground%20truth%203D%20cuboids.%20In%20thorough%0Aexperiments%20we%20validate%20our%20approach%20and%20significantly%20outperform%20the%0Acompetition.%20Finally%2C%20while%20our%20network%20is%20trained%20with%20single%20cuboids%2C%20the%0Aflexibility%20of%20the%20optimization-based%20approach%20allow%20us%20to%20easily%20extend%20to%0Amulti-room%20estimation%2C%20e.g.%20larger%20apartments%20or%20offices.%20Code%20and%20model%0Aweights%20are%20available%20at%20https%3A//github.com/ghanning/PixCuboid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04659v1&entry.124074799=Read"},
{"title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning\n  for Long Video Reasoning", "author": "Haoji Zhang and Xin Gu and Jiawen Li and Chixiang Ma and Sule Bai and Chubin Zhang and Bowen Zhang and Zhichao Zhou and Dongliang He and Yansong Tang", "abstract": "  The video reasoning ability of multimodal large language models (MLLMs) is\ncrucial for downstream tasks like video question answering and temporal\ngrounding. While recent approaches have explored text-based chain-of-thought\n(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal\ninteraction and increased hallucination, especially with longer videos or\nreasoning chains. To address these challenges, we propose Video Intelligence\nvia Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning\nframework. With a visual toolbox, the model can densely sample new video frames\non demand and generate multimodal CoT for precise long video reasoning. We\nobserve that temporal grounding and question answering are mutually beneficial\nfor video understanding tasks. Therefore, we construct two high-quality\nmulti-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and\nMTVR-RL-110k for reinforcement learning. Moreover, we propose a\nDifficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to\nmitigate difficulty imbalance in multi-task reinforcement learning. Extensive\nexperiments on 11 challenging video understanding benchmarks demonstrate the\nadvanced reasoning ability of VITAL, outperforming existing methods in video\nquestion answering and temporal grounding tasks, especially in long video\nscenarios. All code, data and model weight will be made publicly available.\n", "link": "http://arxiv.org/abs/2508.04416v1", "date": "2025-08-06", "relevancy": 2.8834, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5794}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Thinking%20With%20Videos%3A%20Multimodal%20Tool-Augmented%20Reinforcement%20Learning%0A%20%20for%20Long%20Video%20Reasoning&body=Title%3A%20Thinking%20With%20Videos%3A%20Multimodal%20Tool-Augmented%20Reinforcement%20Learning%0A%20%20for%20Long%20Video%20Reasoning%0AAuthor%3A%20Haoji%20Zhang%20and%20Xin%20Gu%20and%20Jiawen%20Li%20and%20Chixiang%20Ma%20and%20Sule%20Bai%20and%20Chubin%20Zhang%20and%20Bowen%20Zhang%20and%20Zhichao%20Zhou%20and%20Dongliang%20He%20and%20Yansong%20Tang%0AAbstract%3A%20%20%20The%20video%20reasoning%20ability%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20is%0Acrucial%20for%20downstream%20tasks%20like%20video%20question%20answering%20and%20temporal%0Agrounding.%20While%20recent%20approaches%20have%20explored%20text-based%20chain-of-thought%0A%28CoT%29%20reasoning%20for%20MLLMs%2C%20these%20methods%20often%20suffer%20from%20limited%20cross-modal%0Ainteraction%20and%20increased%20hallucination%2C%20especially%20with%20longer%20videos%20or%0Areasoning%20chains.%20To%20address%20these%20challenges%2C%20we%20propose%20Video%20Intelligence%0Avia%20Tool-Augmented%20Learning%20%28VITAL%29%2C%20a%20novel%20end-to-end%20agentic%20video%20reasoning%0Aframework.%20With%20a%20visual%20toolbox%2C%20the%20model%20can%20densely%20sample%20new%20video%20frames%0Aon%20demand%20and%20generate%20multimodal%20CoT%20for%20precise%20long%20video%20reasoning.%20We%0Aobserve%20that%20temporal%20grounding%20and%20question%20answering%20are%20mutually%20beneficial%0Afor%20video%20understanding%20tasks.%20Therefore%2C%20we%20construct%20two%20high-quality%0Amulti-task%20video%20reasoning%20datasets%20MTVR-CoT-72k%20for%20supervised%20fine-tuning%20and%0AMTVR-RL-110k%20for%20reinforcement%20learning.%20Moreover%2C%20we%20propose%20a%0ADifficulty-aware%20Group%20Relative%20Policy%20Optimization%20algorithm%20%28DGRPO%29%20to%0Amitigate%20difficulty%20imbalance%20in%20multi-task%20reinforcement%20learning.%20Extensive%0Aexperiments%20on%2011%20challenging%20video%20understanding%20benchmarks%20demonstrate%20the%0Aadvanced%20reasoning%20ability%20of%20VITAL%2C%20outperforming%20existing%20methods%20in%20video%0Aquestion%20answering%20and%20temporal%20grounding%20tasks%2C%20especially%20in%20long%20video%0Ascenarios.%20All%20code%2C%20data%20and%20model%20weight%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinking%2520With%2520Videos%253A%2520Multimodal%2520Tool-Augmented%2520Reinforcement%2520Learning%250A%2520%2520for%2520Long%2520Video%2520Reasoning%26entry.906535625%3DHaoji%2520Zhang%2520and%2520Xin%2520Gu%2520and%2520Jiawen%2520Li%2520and%2520Chixiang%2520Ma%2520and%2520Sule%2520Bai%2520and%2520Chubin%2520Zhang%2520and%2520Bowen%2520Zhang%2520and%2520Zhichao%2520Zhou%2520and%2520Dongliang%2520He%2520and%2520Yansong%2520Tang%26entry.1292438233%3D%2520%2520The%2520video%2520reasoning%2520ability%2520of%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520is%250Acrucial%2520for%2520downstream%2520tasks%2520like%2520video%2520question%2520answering%2520and%2520temporal%250Agrounding.%2520While%2520recent%2520approaches%2520have%2520explored%2520text-based%2520chain-of-thought%250A%2528CoT%2529%2520reasoning%2520for%2520MLLMs%252C%2520these%2520methods%2520often%2520suffer%2520from%2520limited%2520cross-modal%250Ainteraction%2520and%2520increased%2520hallucination%252C%2520especially%2520with%2520longer%2520videos%2520or%250Areasoning%2520chains.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Video%2520Intelligence%250Avia%2520Tool-Augmented%2520Learning%2520%2528VITAL%2529%252C%2520a%2520novel%2520end-to-end%2520agentic%2520video%2520reasoning%250Aframework.%2520With%2520a%2520visual%2520toolbox%252C%2520the%2520model%2520can%2520densely%2520sample%2520new%2520video%2520frames%250Aon%2520demand%2520and%2520generate%2520multimodal%2520CoT%2520for%2520precise%2520long%2520video%2520reasoning.%2520We%250Aobserve%2520that%2520temporal%2520grounding%2520and%2520question%2520answering%2520are%2520mutually%2520beneficial%250Afor%2520video%2520understanding%2520tasks.%2520Therefore%252C%2520we%2520construct%2520two%2520high-quality%250Amulti-task%2520video%2520reasoning%2520datasets%2520MTVR-CoT-72k%2520for%2520supervised%2520fine-tuning%2520and%250AMTVR-RL-110k%2520for%2520reinforcement%2520learning.%2520Moreover%252C%2520we%2520propose%2520a%250ADifficulty-aware%2520Group%2520Relative%2520Policy%2520Optimization%2520algorithm%2520%2528DGRPO%2529%2520to%250Amitigate%2520difficulty%2520imbalance%2520in%2520multi-task%2520reinforcement%2520learning.%2520Extensive%250Aexperiments%2520on%252011%2520challenging%2520video%2520understanding%2520benchmarks%2520demonstrate%2520the%250Aadvanced%2520reasoning%2520ability%2520of%2520VITAL%252C%2520outperforming%2520existing%2520methods%2520in%2520video%250Aquestion%2520answering%2520and%2520temporal%2520grounding%2520tasks%252C%2520especially%2520in%2520long%2520video%250Ascenarios.%2520All%2520code%252C%2520data%2520and%2520model%2520weight%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Thinking%20With%20Videos%3A%20Multimodal%20Tool-Augmented%20Reinforcement%20Learning%0A%20%20for%20Long%20Video%20Reasoning&entry.906535625=Haoji%20Zhang%20and%20Xin%20Gu%20and%20Jiawen%20Li%20and%20Chixiang%20Ma%20and%20Sule%20Bai%20and%20Chubin%20Zhang%20and%20Bowen%20Zhang%20and%20Zhichao%20Zhou%20and%20Dongliang%20He%20and%20Yansong%20Tang&entry.1292438233=%20%20The%20video%20reasoning%20ability%20of%20multimodal%20large%20language%20models%20%28MLLMs%29%20is%0Acrucial%20for%20downstream%20tasks%20like%20video%20question%20answering%20and%20temporal%0Agrounding.%20While%20recent%20approaches%20have%20explored%20text-based%20chain-of-thought%0A%28CoT%29%20reasoning%20for%20MLLMs%2C%20these%20methods%20often%20suffer%20from%20limited%20cross-modal%0Ainteraction%20and%20increased%20hallucination%2C%20especially%20with%20longer%20videos%20or%0Areasoning%20chains.%20To%20address%20these%20challenges%2C%20we%20propose%20Video%20Intelligence%0Avia%20Tool-Augmented%20Learning%20%28VITAL%29%2C%20a%20novel%20end-to-end%20agentic%20video%20reasoning%0Aframework.%20With%20a%20visual%20toolbox%2C%20the%20model%20can%20densely%20sample%20new%20video%20frames%0Aon%20demand%20and%20generate%20multimodal%20CoT%20for%20precise%20long%20video%20reasoning.%20We%0Aobserve%20that%20temporal%20grounding%20and%20question%20answering%20are%20mutually%20beneficial%0Afor%20video%20understanding%20tasks.%20Therefore%2C%20we%20construct%20two%20high-quality%0Amulti-task%20video%20reasoning%20datasets%20MTVR-CoT-72k%20for%20supervised%20fine-tuning%20and%0AMTVR-RL-110k%20for%20reinforcement%20learning.%20Moreover%2C%20we%20propose%20a%0ADifficulty-aware%20Group%20Relative%20Policy%20Optimization%20algorithm%20%28DGRPO%29%20to%0Amitigate%20difficulty%20imbalance%20in%20multi-task%20reinforcement%20learning.%20Extensive%0Aexperiments%20on%2011%20challenging%20video%20understanding%20benchmarks%20demonstrate%20the%0Aadvanced%20reasoning%20ability%20of%20VITAL%2C%20outperforming%20existing%20methods%20in%20video%0Aquestion%20answering%20and%20temporal%20grounding%20tasks%2C%20especially%20in%20long%20video%0Ascenarios.%20All%20code%2C%20data%20and%20model%20weight%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04416v1&entry.124074799=Read"},
{"title": "Hulk: A Universal Knowledge Translator for Human-Centric Tasks", "author": "Yizhou Wang and Yixuan Wu and Weizhen He and Xun Guo and Feng Zhu and Lei Bai and Rui Zhao and Jian Wu and Tong He and Wanli Ouyang and Shixiang Tang", "abstract": "  Human-centric perception tasks, e.g., pedestrian detection, skeleton-based\naction recognition, and pose estimation, have wide industrial applications,\nsuch as metaverse and sports analysis. There is a recent surge to develop\nhuman-centric foundation models that can benefit a broad range of human-centric\nperception tasks. While many human-centric foundation models have achieved\nsuccess, they did not explore 3D and vision-language tasks for human-centric\nand required task-specific finetuning. These limitations restrict their\napplication to more downstream tasks and situations. To tackle these problems,\nwe present Hulk, the first multimodal human-centric generalist model, capable\nof addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks\nwithout task-specific finetuning. The key to achieving this is condensing\nvarious task-specific heads into two general heads, one for discrete\nrepresentations, \\emph{e.g.,} languages, and the other for continuous\nrepresentations, \\emph{e.g.,} location coordinates. The outputs of two heads\ncan be further stacked into four distinct input and output modalities. This\nuniform representation enables Hulk to treat diverse human-centric tasks as\nmodality translation, integrating knowledge across a wide range of tasks.\nComprehensive evaluations of Hulk on 12 benchmarks covering 8 human-centric\ntasks demonstrate the superiority of our proposed method, achieving\nstate-of-the-art performance in 11 benchmarks. The code will be available on\nhttps://github.com/OpenGVLab/Hulk.\n", "link": "http://arxiv.org/abs/2312.01697v5", "date": "2025-08-06", "relevancy": 2.883, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hulk%3A%20A%20Universal%20Knowledge%20Translator%20for%20Human-Centric%20Tasks&body=Title%3A%20Hulk%3A%20A%20Universal%20Knowledge%20Translator%20for%20Human-Centric%20Tasks%0AAuthor%3A%20Yizhou%20Wang%20and%20Yixuan%20Wu%20and%20Weizhen%20He%20and%20Xun%20Guo%20and%20Feng%20Zhu%20and%20Lei%20Bai%20and%20Rui%20Zhao%20and%20Jian%20Wu%20and%20Tong%20He%20and%20Wanli%20Ouyang%20and%20Shixiang%20Tang%0AAbstract%3A%20%20%20Human-centric%20perception%20tasks%2C%20e.g.%2C%20pedestrian%20detection%2C%20skeleton-based%0Aaction%20recognition%2C%20and%20pose%20estimation%2C%20have%20wide%20industrial%20applications%2C%0Asuch%20as%20metaverse%20and%20sports%20analysis.%20There%20is%20a%20recent%20surge%20to%20develop%0Ahuman-centric%20foundation%20models%20that%20can%20benefit%20a%20broad%20range%20of%20human-centric%0Aperception%20tasks.%20While%20many%20human-centric%20foundation%20models%20have%20achieved%0Asuccess%2C%20they%20did%20not%20explore%203D%20and%20vision-language%20tasks%20for%20human-centric%0Aand%20required%20task-specific%20finetuning.%20These%20limitations%20restrict%20their%0Aapplication%20to%20more%20downstream%20tasks%20and%20situations.%20To%20tackle%20these%20problems%2C%0Awe%20present%20Hulk%2C%20the%20first%20multimodal%20human-centric%20generalist%20model%2C%20capable%0Aof%20addressing%202D%20vision%2C%203D%20vision%2C%20skeleton-based%2C%20and%20vision-language%20tasks%0Awithout%20task-specific%20finetuning.%20The%20key%20to%20achieving%20this%20is%20condensing%0Avarious%20task-specific%20heads%20into%20two%20general%20heads%2C%20one%20for%20discrete%0Arepresentations%2C%20%5Cemph%7Be.g.%2C%7D%20languages%2C%20and%20the%20other%20for%20continuous%0Arepresentations%2C%20%5Cemph%7Be.g.%2C%7D%20location%20coordinates.%20The%20outputs%20of%20two%20heads%0Acan%20be%20further%20stacked%20into%20four%20distinct%20input%20and%20output%20modalities.%20This%0Auniform%20representation%20enables%20Hulk%20to%20treat%20diverse%20human-centric%20tasks%20as%0Amodality%20translation%2C%20integrating%20knowledge%20across%20a%20wide%20range%20of%20tasks.%0AComprehensive%20evaluations%20of%20Hulk%20on%2012%20benchmarks%20covering%208%20human-centric%0Atasks%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%2C%20achieving%0Astate-of-the-art%20performance%20in%2011%20benchmarks.%20The%20code%20will%20be%20available%20on%0Ahttps%3A//github.com/OpenGVLab/Hulk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01697v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHulk%253A%2520A%2520Universal%2520Knowledge%2520Translator%2520for%2520Human-Centric%2520Tasks%26entry.906535625%3DYizhou%2520Wang%2520and%2520Yixuan%2520Wu%2520and%2520Weizhen%2520He%2520and%2520Xun%2520Guo%2520and%2520Feng%2520Zhu%2520and%2520Lei%2520Bai%2520and%2520Rui%2520Zhao%2520and%2520Jian%2520Wu%2520and%2520Tong%2520He%2520and%2520Wanli%2520Ouyang%2520and%2520Shixiang%2520Tang%26entry.1292438233%3D%2520%2520Human-centric%2520perception%2520tasks%252C%2520e.g.%252C%2520pedestrian%2520detection%252C%2520skeleton-based%250Aaction%2520recognition%252C%2520and%2520pose%2520estimation%252C%2520have%2520wide%2520industrial%2520applications%252C%250Asuch%2520as%2520metaverse%2520and%2520sports%2520analysis.%2520There%2520is%2520a%2520recent%2520surge%2520to%2520develop%250Ahuman-centric%2520foundation%2520models%2520that%2520can%2520benefit%2520a%2520broad%2520range%2520of%2520human-centric%250Aperception%2520tasks.%2520While%2520many%2520human-centric%2520foundation%2520models%2520have%2520achieved%250Asuccess%252C%2520they%2520did%2520not%2520explore%25203D%2520and%2520vision-language%2520tasks%2520for%2520human-centric%250Aand%2520required%2520task-specific%2520finetuning.%2520These%2520limitations%2520restrict%2520their%250Aapplication%2520to%2520more%2520downstream%2520tasks%2520and%2520situations.%2520To%2520tackle%2520these%2520problems%252C%250Awe%2520present%2520Hulk%252C%2520the%2520first%2520multimodal%2520human-centric%2520generalist%2520model%252C%2520capable%250Aof%2520addressing%25202D%2520vision%252C%25203D%2520vision%252C%2520skeleton-based%252C%2520and%2520vision-language%2520tasks%250Awithout%2520task-specific%2520finetuning.%2520The%2520key%2520to%2520achieving%2520this%2520is%2520condensing%250Avarious%2520task-specific%2520heads%2520into%2520two%2520general%2520heads%252C%2520one%2520for%2520discrete%250Arepresentations%252C%2520%255Cemph%257Be.g.%252C%257D%2520languages%252C%2520and%2520the%2520other%2520for%2520continuous%250Arepresentations%252C%2520%255Cemph%257Be.g.%252C%257D%2520location%2520coordinates.%2520The%2520outputs%2520of%2520two%2520heads%250Acan%2520be%2520further%2520stacked%2520into%2520four%2520distinct%2520input%2520and%2520output%2520modalities.%2520This%250Auniform%2520representation%2520enables%2520Hulk%2520to%2520treat%2520diverse%2520human-centric%2520tasks%2520as%250Amodality%2520translation%252C%2520integrating%2520knowledge%2520across%2520a%2520wide%2520range%2520of%2520tasks.%250AComprehensive%2520evaluations%2520of%2520Hulk%2520on%252012%2520benchmarks%2520covering%25208%2520human-centric%250Atasks%2520demonstrate%2520the%2520superiority%2520of%2520our%2520proposed%2520method%252C%2520achieving%250Astate-of-the-art%2520performance%2520in%252011%2520benchmarks.%2520The%2520code%2520will%2520be%2520available%2520on%250Ahttps%253A//github.com/OpenGVLab/Hulk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01697v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hulk%3A%20A%20Universal%20Knowledge%20Translator%20for%20Human-Centric%20Tasks&entry.906535625=Yizhou%20Wang%20and%20Yixuan%20Wu%20and%20Weizhen%20He%20and%20Xun%20Guo%20and%20Feng%20Zhu%20and%20Lei%20Bai%20and%20Rui%20Zhao%20and%20Jian%20Wu%20and%20Tong%20He%20and%20Wanli%20Ouyang%20and%20Shixiang%20Tang&entry.1292438233=%20%20Human-centric%20perception%20tasks%2C%20e.g.%2C%20pedestrian%20detection%2C%20skeleton-based%0Aaction%20recognition%2C%20and%20pose%20estimation%2C%20have%20wide%20industrial%20applications%2C%0Asuch%20as%20metaverse%20and%20sports%20analysis.%20There%20is%20a%20recent%20surge%20to%20develop%0Ahuman-centric%20foundation%20models%20that%20can%20benefit%20a%20broad%20range%20of%20human-centric%0Aperception%20tasks.%20While%20many%20human-centric%20foundation%20models%20have%20achieved%0Asuccess%2C%20they%20did%20not%20explore%203D%20and%20vision-language%20tasks%20for%20human-centric%0Aand%20required%20task-specific%20finetuning.%20These%20limitations%20restrict%20their%0Aapplication%20to%20more%20downstream%20tasks%20and%20situations.%20To%20tackle%20these%20problems%2C%0Awe%20present%20Hulk%2C%20the%20first%20multimodal%20human-centric%20generalist%20model%2C%20capable%0Aof%20addressing%202D%20vision%2C%203D%20vision%2C%20skeleton-based%2C%20and%20vision-language%20tasks%0Awithout%20task-specific%20finetuning.%20The%20key%20to%20achieving%20this%20is%20condensing%0Avarious%20task-specific%20heads%20into%20two%20general%20heads%2C%20one%20for%20discrete%0Arepresentations%2C%20%5Cemph%7Be.g.%2C%7D%20languages%2C%20and%20the%20other%20for%20continuous%0Arepresentations%2C%20%5Cemph%7Be.g.%2C%7D%20location%20coordinates.%20The%20outputs%20of%20two%20heads%0Acan%20be%20further%20stacked%20into%20four%20distinct%20input%20and%20output%20modalities.%20This%0Auniform%20representation%20enables%20Hulk%20to%20treat%20diverse%20human-centric%20tasks%20as%0Amodality%20translation%2C%20integrating%20knowledge%20across%20a%20wide%20range%20of%20tasks.%0AComprehensive%20evaluations%20of%20Hulk%20on%2012%20benchmarks%20covering%208%20human-centric%0Atasks%20demonstrate%20the%20superiority%20of%20our%20proposed%20method%2C%20achieving%0Astate-of-the-art%20performance%20in%2011%20benchmarks.%20The%20code%20will%20be%20available%20on%0Ahttps%3A//github.com/OpenGVLab/Hulk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01697v5&entry.124074799=Read"},
{"title": "VISO-Grasp: Vision-Language Informed Spatial Object-centric 6-DoF Active\n  View Planning and Grasping in Clutter and Invisibility", "author": "Yitian Shi and Di Wen and Guanqi Chen and Edgar Welte and Sheng Liu and Kunyu Peng and Rainer Stiefelhagen and Rania Rayyes", "abstract": "  We propose VISO-Grasp, a novel vision-language-informed system designed to\nsystematically address visibility constraints for grasping in severely occluded\nenvironments. By leveraging Foundation Models (FMs) for spatial reasoning and\nactive view planning, our framework constructs and updates an instance-centric\nrepresentation of spatial relationships, enhancing grasp success under\nchallenging occlusions. Furthermore, this representation facilitates active\nNext-Best-View (NBV) planning and optimizes sequential grasping strategies when\ndirect grasping is infeasible. Additionally, we introduce a multi-view\nuncertainty-driven grasp fusion mechanism that refines grasp confidence and\ndirectional uncertainty in real-time, ensuring robust and stable grasp\nexecution. Extensive real-world experiments demonstrate that VISO-Grasp\nachieves a success rate of $87.5\\%$ in target-oriented grasping with the fewest\ngrasp attempts outperforming baselines. To the best of our knowledge,\nVISO-Grasp is the first unified framework integrating FMs into target-aware\nactive view planning and 6-DoF grasping in environments with severe occlusions\nand entire invisibility constraints. Code is available at:\nhttps://github.com/YitianShi/vMF-Contact\n", "link": "http://arxiv.org/abs/2503.12609v2", "date": "2025-08-06", "relevancy": 2.8714, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISO-Grasp%3A%20Vision-Language%20Informed%20Spatial%20Object-centric%206-DoF%20Active%0A%20%20View%20Planning%20and%20Grasping%20in%20Clutter%20and%20Invisibility&body=Title%3A%20VISO-Grasp%3A%20Vision-Language%20Informed%20Spatial%20Object-centric%206-DoF%20Active%0A%20%20View%20Planning%20and%20Grasping%20in%20Clutter%20and%20Invisibility%0AAuthor%3A%20Yitian%20Shi%20and%20Di%20Wen%20and%20Guanqi%20Chen%20and%20Edgar%20Welte%20and%20Sheng%20Liu%20and%20Kunyu%20Peng%20and%20Rainer%20Stiefelhagen%20and%20Rania%20Rayyes%0AAbstract%3A%20%20%20We%20propose%20VISO-Grasp%2C%20a%20novel%20vision-language-informed%20system%20designed%20to%0Asystematically%20address%20visibility%20constraints%20for%20grasping%20in%20severely%20occluded%0Aenvironments.%20By%20leveraging%20Foundation%20Models%20%28FMs%29%20for%20spatial%20reasoning%20and%0Aactive%20view%20planning%2C%20our%20framework%20constructs%20and%20updates%20an%20instance-centric%0Arepresentation%20of%20spatial%20relationships%2C%20enhancing%20grasp%20success%20under%0Achallenging%20occlusions.%20Furthermore%2C%20this%20representation%20facilitates%20active%0ANext-Best-View%20%28NBV%29%20planning%20and%20optimizes%20sequential%20grasping%20strategies%20when%0Adirect%20grasping%20is%20infeasible.%20Additionally%2C%20we%20introduce%20a%20multi-view%0Auncertainty-driven%20grasp%20fusion%20mechanism%20that%20refines%20grasp%20confidence%20and%0Adirectional%20uncertainty%20in%20real-time%2C%20ensuring%20robust%20and%20stable%20grasp%0Aexecution.%20Extensive%20real-world%20experiments%20demonstrate%20that%20VISO-Grasp%0Aachieves%20a%20success%20rate%20of%20%2487.5%5C%25%24%20in%20target-oriented%20grasping%20with%20the%20fewest%0Agrasp%20attempts%20outperforming%20baselines.%20To%20the%20best%20of%20our%20knowledge%2C%0AVISO-Grasp%20is%20the%20first%20unified%20framework%20integrating%20FMs%20into%20target-aware%0Aactive%20view%20planning%20and%206-DoF%20grasping%20in%20environments%20with%20severe%20occlusions%0Aand%20entire%20invisibility%20constraints.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/YitianShi/vMF-Contact%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.12609v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISO-Grasp%253A%2520Vision-Language%2520Informed%2520Spatial%2520Object-centric%25206-DoF%2520Active%250A%2520%2520View%2520Planning%2520and%2520Grasping%2520in%2520Clutter%2520and%2520Invisibility%26entry.906535625%3DYitian%2520Shi%2520and%2520Di%2520Wen%2520and%2520Guanqi%2520Chen%2520and%2520Edgar%2520Welte%2520and%2520Sheng%2520Liu%2520and%2520Kunyu%2520Peng%2520and%2520Rainer%2520Stiefelhagen%2520and%2520Rania%2520Rayyes%26entry.1292438233%3D%2520%2520We%2520propose%2520VISO-Grasp%252C%2520a%2520novel%2520vision-language-informed%2520system%2520designed%2520to%250Asystematically%2520address%2520visibility%2520constraints%2520for%2520grasping%2520in%2520severely%2520occluded%250Aenvironments.%2520By%2520leveraging%2520Foundation%2520Models%2520%2528FMs%2529%2520for%2520spatial%2520reasoning%2520and%250Aactive%2520view%2520planning%252C%2520our%2520framework%2520constructs%2520and%2520updates%2520an%2520instance-centric%250Arepresentation%2520of%2520spatial%2520relationships%252C%2520enhancing%2520grasp%2520success%2520under%250Achallenging%2520occlusions.%2520Furthermore%252C%2520this%2520representation%2520facilitates%2520active%250ANext-Best-View%2520%2528NBV%2529%2520planning%2520and%2520optimizes%2520sequential%2520grasping%2520strategies%2520when%250Adirect%2520grasping%2520is%2520infeasible.%2520Additionally%252C%2520we%2520introduce%2520a%2520multi-view%250Auncertainty-driven%2520grasp%2520fusion%2520mechanism%2520that%2520refines%2520grasp%2520confidence%2520and%250Adirectional%2520uncertainty%2520in%2520real-time%252C%2520ensuring%2520robust%2520and%2520stable%2520grasp%250Aexecution.%2520Extensive%2520real-world%2520experiments%2520demonstrate%2520that%2520VISO-Grasp%250Aachieves%2520a%2520success%2520rate%2520of%2520%252487.5%255C%2525%2524%2520in%2520target-oriented%2520grasping%2520with%2520the%2520fewest%250Agrasp%2520attempts%2520outperforming%2520baselines.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250AVISO-Grasp%2520is%2520the%2520first%2520unified%2520framework%2520integrating%2520FMs%2520into%2520target-aware%250Aactive%2520view%2520planning%2520and%25206-DoF%2520grasping%2520in%2520environments%2520with%2520severe%2520occlusions%250Aand%2520entire%2520invisibility%2520constraints.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/YitianShi/vMF-Contact%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.12609v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISO-Grasp%3A%20Vision-Language%20Informed%20Spatial%20Object-centric%206-DoF%20Active%0A%20%20View%20Planning%20and%20Grasping%20in%20Clutter%20and%20Invisibility&entry.906535625=Yitian%20Shi%20and%20Di%20Wen%20and%20Guanqi%20Chen%20and%20Edgar%20Welte%20and%20Sheng%20Liu%20and%20Kunyu%20Peng%20and%20Rainer%20Stiefelhagen%20and%20Rania%20Rayyes&entry.1292438233=%20%20We%20propose%20VISO-Grasp%2C%20a%20novel%20vision-language-informed%20system%20designed%20to%0Asystematically%20address%20visibility%20constraints%20for%20grasping%20in%20severely%20occluded%0Aenvironments.%20By%20leveraging%20Foundation%20Models%20%28FMs%29%20for%20spatial%20reasoning%20and%0Aactive%20view%20planning%2C%20our%20framework%20constructs%20and%20updates%20an%20instance-centric%0Arepresentation%20of%20spatial%20relationships%2C%20enhancing%20grasp%20success%20under%0Achallenging%20occlusions.%20Furthermore%2C%20this%20representation%20facilitates%20active%0ANext-Best-View%20%28NBV%29%20planning%20and%20optimizes%20sequential%20grasping%20strategies%20when%0Adirect%20grasping%20is%20infeasible.%20Additionally%2C%20we%20introduce%20a%20multi-view%0Auncertainty-driven%20grasp%20fusion%20mechanism%20that%20refines%20grasp%20confidence%20and%0Adirectional%20uncertainty%20in%20real-time%2C%20ensuring%20robust%20and%20stable%20grasp%0Aexecution.%20Extensive%20real-world%20experiments%20demonstrate%20that%20VISO-Grasp%0Aachieves%20a%20success%20rate%20of%20%2487.5%5C%25%24%20in%20target-oriented%20grasping%20with%20the%20fewest%0Agrasp%20attempts%20outperforming%20baselines.%20To%20the%20best%20of%20our%20knowledge%2C%0AVISO-Grasp%20is%20the%20first%20unified%20framework%20integrating%20FMs%20into%20target-aware%0Aactive%20view%20planning%20and%206-DoF%20grasping%20in%20environments%20with%20severe%20occlusions%0Aand%20entire%20invisibility%20constraints.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/YitianShi/vMF-Contact%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.12609v2&entry.124074799=Read"},
{"title": "Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success", "author": "George Bredis and Stanislav Dereka and Viacheslav Sinii and Ruslan Rakhimov and Daniil Gavrilov", "abstract": "  Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.\n", "link": "http://arxiv.org/abs/2508.04280v1", "date": "2025-08-06", "relevancy": 2.8622, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision-Language%20Model%20Training%20with%20Reinforcement%20Learning%20in%0A%20%20Synthetic%20Worlds%20for%20Real-World%20Success&body=Title%3A%20Enhancing%20Vision-Language%20Model%20Training%20with%20Reinforcement%20Learning%20in%0A%20%20Synthetic%20Worlds%20for%20Real-World%20Success%0AAuthor%3A%20George%20Bredis%20and%20Stanislav%20Dereka%20and%20Viacheslav%20Sinii%20and%20Ruslan%20Rakhimov%20and%20Daniil%20Gavrilov%0AAbstract%3A%20%20%20Interactive%20multimodal%20agents%20must%20convert%20raw%20visual%20observations%20into%0Acoherent%20sequences%20of%20language-conditioned%20actions%20--%20a%20capability%20that%20current%0Avision-language%20models%20%28VLMs%29%20still%20lack.%20Earlier%20reinforcement-learning%20%28RL%29%0Aefforts%20could%2C%20in%20principle%2C%20endow%20VLMs%20with%20such%20skills%2C%20but%20they%20have%20seldom%0Atested%20whether%20the%20learned%20behaviours%20generalize%20beyond%20their%20training%0Asimulators%2C%20and%20they%20depend%20either%20on%20brittle%20hyperparameter%20tuning%20or%20on%0Adense-reward%20environments%20with%20low%20state%20variability.%20We%20introduce%0AVision-Language%20Decoupled%20Actor-Critic%20%28VL-DAC%29%2C%20a%20lightweight%2C%0Ahyperparameter-free%20RL%20algorithm.%20VL-DAC%20applies%20PPO%20updates%20to%20action%20tokens%0Awhile%20learning%20value%20only%20at%20the%20environment-step%20level%3A%20an%20arrangement%2C%20to%20our%0Aknowledge%2C%20not%20previously%20explored%20for%20large%20VLMs%20or%20LLMs.%20This%20simple%0Adecoupling%20removes%20unstable%20weighting%20terms%20and%20yields%20faster%2C%20more%20reliable%0Aconvergence.%20Training%20a%20single%20VLM%20with%20VL-DAC%20in%20one%20inexpensive%20simulator%20at%0Aa%20time%20%28MiniWorld%2C%20Gym-Cards%2C%20ALFWorld%2C%20or%20WebShop%29%20already%20produces%20policies%0Athat%20generalize%20widely%3A%20%2B50%5C%25%20relative%20on%20BALROG%20%28game-centric%20agentic%0Acontrol%29%2C%20%2B5%5C%25%20relative%20on%20the%20hardest%20part%20of%20VSI-Bench%20%28spatial%20planning%29%2C%0Aand%20%2B2%5C%25%20on%20VisualWebBench%20%28web%20navigation%29%2C%20all%20without%20degrading%20general%0Aimage%20understanding%20accuracy.%20These%20results%20provide%20the%20first%20evidence%20that%20a%0Asimple%20RL%20algorithm%20can%20train%20VLMs%20entirely%20in%20cheap%20synthetic%20worlds%20while%0Adelivering%20measurable%20gains%20on%20real-image%20agentic%2C%20spatial-reasoning%2C%20and%0Aweb-navigation%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision-Language%2520Model%2520Training%2520with%2520Reinforcement%2520Learning%2520in%250A%2520%2520Synthetic%2520Worlds%2520for%2520Real-World%2520Success%26entry.906535625%3DGeorge%2520Bredis%2520and%2520Stanislav%2520Dereka%2520and%2520Viacheslav%2520Sinii%2520and%2520Ruslan%2520Rakhimov%2520and%2520Daniil%2520Gavrilov%26entry.1292438233%3D%2520%2520Interactive%2520multimodal%2520agents%2520must%2520convert%2520raw%2520visual%2520observations%2520into%250Acoherent%2520sequences%2520of%2520language-conditioned%2520actions%2520--%2520a%2520capability%2520that%2520current%250Avision-language%2520models%2520%2528VLMs%2529%2520still%2520lack.%2520Earlier%2520reinforcement-learning%2520%2528RL%2529%250Aefforts%2520could%252C%2520in%2520principle%252C%2520endow%2520VLMs%2520with%2520such%2520skills%252C%2520but%2520they%2520have%2520seldom%250Atested%2520whether%2520the%2520learned%2520behaviours%2520generalize%2520beyond%2520their%2520training%250Asimulators%252C%2520and%2520they%2520depend%2520either%2520on%2520brittle%2520hyperparameter%2520tuning%2520or%2520on%250Adense-reward%2520environments%2520with%2520low%2520state%2520variability.%2520We%2520introduce%250AVision-Language%2520Decoupled%2520Actor-Critic%2520%2528VL-DAC%2529%252C%2520a%2520lightweight%252C%250Ahyperparameter-free%2520RL%2520algorithm.%2520VL-DAC%2520applies%2520PPO%2520updates%2520to%2520action%2520tokens%250Awhile%2520learning%2520value%2520only%2520at%2520the%2520environment-step%2520level%253A%2520an%2520arrangement%252C%2520to%2520our%250Aknowledge%252C%2520not%2520previously%2520explored%2520for%2520large%2520VLMs%2520or%2520LLMs.%2520This%2520simple%250Adecoupling%2520removes%2520unstable%2520weighting%2520terms%2520and%2520yields%2520faster%252C%2520more%2520reliable%250Aconvergence.%2520Training%2520a%2520single%2520VLM%2520with%2520VL-DAC%2520in%2520one%2520inexpensive%2520simulator%2520at%250Aa%2520time%2520%2528MiniWorld%252C%2520Gym-Cards%252C%2520ALFWorld%252C%2520or%2520WebShop%2529%2520already%2520produces%2520policies%250Athat%2520generalize%2520widely%253A%2520%252B50%255C%2525%2520relative%2520on%2520BALROG%2520%2528game-centric%2520agentic%250Acontrol%2529%252C%2520%252B5%255C%2525%2520relative%2520on%2520the%2520hardest%2520part%2520of%2520VSI-Bench%2520%2528spatial%2520planning%2529%252C%250Aand%2520%252B2%255C%2525%2520on%2520VisualWebBench%2520%2528web%2520navigation%2529%252C%2520all%2520without%2520degrading%2520general%250Aimage%2520understanding%2520accuracy.%2520These%2520results%2520provide%2520the%2520first%2520evidence%2520that%2520a%250Asimple%2520RL%2520algorithm%2520can%2520train%2520VLMs%2520entirely%2520in%2520cheap%2520synthetic%2520worlds%2520while%250Adelivering%2520measurable%2520gains%2520on%2520real-image%2520agentic%252C%2520spatial-reasoning%252C%2520and%250Aweb-navigation%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision-Language%20Model%20Training%20with%20Reinforcement%20Learning%20in%0A%20%20Synthetic%20Worlds%20for%20Real-World%20Success&entry.906535625=George%20Bredis%20and%20Stanislav%20Dereka%20and%20Viacheslav%20Sinii%20and%20Ruslan%20Rakhimov%20and%20Daniil%20Gavrilov&entry.1292438233=%20%20Interactive%20multimodal%20agents%20must%20convert%20raw%20visual%20observations%20into%0Acoherent%20sequences%20of%20language-conditioned%20actions%20--%20a%20capability%20that%20current%0Avision-language%20models%20%28VLMs%29%20still%20lack.%20Earlier%20reinforcement-learning%20%28RL%29%0Aefforts%20could%2C%20in%20principle%2C%20endow%20VLMs%20with%20such%20skills%2C%20but%20they%20have%20seldom%0Atested%20whether%20the%20learned%20behaviours%20generalize%20beyond%20their%20training%0Asimulators%2C%20and%20they%20depend%20either%20on%20brittle%20hyperparameter%20tuning%20or%20on%0Adense-reward%20environments%20with%20low%20state%20variability.%20We%20introduce%0AVision-Language%20Decoupled%20Actor-Critic%20%28VL-DAC%29%2C%20a%20lightweight%2C%0Ahyperparameter-free%20RL%20algorithm.%20VL-DAC%20applies%20PPO%20updates%20to%20action%20tokens%0Awhile%20learning%20value%20only%20at%20the%20environment-step%20level%3A%20an%20arrangement%2C%20to%20our%0Aknowledge%2C%20not%20previously%20explored%20for%20large%20VLMs%20or%20LLMs.%20This%20simple%0Adecoupling%20removes%20unstable%20weighting%20terms%20and%20yields%20faster%2C%20more%20reliable%0Aconvergence.%20Training%20a%20single%20VLM%20with%20VL-DAC%20in%20one%20inexpensive%20simulator%20at%0Aa%20time%20%28MiniWorld%2C%20Gym-Cards%2C%20ALFWorld%2C%20or%20WebShop%29%20already%20produces%20policies%0Athat%20generalize%20widely%3A%20%2B50%5C%25%20relative%20on%20BALROG%20%28game-centric%20agentic%0Acontrol%29%2C%20%2B5%5C%25%20relative%20on%20the%20hardest%20part%20of%20VSI-Bench%20%28spatial%20planning%29%2C%0Aand%20%2B2%5C%25%20on%20VisualWebBench%20%28web%20navigation%29%2C%20all%20without%20degrading%20general%0Aimage%20understanding%20accuracy.%20These%20results%20provide%20the%20first%20evidence%20that%20a%0Asimple%20RL%20algorithm%20can%20train%20VLMs%20entirely%20in%20cheap%20synthetic%20worlds%20while%0Adelivering%20measurable%20gains%20on%20real-image%20agentic%2C%20spatial-reasoning%2C%20and%0Aweb-navigation%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04280v1&entry.124074799=Read"},
{"title": "Boosting Visual Knowledge-Intensive Training for LVLMs Through\n  Causality-Driven Visual Object Completion", "author": "Qingguo Hu and Ante Wang and Jia Song and Delai Qiu and Qingsong Liu and Jinsong Su", "abstract": "  Large Vision-Language Models (LVLMs) have experienced significant\nadvancements in recent years. However, their performance still falls short in\ntasks requiring deep visual perception, such as identifying subtle differences\nbetween images. A potential cause is the scarcity of visual knowledge in\npopular instruction-tuning corpora, resulting in inadequate visual perception\nand reasoning capabilities. To address this challenge, we introduce a\nself-improvement framework grounded in a novel visual knowledge-intensive task,\n\\underline{C}ausality-driven \\underline{V}isual object \\underline{C}ompletion\n(CVC). This task requires LVLMs to infer the masked object in an image based on\nits \\textit{causal} relationships with the other visible information. We first\nobtain rich examples cheaply through our automated instance construction\npipeline, without relying on sophisticated LVLMs (\\textit{e.g.}, GPT-4V) or\nhuman assistance. Then, LVLMs effectively self-improve through trial and error\nlearning using these created instances. Our experiments demonstrate substantial\ngains across four challenging specialized tasks and four widely-used\ncomprehensive benchmarks. Especially on specialized tasks, our method achieves\nan average improvement of 5.4\\% and 4.0\\% compared to the corresponding\nbaselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code\nis available at https://github.com/XMUDeepLIT/CVC.\n", "link": "http://arxiv.org/abs/2508.04453v1", "date": "2025-08-06", "relevancy": 2.8602, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5856}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Visual%20Knowledge-Intensive%20Training%20for%20LVLMs%20Through%0A%20%20Causality-Driven%20Visual%20Object%20Completion&body=Title%3A%20Boosting%20Visual%20Knowledge-Intensive%20Training%20for%20LVLMs%20Through%0A%20%20Causality-Driven%20Visual%20Object%20Completion%0AAuthor%3A%20Qingguo%20Hu%20and%20Ante%20Wang%20and%20Jia%20Song%20and%20Delai%20Qiu%20and%20Qingsong%20Liu%20and%20Jinsong%20Su%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20experienced%20significant%0Aadvancements%20in%20recent%20years.%20However%2C%20their%20performance%20still%20falls%20short%20in%0Atasks%20requiring%20deep%20visual%20perception%2C%20such%20as%20identifying%20subtle%20differences%0Abetween%20images.%20A%20potential%20cause%20is%20the%20scarcity%20of%20visual%20knowledge%20in%0Apopular%20instruction-tuning%20corpora%2C%20resulting%20in%20inadequate%20visual%20perception%0Aand%20reasoning%20capabilities.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Aself-improvement%20framework%20grounded%20in%20a%20novel%20visual%20knowledge-intensive%20task%2C%0A%5Cunderline%7BC%7Dausality-driven%20%5Cunderline%7BV%7Disual%20object%20%5Cunderline%7BC%7Dompletion%0A%28CVC%29.%20This%20task%20requires%20LVLMs%20to%20infer%20the%20masked%20object%20in%20an%20image%20based%20on%0Aits%20%5Ctextit%7Bcausal%7D%20relationships%20with%20the%20other%20visible%20information.%20We%20first%0Aobtain%20rich%20examples%20cheaply%20through%20our%20automated%20instance%20construction%0Apipeline%2C%20without%20relying%20on%20sophisticated%20LVLMs%20%28%5Ctextit%7Be.g.%7D%2C%20GPT-4V%29%20or%0Ahuman%20assistance.%20Then%2C%20LVLMs%20effectively%20self-improve%20through%20trial%20and%20error%0Alearning%20using%20these%20created%20instances.%20Our%20experiments%20demonstrate%20substantial%0Agains%20across%20four%20challenging%20specialized%20tasks%20and%20four%20widely-used%0Acomprehensive%20benchmarks.%20Especially%20on%20specialized%20tasks%2C%20our%20method%20achieves%0Aan%20average%20improvement%20of%205.4%5C%25%20and%204.0%5C%25%20compared%20to%20the%20corresponding%0Abaselines%20when%20utilizing%20LLaVA-1.5-7B%20and%20LLaVA-1.5-13B%2C%20respectively.%20The%20code%0Ais%20available%20at%20https%3A//github.com/XMUDeepLIT/CVC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Visual%2520Knowledge-Intensive%2520Training%2520for%2520LVLMs%2520Through%250A%2520%2520Causality-Driven%2520Visual%2520Object%2520Completion%26entry.906535625%3DQingguo%2520Hu%2520and%2520Ante%2520Wang%2520and%2520Jia%2520Song%2520and%2520Delai%2520Qiu%2520and%2520Qingsong%2520Liu%2520and%2520Jinsong%2520Su%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520experienced%2520significant%250Aadvancements%2520in%2520recent%2520years.%2520However%252C%2520their%2520performance%2520still%2520falls%2520short%2520in%250Atasks%2520requiring%2520deep%2520visual%2520perception%252C%2520such%2520as%2520identifying%2520subtle%2520differences%250Abetween%2520images.%2520A%2520potential%2520cause%2520is%2520the%2520scarcity%2520of%2520visual%2520knowledge%2520in%250Apopular%2520instruction-tuning%2520corpora%252C%2520resulting%2520in%2520inadequate%2520visual%2520perception%250Aand%2520reasoning%2520capabilities.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%250Aself-improvement%2520framework%2520grounded%2520in%2520a%2520novel%2520visual%2520knowledge-intensive%2520task%252C%250A%255Cunderline%257BC%257Dausality-driven%2520%255Cunderline%257BV%257Disual%2520object%2520%255Cunderline%257BC%257Dompletion%250A%2528CVC%2529.%2520This%2520task%2520requires%2520LVLMs%2520to%2520infer%2520the%2520masked%2520object%2520in%2520an%2520image%2520based%2520on%250Aits%2520%255Ctextit%257Bcausal%257D%2520relationships%2520with%2520the%2520other%2520visible%2520information.%2520We%2520first%250Aobtain%2520rich%2520examples%2520cheaply%2520through%2520our%2520automated%2520instance%2520construction%250Apipeline%252C%2520without%2520relying%2520on%2520sophisticated%2520LVLMs%2520%2528%255Ctextit%257Be.g.%257D%252C%2520GPT-4V%2529%2520or%250Ahuman%2520assistance.%2520Then%252C%2520LVLMs%2520effectively%2520self-improve%2520through%2520trial%2520and%2520error%250Alearning%2520using%2520these%2520created%2520instances.%2520Our%2520experiments%2520demonstrate%2520substantial%250Agains%2520across%2520four%2520challenging%2520specialized%2520tasks%2520and%2520four%2520widely-used%250Acomprehensive%2520benchmarks.%2520Especially%2520on%2520specialized%2520tasks%252C%2520our%2520method%2520achieves%250Aan%2520average%2520improvement%2520of%25205.4%255C%2525%2520and%25204.0%255C%2525%2520compared%2520to%2520the%2520corresponding%250Abaselines%2520when%2520utilizing%2520LLaVA-1.5-7B%2520and%2520LLaVA-1.5-13B%252C%2520respectively.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/XMUDeepLIT/CVC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Visual%20Knowledge-Intensive%20Training%20for%20LVLMs%20Through%0A%20%20Causality-Driven%20Visual%20Object%20Completion&entry.906535625=Qingguo%20Hu%20and%20Ante%20Wang%20and%20Jia%20Song%20and%20Delai%20Qiu%20and%20Qingsong%20Liu%20and%20Jinsong%20Su&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20experienced%20significant%0Aadvancements%20in%20recent%20years.%20However%2C%20their%20performance%20still%20falls%20short%20in%0Atasks%20requiring%20deep%20visual%20perception%2C%20such%20as%20identifying%20subtle%20differences%0Abetween%20images.%20A%20potential%20cause%20is%20the%20scarcity%20of%20visual%20knowledge%20in%0Apopular%20instruction-tuning%20corpora%2C%20resulting%20in%20inadequate%20visual%20perception%0Aand%20reasoning%20capabilities.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%0Aself-improvement%20framework%20grounded%20in%20a%20novel%20visual%20knowledge-intensive%20task%2C%0A%5Cunderline%7BC%7Dausality-driven%20%5Cunderline%7BV%7Disual%20object%20%5Cunderline%7BC%7Dompletion%0A%28CVC%29.%20This%20task%20requires%20LVLMs%20to%20infer%20the%20masked%20object%20in%20an%20image%20based%20on%0Aits%20%5Ctextit%7Bcausal%7D%20relationships%20with%20the%20other%20visible%20information.%20We%20first%0Aobtain%20rich%20examples%20cheaply%20through%20our%20automated%20instance%20construction%0Apipeline%2C%20without%20relying%20on%20sophisticated%20LVLMs%20%28%5Ctextit%7Be.g.%7D%2C%20GPT-4V%29%20or%0Ahuman%20assistance.%20Then%2C%20LVLMs%20effectively%20self-improve%20through%20trial%20and%20error%0Alearning%20using%20these%20created%20instances.%20Our%20experiments%20demonstrate%20substantial%0Agains%20across%20four%20challenging%20specialized%20tasks%20and%20four%20widely-used%0Acomprehensive%20benchmarks.%20Especially%20on%20specialized%20tasks%2C%20our%20method%20achieves%0Aan%20average%20improvement%20of%205.4%5C%25%20and%204.0%5C%25%20compared%20to%20the%20corresponding%0Abaselines%20when%20utilizing%20LLaVA-1.5-7B%20and%20LLaVA-1.5-13B%2C%20respectively.%20The%20code%0Ais%20available%20at%20https%3A//github.com/XMUDeepLIT/CVC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04453v1&entry.124074799=Read"},
{"title": "VisionTS++: Cross-Modal Time Series Foundation Model with Continual\n  Pre-trained Visual Backbones", "author": "Lefei Shen and Mouxiang Chen and Xu Liu and Han Fu and Xiaoxue Ren and Jianling Sun and Zhuo Li and Chenghao Liu", "abstract": "  Recent studies have revealed that vision models pre-trained on images can\nperform well in time series forecasting by reformulating forecasting as an\nimage reconstruction task, suggesting their potential as universal time series\nfoundation models. However, effective cross-modal transfer from vision to time\nseries remains challenging due to three key discrepancies: (1) data-modality\ngap between structured, bounded image data and unbounded, heterogeneous time\nseries; (2) multivariate-forecasting gap between standard RGB\nthree-channel-based vision models and the need to model time series with\narbitrary numbers of variates; and (3) probabilistic-forecasting gap between\nthe deterministic output formats of most vision models and the requirement for\nuncertainty-aware probabilistic predictions. To bridge these gaps, we propose\nVisionTS++, a vision-model-based TSFM that performs continual pre-training on\nlarge-scale time series datasets, including 3 innovations: (1) a\nvision-model-based filtering mechanism to identify high-quality time series\ndata, thereby mitigating modality gap and improving pre-training stability, (2)\na colorized multivariate conversion method that transforms multivariate time\nseries into multi-subfigure RGB images, capturing complex inter-variate\ndependencies; and (3) a multi-quantile forecasting approach using parallel\nreconstruction heads to generate forecasts of different quantile levels, thus\nmore flexibly approximating arbitrary output distributions without restrictive\nprior distributional assumptions. Evaluated on both in-distribution and\nout-of-distribution TSF benchmarks, \\model achieves SOTA results, outperforming\nspecialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12\nprobabilistic forecasting settings. Our work establishes a new paradigm for\ncross-modal knowledge transfer, advancing the development of universal TSFMs.\n", "link": "http://arxiv.org/abs/2508.04379v1", "date": "2025-08-06", "relevancy": 2.8472, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.567}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionTS%2B%2B%3A%20Cross-Modal%20Time%20Series%20Foundation%20Model%20with%20Continual%0A%20%20Pre-trained%20Visual%20Backbones&body=Title%3A%20VisionTS%2B%2B%3A%20Cross-Modal%20Time%20Series%20Foundation%20Model%20with%20Continual%0A%20%20Pre-trained%20Visual%20Backbones%0AAuthor%3A%20Lefei%20Shen%20and%20Mouxiang%20Chen%20and%20Xu%20Liu%20and%20Han%20Fu%20and%20Xiaoxue%20Ren%20and%20Jianling%20Sun%20and%20Zhuo%20Li%20and%20Chenghao%20Liu%0AAbstract%3A%20%20%20Recent%20studies%20have%20revealed%20that%20vision%20models%20pre-trained%20on%20images%20can%0Aperform%20well%20in%20time%20series%20forecasting%20by%20reformulating%20forecasting%20as%20an%0Aimage%20reconstruction%20task%2C%20suggesting%20their%20potential%20as%20universal%20time%20series%0Afoundation%20models.%20However%2C%20effective%20cross-modal%20transfer%20from%20vision%20to%20time%0Aseries%20remains%20challenging%20due%20to%20three%20key%20discrepancies%3A%20%281%29%20data-modality%0Agap%20between%20structured%2C%20bounded%20image%20data%20and%20unbounded%2C%20heterogeneous%20time%0Aseries%3B%20%282%29%20multivariate-forecasting%20gap%20between%20standard%20RGB%0Athree-channel-based%20vision%20models%20and%20the%20need%20to%20model%20time%20series%20with%0Aarbitrary%20numbers%20of%20variates%3B%20and%20%283%29%20probabilistic-forecasting%20gap%20between%0Athe%20deterministic%20output%20formats%20of%20most%20vision%20models%20and%20the%20requirement%20for%0Auncertainty-aware%20probabilistic%20predictions.%20To%20bridge%20these%20gaps%2C%20we%20propose%0AVisionTS%2B%2B%2C%20a%20vision-model-based%20TSFM%20that%20performs%20continual%20pre-training%20on%0Alarge-scale%20time%20series%20datasets%2C%20including%203%20innovations%3A%20%281%29%20a%0Avision-model-based%20filtering%20mechanism%20to%20identify%20high-quality%20time%20series%0Adata%2C%20thereby%20mitigating%20modality%20gap%20and%20improving%20pre-training%20stability%2C%20%282%29%0Aa%20colorized%20multivariate%20conversion%20method%20that%20transforms%20multivariate%20time%0Aseries%20into%20multi-subfigure%20RGB%20images%2C%20capturing%20complex%20inter-variate%0Adependencies%3B%20and%20%283%29%20a%20multi-quantile%20forecasting%20approach%20using%20parallel%0Areconstruction%20heads%20to%20generate%20forecasts%20of%20different%20quantile%20levels%2C%20thus%0Amore%20flexibly%20approximating%20arbitrary%20output%20distributions%20without%20restrictive%0Aprior%20distributional%20assumptions.%20Evaluated%20on%20both%20in-distribution%20and%0Aout-of-distribution%20TSF%20benchmarks%2C%20%5Cmodel%20achieves%20SOTA%20results%2C%20outperforming%0Aspecialized%20TSFMs%20by%206%25-44%25%20in%20MSE%20reduction%20and%20ranking%20first%20in%209%20out%20of%2012%0Aprobabilistic%20forecasting%20settings.%20Our%20work%20establishes%20a%20new%20paradigm%20for%0Across-modal%20knowledge%20transfer%2C%20advancing%20the%20development%20of%20universal%20TSFMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionTS%252B%252B%253A%2520Cross-Modal%2520Time%2520Series%2520Foundation%2520Model%2520with%2520Continual%250A%2520%2520Pre-trained%2520Visual%2520Backbones%26entry.906535625%3DLefei%2520Shen%2520and%2520Mouxiang%2520Chen%2520and%2520Xu%2520Liu%2520and%2520Han%2520Fu%2520and%2520Xiaoxue%2520Ren%2520and%2520Jianling%2520Sun%2520and%2520Zhuo%2520Li%2520and%2520Chenghao%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520revealed%2520that%2520vision%2520models%2520pre-trained%2520on%2520images%2520can%250Aperform%2520well%2520in%2520time%2520series%2520forecasting%2520by%2520reformulating%2520forecasting%2520as%2520an%250Aimage%2520reconstruction%2520task%252C%2520suggesting%2520their%2520potential%2520as%2520universal%2520time%2520series%250Afoundation%2520models.%2520However%252C%2520effective%2520cross-modal%2520transfer%2520from%2520vision%2520to%2520time%250Aseries%2520remains%2520challenging%2520due%2520to%2520three%2520key%2520discrepancies%253A%2520%25281%2529%2520data-modality%250Agap%2520between%2520structured%252C%2520bounded%2520image%2520data%2520and%2520unbounded%252C%2520heterogeneous%2520time%250Aseries%253B%2520%25282%2529%2520multivariate-forecasting%2520gap%2520between%2520standard%2520RGB%250Athree-channel-based%2520vision%2520models%2520and%2520the%2520need%2520to%2520model%2520time%2520series%2520with%250Aarbitrary%2520numbers%2520of%2520variates%253B%2520and%2520%25283%2529%2520probabilistic-forecasting%2520gap%2520between%250Athe%2520deterministic%2520output%2520formats%2520of%2520most%2520vision%2520models%2520and%2520the%2520requirement%2520for%250Auncertainty-aware%2520probabilistic%2520predictions.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520propose%250AVisionTS%252B%252B%252C%2520a%2520vision-model-based%2520TSFM%2520that%2520performs%2520continual%2520pre-training%2520on%250Alarge-scale%2520time%2520series%2520datasets%252C%2520including%25203%2520innovations%253A%2520%25281%2529%2520a%250Avision-model-based%2520filtering%2520mechanism%2520to%2520identify%2520high-quality%2520time%2520series%250Adata%252C%2520thereby%2520mitigating%2520modality%2520gap%2520and%2520improving%2520pre-training%2520stability%252C%2520%25282%2529%250Aa%2520colorized%2520multivariate%2520conversion%2520method%2520that%2520transforms%2520multivariate%2520time%250Aseries%2520into%2520multi-subfigure%2520RGB%2520images%252C%2520capturing%2520complex%2520inter-variate%250Adependencies%253B%2520and%2520%25283%2529%2520a%2520multi-quantile%2520forecasting%2520approach%2520using%2520parallel%250Areconstruction%2520heads%2520to%2520generate%2520forecasts%2520of%2520different%2520quantile%2520levels%252C%2520thus%250Amore%2520flexibly%2520approximating%2520arbitrary%2520output%2520distributions%2520without%2520restrictive%250Aprior%2520distributional%2520assumptions.%2520Evaluated%2520on%2520both%2520in-distribution%2520and%250Aout-of-distribution%2520TSF%2520benchmarks%252C%2520%255Cmodel%2520achieves%2520SOTA%2520results%252C%2520outperforming%250Aspecialized%2520TSFMs%2520by%25206%2525-44%2525%2520in%2520MSE%2520reduction%2520and%2520ranking%2520first%2520in%25209%2520out%2520of%252012%250Aprobabilistic%2520forecasting%2520settings.%2520Our%2520work%2520establishes%2520a%2520new%2520paradigm%2520for%250Across-modal%2520knowledge%2520transfer%252C%2520advancing%2520the%2520development%2520of%2520universal%2520TSFMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionTS%2B%2B%3A%20Cross-Modal%20Time%20Series%20Foundation%20Model%20with%20Continual%0A%20%20Pre-trained%20Visual%20Backbones&entry.906535625=Lefei%20Shen%20and%20Mouxiang%20Chen%20and%20Xu%20Liu%20and%20Han%20Fu%20and%20Xiaoxue%20Ren%20and%20Jianling%20Sun%20and%20Zhuo%20Li%20and%20Chenghao%20Liu&entry.1292438233=%20%20Recent%20studies%20have%20revealed%20that%20vision%20models%20pre-trained%20on%20images%20can%0Aperform%20well%20in%20time%20series%20forecasting%20by%20reformulating%20forecasting%20as%20an%0Aimage%20reconstruction%20task%2C%20suggesting%20their%20potential%20as%20universal%20time%20series%0Afoundation%20models.%20However%2C%20effective%20cross-modal%20transfer%20from%20vision%20to%20time%0Aseries%20remains%20challenging%20due%20to%20three%20key%20discrepancies%3A%20%281%29%20data-modality%0Agap%20between%20structured%2C%20bounded%20image%20data%20and%20unbounded%2C%20heterogeneous%20time%0Aseries%3B%20%282%29%20multivariate-forecasting%20gap%20between%20standard%20RGB%0Athree-channel-based%20vision%20models%20and%20the%20need%20to%20model%20time%20series%20with%0Aarbitrary%20numbers%20of%20variates%3B%20and%20%283%29%20probabilistic-forecasting%20gap%20between%0Athe%20deterministic%20output%20formats%20of%20most%20vision%20models%20and%20the%20requirement%20for%0Auncertainty-aware%20probabilistic%20predictions.%20To%20bridge%20these%20gaps%2C%20we%20propose%0AVisionTS%2B%2B%2C%20a%20vision-model-based%20TSFM%20that%20performs%20continual%20pre-training%20on%0Alarge-scale%20time%20series%20datasets%2C%20including%203%20innovations%3A%20%281%29%20a%0Avision-model-based%20filtering%20mechanism%20to%20identify%20high-quality%20time%20series%0Adata%2C%20thereby%20mitigating%20modality%20gap%20and%20improving%20pre-training%20stability%2C%20%282%29%0Aa%20colorized%20multivariate%20conversion%20method%20that%20transforms%20multivariate%20time%0Aseries%20into%20multi-subfigure%20RGB%20images%2C%20capturing%20complex%20inter-variate%0Adependencies%3B%20and%20%283%29%20a%20multi-quantile%20forecasting%20approach%20using%20parallel%0Areconstruction%20heads%20to%20generate%20forecasts%20of%20different%20quantile%20levels%2C%20thus%0Amore%20flexibly%20approximating%20arbitrary%20output%20distributions%20without%20restrictive%0Aprior%20distributional%20assumptions.%20Evaluated%20on%20both%20in-distribution%20and%0Aout-of-distribution%20TSF%20benchmarks%2C%20%5Cmodel%20achieves%20SOTA%20results%2C%20outperforming%0Aspecialized%20TSFMs%20by%206%25-44%25%20in%20MSE%20reduction%20and%20ranking%20first%20in%209%20out%20of%2012%0Aprobabilistic%20forecasting%20settings.%20Our%20work%20establishes%20a%20new%20paradigm%20for%0Across-modal%20knowledge%20transfer%2C%20advancing%20the%20development%20of%20universal%20TSFMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04379v1&entry.124074799=Read"},
{"title": "GenEDA: Towards Generative Netlist Functional Reasoning via Cross-Modal\n  Circuit Encoder-Decoder Alignment", "author": "Wenji Fang and Jing Wang and Yao Lu and Shang Liu and Zhiyao Xie", "abstract": "  The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit foundation models are typically\nlimited to standalone encoders for predictive tasks or decoders for generative\ntasks. These two model types are developed independently, operate on different\ncircuit modalities, and reside in separate latent spaces. This restricts their\nability to complement each other for more advanced capabilities. In this work,\nwe present GenEDA, the first framework that cross-modally aligns circuit\nencoders with decoders within a shared latent space. GenEDA bridges the gap\nbetween graph-based circuit representation learning and text-based large\nlanguage models (LLMs), enabling communication between their respective latent\nspaces. To achieve the alignment, we propose two paradigms to support both\nopen-source trainable LLMs and commercial frozen LLMs. We leverage this aligned\narchitecture to develop the first generative foundation model for netlists,\nunleashing LLMs' generative reasoning capability on the low-level and\nbit-blasted netlists. GenEDA enables three unprecedented generative netlist\nfunctional reasoning tasks, where it reversely generates high-level\nfunctionalities such as specifications and RTL code from low-level netlists.\nThese tasks move beyond traditional gate function classification to direct\ngeneration of full-circuit functionality. Experiments demonstrate that GenEDA\nsignificantly boosts advanced LLMs' (e.g., GPT and DeepSeek series) performance\nin all tasks.\n", "link": "http://arxiv.org/abs/2504.09485v2", "date": "2025-08-06", "relevancy": 2.838, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenEDA%3A%20Towards%20Generative%20Netlist%20Functional%20Reasoning%20via%20Cross-Modal%0A%20%20Circuit%20Encoder-Decoder%20Alignment&body=Title%3A%20GenEDA%3A%20Towards%20Generative%20Netlist%20Functional%20Reasoning%20via%20Cross-Modal%0A%20%20Circuit%20Encoder-Decoder%20Alignment%0AAuthor%3A%20Wenji%20Fang%20and%20Jing%20Wang%20and%20Yao%20Lu%20and%20Shang%20Liu%20and%20Zhiyao%20Xie%0AAbstract%3A%20%20%20The%20success%20of%20foundation%20AI%20has%20motivated%20the%20research%20of%20circuit%20foundation%0Amodels%2C%20which%20are%20customized%20to%20assist%20the%20integrated%20circuit%20%28IC%29%20design%0Aprocess.%20However%2C%20existing%20pre-trained%20circuit%20foundation%20models%20are%20typically%0Alimited%20to%20standalone%20encoders%20for%20predictive%20tasks%20or%20decoders%20for%20generative%0Atasks.%20These%20two%20model%20types%20are%20developed%20independently%2C%20operate%20on%20different%0Acircuit%20modalities%2C%20and%20reside%20in%20separate%20latent%20spaces.%20This%20restricts%20their%0Aability%20to%20complement%20each%20other%20for%20more%20advanced%20capabilities.%20In%20this%20work%2C%0Awe%20present%20GenEDA%2C%20the%20first%20framework%20that%20cross-modally%20aligns%20circuit%0Aencoders%20with%20decoders%20within%20a%20shared%20latent%20space.%20GenEDA%20bridges%20the%20gap%0Abetween%20graph-based%20circuit%20representation%20learning%20and%20text-based%20large%0Alanguage%20models%20%28LLMs%29%2C%20enabling%20communication%20between%20their%20respective%20latent%0Aspaces.%20To%20achieve%20the%20alignment%2C%20we%20propose%20two%20paradigms%20to%20support%20both%0Aopen-source%20trainable%20LLMs%20and%20commercial%20frozen%20LLMs.%20We%20leverage%20this%20aligned%0Aarchitecture%20to%20develop%20the%20first%20generative%20foundation%20model%20for%20netlists%2C%0Aunleashing%20LLMs%27%20generative%20reasoning%20capability%20on%20the%20low-level%20and%0Abit-blasted%20netlists.%20GenEDA%20enables%20three%20unprecedented%20generative%20netlist%0Afunctional%20reasoning%20tasks%2C%20where%20it%20reversely%20generates%20high-level%0Afunctionalities%20such%20as%20specifications%20and%20RTL%20code%20from%20low-level%20netlists.%0AThese%20tasks%20move%20beyond%20traditional%20gate%20function%20classification%20to%20direct%0Ageneration%20of%20full-circuit%20functionality.%20Experiments%20demonstrate%20that%20GenEDA%0Asignificantly%20boosts%20advanced%20LLMs%27%20%28e.g.%2C%20GPT%20and%20DeepSeek%20series%29%20performance%0Ain%20all%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09485v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenEDA%253A%2520Towards%2520Generative%2520Netlist%2520Functional%2520Reasoning%2520via%2520Cross-Modal%250A%2520%2520Circuit%2520Encoder-Decoder%2520Alignment%26entry.906535625%3DWenji%2520Fang%2520and%2520Jing%2520Wang%2520and%2520Yao%2520Lu%2520and%2520Shang%2520Liu%2520and%2520Zhiyao%2520Xie%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520foundation%2520AI%2520has%2520motivated%2520the%2520research%2520of%2520circuit%2520foundation%250Amodels%252C%2520which%2520are%2520customized%2520to%2520assist%2520the%2520integrated%2520circuit%2520%2528IC%2529%2520design%250Aprocess.%2520However%252C%2520existing%2520pre-trained%2520circuit%2520foundation%2520models%2520are%2520typically%250Alimited%2520to%2520standalone%2520encoders%2520for%2520predictive%2520tasks%2520or%2520decoders%2520for%2520generative%250Atasks.%2520These%2520two%2520model%2520types%2520are%2520developed%2520independently%252C%2520operate%2520on%2520different%250Acircuit%2520modalities%252C%2520and%2520reside%2520in%2520separate%2520latent%2520spaces.%2520This%2520restricts%2520their%250Aability%2520to%2520complement%2520each%2520other%2520for%2520more%2520advanced%2520capabilities.%2520In%2520this%2520work%252C%250Awe%2520present%2520GenEDA%252C%2520the%2520first%2520framework%2520that%2520cross-modally%2520aligns%2520circuit%250Aencoders%2520with%2520decoders%2520within%2520a%2520shared%2520latent%2520space.%2520GenEDA%2520bridges%2520the%2520gap%250Abetween%2520graph-based%2520circuit%2520representation%2520learning%2520and%2520text-based%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520enabling%2520communication%2520between%2520their%2520respective%2520latent%250Aspaces.%2520To%2520achieve%2520the%2520alignment%252C%2520we%2520propose%2520two%2520paradigms%2520to%2520support%2520both%250Aopen-source%2520trainable%2520LLMs%2520and%2520commercial%2520frozen%2520LLMs.%2520We%2520leverage%2520this%2520aligned%250Aarchitecture%2520to%2520develop%2520the%2520first%2520generative%2520foundation%2520model%2520for%2520netlists%252C%250Aunleashing%2520LLMs%2527%2520generative%2520reasoning%2520capability%2520on%2520the%2520low-level%2520and%250Abit-blasted%2520netlists.%2520GenEDA%2520enables%2520three%2520unprecedented%2520generative%2520netlist%250Afunctional%2520reasoning%2520tasks%252C%2520where%2520it%2520reversely%2520generates%2520high-level%250Afunctionalities%2520such%2520as%2520specifications%2520and%2520RTL%2520code%2520from%2520low-level%2520netlists.%250AThese%2520tasks%2520move%2520beyond%2520traditional%2520gate%2520function%2520classification%2520to%2520direct%250Ageneration%2520of%2520full-circuit%2520functionality.%2520Experiments%2520demonstrate%2520that%2520GenEDA%250Asignificantly%2520boosts%2520advanced%2520LLMs%2527%2520%2528e.g.%252C%2520GPT%2520and%2520DeepSeek%2520series%2529%2520performance%250Ain%2520all%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09485v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenEDA%3A%20Towards%20Generative%20Netlist%20Functional%20Reasoning%20via%20Cross-Modal%0A%20%20Circuit%20Encoder-Decoder%20Alignment&entry.906535625=Wenji%20Fang%20and%20Jing%20Wang%20and%20Yao%20Lu%20and%20Shang%20Liu%20and%20Zhiyao%20Xie&entry.1292438233=%20%20The%20success%20of%20foundation%20AI%20has%20motivated%20the%20research%20of%20circuit%20foundation%0Amodels%2C%20which%20are%20customized%20to%20assist%20the%20integrated%20circuit%20%28IC%29%20design%0Aprocess.%20However%2C%20existing%20pre-trained%20circuit%20foundation%20models%20are%20typically%0Alimited%20to%20standalone%20encoders%20for%20predictive%20tasks%20or%20decoders%20for%20generative%0Atasks.%20These%20two%20model%20types%20are%20developed%20independently%2C%20operate%20on%20different%0Acircuit%20modalities%2C%20and%20reside%20in%20separate%20latent%20spaces.%20This%20restricts%20their%0Aability%20to%20complement%20each%20other%20for%20more%20advanced%20capabilities.%20In%20this%20work%2C%0Awe%20present%20GenEDA%2C%20the%20first%20framework%20that%20cross-modally%20aligns%20circuit%0Aencoders%20with%20decoders%20within%20a%20shared%20latent%20space.%20GenEDA%20bridges%20the%20gap%0Abetween%20graph-based%20circuit%20representation%20learning%20and%20text-based%20large%0Alanguage%20models%20%28LLMs%29%2C%20enabling%20communication%20between%20their%20respective%20latent%0Aspaces.%20To%20achieve%20the%20alignment%2C%20we%20propose%20two%20paradigms%20to%20support%20both%0Aopen-source%20trainable%20LLMs%20and%20commercial%20frozen%20LLMs.%20We%20leverage%20this%20aligned%0Aarchitecture%20to%20develop%20the%20first%20generative%20foundation%20model%20for%20netlists%2C%0Aunleashing%20LLMs%27%20generative%20reasoning%20capability%20on%20the%20low-level%20and%0Abit-blasted%20netlists.%20GenEDA%20enables%20three%20unprecedented%20generative%20netlist%0Afunctional%20reasoning%20tasks%2C%20where%20it%20reversely%20generates%20high-level%0Afunctionalities%20such%20as%20specifications%20and%20RTL%20code%20from%20low-level%20netlists.%0AThese%20tasks%20move%20beyond%20traditional%20gate%20function%20classification%20to%20direct%0Ageneration%20of%20full-circuit%20functionality.%20Experiments%20demonstrate%20that%20GenEDA%0Asignificantly%20boosts%20advanced%20LLMs%27%20%28e.g.%2C%20GPT%20and%20DeepSeek%20series%29%20performance%0Ain%20all%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09485v2&entry.124074799=Read"},
{"title": "DOGR: Towards Versatile Visual Document Grounding and Referring", "author": "Yinan Zhou and Yuxin Chen and Haokun Lin and Yichen Wu and Shuyu Yang and Zhongang Qi and Chen Ma and Li Zhu and Ying Shan", "abstract": "  With recent advances in Multimodal Large Language Models (MLLMs), grounding\nand referring capabilities have gained increasing attention for achieving\ndetailed understanding and flexible user interaction. However, these\ncapabilities still remain underdeveloped in visual document understanding due\nto the scarcity of fine-grained datasets and comprehensive benchmarks. To fill\nthis gap, we propose the DOcument Grounding and Referring data engine\n(DOGR-Engine), which generates two types of high-quality fine-grained document\ndata: (1) multi-granular parsing data to improve text localization and\nrecognition, and (2) instruction-tuning data to activate MLLMs' grounding and\nreferring capabilities in dialogue and reasoning. Using the DOGR-Engine, we\nconstruct DOGR-Bench, a benchmark covering seven grounding and referring tasks\nacross three document types (chart, poster, and PDF document), offering a\ncomprehensive evaluation of fine-grained document understanding. Leveraging the\ngenerated data, we further develop DOGR, a strong baseline model that excels in\ntext localization and recognition, while precisely grounds and refers to key\ntextual information during conversation and reasoning, thereby advancing\ndocument understanding to a finer granularity and enable flexible interaction\nparadigms.\n", "link": "http://arxiv.org/abs/2411.17125v3", "date": "2025-08-06", "relevancy": 2.8084, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5718}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOGR%3A%20Towards%20Versatile%20Visual%20Document%20Grounding%20and%20Referring&body=Title%3A%20DOGR%3A%20Towards%20Versatile%20Visual%20Document%20Grounding%20and%20Referring%0AAuthor%3A%20Yinan%20Zhou%20and%20Yuxin%20Chen%20and%20Haokun%20Lin%20and%20Yichen%20Wu%20and%20Shuyu%20Yang%20and%20Zhongang%20Qi%20and%20Chen%20Ma%20and%20Li%20Zhu%20and%20Ying%20Shan%0AAbstract%3A%20%20%20With%20recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20grounding%0Aand%20referring%20capabilities%20have%20gained%20increasing%20attention%20for%20achieving%0Adetailed%20understanding%20and%20flexible%20user%20interaction.%20However%2C%20these%0Acapabilities%20still%20remain%20underdeveloped%20in%20visual%20document%20understanding%20due%0Ato%20the%20scarcity%20of%20fine-grained%20datasets%20and%20comprehensive%20benchmarks.%20To%20fill%0Athis%20gap%2C%20we%20propose%20the%20DOcument%20Grounding%20and%20Referring%20data%20engine%0A%28DOGR-Engine%29%2C%20which%20generates%20two%20types%20of%20high-quality%20fine-grained%20document%0Adata%3A%20%281%29%20multi-granular%20parsing%20data%20to%20improve%20text%20localization%20and%0Arecognition%2C%20and%20%282%29%20instruction-tuning%20data%20to%20activate%20MLLMs%27%20grounding%20and%0Areferring%20capabilities%20in%20dialogue%20and%20reasoning.%20Using%20the%20DOGR-Engine%2C%20we%0Aconstruct%20DOGR-Bench%2C%20a%20benchmark%20covering%20seven%20grounding%20and%20referring%20tasks%0Aacross%20three%20document%20types%20%28chart%2C%20poster%2C%20and%20PDF%20document%29%2C%20offering%20a%0Acomprehensive%20evaluation%20of%20fine-grained%20document%20understanding.%20Leveraging%20the%0Agenerated%20data%2C%20we%20further%20develop%20DOGR%2C%20a%20strong%20baseline%20model%20that%20excels%20in%0Atext%20localization%20and%20recognition%2C%20while%20precisely%20grounds%20and%20refers%20to%20key%0Atextual%20information%20during%20conversation%20and%20reasoning%2C%20thereby%20advancing%0Adocument%20understanding%20to%20a%20finer%20granularity%20and%20enable%20flexible%20interaction%0Aparadigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17125v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOGR%253A%2520Towards%2520Versatile%2520Visual%2520Document%2520Grounding%2520and%2520Referring%26entry.906535625%3DYinan%2520Zhou%2520and%2520Yuxin%2520Chen%2520and%2520Haokun%2520Lin%2520and%2520Yichen%2520Wu%2520and%2520Shuyu%2520Yang%2520and%2520Zhongang%2520Qi%2520and%2520Chen%2520Ma%2520and%2520Li%2520Zhu%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520With%2520recent%2520advances%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520grounding%250Aand%2520referring%2520capabilities%2520have%2520gained%2520increasing%2520attention%2520for%2520achieving%250Adetailed%2520understanding%2520and%2520flexible%2520user%2520interaction.%2520However%252C%2520these%250Acapabilities%2520still%2520remain%2520underdeveloped%2520in%2520visual%2520document%2520understanding%2520due%250Ato%2520the%2520scarcity%2520of%2520fine-grained%2520datasets%2520and%2520comprehensive%2520benchmarks.%2520To%2520fill%250Athis%2520gap%252C%2520we%2520propose%2520the%2520DOcument%2520Grounding%2520and%2520Referring%2520data%2520engine%250A%2528DOGR-Engine%2529%252C%2520which%2520generates%2520two%2520types%2520of%2520high-quality%2520fine-grained%2520document%250Adata%253A%2520%25281%2529%2520multi-granular%2520parsing%2520data%2520to%2520improve%2520text%2520localization%2520and%250Arecognition%252C%2520and%2520%25282%2529%2520instruction-tuning%2520data%2520to%2520activate%2520MLLMs%2527%2520grounding%2520and%250Areferring%2520capabilities%2520in%2520dialogue%2520and%2520reasoning.%2520Using%2520the%2520DOGR-Engine%252C%2520we%250Aconstruct%2520DOGR-Bench%252C%2520a%2520benchmark%2520covering%2520seven%2520grounding%2520and%2520referring%2520tasks%250Aacross%2520three%2520document%2520types%2520%2528chart%252C%2520poster%252C%2520and%2520PDF%2520document%2529%252C%2520offering%2520a%250Acomprehensive%2520evaluation%2520of%2520fine-grained%2520document%2520understanding.%2520Leveraging%2520the%250Agenerated%2520data%252C%2520we%2520further%2520develop%2520DOGR%252C%2520a%2520strong%2520baseline%2520model%2520that%2520excels%2520in%250Atext%2520localization%2520and%2520recognition%252C%2520while%2520precisely%2520grounds%2520and%2520refers%2520to%2520key%250Atextual%2520information%2520during%2520conversation%2520and%2520reasoning%252C%2520thereby%2520advancing%250Adocument%2520understanding%2520to%2520a%2520finer%2520granularity%2520and%2520enable%2520flexible%2520interaction%250Aparadigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17125v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOGR%3A%20Towards%20Versatile%20Visual%20Document%20Grounding%20and%20Referring&entry.906535625=Yinan%20Zhou%20and%20Yuxin%20Chen%20and%20Haokun%20Lin%20and%20Yichen%20Wu%20and%20Shuyu%20Yang%20and%20Zhongang%20Qi%20and%20Chen%20Ma%20and%20Li%20Zhu%20and%20Ying%20Shan&entry.1292438233=%20%20With%20recent%20advances%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20grounding%0Aand%20referring%20capabilities%20have%20gained%20increasing%20attention%20for%20achieving%0Adetailed%20understanding%20and%20flexible%20user%20interaction.%20However%2C%20these%0Acapabilities%20still%20remain%20underdeveloped%20in%20visual%20document%20understanding%20due%0Ato%20the%20scarcity%20of%20fine-grained%20datasets%20and%20comprehensive%20benchmarks.%20To%20fill%0Athis%20gap%2C%20we%20propose%20the%20DOcument%20Grounding%20and%20Referring%20data%20engine%0A%28DOGR-Engine%29%2C%20which%20generates%20two%20types%20of%20high-quality%20fine-grained%20document%0Adata%3A%20%281%29%20multi-granular%20parsing%20data%20to%20improve%20text%20localization%20and%0Arecognition%2C%20and%20%282%29%20instruction-tuning%20data%20to%20activate%20MLLMs%27%20grounding%20and%0Areferring%20capabilities%20in%20dialogue%20and%20reasoning.%20Using%20the%20DOGR-Engine%2C%20we%0Aconstruct%20DOGR-Bench%2C%20a%20benchmark%20covering%20seven%20grounding%20and%20referring%20tasks%0Aacross%20three%20document%20types%20%28chart%2C%20poster%2C%20and%20PDF%20document%29%2C%20offering%20a%0Acomprehensive%20evaluation%20of%20fine-grained%20document%20understanding.%20Leveraging%20the%0Agenerated%20data%2C%20we%20further%20develop%20DOGR%2C%20a%20strong%20baseline%20model%20that%20excels%20in%0Atext%20localization%20and%20recognition%2C%20while%20precisely%20grounds%20and%20refers%20to%20key%0Atextual%20information%20during%20conversation%20and%20reasoning%2C%20thereby%20advancing%0Adocument%20understanding%20to%20a%20finer%20granularity%20and%20enable%20flexible%20interaction%0Aparadigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17125v3&entry.124074799=Read"},
{"title": "Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video\n  Moment Retrieval", "author": "Junan Lin and Daizong Liu and Xianke Chen and Xiaoye Qu and Xun Yang and Jixiang Zhu and Sanyuan Zhang and Jianfeng Dong", "abstract": "  Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.\n", "link": "http://arxiv.org/abs/2508.04273v1", "date": "2025-08-06", "relevancy": 2.7901, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5521}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Does%20Matter%3A%20Importance-Aware%20Multi-Granularity%20Fusion%20for%20Video%0A%20%20Moment%20Retrieval&body=Title%3A%20Audio%20Does%20Matter%3A%20Importance-Aware%20Multi-Granularity%20Fusion%20for%20Video%0A%20%20Moment%20Retrieval%0AAuthor%3A%20Junan%20Lin%20and%20Daizong%20Liu%20and%20Xianke%20Chen%20and%20Xiaoye%20Qu%20and%20Xun%20Yang%20and%20Jixiang%20Zhu%20and%20Sanyuan%20Zhang%20and%20Jianfeng%20Dong%0AAbstract%3A%20%20%20Video%20Moment%20Retrieval%20%28VMR%29%20aims%20to%20retrieve%20a%20specific%20moment%20semantically%0Arelated%20to%20the%20given%20query.%20To%20tackle%20this%20task%2C%20most%20existing%20VMR%20methods%0Asolely%20focus%20on%20the%20visual%20and%20textual%20modalities%20while%20neglecting%20the%0Acomplementary%20but%20important%20audio%20modality.%20Although%20a%20few%20recent%20works%20try%20to%0Atackle%20the%20joint%20audio-vision-text%20reasoning%2C%20they%20treat%20all%20modalities%20equally%0Aand%20simply%20embed%20them%20without%20fine-grained%20interaction%20for%20moment%20retrieval.%0AThese%20designs%20are%20counter-practical%20as%3A%20Not%20all%20audios%20are%20helpful%20for%20video%0Amoment%20retrieval%2C%20and%20the%20audio%20of%20some%20videos%20may%20be%20complete%20noise%20or%0Abackground%20sound%20that%20is%20meaningless%20to%20the%20moment%20determination.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20Importance-aware%20Multi-Granularity%20fusion%20model%20%28IMG%29%2C%20which%0Alearns%20to%20dynamically%20and%20selectively%20aggregate%20the%20audio-vision-text%20contexts%0Afor%20VMR.%20Specifically%2C%20after%20integrating%20the%20textual%20guidance%20with%20vision%20and%0Aaudio%20separately%2C%20we%20first%20design%20a%20pseudo-label-supervised%20audio%20importance%0Apredictor%20that%20predicts%20the%20importance%20score%20of%20the%20audio%2C%20and%20accordingly%0Aassigns%20weights%20to%20mitigate%20the%20interference%20caused%20by%20noisy%20audio.%20Then%2C%20we%0Adesign%20a%20multi-granularity%20audio%20fusion%20module%20that%20adaptively%20fuses%20audio%20and%0Avisual%20modalities%20at%20local-%2C%20event-%2C%20and%20global-level%2C%20fully%20capturing%20their%0Acomplementary%20contexts.%20We%20further%20propose%20a%20cross-modal%20knowledge%20distillation%0Astrategy%20to%20address%20the%20challenge%20of%20missing%20audio%20modality%20during%20inference.%0ATo%20evaluate%20our%20method%2C%20we%20further%20construct%20a%20new%20VMR%20dataset%2C%20i.e.%2C%0ACharades-AudioMatter%2C%20where%20audio-related%20samples%20are%20manually%20selected%20and%0Are-organized%20from%20the%20original%20Charades-STA%20to%20validate%20the%20model%27s%20capability%0Ain%20utilizing%20audio%20modality.%20Extensive%20experiments%20validate%20the%20effectiveness%0Aof%20our%20method%2C%20achieving%20state-of-the-art%20with%20audio-video%20fusion%20in%20VMR%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/HuiGuanLab/IMG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Does%2520Matter%253A%2520Importance-Aware%2520Multi-Granularity%2520Fusion%2520for%2520Video%250A%2520%2520Moment%2520Retrieval%26entry.906535625%3DJunan%2520Lin%2520and%2520Daizong%2520Liu%2520and%2520Xianke%2520Chen%2520and%2520Xiaoye%2520Qu%2520and%2520Xun%2520Yang%2520and%2520Jixiang%2520Zhu%2520and%2520Sanyuan%2520Zhang%2520and%2520Jianfeng%2520Dong%26entry.1292438233%3D%2520%2520Video%2520Moment%2520Retrieval%2520%2528VMR%2529%2520aims%2520to%2520retrieve%2520a%2520specific%2520moment%2520semantically%250Arelated%2520to%2520the%2520given%2520query.%2520To%2520tackle%2520this%2520task%252C%2520most%2520existing%2520VMR%2520methods%250Asolely%2520focus%2520on%2520the%2520visual%2520and%2520textual%2520modalities%2520while%2520neglecting%2520the%250Acomplementary%2520but%2520important%2520audio%2520modality.%2520Although%2520a%2520few%2520recent%2520works%2520try%2520to%250Atackle%2520the%2520joint%2520audio-vision-text%2520reasoning%252C%2520they%2520treat%2520all%2520modalities%2520equally%250Aand%2520simply%2520embed%2520them%2520without%2520fine-grained%2520interaction%2520for%2520moment%2520retrieval.%250AThese%2520designs%2520are%2520counter-practical%2520as%253A%2520Not%2520all%2520audios%2520are%2520helpful%2520for%2520video%250Amoment%2520retrieval%252C%2520and%2520the%2520audio%2520of%2520some%2520videos%2520may%2520be%2520complete%2520noise%2520or%250Abackground%2520sound%2520that%2520is%2520meaningless%2520to%2520the%2520moment%2520determination.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520novel%2520Importance-aware%2520Multi-Granularity%2520fusion%2520model%2520%2528IMG%2529%252C%2520which%250Alearns%2520to%2520dynamically%2520and%2520selectively%2520aggregate%2520the%2520audio-vision-text%2520contexts%250Afor%2520VMR.%2520Specifically%252C%2520after%2520integrating%2520the%2520textual%2520guidance%2520with%2520vision%2520and%250Aaudio%2520separately%252C%2520we%2520first%2520design%2520a%2520pseudo-label-supervised%2520audio%2520importance%250Apredictor%2520that%2520predicts%2520the%2520importance%2520score%2520of%2520the%2520audio%252C%2520and%2520accordingly%250Aassigns%2520weights%2520to%2520mitigate%2520the%2520interference%2520caused%2520by%2520noisy%2520audio.%2520Then%252C%2520we%250Adesign%2520a%2520multi-granularity%2520audio%2520fusion%2520module%2520that%2520adaptively%2520fuses%2520audio%2520and%250Avisual%2520modalities%2520at%2520local-%252C%2520event-%252C%2520and%2520global-level%252C%2520fully%2520capturing%2520their%250Acomplementary%2520contexts.%2520We%2520further%2520propose%2520a%2520cross-modal%2520knowledge%2520distillation%250Astrategy%2520to%2520address%2520the%2520challenge%2520of%2520missing%2520audio%2520modality%2520during%2520inference.%250ATo%2520evaluate%2520our%2520method%252C%2520we%2520further%2520construct%2520a%2520new%2520VMR%2520dataset%252C%2520i.e.%252C%250ACharades-AudioMatter%252C%2520where%2520audio-related%2520samples%2520are%2520manually%2520selected%2520and%250Are-organized%2520from%2520the%2520original%2520Charades-STA%2520to%2520validate%2520the%2520model%2527s%2520capability%250Ain%2520utilizing%2520audio%2520modality.%2520Extensive%2520experiments%2520validate%2520the%2520effectiveness%250Aof%2520our%2520method%252C%2520achieving%2520state-of-the-art%2520with%2520audio-video%2520fusion%2520in%2520VMR%250Amethods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/HuiGuanLab/IMG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Does%20Matter%3A%20Importance-Aware%20Multi-Granularity%20Fusion%20for%20Video%0A%20%20Moment%20Retrieval&entry.906535625=Junan%20Lin%20and%20Daizong%20Liu%20and%20Xianke%20Chen%20and%20Xiaoye%20Qu%20and%20Xun%20Yang%20and%20Jixiang%20Zhu%20and%20Sanyuan%20Zhang%20and%20Jianfeng%20Dong&entry.1292438233=%20%20Video%20Moment%20Retrieval%20%28VMR%29%20aims%20to%20retrieve%20a%20specific%20moment%20semantically%0Arelated%20to%20the%20given%20query.%20To%20tackle%20this%20task%2C%20most%20existing%20VMR%20methods%0Asolely%20focus%20on%20the%20visual%20and%20textual%20modalities%20while%20neglecting%20the%0Acomplementary%20but%20important%20audio%20modality.%20Although%20a%20few%20recent%20works%20try%20to%0Atackle%20the%20joint%20audio-vision-text%20reasoning%2C%20they%20treat%20all%20modalities%20equally%0Aand%20simply%20embed%20them%20without%20fine-grained%20interaction%20for%20moment%20retrieval.%0AThese%20designs%20are%20counter-practical%20as%3A%20Not%20all%20audios%20are%20helpful%20for%20video%0Amoment%20retrieval%2C%20and%20the%20audio%20of%20some%20videos%20may%20be%20complete%20noise%20or%0Abackground%20sound%20that%20is%20meaningless%20to%20the%20moment%20determination.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20Importance-aware%20Multi-Granularity%20fusion%20model%20%28IMG%29%2C%20which%0Alearns%20to%20dynamically%20and%20selectively%20aggregate%20the%20audio-vision-text%20contexts%0Afor%20VMR.%20Specifically%2C%20after%20integrating%20the%20textual%20guidance%20with%20vision%20and%0Aaudio%20separately%2C%20we%20first%20design%20a%20pseudo-label-supervised%20audio%20importance%0Apredictor%20that%20predicts%20the%20importance%20score%20of%20the%20audio%2C%20and%20accordingly%0Aassigns%20weights%20to%20mitigate%20the%20interference%20caused%20by%20noisy%20audio.%20Then%2C%20we%0Adesign%20a%20multi-granularity%20audio%20fusion%20module%20that%20adaptively%20fuses%20audio%20and%0Avisual%20modalities%20at%20local-%2C%20event-%2C%20and%20global-level%2C%20fully%20capturing%20their%0Acomplementary%20contexts.%20We%20further%20propose%20a%20cross-modal%20knowledge%20distillation%0Astrategy%20to%20address%20the%20challenge%20of%20missing%20audio%20modality%20during%20inference.%0ATo%20evaluate%20our%20method%2C%20we%20further%20construct%20a%20new%20VMR%20dataset%2C%20i.e.%2C%0ACharades-AudioMatter%2C%20where%20audio-related%20samples%20are%20manually%20selected%20and%0Are-organized%20from%20the%20original%20Charades-STA%20to%20validate%20the%20model%27s%20capability%0Ain%20utilizing%20audio%20modality.%20Extensive%20experiments%20validate%20the%20effectiveness%0Aof%20our%20method%2C%20achieving%20state-of-the-art%20with%20audio-video%20fusion%20in%20VMR%0Amethods.%20Our%20code%20is%20available%20at%20https%3A//github.com/HuiGuanLab/IMG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04273v1&entry.124074799=Read"},
{"title": "CHARM: Collaborative Harmonization across Arbitrary Modalities for\n  Modality-agnostic Semantic Segmentation", "author": "Lekang Wen and Jing Xiao and Liang Liao and Jiajun Chen and Mi Wang", "abstract": "  Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene\nunderstanding across arbitrary combinations of input modality. Existing methods\ntypically rely on explicit feature alignment to achieve modal homogenization,\nwhich dilutes the distinctive strengths of each modality and destroys their\ninherent complementarity. To achieve cooperative harmonization rather than\nhomogenization, we propose CHARM, a novel complementary learning framework\ndesigned to implicitly align content while preserving modality-specific\nadvantages through two components: (1) Mutual Perception Unit (MPU), enabling\nimplicit alignment through window-based cross-modal interaction, where\nmodalities serve as both queries and contexts for each other to discover\nmodality-interactive correspondences; (2) A dual-path optimization strategy\nthat decouples training into Collaborative Learning Strategy (CoL) for\ncomplementary fusion learning and Individual Enhancement Strategy (InE) for\nprotected modality-specific optimization. Experiments across multiple datasets\nand backbones indicate that CHARM consistently outperform the baselines, with\nsignificant increment on the fragile modalities. This work shifts the focus\nfrom model homogenization to harmonization, enabling cross-modal\ncomplementarity for true harmony in diversity.\n", "link": "http://arxiv.org/abs/2508.03060v2", "date": "2025-08-06", "relevancy": 2.7787, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6021}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHARM%3A%20Collaborative%20Harmonization%20across%20Arbitrary%20Modalities%20for%0A%20%20Modality-agnostic%20Semantic%20Segmentation&body=Title%3A%20CHARM%3A%20Collaborative%20Harmonization%20across%20Arbitrary%20Modalities%20for%0A%20%20Modality-agnostic%20Semantic%20Segmentation%0AAuthor%3A%20Lekang%20Wen%20and%20Jing%20Xiao%20and%20Liang%20Liao%20and%20Jiajun%20Chen%20and%20Mi%20Wang%0AAbstract%3A%20%20%20Modality-agnostic%20Semantic%20Segmentation%20%28MaSS%29%20aims%20to%20achieve%20robust%20scene%0Aunderstanding%20across%20arbitrary%20combinations%20of%20input%20modality.%20Existing%20methods%0Atypically%20rely%20on%20explicit%20feature%20alignment%20to%20achieve%20modal%20homogenization%2C%0Awhich%20dilutes%20the%20distinctive%20strengths%20of%20each%20modality%20and%20destroys%20their%0Ainherent%20complementarity.%20To%20achieve%20cooperative%20harmonization%20rather%20than%0Ahomogenization%2C%20we%20propose%20CHARM%2C%20a%20novel%20complementary%20learning%20framework%0Adesigned%20to%20implicitly%20align%20content%20while%20preserving%20modality-specific%0Aadvantages%20through%20two%20components%3A%20%281%29%20Mutual%20Perception%20Unit%20%28MPU%29%2C%20enabling%0Aimplicit%20alignment%20through%20window-based%20cross-modal%20interaction%2C%20where%0Amodalities%20serve%20as%20both%20queries%20and%20contexts%20for%20each%20other%20to%20discover%0Amodality-interactive%20correspondences%3B%20%282%29%20A%20dual-path%20optimization%20strategy%0Athat%20decouples%20training%20into%20Collaborative%20Learning%20Strategy%20%28CoL%29%20for%0Acomplementary%20fusion%20learning%20and%20Individual%20Enhancement%20Strategy%20%28InE%29%20for%0Aprotected%20modality-specific%20optimization.%20Experiments%20across%20multiple%20datasets%0Aand%20backbones%20indicate%20that%20CHARM%20consistently%20outperform%20the%20baselines%2C%20with%0Asignificant%20increment%20on%20the%20fragile%20modalities.%20This%20work%20shifts%20the%20focus%0Afrom%20model%20homogenization%20to%20harmonization%2C%20enabling%20cross-modal%0Acomplementarity%20for%20true%20harmony%20in%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHARM%253A%2520Collaborative%2520Harmonization%2520across%2520Arbitrary%2520Modalities%2520for%250A%2520%2520Modality-agnostic%2520Semantic%2520Segmentation%26entry.906535625%3DLekang%2520Wen%2520and%2520Jing%2520Xiao%2520and%2520Liang%2520Liao%2520and%2520Jiajun%2520Chen%2520and%2520Mi%2520Wang%26entry.1292438233%3D%2520%2520Modality-agnostic%2520Semantic%2520Segmentation%2520%2528MaSS%2529%2520aims%2520to%2520achieve%2520robust%2520scene%250Aunderstanding%2520across%2520arbitrary%2520combinations%2520of%2520input%2520modality.%2520Existing%2520methods%250Atypically%2520rely%2520on%2520explicit%2520feature%2520alignment%2520to%2520achieve%2520modal%2520homogenization%252C%250Awhich%2520dilutes%2520the%2520distinctive%2520strengths%2520of%2520each%2520modality%2520and%2520destroys%2520their%250Ainherent%2520complementarity.%2520To%2520achieve%2520cooperative%2520harmonization%2520rather%2520than%250Ahomogenization%252C%2520we%2520propose%2520CHARM%252C%2520a%2520novel%2520complementary%2520learning%2520framework%250Adesigned%2520to%2520implicitly%2520align%2520content%2520while%2520preserving%2520modality-specific%250Aadvantages%2520through%2520two%2520components%253A%2520%25281%2529%2520Mutual%2520Perception%2520Unit%2520%2528MPU%2529%252C%2520enabling%250Aimplicit%2520alignment%2520through%2520window-based%2520cross-modal%2520interaction%252C%2520where%250Amodalities%2520serve%2520as%2520both%2520queries%2520and%2520contexts%2520for%2520each%2520other%2520to%2520discover%250Amodality-interactive%2520correspondences%253B%2520%25282%2529%2520A%2520dual-path%2520optimization%2520strategy%250Athat%2520decouples%2520training%2520into%2520Collaborative%2520Learning%2520Strategy%2520%2528CoL%2529%2520for%250Acomplementary%2520fusion%2520learning%2520and%2520Individual%2520Enhancement%2520Strategy%2520%2528InE%2529%2520for%250Aprotected%2520modality-specific%2520optimization.%2520Experiments%2520across%2520multiple%2520datasets%250Aand%2520backbones%2520indicate%2520that%2520CHARM%2520consistently%2520outperform%2520the%2520baselines%252C%2520with%250Asignificant%2520increment%2520on%2520the%2520fragile%2520modalities.%2520This%2520work%2520shifts%2520the%2520focus%250Afrom%2520model%2520homogenization%2520to%2520harmonization%252C%2520enabling%2520cross-modal%250Acomplementarity%2520for%2520true%2520harmony%2520in%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHARM%3A%20Collaborative%20Harmonization%20across%20Arbitrary%20Modalities%20for%0A%20%20Modality-agnostic%20Semantic%20Segmentation&entry.906535625=Lekang%20Wen%20and%20Jing%20Xiao%20and%20Liang%20Liao%20and%20Jiajun%20Chen%20and%20Mi%20Wang&entry.1292438233=%20%20Modality-agnostic%20Semantic%20Segmentation%20%28MaSS%29%20aims%20to%20achieve%20robust%20scene%0Aunderstanding%20across%20arbitrary%20combinations%20of%20input%20modality.%20Existing%20methods%0Atypically%20rely%20on%20explicit%20feature%20alignment%20to%20achieve%20modal%20homogenization%2C%0Awhich%20dilutes%20the%20distinctive%20strengths%20of%20each%20modality%20and%20destroys%20their%0Ainherent%20complementarity.%20To%20achieve%20cooperative%20harmonization%20rather%20than%0Ahomogenization%2C%20we%20propose%20CHARM%2C%20a%20novel%20complementary%20learning%20framework%0Adesigned%20to%20implicitly%20align%20content%20while%20preserving%20modality-specific%0Aadvantages%20through%20two%20components%3A%20%281%29%20Mutual%20Perception%20Unit%20%28MPU%29%2C%20enabling%0Aimplicit%20alignment%20through%20window-based%20cross-modal%20interaction%2C%20where%0Amodalities%20serve%20as%20both%20queries%20and%20contexts%20for%20each%20other%20to%20discover%0Amodality-interactive%20correspondences%3B%20%282%29%20A%20dual-path%20optimization%20strategy%0Athat%20decouples%20training%20into%20Collaborative%20Learning%20Strategy%20%28CoL%29%20for%0Acomplementary%20fusion%20learning%20and%20Individual%20Enhancement%20Strategy%20%28InE%29%20for%0Aprotected%20modality-specific%20optimization.%20Experiments%20across%20multiple%20datasets%0Aand%20backbones%20indicate%20that%20CHARM%20consistently%20outperform%20the%20baselines%2C%20with%0Asignificant%20increment%20on%20the%20fragile%20modalities.%20This%20work%20shifts%20the%20focus%0Afrom%20model%20homogenization%20to%20harmonization%2C%20enabling%20cross-modal%0Acomplementarity%20for%20true%20harmony%20in%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03060v2&entry.124074799=Read"},
{"title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for\n  Weakly-supervised Dense Audio-Visual Event Localization", "author": "Jinxing Zhou and Ziheng Zhou and Yanghao Zhou and Yuxin Mao and Zhangling Duan and Dan Guo", "abstract": "  The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally\nlocalize events in untrimmed videos that occur simultaneously in both the audio\nand visual modalities. This paper explores DAVEL under a new and more\nchallenging weakly-supervised setting (W-DAVEL task), where only video-level\nevent labels are provided and the temporal boundaries of each event are\nunknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors},\nwhich are defined as reliable timestamps that are well predicted under weak\nsupervision and exhibit highly consistent event semantics across audio and\nvisual modalities. Specifically, we propose a \\textit{Mutual Event Agreement\nEvaluation} module, which generates an agreement score by measuring the\ndiscrepancy between the predicted audio and visual event classes. Then, the\nagreement score is utilized in a \\textit{Cross-modal Salient Anchor\nIdentification} module, which identifies the audio and visual anchor features\nthrough global-video and local temporal window identification mechanisms. The\nanchor features after multimodal integration are fed into an\n\\textit{Anchor-based Temporal Propagation} module to enhance event semantic\nencoding in the original temporal audio and visual features, facilitating\nbetter temporal localization under weak supervision. We establish benchmarks\nfor W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2508.04566v1", "date": "2025-08-06", "relevancy": 2.7707, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLASP%3A%20Cross-modal%20Salient%20Anchor-based%20Semantic%20Propagation%20for%0A%20%20Weakly-supervised%20Dense%20Audio-Visual%20Event%20Localization&body=Title%3A%20CLASP%3A%20Cross-modal%20Salient%20Anchor-based%20Semantic%20Propagation%20for%0A%20%20Weakly-supervised%20Dense%20Audio-Visual%20Event%20Localization%0AAuthor%3A%20Jinxing%20Zhou%20and%20Ziheng%20Zhou%20and%20Yanghao%20Zhou%20and%20Yuxin%20Mao%20and%20Zhangling%20Duan%20and%20Dan%20Guo%0AAbstract%3A%20%20%20The%20Dense%20Audio-Visual%20Event%20Localization%20%28DAVEL%29%20task%20aims%20to%20temporally%0Alocalize%20events%20in%20untrimmed%20videos%20that%20occur%20simultaneously%20in%20both%20the%20audio%0Aand%20visual%20modalities.%20This%20paper%20explores%20DAVEL%20under%20a%20new%20and%20more%0Achallenging%20weakly-supervised%20setting%20%28W-DAVEL%20task%29%2C%20where%20only%20video-level%0Aevent%20labels%20are%20provided%20and%20the%20temporal%20boundaries%20of%20each%20event%20are%0Aunknown.%20We%20address%20W-DAVEL%20by%20exploiting%20%5Ctextit%7Bcross-modal%20salient%20anchors%7D%2C%0Awhich%20are%20defined%20as%20reliable%20timestamps%20that%20are%20well%20predicted%20under%20weak%0Asupervision%20and%20exhibit%20highly%20consistent%20event%20semantics%20across%20audio%20and%0Avisual%20modalities.%20Specifically%2C%20we%20propose%20a%20%5Ctextit%7BMutual%20Event%20Agreement%0AEvaluation%7D%20module%2C%20which%20generates%20an%20agreement%20score%20by%20measuring%20the%0Adiscrepancy%20between%20the%20predicted%20audio%20and%20visual%20event%20classes.%20Then%2C%20the%0Aagreement%20score%20is%20utilized%20in%20a%20%5Ctextit%7BCross-modal%20Salient%20Anchor%0AIdentification%7D%20module%2C%20which%20identifies%20the%20audio%20and%20visual%20anchor%20features%0Athrough%20global-video%20and%20local%20temporal%20window%20identification%20mechanisms.%20The%0Aanchor%20features%20after%20multimodal%20integration%20are%20fed%20into%20an%0A%5Ctextit%7BAnchor-based%20Temporal%20Propagation%7D%20module%20to%20enhance%20event%20semantic%0Aencoding%20in%20the%20original%20temporal%20audio%20and%20visual%20features%2C%20facilitating%0Abetter%20temporal%20localization%20under%20weak%20supervision.%20We%20establish%20benchmarks%0Afor%20W-DAVEL%20on%20both%20the%20UnAV-100%20and%20ActivityNet1.3%20datasets.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLASP%253A%2520Cross-modal%2520Salient%2520Anchor-based%2520Semantic%2520Propagation%2520for%250A%2520%2520Weakly-supervised%2520Dense%2520Audio-Visual%2520Event%2520Localization%26entry.906535625%3DJinxing%2520Zhou%2520and%2520Ziheng%2520Zhou%2520and%2520Yanghao%2520Zhou%2520and%2520Yuxin%2520Mao%2520and%2520Zhangling%2520Duan%2520and%2520Dan%2520Guo%26entry.1292438233%3D%2520%2520The%2520Dense%2520Audio-Visual%2520Event%2520Localization%2520%2528DAVEL%2529%2520task%2520aims%2520to%2520temporally%250Alocalize%2520events%2520in%2520untrimmed%2520videos%2520that%2520occur%2520simultaneously%2520in%2520both%2520the%2520audio%250Aand%2520visual%2520modalities.%2520This%2520paper%2520explores%2520DAVEL%2520under%2520a%2520new%2520and%2520more%250Achallenging%2520weakly-supervised%2520setting%2520%2528W-DAVEL%2520task%2529%252C%2520where%2520only%2520video-level%250Aevent%2520labels%2520are%2520provided%2520and%2520the%2520temporal%2520boundaries%2520of%2520each%2520event%2520are%250Aunknown.%2520We%2520address%2520W-DAVEL%2520by%2520exploiting%2520%255Ctextit%257Bcross-modal%2520salient%2520anchors%257D%252C%250Awhich%2520are%2520defined%2520as%2520reliable%2520timestamps%2520that%2520are%2520well%2520predicted%2520under%2520weak%250Asupervision%2520and%2520exhibit%2520highly%2520consistent%2520event%2520semantics%2520across%2520audio%2520and%250Avisual%2520modalities.%2520Specifically%252C%2520we%2520propose%2520a%2520%255Ctextit%257BMutual%2520Event%2520Agreement%250AEvaluation%257D%2520module%252C%2520which%2520generates%2520an%2520agreement%2520score%2520by%2520measuring%2520the%250Adiscrepancy%2520between%2520the%2520predicted%2520audio%2520and%2520visual%2520event%2520classes.%2520Then%252C%2520the%250Aagreement%2520score%2520is%2520utilized%2520in%2520a%2520%255Ctextit%257BCross-modal%2520Salient%2520Anchor%250AIdentification%257D%2520module%252C%2520which%2520identifies%2520the%2520audio%2520and%2520visual%2520anchor%2520features%250Athrough%2520global-video%2520and%2520local%2520temporal%2520window%2520identification%2520mechanisms.%2520The%250Aanchor%2520features%2520after%2520multimodal%2520integration%2520are%2520fed%2520into%2520an%250A%255Ctextit%257BAnchor-based%2520Temporal%2520Propagation%257D%2520module%2520to%2520enhance%2520event%2520semantic%250Aencoding%2520in%2520the%2520original%2520temporal%2520audio%2520and%2520visual%2520features%252C%2520facilitating%250Abetter%2520temporal%2520localization%2520under%2520weak%2520supervision.%2520We%2520establish%2520benchmarks%250Afor%2520W-DAVEL%2520on%2520both%2520the%2520UnAV-100%2520and%2520ActivityNet1.3%2520datasets.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLASP%3A%20Cross-modal%20Salient%20Anchor-based%20Semantic%20Propagation%20for%0A%20%20Weakly-supervised%20Dense%20Audio-Visual%20Event%20Localization&entry.906535625=Jinxing%20Zhou%20and%20Ziheng%20Zhou%20and%20Yanghao%20Zhou%20and%20Yuxin%20Mao%20and%20Zhangling%20Duan%20and%20Dan%20Guo&entry.1292438233=%20%20The%20Dense%20Audio-Visual%20Event%20Localization%20%28DAVEL%29%20task%20aims%20to%20temporally%0Alocalize%20events%20in%20untrimmed%20videos%20that%20occur%20simultaneously%20in%20both%20the%20audio%0Aand%20visual%20modalities.%20This%20paper%20explores%20DAVEL%20under%20a%20new%20and%20more%0Achallenging%20weakly-supervised%20setting%20%28W-DAVEL%20task%29%2C%20where%20only%20video-level%0Aevent%20labels%20are%20provided%20and%20the%20temporal%20boundaries%20of%20each%20event%20are%0Aunknown.%20We%20address%20W-DAVEL%20by%20exploiting%20%5Ctextit%7Bcross-modal%20salient%20anchors%7D%2C%0Awhich%20are%20defined%20as%20reliable%20timestamps%20that%20are%20well%20predicted%20under%20weak%0Asupervision%20and%20exhibit%20highly%20consistent%20event%20semantics%20across%20audio%20and%0Avisual%20modalities.%20Specifically%2C%20we%20propose%20a%20%5Ctextit%7BMutual%20Event%20Agreement%0AEvaluation%7D%20module%2C%20which%20generates%20an%20agreement%20score%20by%20measuring%20the%0Adiscrepancy%20between%20the%20predicted%20audio%20and%20visual%20event%20classes.%20Then%2C%20the%0Aagreement%20score%20is%20utilized%20in%20a%20%5Ctextit%7BCross-modal%20Salient%20Anchor%0AIdentification%7D%20module%2C%20which%20identifies%20the%20audio%20and%20visual%20anchor%20features%0Athrough%20global-video%20and%20local%20temporal%20window%20identification%20mechanisms.%20The%0Aanchor%20features%20after%20multimodal%20integration%20are%20fed%20into%20an%0A%5Ctextit%7BAnchor-based%20Temporal%20Propagation%7D%20module%20to%20enhance%20event%20semantic%0Aencoding%20in%20the%20original%20temporal%20audio%20and%20visual%20features%2C%20facilitating%0Abetter%20temporal%20localization%20under%20weak%20supervision.%20We%20establish%20benchmarks%0Afor%20W-DAVEL%20on%20both%20the%20UnAV-100%20and%20ActivityNet1.3%20datasets.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04566v1&entry.124074799=Read"},
{"title": "Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater\n  Environments", "author": "Ivana Collado-Gonzalez and John McConnell and Paul Szenher and Brendan Englot", "abstract": "  Scene reconstruction is an essential capability for underwater robots\nnavigating in close proximity to structures. Monocular vision-based\nreconstruction methods are unreliable in turbid waters and lack depth scale\ninformation. Sonars are robust to turbid water and non-uniform lighting\nconditions, however, they have low resolution and elevation ambiguity. This\nwork proposes a real-time opti-acoustic scene reconstruction method that is\nspecially optimized to work in turbid water. Our strategy avoids having to\nidentify point features in visual data and instead identifies regions of\ninterest in the data. We then match relevant regions in the image to\ncorresponding sonar data. A reconstruction is obtained by leveraging range data\nfrom the sonar and elevation data from the camera image. Experimental\ncomparisons against other vision-based and sonar-based approaches at varying\nturbidity levels, and field tests conducted in marina environments, validate\nthe effectiveness of the proposed approach. We have made our code open-source\nto facilitate reproducibility and encourage community engagement.\n", "link": "http://arxiv.org/abs/2508.03408v2", "date": "2025-08-06", "relevancy": 2.7634, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5571}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opti-Acoustic%20Scene%20Reconstruction%20in%20Highly%20Turbid%20Underwater%0A%20%20Environments&body=Title%3A%20Opti-Acoustic%20Scene%20Reconstruction%20in%20Highly%20Turbid%20Underwater%0A%20%20Environments%0AAuthor%3A%20Ivana%20Collado-Gonzalez%20and%20John%20McConnell%20and%20Paul%20Szenher%20and%20Brendan%20Englot%0AAbstract%3A%20%20%20Scene%20reconstruction%20is%20an%20essential%20capability%20for%20underwater%20robots%0Anavigating%20in%20close%20proximity%20to%20structures.%20Monocular%20vision-based%0Areconstruction%20methods%20are%20unreliable%20in%20turbid%20waters%20and%20lack%20depth%20scale%0Ainformation.%20Sonars%20are%20robust%20to%20turbid%20water%20and%20non-uniform%20lighting%0Aconditions%2C%20however%2C%20they%20have%20low%20resolution%20and%20elevation%20ambiguity.%20This%0Awork%20proposes%20a%20real-time%20opti-acoustic%20scene%20reconstruction%20method%20that%20is%0Aspecially%20optimized%20to%20work%20in%20turbid%20water.%20Our%20strategy%20avoids%20having%20to%0Aidentify%20point%20features%20in%20visual%20data%20and%20instead%20identifies%20regions%20of%0Ainterest%20in%20the%20data.%20We%20then%20match%20relevant%20regions%20in%20the%20image%20to%0Acorresponding%20sonar%20data.%20A%20reconstruction%20is%20obtained%20by%20leveraging%20range%20data%0Afrom%20the%20sonar%20and%20elevation%20data%20from%20the%20camera%20image.%20Experimental%0Acomparisons%20against%20other%20vision-based%20and%20sonar-based%20approaches%20at%20varying%0Aturbidity%20levels%2C%20and%20field%20tests%20conducted%20in%20marina%20environments%2C%20validate%0Athe%20effectiveness%20of%20the%20proposed%20approach.%20We%20have%20made%20our%20code%20open-source%0Ato%20facilitate%20reproducibility%20and%20encourage%20community%20engagement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpti-Acoustic%2520Scene%2520Reconstruction%2520in%2520Highly%2520Turbid%2520Underwater%250A%2520%2520Environments%26entry.906535625%3DIvana%2520Collado-Gonzalez%2520and%2520John%2520McConnell%2520and%2520Paul%2520Szenher%2520and%2520Brendan%2520Englot%26entry.1292438233%3D%2520%2520Scene%2520reconstruction%2520is%2520an%2520essential%2520capability%2520for%2520underwater%2520robots%250Anavigating%2520in%2520close%2520proximity%2520to%2520structures.%2520Monocular%2520vision-based%250Areconstruction%2520methods%2520are%2520unreliable%2520in%2520turbid%2520waters%2520and%2520lack%2520depth%2520scale%250Ainformation.%2520Sonars%2520are%2520robust%2520to%2520turbid%2520water%2520and%2520non-uniform%2520lighting%250Aconditions%252C%2520however%252C%2520they%2520have%2520low%2520resolution%2520and%2520elevation%2520ambiguity.%2520This%250Awork%2520proposes%2520a%2520real-time%2520opti-acoustic%2520scene%2520reconstruction%2520method%2520that%2520is%250Aspecially%2520optimized%2520to%2520work%2520in%2520turbid%2520water.%2520Our%2520strategy%2520avoids%2520having%2520to%250Aidentify%2520point%2520features%2520in%2520visual%2520data%2520and%2520instead%2520identifies%2520regions%2520of%250Ainterest%2520in%2520the%2520data.%2520We%2520then%2520match%2520relevant%2520regions%2520in%2520the%2520image%2520to%250Acorresponding%2520sonar%2520data.%2520A%2520reconstruction%2520is%2520obtained%2520by%2520leveraging%2520range%2520data%250Afrom%2520the%2520sonar%2520and%2520elevation%2520data%2520from%2520the%2520camera%2520image.%2520Experimental%250Acomparisons%2520against%2520other%2520vision-based%2520and%2520sonar-based%2520approaches%2520at%2520varying%250Aturbidity%2520levels%252C%2520and%2520field%2520tests%2520conducted%2520in%2520marina%2520environments%252C%2520validate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520approach.%2520We%2520have%2520made%2520our%2520code%2520open-source%250Ato%2520facilitate%2520reproducibility%2520and%2520encourage%2520community%2520engagement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opti-Acoustic%20Scene%20Reconstruction%20in%20Highly%20Turbid%20Underwater%0A%20%20Environments&entry.906535625=Ivana%20Collado-Gonzalez%20and%20John%20McConnell%20and%20Paul%20Szenher%20and%20Brendan%20Englot&entry.1292438233=%20%20Scene%20reconstruction%20is%20an%20essential%20capability%20for%20underwater%20robots%0Anavigating%20in%20close%20proximity%20to%20structures.%20Monocular%20vision-based%0Areconstruction%20methods%20are%20unreliable%20in%20turbid%20waters%20and%20lack%20depth%20scale%0Ainformation.%20Sonars%20are%20robust%20to%20turbid%20water%20and%20non-uniform%20lighting%0Aconditions%2C%20however%2C%20they%20have%20low%20resolution%20and%20elevation%20ambiguity.%20This%0Awork%20proposes%20a%20real-time%20opti-acoustic%20scene%20reconstruction%20method%20that%20is%0Aspecially%20optimized%20to%20work%20in%20turbid%20water.%20Our%20strategy%20avoids%20having%20to%0Aidentify%20point%20features%20in%20visual%20data%20and%20instead%20identifies%20regions%20of%0Ainterest%20in%20the%20data.%20We%20then%20match%20relevant%20regions%20in%20the%20image%20to%0Acorresponding%20sonar%20data.%20A%20reconstruction%20is%20obtained%20by%20leveraging%20range%20data%0Afrom%20the%20sonar%20and%20elevation%20data%20from%20the%20camera%20image.%20Experimental%0Acomparisons%20against%20other%20vision-based%20and%20sonar-based%20approaches%20at%20varying%0Aturbidity%20levels%2C%20and%20field%20tests%20conducted%20in%20marina%20environments%2C%20validate%0Athe%20effectiveness%20of%20the%20proposed%20approach.%20We%20have%20made%20our%20code%20open-source%0Ato%20facilitate%20reproducibility%20and%20encourage%20community%20engagement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03408v2&entry.124074799=Read"},
{"title": "Benchmarking Foundation Models for Mitotic Figure Classification", "author": "Jonas Ammeling and Jonathan Ganz and Emely Rosbach and Ludwig Lausser and Christof A. Bertram and Katharina Breininger and Marc Aubreville", "abstract": "  The performance of deep learning models is known to scale with data quantity\nand diversity. In pathology, as in many other medical imaging domains, the\navailability of labeled images for a specific task is often limited.\nSelf-supervised learning techniques have enabled the use of vast amounts of\nunlabeled data to train large-scale neural networks, i.e., foundation models,\nthat can address the limited data problem by providing semantically rich\nfeature vectors that can generalize well to new tasks with minimal training\neffort increasing model performance and robustness. In this work, we\ninvestigate the use of foundation models for mitotic figure classification. The\nmitotic count, which can be derived from this classification task, is an\nindependent prognostic marker for specific tumors and part of certain tumor\ngrading systems. In particular, we investigate the data scaling laws on\nmultiple current foundation models and evaluate their robustness to unseen\ntumor domains. Next to the commonly used linear probing paradigm, we also adapt\nthe models using low-rank adaptation (LoRA) of their attention mechanisms. We\ncompare all models against end-to-end-trained baselines, both CNNs and Vision\nTransformers. Our results demonstrate that LoRA-adapted foundation models\nprovide superior performance to those adapted with standard linear probing,\nreaching performance levels close to 100% data availability with only 10% of\ntraining data. Furthermore, LoRA-adaptation of the most recent foundation\nmodels almost closes the out-of-domain performance gap when evaluated on unseen\ntumor domains. However, full fine-tuning of traditional architectures still\nyields competitive performance.\n", "link": "http://arxiv.org/abs/2508.04441v1", "date": "2025-08-06", "relevancy": 2.7561, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Foundation%20Models%20for%20Mitotic%20Figure%20Classification&body=Title%3A%20Benchmarking%20Foundation%20Models%20for%20Mitotic%20Figure%20Classification%0AAuthor%3A%20Jonas%20Ammeling%20and%20Jonathan%20Ganz%20and%20Emely%20Rosbach%20and%20Ludwig%20Lausser%20and%20Christof%20A.%20Bertram%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville%0AAbstract%3A%20%20%20The%20performance%20of%20deep%20learning%20models%20is%20known%20to%20scale%20with%20data%20quantity%0Aand%20diversity.%20In%20pathology%2C%20as%20in%20many%20other%20medical%20imaging%20domains%2C%20the%0Aavailability%20of%20labeled%20images%20for%20a%20specific%20task%20is%20often%20limited.%0ASelf-supervised%20learning%20techniques%20have%20enabled%20the%20use%20of%20vast%20amounts%20of%0Aunlabeled%20data%20to%20train%20large-scale%20neural%20networks%2C%20i.e.%2C%20foundation%20models%2C%0Athat%20can%20address%20the%20limited%20data%20problem%20by%20providing%20semantically%20rich%0Afeature%20vectors%20that%20can%20generalize%20well%20to%20new%20tasks%20with%20minimal%20training%0Aeffort%20increasing%20model%20performance%20and%20robustness.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20use%20of%20foundation%20models%20for%20mitotic%20figure%20classification.%20The%0Amitotic%20count%2C%20which%20can%20be%20derived%20from%20this%20classification%20task%2C%20is%20an%0Aindependent%20prognostic%20marker%20for%20specific%20tumors%20and%20part%20of%20certain%20tumor%0Agrading%20systems.%20In%20particular%2C%20we%20investigate%20the%20data%20scaling%20laws%20on%0Amultiple%20current%20foundation%20models%20and%20evaluate%20their%20robustness%20to%20unseen%0Atumor%20domains.%20Next%20to%20the%20commonly%20used%20linear%20probing%20paradigm%2C%20we%20also%20adapt%0Athe%20models%20using%20low-rank%20adaptation%20%28LoRA%29%20of%20their%20attention%20mechanisms.%20We%0Acompare%20all%20models%20against%20end-to-end-trained%20baselines%2C%20both%20CNNs%20and%20Vision%0ATransformers.%20Our%20results%20demonstrate%20that%20LoRA-adapted%20foundation%20models%0Aprovide%20superior%20performance%20to%20those%20adapted%20with%20standard%20linear%20probing%2C%0Areaching%20performance%20levels%20close%20to%20100%25%20data%20availability%20with%20only%2010%25%20of%0Atraining%20data.%20Furthermore%2C%20LoRA-adaptation%20of%20the%20most%20recent%20foundation%0Amodels%20almost%20closes%20the%20out-of-domain%20performance%20gap%20when%20evaluated%20on%20unseen%0Atumor%20domains.%20However%2C%20full%20fine-tuning%20of%20traditional%20architectures%20still%0Ayields%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Foundation%2520Models%2520for%2520Mitotic%2520Figure%2520Classification%26entry.906535625%3DJonas%2520Ammeling%2520and%2520Jonathan%2520Ganz%2520and%2520Emely%2520Rosbach%2520and%2520Ludwig%2520Lausser%2520and%2520Christof%2520A.%2520Bertram%2520and%2520Katharina%2520Breininger%2520and%2520Marc%2520Aubreville%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520deep%2520learning%2520models%2520is%2520known%2520to%2520scale%2520with%2520data%2520quantity%250Aand%2520diversity.%2520In%2520pathology%252C%2520as%2520in%2520many%2520other%2520medical%2520imaging%2520domains%252C%2520the%250Aavailability%2520of%2520labeled%2520images%2520for%2520a%2520specific%2520task%2520is%2520often%2520limited.%250ASelf-supervised%2520learning%2520techniques%2520have%2520enabled%2520the%2520use%2520of%2520vast%2520amounts%2520of%250Aunlabeled%2520data%2520to%2520train%2520large-scale%2520neural%2520networks%252C%2520i.e.%252C%2520foundation%2520models%252C%250Athat%2520can%2520address%2520the%2520limited%2520data%2520problem%2520by%2520providing%2520semantically%2520rich%250Afeature%2520vectors%2520that%2520can%2520generalize%2520well%2520to%2520new%2520tasks%2520with%2520minimal%2520training%250Aeffort%2520increasing%2520model%2520performance%2520and%2520robustness.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520the%2520use%2520of%2520foundation%2520models%2520for%2520mitotic%2520figure%2520classification.%2520The%250Amitotic%2520count%252C%2520which%2520can%2520be%2520derived%2520from%2520this%2520classification%2520task%252C%2520is%2520an%250Aindependent%2520prognostic%2520marker%2520for%2520specific%2520tumors%2520and%2520part%2520of%2520certain%2520tumor%250Agrading%2520systems.%2520In%2520particular%252C%2520we%2520investigate%2520the%2520data%2520scaling%2520laws%2520on%250Amultiple%2520current%2520foundation%2520models%2520and%2520evaluate%2520their%2520robustness%2520to%2520unseen%250Atumor%2520domains.%2520Next%2520to%2520the%2520commonly%2520used%2520linear%2520probing%2520paradigm%252C%2520we%2520also%2520adapt%250Athe%2520models%2520using%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520of%2520their%2520attention%2520mechanisms.%2520We%250Acompare%2520all%2520models%2520against%2520end-to-end-trained%2520baselines%252C%2520both%2520CNNs%2520and%2520Vision%250ATransformers.%2520Our%2520results%2520demonstrate%2520that%2520LoRA-adapted%2520foundation%2520models%250Aprovide%2520superior%2520performance%2520to%2520those%2520adapted%2520with%2520standard%2520linear%2520probing%252C%250Areaching%2520performance%2520levels%2520close%2520to%2520100%2525%2520data%2520availability%2520with%2520only%252010%2525%2520of%250Atraining%2520data.%2520Furthermore%252C%2520LoRA-adaptation%2520of%2520the%2520most%2520recent%2520foundation%250Amodels%2520almost%2520closes%2520the%2520out-of-domain%2520performance%2520gap%2520when%2520evaluated%2520on%2520unseen%250Atumor%2520domains.%2520However%252C%2520full%2520fine-tuning%2520of%2520traditional%2520architectures%2520still%250Ayields%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Foundation%20Models%20for%20Mitotic%20Figure%20Classification&entry.906535625=Jonas%20Ammeling%20and%20Jonathan%20Ganz%20and%20Emely%20Rosbach%20and%20Ludwig%20Lausser%20and%20Christof%20A.%20Bertram%20and%20Katharina%20Breininger%20and%20Marc%20Aubreville&entry.1292438233=%20%20The%20performance%20of%20deep%20learning%20models%20is%20known%20to%20scale%20with%20data%20quantity%0Aand%20diversity.%20In%20pathology%2C%20as%20in%20many%20other%20medical%20imaging%20domains%2C%20the%0Aavailability%20of%20labeled%20images%20for%20a%20specific%20task%20is%20often%20limited.%0ASelf-supervised%20learning%20techniques%20have%20enabled%20the%20use%20of%20vast%20amounts%20of%0Aunlabeled%20data%20to%20train%20large-scale%20neural%20networks%2C%20i.e.%2C%20foundation%20models%2C%0Athat%20can%20address%20the%20limited%20data%20problem%20by%20providing%20semantically%20rich%0Afeature%20vectors%20that%20can%20generalize%20well%20to%20new%20tasks%20with%20minimal%20training%0Aeffort%20increasing%20model%20performance%20and%20robustness.%20In%20this%20work%2C%20we%0Ainvestigate%20the%20use%20of%20foundation%20models%20for%20mitotic%20figure%20classification.%20The%0Amitotic%20count%2C%20which%20can%20be%20derived%20from%20this%20classification%20task%2C%20is%20an%0Aindependent%20prognostic%20marker%20for%20specific%20tumors%20and%20part%20of%20certain%20tumor%0Agrading%20systems.%20In%20particular%2C%20we%20investigate%20the%20data%20scaling%20laws%20on%0Amultiple%20current%20foundation%20models%20and%20evaluate%20their%20robustness%20to%20unseen%0Atumor%20domains.%20Next%20to%20the%20commonly%20used%20linear%20probing%20paradigm%2C%20we%20also%20adapt%0Athe%20models%20using%20low-rank%20adaptation%20%28LoRA%29%20of%20their%20attention%20mechanisms.%20We%0Acompare%20all%20models%20against%20end-to-end-trained%20baselines%2C%20both%20CNNs%20and%20Vision%0ATransformers.%20Our%20results%20demonstrate%20that%20LoRA-adapted%20foundation%20models%0Aprovide%20superior%20performance%20to%20those%20adapted%20with%20standard%20linear%20probing%2C%0Areaching%20performance%20levels%20close%20to%20100%25%20data%20availability%20with%20only%2010%25%20of%0Atraining%20data.%20Furthermore%2C%20LoRA-adaptation%20of%20the%20most%20recent%20foundation%0Amodels%20almost%20closes%20the%20out-of-domain%20performance%20gap%20when%20evaluated%20on%20unseen%0Atumor%20domains.%20However%2C%20full%20fine-tuning%20of%20traditional%20architectures%20still%0Ayields%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04441v1&entry.124074799=Read"},
{"title": "Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and\n  Undressing Synthesis", "author": "Angang Zhang and Fang Deng and Hao Chen and Zhongjian Chen and Junyan Li", "abstract": "  While recent advances in virtual try-on (VTON) have achieved realistic\ngarment transfer to human subjects, its inverse task, virtual try-off (VTOFF),\nwhich aims to reconstruct canonical garment templates from dressed humans,\nremains critically underexplored and lacks systematic investigation. Existing\nworks predominantly treat them as isolated tasks: VTON focuses on garment\ndressing while VTOFF addresses garment extraction, thereby neglecting their\ncomplementary symmetry. To bridge this fundamental gap, we propose the Two-Way\nGarment Transfer Model (TWGTM), to the best of our knowledge, the first unified\nframework for joint clothing-centric image synthesis that simultaneously\nresolves both mask-guided VTON and mask-free VTOFF through bidirectional\nfeature disentanglement. Specifically, our framework employs dual-conditioned\nguidance from both latent and pixel spaces of reference images to seamlessly\nbridge the dual tasks. On the other hand, to resolve the inherent mask\ndependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a\nphased training paradigm that progressively bridges this modality gap.\nExtensive qualitative and quantitative experiments conducted across the\nDressCode and VITON-HD datasets validate the efficacy and competitive edge of\nour proposed approach.\n", "link": "http://arxiv.org/abs/2508.04551v1", "date": "2025-08-06", "relevancy": 2.7468, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6997}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.68}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Two-Way%20Garment%20Transfer%3A%20Unified%20Diffusion%20Framework%20for%20Dressing%20and%0A%20%20Undressing%20Synthesis&body=Title%3A%20Two-Way%20Garment%20Transfer%3A%20Unified%20Diffusion%20Framework%20for%20Dressing%20and%0A%20%20Undressing%20Synthesis%0AAuthor%3A%20Angang%20Zhang%20and%20Fang%20Deng%20and%20Hao%20Chen%20and%20Zhongjian%20Chen%20and%20Junyan%20Li%0AAbstract%3A%20%20%20While%20recent%20advances%20in%20virtual%20try-on%20%28VTON%29%20have%20achieved%20realistic%0Agarment%20transfer%20to%20human%20subjects%2C%20its%20inverse%20task%2C%20virtual%20try-off%20%28VTOFF%29%2C%0Awhich%20aims%20to%20reconstruct%20canonical%20garment%20templates%20from%20dressed%20humans%2C%0Aremains%20critically%20underexplored%20and%20lacks%20systematic%20investigation.%20Existing%0Aworks%20predominantly%20treat%20them%20as%20isolated%20tasks%3A%20VTON%20focuses%20on%20garment%0Adressing%20while%20VTOFF%20addresses%20garment%20extraction%2C%20thereby%20neglecting%20their%0Acomplementary%20symmetry.%20To%20bridge%20this%20fundamental%20gap%2C%20we%20propose%20the%20Two-Way%0AGarment%20Transfer%20Model%20%28TWGTM%29%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20unified%0Aframework%20for%20joint%20clothing-centric%20image%20synthesis%20that%20simultaneously%0Aresolves%20both%20mask-guided%20VTON%20and%20mask-free%20VTOFF%20through%20bidirectional%0Afeature%20disentanglement.%20Specifically%2C%20our%20framework%20employs%20dual-conditioned%0Aguidance%20from%20both%20latent%20and%20pixel%20spaces%20of%20reference%20images%20to%20seamlessly%0Abridge%20the%20dual%20tasks.%20On%20the%20other%20hand%2C%20to%20resolve%20the%20inherent%20mask%0Adependency%20asymmetry%20between%20mask-guided%20VTON%20and%20mask-free%20VTOFF%2C%20we%20devise%20a%0Aphased%20training%20paradigm%20that%20progressively%20bridges%20this%20modality%20gap.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20conducted%20across%20the%0ADressCode%20and%20VITON-HD%20datasets%20validate%20the%20efficacy%20and%20competitive%20edge%20of%0Aour%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwo-Way%2520Garment%2520Transfer%253A%2520Unified%2520Diffusion%2520Framework%2520for%2520Dressing%2520and%250A%2520%2520Undressing%2520Synthesis%26entry.906535625%3DAngang%2520Zhang%2520and%2520Fang%2520Deng%2520and%2520Hao%2520Chen%2520and%2520Zhongjian%2520Chen%2520and%2520Junyan%2520Li%26entry.1292438233%3D%2520%2520While%2520recent%2520advances%2520in%2520virtual%2520try-on%2520%2528VTON%2529%2520have%2520achieved%2520realistic%250Agarment%2520transfer%2520to%2520human%2520subjects%252C%2520its%2520inverse%2520task%252C%2520virtual%2520try-off%2520%2528VTOFF%2529%252C%250Awhich%2520aims%2520to%2520reconstruct%2520canonical%2520garment%2520templates%2520from%2520dressed%2520humans%252C%250Aremains%2520critically%2520underexplored%2520and%2520lacks%2520systematic%2520investigation.%2520Existing%250Aworks%2520predominantly%2520treat%2520them%2520as%2520isolated%2520tasks%253A%2520VTON%2520focuses%2520on%2520garment%250Adressing%2520while%2520VTOFF%2520addresses%2520garment%2520extraction%252C%2520thereby%2520neglecting%2520their%250Acomplementary%2520symmetry.%2520To%2520bridge%2520this%2520fundamental%2520gap%252C%2520we%2520propose%2520the%2520Two-Way%250AGarment%2520Transfer%2520Model%2520%2528TWGTM%2529%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520unified%250Aframework%2520for%2520joint%2520clothing-centric%2520image%2520synthesis%2520that%2520simultaneously%250Aresolves%2520both%2520mask-guided%2520VTON%2520and%2520mask-free%2520VTOFF%2520through%2520bidirectional%250Afeature%2520disentanglement.%2520Specifically%252C%2520our%2520framework%2520employs%2520dual-conditioned%250Aguidance%2520from%2520both%2520latent%2520and%2520pixel%2520spaces%2520of%2520reference%2520images%2520to%2520seamlessly%250Abridge%2520the%2520dual%2520tasks.%2520On%2520the%2520other%2520hand%252C%2520to%2520resolve%2520the%2520inherent%2520mask%250Adependency%2520asymmetry%2520between%2520mask-guided%2520VTON%2520and%2520mask-free%2520VTOFF%252C%2520we%2520devise%2520a%250Aphased%2520training%2520paradigm%2520that%2520progressively%2520bridges%2520this%2520modality%2520gap.%250AExtensive%2520qualitative%2520and%2520quantitative%2520experiments%2520conducted%2520across%2520the%250ADressCode%2520and%2520VITON-HD%2520datasets%2520validate%2520the%2520efficacy%2520and%2520competitive%2520edge%2520of%250Aour%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Two-Way%20Garment%20Transfer%3A%20Unified%20Diffusion%20Framework%20for%20Dressing%20and%0A%20%20Undressing%20Synthesis&entry.906535625=Angang%20Zhang%20and%20Fang%20Deng%20and%20Hao%20Chen%20and%20Zhongjian%20Chen%20and%20Junyan%20Li&entry.1292438233=%20%20While%20recent%20advances%20in%20virtual%20try-on%20%28VTON%29%20have%20achieved%20realistic%0Agarment%20transfer%20to%20human%20subjects%2C%20its%20inverse%20task%2C%20virtual%20try-off%20%28VTOFF%29%2C%0Awhich%20aims%20to%20reconstruct%20canonical%20garment%20templates%20from%20dressed%20humans%2C%0Aremains%20critically%20underexplored%20and%20lacks%20systematic%20investigation.%20Existing%0Aworks%20predominantly%20treat%20them%20as%20isolated%20tasks%3A%20VTON%20focuses%20on%20garment%0Adressing%20while%20VTOFF%20addresses%20garment%20extraction%2C%20thereby%20neglecting%20their%0Acomplementary%20symmetry.%20To%20bridge%20this%20fundamental%20gap%2C%20we%20propose%20the%20Two-Way%0AGarment%20Transfer%20Model%20%28TWGTM%29%2C%20to%20the%20best%20of%20our%20knowledge%2C%20the%20first%20unified%0Aframework%20for%20joint%20clothing-centric%20image%20synthesis%20that%20simultaneously%0Aresolves%20both%20mask-guided%20VTON%20and%20mask-free%20VTOFF%20through%20bidirectional%0Afeature%20disentanglement.%20Specifically%2C%20our%20framework%20employs%20dual-conditioned%0Aguidance%20from%20both%20latent%20and%20pixel%20spaces%20of%20reference%20images%20to%20seamlessly%0Abridge%20the%20dual%20tasks.%20On%20the%20other%20hand%2C%20to%20resolve%20the%20inherent%20mask%0Adependency%20asymmetry%20between%20mask-guided%20VTON%20and%20mask-free%20VTOFF%2C%20we%20devise%20a%0Aphased%20training%20paradigm%20that%20progressively%20bridges%20this%20modality%20gap.%0AExtensive%20qualitative%20and%20quantitative%20experiments%20conducted%20across%20the%0ADressCode%20and%20VITON-HD%20datasets%20validate%20the%20efficacy%20and%20competitive%20edge%20of%0Aour%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04551v1&entry.124074799=Read"},
{"title": "One Model For All: Partial Diffusion for Unified Try-On and Try-Off in\n  Any Pose", "author": "Jinxi Liu and Zijian He and Guangrun Wang and Guanbin Li and Liang Lin", "abstract": "  Recent diffusion-based approaches have made significant advances in\nimage-based virtual try-on, enabling more realistic and end-to-end garment\nsynthesis. However, most existing methods remain constrained by their reliance\non exhibition garments and segmentation masks, as well as their limited ability\nto handle flexible pose variations. These limitations reduce their practicality\nin real-world scenarios-for instance, users cannot easily transfer garments\nworn by one person onto another, and the generated try-on results are typically\nrestricted to the same pose as the reference image. In this paper, we introduce\n\\textbf{OMFA} (\\emph{One Model For All}), a unified diffusion framework for\nboth virtual try-on and try-off that operates without the need for exhibition\ngarments and supports arbitrary poses. For example, OMFA enables removing\ngarments from a source person (try-off) and transferring them onto a target\nperson (try-on), while also allowing the generated target to appear in novel\nposes-even without access to multi-pose images of that person. OMFA is built\nupon a novel \\emph{partial diffusion} strategy that selectively applies noise\nand denoising to individual components of the joint input-such as the garment,\nthe person image, or the face-enabling dynamic subtask control and efficient\nbidirectional garment-person transformation. The framework is entirely\nmask-free and requires only a single portrait and a target pose as input,\nmaking it well-suited for real-world applications. Additionally, by leveraging\nSMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose\ntry-on from just one image. Extensive experiments demonstrate that OMFA\nachieves state-of-the-art results on both try-on and try-off tasks, providing a\npractical and generalizable solution for virtual garment synthesis. The project\npage is here: https://onemodelforall.github.io/.\n", "link": "http://arxiv.org/abs/2508.04559v1", "date": "2025-08-06", "relevancy": 2.7418, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6903}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6839}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Model%20For%20All%3A%20Partial%20Diffusion%20for%20Unified%20Try-On%20and%20Try-Off%20in%0A%20%20Any%20Pose&body=Title%3A%20One%20Model%20For%20All%3A%20Partial%20Diffusion%20for%20Unified%20Try-On%20and%20Try-Off%20in%0A%20%20Any%20Pose%0AAuthor%3A%20Jinxi%20Liu%20and%20Zijian%20He%20and%20Guangrun%20Wang%20and%20Guanbin%20Li%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Recent%20diffusion-based%20approaches%20have%20made%20significant%20advances%20in%0Aimage-based%20virtual%20try-on%2C%20enabling%20more%20realistic%20and%20end-to-end%20garment%0Asynthesis.%20However%2C%20most%20existing%20methods%20remain%20constrained%20by%20their%20reliance%0Aon%20exhibition%20garments%20and%20segmentation%20masks%2C%20as%20well%20as%20their%20limited%20ability%0Ato%20handle%20flexible%20pose%20variations.%20These%20limitations%20reduce%20their%20practicality%0Ain%20real-world%20scenarios-for%20instance%2C%20users%20cannot%20easily%20transfer%20garments%0Aworn%20by%20one%20person%20onto%20another%2C%20and%20the%20generated%20try-on%20results%20are%20typically%0Arestricted%20to%20the%20same%20pose%20as%20the%20reference%20image.%20In%20this%20paper%2C%20we%20introduce%0A%5Ctextbf%7BOMFA%7D%20%28%5Cemph%7BOne%20Model%20For%20All%7D%29%2C%20a%20unified%20diffusion%20framework%20for%0Aboth%20virtual%20try-on%20and%20try-off%20that%20operates%20without%20the%20need%20for%20exhibition%0Agarments%20and%20supports%20arbitrary%20poses.%20For%20example%2C%20OMFA%20enables%20removing%0Agarments%20from%20a%20source%20person%20%28try-off%29%20and%20transferring%20them%20onto%20a%20target%0Aperson%20%28try-on%29%2C%20while%20also%20allowing%20the%20generated%20target%20to%20appear%20in%20novel%0Aposes-even%20without%20access%20to%20multi-pose%20images%20of%20that%20person.%20OMFA%20is%20built%0Aupon%20a%20novel%20%5Cemph%7Bpartial%20diffusion%7D%20strategy%20that%20selectively%20applies%20noise%0Aand%20denoising%20to%20individual%20components%20of%20the%20joint%20input-such%20as%20the%20garment%2C%0Athe%20person%20image%2C%20or%20the%20face-enabling%20dynamic%20subtask%20control%20and%20efficient%0Abidirectional%20garment-person%20transformation.%20The%20framework%20is%20entirely%0Amask-free%20and%20requires%20only%20a%20single%20portrait%20and%20a%20target%20pose%20as%20input%2C%0Amaking%20it%20well-suited%20for%20real-world%20applications.%20Additionally%2C%20by%20leveraging%0ASMPL-X-based%20pose%20conditioning%2C%20OMFA%20supports%20multi-view%20and%20arbitrary-pose%0Atry-on%20from%20just%20one%20image.%20Extensive%20experiments%20demonstrate%20that%20OMFA%0Aachieves%20state-of-the-art%20results%20on%20both%20try-on%20and%20try-off%20tasks%2C%20providing%20a%0Apractical%20and%20generalizable%20solution%20for%20virtual%20garment%20synthesis.%20The%20project%0Apage%20is%20here%3A%20https%3A//onemodelforall.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Model%2520For%2520All%253A%2520Partial%2520Diffusion%2520for%2520Unified%2520Try-On%2520and%2520Try-Off%2520in%250A%2520%2520Any%2520Pose%26entry.906535625%3DJinxi%2520Liu%2520and%2520Zijian%2520He%2520and%2520Guangrun%2520Wang%2520and%2520Guanbin%2520Li%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520diffusion-based%2520approaches%2520have%2520made%2520significant%2520advances%2520in%250Aimage-based%2520virtual%2520try-on%252C%2520enabling%2520more%2520realistic%2520and%2520end-to-end%2520garment%250Asynthesis.%2520However%252C%2520most%2520existing%2520methods%2520remain%2520constrained%2520by%2520their%2520reliance%250Aon%2520exhibition%2520garments%2520and%2520segmentation%2520masks%252C%2520as%2520well%2520as%2520their%2520limited%2520ability%250Ato%2520handle%2520flexible%2520pose%2520variations.%2520These%2520limitations%2520reduce%2520their%2520practicality%250Ain%2520real-world%2520scenarios-for%2520instance%252C%2520users%2520cannot%2520easily%2520transfer%2520garments%250Aworn%2520by%2520one%2520person%2520onto%2520another%252C%2520and%2520the%2520generated%2520try-on%2520results%2520are%2520typically%250Arestricted%2520to%2520the%2520same%2520pose%2520as%2520the%2520reference%2520image.%2520In%2520this%2520paper%252C%2520we%2520introduce%250A%255Ctextbf%257BOMFA%257D%2520%2528%255Cemph%257BOne%2520Model%2520For%2520All%257D%2529%252C%2520a%2520unified%2520diffusion%2520framework%2520for%250Aboth%2520virtual%2520try-on%2520and%2520try-off%2520that%2520operates%2520without%2520the%2520need%2520for%2520exhibition%250Agarments%2520and%2520supports%2520arbitrary%2520poses.%2520For%2520example%252C%2520OMFA%2520enables%2520removing%250Agarments%2520from%2520a%2520source%2520person%2520%2528try-off%2529%2520and%2520transferring%2520them%2520onto%2520a%2520target%250Aperson%2520%2528try-on%2529%252C%2520while%2520also%2520allowing%2520the%2520generated%2520target%2520to%2520appear%2520in%2520novel%250Aposes-even%2520without%2520access%2520to%2520multi-pose%2520images%2520of%2520that%2520person.%2520OMFA%2520is%2520built%250Aupon%2520a%2520novel%2520%255Cemph%257Bpartial%2520diffusion%257D%2520strategy%2520that%2520selectively%2520applies%2520noise%250Aand%2520denoising%2520to%2520individual%2520components%2520of%2520the%2520joint%2520input-such%2520as%2520the%2520garment%252C%250Athe%2520person%2520image%252C%2520or%2520the%2520face-enabling%2520dynamic%2520subtask%2520control%2520and%2520efficient%250Abidirectional%2520garment-person%2520transformation.%2520The%2520framework%2520is%2520entirely%250Amask-free%2520and%2520requires%2520only%2520a%2520single%2520portrait%2520and%2520a%2520target%2520pose%2520as%2520input%252C%250Amaking%2520it%2520well-suited%2520for%2520real-world%2520applications.%2520Additionally%252C%2520by%2520leveraging%250ASMPL-X-based%2520pose%2520conditioning%252C%2520OMFA%2520supports%2520multi-view%2520and%2520arbitrary-pose%250Atry-on%2520from%2520just%2520one%2520image.%2520Extensive%2520experiments%2520demonstrate%2520that%2520OMFA%250Aachieves%2520state-of-the-art%2520results%2520on%2520both%2520try-on%2520and%2520try-off%2520tasks%252C%2520providing%2520a%250Apractical%2520and%2520generalizable%2520solution%2520for%2520virtual%2520garment%2520synthesis.%2520The%2520project%250Apage%2520is%2520here%253A%2520https%253A//onemodelforall.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Model%20For%20All%3A%20Partial%20Diffusion%20for%20Unified%20Try-On%20and%20Try-Off%20in%0A%20%20Any%20Pose&entry.906535625=Jinxi%20Liu%20and%20Zijian%20He%20and%20Guangrun%20Wang%20and%20Guanbin%20Li%20and%20Liang%20Lin&entry.1292438233=%20%20Recent%20diffusion-based%20approaches%20have%20made%20significant%20advances%20in%0Aimage-based%20virtual%20try-on%2C%20enabling%20more%20realistic%20and%20end-to-end%20garment%0Asynthesis.%20However%2C%20most%20existing%20methods%20remain%20constrained%20by%20their%20reliance%0Aon%20exhibition%20garments%20and%20segmentation%20masks%2C%20as%20well%20as%20their%20limited%20ability%0Ato%20handle%20flexible%20pose%20variations.%20These%20limitations%20reduce%20their%20practicality%0Ain%20real-world%20scenarios-for%20instance%2C%20users%20cannot%20easily%20transfer%20garments%0Aworn%20by%20one%20person%20onto%20another%2C%20and%20the%20generated%20try-on%20results%20are%20typically%0Arestricted%20to%20the%20same%20pose%20as%20the%20reference%20image.%20In%20this%20paper%2C%20we%20introduce%0A%5Ctextbf%7BOMFA%7D%20%28%5Cemph%7BOne%20Model%20For%20All%7D%29%2C%20a%20unified%20diffusion%20framework%20for%0Aboth%20virtual%20try-on%20and%20try-off%20that%20operates%20without%20the%20need%20for%20exhibition%0Agarments%20and%20supports%20arbitrary%20poses.%20For%20example%2C%20OMFA%20enables%20removing%0Agarments%20from%20a%20source%20person%20%28try-off%29%20and%20transferring%20them%20onto%20a%20target%0Aperson%20%28try-on%29%2C%20while%20also%20allowing%20the%20generated%20target%20to%20appear%20in%20novel%0Aposes-even%20without%20access%20to%20multi-pose%20images%20of%20that%20person.%20OMFA%20is%20built%0Aupon%20a%20novel%20%5Cemph%7Bpartial%20diffusion%7D%20strategy%20that%20selectively%20applies%20noise%0Aand%20denoising%20to%20individual%20components%20of%20the%20joint%20input-such%20as%20the%20garment%2C%0Athe%20person%20image%2C%20or%20the%20face-enabling%20dynamic%20subtask%20control%20and%20efficient%0Abidirectional%20garment-person%20transformation.%20The%20framework%20is%20entirely%0Amask-free%20and%20requires%20only%20a%20single%20portrait%20and%20a%20target%20pose%20as%20input%2C%0Amaking%20it%20well-suited%20for%20real-world%20applications.%20Additionally%2C%20by%20leveraging%0ASMPL-X-based%20pose%20conditioning%2C%20OMFA%20supports%20multi-view%20and%20arbitrary-pose%0Atry-on%20from%20just%20one%20image.%20Extensive%20experiments%20demonstrate%20that%20OMFA%0Aachieves%20state-of-the-art%20results%20on%20both%20try-on%20and%20try-off%20tasks%2C%20providing%20a%0Apractical%20and%20generalizable%20solution%20for%20virtual%20garment%20synthesis.%20The%20project%0Apage%20is%20here%3A%20https%3A//onemodelforall.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04559v1&entry.124074799=Read"},
{"title": "Through the Magnifying Glass: Adaptive Perception Magnification for\n  Hallucination-Free VLM Decoding", "author": "Shunqi Mao and Chaoyi Zhang and Weidong Cai", "abstract": "  Existing vision-language models (VLMs) often suffer from visual\nhallucination, where the generated responses contain inaccuracies that are not\ngrounded in the visual input. Efforts to address this issue without model\nfinetuning primarily mitigate hallucination by contrastively reducing language\nbiases or amplifying the weights of visual embedding during decoding. However,\nthese approaches remain limited in their ability to capture fine-grained visual\ndetails. In this work, we propose the Perception Magnifier (PM), a novel visual\ndecoding method that iteratively isolates relevant visual tokens based on\nattention and magnifies the corresponding regions, spurring the model to\nconcentrate on fine-grained visual details during decoding. By magnifying\ncritical regions while preserving the structural and contextual information at\neach decoding step, PM allows the VLM to enhance its scrutiny of the visual\ninput, hence producing more accurate and faithful responses. Extensive\nexperimental results demonstrate that PM not only achieves superior\nhallucination mitigation but also enhances language generation while preserving\nstrong reasoning capabilities.\n", "link": "http://arxiv.org/abs/2503.10183v3", "date": "2025-08-06", "relevancy": 2.7363, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Through%20the%20Magnifying%20Glass%3A%20Adaptive%20Perception%20Magnification%20for%0A%20%20Hallucination-Free%20VLM%20Decoding&body=Title%3A%20Through%20the%20Magnifying%20Glass%3A%20Adaptive%20Perception%20Magnification%20for%0A%20%20Hallucination-Free%20VLM%20Decoding%0AAuthor%3A%20Shunqi%20Mao%20and%20Chaoyi%20Zhang%20and%20Weidong%20Cai%0AAbstract%3A%20%20%20Existing%20vision-language%20models%20%28VLMs%29%20often%20suffer%20from%20visual%0Ahallucination%2C%20where%20the%20generated%20responses%20contain%20inaccuracies%20that%20are%20not%0Agrounded%20in%20the%20visual%20input.%20Efforts%20to%20address%20this%20issue%20without%20model%0Afinetuning%20primarily%20mitigate%20hallucination%20by%20contrastively%20reducing%20language%0Abiases%20or%20amplifying%20the%20weights%20of%20visual%20embedding%20during%20decoding.%20However%2C%0Athese%20approaches%20remain%20limited%20in%20their%20ability%20to%20capture%20fine-grained%20visual%0Adetails.%20In%20this%20work%2C%20we%20propose%20the%20Perception%20Magnifier%20%28PM%29%2C%20a%20novel%20visual%0Adecoding%20method%20that%20iteratively%20isolates%20relevant%20visual%20tokens%20based%20on%0Aattention%20and%20magnifies%20the%20corresponding%20regions%2C%20spurring%20the%20model%20to%0Aconcentrate%20on%20fine-grained%20visual%20details%20during%20decoding.%20By%20magnifying%0Acritical%20regions%20while%20preserving%20the%20structural%20and%20contextual%20information%20at%0Aeach%20decoding%20step%2C%20PM%20allows%20the%20VLM%20to%20enhance%20its%20scrutiny%20of%20the%20visual%0Ainput%2C%20hence%20producing%20more%20accurate%20and%20faithful%20responses.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20PM%20not%20only%20achieves%20superior%0Ahallucination%20mitigation%20but%20also%20enhances%20language%20generation%20while%20preserving%0Astrong%20reasoning%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10183v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThrough%2520the%2520Magnifying%2520Glass%253A%2520Adaptive%2520Perception%2520Magnification%2520for%250A%2520%2520Hallucination-Free%2520VLM%2520Decoding%26entry.906535625%3DShunqi%2520Mao%2520and%2520Chaoyi%2520Zhang%2520and%2520Weidong%2520Cai%26entry.1292438233%3D%2520%2520Existing%2520vision-language%2520models%2520%2528VLMs%2529%2520often%2520suffer%2520from%2520visual%250Ahallucination%252C%2520where%2520the%2520generated%2520responses%2520contain%2520inaccuracies%2520that%2520are%2520not%250Agrounded%2520in%2520the%2520visual%2520input.%2520Efforts%2520to%2520address%2520this%2520issue%2520without%2520model%250Afinetuning%2520primarily%2520mitigate%2520hallucination%2520by%2520contrastively%2520reducing%2520language%250Abiases%2520or%2520amplifying%2520the%2520weights%2520of%2520visual%2520embedding%2520during%2520decoding.%2520However%252C%250Athese%2520approaches%2520remain%2520limited%2520in%2520their%2520ability%2520to%2520capture%2520fine-grained%2520visual%250Adetails.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Perception%2520Magnifier%2520%2528PM%2529%252C%2520a%2520novel%2520visual%250Adecoding%2520method%2520that%2520iteratively%2520isolates%2520relevant%2520visual%2520tokens%2520based%2520on%250Aattention%2520and%2520magnifies%2520the%2520corresponding%2520regions%252C%2520spurring%2520the%2520model%2520to%250Aconcentrate%2520on%2520fine-grained%2520visual%2520details%2520during%2520decoding.%2520By%2520magnifying%250Acritical%2520regions%2520while%2520preserving%2520the%2520structural%2520and%2520contextual%2520information%2520at%250Aeach%2520decoding%2520step%252C%2520PM%2520allows%2520the%2520VLM%2520to%2520enhance%2520its%2520scrutiny%2520of%2520the%2520visual%250Ainput%252C%2520hence%2520producing%2520more%2520accurate%2520and%2520faithful%2520responses.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520PM%2520not%2520only%2520achieves%2520superior%250Ahallucination%2520mitigation%2520but%2520also%2520enhances%2520language%2520generation%2520while%2520preserving%250Astrong%2520reasoning%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10183v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Through%20the%20Magnifying%20Glass%3A%20Adaptive%20Perception%20Magnification%20for%0A%20%20Hallucination-Free%20VLM%20Decoding&entry.906535625=Shunqi%20Mao%20and%20Chaoyi%20Zhang%20and%20Weidong%20Cai&entry.1292438233=%20%20Existing%20vision-language%20models%20%28VLMs%29%20often%20suffer%20from%20visual%0Ahallucination%2C%20where%20the%20generated%20responses%20contain%20inaccuracies%20that%20are%20not%0Agrounded%20in%20the%20visual%20input.%20Efforts%20to%20address%20this%20issue%20without%20model%0Afinetuning%20primarily%20mitigate%20hallucination%20by%20contrastively%20reducing%20language%0Abiases%20or%20amplifying%20the%20weights%20of%20visual%20embedding%20during%20decoding.%20However%2C%0Athese%20approaches%20remain%20limited%20in%20their%20ability%20to%20capture%20fine-grained%20visual%0Adetails.%20In%20this%20work%2C%20we%20propose%20the%20Perception%20Magnifier%20%28PM%29%2C%20a%20novel%20visual%0Adecoding%20method%20that%20iteratively%20isolates%20relevant%20visual%20tokens%20based%20on%0Aattention%20and%20magnifies%20the%20corresponding%20regions%2C%20spurring%20the%20model%20to%0Aconcentrate%20on%20fine-grained%20visual%20details%20during%20decoding.%20By%20magnifying%0Acritical%20regions%20while%20preserving%20the%20structural%20and%20contextual%20information%20at%0Aeach%20decoding%20step%2C%20PM%20allows%20the%20VLM%20to%20enhance%20its%20scrutiny%20of%20the%20visual%0Ainput%2C%20hence%20producing%20more%20accurate%20and%20faithful%20responses.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20PM%20not%20only%20achieves%20superior%0Ahallucination%20mitigation%20but%20also%20enhances%20language%20generation%20while%20preserving%0Astrong%20reasoning%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10183v3&entry.124074799=Read"},
{"title": "Do Recommender Systems Really Leverage Multimodal Content? A\n  Comprehensive Analysis on Multimodal Representations for Recommendation", "author": "Claudio Pomo and Matteo Attimonelli and Danilo Danese and Fedelucio Narducci and Tommaso Di Noia", "abstract": "  Multimodal Recommender Systems aim to improve recommendation accuracy by\nintegrating heterogeneous content, such as images and textual metadata. While\neffective, it remains unclear whether their gains stem from true multimodal\nunderstanding or increased model complexity. This work investigates the role of\nmultimodal item embeddings, emphasizing the semantic informativeness of the\nrepresentations. Initial experiments reveal that embeddings from standard\nextractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on\nmodality-specific encoders and ad hoc fusion strategies that lack control over\ncross-modal alignment. To overcome these limitations, we leverage Large\nVision-Language Models (LVLMs) to generate multimodal-by-design embeddings via\nstructured prompts. This approach yields semantically aligned representations\nwithout requiring any fusion. Experiments across multiple settings show notable\nperformance improvements. Furthermore, LVLMs embeddings offer a distinctive\nadvantage: they can be decoded into structured textual descriptions, enabling\ndirect assessment of their multimodal comprehension. When such descriptions are\nincorporated as side content into recommender systems, they improve\nrecommendation performance, empirically validating the semantic depth and\nalignment encoded within LVLMs outputs. Our study highlights the importance of\nsemantically rich representations and positions LVLMs as a compelling\nfoundation for building robust and meaningful multimodal representations in\nrecommendation tasks.\n", "link": "http://arxiv.org/abs/2508.04571v1", "date": "2025-08-06", "relevancy": 2.7362, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Recommender%20Systems%20Really%20Leverage%20Multimodal%20Content%3F%20A%0A%20%20Comprehensive%20Analysis%20on%20Multimodal%20Representations%20for%20Recommendation&body=Title%3A%20Do%20Recommender%20Systems%20Really%20Leverage%20Multimodal%20Content%3F%20A%0A%20%20Comprehensive%20Analysis%20on%20Multimodal%20Representations%20for%20Recommendation%0AAuthor%3A%20Claudio%20Pomo%20and%20Matteo%20Attimonelli%20and%20Danilo%20Danese%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia%0AAbstract%3A%20%20%20Multimodal%20Recommender%20Systems%20aim%20to%20improve%20recommendation%20accuracy%20by%0Aintegrating%20heterogeneous%20content%2C%20such%20as%20images%20and%20textual%20metadata.%20While%0Aeffective%2C%20it%20remains%20unclear%20whether%20their%20gains%20stem%20from%20true%20multimodal%0Aunderstanding%20or%20increased%20model%20complexity.%20This%20work%20investigates%20the%20role%20of%0Amultimodal%20item%20embeddings%2C%20emphasizing%20the%20semantic%20informativeness%20of%20the%0Arepresentations.%20Initial%20experiments%20reveal%20that%20embeddings%20from%20standard%0Aextractors%20%28e.g.%2C%20ResNet50%2C%20Sentence-Bert%29%20enhance%20performance%2C%20but%20rely%20on%0Amodality-specific%20encoders%20and%20ad%20hoc%20fusion%20strategies%20that%20lack%20control%20over%0Across-modal%20alignment.%20To%20overcome%20these%20limitations%2C%20we%20leverage%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20generate%20multimodal-by-design%20embeddings%20via%0Astructured%20prompts.%20This%20approach%20yields%20semantically%20aligned%20representations%0Awithout%20requiring%20any%20fusion.%20Experiments%20across%20multiple%20settings%20show%20notable%0Aperformance%20improvements.%20Furthermore%2C%20LVLMs%20embeddings%20offer%20a%20distinctive%0Aadvantage%3A%20they%20can%20be%20decoded%20into%20structured%20textual%20descriptions%2C%20enabling%0Adirect%20assessment%20of%20their%20multimodal%20comprehension.%20When%20such%20descriptions%20are%0Aincorporated%20as%20side%20content%20into%20recommender%20systems%2C%20they%20improve%0Arecommendation%20performance%2C%20empirically%20validating%20the%20semantic%20depth%20and%0Aalignment%20encoded%20within%20LVLMs%20outputs.%20Our%20study%20highlights%20the%20importance%20of%0Asemantically%20rich%20representations%20and%20positions%20LVLMs%20as%20a%20compelling%0Afoundation%20for%20building%20robust%20and%20meaningful%20multimodal%20representations%20in%0Arecommendation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Recommender%2520Systems%2520Really%2520Leverage%2520Multimodal%2520Content%253F%2520A%250A%2520%2520Comprehensive%2520Analysis%2520on%2520Multimodal%2520Representations%2520for%2520Recommendation%26entry.906535625%3DClaudio%2520Pomo%2520and%2520Matteo%2520Attimonelli%2520and%2520Danilo%2520Danese%2520and%2520Fedelucio%2520Narducci%2520and%2520Tommaso%2520Di%2520Noia%26entry.1292438233%3D%2520%2520Multimodal%2520Recommender%2520Systems%2520aim%2520to%2520improve%2520recommendation%2520accuracy%2520by%250Aintegrating%2520heterogeneous%2520content%252C%2520such%2520as%2520images%2520and%2520textual%2520metadata.%2520While%250Aeffective%252C%2520it%2520remains%2520unclear%2520whether%2520their%2520gains%2520stem%2520from%2520true%2520multimodal%250Aunderstanding%2520or%2520increased%2520model%2520complexity.%2520This%2520work%2520investigates%2520the%2520role%2520of%250Amultimodal%2520item%2520embeddings%252C%2520emphasizing%2520the%2520semantic%2520informativeness%2520of%2520the%250Arepresentations.%2520Initial%2520experiments%2520reveal%2520that%2520embeddings%2520from%2520standard%250Aextractors%2520%2528e.g.%252C%2520ResNet50%252C%2520Sentence-Bert%2529%2520enhance%2520performance%252C%2520but%2520rely%2520on%250Amodality-specific%2520encoders%2520and%2520ad%2520hoc%2520fusion%2520strategies%2520that%2520lack%2520control%2520over%250Across-modal%2520alignment.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520leverage%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520generate%2520multimodal-by-design%2520embeddings%2520via%250Astructured%2520prompts.%2520This%2520approach%2520yields%2520semantically%2520aligned%2520representations%250Awithout%2520requiring%2520any%2520fusion.%2520Experiments%2520across%2520multiple%2520settings%2520show%2520notable%250Aperformance%2520improvements.%2520Furthermore%252C%2520LVLMs%2520embeddings%2520offer%2520a%2520distinctive%250Aadvantage%253A%2520they%2520can%2520be%2520decoded%2520into%2520structured%2520textual%2520descriptions%252C%2520enabling%250Adirect%2520assessment%2520of%2520their%2520multimodal%2520comprehension.%2520When%2520such%2520descriptions%2520are%250Aincorporated%2520as%2520side%2520content%2520into%2520recommender%2520systems%252C%2520they%2520improve%250Arecommendation%2520performance%252C%2520empirically%2520validating%2520the%2520semantic%2520depth%2520and%250Aalignment%2520encoded%2520within%2520LVLMs%2520outputs.%2520Our%2520study%2520highlights%2520the%2520importance%2520of%250Asemantically%2520rich%2520representations%2520and%2520positions%2520LVLMs%2520as%2520a%2520compelling%250Afoundation%2520for%2520building%2520robust%2520and%2520meaningful%2520multimodal%2520representations%2520in%250Arecommendation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Recommender%20Systems%20Really%20Leverage%20Multimodal%20Content%3F%20A%0A%20%20Comprehensive%20Analysis%20on%20Multimodal%20Representations%20for%20Recommendation&entry.906535625=Claudio%20Pomo%20and%20Matteo%20Attimonelli%20and%20Danilo%20Danese%20and%20Fedelucio%20Narducci%20and%20Tommaso%20Di%20Noia&entry.1292438233=%20%20Multimodal%20Recommender%20Systems%20aim%20to%20improve%20recommendation%20accuracy%20by%0Aintegrating%20heterogeneous%20content%2C%20such%20as%20images%20and%20textual%20metadata.%20While%0Aeffective%2C%20it%20remains%20unclear%20whether%20their%20gains%20stem%20from%20true%20multimodal%0Aunderstanding%20or%20increased%20model%20complexity.%20This%20work%20investigates%20the%20role%20of%0Amultimodal%20item%20embeddings%2C%20emphasizing%20the%20semantic%20informativeness%20of%20the%0Arepresentations.%20Initial%20experiments%20reveal%20that%20embeddings%20from%20standard%0Aextractors%20%28e.g.%2C%20ResNet50%2C%20Sentence-Bert%29%20enhance%20performance%2C%20but%20rely%20on%0Amodality-specific%20encoders%20and%20ad%20hoc%20fusion%20strategies%20that%20lack%20control%20over%0Across-modal%20alignment.%20To%20overcome%20these%20limitations%2C%20we%20leverage%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20generate%20multimodal-by-design%20embeddings%20via%0Astructured%20prompts.%20This%20approach%20yields%20semantically%20aligned%20representations%0Awithout%20requiring%20any%20fusion.%20Experiments%20across%20multiple%20settings%20show%20notable%0Aperformance%20improvements.%20Furthermore%2C%20LVLMs%20embeddings%20offer%20a%20distinctive%0Aadvantage%3A%20they%20can%20be%20decoded%20into%20structured%20textual%20descriptions%2C%20enabling%0Adirect%20assessment%20of%20their%20multimodal%20comprehension.%20When%20such%20descriptions%20are%0Aincorporated%20as%20side%20content%20into%20recommender%20systems%2C%20they%20improve%0Arecommendation%20performance%2C%20empirically%20validating%20the%20semantic%20depth%20and%0Aalignment%20encoded%20within%20LVLMs%20outputs.%20Our%20study%20highlights%20the%20importance%20of%0Asemantically%20rich%20representations%20and%20positions%20LVLMs%20as%20a%20compelling%0Afoundation%20for%20building%20robust%20and%20meaningful%20multimodal%20representations%20in%0Arecommendation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04571v1&entry.124074799=Read"},
{"title": "COBRA: A Continual Learning Approach to Vision-Brain Understanding", "author": "Xuan-Bac Nguyen and Manuel Serna-Aguilera and Arabinda Kumar Choudhary and Pawan Sinha and Xin Li and Khoa Luu", "abstract": "  Vision-Brain Understanding (VBU) aims to extract visual information perceived\nby humans from brain activity recorded through functional Magnetic Resonance\nImaging (fMRI). Despite notable advancements in recent years, existing studies\nin VBU continue to face the challenge of catastrophic forgetting, where models\nlose knowledge from prior subjects as they adapt to new ones. Addressing\ncontinual learning in this field is, therefore, essential. This paper\nintroduces a novel framework called Continual Learning for Vision-Brain (COBRA)\nto address continual learning in VBU. Our approach includes three novel\nmodules: a Subject Commonality (SC) module, a Prompt-based Subject Specific\n(PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer\nmodule. The SC module captures shared vision-brain patterns across subjects,\npreserving this knowledge as the model encounters new subjects, thereby\nreducing the impact of catastrophic forgetting. On the other hand, the PSS\nmodule learns unique vision-brain patterns specific to each subject. Finally,\nthe MRIFormer module contains a transformer encoder and decoder that learns the\nfMRI features for VBU from common and specific patterns. In a continual\nlearning setup, COBRA is trained in new PSS and MRIFormer modules for new\nsubjects, leaving the modules of previous subjects unaffected. As a result,\nCOBRA effectively addresses catastrophic forgetting and achieves\nstate-of-the-art performance in both continual learning and vision-brain\nreconstruction tasks, surpassing previous methods.\n", "link": "http://arxiv.org/abs/2411.17475v3", "date": "2025-08-06", "relevancy": 2.73, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COBRA%3A%20A%20Continual%20Learning%20Approach%20to%20Vision-Brain%20Understanding&body=Title%3A%20COBRA%3A%20A%20Continual%20Learning%20Approach%20to%20Vision-Brain%20Understanding%0AAuthor%3A%20Xuan-Bac%20Nguyen%20and%20Manuel%20Serna-Aguilera%20and%20Arabinda%20Kumar%20Choudhary%20and%20Pawan%20Sinha%20and%20Xin%20Li%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Vision-Brain%20Understanding%20%28VBU%29%20aims%20to%20extract%20visual%20information%20perceived%0Aby%20humans%20from%20brain%20activity%20recorded%20through%20functional%20Magnetic%20Resonance%0AImaging%20%28fMRI%29.%20Despite%20notable%20advancements%20in%20recent%20years%2C%20existing%20studies%0Ain%20VBU%20continue%20to%20face%20the%20challenge%20of%20catastrophic%20forgetting%2C%20where%20models%0Alose%20knowledge%20from%20prior%20subjects%20as%20they%20adapt%20to%20new%20ones.%20Addressing%0Acontinual%20learning%20in%20this%20field%20is%2C%20therefore%2C%20essential.%20This%20paper%0Aintroduces%20a%20novel%20framework%20called%20Continual%20Learning%20for%20Vision-Brain%20%28COBRA%29%0Ato%20address%20continual%20learning%20in%20VBU.%20Our%20approach%20includes%20three%20novel%0Amodules%3A%20a%20Subject%20Commonality%20%28SC%29%20module%2C%20a%20Prompt-based%20Subject%20Specific%0A%28PSS%29%20module%2C%20and%20a%20transformer-based%20module%20for%20fMRI%2C%20denoted%20as%20MRIFormer%0Amodule.%20The%20SC%20module%20captures%20shared%20vision-brain%20patterns%20across%20subjects%2C%0Apreserving%20this%20knowledge%20as%20the%20model%20encounters%20new%20subjects%2C%20thereby%0Areducing%20the%20impact%20of%20catastrophic%20forgetting.%20On%20the%20other%20hand%2C%20the%20PSS%0Amodule%20learns%20unique%20vision-brain%20patterns%20specific%20to%20each%20subject.%20Finally%2C%0Athe%20MRIFormer%20module%20contains%20a%20transformer%20encoder%20and%20decoder%20that%20learns%20the%0AfMRI%20features%20for%20VBU%20from%20common%20and%20specific%20patterns.%20In%20a%20continual%0Alearning%20setup%2C%20COBRA%20is%20trained%20in%20new%20PSS%20and%20MRIFormer%20modules%20for%20new%0Asubjects%2C%20leaving%20the%20modules%20of%20previous%20subjects%20unaffected.%20As%20a%20result%2C%0ACOBRA%20effectively%20addresses%20catastrophic%20forgetting%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20continual%20learning%20and%20vision-brain%0Areconstruction%20tasks%2C%20surpassing%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17475v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOBRA%253A%2520A%2520Continual%2520Learning%2520Approach%2520to%2520Vision-Brain%2520Understanding%26entry.906535625%3DXuan-Bac%2520Nguyen%2520and%2520Manuel%2520Serna-Aguilera%2520and%2520Arabinda%2520Kumar%2520Choudhary%2520and%2520Pawan%2520Sinha%2520and%2520Xin%2520Li%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Vision-Brain%2520Understanding%2520%2528VBU%2529%2520aims%2520to%2520extract%2520visual%2520information%2520perceived%250Aby%2520humans%2520from%2520brain%2520activity%2520recorded%2520through%2520functional%2520Magnetic%2520Resonance%250AImaging%2520%2528fMRI%2529.%2520Despite%2520notable%2520advancements%2520in%2520recent%2520years%252C%2520existing%2520studies%250Ain%2520VBU%2520continue%2520to%2520face%2520the%2520challenge%2520of%2520catastrophic%2520forgetting%252C%2520where%2520models%250Alose%2520knowledge%2520from%2520prior%2520subjects%2520as%2520they%2520adapt%2520to%2520new%2520ones.%2520Addressing%250Acontinual%2520learning%2520in%2520this%2520field%2520is%252C%2520therefore%252C%2520essential.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520framework%2520called%2520Continual%2520Learning%2520for%2520Vision-Brain%2520%2528COBRA%2529%250Ato%2520address%2520continual%2520learning%2520in%2520VBU.%2520Our%2520approach%2520includes%2520three%2520novel%250Amodules%253A%2520a%2520Subject%2520Commonality%2520%2528SC%2529%2520module%252C%2520a%2520Prompt-based%2520Subject%2520Specific%250A%2528PSS%2529%2520module%252C%2520and%2520a%2520transformer-based%2520module%2520for%2520fMRI%252C%2520denoted%2520as%2520MRIFormer%250Amodule.%2520The%2520SC%2520module%2520captures%2520shared%2520vision-brain%2520patterns%2520across%2520subjects%252C%250Apreserving%2520this%2520knowledge%2520as%2520the%2520model%2520encounters%2520new%2520subjects%252C%2520thereby%250Areducing%2520the%2520impact%2520of%2520catastrophic%2520forgetting.%2520On%2520the%2520other%2520hand%252C%2520the%2520PSS%250Amodule%2520learns%2520unique%2520vision-brain%2520patterns%2520specific%2520to%2520each%2520subject.%2520Finally%252C%250Athe%2520MRIFormer%2520module%2520contains%2520a%2520transformer%2520encoder%2520and%2520decoder%2520that%2520learns%2520the%250AfMRI%2520features%2520for%2520VBU%2520from%2520common%2520and%2520specific%2520patterns.%2520In%2520a%2520continual%250Alearning%2520setup%252C%2520COBRA%2520is%2520trained%2520in%2520new%2520PSS%2520and%2520MRIFormer%2520modules%2520for%2520new%250Asubjects%252C%2520leaving%2520the%2520modules%2520of%2520previous%2520subjects%2520unaffected.%2520As%2520a%2520result%252C%250ACOBRA%2520effectively%2520addresses%2520catastrophic%2520forgetting%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520continual%2520learning%2520and%2520vision-brain%250Areconstruction%2520tasks%252C%2520surpassing%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17475v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COBRA%3A%20A%20Continual%20Learning%20Approach%20to%20Vision-Brain%20Understanding&entry.906535625=Xuan-Bac%20Nguyen%20and%20Manuel%20Serna-Aguilera%20and%20Arabinda%20Kumar%20Choudhary%20and%20Pawan%20Sinha%20and%20Xin%20Li%20and%20Khoa%20Luu&entry.1292438233=%20%20Vision-Brain%20Understanding%20%28VBU%29%20aims%20to%20extract%20visual%20information%20perceived%0Aby%20humans%20from%20brain%20activity%20recorded%20through%20functional%20Magnetic%20Resonance%0AImaging%20%28fMRI%29.%20Despite%20notable%20advancements%20in%20recent%20years%2C%20existing%20studies%0Ain%20VBU%20continue%20to%20face%20the%20challenge%20of%20catastrophic%20forgetting%2C%20where%20models%0Alose%20knowledge%20from%20prior%20subjects%20as%20they%20adapt%20to%20new%20ones.%20Addressing%0Acontinual%20learning%20in%20this%20field%20is%2C%20therefore%2C%20essential.%20This%20paper%0Aintroduces%20a%20novel%20framework%20called%20Continual%20Learning%20for%20Vision-Brain%20%28COBRA%29%0Ato%20address%20continual%20learning%20in%20VBU.%20Our%20approach%20includes%20three%20novel%0Amodules%3A%20a%20Subject%20Commonality%20%28SC%29%20module%2C%20a%20Prompt-based%20Subject%20Specific%0A%28PSS%29%20module%2C%20and%20a%20transformer-based%20module%20for%20fMRI%2C%20denoted%20as%20MRIFormer%0Amodule.%20The%20SC%20module%20captures%20shared%20vision-brain%20patterns%20across%20subjects%2C%0Apreserving%20this%20knowledge%20as%20the%20model%20encounters%20new%20subjects%2C%20thereby%0Areducing%20the%20impact%20of%20catastrophic%20forgetting.%20On%20the%20other%20hand%2C%20the%20PSS%0Amodule%20learns%20unique%20vision-brain%20patterns%20specific%20to%20each%20subject.%20Finally%2C%0Athe%20MRIFormer%20module%20contains%20a%20transformer%20encoder%20and%20decoder%20that%20learns%20the%0AfMRI%20features%20for%20VBU%20from%20common%20and%20specific%20patterns.%20In%20a%20continual%0Alearning%20setup%2C%20COBRA%20is%20trained%20in%20new%20PSS%20and%20MRIFormer%20modules%20for%20new%0Asubjects%2C%20leaving%20the%20modules%20of%20previous%20subjects%20unaffected.%20As%20a%20result%2C%0ACOBRA%20effectively%20addresses%20catastrophic%20forgetting%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20continual%20learning%20and%20vision-brain%0Areconstruction%20tasks%2C%20surpassing%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17475v3&entry.124074799=Read"},
{"title": "Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs", "author": "Umberto Cappellazzo and Minsu Kim and Stavros Petridis", "abstract": "  Audio-Visual Speech Recognition (AVSR) leverages audio and visual modalities\nto improve robustness in noisy environments. Recent advances in Large Language\nModels (LLMs) show strong performance in speech recognition, including AVSR.\nHowever, the long speech representations lead to high computational costs for\nLLMs. Prior methods compress inputs before feeding them to LLMs, but high\ncompression often harms accuracy. To address this, we propose Llama-MTSK, the\nfirst Matryoshka-based Multimodal LLM for AVSR, which flexibly adapts\naudio-visual token allocation under varying compute constraints. Inspired by\nMatryoshka Representation Learning, our model encodes representations at\nmultiple granularities with a single architecture, avoiding the need for\nseparate models. For efficient fine-tuning, we introduce three LoRA-based\nstrategies using global and scale-specific modules. Evaluations on major AVSR\ndatasets show Llama-MTSK matches or outperforms models trained at fixed\ncompression levels.\n", "link": "http://arxiv.org/abs/2503.06362v2", "date": "2025-08-06", "relevancy": 2.7237, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Audio-Visual%20Speech%20Recognition%20via%20Matryoshka-Based%20Multimodal%0A%20%20LLMs&body=Title%3A%20Adaptive%20Audio-Visual%20Speech%20Recognition%20via%20Matryoshka-Based%20Multimodal%0A%20%20LLMs%0AAuthor%3A%20Umberto%20Cappellazzo%20and%20Minsu%20Kim%20and%20Stavros%20Petridis%0AAbstract%3A%20%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20leverages%20audio%20and%20visual%20modalities%0Ato%20improve%20robustness%20in%20noisy%20environments.%20Recent%20advances%20in%20Large%20Language%0AModels%20%28LLMs%29%20show%20strong%20performance%20in%20speech%20recognition%2C%20including%20AVSR.%0AHowever%2C%20the%20long%20speech%20representations%20lead%20to%20high%20computational%20costs%20for%0ALLMs.%20Prior%20methods%20compress%20inputs%20before%20feeding%20them%20to%20LLMs%2C%20but%20high%0Acompression%20often%20harms%20accuracy.%20To%20address%20this%2C%20we%20propose%20Llama-MTSK%2C%20the%0Afirst%20Matryoshka-based%20Multimodal%20LLM%20for%20AVSR%2C%20which%20flexibly%20adapts%0Aaudio-visual%20token%20allocation%20under%20varying%20compute%20constraints.%20Inspired%20by%0AMatryoshka%20Representation%20Learning%2C%20our%20model%20encodes%20representations%20at%0Amultiple%20granularities%20with%20a%20single%20architecture%2C%20avoiding%20the%20need%20for%0Aseparate%20models.%20For%20efficient%20fine-tuning%2C%20we%20introduce%20three%20LoRA-based%0Astrategies%20using%20global%20and%20scale-specific%20modules.%20Evaluations%20on%20major%20AVSR%0Adatasets%20show%20Llama-MTSK%20matches%20or%20outperforms%20models%20trained%20at%20fixed%0Acompression%20levels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Audio-Visual%2520Speech%2520Recognition%2520via%2520Matryoshka-Based%2520Multimodal%250A%2520%2520LLMs%26entry.906535625%3DUmberto%2520Cappellazzo%2520and%2520Minsu%2520Kim%2520and%2520Stavros%2520Petridis%26entry.1292438233%3D%2520%2520Audio-Visual%2520Speech%2520Recognition%2520%2528AVSR%2529%2520leverages%2520audio%2520and%2520visual%2520modalities%250Ato%2520improve%2520robustness%2520in%2520noisy%2520environments.%2520Recent%2520advances%2520in%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520show%2520strong%2520performance%2520in%2520speech%2520recognition%252C%2520including%2520AVSR.%250AHowever%252C%2520the%2520long%2520speech%2520representations%2520lead%2520to%2520high%2520computational%2520costs%2520for%250ALLMs.%2520Prior%2520methods%2520compress%2520inputs%2520before%2520feeding%2520them%2520to%2520LLMs%252C%2520but%2520high%250Acompression%2520often%2520harms%2520accuracy.%2520To%2520address%2520this%252C%2520we%2520propose%2520Llama-MTSK%252C%2520the%250Afirst%2520Matryoshka-based%2520Multimodal%2520LLM%2520for%2520AVSR%252C%2520which%2520flexibly%2520adapts%250Aaudio-visual%2520token%2520allocation%2520under%2520varying%2520compute%2520constraints.%2520Inspired%2520by%250AMatryoshka%2520Representation%2520Learning%252C%2520our%2520model%2520encodes%2520representations%2520at%250Amultiple%2520granularities%2520with%2520a%2520single%2520architecture%252C%2520avoiding%2520the%2520need%2520for%250Aseparate%2520models.%2520For%2520efficient%2520fine-tuning%252C%2520we%2520introduce%2520three%2520LoRA-based%250Astrategies%2520using%2520global%2520and%2520scale-specific%2520modules.%2520Evaluations%2520on%2520major%2520AVSR%250Adatasets%2520show%2520Llama-MTSK%2520matches%2520or%2520outperforms%2520models%2520trained%2520at%2520fixed%250Acompression%2520levels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Audio-Visual%20Speech%20Recognition%20via%20Matryoshka-Based%20Multimodal%0A%20%20LLMs&entry.906535625=Umberto%20Cappellazzo%20and%20Minsu%20Kim%20and%20Stavros%20Petridis&entry.1292438233=%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20leverages%20audio%20and%20visual%20modalities%0Ato%20improve%20robustness%20in%20noisy%20environments.%20Recent%20advances%20in%20Large%20Language%0AModels%20%28LLMs%29%20show%20strong%20performance%20in%20speech%20recognition%2C%20including%20AVSR.%0AHowever%2C%20the%20long%20speech%20representations%20lead%20to%20high%20computational%20costs%20for%0ALLMs.%20Prior%20methods%20compress%20inputs%20before%20feeding%20them%20to%20LLMs%2C%20but%20high%0Acompression%20often%20harms%20accuracy.%20To%20address%20this%2C%20we%20propose%20Llama-MTSK%2C%20the%0Afirst%20Matryoshka-based%20Multimodal%20LLM%20for%20AVSR%2C%20which%20flexibly%20adapts%0Aaudio-visual%20token%20allocation%20under%20varying%20compute%20constraints.%20Inspired%20by%0AMatryoshka%20Representation%20Learning%2C%20our%20model%20encodes%20representations%20at%0Amultiple%20granularities%20with%20a%20single%20architecture%2C%20avoiding%20the%20need%20for%0Aseparate%20models.%20For%20efficient%20fine-tuning%2C%20we%20introduce%20three%20LoRA-based%0Astrategies%20using%20global%20and%20scale-specific%20modules.%20Evaluations%20on%20major%20AVSR%0Adatasets%20show%20Llama-MTSK%20matches%20or%20outperforms%20models%20trained%20at%20fixed%0Acompression%20levels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06362v2&entry.124074799=Read"},
{"title": "Sign Spotting Disambiguation using Large Language Models", "author": "JianHe Low and Ozge Mercanoglu Sincan and Richard Bowden", "abstract": "  Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.\n", "link": "http://arxiv.org/abs/2507.03703v3", "date": "2025-08-06", "relevancy": 2.7021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5415}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign%20Spotting%20Disambiguation%20using%20Large%20Language%20Models&body=Title%3A%20Sign%20Spotting%20Disambiguation%20using%20Large%20Language%20Models%0AAuthor%3A%20JianHe%20Low%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20spotting%2C%20the%20task%20of%20identifying%20and%20localizing%20individual%20signs%20within%0Acontinuous%20sign%20language%20video%2C%20plays%20a%20pivotal%20role%20in%20scaling%20dataset%0Aannotations%20and%20addressing%20the%20severe%20data%20scarcity%20issue%20in%20sign%20language%0Atranslation.%20While%20automatic%20sign%20spotting%20holds%20great%20promise%20for%20enabling%0Aframe-level%20supervision%20at%20scale%2C%20it%20grapples%20with%20challenges%20such%20as%0Avocabulary%20inflexibility%20and%20ambiguity%20inherent%20in%20continuous%20sign%20streams.%0AHence%2C%20we%20introduce%20a%20novel%2C%20training-free%20framework%20that%20integrates%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20significantly%20enhance%20sign%20spotting%20quality.%20Our%0Aapproach%20extracts%20global%20spatio-temporal%20and%20hand%20shape%20features%2C%20which%20are%0Athen%20matched%20against%20a%20large-scale%20sign%20dictionary%20using%20dynamic%20time%20warping%0Aand%20cosine%20similarity.%20This%20dictionary-based%20matching%20inherently%20offers%0Asuperior%20vocabulary%20flexibility%20without%20requiring%20model%20retraining.%20To%20mitigate%0Anoise%20and%20ambiguity%20from%20the%20matching%20process%2C%20an%20LLM%20performs%20context-aware%0Agloss%20disambiguation%20via%20beam%20search%2C%20notably%20without%20fine-tuning.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20sign%20language%20datasets%20demonstrate%0Aour%20method%27s%20superior%20accuracy%20and%20sentence%20fluency%20compared%20to%20traditional%0Aapproaches%2C%20highlighting%20the%20potential%20of%20LLMs%20in%20advancing%20sign%20spotting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03703v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign%2520Spotting%2520Disambiguation%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DJianHe%2520Low%2520and%2520Ozge%2520Mercanoglu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520spotting%252C%2520the%2520task%2520of%2520identifying%2520and%2520localizing%2520individual%2520signs%2520within%250Acontinuous%2520sign%2520language%2520video%252C%2520plays%2520a%2520pivotal%2520role%2520in%2520scaling%2520dataset%250Aannotations%2520and%2520addressing%2520the%2520severe%2520data%2520scarcity%2520issue%2520in%2520sign%2520language%250Atranslation.%2520While%2520automatic%2520sign%2520spotting%2520holds%2520great%2520promise%2520for%2520enabling%250Aframe-level%2520supervision%2520at%2520scale%252C%2520it%2520grapples%2520with%2520challenges%2520such%2520as%250Avocabulary%2520inflexibility%2520and%2520ambiguity%2520inherent%2520in%2520continuous%2520sign%2520streams.%250AHence%252C%2520we%2520introduce%2520a%2520novel%252C%2520training-free%2520framework%2520that%2520integrates%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520to%2520significantly%2520enhance%2520sign%2520spotting%2520quality.%2520Our%250Aapproach%2520extracts%2520global%2520spatio-temporal%2520and%2520hand%2520shape%2520features%252C%2520which%2520are%250Athen%2520matched%2520against%2520a%2520large-scale%2520sign%2520dictionary%2520using%2520dynamic%2520time%2520warping%250Aand%2520cosine%2520similarity.%2520This%2520dictionary-based%2520matching%2520inherently%2520offers%250Asuperior%2520vocabulary%2520flexibility%2520without%2520requiring%2520model%2520retraining.%2520To%2520mitigate%250Anoise%2520and%2520ambiguity%2520from%2520the%2520matching%2520process%252C%2520an%2520LLM%2520performs%2520context-aware%250Agloss%2520disambiguation%2520via%2520beam%2520search%252C%2520notably%2520without%2520fine-tuning.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520sign%2520language%2520datasets%2520demonstrate%250Aour%2520method%2527s%2520superior%2520accuracy%2520and%2520sentence%2520fluency%2520compared%2520to%2520traditional%250Aapproaches%252C%2520highlighting%2520the%2520potential%2520of%2520LLMs%2520in%2520advancing%2520sign%2520spotting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03703v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign%20Spotting%20Disambiguation%20using%20Large%20Language%20Models&entry.906535625=JianHe%20Low%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20spotting%2C%20the%20task%20of%20identifying%20and%20localizing%20individual%20signs%20within%0Acontinuous%20sign%20language%20video%2C%20plays%20a%20pivotal%20role%20in%20scaling%20dataset%0Aannotations%20and%20addressing%20the%20severe%20data%20scarcity%20issue%20in%20sign%20language%0Atranslation.%20While%20automatic%20sign%20spotting%20holds%20great%20promise%20for%20enabling%0Aframe-level%20supervision%20at%20scale%2C%20it%20grapples%20with%20challenges%20such%20as%0Avocabulary%20inflexibility%20and%20ambiguity%20inherent%20in%20continuous%20sign%20streams.%0AHence%2C%20we%20introduce%20a%20novel%2C%20training-free%20framework%20that%20integrates%20Large%0ALanguage%20Models%20%28LLMs%29%20to%20significantly%20enhance%20sign%20spotting%20quality.%20Our%0Aapproach%20extracts%20global%20spatio-temporal%20and%20hand%20shape%20features%2C%20which%20are%0Athen%20matched%20against%20a%20large-scale%20sign%20dictionary%20using%20dynamic%20time%20warping%0Aand%20cosine%20similarity.%20This%20dictionary-based%20matching%20inherently%20offers%0Asuperior%20vocabulary%20flexibility%20without%20requiring%20model%20retraining.%20To%20mitigate%0Anoise%20and%20ambiguity%20from%20the%20matching%20process%2C%20an%20LLM%20performs%20context-aware%0Agloss%20disambiguation%20via%20beam%20search%2C%20notably%20without%20fine-tuning.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20sign%20language%20datasets%20demonstrate%0Aour%20method%27s%20superior%20accuracy%20and%20sentence%20fluency%20compared%20to%20traditional%0Aapproaches%2C%20highlighting%20the%20potential%20of%20LLMs%20in%20advancing%20sign%20spotting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03703v3&entry.124074799=Read"},
{"title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via\n  Generalizable Gaussian Splatting from Unposed Multi-View Images", "author": "Xiangyu Sun and Haoyi jiang and Liu Liu and Seungtae Nam and Gyeongjin Kang and Xinjie wang and Wei Sui and Zhizhong Su and Wenyu Liu and Xinggang Wang and Eunbyung Park", "abstract": "  Reconstructing and semantically interpreting 3D scenes from sparse 2D views\nremains a fundamental challenge in computer vision. Conventional methods often\ndecouple semantic understanding from reconstruction or necessitate costly\nper-scene optimization, thereby restricting their scalability and\ngeneralizability. In this paper, we introduce Uni3R, a novel feed-forward\nframework that jointly reconstructs a unified 3D scene representation enriched\nwith open-vocabulary semantics, directly from unposed multi-view images. Our\napproach leverages a Cross-View Transformer to robustly integrate information\nacross arbitrary multi-view inputs, which then regresses a set of 3D Gaussian\nprimitives endowed with semantic feature fields. This unified representation\nfacilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic\nsegmentation, and depth prediction, all within a single, feed-forward pass.\nExtensive experiments demonstrate that Uni3R establishes a new state-of-the-art\nacross multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on\nScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D\nscene reconstruction and understanding. The code is available at\nhttps://github.com/HorizonRobotics/Uni3R.\n", "link": "http://arxiv.org/abs/2508.03643v2", "date": "2025-08-06", "relevancy": 2.702, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6856}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6742}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uni3R%3A%20Unified%203D%20Reconstruction%20and%20Semantic%20Understanding%20via%0A%20%20Generalizable%20Gaussian%20Splatting%20from%20Unposed%20Multi-View%20Images&body=Title%3A%20Uni3R%3A%20Unified%203D%20Reconstruction%20and%20Semantic%20Understanding%20via%0A%20%20Generalizable%20Gaussian%20Splatting%20from%20Unposed%20Multi-View%20Images%0AAuthor%3A%20Xiangyu%20Sun%20and%20Haoyi%20jiang%20and%20Liu%20Liu%20and%20Seungtae%20Nam%20and%20Gyeongjin%20Kang%20and%20Xinjie%20wang%20and%20Wei%20Sui%20and%20Zhizhong%20Su%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Eunbyung%20Park%0AAbstract%3A%20%20%20Reconstructing%20and%20semantically%20interpreting%203D%20scenes%20from%20sparse%202D%20views%0Aremains%20a%20fundamental%20challenge%20in%20computer%20vision.%20Conventional%20methods%20often%0Adecouple%20semantic%20understanding%20from%20reconstruction%20or%20necessitate%20costly%0Aper-scene%20optimization%2C%20thereby%20restricting%20their%20scalability%20and%0Ageneralizability.%20In%20this%20paper%2C%20we%20introduce%20Uni3R%2C%20a%20novel%20feed-forward%0Aframework%20that%20jointly%20reconstructs%20a%20unified%203D%20scene%20representation%20enriched%0Awith%20open-vocabulary%20semantics%2C%20directly%20from%20unposed%20multi-view%20images.%20Our%0Aapproach%20leverages%20a%20Cross-View%20Transformer%20to%20robustly%20integrate%20information%0Aacross%20arbitrary%20multi-view%20inputs%2C%20which%20then%20regresses%20a%20set%20of%203D%20Gaussian%0Aprimitives%20endowed%20with%20semantic%20feature%20fields.%20This%20unified%20representation%0Afacilitates%20high-fidelity%20novel%20view%20synthesis%2C%20open-vocabulary%203D%20semantic%0Asegmentation%2C%20and%20depth%20prediction%2C%20all%20within%20a%20single%2C%20feed-forward%20pass.%0AExtensive%20experiments%20demonstrate%20that%20Uni3R%20establishes%20a%20new%20state-of-the-art%0Aacross%20multiple%20benchmarks%2C%20including%2025.07%20PSNR%20on%20RE10K%20and%2055.84%20mIoU%20on%0AScanNet.%20Our%20work%20signifies%20a%20novel%20paradigm%20towards%20generalizable%2C%20unified%203D%0Ascene%20reconstruction%20and%20understanding.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HorizonRobotics/Uni3R.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUni3R%253A%2520Unified%25203D%2520Reconstruction%2520and%2520Semantic%2520Understanding%2520via%250A%2520%2520Generalizable%2520Gaussian%2520Splatting%2520from%2520Unposed%2520Multi-View%2520Images%26entry.906535625%3DXiangyu%2520Sun%2520and%2520Haoyi%2520jiang%2520and%2520Liu%2520Liu%2520and%2520Seungtae%2520Nam%2520and%2520Gyeongjin%2520Kang%2520and%2520Xinjie%2520wang%2520and%2520Wei%2520Sui%2520and%2520Zhizhong%2520Su%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%2520and%2520Eunbyung%2520Park%26entry.1292438233%3D%2520%2520Reconstructing%2520and%2520semantically%2520interpreting%25203D%2520scenes%2520from%2520sparse%25202D%2520views%250Aremains%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision.%2520Conventional%2520methods%2520often%250Adecouple%2520semantic%2520understanding%2520from%2520reconstruction%2520or%2520necessitate%2520costly%250Aper-scene%2520optimization%252C%2520thereby%2520restricting%2520their%2520scalability%2520and%250Ageneralizability.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Uni3R%252C%2520a%2520novel%2520feed-forward%250Aframework%2520that%2520jointly%2520reconstructs%2520a%2520unified%25203D%2520scene%2520representation%2520enriched%250Awith%2520open-vocabulary%2520semantics%252C%2520directly%2520from%2520unposed%2520multi-view%2520images.%2520Our%250Aapproach%2520leverages%2520a%2520Cross-View%2520Transformer%2520to%2520robustly%2520integrate%2520information%250Aacross%2520arbitrary%2520multi-view%2520inputs%252C%2520which%2520then%2520regresses%2520a%2520set%2520of%25203D%2520Gaussian%250Aprimitives%2520endowed%2520with%2520semantic%2520feature%2520fields.%2520This%2520unified%2520representation%250Afacilitates%2520high-fidelity%2520novel%2520view%2520synthesis%252C%2520open-vocabulary%25203D%2520semantic%250Asegmentation%252C%2520and%2520depth%2520prediction%252C%2520all%2520within%2520a%2520single%252C%2520feed-forward%2520pass.%250AExtensive%2520experiments%2520demonstrate%2520that%2520Uni3R%2520establishes%2520a%2520new%2520state-of-the-art%250Aacross%2520multiple%2520benchmarks%252C%2520including%252025.07%2520PSNR%2520on%2520RE10K%2520and%252055.84%2520mIoU%2520on%250AScanNet.%2520Our%2520work%2520signifies%2520a%2520novel%2520paradigm%2520towards%2520generalizable%252C%2520unified%25203D%250Ascene%2520reconstruction%2520and%2520understanding.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/HorizonRobotics/Uni3R.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uni3R%3A%20Unified%203D%20Reconstruction%20and%20Semantic%20Understanding%20via%0A%20%20Generalizable%20Gaussian%20Splatting%20from%20Unposed%20Multi-View%20Images&entry.906535625=Xiangyu%20Sun%20and%20Haoyi%20jiang%20and%20Liu%20Liu%20and%20Seungtae%20Nam%20and%20Gyeongjin%20Kang%20and%20Xinjie%20wang%20and%20Wei%20Sui%20and%20Zhizhong%20Su%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%20and%20Eunbyung%20Park&entry.1292438233=%20%20Reconstructing%20and%20semantically%20interpreting%203D%20scenes%20from%20sparse%202D%20views%0Aremains%20a%20fundamental%20challenge%20in%20computer%20vision.%20Conventional%20methods%20often%0Adecouple%20semantic%20understanding%20from%20reconstruction%20or%20necessitate%20costly%0Aper-scene%20optimization%2C%20thereby%20restricting%20their%20scalability%20and%0Ageneralizability.%20In%20this%20paper%2C%20we%20introduce%20Uni3R%2C%20a%20novel%20feed-forward%0Aframework%20that%20jointly%20reconstructs%20a%20unified%203D%20scene%20representation%20enriched%0Awith%20open-vocabulary%20semantics%2C%20directly%20from%20unposed%20multi-view%20images.%20Our%0Aapproach%20leverages%20a%20Cross-View%20Transformer%20to%20robustly%20integrate%20information%0Aacross%20arbitrary%20multi-view%20inputs%2C%20which%20then%20regresses%20a%20set%20of%203D%20Gaussian%0Aprimitives%20endowed%20with%20semantic%20feature%20fields.%20This%20unified%20representation%0Afacilitates%20high-fidelity%20novel%20view%20synthesis%2C%20open-vocabulary%203D%20semantic%0Asegmentation%2C%20and%20depth%20prediction%2C%20all%20within%20a%20single%2C%20feed-forward%20pass.%0AExtensive%20experiments%20demonstrate%20that%20Uni3R%20establishes%20a%20new%20state-of-the-art%0Aacross%20multiple%20benchmarks%2C%20including%2025.07%20PSNR%20on%20RE10K%20and%2055.84%20mIoU%20on%0AScanNet.%20Our%20work%20signifies%20a%20novel%20paradigm%20towards%20generalizable%2C%20unified%203D%0Ascene%20reconstruction%20and%20understanding.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/HorizonRobotics/Uni3R.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03643v2&entry.124074799=Read"},
{"title": "Basis Selection: Low-Rank Decomposition of Pretrained Large Language\n  Models for Target Applications", "author": "Yang Li and Daniel Agyei Asante and Changsheng Zhao and Ernie Chang and Yangyang Shi and Vikas Chandra", "abstract": "  Large language models (LLMs) significantly enhance the performance of various\napplications, but they are computationally intensive and energy-demanding. This\nmakes it challenging to deploy them on devices with limited resources, such as\npersonal computers and mobile/wearable devices, and results in substantial\ninference costs in resource-rich environments like cloud servers. To extend the\nuse of LLMs, we introduce a low-rank decomposition approach to effectively\ncompress these models, tailored to the requirements of specific applications.\nWe observe that LLMs pretrained on general datasets contain many redundant\ncomponents not needed for particular applications. Our method focuses on\nidentifying and removing these redundant parts, retaining only the necessary\nelements for the target applications. Specifically, we represent the weight\nmatrices of LLMs as a linear combination of base components. We then prune the\nirrelevant bases and enhance the model with new bases beneficial for specific\napplications. Deep compression results on the Llama 2-7b and -13B models,\nconducted on target applications including mathematical reasoning and code\ngeneration, show that our method significantly reduces model size while\nmaintaining comparable accuracy to state-of-the-art low-rank compression\ntechniques.\n", "link": "http://arxiv.org/abs/2405.15877v2", "date": "2025-08-06", "relevancy": 2.6849, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Basis%20Selection%3A%20Low-Rank%20Decomposition%20of%20Pretrained%20Large%20Language%0A%20%20Models%20for%20Target%20Applications&body=Title%3A%20Basis%20Selection%3A%20Low-Rank%20Decomposition%20of%20Pretrained%20Large%20Language%0A%20%20Models%20for%20Target%20Applications%0AAuthor%3A%20Yang%20Li%20and%20Daniel%20Agyei%20Asante%20and%20Changsheng%20Zhao%20and%20Ernie%20Chang%20and%20Yangyang%20Shi%20and%20Vikas%20Chandra%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20significantly%20enhance%20the%20performance%20of%20various%0Aapplications%2C%20but%20they%20are%20computationally%20intensive%20and%20energy-demanding.%20This%0Amakes%20it%20challenging%20to%20deploy%20them%20on%20devices%20with%20limited%20resources%2C%20such%20as%0Apersonal%20computers%20and%20mobile/wearable%20devices%2C%20and%20results%20in%20substantial%0Ainference%20costs%20in%20resource-rich%20environments%20like%20cloud%20servers.%20To%20extend%20the%0Ause%20of%20LLMs%2C%20we%20introduce%20a%20low-rank%20decomposition%20approach%20to%20effectively%0Acompress%20these%20models%2C%20tailored%20to%20the%20requirements%20of%20specific%20applications.%0AWe%20observe%20that%20LLMs%20pretrained%20on%20general%20datasets%20contain%20many%20redundant%0Acomponents%20not%20needed%20for%20particular%20applications.%20Our%20method%20focuses%20on%0Aidentifying%20and%20removing%20these%20redundant%20parts%2C%20retaining%20only%20the%20necessary%0Aelements%20for%20the%20target%20applications.%20Specifically%2C%20we%20represent%20the%20weight%0Amatrices%20of%20LLMs%20as%20a%20linear%20combination%20of%20base%20components.%20We%20then%20prune%20the%0Airrelevant%20bases%20and%20enhance%20the%20model%20with%20new%20bases%20beneficial%20for%20specific%0Aapplications.%20Deep%20compression%20results%20on%20the%20Llama%202-7b%20and%20-13B%20models%2C%0Aconducted%20on%20target%20applications%20including%20mathematical%20reasoning%20and%20code%0Ageneration%2C%20show%20that%20our%20method%20significantly%20reduces%20model%20size%20while%0Amaintaining%20comparable%20accuracy%20to%20state-of-the-art%20low-rank%20compression%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBasis%2520Selection%253A%2520Low-Rank%2520Decomposition%2520of%2520Pretrained%2520Large%2520Language%250A%2520%2520Models%2520for%2520Target%2520Applications%26entry.906535625%3DYang%2520Li%2520and%2520Daniel%2520Agyei%2520Asante%2520and%2520Changsheng%2520Zhao%2520and%2520Ernie%2520Chang%2520and%2520Yangyang%2520Shi%2520and%2520Vikas%2520Chandra%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520significantly%2520enhance%2520the%2520performance%2520of%2520various%250Aapplications%252C%2520but%2520they%2520are%2520computationally%2520intensive%2520and%2520energy-demanding.%2520This%250Amakes%2520it%2520challenging%2520to%2520deploy%2520them%2520on%2520devices%2520with%2520limited%2520resources%252C%2520such%2520as%250Apersonal%2520computers%2520and%2520mobile/wearable%2520devices%252C%2520and%2520results%2520in%2520substantial%250Ainference%2520costs%2520in%2520resource-rich%2520environments%2520like%2520cloud%2520servers.%2520To%2520extend%2520the%250Ause%2520of%2520LLMs%252C%2520we%2520introduce%2520a%2520low-rank%2520decomposition%2520approach%2520to%2520effectively%250Acompress%2520these%2520models%252C%2520tailored%2520to%2520the%2520requirements%2520of%2520specific%2520applications.%250AWe%2520observe%2520that%2520LLMs%2520pretrained%2520on%2520general%2520datasets%2520contain%2520many%2520redundant%250Acomponents%2520not%2520needed%2520for%2520particular%2520applications.%2520Our%2520method%2520focuses%2520on%250Aidentifying%2520and%2520removing%2520these%2520redundant%2520parts%252C%2520retaining%2520only%2520the%2520necessary%250Aelements%2520for%2520the%2520target%2520applications.%2520Specifically%252C%2520we%2520represent%2520the%2520weight%250Amatrices%2520of%2520LLMs%2520as%2520a%2520linear%2520combination%2520of%2520base%2520components.%2520We%2520then%2520prune%2520the%250Airrelevant%2520bases%2520and%2520enhance%2520the%2520model%2520with%2520new%2520bases%2520beneficial%2520for%2520specific%250Aapplications.%2520Deep%2520compression%2520results%2520on%2520the%2520Llama%25202-7b%2520and%2520-13B%2520models%252C%250Aconducted%2520on%2520target%2520applications%2520including%2520mathematical%2520reasoning%2520and%2520code%250Ageneration%252C%2520show%2520that%2520our%2520method%2520significantly%2520reduces%2520model%2520size%2520while%250Amaintaining%2520comparable%2520accuracy%2520to%2520state-of-the-art%2520low-rank%2520compression%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Basis%20Selection%3A%20Low-Rank%20Decomposition%20of%20Pretrained%20Large%20Language%0A%20%20Models%20for%20Target%20Applications&entry.906535625=Yang%20Li%20and%20Daniel%20Agyei%20Asante%20and%20Changsheng%20Zhao%20and%20Ernie%20Chang%20and%20Yangyang%20Shi%20and%20Vikas%20Chandra&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20significantly%20enhance%20the%20performance%20of%20various%0Aapplications%2C%20but%20they%20are%20computationally%20intensive%20and%20energy-demanding.%20This%0Amakes%20it%20challenging%20to%20deploy%20them%20on%20devices%20with%20limited%20resources%2C%20such%20as%0Apersonal%20computers%20and%20mobile/wearable%20devices%2C%20and%20results%20in%20substantial%0Ainference%20costs%20in%20resource-rich%20environments%20like%20cloud%20servers.%20To%20extend%20the%0Ause%20of%20LLMs%2C%20we%20introduce%20a%20low-rank%20decomposition%20approach%20to%20effectively%0Acompress%20these%20models%2C%20tailored%20to%20the%20requirements%20of%20specific%20applications.%0AWe%20observe%20that%20LLMs%20pretrained%20on%20general%20datasets%20contain%20many%20redundant%0Acomponents%20not%20needed%20for%20particular%20applications.%20Our%20method%20focuses%20on%0Aidentifying%20and%20removing%20these%20redundant%20parts%2C%20retaining%20only%20the%20necessary%0Aelements%20for%20the%20target%20applications.%20Specifically%2C%20we%20represent%20the%20weight%0Amatrices%20of%20LLMs%20as%20a%20linear%20combination%20of%20base%20components.%20We%20then%20prune%20the%0Airrelevant%20bases%20and%20enhance%20the%20model%20with%20new%20bases%20beneficial%20for%20specific%0Aapplications.%20Deep%20compression%20results%20on%20the%20Llama%202-7b%20and%20-13B%20models%2C%0Aconducted%20on%20target%20applications%20including%20mathematical%20reasoning%20and%20code%0Ageneration%2C%20show%20that%20our%20method%20significantly%20reduces%20model%20size%20while%0Amaintaining%20comparable%20accuracy%20to%20state-of-the-art%20low-rank%20compression%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15877v2&entry.124074799=Read"},
{"title": "HiD-VAE: Interpretable Generative Recommendation via Hierarchical and\n  Disentangled Semantic IDs", "author": "Dengzhao Fang and Jingtong Gao and Chengcheng Zhu and Yu Li and Xiangyu Zhao and Yi Chang", "abstract": "  Recommender systems are indispensable for helping users navigate the immense\nitem catalogs of modern online platforms. Recently, generative recommendation\nhas emerged as a promising paradigm, unifying the conventional\nretrieve-and-rank pipeline into an end-to-end model capable of dynamic\ngeneration. However, existing generative methods are fundamentally constrained\nby their unsupervised tokenization, which generates semantic IDs suffering from\ntwo critical flaws: (1) they are semantically flat and uninterpretable, lacking\na coherent hierarchy, and (2) they are prone to representation entanglement\n(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.\nTo overcome these limitations, we propose HiD-VAE, a novel framework that\nlearns hierarchically disentangled item representations through two core\ninnovations. First, HiD-VAE pioneers a hierarchically-supervised quantization\nprocess that aligns discrete codes with multi-level item tags, yielding more\nuniform and disentangled IDs. Crucially, the trained codebooks can predict\nhierarchical tags, providing a traceable and interpretable semantic path for\neach recommendation. Second, to combat representation entanglement, HiD-VAE\nincorporates a novel uniqueness loss that directly penalizes latent space\noverlap. This mechanism not only resolves the critical ID collision problem but\nalso promotes recommendation diversity by ensuring a more comprehensive\nutilization of the item representation space. These high-quality, disentangled\nIDs provide a powerful foundation for downstream generative models. Extensive\nexperiments on three public benchmarks validate HiD-VAE's superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://anonymous.4open.science/r/HiD-VAE-84B2.\n", "link": "http://arxiv.org/abs/2508.04618v1", "date": "2025-08-06", "relevancy": 2.6789, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5771}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5184}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiD-VAE%3A%20Interpretable%20Generative%20Recommendation%20via%20Hierarchical%20and%0A%20%20Disentangled%20Semantic%20IDs&body=Title%3A%20HiD-VAE%3A%20Interpretable%20Generative%20Recommendation%20via%20Hierarchical%20and%0A%20%20Disentangled%20Semantic%20IDs%0AAuthor%3A%20Dengzhao%20Fang%20and%20Jingtong%20Gao%20and%20Chengcheng%20Zhu%20and%20Yu%20Li%20and%20Xiangyu%20Zhao%20and%20Yi%20Chang%0AAbstract%3A%20%20%20Recommender%20systems%20are%20indispensable%20for%20helping%20users%20navigate%20the%20immense%0Aitem%20catalogs%20of%20modern%20online%20platforms.%20Recently%2C%20generative%20recommendation%0Ahas%20emerged%20as%20a%20promising%20paradigm%2C%20unifying%20the%20conventional%0Aretrieve-and-rank%20pipeline%20into%20an%20end-to-end%20model%20capable%20of%20dynamic%0Ageneration.%20However%2C%20existing%20generative%20methods%20are%20fundamentally%20constrained%0Aby%20their%20unsupervised%20tokenization%2C%20which%20generates%20semantic%20IDs%20suffering%20from%0Atwo%20critical%20flaws%3A%20%281%29%20they%20are%20semantically%20flat%20and%20uninterpretable%2C%20lacking%0Aa%20coherent%20hierarchy%2C%20and%20%282%29%20they%20are%20prone%20to%20representation%20entanglement%0A%28i.e.%2C%20%60%60ID%20collisions%27%27%29%2C%20which%20harms%20recommendation%20accuracy%20and%20diversity.%0ATo%20overcome%20these%20limitations%2C%20we%20propose%20HiD-VAE%2C%20a%20novel%20framework%20that%0Alearns%20hierarchically%20disentangled%20item%20representations%20through%20two%20core%0Ainnovations.%20First%2C%20HiD-VAE%20pioneers%20a%20hierarchically-supervised%20quantization%0Aprocess%20that%20aligns%20discrete%20codes%20with%20multi-level%20item%20tags%2C%20yielding%20more%0Auniform%20and%20disentangled%20IDs.%20Crucially%2C%20the%20trained%20codebooks%20can%20predict%0Ahierarchical%20tags%2C%20providing%20a%20traceable%20and%20interpretable%20semantic%20path%20for%0Aeach%20recommendation.%20Second%2C%20to%20combat%20representation%20entanglement%2C%20HiD-VAE%0Aincorporates%20a%20novel%20uniqueness%20loss%20that%20directly%20penalizes%20latent%20space%0Aoverlap.%20This%20mechanism%20not%20only%20resolves%20the%20critical%20ID%20collision%20problem%20but%0Aalso%20promotes%20recommendation%20diversity%20by%20ensuring%20a%20more%20comprehensive%0Autilization%20of%20the%20item%20representation%20space.%20These%20high-quality%2C%20disentangled%0AIDs%20provide%20a%20powerful%20foundation%20for%20downstream%20generative%20models.%20Extensive%0Aexperiments%20on%20three%20public%20benchmarks%20validate%20HiD-VAE%27s%20superior%20performance%0Aagainst%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/HiD-VAE-84B2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiD-VAE%253A%2520Interpretable%2520Generative%2520Recommendation%2520via%2520Hierarchical%2520and%250A%2520%2520Disentangled%2520Semantic%2520IDs%26entry.906535625%3DDengzhao%2520Fang%2520and%2520Jingtong%2520Gao%2520and%2520Chengcheng%2520Zhu%2520and%2520Yu%2520Li%2520and%2520Xiangyu%2520Zhao%2520and%2520Yi%2520Chang%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520indispensable%2520for%2520helping%2520users%2520navigate%2520the%2520immense%250Aitem%2520catalogs%2520of%2520modern%2520online%2520platforms.%2520Recently%252C%2520generative%2520recommendation%250Ahas%2520emerged%2520as%2520a%2520promising%2520paradigm%252C%2520unifying%2520the%2520conventional%250Aretrieve-and-rank%2520pipeline%2520into%2520an%2520end-to-end%2520model%2520capable%2520of%2520dynamic%250Ageneration.%2520However%252C%2520existing%2520generative%2520methods%2520are%2520fundamentally%2520constrained%250Aby%2520their%2520unsupervised%2520tokenization%252C%2520which%2520generates%2520semantic%2520IDs%2520suffering%2520from%250Atwo%2520critical%2520flaws%253A%2520%25281%2529%2520they%2520are%2520semantically%2520flat%2520and%2520uninterpretable%252C%2520lacking%250Aa%2520coherent%2520hierarchy%252C%2520and%2520%25282%2529%2520they%2520are%2520prone%2520to%2520representation%2520entanglement%250A%2528i.e.%252C%2520%2560%2560ID%2520collisions%2527%2527%2529%252C%2520which%2520harms%2520recommendation%2520accuracy%2520and%2520diversity.%250ATo%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520HiD-VAE%252C%2520a%2520novel%2520framework%2520that%250Alearns%2520hierarchically%2520disentangled%2520item%2520representations%2520through%2520two%2520core%250Ainnovations.%2520First%252C%2520HiD-VAE%2520pioneers%2520a%2520hierarchically-supervised%2520quantization%250Aprocess%2520that%2520aligns%2520discrete%2520codes%2520with%2520multi-level%2520item%2520tags%252C%2520yielding%2520more%250Auniform%2520and%2520disentangled%2520IDs.%2520Crucially%252C%2520the%2520trained%2520codebooks%2520can%2520predict%250Ahierarchical%2520tags%252C%2520providing%2520a%2520traceable%2520and%2520interpretable%2520semantic%2520path%2520for%250Aeach%2520recommendation.%2520Second%252C%2520to%2520combat%2520representation%2520entanglement%252C%2520HiD-VAE%250Aincorporates%2520a%2520novel%2520uniqueness%2520loss%2520that%2520directly%2520penalizes%2520latent%2520space%250Aoverlap.%2520This%2520mechanism%2520not%2520only%2520resolves%2520the%2520critical%2520ID%2520collision%2520problem%2520but%250Aalso%2520promotes%2520recommendation%2520diversity%2520by%2520ensuring%2520a%2520more%2520comprehensive%250Autilization%2520of%2520the%2520item%2520representation%2520space.%2520These%2520high-quality%252C%2520disentangled%250AIDs%2520provide%2520a%2520powerful%2520foundation%2520for%2520downstream%2520generative%2520models.%2520Extensive%250Aexperiments%2520on%2520three%2520public%2520benchmarks%2520validate%2520HiD-VAE%2527s%2520superior%2520performance%250Aagainst%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/HiD-VAE-84B2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiD-VAE%3A%20Interpretable%20Generative%20Recommendation%20via%20Hierarchical%20and%0A%20%20Disentangled%20Semantic%20IDs&entry.906535625=Dengzhao%20Fang%20and%20Jingtong%20Gao%20and%20Chengcheng%20Zhu%20and%20Yu%20Li%20and%20Xiangyu%20Zhao%20and%20Yi%20Chang&entry.1292438233=%20%20Recommender%20systems%20are%20indispensable%20for%20helping%20users%20navigate%20the%20immense%0Aitem%20catalogs%20of%20modern%20online%20platforms.%20Recently%2C%20generative%20recommendation%0Ahas%20emerged%20as%20a%20promising%20paradigm%2C%20unifying%20the%20conventional%0Aretrieve-and-rank%20pipeline%20into%20an%20end-to-end%20model%20capable%20of%20dynamic%0Ageneration.%20However%2C%20existing%20generative%20methods%20are%20fundamentally%20constrained%0Aby%20their%20unsupervised%20tokenization%2C%20which%20generates%20semantic%20IDs%20suffering%20from%0Atwo%20critical%20flaws%3A%20%281%29%20they%20are%20semantically%20flat%20and%20uninterpretable%2C%20lacking%0Aa%20coherent%20hierarchy%2C%20and%20%282%29%20they%20are%20prone%20to%20representation%20entanglement%0A%28i.e.%2C%20%60%60ID%20collisions%27%27%29%2C%20which%20harms%20recommendation%20accuracy%20and%20diversity.%0ATo%20overcome%20these%20limitations%2C%20we%20propose%20HiD-VAE%2C%20a%20novel%20framework%20that%0Alearns%20hierarchically%20disentangled%20item%20representations%20through%20two%20core%0Ainnovations.%20First%2C%20HiD-VAE%20pioneers%20a%20hierarchically-supervised%20quantization%0Aprocess%20that%20aligns%20discrete%20codes%20with%20multi-level%20item%20tags%2C%20yielding%20more%0Auniform%20and%20disentangled%20IDs.%20Crucially%2C%20the%20trained%20codebooks%20can%20predict%0Ahierarchical%20tags%2C%20providing%20a%20traceable%20and%20interpretable%20semantic%20path%20for%0Aeach%20recommendation.%20Second%2C%20to%20combat%20representation%20entanglement%2C%20HiD-VAE%0Aincorporates%20a%20novel%20uniqueness%20loss%20that%20directly%20penalizes%20latent%20space%0Aoverlap.%20This%20mechanism%20not%20only%20resolves%20the%20critical%20ID%20collision%20problem%20but%0Aalso%20promotes%20recommendation%20diversity%20by%20ensuring%20a%20more%20comprehensive%0Autilization%20of%20the%20item%20representation%20space.%20These%20high-quality%2C%20disentangled%0AIDs%20provide%20a%20powerful%20foundation%20for%20downstream%20generative%20models.%20Extensive%0Aexperiments%20on%20three%20public%20benchmarks%20validate%20HiD-VAE%27s%20superior%20performance%0Aagainst%20state-of-the-art%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/HiD-VAE-84B2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04618v1&entry.124074799=Read"},
{"title": "Upsampling DINOv2 features for unsupervised vision tasks and weakly\n  supervised materials segmentation", "author": "Ronan Docherty and Antonis Vamvakeros and Samuel J. Cooper", "abstract": "  The features of self-supervised vision transformers (ViTs) contain strong\nsemantic and positional information relevant to downstream tasks like object\nlocalization and segmentation. Recent works combine these features with\ntraditional methods like clustering, graph partitioning or region correlations\nto achieve impressive baselines without finetuning or training additional\nnetworks. We leverage upsampled features from ViT networks (e.g DINOv2) in two\nworkflows: in a clustering based approach for object localization and\nsegmentation, and paired with standard classifiers in weakly supervised\nmaterials segmentation. Both show strong performance on benchmarks, especially\nin weakly supervised segmentation where the ViT features capture complex\nrelationships inaccessible to classical approaches. We expect the flexibility\nand generalizability of these features will both speed up and strengthen\nmaterials characterization, from segmentation to property-prediction.\n", "link": "http://arxiv.org/abs/2410.19836v2", "date": "2025-08-06", "relevancy": 2.6528, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Upsampling%20DINOv2%20features%20for%20unsupervised%20vision%20tasks%20and%20weakly%0A%20%20supervised%20materials%20segmentation&body=Title%3A%20Upsampling%20DINOv2%20features%20for%20unsupervised%20vision%20tasks%20and%20weakly%0A%20%20supervised%20materials%20segmentation%0AAuthor%3A%20Ronan%20Docherty%20and%20Antonis%20Vamvakeros%20and%20Samuel%20J.%20Cooper%0AAbstract%3A%20%20%20The%20features%20of%20self-supervised%20vision%20transformers%20%28ViTs%29%20contain%20strong%0Asemantic%20and%20positional%20information%20relevant%20to%20downstream%20tasks%20like%20object%0Alocalization%20and%20segmentation.%20Recent%20works%20combine%20these%20features%20with%0Atraditional%20methods%20like%20clustering%2C%20graph%20partitioning%20or%20region%20correlations%0Ato%20achieve%20impressive%20baselines%20without%20finetuning%20or%20training%20additional%0Anetworks.%20We%20leverage%20upsampled%20features%20from%20ViT%20networks%20%28e.g%20DINOv2%29%20in%20two%0Aworkflows%3A%20in%20a%20clustering%20based%20approach%20for%20object%20localization%20and%0Asegmentation%2C%20and%20paired%20with%20standard%20classifiers%20in%20weakly%20supervised%0Amaterials%20segmentation.%20Both%20show%20strong%20performance%20on%20benchmarks%2C%20especially%0Ain%20weakly%20supervised%20segmentation%20where%20the%20ViT%20features%20capture%20complex%0Arelationships%20inaccessible%20to%20classical%20approaches.%20We%20expect%20the%20flexibility%0Aand%20generalizability%20of%20these%20features%20will%20both%20speed%20up%20and%20strengthen%0Amaterials%20characterization%2C%20from%20segmentation%20to%20property-prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.19836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUpsampling%2520DINOv2%2520features%2520for%2520unsupervised%2520vision%2520tasks%2520and%2520weakly%250A%2520%2520supervised%2520materials%2520segmentation%26entry.906535625%3DRonan%2520Docherty%2520and%2520Antonis%2520Vamvakeros%2520and%2520Samuel%2520J.%2520Cooper%26entry.1292438233%3D%2520%2520The%2520features%2520of%2520self-supervised%2520vision%2520transformers%2520%2528ViTs%2529%2520contain%2520strong%250Asemantic%2520and%2520positional%2520information%2520relevant%2520to%2520downstream%2520tasks%2520like%2520object%250Alocalization%2520and%2520segmentation.%2520Recent%2520works%2520combine%2520these%2520features%2520with%250Atraditional%2520methods%2520like%2520clustering%252C%2520graph%2520partitioning%2520or%2520region%2520correlations%250Ato%2520achieve%2520impressive%2520baselines%2520without%2520finetuning%2520or%2520training%2520additional%250Anetworks.%2520We%2520leverage%2520upsampled%2520features%2520from%2520ViT%2520networks%2520%2528e.g%2520DINOv2%2529%2520in%2520two%250Aworkflows%253A%2520in%2520a%2520clustering%2520based%2520approach%2520for%2520object%2520localization%2520and%250Asegmentation%252C%2520and%2520paired%2520with%2520standard%2520classifiers%2520in%2520weakly%2520supervised%250Amaterials%2520segmentation.%2520Both%2520show%2520strong%2520performance%2520on%2520benchmarks%252C%2520especially%250Ain%2520weakly%2520supervised%2520segmentation%2520where%2520the%2520ViT%2520features%2520capture%2520complex%250Arelationships%2520inaccessible%2520to%2520classical%2520approaches.%2520We%2520expect%2520the%2520flexibility%250Aand%2520generalizability%2520of%2520these%2520features%2520will%2520both%2520speed%2520up%2520and%2520strengthen%250Amaterials%2520characterization%252C%2520from%2520segmentation%2520to%2520property-prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Upsampling%20DINOv2%20features%20for%20unsupervised%20vision%20tasks%20and%20weakly%0A%20%20supervised%20materials%20segmentation&entry.906535625=Ronan%20Docherty%20and%20Antonis%20Vamvakeros%20and%20Samuel%20J.%20Cooper&entry.1292438233=%20%20The%20features%20of%20self-supervised%20vision%20transformers%20%28ViTs%29%20contain%20strong%0Asemantic%20and%20positional%20information%20relevant%20to%20downstream%20tasks%20like%20object%0Alocalization%20and%20segmentation.%20Recent%20works%20combine%20these%20features%20with%0Atraditional%20methods%20like%20clustering%2C%20graph%20partitioning%20or%20region%20correlations%0Ato%20achieve%20impressive%20baselines%20without%20finetuning%20or%20training%20additional%0Anetworks.%20We%20leverage%20upsampled%20features%20from%20ViT%20networks%20%28e.g%20DINOv2%29%20in%20two%0Aworkflows%3A%20in%20a%20clustering%20based%20approach%20for%20object%20localization%20and%0Asegmentation%2C%20and%20paired%20with%20standard%20classifiers%20in%20weakly%20supervised%0Amaterials%20segmentation.%20Both%20show%20strong%20performance%20on%20benchmarks%2C%20especially%0Ain%20weakly%20supervised%20segmentation%20where%20the%20ViT%20features%20capture%20complex%0Arelationships%20inaccessible%20to%20classical%20approaches.%20We%20expect%20the%20flexibility%0Aand%20generalizability%20of%20these%20features%20will%20both%20speed%20up%20and%20strengthen%0Amaterials%20characterization%2C%20from%20segmentation%20to%20property-prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.19836v2&entry.124074799=Read"},
{"title": "OpenDCVCs: A PyTorch Open Source Implementation and Performance\n  Evaluation of the DCVC series Video Codecs", "author": "Yichi Zhang and Fengqing Zhu", "abstract": "  We present OpenDCVCs, an open-source PyTorch implementation designed to\nadvance reproducible research in learned video compression. OpenDCVCs provides\nunified and training-ready implementations of four representative Deep\nContextual Video Compression (DCVC) models--DCVC, DCVC with Temporal Context\nModeling (DCVC-TCM), DCVC with Hybrid Entropy Modeling (DCVC-HEM), and DCVC\nwith Diverse Contexts (DCVC-DC). While the DCVC series achieves substantial\nbitrate reductions over both classical codecs and advanced learned models,\nprevious public code releases have been limited to evaluation codes, presenting\nsignificant barriers to reproducibility, benchmarking, and further development.\nOpenDCVCs bridges this gap by offering a comprehensive, self-contained\nframework that supports both end-to-end training and evaluation for all\nincluded algorithms. The implementation includes detailed documentation,\nevaluation protocols, and extensive benchmarking results across diverse\ndatasets, providing a transparent and consistent foundation for comparison and\nextension. All code and experimental tools are publicly available at\nhttps://gitlab.com/viper-purdue/opendcvcs, empowering the community to\naccelerate research and foster collaboration.\n", "link": "http://arxiv.org/abs/2508.04491v1", "date": "2025-08-06", "relevancy": 2.6409, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.534}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDCVCs%3A%20A%20PyTorch%20Open%20Source%20Implementation%20and%20Performance%0A%20%20Evaluation%20of%20the%20DCVC%20series%20Video%20Codecs&body=Title%3A%20OpenDCVCs%3A%20A%20PyTorch%20Open%20Source%20Implementation%20and%20Performance%0A%20%20Evaluation%20of%20the%20DCVC%20series%20Video%20Codecs%0AAuthor%3A%20Yichi%20Zhang%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20We%20present%20OpenDCVCs%2C%20an%20open-source%20PyTorch%20implementation%20designed%20to%0Aadvance%20reproducible%20research%20in%20learned%20video%20compression.%20OpenDCVCs%20provides%0Aunified%20and%20training-ready%20implementations%20of%20four%20representative%20Deep%0AContextual%20Video%20Compression%20%28DCVC%29%20models--DCVC%2C%20DCVC%20with%20Temporal%20Context%0AModeling%20%28DCVC-TCM%29%2C%20DCVC%20with%20Hybrid%20Entropy%20Modeling%20%28DCVC-HEM%29%2C%20and%20DCVC%0Awith%20Diverse%20Contexts%20%28DCVC-DC%29.%20While%20the%20DCVC%20series%20achieves%20substantial%0Abitrate%20reductions%20over%20both%20classical%20codecs%20and%20advanced%20learned%20models%2C%0Aprevious%20public%20code%20releases%20have%20been%20limited%20to%20evaluation%20codes%2C%20presenting%0Asignificant%20barriers%20to%20reproducibility%2C%20benchmarking%2C%20and%20further%20development.%0AOpenDCVCs%20bridges%20this%20gap%20by%20offering%20a%20comprehensive%2C%20self-contained%0Aframework%20that%20supports%20both%20end-to-end%20training%20and%20evaluation%20for%20all%0Aincluded%20algorithms.%20The%20implementation%20includes%20detailed%20documentation%2C%0Aevaluation%20protocols%2C%20and%20extensive%20benchmarking%20results%20across%20diverse%0Adatasets%2C%20providing%20a%20transparent%20and%20consistent%20foundation%20for%20comparison%20and%0Aextension.%20All%20code%20and%20experimental%20tools%20are%20publicly%20available%20at%0Ahttps%3A//gitlab.com/viper-purdue/opendcvcs%2C%20empowering%20the%20community%20to%0Aaccelerate%20research%20and%20foster%20collaboration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDCVCs%253A%2520A%2520PyTorch%2520Open%2520Source%2520Implementation%2520and%2520Performance%250A%2520%2520Evaluation%2520of%2520the%2520DCVC%2520series%2520Video%2520Codecs%26entry.906535625%3DYichi%2520Zhang%2520and%2520Fengqing%2520Zhu%26entry.1292438233%3D%2520%2520We%2520present%2520OpenDCVCs%252C%2520an%2520open-source%2520PyTorch%2520implementation%2520designed%2520to%250Aadvance%2520reproducible%2520research%2520in%2520learned%2520video%2520compression.%2520OpenDCVCs%2520provides%250Aunified%2520and%2520training-ready%2520implementations%2520of%2520four%2520representative%2520Deep%250AContextual%2520Video%2520Compression%2520%2528DCVC%2529%2520models--DCVC%252C%2520DCVC%2520with%2520Temporal%2520Context%250AModeling%2520%2528DCVC-TCM%2529%252C%2520DCVC%2520with%2520Hybrid%2520Entropy%2520Modeling%2520%2528DCVC-HEM%2529%252C%2520and%2520DCVC%250Awith%2520Diverse%2520Contexts%2520%2528DCVC-DC%2529.%2520While%2520the%2520DCVC%2520series%2520achieves%2520substantial%250Abitrate%2520reductions%2520over%2520both%2520classical%2520codecs%2520and%2520advanced%2520learned%2520models%252C%250Aprevious%2520public%2520code%2520releases%2520have%2520been%2520limited%2520to%2520evaluation%2520codes%252C%2520presenting%250Asignificant%2520barriers%2520to%2520reproducibility%252C%2520benchmarking%252C%2520and%2520further%2520development.%250AOpenDCVCs%2520bridges%2520this%2520gap%2520by%2520offering%2520a%2520comprehensive%252C%2520self-contained%250Aframework%2520that%2520supports%2520both%2520end-to-end%2520training%2520and%2520evaluation%2520for%2520all%250Aincluded%2520algorithms.%2520The%2520implementation%2520includes%2520detailed%2520documentation%252C%250Aevaluation%2520protocols%252C%2520and%2520extensive%2520benchmarking%2520results%2520across%2520diverse%250Adatasets%252C%2520providing%2520a%2520transparent%2520and%2520consistent%2520foundation%2520for%2520comparison%2520and%250Aextension.%2520All%2520code%2520and%2520experimental%2520tools%2520are%2520publicly%2520available%2520at%250Ahttps%253A//gitlab.com/viper-purdue/opendcvcs%252C%2520empowering%2520the%2520community%2520to%250Aaccelerate%2520research%2520and%2520foster%2520collaboration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDCVCs%3A%20A%20PyTorch%20Open%20Source%20Implementation%20and%20Performance%0A%20%20Evaluation%20of%20the%20DCVC%20series%20Video%20Codecs&entry.906535625=Yichi%20Zhang%20and%20Fengqing%20Zhu&entry.1292438233=%20%20We%20present%20OpenDCVCs%2C%20an%20open-source%20PyTorch%20implementation%20designed%20to%0Aadvance%20reproducible%20research%20in%20learned%20video%20compression.%20OpenDCVCs%20provides%0Aunified%20and%20training-ready%20implementations%20of%20four%20representative%20Deep%0AContextual%20Video%20Compression%20%28DCVC%29%20models--DCVC%2C%20DCVC%20with%20Temporal%20Context%0AModeling%20%28DCVC-TCM%29%2C%20DCVC%20with%20Hybrid%20Entropy%20Modeling%20%28DCVC-HEM%29%2C%20and%20DCVC%0Awith%20Diverse%20Contexts%20%28DCVC-DC%29.%20While%20the%20DCVC%20series%20achieves%20substantial%0Abitrate%20reductions%20over%20both%20classical%20codecs%20and%20advanced%20learned%20models%2C%0Aprevious%20public%20code%20releases%20have%20been%20limited%20to%20evaluation%20codes%2C%20presenting%0Asignificant%20barriers%20to%20reproducibility%2C%20benchmarking%2C%20and%20further%20development.%0AOpenDCVCs%20bridges%20this%20gap%20by%20offering%20a%20comprehensive%2C%20self-contained%0Aframework%20that%20supports%20both%20end-to-end%20training%20and%20evaluation%20for%20all%0Aincluded%20algorithms.%20The%20implementation%20includes%20detailed%20documentation%2C%0Aevaluation%20protocols%2C%20and%20extensive%20benchmarking%20results%20across%20diverse%0Adatasets%2C%20providing%20a%20transparent%20and%20consistent%20foundation%20for%20comparison%20and%0Aextension.%20All%20code%20and%20experimental%20tools%20are%20publicly%20available%20at%0Ahttps%3A//gitlab.com/viper-purdue/opendcvcs%2C%20empowering%20the%20community%20to%0Aaccelerate%20research%20and%20foster%20collaboration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04491v1&entry.124074799=Read"},
{"title": "RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor\n  Graph Optimization", "author": "Yanyan Li and Ze Yang and Keisuke Tateno and Federico Tombari Liang Zhao and Gim Hee Lee", "abstract": "  Minimal parametrization of 3D lines plays a critical role in camera\nlocalization and structural mapping. Existing representations in robotics and\ncomputer vision predominantly handle independent lines, overlooking structural\nregularities such as sets of parallel lines that are pervasive in man-made\nenvironments. This paper introduces \\textbf{RiemanLine}, a unified minimal\nrepresentation for 3D lines formulated on Riemannian manifolds that jointly\naccommodates both individual lines and parallel-line groups. Our key idea is to\ndecouple each line landmark into global and local components: a shared\nvanishing direction optimized on the unit sphere $\\mathcal{S}^2$, and scaled\nnormal vectors constrained on orthogonal subspaces, enabling compact encoding\nof structural regularities. For $n$ parallel lines, the proposed representation\nreduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally\nembedding parallelism without explicit constraints. We further integrate this\nparameterization into a factor graph framework, allowing global direction\nalignment and local reprojection optimization within a unified manifold-based\nbundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic\nbenchmarks demonstrate that our method achieves significantly more accurate\npose estimation and line reconstruction, while reducing parameter\ndimensionality and improving convergence stability.\n", "link": "http://arxiv.org/abs/2508.04335v1", "date": "2025-08-06", "relevancy": 2.6388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5317}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5281}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RiemanLine%3A%20Riemannian%20Manifold%20Representation%20of%203D%20Lines%20for%20Factor%0A%20%20Graph%20Optimization&body=Title%3A%20RiemanLine%3A%20Riemannian%20Manifold%20Representation%20of%203D%20Lines%20for%20Factor%0A%20%20Graph%20Optimization%0AAuthor%3A%20Yanyan%20Li%20and%20Ze%20Yang%20and%20Keisuke%20Tateno%20and%20Federico%20Tombari%20Liang%20Zhao%20and%20Gim%20Hee%20Lee%0AAbstract%3A%20%20%20Minimal%20parametrization%20of%203D%20lines%20plays%20a%20critical%20role%20in%20camera%0Alocalization%20and%20structural%20mapping.%20Existing%20representations%20in%20robotics%20and%0Acomputer%20vision%20predominantly%20handle%20independent%20lines%2C%20overlooking%20structural%0Aregularities%20such%20as%20sets%20of%20parallel%20lines%20that%20are%20pervasive%20in%20man-made%0Aenvironments.%20This%20paper%20introduces%20%5Ctextbf%7BRiemanLine%7D%2C%20a%20unified%20minimal%0Arepresentation%20for%203D%20lines%20formulated%20on%20Riemannian%20manifolds%20that%20jointly%0Aaccommodates%20both%20individual%20lines%20and%20parallel-line%20groups.%20Our%20key%20idea%20is%20to%0Adecouple%20each%20line%20landmark%20into%20global%20and%20local%20components%3A%20a%20shared%0Avanishing%20direction%20optimized%20on%20the%20unit%20sphere%20%24%5Cmathcal%7BS%7D%5E2%24%2C%20and%20scaled%0Anormal%20vectors%20constrained%20on%20orthogonal%20subspaces%2C%20enabling%20compact%20encoding%0Aof%20structural%20regularities.%20For%20%24n%24%20parallel%20lines%2C%20the%20proposed%20representation%0Areduces%20the%20parameter%20space%20from%20%244n%24%20%28orthonormal%20form%29%20to%20%242n%2B2%24%2C%20naturally%0Aembedding%20parallelism%20without%20explicit%20constraints.%20We%20further%20integrate%20this%0Aparameterization%20into%20a%20factor%20graph%20framework%2C%20allowing%20global%20direction%0Aalignment%20and%20local%20reprojection%20optimization%20within%20a%20unified%20manifold-based%0Abundle%20adjustment.%20Extensive%20experiments%20on%20ICL-NUIM%2C%20TartanAir%2C%20and%20synthetic%0Abenchmarks%20demonstrate%20that%20our%20method%20achieves%20significantly%20more%20accurate%0Apose%20estimation%20and%20line%20reconstruction%2C%20while%20reducing%20parameter%0Adimensionality%20and%20improving%20convergence%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemanLine%253A%2520Riemannian%2520Manifold%2520Representation%2520of%25203D%2520Lines%2520for%2520Factor%250A%2520%2520Graph%2520Optimization%26entry.906535625%3DYanyan%2520Li%2520and%2520Ze%2520Yang%2520and%2520Keisuke%2520Tateno%2520and%2520Federico%2520Tombari%2520Liang%2520Zhao%2520and%2520Gim%2520Hee%2520Lee%26entry.1292438233%3D%2520%2520Minimal%2520parametrization%2520of%25203D%2520lines%2520plays%2520a%2520critical%2520role%2520in%2520camera%250Alocalization%2520and%2520structural%2520mapping.%2520Existing%2520representations%2520in%2520robotics%2520and%250Acomputer%2520vision%2520predominantly%2520handle%2520independent%2520lines%252C%2520overlooking%2520structural%250Aregularities%2520such%2520as%2520sets%2520of%2520parallel%2520lines%2520that%2520are%2520pervasive%2520in%2520man-made%250Aenvironments.%2520This%2520paper%2520introduces%2520%255Ctextbf%257BRiemanLine%257D%252C%2520a%2520unified%2520minimal%250Arepresentation%2520for%25203D%2520lines%2520formulated%2520on%2520Riemannian%2520manifolds%2520that%2520jointly%250Aaccommodates%2520both%2520individual%2520lines%2520and%2520parallel-line%2520groups.%2520Our%2520key%2520idea%2520is%2520to%250Adecouple%2520each%2520line%2520landmark%2520into%2520global%2520and%2520local%2520components%253A%2520a%2520shared%250Avanishing%2520direction%2520optimized%2520on%2520the%2520unit%2520sphere%2520%2524%255Cmathcal%257BS%257D%255E2%2524%252C%2520and%2520scaled%250Anormal%2520vectors%2520constrained%2520on%2520orthogonal%2520subspaces%252C%2520enabling%2520compact%2520encoding%250Aof%2520structural%2520regularities.%2520For%2520%2524n%2524%2520parallel%2520lines%252C%2520the%2520proposed%2520representation%250Areduces%2520the%2520parameter%2520space%2520from%2520%25244n%2524%2520%2528orthonormal%2520form%2529%2520to%2520%25242n%252B2%2524%252C%2520naturally%250Aembedding%2520parallelism%2520without%2520explicit%2520constraints.%2520We%2520further%2520integrate%2520this%250Aparameterization%2520into%2520a%2520factor%2520graph%2520framework%252C%2520allowing%2520global%2520direction%250Aalignment%2520and%2520local%2520reprojection%2520optimization%2520within%2520a%2520unified%2520manifold-based%250Abundle%2520adjustment.%2520Extensive%2520experiments%2520on%2520ICL-NUIM%252C%2520TartanAir%252C%2520and%2520synthetic%250Abenchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520significantly%2520more%2520accurate%250Apose%2520estimation%2520and%2520line%2520reconstruction%252C%2520while%2520reducing%2520parameter%250Adimensionality%2520and%2520improving%2520convergence%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RiemanLine%3A%20Riemannian%20Manifold%20Representation%20of%203D%20Lines%20for%20Factor%0A%20%20Graph%20Optimization&entry.906535625=Yanyan%20Li%20and%20Ze%20Yang%20and%20Keisuke%20Tateno%20and%20Federico%20Tombari%20Liang%20Zhao%20and%20Gim%20Hee%20Lee&entry.1292438233=%20%20Minimal%20parametrization%20of%203D%20lines%20plays%20a%20critical%20role%20in%20camera%0Alocalization%20and%20structural%20mapping.%20Existing%20representations%20in%20robotics%20and%0Acomputer%20vision%20predominantly%20handle%20independent%20lines%2C%20overlooking%20structural%0Aregularities%20such%20as%20sets%20of%20parallel%20lines%20that%20are%20pervasive%20in%20man-made%0Aenvironments.%20This%20paper%20introduces%20%5Ctextbf%7BRiemanLine%7D%2C%20a%20unified%20minimal%0Arepresentation%20for%203D%20lines%20formulated%20on%20Riemannian%20manifolds%20that%20jointly%0Aaccommodates%20both%20individual%20lines%20and%20parallel-line%20groups.%20Our%20key%20idea%20is%20to%0Adecouple%20each%20line%20landmark%20into%20global%20and%20local%20components%3A%20a%20shared%0Avanishing%20direction%20optimized%20on%20the%20unit%20sphere%20%24%5Cmathcal%7BS%7D%5E2%24%2C%20and%20scaled%0Anormal%20vectors%20constrained%20on%20orthogonal%20subspaces%2C%20enabling%20compact%20encoding%0Aof%20structural%20regularities.%20For%20%24n%24%20parallel%20lines%2C%20the%20proposed%20representation%0Areduces%20the%20parameter%20space%20from%20%244n%24%20%28orthonormal%20form%29%20to%20%242n%2B2%24%2C%20naturally%0Aembedding%20parallelism%20without%20explicit%20constraints.%20We%20further%20integrate%20this%0Aparameterization%20into%20a%20factor%20graph%20framework%2C%20allowing%20global%20direction%0Aalignment%20and%20local%20reprojection%20optimization%20within%20a%20unified%20manifold-based%0Abundle%20adjustment.%20Extensive%20experiments%20on%20ICL-NUIM%2C%20TartanAir%2C%20and%20synthetic%0Abenchmarks%20demonstrate%20that%20our%20method%20achieves%20significantly%20more%20accurate%0Apose%20estimation%20and%20line%20reconstruction%2C%20while%20reducing%20parameter%0Adimensionality%20and%20improving%20convergence%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04335v1&entry.124074799=Read"},
{"title": "Long-Term Visual Object Tracking with Event Cameras: An Associative\n  Memory Augmented Tracker and A Benchmark Dataset", "author": "Xiao Wang and Xufeng Lou and Shiao Wang and Ju Huang and Lan Chen and Bo Jiang", "abstract": "  Existing event stream based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term, large-scale frame-event visual object tracking dataset, termed FELT.\nIt contains 1,044 long-term videos that involve 1.9 million RGB frames and\nevent stream pairs, 60 different target objects, and 14 challenging attributes.\nTo build a solid benchmark, we retrain and evaluate 21 baseline trackers on our\ndataset for future work to compare. In addition, we propose a novel Associative\nMemory Transformer based RGB-Event long-term visual tracker, termed AMTTrack.\nIt follows a one-stream tracking framework and aggregates the multi-scale\nRGB/event template and search tokens effectively via the Hopfield retrieval\nlayer. The framework also embodies another aspect of associative memory by\nmaintaining dynamic template representations through an associative memory\nupdate scheme, which addresses the appearance variation in long-term tracking.\nExtensive experiments on FELT, FE108, VisEvent, and COESOT datasets fully\nvalidated the effectiveness of our proposed tracker. Both the dataset and\nsource code will be released on https://github.com/Event-AHU/FELT_SOT_Benchmark\n", "link": "http://arxiv.org/abs/2403.05839v3", "date": "2025-08-06", "relevancy": 2.6332, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5331}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.524}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Term%20Visual%20Object%20Tracking%20with%20Event%20Cameras%3A%20An%20Associative%0A%20%20Memory%20Augmented%20Tracker%20and%20A%20Benchmark%20Dataset&body=Title%3A%20Long-Term%20Visual%20Object%20Tracking%20with%20Event%20Cameras%3A%20An%20Associative%0A%20%20Memory%20Augmented%20Tracker%20and%20A%20Benchmark%20Dataset%0AAuthor%3A%20Xiao%20Wang%20and%20Xufeng%20Lou%20and%20Shiao%20Wang%20and%20Ju%20Huang%20and%20Lan%20Chen%20and%20Bo%20Jiang%0AAbstract%3A%20%20%20Existing%20event%20stream%20based%20trackers%20undergo%20evaluation%20on%20short-term%0Atracking%20datasets%2C%20however%2C%20the%20tracking%20of%20real-world%20scenarios%20involves%0Along-term%20tracking%2C%20and%20the%20performance%20of%20existing%20tracking%20algorithms%20in%0Athese%20scenarios%20remains%20unclear.%20In%20this%20paper%2C%20we%20first%20propose%20a%20new%0Along-term%2C%20large-scale%20frame-event%20visual%20object%20tracking%20dataset%2C%20termed%20FELT.%0AIt%20contains%201%2C044%20long-term%20videos%20that%20involve%201.9%20million%20RGB%20frames%20and%0Aevent%20stream%20pairs%2C%2060%20different%20target%20objects%2C%20and%2014%20challenging%20attributes.%0ATo%20build%20a%20solid%20benchmark%2C%20we%20retrain%20and%20evaluate%2021%20baseline%20trackers%20on%20our%0Adataset%20for%20future%20work%20to%20compare.%20In%20addition%2C%20we%20propose%20a%20novel%20Associative%0AMemory%20Transformer%20based%20RGB-Event%20long-term%20visual%20tracker%2C%20termed%20AMTTrack.%0AIt%20follows%20a%20one-stream%20tracking%20framework%20and%20aggregates%20the%20multi-scale%0ARGB/event%20template%20and%20search%20tokens%20effectively%20via%20the%20Hopfield%20retrieval%0Alayer.%20The%20framework%20also%20embodies%20another%20aspect%20of%20associative%20memory%20by%0Amaintaining%20dynamic%20template%20representations%20through%20an%20associative%20memory%0Aupdate%20scheme%2C%20which%20addresses%20the%20appearance%20variation%20in%20long-term%20tracking.%0AExtensive%20experiments%20on%20FELT%2C%20FE108%2C%20VisEvent%2C%20and%20COESOT%20datasets%20fully%0Avalidated%20the%20effectiveness%20of%20our%20proposed%20tracker.%20Both%20the%20dataset%20and%0Asource%20code%20will%20be%20released%20on%20https%3A//github.com/Event-AHU/FELT_SOT_Benchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05839v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Term%2520Visual%2520Object%2520Tracking%2520with%2520Event%2520Cameras%253A%2520An%2520Associative%250A%2520%2520Memory%2520Augmented%2520Tracker%2520and%2520A%2520Benchmark%2520Dataset%26entry.906535625%3DXiao%2520Wang%2520and%2520Xufeng%2520Lou%2520and%2520Shiao%2520Wang%2520and%2520Ju%2520Huang%2520and%2520Lan%2520Chen%2520and%2520Bo%2520Jiang%26entry.1292438233%3D%2520%2520Existing%2520event%2520stream%2520based%2520trackers%2520undergo%2520evaluation%2520on%2520short-term%250Atracking%2520datasets%252C%2520however%252C%2520the%2520tracking%2520of%2520real-world%2520scenarios%2520involves%250Along-term%2520tracking%252C%2520and%2520the%2520performance%2520of%2520existing%2520tracking%2520algorithms%2520in%250Athese%2520scenarios%2520remains%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%2520a%2520new%250Along-term%252C%2520large-scale%2520frame-event%2520visual%2520object%2520tracking%2520dataset%252C%2520termed%2520FELT.%250AIt%2520contains%25201%252C044%2520long-term%2520videos%2520that%2520involve%25201.9%2520million%2520RGB%2520frames%2520and%250Aevent%2520stream%2520pairs%252C%252060%2520different%2520target%2520objects%252C%2520and%252014%2520challenging%2520attributes.%250ATo%2520build%2520a%2520solid%2520benchmark%252C%2520we%2520retrain%2520and%2520evaluate%252021%2520baseline%2520trackers%2520on%2520our%250Adataset%2520for%2520future%2520work%2520to%2520compare.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%2520Associative%250AMemory%2520Transformer%2520based%2520RGB-Event%2520long-term%2520visual%2520tracker%252C%2520termed%2520AMTTrack.%250AIt%2520follows%2520a%2520one-stream%2520tracking%2520framework%2520and%2520aggregates%2520the%2520multi-scale%250ARGB/event%2520template%2520and%2520search%2520tokens%2520effectively%2520via%2520the%2520Hopfield%2520retrieval%250Alayer.%2520The%2520framework%2520also%2520embodies%2520another%2520aspect%2520of%2520associative%2520memory%2520by%250Amaintaining%2520dynamic%2520template%2520representations%2520through%2520an%2520associative%2520memory%250Aupdate%2520scheme%252C%2520which%2520addresses%2520the%2520appearance%2520variation%2520in%2520long-term%2520tracking.%250AExtensive%2520experiments%2520on%2520FELT%252C%2520FE108%252C%2520VisEvent%252C%2520and%2520COESOT%2520datasets%2520fully%250Avalidated%2520the%2520effectiveness%2520of%2520our%2520proposed%2520tracker.%2520Both%2520the%2520dataset%2520and%250Asource%2520code%2520will%2520be%2520released%2520on%2520https%253A//github.com/Event-AHU/FELT_SOT_Benchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05839v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Term%20Visual%20Object%20Tracking%20with%20Event%20Cameras%3A%20An%20Associative%0A%20%20Memory%20Augmented%20Tracker%20and%20A%20Benchmark%20Dataset&entry.906535625=Xiao%20Wang%20and%20Xufeng%20Lou%20and%20Shiao%20Wang%20and%20Ju%20Huang%20and%20Lan%20Chen%20and%20Bo%20Jiang&entry.1292438233=%20%20Existing%20event%20stream%20based%20trackers%20undergo%20evaluation%20on%20short-term%0Atracking%20datasets%2C%20however%2C%20the%20tracking%20of%20real-world%20scenarios%20involves%0Along-term%20tracking%2C%20and%20the%20performance%20of%20existing%20tracking%20algorithms%20in%0Athese%20scenarios%20remains%20unclear.%20In%20this%20paper%2C%20we%20first%20propose%20a%20new%0Along-term%2C%20large-scale%20frame-event%20visual%20object%20tracking%20dataset%2C%20termed%20FELT.%0AIt%20contains%201%2C044%20long-term%20videos%20that%20involve%201.9%20million%20RGB%20frames%20and%0Aevent%20stream%20pairs%2C%2060%20different%20target%20objects%2C%20and%2014%20challenging%20attributes.%0ATo%20build%20a%20solid%20benchmark%2C%20we%20retrain%20and%20evaluate%2021%20baseline%20trackers%20on%20our%0Adataset%20for%20future%20work%20to%20compare.%20In%20addition%2C%20we%20propose%20a%20novel%20Associative%0AMemory%20Transformer%20based%20RGB-Event%20long-term%20visual%20tracker%2C%20termed%20AMTTrack.%0AIt%20follows%20a%20one-stream%20tracking%20framework%20and%20aggregates%20the%20multi-scale%0ARGB/event%20template%20and%20search%20tokens%20effectively%20via%20the%20Hopfield%20retrieval%0Alayer.%20The%20framework%20also%20embodies%20another%20aspect%20of%20associative%20memory%20by%0Amaintaining%20dynamic%20template%20representations%20through%20an%20associative%20memory%0Aupdate%20scheme%2C%20which%20addresses%20the%20appearance%20variation%20in%20long-term%20tracking.%0AExtensive%20experiments%20on%20FELT%2C%20FE108%2C%20VisEvent%2C%20and%20COESOT%20datasets%20fully%0Avalidated%20the%20effectiveness%20of%20our%20proposed%20tracker.%20Both%20the%20dataset%20and%0Asource%20code%20will%20be%20released%20on%20https%3A//github.com/Event-AHU/FELT_SOT_Benchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05839v3&entry.124074799=Read"},
{"title": "PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space", "author": "Chenlei Lv and Hui Huang", "abstract": "  Point cloud registration is a classical topic in the field of 3D Vision and\nComputer Graphics. Generally, the implementation of registration is typically\nsensitive to similarity transformations (translation, scaling, and rotation),\nnoisy points, and incomplete geometric structures. Especially, the non-uniform\nscales and defective parts of point clouds increase probability of struck local\noptima in registration task. In this paper, we propose a robust point cloud\nregistration PKSS-Align that can handle various influences, including\nsimilarity transformations, non-uniform densities, random noisy points, and\ndefective parts. The proposed method measures shape feature-based similarity\nbetween point clouds on the Pre-Kendall shape space (PKSS),\n\\textcolor{black}{which is a shape measurement-based scheme and doesn't require\npoint-to-point or point-to-plane metric.} The employed measurement can be\nregarded as the manifold metric that is robust to various representations in\nthe Euclidean coordinate system. Benefited from the measurement, the\ntransformation matrix can be directly generated for point clouds with mentioned\ninfluences at the same time. The proposed method does not require data training\nand complex feature encoding. Based on a simple parallel acceleration, it can\nachieve significant improvement for efficiency and feasibility in practice.\nExperiments demonstrate that our method outperforms the relevant\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2508.04286v1", "date": "2025-08-06", "relevancy": 2.6147, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.536}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5281}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PKSS-Align%3A%20Robust%20Point%20Cloud%20Registration%20on%20Pre-Kendall%20Shape%20Space&body=Title%3A%20PKSS-Align%3A%20Robust%20Point%20Cloud%20Registration%20on%20Pre-Kendall%20Shape%20Space%0AAuthor%3A%20Chenlei%20Lv%20and%20Hui%20Huang%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20classical%20topic%20in%20the%20field%20of%203D%20Vision%20and%0AComputer%20Graphics.%20Generally%2C%20the%20implementation%20of%20registration%20is%20typically%0Asensitive%20to%20similarity%20transformations%20%28translation%2C%20scaling%2C%20and%20rotation%29%2C%0Anoisy%20points%2C%20and%20incomplete%20geometric%20structures.%20Especially%2C%20the%20non-uniform%0Ascales%20and%20defective%20parts%20of%20point%20clouds%20increase%20probability%20of%20struck%20local%0Aoptima%20in%20registration%20task.%20In%20this%20paper%2C%20we%20propose%20a%20robust%20point%20cloud%0Aregistration%20PKSS-Align%20that%20can%20handle%20various%20influences%2C%20including%0Asimilarity%20transformations%2C%20non-uniform%20densities%2C%20random%20noisy%20points%2C%20and%0Adefective%20parts.%20The%20proposed%20method%20measures%20shape%20feature-based%20similarity%0Abetween%20point%20clouds%20on%20the%20Pre-Kendall%20shape%20space%20%28PKSS%29%2C%0A%5Ctextcolor%7Bblack%7D%7Bwhich%20is%20a%20shape%20measurement-based%20scheme%20and%20doesn%27t%20require%0Apoint-to-point%20or%20point-to-plane%20metric.%7D%20The%20employed%20measurement%20can%20be%0Aregarded%20as%20the%20manifold%20metric%20that%20is%20robust%20to%20various%20representations%20in%0Athe%20Euclidean%20coordinate%20system.%20Benefited%20from%20the%20measurement%2C%20the%0Atransformation%20matrix%20can%20be%20directly%20generated%20for%20point%20clouds%20with%20mentioned%0Ainfluences%20at%20the%20same%20time.%20The%20proposed%20method%20does%20not%20require%20data%20training%0Aand%20complex%20feature%20encoding.%20Based%20on%20a%20simple%20parallel%20acceleration%2C%20it%20can%0Aachieve%20significant%20improvement%20for%20efficiency%20and%20feasibility%20in%20practice.%0AExperiments%20demonstrate%20that%20our%20method%20outperforms%20the%20relevant%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPKSS-Align%253A%2520Robust%2520Point%2520Cloud%2520Registration%2520on%2520Pre-Kendall%2520Shape%2520Space%26entry.906535625%3DChenlei%2520Lv%2520and%2520Hui%2520Huang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520a%2520classical%2520topic%2520in%2520the%2520field%2520of%25203D%2520Vision%2520and%250AComputer%2520Graphics.%2520Generally%252C%2520the%2520implementation%2520of%2520registration%2520is%2520typically%250Asensitive%2520to%2520similarity%2520transformations%2520%2528translation%252C%2520scaling%252C%2520and%2520rotation%2529%252C%250Anoisy%2520points%252C%2520and%2520incomplete%2520geometric%2520structures.%2520Especially%252C%2520the%2520non-uniform%250Ascales%2520and%2520defective%2520parts%2520of%2520point%2520clouds%2520increase%2520probability%2520of%2520struck%2520local%250Aoptima%2520in%2520registration%2520task.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520robust%2520point%2520cloud%250Aregistration%2520PKSS-Align%2520that%2520can%2520handle%2520various%2520influences%252C%2520including%250Asimilarity%2520transformations%252C%2520non-uniform%2520densities%252C%2520random%2520noisy%2520points%252C%2520and%250Adefective%2520parts.%2520The%2520proposed%2520method%2520measures%2520shape%2520feature-based%2520similarity%250Abetween%2520point%2520clouds%2520on%2520the%2520Pre-Kendall%2520shape%2520space%2520%2528PKSS%2529%252C%250A%255Ctextcolor%257Bblack%257D%257Bwhich%2520is%2520a%2520shape%2520measurement-based%2520scheme%2520and%2520doesn%2527t%2520require%250Apoint-to-point%2520or%2520point-to-plane%2520metric.%257D%2520The%2520employed%2520measurement%2520can%2520be%250Aregarded%2520as%2520the%2520manifold%2520metric%2520that%2520is%2520robust%2520to%2520various%2520representations%2520in%250Athe%2520Euclidean%2520coordinate%2520system.%2520Benefited%2520from%2520the%2520measurement%252C%2520the%250Atransformation%2520matrix%2520can%2520be%2520directly%2520generated%2520for%2520point%2520clouds%2520with%2520mentioned%250Ainfluences%2520at%2520the%2520same%2520time.%2520The%2520proposed%2520method%2520does%2520not%2520require%2520data%2520training%250Aand%2520complex%2520feature%2520encoding.%2520Based%2520on%2520a%2520simple%2520parallel%2520acceleration%252C%2520it%2520can%250Aachieve%2520significant%2520improvement%2520for%2520efficiency%2520and%2520feasibility%2520in%2520practice.%250AExperiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520relevant%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PKSS-Align%3A%20Robust%20Point%20Cloud%20Registration%20on%20Pre-Kendall%20Shape%20Space&entry.906535625=Chenlei%20Lv%20and%20Hui%20Huang&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20classical%20topic%20in%20the%20field%20of%203D%20Vision%20and%0AComputer%20Graphics.%20Generally%2C%20the%20implementation%20of%20registration%20is%20typically%0Asensitive%20to%20similarity%20transformations%20%28translation%2C%20scaling%2C%20and%20rotation%29%2C%0Anoisy%20points%2C%20and%20incomplete%20geometric%20structures.%20Especially%2C%20the%20non-uniform%0Ascales%20and%20defective%20parts%20of%20point%20clouds%20increase%20probability%20of%20struck%20local%0Aoptima%20in%20registration%20task.%20In%20this%20paper%2C%20we%20propose%20a%20robust%20point%20cloud%0Aregistration%20PKSS-Align%20that%20can%20handle%20various%20influences%2C%20including%0Asimilarity%20transformations%2C%20non-uniform%20densities%2C%20random%20noisy%20points%2C%20and%0Adefective%20parts.%20The%20proposed%20method%20measures%20shape%20feature-based%20similarity%0Abetween%20point%20clouds%20on%20the%20Pre-Kendall%20shape%20space%20%28PKSS%29%2C%0A%5Ctextcolor%7Bblack%7D%7Bwhich%20is%20a%20shape%20measurement-based%20scheme%20and%20doesn%27t%20require%0Apoint-to-point%20or%20point-to-plane%20metric.%7D%20The%20employed%20measurement%20can%20be%0Aregarded%20as%20the%20manifold%20metric%20that%20is%20robust%20to%20various%20representations%20in%0Athe%20Euclidean%20coordinate%20system.%20Benefited%20from%20the%20measurement%2C%20the%0Atransformation%20matrix%20can%20be%20directly%20generated%20for%20point%20clouds%20with%20mentioned%0Ainfluences%20at%20the%20same%20time.%20The%20proposed%20method%20does%20not%20require%20data%20training%0Aand%20complex%20feature%20encoding.%20Based%20on%20a%20simple%20parallel%20acceleration%2C%20it%20can%0Aachieve%20significant%20improvement%20for%20efficiency%20and%20feasibility%20in%20practice.%0AExperiments%20demonstrate%20that%20our%20method%20outperforms%20the%20relevant%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04286v1&entry.124074799=Read"},
{"title": "JointTuner: Appearance-Motion Adaptive Joint Training for Customized\n  Video Generation", "author": "Fangda Chen and Shanshan Zhao and Chuanfu Xu and Long Lan", "abstract": "  Recent advancements in customized video generation have led to significant\nimprovements in the simultaneous adaptation of appearance and motion. While\nprior methods typically decouple appearance and motion training, the stage-wise\nstrategy often introduces concept interference, resulting in inaccurate\nrendering of appearance features or motion patterns. Another challenge is\nappearance contamination, where background and foreground elements from\nreference videos distort the customized subject. In this work, we propose\nJointTuner, a novel framework that enables joint optimization of both\nappearance and motion components by leveraging two key innovations: Synaptic\nLow-Rank Adaptation (Synaptic LoRA) and Appearance-independent Temporal Loss\n(AiT Loss). Synaptic LoRA introduces a synaptic regulator, implemented as a\ncontext-aware linear activation layer, to dynamically guide LoRA modules to\nfocus on either subject appearance or motion patterns, thereby enabling\nconsistent optimization across spatial and temporal dimensions. AiT Loss\ndisrupts the gradient flow of appearance-related components, guiding the model\nto focus exclusively on motion learning and minimizing appearance interference.\nJointTuner is compatible with both UNet-based models (e.g., ZeroScope) and\nDiffusion Transformer-based models (e.g., CogVideoX), supporting the generation\nof longer and higher-quality customized videos. Additionally, we present a\nsystematic evaluation framework for appearance-motion combined customization,\ncovering 90 combinations evaluated along four critical dimensions: semantic\nalignment, motion dynamism, temporal consistency, and perceptual quality. Our\nproject homepage can be found at https://fdchen24.github.io/JointTuner-Website.\n", "link": "http://arxiv.org/abs/2503.23951v2", "date": "2025-08-06", "relevancy": 2.6138, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7508}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6432}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointTuner%3A%20Appearance-Motion%20Adaptive%20Joint%20Training%20for%20Customized%0A%20%20Video%20Generation&body=Title%3A%20JointTuner%3A%20Appearance-Motion%20Adaptive%20Joint%20Training%20for%20Customized%0A%20%20Video%20Generation%0AAuthor%3A%20Fangda%20Chen%20and%20Shanshan%20Zhao%20and%20Chuanfu%20Xu%20and%20Long%20Lan%0AAbstract%3A%20%20%20Recent%20advancements%20in%20customized%20video%20generation%20have%20led%20to%20significant%0Aimprovements%20in%20the%20simultaneous%20adaptation%20of%20appearance%20and%20motion.%20While%0Aprior%20methods%20typically%20decouple%20appearance%20and%20motion%20training%2C%20the%20stage-wise%0Astrategy%20often%20introduces%20concept%20interference%2C%20resulting%20in%20inaccurate%0Arendering%20of%20appearance%20features%20or%20motion%20patterns.%20Another%20challenge%20is%0Aappearance%20contamination%2C%20where%20background%20and%20foreground%20elements%20from%0Areference%20videos%20distort%20the%20customized%20subject.%20In%20this%20work%2C%20we%20propose%0AJointTuner%2C%20a%20novel%20framework%20that%20enables%20joint%20optimization%20of%20both%0Aappearance%20and%20motion%20components%20by%20leveraging%20two%20key%20innovations%3A%20Synaptic%0ALow-Rank%20Adaptation%20%28Synaptic%20LoRA%29%20and%20Appearance-independent%20Temporal%20Loss%0A%28AiT%20Loss%29.%20Synaptic%20LoRA%20introduces%20a%20synaptic%20regulator%2C%20implemented%20as%20a%0Acontext-aware%20linear%20activation%20layer%2C%20to%20dynamically%20guide%20LoRA%20modules%20to%0Afocus%20on%20either%20subject%20appearance%20or%20motion%20patterns%2C%20thereby%20enabling%0Aconsistent%20optimization%20across%20spatial%20and%20temporal%20dimensions.%20AiT%20Loss%0Adisrupts%20the%20gradient%20flow%20of%20appearance-related%20components%2C%20guiding%20the%20model%0Ato%20focus%20exclusively%20on%20motion%20learning%20and%20minimizing%20appearance%20interference.%0AJointTuner%20is%20compatible%20with%20both%20UNet-based%20models%20%28e.g.%2C%20ZeroScope%29%20and%0ADiffusion%20Transformer-based%20models%20%28e.g.%2C%20CogVideoX%29%2C%20supporting%20the%20generation%0Aof%20longer%20and%20higher-quality%20customized%20videos.%20Additionally%2C%20we%20present%20a%0Asystematic%20evaluation%20framework%20for%20appearance-motion%20combined%20customization%2C%0Acovering%2090%20combinations%20evaluated%20along%20four%20critical%20dimensions%3A%20semantic%0Aalignment%2C%20motion%20dynamism%2C%20temporal%20consistency%2C%20and%20perceptual%20quality.%20Our%0Aproject%20homepage%20can%20be%20found%20at%20https%3A//fdchen24.github.io/JointTuner-Website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23951v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointTuner%253A%2520Appearance-Motion%2520Adaptive%2520Joint%2520Training%2520for%2520Customized%250A%2520%2520Video%2520Generation%26entry.906535625%3DFangda%2520Chen%2520and%2520Shanshan%2520Zhao%2520and%2520Chuanfu%2520Xu%2520and%2520Long%2520Lan%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520customized%2520video%2520generation%2520have%2520led%2520to%2520significant%250Aimprovements%2520in%2520the%2520simultaneous%2520adaptation%2520of%2520appearance%2520and%2520motion.%2520While%250Aprior%2520methods%2520typically%2520decouple%2520appearance%2520and%2520motion%2520training%252C%2520the%2520stage-wise%250Astrategy%2520often%2520introduces%2520concept%2520interference%252C%2520resulting%2520in%2520inaccurate%250Arendering%2520of%2520appearance%2520features%2520or%2520motion%2520patterns.%2520Another%2520challenge%2520is%250Aappearance%2520contamination%252C%2520where%2520background%2520and%2520foreground%2520elements%2520from%250Areference%2520videos%2520distort%2520the%2520customized%2520subject.%2520In%2520this%2520work%252C%2520we%2520propose%250AJointTuner%252C%2520a%2520novel%2520framework%2520that%2520enables%2520joint%2520optimization%2520of%2520both%250Aappearance%2520and%2520motion%2520components%2520by%2520leveraging%2520two%2520key%2520innovations%253A%2520Synaptic%250ALow-Rank%2520Adaptation%2520%2528Synaptic%2520LoRA%2529%2520and%2520Appearance-independent%2520Temporal%2520Loss%250A%2528AiT%2520Loss%2529.%2520Synaptic%2520LoRA%2520introduces%2520a%2520synaptic%2520regulator%252C%2520implemented%2520as%2520a%250Acontext-aware%2520linear%2520activation%2520layer%252C%2520to%2520dynamically%2520guide%2520LoRA%2520modules%2520to%250Afocus%2520on%2520either%2520subject%2520appearance%2520or%2520motion%2520patterns%252C%2520thereby%2520enabling%250Aconsistent%2520optimization%2520across%2520spatial%2520and%2520temporal%2520dimensions.%2520AiT%2520Loss%250Adisrupts%2520the%2520gradient%2520flow%2520of%2520appearance-related%2520components%252C%2520guiding%2520the%2520model%250Ato%2520focus%2520exclusively%2520on%2520motion%2520learning%2520and%2520minimizing%2520appearance%2520interference.%250AJointTuner%2520is%2520compatible%2520with%2520both%2520UNet-based%2520models%2520%2528e.g.%252C%2520ZeroScope%2529%2520and%250ADiffusion%2520Transformer-based%2520models%2520%2528e.g.%252C%2520CogVideoX%2529%252C%2520supporting%2520the%2520generation%250Aof%2520longer%2520and%2520higher-quality%2520customized%2520videos.%2520Additionally%252C%2520we%2520present%2520a%250Asystematic%2520evaluation%2520framework%2520for%2520appearance-motion%2520combined%2520customization%252C%250Acovering%252090%2520combinations%2520evaluated%2520along%2520four%2520critical%2520dimensions%253A%2520semantic%250Aalignment%252C%2520motion%2520dynamism%252C%2520temporal%2520consistency%252C%2520and%2520perceptual%2520quality.%2520Our%250Aproject%2520homepage%2520can%2520be%2520found%2520at%2520https%253A//fdchen24.github.io/JointTuner-Website.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23951v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointTuner%3A%20Appearance-Motion%20Adaptive%20Joint%20Training%20for%20Customized%0A%20%20Video%20Generation&entry.906535625=Fangda%20Chen%20and%20Shanshan%20Zhao%20and%20Chuanfu%20Xu%20and%20Long%20Lan&entry.1292438233=%20%20Recent%20advancements%20in%20customized%20video%20generation%20have%20led%20to%20significant%0Aimprovements%20in%20the%20simultaneous%20adaptation%20of%20appearance%20and%20motion.%20While%0Aprior%20methods%20typically%20decouple%20appearance%20and%20motion%20training%2C%20the%20stage-wise%0Astrategy%20often%20introduces%20concept%20interference%2C%20resulting%20in%20inaccurate%0Arendering%20of%20appearance%20features%20or%20motion%20patterns.%20Another%20challenge%20is%0Aappearance%20contamination%2C%20where%20background%20and%20foreground%20elements%20from%0Areference%20videos%20distort%20the%20customized%20subject.%20In%20this%20work%2C%20we%20propose%0AJointTuner%2C%20a%20novel%20framework%20that%20enables%20joint%20optimization%20of%20both%0Aappearance%20and%20motion%20components%20by%20leveraging%20two%20key%20innovations%3A%20Synaptic%0ALow-Rank%20Adaptation%20%28Synaptic%20LoRA%29%20and%20Appearance-independent%20Temporal%20Loss%0A%28AiT%20Loss%29.%20Synaptic%20LoRA%20introduces%20a%20synaptic%20regulator%2C%20implemented%20as%20a%0Acontext-aware%20linear%20activation%20layer%2C%20to%20dynamically%20guide%20LoRA%20modules%20to%0Afocus%20on%20either%20subject%20appearance%20or%20motion%20patterns%2C%20thereby%20enabling%0Aconsistent%20optimization%20across%20spatial%20and%20temporal%20dimensions.%20AiT%20Loss%0Adisrupts%20the%20gradient%20flow%20of%20appearance-related%20components%2C%20guiding%20the%20model%0Ato%20focus%20exclusively%20on%20motion%20learning%20and%20minimizing%20appearance%20interference.%0AJointTuner%20is%20compatible%20with%20both%20UNet-based%20models%20%28e.g.%2C%20ZeroScope%29%20and%0ADiffusion%20Transformer-based%20models%20%28e.g.%2C%20CogVideoX%29%2C%20supporting%20the%20generation%0Aof%20longer%20and%20higher-quality%20customized%20videos.%20Additionally%2C%20we%20present%20a%0Asystematic%20evaluation%20framework%20for%20appearance-motion%20combined%20customization%2C%0Acovering%2090%20combinations%20evaluated%20along%20four%20critical%20dimensions%3A%20semantic%0Aalignment%2C%20motion%20dynamism%2C%20temporal%20consistency%2C%20and%20perceptual%20quality.%20Our%0Aproject%20homepage%20can%20be%20found%20at%20https%3A//fdchen24.github.io/JointTuner-Website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23951v2&entry.124074799=Read"},
{"title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained\n  Patch-Text Alignment", "author": "Lubin Gan and Jing Zhang and Linhao Qu and Yijun Wang and Siying Wu and Xiaoyan Sun", "abstract": "  The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.\n", "link": "http://arxiv.org/abs/2508.01602v2", "date": "2025-08-06", "relevancy": 2.6076, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5183}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Zero-Shot%20Brain%20Tumor%20Subtype%20Classification%20via%20Fine-Grained%0A%20%20Patch-Text%20Alignment&body=Title%3A%20Enhancing%20Zero-Shot%20Brain%20Tumor%20Subtype%20Classification%20via%20Fine-Grained%0A%20%20Patch-Text%20Alignment%0AAuthor%3A%20Lubin%20Gan%20and%20Jing%20Zhang%20and%20Linhao%20Qu%20and%20Yijun%20Wang%20and%20Siying%20Wu%20and%20Xiaoyan%20Sun%0AAbstract%3A%20%20%20The%20fine-grained%20classification%20of%20brain%20tumor%20subtypes%20from%0Ahistopathological%20whole%20slide%20images%20is%20highly%20challenging%20due%20to%20subtle%0Amorphological%20variations%20and%20the%20scarcity%20of%20annotated%20data.%20Although%0Avision-language%20models%20have%20enabled%20promising%20zero-shot%20classification%2C%20their%0Aability%20to%20capture%20fine-grained%20pathological%20features%20remains%20limited%2C%0Aresulting%20in%20suboptimal%20subtype%20discrimination.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20Fine-Grained%20Patch%20Alignment%20Network%20%28FG-PAN%29%2C%20a%20novel%20zero-shot%0Aframework%20tailored%20for%20digital%20pathology.%20FG-PAN%20consists%20of%20two%20key%20modules%3A%0A%281%29%20a%20local%20feature%20refinement%20module%20that%20enhances%20patch-level%20visual%20features%0Aby%20modeling%20spatial%20relationships%20among%20representative%20patches%2C%20and%20%282%29%20a%0Afine-grained%20text%20description%20generation%20module%20that%20leverages%20large%20language%0Amodels%20to%20produce%20pathology-aware%2C%20class-specific%20semantic%20prototypes.%20By%0Aaligning%20refined%20visual%20features%20with%20LLM-generated%20fine-grained%20descriptions%2C%0AFG-PAN%20effectively%20increases%20class%20separability%20in%20both%20visual%20and%20semantic%0Aspaces.%20Extensive%20experiments%20on%20multiple%20public%20pathology%20datasets%2C%20including%0AEBRAINS%20and%20TCGA%2C%20demonstrate%20that%20FG-PAN%20achieves%20state-of-the-art%20performance%0Aand%20robust%20generalization%20in%20zero-shot%20brain%20tumor%20subtype%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01602v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Zero-Shot%2520Brain%2520Tumor%2520Subtype%2520Classification%2520via%2520Fine-Grained%250A%2520%2520Patch-Text%2520Alignment%26entry.906535625%3DLubin%2520Gan%2520and%2520Jing%2520Zhang%2520and%2520Linhao%2520Qu%2520and%2520Yijun%2520Wang%2520and%2520Siying%2520Wu%2520and%2520Xiaoyan%2520Sun%26entry.1292438233%3D%2520%2520The%2520fine-grained%2520classification%2520of%2520brain%2520tumor%2520subtypes%2520from%250Ahistopathological%2520whole%2520slide%2520images%2520is%2520highly%2520challenging%2520due%2520to%2520subtle%250Amorphological%2520variations%2520and%2520the%2520scarcity%2520of%2520annotated%2520data.%2520Although%250Avision-language%2520models%2520have%2520enabled%2520promising%2520zero-shot%2520classification%252C%2520their%250Aability%2520to%2520capture%2520fine-grained%2520pathological%2520features%2520remains%2520limited%252C%250Aresulting%2520in%2520suboptimal%2520subtype%2520discrimination.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520the%2520Fine-Grained%2520Patch%2520Alignment%2520Network%2520%2528FG-PAN%2529%252C%2520a%2520novel%2520zero-shot%250Aframework%2520tailored%2520for%2520digital%2520pathology.%2520FG-PAN%2520consists%2520of%2520two%2520key%2520modules%253A%250A%25281%2529%2520a%2520local%2520feature%2520refinement%2520module%2520that%2520enhances%2520patch-level%2520visual%2520features%250Aby%2520modeling%2520spatial%2520relationships%2520among%2520representative%2520patches%252C%2520and%2520%25282%2529%2520a%250Afine-grained%2520text%2520description%2520generation%2520module%2520that%2520leverages%2520large%2520language%250Amodels%2520to%2520produce%2520pathology-aware%252C%2520class-specific%2520semantic%2520prototypes.%2520By%250Aaligning%2520refined%2520visual%2520features%2520with%2520LLM-generated%2520fine-grained%2520descriptions%252C%250AFG-PAN%2520effectively%2520increases%2520class%2520separability%2520in%2520both%2520visual%2520and%2520semantic%250Aspaces.%2520Extensive%2520experiments%2520on%2520multiple%2520public%2520pathology%2520datasets%252C%2520including%250AEBRAINS%2520and%2520TCGA%252C%2520demonstrate%2520that%2520FG-PAN%2520achieves%2520state-of-the-art%2520performance%250Aand%2520robust%2520generalization%2520in%2520zero-shot%2520brain%2520tumor%2520subtype%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01602v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Zero-Shot%20Brain%20Tumor%20Subtype%20Classification%20via%20Fine-Grained%0A%20%20Patch-Text%20Alignment&entry.906535625=Lubin%20Gan%20and%20Jing%20Zhang%20and%20Linhao%20Qu%20and%20Yijun%20Wang%20and%20Siying%20Wu%20and%20Xiaoyan%20Sun&entry.1292438233=%20%20The%20fine-grained%20classification%20of%20brain%20tumor%20subtypes%20from%0Ahistopathological%20whole%20slide%20images%20is%20highly%20challenging%20due%20to%20subtle%0Amorphological%20variations%20and%20the%20scarcity%20of%20annotated%20data.%20Although%0Avision-language%20models%20have%20enabled%20promising%20zero-shot%20classification%2C%20their%0Aability%20to%20capture%20fine-grained%20pathological%20features%20remains%20limited%2C%0Aresulting%20in%20suboptimal%20subtype%20discrimination.%20To%20address%20these%20challenges%2C%20we%0Apropose%20the%20Fine-Grained%20Patch%20Alignment%20Network%20%28FG-PAN%29%2C%20a%20novel%20zero-shot%0Aframework%20tailored%20for%20digital%20pathology.%20FG-PAN%20consists%20of%20two%20key%20modules%3A%0A%281%29%20a%20local%20feature%20refinement%20module%20that%20enhances%20patch-level%20visual%20features%0Aby%20modeling%20spatial%20relationships%20among%20representative%20patches%2C%20and%20%282%29%20a%0Afine-grained%20text%20description%20generation%20module%20that%20leverages%20large%20language%0Amodels%20to%20produce%20pathology-aware%2C%20class-specific%20semantic%20prototypes.%20By%0Aaligning%20refined%20visual%20features%20with%20LLM-generated%20fine-grained%20descriptions%2C%0AFG-PAN%20effectively%20increases%20class%20separability%20in%20both%20visual%20and%20semantic%0Aspaces.%20Extensive%20experiments%20on%20multiple%20public%20pathology%20datasets%2C%20including%0AEBRAINS%20and%20TCGA%2C%20demonstrate%20that%20FG-PAN%20achieves%20state-of-the-art%20performance%0Aand%20robust%20generalization%20in%20zero-shot%20brain%20tumor%20subtype%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01602v2&entry.124074799=Read"},
{"title": "Learning Robust Intervention Representations with Delta Embeddings", "author": "Panagiotis Alimisis and Christos Diou", "abstract": "  Causal representation learning has attracted significant research interest\nduring the past few years, as a means for improving model generalization and\nrobustness. Causal representations of interventional image pairs, have the\nproperty that only variables corresponding to scene elements affected by the\nintervention / action are changed between the start state and the end state.\nWhile most work in this area has focused on identifying and representing the\nvariables of the scene under a causal model, fewer efforts have focused on\nrepresentations of the interventions themselves. In this work, we show that an\neffective strategy for improving out of distribution (OOD) robustness is to\nfocus on the representation of interventions in the latent space. Specifically,\nwe propose that an intervention can be represented by a Causal Delta Embedding\nthat is invariant to the visual scene and sparse in terms of the causal\nvariables it affects. Leveraging this insight, we propose a framework that is\ncapable of learning causal representations from image pairs, without any\nadditional supervision. Experiments in the Causal Triplet challenge demonstrate\nthat Causal Delta Embeddings are highly effective in OOD settings,\nsignificantly exceeding baseline performance in both synthetic and real-world\nbenchmarks.\n", "link": "http://arxiv.org/abs/2508.04492v1", "date": "2025-08-06", "relevancy": 2.5923, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Robust%20Intervention%20Representations%20with%20Delta%20Embeddings&body=Title%3A%20Learning%20Robust%20Intervention%20Representations%20with%20Delta%20Embeddings%0AAuthor%3A%20Panagiotis%20Alimisis%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Causal%20representation%20learning%20has%20attracted%20significant%20research%20interest%0Aduring%20the%20past%20few%20years%2C%20as%20a%20means%20for%20improving%20model%20generalization%20and%0Arobustness.%20Causal%20representations%20of%20interventional%20image%20pairs%2C%20have%20the%0Aproperty%20that%20only%20variables%20corresponding%20to%20scene%20elements%20affected%20by%20the%0Aintervention%20/%20action%20are%20changed%20between%20the%20start%20state%20and%20the%20end%20state.%0AWhile%20most%20work%20in%20this%20area%20has%20focused%20on%20identifying%20and%20representing%20the%0Avariables%20of%20the%20scene%20under%20a%20causal%20model%2C%20fewer%20efforts%20have%20focused%20on%0Arepresentations%20of%20the%20interventions%20themselves.%20In%20this%20work%2C%20we%20show%20that%20an%0Aeffective%20strategy%20for%20improving%20out%20of%20distribution%20%28OOD%29%20robustness%20is%20to%0Afocus%20on%20the%20representation%20of%20interventions%20in%20the%20latent%20space.%20Specifically%2C%0Awe%20propose%20that%20an%20intervention%20can%20be%20represented%20by%20a%20Causal%20Delta%20Embedding%0Athat%20is%20invariant%20to%20the%20visual%20scene%20and%20sparse%20in%20terms%20of%20the%20causal%0Avariables%20it%20affects.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20framework%20that%20is%0Acapable%20of%20learning%20causal%20representations%20from%20image%20pairs%2C%20without%20any%0Aadditional%20supervision.%20Experiments%20in%20the%20Causal%20Triplet%20challenge%20demonstrate%0Athat%20Causal%20Delta%20Embeddings%20are%20highly%20effective%20in%20OOD%20settings%2C%0Asignificantly%20exceeding%20baseline%20performance%20in%20both%20synthetic%20and%20real-world%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Robust%2520Intervention%2520Representations%2520with%2520Delta%2520Embeddings%26entry.906535625%3DPanagiotis%2520Alimisis%2520and%2520Christos%2520Diou%26entry.1292438233%3D%2520%2520Causal%2520representation%2520learning%2520has%2520attracted%2520significant%2520research%2520interest%250Aduring%2520the%2520past%2520few%2520years%252C%2520as%2520a%2520means%2520for%2520improving%2520model%2520generalization%2520and%250Arobustness.%2520Causal%2520representations%2520of%2520interventional%2520image%2520pairs%252C%2520have%2520the%250Aproperty%2520that%2520only%2520variables%2520corresponding%2520to%2520scene%2520elements%2520affected%2520by%2520the%250Aintervention%2520/%2520action%2520are%2520changed%2520between%2520the%2520start%2520state%2520and%2520the%2520end%2520state.%250AWhile%2520most%2520work%2520in%2520this%2520area%2520has%2520focused%2520on%2520identifying%2520and%2520representing%2520the%250Avariables%2520of%2520the%2520scene%2520under%2520a%2520causal%2520model%252C%2520fewer%2520efforts%2520have%2520focused%2520on%250Arepresentations%2520of%2520the%2520interventions%2520themselves.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520an%250Aeffective%2520strategy%2520for%2520improving%2520out%2520of%2520distribution%2520%2528OOD%2529%2520robustness%2520is%2520to%250Afocus%2520on%2520the%2520representation%2520of%2520interventions%2520in%2520the%2520latent%2520space.%2520Specifically%252C%250Awe%2520propose%2520that%2520an%2520intervention%2520can%2520be%2520represented%2520by%2520a%2520Causal%2520Delta%2520Embedding%250Athat%2520is%2520invariant%2520to%2520the%2520visual%2520scene%2520and%2520sparse%2520in%2520terms%2520of%2520the%2520causal%250Avariables%2520it%2520affects.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520a%2520framework%2520that%2520is%250Acapable%2520of%2520learning%2520causal%2520representations%2520from%2520image%2520pairs%252C%2520without%2520any%250Aadditional%2520supervision.%2520Experiments%2520in%2520the%2520Causal%2520Triplet%2520challenge%2520demonstrate%250Athat%2520Causal%2520Delta%2520Embeddings%2520are%2520highly%2520effective%2520in%2520OOD%2520settings%252C%250Asignificantly%2520exceeding%2520baseline%2520performance%2520in%2520both%2520synthetic%2520and%2520real-world%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Robust%20Intervention%20Representations%20with%20Delta%20Embeddings&entry.906535625=Panagiotis%20Alimisis%20and%20Christos%20Diou&entry.1292438233=%20%20Causal%20representation%20learning%20has%20attracted%20significant%20research%20interest%0Aduring%20the%20past%20few%20years%2C%20as%20a%20means%20for%20improving%20model%20generalization%20and%0Arobustness.%20Causal%20representations%20of%20interventional%20image%20pairs%2C%20have%20the%0Aproperty%20that%20only%20variables%20corresponding%20to%20scene%20elements%20affected%20by%20the%0Aintervention%20/%20action%20are%20changed%20between%20the%20start%20state%20and%20the%20end%20state.%0AWhile%20most%20work%20in%20this%20area%20has%20focused%20on%20identifying%20and%20representing%20the%0Avariables%20of%20the%20scene%20under%20a%20causal%20model%2C%20fewer%20efforts%20have%20focused%20on%0Arepresentations%20of%20the%20interventions%20themselves.%20In%20this%20work%2C%20we%20show%20that%20an%0Aeffective%20strategy%20for%20improving%20out%20of%20distribution%20%28OOD%29%20robustness%20is%20to%0Afocus%20on%20the%20representation%20of%20interventions%20in%20the%20latent%20space.%20Specifically%2C%0Awe%20propose%20that%20an%20intervention%20can%20be%20represented%20by%20a%20Causal%20Delta%20Embedding%0Athat%20is%20invariant%20to%20the%20visual%20scene%20and%20sparse%20in%20terms%20of%20the%20causal%0Avariables%20it%20affects.%20Leveraging%20this%20insight%2C%20we%20propose%20a%20framework%20that%20is%0Acapable%20of%20learning%20causal%20representations%20from%20image%20pairs%2C%20without%20any%0Aadditional%20supervision.%20Experiments%20in%20the%20Causal%20Triplet%20challenge%20demonstrate%0Athat%20Causal%20Delta%20Embeddings%20are%20highly%20effective%20in%20OOD%20settings%2C%0Asignificantly%20exceeding%20baseline%20performance%20in%20both%20synthetic%20and%20real-world%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04492v1&entry.124074799=Read"},
{"title": "Streaming Generated Gaussian Process Experts for Online Learning and\n  Control", "author": "Zewen Yang and Dongfa Zhang and Xiaobing Dai and Fengyi Yu and Chi Zhang and Bingkun Huang and Hamid Sadeghian and Sami Haddadin", "abstract": "  Gaussian Processes (GPs), as a nonparametric learning method, offer flexible\nmodeling capabilities and calibrated uncertainty quantification for function\napproximations. Additionally, GPs support online learning by efficiently\nincorporating new data with polynomial-time computation, making them\nwell-suited for safety-critical dynamical systems that require rapid\nadaptation. However, the inference and online updates of exact GPs, when\nprocessing streaming data, incur cubic computation time and quadratic storage\nmemory complexity, limiting their scalability to large datasets in real-time\nsettings. In this paper, we propose a streaming kernel-induced progressively\ngenerated expert framework of Gaussian processes (SkyGP) that addresses both\ncomputational and memory constraints by maintaining a bounded set of experts,\nwhile inheriting the learning performance guarantees from exact Gaussian\nprocesses. Furthermore, two SkyGP variants are introduced, each tailored to a\nspecific objective, either maximizing prediction accuracy (SkyGP-Dense) or\nimproving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is\nvalidated through extensive benchmarks and real-time control experiments\ndemonstrating its superior performance compared to state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2508.03679v2", "date": "2025-08-06", "relevancy": 2.5817, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5244}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5219}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streaming%20Generated%20Gaussian%20Process%20Experts%20for%20Online%20Learning%20and%0A%20%20Control&body=Title%3A%20Streaming%20Generated%20Gaussian%20Process%20Experts%20for%20Online%20Learning%20and%0A%20%20Control%0AAuthor%3A%20Zewen%20Yang%20and%20Dongfa%20Zhang%20and%20Xiaobing%20Dai%20and%20Fengyi%20Yu%20and%20Chi%20Zhang%20and%20Bingkun%20Huang%20and%20Hamid%20Sadeghian%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20Gaussian%20Processes%20%28GPs%29%2C%20as%20a%20nonparametric%20learning%20method%2C%20offer%20flexible%0Amodeling%20capabilities%20and%20calibrated%20uncertainty%20quantification%20for%20function%0Aapproximations.%20Additionally%2C%20GPs%20support%20online%20learning%20by%20efficiently%0Aincorporating%20new%20data%20with%20polynomial-time%20computation%2C%20making%20them%0Awell-suited%20for%20safety-critical%20dynamical%20systems%20that%20require%20rapid%0Aadaptation.%20However%2C%20the%20inference%20and%20online%20updates%20of%20exact%20GPs%2C%20when%0Aprocessing%20streaming%20data%2C%20incur%20cubic%20computation%20time%20and%20quadratic%20storage%0Amemory%20complexity%2C%20limiting%20their%20scalability%20to%20large%20datasets%20in%20real-time%0Asettings.%20In%20this%20paper%2C%20we%20propose%20a%20streaming%20kernel-induced%20progressively%0Agenerated%20expert%20framework%20of%20Gaussian%20processes%20%28SkyGP%29%20that%20addresses%20both%0Acomputational%20and%20memory%20constraints%20by%20maintaining%20a%20bounded%20set%20of%20experts%2C%0Awhile%20inheriting%20the%20learning%20performance%20guarantees%20from%20exact%20Gaussian%0Aprocesses.%20Furthermore%2C%20two%20SkyGP%20variants%20are%20introduced%2C%20each%20tailored%20to%20a%0Aspecific%20objective%2C%20either%20maximizing%20prediction%20accuracy%20%28SkyGP-Dense%29%20or%0Aimproving%20computational%20efficiency%20%28SkyGP-Fast%29.%20The%20effectiveness%20of%20SkyGP%20is%0Avalidated%20through%20extensive%20benchmarks%20and%20real-time%20control%20experiments%0Ademonstrating%20its%20superior%20performance%20compared%20to%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreaming%2520Generated%2520Gaussian%2520Process%2520Experts%2520for%2520Online%2520Learning%2520and%250A%2520%2520Control%26entry.906535625%3DZewen%2520Yang%2520and%2520Dongfa%2520Zhang%2520and%2520Xiaobing%2520Dai%2520and%2520Fengyi%2520Yu%2520and%2520Chi%2520Zhang%2520and%2520Bingkun%2520Huang%2520and%2520Hamid%2520Sadeghian%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520Gaussian%2520Processes%2520%2528GPs%2529%252C%2520as%2520a%2520nonparametric%2520learning%2520method%252C%2520offer%2520flexible%250Amodeling%2520capabilities%2520and%2520calibrated%2520uncertainty%2520quantification%2520for%2520function%250Aapproximations.%2520Additionally%252C%2520GPs%2520support%2520online%2520learning%2520by%2520efficiently%250Aincorporating%2520new%2520data%2520with%2520polynomial-time%2520computation%252C%2520making%2520them%250Awell-suited%2520for%2520safety-critical%2520dynamical%2520systems%2520that%2520require%2520rapid%250Aadaptation.%2520However%252C%2520the%2520inference%2520and%2520online%2520updates%2520of%2520exact%2520GPs%252C%2520when%250Aprocessing%2520streaming%2520data%252C%2520incur%2520cubic%2520computation%2520time%2520and%2520quadratic%2520storage%250Amemory%2520complexity%252C%2520limiting%2520their%2520scalability%2520to%2520large%2520datasets%2520in%2520real-time%250Asettings.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520streaming%2520kernel-induced%2520progressively%250Agenerated%2520expert%2520framework%2520of%2520Gaussian%2520processes%2520%2528SkyGP%2529%2520that%2520addresses%2520both%250Acomputational%2520and%2520memory%2520constraints%2520by%2520maintaining%2520a%2520bounded%2520set%2520of%2520experts%252C%250Awhile%2520inheriting%2520the%2520learning%2520performance%2520guarantees%2520from%2520exact%2520Gaussian%250Aprocesses.%2520Furthermore%252C%2520two%2520SkyGP%2520variants%2520are%2520introduced%252C%2520each%2520tailored%2520to%2520a%250Aspecific%2520objective%252C%2520either%2520maximizing%2520prediction%2520accuracy%2520%2528SkyGP-Dense%2529%2520or%250Aimproving%2520computational%2520efficiency%2520%2528SkyGP-Fast%2529.%2520The%2520effectiveness%2520of%2520SkyGP%2520is%250Avalidated%2520through%2520extensive%2520benchmarks%2520and%2520real-time%2520control%2520experiments%250Ademonstrating%2520its%2520superior%2520performance%2520compared%2520to%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streaming%20Generated%20Gaussian%20Process%20Experts%20for%20Online%20Learning%20and%0A%20%20Control&entry.906535625=Zewen%20Yang%20and%20Dongfa%20Zhang%20and%20Xiaobing%20Dai%20and%20Fengyi%20Yu%20and%20Chi%20Zhang%20and%20Bingkun%20Huang%20and%20Hamid%20Sadeghian%20and%20Sami%20Haddadin&entry.1292438233=%20%20Gaussian%20Processes%20%28GPs%29%2C%20as%20a%20nonparametric%20learning%20method%2C%20offer%20flexible%0Amodeling%20capabilities%20and%20calibrated%20uncertainty%20quantification%20for%20function%0Aapproximations.%20Additionally%2C%20GPs%20support%20online%20learning%20by%20efficiently%0Aincorporating%20new%20data%20with%20polynomial-time%20computation%2C%20making%20them%0Awell-suited%20for%20safety-critical%20dynamical%20systems%20that%20require%20rapid%0Aadaptation.%20However%2C%20the%20inference%20and%20online%20updates%20of%20exact%20GPs%2C%20when%0Aprocessing%20streaming%20data%2C%20incur%20cubic%20computation%20time%20and%20quadratic%20storage%0Amemory%20complexity%2C%20limiting%20their%20scalability%20to%20large%20datasets%20in%20real-time%0Asettings.%20In%20this%20paper%2C%20we%20propose%20a%20streaming%20kernel-induced%20progressively%0Agenerated%20expert%20framework%20of%20Gaussian%20processes%20%28SkyGP%29%20that%20addresses%20both%0Acomputational%20and%20memory%20constraints%20by%20maintaining%20a%20bounded%20set%20of%20experts%2C%0Awhile%20inheriting%20the%20learning%20performance%20guarantees%20from%20exact%20Gaussian%0Aprocesses.%20Furthermore%2C%20two%20SkyGP%20variants%20are%20introduced%2C%20each%20tailored%20to%20a%0Aspecific%20objective%2C%20either%20maximizing%20prediction%20accuracy%20%28SkyGP-Dense%29%20or%0Aimproving%20computational%20efficiency%20%28SkyGP-Fast%29.%20The%20effectiveness%20of%20SkyGP%20is%0Avalidated%20through%20extensive%20benchmarks%20and%20real-time%20control%20experiments%0Ademonstrating%20its%20superior%20performance%20compared%20to%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03679v2&entry.124074799=Read"},
{"title": "Augmentation-based Domain Generalization and Joint Training from\n  Multiple Source Domains for Whole Heart Segmentation", "author": "Franz Thaler and Darko Stern and Gernot Plank and Martin Urschler", "abstract": "  As the leading cause of death worldwide, cardiovascular diseases motivate the\ndevelopment of more sophisticated methods to analyze the heart and its\nsubstructures from medical images like Computed Tomography (CT) and Magnetic\nResonance (MR). Semantic segmentations of important cardiac structures that\nrepresent the whole heart are useful to assess patient-specific cardiac\nmorphology and pathology. Furthermore, accurate semantic segmentations can be\nused to generate cardiac digital twin models which allows e.g.\nelectrophysiological simulation and personalized therapy planning. Even though\ndeep learning-based methods for medical image segmentation achieved great\nadvancements over the last decade, retaining good performance under domain\nshift -- i.e. when training and test data are sampled from different data\ndistributions -- remains challenging. In order to perform well on domains known\nat training-time, we employ a (1) balanced joint training approach that\nutilizes CT and MR data in equal amounts from different source domains.\nFurther, aiming to alleviate domain shift towards domains only encountered at\ntest-time, we rely on (2) strong intensity and spatial augmentation techniques\nto greatly diversify the available training data. Our proposed whole heart\nsegmentation method, a 5-fold ensemble with our contributions, achieves the\nbest performance for MR data overall and a performance similar to the best\nperformance for CT data when compared to a model trained solely on CT. With\n93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR\ndata, our method demonstrates great potential to efficiently obtain accurate\nsemantic segmentations from which patient-specific cardiac twin models can be\ngenerated.\n", "link": "http://arxiv.org/abs/2508.04552v1", "date": "2025-08-06", "relevancy": 2.5787, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.509}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmentation-based%20Domain%20Generalization%20and%20Joint%20Training%20from%0A%20%20Multiple%20Source%20Domains%20for%20Whole%20Heart%20Segmentation&body=Title%3A%20Augmentation-based%20Domain%20Generalization%20and%20Joint%20Training%20from%0A%20%20Multiple%20Source%20Domains%20for%20Whole%20Heart%20Segmentation%0AAuthor%3A%20Franz%20Thaler%20and%20Darko%20Stern%20and%20Gernot%20Plank%20and%20Martin%20Urschler%0AAbstract%3A%20%20%20As%20the%20leading%20cause%20of%20death%20worldwide%2C%20cardiovascular%20diseases%20motivate%20the%0Adevelopment%20of%20more%20sophisticated%20methods%20to%20analyze%20the%20heart%20and%20its%0Asubstructures%20from%20medical%20images%20like%20Computed%20Tomography%20%28CT%29%20and%20Magnetic%0AResonance%20%28MR%29.%20Semantic%20segmentations%20of%20important%20cardiac%20structures%20that%0Arepresent%20the%20whole%20heart%20are%20useful%20to%20assess%20patient-specific%20cardiac%0Amorphology%20and%20pathology.%20Furthermore%2C%20accurate%20semantic%20segmentations%20can%20be%0Aused%20to%20generate%20cardiac%20digital%20twin%20models%20which%20allows%20e.g.%0Aelectrophysiological%20simulation%20and%20personalized%20therapy%20planning.%20Even%20though%0Adeep%20learning-based%20methods%20for%20medical%20image%20segmentation%20achieved%20great%0Aadvancements%20over%20the%20last%20decade%2C%20retaining%20good%20performance%20under%20domain%0Ashift%20--%20i.e.%20when%20training%20and%20test%20data%20are%20sampled%20from%20different%20data%0Adistributions%20--%20remains%20challenging.%20In%20order%20to%20perform%20well%20on%20domains%20known%0Aat%20training-time%2C%20we%20employ%20a%20%281%29%20balanced%20joint%20training%20approach%20that%0Autilizes%20CT%20and%20MR%20data%20in%20equal%20amounts%20from%20different%20source%20domains.%0AFurther%2C%20aiming%20to%20alleviate%20domain%20shift%20towards%20domains%20only%20encountered%20at%0Atest-time%2C%20we%20rely%20on%20%282%29%20strong%20intensity%20and%20spatial%20augmentation%20techniques%0Ato%20greatly%20diversify%20the%20available%20training%20data.%20Our%20proposed%20whole%20heart%0Asegmentation%20method%2C%20a%205-fold%20ensemble%20with%20our%20contributions%2C%20achieves%20the%0Abest%20performance%20for%20MR%20data%20overall%20and%20a%20performance%20similar%20to%20the%20best%0Aperformance%20for%20CT%20data%20when%20compared%20to%20a%20model%20trained%20solely%20on%20CT.%20With%0A93.33%25%20DSC%20and%200.8388%20mm%20ASSD%20for%20CT%20and%2089.30%25%20DSC%20and%201.2411%20mm%20ASSD%20for%20MR%0Adata%2C%20our%20method%20demonstrates%20great%20potential%20to%20efficiently%20obtain%20accurate%0Asemantic%20segmentations%20from%20which%20patient-specific%20cardiac%20twin%20models%20can%20be%0Agenerated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmentation-based%2520Domain%2520Generalization%2520and%2520Joint%2520Training%2520from%250A%2520%2520Multiple%2520Source%2520Domains%2520for%2520Whole%2520Heart%2520Segmentation%26entry.906535625%3DFranz%2520Thaler%2520and%2520Darko%2520Stern%2520and%2520Gernot%2520Plank%2520and%2520Martin%2520Urschler%26entry.1292438233%3D%2520%2520As%2520the%2520leading%2520cause%2520of%2520death%2520worldwide%252C%2520cardiovascular%2520diseases%2520motivate%2520the%250Adevelopment%2520of%2520more%2520sophisticated%2520methods%2520to%2520analyze%2520the%2520heart%2520and%2520its%250Asubstructures%2520from%2520medical%2520images%2520like%2520Computed%2520Tomography%2520%2528CT%2529%2520and%2520Magnetic%250AResonance%2520%2528MR%2529.%2520Semantic%2520segmentations%2520of%2520important%2520cardiac%2520structures%2520that%250Arepresent%2520the%2520whole%2520heart%2520are%2520useful%2520to%2520assess%2520patient-specific%2520cardiac%250Amorphology%2520and%2520pathology.%2520Furthermore%252C%2520accurate%2520semantic%2520segmentations%2520can%2520be%250Aused%2520to%2520generate%2520cardiac%2520digital%2520twin%2520models%2520which%2520allows%2520e.g.%250Aelectrophysiological%2520simulation%2520and%2520personalized%2520therapy%2520planning.%2520Even%2520though%250Adeep%2520learning-based%2520methods%2520for%2520medical%2520image%2520segmentation%2520achieved%2520great%250Aadvancements%2520over%2520the%2520last%2520decade%252C%2520retaining%2520good%2520performance%2520under%2520domain%250Ashift%2520--%2520i.e.%2520when%2520training%2520and%2520test%2520data%2520are%2520sampled%2520from%2520different%2520data%250Adistributions%2520--%2520remains%2520challenging.%2520In%2520order%2520to%2520perform%2520well%2520on%2520domains%2520known%250Aat%2520training-time%252C%2520we%2520employ%2520a%2520%25281%2529%2520balanced%2520joint%2520training%2520approach%2520that%250Autilizes%2520CT%2520and%2520MR%2520data%2520in%2520equal%2520amounts%2520from%2520different%2520source%2520domains.%250AFurther%252C%2520aiming%2520to%2520alleviate%2520domain%2520shift%2520towards%2520domains%2520only%2520encountered%2520at%250Atest-time%252C%2520we%2520rely%2520on%2520%25282%2529%2520strong%2520intensity%2520and%2520spatial%2520augmentation%2520techniques%250Ato%2520greatly%2520diversify%2520the%2520available%2520training%2520data.%2520Our%2520proposed%2520whole%2520heart%250Asegmentation%2520method%252C%2520a%25205-fold%2520ensemble%2520with%2520our%2520contributions%252C%2520achieves%2520the%250Abest%2520performance%2520for%2520MR%2520data%2520overall%2520and%2520a%2520performance%2520similar%2520to%2520the%2520best%250Aperformance%2520for%2520CT%2520data%2520when%2520compared%2520to%2520a%2520model%2520trained%2520solely%2520on%2520CT.%2520With%250A93.33%2525%2520DSC%2520and%25200.8388%2520mm%2520ASSD%2520for%2520CT%2520and%252089.30%2525%2520DSC%2520and%25201.2411%2520mm%2520ASSD%2520for%2520MR%250Adata%252C%2520our%2520method%2520demonstrates%2520great%2520potential%2520to%2520efficiently%2520obtain%2520accurate%250Asemantic%2520segmentations%2520from%2520which%2520patient-specific%2520cardiac%2520twin%2520models%2520can%2520be%250Agenerated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmentation-based%20Domain%20Generalization%20and%20Joint%20Training%20from%0A%20%20Multiple%20Source%20Domains%20for%20Whole%20Heart%20Segmentation&entry.906535625=Franz%20Thaler%20and%20Darko%20Stern%20and%20Gernot%20Plank%20and%20Martin%20Urschler&entry.1292438233=%20%20As%20the%20leading%20cause%20of%20death%20worldwide%2C%20cardiovascular%20diseases%20motivate%20the%0Adevelopment%20of%20more%20sophisticated%20methods%20to%20analyze%20the%20heart%20and%20its%0Asubstructures%20from%20medical%20images%20like%20Computed%20Tomography%20%28CT%29%20and%20Magnetic%0AResonance%20%28MR%29.%20Semantic%20segmentations%20of%20important%20cardiac%20structures%20that%0Arepresent%20the%20whole%20heart%20are%20useful%20to%20assess%20patient-specific%20cardiac%0Amorphology%20and%20pathology.%20Furthermore%2C%20accurate%20semantic%20segmentations%20can%20be%0Aused%20to%20generate%20cardiac%20digital%20twin%20models%20which%20allows%20e.g.%0Aelectrophysiological%20simulation%20and%20personalized%20therapy%20planning.%20Even%20though%0Adeep%20learning-based%20methods%20for%20medical%20image%20segmentation%20achieved%20great%0Aadvancements%20over%20the%20last%20decade%2C%20retaining%20good%20performance%20under%20domain%0Ashift%20--%20i.e.%20when%20training%20and%20test%20data%20are%20sampled%20from%20different%20data%0Adistributions%20--%20remains%20challenging.%20In%20order%20to%20perform%20well%20on%20domains%20known%0Aat%20training-time%2C%20we%20employ%20a%20%281%29%20balanced%20joint%20training%20approach%20that%0Autilizes%20CT%20and%20MR%20data%20in%20equal%20amounts%20from%20different%20source%20domains.%0AFurther%2C%20aiming%20to%20alleviate%20domain%20shift%20towards%20domains%20only%20encountered%20at%0Atest-time%2C%20we%20rely%20on%20%282%29%20strong%20intensity%20and%20spatial%20augmentation%20techniques%0Ato%20greatly%20diversify%20the%20available%20training%20data.%20Our%20proposed%20whole%20heart%0Asegmentation%20method%2C%20a%205-fold%20ensemble%20with%20our%20contributions%2C%20achieves%20the%0Abest%20performance%20for%20MR%20data%20overall%20and%20a%20performance%20similar%20to%20the%20best%0Aperformance%20for%20CT%20data%20when%20compared%20to%20a%20model%20trained%20solely%20on%20CT.%20With%0A93.33%25%20DSC%20and%200.8388%20mm%20ASSD%20for%20CT%20and%2089.30%25%20DSC%20and%201.2411%20mm%20ASSD%20for%20MR%0Adata%2C%20our%20method%20demonstrates%20great%20potential%20to%20efficiently%20obtain%20accurate%0Asemantic%20segmentations%20from%20which%20patient-specific%20cardiac%20twin%20models%20can%20be%0Agenerated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04552v1&entry.124074799=Read"},
{"title": "A Scalable Pretraining Framework for Link Prediction with Efficient\n  Adaptation", "author": "Yu Song and Zhigang Hua and Harry Shomer and Yan Xie and Jingzhe Liu and Bo Long and Hui Liu", "abstract": "  Link Prediction (LP) is a critical task in graph machine learning. While\nGraph Neural Networks (GNNs) have significantly advanced LP performance\nrecently, existing methods face key challenges including limited supervision\nfrom sparse connectivity, sensitivity to initialization, and poor\ngeneralization under distribution shifts. We explore pretraining as a solution\nto address these challenges. Unlike node classification, LP is inherently a\npairwise task, which requires the integration of both node- and edge-level\ninformation. In this work, we present the first systematic study on the\ntransferability of these distinct modules and propose a late fusion strategy to\neffectively combine their outputs for improved performance. To handle the\ndiversity of pretraining data and avoid negative transfer, we introduce a\nMixture-of-Experts (MoE) framework that captures distinct patterns in separate\nexperts, facilitating seamless application of the pretrained model on diverse\ndownstream datasets. For fast adaptation, we develop a parameter-efficient\ntuning strategy that allows the pretrained model to adapt to unseen datasets\nwith minimal computational overhead. Experiments on 16 datasets across two\ndomains demonstrate the effectiveness of our approach, achieving\nstate-of-the-art performance on low-resource link prediction while obtaining\ncompetitive results compared to end-to-end trained methods, with over 10,000x\nlower computational overhead.\n", "link": "http://arxiv.org/abs/2508.04645v1", "date": "2025-08-06", "relevancy": 2.572, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5244}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Pretraining%20Framework%20for%20Link%20Prediction%20with%20Efficient%0A%20%20Adaptation&body=Title%3A%20A%20Scalable%20Pretraining%20Framework%20for%20Link%20Prediction%20with%20Efficient%0A%20%20Adaptation%0AAuthor%3A%20Yu%20Song%20and%20Zhigang%20Hua%20and%20Harry%20Shomer%20and%20Yan%20Xie%20and%20Jingzhe%20Liu%20and%20Bo%20Long%20and%20Hui%20Liu%0AAbstract%3A%20%20%20Link%20Prediction%20%28LP%29%20is%20a%20critical%20task%20in%20graph%20machine%20learning.%20While%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20significantly%20advanced%20LP%20performance%0Arecently%2C%20existing%20methods%20face%20key%20challenges%20including%20limited%20supervision%0Afrom%20sparse%20connectivity%2C%20sensitivity%20to%20initialization%2C%20and%20poor%0Ageneralization%20under%20distribution%20shifts.%20We%20explore%20pretraining%20as%20a%20solution%0Ato%20address%20these%20challenges.%20Unlike%20node%20classification%2C%20LP%20is%20inherently%20a%0Apairwise%20task%2C%20which%20requires%20the%20integration%20of%20both%20node-%20and%20edge-level%0Ainformation.%20In%20this%20work%2C%20we%20present%20the%20first%20systematic%20study%20on%20the%0Atransferability%20of%20these%20distinct%20modules%20and%20propose%20a%20late%20fusion%20strategy%20to%0Aeffectively%20combine%20their%20outputs%20for%20improved%20performance.%20To%20handle%20the%0Adiversity%20of%20pretraining%20data%20and%20avoid%20negative%20transfer%2C%20we%20introduce%20a%0AMixture-of-Experts%20%28MoE%29%20framework%20that%20captures%20distinct%20patterns%20in%20separate%0Aexperts%2C%20facilitating%20seamless%20application%20of%20the%20pretrained%20model%20on%20diverse%0Adownstream%20datasets.%20For%20fast%20adaptation%2C%20we%20develop%20a%20parameter-efficient%0Atuning%20strategy%20that%20allows%20the%20pretrained%20model%20to%20adapt%20to%20unseen%20datasets%0Awith%20minimal%20computational%20overhead.%20Experiments%20on%2016%20datasets%20across%20two%0Adomains%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0Astate-of-the-art%20performance%20on%20low-resource%20link%20prediction%20while%20obtaining%0Acompetitive%20results%20compared%20to%20end-to-end%20trained%20methods%2C%20with%20over%2010%2C000x%0Alower%20computational%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Pretraining%2520Framework%2520for%2520Link%2520Prediction%2520with%2520Efficient%250A%2520%2520Adaptation%26entry.906535625%3DYu%2520Song%2520and%2520Zhigang%2520Hua%2520and%2520Harry%2520Shomer%2520and%2520Yan%2520Xie%2520and%2520Jingzhe%2520Liu%2520and%2520Bo%2520Long%2520and%2520Hui%2520Liu%26entry.1292438233%3D%2520%2520Link%2520Prediction%2520%2528LP%2529%2520is%2520a%2520critical%2520task%2520in%2520graph%2520machine%2520learning.%2520While%250AGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520significantly%2520advanced%2520LP%2520performance%250Arecently%252C%2520existing%2520methods%2520face%2520key%2520challenges%2520including%2520limited%2520supervision%250Afrom%2520sparse%2520connectivity%252C%2520sensitivity%2520to%2520initialization%252C%2520and%2520poor%250Ageneralization%2520under%2520distribution%2520shifts.%2520We%2520explore%2520pretraining%2520as%2520a%2520solution%250Ato%2520address%2520these%2520challenges.%2520Unlike%2520node%2520classification%252C%2520LP%2520is%2520inherently%2520a%250Apairwise%2520task%252C%2520which%2520requires%2520the%2520integration%2520of%2520both%2520node-%2520and%2520edge-level%250Ainformation.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520systematic%2520study%2520on%2520the%250Atransferability%2520of%2520these%2520distinct%2520modules%2520and%2520propose%2520a%2520late%2520fusion%2520strategy%2520to%250Aeffectively%2520combine%2520their%2520outputs%2520for%2520improved%2520performance.%2520To%2520handle%2520the%250Adiversity%2520of%2520pretraining%2520data%2520and%2520avoid%2520negative%2520transfer%252C%2520we%2520introduce%2520a%250AMixture-of-Experts%2520%2528MoE%2529%2520framework%2520that%2520captures%2520distinct%2520patterns%2520in%2520separate%250Aexperts%252C%2520facilitating%2520seamless%2520application%2520of%2520the%2520pretrained%2520model%2520on%2520diverse%250Adownstream%2520datasets.%2520For%2520fast%2520adaptation%252C%2520we%2520develop%2520a%2520parameter-efficient%250Atuning%2520strategy%2520that%2520allows%2520the%2520pretrained%2520model%2520to%2520adapt%2520to%2520unseen%2520datasets%250Awith%2520minimal%2520computational%2520overhead.%2520Experiments%2520on%252016%2520datasets%2520across%2520two%250Adomains%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%252C%2520achieving%250Astate-of-the-art%2520performance%2520on%2520low-resource%2520link%2520prediction%2520while%2520obtaining%250Acompetitive%2520results%2520compared%2520to%2520end-to-end%2520trained%2520methods%252C%2520with%2520over%252010%252C000x%250Alower%2520computational%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Pretraining%20Framework%20for%20Link%20Prediction%20with%20Efficient%0A%20%20Adaptation&entry.906535625=Yu%20Song%20and%20Zhigang%20Hua%20and%20Harry%20Shomer%20and%20Yan%20Xie%20and%20Jingzhe%20Liu%20and%20Bo%20Long%20and%20Hui%20Liu&entry.1292438233=%20%20Link%20Prediction%20%28LP%29%20is%20a%20critical%20task%20in%20graph%20machine%20learning.%20While%0AGraph%20Neural%20Networks%20%28GNNs%29%20have%20significantly%20advanced%20LP%20performance%0Arecently%2C%20existing%20methods%20face%20key%20challenges%20including%20limited%20supervision%0Afrom%20sparse%20connectivity%2C%20sensitivity%20to%20initialization%2C%20and%20poor%0Ageneralization%20under%20distribution%20shifts.%20We%20explore%20pretraining%20as%20a%20solution%0Ato%20address%20these%20challenges.%20Unlike%20node%20classification%2C%20LP%20is%20inherently%20a%0Apairwise%20task%2C%20which%20requires%20the%20integration%20of%20both%20node-%20and%20edge-level%0Ainformation.%20In%20this%20work%2C%20we%20present%20the%20first%20systematic%20study%20on%20the%0Atransferability%20of%20these%20distinct%20modules%20and%20propose%20a%20late%20fusion%20strategy%20to%0Aeffectively%20combine%20their%20outputs%20for%20improved%20performance.%20To%20handle%20the%0Adiversity%20of%20pretraining%20data%20and%20avoid%20negative%20transfer%2C%20we%20introduce%20a%0AMixture-of-Experts%20%28MoE%29%20framework%20that%20captures%20distinct%20patterns%20in%20separate%0Aexperts%2C%20facilitating%20seamless%20application%20of%20the%20pretrained%20model%20on%20diverse%0Adownstream%20datasets.%20For%20fast%20adaptation%2C%20we%20develop%20a%20parameter-efficient%0Atuning%20strategy%20that%20allows%20the%20pretrained%20model%20to%20adapt%20to%20unseen%20datasets%0Awith%20minimal%20computational%20overhead.%20Experiments%20on%2016%20datasets%20across%20two%0Adomains%20demonstrate%20the%20effectiveness%20of%20our%20approach%2C%20achieving%0Astate-of-the-art%20performance%20on%20low-resource%20link%20prediction%20while%20obtaining%0Acompetitive%20results%20compared%20to%20end-to-end%20trained%20methods%2C%20with%20over%2010%2C000x%0Alower%20computational%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04645v1&entry.124074799=Read"},
{"title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled\n  Instruction Synthesis", "author": "Feifan Song and Bofei Gao and Yifan Song and Yi Liu and Weimin Xiong and Yuyang Song and Tianyu Liu and Guoyin Wang and Houfeng Wang", "abstract": "  Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.\n", "link": "http://arxiv.org/abs/2508.04626v1", "date": "2025-08-06", "relevancy": 2.5654, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P-Aligner%3A%20Enabling%20Pre-Alignment%20of%20Language%20Models%20via%20Principled%0A%20%20Instruction%20Synthesis&body=Title%3A%20P-Aligner%3A%20Enabling%20Pre-Alignment%20of%20Language%20Models%20via%20Principled%0A%20%20Instruction%20Synthesis%0AAuthor%3A%20Feifan%20Song%20and%20Bofei%20Gao%20and%20Yifan%20Song%20and%20Yi%20Liu%20and%20Weimin%20Xiong%20and%20Yuyang%20Song%20and%20Tianyu%20Liu%20and%20Guoyin%20Wang%20and%20Houfeng%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20expected%20to%20produce%20safe%2C%20helpful%2C%20and%0Ahonest%20content%20during%20interaction%20with%20human%20users%2C%20but%20they%20frequently%20fail%20to%0Aalign%20with%20such%20values%20when%20given%20flawed%20instructions%2C%20e.g.%2C%20missing%20context%2C%0Aambiguous%20directives%2C%20or%20inappropriate%20tone%2C%20leaving%20substantial%20room%20for%0Aimprovement%20along%20multiple%20dimensions.%20A%20cost-effective%20yet%20high-impact%20way%20is%0Ato%20pre-align%20instructions%20before%20the%20model%20begins%20decoding.%20Existing%20approaches%0Aeither%20rely%20on%20prohibitive%20test-time%20search%20costs%20or%20end-to-end%20model%20rewrite%2C%0Awhich%20is%20powered%20by%20a%20customized%20training%20corpus%20with%20unclear%20objectives.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20the%20goal%20of%20efficient%20and%20effective%20preference%0Aalignment%20can%20be%20achieved%20by%20P-Aligner%2C%20a%20lightweight%20module%20generating%0Ainstructions%20that%20preserve%20the%20original%20intents%20while%20being%20expressed%20in%20a%20more%0Ahuman-preferred%20form.%20P-Aligner%20is%20trained%20on%20UltraPrompt%2C%20a%20new%20dataset%0Asynthesized%20via%20a%20proposed%20principle-guided%20pipeline%20using%20Monte-Carlo%20Tree%0ASearch%2C%20which%20systematically%20explores%20the%20space%20of%20candidate%20instructions%20that%0Aare%20closely%20tied%20to%20human%20preference.%20Experiments%20across%20different%20methods%20show%0Athat%20P-Aligner%20generally%20outperforms%20strong%20baselines%20across%20various%20models%20and%0Abenchmarks%2C%20including%20average%20win-rate%20gains%20of%2028.35%25%20and%208.69%25%20on%20GPT-4-turbo%0Aand%20Gemma-2-SimPO%2C%20respectively.%20Further%20analyses%20validate%20its%20effectiveness%0Aand%20efficiency%20through%20multiple%20perspectives%2C%20including%20data%20quality%2C%20search%0Astrategies%2C%20iterative%20deployment%2C%20and%20time%20overhead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP-Aligner%253A%2520Enabling%2520Pre-Alignment%2520of%2520Language%2520Models%2520via%2520Principled%250A%2520%2520Instruction%2520Synthesis%26entry.906535625%3DFeifan%2520Song%2520and%2520Bofei%2520Gao%2520and%2520Yifan%2520Song%2520and%2520Yi%2520Liu%2520and%2520Weimin%2520Xiong%2520and%2520Yuyang%2520Song%2520and%2520Tianyu%2520Liu%2520and%2520Guoyin%2520Wang%2520and%2520Houfeng%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520expected%2520to%2520produce%2520safe%252C%2520helpful%252C%2520and%250Ahonest%2520content%2520during%2520interaction%2520with%2520human%2520users%252C%2520but%2520they%2520frequently%2520fail%2520to%250Aalign%2520with%2520such%2520values%2520when%2520given%2520flawed%2520instructions%252C%2520e.g.%252C%2520missing%2520context%252C%250Aambiguous%2520directives%252C%2520or%2520inappropriate%2520tone%252C%2520leaving%2520substantial%2520room%2520for%250Aimprovement%2520along%2520multiple%2520dimensions.%2520A%2520cost-effective%2520yet%2520high-impact%2520way%2520is%250Ato%2520pre-align%2520instructions%2520before%2520the%2520model%2520begins%2520decoding.%2520Existing%2520approaches%250Aeither%2520rely%2520on%2520prohibitive%2520test-time%2520search%2520costs%2520or%2520end-to-end%2520model%2520rewrite%252C%250Awhich%2520is%2520powered%2520by%2520a%2520customized%2520training%2520corpus%2520with%2520unclear%2520objectives.%2520In%250Athis%2520work%252C%2520we%2520demonstrate%2520that%2520the%2520goal%2520of%2520efficient%2520and%2520effective%2520preference%250Aalignment%2520can%2520be%2520achieved%2520by%2520P-Aligner%252C%2520a%2520lightweight%2520module%2520generating%250Ainstructions%2520that%2520preserve%2520the%2520original%2520intents%2520while%2520being%2520expressed%2520in%2520a%2520more%250Ahuman-preferred%2520form.%2520P-Aligner%2520is%2520trained%2520on%2520UltraPrompt%252C%2520a%2520new%2520dataset%250Asynthesized%2520via%2520a%2520proposed%2520principle-guided%2520pipeline%2520using%2520Monte-Carlo%2520Tree%250ASearch%252C%2520which%2520systematically%2520explores%2520the%2520space%2520of%2520candidate%2520instructions%2520that%250Aare%2520closely%2520tied%2520to%2520human%2520preference.%2520Experiments%2520across%2520different%2520methods%2520show%250Athat%2520P-Aligner%2520generally%2520outperforms%2520strong%2520baselines%2520across%2520various%2520models%2520and%250Abenchmarks%252C%2520including%2520average%2520win-rate%2520gains%2520of%252028.35%2525%2520and%25208.69%2525%2520on%2520GPT-4-turbo%250Aand%2520Gemma-2-SimPO%252C%2520respectively.%2520Further%2520analyses%2520validate%2520its%2520effectiveness%250Aand%2520efficiency%2520through%2520multiple%2520perspectives%252C%2520including%2520data%2520quality%252C%2520search%250Astrategies%252C%2520iterative%2520deployment%252C%2520and%2520time%2520overhead.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P-Aligner%3A%20Enabling%20Pre-Alignment%20of%20Language%20Models%20via%20Principled%0A%20%20Instruction%20Synthesis&entry.906535625=Feifan%20Song%20and%20Bofei%20Gao%20and%20Yifan%20Song%20and%20Yi%20Liu%20and%20Weimin%20Xiong%20and%20Yuyang%20Song%20and%20Tianyu%20Liu%20and%20Guoyin%20Wang%20and%20Houfeng%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20expected%20to%20produce%20safe%2C%20helpful%2C%20and%0Ahonest%20content%20during%20interaction%20with%20human%20users%2C%20but%20they%20frequently%20fail%20to%0Aalign%20with%20such%20values%20when%20given%20flawed%20instructions%2C%20e.g.%2C%20missing%20context%2C%0Aambiguous%20directives%2C%20or%20inappropriate%20tone%2C%20leaving%20substantial%20room%20for%0Aimprovement%20along%20multiple%20dimensions.%20A%20cost-effective%20yet%20high-impact%20way%20is%0Ato%20pre-align%20instructions%20before%20the%20model%20begins%20decoding.%20Existing%20approaches%0Aeither%20rely%20on%20prohibitive%20test-time%20search%20costs%20or%20end-to-end%20model%20rewrite%2C%0Awhich%20is%20powered%20by%20a%20customized%20training%20corpus%20with%20unclear%20objectives.%20In%0Athis%20work%2C%20we%20demonstrate%20that%20the%20goal%20of%20efficient%20and%20effective%20preference%0Aalignment%20can%20be%20achieved%20by%20P-Aligner%2C%20a%20lightweight%20module%20generating%0Ainstructions%20that%20preserve%20the%20original%20intents%20while%20being%20expressed%20in%20a%20more%0Ahuman-preferred%20form.%20P-Aligner%20is%20trained%20on%20UltraPrompt%2C%20a%20new%20dataset%0Asynthesized%20via%20a%20proposed%20principle-guided%20pipeline%20using%20Monte-Carlo%20Tree%0ASearch%2C%20which%20systematically%20explores%20the%20space%20of%20candidate%20instructions%20that%0Aare%20closely%20tied%20to%20human%20preference.%20Experiments%20across%20different%20methods%20show%0Athat%20P-Aligner%20generally%20outperforms%20strong%20baselines%20across%20various%20models%20and%0Abenchmarks%2C%20including%20average%20win-rate%20gains%20of%2028.35%25%20and%208.69%25%20on%20GPT-4-turbo%0Aand%20Gemma-2-SimPO%2C%20respectively.%20Further%20analyses%20validate%20its%20effectiveness%0Aand%20efficiency%20through%20multiple%20perspectives%2C%20including%20data%20quality%2C%20search%0Astrategies%2C%20iterative%20deployment%2C%20and%20time%20overhead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04626v1&entry.124074799=Read"},
{"title": "Super Resolved Imaging with Adaptive Optics", "author": "Robin Swanson and Esther Y. H. Lin and Masen Lamb and Suresh Sivanandam and Kiriakos N. Kutulakos", "abstract": "  Astronomical telescopes suffer from a tradeoff between field of view (FoV)\nand image resolution: increasing the FoV leads to an optical field that is\nunder-sampled by the science camera. This work presents a novel computational\nimaging approach to overcome this tradeoff by leveraging the existing adaptive\noptics (AO) systems in modern ground-based telescopes. Our key idea is to use\nthe AO system's deformable mirror to apply a series of learned, precisely\ncontrolled distortions to the optical wavefront, producing a sequence of images\nthat exhibit distinct, high-frequency, sub-pixel shifts. These images can then\nbe jointly upsampled to yield the final super-resolved image. Crucially, we\nshow this can be done while simultaneously maintaining the core AO\noperation--correcting for the unknown and rapidly changing wavefront\ndistortions caused by Earth's atmosphere. To achieve this, we incorporate\nend-to-end optimization of both the induced mirror distortions and the\nupsampling algorithm, such that telescope-specific optics and temporal\nstatistics of atmospheric wavefront distortions are accounted for. Our\nexperimental results with a hardware prototype, as well as simulations,\ndemonstrate significant SNR improvements of up to 12 dB over non-AO\nsuper-resolution baselines, using only existing telescope optics and no\nhardware modifications. Moreover, by using a precise bench-top replica of a\ncomplete telescope and AO system, we show that our methodology can be readily\ntransferred to an operational telescope. Project webpage:\nhttps://www.cs.toronto.edu/~robin/aosr/\n", "link": "http://arxiv.org/abs/2508.04648v1", "date": "2025-08-06", "relevancy": 2.5653, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5223}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5084}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Super%20Resolved%20Imaging%20with%20Adaptive%20Optics&body=Title%3A%20Super%20Resolved%20Imaging%20with%20Adaptive%20Optics%0AAuthor%3A%20Robin%20Swanson%20and%20Esther%20Y.%20H.%20Lin%20and%20Masen%20Lamb%20and%20Suresh%20Sivanandam%20and%20Kiriakos%20N.%20Kutulakos%0AAbstract%3A%20%20%20Astronomical%20telescopes%20suffer%20from%20a%20tradeoff%20between%20field%20of%20view%20%28FoV%29%0Aand%20image%20resolution%3A%20increasing%20the%20FoV%20leads%20to%20an%20optical%20field%20that%20is%0Aunder-sampled%20by%20the%20science%20camera.%20This%20work%20presents%20a%20novel%20computational%0Aimaging%20approach%20to%20overcome%20this%20tradeoff%20by%20leveraging%20the%20existing%20adaptive%0Aoptics%20%28AO%29%20systems%20in%20modern%20ground-based%20telescopes.%20Our%20key%20idea%20is%20to%20use%0Athe%20AO%20system%27s%20deformable%20mirror%20to%20apply%20a%20series%20of%20learned%2C%20precisely%0Acontrolled%20distortions%20to%20the%20optical%20wavefront%2C%20producing%20a%20sequence%20of%20images%0Athat%20exhibit%20distinct%2C%20high-frequency%2C%20sub-pixel%20shifts.%20These%20images%20can%20then%0Abe%20jointly%20upsampled%20to%20yield%20the%20final%20super-resolved%20image.%20Crucially%2C%20we%0Ashow%20this%20can%20be%20done%20while%20simultaneously%20maintaining%20the%20core%20AO%0Aoperation--correcting%20for%20the%20unknown%20and%20rapidly%20changing%20wavefront%0Adistortions%20caused%20by%20Earth%27s%20atmosphere.%20To%20achieve%20this%2C%20we%20incorporate%0Aend-to-end%20optimization%20of%20both%20the%20induced%20mirror%20distortions%20and%20the%0Aupsampling%20algorithm%2C%20such%20that%20telescope-specific%20optics%20and%20temporal%0Astatistics%20of%20atmospheric%20wavefront%20distortions%20are%20accounted%20for.%20Our%0Aexperimental%20results%20with%20a%20hardware%20prototype%2C%20as%20well%20as%20simulations%2C%0Ademonstrate%20significant%20SNR%20improvements%20of%20up%20to%2012%20dB%20over%20non-AO%0Asuper-resolution%20baselines%2C%20using%20only%20existing%20telescope%20optics%20and%20no%0Ahardware%20modifications.%20Moreover%2C%20by%20using%20a%20precise%20bench-top%20replica%20of%20a%0Acomplete%20telescope%20and%20AO%20system%2C%20we%20show%20that%20our%20methodology%20can%20be%20readily%0Atransferred%20to%20an%20operational%20telescope.%20Project%20webpage%3A%0Ahttps%3A//www.cs.toronto.edu/~robin/aosr/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuper%2520Resolved%2520Imaging%2520with%2520Adaptive%2520Optics%26entry.906535625%3DRobin%2520Swanson%2520and%2520Esther%2520Y.%2520H.%2520Lin%2520and%2520Masen%2520Lamb%2520and%2520Suresh%2520Sivanandam%2520and%2520Kiriakos%2520N.%2520Kutulakos%26entry.1292438233%3D%2520%2520Astronomical%2520telescopes%2520suffer%2520from%2520a%2520tradeoff%2520between%2520field%2520of%2520view%2520%2528FoV%2529%250Aand%2520image%2520resolution%253A%2520increasing%2520the%2520FoV%2520leads%2520to%2520an%2520optical%2520field%2520that%2520is%250Aunder-sampled%2520by%2520the%2520science%2520camera.%2520This%2520work%2520presents%2520a%2520novel%2520computational%250Aimaging%2520approach%2520to%2520overcome%2520this%2520tradeoff%2520by%2520leveraging%2520the%2520existing%2520adaptive%250Aoptics%2520%2528AO%2529%2520systems%2520in%2520modern%2520ground-based%2520telescopes.%2520Our%2520key%2520idea%2520is%2520to%2520use%250Athe%2520AO%2520system%2527s%2520deformable%2520mirror%2520to%2520apply%2520a%2520series%2520of%2520learned%252C%2520precisely%250Acontrolled%2520distortions%2520to%2520the%2520optical%2520wavefront%252C%2520producing%2520a%2520sequence%2520of%2520images%250Athat%2520exhibit%2520distinct%252C%2520high-frequency%252C%2520sub-pixel%2520shifts.%2520These%2520images%2520can%2520then%250Abe%2520jointly%2520upsampled%2520to%2520yield%2520the%2520final%2520super-resolved%2520image.%2520Crucially%252C%2520we%250Ashow%2520this%2520can%2520be%2520done%2520while%2520simultaneously%2520maintaining%2520the%2520core%2520AO%250Aoperation--correcting%2520for%2520the%2520unknown%2520and%2520rapidly%2520changing%2520wavefront%250Adistortions%2520caused%2520by%2520Earth%2527s%2520atmosphere.%2520To%2520achieve%2520this%252C%2520we%2520incorporate%250Aend-to-end%2520optimization%2520of%2520both%2520the%2520induced%2520mirror%2520distortions%2520and%2520the%250Aupsampling%2520algorithm%252C%2520such%2520that%2520telescope-specific%2520optics%2520and%2520temporal%250Astatistics%2520of%2520atmospheric%2520wavefront%2520distortions%2520are%2520accounted%2520for.%2520Our%250Aexperimental%2520results%2520with%2520a%2520hardware%2520prototype%252C%2520as%2520well%2520as%2520simulations%252C%250Ademonstrate%2520significant%2520SNR%2520improvements%2520of%2520up%2520to%252012%2520dB%2520over%2520non-AO%250Asuper-resolution%2520baselines%252C%2520using%2520only%2520existing%2520telescope%2520optics%2520and%2520no%250Ahardware%2520modifications.%2520Moreover%252C%2520by%2520using%2520a%2520precise%2520bench-top%2520replica%2520of%2520a%250Acomplete%2520telescope%2520and%2520AO%2520system%252C%2520we%2520show%2520that%2520our%2520methodology%2520can%2520be%2520readily%250Atransferred%2520to%2520an%2520operational%2520telescope.%2520Project%2520webpage%253A%250Ahttps%253A//www.cs.toronto.edu/~robin/aosr/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Super%20Resolved%20Imaging%20with%20Adaptive%20Optics&entry.906535625=Robin%20Swanson%20and%20Esther%20Y.%20H.%20Lin%20and%20Masen%20Lamb%20and%20Suresh%20Sivanandam%20and%20Kiriakos%20N.%20Kutulakos&entry.1292438233=%20%20Astronomical%20telescopes%20suffer%20from%20a%20tradeoff%20between%20field%20of%20view%20%28FoV%29%0Aand%20image%20resolution%3A%20increasing%20the%20FoV%20leads%20to%20an%20optical%20field%20that%20is%0Aunder-sampled%20by%20the%20science%20camera.%20This%20work%20presents%20a%20novel%20computational%0Aimaging%20approach%20to%20overcome%20this%20tradeoff%20by%20leveraging%20the%20existing%20adaptive%0Aoptics%20%28AO%29%20systems%20in%20modern%20ground-based%20telescopes.%20Our%20key%20idea%20is%20to%20use%0Athe%20AO%20system%27s%20deformable%20mirror%20to%20apply%20a%20series%20of%20learned%2C%20precisely%0Acontrolled%20distortions%20to%20the%20optical%20wavefront%2C%20producing%20a%20sequence%20of%20images%0Athat%20exhibit%20distinct%2C%20high-frequency%2C%20sub-pixel%20shifts.%20These%20images%20can%20then%0Abe%20jointly%20upsampled%20to%20yield%20the%20final%20super-resolved%20image.%20Crucially%2C%20we%0Ashow%20this%20can%20be%20done%20while%20simultaneously%20maintaining%20the%20core%20AO%0Aoperation--correcting%20for%20the%20unknown%20and%20rapidly%20changing%20wavefront%0Adistortions%20caused%20by%20Earth%27s%20atmosphere.%20To%20achieve%20this%2C%20we%20incorporate%0Aend-to-end%20optimization%20of%20both%20the%20induced%20mirror%20distortions%20and%20the%0Aupsampling%20algorithm%2C%20such%20that%20telescope-specific%20optics%20and%20temporal%0Astatistics%20of%20atmospheric%20wavefront%20distortions%20are%20accounted%20for.%20Our%0Aexperimental%20results%20with%20a%20hardware%20prototype%2C%20as%20well%20as%20simulations%2C%0Ademonstrate%20significant%20SNR%20improvements%20of%20up%20to%2012%20dB%20over%20non-AO%0Asuper-resolution%20baselines%2C%20using%20only%20existing%20telescope%20optics%20and%20no%0Ahardware%20modifications.%20Moreover%2C%20by%20using%20a%20precise%20bench-top%20replica%20of%20a%0Acomplete%20telescope%20and%20AO%20system%2C%20we%20show%20that%20our%20methodology%20can%20be%20readily%0Atransferred%20to%20an%20operational%20telescope.%20Project%20webpage%3A%0Ahttps%3A//www.cs.toronto.edu/~robin/aosr/%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04648v1&entry.124074799=Read"},
{"title": "TotalRegistrator: Towards a Lightweight Foundation Model for CT Image\n  Registration", "author": "Xuan Loc Pham and Gwendolyn Vuurberg and Marjan Doppen and Joey Roosen and Tip Stille and Thi Quynh Ha and Thuy Duong Quach and Quoc Vu Dang and Manh Ha Luu and Ewoud J. Smit and Hong Son Mai and Mattias Heinrich and Bram van Ginneken and Mathias Prokop and Alessa Hering", "abstract": "  Image registration is a fundamental technique in the analysis of longitudinal\nand multi-phase CT images within clinical practice. However, most existing\nmethods are tailored for single-organ applications, limiting their\ngeneralizability to other anatomical regions. This work presents\nTotalRegistrator, an image registration framework capable of aligning multiple\nanatomical regions simultaneously using a standard UNet architecture and a\nnovel field decomposition strategy. The model is lightweight, requiring only\n11GB of GPU memory for training. To train and evaluate our method, we\nconstructed a large-scale longitudinal dataset comprising 695 whole-body\n(thorax-abdomen-pelvic) paired CT scans from individual patients acquired at\ndifferent time points. We benchmarked TotalRegistrator against a generic\nclassical iterative algorithm and a recent foundation model for image\nregistration. To further assess robustness and generalizability, we evaluated\nour model on three external datasets: the public thoracic and abdominal\ndatasets from the Learn2Reg challenge, and a private multiphase abdominal\ndataset from a collaborating hospital. Experimental results on the in-house\ndataset show that the proposed approach generally surpasses baseline methods in\nmulti-organ abdominal registration, with a slight drop in lung alignment\nperformance. On out-of-distribution datasets, it achieved competitive results\ncompared to leading single-organ models, despite not being fine-tuned for those\ntasks, demonstrating strong generalizability. The source code will be publicly\navailable at: https://github.com/DIAGNijmegen/oncology_image_registration.git.\n", "link": "http://arxiv.org/abs/2508.04450v1", "date": "2025-08-06", "relevancy": 2.5618, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5063}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TotalRegistrator%3A%20Towards%20a%20Lightweight%20Foundation%20Model%20for%20CT%20Image%0A%20%20Registration&body=Title%3A%20TotalRegistrator%3A%20Towards%20a%20Lightweight%20Foundation%20Model%20for%20CT%20Image%0A%20%20Registration%0AAuthor%3A%20Xuan%20Loc%20Pham%20and%20Gwendolyn%20Vuurberg%20and%20Marjan%20Doppen%20and%20Joey%20Roosen%20and%20Tip%20Stille%20and%20Thi%20Quynh%20Ha%20and%20Thuy%20Duong%20Quach%20and%20Quoc%20Vu%20Dang%20and%20Manh%20Ha%20Luu%20and%20Ewoud%20J.%20Smit%20and%20Hong%20Son%20Mai%20and%20Mattias%20Heinrich%20and%20Bram%20van%20Ginneken%20and%20Mathias%20Prokop%20and%20Alessa%20Hering%0AAbstract%3A%20%20%20Image%20registration%20is%20a%20fundamental%20technique%20in%20the%20analysis%20of%20longitudinal%0Aand%20multi-phase%20CT%20images%20within%20clinical%20practice.%20However%2C%20most%20existing%0Amethods%20are%20tailored%20for%20single-organ%20applications%2C%20limiting%20their%0Ageneralizability%20to%20other%20anatomical%20regions.%20This%20work%20presents%0ATotalRegistrator%2C%20an%20image%20registration%20framework%20capable%20of%20aligning%20multiple%0Aanatomical%20regions%20simultaneously%20using%20a%20standard%20UNet%20architecture%20and%20a%0Anovel%20field%20decomposition%20strategy.%20The%20model%20is%20lightweight%2C%20requiring%20only%0A11GB%20of%20GPU%20memory%20for%20training.%20To%20train%20and%20evaluate%20our%20method%2C%20we%0Aconstructed%20a%20large-scale%20longitudinal%20dataset%20comprising%20695%20whole-body%0A%28thorax-abdomen-pelvic%29%20paired%20CT%20scans%20from%20individual%20patients%20acquired%20at%0Adifferent%20time%20points.%20We%20benchmarked%20TotalRegistrator%20against%20a%20generic%0Aclassical%20iterative%20algorithm%20and%20a%20recent%20foundation%20model%20for%20image%0Aregistration.%20To%20further%20assess%20robustness%20and%20generalizability%2C%20we%20evaluated%0Aour%20model%20on%20three%20external%20datasets%3A%20the%20public%20thoracic%20and%20abdominal%0Adatasets%20from%20the%20Learn2Reg%20challenge%2C%20and%20a%20private%20multiphase%20abdominal%0Adataset%20from%20a%20collaborating%20hospital.%20Experimental%20results%20on%20the%20in-house%0Adataset%20show%20that%20the%20proposed%20approach%20generally%20surpasses%20baseline%20methods%20in%0Amulti-organ%20abdominal%20registration%2C%20with%20a%20slight%20drop%20in%20lung%20alignment%0Aperformance.%20On%20out-of-distribution%20datasets%2C%20it%20achieved%20competitive%20results%0Acompared%20to%20leading%20single-organ%20models%2C%20despite%20not%20being%20fine-tuned%20for%20those%0Atasks%2C%20demonstrating%20strong%20generalizability.%20The%20source%20code%20will%20be%20publicly%0Aavailable%20at%3A%20https%3A//github.com/DIAGNijmegen/oncology_image_registration.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTotalRegistrator%253A%2520Towards%2520a%2520Lightweight%2520Foundation%2520Model%2520for%2520CT%2520Image%250A%2520%2520Registration%26entry.906535625%3DXuan%2520Loc%2520Pham%2520and%2520Gwendolyn%2520Vuurberg%2520and%2520Marjan%2520Doppen%2520and%2520Joey%2520Roosen%2520and%2520Tip%2520Stille%2520and%2520Thi%2520Quynh%2520Ha%2520and%2520Thuy%2520Duong%2520Quach%2520and%2520Quoc%2520Vu%2520Dang%2520and%2520Manh%2520Ha%2520Luu%2520and%2520Ewoud%2520J.%2520Smit%2520and%2520Hong%2520Son%2520Mai%2520and%2520Mattias%2520Heinrich%2520and%2520Bram%2520van%2520Ginneken%2520and%2520Mathias%2520Prokop%2520and%2520Alessa%2520Hering%26entry.1292438233%3D%2520%2520Image%2520registration%2520is%2520a%2520fundamental%2520technique%2520in%2520the%2520analysis%2520of%2520longitudinal%250Aand%2520multi-phase%2520CT%2520images%2520within%2520clinical%2520practice.%2520However%252C%2520most%2520existing%250Amethods%2520are%2520tailored%2520for%2520single-organ%2520applications%252C%2520limiting%2520their%250Ageneralizability%2520to%2520other%2520anatomical%2520regions.%2520This%2520work%2520presents%250ATotalRegistrator%252C%2520an%2520image%2520registration%2520framework%2520capable%2520of%2520aligning%2520multiple%250Aanatomical%2520regions%2520simultaneously%2520using%2520a%2520standard%2520UNet%2520architecture%2520and%2520a%250Anovel%2520field%2520decomposition%2520strategy.%2520The%2520model%2520is%2520lightweight%252C%2520requiring%2520only%250A11GB%2520of%2520GPU%2520memory%2520for%2520training.%2520To%2520train%2520and%2520evaluate%2520our%2520method%252C%2520we%250Aconstructed%2520a%2520large-scale%2520longitudinal%2520dataset%2520comprising%2520695%2520whole-body%250A%2528thorax-abdomen-pelvic%2529%2520paired%2520CT%2520scans%2520from%2520individual%2520patients%2520acquired%2520at%250Adifferent%2520time%2520points.%2520We%2520benchmarked%2520TotalRegistrator%2520against%2520a%2520generic%250Aclassical%2520iterative%2520algorithm%2520and%2520a%2520recent%2520foundation%2520model%2520for%2520image%250Aregistration.%2520To%2520further%2520assess%2520robustness%2520and%2520generalizability%252C%2520we%2520evaluated%250Aour%2520model%2520on%2520three%2520external%2520datasets%253A%2520the%2520public%2520thoracic%2520and%2520abdominal%250Adatasets%2520from%2520the%2520Learn2Reg%2520challenge%252C%2520and%2520a%2520private%2520multiphase%2520abdominal%250Adataset%2520from%2520a%2520collaborating%2520hospital.%2520Experimental%2520results%2520on%2520the%2520in-house%250Adataset%2520show%2520that%2520the%2520proposed%2520approach%2520generally%2520surpasses%2520baseline%2520methods%2520in%250Amulti-organ%2520abdominal%2520registration%252C%2520with%2520a%2520slight%2520drop%2520in%2520lung%2520alignment%250Aperformance.%2520On%2520out-of-distribution%2520datasets%252C%2520it%2520achieved%2520competitive%2520results%250Acompared%2520to%2520leading%2520single-organ%2520models%252C%2520despite%2520not%2520being%2520fine-tuned%2520for%2520those%250Atasks%252C%2520demonstrating%2520strong%2520generalizability.%2520The%2520source%2520code%2520will%2520be%2520publicly%250Aavailable%2520at%253A%2520https%253A//github.com/DIAGNijmegen/oncology_image_registration.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TotalRegistrator%3A%20Towards%20a%20Lightweight%20Foundation%20Model%20for%20CT%20Image%0A%20%20Registration&entry.906535625=Xuan%20Loc%20Pham%20and%20Gwendolyn%20Vuurberg%20and%20Marjan%20Doppen%20and%20Joey%20Roosen%20and%20Tip%20Stille%20and%20Thi%20Quynh%20Ha%20and%20Thuy%20Duong%20Quach%20and%20Quoc%20Vu%20Dang%20and%20Manh%20Ha%20Luu%20and%20Ewoud%20J.%20Smit%20and%20Hong%20Son%20Mai%20and%20Mattias%20Heinrich%20and%20Bram%20van%20Ginneken%20and%20Mathias%20Prokop%20and%20Alessa%20Hering&entry.1292438233=%20%20Image%20registration%20is%20a%20fundamental%20technique%20in%20the%20analysis%20of%20longitudinal%0Aand%20multi-phase%20CT%20images%20within%20clinical%20practice.%20However%2C%20most%20existing%0Amethods%20are%20tailored%20for%20single-organ%20applications%2C%20limiting%20their%0Ageneralizability%20to%20other%20anatomical%20regions.%20This%20work%20presents%0ATotalRegistrator%2C%20an%20image%20registration%20framework%20capable%20of%20aligning%20multiple%0Aanatomical%20regions%20simultaneously%20using%20a%20standard%20UNet%20architecture%20and%20a%0Anovel%20field%20decomposition%20strategy.%20The%20model%20is%20lightweight%2C%20requiring%20only%0A11GB%20of%20GPU%20memory%20for%20training.%20To%20train%20and%20evaluate%20our%20method%2C%20we%0Aconstructed%20a%20large-scale%20longitudinal%20dataset%20comprising%20695%20whole-body%0A%28thorax-abdomen-pelvic%29%20paired%20CT%20scans%20from%20individual%20patients%20acquired%20at%0Adifferent%20time%20points.%20We%20benchmarked%20TotalRegistrator%20against%20a%20generic%0Aclassical%20iterative%20algorithm%20and%20a%20recent%20foundation%20model%20for%20image%0Aregistration.%20To%20further%20assess%20robustness%20and%20generalizability%2C%20we%20evaluated%0Aour%20model%20on%20three%20external%20datasets%3A%20the%20public%20thoracic%20and%20abdominal%0Adatasets%20from%20the%20Learn2Reg%20challenge%2C%20and%20a%20private%20multiphase%20abdominal%0Adataset%20from%20a%20collaborating%20hospital.%20Experimental%20results%20on%20the%20in-house%0Adataset%20show%20that%20the%20proposed%20approach%20generally%20surpasses%20baseline%20methods%20in%0Amulti-organ%20abdominal%20registration%2C%20with%20a%20slight%20drop%20in%20lung%20alignment%0Aperformance.%20On%20out-of-distribution%20datasets%2C%20it%20achieved%20competitive%20results%0Acompared%20to%20leading%20single-organ%20models%2C%20despite%20not%20being%20fine-tuned%20for%20those%0Atasks%2C%20demonstrating%20strong%20generalizability.%20The%20source%20code%20will%20be%20publicly%0Aavailable%20at%3A%20https%3A//github.com/DIAGNijmegen/oncology_image_registration.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04450v1&entry.124074799=Read"},
{"title": "Face-voice Association in Multilingual Environments (FAME) 2026\n  Challenge Evaluation Plan", "author": "Marta Moscati and Ahmed Abdullah and Muhammad Saad Saeed and Shah Nawaz and Rohan Kumar Das and Muhammad Zaigham Zaheer and Junaid Mir and Muhammad Haroon Yousaf and Khalid Malik and Markus Schedl", "abstract": "  The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, audio-visual systems are among the\nmost widely used multimodal systems. In the recent years, associating face and\nvoice of a person has gained attention due to the presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) 2026 Challenge focuses on exploring face-voice association\nunder the unique condition of a multilingual scenario. This condition is\ninspired from the fact that half of the world's population is bilingual and\nmost often people communicate under multilingual scenarios. The challenge uses\na dataset named Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baseline models, and task details for the FAME\nChallenge.\n", "link": "http://arxiv.org/abs/2508.04592v1", "date": "2025-08-06", "relevancy": 2.5147, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5106}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face-voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%202026%0A%20%20Challenge%20Evaluation%20Plan&body=Title%3A%20Face-voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%202026%0A%20%20Challenge%20Evaluation%20Plan%0AAuthor%3A%20Marta%20Moscati%20and%20Ahmed%20Abdullah%20and%20Muhammad%20Saad%20Saeed%20and%20Shah%20Nawaz%20and%20Rohan%20Kumar%20Das%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Junaid%20Mir%20and%20Muhammad%20Haroon%20Yousaf%20and%20Khalid%20Malik%20and%20Markus%20Schedl%0AAbstract%3A%20%20%20The%20advancements%20of%20technology%20have%20led%20to%20the%20use%20of%20multimodal%20systems%20in%0Avarious%20real-world%20applications.%20Among%20them%2C%20audio-visual%20systems%20are%20among%20the%0Amost%20widely%20used%20multimodal%20systems.%20In%20the%20recent%20years%2C%20associating%20face%20and%0Avoice%20of%20a%20person%20has%20gained%20attention%20due%20to%20the%20presence%20of%20unique%0Acorrelation%20between%20them.%20The%20Face-voice%20Association%20in%20Multilingual%0AEnvironments%20%28FAME%29%202026%20Challenge%20focuses%20on%20exploring%20face-voice%20association%0Aunder%20the%20unique%20condition%20of%20a%20multilingual%20scenario.%20This%20condition%20is%0Ainspired%20from%20the%20fact%20that%20half%20of%20the%20world%27s%20population%20is%20bilingual%20and%0Amost%20often%20people%20communicate%20under%20multilingual%20scenarios.%20The%20challenge%20uses%0Aa%20dataset%20named%20Multilingual%20Audio-Visual%20%28MAV-Celeb%29%20for%20exploring%20face-voice%0Aassociation%20in%20multilingual%20environments.%20This%20report%20provides%20the%20details%20of%0Athe%20challenge%2C%20dataset%2C%20baseline%20models%2C%20and%20task%20details%20for%20the%20FAME%0AChallenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace-voice%2520Association%2520in%2520Multilingual%2520Environments%2520%2528FAME%2529%25202026%250A%2520%2520Challenge%2520Evaluation%2520Plan%26entry.906535625%3DMarta%2520Moscati%2520and%2520Ahmed%2520Abdullah%2520and%2520Muhammad%2520Saad%2520Saeed%2520and%2520Shah%2520Nawaz%2520and%2520Rohan%2520Kumar%2520Das%2520and%2520Muhammad%2520Zaigham%2520Zaheer%2520and%2520Junaid%2520Mir%2520and%2520Muhammad%2520Haroon%2520Yousaf%2520and%2520Khalid%2520Malik%2520and%2520Markus%2520Schedl%26entry.1292438233%3D%2520%2520The%2520advancements%2520of%2520technology%2520have%2520led%2520to%2520the%2520use%2520of%2520multimodal%2520systems%2520in%250Avarious%2520real-world%2520applications.%2520Among%2520them%252C%2520audio-visual%2520systems%2520are%2520among%2520the%250Amost%2520widely%2520used%2520multimodal%2520systems.%2520In%2520the%2520recent%2520years%252C%2520associating%2520face%2520and%250Avoice%2520of%2520a%2520person%2520has%2520gained%2520attention%2520due%2520to%2520the%2520presence%2520of%2520unique%250Acorrelation%2520between%2520them.%2520The%2520Face-voice%2520Association%2520in%2520Multilingual%250AEnvironments%2520%2528FAME%2529%25202026%2520Challenge%2520focuses%2520on%2520exploring%2520face-voice%2520association%250Aunder%2520the%2520unique%2520condition%2520of%2520a%2520multilingual%2520scenario.%2520This%2520condition%2520is%250Ainspired%2520from%2520the%2520fact%2520that%2520half%2520of%2520the%2520world%2527s%2520population%2520is%2520bilingual%2520and%250Amost%2520often%2520people%2520communicate%2520under%2520multilingual%2520scenarios.%2520The%2520challenge%2520uses%250Aa%2520dataset%2520named%2520Multilingual%2520Audio-Visual%2520%2528MAV-Celeb%2529%2520for%2520exploring%2520face-voice%250Aassociation%2520in%2520multilingual%2520environments.%2520This%2520report%2520provides%2520the%2520details%2520of%250Athe%2520challenge%252C%2520dataset%252C%2520baseline%2520models%252C%2520and%2520task%2520details%2520for%2520the%2520FAME%250AChallenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face-voice%20Association%20in%20Multilingual%20Environments%20%28FAME%29%202026%0A%20%20Challenge%20Evaluation%20Plan&entry.906535625=Marta%20Moscati%20and%20Ahmed%20Abdullah%20and%20Muhammad%20Saad%20Saeed%20and%20Shah%20Nawaz%20and%20Rohan%20Kumar%20Das%20and%20Muhammad%20Zaigham%20Zaheer%20and%20Junaid%20Mir%20and%20Muhammad%20Haroon%20Yousaf%20and%20Khalid%20Malik%20and%20Markus%20Schedl&entry.1292438233=%20%20The%20advancements%20of%20technology%20have%20led%20to%20the%20use%20of%20multimodal%20systems%20in%0Avarious%20real-world%20applications.%20Among%20them%2C%20audio-visual%20systems%20are%20among%20the%0Amost%20widely%20used%20multimodal%20systems.%20In%20the%20recent%20years%2C%20associating%20face%20and%0Avoice%20of%20a%20person%20has%20gained%20attention%20due%20to%20the%20presence%20of%20unique%0Acorrelation%20between%20them.%20The%20Face-voice%20Association%20in%20Multilingual%0AEnvironments%20%28FAME%29%202026%20Challenge%20focuses%20on%20exploring%20face-voice%20association%0Aunder%20the%20unique%20condition%20of%20a%20multilingual%20scenario.%20This%20condition%20is%0Ainspired%20from%20the%20fact%20that%20half%20of%20the%20world%27s%20population%20is%20bilingual%20and%0Amost%20often%20people%20communicate%20under%20multilingual%20scenarios.%20The%20challenge%20uses%0Aa%20dataset%20named%20Multilingual%20Audio-Visual%20%28MAV-Celeb%29%20for%20exploring%20face-voice%0Aassociation%20in%20multilingual%20environments.%20This%20report%20provides%20the%20details%20of%0Athe%20challenge%2C%20dataset%2C%20baseline%20models%2C%20and%20task%20details%20for%20the%20FAME%0AChallenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04592v1&entry.124074799=Read"},
{"title": "Live Music Models", "author": " Lyria Team and Antoine Caillon and Brian McWilliams and Cassie Tarakajian and Ian Simon and Ilaria Manco and Jesse Engel and Noah Constant and Pen Li and Timo I. Denk and Alberto Lalama and Andrea Agostinelli and Anna Huang and Ethan Manilow and George Brower and Hakan Erdogan and Heidi Lei and Itai Rolnick and Ivan Grishchenko and Manu Orsini and Matej Kastelic and Mauricio Zuluaga and Mauro Verzetti and Michael Dooley and Ondrej Skopek and Rafael Ferrer and Zal\u00e1n Borsos and \u00c4aron van den Oord and Douglas Eck and Eli Collins and Jason Baldridge and Tom Hume and Chris Donahue and Kehang Han and Adam Roberts", "abstract": "  We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance.\n", "link": "http://arxiv.org/abs/2508.04651v1", "date": "2025-08-06", "relevancy": 2.4995, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5441}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4831}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Live%20Music%20Models&body=Title%3A%20Live%20Music%20Models%0AAuthor%3A%20%20Lyria%20Team%20and%20Antoine%20Caillon%20and%20Brian%20McWilliams%20and%20Cassie%20Tarakajian%20and%20Ian%20Simon%20and%20Ilaria%20Manco%20and%20Jesse%20Engel%20and%20Noah%20Constant%20and%20Pen%20Li%20and%20Timo%20I.%20Denk%20and%20Alberto%20Lalama%20and%20Andrea%20Agostinelli%20and%20Anna%20Huang%20and%20Ethan%20Manilow%20and%20George%20Brower%20and%20Hakan%20Erdogan%20and%20Heidi%20Lei%20and%20Itai%20Rolnick%20and%20Ivan%20Grishchenko%20and%20Manu%20Orsini%20and%20Matej%20Kastelic%20and%20Mauricio%20Zuluaga%20and%20Mauro%20Verzetti%20and%20Michael%20Dooley%20and%20Ondrej%20Skopek%20and%20Rafael%20Ferrer%20and%20Zal%C3%A1n%20Borsos%20and%20%C3%84aron%20van%20den%20Oord%20and%20Douglas%20Eck%20and%20Eli%20Collins%20and%20Jason%20Baldridge%20and%20Tom%20Hume%20and%20Chris%20Donahue%20and%20Kehang%20Han%20and%20Adam%20Roberts%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20class%20of%20generative%20models%20for%20music%20called%20live%20music%0Amodels%20that%20produce%20a%20continuous%20stream%20of%20music%20in%20real-time%20with%20synchronized%0Auser%20control.%20We%20release%20Magenta%20RealTime%2C%20an%20open-weights%20live%20music%20model%0Athat%20can%20be%20steered%20using%20text%20or%20audio%20prompts%20to%20control%20acoustic%20style.%20On%0Aautomatic%20metrics%20of%20music%20quality%2C%20Magenta%20RealTime%20outperforms%20other%0Aopen-weights%20music%20generation%20models%2C%20despite%20using%20fewer%20parameters%20and%0Aoffering%20first-of-its-kind%20live%20generation%20capabilities.%20We%20also%20release%20Lyria%0ARealTime%2C%20an%20API-based%20model%20with%20extended%20controls%2C%20offering%20access%20to%20our%0Amost%20powerful%20model%20with%20wide%20prompt%20coverage.%20These%20models%20demonstrate%20a%20new%0Aparadigm%20for%20AI-assisted%20music%20creation%20that%20emphasizes%20human-in-the-loop%0Ainteraction%20for%20live%20music%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLive%2520Music%2520Models%26entry.906535625%3D%2520Lyria%2520Team%2520and%2520Antoine%2520Caillon%2520and%2520Brian%2520McWilliams%2520and%2520Cassie%2520Tarakajian%2520and%2520Ian%2520Simon%2520and%2520Ilaria%2520Manco%2520and%2520Jesse%2520Engel%2520and%2520Noah%2520Constant%2520and%2520Pen%2520Li%2520and%2520Timo%2520I.%2520Denk%2520and%2520Alberto%2520Lalama%2520and%2520Andrea%2520Agostinelli%2520and%2520Anna%2520Huang%2520and%2520Ethan%2520Manilow%2520and%2520George%2520Brower%2520and%2520Hakan%2520Erdogan%2520and%2520Heidi%2520Lei%2520and%2520Itai%2520Rolnick%2520and%2520Ivan%2520Grishchenko%2520and%2520Manu%2520Orsini%2520and%2520Matej%2520Kastelic%2520and%2520Mauricio%2520Zuluaga%2520and%2520Mauro%2520Verzetti%2520and%2520Michael%2520Dooley%2520and%2520Ondrej%2520Skopek%2520and%2520Rafael%2520Ferrer%2520and%2520Zal%25C3%25A1n%2520Borsos%2520and%2520%25C3%2584aron%2520van%2520den%2520Oord%2520and%2520Douglas%2520Eck%2520and%2520Eli%2520Collins%2520and%2520Jason%2520Baldridge%2520and%2520Tom%2520Hume%2520and%2520Chris%2520Donahue%2520and%2520Kehang%2520Han%2520and%2520Adam%2520Roberts%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520class%2520of%2520generative%2520models%2520for%2520music%2520called%2520live%2520music%250Amodels%2520that%2520produce%2520a%2520continuous%2520stream%2520of%2520music%2520in%2520real-time%2520with%2520synchronized%250Auser%2520control.%2520We%2520release%2520Magenta%2520RealTime%252C%2520an%2520open-weights%2520live%2520music%2520model%250Athat%2520can%2520be%2520steered%2520using%2520text%2520or%2520audio%2520prompts%2520to%2520control%2520acoustic%2520style.%2520On%250Aautomatic%2520metrics%2520of%2520music%2520quality%252C%2520Magenta%2520RealTime%2520outperforms%2520other%250Aopen-weights%2520music%2520generation%2520models%252C%2520despite%2520using%2520fewer%2520parameters%2520and%250Aoffering%2520first-of-its-kind%2520live%2520generation%2520capabilities.%2520We%2520also%2520release%2520Lyria%250ARealTime%252C%2520an%2520API-based%2520model%2520with%2520extended%2520controls%252C%2520offering%2520access%2520to%2520our%250Amost%2520powerful%2520model%2520with%2520wide%2520prompt%2520coverage.%2520These%2520models%2520demonstrate%2520a%2520new%250Aparadigm%2520for%2520AI-assisted%2520music%2520creation%2520that%2520emphasizes%2520human-in-the-loop%250Ainteraction%2520for%2520live%2520music%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Live%20Music%20Models&entry.906535625=%20Lyria%20Team%20and%20Antoine%20Caillon%20and%20Brian%20McWilliams%20and%20Cassie%20Tarakajian%20and%20Ian%20Simon%20and%20Ilaria%20Manco%20and%20Jesse%20Engel%20and%20Noah%20Constant%20and%20Pen%20Li%20and%20Timo%20I.%20Denk%20and%20Alberto%20Lalama%20and%20Andrea%20Agostinelli%20and%20Anna%20Huang%20and%20Ethan%20Manilow%20and%20George%20Brower%20and%20Hakan%20Erdogan%20and%20Heidi%20Lei%20and%20Itai%20Rolnick%20and%20Ivan%20Grishchenko%20and%20Manu%20Orsini%20and%20Matej%20Kastelic%20and%20Mauricio%20Zuluaga%20and%20Mauro%20Verzetti%20and%20Michael%20Dooley%20and%20Ondrej%20Skopek%20and%20Rafael%20Ferrer%20and%20Zal%C3%A1n%20Borsos%20and%20%C3%84aron%20van%20den%20Oord%20and%20Douglas%20Eck%20and%20Eli%20Collins%20and%20Jason%20Baldridge%20and%20Tom%20Hume%20and%20Chris%20Donahue%20and%20Kehang%20Han%20and%20Adam%20Roberts&entry.1292438233=%20%20We%20introduce%20a%20new%20class%20of%20generative%20models%20for%20music%20called%20live%20music%0Amodels%20that%20produce%20a%20continuous%20stream%20of%20music%20in%20real-time%20with%20synchronized%0Auser%20control.%20We%20release%20Magenta%20RealTime%2C%20an%20open-weights%20live%20music%20model%0Athat%20can%20be%20steered%20using%20text%20or%20audio%20prompts%20to%20control%20acoustic%20style.%20On%0Aautomatic%20metrics%20of%20music%20quality%2C%20Magenta%20RealTime%20outperforms%20other%0Aopen-weights%20music%20generation%20models%2C%20despite%20using%20fewer%20parameters%20and%0Aoffering%20first-of-its-kind%20live%20generation%20capabilities.%20We%20also%20release%20Lyria%0ARealTime%2C%20an%20API-based%20model%20with%20extended%20controls%2C%20offering%20access%20to%20our%0Amost%20powerful%20model%20with%20wide%20prompt%20coverage.%20These%20models%20demonstrate%20a%20new%0Aparadigm%20for%20AI-assisted%20music%20creation%20that%20emphasizes%20human-in-the-loop%0Ainteraction%20for%20live%20music%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04651v1&entry.124074799=Read"},
{"title": "Why are LLMs' abilities emergent?", "author": "Vladim\u00edr Havl\u00edk", "abstract": "  The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.\n", "link": "http://arxiv.org/abs/2508.04401v1", "date": "2025-08-06", "relevancy": 2.4791, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20are%20LLMs%27%20abilities%20emergent%3F&body=Title%3A%20Why%20are%20LLMs%27%20abilities%20emergent%3F%0AAuthor%3A%20Vladim%C3%ADr%20Havl%C3%ADk%0AAbstract%3A%20%20%20The%20remarkable%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generative%20tasks%0Ahas%20raised%20fundamental%20questions%20about%20the%20nature%20of%20their%20acquired%0Acapabilities%2C%20which%20often%20appear%20to%20emerge%20unexpectedly%20without%20explicit%0Atraining.%20This%20paper%20examines%20the%20emergent%20properties%20of%20Deep%20Neural%20Networks%0A%28DNNs%29%20through%20both%20theoretical%20analysis%20and%20empirical%20observation%2C%20addressing%0Athe%20epistemological%20challenge%20of%20%22creation%20without%20understanding%22%20that%0Acharacterises%20contemporary%20AI%20development.%20We%20explore%20how%20the%20neural%20approach%27s%0Areliance%20on%20nonlinear%2C%20stochastic%20processes%20fundamentally%20differs%20from%20symbolic%0Acomputational%20paradigms%2C%20creating%20systems%20whose%20macro-level%20behaviours%20cannot%0Abe%20analytically%20derived%20from%20micro-level%20neuron%20activities.%20Through%20analysis%20of%0Ascaling%20laws%2C%20grokking%20phenomena%2C%20and%20phase%20transitions%20in%20model%20capabilities%2C%0AI%20demonstrate%20that%20emergent%20abilities%20arise%20from%20the%20complex%20dynamics%20of%20highly%0Asensitive%20nonlinear%20systems%20rather%20than%20simply%20from%20parameter%20scaling%20alone.%20My%0Ainvestigation%20reveals%20that%20current%20debates%20over%20metrics%2C%20pre-training%20loss%0Athresholds%2C%20and%20in-context%20learning%20miss%20the%20fundamental%20ontological%20nature%20of%0Aemergence%20in%20DNNs.%20I%20argue%20that%20these%20systems%20exhibit%20genuine%20emergent%0Aproperties%20analogous%20to%20those%20found%20in%20other%20complex%20natural%20phenomena%2C%20where%0Asystemic%20capabilities%20emerge%20from%20cooperative%20interactions%20among%20simple%0Acomponents%20without%20being%20reducible%20to%20their%20individual%20behaviours.%20The%20paper%0Aconcludes%20that%20understanding%20LLM%20capabilities%20requires%20recognising%20DNNs%20as%20a%0Anew%20domain%20of%20complex%20dynamical%20systems%20governed%20by%20universal%20principles%20of%0Aemergence%2C%20similar%20to%20those%20operating%20in%20physics%2C%20chemistry%2C%20and%20biology.%20This%0Aperspective%20shifts%20the%20focus%20from%20purely%20phenomenological%20definitions%20of%0Aemergence%20to%20understanding%20the%20internal%20dynamic%20transformations%20that%20enable%0Athese%20systems%20to%20acquire%20capabilities%20that%20transcend%20their%20individual%0Acomponents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520are%2520LLMs%2527%2520abilities%2520emergent%253F%26entry.906535625%3DVladim%25C3%25ADr%2520Havl%25C3%25ADk%26entry.1292438233%3D%2520%2520The%2520remarkable%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520generative%2520tasks%250Ahas%2520raised%2520fundamental%2520questions%2520about%2520the%2520nature%2520of%2520their%2520acquired%250Acapabilities%252C%2520which%2520often%2520appear%2520to%2520emerge%2520unexpectedly%2520without%2520explicit%250Atraining.%2520This%2520paper%2520examines%2520the%2520emergent%2520properties%2520of%2520Deep%2520Neural%2520Networks%250A%2528DNNs%2529%2520through%2520both%2520theoretical%2520analysis%2520and%2520empirical%2520observation%252C%2520addressing%250Athe%2520epistemological%2520challenge%2520of%2520%2522creation%2520without%2520understanding%2522%2520that%250Acharacterises%2520contemporary%2520AI%2520development.%2520We%2520explore%2520how%2520the%2520neural%2520approach%2527s%250Areliance%2520on%2520nonlinear%252C%2520stochastic%2520processes%2520fundamentally%2520differs%2520from%2520symbolic%250Acomputational%2520paradigms%252C%2520creating%2520systems%2520whose%2520macro-level%2520behaviours%2520cannot%250Abe%2520analytically%2520derived%2520from%2520micro-level%2520neuron%2520activities.%2520Through%2520analysis%2520of%250Ascaling%2520laws%252C%2520grokking%2520phenomena%252C%2520and%2520phase%2520transitions%2520in%2520model%2520capabilities%252C%250AI%2520demonstrate%2520that%2520emergent%2520abilities%2520arise%2520from%2520the%2520complex%2520dynamics%2520of%2520highly%250Asensitive%2520nonlinear%2520systems%2520rather%2520than%2520simply%2520from%2520parameter%2520scaling%2520alone.%2520My%250Ainvestigation%2520reveals%2520that%2520current%2520debates%2520over%2520metrics%252C%2520pre-training%2520loss%250Athresholds%252C%2520and%2520in-context%2520learning%2520miss%2520the%2520fundamental%2520ontological%2520nature%2520of%250Aemergence%2520in%2520DNNs.%2520I%2520argue%2520that%2520these%2520systems%2520exhibit%2520genuine%2520emergent%250Aproperties%2520analogous%2520to%2520those%2520found%2520in%2520other%2520complex%2520natural%2520phenomena%252C%2520where%250Asystemic%2520capabilities%2520emerge%2520from%2520cooperative%2520interactions%2520among%2520simple%250Acomponents%2520without%2520being%2520reducible%2520to%2520their%2520individual%2520behaviours.%2520The%2520paper%250Aconcludes%2520that%2520understanding%2520LLM%2520capabilities%2520requires%2520recognising%2520DNNs%2520as%2520a%250Anew%2520domain%2520of%2520complex%2520dynamical%2520systems%2520governed%2520by%2520universal%2520principles%2520of%250Aemergence%252C%2520similar%2520to%2520those%2520operating%2520in%2520physics%252C%2520chemistry%252C%2520and%2520biology.%2520This%250Aperspective%2520shifts%2520the%2520focus%2520from%2520purely%2520phenomenological%2520definitions%2520of%250Aemergence%2520to%2520understanding%2520the%2520internal%2520dynamic%2520transformations%2520that%2520enable%250Athese%2520systems%2520to%2520acquire%2520capabilities%2520that%2520transcend%2520their%2520individual%250Acomponents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20are%20LLMs%27%20abilities%20emergent%3F&entry.906535625=Vladim%C3%ADr%20Havl%C3%ADk&entry.1292438233=%20%20The%20remarkable%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20generative%20tasks%0Ahas%20raised%20fundamental%20questions%20about%20the%20nature%20of%20their%20acquired%0Acapabilities%2C%20which%20often%20appear%20to%20emerge%20unexpectedly%20without%20explicit%0Atraining.%20This%20paper%20examines%20the%20emergent%20properties%20of%20Deep%20Neural%20Networks%0A%28DNNs%29%20through%20both%20theoretical%20analysis%20and%20empirical%20observation%2C%20addressing%0Athe%20epistemological%20challenge%20of%20%22creation%20without%20understanding%22%20that%0Acharacterises%20contemporary%20AI%20development.%20We%20explore%20how%20the%20neural%20approach%27s%0Areliance%20on%20nonlinear%2C%20stochastic%20processes%20fundamentally%20differs%20from%20symbolic%0Acomputational%20paradigms%2C%20creating%20systems%20whose%20macro-level%20behaviours%20cannot%0Abe%20analytically%20derived%20from%20micro-level%20neuron%20activities.%20Through%20analysis%20of%0Ascaling%20laws%2C%20grokking%20phenomena%2C%20and%20phase%20transitions%20in%20model%20capabilities%2C%0AI%20demonstrate%20that%20emergent%20abilities%20arise%20from%20the%20complex%20dynamics%20of%20highly%0Asensitive%20nonlinear%20systems%20rather%20than%20simply%20from%20parameter%20scaling%20alone.%20My%0Ainvestigation%20reveals%20that%20current%20debates%20over%20metrics%2C%20pre-training%20loss%0Athresholds%2C%20and%20in-context%20learning%20miss%20the%20fundamental%20ontological%20nature%20of%0Aemergence%20in%20DNNs.%20I%20argue%20that%20these%20systems%20exhibit%20genuine%20emergent%0Aproperties%20analogous%20to%20those%20found%20in%20other%20complex%20natural%20phenomena%2C%20where%0Asystemic%20capabilities%20emerge%20from%20cooperative%20interactions%20among%20simple%0Acomponents%20without%20being%20reducible%20to%20their%20individual%20behaviours.%20The%20paper%0Aconcludes%20that%20understanding%20LLM%20capabilities%20requires%20recognising%20DNNs%20as%20a%0Anew%20domain%20of%20complex%20dynamical%20systems%20governed%20by%20universal%20principles%20of%0Aemergence%2C%20similar%20to%20those%20operating%20in%20physics%2C%20chemistry%2C%20and%20biology.%20This%0Aperspective%20shifts%20the%20focus%20from%20purely%20phenomenological%20definitions%20of%0Aemergence%20to%20understanding%20the%20internal%20dynamic%20transformations%20that%20enable%0Athese%20systems%20to%20acquire%20capabilities%20that%20transcend%20their%20individual%0Acomponents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04401v1&entry.124074799=Read"},
{"title": "Modelling and Classifying the Components of a Literature Review", "author": "Francisco Bola\u00f1os and Angelo Salatino and Francesco Osborne and Enrico Motta", "abstract": "  Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.\n", "link": "http://arxiv.org/abs/2508.04337v1", "date": "2025-08-06", "relevancy": 2.464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modelling%20and%20Classifying%20the%20Components%20of%20a%20Literature%20Review&body=Title%3A%20Modelling%20and%20Classifying%20the%20Components%20of%20a%20Literature%20Review%0AAuthor%3A%20Francisco%20Bola%C3%B1os%20and%20Angelo%20Salatino%20and%20Francesco%20Osborne%20and%20Enrico%20Motta%0AAbstract%3A%20%20%20Previous%20work%20has%20demonstrated%20that%20AI%20methods%20for%20analysing%20scientific%0Aliterature%20benefit%20significantly%20from%20annotating%20sentences%20in%20papers%20according%0Ato%20their%20rhetorical%20roles%2C%20such%20as%20research%20gaps%2C%20results%2C%20limitations%2C%0Aextensions%20of%20existing%20methodologies%2C%20and%20others.%20Such%20representations%20also%0Ahave%20the%20potential%20to%20support%20the%20development%20of%20a%20new%20generation%20of%20systems%0Acapable%20of%20producing%20high-quality%20literature%20reviews.%20However%2C%20achieving%20this%0Agoal%20requires%20the%20definition%20of%20a%20relevant%20annotation%20schema%20and%20effective%0Astrategies%20for%20large-scale%20annotation%20of%20the%20literature.%20This%20paper%20addresses%0Athese%20challenges%20by%201%29%20introducing%20a%20novel%20annotation%20schema%20specifically%0Adesigned%20to%20support%20literature%20review%20generation%20and%202%29%20conducting%20a%0Acomprehensive%20evaluation%20of%20a%20wide%20range%20of%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20in%20classifying%20rhetorical%20roles%20according%20to%20this%20schema.%20To%20this%0Aend%2C%20we%20also%20present%20Sci-Sentence%2C%20a%20novel%20multidisciplinary%20benchmark%0Acomprising%20700%20sentences%20manually%20annotated%20by%20domain%20experts%20and%202%2C240%0Asentences%20automatically%20labelled%20using%20LLMs.%20We%20evaluate%2037%20LLMs%20on%20this%0Abenchmark%2C%20spanning%20diverse%20model%20families%20and%20sizes%2C%20using%20both%20zero-shot%0Alearning%20and%20fine-tuning%20approaches.%20The%20experiments%20yield%20several%20novel%0Ainsights%20that%20advance%20the%20state%20of%20the%20art%20in%20this%20challenging%20domain.%20First%2C%0Athe%20current%20generation%20of%20LLMs%20performs%20remarkably%20well%20on%20this%20task%20when%0Afine-tuned%20on%20high-quality%20data%2C%20achieving%20performance%20levels%20above%2096%5C%25%20F1.%0ASecond%2C%20while%20large%20proprietary%20models%20like%20GPT-4o%20achieve%20the%20best%20results%2C%0Asome%20lightweight%20open-source%20alternatives%20also%20demonstrate%20excellent%0Aperformance.%20Finally%2C%20enriching%20the%20training%20data%20with%20semi-synthetic%20examples%0Agenerated%20by%20LLMs%20proves%20beneficial%2C%20enabling%20small%20encoders%20to%20achieve%20robust%0Aresults%20and%20significantly%20enhancing%20the%20performance%20of%20several%20open%20decoder%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModelling%2520and%2520Classifying%2520the%2520Components%2520of%2520a%2520Literature%2520Review%26entry.906535625%3DFrancisco%2520Bola%25C3%25B1os%2520and%2520Angelo%2520Salatino%2520and%2520Francesco%2520Osborne%2520and%2520Enrico%2520Motta%26entry.1292438233%3D%2520%2520Previous%2520work%2520has%2520demonstrated%2520that%2520AI%2520methods%2520for%2520analysing%2520scientific%250Aliterature%2520benefit%2520significantly%2520from%2520annotating%2520sentences%2520in%2520papers%2520according%250Ato%2520their%2520rhetorical%2520roles%252C%2520such%2520as%2520research%2520gaps%252C%2520results%252C%2520limitations%252C%250Aextensions%2520of%2520existing%2520methodologies%252C%2520and%2520others.%2520Such%2520representations%2520also%250Ahave%2520the%2520potential%2520to%2520support%2520the%2520development%2520of%2520a%2520new%2520generation%2520of%2520systems%250Acapable%2520of%2520producing%2520high-quality%2520literature%2520reviews.%2520However%252C%2520achieving%2520this%250Agoal%2520requires%2520the%2520definition%2520of%2520a%2520relevant%2520annotation%2520schema%2520and%2520effective%250Astrategies%2520for%2520large-scale%2520annotation%2520of%2520the%2520literature.%2520This%2520paper%2520addresses%250Athese%2520challenges%2520by%25201%2529%2520introducing%2520a%2520novel%2520annotation%2520schema%2520specifically%250Adesigned%2520to%2520support%2520literature%2520review%2520generation%2520and%25202%2529%2520conducting%2520a%250Acomprehensive%2520evaluation%2520of%2520a%2520wide%2520range%2520of%2520state-of-the-art%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520in%2520classifying%2520rhetorical%2520roles%2520according%2520to%2520this%2520schema.%2520To%2520this%250Aend%252C%2520we%2520also%2520present%2520Sci-Sentence%252C%2520a%2520novel%2520multidisciplinary%2520benchmark%250Acomprising%2520700%2520sentences%2520manually%2520annotated%2520by%2520domain%2520experts%2520and%25202%252C240%250Asentences%2520automatically%2520labelled%2520using%2520LLMs.%2520We%2520evaluate%252037%2520LLMs%2520on%2520this%250Abenchmark%252C%2520spanning%2520diverse%2520model%2520families%2520and%2520sizes%252C%2520using%2520both%2520zero-shot%250Alearning%2520and%2520fine-tuning%2520approaches.%2520The%2520experiments%2520yield%2520several%2520novel%250Ainsights%2520that%2520advance%2520the%2520state%2520of%2520the%2520art%2520in%2520this%2520challenging%2520domain.%2520First%252C%250Athe%2520current%2520generation%2520of%2520LLMs%2520performs%2520remarkably%2520well%2520on%2520this%2520task%2520when%250Afine-tuned%2520on%2520high-quality%2520data%252C%2520achieving%2520performance%2520levels%2520above%252096%255C%2525%2520F1.%250ASecond%252C%2520while%2520large%2520proprietary%2520models%2520like%2520GPT-4o%2520achieve%2520the%2520best%2520results%252C%250Asome%2520lightweight%2520open-source%2520alternatives%2520also%2520demonstrate%2520excellent%250Aperformance.%2520Finally%252C%2520enriching%2520the%2520training%2520data%2520with%2520semi-synthetic%2520examples%250Agenerated%2520by%2520LLMs%2520proves%2520beneficial%252C%2520enabling%2520small%2520encoders%2520to%2520achieve%2520robust%250Aresults%2520and%2520significantly%2520enhancing%2520the%2520performance%2520of%2520several%2520open%2520decoder%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modelling%20and%20Classifying%20the%20Components%20of%20a%20Literature%20Review&entry.906535625=Francisco%20Bola%C3%B1os%20and%20Angelo%20Salatino%20and%20Francesco%20Osborne%20and%20Enrico%20Motta&entry.1292438233=%20%20Previous%20work%20has%20demonstrated%20that%20AI%20methods%20for%20analysing%20scientific%0Aliterature%20benefit%20significantly%20from%20annotating%20sentences%20in%20papers%20according%0Ato%20their%20rhetorical%20roles%2C%20such%20as%20research%20gaps%2C%20results%2C%20limitations%2C%0Aextensions%20of%20existing%20methodologies%2C%20and%20others.%20Such%20representations%20also%0Ahave%20the%20potential%20to%20support%20the%20development%20of%20a%20new%20generation%20of%20systems%0Acapable%20of%20producing%20high-quality%20literature%20reviews.%20However%2C%20achieving%20this%0Agoal%20requires%20the%20definition%20of%20a%20relevant%20annotation%20schema%20and%20effective%0Astrategies%20for%20large-scale%20annotation%20of%20the%20literature.%20This%20paper%20addresses%0Athese%20challenges%20by%201%29%20introducing%20a%20novel%20annotation%20schema%20specifically%0Adesigned%20to%20support%20literature%20review%20generation%20and%202%29%20conducting%20a%0Acomprehensive%20evaluation%20of%20a%20wide%20range%20of%20state-of-the-art%20large%20language%0Amodels%20%28LLMs%29%20in%20classifying%20rhetorical%20roles%20according%20to%20this%20schema.%20To%20this%0Aend%2C%20we%20also%20present%20Sci-Sentence%2C%20a%20novel%20multidisciplinary%20benchmark%0Acomprising%20700%20sentences%20manually%20annotated%20by%20domain%20experts%20and%202%2C240%0Asentences%20automatically%20labelled%20using%20LLMs.%20We%20evaluate%2037%20LLMs%20on%20this%0Abenchmark%2C%20spanning%20diverse%20model%20families%20and%20sizes%2C%20using%20both%20zero-shot%0Alearning%20and%20fine-tuning%20approaches.%20The%20experiments%20yield%20several%20novel%0Ainsights%20that%20advance%20the%20state%20of%20the%20art%20in%20this%20challenging%20domain.%20First%2C%0Athe%20current%20generation%20of%20LLMs%20performs%20remarkably%20well%20on%20this%20task%20when%0Afine-tuned%20on%20high-quality%20data%2C%20achieving%20performance%20levels%20above%2096%5C%25%20F1.%0ASecond%2C%20while%20large%20proprietary%20models%20like%20GPT-4o%20achieve%20the%20best%20results%2C%0Asome%20lightweight%20open-source%20alternatives%20also%20demonstrate%20excellent%0Aperformance.%20Finally%2C%20enriching%20the%20training%20data%20with%20semi-synthetic%20examples%0Agenerated%20by%20LLMs%20proves%20beneficial%2C%20enabling%20small%20encoders%20to%20achieve%20robust%0Aresults%20and%20significantly%20enhancing%20the%20performance%20of%20several%20open%20decoder%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04337v1&entry.124074799=Read"},
{"title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion", "author": "Yutong Wu and Di Huang and Ruosi Wan and Yue Peng and Shijie Shang and Chenrui Cao and Lei Qi and Rui Zhang and Zidong Du and Jie Yan and Xing Hu", "abstract": "  Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.\n", "link": "http://arxiv.org/abs/2508.04440v1", "date": "2025-08-06", "relevancy": 2.4555, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StepFun-Formalizer%3A%20Unlocking%20the%20Autoformalization%20Potential%20of%20LLMs%0A%20%20through%20Knowledge-Reasoning%20Fusion&body=Title%3A%20StepFun-Formalizer%3A%20Unlocking%20the%20Autoformalization%20Potential%20of%20LLMs%0A%20%20through%20Knowledge-Reasoning%20Fusion%0AAuthor%3A%20Yutong%20Wu%20and%20Di%20Huang%20and%20Ruosi%20Wan%20and%20Yue%20Peng%20and%20Shijie%20Shang%20and%20Chenrui%20Cao%20and%20Lei%20Qi%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Jie%20Yan%20and%20Xing%20Hu%0AAbstract%3A%20%20%20Autoformalization%20aims%20to%20translate%20natural-language%20mathematical%20statements%0Ainto%20a%20formal%20language.%20While%20LLMs%20have%20accelerated%20progress%20in%20this%20area%2C%0Aexisting%20methods%20still%20suffer%20from%20low%20accuracy.%20We%20identify%20two%20key%20abilities%0Afor%20effective%20autoformalization%3A%20comprehensive%20mastery%20of%20formal-language%0Adomain%20knowledge%2C%20and%20reasoning%20capability%20of%20natural%20language%20problem%0Aunderstanding%20and%20informal-formal%20alignment.%20Without%20the%20former%2C%20a%20model%20cannot%0Aidentify%20the%20correct%20formal%20objects%3B%20without%20the%20latter%2C%20it%20struggles%20to%0Ainterpret%20real-world%20contexts%20and%20map%20them%20precisely%20into%20formal%20expressions.%0ATo%20address%20these%20gaps%2C%20we%20introduce%20ThinkingF%2C%20a%20data%20synthesis%20and%20training%0Apipeline%20that%20improves%20both%20abilities.%20First%2C%20we%20construct%20two%20datasets%3A%20one%20by%0Adistilling%20and%20selecting%20large-scale%20examples%20rich%20in%20formal%20knowledge%2C%20and%0Aanother%20by%20generating%20informal-to-formal%20reasoning%20trajectories%20guided%20by%0Aexpert-designed%20templates.%20We%20then%20apply%20SFT%20and%20RLVR%20with%20these%20datasets%20to%0Afurther%20fuse%20and%20refine%20the%20two%20abilities.%20The%20resulting%207B%20and%2032B%20models%0Aexhibit%20both%20comprehensive%20formal%20knowledge%20and%20strong%20informal-to-formal%0Areasoning.%20Notably%2C%20StepFun-Formalizer-32B%20achieves%20SOTA%20BEq%401%20scores%20of%2040.5%25%0Aon%20FormalMATH-Lite%20and%2026.7%25%20on%20ProverBench%2C%20surpassing%20all%20prior%0Ageneral-purpose%20and%20specialized%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStepFun-Formalizer%253A%2520Unlocking%2520the%2520Autoformalization%2520Potential%2520of%2520LLMs%250A%2520%2520through%2520Knowledge-Reasoning%2520Fusion%26entry.906535625%3DYutong%2520Wu%2520and%2520Di%2520Huang%2520and%2520Ruosi%2520Wan%2520and%2520Yue%2520Peng%2520and%2520Shijie%2520Shang%2520and%2520Chenrui%2520Cao%2520and%2520Lei%2520Qi%2520and%2520Rui%2520Zhang%2520and%2520Zidong%2520Du%2520and%2520Jie%2520Yan%2520and%2520Xing%2520Hu%26entry.1292438233%3D%2520%2520Autoformalization%2520aims%2520to%2520translate%2520natural-language%2520mathematical%2520statements%250Ainto%2520a%2520formal%2520language.%2520While%2520LLMs%2520have%2520accelerated%2520progress%2520in%2520this%2520area%252C%250Aexisting%2520methods%2520still%2520suffer%2520from%2520low%2520accuracy.%2520We%2520identify%2520two%2520key%2520abilities%250Afor%2520effective%2520autoformalization%253A%2520comprehensive%2520mastery%2520of%2520formal-language%250Adomain%2520knowledge%252C%2520and%2520reasoning%2520capability%2520of%2520natural%2520language%2520problem%250Aunderstanding%2520and%2520informal-formal%2520alignment.%2520Without%2520the%2520former%252C%2520a%2520model%2520cannot%250Aidentify%2520the%2520correct%2520formal%2520objects%253B%2520without%2520the%2520latter%252C%2520it%2520struggles%2520to%250Ainterpret%2520real-world%2520contexts%2520and%2520map%2520them%2520precisely%2520into%2520formal%2520expressions.%250ATo%2520address%2520these%2520gaps%252C%2520we%2520introduce%2520ThinkingF%252C%2520a%2520data%2520synthesis%2520and%2520training%250Apipeline%2520that%2520improves%2520both%2520abilities.%2520First%252C%2520we%2520construct%2520two%2520datasets%253A%2520one%2520by%250Adistilling%2520and%2520selecting%2520large-scale%2520examples%2520rich%2520in%2520formal%2520knowledge%252C%2520and%250Aanother%2520by%2520generating%2520informal-to-formal%2520reasoning%2520trajectories%2520guided%2520by%250Aexpert-designed%2520templates.%2520We%2520then%2520apply%2520SFT%2520and%2520RLVR%2520with%2520these%2520datasets%2520to%250Afurther%2520fuse%2520and%2520refine%2520the%2520two%2520abilities.%2520The%2520resulting%25207B%2520and%252032B%2520models%250Aexhibit%2520both%2520comprehensive%2520formal%2520knowledge%2520and%2520strong%2520informal-to-formal%250Areasoning.%2520Notably%252C%2520StepFun-Formalizer-32B%2520achieves%2520SOTA%2520BEq%25401%2520scores%2520of%252040.5%2525%250Aon%2520FormalMATH-Lite%2520and%252026.7%2525%2520on%2520ProverBench%252C%2520surpassing%2520all%2520prior%250Ageneral-purpose%2520and%2520specialized%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StepFun-Formalizer%3A%20Unlocking%20the%20Autoformalization%20Potential%20of%20LLMs%0A%20%20through%20Knowledge-Reasoning%20Fusion&entry.906535625=Yutong%20Wu%20and%20Di%20Huang%20and%20Ruosi%20Wan%20and%20Yue%20Peng%20and%20Shijie%20Shang%20and%20Chenrui%20Cao%20and%20Lei%20Qi%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Jie%20Yan%20and%20Xing%20Hu&entry.1292438233=%20%20Autoformalization%20aims%20to%20translate%20natural-language%20mathematical%20statements%0Ainto%20a%20formal%20language.%20While%20LLMs%20have%20accelerated%20progress%20in%20this%20area%2C%0Aexisting%20methods%20still%20suffer%20from%20low%20accuracy.%20We%20identify%20two%20key%20abilities%0Afor%20effective%20autoformalization%3A%20comprehensive%20mastery%20of%20formal-language%0Adomain%20knowledge%2C%20and%20reasoning%20capability%20of%20natural%20language%20problem%0Aunderstanding%20and%20informal-formal%20alignment.%20Without%20the%20former%2C%20a%20model%20cannot%0Aidentify%20the%20correct%20formal%20objects%3B%20without%20the%20latter%2C%20it%20struggles%20to%0Ainterpret%20real-world%20contexts%20and%20map%20them%20precisely%20into%20formal%20expressions.%0ATo%20address%20these%20gaps%2C%20we%20introduce%20ThinkingF%2C%20a%20data%20synthesis%20and%20training%0Apipeline%20that%20improves%20both%20abilities.%20First%2C%20we%20construct%20two%20datasets%3A%20one%20by%0Adistilling%20and%20selecting%20large-scale%20examples%20rich%20in%20formal%20knowledge%2C%20and%0Aanother%20by%20generating%20informal-to-formal%20reasoning%20trajectories%20guided%20by%0Aexpert-designed%20templates.%20We%20then%20apply%20SFT%20and%20RLVR%20with%20these%20datasets%20to%0Afurther%20fuse%20and%20refine%20the%20two%20abilities.%20The%20resulting%207B%20and%2032B%20models%0Aexhibit%20both%20comprehensive%20formal%20knowledge%20and%20strong%20informal-to-formal%0Areasoning.%20Notably%2C%20StepFun-Formalizer-32B%20achieves%20SOTA%20BEq%401%20scores%20of%2040.5%25%0Aon%20FormalMATH-Lite%20and%2026.7%25%20on%20ProverBench%2C%20surpassing%20all%20prior%0Ageneral-purpose%20and%20specialized%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04440v1&entry.124074799=Read"},
{"title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient\n  Vision-Language Understanding", "author": "Emmanuelle Bourigault and Pauline Bourigault", "abstract": "  The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding.\n", "link": "http://arxiv.org/abs/2508.04469v1", "date": "2025-08-06", "relevancy": 2.4538, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6329}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6329}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FrEVL%3A%20Leveraging%20Frozen%20Pretrained%20Embeddings%20for%20Efficient%0A%20%20Vision-Language%20Understanding&body=Title%3A%20FrEVL%3A%20Leveraging%20Frozen%20Pretrained%20Embeddings%20for%20Efficient%0A%20%20Vision-Language%20Understanding%0AAuthor%3A%20Emmanuelle%20Bourigault%20and%20Pauline%20Bourigault%0AAbstract%3A%20%20%20The%20deployment%20of%20vision-language%20models%20remains%20constrained%20by%20substantial%0Acomputational%20requirements.%20We%20present%20%5Ctextbf%7BFrEVL%7D%2C%20a%20framework%20exploring%0Awhether%20frozen%20pretrained%20embeddings%20can%20support%20effective%20vision-language%0Aunderstanding.%20Our%20analysis%20reveals%20that%20frozen%20embeddings%20contain%20rich%0Ainformation%20for%20discriminative%20tasks%2C%20achieving%2085%5C%25%20to%2095%5C%25%20of%0Astate-of-the-art%20performance%20on%20standard%20benchmarks%20with%20only%2068.4M%20trainable%0Aparameters.%20This%20performance%20dichotomy%20reveals%20a%20critical%20insight%3A%20frozen%0Aembedding%20effectiveness%20depends%20on%20alignment%20between%20pretraining%20objectives%20and%0Adownstream%20task%20requirements.%20When%20accounting%20for%20end-to-end%20computation%0Aincluding%20embedding%20extraction%2C%20FrEVL%20provides%20%242.3%5Ctimes%24%20speedup%20with%2052%5C%25%0Alower%20energy%20consumption%2C%20making%20it%20suitable%20for%20scenarios%20with%20pre-computable%0Ainputs%20or%20when%20deployment%20constraints%20outweigh%20marginal%20performance%20gains.%20Our%0Aevaluation%20provides%20practitioners%20with%20guidance%20on%20when%20frozen%20embedding%0Aapproaches%20represent%20viable%20alternatives%20to%20full%20model%20deployment.%20We%20will%0Arelease%20our%20complete%20implementation%20and%20evaluation%20framework%20to%20facilitate%0Afurther%20research%20into%20efficient%20multi-modal%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrEVL%253A%2520Leveraging%2520Frozen%2520Pretrained%2520Embeddings%2520for%2520Efficient%250A%2520%2520Vision-Language%2520Understanding%26entry.906535625%3DEmmanuelle%2520Bourigault%2520and%2520Pauline%2520Bourigault%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520vision-language%2520models%2520remains%2520constrained%2520by%2520substantial%250Acomputational%2520requirements.%2520We%2520present%2520%255Ctextbf%257BFrEVL%257D%252C%2520a%2520framework%2520exploring%250Awhether%2520frozen%2520pretrained%2520embeddings%2520can%2520support%2520effective%2520vision-language%250Aunderstanding.%2520Our%2520analysis%2520reveals%2520that%2520frozen%2520embeddings%2520contain%2520rich%250Ainformation%2520for%2520discriminative%2520tasks%252C%2520achieving%252085%255C%2525%2520to%252095%255C%2525%2520of%250Astate-of-the-art%2520performance%2520on%2520standard%2520benchmarks%2520with%2520only%252068.4M%2520trainable%250Aparameters.%2520This%2520performance%2520dichotomy%2520reveals%2520a%2520critical%2520insight%253A%2520frozen%250Aembedding%2520effectiveness%2520depends%2520on%2520alignment%2520between%2520pretraining%2520objectives%2520and%250Adownstream%2520task%2520requirements.%2520When%2520accounting%2520for%2520end-to-end%2520computation%250Aincluding%2520embedding%2520extraction%252C%2520FrEVL%2520provides%2520%25242.3%255Ctimes%2524%2520speedup%2520with%252052%255C%2525%250Alower%2520energy%2520consumption%252C%2520making%2520it%2520suitable%2520for%2520scenarios%2520with%2520pre-computable%250Ainputs%2520or%2520when%2520deployment%2520constraints%2520outweigh%2520marginal%2520performance%2520gains.%2520Our%250Aevaluation%2520provides%2520practitioners%2520with%2520guidance%2520on%2520when%2520frozen%2520embedding%250Aapproaches%2520represent%2520viable%2520alternatives%2520to%2520full%2520model%2520deployment.%2520We%2520will%250Arelease%2520our%2520complete%2520implementation%2520and%2520evaluation%2520framework%2520to%2520facilitate%250Afurther%2520research%2520into%2520efficient%2520multi-modal%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FrEVL%3A%20Leveraging%20Frozen%20Pretrained%20Embeddings%20for%20Efficient%0A%20%20Vision-Language%20Understanding&entry.906535625=Emmanuelle%20Bourigault%20and%20Pauline%20Bourigault&entry.1292438233=%20%20The%20deployment%20of%20vision-language%20models%20remains%20constrained%20by%20substantial%0Acomputational%20requirements.%20We%20present%20%5Ctextbf%7BFrEVL%7D%2C%20a%20framework%20exploring%0Awhether%20frozen%20pretrained%20embeddings%20can%20support%20effective%20vision-language%0Aunderstanding.%20Our%20analysis%20reveals%20that%20frozen%20embeddings%20contain%20rich%0Ainformation%20for%20discriminative%20tasks%2C%20achieving%2085%5C%25%20to%2095%5C%25%20of%0Astate-of-the-art%20performance%20on%20standard%20benchmarks%20with%20only%2068.4M%20trainable%0Aparameters.%20This%20performance%20dichotomy%20reveals%20a%20critical%20insight%3A%20frozen%0Aembedding%20effectiveness%20depends%20on%20alignment%20between%20pretraining%20objectives%20and%0Adownstream%20task%20requirements.%20When%20accounting%20for%20end-to-end%20computation%0Aincluding%20embedding%20extraction%2C%20FrEVL%20provides%20%242.3%5Ctimes%24%20speedup%20with%2052%5C%25%0Alower%20energy%20consumption%2C%20making%20it%20suitable%20for%20scenarios%20with%20pre-computable%0Ainputs%20or%20when%20deployment%20constraints%20outweigh%20marginal%20performance%20gains.%20Our%0Aevaluation%20provides%20practitioners%20with%20guidance%20on%20when%20frozen%20embedding%0Aapproaches%20represent%20viable%20alternatives%20to%20full%20model%20deployment.%20We%20will%0Arelease%20our%20complete%20implementation%20and%20evaluation%20framework%20to%20facilitate%0Afurther%20research%20into%20efficient%20multi-modal%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04469v1&entry.124074799=Read"},
{"title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay", "author": "Yunan Zhang and Shuoran Jiang and Mengchen Zhao and Yuefeng Li and Yang Fan and Xiangping Wu and Qingcai Chen", "abstract": "  The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.\n", "link": "http://arxiv.org/abs/2508.04676v1", "date": "2025-08-06", "relevancy": 2.4422, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4876}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeRe%3A%20Towards%20Efficient%20Anti-Forgetting%20in%20Continual%20Learning%20of%20LLM%20via%0A%20%20General%20Samples%20Replay&body=Title%3A%20GeRe%3A%20Towards%20Efficient%20Anti-Forgetting%20in%20Continual%20Learning%20of%20LLM%20via%0A%20%20General%20Samples%20Replay%0AAuthor%3A%20Yunan%20Zhang%20and%20Shuoran%20Jiang%20and%20Mengchen%20Zhao%20and%20Yuefeng%20Li%20and%20Yang%20Fan%20and%20Xiangping%20Wu%20and%20Qingcai%20Chen%0AAbstract%3A%20%20%20The%20continual%20learning%20capability%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%0Afor%20advancing%20artificial%20general%20intelligence.%20However%2C%20continual%20fine-tuning%0ALLMs%20across%20various%20domains%20often%20suffers%20from%20catastrophic%20forgetting%2C%0Acharacterized%20by%3A%201%29%20significant%20forgetting%20of%20their%20general%20capabilities%2C%20and%0A2%29%20sharp%20performance%20declines%20in%20previously%20learned%20tasks.%20To%20simultaneously%0Aaddress%20both%20issues%20in%20a%20simple%20yet%20stable%20manner%2C%20we%20propose%20General%20Sample%0AReplay%20%28GeRe%29%2C%20a%20framework%20that%20use%20usual%20pretraining%20texts%20for%20efficient%0Aanti-forgetting.%20Beyond%20revisiting%20the%20most%20prevalent%20replay-based%20practices%0Aunder%20GeRe%2C%20we%20further%20leverage%20neural%20states%20to%20introduce%20a%20enhanced%0Aactivation%20states%20constrained%20optimization%20method%20using%20threshold-based%20margin%0A%28TM%29%20loss%2C%20which%20maintains%20activation%20state%20consistency%20during%20replay%20learning.%0AWe%20are%20the%20first%20to%20validate%20that%20a%20small%2C%20fixed%20set%20of%20pre-collected%20general%0Areplay%20samples%20is%20sufficient%20to%20resolve%20both%20concerns--retaining%20general%0Acapabilities%20while%20promoting%20overall%20performance%20across%20sequential%20tasks.%0AIndeed%2C%20the%20former%20can%20inherently%20facilitate%20the%20latter.%20Through%20controlled%0Aexperiments%2C%20we%20systematically%20compare%20TM%20with%20different%20replay%20strategies%0Aunder%20the%20GeRe%20framework%2C%20including%20vanilla%20label%20fitting%2C%20logit%20imitation%20via%0AKL%20divergence%20and%20feature%20imitation%20via%20L1/L2%20losses.%20Results%20demonstrate%20that%0ATM%20consistently%20improves%20performance%20and%20exhibits%20better%20robustness.%20Our%20work%0Apaves%20the%20way%20for%20efficient%20replay%20of%20LLMs%20for%20the%20future.%20Our%20code%20and%20data%0Aare%20available%20at%20https%3A//github.com/Qznan/GeRe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeRe%253A%2520Towards%2520Efficient%2520Anti-Forgetting%2520in%2520Continual%2520Learning%2520of%2520LLM%2520via%250A%2520%2520General%2520Samples%2520Replay%26entry.906535625%3DYunan%2520Zhang%2520and%2520Shuoran%2520Jiang%2520and%2520Mengchen%2520Zhao%2520and%2520Yuefeng%2520Li%2520and%2520Yang%2520Fan%2520and%2520Xiangping%2520Wu%2520and%2520Qingcai%2520Chen%26entry.1292438233%3D%2520%2520The%2520continual%2520learning%2520capability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%250Afor%2520advancing%2520artificial%2520general%2520intelligence.%2520However%252C%2520continual%2520fine-tuning%250ALLMs%2520across%2520various%2520domains%2520often%2520suffers%2520from%2520catastrophic%2520forgetting%252C%250Acharacterized%2520by%253A%25201%2529%2520significant%2520forgetting%2520of%2520their%2520general%2520capabilities%252C%2520and%250A2%2529%2520sharp%2520performance%2520declines%2520in%2520previously%2520learned%2520tasks.%2520To%2520simultaneously%250Aaddress%2520both%2520issues%2520in%2520a%2520simple%2520yet%2520stable%2520manner%252C%2520we%2520propose%2520General%2520Sample%250AReplay%2520%2528GeRe%2529%252C%2520a%2520framework%2520that%2520use%2520usual%2520pretraining%2520texts%2520for%2520efficient%250Aanti-forgetting.%2520Beyond%2520revisiting%2520the%2520most%2520prevalent%2520replay-based%2520practices%250Aunder%2520GeRe%252C%2520we%2520further%2520leverage%2520neural%2520states%2520to%2520introduce%2520a%2520enhanced%250Aactivation%2520states%2520constrained%2520optimization%2520method%2520using%2520threshold-based%2520margin%250A%2528TM%2529%2520loss%252C%2520which%2520maintains%2520activation%2520state%2520consistency%2520during%2520replay%2520learning.%250AWe%2520are%2520the%2520first%2520to%2520validate%2520that%2520a%2520small%252C%2520fixed%2520set%2520of%2520pre-collected%2520general%250Areplay%2520samples%2520is%2520sufficient%2520to%2520resolve%2520both%2520concerns--retaining%2520general%250Acapabilities%2520while%2520promoting%2520overall%2520performance%2520across%2520sequential%2520tasks.%250AIndeed%252C%2520the%2520former%2520can%2520inherently%2520facilitate%2520the%2520latter.%2520Through%2520controlled%250Aexperiments%252C%2520we%2520systematically%2520compare%2520TM%2520with%2520different%2520replay%2520strategies%250Aunder%2520the%2520GeRe%2520framework%252C%2520including%2520vanilla%2520label%2520fitting%252C%2520logit%2520imitation%2520via%250AKL%2520divergence%2520and%2520feature%2520imitation%2520via%2520L1/L2%2520losses.%2520Results%2520demonstrate%2520that%250ATM%2520consistently%2520improves%2520performance%2520and%2520exhibits%2520better%2520robustness.%2520Our%2520work%250Apaves%2520the%2520way%2520for%2520efficient%2520replay%2520of%2520LLMs%2520for%2520the%2520future.%2520Our%2520code%2520and%2520data%250Aare%2520available%2520at%2520https%253A//github.com/Qznan/GeRe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeRe%3A%20Towards%20Efficient%20Anti-Forgetting%20in%20Continual%20Learning%20of%20LLM%20via%0A%20%20General%20Samples%20Replay&entry.906535625=Yunan%20Zhang%20and%20Shuoran%20Jiang%20and%20Mengchen%20Zhao%20and%20Yuefeng%20Li%20and%20Yang%20Fan%20and%20Xiangping%20Wu%20and%20Qingcai%20Chen&entry.1292438233=%20%20The%20continual%20learning%20capability%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%0Afor%20advancing%20artificial%20general%20intelligence.%20However%2C%20continual%20fine-tuning%0ALLMs%20across%20various%20domains%20often%20suffers%20from%20catastrophic%20forgetting%2C%0Acharacterized%20by%3A%201%29%20significant%20forgetting%20of%20their%20general%20capabilities%2C%20and%0A2%29%20sharp%20performance%20declines%20in%20previously%20learned%20tasks.%20To%20simultaneously%0Aaddress%20both%20issues%20in%20a%20simple%20yet%20stable%20manner%2C%20we%20propose%20General%20Sample%0AReplay%20%28GeRe%29%2C%20a%20framework%20that%20use%20usual%20pretraining%20texts%20for%20efficient%0Aanti-forgetting.%20Beyond%20revisiting%20the%20most%20prevalent%20replay-based%20practices%0Aunder%20GeRe%2C%20we%20further%20leverage%20neural%20states%20to%20introduce%20a%20enhanced%0Aactivation%20states%20constrained%20optimization%20method%20using%20threshold-based%20margin%0A%28TM%29%20loss%2C%20which%20maintains%20activation%20state%20consistency%20during%20replay%20learning.%0AWe%20are%20the%20first%20to%20validate%20that%20a%20small%2C%20fixed%20set%20of%20pre-collected%20general%0Areplay%20samples%20is%20sufficient%20to%20resolve%20both%20concerns--retaining%20general%0Acapabilities%20while%20promoting%20overall%20performance%20across%20sequential%20tasks.%0AIndeed%2C%20the%20former%20can%20inherently%20facilitate%20the%20latter.%20Through%20controlled%0Aexperiments%2C%20we%20systematically%20compare%20TM%20with%20different%20replay%20strategies%0Aunder%20the%20GeRe%20framework%2C%20including%20vanilla%20label%20fitting%2C%20logit%20imitation%20via%0AKL%20divergence%20and%20feature%20imitation%20via%20L1/L2%20losses.%20Results%20demonstrate%20that%0ATM%20consistently%20improves%20performance%20and%20exhibits%20better%20robustness.%20Our%20work%0Apaves%20the%20way%20for%20efficient%20replay%20of%20LLMs%20for%20the%20future.%20Our%20code%20and%20data%0Aare%20available%20at%20https%3A//github.com/Qznan/GeRe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04676v1&entry.124074799=Read"},
{"title": "On the Fundamental Impossibility of Hallucination Control in Large\n  Language Models", "author": "Micha\u0142 P. Karpowicz", "abstract": "  This paper establishes a fundamental impossibility theorem: no LLM capable\nperforming non-trivial knowledge aggregation can simultaneously achieve\ntruthful (internally consistent) knowledge representation, semantic information\nconservation, complete revelation of relevant knowledge, and\nknowledge-constrained optimality. This impossibility is not an engineering\nlimitation but arises from the mathematical structure of information\naggregation itself. We establish this result by describing the inference\nprocess as an auction of ideas, where distributed components compete exploiting\ntheir partial knowledge to shape responses. The proof spans three independent\nmathematical domains: mechanism design theory (Green-Laffont), the theory of\nproper scoring rules (Savage), and direct architectural analysis of\ntransformers (Log-Sum-Exp convexity). In particular, we show how in the\nstrictly concave settings the score of an aggregate of diverse beliefs strictly\nexceeds the sum of individual scores. That gap may quantify the creation of\nunattributable certainty or overconfidence -- the mathematical origin of both\nhallucination and creativity, or imagination.\n  To support this analysis, we introduce the complementary concepts of the\nsemantic information measure and the emergence operator to model bounded\nreasoning in a general setting. We prove that while bounded reasoning generates\naccessible information, providing valuable insights and inspirations, idealized\nreasoning strictly preserves semantic content. By demonstrating that\nhallucination and imagination are mathematically identical phenomena-grounded\nin the necessary violation of information conservation-this paper offers a\nprincipled foundation for managing these behaviors in advanced AI systems.\nFinally, we present some speculative ideas to inspire evaluation and\nrefinements of the proposed theory.\n", "link": "http://arxiv.org/abs/2506.06382v4", "date": "2025-08-06", "relevancy": 2.4274, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Fundamental%20Impossibility%20of%20Hallucination%20Control%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20On%20the%20Fundamental%20Impossibility%20of%20Hallucination%20Control%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Micha%C5%82%20P.%20Karpowicz%0AAbstract%3A%20%20%20This%20paper%20establishes%20a%20fundamental%20impossibility%20theorem%3A%20no%20LLM%20capable%0Aperforming%20non-trivial%20knowledge%20aggregation%20can%20simultaneously%20achieve%0Atruthful%20%28internally%20consistent%29%20knowledge%20representation%2C%20semantic%20information%0Aconservation%2C%20complete%20revelation%20of%20relevant%20knowledge%2C%20and%0Aknowledge-constrained%20optimality.%20This%20impossibility%20is%20not%20an%20engineering%0Alimitation%20but%20arises%20from%20the%20mathematical%20structure%20of%20information%0Aaggregation%20itself.%20We%20establish%20this%20result%20by%20describing%20the%20inference%0Aprocess%20as%20an%20auction%20of%20ideas%2C%20where%20distributed%20components%20compete%20exploiting%0Atheir%20partial%20knowledge%20to%20shape%20responses.%20The%20proof%20spans%20three%20independent%0Amathematical%20domains%3A%20mechanism%20design%20theory%20%28Green-Laffont%29%2C%20the%20theory%20of%0Aproper%20scoring%20rules%20%28Savage%29%2C%20and%20direct%20architectural%20analysis%20of%0Atransformers%20%28Log-Sum-Exp%20convexity%29.%20In%20particular%2C%20we%20show%20how%20in%20the%0Astrictly%20concave%20settings%20the%20score%20of%20an%20aggregate%20of%20diverse%20beliefs%20strictly%0Aexceeds%20the%20sum%20of%20individual%20scores.%20That%20gap%20may%20quantify%20the%20creation%20of%0Aunattributable%20certainty%20or%20overconfidence%20--%20the%20mathematical%20origin%20of%20both%0Ahallucination%20and%20creativity%2C%20or%20imagination.%0A%20%20To%20support%20this%20analysis%2C%20we%20introduce%20the%20complementary%20concepts%20of%20the%0Asemantic%20information%20measure%20and%20the%20emergence%20operator%20to%20model%20bounded%0Areasoning%20in%20a%20general%20setting.%20We%20prove%20that%20while%20bounded%20reasoning%20generates%0Aaccessible%20information%2C%20providing%20valuable%20insights%20and%20inspirations%2C%20idealized%0Areasoning%20strictly%20preserves%20semantic%20content.%20By%20demonstrating%20that%0Ahallucination%20and%20imagination%20are%20mathematically%20identical%20phenomena-grounded%0Ain%20the%20necessary%20violation%20of%20information%20conservation-this%20paper%20offers%20a%0Aprincipled%20foundation%20for%20managing%20these%20behaviors%20in%20advanced%20AI%20systems.%0AFinally%2C%20we%20present%20some%20speculative%20ideas%20to%20inspire%20evaluation%20and%0Arefinements%20of%20the%20proposed%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06382v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Fundamental%2520Impossibility%2520of%2520Hallucination%2520Control%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DMicha%25C5%2582%2520P.%2520Karpowicz%26entry.1292438233%3D%2520%2520This%2520paper%2520establishes%2520a%2520fundamental%2520impossibility%2520theorem%253A%2520no%2520LLM%2520capable%250Aperforming%2520non-trivial%2520knowledge%2520aggregation%2520can%2520simultaneously%2520achieve%250Atruthful%2520%2528internally%2520consistent%2529%2520knowledge%2520representation%252C%2520semantic%2520information%250Aconservation%252C%2520complete%2520revelation%2520of%2520relevant%2520knowledge%252C%2520and%250Aknowledge-constrained%2520optimality.%2520This%2520impossibility%2520is%2520not%2520an%2520engineering%250Alimitation%2520but%2520arises%2520from%2520the%2520mathematical%2520structure%2520of%2520information%250Aaggregation%2520itself.%2520We%2520establish%2520this%2520result%2520by%2520describing%2520the%2520inference%250Aprocess%2520as%2520an%2520auction%2520of%2520ideas%252C%2520where%2520distributed%2520components%2520compete%2520exploiting%250Atheir%2520partial%2520knowledge%2520to%2520shape%2520responses.%2520The%2520proof%2520spans%2520three%2520independent%250Amathematical%2520domains%253A%2520mechanism%2520design%2520theory%2520%2528Green-Laffont%2529%252C%2520the%2520theory%2520of%250Aproper%2520scoring%2520rules%2520%2528Savage%2529%252C%2520and%2520direct%2520architectural%2520analysis%2520of%250Atransformers%2520%2528Log-Sum-Exp%2520convexity%2529.%2520In%2520particular%252C%2520we%2520show%2520how%2520in%2520the%250Astrictly%2520concave%2520settings%2520the%2520score%2520of%2520an%2520aggregate%2520of%2520diverse%2520beliefs%2520strictly%250Aexceeds%2520the%2520sum%2520of%2520individual%2520scores.%2520That%2520gap%2520may%2520quantify%2520the%2520creation%2520of%250Aunattributable%2520certainty%2520or%2520overconfidence%2520--%2520the%2520mathematical%2520origin%2520of%2520both%250Ahallucination%2520and%2520creativity%252C%2520or%2520imagination.%250A%2520%2520To%2520support%2520this%2520analysis%252C%2520we%2520introduce%2520the%2520complementary%2520concepts%2520of%2520the%250Asemantic%2520information%2520measure%2520and%2520the%2520emergence%2520operator%2520to%2520model%2520bounded%250Areasoning%2520in%2520a%2520general%2520setting.%2520We%2520prove%2520that%2520while%2520bounded%2520reasoning%2520generates%250Aaccessible%2520information%252C%2520providing%2520valuable%2520insights%2520and%2520inspirations%252C%2520idealized%250Areasoning%2520strictly%2520preserves%2520semantic%2520content.%2520By%2520demonstrating%2520that%250Ahallucination%2520and%2520imagination%2520are%2520mathematically%2520identical%2520phenomena-grounded%250Ain%2520the%2520necessary%2520violation%2520of%2520information%2520conservation-this%2520paper%2520offers%2520a%250Aprincipled%2520foundation%2520for%2520managing%2520these%2520behaviors%2520in%2520advanced%2520AI%2520systems.%250AFinally%252C%2520we%2520present%2520some%2520speculative%2520ideas%2520to%2520inspire%2520evaluation%2520and%250Arefinements%2520of%2520the%2520proposed%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06382v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Fundamental%20Impossibility%20of%20Hallucination%20Control%20in%20Large%0A%20%20Language%20Models&entry.906535625=Micha%C5%82%20P.%20Karpowicz&entry.1292438233=%20%20This%20paper%20establishes%20a%20fundamental%20impossibility%20theorem%3A%20no%20LLM%20capable%0Aperforming%20non-trivial%20knowledge%20aggregation%20can%20simultaneously%20achieve%0Atruthful%20%28internally%20consistent%29%20knowledge%20representation%2C%20semantic%20information%0Aconservation%2C%20complete%20revelation%20of%20relevant%20knowledge%2C%20and%0Aknowledge-constrained%20optimality.%20This%20impossibility%20is%20not%20an%20engineering%0Alimitation%20but%20arises%20from%20the%20mathematical%20structure%20of%20information%0Aaggregation%20itself.%20We%20establish%20this%20result%20by%20describing%20the%20inference%0Aprocess%20as%20an%20auction%20of%20ideas%2C%20where%20distributed%20components%20compete%20exploiting%0Atheir%20partial%20knowledge%20to%20shape%20responses.%20The%20proof%20spans%20three%20independent%0Amathematical%20domains%3A%20mechanism%20design%20theory%20%28Green-Laffont%29%2C%20the%20theory%20of%0Aproper%20scoring%20rules%20%28Savage%29%2C%20and%20direct%20architectural%20analysis%20of%0Atransformers%20%28Log-Sum-Exp%20convexity%29.%20In%20particular%2C%20we%20show%20how%20in%20the%0Astrictly%20concave%20settings%20the%20score%20of%20an%20aggregate%20of%20diverse%20beliefs%20strictly%0Aexceeds%20the%20sum%20of%20individual%20scores.%20That%20gap%20may%20quantify%20the%20creation%20of%0Aunattributable%20certainty%20or%20overconfidence%20--%20the%20mathematical%20origin%20of%20both%0Ahallucination%20and%20creativity%2C%20or%20imagination.%0A%20%20To%20support%20this%20analysis%2C%20we%20introduce%20the%20complementary%20concepts%20of%20the%0Asemantic%20information%20measure%20and%20the%20emergence%20operator%20to%20model%20bounded%0Areasoning%20in%20a%20general%20setting.%20We%20prove%20that%20while%20bounded%20reasoning%20generates%0Aaccessible%20information%2C%20providing%20valuable%20insights%20and%20inspirations%2C%20idealized%0Areasoning%20strictly%20preserves%20semantic%20content.%20By%20demonstrating%20that%0Ahallucination%20and%20imagination%20are%20mathematically%20identical%20phenomena-grounded%0Ain%20the%20necessary%20violation%20of%20information%20conservation-this%20paper%20offers%20a%0Aprincipled%20foundation%20for%20managing%20these%20behaviors%20in%20advanced%20AI%20systems.%0AFinally%2C%20we%20present%20some%20speculative%20ideas%20to%20inspire%20evaluation%20and%0Arefinements%20of%20the%20proposed%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06382v4&entry.124074799=Read"},
{"title": "T2VEval: Benchmark Dataset and Objective Evaluation Method for\n  T2V-generated Videos", "author": "Zelu Qi and Ping Shi and Shuqi Wang and Chaoyang Zhang and Fei Zhao and Zefeng Ying and Da Pan and Xi Yang and Zheqi He and Teng Dai", "abstract": "  Recent advances in text-to-video (T2V) technology, as demonstrated by models\nsuch as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the\napplicability and popularity of the technology. This progress has created a\ngrowing demand for accurate quality assessment metrics to evaluate the\nperceptual quality of T2V-generated videos and optimize video generation\nmodels. However, assessing the quality of text-to-video outputs remain\nchallenging due to the presence of highly complex distortions, such as\nunnatural actions and phenomena that defy human cognition. To address these\nchallenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset\nfor text-to-video quality evaluation, which contains 148 textual prompts and\n1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation,\nwe scored each video on four dimensions in the subjective experiment, which are\noverall impression, text-video consistency, realness, and technical quality.\nBased on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for\nT2V quality evaluation. T2VEval assesses videos across three branches:\ntext-video consistency, realness, and technical quality. Using an\nattention-based fusion module, T2VEval effectively integrates features from\neach branch and predicts scores with the aid of a large language model.\nAdditionally, we implemented a divide-and-conquer training strategy, enabling\neach branch to learn targeted knowledge while maintaining synergy with the\nothers. Experimental results demonstrate that T2VEval achieves state-of-the-art\nperformance across multiple metrics.\n", "link": "http://arxiv.org/abs/2501.08545v7", "date": "2025-08-06", "relevancy": 2.3959, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6287}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6138}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2VEval%3A%20Benchmark%20Dataset%20and%20Objective%20Evaluation%20Method%20for%0A%20%20T2V-generated%20Videos&body=Title%3A%20T2VEval%3A%20Benchmark%20Dataset%20and%20Objective%20Evaluation%20Method%20for%0A%20%20T2V-generated%20Videos%0AAuthor%3A%20Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Chaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan%20and%20Xi%20Yang%20and%20Zheqi%20He%20and%20Teng%20Dai%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20which%20contains%20148%20textual%20prompts%20and%0A1%2C783%20videos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%0Awe%20scored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20divide-and-conquer%20training%20strategy%2C%20enabling%0Aeach%20branch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%0Aothers.%20Experimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08545v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2VEval%253A%2520Benchmark%2520Dataset%2520and%2520Objective%2520Evaluation%2520Method%2520for%250A%2520%2520T2V-generated%2520Videos%26entry.906535625%3DZelu%2520Qi%2520and%2520Ping%2520Shi%2520and%2520Shuqi%2520Wang%2520and%2520Chaoyang%2520Zhang%2520and%2520Fei%2520Zhao%2520and%2520Zefeng%2520Ying%2520and%2520Da%2520Pan%2520and%2520Xi%2520Yang%2520and%2520Zheqi%2520He%2520and%2520Teng%2520Dai%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-video%2520%2528T2V%2529%2520technology%252C%2520as%2520demonstrated%2520by%2520models%250Asuch%2520as%2520Runway%2520Gen-3%252C%2520Pika%252C%2520Sora%252C%2520and%2520Kling%252C%2520have%2520significantly%2520broadened%2520the%250Aapplicability%2520and%2520popularity%2520of%2520the%2520technology.%2520This%2520progress%2520has%2520created%2520a%250Agrowing%2520demand%2520for%2520accurate%2520quality%2520assessment%2520metrics%2520to%2520evaluate%2520the%250Aperceptual%2520quality%2520of%2520T2V-generated%2520videos%2520and%2520optimize%2520video%2520generation%250Amodels.%2520However%252C%2520assessing%2520the%2520quality%2520of%2520text-to-video%2520outputs%2520remain%250Achallenging%2520due%2520to%2520the%2520presence%2520of%2520highly%2520complex%2520distortions%252C%2520such%2520as%250Aunnatural%2520actions%2520and%2520phenomena%2520that%2520defy%2520human%2520cognition.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520constructed%2520T2VEval-Bench%252C%2520a%2520multi-dimensional%2520benchmark%2520dataset%250Afor%2520text-to-video%2520quality%2520evaluation%252C%2520which%2520contains%2520148%2520textual%2520prompts%2520and%250A1%252C783%2520videos%2520generated%2520by%252013%2520T2V%2520models.%2520To%2520ensure%2520a%2520comprehensive%2520evaluation%252C%250Awe%2520scored%2520each%2520video%2520on%2520four%2520dimensions%2520in%2520the%2520subjective%2520experiment%252C%2520which%2520are%250Aoverall%2520impression%252C%2520text-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%250ABased%2520on%2520T2VEval-Bench%252C%2520we%2520developed%2520T2VEval%252C%2520a%2520multi-branch%2520fusion%2520scheme%2520for%250AT2V%2520quality%2520evaluation.%2520T2VEval%2520assesses%2520videos%2520across%2520three%2520branches%253A%250Atext-video%2520consistency%252C%2520realness%252C%2520and%2520technical%2520quality.%2520Using%2520an%250Aattention-based%2520fusion%2520module%252C%2520T2VEval%2520effectively%2520integrates%2520features%2520from%250Aeach%2520branch%2520and%2520predicts%2520scores%2520with%2520the%2520aid%2520of%2520a%2520large%2520language%2520model.%250AAdditionally%252C%2520we%2520implemented%2520a%2520divide-and-conquer%2520training%2520strategy%252C%2520enabling%250Aeach%2520branch%2520to%2520learn%2520targeted%2520knowledge%2520while%2520maintaining%2520synergy%2520with%2520the%250Aothers.%2520Experimental%2520results%2520demonstrate%2520that%2520T2VEval%2520achieves%2520state-of-the-art%250Aperformance%2520across%2520multiple%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08545v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2VEval%3A%20Benchmark%20Dataset%20and%20Objective%20Evaluation%20Method%20for%0A%20%20T2V-generated%20Videos&entry.906535625=Zelu%20Qi%20and%20Ping%20Shi%20and%20Shuqi%20Wang%20and%20Chaoyang%20Zhang%20and%20Fei%20Zhao%20and%20Zefeng%20Ying%20and%20Da%20Pan%20and%20Xi%20Yang%20and%20Zheqi%20He%20and%20Teng%20Dai&entry.1292438233=%20%20Recent%20advances%20in%20text-to-video%20%28T2V%29%20technology%2C%20as%20demonstrated%20by%20models%0Asuch%20as%20Runway%20Gen-3%2C%20Pika%2C%20Sora%2C%20and%20Kling%2C%20have%20significantly%20broadened%20the%0Aapplicability%20and%20popularity%20of%20the%20technology.%20This%20progress%20has%20created%20a%0Agrowing%20demand%20for%20accurate%20quality%20assessment%20metrics%20to%20evaluate%20the%0Aperceptual%20quality%20of%20T2V-generated%20videos%20and%20optimize%20video%20generation%0Amodels.%20However%2C%20assessing%20the%20quality%20of%20text-to-video%20outputs%20remain%0Achallenging%20due%20to%20the%20presence%20of%20highly%20complex%20distortions%2C%20such%20as%0Aunnatural%20actions%20and%20phenomena%20that%20defy%20human%20cognition.%20To%20address%20these%0Achallenges%2C%20we%20constructed%20T2VEval-Bench%2C%20a%20multi-dimensional%20benchmark%20dataset%0Afor%20text-to-video%20quality%20evaluation%2C%20which%20contains%20148%20textual%20prompts%20and%0A1%2C783%20videos%20generated%20by%2013%20T2V%20models.%20To%20ensure%20a%20comprehensive%20evaluation%2C%0Awe%20scored%20each%20video%20on%20four%20dimensions%20in%20the%20subjective%20experiment%2C%20which%20are%0Aoverall%20impression%2C%20text-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%0ABased%20on%20T2VEval-Bench%2C%20we%20developed%20T2VEval%2C%20a%20multi-branch%20fusion%20scheme%20for%0AT2V%20quality%20evaluation.%20T2VEval%20assesses%20videos%20across%20three%20branches%3A%0Atext-video%20consistency%2C%20realness%2C%20and%20technical%20quality.%20Using%20an%0Aattention-based%20fusion%20module%2C%20T2VEval%20effectively%20integrates%20features%20from%0Aeach%20branch%20and%20predicts%20scores%20with%20the%20aid%20of%20a%20large%20language%20model.%0AAdditionally%2C%20we%20implemented%20a%20divide-and-conquer%20training%20strategy%2C%20enabling%0Aeach%20branch%20to%20learn%20targeted%20knowledge%20while%20maintaining%20synergy%20with%20the%0Aothers.%20Experimental%20results%20demonstrate%20that%20T2VEval%20achieves%20state-of-the-art%0Aperformance%20across%20multiple%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08545v7&entry.124074799=Read"},
{"title": "Deep Learning-based Scalable Image-to-3D Facade Parser for Generating\n  Thermal 3D Building Models", "author": "Yinan Yu and Alex Gonzalez-Caceres and Samuel Scheidegger and Sanjay Somanath and Alexander Hollberg", "abstract": "  Renovating existing buildings is essential for climate impact. Early-phase\nrenovation planning requires simulations based on thermal 3D models at Level of\nDetail (LoD) 3, which include features like windows. However, scalable and\naccurate identification of such features remains a challenge. This paper\npresents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that\ngenerates LoD3 thermal models by extracting geometries from images using both\ncomputer vision and deep learning. Unlike existing methods relying on\nsegmentation and projection, SI3FP directly models geometric primitives in the\northographic image plane, providing a unified interface while reducing\nperspective distortions. SI3FP supports both sparse (e.g., Google Street View)\nand dense (e.g., hand-held camera) data sources. Tested on typical Swedish\nresidential buildings, SI3FP achieved approximately 5% error in window-to-wall\nratio estimates, demonstrating sufficient accuracy for early-stage renovation\nanalysis. The pipeline facilitates large-scale energy renovation planning and\nhas broader applications in urban development and planning.\n", "link": "http://arxiv.org/abs/2508.04406v1", "date": "2025-08-06", "relevancy": 2.3816, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6006}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6006}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-based%20Scalable%20Image-to-3D%20Facade%20Parser%20for%20Generating%0A%20%20Thermal%203D%20Building%20Models&body=Title%3A%20Deep%20Learning-based%20Scalable%20Image-to-3D%20Facade%20Parser%20for%20Generating%0A%20%20Thermal%203D%20Building%20Models%0AAuthor%3A%20Yinan%20Yu%20and%20Alex%20Gonzalez-Caceres%20and%20Samuel%20Scheidegger%20and%20Sanjay%20Somanath%20and%20Alexander%20Hollberg%0AAbstract%3A%20%20%20Renovating%20existing%20buildings%20is%20essential%20for%20climate%20impact.%20Early-phase%0Arenovation%20planning%20requires%20simulations%20based%20on%20thermal%203D%20models%20at%20Level%20of%0ADetail%20%28LoD%29%203%2C%20which%20include%20features%20like%20windows.%20However%2C%20scalable%20and%0Aaccurate%20identification%20of%20such%20features%20remains%20a%20challenge.%20This%20paper%0Apresents%20the%20Scalable%20Image-to-3D%20Facade%20Parser%20%28SI3FP%29%2C%20a%20pipeline%20that%0Agenerates%20LoD3%20thermal%20models%20by%20extracting%20geometries%20from%20images%20using%20both%0Acomputer%20vision%20and%20deep%20learning.%20Unlike%20existing%20methods%20relying%20on%0Asegmentation%20and%20projection%2C%20SI3FP%20directly%20models%20geometric%20primitives%20in%20the%0Aorthographic%20image%20plane%2C%20providing%20a%20unified%20interface%20while%20reducing%0Aperspective%20distortions.%20SI3FP%20supports%20both%20sparse%20%28e.g.%2C%20Google%20Street%20View%29%0Aand%20dense%20%28e.g.%2C%20hand-held%20camera%29%20data%20sources.%20Tested%20on%20typical%20Swedish%0Aresidential%20buildings%2C%20SI3FP%20achieved%20approximately%205%25%20error%20in%20window-to-wall%0Aratio%20estimates%2C%20demonstrating%20sufficient%20accuracy%20for%20early-stage%20renovation%0Aanalysis.%20The%20pipeline%20facilitates%20large-scale%20energy%20renovation%20planning%20and%0Ahas%20broader%20applications%20in%20urban%20development%20and%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-based%2520Scalable%2520Image-to-3D%2520Facade%2520Parser%2520for%2520Generating%250A%2520%2520Thermal%25203D%2520Building%2520Models%26entry.906535625%3DYinan%2520Yu%2520and%2520Alex%2520Gonzalez-Caceres%2520and%2520Samuel%2520Scheidegger%2520and%2520Sanjay%2520Somanath%2520and%2520Alexander%2520Hollberg%26entry.1292438233%3D%2520%2520Renovating%2520existing%2520buildings%2520is%2520essential%2520for%2520climate%2520impact.%2520Early-phase%250Arenovation%2520planning%2520requires%2520simulations%2520based%2520on%2520thermal%25203D%2520models%2520at%2520Level%2520of%250ADetail%2520%2528LoD%2529%25203%252C%2520which%2520include%2520features%2520like%2520windows.%2520However%252C%2520scalable%2520and%250Aaccurate%2520identification%2520of%2520such%2520features%2520remains%2520a%2520challenge.%2520This%2520paper%250Apresents%2520the%2520Scalable%2520Image-to-3D%2520Facade%2520Parser%2520%2528SI3FP%2529%252C%2520a%2520pipeline%2520that%250Agenerates%2520LoD3%2520thermal%2520models%2520by%2520extracting%2520geometries%2520from%2520images%2520using%2520both%250Acomputer%2520vision%2520and%2520deep%2520learning.%2520Unlike%2520existing%2520methods%2520relying%2520on%250Asegmentation%2520and%2520projection%252C%2520SI3FP%2520directly%2520models%2520geometric%2520primitives%2520in%2520the%250Aorthographic%2520image%2520plane%252C%2520providing%2520a%2520unified%2520interface%2520while%2520reducing%250Aperspective%2520distortions.%2520SI3FP%2520supports%2520both%2520sparse%2520%2528e.g.%252C%2520Google%2520Street%2520View%2529%250Aand%2520dense%2520%2528e.g.%252C%2520hand-held%2520camera%2529%2520data%2520sources.%2520Tested%2520on%2520typical%2520Swedish%250Aresidential%2520buildings%252C%2520SI3FP%2520achieved%2520approximately%25205%2525%2520error%2520in%2520window-to-wall%250Aratio%2520estimates%252C%2520demonstrating%2520sufficient%2520accuracy%2520for%2520early-stage%2520renovation%250Aanalysis.%2520The%2520pipeline%2520facilitates%2520large-scale%2520energy%2520renovation%2520planning%2520and%250Ahas%2520broader%2520applications%2520in%2520urban%2520development%2520and%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-based%20Scalable%20Image-to-3D%20Facade%20Parser%20for%20Generating%0A%20%20Thermal%203D%20Building%20Models&entry.906535625=Yinan%20Yu%20and%20Alex%20Gonzalez-Caceres%20and%20Samuel%20Scheidegger%20and%20Sanjay%20Somanath%20and%20Alexander%20Hollberg&entry.1292438233=%20%20Renovating%20existing%20buildings%20is%20essential%20for%20climate%20impact.%20Early-phase%0Arenovation%20planning%20requires%20simulations%20based%20on%20thermal%203D%20models%20at%20Level%20of%0ADetail%20%28LoD%29%203%2C%20which%20include%20features%20like%20windows.%20However%2C%20scalable%20and%0Aaccurate%20identification%20of%20such%20features%20remains%20a%20challenge.%20This%20paper%0Apresents%20the%20Scalable%20Image-to-3D%20Facade%20Parser%20%28SI3FP%29%2C%20a%20pipeline%20that%0Agenerates%20LoD3%20thermal%20models%20by%20extracting%20geometries%20from%20images%20using%20both%0Acomputer%20vision%20and%20deep%20learning.%20Unlike%20existing%20methods%20relying%20on%0Asegmentation%20and%20projection%2C%20SI3FP%20directly%20models%20geometric%20primitives%20in%20the%0Aorthographic%20image%20plane%2C%20providing%20a%20unified%20interface%20while%20reducing%0Aperspective%20distortions.%20SI3FP%20supports%20both%20sparse%20%28e.g.%2C%20Google%20Street%20View%29%0Aand%20dense%20%28e.g.%2C%20hand-held%20camera%29%20data%20sources.%20Tested%20on%20typical%20Swedish%0Aresidential%20buildings%2C%20SI3FP%20achieved%20approximately%205%25%20error%20in%20window-to-wall%0Aratio%20estimates%2C%20demonstrating%20sufficient%20accuracy%20for%20early-stage%20renovation%0Aanalysis.%20The%20pipeline%20facilitates%20large-scale%20energy%20renovation%20planning%20and%0Ahas%20broader%20applications%20in%20urban%20development%20and%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04406v1&entry.124074799=Read"},
{"title": "Self-Questioning Language Models", "author": "Lili Chen and Mihir Prabhudesai and Katerina Fragkiadaki and Hao Liu and Deepak Pathak", "abstract": "  Can large language models improve without external data -- by generating\ntheir own questions and answers? We hypothesize that a pre-trained language\nmodel can improve its reasoning skills given only a single prompt specifying\nthe topic (e.g., algebra word problems) and asking the model to generate its\nown questions. To do this, we propose Self-Questioning Language Models (SQLM):\nan asymmetric self-play framework where a proposer is given the topic and\ngenerates a question for a solver, who tries to answer it. Both the proposer\nand solver are trained via reinforcement learning. The proposer receives a\nreward if the problem is not too easy or too difficult, and the solver receives\na reward based on majority voting, a proxy for correctness in the absence of\nground-truth answers. For coding, the proposer can instead generate unit tests\nwhich are used for verification. We study this asymmetric self-play framework\non three benchmarks: three-digit multiplication, algebra problems from the\nOMEGA benchmark, and programming problems from Codeforces. By continually\ngenerating more interesting problems and attempting to solve them, language\nmodels can improve on downstream benchmarks without access to any curated\ntraining datasets.\n", "link": "http://arxiv.org/abs/2508.03682v2", "date": "2025-08-06", "relevancy": 2.3763, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Questioning%20Language%20Models&body=Title%3A%20Self-Questioning%20Language%20Models%0AAuthor%3A%20Lili%20Chen%20and%20Mihir%20Prabhudesai%20and%20Katerina%20Fragkiadaki%20and%20Hao%20Liu%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20Can%20large%20language%20models%20improve%20without%20external%20data%20--%20by%20generating%0Atheir%20own%20questions%20and%20answers%3F%20We%20hypothesize%20that%20a%20pre-trained%20language%0Amodel%20can%20improve%20its%20reasoning%20skills%20given%20only%20a%20single%20prompt%20specifying%0Athe%20topic%20%28e.g.%2C%20algebra%20word%20problems%29%20and%20asking%20the%20model%20to%20generate%20its%0Aown%20questions.%20To%20do%20this%2C%20we%20propose%20Self-Questioning%20Language%20Models%20%28SQLM%29%3A%0Aan%20asymmetric%20self-play%20framework%20where%20a%20proposer%20is%20given%20the%20topic%20and%0Agenerates%20a%20question%20for%20a%20solver%2C%20who%20tries%20to%20answer%20it.%20Both%20the%20proposer%0Aand%20solver%20are%20trained%20via%20reinforcement%20learning.%20The%20proposer%20receives%20a%0Areward%20if%20the%20problem%20is%20not%20too%20easy%20or%20too%20difficult%2C%20and%20the%20solver%20receives%0Aa%20reward%20based%20on%20majority%20voting%2C%20a%20proxy%20for%20correctness%20in%20the%20absence%20of%0Aground-truth%20answers.%20For%20coding%2C%20the%20proposer%20can%20instead%20generate%20unit%20tests%0Awhich%20are%20used%20for%20verification.%20We%20study%20this%20asymmetric%20self-play%20framework%0Aon%20three%20benchmarks%3A%20three-digit%20multiplication%2C%20algebra%20problems%20from%20the%0AOMEGA%20benchmark%2C%20and%20programming%20problems%20from%20Codeforces.%20By%20continually%0Agenerating%20more%20interesting%20problems%20and%20attempting%20to%20solve%20them%2C%20language%0Amodels%20can%20improve%20on%20downstream%20benchmarks%20without%20access%20to%20any%20curated%0Atraining%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03682v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Questioning%2520Language%2520Models%26entry.906535625%3DLili%2520Chen%2520and%2520Mihir%2520Prabhudesai%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Hao%2520Liu%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520Can%2520large%2520language%2520models%2520improve%2520without%2520external%2520data%2520--%2520by%2520generating%250Atheir%2520own%2520questions%2520and%2520answers%253F%2520We%2520hypothesize%2520that%2520a%2520pre-trained%2520language%250Amodel%2520can%2520improve%2520its%2520reasoning%2520skills%2520given%2520only%2520a%2520single%2520prompt%2520specifying%250Athe%2520topic%2520%2528e.g.%252C%2520algebra%2520word%2520problems%2529%2520and%2520asking%2520the%2520model%2520to%2520generate%2520its%250Aown%2520questions.%2520To%2520do%2520this%252C%2520we%2520propose%2520Self-Questioning%2520Language%2520Models%2520%2528SQLM%2529%253A%250Aan%2520asymmetric%2520self-play%2520framework%2520where%2520a%2520proposer%2520is%2520given%2520the%2520topic%2520and%250Agenerates%2520a%2520question%2520for%2520a%2520solver%252C%2520who%2520tries%2520to%2520answer%2520it.%2520Both%2520the%2520proposer%250Aand%2520solver%2520are%2520trained%2520via%2520reinforcement%2520learning.%2520The%2520proposer%2520receives%2520a%250Areward%2520if%2520the%2520problem%2520is%2520not%2520too%2520easy%2520or%2520too%2520difficult%252C%2520and%2520the%2520solver%2520receives%250Aa%2520reward%2520based%2520on%2520majority%2520voting%252C%2520a%2520proxy%2520for%2520correctness%2520in%2520the%2520absence%2520of%250Aground-truth%2520answers.%2520For%2520coding%252C%2520the%2520proposer%2520can%2520instead%2520generate%2520unit%2520tests%250Awhich%2520are%2520used%2520for%2520verification.%2520We%2520study%2520this%2520asymmetric%2520self-play%2520framework%250Aon%2520three%2520benchmarks%253A%2520three-digit%2520multiplication%252C%2520algebra%2520problems%2520from%2520the%250AOMEGA%2520benchmark%252C%2520and%2520programming%2520problems%2520from%2520Codeforces.%2520By%2520continually%250Agenerating%2520more%2520interesting%2520problems%2520and%2520attempting%2520to%2520solve%2520them%252C%2520language%250Amodels%2520can%2520improve%2520on%2520downstream%2520benchmarks%2520without%2520access%2520to%2520any%2520curated%250Atraining%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03682v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Questioning%20Language%20Models&entry.906535625=Lili%20Chen%20and%20Mihir%20Prabhudesai%20and%20Katerina%20Fragkiadaki%20and%20Hao%20Liu%20and%20Deepak%20Pathak&entry.1292438233=%20%20Can%20large%20language%20models%20improve%20without%20external%20data%20--%20by%20generating%0Atheir%20own%20questions%20and%20answers%3F%20We%20hypothesize%20that%20a%20pre-trained%20language%0Amodel%20can%20improve%20its%20reasoning%20skills%20given%20only%20a%20single%20prompt%20specifying%0Athe%20topic%20%28e.g.%2C%20algebra%20word%20problems%29%20and%20asking%20the%20model%20to%20generate%20its%0Aown%20questions.%20To%20do%20this%2C%20we%20propose%20Self-Questioning%20Language%20Models%20%28SQLM%29%3A%0Aan%20asymmetric%20self-play%20framework%20where%20a%20proposer%20is%20given%20the%20topic%20and%0Agenerates%20a%20question%20for%20a%20solver%2C%20who%20tries%20to%20answer%20it.%20Both%20the%20proposer%0Aand%20solver%20are%20trained%20via%20reinforcement%20learning.%20The%20proposer%20receives%20a%0Areward%20if%20the%20problem%20is%20not%20too%20easy%20or%20too%20difficult%2C%20and%20the%20solver%20receives%0Aa%20reward%20based%20on%20majority%20voting%2C%20a%20proxy%20for%20correctness%20in%20the%20absence%20of%0Aground-truth%20answers.%20For%20coding%2C%20the%20proposer%20can%20instead%20generate%20unit%20tests%0Awhich%20are%20used%20for%20verification.%20We%20study%20this%20asymmetric%20self-play%20framework%0Aon%20three%20benchmarks%3A%20three-digit%20multiplication%2C%20algebra%20problems%20from%20the%0AOMEGA%20benchmark%2C%20and%20programming%20problems%20from%20Codeforces.%20By%20continually%0Agenerating%20more%20interesting%20problems%20and%20attempting%20to%20solve%20them%2C%20language%0Amodels%20can%20improve%20on%20downstream%20benchmarks%20without%20access%20to%20any%20curated%0Atraining%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03682v2&entry.124074799=Read"},
{"title": "3DTTNet: Multimodal Fusion-Based 3D Traversable Terrain Modeling for\n  Off-Road Environments", "author": "Zitong Chen and Chao Sun and Shida Nie and Chen Min and Changjiu Ning and Haoyu Li and Bo Wang", "abstract": "  Off-road environments remain significant challenges for autonomous ground\nvehicles, due to the lack of structured roads and the presence of complex\nobstacles, such as uneven terrain, vegetation, and occlusions. Traditional\nperception algorithms, primarily designed for structured environments, often\nfail in unstructured scenarios. In this paper, traversable area recognition is\nachieved through semantic scene completion. A novel multimodal method, 3DTTNet,\nis proposed to generate dense traversable terrain estimations by integrating\nLiDAR point clouds with monocular images from a forward-facing perspective. By\nintegrating multimodal data, environmental feature extraction is strengthened,\nwhich is crucial for accurate terrain modeling in complex terrains.\nFurthermore, RELLIS-OCC, a dataset with 3D traversable annotations, is\nintroduced, incorporating geometric features such as step height, slope, and\nunevenness. Through a comprehensive analysis of vehicle obsta cle-crossing\nconditions and the incorporation of vehicle body structure constraints, four\ntraversability cost labels are generated: lethal, medium-cost, low-cost, and\nfree. Experimental results demonstrate that 3DTTNet outperforms the comparison\napproaches in 3D traversable area recognition, particularly in off-road\nenvironments with irregular geometries and partial occlusions. Specifically,\n3DTTNet achieves a 42\\% improvement in scene completion IoU compared to other\nmodels. The proposed framework is scalable and adaptable to various vehicle\nplatforms, allowing for adjustments to occupancy grid parameters and the\nintegration of advanced dynamic models for traversability cost estimation.\n", "link": "http://arxiv.org/abs/2412.08195v2", "date": "2025-08-06", "relevancy": 2.3745, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5943}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DTTNet%3A%20Multimodal%20Fusion-Based%203D%20Traversable%20Terrain%20Modeling%20for%0A%20%20Off-Road%20Environments&body=Title%3A%203DTTNet%3A%20Multimodal%20Fusion-Based%203D%20Traversable%20Terrain%20Modeling%20for%0A%20%20Off-Road%20Environments%0AAuthor%3A%20Zitong%20Chen%20and%20Chao%20Sun%20and%20Shida%20Nie%20and%20Chen%20Min%20and%20Changjiu%20Ning%20and%20Haoyu%20Li%20and%20Bo%20Wang%0AAbstract%3A%20%20%20Off-road%20environments%20remain%20significant%20challenges%20for%20autonomous%20ground%0Avehicles%2C%20due%20to%20the%20lack%20of%20structured%20roads%20and%20the%20presence%20of%20complex%0Aobstacles%2C%20such%20as%20uneven%20terrain%2C%20vegetation%2C%20and%20occlusions.%20Traditional%0Aperception%20algorithms%2C%20primarily%20designed%20for%20structured%20environments%2C%20often%0Afail%20in%20unstructured%20scenarios.%20In%20this%20paper%2C%20traversable%20area%20recognition%20is%0Aachieved%20through%20semantic%20scene%20completion.%20A%20novel%20multimodal%20method%2C%203DTTNet%2C%0Ais%20proposed%20to%20generate%20dense%20traversable%20terrain%20estimations%20by%20integrating%0ALiDAR%20point%20clouds%20with%20monocular%20images%20from%20a%20forward-facing%20perspective.%20By%0Aintegrating%20multimodal%20data%2C%20environmental%20feature%20extraction%20is%20strengthened%2C%0Awhich%20is%20crucial%20for%20accurate%20terrain%20modeling%20in%20complex%20terrains.%0AFurthermore%2C%20RELLIS-OCC%2C%20a%20dataset%20with%203D%20traversable%20annotations%2C%20is%0Aintroduced%2C%20incorporating%20geometric%20features%20such%20as%20step%20height%2C%20slope%2C%20and%0Aunevenness.%20Through%20a%20comprehensive%20analysis%20of%20vehicle%20obsta%20cle-crossing%0Aconditions%20and%20the%20incorporation%20of%20vehicle%20body%20structure%20constraints%2C%20four%0Atraversability%20cost%20labels%20are%20generated%3A%20lethal%2C%20medium-cost%2C%20low-cost%2C%20and%0Afree.%20Experimental%20results%20demonstrate%20that%203DTTNet%20outperforms%20the%20comparison%0Aapproaches%20in%203D%20traversable%20area%20recognition%2C%20particularly%20in%20off-road%0Aenvironments%20with%20irregular%20geometries%20and%20partial%20occlusions.%20Specifically%2C%0A3DTTNet%20achieves%20a%2042%5C%25%20improvement%20in%20scene%20completion%20IoU%20compared%20to%20other%0Amodels.%20The%20proposed%20framework%20is%20scalable%20and%20adaptable%20to%20various%20vehicle%0Aplatforms%2C%20allowing%20for%20adjustments%20to%20occupancy%20grid%20parameters%20and%20the%0Aintegration%20of%20advanced%20dynamic%20models%20for%20traversability%20cost%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.08195v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DTTNet%253A%2520Multimodal%2520Fusion-Based%25203D%2520Traversable%2520Terrain%2520Modeling%2520for%250A%2520%2520Off-Road%2520Environments%26entry.906535625%3DZitong%2520Chen%2520and%2520Chao%2520Sun%2520and%2520Shida%2520Nie%2520and%2520Chen%2520Min%2520and%2520Changjiu%2520Ning%2520and%2520Haoyu%2520Li%2520and%2520Bo%2520Wang%26entry.1292438233%3D%2520%2520Off-road%2520environments%2520remain%2520significant%2520challenges%2520for%2520autonomous%2520ground%250Avehicles%252C%2520due%2520to%2520the%2520lack%2520of%2520structured%2520roads%2520and%2520the%2520presence%2520of%2520complex%250Aobstacles%252C%2520such%2520as%2520uneven%2520terrain%252C%2520vegetation%252C%2520and%2520occlusions.%2520Traditional%250Aperception%2520algorithms%252C%2520primarily%2520designed%2520for%2520structured%2520environments%252C%2520often%250Afail%2520in%2520unstructured%2520scenarios.%2520In%2520this%2520paper%252C%2520traversable%2520area%2520recognition%2520is%250Aachieved%2520through%2520semantic%2520scene%2520completion.%2520A%2520novel%2520multimodal%2520method%252C%25203DTTNet%252C%250Ais%2520proposed%2520to%2520generate%2520dense%2520traversable%2520terrain%2520estimations%2520by%2520integrating%250ALiDAR%2520point%2520clouds%2520with%2520monocular%2520images%2520from%2520a%2520forward-facing%2520perspective.%2520By%250Aintegrating%2520multimodal%2520data%252C%2520environmental%2520feature%2520extraction%2520is%2520strengthened%252C%250Awhich%2520is%2520crucial%2520for%2520accurate%2520terrain%2520modeling%2520in%2520complex%2520terrains.%250AFurthermore%252C%2520RELLIS-OCC%252C%2520a%2520dataset%2520with%25203D%2520traversable%2520annotations%252C%2520is%250Aintroduced%252C%2520incorporating%2520geometric%2520features%2520such%2520as%2520step%2520height%252C%2520slope%252C%2520and%250Aunevenness.%2520Through%2520a%2520comprehensive%2520analysis%2520of%2520vehicle%2520obsta%2520cle-crossing%250Aconditions%2520and%2520the%2520incorporation%2520of%2520vehicle%2520body%2520structure%2520constraints%252C%2520four%250Atraversability%2520cost%2520labels%2520are%2520generated%253A%2520lethal%252C%2520medium-cost%252C%2520low-cost%252C%2520and%250Afree.%2520Experimental%2520results%2520demonstrate%2520that%25203DTTNet%2520outperforms%2520the%2520comparison%250Aapproaches%2520in%25203D%2520traversable%2520area%2520recognition%252C%2520particularly%2520in%2520off-road%250Aenvironments%2520with%2520irregular%2520geometries%2520and%2520partial%2520occlusions.%2520Specifically%252C%250A3DTTNet%2520achieves%2520a%252042%255C%2525%2520improvement%2520in%2520scene%2520completion%2520IoU%2520compared%2520to%2520other%250Amodels.%2520The%2520proposed%2520framework%2520is%2520scalable%2520and%2520adaptable%2520to%2520various%2520vehicle%250Aplatforms%252C%2520allowing%2520for%2520adjustments%2520to%2520occupancy%2520grid%2520parameters%2520and%2520the%250Aintegration%2520of%2520advanced%2520dynamic%2520models%2520for%2520traversability%2520cost%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.08195v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DTTNet%3A%20Multimodal%20Fusion-Based%203D%20Traversable%20Terrain%20Modeling%20for%0A%20%20Off-Road%20Environments&entry.906535625=Zitong%20Chen%20and%20Chao%20Sun%20and%20Shida%20Nie%20and%20Chen%20Min%20and%20Changjiu%20Ning%20and%20Haoyu%20Li%20and%20Bo%20Wang&entry.1292438233=%20%20Off-road%20environments%20remain%20significant%20challenges%20for%20autonomous%20ground%0Avehicles%2C%20due%20to%20the%20lack%20of%20structured%20roads%20and%20the%20presence%20of%20complex%0Aobstacles%2C%20such%20as%20uneven%20terrain%2C%20vegetation%2C%20and%20occlusions.%20Traditional%0Aperception%20algorithms%2C%20primarily%20designed%20for%20structured%20environments%2C%20often%0Afail%20in%20unstructured%20scenarios.%20In%20this%20paper%2C%20traversable%20area%20recognition%20is%0Aachieved%20through%20semantic%20scene%20completion.%20A%20novel%20multimodal%20method%2C%203DTTNet%2C%0Ais%20proposed%20to%20generate%20dense%20traversable%20terrain%20estimations%20by%20integrating%0ALiDAR%20point%20clouds%20with%20monocular%20images%20from%20a%20forward-facing%20perspective.%20By%0Aintegrating%20multimodal%20data%2C%20environmental%20feature%20extraction%20is%20strengthened%2C%0Awhich%20is%20crucial%20for%20accurate%20terrain%20modeling%20in%20complex%20terrains.%0AFurthermore%2C%20RELLIS-OCC%2C%20a%20dataset%20with%203D%20traversable%20annotations%2C%20is%0Aintroduced%2C%20incorporating%20geometric%20features%20such%20as%20step%20height%2C%20slope%2C%20and%0Aunevenness.%20Through%20a%20comprehensive%20analysis%20of%20vehicle%20obsta%20cle-crossing%0Aconditions%20and%20the%20incorporation%20of%20vehicle%20body%20structure%20constraints%2C%20four%0Atraversability%20cost%20labels%20are%20generated%3A%20lethal%2C%20medium-cost%2C%20low-cost%2C%20and%0Afree.%20Experimental%20results%20demonstrate%20that%203DTTNet%20outperforms%20the%20comparison%0Aapproaches%20in%203D%20traversable%20area%20recognition%2C%20particularly%20in%20off-road%0Aenvironments%20with%20irregular%20geometries%20and%20partial%20occlusions.%20Specifically%2C%0A3DTTNet%20achieves%20a%2042%5C%25%20improvement%20in%20scene%20completion%20IoU%20compared%20to%20other%0Amodels.%20The%20proposed%20framework%20is%20scalable%20and%20adaptable%20to%20various%20vehicle%0Aplatforms%2C%20allowing%20for%20adjustments%20to%20occupancy%20grid%20parameters%20and%20the%0Aintegration%20of%20advanced%20dynamic%20models%20for%20traversability%20cost%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.08195v2&entry.124074799=Read"},
{"title": "Beyond Adapter Retrieval: Latent Geometry-Preserving Composition via\n  Sparse Task Projection", "author": "Pengfei Jin and Peng Shu and Sifan Song and Sekeun Kim and Qing Xiao and Cheng Chen and Tianming Liu and Xiang Li and Quanzheng Li", "abstract": "  Recent advances in parameter-efficient transfer learning have demonstrated\nthe utility of composing LoRA adapters from libraries of pretrained modules.\nHowever, most existing approaches rely on simple retrieval heuristics or\nuniform averaging, which overlook the latent structure of task relationships in\nrepresentation space. We propose a new framework for adapter reuse that moves\nbeyond retrieval, formulating adapter composition as a geometry-aware sparse\nreconstruction problem. Specifically, we represent each task by a latent\nprototype vector derived from the base model's encoder and aim to approximate\nthe target task prototype as a sparse linear combination of retrieved reference\nprototypes, under an $\\ell_1$-regularized optimization objective. The resulting\ncombination weights are then used to blend the corresponding LoRA adapters,\nyielding a composite adapter tailored to the target task. This formulation not\nonly preserves the local geometric structure of the task representation\nmanifold, but also promotes interpretability and efficient reuse by selecting a\nminimal set of relevant adapters. We demonstrate the effectiveness of our\napproach across multiple domains-including medical image segmentation, medical\nreport generation and image synthesis. Our results highlight the benefit of\ncoupling retrieval with latent geometry-aware optimization for improved\nzero-shot generalization.\n", "link": "http://arxiv.org/abs/2410.09908v2", "date": "2025-08-06", "relevancy": 2.3579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6156}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5769}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Adapter%20Retrieval%3A%20Latent%20Geometry-Preserving%20Composition%20via%0A%20%20Sparse%20Task%20Projection&body=Title%3A%20Beyond%20Adapter%20Retrieval%3A%20Latent%20Geometry-Preserving%20Composition%20via%0A%20%20Sparse%20Task%20Projection%0AAuthor%3A%20Pengfei%20Jin%20and%20Peng%20Shu%20and%20Sifan%20Song%20and%20Sekeun%20Kim%20and%20Qing%20Xiao%20and%20Cheng%20Chen%20and%20Tianming%20Liu%20and%20Xiang%20Li%20and%20Quanzheng%20Li%0AAbstract%3A%20%20%20Recent%20advances%20in%20parameter-efficient%20transfer%20learning%20have%20demonstrated%0Athe%20utility%20of%20composing%20LoRA%20adapters%20from%20libraries%20of%20pretrained%20modules.%0AHowever%2C%20most%20existing%20approaches%20rely%20on%20simple%20retrieval%20heuristics%20or%0Auniform%20averaging%2C%20which%20overlook%20the%20latent%20structure%20of%20task%20relationships%20in%0Arepresentation%20space.%20We%20propose%20a%20new%20framework%20for%20adapter%20reuse%20that%20moves%0Abeyond%20retrieval%2C%20formulating%20adapter%20composition%20as%20a%20geometry-aware%20sparse%0Areconstruction%20problem.%20Specifically%2C%20we%20represent%20each%20task%20by%20a%20latent%0Aprototype%20vector%20derived%20from%20the%20base%20model%27s%20encoder%20and%20aim%20to%20approximate%0Athe%20target%20task%20prototype%20as%20a%20sparse%20linear%20combination%20of%20retrieved%20reference%0Aprototypes%2C%20under%20an%20%24%5Cell_1%24-regularized%20optimization%20objective.%20The%20resulting%0Acombination%20weights%20are%20then%20used%20to%20blend%20the%20corresponding%20LoRA%20adapters%2C%0Ayielding%20a%20composite%20adapter%20tailored%20to%20the%20target%20task.%20This%20formulation%20not%0Aonly%20preserves%20the%20local%20geometric%20structure%20of%20the%20task%20representation%0Amanifold%2C%20but%20also%20promotes%20interpretability%20and%20efficient%20reuse%20by%20selecting%20a%0Aminimal%20set%20of%20relevant%20adapters.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20across%20multiple%20domains-including%20medical%20image%20segmentation%2C%20medical%0Areport%20generation%20and%20image%20synthesis.%20Our%20results%20highlight%20the%20benefit%20of%0Acoupling%20retrieval%20with%20latent%20geometry-aware%20optimization%20for%20improved%0Azero-shot%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09908v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Adapter%2520Retrieval%253A%2520Latent%2520Geometry-Preserving%2520Composition%2520via%250A%2520%2520Sparse%2520Task%2520Projection%26entry.906535625%3DPengfei%2520Jin%2520and%2520Peng%2520Shu%2520and%2520Sifan%2520Song%2520and%2520Sekeun%2520Kim%2520and%2520Qing%2520Xiao%2520and%2520Cheng%2520Chen%2520and%2520Tianming%2520Liu%2520and%2520Xiang%2520Li%2520and%2520Quanzheng%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520parameter-efficient%2520transfer%2520learning%2520have%2520demonstrated%250Athe%2520utility%2520of%2520composing%2520LoRA%2520adapters%2520from%2520libraries%2520of%2520pretrained%2520modules.%250AHowever%252C%2520most%2520existing%2520approaches%2520rely%2520on%2520simple%2520retrieval%2520heuristics%2520or%250Auniform%2520averaging%252C%2520which%2520overlook%2520the%2520latent%2520structure%2520of%2520task%2520relationships%2520in%250Arepresentation%2520space.%2520We%2520propose%2520a%2520new%2520framework%2520for%2520adapter%2520reuse%2520that%2520moves%250Abeyond%2520retrieval%252C%2520formulating%2520adapter%2520composition%2520as%2520a%2520geometry-aware%2520sparse%250Areconstruction%2520problem.%2520Specifically%252C%2520we%2520represent%2520each%2520task%2520by%2520a%2520latent%250Aprototype%2520vector%2520derived%2520from%2520the%2520base%2520model%2527s%2520encoder%2520and%2520aim%2520to%2520approximate%250Athe%2520target%2520task%2520prototype%2520as%2520a%2520sparse%2520linear%2520combination%2520of%2520retrieved%2520reference%250Aprototypes%252C%2520under%2520an%2520%2524%255Cell_1%2524-regularized%2520optimization%2520objective.%2520The%2520resulting%250Acombination%2520weights%2520are%2520then%2520used%2520to%2520blend%2520the%2520corresponding%2520LoRA%2520adapters%252C%250Ayielding%2520a%2520composite%2520adapter%2520tailored%2520to%2520the%2520target%2520task.%2520This%2520formulation%2520not%250Aonly%2520preserves%2520the%2520local%2520geometric%2520structure%2520of%2520the%2520task%2520representation%250Amanifold%252C%2520but%2520also%2520promotes%2520interpretability%2520and%2520efficient%2520reuse%2520by%2520selecting%2520a%250Aminimal%2520set%2520of%2520relevant%2520adapters.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%250Aapproach%2520across%2520multiple%2520domains-including%2520medical%2520image%2520segmentation%252C%2520medical%250Areport%2520generation%2520and%2520image%2520synthesis.%2520Our%2520results%2520highlight%2520the%2520benefit%2520of%250Acoupling%2520retrieval%2520with%2520latent%2520geometry-aware%2520optimization%2520for%2520improved%250Azero-shot%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09908v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Adapter%20Retrieval%3A%20Latent%20Geometry-Preserving%20Composition%20via%0A%20%20Sparse%20Task%20Projection&entry.906535625=Pengfei%20Jin%20and%20Peng%20Shu%20and%20Sifan%20Song%20and%20Sekeun%20Kim%20and%20Qing%20Xiao%20and%20Cheng%20Chen%20and%20Tianming%20Liu%20and%20Xiang%20Li%20and%20Quanzheng%20Li&entry.1292438233=%20%20Recent%20advances%20in%20parameter-efficient%20transfer%20learning%20have%20demonstrated%0Athe%20utility%20of%20composing%20LoRA%20adapters%20from%20libraries%20of%20pretrained%20modules.%0AHowever%2C%20most%20existing%20approaches%20rely%20on%20simple%20retrieval%20heuristics%20or%0Auniform%20averaging%2C%20which%20overlook%20the%20latent%20structure%20of%20task%20relationships%20in%0Arepresentation%20space.%20We%20propose%20a%20new%20framework%20for%20adapter%20reuse%20that%20moves%0Abeyond%20retrieval%2C%20formulating%20adapter%20composition%20as%20a%20geometry-aware%20sparse%0Areconstruction%20problem.%20Specifically%2C%20we%20represent%20each%20task%20by%20a%20latent%0Aprototype%20vector%20derived%20from%20the%20base%20model%27s%20encoder%20and%20aim%20to%20approximate%0Athe%20target%20task%20prototype%20as%20a%20sparse%20linear%20combination%20of%20retrieved%20reference%0Aprototypes%2C%20under%20an%20%24%5Cell_1%24-regularized%20optimization%20objective.%20The%20resulting%0Acombination%20weights%20are%20then%20used%20to%20blend%20the%20corresponding%20LoRA%20adapters%2C%0Ayielding%20a%20composite%20adapter%20tailored%20to%20the%20target%20task.%20This%20formulation%20not%0Aonly%20preserves%20the%20local%20geometric%20structure%20of%20the%20task%20representation%0Amanifold%2C%20but%20also%20promotes%20interpretability%20and%20efficient%20reuse%20by%20selecting%20a%0Aminimal%20set%20of%20relevant%20adapters.%20We%20demonstrate%20the%20effectiveness%20of%20our%0Aapproach%20across%20multiple%20domains-including%20medical%20image%20segmentation%2C%20medical%0Areport%20generation%20and%20image%20synthesis.%20Our%20results%20highlight%20the%20benefit%20of%0Acoupling%20retrieval%20with%20latent%20geometry-aware%20optimization%20for%20improved%0Azero-shot%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09908v2&entry.124074799=Read"},
{"title": "ProtoN: Prototype Node Graph Neural Network for Unconstrained\n  Multi-Impression Ear Recognition", "author": "Santhoshkumar Peddi and Sadhvik Bathini and Arun Balasubramanian and Monalisa Sarma and Debasis Samanta", "abstract": "  Ear biometrics offer a stable and contactless modality for identity\nrecognition, yet their effectiveness remains limited by the scarcity of\nannotated data and significant intra-class variability. Existing methods\ntypically extract identity features from individual impressions in isolation,\nrestricting their ability to capture consistent and discriminative\nrepresentations. To overcome these limitations, a few-shot learning framework,\nProtoN, is proposed to jointly process multiple impressions of an identity\nusing a graph-based approach. Each impression is represented as a node in a\nclass-specific graph, alongside a learnable prototype node that encodes\nidentity-level information. This graph is processed by a Prototype Graph Neural\nNetwork (PGNN) layer, specifically designed to refine both impression and\nprototype representations through a dual-path message-passing mechanism. To\nfurther enhance discriminative power, the PGNN incorporates a cross-graph\nprototype alignment strategy that improves class separability by enforcing\nintra-class compactness while maintaining inter-class distinction.\nAdditionally, a hybrid loss function is employed to balance episodic and global\nclassification objectives, thereby improving the overall structure of the\nembedding space. Extensive experiments on five benchmark ear datasets\ndemonstrate that ProtoN achieves state-of-the-art performance, with Rank-1\nidentification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as\n0.025, showing the effectiveness for few-shot ear recognition under limited\ndata conditions.\n", "link": "http://arxiv.org/abs/2508.04381v1", "date": "2025-08-06", "relevancy": 2.3559, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4784}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4718}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtoN%3A%20Prototype%20Node%20Graph%20Neural%20Network%20for%20Unconstrained%0A%20%20Multi-Impression%20Ear%20Recognition&body=Title%3A%20ProtoN%3A%20Prototype%20Node%20Graph%20Neural%20Network%20for%20Unconstrained%0A%20%20Multi-Impression%20Ear%20Recognition%0AAuthor%3A%20Santhoshkumar%20Peddi%20and%20Sadhvik%20Bathini%20and%20Arun%20Balasubramanian%20and%20Monalisa%20Sarma%20and%20Debasis%20Samanta%0AAbstract%3A%20%20%20Ear%20biometrics%20offer%20a%20stable%20and%20contactless%20modality%20for%20identity%0Arecognition%2C%20yet%20their%20effectiveness%20remains%20limited%20by%20the%20scarcity%20of%0Aannotated%20data%20and%20significant%20intra-class%20variability.%20Existing%20methods%0Atypically%20extract%20identity%20features%20from%20individual%20impressions%20in%20isolation%2C%0Arestricting%20their%20ability%20to%20capture%20consistent%20and%20discriminative%0Arepresentations.%20To%20overcome%20these%20limitations%2C%20a%20few-shot%20learning%20framework%2C%0AProtoN%2C%20is%20proposed%20to%20jointly%20process%20multiple%20impressions%20of%20an%20identity%0Ausing%20a%20graph-based%20approach.%20Each%20impression%20is%20represented%20as%20a%20node%20in%20a%0Aclass-specific%20graph%2C%20alongside%20a%20learnable%20prototype%20node%20that%20encodes%0Aidentity-level%20information.%20This%20graph%20is%20processed%20by%20a%20Prototype%20Graph%20Neural%0ANetwork%20%28PGNN%29%20layer%2C%20specifically%20designed%20to%20refine%20both%20impression%20and%0Aprototype%20representations%20through%20a%20dual-path%20message-passing%20mechanism.%20To%0Afurther%20enhance%20discriminative%20power%2C%20the%20PGNN%20incorporates%20a%20cross-graph%0Aprototype%20alignment%20strategy%20that%20improves%20class%20separability%20by%20enforcing%0Aintra-class%20compactness%20while%20maintaining%20inter-class%20distinction.%0AAdditionally%2C%20a%20hybrid%20loss%20function%20is%20employed%20to%20balance%20episodic%20and%20global%0Aclassification%20objectives%2C%20thereby%20improving%20the%20overall%20structure%20of%20the%0Aembedding%20space.%20Extensive%20experiments%20on%20five%20benchmark%20ear%20datasets%0Ademonstrate%20that%20ProtoN%20achieves%20state-of-the-art%20performance%2C%20with%20Rank-1%0Aidentification%20accuracy%20of%20up%20to%2099.60%25%20and%20an%20Equal%20Error%20Rate%20%28EER%29%20as%20low%20as%0A0.025%2C%20showing%20the%20effectiveness%20for%20few-shot%20ear%20recognition%20under%20limited%0Adata%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtoN%253A%2520Prototype%2520Node%2520Graph%2520Neural%2520Network%2520for%2520Unconstrained%250A%2520%2520Multi-Impression%2520Ear%2520Recognition%26entry.906535625%3DSanthoshkumar%2520Peddi%2520and%2520Sadhvik%2520Bathini%2520and%2520Arun%2520Balasubramanian%2520and%2520Monalisa%2520Sarma%2520and%2520Debasis%2520Samanta%26entry.1292438233%3D%2520%2520Ear%2520biometrics%2520offer%2520a%2520stable%2520and%2520contactless%2520modality%2520for%2520identity%250Arecognition%252C%2520yet%2520their%2520effectiveness%2520remains%2520limited%2520by%2520the%2520scarcity%2520of%250Aannotated%2520data%2520and%2520significant%2520intra-class%2520variability.%2520Existing%2520methods%250Atypically%2520extract%2520identity%2520features%2520from%2520individual%2520impressions%2520in%2520isolation%252C%250Arestricting%2520their%2520ability%2520to%2520capture%2520consistent%2520and%2520discriminative%250Arepresentations.%2520To%2520overcome%2520these%2520limitations%252C%2520a%2520few-shot%2520learning%2520framework%252C%250AProtoN%252C%2520is%2520proposed%2520to%2520jointly%2520process%2520multiple%2520impressions%2520of%2520an%2520identity%250Ausing%2520a%2520graph-based%2520approach.%2520Each%2520impression%2520is%2520represented%2520as%2520a%2520node%2520in%2520a%250Aclass-specific%2520graph%252C%2520alongside%2520a%2520learnable%2520prototype%2520node%2520that%2520encodes%250Aidentity-level%2520information.%2520This%2520graph%2520is%2520processed%2520by%2520a%2520Prototype%2520Graph%2520Neural%250ANetwork%2520%2528PGNN%2529%2520layer%252C%2520specifically%2520designed%2520to%2520refine%2520both%2520impression%2520and%250Aprototype%2520representations%2520through%2520a%2520dual-path%2520message-passing%2520mechanism.%2520To%250Afurther%2520enhance%2520discriminative%2520power%252C%2520the%2520PGNN%2520incorporates%2520a%2520cross-graph%250Aprototype%2520alignment%2520strategy%2520that%2520improves%2520class%2520separability%2520by%2520enforcing%250Aintra-class%2520compactness%2520while%2520maintaining%2520inter-class%2520distinction.%250AAdditionally%252C%2520a%2520hybrid%2520loss%2520function%2520is%2520employed%2520to%2520balance%2520episodic%2520and%2520global%250Aclassification%2520objectives%252C%2520thereby%2520improving%2520the%2520overall%2520structure%2520of%2520the%250Aembedding%2520space.%2520Extensive%2520experiments%2520on%2520five%2520benchmark%2520ear%2520datasets%250Ademonstrate%2520that%2520ProtoN%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520Rank-1%250Aidentification%2520accuracy%2520of%2520up%2520to%252099.60%2525%2520and%2520an%2520Equal%2520Error%2520Rate%2520%2528EER%2529%2520as%2520low%2520as%250A0.025%252C%2520showing%2520the%2520effectiveness%2520for%2520few-shot%2520ear%2520recognition%2520under%2520limited%250Adata%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtoN%3A%20Prototype%20Node%20Graph%20Neural%20Network%20for%20Unconstrained%0A%20%20Multi-Impression%20Ear%20Recognition&entry.906535625=Santhoshkumar%20Peddi%20and%20Sadhvik%20Bathini%20and%20Arun%20Balasubramanian%20and%20Monalisa%20Sarma%20and%20Debasis%20Samanta&entry.1292438233=%20%20Ear%20biometrics%20offer%20a%20stable%20and%20contactless%20modality%20for%20identity%0Arecognition%2C%20yet%20their%20effectiveness%20remains%20limited%20by%20the%20scarcity%20of%0Aannotated%20data%20and%20significant%20intra-class%20variability.%20Existing%20methods%0Atypically%20extract%20identity%20features%20from%20individual%20impressions%20in%20isolation%2C%0Arestricting%20their%20ability%20to%20capture%20consistent%20and%20discriminative%0Arepresentations.%20To%20overcome%20these%20limitations%2C%20a%20few-shot%20learning%20framework%2C%0AProtoN%2C%20is%20proposed%20to%20jointly%20process%20multiple%20impressions%20of%20an%20identity%0Ausing%20a%20graph-based%20approach.%20Each%20impression%20is%20represented%20as%20a%20node%20in%20a%0Aclass-specific%20graph%2C%20alongside%20a%20learnable%20prototype%20node%20that%20encodes%0Aidentity-level%20information.%20This%20graph%20is%20processed%20by%20a%20Prototype%20Graph%20Neural%0ANetwork%20%28PGNN%29%20layer%2C%20specifically%20designed%20to%20refine%20both%20impression%20and%0Aprototype%20representations%20through%20a%20dual-path%20message-passing%20mechanism.%20To%0Afurther%20enhance%20discriminative%20power%2C%20the%20PGNN%20incorporates%20a%20cross-graph%0Aprototype%20alignment%20strategy%20that%20improves%20class%20separability%20by%20enforcing%0Aintra-class%20compactness%20while%20maintaining%20inter-class%20distinction.%0AAdditionally%2C%20a%20hybrid%20loss%20function%20is%20employed%20to%20balance%20episodic%20and%20global%0Aclassification%20objectives%2C%20thereby%20improving%20the%20overall%20structure%20of%20the%0Aembedding%20space.%20Extensive%20experiments%20on%20five%20benchmark%20ear%20datasets%0Ademonstrate%20that%20ProtoN%20achieves%20state-of-the-art%20performance%2C%20with%20Rank-1%0Aidentification%20accuracy%20of%20up%20to%2099.60%25%20and%20an%20Equal%20Error%20Rate%20%28EER%29%20as%20low%20as%0A0.025%2C%20showing%20the%20effectiveness%20for%20few-shot%20ear%20recognition%20under%20limited%0Adata%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04381v1&entry.124074799=Read"},
{"title": "Revisiting Continual Semantic Segmentation with Pre-trained Vision\n  Models", "author": "Duzhen Zhang and Yong Ren and Wei Cong and Junhao Zheng and Qiaoyi Su and Shuncheng Jia and Zhong-Zhi Li and Xuanle Zhao and Ye Bai and Feilong Chen and Qi Tian and Tielin Zhang", "abstract": "  Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment\nnovel classes while preserving knowledge of previously encountered ones. Recent\nadvancements in CSS have been largely driven by the adoption of Pre-trained\nVision Models (PVMs) as backbones. Among existing strategies, Direct\nFine-Tuning (DFT), which sequentially fine-tunes the model across classes,\nremains the most straightforward approach. Prior work often regards DFT as a\nperformance lower bound due to its presumed vulnerability to severe\ncatastrophic forgetting, leading to the development of numerous complex\nmitigation techniques. However, we contend that this prevailing assumption is\nflawed. In this paper, we systematically revisit forgetting in DFT across two\nstandard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using\ntwo representative PVM backbones: ResNet101 and Swin-B. Through a detailed\nprobing analysis, our findings reveal that existing methods significantly\nunderestimate the inherent anti-forgetting capabilities of PVMs. Even under\nDFT, PVMs retain previously learned knowledge with minimal forgetting. Further\ninvestigation of the feature space indicates that the observed forgetting\nprimarily arises from the classifier's drift away from the PVM, rather than\nfrom degradation of the backbone representations. Based on this insight, we\npropose DFT*, a simple yet effective enhancement to DFT that incorporates\nstrategies such as freezing the PVM backbone and previously learned\nclassifiers, as well as pre-allocating future classifiers. Extensive\nexperiments show that DFT* consistently achieves competitive or superior\nperformance compared to sixteen state-of-the-art CSS methods, while requiring\nsubstantially fewer trainable parameters and less training time.\n", "link": "http://arxiv.org/abs/2508.04267v1", "date": "2025-08-06", "relevancy": 2.3414, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5897}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Continual%20Semantic%20Segmentation%20with%20Pre-trained%20Vision%0A%20%20Models&body=Title%3A%20Revisiting%20Continual%20Semantic%20Segmentation%20with%20Pre-trained%20Vision%0A%20%20Models%0AAuthor%3A%20Duzhen%20Zhang%20and%20Yong%20Ren%20and%20Wei%20Cong%20and%20Junhao%20Zheng%20and%20Qiaoyi%20Su%20and%20Shuncheng%20Jia%20and%20Zhong-Zhi%20Li%20and%20Xuanle%20Zhao%20and%20Ye%20Bai%20and%20Feilong%20Chen%20and%20Qi%20Tian%20and%20Tielin%20Zhang%0AAbstract%3A%20%20%20Continual%20Semantic%20Segmentation%20%28CSS%29%20seeks%20to%20incrementally%20learn%20to%20segment%0Anovel%20classes%20while%20preserving%20knowledge%20of%20previously%20encountered%20ones.%20Recent%0Aadvancements%20in%20CSS%20have%20been%20largely%20driven%20by%20the%20adoption%20of%20Pre-trained%0AVision%20Models%20%28PVMs%29%20as%20backbones.%20Among%20existing%20strategies%2C%20Direct%0AFine-Tuning%20%28DFT%29%2C%20which%20sequentially%20fine-tunes%20the%20model%20across%20classes%2C%0Aremains%20the%20most%20straightforward%20approach.%20Prior%20work%20often%20regards%20DFT%20as%20a%0Aperformance%20lower%20bound%20due%20to%20its%20presumed%20vulnerability%20to%20severe%0Acatastrophic%20forgetting%2C%20leading%20to%20the%20development%20of%20numerous%20complex%0Amitigation%20techniques.%20However%2C%20we%20contend%20that%20this%20prevailing%20assumption%20is%0Aflawed.%20In%20this%20paper%2C%20we%20systematically%20revisit%20forgetting%20in%20DFT%20across%20two%0Astandard%20benchmarks%2C%20Pascal%20VOC%202012%20and%20ADE20K%2C%20under%20eight%20CSS%20settings%20using%0Atwo%20representative%20PVM%20backbones%3A%20ResNet101%20and%20Swin-B.%20Through%20a%20detailed%0Aprobing%20analysis%2C%20our%20findings%20reveal%20that%20existing%20methods%20significantly%0Aunderestimate%20the%20inherent%20anti-forgetting%20capabilities%20of%20PVMs.%20Even%20under%0ADFT%2C%20PVMs%20retain%20previously%20learned%20knowledge%20with%20minimal%20forgetting.%20Further%0Ainvestigation%20of%20the%20feature%20space%20indicates%20that%20the%20observed%20forgetting%0Aprimarily%20arises%20from%20the%20classifier%27s%20drift%20away%20from%20the%20PVM%2C%20rather%20than%0Afrom%20degradation%20of%20the%20backbone%20representations.%20Based%20on%20this%20insight%2C%20we%0Apropose%20DFT%2A%2C%20a%20simple%20yet%20effective%20enhancement%20to%20DFT%20that%20incorporates%0Astrategies%20such%20as%20freezing%20the%20PVM%20backbone%20and%20previously%20learned%0Aclassifiers%2C%20as%20well%20as%20pre-allocating%20future%20classifiers.%20Extensive%0Aexperiments%20show%20that%20DFT%2A%20consistently%20achieves%20competitive%20or%20superior%0Aperformance%20compared%20to%20sixteen%20state-of-the-art%20CSS%20methods%2C%20while%20requiring%0Asubstantially%20fewer%20trainable%20parameters%20and%20less%20training%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Continual%2520Semantic%2520Segmentation%2520with%2520Pre-trained%2520Vision%250A%2520%2520Models%26entry.906535625%3DDuzhen%2520Zhang%2520and%2520Yong%2520Ren%2520and%2520Wei%2520Cong%2520and%2520Junhao%2520Zheng%2520and%2520Qiaoyi%2520Su%2520and%2520Shuncheng%2520Jia%2520and%2520Zhong-Zhi%2520Li%2520and%2520Xuanle%2520Zhao%2520and%2520Ye%2520Bai%2520and%2520Feilong%2520Chen%2520and%2520Qi%2520Tian%2520and%2520Tielin%2520Zhang%26entry.1292438233%3D%2520%2520Continual%2520Semantic%2520Segmentation%2520%2528CSS%2529%2520seeks%2520to%2520incrementally%2520learn%2520to%2520segment%250Anovel%2520classes%2520while%2520preserving%2520knowledge%2520of%2520previously%2520encountered%2520ones.%2520Recent%250Aadvancements%2520in%2520CSS%2520have%2520been%2520largely%2520driven%2520by%2520the%2520adoption%2520of%2520Pre-trained%250AVision%2520Models%2520%2528PVMs%2529%2520as%2520backbones.%2520Among%2520existing%2520strategies%252C%2520Direct%250AFine-Tuning%2520%2528DFT%2529%252C%2520which%2520sequentially%2520fine-tunes%2520the%2520model%2520across%2520classes%252C%250Aremains%2520the%2520most%2520straightforward%2520approach.%2520Prior%2520work%2520often%2520regards%2520DFT%2520as%2520a%250Aperformance%2520lower%2520bound%2520due%2520to%2520its%2520presumed%2520vulnerability%2520to%2520severe%250Acatastrophic%2520forgetting%252C%2520leading%2520to%2520the%2520development%2520of%2520numerous%2520complex%250Amitigation%2520techniques.%2520However%252C%2520we%2520contend%2520that%2520this%2520prevailing%2520assumption%2520is%250Aflawed.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520revisit%2520forgetting%2520in%2520DFT%2520across%2520two%250Astandard%2520benchmarks%252C%2520Pascal%2520VOC%25202012%2520and%2520ADE20K%252C%2520under%2520eight%2520CSS%2520settings%2520using%250Atwo%2520representative%2520PVM%2520backbones%253A%2520ResNet101%2520and%2520Swin-B.%2520Through%2520a%2520detailed%250Aprobing%2520analysis%252C%2520our%2520findings%2520reveal%2520that%2520existing%2520methods%2520significantly%250Aunderestimate%2520the%2520inherent%2520anti-forgetting%2520capabilities%2520of%2520PVMs.%2520Even%2520under%250ADFT%252C%2520PVMs%2520retain%2520previously%2520learned%2520knowledge%2520with%2520minimal%2520forgetting.%2520Further%250Ainvestigation%2520of%2520the%2520feature%2520space%2520indicates%2520that%2520the%2520observed%2520forgetting%250Aprimarily%2520arises%2520from%2520the%2520classifier%2527s%2520drift%2520away%2520from%2520the%2520PVM%252C%2520rather%2520than%250Afrom%2520degradation%2520of%2520the%2520backbone%2520representations.%2520Based%2520on%2520this%2520insight%252C%2520we%250Apropose%2520DFT%252A%252C%2520a%2520simple%2520yet%2520effective%2520enhancement%2520to%2520DFT%2520that%2520incorporates%250Astrategies%2520such%2520as%2520freezing%2520the%2520PVM%2520backbone%2520and%2520previously%2520learned%250Aclassifiers%252C%2520as%2520well%2520as%2520pre-allocating%2520future%2520classifiers.%2520Extensive%250Aexperiments%2520show%2520that%2520DFT%252A%2520consistently%2520achieves%2520competitive%2520or%2520superior%250Aperformance%2520compared%2520to%2520sixteen%2520state-of-the-art%2520CSS%2520methods%252C%2520while%2520requiring%250Asubstantially%2520fewer%2520trainable%2520parameters%2520and%2520less%2520training%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Continual%20Semantic%20Segmentation%20with%20Pre-trained%20Vision%0A%20%20Models&entry.906535625=Duzhen%20Zhang%20and%20Yong%20Ren%20and%20Wei%20Cong%20and%20Junhao%20Zheng%20and%20Qiaoyi%20Su%20and%20Shuncheng%20Jia%20and%20Zhong-Zhi%20Li%20and%20Xuanle%20Zhao%20and%20Ye%20Bai%20and%20Feilong%20Chen%20and%20Qi%20Tian%20and%20Tielin%20Zhang&entry.1292438233=%20%20Continual%20Semantic%20Segmentation%20%28CSS%29%20seeks%20to%20incrementally%20learn%20to%20segment%0Anovel%20classes%20while%20preserving%20knowledge%20of%20previously%20encountered%20ones.%20Recent%0Aadvancements%20in%20CSS%20have%20been%20largely%20driven%20by%20the%20adoption%20of%20Pre-trained%0AVision%20Models%20%28PVMs%29%20as%20backbones.%20Among%20existing%20strategies%2C%20Direct%0AFine-Tuning%20%28DFT%29%2C%20which%20sequentially%20fine-tunes%20the%20model%20across%20classes%2C%0Aremains%20the%20most%20straightforward%20approach.%20Prior%20work%20often%20regards%20DFT%20as%20a%0Aperformance%20lower%20bound%20due%20to%20its%20presumed%20vulnerability%20to%20severe%0Acatastrophic%20forgetting%2C%20leading%20to%20the%20development%20of%20numerous%20complex%0Amitigation%20techniques.%20However%2C%20we%20contend%20that%20this%20prevailing%20assumption%20is%0Aflawed.%20In%20this%20paper%2C%20we%20systematically%20revisit%20forgetting%20in%20DFT%20across%20two%0Astandard%20benchmarks%2C%20Pascal%20VOC%202012%20and%20ADE20K%2C%20under%20eight%20CSS%20settings%20using%0Atwo%20representative%20PVM%20backbones%3A%20ResNet101%20and%20Swin-B.%20Through%20a%20detailed%0Aprobing%20analysis%2C%20our%20findings%20reveal%20that%20existing%20methods%20significantly%0Aunderestimate%20the%20inherent%20anti-forgetting%20capabilities%20of%20PVMs.%20Even%20under%0ADFT%2C%20PVMs%20retain%20previously%20learned%20knowledge%20with%20minimal%20forgetting.%20Further%0Ainvestigation%20of%20the%20feature%20space%20indicates%20that%20the%20observed%20forgetting%0Aprimarily%20arises%20from%20the%20classifier%27s%20drift%20away%20from%20the%20PVM%2C%20rather%20than%0Afrom%20degradation%20of%20the%20backbone%20representations.%20Based%20on%20this%20insight%2C%20we%0Apropose%20DFT%2A%2C%20a%20simple%20yet%20effective%20enhancement%20to%20DFT%20that%20incorporates%0Astrategies%20such%20as%20freezing%20the%20PVM%20backbone%20and%20previously%20learned%0Aclassifiers%2C%20as%20well%20as%20pre-allocating%20future%20classifiers.%20Extensive%0Aexperiments%20show%20that%20DFT%2A%20consistently%20achieves%20competitive%20or%20superior%0Aperformance%20compared%20to%20sixteen%20state-of-the-art%20CSS%20methods%2C%20while%20requiring%0Asubstantially%20fewer%20trainable%20parameters%20and%20less%20training%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04267v1&entry.124074799=Read"},
{"title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models", "author": "Nima Iji and Kia Dashtipour", "abstract": "  Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.\n", "link": "http://arxiv.org/abs/2508.04350v1", "date": "2025-08-06", "relevancy": 2.3339, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5922}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain%20of%20Questions%3A%20Guiding%20Multimodal%20Curiosity%20in%20Language%20Models&body=Title%3A%20Chain%20of%20Questions%3A%20Guiding%20Multimodal%20Curiosity%20in%20Language%20Models%0AAuthor%3A%20Nima%20Iji%20and%20Kia%20Dashtipour%0AAbstract%3A%20%20%20Reasoning%20capabilities%20in%20large%20language%20models%20%28LLMs%29%20have%20substantially%0Aadvanced%20through%20methods%20such%20as%20chain-of-thought%20and%20explicit%20step-by-step%0Aexplanations.%20However%2C%20these%20improvements%20have%20not%20yet%20fully%20transitioned%20to%0Amultimodal%20contexts%2C%20where%20models%20must%20proactively%20decide%20which%20sensory%0Amodalities%20such%20as%20vision%2C%20audio%2C%20or%20spatial%20perception%20to%20engage%20when%0Ainteracting%20with%20complex%20real-world%20environments.%20In%20this%20paper%2C%20we%20introduce%0Athe%20Chain%20of%20Questions%20%28CoQ%29%20framework%2C%20a%20curiosity-driven%20reasoning%20approach%0Athat%20encourages%20multimodal%20language%20models%20to%20dynamically%20generate%20targeted%0Aquestions%20regarding%20their%20surroundings.%20These%20generated%20questions%20guide%20the%0Amodel%20to%20selectively%20activate%20relevant%20modalities%2C%20thereby%20gathering%20critical%0Ainformation%20necessary%20for%20accurate%20reasoning%20and%20response%20generation.%20We%0Aevaluate%20our%20framework%20on%20a%20novel%20multimodal%20benchmark%20dataset%2C%20assembled%20by%0Aintegrating%20WebGPT%2C%20ScienceQA%2C%20AVSD%2C%20and%20ScanQA%20datasets.%20Experimental%20results%0Ademonstrate%20that%20our%20CoQ%20method%20improves%20a%20foundation%20model%27s%20ability%20to%0Aeffectively%20identify%20and%20integrate%20pertinent%20sensory%20information.%20This%20leads%20to%0Aimproved%20accuracy%2C%20interpretability%2C%20and%20alignment%20of%20the%20reasoning%20process%0Awith%20diverse%20multimodal%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain%2520of%2520Questions%253A%2520Guiding%2520Multimodal%2520Curiosity%2520in%2520Language%2520Models%26entry.906535625%3DNima%2520Iji%2520and%2520Kia%2520Dashtipour%26entry.1292438233%3D%2520%2520Reasoning%2520capabilities%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520substantially%250Aadvanced%2520through%2520methods%2520such%2520as%2520chain-of-thought%2520and%2520explicit%2520step-by-step%250Aexplanations.%2520However%252C%2520these%2520improvements%2520have%2520not%2520yet%2520fully%2520transitioned%2520to%250Amultimodal%2520contexts%252C%2520where%2520models%2520must%2520proactively%2520decide%2520which%2520sensory%250Amodalities%2520such%2520as%2520vision%252C%2520audio%252C%2520or%2520spatial%2520perception%2520to%2520engage%2520when%250Ainteracting%2520with%2520complex%2520real-world%2520environments.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Athe%2520Chain%2520of%2520Questions%2520%2528CoQ%2529%2520framework%252C%2520a%2520curiosity-driven%2520reasoning%2520approach%250Athat%2520encourages%2520multimodal%2520language%2520models%2520to%2520dynamically%2520generate%2520targeted%250Aquestions%2520regarding%2520their%2520surroundings.%2520These%2520generated%2520questions%2520guide%2520the%250Amodel%2520to%2520selectively%2520activate%2520relevant%2520modalities%252C%2520thereby%2520gathering%2520critical%250Ainformation%2520necessary%2520for%2520accurate%2520reasoning%2520and%2520response%2520generation.%2520We%250Aevaluate%2520our%2520framework%2520on%2520a%2520novel%2520multimodal%2520benchmark%2520dataset%252C%2520assembled%2520by%250Aintegrating%2520WebGPT%252C%2520ScienceQA%252C%2520AVSD%252C%2520and%2520ScanQA%2520datasets.%2520Experimental%2520results%250Ademonstrate%2520that%2520our%2520CoQ%2520method%2520improves%2520a%2520foundation%2520model%2527s%2520ability%2520to%250Aeffectively%2520identify%2520and%2520integrate%2520pertinent%2520sensory%2520information.%2520This%2520leads%2520to%250Aimproved%2520accuracy%252C%2520interpretability%252C%2520and%2520alignment%2520of%2520the%2520reasoning%2520process%250Awith%2520diverse%2520multimodal%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain%20of%20Questions%3A%20Guiding%20Multimodal%20Curiosity%20in%20Language%20Models&entry.906535625=Nima%20Iji%20and%20Kia%20Dashtipour&entry.1292438233=%20%20Reasoning%20capabilities%20in%20large%20language%20models%20%28LLMs%29%20have%20substantially%0Aadvanced%20through%20methods%20such%20as%20chain-of-thought%20and%20explicit%20step-by-step%0Aexplanations.%20However%2C%20these%20improvements%20have%20not%20yet%20fully%20transitioned%20to%0Amultimodal%20contexts%2C%20where%20models%20must%20proactively%20decide%20which%20sensory%0Amodalities%20such%20as%20vision%2C%20audio%2C%20or%20spatial%20perception%20to%20engage%20when%0Ainteracting%20with%20complex%20real-world%20environments.%20In%20this%20paper%2C%20we%20introduce%0Athe%20Chain%20of%20Questions%20%28CoQ%29%20framework%2C%20a%20curiosity-driven%20reasoning%20approach%0Athat%20encourages%20multimodal%20language%20models%20to%20dynamically%20generate%20targeted%0Aquestions%20regarding%20their%20surroundings.%20These%20generated%20questions%20guide%20the%0Amodel%20to%20selectively%20activate%20relevant%20modalities%2C%20thereby%20gathering%20critical%0Ainformation%20necessary%20for%20accurate%20reasoning%20and%20response%20generation.%20We%0Aevaluate%20our%20framework%20on%20a%20novel%20multimodal%20benchmark%20dataset%2C%20assembled%20by%0Aintegrating%20WebGPT%2C%20ScienceQA%2C%20AVSD%2C%20and%20ScanQA%20datasets.%20Experimental%20results%0Ademonstrate%20that%20our%20CoQ%20method%20improves%20a%20foundation%20model%27s%20ability%20to%0Aeffectively%20identify%20and%20integrate%20pertinent%20sensory%20information.%20This%20leads%20to%0Aimproved%20accuracy%2C%20interpretability%2C%20and%20alignment%20of%20the%20reasoning%20process%0Awith%20diverse%20multimodal%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04350v1&entry.124074799=Read"},
{"title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for\n  Multi-Agent Perception and Prediction", "author": "Zewei Zhou and Seth Z. Zhao and Tianhui Cai and Zhiyu Huang and Bolei Zhou and Jiaqi Ma", "abstract": "  End-to-end training of multi-agent systems offers significant advantages in\nimproving multi-task performance. However, training such models remains\nchallenging and requires extensive manual design and monitoring. In this work,\nwe introduce TurboTrain, a novel and efficient training framework for\nmulti-agent perception and prediction. TurboTrain comprises two key components:\na multi-agent spatiotemporal pretraining scheme based on masked reconstruction\nlearning and a balanced multi-task learning strategy based on gradient conflict\nsuppression. By streamlining the training process, our framework eliminates the\nneed for manually designing and tuning complex multi-stage training pipelines,\nsubstantially reducing training time and improving performance. We evaluate\nTurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and\ndemonstrate that it further improves the performance of state-of-the-art\nmulti-agent perception and prediction models. Our results highlight that\npretraining effectively captures spatiotemporal multi-agent features and\nsignificantly benefits downstream tasks. Moreover, the proposed balanced\nmulti-task learning strategy enhances detection and prediction.\n", "link": "http://arxiv.org/abs/2508.04682v1", "date": "2025-08-06", "relevancy": 2.3276, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5847}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TurboTrain%3A%20Towards%20Efficient%20and%20Balanced%20Multi-Task%20Learning%20for%0A%20%20Multi-Agent%20Perception%20and%20Prediction&body=Title%3A%20TurboTrain%3A%20Towards%20Efficient%20and%20Balanced%20Multi-Task%20Learning%20for%0A%20%20Multi-Agent%20Perception%20and%20Prediction%0AAuthor%3A%20Zewei%20Zhou%20and%20Seth%20Z.%20Zhao%20and%20Tianhui%20Cai%20and%20Zhiyu%20Huang%20and%20Bolei%20Zhou%20and%20Jiaqi%20Ma%0AAbstract%3A%20%20%20End-to-end%20training%20of%20multi-agent%20systems%20offers%20significant%20advantages%20in%0Aimproving%20multi-task%20performance.%20However%2C%20training%20such%20models%20remains%0Achallenging%20and%20requires%20extensive%20manual%20design%20and%20monitoring.%20In%20this%20work%2C%0Awe%20introduce%20TurboTrain%2C%20a%20novel%20and%20efficient%20training%20framework%20for%0Amulti-agent%20perception%20and%20prediction.%20TurboTrain%20comprises%20two%20key%20components%3A%0Aa%20multi-agent%20spatiotemporal%20pretraining%20scheme%20based%20on%20masked%20reconstruction%0Alearning%20and%20a%20balanced%20multi-task%20learning%20strategy%20based%20on%20gradient%20conflict%0Asuppression.%20By%20streamlining%20the%20training%20process%2C%20our%20framework%20eliminates%20the%0Aneed%20for%20manually%20designing%20and%20tuning%20complex%20multi-stage%20training%20pipelines%2C%0Asubstantially%20reducing%20training%20time%20and%20improving%20performance.%20We%20evaluate%0ATurboTrain%20on%20a%20real-world%20cooperative%20driving%20dataset%2C%20V2XPnP-Seq%2C%20and%0Ademonstrate%20that%20it%20further%20improves%20the%20performance%20of%20state-of-the-art%0Amulti-agent%20perception%20and%20prediction%20models.%20Our%20results%20highlight%20that%0Apretraining%20effectively%20captures%20spatiotemporal%20multi-agent%20features%20and%0Asignificantly%20benefits%20downstream%20tasks.%20Moreover%2C%20the%20proposed%20balanced%0Amulti-task%20learning%20strategy%20enhances%20detection%20and%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTurboTrain%253A%2520Towards%2520Efficient%2520and%2520Balanced%2520Multi-Task%2520Learning%2520for%250A%2520%2520Multi-Agent%2520Perception%2520and%2520Prediction%26entry.906535625%3DZewei%2520Zhou%2520and%2520Seth%2520Z.%2520Zhao%2520and%2520Tianhui%2520Cai%2520and%2520Zhiyu%2520Huang%2520and%2520Bolei%2520Zhou%2520and%2520Jiaqi%2520Ma%26entry.1292438233%3D%2520%2520End-to-end%2520training%2520of%2520multi-agent%2520systems%2520offers%2520significant%2520advantages%2520in%250Aimproving%2520multi-task%2520performance.%2520However%252C%2520training%2520such%2520models%2520remains%250Achallenging%2520and%2520requires%2520extensive%2520manual%2520design%2520and%2520monitoring.%2520In%2520this%2520work%252C%250Awe%2520introduce%2520TurboTrain%252C%2520a%2520novel%2520and%2520efficient%2520training%2520framework%2520for%250Amulti-agent%2520perception%2520and%2520prediction.%2520TurboTrain%2520comprises%2520two%2520key%2520components%253A%250Aa%2520multi-agent%2520spatiotemporal%2520pretraining%2520scheme%2520based%2520on%2520masked%2520reconstruction%250Alearning%2520and%2520a%2520balanced%2520multi-task%2520learning%2520strategy%2520based%2520on%2520gradient%2520conflict%250Asuppression.%2520By%2520streamlining%2520the%2520training%2520process%252C%2520our%2520framework%2520eliminates%2520the%250Aneed%2520for%2520manually%2520designing%2520and%2520tuning%2520complex%2520multi-stage%2520training%2520pipelines%252C%250Asubstantially%2520reducing%2520training%2520time%2520and%2520improving%2520performance.%2520We%2520evaluate%250ATurboTrain%2520on%2520a%2520real-world%2520cooperative%2520driving%2520dataset%252C%2520V2XPnP-Seq%252C%2520and%250Ademonstrate%2520that%2520it%2520further%2520improves%2520the%2520performance%2520of%2520state-of-the-art%250Amulti-agent%2520perception%2520and%2520prediction%2520models.%2520Our%2520results%2520highlight%2520that%250Apretraining%2520effectively%2520captures%2520spatiotemporal%2520multi-agent%2520features%2520and%250Asignificantly%2520benefits%2520downstream%2520tasks.%2520Moreover%252C%2520the%2520proposed%2520balanced%250Amulti-task%2520learning%2520strategy%2520enhances%2520detection%2520and%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TurboTrain%3A%20Towards%20Efficient%20and%20Balanced%20Multi-Task%20Learning%20for%0A%20%20Multi-Agent%20Perception%20and%20Prediction&entry.906535625=Zewei%20Zhou%20and%20Seth%20Z.%20Zhao%20and%20Tianhui%20Cai%20and%20Zhiyu%20Huang%20and%20Bolei%20Zhou%20and%20Jiaqi%20Ma&entry.1292438233=%20%20End-to-end%20training%20of%20multi-agent%20systems%20offers%20significant%20advantages%20in%0Aimproving%20multi-task%20performance.%20However%2C%20training%20such%20models%20remains%0Achallenging%20and%20requires%20extensive%20manual%20design%20and%20monitoring.%20In%20this%20work%2C%0Awe%20introduce%20TurboTrain%2C%20a%20novel%20and%20efficient%20training%20framework%20for%0Amulti-agent%20perception%20and%20prediction.%20TurboTrain%20comprises%20two%20key%20components%3A%0Aa%20multi-agent%20spatiotemporal%20pretraining%20scheme%20based%20on%20masked%20reconstruction%0Alearning%20and%20a%20balanced%20multi-task%20learning%20strategy%20based%20on%20gradient%20conflict%0Asuppression.%20By%20streamlining%20the%20training%20process%2C%20our%20framework%20eliminates%20the%0Aneed%20for%20manually%20designing%20and%20tuning%20complex%20multi-stage%20training%20pipelines%2C%0Asubstantially%20reducing%20training%20time%20and%20improving%20performance.%20We%20evaluate%0ATurboTrain%20on%20a%20real-world%20cooperative%20driving%20dataset%2C%20V2XPnP-Seq%2C%20and%0Ademonstrate%20that%20it%20further%20improves%20the%20performance%20of%20state-of-the-art%0Amulti-agent%20perception%20and%20prediction%20models.%20Our%20results%20highlight%20that%0Apretraining%20effectively%20captures%20spatiotemporal%20multi-agent%20features%20and%0Asignificantly%20benefits%20downstream%20tasks.%20Moreover%2C%20the%20proposed%20balanced%0Amulti-task%20learning%20strategy%20enhances%20detection%20and%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04682v1&entry.124074799=Read"},
{"title": "Composed Object Retrieval: Object-level Retrieval via Composed\n  Expressions", "author": "Tong Wang and Guanyu Yang and Nian Liu and Zongyan Han and Jinxing Zhou and Salman Khan and Fahad Shahbaz Khan", "abstract": "  Retrieving fine-grained visual content based on user intent remains a\nchallenge in multi-modal systems. Although current Composed Image Retrieval\n(CIR) methods combine reference images with retrieval texts, they are\nconstrained to image-level matching and cannot localize specific objects. To\nthis end, we propose Composed Object Retrieval (COR), a brand-new task that\ngoes beyond image-level retrieval to achieve object-level precision, allowing\nthe retrieval and segmentation of target objects based on composed expressions\ncombining reference objects and retrieval texts. COR presents significant\nchallenges in retrieval flexibility, which requires systems to identify\narbitrary objects satisfying composed expressions while avoiding semantically\nsimilar but irrelevant negative objects within the same scene. We construct\nCOR127K, the first large-scale COR benchmark that contains 127,166 retrieval\ntriplets with various semantic transformations in 408 categories. We also\npresent CORE, a unified end-to-end model that integrates reference region\nencoding, adaptive visual-textual interaction, and region-level contrastive\nlearning. Extensive experiments demonstrate that CORE significantly outperforms\nexisting models in both base and novel categories, establishing a simple and\neffective baseline for this challenging task while opening new directions for\nfine-grained multi-modal retrieval research.\n", "link": "http://arxiv.org/abs/2508.04424v1", "date": "2025-08-06", "relevancy": 2.3271, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Composed%20Object%20Retrieval%3A%20Object-level%20Retrieval%20via%20Composed%0A%20%20Expressions&body=Title%3A%20Composed%20Object%20Retrieval%3A%20Object-level%20Retrieval%20via%20Composed%0A%20%20Expressions%0AAuthor%3A%20Tong%20Wang%20and%20Guanyu%20Yang%20and%20Nian%20Liu%20and%20Zongyan%20Han%20and%20Jinxing%20Zhou%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20Retrieving%20fine-grained%20visual%20content%20based%20on%20user%20intent%20remains%20a%0Achallenge%20in%20multi-modal%20systems.%20Although%20current%20Composed%20Image%20Retrieval%0A%28CIR%29%20methods%20combine%20reference%20images%20with%20retrieval%20texts%2C%20they%20are%0Aconstrained%20to%20image-level%20matching%20and%20cannot%20localize%20specific%20objects.%20To%0Athis%20end%2C%20we%20propose%20Composed%20Object%20Retrieval%20%28COR%29%2C%20a%20brand-new%20task%20that%0Agoes%20beyond%20image-level%20retrieval%20to%20achieve%20object-level%20precision%2C%20allowing%0Athe%20retrieval%20and%20segmentation%20of%20target%20objects%20based%20on%20composed%20expressions%0Acombining%20reference%20objects%20and%20retrieval%20texts.%20COR%20presents%20significant%0Achallenges%20in%20retrieval%20flexibility%2C%20which%20requires%20systems%20to%20identify%0Aarbitrary%20objects%20satisfying%20composed%20expressions%20while%20avoiding%20semantically%0Asimilar%20but%20irrelevant%20negative%20objects%20within%20the%20same%20scene.%20We%20construct%0ACOR127K%2C%20the%20first%20large-scale%20COR%20benchmark%20that%20contains%20127%2C166%20retrieval%0Atriplets%20with%20various%20semantic%20transformations%20in%20408%20categories.%20We%20also%0Apresent%20CORE%2C%20a%20unified%20end-to-end%20model%20that%20integrates%20reference%20region%0Aencoding%2C%20adaptive%20visual-textual%20interaction%2C%20and%20region-level%20contrastive%0Alearning.%20Extensive%20experiments%20demonstrate%20that%20CORE%20significantly%20outperforms%0Aexisting%20models%20in%20both%20base%20and%20novel%20categories%2C%20establishing%20a%20simple%20and%0Aeffective%20baseline%20for%20this%20challenging%20task%20while%20opening%20new%20directions%20for%0Afine-grained%20multi-modal%20retrieval%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComposed%2520Object%2520Retrieval%253A%2520Object-level%2520Retrieval%2520via%2520Composed%250A%2520%2520Expressions%26entry.906535625%3DTong%2520Wang%2520and%2520Guanyu%2520Yang%2520and%2520Nian%2520Liu%2520and%2520Zongyan%2520Han%2520and%2520Jinxing%2520Zhou%2520and%2520Salman%2520Khan%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520Retrieving%2520fine-grained%2520visual%2520content%2520based%2520on%2520user%2520intent%2520remains%2520a%250Achallenge%2520in%2520multi-modal%2520systems.%2520Although%2520current%2520Composed%2520Image%2520Retrieval%250A%2528CIR%2529%2520methods%2520combine%2520reference%2520images%2520with%2520retrieval%2520texts%252C%2520they%2520are%250Aconstrained%2520to%2520image-level%2520matching%2520and%2520cannot%2520localize%2520specific%2520objects.%2520To%250Athis%2520end%252C%2520we%2520propose%2520Composed%2520Object%2520Retrieval%2520%2528COR%2529%252C%2520a%2520brand-new%2520task%2520that%250Agoes%2520beyond%2520image-level%2520retrieval%2520to%2520achieve%2520object-level%2520precision%252C%2520allowing%250Athe%2520retrieval%2520and%2520segmentation%2520of%2520target%2520objects%2520based%2520on%2520composed%2520expressions%250Acombining%2520reference%2520objects%2520and%2520retrieval%2520texts.%2520COR%2520presents%2520significant%250Achallenges%2520in%2520retrieval%2520flexibility%252C%2520which%2520requires%2520systems%2520to%2520identify%250Aarbitrary%2520objects%2520satisfying%2520composed%2520expressions%2520while%2520avoiding%2520semantically%250Asimilar%2520but%2520irrelevant%2520negative%2520objects%2520within%2520the%2520same%2520scene.%2520We%2520construct%250ACOR127K%252C%2520the%2520first%2520large-scale%2520COR%2520benchmark%2520that%2520contains%2520127%252C166%2520retrieval%250Atriplets%2520with%2520various%2520semantic%2520transformations%2520in%2520408%2520categories.%2520We%2520also%250Apresent%2520CORE%252C%2520a%2520unified%2520end-to-end%2520model%2520that%2520integrates%2520reference%2520region%250Aencoding%252C%2520adaptive%2520visual-textual%2520interaction%252C%2520and%2520region-level%2520contrastive%250Alearning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520CORE%2520significantly%2520outperforms%250Aexisting%2520models%2520in%2520both%2520base%2520and%2520novel%2520categories%252C%2520establishing%2520a%2520simple%2520and%250Aeffective%2520baseline%2520for%2520this%2520challenging%2520task%2520while%2520opening%2520new%2520directions%2520for%250Afine-grained%2520multi-modal%2520retrieval%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Composed%20Object%20Retrieval%3A%20Object-level%20Retrieval%20via%20Composed%0A%20%20Expressions&entry.906535625=Tong%20Wang%20and%20Guanyu%20Yang%20and%20Nian%20Liu%20and%20Zongyan%20Han%20and%20Jinxing%20Zhou%20and%20Salman%20Khan%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20Retrieving%20fine-grained%20visual%20content%20based%20on%20user%20intent%20remains%20a%0Achallenge%20in%20multi-modal%20systems.%20Although%20current%20Composed%20Image%20Retrieval%0A%28CIR%29%20methods%20combine%20reference%20images%20with%20retrieval%20texts%2C%20they%20are%0Aconstrained%20to%20image-level%20matching%20and%20cannot%20localize%20specific%20objects.%20To%0Athis%20end%2C%20we%20propose%20Composed%20Object%20Retrieval%20%28COR%29%2C%20a%20brand-new%20task%20that%0Agoes%20beyond%20image-level%20retrieval%20to%20achieve%20object-level%20precision%2C%20allowing%0Athe%20retrieval%20and%20segmentation%20of%20target%20objects%20based%20on%20composed%20expressions%0Acombining%20reference%20objects%20and%20retrieval%20texts.%20COR%20presents%20significant%0Achallenges%20in%20retrieval%20flexibility%2C%20which%20requires%20systems%20to%20identify%0Aarbitrary%20objects%20satisfying%20composed%20expressions%20while%20avoiding%20semantically%0Asimilar%20but%20irrelevant%20negative%20objects%20within%20the%20same%20scene.%20We%20construct%0ACOR127K%2C%20the%20first%20large-scale%20COR%20benchmark%20that%20contains%20127%2C166%20retrieval%0Atriplets%20with%20various%20semantic%20transformations%20in%20408%20categories.%20We%20also%0Apresent%20CORE%2C%20a%20unified%20end-to-end%20model%20that%20integrates%20reference%20region%0Aencoding%2C%20adaptive%20visual-textual%20interaction%2C%20and%20region-level%20contrastive%0Alearning.%20Extensive%20experiments%20demonstrate%20that%20CORE%20significantly%20outperforms%0Aexisting%20models%20in%20both%20base%20and%20novel%20categories%2C%20establishing%20a%20simple%20and%0Aeffective%20baseline%20for%20this%20challenging%20task%20while%20opening%20new%20directions%20for%0Afine-grained%20multi-modal%20retrieval%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04424v1&entry.124074799=Read"},
{"title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language\n  Models for Text-Video Retrieval", "author": "Dohwan Ko and Ji Soo Lee and Minhyuk Choi and Zihang Meng and Hyunwoo J. Kim", "abstract": "  Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.\n", "link": "http://arxiv.org/abs/2507.23284v2", "date": "2025-08-06", "relevancy": 2.302, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5952}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5885}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bidirectional%20Likelihood%20Estimation%20with%20Multi-Modal%20Large%20Language%0A%20%20Models%20for%20Text-Video%20Retrieval&body=Title%3A%20Bidirectional%20Likelihood%20Estimation%20with%20Multi-Modal%20Large%20Language%0A%20%20Models%20for%20Text-Video%20Retrieval%0AAuthor%3A%20Dohwan%20Ko%20and%20Ji%20Soo%20Lee%20and%20Minhyuk%20Choi%20and%20Zihang%20Meng%20and%20Hyunwoo%20J.%20Kim%0AAbstract%3A%20%20%20Text-Video%20Retrieval%20aims%20to%20find%20the%20most%20relevant%20text%20%28or%20video%29%20candidate%0Agiven%20a%20video%20%28or%20text%29%20query%20from%20large-scale%20online%20databases.%20Recent%20work%0Aleverages%20multi-modal%20large%20language%20models%20%28MLLMs%29%20to%20improve%20retrieval%2C%0Aespecially%20for%20long%20or%20complex%20query-candidate%20pairs.%20However%2C%20we%20observe%20that%0Athe%20naive%20application%20of%20MLLMs%2C%20i.e.%2C%20retrieval%20based%20on%20candidate%20likelihood%2C%0Aintroduces%20candidate%20prior%20bias%2C%20favoring%20candidates%20with%20inherently%20higher%0Apriors%20over%20those%20more%20relevant%20to%20the%20query.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Aretrieval%20framework%2C%20Bidirectional%20Likelihood%20Estimation%20with%20MLLM%20%28BLiM%29%2C%0Awhich%20leverages%20both%20query%20and%20candidate%20likelihoods%20by%20training%20the%20model%20to%0Agenerate%20text%20from%20a%20given%20video%20as%20well%20as%20video%20features%20from%20a%20given%20text.%0AFurthermore%2C%20we%20introduce%20Candidate%20Prior%20Normalization%20%28CPN%29%2C%20a%20simple%20yet%0Aeffective%20training-free%20score%20calibration%20module%20designed%20to%20mitigate%20candidate%0Aprior%20bias%20in%20candidate%20likelihood.%20On%20four%20Text-Video%20Retrieval%20benchmarks%2C%0Aour%20BLiM%20equipped%20with%20CPN%20outperforms%20previous%20state-of-the-art%20models%20by%206.4%0AR%401%20on%20average%2C%20effectively%20alleviating%20candidate%20prior%20bias%20and%20emphasizing%0Aquery-candidate%20relevance.%20Our%20in-depth%20analysis%20across%20various%20multi-modal%0Atasks%20beyond%20retrieval%20highlights%20the%20broad%20applicability%20of%20CPN%20which%20enhances%0Avisual%20understanding%20by%20reducing%20reliance%20on%20textual%20priors.%20Code%20is%20available%0Aat%20https%3A//github.com/mlvlab/BLiM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23284v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBidirectional%2520Likelihood%2520Estimation%2520with%2520Multi-Modal%2520Large%2520Language%250A%2520%2520Models%2520for%2520Text-Video%2520Retrieval%26entry.906535625%3DDohwan%2520Ko%2520and%2520Ji%2520Soo%2520Lee%2520and%2520Minhyuk%2520Choi%2520and%2520Zihang%2520Meng%2520and%2520Hyunwoo%2520J.%2520Kim%26entry.1292438233%3D%2520%2520Text-Video%2520Retrieval%2520aims%2520to%2520find%2520the%2520most%2520relevant%2520text%2520%2528or%2520video%2529%2520candidate%250Agiven%2520a%2520video%2520%2528or%2520text%2529%2520query%2520from%2520large-scale%2520online%2520databases.%2520Recent%2520work%250Aleverages%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520to%2520improve%2520retrieval%252C%250Aespecially%2520for%2520long%2520or%2520complex%2520query-candidate%2520pairs.%2520However%252C%2520we%2520observe%2520that%250Athe%2520naive%2520application%2520of%2520MLLMs%252C%2520i.e.%252C%2520retrieval%2520based%2520on%2520candidate%2520likelihood%252C%250Aintroduces%2520candidate%2520prior%2520bias%252C%2520favoring%2520candidates%2520with%2520inherently%2520higher%250Apriors%2520over%2520those%2520more%2520relevant%2520to%2520the%2520query.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%250Aretrieval%2520framework%252C%2520Bidirectional%2520Likelihood%2520Estimation%2520with%2520MLLM%2520%2528BLiM%2529%252C%250Awhich%2520leverages%2520both%2520query%2520and%2520candidate%2520likelihoods%2520by%2520training%2520the%2520model%2520to%250Agenerate%2520text%2520from%2520a%2520given%2520video%2520as%2520well%2520as%2520video%2520features%2520from%2520a%2520given%2520text.%250AFurthermore%252C%2520we%2520introduce%2520Candidate%2520Prior%2520Normalization%2520%2528CPN%2529%252C%2520a%2520simple%2520yet%250Aeffective%2520training-free%2520score%2520calibration%2520module%2520designed%2520to%2520mitigate%2520candidate%250Aprior%2520bias%2520in%2520candidate%2520likelihood.%2520On%2520four%2520Text-Video%2520Retrieval%2520benchmarks%252C%250Aour%2520BLiM%2520equipped%2520with%2520CPN%2520outperforms%2520previous%2520state-of-the-art%2520models%2520by%25206.4%250AR%25401%2520on%2520average%252C%2520effectively%2520alleviating%2520candidate%2520prior%2520bias%2520and%2520emphasizing%250Aquery-candidate%2520relevance.%2520Our%2520in-depth%2520analysis%2520across%2520various%2520multi-modal%250Atasks%2520beyond%2520retrieval%2520highlights%2520the%2520broad%2520applicability%2520of%2520CPN%2520which%2520enhances%250Avisual%2520understanding%2520by%2520reducing%2520reliance%2520on%2520textual%2520priors.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/mlvlab/BLiM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23284v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bidirectional%20Likelihood%20Estimation%20with%20Multi-Modal%20Large%20Language%0A%20%20Models%20for%20Text-Video%20Retrieval&entry.906535625=Dohwan%20Ko%20and%20Ji%20Soo%20Lee%20and%20Minhyuk%20Choi%20and%20Zihang%20Meng%20and%20Hyunwoo%20J.%20Kim&entry.1292438233=%20%20Text-Video%20Retrieval%20aims%20to%20find%20the%20most%20relevant%20text%20%28or%20video%29%20candidate%0Agiven%20a%20video%20%28or%20text%29%20query%20from%20large-scale%20online%20databases.%20Recent%20work%0Aleverages%20multi-modal%20large%20language%20models%20%28MLLMs%29%20to%20improve%20retrieval%2C%0Aespecially%20for%20long%20or%20complex%20query-candidate%20pairs.%20However%2C%20we%20observe%20that%0Athe%20naive%20application%20of%20MLLMs%2C%20i.e.%2C%20retrieval%20based%20on%20candidate%20likelihood%2C%0Aintroduces%20candidate%20prior%20bias%2C%20favoring%20candidates%20with%20inherently%20higher%0Apriors%20over%20those%20more%20relevant%20to%20the%20query.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Aretrieval%20framework%2C%20Bidirectional%20Likelihood%20Estimation%20with%20MLLM%20%28BLiM%29%2C%0Awhich%20leverages%20both%20query%20and%20candidate%20likelihoods%20by%20training%20the%20model%20to%0Agenerate%20text%20from%20a%20given%20video%20as%20well%20as%20video%20features%20from%20a%20given%20text.%0AFurthermore%2C%20we%20introduce%20Candidate%20Prior%20Normalization%20%28CPN%29%2C%20a%20simple%20yet%0Aeffective%20training-free%20score%20calibration%20module%20designed%20to%20mitigate%20candidate%0Aprior%20bias%20in%20candidate%20likelihood.%20On%20four%20Text-Video%20Retrieval%20benchmarks%2C%0Aour%20BLiM%20equipped%20with%20CPN%20outperforms%20previous%20state-of-the-art%20models%20by%206.4%0AR%401%20on%20average%2C%20effectively%20alleviating%20candidate%20prior%20bias%20and%20emphasizing%0Aquery-candidate%20relevance.%20Our%20in-depth%20analysis%20across%20various%20multi-modal%0Atasks%20beyond%20retrieval%20highlights%20the%20broad%20applicability%20of%20CPN%20which%20enhances%0Avisual%20understanding%20by%20reducing%20reliance%20on%20textual%20priors.%20Code%20is%20available%0Aat%20https%3A//github.com/mlvlab/BLiM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23284v2&entry.124074799=Read"},
{"title": "A Comprehensive Framework for Uncertainty Quantification of Voxel-wise\n  Supervised Models in IVIM MRI", "author": "Nicola Casali and Alessandro Brusaferri and Giuseppe Baselli and Stefano Fumagalli and Edoardo Micotti and Gianluigi Forloni and Riaz Hussein and Giovanna Rizzo and Alfonso Mastropietro", "abstract": "  Accurate estimation of intravoxel incoherent motion (IVIM) parameters from\ndiffusion-weighted MRI remains challenging due to the ill-posed nature of the\ninverse problem and high sensitivity to noise, particularly in the perfusion\ncompartment. In this work, we propose a probabilistic deep learning framework\nbased on Deep Ensembles (DE) of Mixture Density Networks (MDNs), enabling\nestimation of total predictive uncertainty and decomposition into aleatoric\n(AU) and epistemic (EU) components. The method was benchmarked against non\nprobabilistic neural networks, a Bayesian fitting approach and a probabilistic\nnetwork with single Gaussian parametrization. Supervised training was performed\non synthetic data, and evaluation was conducted on both simulated and two in\nvivo datasets. The reliability of the quantified uncertainties was assessed\nusing calibration curves, output distribution sharpness, and the Continuous\nRanked Probability Score (CRPS). MDNs produced more calibrated and sharper\npredictive distributions for the D and f parameters, although slight\noverconfidence was observed in D*. The Robust Coefficient of Variation (RCV)\nindicated smoother in vivo estimates for D* with MDNs compared to Gaussian\nmodel. Despite the training data covering the expected physiological range,\nelevated EU in vivo suggests a mismatch with real acquisition conditions,\nhighlighting the importance of incorporating EU, which was allowed by DE.\nOverall, we present a comprehensive framework for IVIM fitting with uncertainty\nquantification, which enables the identification and interpretation of\nunreliable estimates. The proposed approach can also be adopted for fitting\nother physical models through appropriate architectural and simulation\nadjustments.\n", "link": "http://arxiv.org/abs/2508.04588v1", "date": "2025-08-06", "relevancy": 2.2964, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5923}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Framework%20for%20Uncertainty%20Quantification%20of%20Voxel-wise%0A%20%20Supervised%20Models%20in%20IVIM%20MRI&body=Title%3A%20A%20Comprehensive%20Framework%20for%20Uncertainty%20Quantification%20of%20Voxel-wise%0A%20%20Supervised%20Models%20in%20IVIM%20MRI%0AAuthor%3A%20Nicola%20Casali%20and%20Alessandro%20Brusaferri%20and%20Giuseppe%20Baselli%20and%20Stefano%20Fumagalli%20and%20Edoardo%20Micotti%20and%20Gianluigi%20Forloni%20and%20Riaz%20Hussein%20and%20Giovanna%20Rizzo%20and%20Alfonso%20Mastropietro%0AAbstract%3A%20%20%20Accurate%20estimation%20of%20intravoxel%20incoherent%20motion%20%28IVIM%29%20parameters%20from%0Adiffusion-weighted%20MRI%20remains%20challenging%20due%20to%20the%20ill-posed%20nature%20of%20the%0Ainverse%20problem%20and%20high%20sensitivity%20to%20noise%2C%20particularly%20in%20the%20perfusion%0Acompartment.%20In%20this%20work%2C%20we%20propose%20a%20probabilistic%20deep%20learning%20framework%0Abased%20on%20Deep%20Ensembles%20%28DE%29%20of%20Mixture%20Density%20Networks%20%28MDNs%29%2C%20enabling%0Aestimation%20of%20total%20predictive%20uncertainty%20and%20decomposition%20into%20aleatoric%0A%28AU%29%20and%20epistemic%20%28EU%29%20components.%20The%20method%20was%20benchmarked%20against%20non%0Aprobabilistic%20neural%20networks%2C%20a%20Bayesian%20fitting%20approach%20and%20a%20probabilistic%0Anetwork%20with%20single%20Gaussian%20parametrization.%20Supervised%20training%20was%20performed%0Aon%20synthetic%20data%2C%20and%20evaluation%20was%20conducted%20on%20both%20simulated%20and%20two%20in%0Avivo%20datasets.%20The%20reliability%20of%20the%20quantified%20uncertainties%20was%20assessed%0Ausing%20calibration%20curves%2C%20output%20distribution%20sharpness%2C%20and%20the%20Continuous%0ARanked%20Probability%20Score%20%28CRPS%29.%20MDNs%20produced%20more%20calibrated%20and%20sharper%0Apredictive%20distributions%20for%20the%20D%20and%20f%20parameters%2C%20although%20slight%0Aoverconfidence%20was%20observed%20in%20D%2A.%20The%20Robust%20Coefficient%20of%20Variation%20%28RCV%29%0Aindicated%20smoother%20in%20vivo%20estimates%20for%20D%2A%20with%20MDNs%20compared%20to%20Gaussian%0Amodel.%20Despite%20the%20training%20data%20covering%20the%20expected%20physiological%20range%2C%0Aelevated%20EU%20in%20vivo%20suggests%20a%20mismatch%20with%20real%20acquisition%20conditions%2C%0Ahighlighting%20the%20importance%20of%20incorporating%20EU%2C%20which%20was%20allowed%20by%20DE.%0AOverall%2C%20we%20present%20a%20comprehensive%20framework%20for%20IVIM%20fitting%20with%20uncertainty%0Aquantification%2C%20which%20enables%20the%20identification%20and%20interpretation%20of%0Aunreliable%20estimates.%20The%20proposed%20approach%20can%20also%20be%20adopted%20for%20fitting%0Aother%20physical%20models%20through%20appropriate%20architectural%20and%20simulation%0Aadjustments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Framework%2520for%2520Uncertainty%2520Quantification%2520of%2520Voxel-wise%250A%2520%2520Supervised%2520Models%2520in%2520IVIM%2520MRI%26entry.906535625%3DNicola%2520Casali%2520and%2520Alessandro%2520Brusaferri%2520and%2520Giuseppe%2520Baselli%2520and%2520Stefano%2520Fumagalli%2520and%2520Edoardo%2520Micotti%2520and%2520Gianluigi%2520Forloni%2520and%2520Riaz%2520Hussein%2520and%2520Giovanna%2520Rizzo%2520and%2520Alfonso%2520Mastropietro%26entry.1292438233%3D%2520%2520Accurate%2520estimation%2520of%2520intravoxel%2520incoherent%2520motion%2520%2528IVIM%2529%2520parameters%2520from%250Adiffusion-weighted%2520MRI%2520remains%2520challenging%2520due%2520to%2520the%2520ill-posed%2520nature%2520of%2520the%250Ainverse%2520problem%2520and%2520high%2520sensitivity%2520to%2520noise%252C%2520particularly%2520in%2520the%2520perfusion%250Acompartment.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520probabilistic%2520deep%2520learning%2520framework%250Abased%2520on%2520Deep%2520Ensembles%2520%2528DE%2529%2520of%2520Mixture%2520Density%2520Networks%2520%2528MDNs%2529%252C%2520enabling%250Aestimation%2520of%2520total%2520predictive%2520uncertainty%2520and%2520decomposition%2520into%2520aleatoric%250A%2528AU%2529%2520and%2520epistemic%2520%2528EU%2529%2520components.%2520The%2520method%2520was%2520benchmarked%2520against%2520non%250Aprobabilistic%2520neural%2520networks%252C%2520a%2520Bayesian%2520fitting%2520approach%2520and%2520a%2520probabilistic%250Anetwork%2520with%2520single%2520Gaussian%2520parametrization.%2520Supervised%2520training%2520was%2520performed%250Aon%2520synthetic%2520data%252C%2520and%2520evaluation%2520was%2520conducted%2520on%2520both%2520simulated%2520and%2520two%2520in%250Avivo%2520datasets.%2520The%2520reliability%2520of%2520the%2520quantified%2520uncertainties%2520was%2520assessed%250Ausing%2520calibration%2520curves%252C%2520output%2520distribution%2520sharpness%252C%2520and%2520the%2520Continuous%250ARanked%2520Probability%2520Score%2520%2528CRPS%2529.%2520MDNs%2520produced%2520more%2520calibrated%2520and%2520sharper%250Apredictive%2520distributions%2520for%2520the%2520D%2520and%2520f%2520parameters%252C%2520although%2520slight%250Aoverconfidence%2520was%2520observed%2520in%2520D%252A.%2520The%2520Robust%2520Coefficient%2520of%2520Variation%2520%2528RCV%2529%250Aindicated%2520smoother%2520in%2520vivo%2520estimates%2520for%2520D%252A%2520with%2520MDNs%2520compared%2520to%2520Gaussian%250Amodel.%2520Despite%2520the%2520training%2520data%2520covering%2520the%2520expected%2520physiological%2520range%252C%250Aelevated%2520EU%2520in%2520vivo%2520suggests%2520a%2520mismatch%2520with%2520real%2520acquisition%2520conditions%252C%250Ahighlighting%2520the%2520importance%2520of%2520incorporating%2520EU%252C%2520which%2520was%2520allowed%2520by%2520DE.%250AOverall%252C%2520we%2520present%2520a%2520comprehensive%2520framework%2520for%2520IVIM%2520fitting%2520with%2520uncertainty%250Aquantification%252C%2520which%2520enables%2520the%2520identification%2520and%2520interpretation%2520of%250Aunreliable%2520estimates.%2520The%2520proposed%2520approach%2520can%2520also%2520be%2520adopted%2520for%2520fitting%250Aother%2520physical%2520models%2520through%2520appropriate%2520architectural%2520and%2520simulation%250Aadjustments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Framework%20for%20Uncertainty%20Quantification%20of%20Voxel-wise%0A%20%20Supervised%20Models%20in%20IVIM%20MRI&entry.906535625=Nicola%20Casali%20and%20Alessandro%20Brusaferri%20and%20Giuseppe%20Baselli%20and%20Stefano%20Fumagalli%20and%20Edoardo%20Micotti%20and%20Gianluigi%20Forloni%20and%20Riaz%20Hussein%20and%20Giovanna%20Rizzo%20and%20Alfonso%20Mastropietro&entry.1292438233=%20%20Accurate%20estimation%20of%20intravoxel%20incoherent%20motion%20%28IVIM%29%20parameters%20from%0Adiffusion-weighted%20MRI%20remains%20challenging%20due%20to%20the%20ill-posed%20nature%20of%20the%0Ainverse%20problem%20and%20high%20sensitivity%20to%20noise%2C%20particularly%20in%20the%20perfusion%0Acompartment.%20In%20this%20work%2C%20we%20propose%20a%20probabilistic%20deep%20learning%20framework%0Abased%20on%20Deep%20Ensembles%20%28DE%29%20of%20Mixture%20Density%20Networks%20%28MDNs%29%2C%20enabling%0Aestimation%20of%20total%20predictive%20uncertainty%20and%20decomposition%20into%20aleatoric%0A%28AU%29%20and%20epistemic%20%28EU%29%20components.%20The%20method%20was%20benchmarked%20against%20non%0Aprobabilistic%20neural%20networks%2C%20a%20Bayesian%20fitting%20approach%20and%20a%20probabilistic%0Anetwork%20with%20single%20Gaussian%20parametrization.%20Supervised%20training%20was%20performed%0Aon%20synthetic%20data%2C%20and%20evaluation%20was%20conducted%20on%20both%20simulated%20and%20two%20in%0Avivo%20datasets.%20The%20reliability%20of%20the%20quantified%20uncertainties%20was%20assessed%0Ausing%20calibration%20curves%2C%20output%20distribution%20sharpness%2C%20and%20the%20Continuous%0ARanked%20Probability%20Score%20%28CRPS%29.%20MDNs%20produced%20more%20calibrated%20and%20sharper%0Apredictive%20distributions%20for%20the%20D%20and%20f%20parameters%2C%20although%20slight%0Aoverconfidence%20was%20observed%20in%20D%2A.%20The%20Robust%20Coefficient%20of%20Variation%20%28RCV%29%0Aindicated%20smoother%20in%20vivo%20estimates%20for%20D%2A%20with%20MDNs%20compared%20to%20Gaussian%0Amodel.%20Despite%20the%20training%20data%20covering%20the%20expected%20physiological%20range%2C%0Aelevated%20EU%20in%20vivo%20suggests%20a%20mismatch%20with%20real%20acquisition%20conditions%2C%0Ahighlighting%20the%20importance%20of%20incorporating%20EU%2C%20which%20was%20allowed%20by%20DE.%0AOverall%2C%20we%20present%20a%20comprehensive%20framework%20for%20IVIM%20fitting%20with%20uncertainty%0Aquantification%2C%20which%20enables%20the%20identification%20and%20interpretation%20of%0Aunreliable%20estimates.%20The%20proposed%20approach%20can%20also%20be%20adopted%20for%20fitting%0Aother%20physical%20models%20through%20appropriate%20architectural%20and%20simulation%0Aadjustments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04588v1&entry.124074799=Read"},
{"title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for\n  Egocentric Human-Object-Human Interactions", "author": "Liang Xu and Chengqun Yang and Zili Lin and Fei Xu and Yifan Liu and Congsheng Xu and Yiyi Zhang and Jie Qin and Xingdong Sheng and Yunhui Liu and Xin Jin and Yichao Yan and Wenjun Zeng and Xiaokang Yang", "abstract": "  Learning action models from real-world human-centric interaction datasets is\nimportant towards building general-purpose intelligent assistants with\nefficiency. However, most existing datasets only offer specialist interaction\ncategory and ignore that AI assistants perceive and act based on first-person\nacquisition. We urge that both the generalist interaction knowledge and\negocentric modality are indispensable. In this paper, we embed the\nmanual-assisted task into a vision-language-action framework, where the\nassistant provides services to the instructor following egocentric vision and\ncommands. With our hybrid RGB-MoCap system, pairs of assistants and instructors\nengage with multiple objects and the scene following GPT-generated scripts.\nUnder this setting, we accomplish InterVLA, the first large-scale\nhuman-object-human interaction dataset with 11.4 hours and 1.2M frames of\nmultimodal data, spanning 2 egocentric and 5 exocentric videos, accurate\nhuman/object motions and verbal commands. Furthermore, we establish novel\nbenchmarks on egocentric human motion estimation, interaction synthesis, and\ninteraction prediction with comprehensive analysis. We believe that our\nInterVLA testbed and the benchmarks will foster future works on building AI\nagents in the physical world.\n", "link": "http://arxiv.org/abs/2508.04681v1", "date": "2025-08-06", "relevancy": 2.2962, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5804}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5722}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceiving%20and%20Acting%20in%20First-Person%3A%20A%20Dataset%20and%20Benchmark%20for%0A%20%20Egocentric%20Human-Object-Human%20Interactions&body=Title%3A%20Perceiving%20and%20Acting%20in%20First-Person%3A%20A%20Dataset%20and%20Benchmark%20for%0A%20%20Egocentric%20Human-Object-Human%20Interactions%0AAuthor%3A%20Liang%20Xu%20and%20Chengqun%20Yang%20and%20Zili%20Lin%20and%20Fei%20Xu%20and%20Yifan%20Liu%20and%20Congsheng%20Xu%20and%20Yiyi%20Zhang%20and%20Jie%20Qin%20and%20Xingdong%20Sheng%20and%20Yunhui%20Liu%20and%20Xin%20Jin%20and%20Yichao%20Yan%20and%20Wenjun%20Zeng%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Learning%20action%20models%20from%20real-world%20human-centric%20interaction%20datasets%20is%0Aimportant%20towards%20building%20general-purpose%20intelligent%20assistants%20with%0Aefficiency.%20However%2C%20most%20existing%20datasets%20only%20offer%20specialist%20interaction%0Acategory%20and%20ignore%20that%20AI%20assistants%20perceive%20and%20act%20based%20on%20first-person%0Aacquisition.%20We%20urge%20that%20both%20the%20generalist%20interaction%20knowledge%20and%0Aegocentric%20modality%20are%20indispensable.%20In%20this%20paper%2C%20we%20embed%20the%0Amanual-assisted%20task%20into%20a%20vision-language-action%20framework%2C%20where%20the%0Aassistant%20provides%20services%20to%20the%20instructor%20following%20egocentric%20vision%20and%0Acommands.%20With%20our%20hybrid%20RGB-MoCap%20system%2C%20pairs%20of%20assistants%20and%20instructors%0Aengage%20with%20multiple%20objects%20and%20the%20scene%20following%20GPT-generated%20scripts.%0AUnder%20this%20setting%2C%20we%20accomplish%20InterVLA%2C%20the%20first%20large-scale%0Ahuman-object-human%20interaction%20dataset%20with%2011.4%20hours%20and%201.2M%20frames%20of%0Amultimodal%20data%2C%20spanning%202%20egocentric%20and%205%20exocentric%20videos%2C%20accurate%0Ahuman/object%20motions%20and%20verbal%20commands.%20Furthermore%2C%20we%20establish%20novel%0Abenchmarks%20on%20egocentric%20human%20motion%20estimation%2C%20interaction%20synthesis%2C%20and%0Ainteraction%20prediction%20with%20comprehensive%20analysis.%20We%20believe%20that%20our%0AInterVLA%20testbed%20and%20the%20benchmarks%20will%20foster%20future%20works%20on%20building%20AI%0Aagents%20in%20the%20physical%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04681v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceiving%2520and%2520Acting%2520in%2520First-Person%253A%2520A%2520Dataset%2520and%2520Benchmark%2520for%250A%2520%2520Egocentric%2520Human-Object-Human%2520Interactions%26entry.906535625%3DLiang%2520Xu%2520and%2520Chengqun%2520Yang%2520and%2520Zili%2520Lin%2520and%2520Fei%2520Xu%2520and%2520Yifan%2520Liu%2520and%2520Congsheng%2520Xu%2520and%2520Yiyi%2520Zhang%2520and%2520Jie%2520Qin%2520and%2520Xingdong%2520Sheng%2520and%2520Yunhui%2520Liu%2520and%2520Xin%2520Jin%2520and%2520Yichao%2520Yan%2520and%2520Wenjun%2520Zeng%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Learning%2520action%2520models%2520from%2520real-world%2520human-centric%2520interaction%2520datasets%2520is%250Aimportant%2520towards%2520building%2520general-purpose%2520intelligent%2520assistants%2520with%250Aefficiency.%2520However%252C%2520most%2520existing%2520datasets%2520only%2520offer%2520specialist%2520interaction%250Acategory%2520and%2520ignore%2520that%2520AI%2520assistants%2520perceive%2520and%2520act%2520based%2520on%2520first-person%250Aacquisition.%2520We%2520urge%2520that%2520both%2520the%2520generalist%2520interaction%2520knowledge%2520and%250Aegocentric%2520modality%2520are%2520indispensable.%2520In%2520this%2520paper%252C%2520we%2520embed%2520the%250Amanual-assisted%2520task%2520into%2520a%2520vision-language-action%2520framework%252C%2520where%2520the%250Aassistant%2520provides%2520services%2520to%2520the%2520instructor%2520following%2520egocentric%2520vision%2520and%250Acommands.%2520With%2520our%2520hybrid%2520RGB-MoCap%2520system%252C%2520pairs%2520of%2520assistants%2520and%2520instructors%250Aengage%2520with%2520multiple%2520objects%2520and%2520the%2520scene%2520following%2520GPT-generated%2520scripts.%250AUnder%2520this%2520setting%252C%2520we%2520accomplish%2520InterVLA%252C%2520the%2520first%2520large-scale%250Ahuman-object-human%2520interaction%2520dataset%2520with%252011.4%2520hours%2520and%25201.2M%2520frames%2520of%250Amultimodal%2520data%252C%2520spanning%25202%2520egocentric%2520and%25205%2520exocentric%2520videos%252C%2520accurate%250Ahuman/object%2520motions%2520and%2520verbal%2520commands.%2520Furthermore%252C%2520we%2520establish%2520novel%250Abenchmarks%2520on%2520egocentric%2520human%2520motion%2520estimation%252C%2520interaction%2520synthesis%252C%2520and%250Ainteraction%2520prediction%2520with%2520comprehensive%2520analysis.%2520We%2520believe%2520that%2520our%250AInterVLA%2520testbed%2520and%2520the%2520benchmarks%2520will%2520foster%2520future%2520works%2520on%2520building%2520AI%250Aagents%2520in%2520the%2520physical%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04681v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceiving%20and%20Acting%20in%20First-Person%3A%20A%20Dataset%20and%20Benchmark%20for%0A%20%20Egocentric%20Human-Object-Human%20Interactions&entry.906535625=Liang%20Xu%20and%20Chengqun%20Yang%20and%20Zili%20Lin%20and%20Fei%20Xu%20and%20Yifan%20Liu%20and%20Congsheng%20Xu%20and%20Yiyi%20Zhang%20and%20Jie%20Qin%20and%20Xingdong%20Sheng%20and%20Yunhui%20Liu%20and%20Xin%20Jin%20and%20Yichao%20Yan%20and%20Wenjun%20Zeng%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Learning%20action%20models%20from%20real-world%20human-centric%20interaction%20datasets%20is%0Aimportant%20towards%20building%20general-purpose%20intelligent%20assistants%20with%0Aefficiency.%20However%2C%20most%20existing%20datasets%20only%20offer%20specialist%20interaction%0Acategory%20and%20ignore%20that%20AI%20assistants%20perceive%20and%20act%20based%20on%20first-person%0Aacquisition.%20We%20urge%20that%20both%20the%20generalist%20interaction%20knowledge%20and%0Aegocentric%20modality%20are%20indispensable.%20In%20this%20paper%2C%20we%20embed%20the%0Amanual-assisted%20task%20into%20a%20vision-language-action%20framework%2C%20where%20the%0Aassistant%20provides%20services%20to%20the%20instructor%20following%20egocentric%20vision%20and%0Acommands.%20With%20our%20hybrid%20RGB-MoCap%20system%2C%20pairs%20of%20assistants%20and%20instructors%0Aengage%20with%20multiple%20objects%20and%20the%20scene%20following%20GPT-generated%20scripts.%0AUnder%20this%20setting%2C%20we%20accomplish%20InterVLA%2C%20the%20first%20large-scale%0Ahuman-object-human%20interaction%20dataset%20with%2011.4%20hours%20and%201.2M%20frames%20of%0Amultimodal%20data%2C%20spanning%202%20egocentric%20and%205%20exocentric%20videos%2C%20accurate%0Ahuman/object%20motions%20and%20verbal%20commands.%20Furthermore%2C%20we%20establish%20novel%0Abenchmarks%20on%20egocentric%20human%20motion%20estimation%2C%20interaction%20synthesis%2C%20and%0Ainteraction%20prediction%20with%20comprehensive%20analysis.%20We%20believe%20that%20our%0AInterVLA%20testbed%20and%20the%20benchmarks%20will%20foster%20future%20works%20on%20building%20AI%0Aagents%20in%20the%20physical%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04681v1&entry.124074799=Read"},
{"title": "Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for\n  Low-Light Image Enhancement", "author": "Daniel Torres and Joan Duran and Julia Navarro and Catalina Sbert", "abstract": "  Images captured under low-light conditions present significant limitations in\nmany applications, as poor lighting can obscure details, reduce contrast, and\nhide noise. Removing the illumination effects and enhancing the quality of such\nimages is crucial for many tasks, such as image segmentation and object\ndetection. In this paper, we propose a variational method for low-light image\nenhancement based on the Retinex decomposition into illumination, reflectance,\nand noise components. A color correction pre-processing step is applied to the\nlow-light image, which is then used as the observed input in the decomposition.\nMoreover, our model integrates a novel nonlocal gradient-type fidelity term\ndesigned to preserve structural details. Additionally, we propose an automatic\ngamma correction module. Building on the proposed variational approach, we\nextend the model by introducing its deep unfolding counterpart, in which the\nproximal operators are replaced with learnable networks. We propose\ncross-attention mechanisms to capture long-range dependencies in both the\nnonlocal prior of the reflectance and the nonlocal gradient-based constraint.\nExperimental results demonstrate that both methods compare favorably with\nseveral recent and state-of-the-art techniques across different datasets. In\nparticular, despite not relying on learning strategies, the variational model\noutperforms most deep learning approaches both visually and in terms of quality\nmetrics.\n", "link": "http://arxiv.org/abs/2504.07810v2", "date": "2025-08-06", "relevancy": 2.295, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5943}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5756}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonlocal%20Retinex-Based%20Variational%20Model%20and%20its%20Deep%20Unfolding%20Twin%20for%0A%20%20Low-Light%20Image%20Enhancement&body=Title%3A%20Nonlocal%20Retinex-Based%20Variational%20Model%20and%20its%20Deep%20Unfolding%20Twin%20for%0A%20%20Low-Light%20Image%20Enhancement%0AAuthor%3A%20Daniel%20Torres%20and%20Joan%20Duran%20and%20Julia%20Navarro%20and%20Catalina%20Sbert%0AAbstract%3A%20%20%20Images%20captured%20under%20low-light%20conditions%20present%20significant%20limitations%20in%0Amany%20applications%2C%20as%20poor%20lighting%20can%20obscure%20details%2C%20reduce%20contrast%2C%20and%0Ahide%20noise.%20Removing%20the%20illumination%20effects%20and%20enhancing%20the%20quality%20of%20such%0Aimages%20is%20crucial%20for%20many%20tasks%2C%20such%20as%20image%20segmentation%20and%20object%0Adetection.%20In%20this%20paper%2C%20we%20propose%20a%20variational%20method%20for%20low-light%20image%0Aenhancement%20based%20on%20the%20Retinex%20decomposition%20into%20illumination%2C%20reflectance%2C%0Aand%20noise%20components.%20A%20color%20correction%20pre-processing%20step%20is%20applied%20to%20the%0Alow-light%20image%2C%20which%20is%20then%20used%20as%20the%20observed%20input%20in%20the%20decomposition.%0AMoreover%2C%20our%20model%20integrates%20a%20novel%20nonlocal%20gradient-type%20fidelity%20term%0Adesigned%20to%20preserve%20structural%20details.%20Additionally%2C%20we%20propose%20an%20automatic%0Agamma%20correction%20module.%20Building%20on%20the%20proposed%20variational%20approach%2C%20we%0Aextend%20the%20model%20by%20introducing%20its%20deep%20unfolding%20counterpart%2C%20in%20which%20the%0Aproximal%20operators%20are%20replaced%20with%20learnable%20networks.%20We%20propose%0Across-attention%20mechanisms%20to%20capture%20long-range%20dependencies%20in%20both%20the%0Anonlocal%20prior%20of%20the%20reflectance%20and%20the%20nonlocal%20gradient-based%20constraint.%0AExperimental%20results%20demonstrate%20that%20both%20methods%20compare%20favorably%20with%0Aseveral%20recent%20and%20state-of-the-art%20techniques%20across%20different%20datasets.%20In%0Aparticular%2C%20despite%20not%20relying%20on%20learning%20strategies%2C%20the%20variational%20model%0Aoutperforms%20most%20deep%20learning%20approaches%20both%20visually%20and%20in%20terms%20of%20quality%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.07810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonlocal%2520Retinex-Based%2520Variational%2520Model%2520and%2520its%2520Deep%2520Unfolding%2520Twin%2520for%250A%2520%2520Low-Light%2520Image%2520Enhancement%26entry.906535625%3DDaniel%2520Torres%2520and%2520Joan%2520Duran%2520and%2520Julia%2520Navarro%2520and%2520Catalina%2520Sbert%26entry.1292438233%3D%2520%2520Images%2520captured%2520under%2520low-light%2520conditions%2520present%2520significant%2520limitations%2520in%250Amany%2520applications%252C%2520as%2520poor%2520lighting%2520can%2520obscure%2520details%252C%2520reduce%2520contrast%252C%2520and%250Ahide%2520noise.%2520Removing%2520the%2520illumination%2520effects%2520and%2520enhancing%2520the%2520quality%2520of%2520such%250Aimages%2520is%2520crucial%2520for%2520many%2520tasks%252C%2520such%2520as%2520image%2520segmentation%2520and%2520object%250Adetection.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520variational%2520method%2520for%2520low-light%2520image%250Aenhancement%2520based%2520on%2520the%2520Retinex%2520decomposition%2520into%2520illumination%252C%2520reflectance%252C%250Aand%2520noise%2520components.%2520A%2520color%2520correction%2520pre-processing%2520step%2520is%2520applied%2520to%2520the%250Alow-light%2520image%252C%2520which%2520is%2520then%2520used%2520as%2520the%2520observed%2520input%2520in%2520the%2520decomposition.%250AMoreover%252C%2520our%2520model%2520integrates%2520a%2520novel%2520nonlocal%2520gradient-type%2520fidelity%2520term%250Adesigned%2520to%2520preserve%2520structural%2520details.%2520Additionally%252C%2520we%2520propose%2520an%2520automatic%250Agamma%2520correction%2520module.%2520Building%2520on%2520the%2520proposed%2520variational%2520approach%252C%2520we%250Aextend%2520the%2520model%2520by%2520introducing%2520its%2520deep%2520unfolding%2520counterpart%252C%2520in%2520which%2520the%250Aproximal%2520operators%2520are%2520replaced%2520with%2520learnable%2520networks.%2520We%2520propose%250Across-attention%2520mechanisms%2520to%2520capture%2520long-range%2520dependencies%2520in%2520both%2520the%250Anonlocal%2520prior%2520of%2520the%2520reflectance%2520and%2520the%2520nonlocal%2520gradient-based%2520constraint.%250AExperimental%2520results%2520demonstrate%2520that%2520both%2520methods%2520compare%2520favorably%2520with%250Aseveral%2520recent%2520and%2520state-of-the-art%2520techniques%2520across%2520different%2520datasets.%2520In%250Aparticular%252C%2520despite%2520not%2520relying%2520on%2520learning%2520strategies%252C%2520the%2520variational%2520model%250Aoutperforms%2520most%2520deep%2520learning%2520approaches%2520both%2520visually%2520and%2520in%2520terms%2520of%2520quality%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonlocal%20Retinex-Based%20Variational%20Model%20and%20its%20Deep%20Unfolding%20Twin%20for%0A%20%20Low-Light%20Image%20Enhancement&entry.906535625=Daniel%20Torres%20and%20Joan%20Duran%20and%20Julia%20Navarro%20and%20Catalina%20Sbert&entry.1292438233=%20%20Images%20captured%20under%20low-light%20conditions%20present%20significant%20limitations%20in%0Amany%20applications%2C%20as%20poor%20lighting%20can%20obscure%20details%2C%20reduce%20contrast%2C%20and%0Ahide%20noise.%20Removing%20the%20illumination%20effects%20and%20enhancing%20the%20quality%20of%20such%0Aimages%20is%20crucial%20for%20many%20tasks%2C%20such%20as%20image%20segmentation%20and%20object%0Adetection.%20In%20this%20paper%2C%20we%20propose%20a%20variational%20method%20for%20low-light%20image%0Aenhancement%20based%20on%20the%20Retinex%20decomposition%20into%20illumination%2C%20reflectance%2C%0Aand%20noise%20components.%20A%20color%20correction%20pre-processing%20step%20is%20applied%20to%20the%0Alow-light%20image%2C%20which%20is%20then%20used%20as%20the%20observed%20input%20in%20the%20decomposition.%0AMoreover%2C%20our%20model%20integrates%20a%20novel%20nonlocal%20gradient-type%20fidelity%20term%0Adesigned%20to%20preserve%20structural%20details.%20Additionally%2C%20we%20propose%20an%20automatic%0Agamma%20correction%20module.%20Building%20on%20the%20proposed%20variational%20approach%2C%20we%0Aextend%20the%20model%20by%20introducing%20its%20deep%20unfolding%20counterpart%2C%20in%20which%20the%0Aproximal%20operators%20are%20replaced%20with%20learnable%20networks.%20We%20propose%0Across-attention%20mechanisms%20to%20capture%20long-range%20dependencies%20in%20both%20the%0Anonlocal%20prior%20of%20the%20reflectance%20and%20the%20nonlocal%20gradient-based%20constraint.%0AExperimental%20results%20demonstrate%20that%20both%20methods%20compare%20favorably%20with%0Aseveral%20recent%20and%20state-of-the-art%20techniques%20across%20different%20datasets.%20In%0Aparticular%2C%20despite%20not%20relying%20on%20learning%20strategies%2C%20the%20variational%20model%0Aoutperforms%20most%20deep%20learning%20approaches%20both%20visually%20and%20in%20terms%20of%20quality%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.07810v2&entry.124074799=Read"},
{"title": "DDTracking: A Deep Generative Framework for Diffusion MRI Tractography\n  with Streamline Local-Global Spatiotemporal Modeling", "author": "Yijie Li and Wei Zhang and Xi Zhu and Ye Wu and Yogesh Rathi and Lauren J. O'Donnell and Fan Zhang", "abstract": "  This paper presents DDTracking, a novel deep generative framework for\ndiffusion MRI tractography that formulates streamline propagation as a\nconditional denoising diffusion process. In DDTracking, we introduce a\ndual-pathway encoding network that jointly models local spatial encoding\n(capturing fine-scale structural details at each streamline point) and global\ntemporal dependencies (ensuring long-range consistency across the entire\nstreamline). Furthermore, we design a conditional diffusion model module, which\nleverages the learned local and global embeddings to predict streamline\npropagation orientations for tractography in an end-to-end trainable manner. We\nconduct a comprehensive evaluation across diverse, independently acquired dMRI\ndatasets, including both synthetic and clinical data. Experiments on two\nwell-established benchmarks with ground truth (ISMRM Challenge and\nTractoInferno) demonstrate that DDTracking largely outperforms current\nstate-of-the-art tractography methods. Furthermore, our results highlight\nDDTracking's strong generalizability across heterogeneous datasets, spanning\nvarying health conditions, age groups, imaging protocols, and scanner types.\nCollectively, DDTracking offers anatomically plausible and robust tractography,\npresenting a scalable, adaptable, and end-to-end learnable solution for broad\ndMRI applications. Code is available at:\nhttps://github.com/yishengpoxiao/DDtracking.git\n", "link": "http://arxiv.org/abs/2508.04568v1", "date": "2025-08-06", "relevancy": 2.2948, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5897}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5816}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DDTracking%3A%20A%20Deep%20Generative%20Framework%20for%20Diffusion%20MRI%20Tractography%0A%20%20with%20Streamline%20Local-Global%20Spatiotemporal%20Modeling&body=Title%3A%20DDTracking%3A%20A%20Deep%20Generative%20Framework%20for%20Diffusion%20MRI%20Tractography%0A%20%20with%20Streamline%20Local-Global%20Spatiotemporal%20Modeling%0AAuthor%3A%20Yijie%20Li%20and%20Wei%20Zhang%20and%20Xi%20Zhu%20and%20Ye%20Wu%20and%20Yogesh%20Rathi%20and%20Lauren%20J.%20O%27Donnell%20and%20Fan%20Zhang%0AAbstract%3A%20%20%20This%20paper%20presents%20DDTracking%2C%20a%20novel%20deep%20generative%20framework%20for%0Adiffusion%20MRI%20tractography%20that%20formulates%20streamline%20propagation%20as%20a%0Aconditional%20denoising%20diffusion%20process.%20In%20DDTracking%2C%20we%20introduce%20a%0Adual-pathway%20encoding%20network%20that%20jointly%20models%20local%20spatial%20encoding%0A%28capturing%20fine-scale%20structural%20details%20at%20each%20streamline%20point%29%20and%20global%0Atemporal%20dependencies%20%28ensuring%20long-range%20consistency%20across%20the%20entire%0Astreamline%29.%20Furthermore%2C%20we%20design%20a%20conditional%20diffusion%20model%20module%2C%20which%0Aleverages%20the%20learned%20local%20and%20global%20embeddings%20to%20predict%20streamline%0Apropagation%20orientations%20for%20tractography%20in%20an%20end-to-end%20trainable%20manner.%20We%0Aconduct%20a%20comprehensive%20evaluation%20across%20diverse%2C%20independently%20acquired%20dMRI%0Adatasets%2C%20including%20both%20synthetic%20and%20clinical%20data.%20Experiments%20on%20two%0Awell-established%20benchmarks%20with%20ground%20truth%20%28ISMRM%20Challenge%20and%0ATractoInferno%29%20demonstrate%20that%20DDTracking%20largely%20outperforms%20current%0Astate-of-the-art%20tractography%20methods.%20Furthermore%2C%20our%20results%20highlight%0ADDTracking%27s%20strong%20generalizability%20across%20heterogeneous%20datasets%2C%20spanning%0Avarying%20health%20conditions%2C%20age%20groups%2C%20imaging%20protocols%2C%20and%20scanner%20types.%0ACollectively%2C%20DDTracking%20offers%20anatomically%20plausible%20and%20robust%20tractography%2C%0Apresenting%20a%20scalable%2C%20adaptable%2C%20and%20end-to-end%20learnable%20solution%20for%20broad%0AdMRI%20applications.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/yishengpoxiao/DDtracking.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDDTracking%253A%2520A%2520Deep%2520Generative%2520Framework%2520for%2520Diffusion%2520MRI%2520Tractography%250A%2520%2520with%2520Streamline%2520Local-Global%2520Spatiotemporal%2520Modeling%26entry.906535625%3DYijie%2520Li%2520and%2520Wei%2520Zhang%2520and%2520Xi%2520Zhu%2520and%2520Ye%2520Wu%2520and%2520Yogesh%2520Rathi%2520and%2520Lauren%2520J.%2520O%2527Donnell%2520and%2520Fan%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DDTracking%252C%2520a%2520novel%2520deep%2520generative%2520framework%2520for%250Adiffusion%2520MRI%2520tractography%2520that%2520formulates%2520streamline%2520propagation%2520as%2520a%250Aconditional%2520denoising%2520diffusion%2520process.%2520In%2520DDTracking%252C%2520we%2520introduce%2520a%250Adual-pathway%2520encoding%2520network%2520that%2520jointly%2520models%2520local%2520spatial%2520encoding%250A%2528capturing%2520fine-scale%2520structural%2520details%2520at%2520each%2520streamline%2520point%2529%2520and%2520global%250Atemporal%2520dependencies%2520%2528ensuring%2520long-range%2520consistency%2520across%2520the%2520entire%250Astreamline%2529.%2520Furthermore%252C%2520we%2520design%2520a%2520conditional%2520diffusion%2520model%2520module%252C%2520which%250Aleverages%2520the%2520learned%2520local%2520and%2520global%2520embeddings%2520to%2520predict%2520streamline%250Apropagation%2520orientations%2520for%2520tractography%2520in%2520an%2520end-to-end%2520trainable%2520manner.%2520We%250Aconduct%2520a%2520comprehensive%2520evaluation%2520across%2520diverse%252C%2520independently%2520acquired%2520dMRI%250Adatasets%252C%2520including%2520both%2520synthetic%2520and%2520clinical%2520data.%2520Experiments%2520on%2520two%250Awell-established%2520benchmarks%2520with%2520ground%2520truth%2520%2528ISMRM%2520Challenge%2520and%250ATractoInferno%2529%2520demonstrate%2520that%2520DDTracking%2520largely%2520outperforms%2520current%250Astate-of-the-art%2520tractography%2520methods.%2520Furthermore%252C%2520our%2520results%2520highlight%250ADDTracking%2527s%2520strong%2520generalizability%2520across%2520heterogeneous%2520datasets%252C%2520spanning%250Avarying%2520health%2520conditions%252C%2520age%2520groups%252C%2520imaging%2520protocols%252C%2520and%2520scanner%2520types.%250ACollectively%252C%2520DDTracking%2520offers%2520anatomically%2520plausible%2520and%2520robust%2520tractography%252C%250Apresenting%2520a%2520scalable%252C%2520adaptable%252C%2520and%2520end-to-end%2520learnable%2520solution%2520for%2520broad%250AdMRI%2520applications.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/yishengpoxiao/DDtracking.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DDTracking%3A%20A%20Deep%20Generative%20Framework%20for%20Diffusion%20MRI%20Tractography%0A%20%20with%20Streamline%20Local-Global%20Spatiotemporal%20Modeling&entry.906535625=Yijie%20Li%20and%20Wei%20Zhang%20and%20Xi%20Zhu%20and%20Ye%20Wu%20and%20Yogesh%20Rathi%20and%20Lauren%20J.%20O%27Donnell%20and%20Fan%20Zhang&entry.1292438233=%20%20This%20paper%20presents%20DDTracking%2C%20a%20novel%20deep%20generative%20framework%20for%0Adiffusion%20MRI%20tractography%20that%20formulates%20streamline%20propagation%20as%20a%0Aconditional%20denoising%20diffusion%20process.%20In%20DDTracking%2C%20we%20introduce%20a%0Adual-pathway%20encoding%20network%20that%20jointly%20models%20local%20spatial%20encoding%0A%28capturing%20fine-scale%20structural%20details%20at%20each%20streamline%20point%29%20and%20global%0Atemporal%20dependencies%20%28ensuring%20long-range%20consistency%20across%20the%20entire%0Astreamline%29.%20Furthermore%2C%20we%20design%20a%20conditional%20diffusion%20model%20module%2C%20which%0Aleverages%20the%20learned%20local%20and%20global%20embeddings%20to%20predict%20streamline%0Apropagation%20orientations%20for%20tractography%20in%20an%20end-to-end%20trainable%20manner.%20We%0Aconduct%20a%20comprehensive%20evaluation%20across%20diverse%2C%20independently%20acquired%20dMRI%0Adatasets%2C%20including%20both%20synthetic%20and%20clinical%20data.%20Experiments%20on%20two%0Awell-established%20benchmarks%20with%20ground%20truth%20%28ISMRM%20Challenge%20and%0ATractoInferno%29%20demonstrate%20that%20DDTracking%20largely%20outperforms%20current%0Astate-of-the-art%20tractography%20methods.%20Furthermore%2C%20our%20results%20highlight%0ADDTracking%27s%20strong%20generalizability%20across%20heterogeneous%20datasets%2C%20spanning%0Avarying%20health%20conditions%2C%20age%20groups%2C%20imaging%20protocols%2C%20and%20scanner%20types.%0ACollectively%2C%20DDTracking%20offers%20anatomically%20plausible%20and%20robust%20tractography%2C%0Apresenting%20a%20scalable%2C%20adaptable%2C%20and%20end-to-end%20learnable%20solution%20for%20broad%0AdMRI%20applications.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/yishengpoxiao/DDtracking.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04568v1&entry.124074799=Read"},
{"title": "BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning", "author": "Ziyang Leng and Jiawei Yang and Zhicheng Ren and Bolei Zhou", "abstract": "  We present BEVCon, a simple yet effective contrastive learning framework\ndesigned to improve Bird's Eye View (BEV) perception in autonomous driving. BEV\nperception offers a top-down-view representation of the surrounding\nenvironment, making it crucial for 3D object detection, segmentation, and\ntrajectory prediction tasks. While prior work has primarily focused on\nenhancing BEV encoders and task-specific heads, we address the underexplored\npotential of representation learning in BEV models. BEVCon introduces two\ncontrastive learning modules: an instance feature contrast module for refining\nBEV features and a perspective view contrast module that enhances the image\nbackbone. The dense contrastive learning designed on top of detection losses\nleads to improved feature representations across both the BEV encoder and the\nbackbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon\nachieves consistent performance gains, achieving up to +2.4% mAP improvement\nover state-of-the-art baselines. Our results highlight the critical role of\nrepresentation learning in BEV perception and offer a complementary avenue to\nconventional task-specific optimizations.\n", "link": "http://arxiv.org/abs/2508.04702v1", "date": "2025-08-06", "relevancy": 2.2945, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEVCon%3A%20Advancing%20Bird%27s%20Eye%20View%20Perception%20with%20Contrastive%20Learning&body=Title%3A%20BEVCon%3A%20Advancing%20Bird%27s%20Eye%20View%20Perception%20with%20Contrastive%20Learning%0AAuthor%3A%20Ziyang%20Leng%20and%20Jiawei%20Yang%20and%20Zhicheng%20Ren%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20We%20present%20BEVCon%2C%20a%20simple%20yet%20effective%20contrastive%20learning%20framework%0Adesigned%20to%20improve%20Bird%27s%20Eye%20View%20%28BEV%29%20perception%20in%20autonomous%20driving.%20BEV%0Aperception%20offers%20a%20top-down-view%20representation%20of%20the%20surrounding%0Aenvironment%2C%20making%20it%20crucial%20for%203D%20object%20detection%2C%20segmentation%2C%20and%0Atrajectory%20prediction%20tasks.%20While%20prior%20work%20has%20primarily%20focused%20on%0Aenhancing%20BEV%20encoders%20and%20task-specific%20heads%2C%20we%20address%20the%20underexplored%0Apotential%20of%20representation%20learning%20in%20BEV%20models.%20BEVCon%20introduces%20two%0Acontrastive%20learning%20modules%3A%20an%20instance%20feature%20contrast%20module%20for%20refining%0ABEV%20features%20and%20a%20perspective%20view%20contrast%20module%20that%20enhances%20the%20image%0Abackbone.%20The%20dense%20contrastive%20learning%20designed%20on%20top%20of%20detection%20losses%0Aleads%20to%20improved%20feature%20representations%20across%20both%20the%20BEV%20encoder%20and%20the%0Abackbone.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20BEVCon%0Aachieves%20consistent%20performance%20gains%2C%20achieving%20up%20to%20%2B2.4%25%20mAP%20improvement%0Aover%20state-of-the-art%20baselines.%20Our%20results%20highlight%20the%20critical%20role%20of%0Arepresentation%20learning%20in%20BEV%20perception%20and%20offer%20a%20complementary%20avenue%20to%0Aconventional%20task-specific%20optimizations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEVCon%253A%2520Advancing%2520Bird%2527s%2520Eye%2520View%2520Perception%2520with%2520Contrastive%2520Learning%26entry.906535625%3DZiyang%2520Leng%2520and%2520Jiawei%2520Yang%2520and%2520Zhicheng%2520Ren%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520BEVCon%252C%2520a%2520simple%2520yet%2520effective%2520contrastive%2520learning%2520framework%250Adesigned%2520to%2520improve%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520perception%2520in%2520autonomous%2520driving.%2520BEV%250Aperception%2520offers%2520a%2520top-down-view%2520representation%2520of%2520the%2520surrounding%250Aenvironment%252C%2520making%2520it%2520crucial%2520for%25203D%2520object%2520detection%252C%2520segmentation%252C%2520and%250Atrajectory%2520prediction%2520tasks.%2520While%2520prior%2520work%2520has%2520primarily%2520focused%2520on%250Aenhancing%2520BEV%2520encoders%2520and%2520task-specific%2520heads%252C%2520we%2520address%2520the%2520underexplored%250Apotential%2520of%2520representation%2520learning%2520in%2520BEV%2520models.%2520BEVCon%2520introduces%2520two%250Acontrastive%2520learning%2520modules%253A%2520an%2520instance%2520feature%2520contrast%2520module%2520for%2520refining%250ABEV%2520features%2520and%2520a%2520perspective%2520view%2520contrast%2520module%2520that%2520enhances%2520the%2520image%250Abackbone.%2520The%2520dense%2520contrastive%2520learning%2520designed%2520on%2520top%2520of%2520detection%2520losses%250Aleads%2520to%2520improved%2520feature%2520representations%2520across%2520both%2520the%2520BEV%2520encoder%2520and%2520the%250Abackbone.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520demonstrate%2520that%2520BEVCon%250Aachieves%2520consistent%2520performance%2520gains%252C%2520achieving%2520up%2520to%2520%252B2.4%2525%2520mAP%2520improvement%250Aover%2520state-of-the-art%2520baselines.%2520Our%2520results%2520highlight%2520the%2520critical%2520role%2520of%250Arepresentation%2520learning%2520in%2520BEV%2520perception%2520and%2520offer%2520a%2520complementary%2520avenue%2520to%250Aconventional%2520task-specific%2520optimizations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEVCon%3A%20Advancing%20Bird%27s%20Eye%20View%20Perception%20with%20Contrastive%20Learning&entry.906535625=Ziyang%20Leng%20and%20Jiawei%20Yang%20and%20Zhicheng%20Ren%20and%20Bolei%20Zhou&entry.1292438233=%20%20We%20present%20BEVCon%2C%20a%20simple%20yet%20effective%20contrastive%20learning%20framework%0Adesigned%20to%20improve%20Bird%27s%20Eye%20View%20%28BEV%29%20perception%20in%20autonomous%20driving.%20BEV%0Aperception%20offers%20a%20top-down-view%20representation%20of%20the%20surrounding%0Aenvironment%2C%20making%20it%20crucial%20for%203D%20object%20detection%2C%20segmentation%2C%20and%0Atrajectory%20prediction%20tasks.%20While%20prior%20work%20has%20primarily%20focused%20on%0Aenhancing%20BEV%20encoders%20and%20task-specific%20heads%2C%20we%20address%20the%20underexplored%0Apotential%20of%20representation%20learning%20in%20BEV%20models.%20BEVCon%20introduces%20two%0Acontrastive%20learning%20modules%3A%20an%20instance%20feature%20contrast%20module%20for%20refining%0ABEV%20features%20and%20a%20perspective%20view%20contrast%20module%20that%20enhances%20the%20image%0Abackbone.%20The%20dense%20contrastive%20learning%20designed%20on%20top%20of%20detection%20losses%0Aleads%20to%20improved%20feature%20representations%20across%20both%20the%20BEV%20encoder%20and%20the%0Abackbone.%20Extensive%20experiments%20on%20the%20nuScenes%20dataset%20demonstrate%20that%20BEVCon%0Aachieves%20consistent%20performance%20gains%2C%20achieving%20up%20to%20%2B2.4%25%20mAP%20improvement%0Aover%20state-of-the-art%20baselines.%20Our%20results%20highlight%20the%20critical%20role%20of%0Arepresentation%20learning%20in%20BEV%20perception%20and%20offer%20a%20complementary%20avenue%20to%0Aconventional%20task-specific%20optimizations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04702v1&entry.124074799=Read"},
{"title": "CauKer: classification time series foundation models can be pretrained\n  on synthetic data only", "author": "Shifeng Xie and Vasilii Feofanov and Marius Alonso and Ambroise Odonnat and Jianfeng Zhang and Themis Palpanas and Ievgen Redko", "abstract": "  Time series foundation models (TSFMs) have recently gained significant\nattention due to their strong zero-shot capabilities and widespread real-world\napplications. Such models typically require a computationally costly\npretraining on large-scale, carefully curated collections of real-world\nsequences. To allow for a sample-efficient pretraining of TSFMs, we propose\nCauKer, a novel algorithm designed to generate diverse, causally coherent\nsynthetic time series with realistic trends, seasonality, and nonlinear\ninteractions. CauKer combines Gaussian Process (GP) kernel composition with\nStructural Causal Models (SCM) to produce data for sample-efficient pretraining\nof state-of-the-art classification TSFMs having different architectures and\nfollowing different pretraining approaches. Additionally, our experiments\nreveal that CauKer-generated datasets exhibit clear scaling laws for both\ndataset size (10K to 10M samples) and model capacity (1M to 783M parameters),\nunlike real-world datasets, which display irregular scaling behavior.\n", "link": "http://arxiv.org/abs/2508.02879v2", "date": "2025-08-06", "relevancy": 2.2944, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4671}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CauKer%3A%20classification%20time%20series%20foundation%20models%20can%20be%20pretrained%0A%20%20on%20synthetic%20data%20only&body=Title%3A%20CauKer%3A%20classification%20time%20series%20foundation%20models%20can%20be%20pretrained%0A%20%20on%20synthetic%20data%20only%0AAuthor%3A%20Shifeng%20Xie%20and%20Vasilii%20Feofanov%20and%20Marius%20Alonso%20and%20Ambroise%20Odonnat%20and%20Jianfeng%20Zhang%20and%20Themis%20Palpanas%20and%20Ievgen%20Redko%0AAbstract%3A%20%20%20Time%20series%20foundation%20models%20%28TSFMs%29%20have%20recently%20gained%20significant%0Aattention%20due%20to%20their%20strong%20zero-shot%20capabilities%20and%20widespread%20real-world%0Aapplications.%20Such%20models%20typically%20require%20a%20computationally%20costly%0Apretraining%20on%20large-scale%2C%20carefully%20curated%20collections%20of%20real-world%0Asequences.%20To%20allow%20for%20a%20sample-efficient%20pretraining%20of%20TSFMs%2C%20we%20propose%0ACauKer%2C%20a%20novel%20algorithm%20designed%20to%20generate%20diverse%2C%20causally%20coherent%0Asynthetic%20time%20series%20with%20realistic%20trends%2C%20seasonality%2C%20and%20nonlinear%0Ainteractions.%20CauKer%20combines%20Gaussian%20Process%20%28GP%29%20kernel%20composition%20with%0AStructural%20Causal%20Models%20%28SCM%29%20to%20produce%20data%20for%20sample-efficient%20pretraining%0Aof%20state-of-the-art%20classification%20TSFMs%20having%20different%20architectures%20and%0Afollowing%20different%20pretraining%20approaches.%20Additionally%2C%20our%20experiments%0Areveal%20that%20CauKer-generated%20datasets%20exhibit%20clear%20scaling%20laws%20for%20both%0Adataset%20size%20%2810K%20to%2010M%20samples%29%20and%20model%20capacity%20%281M%20to%20783M%20parameters%29%2C%0Aunlike%20real-world%20datasets%2C%20which%20display%20irregular%20scaling%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.02879v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCauKer%253A%2520classification%2520time%2520series%2520foundation%2520models%2520can%2520be%2520pretrained%250A%2520%2520on%2520synthetic%2520data%2520only%26entry.906535625%3DShifeng%2520Xie%2520and%2520Vasilii%2520Feofanov%2520and%2520Marius%2520Alonso%2520and%2520Ambroise%2520Odonnat%2520and%2520Jianfeng%2520Zhang%2520and%2520Themis%2520Palpanas%2520and%2520Ievgen%2520Redko%26entry.1292438233%3D%2520%2520Time%2520series%2520foundation%2520models%2520%2528TSFMs%2529%2520have%2520recently%2520gained%2520significant%250Aattention%2520due%2520to%2520their%2520strong%2520zero-shot%2520capabilities%2520and%2520widespread%2520real-world%250Aapplications.%2520Such%2520models%2520typically%2520require%2520a%2520computationally%2520costly%250Apretraining%2520on%2520large-scale%252C%2520carefully%2520curated%2520collections%2520of%2520real-world%250Asequences.%2520To%2520allow%2520for%2520a%2520sample-efficient%2520pretraining%2520of%2520TSFMs%252C%2520we%2520propose%250ACauKer%252C%2520a%2520novel%2520algorithm%2520designed%2520to%2520generate%2520diverse%252C%2520causally%2520coherent%250Asynthetic%2520time%2520series%2520with%2520realistic%2520trends%252C%2520seasonality%252C%2520and%2520nonlinear%250Ainteractions.%2520CauKer%2520combines%2520Gaussian%2520Process%2520%2528GP%2529%2520kernel%2520composition%2520with%250AStructural%2520Causal%2520Models%2520%2528SCM%2529%2520to%2520produce%2520data%2520for%2520sample-efficient%2520pretraining%250Aof%2520state-of-the-art%2520classification%2520TSFMs%2520having%2520different%2520architectures%2520and%250Afollowing%2520different%2520pretraining%2520approaches.%2520Additionally%252C%2520our%2520experiments%250Areveal%2520that%2520CauKer-generated%2520datasets%2520exhibit%2520clear%2520scaling%2520laws%2520for%2520both%250Adataset%2520size%2520%252810K%2520to%252010M%2520samples%2529%2520and%2520model%2520capacity%2520%25281M%2520to%2520783M%2520parameters%2529%252C%250Aunlike%2520real-world%2520datasets%252C%2520which%2520display%2520irregular%2520scaling%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02879v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CauKer%3A%20classification%20time%20series%20foundation%20models%20can%20be%20pretrained%0A%20%20on%20synthetic%20data%20only&entry.906535625=Shifeng%20Xie%20and%20Vasilii%20Feofanov%20and%20Marius%20Alonso%20and%20Ambroise%20Odonnat%20and%20Jianfeng%20Zhang%20and%20Themis%20Palpanas%20and%20Ievgen%20Redko&entry.1292438233=%20%20Time%20series%20foundation%20models%20%28TSFMs%29%20have%20recently%20gained%20significant%0Aattention%20due%20to%20their%20strong%20zero-shot%20capabilities%20and%20widespread%20real-world%0Aapplications.%20Such%20models%20typically%20require%20a%20computationally%20costly%0Apretraining%20on%20large-scale%2C%20carefully%20curated%20collections%20of%20real-world%0Asequences.%20To%20allow%20for%20a%20sample-efficient%20pretraining%20of%20TSFMs%2C%20we%20propose%0ACauKer%2C%20a%20novel%20algorithm%20designed%20to%20generate%20diverse%2C%20causally%20coherent%0Asynthetic%20time%20series%20with%20realistic%20trends%2C%20seasonality%2C%20and%20nonlinear%0Ainteractions.%20CauKer%20combines%20Gaussian%20Process%20%28GP%29%20kernel%20composition%20with%0AStructural%20Causal%20Models%20%28SCM%29%20to%20produce%20data%20for%20sample-efficient%20pretraining%0Aof%20state-of-the-art%20classification%20TSFMs%20having%20different%20architectures%20and%0Afollowing%20different%20pretraining%20approaches.%20Additionally%2C%20our%20experiments%0Areveal%20that%20CauKer-generated%20datasets%20exhibit%20clear%20scaling%20laws%20for%20both%0Adataset%20size%20%2810K%20to%2010M%20samples%29%20and%20model%20capacity%20%281M%20to%20783M%20parameters%29%2C%0Aunlike%20real-world%20datasets%2C%20which%20display%20irregular%20scaling%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.02879v2&entry.124074799=Read"},
{"title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience", "author": "Zeyi Sun and Ziyu Liu and Yuhang Zang and Yuhang Cao and Xiaoyi Dong and Tong Wu and Dahua Lin and Jiaqi Wang", "abstract": "  Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.\n", "link": "http://arxiv.org/abs/2508.04700v1", "date": "2025-08-06", "relevancy": 2.2909, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6042}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5687}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SEAgent%3A%20Self-Evolving%20Computer%20Use%20Agent%20with%20Autonomous%20Learning%20from%0A%20%20Experience&body=Title%3A%20SEAgent%3A%20Self-Evolving%20Computer%20Use%20Agent%20with%20Autonomous%20Learning%20from%0A%20%20Experience%0AAuthor%3A%20Zeyi%20Sun%20and%20Ziyu%20Liu%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Xiaoyi%20Dong%20and%20Tong%20Wu%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Repurposing%20large%20vision-language%20models%20%28LVLMs%29%20as%20computer%20use%20agents%0A%28CUAs%29%20has%20led%20to%20substantial%20breakthroughs%2C%20primarily%20driven%20by%20human-labeled%0Adata.%20However%2C%20these%20models%20often%20struggle%20with%20novel%20and%20specialized%20software%2C%0Aparticularly%20in%20scenarios%20lacking%20human%20annotations.%20To%20address%20this%20challenge%2C%0Awe%20propose%20SEAgent%2C%20an%20agentic%20self-evolving%20framework%20enabling%20CUAs%20to%0Aautonomously%20evolve%20through%20interactions%20with%20unfamiliar%20software.%0ASpecifically%2C%20SEAgent%20empowers%20computer-use%20agents%20to%20autonomously%20master%20novel%0Asoftware%20environments%20via%20experiential%20learning%2C%20where%20agents%20explore%20new%0Asoftware%2C%20learn%20through%20iterative%20trial-and-error%2C%20and%20progressively%20tackle%0Aauto-generated%20tasks%20organized%20from%20simple%20to%20complex.%20To%20achieve%20this%20goal%2C%20we%0Adesign%20a%20World%20State%20Model%20for%20step-wise%20trajectory%20assessment%2C%20along%20with%20a%0ACurriculum%20Generator%20that%20generates%20increasingly%20diverse%20and%20challenging%20tasks.%0AThe%20agent%27s%20policy%20is%20updated%20through%20experiential%20learning%2C%20comprised%20of%0Aadversarial%20imitation%20of%20failure%20actions%20and%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20on%20successful%20ones.%20Furthermore%2C%20we%20introduce%20a%20specialist-to-generalist%0Atraining%20strategy%20that%20integrates%20individual%20experiential%20insights%20from%0Aspecialist%20agents%2C%20facilitating%20the%20development%20of%20a%20stronger%20generalist%20CUA%0Acapable%20of%20continuous%20autonomous%20evolution.%20This%20unified%20agent%20ultimately%0Aachieves%20performance%20surpassing%20ensembles%20of%20individual%20specialist%20agents%20on%0Atheir%20specialized%20software.%20We%20validate%20the%20effectiveness%20of%20SEAgent%20across%0Afive%20novel%20software%20environments%20within%20OS-World.%20Our%20approach%20achieves%20a%0Asignificant%20improvement%20of%2023.2%25%20in%20success%20rate%2C%20from%2011.3%25%20to%2034.5%25%2C%20over%20a%0Acompetitive%20open-source%20CUA%2C%20i.e.%2C%20UI-TARS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSEAgent%253A%2520Self-Evolving%2520Computer%2520Use%2520Agent%2520with%2520Autonomous%2520Learning%2520from%250A%2520%2520Experience%26entry.906535625%3DZeyi%2520Sun%2520and%2520Ziyu%2520Liu%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Xiaoyi%2520Dong%2520and%2520Tong%2520Wu%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Repurposing%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520as%2520computer%2520use%2520agents%250A%2528CUAs%2529%2520has%2520led%2520to%2520substantial%2520breakthroughs%252C%2520primarily%2520driven%2520by%2520human-labeled%250Adata.%2520However%252C%2520these%2520models%2520often%2520struggle%2520with%2520novel%2520and%2520specialized%2520software%252C%250Aparticularly%2520in%2520scenarios%2520lacking%2520human%2520annotations.%2520To%2520address%2520this%2520challenge%252C%250Awe%2520propose%2520SEAgent%252C%2520an%2520agentic%2520self-evolving%2520framework%2520enabling%2520CUAs%2520to%250Aautonomously%2520evolve%2520through%2520interactions%2520with%2520unfamiliar%2520software.%250ASpecifically%252C%2520SEAgent%2520empowers%2520computer-use%2520agents%2520to%2520autonomously%2520master%2520novel%250Asoftware%2520environments%2520via%2520experiential%2520learning%252C%2520where%2520agents%2520explore%2520new%250Asoftware%252C%2520learn%2520through%2520iterative%2520trial-and-error%252C%2520and%2520progressively%2520tackle%250Aauto-generated%2520tasks%2520organized%2520from%2520simple%2520to%2520complex.%2520To%2520achieve%2520this%2520goal%252C%2520we%250Adesign%2520a%2520World%2520State%2520Model%2520for%2520step-wise%2520trajectory%2520assessment%252C%2520along%2520with%2520a%250ACurriculum%2520Generator%2520that%2520generates%2520increasingly%2520diverse%2520and%2520challenging%2520tasks.%250AThe%2520agent%2527s%2520policy%2520is%2520updated%2520through%2520experiential%2520learning%252C%2520comprised%2520of%250Aadversarial%2520imitation%2520of%2520failure%2520actions%2520and%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%2520on%2520successful%2520ones.%2520Furthermore%252C%2520we%2520introduce%2520a%2520specialist-to-generalist%250Atraining%2520strategy%2520that%2520integrates%2520individual%2520experiential%2520insights%2520from%250Aspecialist%2520agents%252C%2520facilitating%2520the%2520development%2520of%2520a%2520stronger%2520generalist%2520CUA%250Acapable%2520of%2520continuous%2520autonomous%2520evolution.%2520This%2520unified%2520agent%2520ultimately%250Aachieves%2520performance%2520surpassing%2520ensembles%2520of%2520individual%2520specialist%2520agents%2520on%250Atheir%2520specialized%2520software.%2520We%2520validate%2520the%2520effectiveness%2520of%2520SEAgent%2520across%250Afive%2520novel%2520software%2520environments%2520within%2520OS-World.%2520Our%2520approach%2520achieves%2520a%250Asignificant%2520improvement%2520of%252023.2%2525%2520in%2520success%2520rate%252C%2520from%252011.3%2525%2520to%252034.5%2525%252C%2520over%2520a%250Acompetitive%2520open-source%2520CUA%252C%2520i.e.%252C%2520UI-TARS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SEAgent%3A%20Self-Evolving%20Computer%20Use%20Agent%20with%20Autonomous%20Learning%20from%0A%20%20Experience&entry.906535625=Zeyi%20Sun%20and%20Ziyu%20Liu%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Xiaoyi%20Dong%20and%20Tong%20Wu%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Repurposing%20large%20vision-language%20models%20%28LVLMs%29%20as%20computer%20use%20agents%0A%28CUAs%29%20has%20led%20to%20substantial%20breakthroughs%2C%20primarily%20driven%20by%20human-labeled%0Adata.%20However%2C%20these%20models%20often%20struggle%20with%20novel%20and%20specialized%20software%2C%0Aparticularly%20in%20scenarios%20lacking%20human%20annotations.%20To%20address%20this%20challenge%2C%0Awe%20propose%20SEAgent%2C%20an%20agentic%20self-evolving%20framework%20enabling%20CUAs%20to%0Aautonomously%20evolve%20through%20interactions%20with%20unfamiliar%20software.%0ASpecifically%2C%20SEAgent%20empowers%20computer-use%20agents%20to%20autonomously%20master%20novel%0Asoftware%20environments%20via%20experiential%20learning%2C%20where%20agents%20explore%20new%0Asoftware%2C%20learn%20through%20iterative%20trial-and-error%2C%20and%20progressively%20tackle%0Aauto-generated%20tasks%20organized%20from%20simple%20to%20complex.%20To%20achieve%20this%20goal%2C%20we%0Adesign%20a%20World%20State%20Model%20for%20step-wise%20trajectory%20assessment%2C%20along%20with%20a%0ACurriculum%20Generator%20that%20generates%20increasingly%20diverse%20and%20challenging%20tasks.%0AThe%20agent%27s%20policy%20is%20updated%20through%20experiential%20learning%2C%20comprised%20of%0Aadversarial%20imitation%20of%20failure%20actions%20and%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%20on%20successful%20ones.%20Furthermore%2C%20we%20introduce%20a%20specialist-to-generalist%0Atraining%20strategy%20that%20integrates%20individual%20experiential%20insights%20from%0Aspecialist%20agents%2C%20facilitating%20the%20development%20of%20a%20stronger%20generalist%20CUA%0Acapable%20of%20continuous%20autonomous%20evolution.%20This%20unified%20agent%20ultimately%0Aachieves%20performance%20surpassing%20ensembles%20of%20individual%20specialist%20agents%20on%0Atheir%20specialized%20software.%20We%20validate%20the%20effectiveness%20of%20SEAgent%20across%0Afive%20novel%20software%20environments%20within%20OS-World.%20Our%20approach%20achieves%20a%0Asignificant%20improvement%20of%2023.2%25%20in%20success%20rate%2C%20from%2011.3%25%20to%2034.5%25%2C%20over%20a%0Acompetitive%20open-source%20CUA%2C%20i.e.%2C%20UI-TARS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04700v1&entry.124074799=Read"},
{"title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for\n  Malaysian Secondary Mathematics Using Generative AI", "author": "Rohaizah Abdul Wahid and Muhamad Said Nizamuddin Nadim and Suliana Sulaiman and Syahmi Akmal Shaharudin and Muhammad Danial Jupikil and Iqqwan Jasman Su Azlan Su", "abstract": "  This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions.\n", "link": "http://arxiv.org/abs/2508.04442v1", "date": "2025-08-06", "relevancy": 2.2895, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4687}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4566}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Generation%20of%20Curriculum-Aligned%20Multiple-Choice%20Questions%20for%0A%20%20Malaysian%20Secondary%20Mathematics%20Using%20Generative%20AI&body=Title%3A%20Automated%20Generation%20of%20Curriculum-Aligned%20Multiple-Choice%20Questions%20for%0A%20%20Malaysian%20Secondary%20Mathematics%20Using%20Generative%20AI%0AAuthor%3A%20Rohaizah%20Abdul%20Wahid%20and%20Muhamad%20Said%20Nizamuddin%20Nadim%20and%20Suliana%20Sulaiman%20and%20Syahmi%20Akmal%20Shaharudin%20and%20Muhammad%20Danial%20Jupikil%20and%20Iqqwan%20Jasman%20Su%20Azlan%20Su%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20critical%20need%20for%20scalable%20and%20high-quality%0Aeducational%20assessment%20tools%20within%20the%20Malaysian%20education%20system.%20It%0Ahighlights%20the%20potential%20of%20Generative%20AI%20%28GenAI%29%20while%20acknowledging%20the%0Asignificant%20challenges%20of%20ensuring%20factual%20accuracy%20and%20curriculum%20alignment%2C%0Aespecially%20for%20low-resource%20languages%20like%20Bahasa%20Melayu.%20This%20research%0Aintroduces%20and%20compares%20four%20incremental%20pipelines%20for%20generating%20Form%201%0AMathematics%20multiple-choice%20questions%20%28MCQs%29%20in%20Bahasa%20Melayu%20using%20OpenAI%27s%0AGPT-4o.%20The%20methods%20range%20from%20non-grounded%20prompting%20%28structured%20and%20basic%29%20to%0ARetrieval-Augmented%20Generation%20%28RAG%29%20approaches%20%28one%20using%20the%20LangChain%0Aframework%2C%20one%20implemented%20manually%29.%20The%20system%20is%20grounded%20in%20official%0Acurriculum%20documents%2C%20including%20teacher-prepared%20notes%20and%20the%20yearly%20teaching%0Aplan%20%28RPT%29.%20A%20dual-pronged%20automated%20evaluation%20framework%20is%20employed%20to%20assess%0Athe%20generated%20questions.%20Curriculum%20alignment%20is%20measured%20using%20Semantic%0ATextual%20Similarity%20%28STS%29%20against%20the%20RPT%2C%20while%20contextual%20validity%20is%20verified%0Athrough%20a%20novel%20RAG-based%20Question-Answering%20%28RAG-QA%29%20method.%20The%20results%0Ademonstrate%20that%20RAG-based%20pipelines%20significantly%20outperform%20non-grounded%0Aprompting%20methods%2C%20producing%20questions%20with%20higher%20curriculum%20alignment%20and%0Afactual%20validity.%20The%20study%20further%20analyzes%20the%20trade-offs%20between%20the%20ease%20of%0Aimplementation%20of%20framework-based%20RAG%20and%20the%20fine-grained%20control%20offered%20by%20a%0Amanual%20pipeline.%20This%20work%20presents%20a%20validated%20methodology%20for%20generating%0Acurriculum-specific%20educational%20content%20in%20a%20low-resource%20language%2C%20introduces%0Aa%20symbiotic%20RAG-QA%20evaluation%20technique%2C%20and%20provides%20actionable%20insights%20for%0Athe%20development%20and%20deployment%20of%20practical%20EdTech%20solutions%20in%20Malaysia%20and%0Asimilar%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Generation%2520of%2520Curriculum-Aligned%2520Multiple-Choice%2520Questions%2520for%250A%2520%2520Malaysian%2520Secondary%2520Mathematics%2520Using%2520Generative%2520AI%26entry.906535625%3DRohaizah%2520Abdul%2520Wahid%2520and%2520Muhamad%2520Said%2520Nizamuddin%2520Nadim%2520and%2520Suliana%2520Sulaiman%2520and%2520Syahmi%2520Akmal%2520Shaharudin%2520and%2520Muhammad%2520Danial%2520Jupikil%2520and%2520Iqqwan%2520Jasman%2520Su%2520Azlan%2520Su%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520critical%2520need%2520for%2520scalable%2520and%2520high-quality%250Aeducational%2520assessment%2520tools%2520within%2520the%2520Malaysian%2520education%2520system.%2520It%250Ahighlights%2520the%2520potential%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520while%2520acknowledging%2520the%250Asignificant%2520challenges%2520of%2520ensuring%2520factual%2520accuracy%2520and%2520curriculum%2520alignment%252C%250Aespecially%2520for%2520low-resource%2520languages%2520like%2520Bahasa%2520Melayu.%2520This%2520research%250Aintroduces%2520and%2520compares%2520four%2520incremental%2520pipelines%2520for%2520generating%2520Form%25201%250AMathematics%2520multiple-choice%2520questions%2520%2528MCQs%2529%2520in%2520Bahasa%2520Melayu%2520using%2520OpenAI%2527s%250AGPT-4o.%2520The%2520methods%2520range%2520from%2520non-grounded%2520prompting%2520%2528structured%2520and%2520basic%2529%2520to%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520approaches%2520%2528one%2520using%2520the%2520LangChain%250Aframework%252C%2520one%2520implemented%2520manually%2529.%2520The%2520system%2520is%2520grounded%2520in%2520official%250Acurriculum%2520documents%252C%2520including%2520teacher-prepared%2520notes%2520and%2520the%2520yearly%2520teaching%250Aplan%2520%2528RPT%2529.%2520A%2520dual-pronged%2520automated%2520evaluation%2520framework%2520is%2520employed%2520to%2520assess%250Athe%2520generated%2520questions.%2520Curriculum%2520alignment%2520is%2520measured%2520using%2520Semantic%250ATextual%2520Similarity%2520%2528STS%2529%2520against%2520the%2520RPT%252C%2520while%2520contextual%2520validity%2520is%2520verified%250Athrough%2520a%2520novel%2520RAG-based%2520Question-Answering%2520%2528RAG-QA%2529%2520method.%2520The%2520results%250Ademonstrate%2520that%2520RAG-based%2520pipelines%2520significantly%2520outperform%2520non-grounded%250Aprompting%2520methods%252C%2520producing%2520questions%2520with%2520higher%2520curriculum%2520alignment%2520and%250Afactual%2520validity.%2520The%2520study%2520further%2520analyzes%2520the%2520trade-offs%2520between%2520the%2520ease%2520of%250Aimplementation%2520of%2520framework-based%2520RAG%2520and%2520the%2520fine-grained%2520control%2520offered%2520by%2520a%250Amanual%2520pipeline.%2520This%2520work%2520presents%2520a%2520validated%2520methodology%2520for%2520generating%250Acurriculum-specific%2520educational%2520content%2520in%2520a%2520low-resource%2520language%252C%2520introduces%250Aa%2520symbiotic%2520RAG-QA%2520evaluation%2520technique%252C%2520and%2520provides%2520actionable%2520insights%2520for%250Athe%2520development%2520and%2520deployment%2520of%2520practical%2520EdTech%2520solutions%2520in%2520Malaysia%2520and%250Asimilar%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Generation%20of%20Curriculum-Aligned%20Multiple-Choice%20Questions%20for%0A%20%20Malaysian%20Secondary%20Mathematics%20Using%20Generative%20AI&entry.906535625=Rohaizah%20Abdul%20Wahid%20and%20Muhamad%20Said%20Nizamuddin%20Nadim%20and%20Suliana%20Sulaiman%20and%20Syahmi%20Akmal%20Shaharudin%20and%20Muhammad%20Danial%20Jupikil%20and%20Iqqwan%20Jasman%20Su%20Azlan%20Su&entry.1292438233=%20%20This%20paper%20addresses%20the%20critical%20need%20for%20scalable%20and%20high-quality%0Aeducational%20assessment%20tools%20within%20the%20Malaysian%20education%20system.%20It%0Ahighlights%20the%20potential%20of%20Generative%20AI%20%28GenAI%29%20while%20acknowledging%20the%0Asignificant%20challenges%20of%20ensuring%20factual%20accuracy%20and%20curriculum%20alignment%2C%0Aespecially%20for%20low-resource%20languages%20like%20Bahasa%20Melayu.%20This%20research%0Aintroduces%20and%20compares%20four%20incremental%20pipelines%20for%20generating%20Form%201%0AMathematics%20multiple-choice%20questions%20%28MCQs%29%20in%20Bahasa%20Melayu%20using%20OpenAI%27s%0AGPT-4o.%20The%20methods%20range%20from%20non-grounded%20prompting%20%28structured%20and%20basic%29%20to%0ARetrieval-Augmented%20Generation%20%28RAG%29%20approaches%20%28one%20using%20the%20LangChain%0Aframework%2C%20one%20implemented%20manually%29.%20The%20system%20is%20grounded%20in%20official%0Acurriculum%20documents%2C%20including%20teacher-prepared%20notes%20and%20the%20yearly%20teaching%0Aplan%20%28RPT%29.%20A%20dual-pronged%20automated%20evaluation%20framework%20is%20employed%20to%20assess%0Athe%20generated%20questions.%20Curriculum%20alignment%20is%20measured%20using%20Semantic%0ATextual%20Similarity%20%28STS%29%20against%20the%20RPT%2C%20while%20contextual%20validity%20is%20verified%0Athrough%20a%20novel%20RAG-based%20Question-Answering%20%28RAG-QA%29%20method.%20The%20results%0Ademonstrate%20that%20RAG-based%20pipelines%20significantly%20outperform%20non-grounded%0Aprompting%20methods%2C%20producing%20questions%20with%20higher%20curriculum%20alignment%20and%0Afactual%20validity.%20The%20study%20further%20analyzes%20the%20trade-offs%20between%20the%20ease%20of%0Aimplementation%20of%20framework-based%20RAG%20and%20the%20fine-grained%20control%20offered%20by%20a%0Amanual%20pipeline.%20This%20work%20presents%20a%20validated%20methodology%20for%20generating%0Acurriculum-specific%20educational%20content%20in%20a%20low-resource%20language%2C%20introduces%0Aa%20symbiotic%20RAG-QA%20evaluation%20technique%2C%20and%20provides%20actionable%20insights%20for%0Athe%20development%20and%20deployment%20of%20practical%20EdTech%20solutions%20in%20Malaysia%20and%0Asimilar%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04442v1&entry.124074799=Read"},
{"title": "Open Scene Graphs for Open-World Object-Goal Navigation", "author": "Joel Loo and Zhanxin Wu and David Hsu", "abstract": "  How can we build general-purpose robot systems for open-world semantic\nnavigation, e.g., searching a novel environment for a target object specified\nin natural language? To tackle this challenge, we introduce OSG Navigator, a\nmodular system composed of foundation models, for open-world Object-Goal\nNavigation (ObjectNav). Foundation models provide enormous semantic knowledge\nabout the world, but struggle to organise and maintain spatial information\neffectively at scale. Key to OSG Navigator is the Open Scene Graph\nrepresentation, which acts as spatial memory for OSG Navigator. It organises\nspatial information hierarchically using OSG schemas, which are templates, each\ndescribing the common structure of a class of environments. OSG schemas can be\nautomatically generated from simple semantic labels of a given environment,\ne.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to\nnew environment types. We conducted experiments using both Fetch and Spot\nrobots in simulation and in the real world, showing that OSG Navigator achieves\nstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shot\nover diverse goals, environments, and robot embodiments.\n", "link": "http://arxiv.org/abs/2508.04678v1", "date": "2025-08-06", "relevancy": 2.279, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20Scene%20Graphs%20for%20Open-World%20Object-Goal%20Navigation&body=Title%3A%20Open%20Scene%20Graphs%20for%20Open-World%20Object-Goal%20Navigation%0AAuthor%3A%20Joel%20Loo%20and%20Zhanxin%20Wu%20and%20David%20Hsu%0AAbstract%3A%20%20%20How%20can%20we%20build%20general-purpose%20robot%20systems%20for%20open-world%20semantic%0Anavigation%2C%20e.g.%2C%20searching%20a%20novel%20environment%20for%20a%20target%20object%20specified%0Ain%20natural%20language%3F%20To%20tackle%20this%20challenge%2C%20we%20introduce%20OSG%20Navigator%2C%20a%0Amodular%20system%20composed%20of%20foundation%20models%2C%20for%20open-world%20Object-Goal%0ANavigation%20%28ObjectNav%29.%20Foundation%20models%20provide%20enormous%20semantic%20knowledge%0Aabout%20the%20world%2C%20but%20struggle%20to%20organise%20and%20maintain%20spatial%20information%0Aeffectively%20at%20scale.%20Key%20to%20OSG%20Navigator%20is%20the%20Open%20Scene%20Graph%0Arepresentation%2C%20which%20acts%20as%20spatial%20memory%20for%20OSG%20Navigator.%20It%20organises%0Aspatial%20information%20hierarchically%20using%20OSG%20schemas%2C%20which%20are%20templates%2C%20each%0Adescribing%20the%20common%20structure%20of%20a%20class%20of%20environments.%20OSG%20schemas%20can%20be%0Aautomatically%20generated%20from%20simple%20semantic%20labels%20of%20a%20given%20environment%2C%0Ae.g.%2C%20%22home%22%20or%20%22supermarket%22.%20They%20enable%20OSG%20Navigator%20to%20adapt%20zero-shot%20to%0Anew%20environment%20types.%20We%20conducted%20experiments%20using%20both%20Fetch%20and%20Spot%0Arobots%20in%20simulation%20and%20in%20the%20real%20world%2C%20showing%20that%20OSG%20Navigator%20achieves%0Astate-of-the-art%20performance%20on%20ObjectNav%20benchmarks%20and%20generalises%20zero-shot%0Aover%20diverse%20goals%2C%20environments%2C%20and%20robot%20embodiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520Scene%2520Graphs%2520for%2520Open-World%2520Object-Goal%2520Navigation%26entry.906535625%3DJoel%2520Loo%2520and%2520Zhanxin%2520Wu%2520and%2520David%2520Hsu%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520build%2520general-purpose%2520robot%2520systems%2520for%2520open-world%2520semantic%250Anavigation%252C%2520e.g.%252C%2520searching%2520a%2520novel%2520environment%2520for%2520a%2520target%2520object%2520specified%250Ain%2520natural%2520language%253F%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520introduce%2520OSG%2520Navigator%252C%2520a%250Amodular%2520system%2520composed%2520of%2520foundation%2520models%252C%2520for%2520open-world%2520Object-Goal%250ANavigation%2520%2528ObjectNav%2529.%2520Foundation%2520models%2520provide%2520enormous%2520semantic%2520knowledge%250Aabout%2520the%2520world%252C%2520but%2520struggle%2520to%2520organise%2520and%2520maintain%2520spatial%2520information%250Aeffectively%2520at%2520scale.%2520Key%2520to%2520OSG%2520Navigator%2520is%2520the%2520Open%2520Scene%2520Graph%250Arepresentation%252C%2520which%2520acts%2520as%2520spatial%2520memory%2520for%2520OSG%2520Navigator.%2520It%2520organises%250Aspatial%2520information%2520hierarchically%2520using%2520OSG%2520schemas%252C%2520which%2520are%2520templates%252C%2520each%250Adescribing%2520the%2520common%2520structure%2520of%2520a%2520class%2520of%2520environments.%2520OSG%2520schemas%2520can%2520be%250Aautomatically%2520generated%2520from%2520simple%2520semantic%2520labels%2520of%2520a%2520given%2520environment%252C%250Ae.g.%252C%2520%2522home%2522%2520or%2520%2522supermarket%2522.%2520They%2520enable%2520OSG%2520Navigator%2520to%2520adapt%2520zero-shot%2520to%250Anew%2520environment%2520types.%2520We%2520conducted%2520experiments%2520using%2520both%2520Fetch%2520and%2520Spot%250Arobots%2520in%2520simulation%2520and%2520in%2520the%2520real%2520world%252C%2520showing%2520that%2520OSG%2520Navigator%2520achieves%250Astate-of-the-art%2520performance%2520on%2520ObjectNav%2520benchmarks%2520and%2520generalises%2520zero-shot%250Aover%2520diverse%2520goals%252C%2520environments%252C%2520and%2520robot%2520embodiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20Scene%20Graphs%20for%20Open-World%20Object-Goal%20Navigation&entry.906535625=Joel%20Loo%20and%20Zhanxin%20Wu%20and%20David%20Hsu&entry.1292438233=%20%20How%20can%20we%20build%20general-purpose%20robot%20systems%20for%20open-world%20semantic%0Anavigation%2C%20e.g.%2C%20searching%20a%20novel%20environment%20for%20a%20target%20object%20specified%0Ain%20natural%20language%3F%20To%20tackle%20this%20challenge%2C%20we%20introduce%20OSG%20Navigator%2C%20a%0Amodular%20system%20composed%20of%20foundation%20models%2C%20for%20open-world%20Object-Goal%0ANavigation%20%28ObjectNav%29.%20Foundation%20models%20provide%20enormous%20semantic%20knowledge%0Aabout%20the%20world%2C%20but%20struggle%20to%20organise%20and%20maintain%20spatial%20information%0Aeffectively%20at%20scale.%20Key%20to%20OSG%20Navigator%20is%20the%20Open%20Scene%20Graph%0Arepresentation%2C%20which%20acts%20as%20spatial%20memory%20for%20OSG%20Navigator.%20It%20organises%0Aspatial%20information%20hierarchically%20using%20OSG%20schemas%2C%20which%20are%20templates%2C%20each%0Adescribing%20the%20common%20structure%20of%20a%20class%20of%20environments.%20OSG%20schemas%20can%20be%0Aautomatically%20generated%20from%20simple%20semantic%20labels%20of%20a%20given%20environment%2C%0Ae.g.%2C%20%22home%22%20or%20%22supermarket%22.%20They%20enable%20OSG%20Navigator%20to%20adapt%20zero-shot%20to%0Anew%20environment%20types.%20We%20conducted%20experiments%20using%20both%20Fetch%20and%20Spot%0Arobots%20in%20simulation%20and%20in%20the%20real%20world%2C%20showing%20that%20OSG%20Navigator%20achieves%0Astate-of-the-art%20performance%20on%20ObjectNav%20benchmarks%20and%20generalises%20zero-shot%0Aover%20diverse%20goals%2C%20environments%2C%20and%20robot%20embodiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04678v1&entry.124074799=Read"},
{"title": "NCCR: to Evaluate the Robustness of Neural Networks and Adversarial\n  Examples", "author": "Shi Pu and Fu Song and Wenjie Wang", "abstract": "  Neural networks have received a lot of attention recently, and related\nsecurity issues have come with it. Many studies have shown that neural networks\nare vulnerable to adversarial examples that have been artificially perturbed\nwith modification, which is too small to be distinguishable by human\nperception. Different attacks and defenses have been proposed to solve these\nproblems, but there is little research on evaluating the robustness of neural\nnetworks and their inputs. In this work, we propose a metric called the neuron\ncover change rate (NCCR) to measure the ability of deep learning models to\nresist attacks and the stability of adversarial examples. NCCR monitors\nalterations in the output of specifically chosen neurons when the input is\nperturbed, and networks with a smaller degree of variation are considered to be\nmore robust. The results of the experiment on image recognition and the speaker\nrecognition model show that our metrics can provide a good assessment of the\nrobustness of neural networks or their inputs. It can also be used to detect\nwhether an input is adversarial or not, as adversarial examples are always less\nrobust.\n", "link": "http://arxiv.org/abs/2507.21483v2", "date": "2025-08-06", "relevancy": 2.2702, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4726}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4541}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NCCR%3A%20to%20Evaluate%20the%20Robustness%20of%20Neural%20Networks%20and%20Adversarial%0A%20%20Examples&body=Title%3A%20NCCR%3A%20to%20Evaluate%20the%20Robustness%20of%20Neural%20Networks%20and%20Adversarial%0A%20%20Examples%0AAuthor%3A%20Shi%20Pu%20and%20Fu%20Song%20and%20Wenjie%20Wang%0AAbstract%3A%20%20%20Neural%20networks%20have%20received%20a%20lot%20of%20attention%20recently%2C%20and%20related%0Asecurity%20issues%20have%20come%20with%20it.%20Many%20studies%20have%20shown%20that%20neural%20networks%0Aare%20vulnerable%20to%20adversarial%20examples%20that%20have%20been%20artificially%20perturbed%0Awith%20modification%2C%20which%20is%20too%20small%20to%20be%20distinguishable%20by%20human%0Aperception.%20Different%20attacks%20and%20defenses%20have%20been%20proposed%20to%20solve%20these%0Aproblems%2C%20but%20there%20is%20little%20research%20on%20evaluating%20the%20robustness%20of%20neural%0Anetworks%20and%20their%20inputs.%20In%20this%20work%2C%20we%20propose%20a%20metric%20called%20the%20neuron%0Acover%20change%20rate%20%28NCCR%29%20to%20measure%20the%20ability%20of%20deep%20learning%20models%20to%0Aresist%20attacks%20and%20the%20stability%20of%20adversarial%20examples.%20NCCR%20monitors%0Aalterations%20in%20the%20output%20of%20specifically%20chosen%20neurons%20when%20the%20input%20is%0Aperturbed%2C%20and%20networks%20with%20a%20smaller%20degree%20of%20variation%20are%20considered%20to%20be%0Amore%20robust.%20The%20results%20of%20the%20experiment%20on%20image%20recognition%20and%20the%20speaker%0Arecognition%20model%20show%20that%20our%20metrics%20can%20provide%20a%20good%20assessment%20of%20the%0Arobustness%20of%20neural%20networks%20or%20their%20inputs.%20It%20can%20also%20be%20used%20to%20detect%0Awhether%20an%20input%20is%20adversarial%20or%20not%2C%20as%20adversarial%20examples%20are%20always%20less%0Arobust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21483v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNCCR%253A%2520to%2520Evaluate%2520the%2520Robustness%2520of%2520Neural%2520Networks%2520and%2520Adversarial%250A%2520%2520Examples%26entry.906535625%3DShi%2520Pu%2520and%2520Fu%2520Song%2520and%2520Wenjie%2520Wang%26entry.1292438233%3D%2520%2520Neural%2520networks%2520have%2520received%2520a%2520lot%2520of%2520attention%2520recently%252C%2520and%2520related%250Asecurity%2520issues%2520have%2520come%2520with%2520it.%2520Many%2520studies%2520have%2520shown%2520that%2520neural%2520networks%250Aare%2520vulnerable%2520to%2520adversarial%2520examples%2520that%2520have%2520been%2520artificially%2520perturbed%250Awith%2520modification%252C%2520which%2520is%2520too%2520small%2520to%2520be%2520distinguishable%2520by%2520human%250Aperception.%2520Different%2520attacks%2520and%2520defenses%2520have%2520been%2520proposed%2520to%2520solve%2520these%250Aproblems%252C%2520but%2520there%2520is%2520little%2520research%2520on%2520evaluating%2520the%2520robustness%2520of%2520neural%250Anetworks%2520and%2520their%2520inputs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520metric%2520called%2520the%2520neuron%250Acover%2520change%2520rate%2520%2528NCCR%2529%2520to%2520measure%2520the%2520ability%2520of%2520deep%2520learning%2520models%2520to%250Aresist%2520attacks%2520and%2520the%2520stability%2520of%2520adversarial%2520examples.%2520NCCR%2520monitors%250Aalterations%2520in%2520the%2520output%2520of%2520specifically%2520chosen%2520neurons%2520when%2520the%2520input%2520is%250Aperturbed%252C%2520and%2520networks%2520with%2520a%2520smaller%2520degree%2520of%2520variation%2520are%2520considered%2520to%2520be%250Amore%2520robust.%2520The%2520results%2520of%2520the%2520experiment%2520on%2520image%2520recognition%2520and%2520the%2520speaker%250Arecognition%2520model%2520show%2520that%2520our%2520metrics%2520can%2520provide%2520a%2520good%2520assessment%2520of%2520the%250Arobustness%2520of%2520neural%2520networks%2520or%2520their%2520inputs.%2520It%2520can%2520also%2520be%2520used%2520to%2520detect%250Awhether%2520an%2520input%2520is%2520adversarial%2520or%2520not%252C%2520as%2520adversarial%2520examples%2520are%2520always%2520less%250Arobust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21483v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NCCR%3A%20to%20Evaluate%20the%20Robustness%20of%20Neural%20Networks%20and%20Adversarial%0A%20%20Examples&entry.906535625=Shi%20Pu%20and%20Fu%20Song%20and%20Wenjie%20Wang&entry.1292438233=%20%20Neural%20networks%20have%20received%20a%20lot%20of%20attention%20recently%2C%20and%20related%0Asecurity%20issues%20have%20come%20with%20it.%20Many%20studies%20have%20shown%20that%20neural%20networks%0Aare%20vulnerable%20to%20adversarial%20examples%20that%20have%20been%20artificially%20perturbed%0Awith%20modification%2C%20which%20is%20too%20small%20to%20be%20distinguishable%20by%20human%0Aperception.%20Different%20attacks%20and%20defenses%20have%20been%20proposed%20to%20solve%20these%0Aproblems%2C%20but%20there%20is%20little%20research%20on%20evaluating%20the%20robustness%20of%20neural%0Anetworks%20and%20their%20inputs.%20In%20this%20work%2C%20we%20propose%20a%20metric%20called%20the%20neuron%0Acover%20change%20rate%20%28NCCR%29%20to%20measure%20the%20ability%20of%20deep%20learning%20models%20to%0Aresist%20attacks%20and%20the%20stability%20of%20adversarial%20examples.%20NCCR%20monitors%0Aalterations%20in%20the%20output%20of%20specifically%20chosen%20neurons%20when%20the%20input%20is%0Aperturbed%2C%20and%20networks%20with%20a%20smaller%20degree%20of%20variation%20are%20considered%20to%20be%0Amore%20robust.%20The%20results%20of%20the%20experiment%20on%20image%20recognition%20and%20the%20speaker%0Arecognition%20model%20show%20that%20our%20metrics%20can%20provide%20a%20good%20assessment%20of%20the%0Arobustness%20of%20neural%20networks%20or%20their%20inputs.%20It%20can%20also%20be%20used%20to%20detect%0Awhether%20an%20input%20is%20adversarial%20or%20not%2C%20as%20adversarial%20examples%20are%20always%20less%0Arobust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21483v2&entry.124074799=Read"},
{"title": "Personalized One-shot Federated Graph Learning for Heterogeneous Clients", "author": "Guochen Yan and Xunkai Li and Luyuan Xie and Qingni Shen and Yuejian Fang and Zhonghai Wu", "abstract": "  Federated Graph Learning (FGL) has emerged as a promising paradigm for\nbreaking data silos among distributed private graphs. In practical scenarios\ninvolving heterogeneous distributed graph data, personalized Federated Graph\nLearning (pFGL) aims to enhance model utility by training personalized models\ntailored to client needs. However, existing pFGL methods often require numerous\ncommunication rounds under heterogeneous graphs, leading to significant\ncommunication overhead and security concerns. While One-shot Federated Learning\n(OFL) enables collaboration in a single round, existing OFL methods are\ndesigned for image-centric tasks and are ineffective for graph data, leaving a\ncritical gap in the field. Additionally, personalized models derived from\nexisting methods suffer from bias, failing to effectively generalize to the\nminority. To address these challenges, we propose the first \\textbf{O}ne-shot\n\\textbf{p}ersonalized \\textbf{F}ederated \\textbf{G}raph \\textbf{L}earning\nmethod (\\textbf{O-pFGL}) for node classification, compatible with Secure\nAggregation protocols for privacy preservation. Specifically, for effective\ngraph learning in one communication round, our method estimates and aggregates\nclass-wise feature distribution statistics to construct a global surrogate\ngraph on the server, facilitating the training of a global graph model. To\nmitigate bias, we introduce a two-stage personalized training approach that\nadaptively balances local personal information and global insights from the\nsurrogate graph, improving both personalization and generalization. Extensive\nexperiments on 14 diverse real-world graph datasets demonstrate that our method\nsignificantly outperforms state-of-the-art baselines across various settings.\n", "link": "http://arxiv.org/abs/2411.11304v7", "date": "2025-08-06", "relevancy": 2.2659, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4856}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4376}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20One-shot%20Federated%20Graph%20Learning%20for%20Heterogeneous%20Clients&body=Title%3A%20Personalized%20One-shot%20Federated%20Graph%20Learning%20for%20Heterogeneous%20Clients%0AAuthor%3A%20Guochen%20Yan%20and%20Xunkai%20Li%20and%20Luyuan%20Xie%20and%20Qingni%20Shen%20and%20Yuejian%20Fang%20and%20Zhonghai%20Wu%0AAbstract%3A%20%20%20Federated%20Graph%20Learning%20%28FGL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Abreaking%20data%20silos%20among%20distributed%20private%20graphs.%20In%20practical%20scenarios%0Ainvolving%20heterogeneous%20distributed%20graph%20data%2C%20personalized%20Federated%20Graph%0ALearning%20%28pFGL%29%20aims%20to%20enhance%20model%20utility%20by%20training%20personalized%20models%0Atailored%20to%20client%20needs.%20However%2C%20existing%20pFGL%20methods%20often%20require%20numerous%0Acommunication%20rounds%20under%20heterogeneous%20graphs%2C%20leading%20to%20significant%0Acommunication%20overhead%20and%20security%20concerns.%20While%20One-shot%20Federated%20Learning%0A%28OFL%29%20enables%20collaboration%20in%20a%20single%20round%2C%20existing%20OFL%20methods%20are%0Adesigned%20for%20image-centric%20tasks%20and%20are%20ineffective%20for%20graph%20data%2C%20leaving%20a%0Acritical%20gap%20in%20the%20field.%20Additionally%2C%20personalized%20models%20derived%20from%0Aexisting%20methods%20suffer%20from%20bias%2C%20failing%20to%20effectively%20generalize%20to%20the%0Aminority.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20%5Ctextbf%7BO%7Dne-shot%0A%5Ctextbf%7Bp%7Dersonalized%20%5Ctextbf%7BF%7Dederated%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BL%7Dearning%0Amethod%20%28%5Ctextbf%7BO-pFGL%7D%29%20for%20node%20classification%2C%20compatible%20with%20Secure%0AAggregation%20protocols%20for%20privacy%20preservation.%20Specifically%2C%20for%20effective%0Agraph%20learning%20in%20one%20communication%20round%2C%20our%20method%20estimates%20and%20aggregates%0Aclass-wise%20feature%20distribution%20statistics%20to%20construct%20a%20global%20surrogate%0Agraph%20on%20the%20server%2C%20facilitating%20the%20training%20of%20a%20global%20graph%20model.%20To%0Amitigate%20bias%2C%20we%20introduce%20a%20two-stage%20personalized%20training%20approach%20that%0Aadaptively%20balances%20local%20personal%20information%20and%20global%20insights%20from%20the%0Asurrogate%20graph%2C%20improving%20both%20personalization%20and%20generalization.%20Extensive%0Aexperiments%20on%2014%20diverse%20real-world%20graph%20datasets%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20baselines%20across%20various%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11304v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520One-shot%2520Federated%2520Graph%2520Learning%2520for%2520Heterogeneous%2520Clients%26entry.906535625%3DGuochen%2520Yan%2520and%2520Xunkai%2520Li%2520and%2520Luyuan%2520Xie%2520and%2520Qingni%2520Shen%2520and%2520Yuejian%2520Fang%2520and%2520Zhonghai%2520Wu%26entry.1292438233%3D%2520%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%250Abreaking%2520data%2520silos%2520among%2520distributed%2520private%2520graphs.%2520In%2520practical%2520scenarios%250Ainvolving%2520heterogeneous%2520distributed%2520graph%2520data%252C%2520personalized%2520Federated%2520Graph%250ALearning%2520%2528pFGL%2529%2520aims%2520to%2520enhance%2520model%2520utility%2520by%2520training%2520personalized%2520models%250Atailored%2520to%2520client%2520needs.%2520However%252C%2520existing%2520pFGL%2520methods%2520often%2520require%2520numerous%250Acommunication%2520rounds%2520under%2520heterogeneous%2520graphs%252C%2520leading%2520to%2520significant%250Acommunication%2520overhead%2520and%2520security%2520concerns.%2520While%2520One-shot%2520Federated%2520Learning%250A%2528OFL%2529%2520enables%2520collaboration%2520in%2520a%2520single%2520round%252C%2520existing%2520OFL%2520methods%2520are%250Adesigned%2520for%2520image-centric%2520tasks%2520and%2520are%2520ineffective%2520for%2520graph%2520data%252C%2520leaving%2520a%250Acritical%2520gap%2520in%2520the%2520field.%2520Additionally%252C%2520personalized%2520models%2520derived%2520from%250Aexisting%2520methods%2520suffer%2520from%2520bias%252C%2520failing%2520to%2520effectively%2520generalize%2520to%2520the%250Aminority.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520the%2520first%2520%255Ctextbf%257BO%257Dne-shot%250A%255Ctextbf%257Bp%257Dersonalized%2520%255Ctextbf%257BF%257Dederated%2520%255Ctextbf%257BG%257Draph%2520%255Ctextbf%257BL%257Dearning%250Amethod%2520%2528%255Ctextbf%257BO-pFGL%257D%2529%2520for%2520node%2520classification%252C%2520compatible%2520with%2520Secure%250AAggregation%2520protocols%2520for%2520privacy%2520preservation.%2520Specifically%252C%2520for%2520effective%250Agraph%2520learning%2520in%2520one%2520communication%2520round%252C%2520our%2520method%2520estimates%2520and%2520aggregates%250Aclass-wise%2520feature%2520distribution%2520statistics%2520to%2520construct%2520a%2520global%2520surrogate%250Agraph%2520on%2520the%2520server%252C%2520facilitating%2520the%2520training%2520of%2520a%2520global%2520graph%2520model.%2520To%250Amitigate%2520bias%252C%2520we%2520introduce%2520a%2520two-stage%2520personalized%2520training%2520approach%2520that%250Aadaptively%2520balances%2520local%2520personal%2520information%2520and%2520global%2520insights%2520from%2520the%250Asurrogate%2520graph%252C%2520improving%2520both%2520personalization%2520and%2520generalization.%2520Extensive%250Aexperiments%2520on%252014%2520diverse%2520real-world%2520graph%2520datasets%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520baselines%2520across%2520various%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11304v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20One-shot%20Federated%20Graph%20Learning%20for%20Heterogeneous%20Clients&entry.906535625=Guochen%20Yan%20and%20Xunkai%20Li%20and%20Luyuan%20Xie%20and%20Qingni%20Shen%20and%20Yuejian%20Fang%20and%20Zhonghai%20Wu&entry.1292438233=%20%20Federated%20Graph%20Learning%20%28FGL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Abreaking%20data%20silos%20among%20distributed%20private%20graphs.%20In%20practical%20scenarios%0Ainvolving%20heterogeneous%20distributed%20graph%20data%2C%20personalized%20Federated%20Graph%0ALearning%20%28pFGL%29%20aims%20to%20enhance%20model%20utility%20by%20training%20personalized%20models%0Atailored%20to%20client%20needs.%20However%2C%20existing%20pFGL%20methods%20often%20require%20numerous%0Acommunication%20rounds%20under%20heterogeneous%20graphs%2C%20leading%20to%20significant%0Acommunication%20overhead%20and%20security%20concerns.%20While%20One-shot%20Federated%20Learning%0A%28OFL%29%20enables%20collaboration%20in%20a%20single%20round%2C%20existing%20OFL%20methods%20are%0Adesigned%20for%20image-centric%20tasks%20and%20are%20ineffective%20for%20graph%20data%2C%20leaving%20a%0Acritical%20gap%20in%20the%20field.%20Additionally%2C%20personalized%20models%20derived%20from%0Aexisting%20methods%20suffer%20from%20bias%2C%20failing%20to%20effectively%20generalize%20to%20the%0Aminority.%20To%20address%20these%20challenges%2C%20we%20propose%20the%20first%20%5Ctextbf%7BO%7Dne-shot%0A%5Ctextbf%7Bp%7Dersonalized%20%5Ctextbf%7BF%7Dederated%20%5Ctextbf%7BG%7Draph%20%5Ctextbf%7BL%7Dearning%0Amethod%20%28%5Ctextbf%7BO-pFGL%7D%29%20for%20node%20classification%2C%20compatible%20with%20Secure%0AAggregation%20protocols%20for%20privacy%20preservation.%20Specifically%2C%20for%20effective%0Agraph%20learning%20in%20one%20communication%20round%2C%20our%20method%20estimates%20and%20aggregates%0Aclass-wise%20feature%20distribution%20statistics%20to%20construct%20a%20global%20surrogate%0Agraph%20on%20the%20server%2C%20facilitating%20the%20training%20of%20a%20global%20graph%20model.%20To%0Amitigate%20bias%2C%20we%20introduce%20a%20two-stage%20personalized%20training%20approach%20that%0Aadaptively%20balances%20local%20personal%20information%20and%20global%20insights%20from%20the%0Asurrogate%20graph%2C%20improving%20both%20personalization%20and%20generalization.%20Extensive%0Aexperiments%20on%2014%20diverse%20real-world%20graph%20datasets%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20baselines%20across%20various%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11304v7&entry.124074799=Read"},
{"title": "Efficient Inter-Task Attention for Multitask Transformer Models", "author": "Christian Bohn and Thomas Kurbiel and Klaus Friedrichs and Hasan Tercan and Tobias Meisen", "abstract": "  In both Computer Vision and the wider Deep Learning field, the Transformer\narchitecture is well-established as state-of-the-art for many applications. For\nMultitask Learning, however, where there may be many more queries necessary\ncompared to single-task models, its Multi-Head-Attention often approaches the\nlimits of what is computationally feasible considering practical hardware\nlimitations. This is due to the fact that the size of the attention matrix\nscales quadratically with the number of tasks (assuming roughly equal numbers\nof queries for all tasks). As a solution, we propose our novel Deformable\nInter-Task Self-Attention for Multitask models that enables the much more\nefficient aggregation of information across the feature maps from different\ntasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we\ndemonstrate an order-of-magnitude reduction in both FLOPs count and inference\nlatency. At the same time, we also achieve substantial improvements by up to\n7.4% in the individual tasks' prediction quality metrics.\n", "link": "http://arxiv.org/abs/2508.04422v1", "date": "2025-08-06", "relevancy": 2.2653, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5843}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Inter-Task%20Attention%20for%20Multitask%20Transformer%20Models&body=Title%3A%20Efficient%20Inter-Task%20Attention%20for%20Multitask%20Transformer%20Models%0AAuthor%3A%20Christian%20Bohn%20and%20Thomas%20Kurbiel%20and%20Klaus%20Friedrichs%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen%0AAbstract%3A%20%20%20In%20both%20Computer%20Vision%20and%20the%20wider%20Deep%20Learning%20field%2C%20the%20Transformer%0Aarchitecture%20is%20well-established%20as%20state-of-the-art%20for%20many%20applications.%20For%0AMultitask%20Learning%2C%20however%2C%20where%20there%20may%20be%20many%20more%20queries%20necessary%0Acompared%20to%20single-task%20models%2C%20its%20Multi-Head-Attention%20often%20approaches%20the%0Alimits%20of%20what%20is%20computationally%20feasible%20considering%20practical%20hardware%0Alimitations.%20This%20is%20due%20to%20the%20fact%20that%20the%20size%20of%20the%20attention%20matrix%0Ascales%20quadratically%20with%20the%20number%20of%20tasks%20%28assuming%20roughly%20equal%20numbers%0Aof%20queries%20for%20all%20tasks%29.%20As%20a%20solution%2C%20we%20propose%20our%20novel%20Deformable%0AInter-Task%20Self-Attention%20for%20Multitask%20models%20that%20enables%20the%20much%20more%0Aefficient%20aggregation%20of%20information%20across%20the%20feature%20maps%20from%20different%0Atasks.%20In%20our%20experiments%20on%20the%20NYUD-v2%20and%20PASCAL-Context%20datasets%2C%20we%0Ademonstrate%20an%20order-of-magnitude%20reduction%20in%20both%20FLOPs%20count%20and%20inference%0Alatency.%20At%20the%20same%20time%2C%20we%20also%20achieve%20substantial%20improvements%20by%20up%20to%0A7.4%25%20in%20the%20individual%20tasks%27%20prediction%20quality%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Inter-Task%2520Attention%2520for%2520Multitask%2520Transformer%2520Models%26entry.906535625%3DChristian%2520Bohn%2520and%2520Thomas%2520Kurbiel%2520and%2520Klaus%2520Friedrichs%2520and%2520Hasan%2520Tercan%2520and%2520Tobias%2520Meisen%26entry.1292438233%3D%2520%2520In%2520both%2520Computer%2520Vision%2520and%2520the%2520wider%2520Deep%2520Learning%2520field%252C%2520the%2520Transformer%250Aarchitecture%2520is%2520well-established%2520as%2520state-of-the-art%2520for%2520many%2520applications.%2520For%250AMultitask%2520Learning%252C%2520however%252C%2520where%2520there%2520may%2520be%2520many%2520more%2520queries%2520necessary%250Acompared%2520to%2520single-task%2520models%252C%2520its%2520Multi-Head-Attention%2520often%2520approaches%2520the%250Alimits%2520of%2520what%2520is%2520computationally%2520feasible%2520considering%2520practical%2520hardware%250Alimitations.%2520This%2520is%2520due%2520to%2520the%2520fact%2520that%2520the%2520size%2520of%2520the%2520attention%2520matrix%250Ascales%2520quadratically%2520with%2520the%2520number%2520of%2520tasks%2520%2528assuming%2520roughly%2520equal%2520numbers%250Aof%2520queries%2520for%2520all%2520tasks%2529.%2520As%2520a%2520solution%252C%2520we%2520propose%2520our%2520novel%2520Deformable%250AInter-Task%2520Self-Attention%2520for%2520Multitask%2520models%2520that%2520enables%2520the%2520much%2520more%250Aefficient%2520aggregation%2520of%2520information%2520across%2520the%2520feature%2520maps%2520from%2520different%250Atasks.%2520In%2520our%2520experiments%2520on%2520the%2520NYUD-v2%2520and%2520PASCAL-Context%2520datasets%252C%2520we%250Ademonstrate%2520an%2520order-of-magnitude%2520reduction%2520in%2520both%2520FLOPs%2520count%2520and%2520inference%250Alatency.%2520At%2520the%2520same%2520time%252C%2520we%2520also%2520achieve%2520substantial%2520improvements%2520by%2520up%2520to%250A7.4%2525%2520in%2520the%2520individual%2520tasks%2527%2520prediction%2520quality%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Inter-Task%20Attention%20for%20Multitask%20Transformer%20Models&entry.906535625=Christian%20Bohn%20and%20Thomas%20Kurbiel%20and%20Klaus%20Friedrichs%20and%20Hasan%20Tercan%20and%20Tobias%20Meisen&entry.1292438233=%20%20In%20both%20Computer%20Vision%20and%20the%20wider%20Deep%20Learning%20field%2C%20the%20Transformer%0Aarchitecture%20is%20well-established%20as%20state-of-the-art%20for%20many%20applications.%20For%0AMultitask%20Learning%2C%20however%2C%20where%20there%20may%20be%20many%20more%20queries%20necessary%0Acompared%20to%20single-task%20models%2C%20its%20Multi-Head-Attention%20often%20approaches%20the%0Alimits%20of%20what%20is%20computationally%20feasible%20considering%20practical%20hardware%0Alimitations.%20This%20is%20due%20to%20the%20fact%20that%20the%20size%20of%20the%20attention%20matrix%0Ascales%20quadratically%20with%20the%20number%20of%20tasks%20%28assuming%20roughly%20equal%20numbers%0Aof%20queries%20for%20all%20tasks%29.%20As%20a%20solution%2C%20we%20propose%20our%20novel%20Deformable%0AInter-Task%20Self-Attention%20for%20Multitask%20models%20that%20enables%20the%20much%20more%0Aefficient%20aggregation%20of%20information%20across%20the%20feature%20maps%20from%20different%0Atasks.%20In%20our%20experiments%20on%20the%20NYUD-v2%20and%20PASCAL-Context%20datasets%2C%20we%0Ademonstrate%20an%20order-of-magnitude%20reduction%20in%20both%20FLOPs%20count%20and%20inference%0Alatency.%20At%20the%20same%20time%2C%20we%20also%20achieve%20substantial%20improvements%20by%20up%20to%0A7.4%25%20in%20the%20individual%20tasks%27%20prediction%20quality%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04422v1&entry.124074799=Read"},
{"title": "PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware\n  Selection of Generative Models and LLMs", "author": "Xiaoyan Hu and Ho-fung Leung and Farzan Farnia", "abstract": "  Selecting a sample generation scheme from multiple prompt-based generative\nmodels, including large language models (LLMs) and prompt-guided image and\nvideo generation models, is typically addressed by choosing the model that\nmaximizes an averaged evaluation score. However, this score-based selection\noverlooks the possibility that different models achieve the best generation\nperformance for different types of text prompts. An online identification of\nthe best generation model for various input prompts can reduce the costs\nassociated with querying sub-optimal models. In this work, we explore the\npossibility of varying rankings of text-based generative models for different\ntext prompts and propose an online learning framework to predict the best data\ngeneration model for a given input prompt. The proposed PAK-UCB algorithm\naddresses a contextual bandit (CB) setting with shared context variables across\nthe arms, utilizing the generated data to update kernel-based functions that\npredict the score of each model available for unseen text prompts.\nAdditionally, we leverage random Fourier features (RFF) to accelerate the\nonline learning process of PAK-UCB. Our numerical experiments on real and\nsimulated text-to-image and image-to-text generative models show that RFF-UCB\nperforms successfully in identifying the best generation model across different\nsample types. The code is available at:\ngithub.com/yannxiaoyanhu/dgm-online-select.\n", "link": "http://arxiv.org/abs/2410.13287v5", "date": "2025-08-06", "relevancy": 2.2608, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.577}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5581}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAK-UCB%20Contextual%20Bandit%3A%20An%20Online%20Learning%20Approach%20to%20Prompt-Aware%0A%20%20Selection%20of%20Generative%20Models%20and%20LLMs&body=Title%3A%20PAK-UCB%20Contextual%20Bandit%3A%20An%20Online%20Learning%20Approach%20to%20Prompt-Aware%0A%20%20Selection%20of%20Generative%20Models%20and%20LLMs%0AAuthor%3A%20Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20Selecting%20a%20sample%20generation%20scheme%20from%20multiple%20prompt-based%20generative%0Amodels%2C%20including%20large%20language%20models%20%28LLMs%29%20and%20prompt-guided%20image%20and%0Avideo%20generation%20models%2C%20is%20typically%20addressed%20by%20choosing%20the%20model%20that%0Amaximizes%20an%20averaged%20evaluation%20score.%20However%2C%20this%20score-based%20selection%0Aoverlooks%20the%20possibility%20that%20different%20models%20achieve%20the%20best%20generation%0Aperformance%20for%20different%20types%20of%20text%20prompts.%20An%20online%20identification%20of%0Athe%20best%20generation%20model%20for%20various%20input%20prompts%20can%20reduce%20the%20costs%0Aassociated%20with%20querying%20sub-optimal%20models.%20In%20this%20work%2C%20we%20explore%20the%0Apossibility%20of%20varying%20rankings%20of%20text-based%20generative%20models%20for%20different%0Atext%20prompts%20and%20propose%20an%20online%20learning%20framework%20to%20predict%20the%20best%20data%0Ageneration%20model%20for%20a%20given%20input%20prompt.%20The%20proposed%20PAK-UCB%20algorithm%0Aaddresses%20a%20contextual%20bandit%20%28CB%29%20setting%20with%20shared%20context%20variables%20across%0Athe%20arms%2C%20utilizing%20the%20generated%20data%20to%20update%20kernel-based%20functions%20that%0Apredict%20the%20score%20of%20each%20model%20available%20for%20unseen%20text%20prompts.%0AAdditionally%2C%20we%20leverage%20random%20Fourier%20features%20%28RFF%29%20to%20accelerate%20the%0Aonline%20learning%20process%20of%20PAK-UCB.%20Our%20numerical%20experiments%20on%20real%20and%0Asimulated%20text-to-image%20and%20image-to-text%20generative%20models%20show%20that%20RFF-UCB%0Aperforms%20successfully%20in%20identifying%20the%20best%20generation%20model%20across%20different%0Asample%20types.%20The%20code%20is%20available%20at%3A%0Agithub.com/yannxiaoyanhu/dgm-online-select.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13287v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAK-UCB%2520Contextual%2520Bandit%253A%2520An%2520Online%2520Learning%2520Approach%2520to%2520Prompt-Aware%250A%2520%2520Selection%2520of%2520Generative%2520Models%2520and%2520LLMs%26entry.906535625%3DXiaoyan%2520Hu%2520and%2520Ho-fung%2520Leung%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520Selecting%2520a%2520sample%2520generation%2520scheme%2520from%2520multiple%2520prompt-based%2520generative%250Amodels%252C%2520including%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520prompt-guided%2520image%2520and%250Avideo%2520generation%2520models%252C%2520is%2520typically%2520addressed%2520by%2520choosing%2520the%2520model%2520that%250Amaximizes%2520an%2520averaged%2520evaluation%2520score.%2520However%252C%2520this%2520score-based%2520selection%250Aoverlooks%2520the%2520possibility%2520that%2520different%2520models%2520achieve%2520the%2520best%2520generation%250Aperformance%2520for%2520different%2520types%2520of%2520text%2520prompts.%2520An%2520online%2520identification%2520of%250Athe%2520best%2520generation%2520model%2520for%2520various%2520input%2520prompts%2520can%2520reduce%2520the%2520costs%250Aassociated%2520with%2520querying%2520sub-optimal%2520models.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Apossibility%2520of%2520varying%2520rankings%2520of%2520text-based%2520generative%2520models%2520for%2520different%250Atext%2520prompts%2520and%2520propose%2520an%2520online%2520learning%2520framework%2520to%2520predict%2520the%2520best%2520data%250Ageneration%2520model%2520for%2520a%2520given%2520input%2520prompt.%2520The%2520proposed%2520PAK-UCB%2520algorithm%250Aaddresses%2520a%2520contextual%2520bandit%2520%2528CB%2529%2520setting%2520with%2520shared%2520context%2520variables%2520across%250Athe%2520arms%252C%2520utilizing%2520the%2520generated%2520data%2520to%2520update%2520kernel-based%2520functions%2520that%250Apredict%2520the%2520score%2520of%2520each%2520model%2520available%2520for%2520unseen%2520text%2520prompts.%250AAdditionally%252C%2520we%2520leverage%2520random%2520Fourier%2520features%2520%2528RFF%2529%2520to%2520accelerate%2520the%250Aonline%2520learning%2520process%2520of%2520PAK-UCB.%2520Our%2520numerical%2520experiments%2520on%2520real%2520and%250Asimulated%2520text-to-image%2520and%2520image-to-text%2520generative%2520models%2520show%2520that%2520RFF-UCB%250Aperforms%2520successfully%2520in%2520identifying%2520the%2520best%2520generation%2520model%2520across%2520different%250Asample%2520types.%2520The%2520code%2520is%2520available%2520at%253A%250Agithub.com/yannxiaoyanhu/dgm-online-select.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13287v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAK-UCB%20Contextual%20Bandit%3A%20An%20Online%20Learning%20Approach%20to%20Prompt-Aware%0A%20%20Selection%20of%20Generative%20Models%20and%20LLMs&entry.906535625=Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia&entry.1292438233=%20%20Selecting%20a%20sample%20generation%20scheme%20from%20multiple%20prompt-based%20generative%0Amodels%2C%20including%20large%20language%20models%20%28LLMs%29%20and%20prompt-guided%20image%20and%0Avideo%20generation%20models%2C%20is%20typically%20addressed%20by%20choosing%20the%20model%20that%0Amaximizes%20an%20averaged%20evaluation%20score.%20However%2C%20this%20score-based%20selection%0Aoverlooks%20the%20possibility%20that%20different%20models%20achieve%20the%20best%20generation%0Aperformance%20for%20different%20types%20of%20text%20prompts.%20An%20online%20identification%20of%0Athe%20best%20generation%20model%20for%20various%20input%20prompts%20can%20reduce%20the%20costs%0Aassociated%20with%20querying%20sub-optimal%20models.%20In%20this%20work%2C%20we%20explore%20the%0Apossibility%20of%20varying%20rankings%20of%20text-based%20generative%20models%20for%20different%0Atext%20prompts%20and%20propose%20an%20online%20learning%20framework%20to%20predict%20the%20best%20data%0Ageneration%20model%20for%20a%20given%20input%20prompt.%20The%20proposed%20PAK-UCB%20algorithm%0Aaddresses%20a%20contextual%20bandit%20%28CB%29%20setting%20with%20shared%20context%20variables%20across%0Athe%20arms%2C%20utilizing%20the%20generated%20data%20to%20update%20kernel-based%20functions%20that%0Apredict%20the%20score%20of%20each%20model%20available%20for%20unseen%20text%20prompts.%0AAdditionally%2C%20we%20leverage%20random%20Fourier%20features%20%28RFF%29%20to%20accelerate%20the%0Aonline%20learning%20process%20of%20PAK-UCB.%20Our%20numerical%20experiments%20on%20real%20and%0Asimulated%20text-to-image%20and%20image-to-text%20generative%20models%20show%20that%20RFF-UCB%0Aperforms%20successfully%20in%20identifying%20the%20best%20generation%20model%20across%20different%0Asample%20types.%20The%20code%20is%20available%20at%3A%0Agithub.com/yannxiaoyanhu/dgm-online-select.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13287v5&entry.124074799=Read"},
{"title": "AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time\n  Detection of Industrial Smoke Emissions", "author": "Mikhail Bychkov and Matey Yordanov and Andrei Kuchma", "abstract": "  This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework\ndesigned for robust, real-time detection and classification of industrial smoke\nemissions. The framework addresses critical limitations of current monitoring\nsystems, which often lack the specificity to distinguish smoke types and\nstruggle with environmental variability. AURA leverages both the dynamic\nmovement patterns and the distinct color characteristics of industrial smoke to\nprovide enhanced accuracy and reduced false positives. This framework aims to\nsignificantly improve environmental compliance, operational safety, and public\nhealth outcomes by enabling precise, automated monitoring of industrial\nemissions.\n", "link": "http://arxiv.org/abs/2508.01095v2", "date": "2025-08-06", "relevancy": 2.2603, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4632}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4549}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AURA%3A%20A%20Hybrid%20Spatiotemporal-Chromatic%20Framework%20for%20Robust%2C%20Real-Time%0A%20%20Detection%20of%20Industrial%20Smoke%20Emissions&body=Title%3A%20AURA%3A%20A%20Hybrid%20Spatiotemporal-Chromatic%20Framework%20for%20Robust%2C%20Real-Time%0A%20%20Detection%20of%20Industrial%20Smoke%20Emissions%0AAuthor%3A%20Mikhail%20Bychkov%20and%20Matey%20Yordanov%20and%20Andrei%20Kuchma%0AAbstract%3A%20%20%20This%20paper%20introduces%20AURA%2C%20a%20novel%20hybrid%20spatiotemporal-chromatic%20framework%0Adesigned%20for%20robust%2C%20real-time%20detection%20and%20classification%20of%20industrial%20smoke%0Aemissions.%20The%20framework%20addresses%20critical%20limitations%20of%20current%20monitoring%0Asystems%2C%20which%20often%20lack%20the%20specificity%20to%20distinguish%20smoke%20types%20and%0Astruggle%20with%20environmental%20variability.%20AURA%20leverages%20both%20the%20dynamic%0Amovement%20patterns%20and%20the%20distinct%20color%20characteristics%20of%20industrial%20smoke%20to%0Aprovide%20enhanced%20accuracy%20and%20reduced%20false%20positives.%20This%20framework%20aims%20to%0Asignificantly%20improve%20environmental%20compliance%2C%20operational%20safety%2C%20and%20public%0Ahealth%20outcomes%20by%20enabling%20precise%2C%20automated%20monitoring%20of%20industrial%0Aemissions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAURA%253A%2520A%2520Hybrid%2520Spatiotemporal-Chromatic%2520Framework%2520for%2520Robust%252C%2520Real-Time%250A%2520%2520Detection%2520of%2520Industrial%2520Smoke%2520Emissions%26entry.906535625%3DMikhail%2520Bychkov%2520and%2520Matey%2520Yordanov%2520and%2520Andrei%2520Kuchma%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520AURA%252C%2520a%2520novel%2520hybrid%2520spatiotemporal-chromatic%2520framework%250Adesigned%2520for%2520robust%252C%2520real-time%2520detection%2520and%2520classification%2520of%2520industrial%2520smoke%250Aemissions.%2520The%2520framework%2520addresses%2520critical%2520limitations%2520of%2520current%2520monitoring%250Asystems%252C%2520which%2520often%2520lack%2520the%2520specificity%2520to%2520distinguish%2520smoke%2520types%2520and%250Astruggle%2520with%2520environmental%2520variability.%2520AURA%2520leverages%2520both%2520the%2520dynamic%250Amovement%2520patterns%2520and%2520the%2520distinct%2520color%2520characteristics%2520of%2520industrial%2520smoke%2520to%250Aprovide%2520enhanced%2520accuracy%2520and%2520reduced%2520false%2520positives.%2520This%2520framework%2520aims%2520to%250Asignificantly%2520improve%2520environmental%2520compliance%252C%2520operational%2520safety%252C%2520and%2520public%250Ahealth%2520outcomes%2520by%2520enabling%2520precise%252C%2520automated%2520monitoring%2520of%2520industrial%250Aemissions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AURA%3A%20A%20Hybrid%20Spatiotemporal-Chromatic%20Framework%20for%20Robust%2C%20Real-Time%0A%20%20Detection%20of%20Industrial%20Smoke%20Emissions&entry.906535625=Mikhail%20Bychkov%20and%20Matey%20Yordanov%20and%20Andrei%20Kuchma&entry.1292438233=%20%20This%20paper%20introduces%20AURA%2C%20a%20novel%20hybrid%20spatiotemporal-chromatic%20framework%0Adesigned%20for%20robust%2C%20real-time%20detection%20and%20classification%20of%20industrial%20smoke%0Aemissions.%20The%20framework%20addresses%20critical%20limitations%20of%20current%20monitoring%0Asystems%2C%20which%20often%20lack%20the%20specificity%20to%20distinguish%20smoke%20types%20and%0Astruggle%20with%20environmental%20variability.%20AURA%20leverages%20both%20the%20dynamic%0Amovement%20patterns%20and%20the%20distinct%20color%20characteristics%20of%20industrial%20smoke%20to%0Aprovide%20enhanced%20accuracy%20and%20reduced%20false%20positives.%20This%20framework%20aims%20to%0Asignificantly%20improve%20environmental%20compliance%2C%20operational%20safety%2C%20and%20public%0Ahealth%20outcomes%20by%20enabling%20precise%2C%20automated%20monitoring%20of%20industrial%0Aemissions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01095v2&entry.124074799=Read"},
{"title": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal\n  Sensor Fusion", "author": "Keivan Faghih Niresi and Ismail Nejjar and Olga Fink", "abstract": "  The growing deployment of low-cost, distributed sensor networks in\nenvironmental and biomedical domains has enabled continuous, large-scale health\nmonitoring. However, these systems often face challenges related to degraded\ndata quality caused by sensor drift, noise, and insufficient calibration --\nfactors that limit their reliability in real-world applications. Traditional\nmachine learning methods for sensor fusion and calibration rely on extensive\nfeature engineering and struggle to capture spatial-temporal dependencies or\nadapt to distribution shifts across varying deployment conditions. To address\nthese challenges, we propose a novel unsupervised domain adaptation (UDA)\nmethod tailored for regression tasks. Our proposed method integrates\neffectively with Spatial-Temporal Graph Neural Networks and leverages the\nalignment of perturbed inverse Gram matrices between source and target domains,\ndrawing inspiration from Tikhonov regularization. This approach enables\nscalable and efficient domain adaptation without requiring labeled data in the\ntarget domain. We validate our novel method on real-world datasets from two\ndistinct applications: air quality monitoring and EEG signal reconstruction.\nOur method achieves state-of-the-art performance which paves the way for more\nrobust and transferable sensor fusion models in both environmental and\nphysiological contexts. Our code is available at\nhttps://github.com/EPFL-IMOS/TikUDA.\n", "link": "http://arxiv.org/abs/2411.06917v2", "date": "2025-08-06", "relevancy": 2.244, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5759}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Unsupervised%20Domain%20Adaptation%20Regression%20for%20Spatial-Temporal%0A%20%20Sensor%20Fusion&body=Title%3A%20Efficient%20Unsupervised%20Domain%20Adaptation%20Regression%20for%20Spatial-Temporal%0A%20%20Sensor%20Fusion%0AAuthor%3A%20Keivan%20Faghih%20Niresi%20and%20Ismail%20Nejjar%20and%20Olga%20Fink%0AAbstract%3A%20%20%20The%20growing%20deployment%20of%20low-cost%2C%20distributed%20sensor%20networks%20in%0Aenvironmental%20and%20biomedical%20domains%20has%20enabled%20continuous%2C%20large-scale%20health%0Amonitoring.%20However%2C%20these%20systems%20often%20face%20challenges%20related%20to%20degraded%0Adata%20quality%20caused%20by%20sensor%20drift%2C%20noise%2C%20and%20insufficient%20calibration%20--%0Afactors%20that%20limit%20their%20reliability%20in%20real-world%20applications.%20Traditional%0Amachine%20learning%20methods%20for%20sensor%20fusion%20and%20calibration%20rely%20on%20extensive%0Afeature%20engineering%20and%20struggle%20to%20capture%20spatial-temporal%20dependencies%20or%0Aadapt%20to%20distribution%20shifts%20across%20varying%20deployment%20conditions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20novel%20unsupervised%20domain%20adaptation%20%28UDA%29%0Amethod%20tailored%20for%20regression%20tasks.%20Our%20proposed%20method%20integrates%0Aeffectively%20with%20Spatial-Temporal%20Graph%20Neural%20Networks%20and%20leverages%20the%0Aalignment%20of%20perturbed%20inverse%20Gram%20matrices%20between%20source%20and%20target%20domains%2C%0Adrawing%20inspiration%20from%20Tikhonov%20regularization.%20This%20approach%20enables%0Ascalable%20and%20efficient%20domain%20adaptation%20without%20requiring%20labeled%20data%20in%20the%0Atarget%20domain.%20We%20validate%20our%20novel%20method%20on%20real-world%20datasets%20from%20two%0Adistinct%20applications%3A%20air%20quality%20monitoring%20and%20EEG%20signal%20reconstruction.%0AOur%20method%20achieves%20state-of-the-art%20performance%20which%20paves%20the%20way%20for%20more%0Arobust%20and%20transferable%20sensor%20fusion%20models%20in%20both%20environmental%20and%0Aphysiological%20contexts.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/EPFL-IMOS/TikUDA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Unsupervised%2520Domain%2520Adaptation%2520Regression%2520for%2520Spatial-Temporal%250A%2520%2520Sensor%2520Fusion%26entry.906535625%3DKeivan%2520Faghih%2520Niresi%2520and%2520Ismail%2520Nejjar%2520and%2520Olga%2520Fink%26entry.1292438233%3D%2520%2520The%2520growing%2520deployment%2520of%2520low-cost%252C%2520distributed%2520sensor%2520networks%2520in%250Aenvironmental%2520and%2520biomedical%2520domains%2520has%2520enabled%2520continuous%252C%2520large-scale%2520health%250Amonitoring.%2520However%252C%2520these%2520systems%2520often%2520face%2520challenges%2520related%2520to%2520degraded%250Adata%2520quality%2520caused%2520by%2520sensor%2520drift%252C%2520noise%252C%2520and%2520insufficient%2520calibration%2520--%250Afactors%2520that%2520limit%2520their%2520reliability%2520in%2520real-world%2520applications.%2520Traditional%250Amachine%2520learning%2520methods%2520for%2520sensor%2520fusion%2520and%2520calibration%2520rely%2520on%2520extensive%250Afeature%2520engineering%2520and%2520struggle%2520to%2520capture%2520spatial-temporal%2520dependencies%2520or%250Aadapt%2520to%2520distribution%2520shifts%2520across%2520varying%2520deployment%2520conditions.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520unsupervised%2520domain%2520adaptation%2520%2528UDA%2529%250Amethod%2520tailored%2520for%2520regression%2520tasks.%2520Our%2520proposed%2520method%2520integrates%250Aeffectively%2520with%2520Spatial-Temporal%2520Graph%2520Neural%2520Networks%2520and%2520leverages%2520the%250Aalignment%2520of%2520perturbed%2520inverse%2520Gram%2520matrices%2520between%2520source%2520and%2520target%2520domains%252C%250Adrawing%2520inspiration%2520from%2520Tikhonov%2520regularization.%2520This%2520approach%2520enables%250Ascalable%2520and%2520efficient%2520domain%2520adaptation%2520without%2520requiring%2520labeled%2520data%2520in%2520the%250Atarget%2520domain.%2520We%2520validate%2520our%2520novel%2520method%2520on%2520real-world%2520datasets%2520from%2520two%250Adistinct%2520applications%253A%2520air%2520quality%2520monitoring%2520and%2520EEG%2520signal%2520reconstruction.%250AOur%2520method%2520achieves%2520state-of-the-art%2520performance%2520which%2520paves%2520the%2520way%2520for%2520more%250Arobust%2520and%2520transferable%2520sensor%2520fusion%2520models%2520in%2520both%2520environmental%2520and%250Aphysiological%2520contexts.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/EPFL-IMOS/TikUDA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Unsupervised%20Domain%20Adaptation%20Regression%20for%20Spatial-Temporal%0A%20%20Sensor%20Fusion&entry.906535625=Keivan%20Faghih%20Niresi%20and%20Ismail%20Nejjar%20and%20Olga%20Fink&entry.1292438233=%20%20The%20growing%20deployment%20of%20low-cost%2C%20distributed%20sensor%20networks%20in%0Aenvironmental%20and%20biomedical%20domains%20has%20enabled%20continuous%2C%20large-scale%20health%0Amonitoring.%20However%2C%20these%20systems%20often%20face%20challenges%20related%20to%20degraded%0Adata%20quality%20caused%20by%20sensor%20drift%2C%20noise%2C%20and%20insufficient%20calibration%20--%0Afactors%20that%20limit%20their%20reliability%20in%20real-world%20applications.%20Traditional%0Amachine%20learning%20methods%20for%20sensor%20fusion%20and%20calibration%20rely%20on%20extensive%0Afeature%20engineering%20and%20struggle%20to%20capture%20spatial-temporal%20dependencies%20or%0Aadapt%20to%20distribution%20shifts%20across%20varying%20deployment%20conditions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20novel%20unsupervised%20domain%20adaptation%20%28UDA%29%0Amethod%20tailored%20for%20regression%20tasks.%20Our%20proposed%20method%20integrates%0Aeffectively%20with%20Spatial-Temporal%20Graph%20Neural%20Networks%20and%20leverages%20the%0Aalignment%20of%20perturbed%20inverse%20Gram%20matrices%20between%20source%20and%20target%20domains%2C%0Adrawing%20inspiration%20from%20Tikhonov%20regularization.%20This%20approach%20enables%0Ascalable%20and%20efficient%20domain%20adaptation%20without%20requiring%20labeled%20data%20in%20the%0Atarget%20domain.%20We%20validate%20our%20novel%20method%20on%20real-world%20datasets%20from%20two%0Adistinct%20applications%3A%20air%20quality%20monitoring%20and%20EEG%20signal%20reconstruction.%0AOur%20method%20achieves%20state-of-the-art%20performance%20which%20paves%20the%20way%20for%20more%0Arobust%20and%20transferable%20sensor%20fusion%20models%20in%20both%20environmental%20and%0Aphysiological%20contexts.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/EPFL-IMOS/TikUDA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06917v2&entry.124074799=Read"},
{"title": "Decoding the Multimodal Maze: A Systematic Review on the Adoption of\n  Explainability in Multimodal Attention-based Models", "author": "Md Raisul Kibria and S\u00e9bastien Lafond and Janan Arslan", "abstract": "  Multimodal learning has witnessed remarkable advancements in recent years,\nparticularly with the integration of attention-based models, leading to\nsignificant performance gains across a variety of tasks. Parallel to this\nprogress, the demand for explainable artificial intelligence (XAI) has spurred\na growing body of research aimed at interpreting the complex decision-making\nprocesses of these models. This systematic literature review analyzes research\npublished between January 2020 and early 2024 that focuses on the\nexplainability of multimodal models. Framed within the broader goals of XAI, we\nexamine the literature across multiple dimensions, including model\narchitecture, modalities involved, explanation algorithms and evaluation\nmethodologies. Our analysis reveals that the majority of studies are\nconcentrated on vision-language and language-only models, with attention-based\ntechniques being the most commonly employed for explanation. However, these\nmethods often fall short in capturing the full spectrum of interactions between\nmodalities, a challenge further compounded by the architectural heterogeneity\nacross domains. Importantly, we find that evaluation methods for XAI in\nmultimodal settings are largely non-systematic, lacking consistency,\nrobustness, and consideration for modality-specific cognitive and contextual\nfactors. Based on these findings, we provide a comprehensive set of\nrecommendations aimed at promoting rigorous, transparent, and standardized\nevaluation and reporting practices in multimodal XAI research. Our goal is to\nsupport future research in more interpretable, accountable, and responsible\nmulitmodal AI systems, with explainability at their core.\n", "link": "http://arxiv.org/abs/2508.04427v1", "date": "2025-08-06", "relevancy": 2.2424, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20the%20Multimodal%20Maze%3A%20A%20Systematic%20Review%20on%20the%20Adoption%20of%0A%20%20Explainability%20in%20Multimodal%20Attention-based%20Models&body=Title%3A%20Decoding%20the%20Multimodal%20Maze%3A%20A%20Systematic%20Review%20on%20the%20Adoption%20of%0A%20%20Explainability%20in%20Multimodal%20Attention-based%20Models%0AAuthor%3A%20Md%20Raisul%20Kibria%20and%20S%C3%A9bastien%20Lafond%20and%20Janan%20Arslan%0AAbstract%3A%20%20%20Multimodal%20learning%20has%20witnessed%20remarkable%20advancements%20in%20recent%20years%2C%0Aparticularly%20with%20the%20integration%20of%20attention-based%20models%2C%20leading%20to%0Asignificant%20performance%20gains%20across%20a%20variety%20of%20tasks.%20Parallel%20to%20this%0Aprogress%2C%20the%20demand%20for%20explainable%20artificial%20intelligence%20%28XAI%29%20has%20spurred%0Aa%20growing%20body%20of%20research%20aimed%20at%20interpreting%20the%20complex%20decision-making%0Aprocesses%20of%20these%20models.%20This%20systematic%20literature%20review%20analyzes%20research%0Apublished%20between%20January%202020%20and%20early%202024%20that%20focuses%20on%20the%0Aexplainability%20of%20multimodal%20models.%20Framed%20within%20the%20broader%20goals%20of%20XAI%2C%20we%0Aexamine%20the%20literature%20across%20multiple%20dimensions%2C%20including%20model%0Aarchitecture%2C%20modalities%20involved%2C%20explanation%20algorithms%20and%20evaluation%0Amethodologies.%20Our%20analysis%20reveals%20that%20the%20majority%20of%20studies%20are%0Aconcentrated%20on%20vision-language%20and%20language-only%20models%2C%20with%20attention-based%0Atechniques%20being%20the%20most%20commonly%20employed%20for%20explanation.%20However%2C%20these%0Amethods%20often%20fall%20short%20in%20capturing%20the%20full%20spectrum%20of%20interactions%20between%0Amodalities%2C%20a%20challenge%20further%20compounded%20by%20the%20architectural%20heterogeneity%0Aacross%20domains.%20Importantly%2C%20we%20find%20that%20evaluation%20methods%20for%20XAI%20in%0Amultimodal%20settings%20are%20largely%20non-systematic%2C%20lacking%20consistency%2C%0Arobustness%2C%20and%20consideration%20for%20modality-specific%20cognitive%20and%20contextual%0Afactors.%20Based%20on%20these%20findings%2C%20we%20provide%20a%20comprehensive%20set%20of%0Arecommendations%20aimed%20at%20promoting%20rigorous%2C%20transparent%2C%20and%20standardized%0Aevaluation%20and%20reporting%20practices%20in%20multimodal%20XAI%20research.%20Our%20goal%20is%20to%0Asupport%20future%20research%20in%20more%20interpretable%2C%20accountable%2C%20and%20responsible%0Amulitmodal%20AI%20systems%2C%20with%20explainability%20at%20their%20core.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520the%2520Multimodal%2520Maze%253A%2520A%2520Systematic%2520Review%2520on%2520the%2520Adoption%2520of%250A%2520%2520Explainability%2520in%2520Multimodal%2520Attention-based%2520Models%26entry.906535625%3DMd%2520Raisul%2520Kibria%2520and%2520S%25C3%25A9bastien%2520Lafond%2520and%2520Janan%2520Arslan%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520has%2520witnessed%2520remarkable%2520advancements%2520in%2520recent%2520years%252C%250Aparticularly%2520with%2520the%2520integration%2520of%2520attention-based%2520models%252C%2520leading%2520to%250Asignificant%2520performance%2520gains%2520across%2520a%2520variety%2520of%2520tasks.%2520Parallel%2520to%2520this%250Aprogress%252C%2520the%2520demand%2520for%2520explainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520has%2520spurred%250Aa%2520growing%2520body%2520of%2520research%2520aimed%2520at%2520interpreting%2520the%2520complex%2520decision-making%250Aprocesses%2520of%2520these%2520models.%2520This%2520systematic%2520literature%2520review%2520analyzes%2520research%250Apublished%2520between%2520January%25202020%2520and%2520early%25202024%2520that%2520focuses%2520on%2520the%250Aexplainability%2520of%2520multimodal%2520models.%2520Framed%2520within%2520the%2520broader%2520goals%2520of%2520XAI%252C%2520we%250Aexamine%2520the%2520literature%2520across%2520multiple%2520dimensions%252C%2520including%2520model%250Aarchitecture%252C%2520modalities%2520involved%252C%2520explanation%2520algorithms%2520and%2520evaluation%250Amethodologies.%2520Our%2520analysis%2520reveals%2520that%2520the%2520majority%2520of%2520studies%2520are%250Aconcentrated%2520on%2520vision-language%2520and%2520language-only%2520models%252C%2520with%2520attention-based%250Atechniques%2520being%2520the%2520most%2520commonly%2520employed%2520for%2520explanation.%2520However%252C%2520these%250Amethods%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520full%2520spectrum%2520of%2520interactions%2520between%250Amodalities%252C%2520a%2520challenge%2520further%2520compounded%2520by%2520the%2520architectural%2520heterogeneity%250Aacross%2520domains.%2520Importantly%252C%2520we%2520find%2520that%2520evaluation%2520methods%2520for%2520XAI%2520in%250Amultimodal%2520settings%2520are%2520largely%2520non-systematic%252C%2520lacking%2520consistency%252C%250Arobustness%252C%2520and%2520consideration%2520for%2520modality-specific%2520cognitive%2520and%2520contextual%250Afactors.%2520Based%2520on%2520these%2520findings%252C%2520we%2520provide%2520a%2520comprehensive%2520set%2520of%250Arecommendations%2520aimed%2520at%2520promoting%2520rigorous%252C%2520transparent%252C%2520and%2520standardized%250Aevaluation%2520and%2520reporting%2520practices%2520in%2520multimodal%2520XAI%2520research.%2520Our%2520goal%2520is%2520to%250Asupport%2520future%2520research%2520in%2520more%2520interpretable%252C%2520accountable%252C%2520and%2520responsible%250Amulitmodal%2520AI%2520systems%252C%2520with%2520explainability%2520at%2520their%2520core.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20the%20Multimodal%20Maze%3A%20A%20Systematic%20Review%20on%20the%20Adoption%20of%0A%20%20Explainability%20in%20Multimodal%20Attention-based%20Models&entry.906535625=Md%20Raisul%20Kibria%20and%20S%C3%A9bastien%20Lafond%20and%20Janan%20Arslan&entry.1292438233=%20%20Multimodal%20learning%20has%20witnessed%20remarkable%20advancements%20in%20recent%20years%2C%0Aparticularly%20with%20the%20integration%20of%20attention-based%20models%2C%20leading%20to%0Asignificant%20performance%20gains%20across%20a%20variety%20of%20tasks.%20Parallel%20to%20this%0Aprogress%2C%20the%20demand%20for%20explainable%20artificial%20intelligence%20%28XAI%29%20has%20spurred%0Aa%20growing%20body%20of%20research%20aimed%20at%20interpreting%20the%20complex%20decision-making%0Aprocesses%20of%20these%20models.%20This%20systematic%20literature%20review%20analyzes%20research%0Apublished%20between%20January%202020%20and%20early%202024%20that%20focuses%20on%20the%0Aexplainability%20of%20multimodal%20models.%20Framed%20within%20the%20broader%20goals%20of%20XAI%2C%20we%0Aexamine%20the%20literature%20across%20multiple%20dimensions%2C%20including%20model%0Aarchitecture%2C%20modalities%20involved%2C%20explanation%20algorithms%20and%20evaluation%0Amethodologies.%20Our%20analysis%20reveals%20that%20the%20majority%20of%20studies%20are%0Aconcentrated%20on%20vision-language%20and%20language-only%20models%2C%20with%20attention-based%0Atechniques%20being%20the%20most%20commonly%20employed%20for%20explanation.%20However%2C%20these%0Amethods%20often%20fall%20short%20in%20capturing%20the%20full%20spectrum%20of%20interactions%20between%0Amodalities%2C%20a%20challenge%20further%20compounded%20by%20the%20architectural%20heterogeneity%0Aacross%20domains.%20Importantly%2C%20we%20find%20that%20evaluation%20methods%20for%20XAI%20in%0Amultimodal%20settings%20are%20largely%20non-systematic%2C%20lacking%20consistency%2C%0Arobustness%2C%20and%20consideration%20for%20modality-specific%20cognitive%20and%20contextual%0Afactors.%20Based%20on%20these%20findings%2C%20we%20provide%20a%20comprehensive%20set%20of%0Arecommendations%20aimed%20at%20promoting%20rigorous%2C%20transparent%2C%20and%20standardized%0Aevaluation%20and%20reporting%20practices%20in%20multimodal%20XAI%20research.%20Our%20goal%20is%20to%0Asupport%20future%20research%20in%20more%20interpretable%2C%20accountable%2C%20and%20responsible%0Amulitmodal%20AI%20systems%2C%20with%20explainability%20at%20their%20core.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04427v1&entry.124074799=Read"},
{"title": "LA-CaRe-CNN: Cascading Refinement CNN for Left Atrial Scar Segmentation", "author": "Franz Thaler and Darko Stern and Gernot Plank and Martin Urschler", "abstract": "  Atrial fibrillation (AF) represents the most prevalent type of cardiac\narrhythmia for which treatment may require patients to undergo ablation\ntherapy. In this surgery cardiac tissues are locally scarred on purpose to\nprevent electrical signals from causing arrhythmia. Patient-specific cardiac\ndigital twin models show great potential for personalized ablation therapy,\nhowever, they demand accurate semantic segmentation of healthy and scarred\ntissue typically obtained from late gadolinium enhanced (LGE) magnetic\nresonance (MR) scans. In this work we propose the Left Atrial Cascading\nRefinement CNN (LA-CaRe-CNN), which aims to accurately segment the left atrium\nas well as left atrial scar tissue from LGE MR scans. LA-CaRe-CNN is a 2-stage\nCNN cascade that is trained end-to-end in 3D, where Stage 1 generates a\nprediction for the left atrium, which is then refined in Stage 2 in conjunction\nwith the original image information to obtain a prediction for the left atrial\nscar tissue. To account for domain shift towards domains unknown during\ntraining, we employ strong intensity and spatial augmentation to increase the\ndiversity of the training dataset. Our proposed method based on a 5-fold\nensemble achieves great segmentation results, namely, 89.21% DSC and 1.6969 mm\nASSD for the left atrium, as well as 64.59% DSC and 91.80% G-DSC for the more\nchallenging left atrial scar tissue. Thus, segmentations obtained through\nLA-CaRe-CNN show great potential for the generation of patient-specific cardiac\ndigital twin models and downstream tasks like personalized targeted ablation\ntherapy to treat AF.\n", "link": "http://arxiv.org/abs/2508.04553v1", "date": "2025-08-06", "relevancy": 2.2414, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4562}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.445}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LA-CaRe-CNN%3A%20Cascading%20Refinement%20CNN%20for%20Left%20Atrial%20Scar%20Segmentation&body=Title%3A%20LA-CaRe-CNN%3A%20Cascading%20Refinement%20CNN%20for%20Left%20Atrial%20Scar%20Segmentation%0AAuthor%3A%20Franz%20Thaler%20and%20Darko%20Stern%20and%20Gernot%20Plank%20and%20Martin%20Urschler%0AAbstract%3A%20%20%20Atrial%20fibrillation%20%28AF%29%20represents%20the%20most%20prevalent%20type%20of%20cardiac%0Aarrhythmia%20for%20which%20treatment%20may%20require%20patients%20to%20undergo%20ablation%0Atherapy.%20In%20this%20surgery%20cardiac%20tissues%20are%20locally%20scarred%20on%20purpose%20to%0Aprevent%20electrical%20signals%20from%20causing%20arrhythmia.%20Patient-specific%20cardiac%0Adigital%20twin%20models%20show%20great%20potential%20for%20personalized%20ablation%20therapy%2C%0Ahowever%2C%20they%20demand%20accurate%20semantic%20segmentation%20of%20healthy%20and%20scarred%0Atissue%20typically%20obtained%20from%20late%20gadolinium%20enhanced%20%28LGE%29%20magnetic%0Aresonance%20%28MR%29%20scans.%20In%20this%20work%20we%20propose%20the%20Left%20Atrial%20Cascading%0ARefinement%20CNN%20%28LA-CaRe-CNN%29%2C%20which%20aims%20to%20accurately%20segment%20the%20left%20atrium%0Aas%20well%20as%20left%20atrial%20scar%20tissue%20from%20LGE%20MR%20scans.%20LA-CaRe-CNN%20is%20a%202-stage%0ACNN%20cascade%20that%20is%20trained%20end-to-end%20in%203D%2C%20where%20Stage%201%20generates%20a%0Aprediction%20for%20the%20left%20atrium%2C%20which%20is%20then%20refined%20in%20Stage%202%20in%20conjunction%0Awith%20the%20original%20image%20information%20to%20obtain%20a%20prediction%20for%20the%20left%20atrial%0Ascar%20tissue.%20To%20account%20for%20domain%20shift%20towards%20domains%20unknown%20during%0Atraining%2C%20we%20employ%20strong%20intensity%20and%20spatial%20augmentation%20to%20increase%20the%0Adiversity%20of%20the%20training%20dataset.%20Our%20proposed%20method%20based%20on%20a%205-fold%0Aensemble%20achieves%20great%20segmentation%20results%2C%20namely%2C%2089.21%25%20DSC%20and%201.6969%20mm%0AASSD%20for%20the%20left%20atrium%2C%20as%20well%20as%2064.59%25%20DSC%20and%2091.80%25%20G-DSC%20for%20the%20more%0Achallenging%20left%20atrial%20scar%20tissue.%20Thus%2C%20segmentations%20obtained%20through%0ALA-CaRe-CNN%20show%20great%20potential%20for%20the%20generation%20of%20patient-specific%20cardiac%0Adigital%20twin%20models%20and%20downstream%20tasks%20like%20personalized%20targeted%20ablation%0Atherapy%20to%20treat%20AF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLA-CaRe-CNN%253A%2520Cascading%2520Refinement%2520CNN%2520for%2520Left%2520Atrial%2520Scar%2520Segmentation%26entry.906535625%3DFranz%2520Thaler%2520and%2520Darko%2520Stern%2520and%2520Gernot%2520Plank%2520and%2520Martin%2520Urschler%26entry.1292438233%3D%2520%2520Atrial%2520fibrillation%2520%2528AF%2529%2520represents%2520the%2520most%2520prevalent%2520type%2520of%2520cardiac%250Aarrhythmia%2520for%2520which%2520treatment%2520may%2520require%2520patients%2520to%2520undergo%2520ablation%250Atherapy.%2520In%2520this%2520surgery%2520cardiac%2520tissues%2520are%2520locally%2520scarred%2520on%2520purpose%2520to%250Aprevent%2520electrical%2520signals%2520from%2520causing%2520arrhythmia.%2520Patient-specific%2520cardiac%250Adigital%2520twin%2520models%2520show%2520great%2520potential%2520for%2520personalized%2520ablation%2520therapy%252C%250Ahowever%252C%2520they%2520demand%2520accurate%2520semantic%2520segmentation%2520of%2520healthy%2520and%2520scarred%250Atissue%2520typically%2520obtained%2520from%2520late%2520gadolinium%2520enhanced%2520%2528LGE%2529%2520magnetic%250Aresonance%2520%2528MR%2529%2520scans.%2520In%2520this%2520work%2520we%2520propose%2520the%2520Left%2520Atrial%2520Cascading%250ARefinement%2520CNN%2520%2528LA-CaRe-CNN%2529%252C%2520which%2520aims%2520to%2520accurately%2520segment%2520the%2520left%2520atrium%250Aas%2520well%2520as%2520left%2520atrial%2520scar%2520tissue%2520from%2520LGE%2520MR%2520scans.%2520LA-CaRe-CNN%2520is%2520a%25202-stage%250ACNN%2520cascade%2520that%2520is%2520trained%2520end-to-end%2520in%25203D%252C%2520where%2520Stage%25201%2520generates%2520a%250Aprediction%2520for%2520the%2520left%2520atrium%252C%2520which%2520is%2520then%2520refined%2520in%2520Stage%25202%2520in%2520conjunction%250Awith%2520the%2520original%2520image%2520information%2520to%2520obtain%2520a%2520prediction%2520for%2520the%2520left%2520atrial%250Ascar%2520tissue.%2520To%2520account%2520for%2520domain%2520shift%2520towards%2520domains%2520unknown%2520during%250Atraining%252C%2520we%2520employ%2520strong%2520intensity%2520and%2520spatial%2520augmentation%2520to%2520increase%2520the%250Adiversity%2520of%2520the%2520training%2520dataset.%2520Our%2520proposed%2520method%2520based%2520on%2520a%25205-fold%250Aensemble%2520achieves%2520great%2520segmentation%2520results%252C%2520namely%252C%252089.21%2525%2520DSC%2520and%25201.6969%2520mm%250AASSD%2520for%2520the%2520left%2520atrium%252C%2520as%2520well%2520as%252064.59%2525%2520DSC%2520and%252091.80%2525%2520G-DSC%2520for%2520the%2520more%250Achallenging%2520left%2520atrial%2520scar%2520tissue.%2520Thus%252C%2520segmentations%2520obtained%2520through%250ALA-CaRe-CNN%2520show%2520great%2520potential%2520for%2520the%2520generation%2520of%2520patient-specific%2520cardiac%250Adigital%2520twin%2520models%2520and%2520downstream%2520tasks%2520like%2520personalized%2520targeted%2520ablation%250Atherapy%2520to%2520treat%2520AF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LA-CaRe-CNN%3A%20Cascading%20Refinement%20CNN%20for%20Left%20Atrial%20Scar%20Segmentation&entry.906535625=Franz%20Thaler%20and%20Darko%20Stern%20and%20Gernot%20Plank%20and%20Martin%20Urschler&entry.1292438233=%20%20Atrial%20fibrillation%20%28AF%29%20represents%20the%20most%20prevalent%20type%20of%20cardiac%0Aarrhythmia%20for%20which%20treatment%20may%20require%20patients%20to%20undergo%20ablation%0Atherapy.%20In%20this%20surgery%20cardiac%20tissues%20are%20locally%20scarred%20on%20purpose%20to%0Aprevent%20electrical%20signals%20from%20causing%20arrhythmia.%20Patient-specific%20cardiac%0Adigital%20twin%20models%20show%20great%20potential%20for%20personalized%20ablation%20therapy%2C%0Ahowever%2C%20they%20demand%20accurate%20semantic%20segmentation%20of%20healthy%20and%20scarred%0Atissue%20typically%20obtained%20from%20late%20gadolinium%20enhanced%20%28LGE%29%20magnetic%0Aresonance%20%28MR%29%20scans.%20In%20this%20work%20we%20propose%20the%20Left%20Atrial%20Cascading%0ARefinement%20CNN%20%28LA-CaRe-CNN%29%2C%20which%20aims%20to%20accurately%20segment%20the%20left%20atrium%0Aas%20well%20as%20left%20atrial%20scar%20tissue%20from%20LGE%20MR%20scans.%20LA-CaRe-CNN%20is%20a%202-stage%0ACNN%20cascade%20that%20is%20trained%20end-to-end%20in%203D%2C%20where%20Stage%201%20generates%20a%0Aprediction%20for%20the%20left%20atrium%2C%20which%20is%20then%20refined%20in%20Stage%202%20in%20conjunction%0Awith%20the%20original%20image%20information%20to%20obtain%20a%20prediction%20for%20the%20left%20atrial%0Ascar%20tissue.%20To%20account%20for%20domain%20shift%20towards%20domains%20unknown%20during%0Atraining%2C%20we%20employ%20strong%20intensity%20and%20spatial%20augmentation%20to%20increase%20the%0Adiversity%20of%20the%20training%20dataset.%20Our%20proposed%20method%20based%20on%20a%205-fold%0Aensemble%20achieves%20great%20segmentation%20results%2C%20namely%2C%2089.21%25%20DSC%20and%201.6969%20mm%0AASSD%20for%20the%20left%20atrium%2C%20as%20well%20as%2064.59%25%20DSC%20and%2091.80%25%20G-DSC%20for%20the%20more%0Achallenging%20left%20atrial%20scar%20tissue.%20Thus%2C%20segmentations%20obtained%20through%0ALA-CaRe-CNN%20show%20great%20potential%20for%20the%20generation%20of%20patient-specific%20cardiac%0Adigital%20twin%20models%20and%20downstream%20tasks%20like%20personalized%20targeted%20ablation%0Atherapy%20to%20treat%20AF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04553v1&entry.124074799=Read"},
{"title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring\n  Audio-Visual Segmentation", "author": "Jinxing Zhou and Yanghao Zhou and Mingfei Han and Tong Wang and Xiaojun Chang and Hisham Cholakkal and Rao Muhammad Anwer", "abstract": "  Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects\nin audible videos based on given reference expressions. Prior works typically\nrely on learning latent embeddings via multimodal fusion to prompt a tunable\nSAM/SAM2 decoder for segmentation, which requires strong pixel-level\nsupervision and lacks interpretability. From a novel perspective of explicit\nreference understanding, we propose TGS-Agent, which decomposes the task into a\nThink-Ground-Segment process, mimicking the human reasoning procedure by first\nidentifying the referred object through multimodal analysis, followed by\ncoarse-grained grounding and precise segmentation. To this end, we first\npropose Ref-Thinker, a multimodal language model capable of reasoning over\ntextual, visual, and auditory cues. We construct an instruction-tuning dataset\nwith explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The\nobject description inferred by Ref-Thinker is used as an explicit prompt for\nGrounding-DINO and SAM2, which perform grounding and segmentation without\nrelying on pixel-level supervision. Additionally, we introduce\nR\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and\nreasoning-intensive references for better evaluating model generalization. Our\napproach achieves state-of-the-art results on both standard Ref-AVSBench and\nproposed R\\textsuperscript{2}-AVSBench. Code will be available at\nhttps://github.com/jasongief/TGS-Agent.\n", "link": "http://arxiv.org/abs/2508.04418v1", "date": "2025-08-06", "relevancy": 2.2347, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think%20Before%20You%20Segment%3A%20An%20Object-aware%20Reasoning%20Agent%20for%20Referring%0A%20%20Audio-Visual%20Segmentation&body=Title%3A%20Think%20Before%20You%20Segment%3A%20An%20Object-aware%20Reasoning%20Agent%20for%20Referring%0A%20%20Audio-Visual%20Segmentation%0AAuthor%3A%20Jinxing%20Zhou%20and%20Yanghao%20Zhou%20and%20Mingfei%20Han%20and%20Tong%20Wang%20and%20Xiaojun%20Chang%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%0AAbstract%3A%20%20%20Referring%20Audio-Visual%20Segmentation%20%28Ref-AVS%29%20aims%20to%20segment%20target%20objects%0Ain%20audible%20videos%20based%20on%20given%20reference%20expressions.%20Prior%20works%20typically%0Arely%20on%20learning%20latent%20embeddings%20via%20multimodal%20fusion%20to%20prompt%20a%20tunable%0ASAM/SAM2%20decoder%20for%20segmentation%2C%20which%20requires%20strong%20pixel-level%0Asupervision%20and%20lacks%20interpretability.%20From%20a%20novel%20perspective%20of%20explicit%0Areference%20understanding%2C%20we%20propose%20TGS-Agent%2C%20which%20decomposes%20the%20task%20into%20a%0AThink-Ground-Segment%20process%2C%20mimicking%20the%20human%20reasoning%20procedure%20by%20first%0Aidentifying%20the%20referred%20object%20through%20multimodal%20analysis%2C%20followed%20by%0Acoarse-grained%20grounding%20and%20precise%20segmentation.%20To%20this%20end%2C%20we%20first%0Apropose%20Ref-Thinker%2C%20a%20multimodal%20language%20model%20capable%20of%20reasoning%20over%0Atextual%2C%20visual%2C%20and%20auditory%20cues.%20We%20construct%20an%20instruction-tuning%20dataset%0Awith%20explicit%20object-aware%20think-answer%20chains%20for%20Ref-Thinker%20fine-tuning.%20The%0Aobject%20description%20inferred%20by%20Ref-Thinker%20is%20used%20as%20an%20explicit%20prompt%20for%0AGrounding-DINO%20and%20SAM2%2C%20which%20perform%20grounding%20and%20segmentation%20without%0Arelying%20on%20pixel-level%20supervision.%20Additionally%2C%20we%20introduce%0AR%5Ctextsuperscript%7B2%7D-AVSBench%2C%20a%20new%20benchmark%20with%20linguistically%20diverse%20and%0Areasoning-intensive%20references%20for%20better%20evaluating%20model%20generalization.%20Our%0Aapproach%20achieves%20state-of-the-art%20results%20on%20both%20standard%20Ref-AVSBench%20and%0Aproposed%20R%5Ctextsuperscript%7B2%7D-AVSBench.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/jasongief/TGS-Agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink%2520Before%2520You%2520Segment%253A%2520An%2520Object-aware%2520Reasoning%2520Agent%2520for%2520Referring%250A%2520%2520Audio-Visual%2520Segmentation%26entry.906535625%3DJinxing%2520Zhou%2520and%2520Yanghao%2520Zhou%2520and%2520Mingfei%2520Han%2520and%2520Tong%2520Wang%2520and%2520Xiaojun%2520Chang%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%26entry.1292438233%3D%2520%2520Referring%2520Audio-Visual%2520Segmentation%2520%2528Ref-AVS%2529%2520aims%2520to%2520segment%2520target%2520objects%250Ain%2520audible%2520videos%2520based%2520on%2520given%2520reference%2520expressions.%2520Prior%2520works%2520typically%250Arely%2520on%2520learning%2520latent%2520embeddings%2520via%2520multimodal%2520fusion%2520to%2520prompt%2520a%2520tunable%250ASAM/SAM2%2520decoder%2520for%2520segmentation%252C%2520which%2520requires%2520strong%2520pixel-level%250Asupervision%2520and%2520lacks%2520interpretability.%2520From%2520a%2520novel%2520perspective%2520of%2520explicit%250Areference%2520understanding%252C%2520we%2520propose%2520TGS-Agent%252C%2520which%2520decomposes%2520the%2520task%2520into%2520a%250AThink-Ground-Segment%2520process%252C%2520mimicking%2520the%2520human%2520reasoning%2520procedure%2520by%2520first%250Aidentifying%2520the%2520referred%2520object%2520through%2520multimodal%2520analysis%252C%2520followed%2520by%250Acoarse-grained%2520grounding%2520and%2520precise%2520segmentation.%2520To%2520this%2520end%252C%2520we%2520first%250Apropose%2520Ref-Thinker%252C%2520a%2520multimodal%2520language%2520model%2520capable%2520of%2520reasoning%2520over%250Atextual%252C%2520visual%252C%2520and%2520auditory%2520cues.%2520We%2520construct%2520an%2520instruction-tuning%2520dataset%250Awith%2520explicit%2520object-aware%2520think-answer%2520chains%2520for%2520Ref-Thinker%2520fine-tuning.%2520The%250Aobject%2520description%2520inferred%2520by%2520Ref-Thinker%2520is%2520used%2520as%2520an%2520explicit%2520prompt%2520for%250AGrounding-DINO%2520and%2520SAM2%252C%2520which%2520perform%2520grounding%2520and%2520segmentation%2520without%250Arelying%2520on%2520pixel-level%2520supervision.%2520Additionally%252C%2520we%2520introduce%250AR%255Ctextsuperscript%257B2%257D-AVSBench%252C%2520a%2520new%2520benchmark%2520with%2520linguistically%2520diverse%2520and%250Areasoning-intensive%2520references%2520for%2520better%2520evaluating%2520model%2520generalization.%2520Our%250Aapproach%2520achieves%2520state-of-the-art%2520results%2520on%2520both%2520standard%2520Ref-AVSBench%2520and%250Aproposed%2520R%255Ctextsuperscript%257B2%257D-AVSBench.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/jasongief/TGS-Agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think%20Before%20You%20Segment%3A%20An%20Object-aware%20Reasoning%20Agent%20for%20Referring%0A%20%20Audio-Visual%20Segmentation&entry.906535625=Jinxing%20Zhou%20and%20Yanghao%20Zhou%20and%20Mingfei%20Han%20and%20Tong%20Wang%20and%20Xiaojun%20Chang%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer&entry.1292438233=%20%20Referring%20Audio-Visual%20Segmentation%20%28Ref-AVS%29%20aims%20to%20segment%20target%20objects%0Ain%20audible%20videos%20based%20on%20given%20reference%20expressions.%20Prior%20works%20typically%0Arely%20on%20learning%20latent%20embeddings%20via%20multimodal%20fusion%20to%20prompt%20a%20tunable%0ASAM/SAM2%20decoder%20for%20segmentation%2C%20which%20requires%20strong%20pixel-level%0Asupervision%20and%20lacks%20interpretability.%20From%20a%20novel%20perspective%20of%20explicit%0Areference%20understanding%2C%20we%20propose%20TGS-Agent%2C%20which%20decomposes%20the%20task%20into%20a%0AThink-Ground-Segment%20process%2C%20mimicking%20the%20human%20reasoning%20procedure%20by%20first%0Aidentifying%20the%20referred%20object%20through%20multimodal%20analysis%2C%20followed%20by%0Acoarse-grained%20grounding%20and%20precise%20segmentation.%20To%20this%20end%2C%20we%20first%0Apropose%20Ref-Thinker%2C%20a%20multimodal%20language%20model%20capable%20of%20reasoning%20over%0Atextual%2C%20visual%2C%20and%20auditory%20cues.%20We%20construct%20an%20instruction-tuning%20dataset%0Awith%20explicit%20object-aware%20think-answer%20chains%20for%20Ref-Thinker%20fine-tuning.%20The%0Aobject%20description%20inferred%20by%20Ref-Thinker%20is%20used%20as%20an%20explicit%20prompt%20for%0AGrounding-DINO%20and%20SAM2%2C%20which%20perform%20grounding%20and%20segmentation%20without%0Arelying%20on%20pixel-level%20supervision.%20Additionally%2C%20we%20introduce%0AR%5Ctextsuperscript%7B2%7D-AVSBench%2C%20a%20new%20benchmark%20with%20linguistically%20diverse%20and%0Areasoning-intensive%20references%20for%20better%20evaluating%20model%20generalization.%20Our%0Aapproach%20achieves%20state-of-the-art%20results%20on%20both%20standard%20Ref-AVSBench%20and%0Aproposed%20R%5Ctextsuperscript%7B2%7D-AVSBench.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/jasongief/TGS-Agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04418v1&entry.124074799=Read"},
{"title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language\n  Understanding", "author": "Canhui Tang and Zifan Han and Hongbo Sun and Sanping Zhou and Xuchong Zhang and Xin Wei and Ye Yuan and Jinglin Xu and Hao Sun", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in vision-language tasks, yet they still face challenges when\nprocessing long-duration video inputs. The limitation arises from MLLMs'\ncontext limit and training costs, necessitating sparse frame sampling before\nfeeding videos into MLLMs. Existing video MLLMs adopt training-free uniform\nsampling or keyframe search, which may miss critical events or be constrained\nby the pre-trained models' event understanding capabilities. Meanwhile,\nbuilding a training-based method remains challenging due to the unsupervised\nand non-differentiable nature of sparse frame sampling. To address these\nproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancing\nMLLMs' long-form video-language understanding via reinforcement learning.\nSpecifically, we first propose a trainable event-aware temporal agent, which\ncaptures event-query correlation for performing probabilistic keyframe\nselection. Then, we propose the TSPO reinforcement learning paradigm, which\nmodels keyframe selection and language generation as a joint decision-making\nprocess, enabling end-to-end group relative optimization with efficient\nrule-based rewards. Furthermore, for the TSPO's training, we propose a long\nvideo training data construction pipeline with comprehensive temporal data and\nvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answering\naccuracy and temporal locating reward mechanisms to optimize the temporal\nsampling policy. Comprehensive experiments show that our TSPO achieves\nstate-of-the-art performance across multiple long video understanding\nbenchmarks, and shows transferable ability across different cutting-edge\nVideo-MLLMs.\n", "link": "http://arxiv.org/abs/2508.04369v1", "date": "2025-08-06", "relevancy": 2.233, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5728}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSPO%3A%20Temporal%20Sampling%20Policy%20Optimization%20for%20Long-form%20Video%20Language%0A%20%20Understanding&body=Title%3A%20TSPO%3A%20Temporal%20Sampling%20Policy%20Optimization%20for%20Long-form%20Video%20Language%0A%20%20Understanding%0AAuthor%3A%20Canhui%20Tang%20and%20Zifan%20Han%20and%20Hongbo%20Sun%20and%20Sanping%20Zhou%20and%20Xuchong%20Zhang%20and%20Xin%20Wei%20and%20Ye%20Yuan%20and%20Jinglin%20Xu%20and%20Hao%20Sun%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aprogress%20in%20vision-language%20tasks%2C%20yet%20they%20still%20face%20challenges%20when%0Aprocessing%20long-duration%20video%20inputs.%20The%20limitation%20arises%20from%20MLLMs%27%0Acontext%20limit%20and%20training%20costs%2C%20necessitating%20sparse%20frame%20sampling%20before%0Afeeding%20videos%20into%20MLLMs.%20Existing%20video%20MLLMs%20adopt%20training-free%20uniform%0Asampling%20or%20keyframe%20search%2C%20which%20may%20miss%20critical%20events%20or%20be%20constrained%0Aby%20the%20pre-trained%20models%27%20event%20understanding%20capabilities.%20Meanwhile%2C%0Abuilding%20a%20training-based%20method%20remains%20challenging%20due%20to%20the%20unsupervised%0Aand%20non-differentiable%20nature%20of%20sparse%20frame%20sampling.%20To%20address%20these%0Aproblems%2C%20we%20propose%20Temporal%20Sampling%20Policy%20Optimization%20%28TSPO%29%2C%20advancing%0AMLLMs%27%20long-form%20video-language%20understanding%20via%20reinforcement%20learning.%0ASpecifically%2C%20we%20first%20propose%20a%20trainable%20event-aware%20temporal%20agent%2C%20which%0Acaptures%20event-query%20correlation%20for%20performing%20probabilistic%20keyframe%0Aselection.%20Then%2C%20we%20propose%20the%20TSPO%20reinforcement%20learning%20paradigm%2C%20which%0Amodels%20keyframe%20selection%20and%20language%20generation%20as%20a%20joint%20decision-making%0Aprocess%2C%20enabling%20end-to-end%20group%20relative%20optimization%20with%20efficient%0Arule-based%20rewards.%20Furthermore%2C%20for%20the%20TSPO%27s%20training%2C%20we%20propose%20a%20long%0Avideo%20training%20data%20construction%20pipeline%20with%20comprehensive%20temporal%20data%20and%0Avideo%20Needle-in-a-Haystack%20data.%20Finally%2C%20we%20incorporate%20rule-based%20answering%0Aaccuracy%20and%20temporal%20locating%20reward%20mechanisms%20to%20optimize%20the%20temporal%0Asampling%20policy.%20Comprehensive%20experiments%20show%20that%20our%20TSPO%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20long%20video%20understanding%0Abenchmarks%2C%20and%20shows%20transferable%20ability%20across%20different%20cutting-edge%0AVideo-MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSPO%253A%2520Temporal%2520Sampling%2520Policy%2520Optimization%2520for%2520Long-form%2520Video%2520Language%250A%2520%2520Understanding%26entry.906535625%3DCanhui%2520Tang%2520and%2520Zifan%2520Han%2520and%2520Hongbo%2520Sun%2520and%2520Sanping%2520Zhou%2520and%2520Xuchong%2520Zhang%2520and%2520Xin%2520Wei%2520and%2520Ye%2520Yuan%2520and%2520Jinglin%2520Xu%2520and%2520Hao%2520Sun%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%250Aprogress%2520in%2520vision-language%2520tasks%252C%2520yet%2520they%2520still%2520face%2520challenges%2520when%250Aprocessing%2520long-duration%2520video%2520inputs.%2520The%2520limitation%2520arises%2520from%2520MLLMs%2527%250Acontext%2520limit%2520and%2520training%2520costs%252C%2520necessitating%2520sparse%2520frame%2520sampling%2520before%250Afeeding%2520videos%2520into%2520MLLMs.%2520Existing%2520video%2520MLLMs%2520adopt%2520training-free%2520uniform%250Asampling%2520or%2520keyframe%2520search%252C%2520which%2520may%2520miss%2520critical%2520events%2520or%2520be%2520constrained%250Aby%2520the%2520pre-trained%2520models%2527%2520event%2520understanding%2520capabilities.%2520Meanwhile%252C%250Abuilding%2520a%2520training-based%2520method%2520remains%2520challenging%2520due%2520to%2520the%2520unsupervised%250Aand%2520non-differentiable%2520nature%2520of%2520sparse%2520frame%2520sampling.%2520To%2520address%2520these%250Aproblems%252C%2520we%2520propose%2520Temporal%2520Sampling%2520Policy%2520Optimization%2520%2528TSPO%2529%252C%2520advancing%250AMLLMs%2527%2520long-form%2520video-language%2520understanding%2520via%2520reinforcement%2520learning.%250ASpecifically%252C%2520we%2520first%2520propose%2520a%2520trainable%2520event-aware%2520temporal%2520agent%252C%2520which%250Acaptures%2520event-query%2520correlation%2520for%2520performing%2520probabilistic%2520keyframe%250Aselection.%2520Then%252C%2520we%2520propose%2520the%2520TSPO%2520reinforcement%2520learning%2520paradigm%252C%2520which%250Amodels%2520keyframe%2520selection%2520and%2520language%2520generation%2520as%2520a%2520joint%2520decision-making%250Aprocess%252C%2520enabling%2520end-to-end%2520group%2520relative%2520optimization%2520with%2520efficient%250Arule-based%2520rewards.%2520Furthermore%252C%2520for%2520the%2520TSPO%2527s%2520training%252C%2520we%2520propose%2520a%2520long%250Avideo%2520training%2520data%2520construction%2520pipeline%2520with%2520comprehensive%2520temporal%2520data%2520and%250Avideo%2520Needle-in-a-Haystack%2520data.%2520Finally%252C%2520we%2520incorporate%2520rule-based%2520answering%250Aaccuracy%2520and%2520temporal%2520locating%2520reward%2520mechanisms%2520to%2520optimize%2520the%2520temporal%250Asampling%2520policy.%2520Comprehensive%2520experiments%2520show%2520that%2520our%2520TSPO%2520achieves%250Astate-of-the-art%2520performance%2520across%2520multiple%2520long%2520video%2520understanding%250Abenchmarks%252C%2520and%2520shows%2520transferable%2520ability%2520across%2520different%2520cutting-edge%250AVideo-MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSPO%3A%20Temporal%20Sampling%20Policy%20Optimization%20for%20Long-form%20Video%20Language%0A%20%20Understanding&entry.906535625=Canhui%20Tang%20and%20Zifan%20Han%20and%20Hongbo%20Sun%20and%20Sanping%20Zhou%20and%20Xuchong%20Zhang%20and%20Xin%20Wei%20and%20Ye%20Yuan%20and%20Jinglin%20Xu%20and%20Hao%20Sun&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%0Aprogress%20in%20vision-language%20tasks%2C%20yet%20they%20still%20face%20challenges%20when%0Aprocessing%20long-duration%20video%20inputs.%20The%20limitation%20arises%20from%20MLLMs%27%0Acontext%20limit%20and%20training%20costs%2C%20necessitating%20sparse%20frame%20sampling%20before%0Afeeding%20videos%20into%20MLLMs.%20Existing%20video%20MLLMs%20adopt%20training-free%20uniform%0Asampling%20or%20keyframe%20search%2C%20which%20may%20miss%20critical%20events%20or%20be%20constrained%0Aby%20the%20pre-trained%20models%27%20event%20understanding%20capabilities.%20Meanwhile%2C%0Abuilding%20a%20training-based%20method%20remains%20challenging%20due%20to%20the%20unsupervised%0Aand%20non-differentiable%20nature%20of%20sparse%20frame%20sampling.%20To%20address%20these%0Aproblems%2C%20we%20propose%20Temporal%20Sampling%20Policy%20Optimization%20%28TSPO%29%2C%20advancing%0AMLLMs%27%20long-form%20video-language%20understanding%20via%20reinforcement%20learning.%0ASpecifically%2C%20we%20first%20propose%20a%20trainable%20event-aware%20temporal%20agent%2C%20which%0Acaptures%20event-query%20correlation%20for%20performing%20probabilistic%20keyframe%0Aselection.%20Then%2C%20we%20propose%20the%20TSPO%20reinforcement%20learning%20paradigm%2C%20which%0Amodels%20keyframe%20selection%20and%20language%20generation%20as%20a%20joint%20decision-making%0Aprocess%2C%20enabling%20end-to-end%20group%20relative%20optimization%20with%20efficient%0Arule-based%20rewards.%20Furthermore%2C%20for%20the%20TSPO%27s%20training%2C%20we%20propose%20a%20long%0Avideo%20training%20data%20construction%20pipeline%20with%20comprehensive%20temporal%20data%20and%0Avideo%20Needle-in-a-Haystack%20data.%20Finally%2C%20we%20incorporate%20rule-based%20answering%0Aaccuracy%20and%20temporal%20locating%20reward%20mechanisms%20to%20optimize%20the%20temporal%0Asampling%20policy.%20Comprehensive%20experiments%20show%20that%20our%20TSPO%20achieves%0Astate-of-the-art%20performance%20across%20multiple%20long%20video%20understanding%0Abenchmarks%2C%20and%20shows%20transferable%20ability%20across%20different%20cutting-edge%0AVideo-MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04369v1&entry.124074799=Read"},
{"title": "Hierarchical Event Memory for Accurate and Low-latency Online Video\n  Temporal Grounding", "author": "Minghang Zheng and Yuxin Peng and Benyuan Sun and Yi Yang and Yang Liu", "abstract": "  In this paper, we tackle the task of online video temporal grounding (OnVTG),\nwhich requires the model to locate events related to a given text query within\na video stream. Unlike regular video temporal grounding, OnVTG requires the\nmodel to make predictions without observing future frames. As online videos are\nstreaming inputs and can go on indefinitely, it is impractical and inefficient\nto store all historical inputs. The existing OnVTG models employ memory to\nstore recent historical video frame features and predict scores indicating\nwhether the current frame corresponds to the start or end time of the target\nevent. However, these methods lack effective event modeling and cannot retain\nlong-term historical information, leading to low performance. To tackle these\nchallenges, we propose a hierarchical event memory for OnVTG. We propose an\nevent-based OnVTG framework that makes predictions based on event proposals\nthat model event-level information with various durations. To preserve\nhistorically valuable event information, we introduce a hierarchical event\nmemory that retains historical events, allowing the model to access both recent\nand long-term information. To enable the real-time prediction, we further\npropose a future prediction branch that predicts whether the target event will\noccur shortly and further regresses the start time of the event. We achieve\nstate-of-the-art performance on the TACoS, ActivityNet Captions, and MAD\ndatasets. Code is available at https://github.com/minghangz/OnVTG.\n", "link": "http://arxiv.org/abs/2508.04546v1", "date": "2025-08-06", "relevancy": 2.1846, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5635}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5473}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Event%20Memory%20for%20Accurate%20and%20Low-latency%20Online%20Video%0A%20%20Temporal%20Grounding&body=Title%3A%20Hierarchical%20Event%20Memory%20for%20Accurate%20and%20Low-latency%20Online%20Video%0A%20%20Temporal%20Grounding%0AAuthor%3A%20Minghang%20Zheng%20and%20Yuxin%20Peng%20and%20Benyuan%20Sun%20and%20Yi%20Yang%20and%20Yang%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20tackle%20the%20task%20of%20online%20video%20temporal%20grounding%20%28OnVTG%29%2C%0Awhich%20requires%20the%20model%20to%20locate%20events%20related%20to%20a%20given%20text%20query%20within%0Aa%20video%20stream.%20Unlike%20regular%20video%20temporal%20grounding%2C%20OnVTG%20requires%20the%0Amodel%20to%20make%20predictions%20without%20observing%20future%20frames.%20As%20online%20videos%20are%0Astreaming%20inputs%20and%20can%20go%20on%20indefinitely%2C%20it%20is%20impractical%20and%20inefficient%0Ato%20store%20all%20historical%20inputs.%20The%20existing%20OnVTG%20models%20employ%20memory%20to%0Astore%20recent%20historical%20video%20frame%20features%20and%20predict%20scores%20indicating%0Awhether%20the%20current%20frame%20corresponds%20to%20the%20start%20or%20end%20time%20of%20the%20target%0Aevent.%20However%2C%20these%20methods%20lack%20effective%20event%20modeling%20and%20cannot%20retain%0Along-term%20historical%20information%2C%20leading%20to%20low%20performance.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20hierarchical%20event%20memory%20for%20OnVTG.%20We%20propose%20an%0Aevent-based%20OnVTG%20framework%20that%20makes%20predictions%20based%20on%20event%20proposals%0Athat%20model%20event-level%20information%20with%20various%20durations.%20To%20preserve%0Ahistorically%20valuable%20event%20information%2C%20we%20introduce%20a%20hierarchical%20event%0Amemory%20that%20retains%20historical%20events%2C%20allowing%20the%20model%20to%20access%20both%20recent%0Aand%20long-term%20information.%20To%20enable%20the%20real-time%20prediction%2C%20we%20further%0Apropose%20a%20future%20prediction%20branch%20that%20predicts%20whether%20the%20target%20event%20will%0Aoccur%20shortly%20and%20further%20regresses%20the%20start%20time%20of%20the%20event.%20We%20achieve%0Astate-of-the-art%20performance%20on%20the%20TACoS%2C%20ActivityNet%20Captions%2C%20and%20MAD%0Adatasets.%20Code%20is%20available%20at%20https%3A//github.com/minghangz/OnVTG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Event%2520Memory%2520for%2520Accurate%2520and%2520Low-latency%2520Online%2520Video%250A%2520%2520Temporal%2520Grounding%26entry.906535625%3DMinghang%2520Zheng%2520and%2520Yuxin%2520Peng%2520and%2520Benyuan%2520Sun%2520and%2520Yi%2520Yang%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520task%2520of%2520online%2520video%2520temporal%2520grounding%2520%2528OnVTG%2529%252C%250Awhich%2520requires%2520the%2520model%2520to%2520locate%2520events%2520related%2520to%2520a%2520given%2520text%2520query%2520within%250Aa%2520video%2520stream.%2520Unlike%2520regular%2520video%2520temporal%2520grounding%252C%2520OnVTG%2520requires%2520the%250Amodel%2520to%2520make%2520predictions%2520without%2520observing%2520future%2520frames.%2520As%2520online%2520videos%2520are%250Astreaming%2520inputs%2520and%2520can%2520go%2520on%2520indefinitely%252C%2520it%2520is%2520impractical%2520and%2520inefficient%250Ato%2520store%2520all%2520historical%2520inputs.%2520The%2520existing%2520OnVTG%2520models%2520employ%2520memory%2520to%250Astore%2520recent%2520historical%2520video%2520frame%2520features%2520and%2520predict%2520scores%2520indicating%250Awhether%2520the%2520current%2520frame%2520corresponds%2520to%2520the%2520start%2520or%2520end%2520time%2520of%2520the%2520target%250Aevent.%2520However%252C%2520these%2520methods%2520lack%2520effective%2520event%2520modeling%2520and%2520cannot%2520retain%250Along-term%2520historical%2520information%252C%2520leading%2520to%2520low%2520performance.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520hierarchical%2520event%2520memory%2520for%2520OnVTG.%2520We%2520propose%2520an%250Aevent-based%2520OnVTG%2520framework%2520that%2520makes%2520predictions%2520based%2520on%2520event%2520proposals%250Athat%2520model%2520event-level%2520information%2520with%2520various%2520durations.%2520To%2520preserve%250Ahistorically%2520valuable%2520event%2520information%252C%2520we%2520introduce%2520a%2520hierarchical%2520event%250Amemory%2520that%2520retains%2520historical%2520events%252C%2520allowing%2520the%2520model%2520to%2520access%2520both%2520recent%250Aand%2520long-term%2520information.%2520To%2520enable%2520the%2520real-time%2520prediction%252C%2520we%2520further%250Apropose%2520a%2520future%2520prediction%2520branch%2520that%2520predicts%2520whether%2520the%2520target%2520event%2520will%250Aoccur%2520shortly%2520and%2520further%2520regresses%2520the%2520start%2520time%2520of%2520the%2520event.%2520We%2520achieve%250Astate-of-the-art%2520performance%2520on%2520the%2520TACoS%252C%2520ActivityNet%2520Captions%252C%2520and%2520MAD%250Adatasets.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/minghangz/OnVTG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Event%20Memory%20for%20Accurate%20and%20Low-latency%20Online%20Video%0A%20%20Temporal%20Grounding&entry.906535625=Minghang%20Zheng%20and%20Yuxin%20Peng%20and%20Benyuan%20Sun%20and%20Yi%20Yang%20and%20Yang%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20tackle%20the%20task%20of%20online%20video%20temporal%20grounding%20%28OnVTG%29%2C%0Awhich%20requires%20the%20model%20to%20locate%20events%20related%20to%20a%20given%20text%20query%20within%0Aa%20video%20stream.%20Unlike%20regular%20video%20temporal%20grounding%2C%20OnVTG%20requires%20the%0Amodel%20to%20make%20predictions%20without%20observing%20future%20frames.%20As%20online%20videos%20are%0Astreaming%20inputs%20and%20can%20go%20on%20indefinitely%2C%20it%20is%20impractical%20and%20inefficient%0Ato%20store%20all%20historical%20inputs.%20The%20existing%20OnVTG%20models%20employ%20memory%20to%0Astore%20recent%20historical%20video%20frame%20features%20and%20predict%20scores%20indicating%0Awhether%20the%20current%20frame%20corresponds%20to%20the%20start%20or%20end%20time%20of%20the%20target%0Aevent.%20However%2C%20these%20methods%20lack%20effective%20event%20modeling%20and%20cannot%20retain%0Along-term%20historical%20information%2C%20leading%20to%20low%20performance.%20To%20tackle%20these%0Achallenges%2C%20we%20propose%20a%20hierarchical%20event%20memory%20for%20OnVTG.%20We%20propose%20an%0Aevent-based%20OnVTG%20framework%20that%20makes%20predictions%20based%20on%20event%20proposals%0Athat%20model%20event-level%20information%20with%20various%20durations.%20To%20preserve%0Ahistorically%20valuable%20event%20information%2C%20we%20introduce%20a%20hierarchical%20event%0Amemory%20that%20retains%20historical%20events%2C%20allowing%20the%20model%20to%20access%20both%20recent%0Aand%20long-term%20information.%20To%20enable%20the%20real-time%20prediction%2C%20we%20further%0Apropose%20a%20future%20prediction%20branch%20that%20predicts%20whether%20the%20target%20event%20will%0Aoccur%20shortly%20and%20further%20regresses%20the%20start%20time%20of%20the%20event.%20We%20achieve%0Astate-of-the-art%20performance%20on%20the%20TACoS%2C%20ActivityNet%20Captions%2C%20and%20MAD%0Adatasets.%20Code%20is%20available%20at%20https%3A//github.com/minghangz/OnVTG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04546v1&entry.124074799=Read"},
{"title": "Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation", "author": "Askar Tsyganov and Evgeny Frolov and Sergey Samsonov and Maxim Rakhuba", "abstract": "  In this paper, we propose new randomized algorithms for estimating the\ntwo-to-infinity and one-to-two norms in a matrix-free setting, using only\nmatrix-vector multiplications. Our methods are based on appropriate\nmodifications of Hutchinson's diagonal estimator and its Hutch++ version. We\nprovide oracle complexity bounds for both modifications. We further illustrate\nthe practical utility of our algorithms for Jacobian-based regularization in\ndeep neural network training on image classification tasks. We also demonstrate\nthat our methodology can be applied to mitigate the effect of adversarial\nattacks in the domain of recommender systems.\n", "link": "http://arxiv.org/abs/2508.04444v1", "date": "2025-08-06", "relevancy": 1.3585, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4672}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.44}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matrix-Free%20Two-to-Infinity%20and%20One-to-Two%20Norms%20Estimation&body=Title%3A%20Matrix-Free%20Two-to-Infinity%20and%20One-to-Two%20Norms%20Estimation%0AAuthor%3A%20Askar%20Tsyganov%20and%20Evgeny%20Frolov%20and%20Sergey%20Samsonov%20and%20Maxim%20Rakhuba%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20new%20randomized%20algorithms%20for%20estimating%20the%0Atwo-to-infinity%20and%20one-to-two%20norms%20in%20a%20matrix-free%20setting%2C%20using%20only%0Amatrix-vector%20multiplications.%20Our%20methods%20are%20based%20on%20appropriate%0Amodifications%20of%20Hutchinson%27s%20diagonal%20estimator%20and%20its%20Hutch%2B%2B%20version.%20We%0Aprovide%20oracle%20complexity%20bounds%20for%20both%20modifications.%20We%20further%20illustrate%0Athe%20practical%20utility%20of%20our%20algorithms%20for%20Jacobian-based%20regularization%20in%0Adeep%20neural%20network%20training%20on%20image%20classification%20tasks.%20We%20also%20demonstrate%0Athat%20our%20methodology%20can%20be%20applied%20to%20mitigate%20the%20effect%20of%20adversarial%0Aattacks%20in%20the%20domain%20of%20recommender%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatrix-Free%2520Two-to-Infinity%2520and%2520One-to-Two%2520Norms%2520Estimation%26entry.906535625%3DAskar%2520Tsyganov%2520and%2520Evgeny%2520Frolov%2520and%2520Sergey%2520Samsonov%2520and%2520Maxim%2520Rakhuba%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520new%2520randomized%2520algorithms%2520for%2520estimating%2520the%250Atwo-to-infinity%2520and%2520one-to-two%2520norms%2520in%2520a%2520matrix-free%2520setting%252C%2520using%2520only%250Amatrix-vector%2520multiplications.%2520Our%2520methods%2520are%2520based%2520on%2520appropriate%250Amodifications%2520of%2520Hutchinson%2527s%2520diagonal%2520estimator%2520and%2520its%2520Hutch%252B%252B%2520version.%2520We%250Aprovide%2520oracle%2520complexity%2520bounds%2520for%2520both%2520modifications.%2520We%2520further%2520illustrate%250Athe%2520practical%2520utility%2520of%2520our%2520algorithms%2520for%2520Jacobian-based%2520regularization%2520in%250Adeep%2520neural%2520network%2520training%2520on%2520image%2520classification%2520tasks.%2520We%2520also%2520demonstrate%250Athat%2520our%2520methodology%2520can%2520be%2520applied%2520to%2520mitigate%2520the%2520effect%2520of%2520adversarial%250Aattacks%2520in%2520the%2520domain%2520of%2520recommender%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix-Free%20Two-to-Infinity%20and%20One-to-Two%20Norms%20Estimation&entry.906535625=Askar%20Tsyganov%20and%20Evgeny%20Frolov%20and%20Sergey%20Samsonov%20and%20Maxim%20Rakhuba&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20new%20randomized%20algorithms%20for%20estimating%20the%0Atwo-to-infinity%20and%20one-to-two%20norms%20in%20a%20matrix-free%20setting%2C%20using%20only%0Amatrix-vector%20multiplications.%20Our%20methods%20are%20based%20on%20appropriate%0Amodifications%20of%20Hutchinson%27s%20diagonal%20estimator%20and%20its%20Hutch%2B%2B%20version.%20We%0Aprovide%20oracle%20complexity%20bounds%20for%20both%20modifications.%20We%20further%20illustrate%0Athe%20practical%20utility%20of%20our%20algorithms%20for%20Jacobian-based%20regularization%20in%0Adeep%20neural%20network%20training%20on%20image%20classification%20tasks.%20We%20also%20demonstrate%0Athat%20our%20methodology%20can%20be%20applied%20to%20mitigate%20the%20effect%20of%20adversarial%0Aattacks%20in%20the%20domain%20of%20recommender%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04444v1&entry.124074799=Read"},
{"title": "HALO: Hindsight-Augmented Learning for Online Auto-Bidding", "author": "Pusen Dong and Chenglong Cao and Xinyu Zhou and Jirong You and Linhe Xu and Feifan Xu and Shuo Yuan", "abstract": "  Digital advertising platforms operate millisecond-level auctions through\nReal-Time Bidding (RTB) systems, where advertisers compete for ad impressions\nthrough algorithmic bids. This dynamic mechanism enables precise audience\ntargeting but introduces profound operational complexity due to advertiser\nheterogeneity: budgets and ROI targets span orders of magnitude across\nadvertisers, from individual merchants to multinational brands. This diversity\ncreates a demanding adaptation landscape for Multi-Constraint Bidding (MCB).\nTraditional auto-bidding solutions fail in this environment due to two critical\nflaws: 1) severe sample inefficiency, where failed explorations under specific\nconstraints yield no transferable knowledge for new budget-ROI combinations,\nand 2) limited generalization under constraint shifts, as they ignore physical\nrelationships between constraints and bidding coefficients. To address this, we\npropose HALO: Hindsight-Augmented Learning for Online Auto-Bidding. HALO\nintroduces a theoretically grounded hindsight mechanism that repurposes all\nexplorations into training data for arbitrary constraint configuration via\ntrajectory reorientation. Further, it employs B-spline functional\nrepresentation, enabling continuous, derivative-aware bid mapping across\nconstraint spaces. HALO ensures robust adaptation even when budget/ROI\nrequirements differ drastically from training scenarios. Industrial dataset\nevaluations demonstrate the superiority of HALO in handling multi-scale\nconstraints, reducing constraint violations while improving GMV.\n", "link": "http://arxiv.org/abs/2508.03267v2", "date": "2025-08-06", "relevancy": 1.9618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5275}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HALO%3A%20Hindsight-Augmented%20Learning%20for%20Online%20Auto-Bidding&body=Title%3A%20HALO%3A%20Hindsight-Augmented%20Learning%20for%20Online%20Auto-Bidding%0AAuthor%3A%20Pusen%20Dong%20and%20Chenglong%20Cao%20and%20Xinyu%20Zhou%20and%20Jirong%20You%20and%20Linhe%20Xu%20and%20Feifan%20Xu%20and%20Shuo%20Yuan%0AAbstract%3A%20%20%20Digital%20advertising%20platforms%20operate%20millisecond-level%20auctions%20through%0AReal-Time%20Bidding%20%28RTB%29%20systems%2C%20where%20advertisers%20compete%20for%20ad%20impressions%0Athrough%20algorithmic%20bids.%20This%20dynamic%20mechanism%20enables%20precise%20audience%0Atargeting%20but%20introduces%20profound%20operational%20complexity%20due%20to%20advertiser%0Aheterogeneity%3A%20budgets%20and%20ROI%20targets%20span%20orders%20of%20magnitude%20across%0Aadvertisers%2C%20from%20individual%20merchants%20to%20multinational%20brands.%20This%20diversity%0Acreates%20a%20demanding%20adaptation%20landscape%20for%20Multi-Constraint%20Bidding%20%28MCB%29.%0ATraditional%20auto-bidding%20solutions%20fail%20in%20this%20environment%20due%20to%20two%20critical%0Aflaws%3A%201%29%20severe%20sample%20inefficiency%2C%20where%20failed%20explorations%20under%20specific%0Aconstraints%20yield%20no%20transferable%20knowledge%20for%20new%20budget-ROI%20combinations%2C%0Aand%202%29%20limited%20generalization%20under%20constraint%20shifts%2C%20as%20they%20ignore%20physical%0Arelationships%20between%20constraints%20and%20bidding%20coefficients.%20To%20address%20this%2C%20we%0Apropose%20HALO%3A%20Hindsight-Augmented%20Learning%20for%20Online%20Auto-Bidding.%20HALO%0Aintroduces%20a%20theoretically%20grounded%20hindsight%20mechanism%20that%20repurposes%20all%0Aexplorations%20into%20training%20data%20for%20arbitrary%20constraint%20configuration%20via%0Atrajectory%20reorientation.%20Further%2C%20it%20employs%20B-spline%20functional%0Arepresentation%2C%20enabling%20continuous%2C%20derivative-aware%20bid%20mapping%20across%0Aconstraint%20spaces.%20HALO%20ensures%20robust%20adaptation%20even%20when%20budget/ROI%0Arequirements%20differ%20drastically%20from%20training%20scenarios.%20Industrial%20dataset%0Aevaluations%20demonstrate%20the%20superiority%20of%20HALO%20in%20handling%20multi-scale%0Aconstraints%2C%20reducing%20constraint%20violations%20while%20improving%20GMV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03267v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHALO%253A%2520Hindsight-Augmented%2520Learning%2520for%2520Online%2520Auto-Bidding%26entry.906535625%3DPusen%2520Dong%2520and%2520Chenglong%2520Cao%2520and%2520Xinyu%2520Zhou%2520and%2520Jirong%2520You%2520and%2520Linhe%2520Xu%2520and%2520Feifan%2520Xu%2520and%2520Shuo%2520Yuan%26entry.1292438233%3D%2520%2520Digital%2520advertising%2520platforms%2520operate%2520millisecond-level%2520auctions%2520through%250AReal-Time%2520Bidding%2520%2528RTB%2529%2520systems%252C%2520where%2520advertisers%2520compete%2520for%2520ad%2520impressions%250Athrough%2520algorithmic%2520bids.%2520This%2520dynamic%2520mechanism%2520enables%2520precise%2520audience%250Atargeting%2520but%2520introduces%2520profound%2520operational%2520complexity%2520due%2520to%2520advertiser%250Aheterogeneity%253A%2520budgets%2520and%2520ROI%2520targets%2520span%2520orders%2520of%2520magnitude%2520across%250Aadvertisers%252C%2520from%2520individual%2520merchants%2520to%2520multinational%2520brands.%2520This%2520diversity%250Acreates%2520a%2520demanding%2520adaptation%2520landscape%2520for%2520Multi-Constraint%2520Bidding%2520%2528MCB%2529.%250ATraditional%2520auto-bidding%2520solutions%2520fail%2520in%2520this%2520environment%2520due%2520to%2520two%2520critical%250Aflaws%253A%25201%2529%2520severe%2520sample%2520inefficiency%252C%2520where%2520failed%2520explorations%2520under%2520specific%250Aconstraints%2520yield%2520no%2520transferable%2520knowledge%2520for%2520new%2520budget-ROI%2520combinations%252C%250Aand%25202%2529%2520limited%2520generalization%2520under%2520constraint%2520shifts%252C%2520as%2520they%2520ignore%2520physical%250Arelationships%2520between%2520constraints%2520and%2520bidding%2520coefficients.%2520To%2520address%2520this%252C%2520we%250Apropose%2520HALO%253A%2520Hindsight-Augmented%2520Learning%2520for%2520Online%2520Auto-Bidding.%2520HALO%250Aintroduces%2520a%2520theoretically%2520grounded%2520hindsight%2520mechanism%2520that%2520repurposes%2520all%250Aexplorations%2520into%2520training%2520data%2520for%2520arbitrary%2520constraint%2520configuration%2520via%250Atrajectory%2520reorientation.%2520Further%252C%2520it%2520employs%2520B-spline%2520functional%250Arepresentation%252C%2520enabling%2520continuous%252C%2520derivative-aware%2520bid%2520mapping%2520across%250Aconstraint%2520spaces.%2520HALO%2520ensures%2520robust%2520adaptation%2520even%2520when%2520budget/ROI%250Arequirements%2520differ%2520drastically%2520from%2520training%2520scenarios.%2520Industrial%2520dataset%250Aevaluations%2520demonstrate%2520the%2520superiority%2520of%2520HALO%2520in%2520handling%2520multi-scale%250Aconstraints%252C%2520reducing%2520constraint%2520violations%2520while%2520improving%2520GMV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03267v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HALO%3A%20Hindsight-Augmented%20Learning%20for%20Online%20Auto-Bidding&entry.906535625=Pusen%20Dong%20and%20Chenglong%20Cao%20and%20Xinyu%20Zhou%20and%20Jirong%20You%20and%20Linhe%20Xu%20and%20Feifan%20Xu%20and%20Shuo%20Yuan&entry.1292438233=%20%20Digital%20advertising%20platforms%20operate%20millisecond-level%20auctions%20through%0AReal-Time%20Bidding%20%28RTB%29%20systems%2C%20where%20advertisers%20compete%20for%20ad%20impressions%0Athrough%20algorithmic%20bids.%20This%20dynamic%20mechanism%20enables%20precise%20audience%0Atargeting%20but%20introduces%20profound%20operational%20complexity%20due%20to%20advertiser%0Aheterogeneity%3A%20budgets%20and%20ROI%20targets%20span%20orders%20of%20magnitude%20across%0Aadvertisers%2C%20from%20individual%20merchants%20to%20multinational%20brands.%20This%20diversity%0Acreates%20a%20demanding%20adaptation%20landscape%20for%20Multi-Constraint%20Bidding%20%28MCB%29.%0ATraditional%20auto-bidding%20solutions%20fail%20in%20this%20environment%20due%20to%20two%20critical%0Aflaws%3A%201%29%20severe%20sample%20inefficiency%2C%20where%20failed%20explorations%20under%20specific%0Aconstraints%20yield%20no%20transferable%20knowledge%20for%20new%20budget-ROI%20combinations%2C%0Aand%202%29%20limited%20generalization%20under%20constraint%20shifts%2C%20as%20they%20ignore%20physical%0Arelationships%20between%20constraints%20and%20bidding%20coefficients.%20To%20address%20this%2C%20we%0Apropose%20HALO%3A%20Hindsight-Augmented%20Learning%20for%20Online%20Auto-Bidding.%20HALO%0Aintroduces%20a%20theoretically%20grounded%20hindsight%20mechanism%20that%20repurposes%20all%0Aexplorations%20into%20training%20data%20for%20arbitrary%20constraint%20configuration%20via%0Atrajectory%20reorientation.%20Further%2C%20it%20employs%20B-spline%20functional%0Arepresentation%2C%20enabling%20continuous%2C%20derivative-aware%20bid%20mapping%20across%0Aconstraint%20spaces.%20HALO%20ensures%20robust%20adaptation%20even%20when%20budget/ROI%0Arequirements%20differ%20drastically%20from%20training%20scenarios.%20Industrial%20dataset%0Aevaluations%20demonstrate%20the%20superiority%20of%20HALO%20in%20handling%20multi-scale%0Aconstraints%2C%20reducing%20constraint%20violations%20while%20improving%20GMV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03267v2&entry.124074799=Read"},
{"title": "Attack Pattern Mining to Discover Hidden Threats to Industrial Control\n  Systems", "author": "Muhammad Azmi Umer and Chuadhry Mujeeb Ahmed and Aditya Mathur and Muhammad Taha Jilani", "abstract": "  This work focuses on validation of attack pattern mining in the context of\nIndustrial Control System (ICS) security. A comprehensive security assessment\nof an ICS requires generating a large and variety of attack patterns. For this\npurpose we have proposed a data driven technique to generate attack patterns\nfor an ICS. The proposed technique has been used to generate over 100,000\nattack patterns from data gathered from an operational water treatment plant.\nIn this work we present a detailed case study to validate the attack patterns.\n", "link": "http://arxiv.org/abs/2508.04561v1", "date": "2025-08-06", "relevancy": 1.4658, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3914}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3625}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attack%20Pattern%20Mining%20to%20Discover%20Hidden%20Threats%20to%20Industrial%20Control%0A%20%20Systems&body=Title%3A%20Attack%20Pattern%20Mining%20to%20Discover%20Hidden%20Threats%20to%20Industrial%20Control%0A%20%20Systems%0AAuthor%3A%20Muhammad%20Azmi%20Umer%20and%20Chuadhry%20Mujeeb%20Ahmed%20and%20Aditya%20Mathur%20and%20Muhammad%20Taha%20Jilani%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20validation%20of%20attack%20pattern%20mining%20in%20the%20context%20of%0AIndustrial%20Control%20System%20%28ICS%29%20security.%20A%20comprehensive%20security%20assessment%0Aof%20an%20ICS%20requires%20generating%20a%20large%20and%20variety%20of%20attack%20patterns.%20For%20this%0Apurpose%20we%20have%20proposed%20a%20data%20driven%20technique%20to%20generate%20attack%20patterns%0Afor%20an%20ICS.%20The%20proposed%20technique%20has%20been%20used%20to%20generate%20over%20100%2C000%0Aattack%20patterns%20from%20data%20gathered%20from%20an%20operational%20water%20treatment%20plant.%0AIn%20this%20work%20we%20present%20a%20detailed%20case%20study%20to%20validate%20the%20attack%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttack%2520Pattern%2520Mining%2520to%2520Discover%2520Hidden%2520Threats%2520to%2520Industrial%2520Control%250A%2520%2520Systems%26entry.906535625%3DMuhammad%2520Azmi%2520Umer%2520and%2520Chuadhry%2520Mujeeb%2520Ahmed%2520and%2520Aditya%2520Mathur%2520and%2520Muhammad%2520Taha%2520Jilani%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520validation%2520of%2520attack%2520pattern%2520mining%2520in%2520the%2520context%2520of%250AIndustrial%2520Control%2520System%2520%2528ICS%2529%2520security.%2520A%2520comprehensive%2520security%2520assessment%250Aof%2520an%2520ICS%2520requires%2520generating%2520a%2520large%2520and%2520variety%2520of%2520attack%2520patterns.%2520For%2520this%250Apurpose%2520we%2520have%2520proposed%2520a%2520data%2520driven%2520technique%2520to%2520generate%2520attack%2520patterns%250Afor%2520an%2520ICS.%2520The%2520proposed%2520technique%2520has%2520been%2520used%2520to%2520generate%2520over%2520100%252C000%250Aattack%2520patterns%2520from%2520data%2520gathered%2520from%2520an%2520operational%2520water%2520treatment%2520plant.%250AIn%2520this%2520work%2520we%2520present%2520a%2520detailed%2520case%2520study%2520to%2520validate%2520the%2520attack%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attack%20Pattern%20Mining%20to%20Discover%20Hidden%20Threats%20to%20Industrial%20Control%0A%20%20Systems&entry.906535625=Muhammad%20Azmi%20Umer%20and%20Chuadhry%20Mujeeb%20Ahmed%20and%20Aditya%20Mathur%20and%20Muhammad%20Taha%20Jilani&entry.1292438233=%20%20This%20work%20focuses%20on%20validation%20of%20attack%20pattern%20mining%20in%20the%20context%20of%0AIndustrial%20Control%20System%20%28ICS%29%20security.%20A%20comprehensive%20security%20assessment%0Aof%20an%20ICS%20requires%20generating%20a%20large%20and%20variety%20of%20attack%20patterns.%20For%20this%0Apurpose%20we%20have%20proposed%20a%20data%20driven%20technique%20to%20generate%20attack%20patterns%0Afor%20an%20ICS.%20The%20proposed%20technique%20has%20been%20used%20to%20generate%20over%20100%2C000%0Aattack%20patterns%20from%20data%20gathered%20from%20an%20operational%20water%20treatment%20plant.%0AIn%20this%20work%20we%20present%20a%20detailed%20case%20study%20to%20validate%20the%20attack%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04561v1&entry.124074799=Read"},
{"title": "How Does Bilateral Ear Symmetry Affect Deep Ear Features?", "author": "Kagan Ozturk and Deeksha Arun and Kevin W. Bowyer and Patrick Flynn", "abstract": "  Ear recognition has gained attention as a reliable biometric technique due to\nthe distinctive characteristics of human ears. With the increasing availability\nof large-scale datasets, convolutional neural networks (CNNs) have been widely\nadopted to learn features directly from raw ear images, outperforming\ntraditional hand-crafted methods. However, the effect of bilateral ear symmetry\non the features learned by CNNs has received little attention in recent\nstudies. In this paper, we investigate how bilateral ear symmetry influences\nthe effectiveness of CNN-based ear recognition. To this end, we first develop\nan ear side classifier to automatically categorize ear images as either left or\nright. We then explore the impact of incorporating this side information during\nboth training and test. Cross-dataset evaluations are conducted on five\ndatasets. Our results suggest that treating left and right ears separately\nduring training and testing can lead to notable performance improvements.\nFurthermore, our ablation studies on alignment strategies, input sizes, and\nvarious hyperparameter settings provide practical insights into training\nCNN-based ear recognition systems on large-scale datasets to achieve higher\nverification rates.\n", "link": "http://arxiv.org/abs/2508.04614v1", "date": "2025-08-06", "relevancy": 2.1865, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.454}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4346}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Does%20Bilateral%20Ear%20Symmetry%20Affect%20Deep%20Ear%20Features%3F&body=Title%3A%20How%20Does%20Bilateral%20Ear%20Symmetry%20Affect%20Deep%20Ear%20Features%3F%0AAuthor%3A%20Kagan%20Ozturk%20and%20Deeksha%20Arun%20and%20Kevin%20W.%20Bowyer%20and%20Patrick%20Flynn%0AAbstract%3A%20%20%20Ear%20recognition%20has%20gained%20attention%20as%20a%20reliable%20biometric%20technique%20due%20to%0Athe%20distinctive%20characteristics%20of%20human%20ears.%20With%20the%20increasing%20availability%0Aof%20large-scale%20datasets%2C%20convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20widely%0Aadopted%20to%20learn%20features%20directly%20from%20raw%20ear%20images%2C%20outperforming%0Atraditional%20hand-crafted%20methods.%20However%2C%20the%20effect%20of%20bilateral%20ear%20symmetry%0Aon%20the%20features%20learned%20by%20CNNs%20has%20received%20little%20attention%20in%20recent%0Astudies.%20In%20this%20paper%2C%20we%20investigate%20how%20bilateral%20ear%20symmetry%20influences%0Athe%20effectiveness%20of%20CNN-based%20ear%20recognition.%20To%20this%20end%2C%20we%20first%20develop%0Aan%20ear%20side%20classifier%20to%20automatically%20categorize%20ear%20images%20as%20either%20left%20or%0Aright.%20We%20then%20explore%20the%20impact%20of%20incorporating%20this%20side%20information%20during%0Aboth%20training%20and%20test.%20Cross-dataset%20evaluations%20are%20conducted%20on%20five%0Adatasets.%20Our%20results%20suggest%20that%20treating%20left%20and%20right%20ears%20separately%0Aduring%20training%20and%20testing%20can%20lead%20to%20notable%20performance%20improvements.%0AFurthermore%2C%20our%20ablation%20studies%20on%20alignment%20strategies%2C%20input%20sizes%2C%20and%0Avarious%20hyperparameter%20settings%20provide%20practical%20insights%20into%20training%0ACNN-based%20ear%20recognition%20systems%20on%20large-scale%20datasets%20to%20achieve%20higher%0Averification%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Does%2520Bilateral%2520Ear%2520Symmetry%2520Affect%2520Deep%2520Ear%2520Features%253F%26entry.906535625%3DKagan%2520Ozturk%2520and%2520Deeksha%2520Arun%2520and%2520Kevin%2520W.%2520Bowyer%2520and%2520Patrick%2520Flynn%26entry.1292438233%3D%2520%2520Ear%2520recognition%2520has%2520gained%2520attention%2520as%2520a%2520reliable%2520biometric%2520technique%2520due%2520to%250Athe%2520distinctive%2520characteristics%2520of%2520human%2520ears.%2520With%2520the%2520increasing%2520availability%250Aof%2520large-scale%2520datasets%252C%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520been%2520widely%250Aadopted%2520to%2520learn%2520features%2520directly%2520from%2520raw%2520ear%2520images%252C%2520outperforming%250Atraditional%2520hand-crafted%2520methods.%2520However%252C%2520the%2520effect%2520of%2520bilateral%2520ear%2520symmetry%250Aon%2520the%2520features%2520learned%2520by%2520CNNs%2520has%2520received%2520little%2520attention%2520in%2520recent%250Astudies.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520bilateral%2520ear%2520symmetry%2520influences%250Athe%2520effectiveness%2520of%2520CNN-based%2520ear%2520recognition.%2520To%2520this%2520end%252C%2520we%2520first%2520develop%250Aan%2520ear%2520side%2520classifier%2520to%2520automatically%2520categorize%2520ear%2520images%2520as%2520either%2520left%2520or%250Aright.%2520We%2520then%2520explore%2520the%2520impact%2520of%2520incorporating%2520this%2520side%2520information%2520during%250Aboth%2520training%2520and%2520test.%2520Cross-dataset%2520evaluations%2520are%2520conducted%2520on%2520five%250Adatasets.%2520Our%2520results%2520suggest%2520that%2520treating%2520left%2520and%2520right%2520ears%2520separately%250Aduring%2520training%2520and%2520testing%2520can%2520lead%2520to%2520notable%2520performance%2520improvements.%250AFurthermore%252C%2520our%2520ablation%2520studies%2520on%2520alignment%2520strategies%252C%2520input%2520sizes%252C%2520and%250Avarious%2520hyperparameter%2520settings%2520provide%2520practical%2520insights%2520into%2520training%250ACNN-based%2520ear%2520recognition%2520systems%2520on%2520large-scale%2520datasets%2520to%2520achieve%2520higher%250Averification%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Does%20Bilateral%20Ear%20Symmetry%20Affect%20Deep%20Ear%20Features%3F&entry.906535625=Kagan%20Ozturk%20and%20Deeksha%20Arun%20and%20Kevin%20W.%20Bowyer%20and%20Patrick%20Flynn&entry.1292438233=%20%20Ear%20recognition%20has%20gained%20attention%20as%20a%20reliable%20biometric%20technique%20due%20to%0Athe%20distinctive%20characteristics%20of%20human%20ears.%20With%20the%20increasing%20availability%0Aof%20large-scale%20datasets%2C%20convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20widely%0Aadopted%20to%20learn%20features%20directly%20from%20raw%20ear%20images%2C%20outperforming%0Atraditional%20hand-crafted%20methods.%20However%2C%20the%20effect%20of%20bilateral%20ear%20symmetry%0Aon%20the%20features%20learned%20by%20CNNs%20has%20received%20little%20attention%20in%20recent%0Astudies.%20In%20this%20paper%2C%20we%20investigate%20how%20bilateral%20ear%20symmetry%20influences%0Athe%20effectiveness%20of%20CNN-based%20ear%20recognition.%20To%20this%20end%2C%20we%20first%20develop%0Aan%20ear%20side%20classifier%20to%20automatically%20categorize%20ear%20images%20as%20either%20left%20or%0Aright.%20We%20then%20explore%20the%20impact%20of%20incorporating%20this%20side%20information%20during%0Aboth%20training%20and%20test.%20Cross-dataset%20evaluations%20are%20conducted%20on%20five%0Adatasets.%20Our%20results%20suggest%20that%20treating%20left%20and%20right%20ears%20separately%0Aduring%20training%20and%20testing%20can%20lead%20to%20notable%20performance%20improvements.%0AFurthermore%2C%20our%20ablation%20studies%20on%20alignment%20strategies%2C%20input%20sizes%2C%20and%0Avarious%20hyperparameter%20settings%20provide%20practical%20insights%20into%20training%0ACNN-based%20ear%20recognition%20systems%20on%20large-scale%20datasets%20to%20achieve%20higher%0Averification%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04614v1&entry.124074799=Read"},
{"title": "Heterogeneity-Oblivious Robust Federated Learning", "author": "Weiyao Zhang and Jinyang Li and Qi Song and Miao Wang and Chungang Lin and Haitong Luo and Xuying Meng and Yujun Zhang", "abstract": "  Federated Learning (FL) remains highly vulnerable to poisoning attacks,\nespecially under real-world hyper-heterogeneity, where clients differ\nsignificantly in data distributions, communication capabilities, and model\narchitectures. Such heterogeneity not only undermines the effectiveness of\naggregation strategies but also makes attacks more difficult to detect.\nFurthermore, high-dimensional models expand the attack surface. To address\nthese challenges, we propose Horus, a heterogeneity-oblivious robust FL\nframework centered on low-rank adaptations (LoRAs). Rather than aggregating\nfull model parameters, Horus inserts LoRAs into empirically stable layers and\naggregates only LoRAs to reduce the attack uncover a key empirical observation\nthat the input projection (LoRA-A) is markedly more stable than the output\nprojection (LoRA-B) under heterogeneity and poisoning. Leveraging this, we\ndesign a Heterogeneity-Oblivious Poisoning Score using the features from LoRA-A\nto filter poisoned clients. For the remaining benign clients, we propose\nprojection-aware aggregation mechanism to preserve collaborative signals while\nsuppressing drifts, which reweights client updates by consistency with the\nglobal directions. Extensive experiments across diverse datasets, model\narchitectures, and attacks demonstrate that Horus consistently outperforms\nstate-of-the-art baselines in both robustness and accuracy.\n", "link": "http://arxiv.org/abs/2508.03579v2", "date": "2025-08-06", "relevancy": 1.8853, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4818}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4695}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Heterogeneity-Oblivious%20Robust%20Federated%20Learning&body=Title%3A%20Heterogeneity-Oblivious%20Robust%20Federated%20Learning%0AAuthor%3A%20Weiyao%20Zhang%20and%20Jinyang%20Li%20and%20Qi%20Song%20and%20Miao%20Wang%20and%20Chungang%20Lin%20and%20Haitong%20Luo%20and%20Xuying%20Meng%20and%20Yujun%20Zhang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20remains%20highly%20vulnerable%20to%20poisoning%20attacks%2C%0Aespecially%20under%20real-world%20hyper-heterogeneity%2C%20where%20clients%20differ%0Asignificantly%20in%20data%20distributions%2C%20communication%20capabilities%2C%20and%20model%0Aarchitectures.%20Such%20heterogeneity%20not%20only%20undermines%20the%20effectiveness%20of%0Aaggregation%20strategies%20but%20also%20makes%20attacks%20more%20difficult%20to%20detect.%0AFurthermore%2C%20high-dimensional%20models%20expand%20the%20attack%20surface.%20To%20address%0Athese%20challenges%2C%20we%20propose%20Horus%2C%20a%20heterogeneity-oblivious%20robust%20FL%0Aframework%20centered%20on%20low-rank%20adaptations%20%28LoRAs%29.%20Rather%20than%20aggregating%0Afull%20model%20parameters%2C%20Horus%20inserts%20LoRAs%20into%20empirically%20stable%20layers%20and%0Aaggregates%20only%20LoRAs%20to%20reduce%20the%20attack%20uncover%20a%20key%20empirical%20observation%0Athat%20the%20input%20projection%20%28LoRA-A%29%20is%20markedly%20more%20stable%20than%20the%20output%0Aprojection%20%28LoRA-B%29%20under%20heterogeneity%20and%20poisoning.%20Leveraging%20this%2C%20we%0Adesign%20a%20Heterogeneity-Oblivious%20Poisoning%20Score%20using%20the%20features%20from%20LoRA-A%0Ato%20filter%20poisoned%20clients.%20For%20the%20remaining%20benign%20clients%2C%20we%20propose%0Aprojection-aware%20aggregation%20mechanism%20to%20preserve%20collaborative%20signals%20while%0Asuppressing%20drifts%2C%20which%20reweights%20client%20updates%20by%20consistency%20with%20the%0Aglobal%20directions.%20Extensive%20experiments%20across%20diverse%20datasets%2C%20model%0Aarchitectures%2C%20and%20attacks%20demonstrate%20that%20Horus%20consistently%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20robustness%20and%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.03579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeterogeneity-Oblivious%2520Robust%2520Federated%2520Learning%26entry.906535625%3DWeiyao%2520Zhang%2520and%2520Jinyang%2520Li%2520and%2520Qi%2520Song%2520and%2520Miao%2520Wang%2520and%2520Chungang%2520Lin%2520and%2520Haitong%2520Luo%2520and%2520Xuying%2520Meng%2520and%2520Yujun%2520Zhang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520remains%2520highly%2520vulnerable%2520to%2520poisoning%2520attacks%252C%250Aespecially%2520under%2520real-world%2520hyper-heterogeneity%252C%2520where%2520clients%2520differ%250Asignificantly%2520in%2520data%2520distributions%252C%2520communication%2520capabilities%252C%2520and%2520model%250Aarchitectures.%2520Such%2520heterogeneity%2520not%2520only%2520undermines%2520the%2520effectiveness%2520of%250Aaggregation%2520strategies%2520but%2520also%2520makes%2520attacks%2520more%2520difficult%2520to%2520detect.%250AFurthermore%252C%2520high-dimensional%2520models%2520expand%2520the%2520attack%2520surface.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520Horus%252C%2520a%2520heterogeneity-oblivious%2520robust%2520FL%250Aframework%2520centered%2520on%2520low-rank%2520adaptations%2520%2528LoRAs%2529.%2520Rather%2520than%2520aggregating%250Afull%2520model%2520parameters%252C%2520Horus%2520inserts%2520LoRAs%2520into%2520empirically%2520stable%2520layers%2520and%250Aaggregates%2520only%2520LoRAs%2520to%2520reduce%2520the%2520attack%2520uncover%2520a%2520key%2520empirical%2520observation%250Athat%2520the%2520input%2520projection%2520%2528LoRA-A%2529%2520is%2520markedly%2520more%2520stable%2520than%2520the%2520output%250Aprojection%2520%2528LoRA-B%2529%2520under%2520heterogeneity%2520and%2520poisoning.%2520Leveraging%2520this%252C%2520we%250Adesign%2520a%2520Heterogeneity-Oblivious%2520Poisoning%2520Score%2520using%2520the%2520features%2520from%2520LoRA-A%250Ato%2520filter%2520poisoned%2520clients.%2520For%2520the%2520remaining%2520benign%2520clients%252C%2520we%2520propose%250Aprojection-aware%2520aggregation%2520mechanism%2520to%2520preserve%2520collaborative%2520signals%2520while%250Asuppressing%2520drifts%252C%2520which%2520reweights%2520client%2520updates%2520by%2520consistency%2520with%2520the%250Aglobal%2520directions.%2520Extensive%2520experiments%2520across%2520diverse%2520datasets%252C%2520model%250Aarchitectures%252C%2520and%2520attacks%2520demonstrate%2520that%2520Horus%2520consistently%2520outperforms%250Astate-of-the-art%2520baselines%2520in%2520both%2520robustness%2520and%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Heterogeneity-Oblivious%20Robust%20Federated%20Learning&entry.906535625=Weiyao%20Zhang%20and%20Jinyang%20Li%20and%20Qi%20Song%20and%20Miao%20Wang%20and%20Chungang%20Lin%20and%20Haitong%20Luo%20and%20Xuying%20Meng%20and%20Yujun%20Zhang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20remains%20highly%20vulnerable%20to%20poisoning%20attacks%2C%0Aespecially%20under%20real-world%20hyper-heterogeneity%2C%20where%20clients%20differ%0Asignificantly%20in%20data%20distributions%2C%20communication%20capabilities%2C%20and%20model%0Aarchitectures.%20Such%20heterogeneity%20not%20only%20undermines%20the%20effectiveness%20of%0Aaggregation%20strategies%20but%20also%20makes%20attacks%20more%20difficult%20to%20detect.%0AFurthermore%2C%20high-dimensional%20models%20expand%20the%20attack%20surface.%20To%20address%0Athese%20challenges%2C%20we%20propose%20Horus%2C%20a%20heterogeneity-oblivious%20robust%20FL%0Aframework%20centered%20on%20low-rank%20adaptations%20%28LoRAs%29.%20Rather%20than%20aggregating%0Afull%20model%20parameters%2C%20Horus%20inserts%20LoRAs%20into%20empirically%20stable%20layers%20and%0Aaggregates%20only%20LoRAs%20to%20reduce%20the%20attack%20uncover%20a%20key%20empirical%20observation%0Athat%20the%20input%20projection%20%28LoRA-A%29%20is%20markedly%20more%20stable%20than%20the%20output%0Aprojection%20%28LoRA-B%29%20under%20heterogeneity%20and%20poisoning.%20Leveraging%20this%2C%20we%0Adesign%20a%20Heterogeneity-Oblivious%20Poisoning%20Score%20using%20the%20features%20from%20LoRA-A%0Ato%20filter%20poisoned%20clients.%20For%20the%20remaining%20benign%20clients%2C%20we%20propose%0Aprojection-aware%20aggregation%20mechanism%20to%20preserve%20collaborative%20signals%20while%0Asuppressing%20drifts%2C%20which%20reweights%20client%20updates%20by%20consistency%20with%20the%0Aglobal%20directions.%20Extensive%20experiments%20across%20diverse%20datasets%2C%20model%0Aarchitectures%2C%20and%20attacks%20demonstrate%20that%20Horus%20consistently%20outperforms%0Astate-of-the-art%20baselines%20in%20both%20robustness%20and%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.03579v2&entry.124074799=Read"},
{"title": "Metric Learning in an RKHS", "author": "Gokcan Tatli and Yi Chen and Blake Mason and Robert Nowak and Ramya Korlakai Vinayak", "abstract": "  Metric learning from a set of triplet comparisons in the form of \"Do you\nthink item h is more similar to item i or item j?\", indicating similarity and\ndifferences between items, plays a key role in various applications including\nimage retrieval, recommendation systems, and cognitive psychology. The goal is\nto learn a metric in the RKHS that reflects the comparisons. Nonlinear metric\nlearning using kernel methods and neural networks have shown great empirical\npromise. While previous works have addressed certain aspects of this problem,\nthere is little or no theoretical understanding of such methods. The exception\nis the special (linear) case in which the RKHS is the standard Euclidean space\n$\\mathbb{R}^d$; there is a comprehensive theory for metric learning in\n$\\mathbb{R}^d$. This paper develops a general RKHS framework for metric\nlearning and provides novel generalization guarantees and sample complexity\nbounds. We validate our findings through a set of simulations and experiments\non real datasets. Our code is publicly available at\nhttps://github.com/RamyaLab/metric-learning-RKHS.\n", "link": "http://arxiv.org/abs/2508.04476v1", "date": "2025-08-06", "relevancy": 1.8386, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4783}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metric%20Learning%20in%20an%20RKHS&body=Title%3A%20Metric%20Learning%20in%20an%20RKHS%0AAuthor%3A%20Gokcan%20Tatli%20and%20Yi%20Chen%20and%20Blake%20Mason%20and%20Robert%20Nowak%20and%20Ramya%20Korlakai%20Vinayak%0AAbstract%3A%20%20%20Metric%20learning%20from%20a%20set%20of%20triplet%20comparisons%20in%20the%20form%20of%20%22Do%20you%0Athink%20item%20h%20is%20more%20similar%20to%20item%20i%20or%20item%20j%3F%22%2C%20indicating%20similarity%20and%0Adifferences%20between%20items%2C%20plays%20a%20key%20role%20in%20various%20applications%20including%0Aimage%20retrieval%2C%20recommendation%20systems%2C%20and%20cognitive%20psychology.%20The%20goal%20is%0Ato%20learn%20a%20metric%20in%20the%20RKHS%20that%20reflects%20the%20comparisons.%20Nonlinear%20metric%0Alearning%20using%20kernel%20methods%20and%20neural%20networks%20have%20shown%20great%20empirical%0Apromise.%20While%20previous%20works%20have%20addressed%20certain%20aspects%20of%20this%20problem%2C%0Athere%20is%20little%20or%20no%20theoretical%20understanding%20of%20such%20methods.%20The%20exception%0Ais%20the%20special%20%28linear%29%20case%20in%20which%20the%20RKHS%20is%20the%20standard%20Euclidean%20space%0A%24%5Cmathbb%7BR%7D%5Ed%24%3B%20there%20is%20a%20comprehensive%20theory%20for%20metric%20learning%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20This%20paper%20develops%20a%20general%20RKHS%20framework%20for%20metric%0Alearning%20and%20provides%20novel%20generalization%20guarantees%20and%20sample%20complexity%0Abounds.%20We%20validate%20our%20findings%20through%20a%20set%20of%20simulations%20and%20experiments%0Aon%20real%20datasets.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/RamyaLab/metric-learning-RKHS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetric%2520Learning%2520in%2520an%2520RKHS%26entry.906535625%3DGokcan%2520Tatli%2520and%2520Yi%2520Chen%2520and%2520Blake%2520Mason%2520and%2520Robert%2520Nowak%2520and%2520Ramya%2520Korlakai%2520Vinayak%26entry.1292438233%3D%2520%2520Metric%2520learning%2520from%2520a%2520set%2520of%2520triplet%2520comparisons%2520in%2520the%2520form%2520of%2520%2522Do%2520you%250Athink%2520item%2520h%2520is%2520more%2520similar%2520to%2520item%2520i%2520or%2520item%2520j%253F%2522%252C%2520indicating%2520similarity%2520and%250Adifferences%2520between%2520items%252C%2520plays%2520a%2520key%2520role%2520in%2520various%2520applications%2520including%250Aimage%2520retrieval%252C%2520recommendation%2520systems%252C%2520and%2520cognitive%2520psychology.%2520The%2520goal%2520is%250Ato%2520learn%2520a%2520metric%2520in%2520the%2520RKHS%2520that%2520reflects%2520the%2520comparisons.%2520Nonlinear%2520metric%250Alearning%2520using%2520kernel%2520methods%2520and%2520neural%2520networks%2520have%2520shown%2520great%2520empirical%250Apromise.%2520While%2520previous%2520works%2520have%2520addressed%2520certain%2520aspects%2520of%2520this%2520problem%252C%250Athere%2520is%2520little%2520or%2520no%2520theoretical%2520understanding%2520of%2520such%2520methods.%2520The%2520exception%250Ais%2520the%2520special%2520%2528linear%2529%2520case%2520in%2520which%2520the%2520RKHS%2520is%2520the%2520standard%2520Euclidean%2520space%250A%2524%255Cmathbb%257BR%257D%255Ed%2524%253B%2520there%2520is%2520a%2520comprehensive%2520theory%2520for%2520metric%2520learning%2520in%250A%2524%255Cmathbb%257BR%257D%255Ed%2524.%2520This%2520paper%2520develops%2520a%2520general%2520RKHS%2520framework%2520for%2520metric%250Alearning%2520and%2520provides%2520novel%2520generalization%2520guarantees%2520and%2520sample%2520complexity%250Abounds.%2520We%2520validate%2520our%2520findings%2520through%2520a%2520set%2520of%2520simulations%2520and%2520experiments%250Aon%2520real%2520datasets.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/RamyaLab/metric-learning-RKHS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metric%20Learning%20in%20an%20RKHS&entry.906535625=Gokcan%20Tatli%20and%20Yi%20Chen%20and%20Blake%20Mason%20and%20Robert%20Nowak%20and%20Ramya%20Korlakai%20Vinayak&entry.1292438233=%20%20Metric%20learning%20from%20a%20set%20of%20triplet%20comparisons%20in%20the%20form%20of%20%22Do%20you%0Athink%20item%20h%20is%20more%20similar%20to%20item%20i%20or%20item%20j%3F%22%2C%20indicating%20similarity%20and%0Adifferences%20between%20items%2C%20plays%20a%20key%20role%20in%20various%20applications%20including%0Aimage%20retrieval%2C%20recommendation%20systems%2C%20and%20cognitive%20psychology.%20The%20goal%20is%0Ato%20learn%20a%20metric%20in%20the%20RKHS%20that%20reflects%20the%20comparisons.%20Nonlinear%20metric%0Alearning%20using%20kernel%20methods%20and%20neural%20networks%20have%20shown%20great%20empirical%0Apromise.%20While%20previous%20works%20have%20addressed%20certain%20aspects%20of%20this%20problem%2C%0Athere%20is%20little%20or%20no%20theoretical%20understanding%20of%20such%20methods.%20The%20exception%0Ais%20the%20special%20%28linear%29%20case%20in%20which%20the%20RKHS%20is%20the%20standard%20Euclidean%20space%0A%24%5Cmathbb%7BR%7D%5Ed%24%3B%20there%20is%20a%20comprehensive%20theory%20for%20metric%20learning%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20This%20paper%20develops%20a%20general%20RKHS%20framework%20for%20metric%0Alearning%20and%20provides%20novel%20generalization%20guarantees%20and%20sample%20complexity%0Abounds.%20We%20validate%20our%20findings%20through%20a%20set%20of%20simulations%20and%20experiments%0Aon%20real%20datasets.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/RamyaLab/metric-learning-RKHS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04476v1&entry.124074799=Read"},
{"title": "PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for\n  Efficient CNNs", "author": "Lukas Meiner and Jens Mehnert and Alexandru Paul Condurache", "abstract": "  Convolutional neural networks (CNNs) are crucial for computer vision tasks on\nresource-constrained devices. Quantization effectively compresses these models,\nreducing storage size and energy cost. However, in modern depthwise-separable\narchitectures, the computational cost is distributed unevenly across its\ncomponents, with pointwise operations being the most expensive. By applying a\ngeneral quantization scheme to this imbalanced cost distribution, existing\nquantization approaches fail to fully exploit potential efficiency gains. To\nthis end, we introduce PROM, a straightforward approach for quantizing modern\ndepthwise-separable convolutional networks by selectively using two distinct\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\nweights, while the remaining modules use 8-bit weights, which is achieved\nthrough a simple quantization-aware training procedure. Additionally, by\nquantizing activations to 8-bit, our method transforms pointwise convolutions\nwith ternary weights into int8 additions, which enjoy broad support across\nhardware platforms and effectively eliminates the need for expensive\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\ncompared to the float16 baseline while retaining similar classification\nperformance on ImageNet. Our method advances the Pareto frontier for energy\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\nPROM addresses the challenges of quantizing depthwise-separable convolutional\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\nenergy cost and storage size.\n", "link": "http://arxiv.org/abs/2505.03254v2", "date": "2025-08-06", "relevancy": 2.0588, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5476}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5095}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROM%3A%20Prioritize%20Reduction%20of%20Multiplications%20Over%20Lower%20Bit-Widths%20for%0A%20%20Efficient%20CNNs&body=Title%3A%20PROM%3A%20Prioritize%20Reduction%20of%20Multiplications%20Over%20Lower%20Bit-Widths%20for%0A%20%20Efficient%20CNNs%0AAuthor%3A%20Lukas%20Meiner%20and%20Jens%20Mehnert%20and%20Alexandru%20Paul%20Condurache%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20crucial%20for%20computer%20vision%20tasks%20on%0Aresource-constrained%20devices.%20Quantization%20effectively%20compresses%20these%20models%2C%0Areducing%20storage%20size%20and%20energy%20cost.%20However%2C%20in%20modern%20depthwise-separable%0Aarchitectures%2C%20the%20computational%20cost%20is%20distributed%20unevenly%20across%20its%0Acomponents%2C%20with%20pointwise%20operations%20being%20the%20most%20expensive.%20By%20applying%20a%0Ageneral%20quantization%20scheme%20to%20this%20imbalanced%20cost%20distribution%2C%20existing%0Aquantization%20approaches%20fail%20to%20fully%20exploit%20potential%20efficiency%20gains.%20To%0Athis%20end%2C%20we%20introduce%20PROM%2C%20a%20straightforward%20approach%20for%20quantizing%20modern%0Adepthwise-separable%20convolutional%20networks%20by%20selectively%20using%20two%20distinct%0Abit-widths.%20Specifically%2C%20pointwise%20convolutions%20are%20quantized%20to%20ternary%0Aweights%2C%20while%20the%20remaining%20modules%20use%208-bit%20weights%2C%20which%20is%20achieved%0Athrough%20a%20simple%20quantization-aware%20training%20procedure.%20Additionally%2C%20by%0Aquantizing%20activations%20to%208-bit%2C%20our%20method%20transforms%20pointwise%20convolutions%0Awith%20ternary%20weights%20into%20int8%20additions%2C%20which%20enjoy%20broad%20support%20across%0Ahardware%20platforms%20and%20effectively%20eliminates%20the%20need%20for%20expensive%0Amultiplications.%20Applying%20PROM%20to%20MobileNetV2%20reduces%20the%20model%27s%20energy%20cost%0Aby%20more%20than%20an%20order%20of%20magnitude%20%2823.9x%29%20and%20its%20storage%20size%20by%202.7x%0Acompared%20to%20the%20float16%20baseline%20while%20retaining%20similar%20classification%0Aperformance%20on%20ImageNet.%20Our%20method%20advances%20the%20Pareto%20frontier%20for%20energy%0Aconsumption%20vs.%20top-1%20accuracy%20for%20quantized%20convolutional%20models%20on%20ImageNet.%0APROM%20addresses%20the%20challenges%20of%20quantizing%20depthwise-separable%20convolutional%0Anetworks%20to%20both%20ternary%20and%208-bit%20weights%2C%20offering%20a%20simple%20way%20to%20reduce%0Aenergy%20cost%20and%20storage%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROM%253A%2520Prioritize%2520Reduction%2520of%2520Multiplications%2520Over%2520Lower%2520Bit-Widths%2520for%250A%2520%2520Efficient%2520CNNs%26entry.906535625%3DLukas%2520Meiner%2520and%2520Jens%2520Mehnert%2520and%2520Alexandru%2520Paul%2520Condurache%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520crucial%2520for%2520computer%2520vision%2520tasks%2520on%250Aresource-constrained%2520devices.%2520Quantization%2520effectively%2520compresses%2520these%2520models%252C%250Areducing%2520storage%2520size%2520and%2520energy%2520cost.%2520However%252C%2520in%2520modern%2520depthwise-separable%250Aarchitectures%252C%2520the%2520computational%2520cost%2520is%2520distributed%2520unevenly%2520across%2520its%250Acomponents%252C%2520with%2520pointwise%2520operations%2520being%2520the%2520most%2520expensive.%2520By%2520applying%2520a%250Ageneral%2520quantization%2520scheme%2520to%2520this%2520imbalanced%2520cost%2520distribution%252C%2520existing%250Aquantization%2520approaches%2520fail%2520to%2520fully%2520exploit%2520potential%2520efficiency%2520gains.%2520To%250Athis%2520end%252C%2520we%2520introduce%2520PROM%252C%2520a%2520straightforward%2520approach%2520for%2520quantizing%2520modern%250Adepthwise-separable%2520convolutional%2520networks%2520by%2520selectively%2520using%2520two%2520distinct%250Abit-widths.%2520Specifically%252C%2520pointwise%2520convolutions%2520are%2520quantized%2520to%2520ternary%250Aweights%252C%2520while%2520the%2520remaining%2520modules%2520use%25208-bit%2520weights%252C%2520which%2520is%2520achieved%250Athrough%2520a%2520simple%2520quantization-aware%2520training%2520procedure.%2520Additionally%252C%2520by%250Aquantizing%2520activations%2520to%25208-bit%252C%2520our%2520method%2520transforms%2520pointwise%2520convolutions%250Awith%2520ternary%2520weights%2520into%2520int8%2520additions%252C%2520which%2520enjoy%2520broad%2520support%2520across%250Ahardware%2520platforms%2520and%2520effectively%2520eliminates%2520the%2520need%2520for%2520expensive%250Amultiplications.%2520Applying%2520PROM%2520to%2520MobileNetV2%2520reduces%2520the%2520model%2527s%2520energy%2520cost%250Aby%2520more%2520than%2520an%2520order%2520of%2520magnitude%2520%252823.9x%2529%2520and%2520its%2520storage%2520size%2520by%25202.7x%250Acompared%2520to%2520the%2520float16%2520baseline%2520while%2520retaining%2520similar%2520classification%250Aperformance%2520on%2520ImageNet.%2520Our%2520method%2520advances%2520the%2520Pareto%2520frontier%2520for%2520energy%250Aconsumption%2520vs.%2520top-1%2520accuracy%2520for%2520quantized%2520convolutional%2520models%2520on%2520ImageNet.%250APROM%2520addresses%2520the%2520challenges%2520of%2520quantizing%2520depthwise-separable%2520convolutional%250Anetworks%2520to%2520both%2520ternary%2520and%25208-bit%2520weights%252C%2520offering%2520a%2520simple%2520way%2520to%2520reduce%250Aenergy%2520cost%2520and%2520storage%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROM%3A%20Prioritize%20Reduction%20of%20Multiplications%20Over%20Lower%20Bit-Widths%20for%0A%20%20Efficient%20CNNs&entry.906535625=Lukas%20Meiner%20and%20Jens%20Mehnert%20and%20Alexandru%20Paul%20Condurache&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20crucial%20for%20computer%20vision%20tasks%20on%0Aresource-constrained%20devices.%20Quantization%20effectively%20compresses%20these%20models%2C%0Areducing%20storage%20size%20and%20energy%20cost.%20However%2C%20in%20modern%20depthwise-separable%0Aarchitectures%2C%20the%20computational%20cost%20is%20distributed%20unevenly%20across%20its%0Acomponents%2C%20with%20pointwise%20operations%20being%20the%20most%20expensive.%20By%20applying%20a%0Ageneral%20quantization%20scheme%20to%20this%20imbalanced%20cost%20distribution%2C%20existing%0Aquantization%20approaches%20fail%20to%20fully%20exploit%20potential%20efficiency%20gains.%20To%0Athis%20end%2C%20we%20introduce%20PROM%2C%20a%20straightforward%20approach%20for%20quantizing%20modern%0Adepthwise-separable%20convolutional%20networks%20by%20selectively%20using%20two%20distinct%0Abit-widths.%20Specifically%2C%20pointwise%20convolutions%20are%20quantized%20to%20ternary%0Aweights%2C%20while%20the%20remaining%20modules%20use%208-bit%20weights%2C%20which%20is%20achieved%0Athrough%20a%20simple%20quantization-aware%20training%20procedure.%20Additionally%2C%20by%0Aquantizing%20activations%20to%208-bit%2C%20our%20method%20transforms%20pointwise%20convolutions%0Awith%20ternary%20weights%20into%20int8%20additions%2C%20which%20enjoy%20broad%20support%20across%0Ahardware%20platforms%20and%20effectively%20eliminates%20the%20need%20for%20expensive%0Amultiplications.%20Applying%20PROM%20to%20MobileNetV2%20reduces%20the%20model%27s%20energy%20cost%0Aby%20more%20than%20an%20order%20of%20magnitude%20%2823.9x%29%20and%20its%20storage%20size%20by%202.7x%0Acompared%20to%20the%20float16%20baseline%20while%20retaining%20similar%20classification%0Aperformance%20on%20ImageNet.%20Our%20method%20advances%20the%20Pareto%20frontier%20for%20energy%0Aconsumption%20vs.%20top-1%20accuracy%20for%20quantized%20convolutional%20models%20on%20ImageNet.%0APROM%20addresses%20the%20challenges%20of%20quantizing%20depthwise-separable%20convolutional%0Anetworks%20to%20both%20ternary%20and%208-bit%20weights%2C%20offering%20a%20simple%20way%20to%20reduce%0Aenergy%20cost%20and%20storage%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03254v2&entry.124074799=Read"},
{"title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation\n  Acceleration via Multi-Head Speculative Decoding", "author": "Dian Chen and Yansong Qu and Xinyang Li and Ming Li and Shengchuan Zhang", "abstract": "  Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.\n", "link": "http://arxiv.org/abs/2507.23777v2", "date": "2025-08-06", "relevancy": 2.1683, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5763}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XSpecMesh%3A%20Quality-Preserving%20Auto-Regressive%20Mesh%20Generation%0A%20%20Acceleration%20via%20Multi-Head%20Speculative%20Decoding&body=Title%3A%20XSpecMesh%3A%20Quality-Preserving%20Auto-Regressive%20Mesh%20Generation%0A%20%20Acceleration%20via%20Multi-Head%20Speculative%20Decoding%0AAuthor%3A%20Dian%20Chen%20and%20Yansong%20Qu%20and%20Xinyang%20Li%20and%20Ming%20Li%20and%20Shengchuan%20Zhang%0AAbstract%3A%20%20%20Current%20auto-regressive%20models%20can%20generate%20high-quality%2C%20topologically%0Aprecise%20meshes%3B%20however%2C%20they%20necessitate%20thousands-or%20even%20tens%20of%0Athousands-of%20next-token%20predictions%20during%20inference%2C%20resulting%20in%20substantial%0Alatency.%20We%20introduce%20XSpecMesh%2C%20a%20quality-preserving%20acceleration%20method%20for%0Aauto-regressive%20mesh%20generation%20models.%20XSpecMesh%20employs%20a%20lightweight%2C%0Amulti-head%20speculative%20decoding%20scheme%20to%20predict%20multiple%20tokens%20in%20parallel%0Awithin%20a%20single%20forward%20pass%2C%20thereby%20accelerating%20inference.%20We%20further%0Apropose%20a%20verification%20and%20resampling%20strategy%3A%20the%20backbone%20model%20verifies%0Aeach%20predicted%20token%20and%20resamples%20any%20tokens%20that%20do%20not%20meet%20the%20quality%0Acriteria.%20In%20addition%2C%20we%20propose%20a%20distillation%20strategy%20that%20trains%20the%0Alightweight%20decoding%20heads%20by%20distilling%20from%20the%20backbone%20model%2C%20encouraging%0Atheir%20prediction%20distributions%20to%20align%20and%20improving%20the%20success%20rate%20of%0Aspeculative%20predictions.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20a%201.7x%20speedup%20without%20sacrificing%20generation%20quality.%20Our%20code%20will%0Abe%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23777v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXSpecMesh%253A%2520Quality-Preserving%2520Auto-Regressive%2520Mesh%2520Generation%250A%2520%2520Acceleration%2520via%2520Multi-Head%2520Speculative%2520Decoding%26entry.906535625%3DDian%2520Chen%2520and%2520Yansong%2520Qu%2520and%2520Xinyang%2520Li%2520and%2520Ming%2520Li%2520and%2520Shengchuan%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520auto-regressive%2520models%2520can%2520generate%2520high-quality%252C%2520topologically%250Aprecise%2520meshes%253B%2520however%252C%2520they%2520necessitate%2520thousands-or%2520even%2520tens%2520of%250Athousands-of%2520next-token%2520predictions%2520during%2520inference%252C%2520resulting%2520in%2520substantial%250Alatency.%2520We%2520introduce%2520XSpecMesh%252C%2520a%2520quality-preserving%2520acceleration%2520method%2520for%250Aauto-regressive%2520mesh%2520generation%2520models.%2520XSpecMesh%2520employs%2520a%2520lightweight%252C%250Amulti-head%2520speculative%2520decoding%2520scheme%2520to%2520predict%2520multiple%2520tokens%2520in%2520parallel%250Awithin%2520a%2520single%2520forward%2520pass%252C%2520thereby%2520accelerating%2520inference.%2520We%2520further%250Apropose%2520a%2520verification%2520and%2520resampling%2520strategy%253A%2520the%2520backbone%2520model%2520verifies%250Aeach%2520predicted%2520token%2520and%2520resamples%2520any%2520tokens%2520that%2520do%2520not%2520meet%2520the%2520quality%250Acriteria.%2520In%2520addition%252C%2520we%2520propose%2520a%2520distillation%2520strategy%2520that%2520trains%2520the%250Alightweight%2520decoding%2520heads%2520by%2520distilling%2520from%2520the%2520backbone%2520model%252C%2520encouraging%250Atheir%2520prediction%2520distributions%2520to%2520align%2520and%2520improving%2520the%2520success%2520rate%2520of%250Aspeculative%2520predictions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520a%25201.7x%2520speedup%2520without%2520sacrificing%2520generation%2520quality.%2520Our%2520code%2520will%250Abe%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23777v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XSpecMesh%3A%20Quality-Preserving%20Auto-Regressive%20Mesh%20Generation%0A%20%20Acceleration%20via%20Multi-Head%20Speculative%20Decoding&entry.906535625=Dian%20Chen%20and%20Yansong%20Qu%20and%20Xinyang%20Li%20and%20Ming%20Li%20and%20Shengchuan%20Zhang&entry.1292438233=%20%20Current%20auto-regressive%20models%20can%20generate%20high-quality%2C%20topologically%0Aprecise%20meshes%3B%20however%2C%20they%20necessitate%20thousands-or%20even%20tens%20of%0Athousands-of%20next-token%20predictions%20during%20inference%2C%20resulting%20in%20substantial%0Alatency.%20We%20introduce%20XSpecMesh%2C%20a%20quality-preserving%20acceleration%20method%20for%0Aauto-regressive%20mesh%20generation%20models.%20XSpecMesh%20employs%20a%20lightweight%2C%0Amulti-head%20speculative%20decoding%20scheme%20to%20predict%20multiple%20tokens%20in%20parallel%0Awithin%20a%20single%20forward%20pass%2C%20thereby%20accelerating%20inference.%20We%20further%0Apropose%20a%20verification%20and%20resampling%20strategy%3A%20the%20backbone%20model%20verifies%0Aeach%20predicted%20token%20and%20resamples%20any%20tokens%20that%20do%20not%20meet%20the%20quality%0Acriteria.%20In%20addition%2C%20we%20propose%20a%20distillation%20strategy%20that%20trains%20the%0Alightweight%20decoding%20heads%20by%20distilling%20from%20the%20backbone%20model%2C%20encouraging%0Atheir%20prediction%20distributions%20to%20align%20and%20improving%20the%20success%20rate%20of%0Aspeculative%20predictions.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20a%201.7x%20speedup%20without%20sacrificing%20generation%20quality.%20Our%20code%20will%0Abe%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23777v2&entry.124074799=Read"},
{"title": "Who cuts emissions, who turns up the heat? causal machine learning\n  estimates of energy efficiency interventions", "author": "Bernardino D'Amico and Francesco Pomponi and Jay H. Arehart and Lina Khaddour", "abstract": "  Reducing domestic energy demand is central to climate mitigation and fuel\npoverty strategies, yet the impact of energy efficiency interventions is highly\nheterogeneous. Using a causal machine learning model trained on nationally\nrepresentative data of the English housing stock, we estimate average and\nconditional treatment effects of wall insulation on gas consumption, focusing\non distributional effects across energy burden subgroups. While interventions\nreduce gas demand on average (by as much as 19 percent), low energy burden\ngroups achieve substantial savings, whereas those experiencing high energy\nburdens see little to no reduction. This pattern reflects a\nbehaviourally-driven mechanism: households constrained by high costs-to-income\nratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort\nrather than lowering consumption. Far from wasteful, such responses represent\nrational adjustments in contexts of prior deprivation, with potential\nco-benefits for health and well-being. These findings call for a broader\nevaluation framework that accounts for both climate impacts and the equity\nimplications of domestic energy policy.\n", "link": "http://arxiv.org/abs/2508.04478v1", "date": "2025-08-06", "relevancy": 0.7824, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4078}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4043}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Who%20cuts%20emissions%2C%20who%20turns%20up%20the%20heat%3F%20causal%20machine%20learning%0A%20%20estimates%20of%20energy%20efficiency%20interventions&body=Title%3A%20Who%20cuts%20emissions%2C%20who%20turns%20up%20the%20heat%3F%20causal%20machine%20learning%0A%20%20estimates%20of%20energy%20efficiency%20interventions%0AAuthor%3A%20Bernardino%20D%27Amico%20and%20Francesco%20Pomponi%20and%20Jay%20H.%20Arehart%20and%20Lina%20Khaddour%0AAbstract%3A%20%20%20Reducing%20domestic%20energy%20demand%20is%20central%20to%20climate%20mitigation%20and%20fuel%0Apoverty%20strategies%2C%20yet%20the%20impact%20of%20energy%20efficiency%20interventions%20is%20highly%0Aheterogeneous.%20Using%20a%20causal%20machine%20learning%20model%20trained%20on%20nationally%0Arepresentative%20data%20of%20the%20English%20housing%20stock%2C%20we%20estimate%20average%20and%0Aconditional%20treatment%20effects%20of%20wall%20insulation%20on%20gas%20consumption%2C%20focusing%0Aon%20distributional%20effects%20across%20energy%20burden%20subgroups.%20While%20interventions%0Areduce%20gas%20demand%20on%20average%20%28by%20as%20much%20as%2019%20percent%29%2C%20low%20energy%20burden%0Agroups%20achieve%20substantial%20savings%2C%20whereas%20those%20experiencing%20high%20energy%0Aburdens%20see%20little%20to%20no%20reduction.%20This%20pattern%20reflects%20a%0Abehaviourally-driven%20mechanism%3A%20households%20constrained%20by%20high%20costs-to-income%0Aratios%20%28e.g.%20more%20than%200.1%29%20reallocate%20savings%20toward%20improved%20thermal%20comfort%0Arather%20than%20lowering%20consumption.%20Far%20from%20wasteful%2C%20such%20responses%20represent%0Arational%20adjustments%20in%20contexts%20of%20prior%20deprivation%2C%20with%20potential%0Aco-benefits%20for%20health%20and%20well-being.%20These%20findings%20call%20for%20a%20broader%0Aevaluation%20framework%20that%20accounts%20for%20both%20climate%20impacts%20and%20the%20equity%0Aimplications%20of%20domestic%20energy%20policy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWho%2520cuts%2520emissions%252C%2520who%2520turns%2520up%2520the%2520heat%253F%2520causal%2520machine%2520learning%250A%2520%2520estimates%2520of%2520energy%2520efficiency%2520interventions%26entry.906535625%3DBernardino%2520D%2527Amico%2520and%2520Francesco%2520Pomponi%2520and%2520Jay%2520H.%2520Arehart%2520and%2520Lina%2520Khaddour%26entry.1292438233%3D%2520%2520Reducing%2520domestic%2520energy%2520demand%2520is%2520central%2520to%2520climate%2520mitigation%2520and%2520fuel%250Apoverty%2520strategies%252C%2520yet%2520the%2520impact%2520of%2520energy%2520efficiency%2520interventions%2520is%2520highly%250Aheterogeneous.%2520Using%2520a%2520causal%2520machine%2520learning%2520model%2520trained%2520on%2520nationally%250Arepresentative%2520data%2520of%2520the%2520English%2520housing%2520stock%252C%2520we%2520estimate%2520average%2520and%250Aconditional%2520treatment%2520effects%2520of%2520wall%2520insulation%2520on%2520gas%2520consumption%252C%2520focusing%250Aon%2520distributional%2520effects%2520across%2520energy%2520burden%2520subgroups.%2520While%2520interventions%250Areduce%2520gas%2520demand%2520on%2520average%2520%2528by%2520as%2520much%2520as%252019%2520percent%2529%252C%2520low%2520energy%2520burden%250Agroups%2520achieve%2520substantial%2520savings%252C%2520whereas%2520those%2520experiencing%2520high%2520energy%250Aburdens%2520see%2520little%2520to%2520no%2520reduction.%2520This%2520pattern%2520reflects%2520a%250Abehaviourally-driven%2520mechanism%253A%2520households%2520constrained%2520by%2520high%2520costs-to-income%250Aratios%2520%2528e.g.%2520more%2520than%25200.1%2529%2520reallocate%2520savings%2520toward%2520improved%2520thermal%2520comfort%250Arather%2520than%2520lowering%2520consumption.%2520Far%2520from%2520wasteful%252C%2520such%2520responses%2520represent%250Arational%2520adjustments%2520in%2520contexts%2520of%2520prior%2520deprivation%252C%2520with%2520potential%250Aco-benefits%2520for%2520health%2520and%2520well-being.%2520These%2520findings%2520call%2520for%2520a%2520broader%250Aevaluation%2520framework%2520that%2520accounts%2520for%2520both%2520climate%2520impacts%2520and%2520the%2520equity%250Aimplications%2520of%2520domestic%2520energy%2520policy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20cuts%20emissions%2C%20who%20turns%20up%20the%20heat%3F%20causal%20machine%20learning%0A%20%20estimates%20of%20energy%20efficiency%20interventions&entry.906535625=Bernardino%20D%27Amico%20and%20Francesco%20Pomponi%20and%20Jay%20H.%20Arehart%20and%20Lina%20Khaddour&entry.1292438233=%20%20Reducing%20domestic%20energy%20demand%20is%20central%20to%20climate%20mitigation%20and%20fuel%0Apoverty%20strategies%2C%20yet%20the%20impact%20of%20energy%20efficiency%20interventions%20is%20highly%0Aheterogeneous.%20Using%20a%20causal%20machine%20learning%20model%20trained%20on%20nationally%0Arepresentative%20data%20of%20the%20English%20housing%20stock%2C%20we%20estimate%20average%20and%0Aconditional%20treatment%20effects%20of%20wall%20insulation%20on%20gas%20consumption%2C%20focusing%0Aon%20distributional%20effects%20across%20energy%20burden%20subgroups.%20While%20interventions%0Areduce%20gas%20demand%20on%20average%20%28by%20as%20much%20as%2019%20percent%29%2C%20low%20energy%20burden%0Agroups%20achieve%20substantial%20savings%2C%20whereas%20those%20experiencing%20high%20energy%0Aburdens%20see%20little%20to%20no%20reduction.%20This%20pattern%20reflects%20a%0Abehaviourally-driven%20mechanism%3A%20households%20constrained%20by%20high%20costs-to-income%0Aratios%20%28e.g.%20more%20than%200.1%29%20reallocate%20savings%20toward%20improved%20thermal%20comfort%0Arather%20than%20lowering%20consumption.%20Far%20from%20wasteful%2C%20such%20responses%20represent%0Arational%20adjustments%20in%20contexts%20of%20prior%20deprivation%2C%20with%20potential%0Aco-benefits%20for%20health%20and%20well-being.%20These%20findings%20call%20for%20a%20broader%0Aevaluation%20framework%20that%20accounts%20for%20both%20climate%20impacts%20and%20the%20equity%0Aimplications%20of%20domestic%20energy%20policy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04478v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


