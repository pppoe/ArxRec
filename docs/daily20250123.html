<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250122.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Sketch and Patch: Efficient 3D Gaussian Representation for Man-Made\n  Scenes", "author": "Yuang Shi and Simone Gasparini and G\u00e9raldine Morin and Chenggang Yang and Wei Tsang Ooi", "abstract": "  3D Gaussian Splatting (3DGS) has emerged as a promising representation for\nphotorealistic rendering of 3D scenes. However, its high storage requirements\npose significant challenges for practical applications. We observe that\nGaussians exhibit distinct roles and characteristics that are analogous to\ntraditional artistic techniques -- Like how artists first sketch outlines\nbefore filling in broader areas with color, some Gaussians capture\nhigh-frequency features like edges and contours; While other Gaussians\nrepresent broader, smoother regions, that are analogous to broader brush\nstrokes that add volume and depth to a painting. Based on this observation, we\npropose a novel hybrid representation that categorizes Gaussians into (i)\nSketch Gaussians, which define scene boundaries, and (ii) Patch Gaussians,\nwhich cover smooth regions. Sketch Gaussians are efficiently encoded using\nparametric models, leveraging their geometric coherence, while Patch Gaussians\nundergo optimized pruning, retraining, and vector quantization to maintain\nvolumetric consistency and storage efficiency. Our comprehensive evaluation\nacross diverse indoor and outdoor scenes demonstrates that this structure-aware\napproach achieves up to 32.62% improvement in PSNR, 19.12% in SSIM, and 45.41%\nin LPIPS at equivalent model sizes, and correspondingly, for an indoor scene,\nour model maintains the visual quality with 2.3% of the original model size.\n", "link": "http://arxiv.org/abs/2501.13045v1", "date": "2025-01-22", "relevancy": 3.3113, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6734}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6654}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch%20and%20Patch%3A%20Efficient%203D%20Gaussian%20Representation%20for%20Man-Made%0A%20%20Scenes&body=Title%3A%20Sketch%20and%20Patch%3A%20Efficient%203D%20Gaussian%20Representation%20for%20Man-Made%0A%20%20Scenes%0AAuthor%3A%20Yuang%20Shi%20and%20Simone%20Gasparini%20and%20G%C3%A9raldine%20Morin%20and%20Chenggang%20Yang%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20representation%20for%0Aphotorealistic%20rendering%20of%203D%20scenes.%20However%2C%20its%20high%20storage%20requirements%0Apose%20significant%20challenges%20for%20practical%20applications.%20We%20observe%20that%0AGaussians%20exhibit%20distinct%20roles%20and%20characteristics%20that%20are%20analogous%20to%0Atraditional%20artistic%20techniques%20--%20Like%20how%20artists%20first%20sketch%20outlines%0Abefore%20filling%20in%20broader%20areas%20with%20color%2C%20some%20Gaussians%20capture%0Ahigh-frequency%20features%20like%20edges%20and%20contours%3B%20While%20other%20Gaussians%0Arepresent%20broader%2C%20smoother%20regions%2C%20that%20are%20analogous%20to%20broader%20brush%0Astrokes%20that%20add%20volume%20and%20depth%20to%20a%20painting.%20Based%20on%20this%20observation%2C%20we%0Apropose%20a%20novel%20hybrid%20representation%20that%20categorizes%20Gaussians%20into%20%28i%29%0ASketch%20Gaussians%2C%20which%20define%20scene%20boundaries%2C%20and%20%28ii%29%20Patch%20Gaussians%2C%0Awhich%20cover%20smooth%20regions.%20Sketch%20Gaussians%20are%20efficiently%20encoded%20using%0Aparametric%20models%2C%20leveraging%20their%20geometric%20coherence%2C%20while%20Patch%20Gaussians%0Aundergo%20optimized%20pruning%2C%20retraining%2C%20and%20vector%20quantization%20to%20maintain%0Avolumetric%20consistency%20and%20storage%20efficiency.%20Our%20comprehensive%20evaluation%0Aacross%20diverse%20indoor%20and%20outdoor%20scenes%20demonstrates%20that%20this%20structure-aware%0Aapproach%20achieves%20up%20to%2032.62%25%20improvement%20in%20PSNR%2C%2019.12%25%20in%20SSIM%2C%20and%2045.41%25%0Ain%20LPIPS%20at%20equivalent%20model%20sizes%2C%20and%20correspondingly%2C%20for%20an%20indoor%20scene%2C%0Aour%20model%20maintains%20the%20visual%20quality%20with%202.3%25%20of%20the%20original%20model%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch%2520and%2520Patch%253A%2520Efficient%25203D%2520Gaussian%2520Representation%2520for%2520Man-Made%250A%2520%2520Scenes%26entry.906535625%3DYuang%2520Shi%2520and%2520Simone%2520Gasparini%2520and%2520G%25C3%25A9raldine%2520Morin%2520and%2520Chenggang%2520Yang%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%2520a%2520promising%2520representation%2520for%250Aphotorealistic%2520rendering%2520of%25203D%2520scenes.%2520However%252C%2520its%2520high%2520storage%2520requirements%250Apose%2520significant%2520challenges%2520for%2520practical%2520applications.%2520We%2520observe%2520that%250AGaussians%2520exhibit%2520distinct%2520roles%2520and%2520characteristics%2520that%2520are%2520analogous%2520to%250Atraditional%2520artistic%2520techniques%2520--%2520Like%2520how%2520artists%2520first%2520sketch%2520outlines%250Abefore%2520filling%2520in%2520broader%2520areas%2520with%2520color%252C%2520some%2520Gaussians%2520capture%250Ahigh-frequency%2520features%2520like%2520edges%2520and%2520contours%253B%2520While%2520other%2520Gaussians%250Arepresent%2520broader%252C%2520smoother%2520regions%252C%2520that%2520are%2520analogous%2520to%2520broader%2520brush%250Astrokes%2520that%2520add%2520volume%2520and%2520depth%2520to%2520a%2520painting.%2520Based%2520on%2520this%2520observation%252C%2520we%250Apropose%2520a%2520novel%2520hybrid%2520representation%2520that%2520categorizes%2520Gaussians%2520into%2520%2528i%2529%250ASketch%2520Gaussians%252C%2520which%2520define%2520scene%2520boundaries%252C%2520and%2520%2528ii%2529%2520Patch%2520Gaussians%252C%250Awhich%2520cover%2520smooth%2520regions.%2520Sketch%2520Gaussians%2520are%2520efficiently%2520encoded%2520using%250Aparametric%2520models%252C%2520leveraging%2520their%2520geometric%2520coherence%252C%2520while%2520Patch%2520Gaussians%250Aundergo%2520optimized%2520pruning%252C%2520retraining%252C%2520and%2520vector%2520quantization%2520to%2520maintain%250Avolumetric%2520consistency%2520and%2520storage%2520efficiency.%2520Our%2520comprehensive%2520evaluation%250Aacross%2520diverse%2520indoor%2520and%2520outdoor%2520scenes%2520demonstrates%2520that%2520this%2520structure-aware%250Aapproach%2520achieves%2520up%2520to%252032.62%2525%2520improvement%2520in%2520PSNR%252C%252019.12%2525%2520in%2520SSIM%252C%2520and%252045.41%2525%250Ain%2520LPIPS%2520at%2520equivalent%2520model%2520sizes%252C%2520and%2520correspondingly%252C%2520for%2520an%2520indoor%2520scene%252C%250Aour%2520model%2520maintains%2520the%2520visual%2520quality%2520with%25202.3%2525%2520of%2520the%2520original%2520model%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch%20and%20Patch%3A%20Efficient%203D%20Gaussian%20Representation%20for%20Man-Made%0A%20%20Scenes&entry.906535625=Yuang%20Shi%20and%20Simone%20Gasparini%20and%20G%C3%A9raldine%20Morin%20and%20Chenggang%20Yang%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%20a%20promising%20representation%20for%0Aphotorealistic%20rendering%20of%203D%20scenes.%20However%2C%20its%20high%20storage%20requirements%0Apose%20significant%20challenges%20for%20practical%20applications.%20We%20observe%20that%0AGaussians%20exhibit%20distinct%20roles%20and%20characteristics%20that%20are%20analogous%20to%0Atraditional%20artistic%20techniques%20--%20Like%20how%20artists%20first%20sketch%20outlines%0Abefore%20filling%20in%20broader%20areas%20with%20color%2C%20some%20Gaussians%20capture%0Ahigh-frequency%20features%20like%20edges%20and%20contours%3B%20While%20other%20Gaussians%0Arepresent%20broader%2C%20smoother%20regions%2C%20that%20are%20analogous%20to%20broader%20brush%0Astrokes%20that%20add%20volume%20and%20depth%20to%20a%20painting.%20Based%20on%20this%20observation%2C%20we%0Apropose%20a%20novel%20hybrid%20representation%20that%20categorizes%20Gaussians%20into%20%28i%29%0ASketch%20Gaussians%2C%20which%20define%20scene%20boundaries%2C%20and%20%28ii%29%20Patch%20Gaussians%2C%0Awhich%20cover%20smooth%20regions.%20Sketch%20Gaussians%20are%20efficiently%20encoded%20using%0Aparametric%20models%2C%20leveraging%20their%20geometric%20coherence%2C%20while%20Patch%20Gaussians%0Aundergo%20optimized%20pruning%2C%20retraining%2C%20and%20vector%20quantization%20to%20maintain%0Avolumetric%20consistency%20and%20storage%20efficiency.%20Our%20comprehensive%20evaluation%0Aacross%20diverse%20indoor%20and%20outdoor%20scenes%20demonstrates%20that%20this%20structure-aware%0Aapproach%20achieves%20up%20to%2032.62%25%20improvement%20in%20PSNR%2C%2019.12%25%20in%20SSIM%2C%20and%2045.41%25%0Ain%20LPIPS%20at%20equivalent%20model%20sizes%2C%20and%20correspondingly%2C%20for%20an%20indoor%20scene%2C%0Aour%20model%20maintains%20the%20visual%20quality%20with%202.3%25%20of%20the%20original%20model%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13045v1&entry.124074799=Read"},
{"title": "Search3D: Hierarchical Open-Vocabulary 3D Segmentation", "author": "Ayca Takmaz and Alexandros Delitzas and Robert W. Sumner and Francis Engelmann and Johanna Wald and Federico Tombari", "abstract": "  Open-vocabulary 3D segmentation enables exploration of 3D spaces using\nfree-form text descriptions. Existing methods for open-vocabulary 3D instance\nsegmentation primarily focus on identifying object-level instances but struggle\nwith finer-grained scene entities such as object parts, or regions described by\ngeneric attributes. In this work, we introduce Search3D, an approach to\nconstruct hierarchical open-vocabulary 3D scene representations, enabling 3D\nsearch at multiple levels of granularity: fine-grained object parts, entire\nobjects, or regions described by attributes like materials. Unlike prior\nmethods, Search3D shifts towards a more flexible open-vocabulary 3D search\nparadigm, moving beyond explicit object-centric queries. For systematic\nevaluation, we further contribute a scene-scale open-vocabulary 3D part\nsegmentation benchmark based on MultiScan, along with a set of open-vocabulary\nfine-grained part annotations on ScanNet++. Search3D outperforms baselines in\nscene-scale open-vocabulary 3D part segmentation, while maintaining strong\nperformance in segmenting 3D objects and materials. Our project page is\nhttp://search3d-segmentation.github.io.\n", "link": "http://arxiv.org/abs/2409.18431v2", "date": "2025-01-22", "relevancy": 3.2307, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6721}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search3D%3A%20Hierarchical%20Open-Vocabulary%203D%20Segmentation&body=Title%3A%20Search3D%3A%20Hierarchical%20Open-Vocabulary%203D%20Segmentation%0AAuthor%3A%20Ayca%20Takmaz%20and%20Alexandros%20Delitzas%20and%20Robert%20W.%20Sumner%20and%20Francis%20Engelmann%20and%20Johanna%20Wald%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Open-vocabulary%203D%20segmentation%20enables%20exploration%20of%203D%20spaces%20using%0Afree-form%20text%20descriptions.%20Existing%20methods%20for%20open-vocabulary%203D%20instance%0Asegmentation%20primarily%20focus%20on%20identifying%20object-level%20instances%20but%20struggle%0Awith%20finer-grained%20scene%20entities%20such%20as%20object%20parts%2C%20or%20regions%20described%20by%0Ageneric%20attributes.%20In%20this%20work%2C%20we%20introduce%20Search3D%2C%20an%20approach%20to%0Aconstruct%20hierarchical%20open-vocabulary%203D%20scene%20representations%2C%20enabling%203D%0Asearch%20at%20multiple%20levels%20of%20granularity%3A%20fine-grained%20object%20parts%2C%20entire%0Aobjects%2C%20or%20regions%20described%20by%20attributes%20like%20materials.%20Unlike%20prior%0Amethods%2C%20Search3D%20shifts%20towards%20a%20more%20flexible%20open-vocabulary%203D%20search%0Aparadigm%2C%20moving%20beyond%20explicit%20object-centric%20queries.%20For%20systematic%0Aevaluation%2C%20we%20further%20contribute%20a%20scene-scale%20open-vocabulary%203D%20part%0Asegmentation%20benchmark%20based%20on%20MultiScan%2C%20along%20with%20a%20set%20of%20open-vocabulary%0Afine-grained%20part%20annotations%20on%20ScanNet%2B%2B.%20Search3D%20outperforms%20baselines%20in%0Ascene-scale%20open-vocabulary%203D%20part%20segmentation%2C%20while%20maintaining%20strong%0Aperformance%20in%20segmenting%203D%20objects%20and%20materials.%20Our%20project%20page%20is%0Ahttp%3A//search3d-segmentation.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.18431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch3D%253A%2520Hierarchical%2520Open-Vocabulary%25203D%2520Segmentation%26entry.906535625%3DAyca%2520Takmaz%2520and%2520Alexandros%2520Delitzas%2520and%2520Robert%2520W.%2520Sumner%2520and%2520Francis%2520Engelmann%2520and%2520Johanna%2520Wald%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520segmentation%2520enables%2520exploration%2520of%25203D%2520spaces%2520using%250Afree-form%2520text%2520descriptions.%2520Existing%2520methods%2520for%2520open-vocabulary%25203D%2520instance%250Asegmentation%2520primarily%2520focus%2520on%2520identifying%2520object-level%2520instances%2520but%2520struggle%250Awith%2520finer-grained%2520scene%2520entities%2520such%2520as%2520object%2520parts%252C%2520or%2520regions%2520described%2520by%250Ageneric%2520attributes.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Search3D%252C%2520an%2520approach%2520to%250Aconstruct%2520hierarchical%2520open-vocabulary%25203D%2520scene%2520representations%252C%2520enabling%25203D%250Asearch%2520at%2520multiple%2520levels%2520of%2520granularity%253A%2520fine-grained%2520object%2520parts%252C%2520entire%250Aobjects%252C%2520or%2520regions%2520described%2520by%2520attributes%2520like%2520materials.%2520Unlike%2520prior%250Amethods%252C%2520Search3D%2520shifts%2520towards%2520a%2520more%2520flexible%2520open-vocabulary%25203D%2520search%250Aparadigm%252C%2520moving%2520beyond%2520explicit%2520object-centric%2520queries.%2520For%2520systematic%250Aevaluation%252C%2520we%2520further%2520contribute%2520a%2520scene-scale%2520open-vocabulary%25203D%2520part%250Asegmentation%2520benchmark%2520based%2520on%2520MultiScan%252C%2520along%2520with%2520a%2520set%2520of%2520open-vocabulary%250Afine-grained%2520part%2520annotations%2520on%2520ScanNet%252B%252B.%2520Search3D%2520outperforms%2520baselines%2520in%250Ascene-scale%2520open-vocabulary%25203D%2520part%2520segmentation%252C%2520while%2520maintaining%2520strong%250Aperformance%2520in%2520segmenting%25203D%2520objects%2520and%2520materials.%2520Our%2520project%2520page%2520is%250Ahttp%253A//search3d-segmentation.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.18431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search3D%3A%20Hierarchical%20Open-Vocabulary%203D%20Segmentation&entry.906535625=Ayca%20Takmaz%20and%20Alexandros%20Delitzas%20and%20Robert%20W.%20Sumner%20and%20Francis%20Engelmann%20and%20Johanna%20Wald%20and%20Federico%20Tombari&entry.1292438233=%20%20Open-vocabulary%203D%20segmentation%20enables%20exploration%20of%203D%20spaces%20using%0Afree-form%20text%20descriptions.%20Existing%20methods%20for%20open-vocabulary%203D%20instance%0Asegmentation%20primarily%20focus%20on%20identifying%20object-level%20instances%20but%20struggle%0Awith%20finer-grained%20scene%20entities%20such%20as%20object%20parts%2C%20or%20regions%20described%20by%0Ageneric%20attributes.%20In%20this%20work%2C%20we%20introduce%20Search3D%2C%20an%20approach%20to%0Aconstruct%20hierarchical%20open-vocabulary%203D%20scene%20representations%2C%20enabling%203D%0Asearch%20at%20multiple%20levels%20of%20granularity%3A%20fine-grained%20object%20parts%2C%20entire%0Aobjects%2C%20or%20regions%20described%20by%20attributes%20like%20materials.%20Unlike%20prior%0Amethods%2C%20Search3D%20shifts%20towards%20a%20more%20flexible%20open-vocabulary%203D%20search%0Aparadigm%2C%20moving%20beyond%20explicit%20object-centric%20queries.%20For%20systematic%0Aevaluation%2C%20we%20further%20contribute%20a%20scene-scale%20open-vocabulary%203D%20part%0Asegmentation%20benchmark%20based%20on%20MultiScan%2C%20along%20with%20a%20set%20of%20open-vocabulary%0Afine-grained%20part%20annotations%20on%20ScanNet%2B%2B.%20Search3D%20outperforms%20baselines%20in%0Ascene-scale%20open-vocabulary%203D%20part%20segmentation%2C%20while%20maintaining%20strong%0Aperformance%20in%20segmenting%203D%20objects%20and%20materials.%20Our%20project%20page%20is%0Ahttp%3A//search3d-segmentation.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.18431v2&entry.124074799=Read"},
{"title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding", "author": "Boqiang Zhang and Kehan Li and Zesen Cheng and Zhiqiang Hu and Yuqian Yuan and Guanzheng Chen and Sicong Leng and Yuming Jiang and Hang Zhang and Xin Li and Peng Jin and Wenqi Zhang and Fan Wang and Lidong Bing and Deli Zhao", "abstract": "  In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nvision-centric alignment stage, which warms up the vision encoder and\nprojector; 2) vision-language pretraining stage, which jointly tunes the vision\nencoder, projector, and LLM with large-scale image-text data covering multiple\ntypes (including scene images, documents, charts) as well as text-only data. 3)\nmulti-task fine-tuning stage, which incorporates image-text SFT data for\ndownstream tasks and video-text data to establish a foundation for video\nunderstanding. 4) video-centric fine-tuning, which further improves the model's\ncapability in video understanding. As for the framework design, to better\ncapture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.\n", "link": "http://arxiv.org/abs/2501.13106v1", "date": "2025-01-22", "relevancy": 3.2303, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6661}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLLaMA%203%3A%20Frontier%20Multimodal%20Foundation%20Models%20for%20Image%20and%20Video%0A%20%20Understanding&body=Title%3A%20VideoLLaMA%203%3A%20Frontier%20Multimodal%20Foundation%20Models%20for%20Image%20and%20Video%0A%20%20Understanding%0AAuthor%3A%20Boqiang%20Zhang%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Zhiqiang%20Hu%20and%20Yuqian%20Yuan%20and%20Guanzheng%20Chen%20and%20Sicong%20Leng%20and%20Yuming%20Jiang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Peng%20Jin%20and%20Wenqi%20Zhang%20and%20Fan%20Wang%20and%20Lidong%20Bing%20and%20Deli%20Zhao%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20VideoLLaMA3%2C%20a%20more%20advanced%20multimodal%20foundation%0Amodel%20for%20image%20and%20video%20understanding.%20The%20core%20design%20philosophy%20of%0AVideoLLaMA3%20is%20vision-centric.%20The%20meaning%20of%20%22vision-centric%22%20is%20two-fold%3A%20the%0Avision-centric%20training%20paradigm%20and%20vision-centric%20framework%20design.%20The%20key%0Ainsight%20of%20our%20vision-centric%20training%20paradigm%20is%20that%20high-quality%20image-text%0Adata%20is%20crucial%20for%20both%20image%20and%20video%20understanding.%20Instead%20of%20preparing%0Amassive%20video-text%20datasets%2C%20we%20focus%20on%20constructing%20large-scale%20and%0Ahigh-quality%20image-text%20datasets.%20VideoLLaMA3%20has%20four%20training%20stages%3A%201%29%0Avision-centric%20alignment%20stage%2C%20which%20warms%20up%20the%20vision%20encoder%20and%0Aprojector%3B%202%29%20vision-language%20pretraining%20stage%2C%20which%20jointly%20tunes%20the%20vision%0Aencoder%2C%20projector%2C%20and%20LLM%20with%20large-scale%20image-text%20data%20covering%20multiple%0Atypes%20%28including%20scene%20images%2C%20documents%2C%20charts%29%20as%20well%20as%20text-only%20data.%203%29%0Amulti-task%20fine-tuning%20stage%2C%20which%20incorporates%20image-text%20SFT%20data%20for%0Adownstream%20tasks%20and%20video-text%20data%20to%20establish%20a%20foundation%20for%20video%0Aunderstanding.%204%29%20video-centric%20fine-tuning%2C%20which%20further%20improves%20the%20model%27s%0Acapability%20in%20video%20understanding.%20As%20for%20the%20framework%20design%2C%20to%20better%0Acapture%20fine-grained%20details%20in%20images%2C%20the%20pretrained%20vision%20encoder%20is%0Aadapted%20to%20encode%20images%20of%20varying%20sizes%20into%20vision%20tokens%20with%20corresponding%0Anumbers%2C%20rather%20than%20a%20fixed%20number%20of%20tokens.%20For%20video%20inputs%2C%20we%20reduce%20the%0Anumber%20of%20vision%20tokens%20according%20to%20their%20similarity%20so%20that%20the%0Arepresentation%20of%20videos%20will%20be%20more%20precise%20and%20compact.%20Benefit%20from%0Avision-centric%20designs%2C%20VideoLLaMA3%20achieves%20compelling%20performances%20in%20both%0Aimage%20and%20video%20understanding%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLLaMA%25203%253A%2520Frontier%2520Multimodal%2520Foundation%2520Models%2520for%2520Image%2520and%2520Video%250A%2520%2520Understanding%26entry.906535625%3DBoqiang%2520Zhang%2520and%2520Kehan%2520Li%2520and%2520Zesen%2520Cheng%2520and%2520Zhiqiang%2520Hu%2520and%2520Yuqian%2520Yuan%2520and%2520Guanzheng%2520Chen%2520and%2520Sicong%2520Leng%2520and%2520Yuming%2520Jiang%2520and%2520Hang%2520Zhang%2520and%2520Xin%2520Li%2520and%2520Peng%2520Jin%2520and%2520Wenqi%2520Zhang%2520and%2520Fan%2520Wang%2520and%2520Lidong%2520Bing%2520and%2520Deli%2520Zhao%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520VideoLLaMA3%252C%2520a%2520more%2520advanced%2520multimodal%2520foundation%250Amodel%2520for%2520image%2520and%2520video%2520understanding.%2520The%2520core%2520design%2520philosophy%2520of%250AVideoLLaMA3%2520is%2520vision-centric.%2520The%2520meaning%2520of%2520%2522vision-centric%2522%2520is%2520two-fold%253A%2520the%250Avision-centric%2520training%2520paradigm%2520and%2520vision-centric%2520framework%2520design.%2520The%2520key%250Ainsight%2520of%2520our%2520vision-centric%2520training%2520paradigm%2520is%2520that%2520high-quality%2520image-text%250Adata%2520is%2520crucial%2520for%2520both%2520image%2520and%2520video%2520understanding.%2520Instead%2520of%2520preparing%250Amassive%2520video-text%2520datasets%252C%2520we%2520focus%2520on%2520constructing%2520large-scale%2520and%250Ahigh-quality%2520image-text%2520datasets.%2520VideoLLaMA3%2520has%2520four%2520training%2520stages%253A%25201%2529%250Avision-centric%2520alignment%2520stage%252C%2520which%2520warms%2520up%2520the%2520vision%2520encoder%2520and%250Aprojector%253B%25202%2529%2520vision-language%2520pretraining%2520stage%252C%2520which%2520jointly%2520tunes%2520the%2520vision%250Aencoder%252C%2520projector%252C%2520and%2520LLM%2520with%2520large-scale%2520image-text%2520data%2520covering%2520multiple%250Atypes%2520%2528including%2520scene%2520images%252C%2520documents%252C%2520charts%2529%2520as%2520well%2520as%2520text-only%2520data.%25203%2529%250Amulti-task%2520fine-tuning%2520stage%252C%2520which%2520incorporates%2520image-text%2520SFT%2520data%2520for%250Adownstream%2520tasks%2520and%2520video-text%2520data%2520to%2520establish%2520a%2520foundation%2520for%2520video%250Aunderstanding.%25204%2529%2520video-centric%2520fine-tuning%252C%2520which%2520further%2520improves%2520the%2520model%2527s%250Acapability%2520in%2520video%2520understanding.%2520As%2520for%2520the%2520framework%2520design%252C%2520to%2520better%250Acapture%2520fine-grained%2520details%2520in%2520images%252C%2520the%2520pretrained%2520vision%2520encoder%2520is%250Aadapted%2520to%2520encode%2520images%2520of%2520varying%2520sizes%2520into%2520vision%2520tokens%2520with%2520corresponding%250Anumbers%252C%2520rather%2520than%2520a%2520fixed%2520number%2520of%2520tokens.%2520For%2520video%2520inputs%252C%2520we%2520reduce%2520the%250Anumber%2520of%2520vision%2520tokens%2520according%2520to%2520their%2520similarity%2520so%2520that%2520the%250Arepresentation%2520of%2520videos%2520will%2520be%2520more%2520precise%2520and%2520compact.%2520Benefit%2520from%250Avision-centric%2520designs%252C%2520VideoLLaMA3%2520achieves%2520compelling%2520performances%2520in%2520both%250Aimage%2520and%2520video%2520understanding%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLLaMA%203%3A%20Frontier%20Multimodal%20Foundation%20Models%20for%20Image%20and%20Video%0A%20%20Understanding&entry.906535625=Boqiang%20Zhang%20and%20Kehan%20Li%20and%20Zesen%20Cheng%20and%20Zhiqiang%20Hu%20and%20Yuqian%20Yuan%20and%20Guanzheng%20Chen%20and%20Sicong%20Leng%20and%20Yuming%20Jiang%20and%20Hang%20Zhang%20and%20Xin%20Li%20and%20Peng%20Jin%20and%20Wenqi%20Zhang%20and%20Fan%20Wang%20and%20Lidong%20Bing%20and%20Deli%20Zhao&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20VideoLLaMA3%2C%20a%20more%20advanced%20multimodal%20foundation%0Amodel%20for%20image%20and%20video%20understanding.%20The%20core%20design%20philosophy%20of%0AVideoLLaMA3%20is%20vision-centric.%20The%20meaning%20of%20%22vision-centric%22%20is%20two-fold%3A%20the%0Avision-centric%20training%20paradigm%20and%20vision-centric%20framework%20design.%20The%20key%0Ainsight%20of%20our%20vision-centric%20training%20paradigm%20is%20that%20high-quality%20image-text%0Adata%20is%20crucial%20for%20both%20image%20and%20video%20understanding.%20Instead%20of%20preparing%0Amassive%20video-text%20datasets%2C%20we%20focus%20on%20constructing%20large-scale%20and%0Ahigh-quality%20image-text%20datasets.%20VideoLLaMA3%20has%20four%20training%20stages%3A%201%29%0Avision-centric%20alignment%20stage%2C%20which%20warms%20up%20the%20vision%20encoder%20and%0Aprojector%3B%202%29%20vision-language%20pretraining%20stage%2C%20which%20jointly%20tunes%20the%20vision%0Aencoder%2C%20projector%2C%20and%20LLM%20with%20large-scale%20image-text%20data%20covering%20multiple%0Atypes%20%28including%20scene%20images%2C%20documents%2C%20charts%29%20as%20well%20as%20text-only%20data.%203%29%0Amulti-task%20fine-tuning%20stage%2C%20which%20incorporates%20image-text%20SFT%20data%20for%0Adownstream%20tasks%20and%20video-text%20data%20to%20establish%20a%20foundation%20for%20video%0Aunderstanding.%204%29%20video-centric%20fine-tuning%2C%20which%20further%20improves%20the%20model%27s%0Acapability%20in%20video%20understanding.%20As%20for%20the%20framework%20design%2C%20to%20better%0Acapture%20fine-grained%20details%20in%20images%2C%20the%20pretrained%20vision%20encoder%20is%0Aadapted%20to%20encode%20images%20of%20varying%20sizes%20into%20vision%20tokens%20with%20corresponding%0Anumbers%2C%20rather%20than%20a%20fixed%20number%20of%20tokens.%20For%20video%20inputs%2C%20we%20reduce%20the%0Anumber%20of%20vision%20tokens%20according%20to%20their%20similarity%20so%20that%20the%0Arepresentation%20of%20videos%20will%20be%20more%20precise%20and%20compact.%20Benefit%20from%0Avision-centric%20designs%2C%20VideoLLaMA3%20achieves%20compelling%20performances%20in%20both%0Aimage%20and%20video%20understanding%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13106v1&entry.124074799=Read"},
{"title": "GSVC: Efficient Video Representation and Compression Through 2D Gaussian\n  Splatting", "author": "Longan Wang and Yuang Shi and Wei Tsang Ooi", "abstract": "  3D Gaussian splats have emerged as a revolutionary, effective, learned\nrepresentation for static 3D scenes. In this work, we explore using 2D Gaussian\nsplats as a new primitive for representing videos. We propose GSVC, an approach\nto learning a set of 2D Gaussian splats that can effectively represent and\ncompress video frames. GSVC incorporates the following techniques: (i) To\nexploit temporal redundancy among adjacent frames, which can speed up training\nand improve the compression efficiency, we predict the Gaussian splats of a\nframe based on its previous frame; (ii) To control the trade-offs between file\nsize and quality, we remove Gaussian splats with low contribution to the video\nquality; (iii) To capture dynamics in videos, we randomly add Gaussian splats\nto fit content with large motion or newly-appeared objects; (iv) To handle\nsignificant changes in the scene, we detect key frames based on loss\ndifferences during the learning process. Experiment results show that GSVC\nachieves good rate-distortion trade-offs, comparable to state-of-the-art video\ncodecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080\nvideo.\n", "link": "http://arxiv.org/abs/2501.12060v2", "date": "2025-01-22", "relevancy": 3.2281, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6703}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6388}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSVC%3A%20Efficient%20Video%20Representation%20and%20Compression%20Through%202D%20Gaussian%0A%20%20Splatting&body=Title%3A%20GSVC%3A%20Efficient%20Video%20Representation%20and%20Compression%20Through%202D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Longan%20Wang%20and%20Yuang%20Shi%20and%20Wei%20Tsang%20Ooi%0AAbstract%3A%20%20%203D%20Gaussian%20splats%20have%20emerged%20as%20a%20revolutionary%2C%20effective%2C%20learned%0Arepresentation%20for%20static%203D%20scenes.%20In%20this%20work%2C%20we%20explore%20using%202D%20Gaussian%0Asplats%20as%20a%20new%20primitive%20for%20representing%20videos.%20We%20propose%20GSVC%2C%20an%20approach%0Ato%20learning%20a%20set%20of%202D%20Gaussian%20splats%20that%20can%20effectively%20represent%20and%0Acompress%20video%20frames.%20GSVC%20incorporates%20the%20following%20techniques%3A%20%28i%29%20To%0Aexploit%20temporal%20redundancy%20among%20adjacent%20frames%2C%20which%20can%20speed%20up%20training%0Aand%20improve%20the%20compression%20efficiency%2C%20we%20predict%20the%20Gaussian%20splats%20of%20a%0Aframe%20based%20on%20its%20previous%20frame%3B%20%28ii%29%20To%20control%20the%20trade-offs%20between%20file%0Asize%20and%20quality%2C%20we%20remove%20Gaussian%20splats%20with%20low%20contribution%20to%20the%20video%0Aquality%3B%20%28iii%29%20To%20capture%20dynamics%20in%20videos%2C%20we%20randomly%20add%20Gaussian%20splats%0Ato%20fit%20content%20with%20large%20motion%20or%20newly-appeared%20objects%3B%20%28iv%29%20To%20handle%0Asignificant%20changes%20in%20the%20scene%2C%20we%20detect%20key%20frames%20based%20on%20loss%0Adifferences%20during%20the%20learning%20process.%20Experiment%20results%20show%20that%20GSVC%0Aachieves%20good%20rate-distortion%20trade-offs%2C%20comparable%20to%20state-of-the-art%20video%0Acodecs%20such%20as%20AV1%20and%20VVC%2C%20and%20a%20rendering%20speed%20of%201500%20fps%20for%20a%201920x1080%0Avideo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSVC%253A%2520Efficient%2520Video%2520Representation%2520and%2520Compression%2520Through%25202D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DLongan%2520Wang%2520and%2520Yuang%2520Shi%2520and%2520Wei%2520Tsang%2520Ooi%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520splats%2520have%2520emerged%2520as%2520a%2520revolutionary%252C%2520effective%252C%2520learned%250Arepresentation%2520for%2520static%25203D%2520scenes.%2520In%2520this%2520work%252C%2520we%2520explore%2520using%25202D%2520Gaussian%250Asplats%2520as%2520a%2520new%2520primitive%2520for%2520representing%2520videos.%2520We%2520propose%2520GSVC%252C%2520an%2520approach%250Ato%2520learning%2520a%2520set%2520of%25202D%2520Gaussian%2520splats%2520that%2520can%2520effectively%2520represent%2520and%250Acompress%2520video%2520frames.%2520GSVC%2520incorporates%2520the%2520following%2520techniques%253A%2520%2528i%2529%2520To%250Aexploit%2520temporal%2520redundancy%2520among%2520adjacent%2520frames%252C%2520which%2520can%2520speed%2520up%2520training%250Aand%2520improve%2520the%2520compression%2520efficiency%252C%2520we%2520predict%2520the%2520Gaussian%2520splats%2520of%2520a%250Aframe%2520based%2520on%2520its%2520previous%2520frame%253B%2520%2528ii%2529%2520To%2520control%2520the%2520trade-offs%2520between%2520file%250Asize%2520and%2520quality%252C%2520we%2520remove%2520Gaussian%2520splats%2520with%2520low%2520contribution%2520to%2520the%2520video%250Aquality%253B%2520%2528iii%2529%2520To%2520capture%2520dynamics%2520in%2520videos%252C%2520we%2520randomly%2520add%2520Gaussian%2520splats%250Ato%2520fit%2520content%2520with%2520large%2520motion%2520or%2520newly-appeared%2520objects%253B%2520%2528iv%2529%2520To%2520handle%250Asignificant%2520changes%2520in%2520the%2520scene%252C%2520we%2520detect%2520key%2520frames%2520based%2520on%2520loss%250Adifferences%2520during%2520the%2520learning%2520process.%2520Experiment%2520results%2520show%2520that%2520GSVC%250Aachieves%2520good%2520rate-distortion%2520trade-offs%252C%2520comparable%2520to%2520state-of-the-art%2520video%250Acodecs%2520such%2520as%2520AV1%2520and%2520VVC%252C%2520and%2520a%2520rendering%2520speed%2520of%25201500%2520fps%2520for%2520a%25201920x1080%250Avideo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSVC%3A%20Efficient%20Video%20Representation%20and%20Compression%20Through%202D%20Gaussian%0A%20%20Splatting&entry.906535625=Longan%20Wang%20and%20Yuang%20Shi%20and%20Wei%20Tsang%20Ooi&entry.1292438233=%20%203D%20Gaussian%20splats%20have%20emerged%20as%20a%20revolutionary%2C%20effective%2C%20learned%0Arepresentation%20for%20static%203D%20scenes.%20In%20this%20work%2C%20we%20explore%20using%202D%20Gaussian%0Asplats%20as%20a%20new%20primitive%20for%20representing%20videos.%20We%20propose%20GSVC%2C%20an%20approach%0Ato%20learning%20a%20set%20of%202D%20Gaussian%20splats%20that%20can%20effectively%20represent%20and%0Acompress%20video%20frames.%20GSVC%20incorporates%20the%20following%20techniques%3A%20%28i%29%20To%0Aexploit%20temporal%20redundancy%20among%20adjacent%20frames%2C%20which%20can%20speed%20up%20training%0Aand%20improve%20the%20compression%20efficiency%2C%20we%20predict%20the%20Gaussian%20splats%20of%20a%0Aframe%20based%20on%20its%20previous%20frame%3B%20%28ii%29%20To%20control%20the%20trade-offs%20between%20file%0Asize%20and%20quality%2C%20we%20remove%20Gaussian%20splats%20with%20low%20contribution%20to%20the%20video%0Aquality%3B%20%28iii%29%20To%20capture%20dynamics%20in%20videos%2C%20we%20randomly%20add%20Gaussian%20splats%0Ato%20fit%20content%20with%20large%20motion%20or%20newly-appeared%20objects%3B%20%28iv%29%20To%20handle%0Asignificant%20changes%20in%20the%20scene%2C%20we%20detect%20key%20frames%20based%20on%20loss%0Adifferences%20during%20the%20learning%20process.%20Experiment%20results%20show%20that%20GSVC%0Aachieves%20good%20rate-distortion%20trade-offs%2C%20comparable%20to%20state-of-the-art%20video%0Acodecs%20such%20as%20AV1%20and%20VVC%2C%20and%20a%20rendering%20speed%20of%201500%20fps%20for%20a%201920x1080%0Avideo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12060v2&entry.124074799=Read"},
{"title": "InternVideo2.5: Empowering Video MLLMs with Long and Rich Context\n  Modeling", "author": "Yi Wang and Xinhao Li and Ziang Yan and Yinan He and Jiashuo Yu and Xiangyu Zeng and Chenting Wang and Changlian Ma and Haian Huang and Jianfei Gao and Min Dou and Kai Chen and Wenhai Wang and Yu Qiao and Yali Wang and Limin Wang", "abstract": "  This paper aims to improve the performance of video multimodal large language\nmodels (MLLM) via long and rich context (LRC) modeling. As a result, we develop\na new version of InternVideo2.5 with a focus on enhancing the original MLLMs'\nability to perceive fine-grained details and capture long-form temporal\nstructure in videos. Specifically, our approach incorporates dense vision task\nannotations into MLLMs using direct preference optimization and develops\ncompact spatiotemporal representations through adaptive hierarchical token\ncompression. Experimental results demonstrate this unique design of LRC greatly\nimproves the results of video MLLM in mainstream video understanding benchmarks\n(short & long), enabling the MLLM to memorize significantly longer video inputs\n(at least 6x longer than the original), and master specialized vision\ncapabilities like object tracking and segmentation. Our work highlights the\nimportance of multimodal context richness (length and fineness) in empowering\nMLLM's innate abilites (focus and memory), providing new insights for future\nresearch on video MLLM. Code and models are available at\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5\n", "link": "http://arxiv.org/abs/2501.12386v2", "date": "2025-01-22", "relevancy": 3.0905, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVideo2.5%3A%20Empowering%20Video%20MLLMs%20with%20Long%20and%20Rich%20Context%0A%20%20Modeling&body=Title%3A%20InternVideo2.5%3A%20Empowering%20Video%20MLLMs%20with%20Long%20and%20Rich%20Context%0A%20%20Modeling%0AAuthor%3A%20Yi%20Wang%20and%20Xinhao%20Li%20and%20Ziang%20Yan%20and%20Yinan%20He%20and%20Jiashuo%20Yu%20and%20Xiangyu%20Zeng%20and%20Chenting%20Wang%20and%20Changlian%20Ma%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Min%20Dou%20and%20Kai%20Chen%20and%20Wenhai%20Wang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20improve%20the%20performance%20of%20video%20multimodal%20large%20language%0Amodels%20%28MLLM%29%20via%20long%20and%20rich%20context%20%28LRC%29%20modeling.%20As%20a%20result%2C%20we%20develop%0Aa%20new%20version%20of%20InternVideo2.5%20with%20a%20focus%20on%20enhancing%20the%20original%20MLLMs%27%0Aability%20to%20perceive%20fine-grained%20details%20and%20capture%20long-form%20temporal%0Astructure%20in%20videos.%20Specifically%2C%20our%20approach%20incorporates%20dense%20vision%20task%0Aannotations%20into%20MLLMs%20using%20direct%20preference%20optimization%20and%20develops%0Acompact%20spatiotemporal%20representations%20through%20adaptive%20hierarchical%20token%0Acompression.%20Experimental%20results%20demonstrate%20this%20unique%20design%20of%20LRC%20greatly%0Aimproves%20the%20results%20of%20video%20MLLM%20in%20mainstream%20video%20understanding%20benchmarks%0A%28short%20%26%20long%29%2C%20enabling%20the%20MLLM%20to%20memorize%20significantly%20longer%20video%20inputs%0A%28at%20least%206x%20longer%20than%20the%20original%29%2C%20and%20master%20specialized%20vision%0Acapabilities%20like%20object%20tracking%20and%20segmentation.%20Our%20work%20highlights%20the%0Aimportance%20of%20multimodal%20context%20richness%20%28length%20and%20fineness%29%20in%20empowering%0AMLLM%27s%20innate%20abilites%20%28focus%20and%20memory%29%2C%20providing%20new%20insights%20for%20future%0Aresearch%20on%20video%20MLLM.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12386v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVideo2.5%253A%2520Empowering%2520Video%2520MLLMs%2520with%2520Long%2520and%2520Rich%2520Context%250A%2520%2520Modeling%26entry.906535625%3DYi%2520Wang%2520and%2520Xinhao%2520Li%2520and%2520Ziang%2520Yan%2520and%2520Yinan%2520He%2520and%2520Jiashuo%2520Yu%2520and%2520Xiangyu%2520Zeng%2520and%2520Chenting%2520Wang%2520and%2520Changlian%2520Ma%2520and%2520Haian%2520Huang%2520and%2520Jianfei%2520Gao%2520and%2520Min%2520Dou%2520and%2520Kai%2520Chen%2520and%2520Wenhai%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520improve%2520the%2520performance%2520of%2520video%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLM%2529%2520via%2520long%2520and%2520rich%2520context%2520%2528LRC%2529%2520modeling.%2520As%2520a%2520result%252C%2520we%2520develop%250Aa%2520new%2520version%2520of%2520InternVideo2.5%2520with%2520a%2520focus%2520on%2520enhancing%2520the%2520original%2520MLLMs%2527%250Aability%2520to%2520perceive%2520fine-grained%2520details%2520and%2520capture%2520long-form%2520temporal%250Astructure%2520in%2520videos.%2520Specifically%252C%2520our%2520approach%2520incorporates%2520dense%2520vision%2520task%250Aannotations%2520into%2520MLLMs%2520using%2520direct%2520preference%2520optimization%2520and%2520develops%250Acompact%2520spatiotemporal%2520representations%2520through%2520adaptive%2520hierarchical%2520token%250Acompression.%2520Experimental%2520results%2520demonstrate%2520this%2520unique%2520design%2520of%2520LRC%2520greatly%250Aimproves%2520the%2520results%2520of%2520video%2520MLLM%2520in%2520mainstream%2520video%2520understanding%2520benchmarks%250A%2528short%2520%2526%2520long%2529%252C%2520enabling%2520the%2520MLLM%2520to%2520memorize%2520significantly%2520longer%2520video%2520inputs%250A%2528at%2520least%25206x%2520longer%2520than%2520the%2520original%2529%252C%2520and%2520master%2520specialized%2520vision%250Acapabilities%2520like%2520object%2520tracking%2520and%2520segmentation.%2520Our%2520work%2520highlights%2520the%250Aimportance%2520of%2520multimodal%2520context%2520richness%2520%2528length%2520and%2520fineness%2529%2520in%2520empowering%250AMLLM%2527s%2520innate%2520abilites%2520%2528focus%2520and%2520memory%2529%252C%2520providing%2520new%2520insights%2520for%2520future%250Aresearch%2520on%2520video%2520MLLM.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12386v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVideo2.5%3A%20Empowering%20Video%20MLLMs%20with%20Long%20and%20Rich%20Context%0A%20%20Modeling&entry.906535625=Yi%20Wang%20and%20Xinhao%20Li%20and%20Ziang%20Yan%20and%20Yinan%20He%20and%20Jiashuo%20Yu%20and%20Xiangyu%20Zeng%20and%20Chenting%20Wang%20and%20Changlian%20Ma%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Min%20Dou%20and%20Kai%20Chen%20and%20Wenhai%20Wang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20This%20paper%20aims%20to%20improve%20the%20performance%20of%20video%20multimodal%20large%20language%0Amodels%20%28MLLM%29%20via%20long%20and%20rich%20context%20%28LRC%29%20modeling.%20As%20a%20result%2C%20we%20develop%0Aa%20new%20version%20of%20InternVideo2.5%20with%20a%20focus%20on%20enhancing%20the%20original%20MLLMs%27%0Aability%20to%20perceive%20fine-grained%20details%20and%20capture%20long-form%20temporal%0Astructure%20in%20videos.%20Specifically%2C%20our%20approach%20incorporates%20dense%20vision%20task%0Aannotations%20into%20MLLMs%20using%20direct%20preference%20optimization%20and%20develops%0Acompact%20spatiotemporal%20representations%20through%20adaptive%20hierarchical%20token%0Acompression.%20Experimental%20results%20demonstrate%20this%20unique%20design%20of%20LRC%20greatly%0Aimproves%20the%20results%20of%20video%20MLLM%20in%20mainstream%20video%20understanding%20benchmarks%0A%28short%20%26%20long%29%2C%20enabling%20the%20MLLM%20to%20memorize%20significantly%20longer%20video%20inputs%0A%28at%20least%206x%20longer%20than%20the%20original%29%2C%20and%20master%20specialized%20vision%0Acapabilities%20like%20object%20tracking%20and%20segmentation.%20Our%20work%20highlights%20the%0Aimportance%20of%20multimodal%20context%20richness%20%28length%20and%20fineness%29%20in%20empowering%0AMLLM%27s%20innate%20abilites%20%28focus%20and%20memory%29%2C%20providing%20new%20insights%20for%20future%0Aresearch%20on%20video%20MLLM.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12386v2&entry.124074799=Read"},
{"title": "VisMin: Visual Minimal-Change Understanding", "author": "Rabiul Awal and Saba Ahmadi and Le Zhang and Aishwarya Agrawal", "abstract": "  Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar captions given an image. In this paper, we introduce a new, challenging\nbenchmark termed Visual Minimal-Change Understanding (VisMin), which requires\nmodels to predict the correct image-caption match given two images and two\ncaptions. The image pair and caption pair contain minimal changes, i.e., only\none aspect changes at a time from among the following: object, attribute,\ncount, and spatial relation. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at https://vismin.net/.\n", "link": "http://arxiv.org/abs/2407.16772v2", "date": "2025-01-22", "relevancy": 2.9809, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6099}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6099}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisMin%3A%20Visual%20Minimal-Change%20Understanding&body=Title%3A%20VisMin%3A%20Visual%20Minimal-Change%20Understanding%0AAuthor%3A%20Rabiul%20Awal%20and%20Saba%20Ahmadi%20and%20Le%20Zhang%20and%20Aishwarya%20Agrawal%0AAbstract%3A%20%20%20Fine-grained%20understanding%20of%20objects%2C%20attributes%2C%20and%20relationships%20between%0Aobjects%20is%20crucial%20for%20visual-language%20models%20%28VLMs%29.%20Existing%20benchmarks%0Aprimarily%20focus%20on%20evaluating%20VLMs%27%20capability%20to%20distinguish%20between%20two%20very%0Asimilar%20captions%20given%20an%20image.%20In%20this%20paper%2C%20we%20introduce%20a%20new%2C%20challenging%0Abenchmark%20termed%20Visual%20Minimal-Change%20Understanding%20%28VisMin%29%2C%20which%20requires%0Amodels%20to%20predict%20the%20correct%20image-caption%20match%20given%20two%20images%20and%20two%0Acaptions.%20The%20image%20pair%20and%20caption%20pair%20contain%20minimal%20changes%2C%20i.e.%2C%20only%0Aone%20aspect%20changes%20at%20a%20time%20from%20among%20the%20following%3A%20object%2C%20attribute%2C%0Acount%2C%20and%20spatial%20relation.%20These%20changes%20test%20the%20models%27%20understanding%20of%0Aobjects%2C%20attributes%20%28such%20as%20color%2C%20material%2C%20shape%29%2C%20counts%2C%20and%20spatial%0Arelationships%20between%20objects.%20We%20built%20an%20automatic%20framework%20using%20large%0Alanguage%20models%20and%20diffusion%20models%2C%20followed%20by%20a%20rigorous%204-step%0Averification%20process%20by%20human%20annotators.%20Empirical%20experiments%20reveal%20that%0Acurrent%20VLMs%20exhibit%20notable%20deficiencies%20in%20understanding%20spatial%0Arelationships%20and%20counting%20abilities.%20We%20also%20generate%20a%20large-scale%20training%0Adataset%20to%20finetune%20CLIP%20and%20Idefics2%2C%20showing%20significant%20improvements%20in%0Afine-grained%20understanding%20across%20benchmarks%20and%20in%20CLIP%27s%20general%20image-text%0Aalignment.%20We%20release%20all%20resources%2C%20including%20the%20benchmark%2C%20training%20data%2C%0Aand%20finetuned%20model%20checkpoints%2C%20at%20https%3A//vismin.net/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisMin%253A%2520Visual%2520Minimal-Change%2520Understanding%26entry.906535625%3DRabiul%2520Awal%2520and%2520Saba%2520Ahmadi%2520and%2520Le%2520Zhang%2520and%2520Aishwarya%2520Agrawal%26entry.1292438233%3D%2520%2520Fine-grained%2520understanding%2520of%2520objects%252C%2520attributes%252C%2520and%2520relationships%2520between%250Aobjects%2520is%2520crucial%2520for%2520visual-language%2520models%2520%2528VLMs%2529.%2520Existing%2520benchmarks%250Aprimarily%2520focus%2520on%2520evaluating%2520VLMs%2527%2520capability%2520to%2520distinguish%2520between%2520two%2520very%250Asimilar%2520captions%2520given%2520an%2520image.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%252C%2520challenging%250Abenchmark%2520termed%2520Visual%2520Minimal-Change%2520Understanding%2520%2528VisMin%2529%252C%2520which%2520requires%250Amodels%2520to%2520predict%2520the%2520correct%2520image-caption%2520match%2520given%2520two%2520images%2520and%2520two%250Acaptions.%2520The%2520image%2520pair%2520and%2520caption%2520pair%2520contain%2520minimal%2520changes%252C%2520i.e.%252C%2520only%250Aone%2520aspect%2520changes%2520at%2520a%2520time%2520from%2520among%2520the%2520following%253A%2520object%252C%2520attribute%252C%250Acount%252C%2520and%2520spatial%2520relation.%2520These%2520changes%2520test%2520the%2520models%2527%2520understanding%2520of%250Aobjects%252C%2520attributes%2520%2528such%2520as%2520color%252C%2520material%252C%2520shape%2529%252C%2520counts%252C%2520and%2520spatial%250Arelationships%2520between%2520objects.%2520We%2520built%2520an%2520automatic%2520framework%2520using%2520large%250Alanguage%2520models%2520and%2520diffusion%2520models%252C%2520followed%2520by%2520a%2520rigorous%25204-step%250Averification%2520process%2520by%2520human%2520annotators.%2520Empirical%2520experiments%2520reveal%2520that%250Acurrent%2520VLMs%2520exhibit%2520notable%2520deficiencies%2520in%2520understanding%2520spatial%250Arelationships%2520and%2520counting%2520abilities.%2520We%2520also%2520generate%2520a%2520large-scale%2520training%250Adataset%2520to%2520finetune%2520CLIP%2520and%2520Idefics2%252C%2520showing%2520significant%2520improvements%2520in%250Afine-grained%2520understanding%2520across%2520benchmarks%2520and%2520in%2520CLIP%2527s%2520general%2520image-text%250Aalignment.%2520We%2520release%2520all%2520resources%252C%2520including%2520the%2520benchmark%252C%2520training%2520data%252C%250Aand%2520finetuned%2520model%2520checkpoints%252C%2520at%2520https%253A//vismin.net/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisMin%3A%20Visual%20Minimal-Change%20Understanding&entry.906535625=Rabiul%20Awal%20and%20Saba%20Ahmadi%20and%20Le%20Zhang%20and%20Aishwarya%20Agrawal&entry.1292438233=%20%20Fine-grained%20understanding%20of%20objects%2C%20attributes%2C%20and%20relationships%20between%0Aobjects%20is%20crucial%20for%20visual-language%20models%20%28VLMs%29.%20Existing%20benchmarks%0Aprimarily%20focus%20on%20evaluating%20VLMs%27%20capability%20to%20distinguish%20between%20two%20very%0Asimilar%20captions%20given%20an%20image.%20In%20this%20paper%2C%20we%20introduce%20a%20new%2C%20challenging%0Abenchmark%20termed%20Visual%20Minimal-Change%20Understanding%20%28VisMin%29%2C%20which%20requires%0Amodels%20to%20predict%20the%20correct%20image-caption%20match%20given%20two%20images%20and%20two%0Acaptions.%20The%20image%20pair%20and%20caption%20pair%20contain%20minimal%20changes%2C%20i.e.%2C%20only%0Aone%20aspect%20changes%20at%20a%20time%20from%20among%20the%20following%3A%20object%2C%20attribute%2C%0Acount%2C%20and%20spatial%20relation.%20These%20changes%20test%20the%20models%27%20understanding%20of%0Aobjects%2C%20attributes%20%28such%20as%20color%2C%20material%2C%20shape%29%2C%20counts%2C%20and%20spatial%0Arelationships%20between%20objects.%20We%20built%20an%20automatic%20framework%20using%20large%0Alanguage%20models%20and%20diffusion%20models%2C%20followed%20by%20a%20rigorous%204-step%0Averification%20process%20by%20human%20annotators.%20Empirical%20experiments%20reveal%20that%0Acurrent%20VLMs%20exhibit%20notable%20deficiencies%20in%20understanding%20spatial%0Arelationships%20and%20counting%20abilities.%20We%20also%20generate%20a%20large-scale%20training%0Adataset%20to%20finetune%20CLIP%20and%20Idefics2%2C%20showing%20significant%20improvements%20in%0Afine-grained%20understanding%20across%20benchmarks%20and%20in%20CLIP%27s%20general%20image-text%0Aalignment.%20We%20release%20all%20resources%2C%20including%20the%20benchmark%2C%20training%20data%2C%0Aand%20finetuned%20model%20checkpoints%2C%20at%20https%3A//vismin.net/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16772v2&entry.124074799=Read"},
{"title": "Neural Radiance Fields for the Real World: A Survey", "author": "Wenhui Xiao and Remi Chierchia and Rodrigo Santa Cruz and Xuesong Li and David Ahmedt-Aristizabal and Olivier Salvado and Clinton Fookes and Leo Lebrat", "abstract": "  Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since\nrelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,\nadvancing different fields and applications such as scene understanding, 3D\ncontent generation, and robotics. Despite significant research progress, a\nthorough review of recent innovations, applications, and challenges is lacking.\nThis survey compiles key theoretical advancements and alternative\nrepresentations and investigates emerging challenges. It further explores\napplications on reconstruction, highlights NeRFs' impact on computer vision and\nrobotics, and reviews essential datasets and toolkits. By identifying gaps in\nthe literature, this survey discusses open challenges and offers directions for\nfuture research.\n", "link": "http://arxiv.org/abs/2501.13104v1", "date": "2025-01-22", "relevancy": 2.9219, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.592}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Radiance%20Fields%20for%20the%20Real%20World%3A%20A%20Survey&body=Title%3A%20Neural%20Radiance%20Fields%20for%20the%20Real%20World%3A%20A%20Survey%0AAuthor%3A%20Wenhui%20Xiao%20and%20Remi%20Chierchia%20and%20Rodrigo%20Santa%20Cruz%20and%20Xuesong%20Li%20and%20David%20Ahmedt-Aristizabal%20and%20Olivier%20Salvado%20and%20Clinton%20Fookes%20and%20Leo%20Lebrat%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20remodeled%203D%20scene%20representation%20since%0Arelease.%20NeRFs%20can%20effectively%20reconstruct%20complex%203D%20scenes%20from%202D%20images%2C%0Aadvancing%20different%20fields%20and%20applications%20such%20as%20scene%20understanding%2C%203D%0Acontent%20generation%2C%20and%20robotics.%20Despite%20significant%20research%20progress%2C%20a%0Athorough%20review%20of%20recent%20innovations%2C%20applications%2C%20and%20challenges%20is%20lacking.%0AThis%20survey%20compiles%20key%20theoretical%20advancements%20and%20alternative%0Arepresentations%20and%20investigates%20emerging%20challenges.%20It%20further%20explores%0Aapplications%20on%20reconstruction%2C%20highlights%20NeRFs%27%20impact%20on%20computer%20vision%20and%0Arobotics%2C%20and%20reviews%20essential%20datasets%20and%20toolkits.%20By%20identifying%20gaps%20in%0Athe%20literature%2C%20this%20survey%20discusses%20open%20challenges%20and%20offers%20directions%20for%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Radiance%2520Fields%2520for%2520the%2520Real%2520World%253A%2520A%2520Survey%26entry.906535625%3DWenhui%2520Xiao%2520and%2520Remi%2520Chierchia%2520and%2520Rodrigo%2520Santa%2520Cruz%2520and%2520Xuesong%2520Li%2520and%2520David%2520Ahmedt-Aristizabal%2520and%2520Olivier%2520Salvado%2520and%2520Clinton%2520Fookes%2520and%2520Leo%2520Lebrat%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520have%2520remodeled%25203D%2520scene%2520representation%2520since%250Arelease.%2520NeRFs%2520can%2520effectively%2520reconstruct%2520complex%25203D%2520scenes%2520from%25202D%2520images%252C%250Aadvancing%2520different%2520fields%2520and%2520applications%2520such%2520as%2520scene%2520understanding%252C%25203D%250Acontent%2520generation%252C%2520and%2520robotics.%2520Despite%2520significant%2520research%2520progress%252C%2520a%250Athorough%2520review%2520of%2520recent%2520innovations%252C%2520applications%252C%2520and%2520challenges%2520is%2520lacking.%250AThis%2520survey%2520compiles%2520key%2520theoretical%2520advancements%2520and%2520alternative%250Arepresentations%2520and%2520investigates%2520emerging%2520challenges.%2520It%2520further%2520explores%250Aapplications%2520on%2520reconstruction%252C%2520highlights%2520NeRFs%2527%2520impact%2520on%2520computer%2520vision%2520and%250Arobotics%252C%2520and%2520reviews%2520essential%2520datasets%2520and%2520toolkits.%2520By%2520identifying%2520gaps%2520in%250Athe%2520literature%252C%2520this%2520survey%2520discusses%2520open%2520challenges%2520and%2520offers%2520directions%2520for%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Radiance%20Fields%20for%20the%20Real%20World%3A%20A%20Survey&entry.906535625=Wenhui%20Xiao%20and%20Remi%20Chierchia%20and%20Rodrigo%20Santa%20Cruz%20and%20Xuesong%20Li%20and%20David%20Ahmedt-Aristizabal%20and%20Olivier%20Salvado%20and%20Clinton%20Fookes%20and%20Leo%20Lebrat&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRFs%29%20have%20remodeled%203D%20scene%20representation%20since%0Arelease.%20NeRFs%20can%20effectively%20reconstruct%20complex%203D%20scenes%20from%202D%20images%2C%0Aadvancing%20different%20fields%20and%20applications%20such%20as%20scene%20understanding%2C%203D%0Acontent%20generation%2C%20and%20robotics.%20Despite%20significant%20research%20progress%2C%20a%0Athorough%20review%20of%20recent%20innovations%2C%20applications%2C%20and%20challenges%20is%20lacking.%0AThis%20survey%20compiles%20key%20theoretical%20advancements%20and%20alternative%0Arepresentations%20and%20investigates%20emerging%20challenges.%20It%20further%20explores%0Aapplications%20on%20reconstruction%2C%20highlights%20NeRFs%27%20impact%20on%20computer%20vision%20and%0Arobotics%2C%20and%20reviews%20essential%20datasets%20and%20toolkits.%20By%20identifying%20gaps%20in%0Athe%20literature%2C%20this%20survey%20discusses%20open%20challenges%20and%20offers%20directions%20for%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13104v1&entry.124074799=Read"},
{"title": "Beyond the Lungs: Extending the Field of View in Chest CT with Latent\n  Diffusion Models", "author": "Lianrui Zuo and Kaiwen Xu and Dingjie Su and Xin Yu and Aravind R. Krishnan and Yihao Liu and Shunxing Bao and Thomas Li and Kim L. Sandler and Fabien Maldonado and Bennett A. Landman", "abstract": "  The interconnection between the human lungs and other organs, such as the\nliver and kidneys, is crucial for understanding the underlying risks and\neffects of lung diseases and improving patient care. However, most research\nchest CT imaging is focused solely on the lungs due to considerations of cost\nand radiation dose. This restricted field of view (FOV) in the acquired images\nposes challenges to comprehensive analysis and hinders the ability to gain\ninsights into the impact of lung diseases on other organs. To address this, we\npropose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel\napproach to capture the inter-organ relationships from CT images and extend the\nFOV of chest CT images. Our approach first trains a variational autoencoder\n(VAE) to encode 2D axial CT slices individually, then stacks the latent\nrepresentations of the VAE to form a 3D context for training a latent diffusion\nmodel. Once trained, our approach extends the FOV of CT images in the\nz-direction by generating new axial slices in a zero-shot manner. We evaluated\nour approach on the National Lung Screening Trial (NLST) dataset, and results\nsuggest that it effectively extends the FOV to include the liver and kidneys,\nwhich are not completely covered in the original NLST data acquisition.\nQuantitative results on a held-out whole-body dataset demonstrate that the\ngenerated slices exhibit high fidelity with acquired data, achieving an SSIM of\n0.81.\n", "link": "http://arxiv.org/abs/2501.13068v1", "date": "2025-01-22", "relevancy": 2.877, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5819}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Lungs%3A%20Extending%20the%20Field%20of%20View%20in%20Chest%20CT%20with%20Latent%0A%20%20Diffusion%20Models&body=Title%3A%20Beyond%20the%20Lungs%3A%20Extending%20the%20Field%20of%20View%20in%20Chest%20CT%20with%20Latent%0A%20%20Diffusion%20Models%0AAuthor%3A%20Lianrui%20Zuo%20and%20Kaiwen%20Xu%20and%20Dingjie%20Su%20and%20Xin%20Yu%20and%20Aravind%20R.%20Krishnan%20and%20Yihao%20Liu%20and%20Shunxing%20Bao%20and%20Thomas%20Li%20and%20Kim%20L.%20Sandler%20and%20Fabien%20Maldonado%20and%20Bennett%20A.%20Landman%0AAbstract%3A%20%20%20The%20interconnection%20between%20the%20human%20lungs%20and%20other%20organs%2C%20such%20as%20the%0Aliver%20and%20kidneys%2C%20is%20crucial%20for%20understanding%20the%20underlying%20risks%20and%0Aeffects%20of%20lung%20diseases%20and%20improving%20patient%20care.%20However%2C%20most%20research%0Achest%20CT%20imaging%20is%20focused%20solely%20on%20the%20lungs%20due%20to%20considerations%20of%20cost%0Aand%20radiation%20dose.%20This%20restricted%20field%20of%20view%20%28FOV%29%20in%20the%20acquired%20images%0Aposes%20challenges%20to%20comprehensive%20analysis%20and%20hinders%20the%20ability%20to%20gain%0Ainsights%20into%20the%20impact%20of%20lung%20diseases%20on%20other%20organs.%20To%20address%20this%2C%20we%0Apropose%20SCOPE%20%28Spatial%20Coverage%20Optimization%20with%20Prior%20Encoding%29%2C%20a%20novel%0Aapproach%20to%20capture%20the%20inter-organ%20relationships%20from%20CT%20images%20and%20extend%20the%0AFOV%20of%20chest%20CT%20images.%20Our%20approach%20first%20trains%20a%20variational%20autoencoder%0A%28VAE%29%20to%20encode%202D%20axial%20CT%20slices%20individually%2C%20then%20stacks%20the%20latent%0Arepresentations%20of%20the%20VAE%20to%20form%20a%203D%20context%20for%20training%20a%20latent%20diffusion%0Amodel.%20Once%20trained%2C%20our%20approach%20extends%20the%20FOV%20of%20CT%20images%20in%20the%0Az-direction%20by%20generating%20new%20axial%20slices%20in%20a%20zero-shot%20manner.%20We%20evaluated%0Aour%20approach%20on%20the%20National%20Lung%20Screening%20Trial%20%28NLST%29%20dataset%2C%20and%20results%0Asuggest%20that%20it%20effectively%20extends%20the%20FOV%20to%20include%20the%20liver%20and%20kidneys%2C%0Awhich%20are%20not%20completely%20covered%20in%20the%20original%20NLST%20data%20acquisition.%0AQuantitative%20results%20on%20a%20held-out%20whole-body%20dataset%20demonstrate%20that%20the%0Agenerated%20slices%20exhibit%20high%20fidelity%20with%20acquired%20data%2C%20achieving%20an%20SSIM%20of%0A0.81.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Lungs%253A%2520Extending%2520the%2520Field%2520of%2520View%2520in%2520Chest%2520CT%2520with%2520Latent%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DLianrui%2520Zuo%2520and%2520Kaiwen%2520Xu%2520and%2520Dingjie%2520Su%2520and%2520Xin%2520Yu%2520and%2520Aravind%2520R.%2520Krishnan%2520and%2520Yihao%2520Liu%2520and%2520Shunxing%2520Bao%2520and%2520Thomas%2520Li%2520and%2520Kim%2520L.%2520Sandler%2520and%2520Fabien%2520Maldonado%2520and%2520Bennett%2520A.%2520Landman%26entry.1292438233%3D%2520%2520The%2520interconnection%2520between%2520the%2520human%2520lungs%2520and%2520other%2520organs%252C%2520such%2520as%2520the%250Aliver%2520and%2520kidneys%252C%2520is%2520crucial%2520for%2520understanding%2520the%2520underlying%2520risks%2520and%250Aeffects%2520of%2520lung%2520diseases%2520and%2520improving%2520patient%2520care.%2520However%252C%2520most%2520research%250Achest%2520CT%2520imaging%2520is%2520focused%2520solely%2520on%2520the%2520lungs%2520due%2520to%2520considerations%2520of%2520cost%250Aand%2520radiation%2520dose.%2520This%2520restricted%2520field%2520of%2520view%2520%2528FOV%2529%2520in%2520the%2520acquired%2520images%250Aposes%2520challenges%2520to%2520comprehensive%2520analysis%2520and%2520hinders%2520the%2520ability%2520to%2520gain%250Ainsights%2520into%2520the%2520impact%2520of%2520lung%2520diseases%2520on%2520other%2520organs.%2520To%2520address%2520this%252C%2520we%250Apropose%2520SCOPE%2520%2528Spatial%2520Coverage%2520Optimization%2520with%2520Prior%2520Encoding%2529%252C%2520a%2520novel%250Aapproach%2520to%2520capture%2520the%2520inter-organ%2520relationships%2520from%2520CT%2520images%2520and%2520extend%2520the%250AFOV%2520of%2520chest%2520CT%2520images.%2520Our%2520approach%2520first%2520trains%2520a%2520variational%2520autoencoder%250A%2528VAE%2529%2520to%2520encode%25202D%2520axial%2520CT%2520slices%2520individually%252C%2520then%2520stacks%2520the%2520latent%250Arepresentations%2520of%2520the%2520VAE%2520to%2520form%2520a%25203D%2520context%2520for%2520training%2520a%2520latent%2520diffusion%250Amodel.%2520Once%2520trained%252C%2520our%2520approach%2520extends%2520the%2520FOV%2520of%2520CT%2520images%2520in%2520the%250Az-direction%2520by%2520generating%2520new%2520axial%2520slices%2520in%2520a%2520zero-shot%2520manner.%2520We%2520evaluated%250Aour%2520approach%2520on%2520the%2520National%2520Lung%2520Screening%2520Trial%2520%2528NLST%2529%2520dataset%252C%2520and%2520results%250Asuggest%2520that%2520it%2520effectively%2520extends%2520the%2520FOV%2520to%2520include%2520the%2520liver%2520and%2520kidneys%252C%250Awhich%2520are%2520not%2520completely%2520covered%2520in%2520the%2520original%2520NLST%2520data%2520acquisition.%250AQuantitative%2520results%2520on%2520a%2520held-out%2520whole-body%2520dataset%2520demonstrate%2520that%2520the%250Agenerated%2520slices%2520exhibit%2520high%2520fidelity%2520with%2520acquired%2520data%252C%2520achieving%2520an%2520SSIM%2520of%250A0.81.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Lungs%3A%20Extending%20the%20Field%20of%20View%20in%20Chest%20CT%20with%20Latent%0A%20%20Diffusion%20Models&entry.906535625=Lianrui%20Zuo%20and%20Kaiwen%20Xu%20and%20Dingjie%20Su%20and%20Xin%20Yu%20and%20Aravind%20R.%20Krishnan%20and%20Yihao%20Liu%20and%20Shunxing%20Bao%20and%20Thomas%20Li%20and%20Kim%20L.%20Sandler%20and%20Fabien%20Maldonado%20and%20Bennett%20A.%20Landman&entry.1292438233=%20%20The%20interconnection%20between%20the%20human%20lungs%20and%20other%20organs%2C%20such%20as%20the%0Aliver%20and%20kidneys%2C%20is%20crucial%20for%20understanding%20the%20underlying%20risks%20and%0Aeffects%20of%20lung%20diseases%20and%20improving%20patient%20care.%20However%2C%20most%20research%0Achest%20CT%20imaging%20is%20focused%20solely%20on%20the%20lungs%20due%20to%20considerations%20of%20cost%0Aand%20radiation%20dose.%20This%20restricted%20field%20of%20view%20%28FOV%29%20in%20the%20acquired%20images%0Aposes%20challenges%20to%20comprehensive%20analysis%20and%20hinders%20the%20ability%20to%20gain%0Ainsights%20into%20the%20impact%20of%20lung%20diseases%20on%20other%20organs.%20To%20address%20this%2C%20we%0Apropose%20SCOPE%20%28Spatial%20Coverage%20Optimization%20with%20Prior%20Encoding%29%2C%20a%20novel%0Aapproach%20to%20capture%20the%20inter-organ%20relationships%20from%20CT%20images%20and%20extend%20the%0AFOV%20of%20chest%20CT%20images.%20Our%20approach%20first%20trains%20a%20variational%20autoencoder%0A%28VAE%29%20to%20encode%202D%20axial%20CT%20slices%20individually%2C%20then%20stacks%20the%20latent%0Arepresentations%20of%20the%20VAE%20to%20form%20a%203D%20context%20for%20training%20a%20latent%20diffusion%0Amodel.%20Once%20trained%2C%20our%20approach%20extends%20the%20FOV%20of%20CT%20images%20in%20the%0Az-direction%20by%20generating%20new%20axial%20slices%20in%20a%20zero-shot%20manner.%20We%20evaluated%0Aour%20approach%20on%20the%20National%20Lung%20Screening%20Trial%20%28NLST%29%20dataset%2C%20and%20results%0Asuggest%20that%20it%20effectively%20extends%20the%20FOV%20to%20include%20the%20liver%20and%20kidneys%2C%0Awhich%20are%20not%20completely%20covered%20in%20the%20original%20NLST%20data%20acquisition.%0AQuantitative%20results%20on%20a%20held-out%20whole-body%20dataset%20demonstrate%20that%20the%0Agenerated%20slices%20exhibit%20high%20fidelity%20with%20acquired%20data%2C%20achieving%20an%20SSIM%20of%0A0.81.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13068v1&entry.124074799=Read"},
{"title": "Grid-based Submap Joining: An Efficient Algorithm for Simultaneously\n  Optimizing Global Occupancy Map and Local Submap Frames", "author": "Yingyu Wang and Liang Zhao and Shoudong Huang", "abstract": "  Optimizing robot poses and the map simultaneously has been shown to provide\nmore accurate SLAM results. However, for non-feature based SLAM approaches,\ndirectly optimizing all the robot poses and the whole map will greatly increase\nthe computational cost, making SLAM problems difficult to solve in large-scale\nenvironments. To solve the 2D non-feature based SLAM problem in large-scale\nenvironments more accurately and efficiently, we propose the grid-based submap\njoining method. Specifically, we first formulate the 2D grid-based submap\njoining problem as a non-linear least squares (NLLS) form to optimize the\nglobal occupancy map and local submap frames simultaneously. We then prove that\nin solving the NLLS problem using Gauss-Newton (GN) method, the increments of\nthe poses in each iteration are independent of the occupancy values of the\nglobal occupancy map. Based on this property, we propose a poseonly GN\nalgorithm equivalent to full GN method to solve the NLLS problem. The proposed\nsubmap joining algorithm is very efficient due to the independent property and\nthe pose-only solution. Evaluations using simulations and publicly available\npractical 2D laser datasets confirm the outperformance of our proposed method\ncompared to the state-of-the-art methods in terms of efficiency and accuracy,\nas well as the ability to solve the grid-based SLAM problem in very large-scale\nenvironments.\n", "link": "http://arxiv.org/abs/2501.12764v1", "date": "2025-01-22", "relevancy": 2.8714, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6232}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5744}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grid-based%20Submap%20Joining%3A%20An%20Efficient%20Algorithm%20for%20Simultaneously%0A%20%20Optimizing%20Global%20Occupancy%20Map%20and%20Local%20Submap%20Frames&body=Title%3A%20Grid-based%20Submap%20Joining%3A%20An%20Efficient%20Algorithm%20for%20Simultaneously%0A%20%20Optimizing%20Global%20Occupancy%20Map%20and%20Local%20Submap%20Frames%0AAuthor%3A%20Yingyu%20Wang%20and%20Liang%20Zhao%20and%20Shoudong%20Huang%0AAbstract%3A%20%20%20Optimizing%20robot%20poses%20and%20the%20map%20simultaneously%20has%20been%20shown%20to%20provide%0Amore%20accurate%20SLAM%20results.%20However%2C%20for%20non-feature%20based%20SLAM%20approaches%2C%0Adirectly%20optimizing%20all%20the%20robot%20poses%20and%20the%20whole%20map%20will%20greatly%20increase%0Athe%20computational%20cost%2C%20making%20SLAM%20problems%20difficult%20to%20solve%20in%20large-scale%0Aenvironments.%20To%20solve%20the%202D%20non-feature%20based%20SLAM%20problem%20in%20large-scale%0Aenvironments%20more%20accurately%20and%20efficiently%2C%20we%20propose%20the%20grid-based%20submap%0Ajoining%20method.%20Specifically%2C%20we%20first%20formulate%20the%202D%20grid-based%20submap%0Ajoining%20problem%20as%20a%20non-linear%20least%20squares%20%28NLLS%29%20form%20to%20optimize%20the%0Aglobal%20occupancy%20map%20and%20local%20submap%20frames%20simultaneously.%20We%20then%20prove%20that%0Ain%20solving%20the%20NLLS%20problem%20using%20Gauss-Newton%20%28GN%29%20method%2C%20the%20increments%20of%0Athe%20poses%20in%20each%20iteration%20are%20independent%20of%20the%20occupancy%20values%20of%20the%0Aglobal%20occupancy%20map.%20Based%20on%20this%20property%2C%20we%20propose%20a%20poseonly%20GN%0Aalgorithm%20equivalent%20to%20full%20GN%20method%20to%20solve%20the%20NLLS%20problem.%20The%20proposed%0Asubmap%20joining%20algorithm%20is%20very%20efficient%20due%20to%20the%20independent%20property%20and%0Athe%20pose-only%20solution.%20Evaluations%20using%20simulations%20and%20publicly%20available%0Apractical%202D%20laser%20datasets%20confirm%20the%20outperformance%20of%20our%20proposed%20method%0Acompared%20to%20the%20state-of-the-art%20methods%20in%20terms%20of%20efficiency%20and%20accuracy%2C%0Aas%20well%20as%20the%20ability%20to%20solve%20the%20grid-based%20SLAM%20problem%20in%20very%20large-scale%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrid-based%2520Submap%2520Joining%253A%2520An%2520Efficient%2520Algorithm%2520for%2520Simultaneously%250A%2520%2520Optimizing%2520Global%2520Occupancy%2520Map%2520and%2520Local%2520Submap%2520Frames%26entry.906535625%3DYingyu%2520Wang%2520and%2520Liang%2520Zhao%2520and%2520Shoudong%2520Huang%26entry.1292438233%3D%2520%2520Optimizing%2520robot%2520poses%2520and%2520the%2520map%2520simultaneously%2520has%2520been%2520shown%2520to%2520provide%250Amore%2520accurate%2520SLAM%2520results.%2520However%252C%2520for%2520non-feature%2520based%2520SLAM%2520approaches%252C%250Adirectly%2520optimizing%2520all%2520the%2520robot%2520poses%2520and%2520the%2520whole%2520map%2520will%2520greatly%2520increase%250Athe%2520computational%2520cost%252C%2520making%2520SLAM%2520problems%2520difficult%2520to%2520solve%2520in%2520large-scale%250Aenvironments.%2520To%2520solve%2520the%25202D%2520non-feature%2520based%2520SLAM%2520problem%2520in%2520large-scale%250Aenvironments%2520more%2520accurately%2520and%2520efficiently%252C%2520we%2520propose%2520the%2520grid-based%2520submap%250Ajoining%2520method.%2520Specifically%252C%2520we%2520first%2520formulate%2520the%25202D%2520grid-based%2520submap%250Ajoining%2520problem%2520as%2520a%2520non-linear%2520least%2520squares%2520%2528NLLS%2529%2520form%2520to%2520optimize%2520the%250Aglobal%2520occupancy%2520map%2520and%2520local%2520submap%2520frames%2520simultaneously.%2520We%2520then%2520prove%2520that%250Ain%2520solving%2520the%2520NLLS%2520problem%2520using%2520Gauss-Newton%2520%2528GN%2529%2520method%252C%2520the%2520increments%2520of%250Athe%2520poses%2520in%2520each%2520iteration%2520are%2520independent%2520of%2520the%2520occupancy%2520values%2520of%2520the%250Aglobal%2520occupancy%2520map.%2520Based%2520on%2520this%2520property%252C%2520we%2520propose%2520a%2520poseonly%2520GN%250Aalgorithm%2520equivalent%2520to%2520full%2520GN%2520method%2520to%2520solve%2520the%2520NLLS%2520problem.%2520The%2520proposed%250Asubmap%2520joining%2520algorithm%2520is%2520very%2520efficient%2520due%2520to%2520the%2520independent%2520property%2520and%250Athe%2520pose-only%2520solution.%2520Evaluations%2520using%2520simulations%2520and%2520publicly%2520available%250Apractical%25202D%2520laser%2520datasets%2520confirm%2520the%2520outperformance%2520of%2520our%2520proposed%2520method%250Acompared%2520to%2520the%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520efficiency%2520and%2520accuracy%252C%250Aas%2520well%2520as%2520the%2520ability%2520to%2520solve%2520the%2520grid-based%2520SLAM%2520problem%2520in%2520very%2520large-scale%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grid-based%20Submap%20Joining%3A%20An%20Efficient%20Algorithm%20for%20Simultaneously%0A%20%20Optimizing%20Global%20Occupancy%20Map%20and%20Local%20Submap%20Frames&entry.906535625=Yingyu%20Wang%20and%20Liang%20Zhao%20and%20Shoudong%20Huang&entry.1292438233=%20%20Optimizing%20robot%20poses%20and%20the%20map%20simultaneously%20has%20been%20shown%20to%20provide%0Amore%20accurate%20SLAM%20results.%20However%2C%20for%20non-feature%20based%20SLAM%20approaches%2C%0Adirectly%20optimizing%20all%20the%20robot%20poses%20and%20the%20whole%20map%20will%20greatly%20increase%0Athe%20computational%20cost%2C%20making%20SLAM%20problems%20difficult%20to%20solve%20in%20large-scale%0Aenvironments.%20To%20solve%20the%202D%20non-feature%20based%20SLAM%20problem%20in%20large-scale%0Aenvironments%20more%20accurately%20and%20efficiently%2C%20we%20propose%20the%20grid-based%20submap%0Ajoining%20method.%20Specifically%2C%20we%20first%20formulate%20the%202D%20grid-based%20submap%0Ajoining%20problem%20as%20a%20non-linear%20least%20squares%20%28NLLS%29%20form%20to%20optimize%20the%0Aglobal%20occupancy%20map%20and%20local%20submap%20frames%20simultaneously.%20We%20then%20prove%20that%0Ain%20solving%20the%20NLLS%20problem%20using%20Gauss-Newton%20%28GN%29%20method%2C%20the%20increments%20of%0Athe%20poses%20in%20each%20iteration%20are%20independent%20of%20the%20occupancy%20values%20of%20the%0Aglobal%20occupancy%20map.%20Based%20on%20this%20property%2C%20we%20propose%20a%20poseonly%20GN%0Aalgorithm%20equivalent%20to%20full%20GN%20method%20to%20solve%20the%20NLLS%20problem.%20The%20proposed%0Asubmap%20joining%20algorithm%20is%20very%20efficient%20due%20to%20the%20independent%20property%20and%0Athe%20pose-only%20solution.%20Evaluations%20using%20simulations%20and%20publicly%20available%0Apractical%202D%20laser%20datasets%20confirm%20the%20outperformance%20of%20our%20proposed%20method%0Acompared%20to%20the%20state-of-the-art%20methods%20in%20terms%20of%20efficiency%20and%20accuracy%2C%0Aas%20well%20as%20the%20ability%20to%20solve%20the%20grid-based%20SLAM%20problem%20in%20very%20large-scale%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12764v1&entry.124074799=Read"},
{"title": "Robust Body Composition Analysis by Generating 3D CT Volumes from\n  Limited 2D Slices", "author": "Lianrui Zuo and Xin Yu and Dingjie Su and Kaiwen Xu and Aravind R. Krishnan and Yihao Liu and Shunxing Bao and Fabien Maldonado and Luigi Ferrucci and Bennett A. Landman", "abstract": "  Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%.\n", "link": "http://arxiv.org/abs/2501.13071v1", "date": "2025-01-22", "relevancy": 2.8576, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5784}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5784}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Body%20Composition%20Analysis%20by%20Generating%203D%20CT%20Volumes%20from%0A%20%20Limited%202D%20Slices&body=Title%3A%20Robust%20Body%20Composition%20Analysis%20by%20Generating%203D%20CT%20Volumes%20from%0A%20%20Limited%202D%20Slices%0AAuthor%3A%20Lianrui%20Zuo%20and%20Xin%20Yu%20and%20Dingjie%20Su%20and%20Kaiwen%20Xu%20and%20Aravind%20R.%20Krishnan%20and%20Yihao%20Liu%20and%20Shunxing%20Bao%20and%20Fabien%20Maldonado%20and%20Luigi%20Ferrucci%20and%20Bennett%20A.%20Landman%0AAbstract%3A%20%20%20Body%20composition%20analysis%20provides%20valuable%20insights%20into%20aging%2C%20disease%0Aprogression%2C%20and%20overall%20health%20conditions.%20Due%20to%20concerns%20of%20radiation%0Aexposure%2C%20two-dimensional%20%282D%29%20single-slice%20computed%20tomography%20%28CT%29%20imaging%0Ahas%20been%20used%20repeatedly%20for%20body%20composition%20analysis.%20However%2C%20this%20approach%0Aintroduces%20significant%20spatial%20variability%20that%20can%20impact%20the%20accuracy%20and%0Arobustness%20of%20the%20analysis.%20To%20mitigate%20this%20issue%20and%20facilitate%20body%0Acomposition%20analysis%2C%20this%20paper%20presents%20a%20novel%20method%20to%20generate%203D%20CT%0Avolumes%20from%20limited%20number%20of%202D%20slices%20using%20a%20latent%20diffusion%20model%20%28LDM%29.%0AOur%20approach%20first%20maps%202D%20slices%20into%20a%20latent%20representation%20space%20using%20a%0Avariational%20autoencoder.%20An%20LDM%20is%20then%20trained%20to%20capture%20the%203D%20context%20of%20a%0Astack%20of%20these%20latent%20representations.%20To%20accurately%20interpolate%0Aintermediateslices%20and%20construct%20a%20full%203D%20volume%2C%20we%20utilize%20body%20part%0Aregression%20to%20determine%20the%20spatial%20location%20and%20distance%20between%20the%20acquired%0Aslices.%20Experiments%20on%20both%20in-house%20and%20public%203D%20abdominal%20CT%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20enhances%20body%20composition%0Aanalysis%20compared%20to%20traditional%202D-based%20analysis%2C%20with%20a%20reduced%20error%20rate%0Afrom%2023.3%25%20to%2015.2%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Body%2520Composition%2520Analysis%2520by%2520Generating%25203D%2520CT%2520Volumes%2520from%250A%2520%2520Limited%25202D%2520Slices%26entry.906535625%3DLianrui%2520Zuo%2520and%2520Xin%2520Yu%2520and%2520Dingjie%2520Su%2520and%2520Kaiwen%2520Xu%2520and%2520Aravind%2520R.%2520Krishnan%2520and%2520Yihao%2520Liu%2520and%2520Shunxing%2520Bao%2520and%2520Fabien%2520Maldonado%2520and%2520Luigi%2520Ferrucci%2520and%2520Bennett%2520A.%2520Landman%26entry.1292438233%3D%2520%2520Body%2520composition%2520analysis%2520provides%2520valuable%2520insights%2520into%2520aging%252C%2520disease%250Aprogression%252C%2520and%2520overall%2520health%2520conditions.%2520Due%2520to%2520concerns%2520of%2520radiation%250Aexposure%252C%2520two-dimensional%2520%25282D%2529%2520single-slice%2520computed%2520tomography%2520%2528CT%2529%2520imaging%250Ahas%2520been%2520used%2520repeatedly%2520for%2520body%2520composition%2520analysis.%2520However%252C%2520this%2520approach%250Aintroduces%2520significant%2520spatial%2520variability%2520that%2520can%2520impact%2520the%2520accuracy%2520and%250Arobustness%2520of%2520the%2520analysis.%2520To%2520mitigate%2520this%2520issue%2520and%2520facilitate%2520body%250Acomposition%2520analysis%252C%2520this%2520paper%2520presents%2520a%2520novel%2520method%2520to%2520generate%25203D%2520CT%250Avolumes%2520from%2520limited%2520number%2520of%25202D%2520slices%2520using%2520a%2520latent%2520diffusion%2520model%2520%2528LDM%2529.%250AOur%2520approach%2520first%2520maps%25202D%2520slices%2520into%2520a%2520latent%2520representation%2520space%2520using%2520a%250Avariational%2520autoencoder.%2520An%2520LDM%2520is%2520then%2520trained%2520to%2520capture%2520the%25203D%2520context%2520of%2520a%250Astack%2520of%2520these%2520latent%2520representations.%2520To%2520accurately%2520interpolate%250Aintermediateslices%2520and%2520construct%2520a%2520full%25203D%2520volume%252C%2520we%2520utilize%2520body%2520part%250Aregression%2520to%2520determine%2520the%2520spatial%2520location%2520and%2520distance%2520between%2520the%2520acquired%250Aslices.%2520Experiments%2520on%2520both%2520in-house%2520and%2520public%25203D%2520abdominal%2520CT%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520enhances%2520body%2520composition%250Aanalysis%2520compared%2520to%2520traditional%25202D-based%2520analysis%252C%2520with%2520a%2520reduced%2520error%2520rate%250Afrom%252023.3%2525%2520to%252015.2%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Body%20Composition%20Analysis%20by%20Generating%203D%20CT%20Volumes%20from%0A%20%20Limited%202D%20Slices&entry.906535625=Lianrui%20Zuo%20and%20Xin%20Yu%20and%20Dingjie%20Su%20and%20Kaiwen%20Xu%20and%20Aravind%20R.%20Krishnan%20and%20Yihao%20Liu%20and%20Shunxing%20Bao%20and%20Fabien%20Maldonado%20and%20Luigi%20Ferrucci%20and%20Bennett%20A.%20Landman&entry.1292438233=%20%20Body%20composition%20analysis%20provides%20valuable%20insights%20into%20aging%2C%20disease%0Aprogression%2C%20and%20overall%20health%20conditions.%20Due%20to%20concerns%20of%20radiation%0Aexposure%2C%20two-dimensional%20%282D%29%20single-slice%20computed%20tomography%20%28CT%29%20imaging%0Ahas%20been%20used%20repeatedly%20for%20body%20composition%20analysis.%20However%2C%20this%20approach%0Aintroduces%20significant%20spatial%20variability%20that%20can%20impact%20the%20accuracy%20and%0Arobustness%20of%20the%20analysis.%20To%20mitigate%20this%20issue%20and%20facilitate%20body%0Acomposition%20analysis%2C%20this%20paper%20presents%20a%20novel%20method%20to%20generate%203D%20CT%0Avolumes%20from%20limited%20number%20of%202D%20slices%20using%20a%20latent%20diffusion%20model%20%28LDM%29.%0AOur%20approach%20first%20maps%202D%20slices%20into%20a%20latent%20representation%20space%20using%20a%0Avariational%20autoencoder.%20An%20LDM%20is%20then%20trained%20to%20capture%20the%203D%20context%20of%20a%0Astack%20of%20these%20latent%20representations.%20To%20accurately%20interpolate%0Aintermediateslices%20and%20construct%20a%20full%203D%20volume%2C%20we%20utilize%20body%20part%0Aregression%20to%20determine%20the%20spatial%20location%20and%20distance%20between%20the%20acquired%0Aslices.%20Experiments%20on%20both%20in-house%20and%20public%203D%20abdominal%20CT%20datasets%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20enhances%20body%20composition%0Aanalysis%20compared%20to%20traditional%202D-based%20analysis%2C%20with%20a%20reduced%20error%20rate%0Afrom%2023.3%25%20to%2015.2%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13071v1&entry.124074799=Read"},
{"title": "Cross-D Conv: Cross-Dimensional Transferable Knowledge Base via Fourier\n  Shifting Operation", "author": "Mehmet Can Yavuz and Yang Yang", "abstract": "  In biomedical imaging analysis, the dichotomy between 2D and 3D data presents\na significant challenge. While 3D volumes offer superior real-world\napplicability, they are less available for each modality and not easy to train\nin large scale, whereas 2D samples are abundant but less comprehensive. This\npaper introduces \\texttt{Cross-D Conv} operation, a novel approach that bridges\nthe dimensional gap by learning the phase shifting in the Fourier domain. Our\nmethod enables seamless weight transfer between 2D and 3D convolution\noperations, effectively facilitating cross-dimensional learning. The proposed\narchitecture leverages the abundance of 2D training data to enhance 3D model\nperformance, offering a practical solution to the multimodal data scarcity\nchallenge in 3D medical model pretraining. Experimental validation on the\nRadImagenet (2D) and multimodal volumetric sets demonstrates that our approach\nachieves comparable or superior performance in feature quality assessment. The\nenhanced convolution operation presents new opportunities for developing\nefficient classification and segmentation models in medical imaging. This work\nrepresents an advancement in cross-dimensional and multimodal medical image\nanalysis, offering a robust framework for utilizing 2D priors in 3D model\npretraining while maintaining computational efficiency of 2D training.\n", "link": "http://arxiv.org/abs/2411.02441v3", "date": "2025-01-22", "relevancy": 2.7705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5885}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5377}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-D%20Conv%3A%20Cross-Dimensional%20Transferable%20Knowledge%20Base%20via%20Fourier%0A%20%20Shifting%20Operation&body=Title%3A%20Cross-D%20Conv%3A%20Cross-Dimensional%20Transferable%20Knowledge%20Base%20via%20Fourier%0A%20%20Shifting%20Operation%0AAuthor%3A%20Mehmet%20Can%20Yavuz%20and%20Yang%20Yang%0AAbstract%3A%20%20%20In%20biomedical%20imaging%20analysis%2C%20the%20dichotomy%20between%202D%20and%203D%20data%20presents%0Aa%20significant%20challenge.%20While%203D%20volumes%20offer%20superior%20real-world%0Aapplicability%2C%20they%20are%20less%20available%20for%20each%20modality%20and%20not%20easy%20to%20train%0Ain%20large%20scale%2C%20whereas%202D%20samples%20are%20abundant%20but%20less%20comprehensive.%20This%0Apaper%20introduces%20%5Ctexttt%7BCross-D%20Conv%7D%20operation%2C%20a%20novel%20approach%20that%20bridges%0Athe%20dimensional%20gap%20by%20learning%20the%20phase%20shifting%20in%20the%20Fourier%20domain.%20Our%0Amethod%20enables%20seamless%20weight%20transfer%20between%202D%20and%203D%20convolution%0Aoperations%2C%20effectively%20facilitating%20cross-dimensional%20learning.%20The%20proposed%0Aarchitecture%20leverages%20the%20abundance%20of%202D%20training%20data%20to%20enhance%203D%20model%0Aperformance%2C%20offering%20a%20practical%20solution%20to%20the%20multimodal%20data%20scarcity%0Achallenge%20in%203D%20medical%20model%20pretraining.%20Experimental%20validation%20on%20the%0ARadImagenet%20%282D%29%20and%20multimodal%20volumetric%20sets%20demonstrates%20that%20our%20approach%0Aachieves%20comparable%20or%20superior%20performance%20in%20feature%20quality%20assessment.%20The%0Aenhanced%20convolution%20operation%20presents%20new%20opportunities%20for%20developing%0Aefficient%20classification%20and%20segmentation%20models%20in%20medical%20imaging.%20This%20work%0Arepresents%20an%20advancement%20in%20cross-dimensional%20and%20multimodal%20medical%20image%0Aanalysis%2C%20offering%20a%20robust%20framework%20for%20utilizing%202D%20priors%20in%203D%20model%0Apretraining%20while%20maintaining%20computational%20efficiency%20of%202D%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-D%2520Conv%253A%2520Cross-Dimensional%2520Transferable%2520Knowledge%2520Base%2520via%2520Fourier%250A%2520%2520Shifting%2520Operation%26entry.906535625%3DMehmet%2520Can%2520Yavuz%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520In%2520biomedical%2520imaging%2520analysis%252C%2520the%2520dichotomy%2520between%25202D%2520and%25203D%2520data%2520presents%250Aa%2520significant%2520challenge.%2520While%25203D%2520volumes%2520offer%2520superior%2520real-world%250Aapplicability%252C%2520they%2520are%2520less%2520available%2520for%2520each%2520modality%2520and%2520not%2520easy%2520to%2520train%250Ain%2520large%2520scale%252C%2520whereas%25202D%2520samples%2520are%2520abundant%2520but%2520less%2520comprehensive.%2520This%250Apaper%2520introduces%2520%255Ctexttt%257BCross-D%2520Conv%257D%2520operation%252C%2520a%2520novel%2520approach%2520that%2520bridges%250Athe%2520dimensional%2520gap%2520by%2520learning%2520the%2520phase%2520shifting%2520in%2520the%2520Fourier%2520domain.%2520Our%250Amethod%2520enables%2520seamless%2520weight%2520transfer%2520between%25202D%2520and%25203D%2520convolution%250Aoperations%252C%2520effectively%2520facilitating%2520cross-dimensional%2520learning.%2520The%2520proposed%250Aarchitecture%2520leverages%2520the%2520abundance%2520of%25202D%2520training%2520data%2520to%2520enhance%25203D%2520model%250Aperformance%252C%2520offering%2520a%2520practical%2520solution%2520to%2520the%2520multimodal%2520data%2520scarcity%250Achallenge%2520in%25203D%2520medical%2520model%2520pretraining.%2520Experimental%2520validation%2520on%2520the%250ARadImagenet%2520%25282D%2529%2520and%2520multimodal%2520volumetric%2520sets%2520demonstrates%2520that%2520our%2520approach%250Aachieves%2520comparable%2520or%2520superior%2520performance%2520in%2520feature%2520quality%2520assessment.%2520The%250Aenhanced%2520convolution%2520operation%2520presents%2520new%2520opportunities%2520for%2520developing%250Aefficient%2520classification%2520and%2520segmentation%2520models%2520in%2520medical%2520imaging.%2520This%2520work%250Arepresents%2520an%2520advancement%2520in%2520cross-dimensional%2520and%2520multimodal%2520medical%2520image%250Aanalysis%252C%2520offering%2520a%2520robust%2520framework%2520for%2520utilizing%25202D%2520priors%2520in%25203D%2520model%250Apretraining%2520while%2520maintaining%2520computational%2520efficiency%2520of%25202D%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-D%20Conv%3A%20Cross-Dimensional%20Transferable%20Knowledge%20Base%20via%20Fourier%0A%20%20Shifting%20Operation&entry.906535625=Mehmet%20Can%20Yavuz%20and%20Yang%20Yang&entry.1292438233=%20%20In%20biomedical%20imaging%20analysis%2C%20the%20dichotomy%20between%202D%20and%203D%20data%20presents%0Aa%20significant%20challenge.%20While%203D%20volumes%20offer%20superior%20real-world%0Aapplicability%2C%20they%20are%20less%20available%20for%20each%20modality%20and%20not%20easy%20to%20train%0Ain%20large%20scale%2C%20whereas%202D%20samples%20are%20abundant%20but%20less%20comprehensive.%20This%0Apaper%20introduces%20%5Ctexttt%7BCross-D%20Conv%7D%20operation%2C%20a%20novel%20approach%20that%20bridges%0Athe%20dimensional%20gap%20by%20learning%20the%20phase%20shifting%20in%20the%20Fourier%20domain.%20Our%0Amethod%20enables%20seamless%20weight%20transfer%20between%202D%20and%203D%20convolution%0Aoperations%2C%20effectively%20facilitating%20cross-dimensional%20learning.%20The%20proposed%0Aarchitecture%20leverages%20the%20abundance%20of%202D%20training%20data%20to%20enhance%203D%20model%0Aperformance%2C%20offering%20a%20practical%20solution%20to%20the%20multimodal%20data%20scarcity%0Achallenge%20in%203D%20medical%20model%20pretraining.%20Experimental%20validation%20on%20the%0ARadImagenet%20%282D%29%20and%20multimodal%20volumetric%20sets%20demonstrates%20that%20our%20approach%0Aachieves%20comparable%20or%20superior%20performance%20in%20feature%20quality%20assessment.%20The%0Aenhanced%20convolution%20operation%20presents%20new%20opportunities%20for%20developing%0Aefficient%20classification%20and%20segmentation%20models%20in%20medical%20imaging.%20This%20work%0Arepresents%20an%20advancement%20in%20cross-dimensional%20and%20multimodal%20medical%20image%0Aanalysis%2C%20offering%20a%20robust%20framework%20for%20utilizing%202D%20priors%20in%203D%20model%0Apretraining%20while%20maintaining%20computational%20efficiency%20of%202D%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02441v3&entry.124074799=Read"},
{"title": "3D Object Manipulation in a Single Image using Generative Models", "author": "Ruisi Zhao and Zechuan Zhang and Zongxin Yang and Yi Yang", "abstract": "  Object manipulation in images aims to not only edit the object's presentation\nbut also gift objects with motion. Previous methods encountered challenges in\nconcurrently handling static editing and dynamic generation, while also\nstruggling to achieve fidelity in object appearance and scene lighting. In this\nwork, we introduce \\textbf{OMG3D}, a novel framework that integrates the\nprecise geometric control with the generative power of diffusion models, thus\nachieving significant enhancements in visual performance. Our framework first\nconverts 2D objects into 3D, enabling user-directed modifications and lifelike\nmotions at the geometric level. To address texture realism, we propose\nCustomRefiner, a texture refinement module that pre-train a customized\ndiffusion model, aligning the details and style of coarse renderings of 3D\nrough model with the original image, further refine the texture. Additionally,\nwe introduce IllumiCombiner, a lighting processing module that estimates and\ncorrects background lighting to match human visual perception, resulting in\nmore realistic shadow effects. Extensive experiments demonstrate the\noutstanding visual performance of our approach in both static and dynamic\nscenarios. Remarkably, all these steps can be done using one NVIDIA 3090.\nProject page is at https://whalesong-zrs.github.io/OMG3D-projectpage/\n", "link": "http://arxiv.org/abs/2501.12935v1", "date": "2025-01-22", "relevancy": 2.7306, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.7034}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6785}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Object%20Manipulation%20in%20a%20Single%20Image%20using%20Generative%20Models&body=Title%3A%203D%20Object%20Manipulation%20in%20a%20Single%20Image%20using%20Generative%20Models%0AAuthor%3A%20Ruisi%20Zhao%20and%20Zechuan%20Zhang%20and%20Zongxin%20Yang%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Object%20manipulation%20in%20images%20aims%20to%20not%20only%20edit%20the%20object%27s%20presentation%0Abut%20also%20gift%20objects%20with%20motion.%20Previous%20methods%20encountered%20challenges%20in%0Aconcurrently%20handling%20static%20editing%20and%20dynamic%20generation%2C%20while%20also%0Astruggling%20to%20achieve%20fidelity%20in%20object%20appearance%20and%20scene%20lighting.%20In%20this%0Awork%2C%20we%20introduce%20%5Ctextbf%7BOMG3D%7D%2C%20a%20novel%20framework%20that%20integrates%20the%0Aprecise%20geometric%20control%20with%20the%20generative%20power%20of%20diffusion%20models%2C%20thus%0Aachieving%20significant%20enhancements%20in%20visual%20performance.%20Our%20framework%20first%0Aconverts%202D%20objects%20into%203D%2C%20enabling%20user-directed%20modifications%20and%20lifelike%0Amotions%20at%20the%20geometric%20level.%20To%20address%20texture%20realism%2C%20we%20propose%0ACustomRefiner%2C%20a%20texture%20refinement%20module%20that%20pre-train%20a%20customized%0Adiffusion%20model%2C%20aligning%20the%20details%20and%20style%20of%20coarse%20renderings%20of%203D%0Arough%20model%20with%20the%20original%20image%2C%20further%20refine%20the%20texture.%20Additionally%2C%0Awe%20introduce%20IllumiCombiner%2C%20a%20lighting%20processing%20module%20that%20estimates%20and%0Acorrects%20background%20lighting%20to%20match%20human%20visual%20perception%2C%20resulting%20in%0Amore%20realistic%20shadow%20effects.%20Extensive%20experiments%20demonstrate%20the%0Aoutstanding%20visual%20performance%20of%20our%20approach%20in%20both%20static%20and%20dynamic%0Ascenarios.%20Remarkably%2C%20all%20these%20steps%20can%20be%20done%20using%20one%20NVIDIA%203090.%0AProject%20page%20is%20at%20https%3A//whalesong-zrs.github.io/OMG3D-projectpage/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Object%2520Manipulation%2520in%2520a%2520Single%2520Image%2520using%2520Generative%2520Models%26entry.906535625%3DRuisi%2520Zhao%2520and%2520Zechuan%2520Zhang%2520and%2520Zongxin%2520Yang%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520Object%2520manipulation%2520in%2520images%2520aims%2520to%2520not%2520only%2520edit%2520the%2520object%2527s%2520presentation%250Abut%2520also%2520gift%2520objects%2520with%2520motion.%2520Previous%2520methods%2520encountered%2520challenges%2520in%250Aconcurrently%2520handling%2520static%2520editing%2520and%2520dynamic%2520generation%252C%2520while%2520also%250Astruggling%2520to%2520achieve%2520fidelity%2520in%2520object%2520appearance%2520and%2520scene%2520lighting.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520%255Ctextbf%257BOMG3D%257D%252C%2520a%2520novel%2520framework%2520that%2520integrates%2520the%250Aprecise%2520geometric%2520control%2520with%2520the%2520generative%2520power%2520of%2520diffusion%2520models%252C%2520thus%250Aachieving%2520significant%2520enhancements%2520in%2520visual%2520performance.%2520Our%2520framework%2520first%250Aconverts%25202D%2520objects%2520into%25203D%252C%2520enabling%2520user-directed%2520modifications%2520and%2520lifelike%250Amotions%2520at%2520the%2520geometric%2520level.%2520To%2520address%2520texture%2520realism%252C%2520we%2520propose%250ACustomRefiner%252C%2520a%2520texture%2520refinement%2520module%2520that%2520pre-train%2520a%2520customized%250Adiffusion%2520model%252C%2520aligning%2520the%2520details%2520and%2520style%2520of%2520coarse%2520renderings%2520of%25203D%250Arough%2520model%2520with%2520the%2520original%2520image%252C%2520further%2520refine%2520the%2520texture.%2520Additionally%252C%250Awe%2520introduce%2520IllumiCombiner%252C%2520a%2520lighting%2520processing%2520module%2520that%2520estimates%2520and%250Acorrects%2520background%2520lighting%2520to%2520match%2520human%2520visual%2520perception%252C%2520resulting%2520in%250Amore%2520realistic%2520shadow%2520effects.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aoutstanding%2520visual%2520performance%2520of%2520our%2520approach%2520in%2520both%2520static%2520and%2520dynamic%250Ascenarios.%2520Remarkably%252C%2520all%2520these%2520steps%2520can%2520be%2520done%2520using%2520one%2520NVIDIA%25203090.%250AProject%2520page%2520is%2520at%2520https%253A//whalesong-zrs.github.io/OMG3D-projectpage/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Object%20Manipulation%20in%20a%20Single%20Image%20using%20Generative%20Models&entry.906535625=Ruisi%20Zhao%20and%20Zechuan%20Zhang%20and%20Zongxin%20Yang%20and%20Yi%20Yang&entry.1292438233=%20%20Object%20manipulation%20in%20images%20aims%20to%20not%20only%20edit%20the%20object%27s%20presentation%0Abut%20also%20gift%20objects%20with%20motion.%20Previous%20methods%20encountered%20challenges%20in%0Aconcurrently%20handling%20static%20editing%20and%20dynamic%20generation%2C%20while%20also%0Astruggling%20to%20achieve%20fidelity%20in%20object%20appearance%20and%20scene%20lighting.%20In%20this%0Awork%2C%20we%20introduce%20%5Ctextbf%7BOMG3D%7D%2C%20a%20novel%20framework%20that%20integrates%20the%0Aprecise%20geometric%20control%20with%20the%20generative%20power%20of%20diffusion%20models%2C%20thus%0Aachieving%20significant%20enhancements%20in%20visual%20performance.%20Our%20framework%20first%0Aconverts%202D%20objects%20into%203D%2C%20enabling%20user-directed%20modifications%20and%20lifelike%0Amotions%20at%20the%20geometric%20level.%20To%20address%20texture%20realism%2C%20we%20propose%0ACustomRefiner%2C%20a%20texture%20refinement%20module%20that%20pre-train%20a%20customized%0Adiffusion%20model%2C%20aligning%20the%20details%20and%20style%20of%20coarse%20renderings%20of%203D%0Arough%20model%20with%20the%20original%20image%2C%20further%20refine%20the%20texture.%20Additionally%2C%0Awe%20introduce%20IllumiCombiner%2C%20a%20lighting%20processing%20module%20that%20estimates%20and%0Acorrects%20background%20lighting%20to%20match%20human%20visual%20perception%2C%20resulting%20in%0Amore%20realistic%20shadow%20effects.%20Extensive%20experiments%20demonstrate%20the%0Aoutstanding%20visual%20performance%20of%20our%20approach%20in%20both%20static%20and%20dynamic%0Ascenarios.%20Remarkably%2C%20all%20these%20steps%20can%20be%20done%20using%20one%20NVIDIA%203090.%0AProject%20page%20is%20at%20https%3A//whalesong-zrs.github.io/OMG3D-projectpage/%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12935v1&entry.124074799=Read"},
{"title": "GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model\n  for Multi-organ Segmentation", "author": "Ruicheng Zhang and Haowei Guo and Zeyu Zhang and Puxin Yan and Shen Zhao", "abstract": "  Multi-organ segmentation is a critical yet challenging task due to complex\nanatomical backgrounds, blurred boundaries, and diverse morphologies. This\nstudy introduces the Gradient-aware Adaptive Momentum Evolution Deep Snake\n(GAMED-Snake) model, which establishes a novel paradigm for contour-based\nsegmentation by integrating gradient-based learning with adaptive momentum\nevolution mechanisms. The GAMED-Snake model incorporates three major\ninnovations: First, the Distance Energy Map Prior (DEMP) generates a\npixel-level force field that effectively attracts contour points towards the\ntrue boundaries, even in scenarios with complex backgrounds and blurred edges.\nSecond, the Differential Convolution Inception Module (DCIM) precisely extracts\ncomprehensive energy gradients, significantly enhancing segmentation accuracy.\nThird, the Adaptive Momentum Evolution Mechanism (AMEM) employs cross-attention\nto establish dynamic features across different iterations of evolution,\nenabling precise boundary alignment for diverse morphologies. Experimental\nresults on four challenging multi-organ segmentation datasets demonstrate that\nGAMED-Snake improves the mDice metric by approximately 2% compared to\nstate-of-the-art methods. Code will be available at\nhttps://github.com/SYSUzrc/GAMED-Snake.\n", "link": "http://arxiv.org/abs/2501.12844v1", "date": "2025-01-22", "relevancy": 2.7182, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5383}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAMED-Snake%3A%20Gradient-aware%20Adaptive%20Momentum%20Evolution%20Deep%20Snake%20Model%0A%20%20for%20Multi-organ%20Segmentation&body=Title%3A%20GAMED-Snake%3A%20Gradient-aware%20Adaptive%20Momentum%20Evolution%20Deep%20Snake%20Model%0A%20%20for%20Multi-organ%20Segmentation%0AAuthor%3A%20Ruicheng%20Zhang%20and%20Haowei%20Guo%20and%20Zeyu%20Zhang%20and%20Puxin%20Yan%20and%20Shen%20Zhao%0AAbstract%3A%20%20%20Multi-organ%20segmentation%20is%20a%20critical%20yet%20challenging%20task%20due%20to%20complex%0Aanatomical%20backgrounds%2C%20blurred%20boundaries%2C%20and%20diverse%20morphologies.%20This%0Astudy%20introduces%20the%20Gradient-aware%20Adaptive%20Momentum%20Evolution%20Deep%20Snake%0A%28GAMED-Snake%29%20model%2C%20which%20establishes%20a%20novel%20paradigm%20for%20contour-based%0Asegmentation%20by%20integrating%20gradient-based%20learning%20with%20adaptive%20momentum%0Aevolution%20mechanisms.%20The%20GAMED-Snake%20model%20incorporates%20three%20major%0Ainnovations%3A%20First%2C%20the%20Distance%20Energy%20Map%20Prior%20%28DEMP%29%20generates%20a%0Apixel-level%20force%20field%20that%20effectively%20attracts%20contour%20points%20towards%20the%0Atrue%20boundaries%2C%20even%20in%20scenarios%20with%20complex%20backgrounds%20and%20blurred%20edges.%0ASecond%2C%20the%20Differential%20Convolution%20Inception%20Module%20%28DCIM%29%20precisely%20extracts%0Acomprehensive%20energy%20gradients%2C%20significantly%20enhancing%20segmentation%20accuracy.%0AThird%2C%20the%20Adaptive%20Momentum%20Evolution%20Mechanism%20%28AMEM%29%20employs%20cross-attention%0Ato%20establish%20dynamic%20features%20across%20different%20iterations%20of%20evolution%2C%0Aenabling%20precise%20boundary%20alignment%20for%20diverse%20morphologies.%20Experimental%0Aresults%20on%20four%20challenging%20multi-organ%20segmentation%20datasets%20demonstrate%20that%0AGAMED-Snake%20improves%20the%20mDice%20metric%20by%20approximately%202%25%20compared%20to%0Astate-of-the-art%20methods.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/SYSUzrc/GAMED-Snake.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAMED-Snake%253A%2520Gradient-aware%2520Adaptive%2520Momentum%2520Evolution%2520Deep%2520Snake%2520Model%250A%2520%2520for%2520Multi-organ%2520Segmentation%26entry.906535625%3DRuicheng%2520Zhang%2520and%2520Haowei%2520Guo%2520and%2520Zeyu%2520Zhang%2520and%2520Puxin%2520Yan%2520and%2520Shen%2520Zhao%26entry.1292438233%3D%2520%2520Multi-organ%2520segmentation%2520is%2520a%2520critical%2520yet%2520challenging%2520task%2520due%2520to%2520complex%250Aanatomical%2520backgrounds%252C%2520blurred%2520boundaries%252C%2520and%2520diverse%2520morphologies.%2520This%250Astudy%2520introduces%2520the%2520Gradient-aware%2520Adaptive%2520Momentum%2520Evolution%2520Deep%2520Snake%250A%2528GAMED-Snake%2529%2520model%252C%2520which%2520establishes%2520a%2520novel%2520paradigm%2520for%2520contour-based%250Asegmentation%2520by%2520integrating%2520gradient-based%2520learning%2520with%2520adaptive%2520momentum%250Aevolution%2520mechanisms.%2520The%2520GAMED-Snake%2520model%2520incorporates%2520three%2520major%250Ainnovations%253A%2520First%252C%2520the%2520Distance%2520Energy%2520Map%2520Prior%2520%2528DEMP%2529%2520generates%2520a%250Apixel-level%2520force%2520field%2520that%2520effectively%2520attracts%2520contour%2520points%2520towards%2520the%250Atrue%2520boundaries%252C%2520even%2520in%2520scenarios%2520with%2520complex%2520backgrounds%2520and%2520blurred%2520edges.%250ASecond%252C%2520the%2520Differential%2520Convolution%2520Inception%2520Module%2520%2528DCIM%2529%2520precisely%2520extracts%250Acomprehensive%2520energy%2520gradients%252C%2520significantly%2520enhancing%2520segmentation%2520accuracy.%250AThird%252C%2520the%2520Adaptive%2520Momentum%2520Evolution%2520Mechanism%2520%2528AMEM%2529%2520employs%2520cross-attention%250Ato%2520establish%2520dynamic%2520features%2520across%2520different%2520iterations%2520of%2520evolution%252C%250Aenabling%2520precise%2520boundary%2520alignment%2520for%2520diverse%2520morphologies.%2520Experimental%250Aresults%2520on%2520four%2520challenging%2520multi-organ%2520segmentation%2520datasets%2520demonstrate%2520that%250AGAMED-Snake%2520improves%2520the%2520mDice%2520metric%2520by%2520approximately%25202%2525%2520compared%2520to%250Astate-of-the-art%2520methods.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/SYSUzrc/GAMED-Snake.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAMED-Snake%3A%20Gradient-aware%20Adaptive%20Momentum%20Evolution%20Deep%20Snake%20Model%0A%20%20for%20Multi-organ%20Segmentation&entry.906535625=Ruicheng%20Zhang%20and%20Haowei%20Guo%20and%20Zeyu%20Zhang%20and%20Puxin%20Yan%20and%20Shen%20Zhao&entry.1292438233=%20%20Multi-organ%20segmentation%20is%20a%20critical%20yet%20challenging%20task%20due%20to%20complex%0Aanatomical%20backgrounds%2C%20blurred%20boundaries%2C%20and%20diverse%20morphologies.%20This%0Astudy%20introduces%20the%20Gradient-aware%20Adaptive%20Momentum%20Evolution%20Deep%20Snake%0A%28GAMED-Snake%29%20model%2C%20which%20establishes%20a%20novel%20paradigm%20for%20contour-based%0Asegmentation%20by%20integrating%20gradient-based%20learning%20with%20adaptive%20momentum%0Aevolution%20mechanisms.%20The%20GAMED-Snake%20model%20incorporates%20three%20major%0Ainnovations%3A%20First%2C%20the%20Distance%20Energy%20Map%20Prior%20%28DEMP%29%20generates%20a%0Apixel-level%20force%20field%20that%20effectively%20attracts%20contour%20points%20towards%20the%0Atrue%20boundaries%2C%20even%20in%20scenarios%20with%20complex%20backgrounds%20and%20blurred%20edges.%0ASecond%2C%20the%20Differential%20Convolution%20Inception%20Module%20%28DCIM%29%20precisely%20extracts%0Acomprehensive%20energy%20gradients%2C%20significantly%20enhancing%20segmentation%20accuracy.%0AThird%2C%20the%20Adaptive%20Momentum%20Evolution%20Mechanism%20%28AMEM%29%20employs%20cross-attention%0Ato%20establish%20dynamic%20features%20across%20different%20iterations%20of%20evolution%2C%0Aenabling%20precise%20boundary%20alignment%20for%20diverse%20morphologies.%20Experimental%0Aresults%20on%20four%20challenging%20multi-organ%20segmentation%20datasets%20demonstrate%20that%0AGAMED-Snake%20improves%20the%20mDice%20metric%20by%20approximately%202%25%20compared%20to%0Astate-of-the-art%20methods.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/SYSUzrc/GAMED-Snake.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12844v1&entry.124074799=Read"},
{"title": "PairingNet: A Learning-based Pair-searching and -matching Network for\n  Image Fragments", "author": "Rixin Zhou and Ding Xia and Yi Zhang and Honglin Pang and Xi Yang and Chuntao Li", "abstract": "  In this paper, we propose a learning-based image fragment pair-searching and\n-matching approach to solve the challenging restoration problem. Existing works\nuse rule-based methods to match similar contour shapes or textures, which are\nalways difficult to tune hyperparameters for extensive data and computationally\ntime-consuming. Therefore, we propose a neural network that can effectively\nutilize neighbor textures with contour shape information to fundamentally\nimprove performance. First, we employ a graph-based network to extract the\nlocal contour and texture features of fragments. Then, for the pair-searching\ntask, we adopt a linear transformer-based module to integrate these local\nfeatures and use contrastive loss to encode the global features of each\nfragment. For the pair-matching task, we design a weighted fusion module to\ndynamically fuse extracted local contour and texture features, and formulate a\nsimilarity matrix for each pair of fragments to calculate the matching score\nand infer the adjacent segment of contours. To faithfully evaluate our proposed\nnetwork, we created a new image fragment dataset through an algorithm we\ndesigned that tears complete images into irregular fragments. The experimental\nresults show that our proposed network achieves excellent pair-searching\naccuracy, reduces matching errors, and significantly reduces computational\ntime. Details, sourcecode, and data are available in our supplementary\nmaterial.\n", "link": "http://arxiv.org/abs/2312.08704v2", "date": "2025-01-22", "relevancy": 2.7042, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5687}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PairingNet%3A%20A%20Learning-based%20Pair-searching%20and%20-matching%20Network%20for%0A%20%20Image%20Fragments&body=Title%3A%20PairingNet%3A%20A%20Learning-based%20Pair-searching%20and%20-matching%20Network%20for%0A%20%20Image%20Fragments%0AAuthor%3A%20Rixin%20Zhou%20and%20Ding%20Xia%20and%20Yi%20Zhang%20and%20Honglin%20Pang%20and%20Xi%20Yang%20and%20Chuntao%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20learning-based%20image%20fragment%20pair-searching%20and%0A-matching%20approach%20to%20solve%20the%20challenging%20restoration%20problem.%20Existing%20works%0Ause%20rule-based%20methods%20to%20match%20similar%20contour%20shapes%20or%20textures%2C%20which%20are%0Aalways%20difficult%20to%20tune%20hyperparameters%20for%20extensive%20data%20and%20computationally%0Atime-consuming.%20Therefore%2C%20we%20propose%20a%20neural%20network%20that%20can%20effectively%0Autilize%20neighbor%20textures%20with%20contour%20shape%20information%20to%20fundamentally%0Aimprove%20performance.%20First%2C%20we%20employ%20a%20graph-based%20network%20to%20extract%20the%0Alocal%20contour%20and%20texture%20features%20of%20fragments.%20Then%2C%20for%20the%20pair-searching%0Atask%2C%20we%20adopt%20a%20linear%20transformer-based%20module%20to%20integrate%20these%20local%0Afeatures%20and%20use%20contrastive%20loss%20to%20encode%20the%20global%20features%20of%20each%0Afragment.%20For%20the%20pair-matching%20task%2C%20we%20design%20a%20weighted%20fusion%20module%20to%0Adynamically%20fuse%20extracted%20local%20contour%20and%20texture%20features%2C%20and%20formulate%20a%0Asimilarity%20matrix%20for%20each%20pair%20of%20fragments%20to%20calculate%20the%20matching%20score%0Aand%20infer%20the%20adjacent%20segment%20of%20contours.%20To%20faithfully%20evaluate%20our%20proposed%0Anetwork%2C%20we%20created%20a%20new%20image%20fragment%20dataset%20through%20an%20algorithm%20we%0Adesigned%20that%20tears%20complete%20images%20into%20irregular%20fragments.%20The%20experimental%0Aresults%20show%20that%20our%20proposed%20network%20achieves%20excellent%20pair-searching%0Aaccuracy%2C%20reduces%20matching%20errors%2C%20and%20significantly%20reduces%20computational%0Atime.%20Details%2C%20sourcecode%2C%20and%20data%20are%20available%20in%20our%20supplementary%0Amaterial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPairingNet%253A%2520A%2520Learning-based%2520Pair-searching%2520and%2520-matching%2520Network%2520for%250A%2520%2520Image%2520Fragments%26entry.906535625%3DRixin%2520Zhou%2520and%2520Ding%2520Xia%2520and%2520Yi%2520Zhang%2520and%2520Honglin%2520Pang%2520and%2520Xi%2520Yang%2520and%2520Chuntao%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520learning-based%2520image%2520fragment%2520pair-searching%2520and%250A-matching%2520approach%2520to%2520solve%2520the%2520challenging%2520restoration%2520problem.%2520Existing%2520works%250Ause%2520rule-based%2520methods%2520to%2520match%2520similar%2520contour%2520shapes%2520or%2520textures%252C%2520which%2520are%250Aalways%2520difficult%2520to%2520tune%2520hyperparameters%2520for%2520extensive%2520data%2520and%2520computationally%250Atime-consuming.%2520Therefore%252C%2520we%2520propose%2520a%2520neural%2520network%2520that%2520can%2520effectively%250Autilize%2520neighbor%2520textures%2520with%2520contour%2520shape%2520information%2520to%2520fundamentally%250Aimprove%2520performance.%2520First%252C%2520we%2520employ%2520a%2520graph-based%2520network%2520to%2520extract%2520the%250Alocal%2520contour%2520and%2520texture%2520features%2520of%2520fragments.%2520Then%252C%2520for%2520the%2520pair-searching%250Atask%252C%2520we%2520adopt%2520a%2520linear%2520transformer-based%2520module%2520to%2520integrate%2520these%2520local%250Afeatures%2520and%2520use%2520contrastive%2520loss%2520to%2520encode%2520the%2520global%2520features%2520of%2520each%250Afragment.%2520For%2520the%2520pair-matching%2520task%252C%2520we%2520design%2520a%2520weighted%2520fusion%2520module%2520to%250Adynamically%2520fuse%2520extracted%2520local%2520contour%2520and%2520texture%2520features%252C%2520and%2520formulate%2520a%250Asimilarity%2520matrix%2520for%2520each%2520pair%2520of%2520fragments%2520to%2520calculate%2520the%2520matching%2520score%250Aand%2520infer%2520the%2520adjacent%2520segment%2520of%2520contours.%2520To%2520faithfully%2520evaluate%2520our%2520proposed%250Anetwork%252C%2520we%2520created%2520a%2520new%2520image%2520fragment%2520dataset%2520through%2520an%2520algorithm%2520we%250Adesigned%2520that%2520tears%2520complete%2520images%2520into%2520irregular%2520fragments.%2520The%2520experimental%250Aresults%2520show%2520that%2520our%2520proposed%2520network%2520achieves%2520excellent%2520pair-searching%250Aaccuracy%252C%2520reduces%2520matching%2520errors%252C%2520and%2520significantly%2520reduces%2520computational%250Atime.%2520Details%252C%2520sourcecode%252C%2520and%2520data%2520are%2520available%2520in%2520our%2520supplementary%250Amaterial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PairingNet%3A%20A%20Learning-based%20Pair-searching%20and%20-matching%20Network%20for%0A%20%20Image%20Fragments&entry.906535625=Rixin%20Zhou%20and%20Ding%20Xia%20and%20Yi%20Zhang%20and%20Honglin%20Pang%20and%20Xi%20Yang%20and%20Chuntao%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20learning-based%20image%20fragment%20pair-searching%20and%0A-matching%20approach%20to%20solve%20the%20challenging%20restoration%20problem.%20Existing%20works%0Ause%20rule-based%20methods%20to%20match%20similar%20contour%20shapes%20or%20textures%2C%20which%20are%0Aalways%20difficult%20to%20tune%20hyperparameters%20for%20extensive%20data%20and%20computationally%0Atime-consuming.%20Therefore%2C%20we%20propose%20a%20neural%20network%20that%20can%20effectively%0Autilize%20neighbor%20textures%20with%20contour%20shape%20information%20to%20fundamentally%0Aimprove%20performance.%20First%2C%20we%20employ%20a%20graph-based%20network%20to%20extract%20the%0Alocal%20contour%20and%20texture%20features%20of%20fragments.%20Then%2C%20for%20the%20pair-searching%0Atask%2C%20we%20adopt%20a%20linear%20transformer-based%20module%20to%20integrate%20these%20local%0Afeatures%20and%20use%20contrastive%20loss%20to%20encode%20the%20global%20features%20of%20each%0Afragment.%20For%20the%20pair-matching%20task%2C%20we%20design%20a%20weighted%20fusion%20module%20to%0Adynamically%20fuse%20extracted%20local%20contour%20and%20texture%20features%2C%20and%20formulate%20a%0Asimilarity%20matrix%20for%20each%20pair%20of%20fragments%20to%20calculate%20the%20matching%20score%0Aand%20infer%20the%20adjacent%20segment%20of%20contours.%20To%20faithfully%20evaluate%20our%20proposed%0Anetwork%2C%20we%20created%20a%20new%20image%20fragment%20dataset%20through%20an%20algorithm%20we%0Adesigned%20that%20tears%20complete%20images%20into%20irregular%20fragments.%20The%20experimental%0Aresults%20show%20that%20our%20proposed%20network%20achieves%20excellent%20pair-searching%0Aaccuracy%2C%20reduces%20matching%20errors%2C%20and%20significantly%20reduces%20computational%0Atime.%20Details%2C%20sourcecode%2C%20and%20data%20are%20available%20in%20our%20supplementary%0Amaterial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08704v2&entry.124074799=Read"},
{"title": "Contrastive Language-Structure Pre-training Driven by Materials Science\n  Literature", "author": "Yuta Suzuki and Tatsunori Taniai and Ryo Igarashi and Kotaro Saito and Naoya Chiba and Yoshitaka Ushiku and Kanta Ono", "abstract": "  Understanding structure-property relationships is an essential yet\nchallenging aspect of materials discovery and development. To facilitate this\nprocess, recent studies in materials informatics have sought latent embedding\nspaces of crystal structures to capture their similarities based on properties\nand functionalities. However, abstract feature-based embedding spaces are\nhuman-unfriendly and prevent intuitive and efficient exploration of the vast\nmaterials space. Here we introduce Contrastive Language--Structure Pre-training\n(CLaSP), a learning paradigm for constructing crossmodal embedding spaces\nbetween crystal structures and texts. CLaSP aims to achieve material embeddings\nthat 1) capture property- and functionality-related similarities between\ncrystal structures and 2) allow intuitive retrieval of materials via\nuser-provided description texts as queries. To compensate for the lack of\nsufficient datasets linking crystal structures with textual descriptions, CLaSP\nleverages a dataset of over 400,000 published crystal structures and\ncorresponding publication records, including paper titles and abstracts, for\ntraining. We demonstrate the effectiveness of CLaSP through text-based crystal\nstructure screening and embedding space visualization.\n", "link": "http://arxiv.org/abs/2501.12919v1", "date": "2025-01-22", "relevancy": 2.6605, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5381}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20Language-Structure%20Pre-training%20Driven%20by%20Materials%20Science%0A%20%20Literature&body=Title%3A%20Contrastive%20Language-Structure%20Pre-training%20Driven%20by%20Materials%20Science%0A%20%20Literature%0AAuthor%3A%20Yuta%20Suzuki%20and%20Tatsunori%20Taniai%20and%20Ryo%20Igarashi%20and%20Kotaro%20Saito%20and%20Naoya%20Chiba%20and%20Yoshitaka%20Ushiku%20and%20Kanta%20Ono%0AAbstract%3A%20%20%20Understanding%20structure-property%20relationships%20is%20an%20essential%20yet%0Achallenging%20aspect%20of%20materials%20discovery%20and%20development.%20To%20facilitate%20this%0Aprocess%2C%20recent%20studies%20in%20materials%20informatics%20have%20sought%20latent%20embedding%0Aspaces%20of%20crystal%20structures%20to%20capture%20their%20similarities%20based%20on%20properties%0Aand%20functionalities.%20However%2C%20abstract%20feature-based%20embedding%20spaces%20are%0Ahuman-unfriendly%20and%20prevent%20intuitive%20and%20efficient%20exploration%20of%20the%20vast%0Amaterials%20space.%20Here%20we%20introduce%20Contrastive%20Language--Structure%20Pre-training%0A%28CLaSP%29%2C%20a%20learning%20paradigm%20for%20constructing%20crossmodal%20embedding%20spaces%0Abetween%20crystal%20structures%20and%20texts.%20CLaSP%20aims%20to%20achieve%20material%20embeddings%0Athat%201%29%20capture%20property-%20and%20functionality-related%20similarities%20between%0Acrystal%20structures%20and%202%29%20allow%20intuitive%20retrieval%20of%20materials%20via%0Auser-provided%20description%20texts%20as%20queries.%20To%20compensate%20for%20the%20lack%20of%0Asufficient%20datasets%20linking%20crystal%20structures%20with%20textual%20descriptions%2C%20CLaSP%0Aleverages%20a%20dataset%20of%20over%20400%2C000%20published%20crystal%20structures%20and%0Acorresponding%20publication%20records%2C%20including%20paper%20titles%20and%20abstracts%2C%20for%0Atraining.%20We%20demonstrate%20the%20effectiveness%20of%20CLaSP%20through%20text-based%20crystal%0Astructure%20screening%20and%20embedding%20space%20visualization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520Language-Structure%2520Pre-training%2520Driven%2520by%2520Materials%2520Science%250A%2520%2520Literature%26entry.906535625%3DYuta%2520Suzuki%2520and%2520Tatsunori%2520Taniai%2520and%2520Ryo%2520Igarashi%2520and%2520Kotaro%2520Saito%2520and%2520Naoya%2520Chiba%2520and%2520Yoshitaka%2520Ushiku%2520and%2520Kanta%2520Ono%26entry.1292438233%3D%2520%2520Understanding%2520structure-property%2520relationships%2520is%2520an%2520essential%2520yet%250Achallenging%2520aspect%2520of%2520materials%2520discovery%2520and%2520development.%2520To%2520facilitate%2520this%250Aprocess%252C%2520recent%2520studies%2520in%2520materials%2520informatics%2520have%2520sought%2520latent%2520embedding%250Aspaces%2520of%2520crystal%2520structures%2520to%2520capture%2520their%2520similarities%2520based%2520on%2520properties%250Aand%2520functionalities.%2520However%252C%2520abstract%2520feature-based%2520embedding%2520spaces%2520are%250Ahuman-unfriendly%2520and%2520prevent%2520intuitive%2520and%2520efficient%2520exploration%2520of%2520the%2520vast%250Amaterials%2520space.%2520Here%2520we%2520introduce%2520Contrastive%2520Language--Structure%2520Pre-training%250A%2528CLaSP%2529%252C%2520a%2520learning%2520paradigm%2520for%2520constructing%2520crossmodal%2520embedding%2520spaces%250Abetween%2520crystal%2520structures%2520and%2520texts.%2520CLaSP%2520aims%2520to%2520achieve%2520material%2520embeddings%250Athat%25201%2529%2520capture%2520property-%2520and%2520functionality-related%2520similarities%2520between%250Acrystal%2520structures%2520and%25202%2529%2520allow%2520intuitive%2520retrieval%2520of%2520materials%2520via%250Auser-provided%2520description%2520texts%2520as%2520queries.%2520To%2520compensate%2520for%2520the%2520lack%2520of%250Asufficient%2520datasets%2520linking%2520crystal%2520structures%2520with%2520textual%2520descriptions%252C%2520CLaSP%250Aleverages%2520a%2520dataset%2520of%2520over%2520400%252C000%2520published%2520crystal%2520structures%2520and%250Acorresponding%2520publication%2520records%252C%2520including%2520paper%2520titles%2520and%2520abstracts%252C%2520for%250Atraining.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520CLaSP%2520through%2520text-based%2520crystal%250Astructure%2520screening%2520and%2520embedding%2520space%2520visualization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20Language-Structure%20Pre-training%20Driven%20by%20Materials%20Science%0A%20%20Literature&entry.906535625=Yuta%20Suzuki%20and%20Tatsunori%20Taniai%20and%20Ryo%20Igarashi%20and%20Kotaro%20Saito%20and%20Naoya%20Chiba%20and%20Yoshitaka%20Ushiku%20and%20Kanta%20Ono&entry.1292438233=%20%20Understanding%20structure-property%20relationships%20is%20an%20essential%20yet%0Achallenging%20aspect%20of%20materials%20discovery%20and%20development.%20To%20facilitate%20this%0Aprocess%2C%20recent%20studies%20in%20materials%20informatics%20have%20sought%20latent%20embedding%0Aspaces%20of%20crystal%20structures%20to%20capture%20their%20similarities%20based%20on%20properties%0Aand%20functionalities.%20However%2C%20abstract%20feature-based%20embedding%20spaces%20are%0Ahuman-unfriendly%20and%20prevent%20intuitive%20and%20efficient%20exploration%20of%20the%20vast%0Amaterials%20space.%20Here%20we%20introduce%20Contrastive%20Language--Structure%20Pre-training%0A%28CLaSP%29%2C%20a%20learning%20paradigm%20for%20constructing%20crossmodal%20embedding%20spaces%0Abetween%20crystal%20structures%20and%20texts.%20CLaSP%20aims%20to%20achieve%20material%20embeddings%0Athat%201%29%20capture%20property-%20and%20functionality-related%20similarities%20between%0Acrystal%20structures%20and%202%29%20allow%20intuitive%20retrieval%20of%20materials%20via%0Auser-provided%20description%20texts%20as%20queries.%20To%20compensate%20for%20the%20lack%20of%0Asufficient%20datasets%20linking%20crystal%20structures%20with%20textual%20descriptions%2C%20CLaSP%0Aleverages%20a%20dataset%20of%20over%20400%2C000%20published%20crystal%20structures%20and%0Acorresponding%20publication%20records%2C%20including%20paper%20titles%20and%20abstracts%2C%20for%0Atraining.%20We%20demonstrate%20the%20effectiveness%20of%20CLaSP%20through%20text-based%20crystal%0Astructure%20screening%20and%20embedding%20space%20visualization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12919v1&entry.124074799=Read"},
{"title": "Learning accurate rigid registration for longitudinal brain MRI from\n  synthetic data", "author": "Jingru Fu and Adrian V. Dalca and Bruce Fischl and Rodrigo Moreno and Malte Hoffmann", "abstract": "  Rigid registration aims to determine the translations and rotations necessary\nto align features in a pair of images. While recent machine learning methods\nhave become state-of-the-art for linear and deformable registration across\nsubjects, they have demonstrated limitations when applied to longitudinal\n(within-subject) registration, where achieving precise alignment is critical.\nBuilding on an existing framework for anatomy-aware, acquisition-agnostic\naffine registration, we propose a model optimized for longitudinal, rigid brain\nregistration. By training the model with synthetic within-subject pairs\naugmented with rigid and subtle nonlinear transforms, the model estimates more\naccurate rigid transforms than previous cross-subject networks and performs\nrobustly on longitudinal registration pairs within and across magnetic\nresonance imaging (MRI) contrasts.\n", "link": "http://arxiv.org/abs/2501.13010v1", "date": "2025-01-22", "relevancy": 2.5972, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5306}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20accurate%20rigid%20registration%20for%20longitudinal%20brain%20MRI%20from%0A%20%20synthetic%20data&body=Title%3A%20Learning%20accurate%20rigid%20registration%20for%20longitudinal%20brain%20MRI%20from%0A%20%20synthetic%20data%0AAuthor%3A%20Jingru%20Fu%20and%20Adrian%20V.%20Dalca%20and%20Bruce%20Fischl%20and%20Rodrigo%20Moreno%20and%20Malte%20Hoffmann%0AAbstract%3A%20%20%20Rigid%20registration%20aims%20to%20determine%20the%20translations%20and%20rotations%20necessary%0Ato%20align%20features%20in%20a%20pair%20of%20images.%20While%20recent%20machine%20learning%20methods%0Ahave%20become%20state-of-the-art%20for%20linear%20and%20deformable%20registration%20across%0Asubjects%2C%20they%20have%20demonstrated%20limitations%20when%20applied%20to%20longitudinal%0A%28within-subject%29%20registration%2C%20where%20achieving%20precise%20alignment%20is%20critical.%0ABuilding%20on%20an%20existing%20framework%20for%20anatomy-aware%2C%20acquisition-agnostic%0Aaffine%20registration%2C%20we%20propose%20a%20model%20optimized%20for%20longitudinal%2C%20rigid%20brain%0Aregistration.%20By%20training%20the%20model%20with%20synthetic%20within-subject%20pairs%0Aaugmented%20with%20rigid%20and%20subtle%20nonlinear%20transforms%2C%20the%20model%20estimates%20more%0Aaccurate%20rigid%20transforms%20than%20previous%20cross-subject%20networks%20and%20performs%0Arobustly%20on%20longitudinal%20registration%20pairs%20within%20and%20across%20magnetic%0Aresonance%20imaging%20%28MRI%29%20contrasts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520accurate%2520rigid%2520registration%2520for%2520longitudinal%2520brain%2520MRI%2520from%250A%2520%2520synthetic%2520data%26entry.906535625%3DJingru%2520Fu%2520and%2520Adrian%2520V.%2520Dalca%2520and%2520Bruce%2520Fischl%2520and%2520Rodrigo%2520Moreno%2520and%2520Malte%2520Hoffmann%26entry.1292438233%3D%2520%2520Rigid%2520registration%2520aims%2520to%2520determine%2520the%2520translations%2520and%2520rotations%2520necessary%250Ato%2520align%2520features%2520in%2520a%2520pair%2520of%2520images.%2520While%2520recent%2520machine%2520learning%2520methods%250Ahave%2520become%2520state-of-the-art%2520for%2520linear%2520and%2520deformable%2520registration%2520across%250Asubjects%252C%2520they%2520have%2520demonstrated%2520limitations%2520when%2520applied%2520to%2520longitudinal%250A%2528within-subject%2529%2520registration%252C%2520where%2520achieving%2520precise%2520alignment%2520is%2520critical.%250ABuilding%2520on%2520an%2520existing%2520framework%2520for%2520anatomy-aware%252C%2520acquisition-agnostic%250Aaffine%2520registration%252C%2520we%2520propose%2520a%2520model%2520optimized%2520for%2520longitudinal%252C%2520rigid%2520brain%250Aregistration.%2520By%2520training%2520the%2520model%2520with%2520synthetic%2520within-subject%2520pairs%250Aaugmented%2520with%2520rigid%2520and%2520subtle%2520nonlinear%2520transforms%252C%2520the%2520model%2520estimates%2520more%250Aaccurate%2520rigid%2520transforms%2520than%2520previous%2520cross-subject%2520networks%2520and%2520performs%250Arobustly%2520on%2520longitudinal%2520registration%2520pairs%2520within%2520and%2520across%2520magnetic%250Aresonance%2520imaging%2520%2528MRI%2529%2520contrasts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20accurate%20rigid%20registration%20for%20longitudinal%20brain%20MRI%20from%0A%20%20synthetic%20data&entry.906535625=Jingru%20Fu%20and%20Adrian%20V.%20Dalca%20and%20Bruce%20Fischl%20and%20Rodrigo%20Moreno%20and%20Malte%20Hoffmann&entry.1292438233=%20%20Rigid%20registration%20aims%20to%20determine%20the%20translations%20and%20rotations%20necessary%0Ato%20align%20features%20in%20a%20pair%20of%20images.%20While%20recent%20machine%20learning%20methods%0Ahave%20become%20state-of-the-art%20for%20linear%20and%20deformable%20registration%20across%0Asubjects%2C%20they%20have%20demonstrated%20limitations%20when%20applied%20to%20longitudinal%0A%28within-subject%29%20registration%2C%20where%20achieving%20precise%20alignment%20is%20critical.%0ABuilding%20on%20an%20existing%20framework%20for%20anatomy-aware%2C%20acquisition-agnostic%0Aaffine%20registration%2C%20we%20propose%20a%20model%20optimized%20for%20longitudinal%2C%20rigid%20brain%0Aregistration.%20By%20training%20the%20model%20with%20synthetic%20within-subject%20pairs%0Aaugmented%20with%20rigid%20and%20subtle%20nonlinear%20transforms%2C%20the%20model%20estimates%20more%0Aaccurate%20rigid%20transforms%20than%20previous%20cross-subject%20networks%20and%20performs%0Arobustly%20on%20longitudinal%20registration%20pairs%20within%20and%20across%20magnetic%0Aresonance%20imaging%20%28MRI%29%20contrasts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13010v1&entry.124074799=Read"},
{"title": "MorphoSkel3D: Morphological Skeletonization of 3D Point Clouds for\n  Informed Sampling in Object Classification and Retrieval", "author": "Pierre Onghena and Santiago Velasco-Forero and Beatriz Marcotegui", "abstract": "  Point clouds are a set of data points in space to represent the 3D geometry\nof objects. A fundamental step in the processing is to identify a subset of\npoints to represent the shape. While traditional sampling methods often ignore\nto incorporate geometrical information, recent developments in learning-based\nsampling models have achieved significant levels of performance. With the\nintegration of geometrical priors, the ability to learn and preserve the\nunderlying structure can be enhanced when sampling. To shed light into the\nshape, a qualitative skeleton serves as an effective descriptor to guide\nsampling for both local and global geometries. In this paper, we introduce\nMorphoSkel3D as a new technique based on morphology to facilitate an efficient\nskeletonization of shapes. With its low computational cost, MorphoSkel3D is a\nunique, rule-based algorithm to benchmark its quality and performance on two\nlarge datasets, ModelNet and ShapeNet, under different sampling ratios. The\nresults show that training with MorphoSkel3D leads to an informed and more\naccurate sampling in the practical application of object classification and\npoint cloud retrieval.\n", "link": "http://arxiv.org/abs/2501.12974v1", "date": "2025-01-22", "relevancy": 2.5954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5131}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MorphoSkel3D%3A%20Morphological%20Skeletonization%20of%203D%20Point%20Clouds%20for%0A%20%20Informed%20Sampling%20in%20Object%20Classification%20and%20Retrieval&body=Title%3A%20MorphoSkel3D%3A%20Morphological%20Skeletonization%20of%203D%20Point%20Clouds%20for%0A%20%20Informed%20Sampling%20in%20Object%20Classification%20and%20Retrieval%0AAuthor%3A%20Pierre%20Onghena%20and%20Santiago%20Velasco-Forero%20and%20Beatriz%20Marcotegui%0AAbstract%3A%20%20%20Point%20clouds%20are%20a%20set%20of%20data%20points%20in%20space%20to%20represent%20the%203D%20geometry%0Aof%20objects.%20A%20fundamental%20step%20in%20the%20processing%20is%20to%20identify%20a%20subset%20of%0Apoints%20to%20represent%20the%20shape.%20While%20traditional%20sampling%20methods%20often%20ignore%0Ato%20incorporate%20geometrical%20information%2C%20recent%20developments%20in%20learning-based%0Asampling%20models%20have%20achieved%20significant%20levels%20of%20performance.%20With%20the%0Aintegration%20of%20geometrical%20priors%2C%20the%20ability%20to%20learn%20and%20preserve%20the%0Aunderlying%20structure%20can%20be%20enhanced%20when%20sampling.%20To%20shed%20light%20into%20the%0Ashape%2C%20a%20qualitative%20skeleton%20serves%20as%20an%20effective%20descriptor%20to%20guide%0Asampling%20for%20both%20local%20and%20global%20geometries.%20In%20this%20paper%2C%20we%20introduce%0AMorphoSkel3D%20as%20a%20new%20technique%20based%20on%20morphology%20to%20facilitate%20an%20efficient%0Askeletonization%20of%20shapes.%20With%20its%20low%20computational%20cost%2C%20MorphoSkel3D%20is%20a%0Aunique%2C%20rule-based%20algorithm%20to%20benchmark%20its%20quality%20and%20performance%20on%20two%0Alarge%20datasets%2C%20ModelNet%20and%20ShapeNet%2C%20under%20different%20sampling%20ratios.%20The%0Aresults%20show%20that%20training%20with%20MorphoSkel3D%20leads%20to%20an%20informed%20and%20more%0Aaccurate%20sampling%20in%20the%20practical%20application%20of%20object%20classification%20and%0Apoint%20cloud%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphoSkel3D%253A%2520Morphological%2520Skeletonization%2520of%25203D%2520Point%2520Clouds%2520for%250A%2520%2520Informed%2520Sampling%2520in%2520Object%2520Classification%2520and%2520Retrieval%26entry.906535625%3DPierre%2520Onghena%2520and%2520Santiago%2520Velasco-Forero%2520and%2520Beatriz%2520Marcotegui%26entry.1292438233%3D%2520%2520Point%2520clouds%2520are%2520a%2520set%2520of%2520data%2520points%2520in%2520space%2520to%2520represent%2520the%25203D%2520geometry%250Aof%2520objects.%2520A%2520fundamental%2520step%2520in%2520the%2520processing%2520is%2520to%2520identify%2520a%2520subset%2520of%250Apoints%2520to%2520represent%2520the%2520shape.%2520While%2520traditional%2520sampling%2520methods%2520often%2520ignore%250Ato%2520incorporate%2520geometrical%2520information%252C%2520recent%2520developments%2520in%2520learning-based%250Asampling%2520models%2520have%2520achieved%2520significant%2520levels%2520of%2520performance.%2520With%2520the%250Aintegration%2520of%2520geometrical%2520priors%252C%2520the%2520ability%2520to%2520learn%2520and%2520preserve%2520the%250Aunderlying%2520structure%2520can%2520be%2520enhanced%2520when%2520sampling.%2520To%2520shed%2520light%2520into%2520the%250Ashape%252C%2520a%2520qualitative%2520skeleton%2520serves%2520as%2520an%2520effective%2520descriptor%2520to%2520guide%250Asampling%2520for%2520both%2520local%2520and%2520global%2520geometries.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AMorphoSkel3D%2520as%2520a%2520new%2520technique%2520based%2520on%2520morphology%2520to%2520facilitate%2520an%2520efficient%250Askeletonization%2520of%2520shapes.%2520With%2520its%2520low%2520computational%2520cost%252C%2520MorphoSkel3D%2520is%2520a%250Aunique%252C%2520rule-based%2520algorithm%2520to%2520benchmark%2520its%2520quality%2520and%2520performance%2520on%2520two%250Alarge%2520datasets%252C%2520ModelNet%2520and%2520ShapeNet%252C%2520under%2520different%2520sampling%2520ratios.%2520The%250Aresults%2520show%2520that%2520training%2520with%2520MorphoSkel3D%2520leads%2520to%2520an%2520informed%2520and%2520more%250Aaccurate%2520sampling%2520in%2520the%2520practical%2520application%2520of%2520object%2520classification%2520and%250Apoint%2520cloud%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MorphoSkel3D%3A%20Morphological%20Skeletonization%20of%203D%20Point%20Clouds%20for%0A%20%20Informed%20Sampling%20in%20Object%20Classification%20and%20Retrieval&entry.906535625=Pierre%20Onghena%20and%20Santiago%20Velasco-Forero%20and%20Beatriz%20Marcotegui&entry.1292438233=%20%20Point%20clouds%20are%20a%20set%20of%20data%20points%20in%20space%20to%20represent%20the%203D%20geometry%0Aof%20objects.%20A%20fundamental%20step%20in%20the%20processing%20is%20to%20identify%20a%20subset%20of%0Apoints%20to%20represent%20the%20shape.%20While%20traditional%20sampling%20methods%20often%20ignore%0Ato%20incorporate%20geometrical%20information%2C%20recent%20developments%20in%20learning-based%0Asampling%20models%20have%20achieved%20significant%20levels%20of%20performance.%20With%20the%0Aintegration%20of%20geometrical%20priors%2C%20the%20ability%20to%20learn%20and%20preserve%20the%0Aunderlying%20structure%20can%20be%20enhanced%20when%20sampling.%20To%20shed%20light%20into%20the%0Ashape%2C%20a%20qualitative%20skeleton%20serves%20as%20an%20effective%20descriptor%20to%20guide%0Asampling%20for%20both%20local%20and%20global%20geometries.%20In%20this%20paper%2C%20we%20introduce%0AMorphoSkel3D%20as%20a%20new%20technique%20based%20on%20morphology%20to%20facilitate%20an%20efficient%0Askeletonization%20of%20shapes.%20With%20its%20low%20computational%20cost%2C%20MorphoSkel3D%20is%20a%0Aunique%2C%20rule-based%20algorithm%20to%20benchmark%20its%20quality%20and%20performance%20on%20two%0Alarge%20datasets%2C%20ModelNet%20and%20ShapeNet%2C%20under%20different%20sampling%20ratios.%20The%0Aresults%20show%20that%20training%20with%20MorphoSkel3D%20leads%20to%20an%20informed%20and%20more%0Aaccurate%20sampling%20in%20the%20practical%20application%20of%20object%20classification%20and%0Apoint%20cloud%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12974v1&entry.124074799=Read"},
{"title": "HierPromptLM: A Pure PLM-based Framework for Representation Learning on\n  Heterogeneous Text-rich Networks", "author": "Qiuyu Zhu and Liang Zhang and Qianxiong Xu and Cheng Long", "abstract": "  Representation learning on heterogeneous text-rich networks (HTRNs), which\nconsist of multiple types of nodes and edges with each node associated with\ntextual information, is essential for various real-world applications. Given\nthe success of pretrained language models (PLMs) in processing text data,\nrecent efforts have focused on integrating PLMs into HTRN representation\nlearning. These methods typically handle textual and structural information\nseparately, using both PLMs and heterogeneous graph neural networks (HGNNs).\nHowever, this separation fails to capture the critical interactions between\nthese two types of information within HTRNs. Additionally, it necessitates an\nextra alignment step, which is challenging due to the fundamental differences\nbetween distinct embedding spaces generated by PLMs and HGNNs. To deal with it,\nwe propose HierPromptLM, a novel pure PLM-based framework that seamlessly\nmodels both text data and graph structures without the need for separate\nprocessing. Firstly, we develop a Hierarchical Prompt module that employs\nprompt learning to integrate text data and heterogeneous graph structures at\nboth the node and edge levels, within a unified textual space. Building upon\nthis foundation, we further introduce two innovative HTRN-tailored pretraining\ntasks to fine-tune PLMs for representation learning by emphasizing the inherent\nheterogeneity and interactions between textual and structural information\nwithin HTRNs. Extensive experiments on two real-world HTRN datasets demonstrate\nHierPromptLM outperforms state-of-the-art methods, achieving significant\nimprovements of up to 6.08% for node classification and 10.84% for link\nprediction.\n", "link": "http://arxiv.org/abs/2501.12857v1", "date": "2025-01-22", "relevancy": 2.5619, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5405}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5252}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HierPromptLM%3A%20A%20Pure%20PLM-based%20Framework%20for%20Representation%20Learning%20on%0A%20%20Heterogeneous%20Text-rich%20Networks&body=Title%3A%20HierPromptLM%3A%20A%20Pure%20PLM-based%20Framework%20for%20Representation%20Learning%20on%0A%20%20Heterogeneous%20Text-rich%20Networks%0AAuthor%3A%20Qiuyu%20Zhu%20and%20Liang%20Zhang%20and%20Qianxiong%20Xu%20and%20Cheng%20Long%0AAbstract%3A%20%20%20Representation%20learning%20on%20heterogeneous%20text-rich%20networks%20%28HTRNs%29%2C%20which%0Aconsist%20of%20multiple%20types%20of%20nodes%20and%20edges%20with%20each%20node%20associated%20with%0Atextual%20information%2C%20is%20essential%20for%20various%20real-world%20applications.%20Given%0Athe%20success%20of%20pretrained%20language%20models%20%28PLMs%29%20in%20processing%20text%20data%2C%0Arecent%20efforts%20have%20focused%20on%20integrating%20PLMs%20into%20HTRN%20representation%0Alearning.%20These%20methods%20typically%20handle%20textual%20and%20structural%20information%0Aseparately%2C%20using%20both%20PLMs%20and%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29.%0AHowever%2C%20this%20separation%20fails%20to%20capture%20the%20critical%20interactions%20between%0Athese%20two%20types%20of%20information%20within%20HTRNs.%20Additionally%2C%20it%20necessitates%20an%0Aextra%20alignment%20step%2C%20which%20is%20challenging%20due%20to%20the%20fundamental%20differences%0Abetween%20distinct%20embedding%20spaces%20generated%20by%20PLMs%20and%20HGNNs.%20To%20deal%20with%20it%2C%0Awe%20propose%20HierPromptLM%2C%20a%20novel%20pure%20PLM-based%20framework%20that%20seamlessly%0Amodels%20both%20text%20data%20and%20graph%20structures%20without%20the%20need%20for%20separate%0Aprocessing.%20Firstly%2C%20we%20develop%20a%20Hierarchical%20Prompt%20module%20that%20employs%0Aprompt%20learning%20to%20integrate%20text%20data%20and%20heterogeneous%20graph%20structures%20at%0Aboth%20the%20node%20and%20edge%20levels%2C%20within%20a%20unified%20textual%20space.%20Building%20upon%0Athis%20foundation%2C%20we%20further%20introduce%20two%20innovative%20HTRN-tailored%20pretraining%0Atasks%20to%20fine-tune%20PLMs%20for%20representation%20learning%20by%20emphasizing%20the%20inherent%0Aheterogeneity%20and%20interactions%20between%20textual%20and%20structural%20information%0Awithin%20HTRNs.%20Extensive%20experiments%20on%20two%20real-world%20HTRN%20datasets%20demonstrate%0AHierPromptLM%20outperforms%20state-of-the-art%20methods%2C%20achieving%20significant%0Aimprovements%20of%20up%20to%206.08%25%20for%20node%20classification%20and%2010.84%25%20for%20link%0Aprediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierPromptLM%253A%2520A%2520Pure%2520PLM-based%2520Framework%2520for%2520Representation%2520Learning%2520on%250A%2520%2520Heterogeneous%2520Text-rich%2520Networks%26entry.906535625%3DQiuyu%2520Zhu%2520and%2520Liang%2520Zhang%2520and%2520Qianxiong%2520Xu%2520and%2520Cheng%2520Long%26entry.1292438233%3D%2520%2520Representation%2520learning%2520on%2520heterogeneous%2520text-rich%2520networks%2520%2528HTRNs%2529%252C%2520which%250Aconsist%2520of%2520multiple%2520types%2520of%2520nodes%2520and%2520edges%2520with%2520each%2520node%2520associated%2520with%250Atextual%2520information%252C%2520is%2520essential%2520for%2520various%2520real-world%2520applications.%2520Given%250Athe%2520success%2520of%2520pretrained%2520language%2520models%2520%2528PLMs%2529%2520in%2520processing%2520text%2520data%252C%250Arecent%2520efforts%2520have%2520focused%2520on%2520integrating%2520PLMs%2520into%2520HTRN%2520representation%250Alearning.%2520These%2520methods%2520typically%2520handle%2520textual%2520and%2520structural%2520information%250Aseparately%252C%2520using%2520both%2520PLMs%2520and%2520heterogeneous%2520graph%2520neural%2520networks%2520%2528HGNNs%2529.%250AHowever%252C%2520this%2520separation%2520fails%2520to%2520capture%2520the%2520critical%2520interactions%2520between%250Athese%2520two%2520types%2520of%2520information%2520within%2520HTRNs.%2520Additionally%252C%2520it%2520necessitates%2520an%250Aextra%2520alignment%2520step%252C%2520which%2520is%2520challenging%2520due%2520to%2520the%2520fundamental%2520differences%250Abetween%2520distinct%2520embedding%2520spaces%2520generated%2520by%2520PLMs%2520and%2520HGNNs.%2520To%2520deal%2520with%2520it%252C%250Awe%2520propose%2520HierPromptLM%252C%2520a%2520novel%2520pure%2520PLM-based%2520framework%2520that%2520seamlessly%250Amodels%2520both%2520text%2520data%2520and%2520graph%2520structures%2520without%2520the%2520need%2520for%2520separate%250Aprocessing.%2520Firstly%252C%2520we%2520develop%2520a%2520Hierarchical%2520Prompt%2520module%2520that%2520employs%250Aprompt%2520learning%2520to%2520integrate%2520text%2520data%2520and%2520heterogeneous%2520graph%2520structures%2520at%250Aboth%2520the%2520node%2520and%2520edge%2520levels%252C%2520within%2520a%2520unified%2520textual%2520space.%2520Building%2520upon%250Athis%2520foundation%252C%2520we%2520further%2520introduce%2520two%2520innovative%2520HTRN-tailored%2520pretraining%250Atasks%2520to%2520fine-tune%2520PLMs%2520for%2520representation%2520learning%2520by%2520emphasizing%2520the%2520inherent%250Aheterogeneity%2520and%2520interactions%2520between%2520textual%2520and%2520structural%2520information%250Awithin%2520HTRNs.%2520Extensive%2520experiments%2520on%2520two%2520real-world%2520HTRN%2520datasets%2520demonstrate%250AHierPromptLM%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520significant%250Aimprovements%2520of%2520up%2520to%25206.08%2525%2520for%2520node%2520classification%2520and%252010.84%2525%2520for%2520link%250Aprediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HierPromptLM%3A%20A%20Pure%20PLM-based%20Framework%20for%20Representation%20Learning%20on%0A%20%20Heterogeneous%20Text-rich%20Networks&entry.906535625=Qiuyu%20Zhu%20and%20Liang%20Zhang%20and%20Qianxiong%20Xu%20and%20Cheng%20Long&entry.1292438233=%20%20Representation%20learning%20on%20heterogeneous%20text-rich%20networks%20%28HTRNs%29%2C%20which%0Aconsist%20of%20multiple%20types%20of%20nodes%20and%20edges%20with%20each%20node%20associated%20with%0Atextual%20information%2C%20is%20essential%20for%20various%20real-world%20applications.%20Given%0Athe%20success%20of%20pretrained%20language%20models%20%28PLMs%29%20in%20processing%20text%20data%2C%0Arecent%20efforts%20have%20focused%20on%20integrating%20PLMs%20into%20HTRN%20representation%0Alearning.%20These%20methods%20typically%20handle%20textual%20and%20structural%20information%0Aseparately%2C%20using%20both%20PLMs%20and%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29.%0AHowever%2C%20this%20separation%20fails%20to%20capture%20the%20critical%20interactions%20between%0Athese%20two%20types%20of%20information%20within%20HTRNs.%20Additionally%2C%20it%20necessitates%20an%0Aextra%20alignment%20step%2C%20which%20is%20challenging%20due%20to%20the%20fundamental%20differences%0Abetween%20distinct%20embedding%20spaces%20generated%20by%20PLMs%20and%20HGNNs.%20To%20deal%20with%20it%2C%0Awe%20propose%20HierPromptLM%2C%20a%20novel%20pure%20PLM-based%20framework%20that%20seamlessly%0Amodels%20both%20text%20data%20and%20graph%20structures%20without%20the%20need%20for%20separate%0Aprocessing.%20Firstly%2C%20we%20develop%20a%20Hierarchical%20Prompt%20module%20that%20employs%0Aprompt%20learning%20to%20integrate%20text%20data%20and%20heterogeneous%20graph%20structures%20at%0Aboth%20the%20node%20and%20edge%20levels%2C%20within%20a%20unified%20textual%20space.%20Building%20upon%0Athis%20foundation%2C%20we%20further%20introduce%20two%20innovative%20HTRN-tailored%20pretraining%0Atasks%20to%20fine-tune%20PLMs%20for%20representation%20learning%20by%20emphasizing%20the%20inherent%0Aheterogeneity%20and%20interactions%20between%20textual%20and%20structural%20information%0Awithin%20HTRNs.%20Extensive%20experiments%20on%20two%20real-world%20HTRN%20datasets%20demonstrate%0AHierPromptLM%20outperforms%20state-of-the-art%20methods%2C%20achieving%20significant%0Aimprovements%20of%20up%20to%206.08%25%20for%20node%20classification%20and%2010.84%25%20for%20link%0Aprediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12857v1&entry.124074799=Read"},
{"title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models", "author": "Eduardo Fernandes Montesuma and Fred Maurice Ngol\u00e8 Mboula and Antoine Souloumiac", "abstract": "  Machine learning systems operate under the assumption that training and test\ndata are sampled from a fixed probability distribution. However, this\nassumptions is rarely verified in practice, as the conditions upon which data\nwas acquired are likely to change. In this context, the adaptation of the\nunsupervised domain requires minimal access to the data of the new conditions\nfor learning models robust to changes in the data distribution. Optimal\ntransport is a theoretically grounded tool for analyzing changes in\ndistribution, especially as it allows the mapping between domains. However,\nthese methods are usually computationally expensive as their complexity scales\ncubically with the number of samples. In this work, we explore optimal\ntransport between Gaussian Mixture Models (GMMs), which is conveniently written\nin terms of the components of source and target GMMs. We experiment with 9\nbenchmarks, with a total of $85$ adaptation tasks, showing that our methods are\nmore efficient than previous shallow domain adaptation methods, and they scale\nwell with number of samples $n$ and dimensions $d$.\n", "link": "http://arxiv.org/abs/2403.13847v2", "date": "2025-01-22", "relevancy": 2.4933, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4907}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Transport%20for%20Domain%20Adaptation%20through%20Gaussian%20Mixture%20Models&body=Title%3A%20Optimal%20Transport%20for%20Domain%20Adaptation%20through%20Gaussian%20Mixture%20Models%0AAuthor%3A%20Eduardo%20Fernandes%20Montesuma%20and%20Fred%20Maurice%20Ngol%C3%A8%20Mboula%20and%20Antoine%20Souloumiac%0AAbstract%3A%20%20%20Machine%20learning%20systems%20operate%20under%20the%20assumption%20that%20training%20and%20test%0Adata%20are%20sampled%20from%20a%20fixed%20probability%20distribution.%20However%2C%20this%0Aassumptions%20is%20rarely%20verified%20in%20practice%2C%20as%20the%20conditions%20upon%20which%20data%0Awas%20acquired%20are%20likely%20to%20change.%20In%20this%20context%2C%20the%20adaptation%20of%20the%0Aunsupervised%20domain%20requires%20minimal%20access%20to%20the%20data%20of%20the%20new%20conditions%0Afor%20learning%20models%20robust%20to%20changes%20in%20the%20data%20distribution.%20Optimal%0Atransport%20is%20a%20theoretically%20grounded%20tool%20for%20analyzing%20changes%20in%0Adistribution%2C%20especially%20as%20it%20allows%20the%20mapping%20between%20domains.%20However%2C%0Athese%20methods%20are%20usually%20computationally%20expensive%20as%20their%20complexity%20scales%0Acubically%20with%20the%20number%20of%20samples.%20In%20this%20work%2C%20we%20explore%20optimal%0Atransport%20between%20Gaussian%20Mixture%20Models%20%28GMMs%29%2C%20which%20is%20conveniently%20written%0Ain%20terms%20of%20the%20components%20of%20source%20and%20target%20GMMs.%20We%20experiment%20with%209%0Abenchmarks%2C%20with%20a%20total%20of%20%2485%24%20adaptation%20tasks%2C%20showing%20that%20our%20methods%20are%0Amore%20efficient%20than%20previous%20shallow%20domain%20adaptation%20methods%2C%20and%20they%20scale%0Awell%20with%20number%20of%20samples%20%24n%24%20and%20dimensions%20%24d%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Transport%2520for%2520Domain%2520Adaptation%2520through%2520Gaussian%2520Mixture%2520Models%26entry.906535625%3DEduardo%2520Fernandes%2520Montesuma%2520and%2520Fred%2520Maurice%2520Ngol%25C3%25A8%2520Mboula%2520and%2520Antoine%2520Souloumiac%26entry.1292438233%3D%2520%2520Machine%2520learning%2520systems%2520operate%2520under%2520the%2520assumption%2520that%2520training%2520and%2520test%250Adata%2520are%2520sampled%2520from%2520a%2520fixed%2520probability%2520distribution.%2520However%252C%2520this%250Aassumptions%2520is%2520rarely%2520verified%2520in%2520practice%252C%2520as%2520the%2520conditions%2520upon%2520which%2520data%250Awas%2520acquired%2520are%2520likely%2520to%2520change.%2520In%2520this%2520context%252C%2520the%2520adaptation%2520of%2520the%250Aunsupervised%2520domain%2520requires%2520minimal%2520access%2520to%2520the%2520data%2520of%2520the%2520new%2520conditions%250Afor%2520learning%2520models%2520robust%2520to%2520changes%2520in%2520the%2520data%2520distribution.%2520Optimal%250Atransport%2520is%2520a%2520theoretically%2520grounded%2520tool%2520for%2520analyzing%2520changes%2520in%250Adistribution%252C%2520especially%2520as%2520it%2520allows%2520the%2520mapping%2520between%2520domains.%2520However%252C%250Athese%2520methods%2520are%2520usually%2520computationally%2520expensive%2520as%2520their%2520complexity%2520scales%250Acubically%2520with%2520the%2520number%2520of%2520samples.%2520In%2520this%2520work%252C%2520we%2520explore%2520optimal%250Atransport%2520between%2520Gaussian%2520Mixture%2520Models%2520%2528GMMs%2529%252C%2520which%2520is%2520conveniently%2520written%250Ain%2520terms%2520of%2520the%2520components%2520of%2520source%2520and%2520target%2520GMMs.%2520We%2520experiment%2520with%25209%250Abenchmarks%252C%2520with%2520a%2520total%2520of%2520%252485%2524%2520adaptation%2520tasks%252C%2520showing%2520that%2520our%2520methods%2520are%250Amore%2520efficient%2520than%2520previous%2520shallow%2520domain%2520adaptation%2520methods%252C%2520and%2520they%2520scale%250Awell%2520with%2520number%2520of%2520samples%2520%2524n%2524%2520and%2520dimensions%2520%2524d%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport%20for%20Domain%20Adaptation%20through%20Gaussian%20Mixture%20Models&entry.906535625=Eduardo%20Fernandes%20Montesuma%20and%20Fred%20Maurice%20Ngol%C3%A8%20Mboula%20and%20Antoine%20Souloumiac&entry.1292438233=%20%20Machine%20learning%20systems%20operate%20under%20the%20assumption%20that%20training%20and%20test%0Adata%20are%20sampled%20from%20a%20fixed%20probability%20distribution.%20However%2C%20this%0Aassumptions%20is%20rarely%20verified%20in%20practice%2C%20as%20the%20conditions%20upon%20which%20data%0Awas%20acquired%20are%20likely%20to%20change.%20In%20this%20context%2C%20the%20adaptation%20of%20the%0Aunsupervised%20domain%20requires%20minimal%20access%20to%20the%20data%20of%20the%20new%20conditions%0Afor%20learning%20models%20robust%20to%20changes%20in%20the%20data%20distribution.%20Optimal%0Atransport%20is%20a%20theoretically%20grounded%20tool%20for%20analyzing%20changes%20in%0Adistribution%2C%20especially%20as%20it%20allows%20the%20mapping%20between%20domains.%20However%2C%0Athese%20methods%20are%20usually%20computationally%20expensive%20as%20their%20complexity%20scales%0Acubically%20with%20the%20number%20of%20samples.%20In%20this%20work%2C%20we%20explore%20optimal%0Atransport%20between%20Gaussian%20Mixture%20Models%20%28GMMs%29%2C%20which%20is%20conveniently%20written%0Ain%20terms%20of%20the%20components%20of%20source%20and%20target%20GMMs.%20We%20experiment%20with%209%0Abenchmarks%2C%20with%20a%20total%20of%20%2485%24%20adaptation%20tasks%2C%20showing%20that%20our%20methods%20are%0Amore%20efficient%20than%20previous%20shallow%20domain%20adaptation%20methods%2C%20and%20they%20scale%0Awell%20with%20number%20of%20samples%20%24n%24%20and%20dimensions%20%24d%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13847v2&entry.124074799=Read"},
{"title": "Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D\n  Assets Generation", "author": "Zibo Zhao and Zeqiang Lai and Qingxiang Lin and Yunfei Zhao and Haolin Liu and Shuhui Yang and Yifei Feng and Mingxin Yang and Sheng Zhang and Xianghui Yang and Huiwen Shi and Sicong Liu and Junta Wu and Yihang Lian and Fan Yang and Ruining Tang and Zebin He and Xinzhou Wang and Jian Liu and Xuhui Zuo and Zhuo Chen and Biwen Lei and Haohan Weng and Jing Xu and Yiling Zhu and Xinhai Liu and Lixin Xu and Changrong Hu and Tianyu Huang and Lifu Wang and Jihong Zhang and Meng Chen and Liang Dong and Yiwen Jia and Yulin Cai and Jiaao Yu and Yixuan Tang and Hao Zhang and Zheng Ye and Peng He and Runzhou Wu and Chao Zhang and Yonghao Tan and Jie Xiao and Yangyu Tao and Jianchen Zhu and Jinbao Xue and Kai Liu and Chongqing Zhao and Xinming Wu and Zhichao Hu and Lei Qin and Jianbing Peng and Zhan Li and Minghui Chen and Xipeng Zhang and Lin Niu and Paige Wang and Yingkai Wang and Haozhao Kuang and Zhongyi Fan and Xu Zheng and Weihao Zhuang and YingPing He and Tian Liu and Yong Yang and Di Wang and Yuhong Liu and Jie Jiang and Jingwei Huang and Chunchao Guo", "abstract": "  We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for\ngenerating high-resolution textured 3D assets. This system includes two\nfoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,\nand a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape\ngenerative model, built on a scalable flow-based diffusion transformer, aims to\ncreate geometry that properly aligns with a given condition image, laying a\nsolid foundation for downstream applications. The texture synthesis model,\nbenefiting from strong geometric and diffusion priors, produces high-resolution\nand vibrant texture maps for either generated or hand-crafted meshes.\nFurthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production\nplatform that simplifies the re-creation process of 3D assets. It allows both\nprofessional and amateur users to manipulate or even animate their meshes\nefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0\noutperforms previous state-of-the-art models, including the open-source models\nand closed-source models in geometry details, condition alignment, texture\nquality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps\nin the open-source 3D community for large-scale foundation generative models.\nThe code and pre-trained weights of our models are available at:\nhttps://github.com/Tencent/Hunyuan3D-2\n", "link": "http://arxiv.org/abs/2501.12202v2", "date": "2025-01-22", "relevancy": 2.4715, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6189}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6189}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunyuan3D%202.0%3A%20Scaling%20Diffusion%20Models%20for%20High%20Resolution%20Textured%203D%0A%20%20Assets%20Generation&body=Title%3A%20Hunyuan3D%202.0%3A%20Scaling%20Diffusion%20Models%20for%20High%20Resolution%20Textured%203D%0A%20%20Assets%20Generation%0AAuthor%3A%20Zibo%20Zhao%20and%20Zeqiang%20Lai%20and%20Qingxiang%20Lin%20and%20Yunfei%20Zhao%20and%20Haolin%20Liu%20and%20Shuhui%20Yang%20and%20Yifei%20Feng%20and%20Mingxin%20Yang%20and%20Sheng%20Zhang%20and%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Sicong%20Liu%20and%20Junta%20Wu%20and%20Yihang%20Lian%20and%20Fan%20Yang%20and%20Ruining%20Tang%20and%20Zebin%20He%20and%20Xinzhou%20Wang%20and%20Jian%20Liu%20and%20Xuhui%20Zuo%20and%20Zhuo%20Chen%20and%20Biwen%20Lei%20and%20Haohan%20Weng%20and%20Jing%20Xu%20and%20Yiling%20Zhu%20and%20Xinhai%20Liu%20and%20Lixin%20Xu%20and%20Changrong%20Hu%20and%20Tianyu%20Huang%20and%20Lifu%20Wang%20and%20Jihong%20Zhang%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Yiwen%20Jia%20and%20Yulin%20Cai%20and%20Jiaao%20Yu%20and%20Yixuan%20Tang%20and%20Hao%20Zhang%20and%20Zheng%20Ye%20and%20Peng%20He%20and%20Runzhou%20Wu%20and%20Chao%20Zhang%20and%20Yonghao%20Tan%20and%20Jie%20Xiao%20and%20Yangyu%20Tao%20and%20Jianchen%20Zhu%20and%20Jinbao%20Xue%20and%20Kai%20Liu%20and%20Chongqing%20Zhao%20and%20Xinming%20Wu%20and%20Zhichao%20Hu%20and%20Lei%20Qin%20and%20Jianbing%20Peng%20and%20Zhan%20Li%20and%20Minghui%20Chen%20and%20Xipeng%20Zhang%20and%20Lin%20Niu%20and%20Paige%20Wang%20and%20Yingkai%20Wang%20and%20Haozhao%20Kuang%20and%20Zhongyi%20Fan%20and%20Xu%20Zheng%20and%20Weihao%20Zhuang%20and%20YingPing%20He%20and%20Tian%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20Jie%20Jiang%20and%20Jingwei%20Huang%20and%20Chunchao%20Guo%0AAbstract%3A%20%20%20We%20present%20Hunyuan3D%202.0%2C%20an%20advanced%20large-scale%203D%20synthesis%20system%20for%0Agenerating%20high-resolution%20textured%203D%20assets.%20This%20system%20includes%20two%0Afoundation%20components%3A%20a%20large-scale%20shape%20generation%20model%20--%20Hunyuan3D-DiT%2C%0Aand%20a%20large-scale%20texture%20synthesis%20model%20--%20Hunyuan3D-Paint.%20The%20shape%0Agenerative%20model%2C%20built%20on%20a%20scalable%20flow-based%20diffusion%20transformer%2C%20aims%20to%0Acreate%20geometry%20that%20properly%20aligns%20with%20a%20given%20condition%20image%2C%20laying%20a%0Asolid%20foundation%20for%20downstream%20applications.%20The%20texture%20synthesis%20model%2C%0Abenefiting%20from%20strong%20geometric%20and%20diffusion%20priors%2C%20produces%20high-resolution%0Aand%20vibrant%20texture%20maps%20for%20either%20generated%20or%20hand-crafted%20meshes.%0AFurthermore%2C%20we%20build%20Hunyuan3D-Studio%20--%20a%20versatile%2C%20user-friendly%20production%0Aplatform%20that%20simplifies%20the%20re-creation%20process%20of%203D%20assets.%20It%20allows%20both%0Aprofessional%20and%20amateur%20users%20to%20manipulate%20or%20even%20animate%20their%20meshes%0Aefficiently.%20We%20systematically%20evaluate%20our%20models%2C%20showing%20that%20Hunyuan3D%202.0%0Aoutperforms%20previous%20state-of-the-art%20models%2C%20including%20the%20open-source%20models%0Aand%20closed-source%20models%20in%20geometry%20details%2C%20condition%20alignment%2C%20texture%0Aquality%2C%20and%20etc.%20Hunyuan3D%202.0%20is%20publicly%20released%20in%20order%20to%20fill%20the%20gaps%0Ain%20the%20open-source%203D%20community%20for%20large-scale%20foundation%20generative%20models.%0AThe%20code%20and%20pre-trained%20weights%20of%20our%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/Tencent/Hunyuan3D-2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuan3D%25202.0%253A%2520Scaling%2520Diffusion%2520Models%2520for%2520High%2520Resolution%2520Textured%25203D%250A%2520%2520Assets%2520Generation%26entry.906535625%3DZibo%2520Zhao%2520and%2520Zeqiang%2520Lai%2520and%2520Qingxiang%2520Lin%2520and%2520Yunfei%2520Zhao%2520and%2520Haolin%2520Liu%2520and%2520Shuhui%2520Yang%2520and%2520Yifei%2520Feng%2520and%2520Mingxin%2520Yang%2520and%2520Sheng%2520Zhang%2520and%2520Xianghui%2520Yang%2520and%2520Huiwen%2520Shi%2520and%2520Sicong%2520Liu%2520and%2520Junta%2520Wu%2520and%2520Yihang%2520Lian%2520and%2520Fan%2520Yang%2520and%2520Ruining%2520Tang%2520and%2520Zebin%2520He%2520and%2520Xinzhou%2520Wang%2520and%2520Jian%2520Liu%2520and%2520Xuhui%2520Zuo%2520and%2520Zhuo%2520Chen%2520and%2520Biwen%2520Lei%2520and%2520Haohan%2520Weng%2520and%2520Jing%2520Xu%2520and%2520Yiling%2520Zhu%2520and%2520Xinhai%2520Liu%2520and%2520Lixin%2520Xu%2520and%2520Changrong%2520Hu%2520and%2520Tianyu%2520Huang%2520and%2520Lifu%2520Wang%2520and%2520Jihong%2520Zhang%2520and%2520Meng%2520Chen%2520and%2520Liang%2520Dong%2520and%2520Yiwen%2520Jia%2520and%2520Yulin%2520Cai%2520and%2520Jiaao%2520Yu%2520and%2520Yixuan%2520Tang%2520and%2520Hao%2520Zhang%2520and%2520Zheng%2520Ye%2520and%2520Peng%2520He%2520and%2520Runzhou%2520Wu%2520and%2520Chao%2520Zhang%2520and%2520Yonghao%2520Tan%2520and%2520Jie%2520Xiao%2520and%2520Yangyu%2520Tao%2520and%2520Jianchen%2520Zhu%2520and%2520Jinbao%2520Xue%2520and%2520Kai%2520Liu%2520and%2520Chongqing%2520Zhao%2520and%2520Xinming%2520Wu%2520and%2520Zhichao%2520Hu%2520and%2520Lei%2520Qin%2520and%2520Jianbing%2520Peng%2520and%2520Zhan%2520Li%2520and%2520Minghui%2520Chen%2520and%2520Xipeng%2520Zhang%2520and%2520Lin%2520Niu%2520and%2520Paige%2520Wang%2520and%2520Yingkai%2520Wang%2520and%2520Haozhao%2520Kuang%2520and%2520Zhongyi%2520Fan%2520and%2520Xu%2520Zheng%2520and%2520Weihao%2520Zhuang%2520and%2520YingPing%2520He%2520and%2520Tian%2520Liu%2520and%2520Yong%2520Yang%2520and%2520Di%2520Wang%2520and%2520Yuhong%2520Liu%2520and%2520Jie%2520Jiang%2520and%2520Jingwei%2520Huang%2520and%2520Chunchao%2520Guo%26entry.1292438233%3D%2520%2520We%2520present%2520Hunyuan3D%25202.0%252C%2520an%2520advanced%2520large-scale%25203D%2520synthesis%2520system%2520for%250Agenerating%2520high-resolution%2520textured%25203D%2520assets.%2520This%2520system%2520includes%2520two%250Afoundation%2520components%253A%2520a%2520large-scale%2520shape%2520generation%2520model%2520--%2520Hunyuan3D-DiT%252C%250Aand%2520a%2520large-scale%2520texture%2520synthesis%2520model%2520--%2520Hunyuan3D-Paint.%2520The%2520shape%250Agenerative%2520model%252C%2520built%2520on%2520a%2520scalable%2520flow-based%2520diffusion%2520transformer%252C%2520aims%2520to%250Acreate%2520geometry%2520that%2520properly%2520aligns%2520with%2520a%2520given%2520condition%2520image%252C%2520laying%2520a%250Asolid%2520foundation%2520for%2520downstream%2520applications.%2520The%2520texture%2520synthesis%2520model%252C%250Abenefiting%2520from%2520strong%2520geometric%2520and%2520diffusion%2520priors%252C%2520produces%2520high-resolution%250Aand%2520vibrant%2520texture%2520maps%2520for%2520either%2520generated%2520or%2520hand-crafted%2520meshes.%250AFurthermore%252C%2520we%2520build%2520Hunyuan3D-Studio%2520--%2520a%2520versatile%252C%2520user-friendly%2520production%250Aplatform%2520that%2520simplifies%2520the%2520re-creation%2520process%2520of%25203D%2520assets.%2520It%2520allows%2520both%250Aprofessional%2520and%2520amateur%2520users%2520to%2520manipulate%2520or%2520even%2520animate%2520their%2520meshes%250Aefficiently.%2520We%2520systematically%2520evaluate%2520our%2520models%252C%2520showing%2520that%2520Hunyuan3D%25202.0%250Aoutperforms%2520previous%2520state-of-the-art%2520models%252C%2520including%2520the%2520open-source%2520models%250Aand%2520closed-source%2520models%2520in%2520geometry%2520details%252C%2520condition%2520alignment%252C%2520texture%250Aquality%252C%2520and%2520etc.%2520Hunyuan3D%25202.0%2520is%2520publicly%2520released%2520in%2520order%2520to%2520fill%2520the%2520gaps%250Ain%2520the%2520open-source%25203D%2520community%2520for%2520large-scale%2520foundation%2520generative%2520models.%250AThe%2520code%2520and%2520pre-trained%2520weights%2520of%2520our%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/Tencent/Hunyuan3D-2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunyuan3D%202.0%3A%20Scaling%20Diffusion%20Models%20for%20High%20Resolution%20Textured%203D%0A%20%20Assets%20Generation&entry.906535625=Zibo%20Zhao%20and%20Zeqiang%20Lai%20and%20Qingxiang%20Lin%20and%20Yunfei%20Zhao%20and%20Haolin%20Liu%20and%20Shuhui%20Yang%20and%20Yifei%20Feng%20and%20Mingxin%20Yang%20and%20Sheng%20Zhang%20and%20Xianghui%20Yang%20and%20Huiwen%20Shi%20and%20Sicong%20Liu%20and%20Junta%20Wu%20and%20Yihang%20Lian%20and%20Fan%20Yang%20and%20Ruining%20Tang%20and%20Zebin%20He%20and%20Xinzhou%20Wang%20and%20Jian%20Liu%20and%20Xuhui%20Zuo%20and%20Zhuo%20Chen%20and%20Biwen%20Lei%20and%20Haohan%20Weng%20and%20Jing%20Xu%20and%20Yiling%20Zhu%20and%20Xinhai%20Liu%20and%20Lixin%20Xu%20and%20Changrong%20Hu%20and%20Tianyu%20Huang%20and%20Lifu%20Wang%20and%20Jihong%20Zhang%20and%20Meng%20Chen%20and%20Liang%20Dong%20and%20Yiwen%20Jia%20and%20Yulin%20Cai%20and%20Jiaao%20Yu%20and%20Yixuan%20Tang%20and%20Hao%20Zhang%20and%20Zheng%20Ye%20and%20Peng%20He%20and%20Runzhou%20Wu%20and%20Chao%20Zhang%20and%20Yonghao%20Tan%20and%20Jie%20Xiao%20and%20Yangyu%20Tao%20and%20Jianchen%20Zhu%20and%20Jinbao%20Xue%20and%20Kai%20Liu%20and%20Chongqing%20Zhao%20and%20Xinming%20Wu%20and%20Zhichao%20Hu%20and%20Lei%20Qin%20and%20Jianbing%20Peng%20and%20Zhan%20Li%20and%20Minghui%20Chen%20and%20Xipeng%20Zhang%20and%20Lin%20Niu%20and%20Paige%20Wang%20and%20Yingkai%20Wang%20and%20Haozhao%20Kuang%20and%20Zhongyi%20Fan%20and%20Xu%20Zheng%20and%20Weihao%20Zhuang%20and%20YingPing%20He%20and%20Tian%20Liu%20and%20Yong%20Yang%20and%20Di%20Wang%20and%20Yuhong%20Liu%20and%20Jie%20Jiang%20and%20Jingwei%20Huang%20and%20Chunchao%20Guo&entry.1292438233=%20%20We%20present%20Hunyuan3D%202.0%2C%20an%20advanced%20large-scale%203D%20synthesis%20system%20for%0Agenerating%20high-resolution%20textured%203D%20assets.%20This%20system%20includes%20two%0Afoundation%20components%3A%20a%20large-scale%20shape%20generation%20model%20--%20Hunyuan3D-DiT%2C%0Aand%20a%20large-scale%20texture%20synthesis%20model%20--%20Hunyuan3D-Paint.%20The%20shape%0Agenerative%20model%2C%20built%20on%20a%20scalable%20flow-based%20diffusion%20transformer%2C%20aims%20to%0Acreate%20geometry%20that%20properly%20aligns%20with%20a%20given%20condition%20image%2C%20laying%20a%0Asolid%20foundation%20for%20downstream%20applications.%20The%20texture%20synthesis%20model%2C%0Abenefiting%20from%20strong%20geometric%20and%20diffusion%20priors%2C%20produces%20high-resolution%0Aand%20vibrant%20texture%20maps%20for%20either%20generated%20or%20hand-crafted%20meshes.%0AFurthermore%2C%20we%20build%20Hunyuan3D-Studio%20--%20a%20versatile%2C%20user-friendly%20production%0Aplatform%20that%20simplifies%20the%20re-creation%20process%20of%203D%20assets.%20It%20allows%20both%0Aprofessional%20and%20amateur%20users%20to%20manipulate%20or%20even%20animate%20their%20meshes%0Aefficiently.%20We%20systematically%20evaluate%20our%20models%2C%20showing%20that%20Hunyuan3D%202.0%0Aoutperforms%20previous%20state-of-the-art%20models%2C%20including%20the%20open-source%20models%0Aand%20closed-source%20models%20in%20geometry%20details%2C%20condition%20alignment%2C%20texture%0Aquality%2C%20and%20etc.%20Hunyuan3D%202.0%20is%20publicly%20released%20in%20order%20to%20fill%20the%20gaps%0Ain%20the%20open-source%203D%20community%20for%20large-scale%20foundation%20generative%20models.%0AThe%20code%20and%20pre-trained%20weights%20of%20our%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/Tencent/Hunyuan3D-2%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12202v2&entry.124074799=Read"},
{"title": "PreciseCam: Precise Camera Control for Text-to-Image Generation", "author": "Edurne Bernal-Berdun and Ana Serrano and Belen Masia and Matheus Gadelha and Yannick Hold-Geoffroy and Xin Sun and Diego Gutierrez", "abstract": "  Images as an artistic medium often rely on specific camera angles and lens\ndistortions to convey ideas or emotions; however, such precise control is\nmissing in current text-to-image models. We propose an efficient and general\nsolution that allows precise control over the camera when generating both\nphotographic and artistic images. Unlike prior methods that rely on predefined\nshots, we rely solely on four simple extrinsic and intrinsic camera parameters,\nremoving the need for pre-existing geometry, reference 3D objects, and\nmulti-view data. We also present a novel dataset with more than 57,000 images,\nalong with their text prompts and ground-truth camera parameters. Our\nevaluation shows precise camera control in text-to-image generation, surpassing\ntraditional prompt engineering approaches. Our data, model, and code are\npublicly available at https://graphics.unizar.es/projects/PreciseCam2024.\n", "link": "http://arxiv.org/abs/2501.12910v1", "date": "2025-01-22", "relevancy": 2.4276, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6859}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5911}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PreciseCam%3A%20Precise%20Camera%20Control%20for%20Text-to-Image%20Generation&body=Title%3A%20PreciseCam%3A%20Precise%20Camera%20Control%20for%20Text-to-Image%20Generation%0AAuthor%3A%20Edurne%20Bernal-Berdun%20and%20Ana%20Serrano%20and%20Belen%20Masia%20and%20Matheus%20Gadelha%20and%20Yannick%20Hold-Geoffroy%20and%20Xin%20Sun%20and%20Diego%20Gutierrez%0AAbstract%3A%20%20%20Images%20as%20an%20artistic%20medium%20often%20rely%20on%20specific%20camera%20angles%20and%20lens%0Adistortions%20to%20convey%20ideas%20or%20emotions%3B%20however%2C%20such%20precise%20control%20is%0Amissing%20in%20current%20text-to-image%20models.%20We%20propose%20an%20efficient%20and%20general%0Asolution%20that%20allows%20precise%20control%20over%20the%20camera%20when%20generating%20both%0Aphotographic%20and%20artistic%20images.%20Unlike%20prior%20methods%20that%20rely%20on%20predefined%0Ashots%2C%20we%20rely%20solely%20on%20four%20simple%20extrinsic%20and%20intrinsic%20camera%20parameters%2C%0Aremoving%20the%20need%20for%20pre-existing%20geometry%2C%20reference%203D%20objects%2C%20and%0Amulti-view%20data.%20We%20also%20present%20a%20novel%20dataset%20with%20more%20than%2057%2C000%20images%2C%0Aalong%20with%20their%20text%20prompts%20and%20ground-truth%20camera%20parameters.%20Our%0Aevaluation%20shows%20precise%20camera%20control%20in%20text-to-image%20generation%2C%20surpassing%0Atraditional%20prompt%20engineering%20approaches.%20Our%20data%2C%20model%2C%20and%20code%20are%0Apublicly%20available%20at%20https%3A//graphics.unizar.es/projects/PreciseCam2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreciseCam%253A%2520Precise%2520Camera%2520Control%2520for%2520Text-to-Image%2520Generation%26entry.906535625%3DEdurne%2520Bernal-Berdun%2520and%2520Ana%2520Serrano%2520and%2520Belen%2520Masia%2520and%2520Matheus%2520Gadelha%2520and%2520Yannick%2520Hold-Geoffroy%2520and%2520Xin%2520Sun%2520and%2520Diego%2520Gutierrez%26entry.1292438233%3D%2520%2520Images%2520as%2520an%2520artistic%2520medium%2520often%2520rely%2520on%2520specific%2520camera%2520angles%2520and%2520lens%250Adistortions%2520to%2520convey%2520ideas%2520or%2520emotions%253B%2520however%252C%2520such%2520precise%2520control%2520is%250Amissing%2520in%2520current%2520text-to-image%2520models.%2520We%2520propose%2520an%2520efficient%2520and%2520general%250Asolution%2520that%2520allows%2520precise%2520control%2520over%2520the%2520camera%2520when%2520generating%2520both%250Aphotographic%2520and%2520artistic%2520images.%2520Unlike%2520prior%2520methods%2520that%2520rely%2520on%2520predefined%250Ashots%252C%2520we%2520rely%2520solely%2520on%2520four%2520simple%2520extrinsic%2520and%2520intrinsic%2520camera%2520parameters%252C%250Aremoving%2520the%2520need%2520for%2520pre-existing%2520geometry%252C%2520reference%25203D%2520objects%252C%2520and%250Amulti-view%2520data.%2520We%2520also%2520present%2520a%2520novel%2520dataset%2520with%2520more%2520than%252057%252C000%2520images%252C%250Aalong%2520with%2520their%2520text%2520prompts%2520and%2520ground-truth%2520camera%2520parameters.%2520Our%250Aevaluation%2520shows%2520precise%2520camera%2520control%2520in%2520text-to-image%2520generation%252C%2520surpassing%250Atraditional%2520prompt%2520engineering%2520approaches.%2520Our%2520data%252C%2520model%252C%2520and%2520code%2520are%250Apublicly%2520available%2520at%2520https%253A//graphics.unizar.es/projects/PreciseCam2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PreciseCam%3A%20Precise%20Camera%20Control%20for%20Text-to-Image%20Generation&entry.906535625=Edurne%20Bernal-Berdun%20and%20Ana%20Serrano%20and%20Belen%20Masia%20and%20Matheus%20Gadelha%20and%20Yannick%20Hold-Geoffroy%20and%20Xin%20Sun%20and%20Diego%20Gutierrez&entry.1292438233=%20%20Images%20as%20an%20artistic%20medium%20often%20rely%20on%20specific%20camera%20angles%20and%20lens%0Adistortions%20to%20convey%20ideas%20or%20emotions%3B%20however%2C%20such%20precise%20control%20is%0Amissing%20in%20current%20text-to-image%20models.%20We%20propose%20an%20efficient%20and%20general%0Asolution%20that%20allows%20precise%20control%20over%20the%20camera%20when%20generating%20both%0Aphotographic%20and%20artistic%20images.%20Unlike%20prior%20methods%20that%20rely%20on%20predefined%0Ashots%2C%20we%20rely%20solely%20on%20four%20simple%20extrinsic%20and%20intrinsic%20camera%20parameters%2C%0Aremoving%20the%20need%20for%20pre-existing%20geometry%2C%20reference%203D%20objects%2C%20and%0Amulti-view%20data.%20We%20also%20present%20a%20novel%20dataset%20with%20more%20than%2057%2C000%20images%2C%0Aalong%20with%20their%20text%20prompts%20and%20ground-truth%20camera%20parameters.%20Our%0Aevaluation%20shows%20precise%20camera%20control%20in%20text-to-image%20generation%2C%20surpassing%0Atraditional%20prompt%20engineering%20approaches.%20Our%20data%2C%20model%2C%20and%20code%20are%0Apublicly%20available%20at%20https%3A//graphics.unizar.es/projects/PreciseCam2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12910v1&entry.124074799=Read"},
{"title": "Predicate Debiasing in Vision-Language Models Integration for Scene\n  Graph Generation Enhancement", "author": "Yuxuan Wang and Xiaoyuan Liu", "abstract": "  Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between\nobjects. This complexity and diversity in SGG leads to underrepresentation,\nwhere parts of triplet labels are rare or even unseen during training,\nresulting in imprecise predictions. To tackle this, we propose integrating the\npretrained Vision-language Models to enhance representation. However, due to\nthe gap between pretraining and SGG, direct inference of pretrained VLMs on SGG\nleads to severe bias, which stems from the imbalanced predicates distribution\nin the pretraining language set. To alleviate the bias, we introduce a novel LM\nEstimation to approximate the unattainable predicates distribution. Finally, we\nensemble the debiased VLMs with SGG models to enhance the representation, where\nwe design a certainty-aware indicator to score each sample and dynamically\nadjust the ensemble weights. Our training-free method effectively addresses the\npredicates bias in pretrained VLMs, enhances SGG's representation, and\nsignificantly improve the performance.\n", "link": "http://arxiv.org/abs/2403.16184v2", "date": "2025-01-22", "relevancy": 2.4196, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.62}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.62}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicate%20Debiasing%20in%20Vision-Language%20Models%20Integration%20for%20Scene%0A%20%20Graph%20Generation%20Enhancement&body=Title%3A%20Predicate%20Debiasing%20in%20Vision-Language%20Models%20Integration%20for%20Scene%0A%20%20Graph%20Generation%20Enhancement%0AAuthor%3A%20Yuxuan%20Wang%20and%20Xiaoyuan%20Liu%0AAbstract%3A%20%20%20Scene%20Graph%20Generation%20%28SGG%29%20provides%20basic%20language%20representation%20of%20visual%0Ascenes%2C%20requiring%20models%20to%20grasp%20complex%20and%20diverse%20semantics%20between%0Aobjects.%20This%20complexity%20and%20diversity%20in%20SGG%20leads%20to%20underrepresentation%2C%0Awhere%20parts%20of%20triplet%20labels%20are%20rare%20or%20even%20unseen%20during%20training%2C%0Aresulting%20in%20imprecise%20predictions.%20To%20tackle%20this%2C%20we%20propose%20integrating%20the%0Apretrained%20Vision-language%20Models%20to%20enhance%20representation.%20However%2C%20due%20to%0Athe%20gap%20between%20pretraining%20and%20SGG%2C%20direct%20inference%20of%20pretrained%20VLMs%20on%20SGG%0Aleads%20to%20severe%20bias%2C%20which%20stems%20from%20the%20imbalanced%20predicates%20distribution%0Ain%20the%20pretraining%20language%20set.%20To%20alleviate%20the%20bias%2C%20we%20introduce%20a%20novel%20LM%0AEstimation%20to%20approximate%20the%20unattainable%20predicates%20distribution.%20Finally%2C%20we%0Aensemble%20the%20debiased%20VLMs%20with%20SGG%20models%20to%20enhance%20the%20representation%2C%20where%0Awe%20design%20a%20certainty-aware%20indicator%20to%20score%20each%20sample%20and%20dynamically%0Aadjust%20the%20ensemble%20weights.%20Our%20training-free%20method%20effectively%20addresses%20the%0Apredicates%20bias%20in%20pretrained%20VLMs%2C%20enhances%20SGG%27s%20representation%2C%20and%0Asignificantly%20improve%20the%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicate%2520Debiasing%2520in%2520Vision-Language%2520Models%2520Integration%2520for%2520Scene%250A%2520%2520Graph%2520Generation%2520Enhancement%26entry.906535625%3DYuxuan%2520Wang%2520and%2520Xiaoyuan%2520Liu%26entry.1292438233%3D%2520%2520Scene%2520Graph%2520Generation%2520%2528SGG%2529%2520provides%2520basic%2520language%2520representation%2520of%2520visual%250Ascenes%252C%2520requiring%2520models%2520to%2520grasp%2520complex%2520and%2520diverse%2520semantics%2520between%250Aobjects.%2520This%2520complexity%2520and%2520diversity%2520in%2520SGG%2520leads%2520to%2520underrepresentation%252C%250Awhere%2520parts%2520of%2520triplet%2520labels%2520are%2520rare%2520or%2520even%2520unseen%2520during%2520training%252C%250Aresulting%2520in%2520imprecise%2520predictions.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520integrating%2520the%250Apretrained%2520Vision-language%2520Models%2520to%2520enhance%2520representation.%2520However%252C%2520due%2520to%250Athe%2520gap%2520between%2520pretraining%2520and%2520SGG%252C%2520direct%2520inference%2520of%2520pretrained%2520VLMs%2520on%2520SGG%250Aleads%2520to%2520severe%2520bias%252C%2520which%2520stems%2520from%2520the%2520imbalanced%2520predicates%2520distribution%250Ain%2520the%2520pretraining%2520language%2520set.%2520To%2520alleviate%2520the%2520bias%252C%2520we%2520introduce%2520a%2520novel%2520LM%250AEstimation%2520to%2520approximate%2520the%2520unattainable%2520predicates%2520distribution.%2520Finally%252C%2520we%250Aensemble%2520the%2520debiased%2520VLMs%2520with%2520SGG%2520models%2520to%2520enhance%2520the%2520representation%252C%2520where%250Awe%2520design%2520a%2520certainty-aware%2520indicator%2520to%2520score%2520each%2520sample%2520and%2520dynamically%250Aadjust%2520the%2520ensemble%2520weights.%2520Our%2520training-free%2520method%2520effectively%2520addresses%2520the%250Apredicates%2520bias%2520in%2520pretrained%2520VLMs%252C%2520enhances%2520SGG%2527s%2520representation%252C%2520and%250Asignificantly%2520improve%2520the%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicate%20Debiasing%20in%20Vision-Language%20Models%20Integration%20for%20Scene%0A%20%20Graph%20Generation%20Enhancement&entry.906535625=Yuxuan%20Wang%20and%20Xiaoyuan%20Liu&entry.1292438233=%20%20Scene%20Graph%20Generation%20%28SGG%29%20provides%20basic%20language%20representation%20of%20visual%0Ascenes%2C%20requiring%20models%20to%20grasp%20complex%20and%20diverse%20semantics%20between%0Aobjects.%20This%20complexity%20and%20diversity%20in%20SGG%20leads%20to%20underrepresentation%2C%0Awhere%20parts%20of%20triplet%20labels%20are%20rare%20or%20even%20unseen%20during%20training%2C%0Aresulting%20in%20imprecise%20predictions.%20To%20tackle%20this%2C%20we%20propose%20integrating%20the%0Apretrained%20Vision-language%20Models%20to%20enhance%20representation.%20However%2C%20due%20to%0Athe%20gap%20between%20pretraining%20and%20SGG%2C%20direct%20inference%20of%20pretrained%20VLMs%20on%20SGG%0Aleads%20to%20severe%20bias%2C%20which%20stems%20from%20the%20imbalanced%20predicates%20distribution%0Ain%20the%20pretraining%20language%20set.%20To%20alleviate%20the%20bias%2C%20we%20introduce%20a%20novel%20LM%0AEstimation%20to%20approximate%20the%20unattainable%20predicates%20distribution.%20Finally%2C%20we%0Aensemble%20the%20debiased%20VLMs%20with%20SGG%20models%20to%20enhance%20the%20representation%2C%20where%0Awe%20design%20a%20certainty-aware%20indicator%20to%20score%20each%20sample%20and%20dynamically%0Aadjust%20the%20ensemble%20weights.%20Our%20training-free%20method%20effectively%20addresses%20the%0Apredicates%20bias%20in%20pretrained%20VLMs%2C%20enhances%20SGG%27s%20representation%2C%20and%0Asignificantly%20improve%20the%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16184v2&entry.124074799=Read"},
{"title": "Learning Graph Node Embeddings by Smooth Pair Sampling", "author": "Konstantin Kutzkov", "abstract": "  Random walk-based node embedding algorithms have attracted a lot of attention\ndue to their scalability and ease of implementation. Previous research has\nfocused on different walk strategies, optimization objectives, and embedding\nlearning models. Inspired by observations on real data, we take a different\napproach and propose a new regularization technique. More precisely, the\nfrequencies of node pairs generated by the skip-gram model on random walk node\nsequences follow a highly skewed distribution which causes learning to be\ndominated by a fraction of the pairs. We address the issue by designing an\nefficient sampling procedure that generates node pairs according to their {\\em\nsmoothed frequency}. Theoretical and experimental results demonstrate the\nadvantages of our approach.\n", "link": "http://arxiv.org/abs/2501.12884v1", "date": "2025-01-22", "relevancy": 2.4146, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5105}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Graph%20Node%20Embeddings%20by%20Smooth%20Pair%20Sampling&body=Title%3A%20Learning%20Graph%20Node%20Embeddings%20by%20Smooth%20Pair%20Sampling%0AAuthor%3A%20Konstantin%20Kutzkov%0AAbstract%3A%20%20%20Random%20walk-based%20node%20embedding%20algorithms%20have%20attracted%20a%20lot%20of%20attention%0Adue%20to%20their%20scalability%20and%20ease%20of%20implementation.%20Previous%20research%20has%0Afocused%20on%20different%20walk%20strategies%2C%20optimization%20objectives%2C%20and%20embedding%0Alearning%20models.%20Inspired%20by%20observations%20on%20real%20data%2C%20we%20take%20a%20different%0Aapproach%20and%20propose%20a%20new%20regularization%20technique.%20More%20precisely%2C%20the%0Afrequencies%20of%20node%20pairs%20generated%20by%20the%20skip-gram%20model%20on%20random%20walk%20node%0Asequences%20follow%20a%20highly%20skewed%20distribution%20which%20causes%20learning%20to%20be%0Adominated%20by%20a%20fraction%20of%20the%20pairs.%20We%20address%20the%20issue%20by%20designing%20an%0Aefficient%20sampling%20procedure%20that%20generates%20node%20pairs%20according%20to%20their%20%7B%5Cem%0Asmoothed%20frequency%7D.%20Theoretical%20and%20experimental%20results%20demonstrate%20the%0Aadvantages%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Graph%2520Node%2520Embeddings%2520by%2520Smooth%2520Pair%2520Sampling%26entry.906535625%3DKonstantin%2520Kutzkov%26entry.1292438233%3D%2520%2520Random%2520walk-based%2520node%2520embedding%2520algorithms%2520have%2520attracted%2520a%2520lot%2520of%2520attention%250Adue%2520to%2520their%2520scalability%2520and%2520ease%2520of%2520implementation.%2520Previous%2520research%2520has%250Afocused%2520on%2520different%2520walk%2520strategies%252C%2520optimization%2520objectives%252C%2520and%2520embedding%250Alearning%2520models.%2520Inspired%2520by%2520observations%2520on%2520real%2520data%252C%2520we%2520take%2520a%2520different%250Aapproach%2520and%2520propose%2520a%2520new%2520regularization%2520technique.%2520More%2520precisely%252C%2520the%250Afrequencies%2520of%2520node%2520pairs%2520generated%2520by%2520the%2520skip-gram%2520model%2520on%2520random%2520walk%2520node%250Asequences%2520follow%2520a%2520highly%2520skewed%2520distribution%2520which%2520causes%2520learning%2520to%2520be%250Adominated%2520by%2520a%2520fraction%2520of%2520the%2520pairs.%2520We%2520address%2520the%2520issue%2520by%2520designing%2520an%250Aefficient%2520sampling%2520procedure%2520that%2520generates%2520node%2520pairs%2520according%2520to%2520their%2520%257B%255Cem%250Asmoothed%2520frequency%257D.%2520Theoretical%2520and%2520experimental%2520results%2520demonstrate%2520the%250Aadvantages%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Graph%20Node%20Embeddings%20by%20Smooth%20Pair%20Sampling&entry.906535625=Konstantin%20Kutzkov&entry.1292438233=%20%20Random%20walk-based%20node%20embedding%20algorithms%20have%20attracted%20a%20lot%20of%20attention%0Adue%20to%20their%20scalability%20and%20ease%20of%20implementation.%20Previous%20research%20has%0Afocused%20on%20different%20walk%20strategies%2C%20optimization%20objectives%2C%20and%20embedding%0Alearning%20models.%20Inspired%20by%20observations%20on%20real%20data%2C%20we%20take%20a%20different%0Aapproach%20and%20propose%20a%20new%20regularization%20technique.%20More%20precisely%2C%20the%0Afrequencies%20of%20node%20pairs%20generated%20by%20the%20skip-gram%20model%20on%20random%20walk%20node%0Asequences%20follow%20a%20highly%20skewed%20distribution%20which%20causes%20learning%20to%20be%0Adominated%20by%20a%20fraction%20of%20the%20pairs.%20We%20address%20the%20issue%20by%20designing%20an%0Aefficient%20sampling%20procedure%20that%20generates%20node%20pairs%20according%20to%20their%20%7B%5Cem%0Asmoothed%20frequency%7D.%20Theoretical%20and%20experimental%20results%20demonstrate%20the%0Aadvantages%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12884v1&entry.124074799=Read"},
{"title": "SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP", "author": "Yusuke Hirota and Min-Hung Chen and Chien-Yi Wang and Yuta Nakashima and Yu-Chiang Frank Wang and Ryo Hachiuma", "abstract": "  Large-scale vision-language models, such as CLIP, are known to contain\nsocietal bias regarding protected attributes (e.g., gender, age). This paper\naims to address the problems of societal bias in CLIP. Although previous\nstudies have proposed to debias societal bias through adversarial learning or\ntest-time projecting, our comprehensive study of these works identifies two\ncritical limitations: 1) loss of attribute information when it is explicitly\ndisclosed in the input and 2) use of the attribute annotations during debiasing\nprocess. To mitigate societal bias in CLIP and overcome these limitations\nsimultaneously, we introduce a simple-yet-effective debiasing method called\nSANER (societal attribute neutralizer) that eliminates attribute information\nfrom CLIP text features only of attribute-neutral descriptions. Experimental\nresults show that SANER, which does not require attribute annotations and\npreserves original information for attribute-specific descriptions,\ndemonstrates superior debiasing ability than the existing methods.\nAdditionally, we observe that SANER does not require retraining CLIP from\nscratch with the original dataset. Moreover, the debiased model can be directly\napplied to the text-to-image generation model by simply replacing the text\nencoder.\n", "link": "http://arxiv.org/abs/2408.10202v3", "date": "2025-01-22", "relevancy": 2.3938, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SANER%3A%20Annotation-free%20Societal%20Attribute%20Neutralizer%20for%20Debiasing%20CLIP&body=Title%3A%20SANER%3A%20Annotation-free%20Societal%20Attribute%20Neutralizer%20for%20Debiasing%20CLIP%0AAuthor%3A%20Yusuke%20Hirota%20and%20Min-Hung%20Chen%20and%20Chien-Yi%20Wang%20and%20Yuta%20Nakashima%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ryo%20Hachiuma%0AAbstract%3A%20%20%20Large-scale%20vision-language%20models%2C%20such%20as%20CLIP%2C%20are%20known%20to%20contain%0Asocietal%20bias%20regarding%20protected%20attributes%20%28e.g.%2C%20gender%2C%20age%29.%20This%20paper%0Aaims%20to%20address%20the%20problems%20of%20societal%20bias%20in%20CLIP.%20Although%20previous%0Astudies%20have%20proposed%20to%20debias%20societal%20bias%20through%20adversarial%20learning%20or%0Atest-time%20projecting%2C%20our%20comprehensive%20study%20of%20these%20works%20identifies%20two%0Acritical%20limitations%3A%201%29%20loss%20of%20attribute%20information%20when%20it%20is%20explicitly%0Adisclosed%20in%20the%20input%20and%202%29%20use%20of%20the%20attribute%20annotations%20during%20debiasing%0Aprocess.%20To%20mitigate%20societal%20bias%20in%20CLIP%20and%20overcome%20these%20limitations%0Asimultaneously%2C%20we%20introduce%20a%20simple-yet-effective%20debiasing%20method%20called%0ASANER%20%28societal%20attribute%20neutralizer%29%20that%20eliminates%20attribute%20information%0Afrom%20CLIP%20text%20features%20only%20of%20attribute-neutral%20descriptions.%20Experimental%0Aresults%20show%20that%20SANER%2C%20which%20does%20not%20require%20attribute%20annotations%20and%0Apreserves%20original%20information%20for%20attribute-specific%20descriptions%2C%0Ademonstrates%20superior%20debiasing%20ability%20than%20the%20existing%20methods.%0AAdditionally%2C%20we%20observe%20that%20SANER%20does%20not%20require%20retraining%20CLIP%20from%0Ascratch%20with%20the%20original%20dataset.%20Moreover%2C%20the%20debiased%20model%20can%20be%20directly%0Aapplied%20to%20the%20text-to-image%20generation%20model%20by%20simply%20replacing%20the%20text%0Aencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10202v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSANER%253A%2520Annotation-free%2520Societal%2520Attribute%2520Neutralizer%2520for%2520Debiasing%2520CLIP%26entry.906535625%3DYusuke%2520Hirota%2520and%2520Min-Hung%2520Chen%2520and%2520Chien-Yi%2520Wang%2520and%2520Yuta%2520Nakashima%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Ryo%2520Hachiuma%26entry.1292438233%3D%2520%2520Large-scale%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520are%2520known%2520to%2520contain%250Asocietal%2520bias%2520regarding%2520protected%2520attributes%2520%2528e.g.%252C%2520gender%252C%2520age%2529.%2520This%2520paper%250Aaims%2520to%2520address%2520the%2520problems%2520of%2520societal%2520bias%2520in%2520CLIP.%2520Although%2520previous%250Astudies%2520have%2520proposed%2520to%2520debias%2520societal%2520bias%2520through%2520adversarial%2520learning%2520or%250Atest-time%2520projecting%252C%2520our%2520comprehensive%2520study%2520of%2520these%2520works%2520identifies%2520two%250Acritical%2520limitations%253A%25201%2529%2520loss%2520of%2520attribute%2520information%2520when%2520it%2520is%2520explicitly%250Adisclosed%2520in%2520the%2520input%2520and%25202%2529%2520use%2520of%2520the%2520attribute%2520annotations%2520during%2520debiasing%250Aprocess.%2520To%2520mitigate%2520societal%2520bias%2520in%2520CLIP%2520and%2520overcome%2520these%2520limitations%250Asimultaneously%252C%2520we%2520introduce%2520a%2520simple-yet-effective%2520debiasing%2520method%2520called%250ASANER%2520%2528societal%2520attribute%2520neutralizer%2529%2520that%2520eliminates%2520attribute%2520information%250Afrom%2520CLIP%2520text%2520features%2520only%2520of%2520attribute-neutral%2520descriptions.%2520Experimental%250Aresults%2520show%2520that%2520SANER%252C%2520which%2520does%2520not%2520require%2520attribute%2520annotations%2520and%250Apreserves%2520original%2520information%2520for%2520attribute-specific%2520descriptions%252C%250Ademonstrates%2520superior%2520debiasing%2520ability%2520than%2520the%2520existing%2520methods.%250AAdditionally%252C%2520we%2520observe%2520that%2520SANER%2520does%2520not%2520require%2520retraining%2520CLIP%2520from%250Ascratch%2520with%2520the%2520original%2520dataset.%2520Moreover%252C%2520the%2520debiased%2520model%2520can%2520be%2520directly%250Aapplied%2520to%2520the%2520text-to-image%2520generation%2520model%2520by%2520simply%2520replacing%2520the%2520text%250Aencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10202v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SANER%3A%20Annotation-free%20Societal%20Attribute%20Neutralizer%20for%20Debiasing%20CLIP&entry.906535625=Yusuke%20Hirota%20and%20Min-Hung%20Chen%20and%20Chien-Yi%20Wang%20and%20Yuta%20Nakashima%20and%20Yu-Chiang%20Frank%20Wang%20and%20Ryo%20Hachiuma&entry.1292438233=%20%20Large-scale%20vision-language%20models%2C%20such%20as%20CLIP%2C%20are%20known%20to%20contain%0Asocietal%20bias%20regarding%20protected%20attributes%20%28e.g.%2C%20gender%2C%20age%29.%20This%20paper%0Aaims%20to%20address%20the%20problems%20of%20societal%20bias%20in%20CLIP.%20Although%20previous%0Astudies%20have%20proposed%20to%20debias%20societal%20bias%20through%20adversarial%20learning%20or%0Atest-time%20projecting%2C%20our%20comprehensive%20study%20of%20these%20works%20identifies%20two%0Acritical%20limitations%3A%201%29%20loss%20of%20attribute%20information%20when%20it%20is%20explicitly%0Adisclosed%20in%20the%20input%20and%202%29%20use%20of%20the%20attribute%20annotations%20during%20debiasing%0Aprocess.%20To%20mitigate%20societal%20bias%20in%20CLIP%20and%20overcome%20these%20limitations%0Asimultaneously%2C%20we%20introduce%20a%20simple-yet-effective%20debiasing%20method%20called%0ASANER%20%28societal%20attribute%20neutralizer%29%20that%20eliminates%20attribute%20information%0Afrom%20CLIP%20text%20features%20only%20of%20attribute-neutral%20descriptions.%20Experimental%0Aresults%20show%20that%20SANER%2C%20which%20does%20not%20require%20attribute%20annotations%20and%0Apreserves%20original%20information%20for%20attribute-specific%20descriptions%2C%0Ademonstrates%20superior%20debiasing%20ability%20than%20the%20existing%20methods.%0AAdditionally%2C%20we%20observe%20that%20SANER%20does%20not%20require%20retraining%20CLIP%20from%0Ascratch%20with%20the%20original%20dataset.%20Moreover%2C%20the%20debiased%20model%20can%20be%20directly%0Aapplied%20to%20the%20text-to-image%20generation%20model%20by%20simply%20replacing%20the%20text%0Aencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10202v3&entry.124074799=Read"},
{"title": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG", "author": "Hasan Md Tusfiqur Alam and Devansh Srivastav and Md Abdul Kadir and Daniel Sonntag", "abstract": "  Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings. Our code is available at\nhttps://github.com/tifat58/IRR-with-CBM-RAG.git.\n", "link": "http://arxiv.org/abs/2412.16086v2", "date": "2025-01-22", "relevancy": 2.3887, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4781}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4781}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretable%20Radiology%20Report%20Generation%20via%20Concept%0A%20%20Bottlenecks%20using%20a%20Multi-Agentic%20RAG&body=Title%3A%20Towards%20Interpretable%20Radiology%20Report%20Generation%20via%20Concept%0A%20%20Bottlenecks%20using%20a%20Multi-Agentic%20RAG%0AAuthor%3A%20Hasan%20Md%20Tusfiqur%20Alam%20and%20Devansh%20Srivastav%20and%20Md%20Abdul%20Kadir%20and%20Daniel%20Sonntag%0AAbstract%3A%20%20%20Deep%20learning%20has%20advanced%20medical%20image%20classification%2C%20but%20interpretability%0Achallenges%20hinder%20its%20clinical%20adoption.%20This%20study%20enhances%20interpretability%0Ain%20Chest%20X-ray%20%28CXR%29%20classification%20by%20using%20concept%20bottleneck%20models%20%28CBMs%29%0Aand%20a%20multi-agent%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20for%20report%0Ageneration.%20By%20modeling%20relationships%20between%20visual%20features%20and%20clinical%0Aconcepts%2C%20we%20create%20interpretable%20concept%20vectors%20that%20guide%20a%20multi-agent%20RAG%0Asystem%20to%20generate%20radiology%20reports%2C%20enhancing%20clinical%20relevance%2C%0Aexplainability%2C%20and%20transparency.%20Evaluation%20of%20the%20generated%20reports%20using%20an%0ALLM-as-a-judge%20confirmed%20the%20interpretability%20and%20clinical%20utility%20of%20our%0Amodel%27s%20outputs.%20On%20the%20COVID-QU%20dataset%2C%20our%20model%20achieved%2081%25%20classification%0Aaccuracy%20and%20demonstrated%20robust%20report%20generation%20performance%2C%20with%20five%20key%0Ametrics%20ranging%20between%2084%25%20and%2090%25.%20This%20interpretable%20multi-agent%20framework%0Abridges%20the%20gap%20between%20high-performance%20AI%20and%20the%20explainability%20required%20for%0Areliable%20AI-driven%20CXR%20analysis%20in%20clinical%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tifat58/IRR-with-CBM-RAG.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretable%2520Radiology%2520Report%2520Generation%2520via%2520Concept%250A%2520%2520Bottlenecks%2520using%2520a%2520Multi-Agentic%2520RAG%26entry.906535625%3DHasan%2520Md%2520Tusfiqur%2520Alam%2520and%2520Devansh%2520Srivastav%2520and%2520Md%2520Abdul%2520Kadir%2520and%2520Daniel%2520Sonntag%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520advanced%2520medical%2520image%2520classification%252C%2520but%2520interpretability%250Achallenges%2520hinder%2520its%2520clinical%2520adoption.%2520This%2520study%2520enhances%2520interpretability%250Ain%2520Chest%2520X-ray%2520%2528CXR%2529%2520classification%2520by%2520using%2520concept%2520bottleneck%2520models%2520%2528CBMs%2529%250Aand%2520a%2520multi-agent%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520system%2520for%2520report%250Ageneration.%2520By%2520modeling%2520relationships%2520between%2520visual%2520features%2520and%2520clinical%250Aconcepts%252C%2520we%2520create%2520interpretable%2520concept%2520vectors%2520that%2520guide%2520a%2520multi-agent%2520RAG%250Asystem%2520to%2520generate%2520radiology%2520reports%252C%2520enhancing%2520clinical%2520relevance%252C%250Aexplainability%252C%2520and%2520transparency.%2520Evaluation%2520of%2520the%2520generated%2520reports%2520using%2520an%250ALLM-as-a-judge%2520confirmed%2520the%2520interpretability%2520and%2520clinical%2520utility%2520of%2520our%250Amodel%2527s%2520outputs.%2520On%2520the%2520COVID-QU%2520dataset%252C%2520our%2520model%2520achieved%252081%2525%2520classification%250Aaccuracy%2520and%2520demonstrated%2520robust%2520report%2520generation%2520performance%252C%2520with%2520five%2520key%250Ametrics%2520ranging%2520between%252084%2525%2520and%252090%2525.%2520This%2520interpretable%2520multi-agent%2520framework%250Abridges%2520the%2520gap%2520between%2520high-performance%2520AI%2520and%2520the%2520explainability%2520required%2520for%250Areliable%2520AI-driven%2520CXR%2520analysis%2520in%2520clinical%2520settings.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/tifat58/IRR-with-CBM-RAG.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretable%20Radiology%20Report%20Generation%20via%20Concept%0A%20%20Bottlenecks%20using%20a%20Multi-Agentic%20RAG&entry.906535625=Hasan%20Md%20Tusfiqur%20Alam%20and%20Devansh%20Srivastav%20and%20Md%20Abdul%20Kadir%20and%20Daniel%20Sonntag&entry.1292438233=%20%20Deep%20learning%20has%20advanced%20medical%20image%20classification%2C%20but%20interpretability%0Achallenges%20hinder%20its%20clinical%20adoption.%20This%20study%20enhances%20interpretability%0Ain%20Chest%20X-ray%20%28CXR%29%20classification%20by%20using%20concept%20bottleneck%20models%20%28CBMs%29%0Aand%20a%20multi-agent%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20for%20report%0Ageneration.%20By%20modeling%20relationships%20between%20visual%20features%20and%20clinical%0Aconcepts%2C%20we%20create%20interpretable%20concept%20vectors%20that%20guide%20a%20multi-agent%20RAG%0Asystem%20to%20generate%20radiology%20reports%2C%20enhancing%20clinical%20relevance%2C%0Aexplainability%2C%20and%20transparency.%20Evaluation%20of%20the%20generated%20reports%20using%20an%0ALLM-as-a-judge%20confirmed%20the%20interpretability%20and%20clinical%20utility%20of%20our%0Amodel%27s%20outputs.%20On%20the%20COVID-QU%20dataset%2C%20our%20model%20achieved%2081%25%20classification%0Aaccuracy%20and%20demonstrated%20robust%20report%20generation%20performance%2C%20with%20five%20key%0Ametrics%20ranging%20between%2084%25%20and%2090%25.%20This%20interpretable%20multi-agent%20framework%0Abridges%20the%20gap%20between%20high-performance%20AI%20and%20the%20explainability%20required%20for%0Areliable%20AI-driven%20CXR%20analysis%20in%20clinical%20settings.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/tifat58/IRR-with-CBM-RAG.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16086v2&entry.124074799=Read"},
{"title": "PSGSL: A Probabilistic Framework Integrating Semantic Scene\n  Understanding and Gas Sensing for Gas Source Localization", "author": "Pepe Ojeda and Javier Monroy and Javier Gonzalez-Jimenez", "abstract": "  Semantic scene understanding allows a robotic agent to reason about problems\nin complex ways, using information from multiple and varied sensors to make\ndeductions about a particular matter. As a result, this form of intelligent\nrobotics is capable of performing more complex tasks and achieving more precise\nresults than simpler approaches based on single data sources. However, these\nimproved capabilities come at the cost of higher complexity, both computational\nand in terms of design. Due to the increased design complexity, formal\napproaches for exploiting semantic understanding become necessary.\n  We present here a probabilistic formulation for integrating semantic\nknowledge into the process of gas source localization (GSL). The problem of GSL\nposes many unsolved challenges, and proposed solutions need to contend with the\nconstraining limitations of sensing hardware. By exploiting semantic scene\nunderstanding, we can leverage other sources of information, such as vision, to\nimprove the estimation of the source location. We show how our formulation can\nbe applied to pre-existing GSL algorithms and the effect that including\nsemantic data has on the produced estimations of the location of the source.\n", "link": "http://arxiv.org/abs/2501.12812v1", "date": "2025-01-22", "relevancy": 2.3582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6423}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5831}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSGSL%3A%20A%20Probabilistic%20Framework%20Integrating%20Semantic%20Scene%0A%20%20Understanding%20and%20Gas%20Sensing%20for%20Gas%20Source%20Localization&body=Title%3A%20PSGSL%3A%20A%20Probabilistic%20Framework%20Integrating%20Semantic%20Scene%0A%20%20Understanding%20and%20Gas%20Sensing%20for%20Gas%20Source%20Localization%0AAuthor%3A%20Pepe%20Ojeda%20and%20Javier%20Monroy%20and%20Javier%20Gonzalez-Jimenez%0AAbstract%3A%20%20%20Semantic%20scene%20understanding%20allows%20a%20robotic%20agent%20to%20reason%20about%20problems%0Ain%20complex%20ways%2C%20using%20information%20from%20multiple%20and%20varied%20sensors%20to%20make%0Adeductions%20about%20a%20particular%20matter.%20As%20a%20result%2C%20this%20form%20of%20intelligent%0Arobotics%20is%20capable%20of%20performing%20more%20complex%20tasks%20and%20achieving%20more%20precise%0Aresults%20than%20simpler%20approaches%20based%20on%20single%20data%20sources.%20However%2C%20these%0Aimproved%20capabilities%20come%20at%20the%20cost%20of%20higher%20complexity%2C%20both%20computational%0Aand%20in%20terms%20of%20design.%20Due%20to%20the%20increased%20design%20complexity%2C%20formal%0Aapproaches%20for%20exploiting%20semantic%20understanding%20become%20necessary.%0A%20%20We%20present%20here%20a%20probabilistic%20formulation%20for%20integrating%20semantic%0Aknowledge%20into%20the%20process%20of%20gas%20source%20localization%20%28GSL%29.%20The%20problem%20of%20GSL%0Aposes%20many%20unsolved%20challenges%2C%20and%20proposed%20solutions%20need%20to%20contend%20with%20the%0Aconstraining%20limitations%20of%20sensing%20hardware.%20By%20exploiting%20semantic%20scene%0Aunderstanding%2C%20we%20can%20leverage%20other%20sources%20of%20information%2C%20such%20as%20vision%2C%20to%0Aimprove%20the%20estimation%20of%20the%20source%20location.%20We%20show%20how%20our%20formulation%20can%0Abe%20applied%20to%20pre-existing%20GSL%20algorithms%20and%20the%20effect%20that%20including%0Asemantic%20data%20has%20on%20the%20produced%20estimations%20of%20the%20location%20of%20the%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12812v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSGSL%253A%2520A%2520Probabilistic%2520Framework%2520Integrating%2520Semantic%2520Scene%250A%2520%2520Understanding%2520and%2520Gas%2520Sensing%2520for%2520Gas%2520Source%2520Localization%26entry.906535625%3DPepe%2520Ojeda%2520and%2520Javier%2520Monroy%2520and%2520Javier%2520Gonzalez-Jimenez%26entry.1292438233%3D%2520%2520Semantic%2520scene%2520understanding%2520allows%2520a%2520robotic%2520agent%2520to%2520reason%2520about%2520problems%250Ain%2520complex%2520ways%252C%2520using%2520information%2520from%2520multiple%2520and%2520varied%2520sensors%2520to%2520make%250Adeductions%2520about%2520a%2520particular%2520matter.%2520As%2520a%2520result%252C%2520this%2520form%2520of%2520intelligent%250Arobotics%2520is%2520capable%2520of%2520performing%2520more%2520complex%2520tasks%2520and%2520achieving%2520more%2520precise%250Aresults%2520than%2520simpler%2520approaches%2520based%2520on%2520single%2520data%2520sources.%2520However%252C%2520these%250Aimproved%2520capabilities%2520come%2520at%2520the%2520cost%2520of%2520higher%2520complexity%252C%2520both%2520computational%250Aand%2520in%2520terms%2520of%2520design.%2520Due%2520to%2520the%2520increased%2520design%2520complexity%252C%2520formal%250Aapproaches%2520for%2520exploiting%2520semantic%2520understanding%2520become%2520necessary.%250A%2520%2520We%2520present%2520here%2520a%2520probabilistic%2520formulation%2520for%2520integrating%2520semantic%250Aknowledge%2520into%2520the%2520process%2520of%2520gas%2520source%2520localization%2520%2528GSL%2529.%2520The%2520problem%2520of%2520GSL%250Aposes%2520many%2520unsolved%2520challenges%252C%2520and%2520proposed%2520solutions%2520need%2520to%2520contend%2520with%2520the%250Aconstraining%2520limitations%2520of%2520sensing%2520hardware.%2520By%2520exploiting%2520semantic%2520scene%250Aunderstanding%252C%2520we%2520can%2520leverage%2520other%2520sources%2520of%2520information%252C%2520such%2520as%2520vision%252C%2520to%250Aimprove%2520the%2520estimation%2520of%2520the%2520source%2520location.%2520We%2520show%2520how%2520our%2520formulation%2520can%250Abe%2520applied%2520to%2520pre-existing%2520GSL%2520algorithms%2520and%2520the%2520effect%2520that%2520including%250Asemantic%2520data%2520has%2520on%2520the%2520produced%2520estimations%2520of%2520the%2520location%2520of%2520the%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12812v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSGSL%3A%20A%20Probabilistic%20Framework%20Integrating%20Semantic%20Scene%0A%20%20Understanding%20and%20Gas%20Sensing%20for%20Gas%20Source%20Localization&entry.906535625=Pepe%20Ojeda%20and%20Javier%20Monroy%20and%20Javier%20Gonzalez-Jimenez&entry.1292438233=%20%20Semantic%20scene%20understanding%20allows%20a%20robotic%20agent%20to%20reason%20about%20problems%0Ain%20complex%20ways%2C%20using%20information%20from%20multiple%20and%20varied%20sensors%20to%20make%0Adeductions%20about%20a%20particular%20matter.%20As%20a%20result%2C%20this%20form%20of%20intelligent%0Arobotics%20is%20capable%20of%20performing%20more%20complex%20tasks%20and%20achieving%20more%20precise%0Aresults%20than%20simpler%20approaches%20based%20on%20single%20data%20sources.%20However%2C%20these%0Aimproved%20capabilities%20come%20at%20the%20cost%20of%20higher%20complexity%2C%20both%20computational%0Aand%20in%20terms%20of%20design.%20Due%20to%20the%20increased%20design%20complexity%2C%20formal%0Aapproaches%20for%20exploiting%20semantic%20understanding%20become%20necessary.%0A%20%20We%20present%20here%20a%20probabilistic%20formulation%20for%20integrating%20semantic%0Aknowledge%20into%20the%20process%20of%20gas%20source%20localization%20%28GSL%29.%20The%20problem%20of%20GSL%0Aposes%20many%20unsolved%20challenges%2C%20and%20proposed%20solutions%20need%20to%20contend%20with%20the%0Aconstraining%20limitations%20of%20sensing%20hardware.%20By%20exploiting%20semantic%20scene%0Aunderstanding%2C%20we%20can%20leverage%20other%20sources%20of%20information%2C%20such%20as%20vision%2C%20to%0Aimprove%20the%20estimation%20of%20the%20source%20location.%20We%20show%20how%20our%20formulation%20can%0Abe%20applied%20to%20pre-existing%20GSL%20algorithms%20and%20the%20effect%20that%20including%0Asemantic%20data%20has%20on%20the%20produced%20estimations%20of%20the%20location%20of%20the%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12812v1&entry.124074799=Read"},
{"title": "Learning to Mask and Permute Visual Tokens for Vision Transformer\n  Pre-Training", "author": "Lorenzo Baraldi and Roberto Amoroso and Marcella Cornia and Lorenzo Baraldi and Andrea Pilzer and Rita Cucchiara", "abstract": "  The use of self-supervised pre-training has emerged as a promising approach\nto enhance the performance of many different visual tasks. In this context,\nrecent approaches have employed the Masked Image Modeling paradigm, which\npre-trains a backbone by reconstructing visual tokens associated with randomly\nmasked image patches. This masking approach, however, introduces noise into the\ninput data during pre-training, leading to discrepancies that can impair\nperformance during the fine-tuning phase. Furthermore, input masking neglects\nthe dependencies between corrupted patches, increasing the inconsistencies\nobserved in downstream fine-tuning tasks. To overcome these issues, we propose\na new self-supervised pre-training approach, named Masked and Permuted Vision\nTransformer (MaPeT), that employs autoregressive and permuted predictions to\ncapture intra-patch dependencies. In addition, MaPeT employs auxiliary\npositional information to reduce the disparity between the pre-training and\nfine-tuning phases. In our experiments, we employ a fair setting to ensure\nreliable and meaningful comparisons and conduct investigations on multiple\nvisual tokenizers, including our proposed $k$-CLIP which directly employs\ndiscretized CLIP features. Our results demonstrate that MaPeT achieves\ncompetitive performance on ImageNet, compared to baselines and competitors\nunder the same model setting. We release an implementation of our code and\nmodels at https://github.com/aimagelab/MaPeT.\n", "link": "http://arxiv.org/abs/2306.07346v2", "date": "2025-01-22", "relevancy": 2.3462, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6249}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5732}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Mask%20and%20Permute%20Visual%20Tokens%20for%20Vision%20Transformer%0A%20%20Pre-Training&body=Title%3A%20Learning%20to%20Mask%20and%20Permute%20Visual%20Tokens%20for%20Vision%20Transformer%0A%20%20Pre-Training%0AAuthor%3A%20Lorenzo%20Baraldi%20and%20Roberto%20Amoroso%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Andrea%20Pilzer%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20The%20use%20of%20self-supervised%20pre-training%20has%20emerged%20as%20a%20promising%20approach%0Ato%20enhance%20the%20performance%20of%20many%20different%20visual%20tasks.%20In%20this%20context%2C%0Arecent%20approaches%20have%20employed%20the%20Masked%20Image%20Modeling%20paradigm%2C%20which%0Apre-trains%20a%20backbone%20by%20reconstructing%20visual%20tokens%20associated%20with%20randomly%0Amasked%20image%20patches.%20This%20masking%20approach%2C%20however%2C%20introduces%20noise%20into%20the%0Ainput%20data%20during%20pre-training%2C%20leading%20to%20discrepancies%20that%20can%20impair%0Aperformance%20during%20the%20fine-tuning%20phase.%20Furthermore%2C%20input%20masking%20neglects%0Athe%20dependencies%20between%20corrupted%20patches%2C%20increasing%20the%20inconsistencies%0Aobserved%20in%20downstream%20fine-tuning%20tasks.%20To%20overcome%20these%20issues%2C%20we%20propose%0Aa%20new%20self-supervised%20pre-training%20approach%2C%20named%20Masked%20and%20Permuted%20Vision%0ATransformer%20%28MaPeT%29%2C%20that%20employs%20autoregressive%20and%20permuted%20predictions%20to%0Acapture%20intra-patch%20dependencies.%20In%20addition%2C%20MaPeT%20employs%20auxiliary%0Apositional%20information%20to%20reduce%20the%20disparity%20between%20the%20pre-training%20and%0Afine-tuning%20phases.%20In%20our%20experiments%2C%20we%20employ%20a%20fair%20setting%20to%20ensure%0Areliable%20and%20meaningful%20comparisons%20and%20conduct%20investigations%20on%20multiple%0Avisual%20tokenizers%2C%20including%20our%20proposed%20%24k%24-CLIP%20which%20directly%20employs%0Adiscretized%20CLIP%20features.%20Our%20results%20demonstrate%20that%20MaPeT%20achieves%0Acompetitive%20performance%20on%20ImageNet%2C%20compared%20to%20baselines%20and%20competitors%0Aunder%20the%20same%20model%20setting.%20We%20release%20an%20implementation%20of%20our%20code%20and%0Amodels%20at%20https%3A//github.com/aimagelab/MaPeT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07346v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Mask%2520and%2520Permute%2520Visual%2520Tokens%2520for%2520Vision%2520Transformer%250A%2520%2520Pre-Training%26entry.906535625%3DLorenzo%2520Baraldi%2520and%2520Roberto%2520Amoroso%2520and%2520Marcella%2520Cornia%2520and%2520Lorenzo%2520Baraldi%2520and%2520Andrea%2520Pilzer%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520self-supervised%2520pre-training%2520has%2520emerged%2520as%2520a%2520promising%2520approach%250Ato%2520enhance%2520the%2520performance%2520of%2520many%2520different%2520visual%2520tasks.%2520In%2520this%2520context%252C%250Arecent%2520approaches%2520have%2520employed%2520the%2520Masked%2520Image%2520Modeling%2520paradigm%252C%2520which%250Apre-trains%2520a%2520backbone%2520by%2520reconstructing%2520visual%2520tokens%2520associated%2520with%2520randomly%250Amasked%2520image%2520patches.%2520This%2520masking%2520approach%252C%2520however%252C%2520introduces%2520noise%2520into%2520the%250Ainput%2520data%2520during%2520pre-training%252C%2520leading%2520to%2520discrepancies%2520that%2520can%2520impair%250Aperformance%2520during%2520the%2520fine-tuning%2520phase.%2520Furthermore%252C%2520input%2520masking%2520neglects%250Athe%2520dependencies%2520between%2520corrupted%2520patches%252C%2520increasing%2520the%2520inconsistencies%250Aobserved%2520in%2520downstream%2520fine-tuning%2520tasks.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%250Aa%2520new%2520self-supervised%2520pre-training%2520approach%252C%2520named%2520Masked%2520and%2520Permuted%2520Vision%250ATransformer%2520%2528MaPeT%2529%252C%2520that%2520employs%2520autoregressive%2520and%2520permuted%2520predictions%2520to%250Acapture%2520intra-patch%2520dependencies.%2520In%2520addition%252C%2520MaPeT%2520employs%2520auxiliary%250Apositional%2520information%2520to%2520reduce%2520the%2520disparity%2520between%2520the%2520pre-training%2520and%250Afine-tuning%2520phases.%2520In%2520our%2520experiments%252C%2520we%2520employ%2520a%2520fair%2520setting%2520to%2520ensure%250Areliable%2520and%2520meaningful%2520comparisons%2520and%2520conduct%2520investigations%2520on%2520multiple%250Avisual%2520tokenizers%252C%2520including%2520our%2520proposed%2520%2524k%2524-CLIP%2520which%2520directly%2520employs%250Adiscretized%2520CLIP%2520features.%2520Our%2520results%2520demonstrate%2520that%2520MaPeT%2520achieves%250Acompetitive%2520performance%2520on%2520ImageNet%252C%2520compared%2520to%2520baselines%2520and%2520competitors%250Aunder%2520the%2520same%2520model%2520setting.%2520We%2520release%2520an%2520implementation%2520of%2520our%2520code%2520and%250Amodels%2520at%2520https%253A//github.com/aimagelab/MaPeT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07346v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Mask%20and%20Permute%20Visual%20Tokens%20for%20Vision%20Transformer%0A%20%20Pre-Training&entry.906535625=Lorenzo%20Baraldi%20and%20Roberto%20Amoroso%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Andrea%20Pilzer%20and%20Rita%20Cucchiara&entry.1292438233=%20%20The%20use%20of%20self-supervised%20pre-training%20has%20emerged%20as%20a%20promising%20approach%0Ato%20enhance%20the%20performance%20of%20many%20different%20visual%20tasks.%20In%20this%20context%2C%0Arecent%20approaches%20have%20employed%20the%20Masked%20Image%20Modeling%20paradigm%2C%20which%0Apre-trains%20a%20backbone%20by%20reconstructing%20visual%20tokens%20associated%20with%20randomly%0Amasked%20image%20patches.%20This%20masking%20approach%2C%20however%2C%20introduces%20noise%20into%20the%0Ainput%20data%20during%20pre-training%2C%20leading%20to%20discrepancies%20that%20can%20impair%0Aperformance%20during%20the%20fine-tuning%20phase.%20Furthermore%2C%20input%20masking%20neglects%0Athe%20dependencies%20between%20corrupted%20patches%2C%20increasing%20the%20inconsistencies%0Aobserved%20in%20downstream%20fine-tuning%20tasks.%20To%20overcome%20these%20issues%2C%20we%20propose%0Aa%20new%20self-supervised%20pre-training%20approach%2C%20named%20Masked%20and%20Permuted%20Vision%0ATransformer%20%28MaPeT%29%2C%20that%20employs%20autoregressive%20and%20permuted%20predictions%20to%0Acapture%20intra-patch%20dependencies.%20In%20addition%2C%20MaPeT%20employs%20auxiliary%0Apositional%20information%20to%20reduce%20the%20disparity%20between%20the%20pre-training%20and%0Afine-tuning%20phases.%20In%20our%20experiments%2C%20we%20employ%20a%20fair%20setting%20to%20ensure%0Areliable%20and%20meaningful%20comparisons%20and%20conduct%20investigations%20on%20multiple%0Avisual%20tokenizers%2C%20including%20our%20proposed%20%24k%24-CLIP%20which%20directly%20employs%0Adiscretized%20CLIP%20features.%20Our%20results%20demonstrate%20that%20MaPeT%20achieves%0Acompetitive%20performance%20on%20ImageNet%2C%20compared%20to%20baselines%20and%20competitors%0Aunder%20the%20same%20model%20setting.%20We%20release%20an%20implementation%20of%20our%20code%20and%0Amodels%20at%20https%3A//github.com/aimagelab/MaPeT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07346v2&entry.124074799=Read"},
{"title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek", "author": "John Pavlopoulos and Juli Bakagianni and Kanella Pouli and Maria Gavriilidou", "abstract": "  Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact.\n", "link": "http://arxiv.org/abs/2501.12826v1", "date": "2025-01-22", "relevancy": 2.3159, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open%20or%20Closed%20LLM%20for%20Lesser-Resourced%20Languages%3F%20Lessons%20from%20Greek&body=Title%3A%20Open%20or%20Closed%20LLM%20for%20Lesser-Resourced%20Languages%3F%20Lessons%20from%20Greek%0AAuthor%3A%20John%20Pavlopoulos%20and%20Juli%20Bakagianni%20and%20Kanella%20Pouli%20and%20Maria%20Gavriilidou%0AAbstract%3A%20%20%20Natural%20Language%20Processing%20%28NLP%29%20for%20lesser-resourced%20languages%20faces%0Apersistent%20challenges%2C%20including%20limited%20datasets%2C%20inherited%20biases%20from%0Ahigh-resource%20languages%2C%20and%20the%20need%20for%20domain-specific%20solutions.%20This%20study%0Aaddresses%20these%20gaps%20for%20Modern%20Greek%20through%20three%20key%20contributions.%20First%2C%0Awe%20evaluate%20the%20performance%20of%20open-source%20%28Llama-70b%29%20and%20closed-source%0A%28GPT-4o%20mini%29%20large%20language%20models%20%28LLMs%29%20on%20seven%20core%20NLP%20tasks%20with%20dataset%0Aavailability%2C%20revealing%20task-specific%20strengths%2C%20weaknesses%2C%20and%20parity%20in%0Atheir%20performance.%20Second%2C%20we%20expand%20the%20scope%20of%20Greek%20NLP%20by%20reframing%0AAuthorship%20Attribution%20as%20a%20tool%20to%20assess%20potential%20data%20usage%20by%20LLMs%20in%0Apre-training%2C%20with%20high%200-shot%20accuracy%20suggesting%20ethical%20implications%20for%0Adata%20provenance.%20Third%2C%20we%20showcase%20a%20legal%20NLP%20case%20study%2C%20where%20a%20Summarize%2C%0ATranslate%2C%20and%20Embed%20%28STE%29%20methodology%20outperforms%20the%20traditional%20TF-IDF%0Aapproach%20for%20clustering%20%5Cemph%7Blong%7D%20legal%20texts.%20Together%2C%20these%20contributions%0Aprovide%20a%20roadmap%20to%20advance%20NLP%20in%20lesser-resourced%20languages%2C%20bridging%20gaps%0Ain%20model%20evaluation%2C%20task%20innovation%2C%20and%20real-world%20impact.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen%2520or%2520Closed%2520LLM%2520for%2520Lesser-Resourced%2520Languages%253F%2520Lessons%2520from%2520Greek%26entry.906535625%3DJohn%2520Pavlopoulos%2520and%2520Juli%2520Bakagianni%2520and%2520Kanella%2520Pouli%2520and%2520Maria%2520Gavriilidou%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520for%2520lesser-resourced%2520languages%2520faces%250Apersistent%2520challenges%252C%2520including%2520limited%2520datasets%252C%2520inherited%2520biases%2520from%250Ahigh-resource%2520languages%252C%2520and%2520the%2520need%2520for%2520domain-specific%2520solutions.%2520This%2520study%250Aaddresses%2520these%2520gaps%2520for%2520Modern%2520Greek%2520through%2520three%2520key%2520contributions.%2520First%252C%250Awe%2520evaluate%2520the%2520performance%2520of%2520open-source%2520%2528Llama-70b%2529%2520and%2520closed-source%250A%2528GPT-4o%2520mini%2529%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520seven%2520core%2520NLP%2520tasks%2520with%2520dataset%250Aavailability%252C%2520revealing%2520task-specific%2520strengths%252C%2520weaknesses%252C%2520and%2520parity%2520in%250Atheir%2520performance.%2520Second%252C%2520we%2520expand%2520the%2520scope%2520of%2520Greek%2520NLP%2520by%2520reframing%250AAuthorship%2520Attribution%2520as%2520a%2520tool%2520to%2520assess%2520potential%2520data%2520usage%2520by%2520LLMs%2520in%250Apre-training%252C%2520with%2520high%25200-shot%2520accuracy%2520suggesting%2520ethical%2520implications%2520for%250Adata%2520provenance.%2520Third%252C%2520we%2520showcase%2520a%2520legal%2520NLP%2520case%2520study%252C%2520where%2520a%2520Summarize%252C%250ATranslate%252C%2520and%2520Embed%2520%2528STE%2529%2520methodology%2520outperforms%2520the%2520traditional%2520TF-IDF%250Aapproach%2520for%2520clustering%2520%255Cemph%257Blong%257D%2520legal%2520texts.%2520Together%252C%2520these%2520contributions%250Aprovide%2520a%2520roadmap%2520to%2520advance%2520NLP%2520in%2520lesser-resourced%2520languages%252C%2520bridging%2520gaps%250Ain%2520model%2520evaluation%252C%2520task%2520innovation%252C%2520and%2520real-world%2520impact.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open%20or%20Closed%20LLM%20for%20Lesser-Resourced%20Languages%3F%20Lessons%20from%20Greek&entry.906535625=John%20Pavlopoulos%20and%20Juli%20Bakagianni%20and%20Kanella%20Pouli%20and%20Maria%20Gavriilidou&entry.1292438233=%20%20Natural%20Language%20Processing%20%28NLP%29%20for%20lesser-resourced%20languages%20faces%0Apersistent%20challenges%2C%20including%20limited%20datasets%2C%20inherited%20biases%20from%0Ahigh-resource%20languages%2C%20and%20the%20need%20for%20domain-specific%20solutions.%20This%20study%0Aaddresses%20these%20gaps%20for%20Modern%20Greek%20through%20three%20key%20contributions.%20First%2C%0Awe%20evaluate%20the%20performance%20of%20open-source%20%28Llama-70b%29%20and%20closed-source%0A%28GPT-4o%20mini%29%20large%20language%20models%20%28LLMs%29%20on%20seven%20core%20NLP%20tasks%20with%20dataset%0Aavailability%2C%20revealing%20task-specific%20strengths%2C%20weaknesses%2C%20and%20parity%20in%0Atheir%20performance.%20Second%2C%20we%20expand%20the%20scope%20of%20Greek%20NLP%20by%20reframing%0AAuthorship%20Attribution%20as%20a%20tool%20to%20assess%20potential%20data%20usage%20by%20LLMs%20in%0Apre-training%2C%20with%20high%200-shot%20accuracy%20suggesting%20ethical%20implications%20for%0Adata%20provenance.%20Third%2C%20we%20showcase%20a%20legal%20NLP%20case%20study%2C%20where%20a%20Summarize%2C%0ATranslate%2C%20and%20Embed%20%28STE%29%20methodology%20outperforms%20the%20traditional%20TF-IDF%0Aapproach%20for%20clustering%20%5Cemph%7Blong%7D%20legal%20texts.%20Together%2C%20these%20contributions%0Aprovide%20a%20roadmap%20to%20advance%20NLP%20in%20lesser-resourced%20languages%2C%20bridging%20gaps%0Ain%20model%20evaluation%2C%20task%20innovation%2C%20and%20real-world%20impact.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12826v1&entry.124074799=Read"},
{"title": "Developer Perspectives on Licensing and Copyright Issues Arising from\n  Generative AI for Coding", "author": "Trevor Stalnaker and Nathan Wintersgill and Oscar Chaparro and Laura A. Heymann and Massimiliano Di Penta and Daniel M German and Denys Poshyvanyk", "abstract": "  Generative AI (GenAI) tools have already started to transform software\ndevelopment practices. Despite their utility in tasks such as writing code, the\nuse of these tools raises important legal questions and potential risks,\nparticularly those associated with copyright law. In the midst of this\nuncertainty, this paper presents a study jointly conducted by software\nengineering and legal researchers that surveyed 574 GitHub developers who use\nGenAI tools for development activities. The survey and follow-up interviews\nprobed the developers' opinions on emerging legal issues as well as their\nperception of copyrightability, ownership of generated code, and related\nconsiderations. We also investigate potential developer misconceptions, the\nimpact of GenAI on developers' work, and developers' awareness of\nlicensing/copyright risks. Qualitative and quantitative analysis showed that\ndevelopers' opinions on copyright issues vary broadly and that many developers\nare aware of the nuances these legal questions involve. We provide: (1) a\nsurvey of 574 developers on the licensing and copyright aspects of GenAI for\ncoding, (2) a snapshot of practitioners' views at a time when GenAI and\nperceptions of it are rapidly evolving, and (3) an analysis of developers'\nviews, yielding insights and recommendations that can inform future regulatory\ndecisions in this evolving field.\n", "link": "http://arxiv.org/abs/2411.10877v2", "date": "2025-01-22", "relevancy": 2.2987, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4685}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4652}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Developer%20Perspectives%20on%20Licensing%20and%20Copyright%20Issues%20Arising%20from%0A%20%20Generative%20AI%20for%20Coding&body=Title%3A%20Developer%20Perspectives%20on%20Licensing%20and%20Copyright%20Issues%20Arising%20from%0A%20%20Generative%20AI%20for%20Coding%0AAuthor%3A%20Trevor%20Stalnaker%20and%20Nathan%20Wintersgill%20and%20Oscar%20Chaparro%20and%20Laura%20A.%20Heymann%20and%20Massimiliano%20Di%20Penta%20and%20Daniel%20M%20German%20and%20Denys%20Poshyvanyk%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20tools%20have%20already%20started%20to%20transform%20software%0Adevelopment%20practices.%20Despite%20their%20utility%20in%20tasks%20such%20as%20writing%20code%2C%20the%0Ause%20of%20these%20tools%20raises%20important%20legal%20questions%20and%20potential%20risks%2C%0Aparticularly%20those%20associated%20with%20copyright%20law.%20In%20the%20midst%20of%20this%0Auncertainty%2C%20this%20paper%20presents%20a%20study%20jointly%20conducted%20by%20software%0Aengineering%20and%20legal%20researchers%20that%20surveyed%20574%20GitHub%20developers%20who%20use%0AGenAI%20tools%20for%20development%20activities.%20The%20survey%20and%20follow-up%20interviews%0Aprobed%20the%20developers%27%20opinions%20on%20emerging%20legal%20issues%20as%20well%20as%20their%0Aperception%20of%20copyrightability%2C%20ownership%20of%20generated%20code%2C%20and%20related%0Aconsiderations.%20We%20also%20investigate%20potential%20developer%20misconceptions%2C%20the%0Aimpact%20of%20GenAI%20on%20developers%27%20work%2C%20and%20developers%27%20awareness%20of%0Alicensing/copyright%20risks.%20Qualitative%20and%20quantitative%20analysis%20showed%20that%0Adevelopers%27%20opinions%20on%20copyright%20issues%20vary%20broadly%20and%20that%20many%20developers%0Aare%20aware%20of%20the%20nuances%20these%20legal%20questions%20involve.%20We%20provide%3A%20%281%29%20a%0Asurvey%20of%20574%20developers%20on%20the%20licensing%20and%20copyright%20aspects%20of%20GenAI%20for%0Acoding%2C%20%282%29%20a%20snapshot%20of%20practitioners%27%20views%20at%20a%20time%20when%20GenAI%20and%0Aperceptions%20of%20it%20are%20rapidly%20evolving%2C%20and%20%283%29%20an%20analysis%20of%20developers%27%0Aviews%2C%20yielding%20insights%20and%20recommendations%20that%20can%20inform%20future%20regulatory%0Adecisions%20in%20this%20evolving%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10877v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeveloper%2520Perspectives%2520on%2520Licensing%2520and%2520Copyright%2520Issues%2520Arising%2520from%250A%2520%2520Generative%2520AI%2520for%2520Coding%26entry.906535625%3DTrevor%2520Stalnaker%2520and%2520Nathan%2520Wintersgill%2520and%2520Oscar%2520Chaparro%2520and%2520Laura%2520A.%2520Heymann%2520and%2520Massimiliano%2520Di%2520Penta%2520and%2520Daniel%2520M%2520German%2520and%2520Denys%2520Poshyvanyk%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520tools%2520have%2520already%2520started%2520to%2520transform%2520software%250Adevelopment%2520practices.%2520Despite%2520their%2520utility%2520in%2520tasks%2520such%2520as%2520writing%2520code%252C%2520the%250Ause%2520of%2520these%2520tools%2520raises%2520important%2520legal%2520questions%2520and%2520potential%2520risks%252C%250Aparticularly%2520those%2520associated%2520with%2520copyright%2520law.%2520In%2520the%2520midst%2520of%2520this%250Auncertainty%252C%2520this%2520paper%2520presents%2520a%2520study%2520jointly%2520conducted%2520by%2520software%250Aengineering%2520and%2520legal%2520researchers%2520that%2520surveyed%2520574%2520GitHub%2520developers%2520who%2520use%250AGenAI%2520tools%2520for%2520development%2520activities.%2520The%2520survey%2520and%2520follow-up%2520interviews%250Aprobed%2520the%2520developers%2527%2520opinions%2520on%2520emerging%2520legal%2520issues%2520as%2520well%2520as%2520their%250Aperception%2520of%2520copyrightability%252C%2520ownership%2520of%2520generated%2520code%252C%2520and%2520related%250Aconsiderations.%2520We%2520also%2520investigate%2520potential%2520developer%2520misconceptions%252C%2520the%250Aimpact%2520of%2520GenAI%2520on%2520developers%2527%2520work%252C%2520and%2520developers%2527%2520awareness%2520of%250Alicensing/copyright%2520risks.%2520Qualitative%2520and%2520quantitative%2520analysis%2520showed%2520that%250Adevelopers%2527%2520opinions%2520on%2520copyright%2520issues%2520vary%2520broadly%2520and%2520that%2520many%2520developers%250Aare%2520aware%2520of%2520the%2520nuances%2520these%2520legal%2520questions%2520involve.%2520We%2520provide%253A%2520%25281%2529%2520a%250Asurvey%2520of%2520574%2520developers%2520on%2520the%2520licensing%2520and%2520copyright%2520aspects%2520of%2520GenAI%2520for%250Acoding%252C%2520%25282%2529%2520a%2520snapshot%2520of%2520practitioners%2527%2520views%2520at%2520a%2520time%2520when%2520GenAI%2520and%250Aperceptions%2520of%2520it%2520are%2520rapidly%2520evolving%252C%2520and%2520%25283%2529%2520an%2520analysis%2520of%2520developers%2527%250Aviews%252C%2520yielding%2520insights%2520and%2520recommendations%2520that%2520can%2520inform%2520future%2520regulatory%250Adecisions%2520in%2520this%2520evolving%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10877v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Developer%20Perspectives%20on%20Licensing%20and%20Copyright%20Issues%20Arising%20from%0A%20%20Generative%20AI%20for%20Coding&entry.906535625=Trevor%20Stalnaker%20and%20Nathan%20Wintersgill%20and%20Oscar%20Chaparro%20and%20Laura%20A.%20Heymann%20and%20Massimiliano%20Di%20Penta%20and%20Daniel%20M%20German%20and%20Denys%20Poshyvanyk&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20tools%20have%20already%20started%20to%20transform%20software%0Adevelopment%20practices.%20Despite%20their%20utility%20in%20tasks%20such%20as%20writing%20code%2C%20the%0Ause%20of%20these%20tools%20raises%20important%20legal%20questions%20and%20potential%20risks%2C%0Aparticularly%20those%20associated%20with%20copyright%20law.%20In%20the%20midst%20of%20this%0Auncertainty%2C%20this%20paper%20presents%20a%20study%20jointly%20conducted%20by%20software%0Aengineering%20and%20legal%20researchers%20that%20surveyed%20574%20GitHub%20developers%20who%20use%0AGenAI%20tools%20for%20development%20activities.%20The%20survey%20and%20follow-up%20interviews%0Aprobed%20the%20developers%27%20opinions%20on%20emerging%20legal%20issues%20as%20well%20as%20their%0Aperception%20of%20copyrightability%2C%20ownership%20of%20generated%20code%2C%20and%20related%0Aconsiderations.%20We%20also%20investigate%20potential%20developer%20misconceptions%2C%20the%0Aimpact%20of%20GenAI%20on%20developers%27%20work%2C%20and%20developers%27%20awareness%20of%0Alicensing/copyright%20risks.%20Qualitative%20and%20quantitative%20analysis%20showed%20that%0Adevelopers%27%20opinions%20on%20copyright%20issues%20vary%20broadly%20and%20that%20many%20developers%0Aare%20aware%20of%20the%20nuances%20these%20legal%20questions%20involve.%20We%20provide%3A%20%281%29%20a%0Asurvey%20of%20574%20developers%20on%20the%20licensing%20and%20copyright%20aspects%20of%20GenAI%20for%0Acoding%2C%20%282%29%20a%20snapshot%20of%20practitioners%27%20views%20at%20a%20time%20when%20GenAI%20and%0Aperceptions%20of%20it%20are%20rapidly%20evolving%2C%20and%20%283%29%20an%20analysis%20of%20developers%27%0Aviews%2C%20yielding%20insights%20and%20recommendations%20that%20can%20inform%20future%20regulatory%0Adecisions%20in%20this%20evolving%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10877v2&entry.124074799=Read"},
{"title": "FDG-Diff: Frequency-Domain-Guided Diffusion Framework for Compressed\n  Hazy Image Restoration", "author": "Ruicheng Zhang and Kanghui Tian and Zeyu Zhang and Qixiang Liu and Zhi Jin", "abstract": "  In this study, we reveal that the interaction between haze degradation and\nJPEG compression introduces complex joint loss effects, which significantly\ncomplicate image restoration. Existing dehazing models often neglect\ncompression effects, which limits their effectiveness in practical\napplications. To address these challenges, we introduce three key\ncontributions. First, we design FDG-Diff, a novel frequency-domain-guided\ndehazing framework that improves JPEG image restoration by leveraging\nfrequency-domain information. Second, we introduce the High-Frequency\nCompensation Module (HFCM), which enhances spatial-domain detail restoration by\nincorporating frequency-domain augmentation techniques into a diffusion-based\nrestoration framework. Lastly, the introduction of the Degradation-Aware\nDenoising Timestep Predictor (DADTP) module further enhances restoration\nquality by enabling adaptive region-specific restoration, effectively\naddressing regional degradation inconsistencies in compressed hazy images.\nExperimental results across multiple compressed dehazing datasets demonstrate\nthat our method consistently outperforms the latest state-of-the-art\napproaches. Code be available at https://github.com/SYSUzrc/FDG-Diff.\n", "link": "http://arxiv.org/abs/2501.12832v1", "date": "2025-01-22", "relevancy": 2.2613, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5931}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5833}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FDG-Diff%3A%20Frequency-Domain-Guided%20Diffusion%20Framework%20for%20Compressed%0A%20%20Hazy%20Image%20Restoration&body=Title%3A%20FDG-Diff%3A%20Frequency-Domain-Guided%20Diffusion%20Framework%20for%20Compressed%0A%20%20Hazy%20Image%20Restoration%0AAuthor%3A%20Ruicheng%20Zhang%20and%20Kanghui%20Tian%20and%20Zeyu%20Zhang%20and%20Qixiang%20Liu%20and%20Zhi%20Jin%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20reveal%20that%20the%20interaction%20between%20haze%20degradation%20and%0AJPEG%20compression%20introduces%20complex%20joint%20loss%20effects%2C%20which%20significantly%0Acomplicate%20image%20restoration.%20Existing%20dehazing%20models%20often%20neglect%0Acompression%20effects%2C%20which%20limits%20their%20effectiveness%20in%20practical%0Aapplications.%20To%20address%20these%20challenges%2C%20we%20introduce%20three%20key%0Acontributions.%20First%2C%20we%20design%20FDG-Diff%2C%20a%20novel%20frequency-domain-guided%0Adehazing%20framework%20that%20improves%20JPEG%20image%20restoration%20by%20leveraging%0Afrequency-domain%20information.%20Second%2C%20we%20introduce%20the%20High-Frequency%0ACompensation%20Module%20%28HFCM%29%2C%20which%20enhances%20spatial-domain%20detail%20restoration%20by%0Aincorporating%20frequency-domain%20augmentation%20techniques%20into%20a%20diffusion-based%0Arestoration%20framework.%20Lastly%2C%20the%20introduction%20of%20the%20Degradation-Aware%0ADenoising%20Timestep%20Predictor%20%28DADTP%29%20module%20further%20enhances%20restoration%0Aquality%20by%20enabling%20adaptive%20region-specific%20restoration%2C%20effectively%0Aaddressing%20regional%20degradation%20inconsistencies%20in%20compressed%20hazy%20images.%0AExperimental%20results%20across%20multiple%20compressed%20dehazing%20datasets%20demonstrate%0Athat%20our%20method%20consistently%20outperforms%20the%20latest%20state-of-the-art%0Aapproaches.%20Code%20be%20available%20at%20https%3A//github.com/SYSUzrc/FDG-Diff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFDG-Diff%253A%2520Frequency-Domain-Guided%2520Diffusion%2520Framework%2520for%2520Compressed%250A%2520%2520Hazy%2520Image%2520Restoration%26entry.906535625%3DRuicheng%2520Zhang%2520and%2520Kanghui%2520Tian%2520and%2520Zeyu%2520Zhang%2520and%2520Qixiang%2520Liu%2520and%2520Zhi%2520Jin%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520reveal%2520that%2520the%2520interaction%2520between%2520haze%2520degradation%2520and%250AJPEG%2520compression%2520introduces%2520complex%2520joint%2520loss%2520effects%252C%2520which%2520significantly%250Acomplicate%2520image%2520restoration.%2520Existing%2520dehazing%2520models%2520often%2520neglect%250Acompression%2520effects%252C%2520which%2520limits%2520their%2520effectiveness%2520in%2520practical%250Aapplications.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520three%2520key%250Acontributions.%2520First%252C%2520we%2520design%2520FDG-Diff%252C%2520a%2520novel%2520frequency-domain-guided%250Adehazing%2520framework%2520that%2520improves%2520JPEG%2520image%2520restoration%2520by%2520leveraging%250Afrequency-domain%2520information.%2520Second%252C%2520we%2520introduce%2520the%2520High-Frequency%250ACompensation%2520Module%2520%2528HFCM%2529%252C%2520which%2520enhances%2520spatial-domain%2520detail%2520restoration%2520by%250Aincorporating%2520frequency-domain%2520augmentation%2520techniques%2520into%2520a%2520diffusion-based%250Arestoration%2520framework.%2520Lastly%252C%2520the%2520introduction%2520of%2520the%2520Degradation-Aware%250ADenoising%2520Timestep%2520Predictor%2520%2528DADTP%2529%2520module%2520further%2520enhances%2520restoration%250Aquality%2520by%2520enabling%2520adaptive%2520region-specific%2520restoration%252C%2520effectively%250Aaddressing%2520regional%2520degradation%2520inconsistencies%2520in%2520compressed%2520hazy%2520images.%250AExperimental%2520results%2520across%2520multiple%2520compressed%2520dehazing%2520datasets%2520demonstrate%250Athat%2520our%2520method%2520consistently%2520outperforms%2520the%2520latest%2520state-of-the-art%250Aapproaches.%2520Code%2520be%2520available%2520at%2520https%253A//github.com/SYSUzrc/FDG-Diff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FDG-Diff%3A%20Frequency-Domain-Guided%20Diffusion%20Framework%20for%20Compressed%0A%20%20Hazy%20Image%20Restoration&entry.906535625=Ruicheng%20Zhang%20and%20Kanghui%20Tian%20and%20Zeyu%20Zhang%20and%20Qixiang%20Liu%20and%20Zhi%20Jin&entry.1292438233=%20%20In%20this%20study%2C%20we%20reveal%20that%20the%20interaction%20between%20haze%20degradation%20and%0AJPEG%20compression%20introduces%20complex%20joint%20loss%20effects%2C%20which%20significantly%0Acomplicate%20image%20restoration.%20Existing%20dehazing%20models%20often%20neglect%0Acompression%20effects%2C%20which%20limits%20their%20effectiveness%20in%20practical%0Aapplications.%20To%20address%20these%20challenges%2C%20we%20introduce%20three%20key%0Acontributions.%20First%2C%20we%20design%20FDG-Diff%2C%20a%20novel%20frequency-domain-guided%0Adehazing%20framework%20that%20improves%20JPEG%20image%20restoration%20by%20leveraging%0Afrequency-domain%20information.%20Second%2C%20we%20introduce%20the%20High-Frequency%0ACompensation%20Module%20%28HFCM%29%2C%20which%20enhances%20spatial-domain%20detail%20restoration%20by%0Aincorporating%20frequency-domain%20augmentation%20techniques%20into%20a%20diffusion-based%0Arestoration%20framework.%20Lastly%2C%20the%20introduction%20of%20the%20Degradation-Aware%0ADenoising%20Timestep%20Predictor%20%28DADTP%29%20module%20further%20enhances%20restoration%0Aquality%20by%20enabling%20adaptive%20region-specific%20restoration%2C%20effectively%0Aaddressing%20regional%20degradation%20inconsistencies%20in%20compressed%20hazy%20images.%0AExperimental%20results%20across%20multiple%20compressed%20dehazing%20datasets%20demonstrate%0Athat%20our%20method%20consistently%20outperforms%20the%20latest%20state-of-the-art%0Aapproaches.%20Code%20be%20available%20at%20https%3A//github.com/SYSUzrc/FDG-Diff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12832v1&entry.124074799=Read"},
{"title": "STMDNet: A Lightweight Directional Framework for Motion Pattern\n  Recognition of Tiny Targets", "author": "Mingshuo Xu and Hao Luan and Zhou Daniel Hao and Jigen Peng and Shigang Yue", "abstract": "  Recognizing motions of tiny targets - only few dozen pixels - in cluttered\nbackgrounds remains a fundamental challenge when standard feature-based or deep\nlearning methods fail under scarce visual cues. We propose STMDNet, a\nmodel-based computational framework to Recognize motions of tiny targets at\nvariable velocities under low-sampling frequency scenarios. STMDNet designs a\nnovel dual-dynamics-and-correlation mechanism, harnessing ipsilateral\nexcitation to integrate target cues and leakage-enhancing-type contralateral\ninhibition to suppress large-object and background motion interference.\nMoreover, we develop the first collaborative directional encoding-decoding\nstrategy that determines the motion direction from only one correlation per\nspatial location, cutting computational costs to one-eighth of prior methods.\nFurther, simply substituting the backbone of a strong STMD model with STMDNet\nraises AUC by 24%, yielding an enhanced STMDNet-F. Evaluations on real-world\nlow sampling frequency datasets show state-of-the-art results, surpassing the\ndeep learning baseline. Across diverse speeds, STMDNet-F improves mF1 by 19%,\n16%, and 8% at 240Hz, 120Hz, and 60Hz, respectively, while STMDNet achieves 87\nFPS on a single CPU thread. These advances highlight STMDNet as a\nnext-generation backbone for tiny target motion pattern recognition and\nunderscore its broader potential to revitalize model-based visual approaches in\nmotion detection.\n", "link": "http://arxiv.org/abs/2501.13054v1", "date": "2025-01-22", "relevancy": 2.2608, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5928}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5697}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STMDNet%3A%20A%20Lightweight%20Directional%20Framework%20for%20Motion%20Pattern%0A%20%20Recognition%20of%20Tiny%20Targets&body=Title%3A%20STMDNet%3A%20A%20Lightweight%20Directional%20Framework%20for%20Motion%20Pattern%0A%20%20Recognition%20of%20Tiny%20Targets%0AAuthor%3A%20Mingshuo%20Xu%20and%20Hao%20Luan%20and%20Zhou%20Daniel%20Hao%20and%20Jigen%20Peng%20and%20Shigang%20Yue%0AAbstract%3A%20%20%20Recognizing%20motions%20of%20tiny%20targets%20-%20only%20few%20dozen%20pixels%20-%20in%20cluttered%0Abackgrounds%20remains%20a%20fundamental%20challenge%20when%20standard%20feature-based%20or%20deep%0Alearning%20methods%20fail%20under%20scarce%20visual%20cues.%20We%20propose%20STMDNet%2C%20a%0Amodel-based%20computational%20framework%20to%20Recognize%20motions%20of%20tiny%20targets%20at%0Avariable%20velocities%20under%20low-sampling%20frequency%20scenarios.%20STMDNet%20designs%20a%0Anovel%20dual-dynamics-and-correlation%20mechanism%2C%20harnessing%20ipsilateral%0Aexcitation%20to%20integrate%20target%20cues%20and%20leakage-enhancing-type%20contralateral%0Ainhibition%20to%20suppress%20large-object%20and%20background%20motion%20interference.%0AMoreover%2C%20we%20develop%20the%20first%20collaborative%20directional%20encoding-decoding%0Astrategy%20that%20determines%20the%20motion%20direction%20from%20only%20one%20correlation%20per%0Aspatial%20location%2C%20cutting%20computational%20costs%20to%20one-eighth%20of%20prior%20methods.%0AFurther%2C%20simply%20substituting%20the%20backbone%20of%20a%20strong%20STMD%20model%20with%20STMDNet%0Araises%20AUC%20by%2024%25%2C%20yielding%20an%20enhanced%20STMDNet-F.%20Evaluations%20on%20real-world%0Alow%20sampling%20frequency%20datasets%20show%20state-of-the-art%20results%2C%20surpassing%20the%0Adeep%20learning%20baseline.%20Across%20diverse%20speeds%2C%20STMDNet-F%20improves%20mF1%20by%2019%25%2C%0A16%25%2C%20and%208%25%20at%20240Hz%2C%20120Hz%2C%20and%2060Hz%2C%20respectively%2C%20while%20STMDNet%20achieves%2087%0AFPS%20on%20a%20single%20CPU%20thread.%20These%20advances%20highlight%20STMDNet%20as%20a%0Anext-generation%20backbone%20for%20tiny%20target%20motion%20pattern%20recognition%20and%0Aunderscore%20its%20broader%20potential%20to%20revitalize%20model-based%20visual%20approaches%20in%0Amotion%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTMDNet%253A%2520A%2520Lightweight%2520Directional%2520Framework%2520for%2520Motion%2520Pattern%250A%2520%2520Recognition%2520of%2520Tiny%2520Targets%26entry.906535625%3DMingshuo%2520Xu%2520and%2520Hao%2520Luan%2520and%2520Zhou%2520Daniel%2520Hao%2520and%2520Jigen%2520Peng%2520and%2520Shigang%2520Yue%26entry.1292438233%3D%2520%2520Recognizing%2520motions%2520of%2520tiny%2520targets%2520-%2520only%2520few%2520dozen%2520pixels%2520-%2520in%2520cluttered%250Abackgrounds%2520remains%2520a%2520fundamental%2520challenge%2520when%2520standard%2520feature-based%2520or%2520deep%250Alearning%2520methods%2520fail%2520under%2520scarce%2520visual%2520cues.%2520We%2520propose%2520STMDNet%252C%2520a%250Amodel-based%2520computational%2520framework%2520to%2520Recognize%2520motions%2520of%2520tiny%2520targets%2520at%250Avariable%2520velocities%2520under%2520low-sampling%2520frequency%2520scenarios.%2520STMDNet%2520designs%2520a%250Anovel%2520dual-dynamics-and-correlation%2520mechanism%252C%2520harnessing%2520ipsilateral%250Aexcitation%2520to%2520integrate%2520target%2520cues%2520and%2520leakage-enhancing-type%2520contralateral%250Ainhibition%2520to%2520suppress%2520large-object%2520and%2520background%2520motion%2520interference.%250AMoreover%252C%2520we%2520develop%2520the%2520first%2520collaborative%2520directional%2520encoding-decoding%250Astrategy%2520that%2520determines%2520the%2520motion%2520direction%2520from%2520only%2520one%2520correlation%2520per%250Aspatial%2520location%252C%2520cutting%2520computational%2520costs%2520to%2520one-eighth%2520of%2520prior%2520methods.%250AFurther%252C%2520simply%2520substituting%2520the%2520backbone%2520of%2520a%2520strong%2520STMD%2520model%2520with%2520STMDNet%250Araises%2520AUC%2520by%252024%2525%252C%2520yielding%2520an%2520enhanced%2520STMDNet-F.%2520Evaluations%2520on%2520real-world%250Alow%2520sampling%2520frequency%2520datasets%2520show%2520state-of-the-art%2520results%252C%2520surpassing%2520the%250Adeep%2520learning%2520baseline.%2520Across%2520diverse%2520speeds%252C%2520STMDNet-F%2520improves%2520mF1%2520by%252019%2525%252C%250A16%2525%252C%2520and%25208%2525%2520at%2520240Hz%252C%2520120Hz%252C%2520and%252060Hz%252C%2520respectively%252C%2520while%2520STMDNet%2520achieves%252087%250AFPS%2520on%2520a%2520single%2520CPU%2520thread.%2520These%2520advances%2520highlight%2520STMDNet%2520as%2520a%250Anext-generation%2520backbone%2520for%2520tiny%2520target%2520motion%2520pattern%2520recognition%2520and%250Aunderscore%2520its%2520broader%2520potential%2520to%2520revitalize%2520model-based%2520visual%2520approaches%2520in%250Amotion%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STMDNet%3A%20A%20Lightweight%20Directional%20Framework%20for%20Motion%20Pattern%0A%20%20Recognition%20of%20Tiny%20Targets&entry.906535625=Mingshuo%20Xu%20and%20Hao%20Luan%20and%20Zhou%20Daniel%20Hao%20and%20Jigen%20Peng%20and%20Shigang%20Yue&entry.1292438233=%20%20Recognizing%20motions%20of%20tiny%20targets%20-%20only%20few%20dozen%20pixels%20-%20in%20cluttered%0Abackgrounds%20remains%20a%20fundamental%20challenge%20when%20standard%20feature-based%20or%20deep%0Alearning%20methods%20fail%20under%20scarce%20visual%20cues.%20We%20propose%20STMDNet%2C%20a%0Amodel-based%20computational%20framework%20to%20Recognize%20motions%20of%20tiny%20targets%20at%0Avariable%20velocities%20under%20low-sampling%20frequency%20scenarios.%20STMDNet%20designs%20a%0Anovel%20dual-dynamics-and-correlation%20mechanism%2C%20harnessing%20ipsilateral%0Aexcitation%20to%20integrate%20target%20cues%20and%20leakage-enhancing-type%20contralateral%0Ainhibition%20to%20suppress%20large-object%20and%20background%20motion%20interference.%0AMoreover%2C%20we%20develop%20the%20first%20collaborative%20directional%20encoding-decoding%0Astrategy%20that%20determines%20the%20motion%20direction%20from%20only%20one%20correlation%20per%0Aspatial%20location%2C%20cutting%20computational%20costs%20to%20one-eighth%20of%20prior%20methods.%0AFurther%2C%20simply%20substituting%20the%20backbone%20of%20a%20strong%20STMD%20model%20with%20STMDNet%0Araises%20AUC%20by%2024%25%2C%20yielding%20an%20enhanced%20STMDNet-F.%20Evaluations%20on%20real-world%0Alow%20sampling%20frequency%20datasets%20show%20state-of-the-art%20results%2C%20surpassing%20the%0Adeep%20learning%20baseline.%20Across%20diverse%20speeds%2C%20STMDNet-F%20improves%20mF1%20by%2019%25%2C%0A16%25%2C%20and%208%25%20at%20240Hz%2C%20120Hz%2C%20and%2060Hz%2C%20respectively%2C%20while%20STMDNet%20achieves%2087%0AFPS%20on%20a%20single%20CPU%20thread.%20These%20advances%20highlight%20STMDNet%20as%20a%0Anext-generation%20backbone%20for%20tiny%20target%20motion%20pattern%20recognition%20and%0Aunderscore%20its%20broader%20potential%20to%20revitalize%20model-based%20visual%20approaches%20in%0Amotion%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13054v1&entry.124074799=Read"},
{"title": "Learning To Guide Human Decision Makers With Vision-Language Models", "author": "Debodeep Banerjee and Stefano Teso and Burcu Sayin and Andrea Passerini", "abstract": "  There is increasing interest in developing AIs for assisting human\ndecision-making in high-stakes tasks, such as medical diagnosis, for the\npurpose of improving decision quality and reducing cognitive strain. Mainstream\napproaches team up an expert with a machine learning model to which safer\ndecisions are offloaded, thus letting the former focus on cases that demand\ntheir attention. his separation of responsibilities setup, however, is\ninadequate for high-stakes scenarios. On the one hand, the expert may end up\nover-relying on the machine's decisions due to anchoring bias, thus losing the\nhuman oversight that is increasingly being required by regulatory agencies to\nensure trustworthy AI. On the other hand, the expert is left entirely\nunassisted on the (typically hardest) decisions on which the model abstained.\nAs a remedy, we introduce learning to guide (LTG), an alternative framework in\nwhich - rather than taking control from the human expert - the machine provides\nguidance useful for decision making, and the human is entirely responsible for\ncoming up with a decision. In order to ensure guidance is interpretable} and\ntask-specific, we develop SLOG, an approach for turning any vision-language\nmodel into a capable generator of textual guidance by leveraging a modicum of\nhuman feedback. Our empirical evaluation highlights the promise of \\method on a\nchallenging, real-world medical diagnosis task.\n", "link": "http://arxiv.org/abs/2403.16501v3", "date": "2025-01-22", "relevancy": 2.2341, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20To%20Guide%20Human%20Decision%20Makers%20With%20Vision-Language%20Models&body=Title%3A%20Learning%20To%20Guide%20Human%20Decision%20Makers%20With%20Vision-Language%20Models%0AAuthor%3A%20Debodeep%20Banerjee%20and%20Stefano%20Teso%20and%20Burcu%20Sayin%20and%20Andrea%20Passerini%0AAbstract%3A%20%20%20There%20is%20increasing%20interest%20in%20developing%20AIs%20for%20assisting%20human%0Adecision-making%20in%20high-stakes%20tasks%2C%20such%20as%20medical%20diagnosis%2C%20for%20the%0Apurpose%20of%20improving%20decision%20quality%20and%20reducing%20cognitive%20strain.%20Mainstream%0Aapproaches%20team%20up%20an%20expert%20with%20a%20machine%20learning%20model%20to%20which%20safer%0Adecisions%20are%20offloaded%2C%20thus%20letting%20the%20former%20focus%20on%20cases%20that%20demand%0Atheir%20attention.%20his%20separation%20of%20responsibilities%20setup%2C%20however%2C%20is%0Ainadequate%20for%20high-stakes%20scenarios.%20On%20the%20one%20hand%2C%20the%20expert%20may%20end%20up%0Aover-relying%20on%20the%20machine%27s%20decisions%20due%20to%20anchoring%20bias%2C%20thus%20losing%20the%0Ahuman%20oversight%20that%20is%20increasingly%20being%20required%20by%20regulatory%20agencies%20to%0Aensure%20trustworthy%20AI.%20On%20the%20other%20hand%2C%20the%20expert%20is%20left%20entirely%0Aunassisted%20on%20the%20%28typically%20hardest%29%20decisions%20on%20which%20the%20model%20abstained.%0AAs%20a%20remedy%2C%20we%20introduce%20learning%20to%20guide%20%28LTG%29%2C%20an%20alternative%20framework%20in%0Awhich%20-%20rather%20than%20taking%20control%20from%20the%20human%20expert%20-%20the%20machine%20provides%0Aguidance%20useful%20for%20decision%20making%2C%20and%20the%20human%20is%20entirely%20responsible%20for%0Acoming%20up%20with%20a%20decision.%20In%20order%20to%20ensure%20guidance%20is%20interpretable%7D%20and%0Atask-specific%2C%20we%20develop%20SLOG%2C%20an%20approach%20for%20turning%20any%20vision-language%0Amodel%20into%20a%20capable%20generator%20of%20textual%20guidance%20by%20leveraging%20a%20modicum%20of%0Ahuman%20feedback.%20Our%20empirical%20evaluation%20highlights%20the%20promise%20of%20%5Cmethod%20on%20a%0Achallenging%2C%20real-world%20medical%20diagnosis%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16501v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520To%2520Guide%2520Human%2520Decision%2520Makers%2520With%2520Vision-Language%2520Models%26entry.906535625%3DDebodeep%2520Banerjee%2520and%2520Stefano%2520Teso%2520and%2520Burcu%2520Sayin%2520and%2520Andrea%2520Passerini%26entry.1292438233%3D%2520%2520There%2520is%2520increasing%2520interest%2520in%2520developing%2520AIs%2520for%2520assisting%2520human%250Adecision-making%2520in%2520high-stakes%2520tasks%252C%2520such%2520as%2520medical%2520diagnosis%252C%2520for%2520the%250Apurpose%2520of%2520improving%2520decision%2520quality%2520and%2520reducing%2520cognitive%2520strain.%2520Mainstream%250Aapproaches%2520team%2520up%2520an%2520expert%2520with%2520a%2520machine%2520learning%2520model%2520to%2520which%2520safer%250Adecisions%2520are%2520offloaded%252C%2520thus%2520letting%2520the%2520former%2520focus%2520on%2520cases%2520that%2520demand%250Atheir%2520attention.%2520his%2520separation%2520of%2520responsibilities%2520setup%252C%2520however%252C%2520is%250Ainadequate%2520for%2520high-stakes%2520scenarios.%2520On%2520the%2520one%2520hand%252C%2520the%2520expert%2520may%2520end%2520up%250Aover-relying%2520on%2520the%2520machine%2527s%2520decisions%2520due%2520to%2520anchoring%2520bias%252C%2520thus%2520losing%2520the%250Ahuman%2520oversight%2520that%2520is%2520increasingly%2520being%2520required%2520by%2520regulatory%2520agencies%2520to%250Aensure%2520trustworthy%2520AI.%2520On%2520the%2520other%2520hand%252C%2520the%2520expert%2520is%2520left%2520entirely%250Aunassisted%2520on%2520the%2520%2528typically%2520hardest%2529%2520decisions%2520on%2520which%2520the%2520model%2520abstained.%250AAs%2520a%2520remedy%252C%2520we%2520introduce%2520learning%2520to%2520guide%2520%2528LTG%2529%252C%2520an%2520alternative%2520framework%2520in%250Awhich%2520-%2520rather%2520than%2520taking%2520control%2520from%2520the%2520human%2520expert%2520-%2520the%2520machine%2520provides%250Aguidance%2520useful%2520for%2520decision%2520making%252C%2520and%2520the%2520human%2520is%2520entirely%2520responsible%2520for%250Acoming%2520up%2520with%2520a%2520decision.%2520In%2520order%2520to%2520ensure%2520guidance%2520is%2520interpretable%257D%2520and%250Atask-specific%252C%2520we%2520develop%2520SLOG%252C%2520an%2520approach%2520for%2520turning%2520any%2520vision-language%250Amodel%2520into%2520a%2520capable%2520generator%2520of%2520textual%2520guidance%2520by%2520leveraging%2520a%2520modicum%2520of%250Ahuman%2520feedback.%2520Our%2520empirical%2520evaluation%2520highlights%2520the%2520promise%2520of%2520%255Cmethod%2520on%2520a%250Achallenging%252C%2520real-world%2520medical%2520diagnosis%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16501v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20To%20Guide%20Human%20Decision%20Makers%20With%20Vision-Language%20Models&entry.906535625=Debodeep%20Banerjee%20and%20Stefano%20Teso%20and%20Burcu%20Sayin%20and%20Andrea%20Passerini&entry.1292438233=%20%20There%20is%20increasing%20interest%20in%20developing%20AIs%20for%20assisting%20human%0Adecision-making%20in%20high-stakes%20tasks%2C%20such%20as%20medical%20diagnosis%2C%20for%20the%0Apurpose%20of%20improving%20decision%20quality%20and%20reducing%20cognitive%20strain.%20Mainstream%0Aapproaches%20team%20up%20an%20expert%20with%20a%20machine%20learning%20model%20to%20which%20safer%0Adecisions%20are%20offloaded%2C%20thus%20letting%20the%20former%20focus%20on%20cases%20that%20demand%0Atheir%20attention.%20his%20separation%20of%20responsibilities%20setup%2C%20however%2C%20is%0Ainadequate%20for%20high-stakes%20scenarios.%20On%20the%20one%20hand%2C%20the%20expert%20may%20end%20up%0Aover-relying%20on%20the%20machine%27s%20decisions%20due%20to%20anchoring%20bias%2C%20thus%20losing%20the%0Ahuman%20oversight%20that%20is%20increasingly%20being%20required%20by%20regulatory%20agencies%20to%0Aensure%20trustworthy%20AI.%20On%20the%20other%20hand%2C%20the%20expert%20is%20left%20entirely%0Aunassisted%20on%20the%20%28typically%20hardest%29%20decisions%20on%20which%20the%20model%20abstained.%0AAs%20a%20remedy%2C%20we%20introduce%20learning%20to%20guide%20%28LTG%29%2C%20an%20alternative%20framework%20in%0Awhich%20-%20rather%20than%20taking%20control%20from%20the%20human%20expert%20-%20the%20machine%20provides%0Aguidance%20useful%20for%20decision%20making%2C%20and%20the%20human%20is%20entirely%20responsible%20for%0Acoming%20up%20with%20a%20decision.%20In%20order%20to%20ensure%20guidance%20is%20interpretable%7D%20and%0Atask-specific%2C%20we%20develop%20SLOG%2C%20an%20approach%20for%20turning%20any%20vision-language%0Amodel%20into%20a%20capable%20generator%20of%20textual%20guidance%20by%20leveraging%20a%20modicum%20of%0Ahuman%20feedback.%20Our%20empirical%20evaluation%20highlights%20the%20promise%20of%20%5Cmethod%20on%20a%0Achallenging%2C%20real-world%20medical%20diagnosis%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16501v3&entry.124074799=Read"},
{"title": "A Novel Tracking Framework for Devices in X-ray Leveraging Supplementary\n  Cue-Driven Self-Supervised Features", "author": "Saahil Islam and Venkatesh N. Murthy and Dominik Neumann and Serkan Cimen and Puneet Sharma and Andreas Maier and Dorin Comaniciu and Florin C. Ghesu", "abstract": "  To restore proper blood flow in blocked coronary arteries via angioplasty\nprocedure, accurate placement of devices such as catheters, balloons, and\nstents under live fluoroscopy or diagnostic angiography is crucial. Identified\nballoon markers help in enhancing stent visibility in X-ray sequences, while\nthe catheter tip aids in precise navigation and co-registering vessel\nstructures, reducing the need for contrast in angiography. However, accurate\ndetection of these devices in interventional X-ray sequences faces significant\nchallenges, particularly due to occlusions from contrasted vessels and other\ndevices and distractions from surrounding, resulting in the failure to track\nsuch small objects. While most tracking methods rely on spatial correlation of\npast and current appearance, they often lack strong motion comprehension\nessential for navigating through these challenging conditions, and fail to\neffectively detect multiple instances in the scene. To overcome these\nlimitations, we propose a self-supervised learning approach that enhances its\nspatio-temporal understanding by incorporating supplementary cues and learning\nacross multiple representation spaces on a large dataset. Followed by that, we\nintroduce a generic real-time tracking framework that effectively leverages the\npretrained spatio-temporal network and also takes the historical appearance and\ntrajectory data into account. This results in enhanced localization of multiple\ninstances of device landmarks. Our method outperforms state-of-the-art methods\nin interventional X-ray device tracking, especially stability and robustness,\nachieving an 87% reduction in max error for balloon marker detection and a 61%\nreduction in max error for catheter tip detection.\n", "link": "http://arxiv.org/abs/2501.12958v1", "date": "2025-01-22", "relevancy": 2.2332, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.593}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Tracking%20Framework%20for%20Devices%20in%20X-ray%20Leveraging%20Supplementary%0A%20%20Cue-Driven%20Self-Supervised%20Features&body=Title%3A%20A%20Novel%20Tracking%20Framework%20for%20Devices%20in%20X-ray%20Leveraging%20Supplementary%0A%20%20Cue-Driven%20Self-Supervised%20Features%0AAuthor%3A%20Saahil%20Islam%20and%20Venkatesh%20N.%20Murthy%20and%20Dominik%20Neumann%20and%20Serkan%20Cimen%20and%20Puneet%20Sharma%20and%20Andreas%20Maier%20and%20Dorin%20Comaniciu%20and%20Florin%20C.%20Ghesu%0AAbstract%3A%20%20%20To%20restore%20proper%20blood%20flow%20in%20blocked%20coronary%20arteries%20via%20angioplasty%0Aprocedure%2C%20accurate%20placement%20of%20devices%20such%20as%20catheters%2C%20balloons%2C%20and%0Astents%20under%20live%20fluoroscopy%20or%20diagnostic%20angiography%20is%20crucial.%20Identified%0Aballoon%20markers%20help%20in%20enhancing%20stent%20visibility%20in%20X-ray%20sequences%2C%20while%0Athe%20catheter%20tip%20aids%20in%20precise%20navigation%20and%20co-registering%20vessel%0Astructures%2C%20reducing%20the%20need%20for%20contrast%20in%20angiography.%20However%2C%20accurate%0Adetection%20of%20these%20devices%20in%20interventional%20X-ray%20sequences%20faces%20significant%0Achallenges%2C%20particularly%20due%20to%20occlusions%20from%20contrasted%20vessels%20and%20other%0Adevices%20and%20distractions%20from%20surrounding%2C%20resulting%20in%20the%20failure%20to%20track%0Asuch%20small%20objects.%20While%20most%20tracking%20methods%20rely%20on%20spatial%20correlation%20of%0Apast%20and%20current%20appearance%2C%20they%20often%20lack%20strong%20motion%20comprehension%0Aessential%20for%20navigating%20through%20these%20challenging%20conditions%2C%20and%20fail%20to%0Aeffectively%20detect%20multiple%20instances%20in%20the%20scene.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20self-supervised%20learning%20approach%20that%20enhances%20its%0Aspatio-temporal%20understanding%20by%20incorporating%20supplementary%20cues%20and%20learning%0Aacross%20multiple%20representation%20spaces%20on%20a%20large%20dataset.%20Followed%20by%20that%2C%20we%0Aintroduce%20a%20generic%20real-time%20tracking%20framework%20that%20effectively%20leverages%20the%0Apretrained%20spatio-temporal%20network%20and%20also%20takes%20the%20historical%20appearance%20and%0Atrajectory%20data%20into%20account.%20This%20results%20in%20enhanced%20localization%20of%20multiple%0Ainstances%20of%20device%20landmarks.%20Our%20method%20outperforms%20state-of-the-art%20methods%0Ain%20interventional%20X-ray%20device%20tracking%2C%20especially%20stability%20and%20robustness%2C%0Aachieving%20an%2087%25%20reduction%20in%20max%20error%20for%20balloon%20marker%20detection%20and%20a%2061%25%0Areduction%20in%20max%20error%20for%20catheter%20tip%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Tracking%2520Framework%2520for%2520Devices%2520in%2520X-ray%2520Leveraging%2520Supplementary%250A%2520%2520Cue-Driven%2520Self-Supervised%2520Features%26entry.906535625%3DSaahil%2520Islam%2520and%2520Venkatesh%2520N.%2520Murthy%2520and%2520Dominik%2520Neumann%2520and%2520Serkan%2520Cimen%2520and%2520Puneet%2520Sharma%2520and%2520Andreas%2520Maier%2520and%2520Dorin%2520Comaniciu%2520and%2520Florin%2520C.%2520Ghesu%26entry.1292438233%3D%2520%2520To%2520restore%2520proper%2520blood%2520flow%2520in%2520blocked%2520coronary%2520arteries%2520via%2520angioplasty%250Aprocedure%252C%2520accurate%2520placement%2520of%2520devices%2520such%2520as%2520catheters%252C%2520balloons%252C%2520and%250Astents%2520under%2520live%2520fluoroscopy%2520or%2520diagnostic%2520angiography%2520is%2520crucial.%2520Identified%250Aballoon%2520markers%2520help%2520in%2520enhancing%2520stent%2520visibility%2520in%2520X-ray%2520sequences%252C%2520while%250Athe%2520catheter%2520tip%2520aids%2520in%2520precise%2520navigation%2520and%2520co-registering%2520vessel%250Astructures%252C%2520reducing%2520the%2520need%2520for%2520contrast%2520in%2520angiography.%2520However%252C%2520accurate%250Adetection%2520of%2520these%2520devices%2520in%2520interventional%2520X-ray%2520sequences%2520faces%2520significant%250Achallenges%252C%2520particularly%2520due%2520to%2520occlusions%2520from%2520contrasted%2520vessels%2520and%2520other%250Adevices%2520and%2520distractions%2520from%2520surrounding%252C%2520resulting%2520in%2520the%2520failure%2520to%2520track%250Asuch%2520small%2520objects.%2520While%2520most%2520tracking%2520methods%2520rely%2520on%2520spatial%2520correlation%2520of%250Apast%2520and%2520current%2520appearance%252C%2520they%2520often%2520lack%2520strong%2520motion%2520comprehension%250Aessential%2520for%2520navigating%2520through%2520these%2520challenging%2520conditions%252C%2520and%2520fail%2520to%250Aeffectively%2520detect%2520multiple%2520instances%2520in%2520the%2520scene.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520self-supervised%2520learning%2520approach%2520that%2520enhances%2520its%250Aspatio-temporal%2520understanding%2520by%2520incorporating%2520supplementary%2520cues%2520and%2520learning%250Aacross%2520multiple%2520representation%2520spaces%2520on%2520a%2520large%2520dataset.%2520Followed%2520by%2520that%252C%2520we%250Aintroduce%2520a%2520generic%2520real-time%2520tracking%2520framework%2520that%2520effectively%2520leverages%2520the%250Apretrained%2520spatio-temporal%2520network%2520and%2520also%2520takes%2520the%2520historical%2520appearance%2520and%250Atrajectory%2520data%2520into%2520account.%2520This%2520results%2520in%2520enhanced%2520localization%2520of%2520multiple%250Ainstances%2520of%2520device%2520landmarks.%2520Our%2520method%2520outperforms%2520state-of-the-art%2520methods%250Ain%2520interventional%2520X-ray%2520device%2520tracking%252C%2520especially%2520stability%2520and%2520robustness%252C%250Aachieving%2520an%252087%2525%2520reduction%2520in%2520max%2520error%2520for%2520balloon%2520marker%2520detection%2520and%2520a%252061%2525%250Areduction%2520in%2520max%2520error%2520for%2520catheter%2520tip%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Tracking%20Framework%20for%20Devices%20in%20X-ray%20Leveraging%20Supplementary%0A%20%20Cue-Driven%20Self-Supervised%20Features&entry.906535625=Saahil%20Islam%20and%20Venkatesh%20N.%20Murthy%20and%20Dominik%20Neumann%20and%20Serkan%20Cimen%20and%20Puneet%20Sharma%20and%20Andreas%20Maier%20and%20Dorin%20Comaniciu%20and%20Florin%20C.%20Ghesu&entry.1292438233=%20%20To%20restore%20proper%20blood%20flow%20in%20blocked%20coronary%20arteries%20via%20angioplasty%0Aprocedure%2C%20accurate%20placement%20of%20devices%20such%20as%20catheters%2C%20balloons%2C%20and%0Astents%20under%20live%20fluoroscopy%20or%20diagnostic%20angiography%20is%20crucial.%20Identified%0Aballoon%20markers%20help%20in%20enhancing%20stent%20visibility%20in%20X-ray%20sequences%2C%20while%0Athe%20catheter%20tip%20aids%20in%20precise%20navigation%20and%20co-registering%20vessel%0Astructures%2C%20reducing%20the%20need%20for%20contrast%20in%20angiography.%20However%2C%20accurate%0Adetection%20of%20these%20devices%20in%20interventional%20X-ray%20sequences%20faces%20significant%0Achallenges%2C%20particularly%20due%20to%20occlusions%20from%20contrasted%20vessels%20and%20other%0Adevices%20and%20distractions%20from%20surrounding%2C%20resulting%20in%20the%20failure%20to%20track%0Asuch%20small%20objects.%20While%20most%20tracking%20methods%20rely%20on%20spatial%20correlation%20of%0Apast%20and%20current%20appearance%2C%20they%20often%20lack%20strong%20motion%20comprehension%0Aessential%20for%20navigating%20through%20these%20challenging%20conditions%2C%20and%20fail%20to%0Aeffectively%20detect%20multiple%20instances%20in%20the%20scene.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20self-supervised%20learning%20approach%20that%20enhances%20its%0Aspatio-temporal%20understanding%20by%20incorporating%20supplementary%20cues%20and%20learning%0Aacross%20multiple%20representation%20spaces%20on%20a%20large%20dataset.%20Followed%20by%20that%2C%20we%0Aintroduce%20a%20generic%20real-time%20tracking%20framework%20that%20effectively%20leverages%20the%0Apretrained%20spatio-temporal%20network%20and%20also%20takes%20the%20historical%20appearance%20and%0Atrajectory%20data%20into%20account.%20This%20results%20in%20enhanced%20localization%20of%20multiple%0Ainstances%20of%20device%20landmarks.%20Our%20method%20outperforms%20state-of-the-art%20methods%0Ain%20interventional%20X-ray%20device%20tracking%2C%20especially%20stability%20and%20robustness%2C%0Aachieving%20an%2087%25%20reduction%20in%20max%20error%20for%20balloon%20marker%20detection%20and%20a%2061%25%0Areduction%20in%20max%20error%20for%20catheter%20tip%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12958v1&entry.124074799=Read"},
{"title": "Unified CNNs and transformers underlying learning mechanism reveals\n  multi-head attention modus vivendi", "author": "Ella Koresh and Ronit D. Gross and Yuval Meir and Yarden Tzach and Tal Halevi and Ido Kanter", "abstract": "  Convolutional neural networks (CNNs) evaluate short-range correlations in\ninput images which progress along the layers, whereas vision transformer (ViT)\narchitectures evaluate long-range correlations, using repeated transformer\nencoders composed of fully connected layers. Both are designed to solve complex\nclassification tasks but from different perspectives. This study demonstrates\nthat CNNs and ViT architectures stem from a unified underlying learning\nmechanism, which quantitatively measures the single-nodal performance (SNP) of\neach node in feedforward (FF) and multi-head attention (MHA) subblocks. Each\nnode identifies small clusters of possible output labels, with additional noise\nrepresented as labels outside these clusters. These features are progressively\nsharpened along the transformer encoders, enhancing the signal-to-noise ratio.\nThis unified underlying learning mechanism leads to two main findings. First,\nit enables an efficient applied nodal diagonal connection (ANDC) pruning\ntechnique without affecting the accuracy. Second, based on the SNP, spontaneous\nsymmetry breaking occurs among the MHA heads, such that each head focuses its\nattention on a subset of labels through cooperation among its SNPs.\nConsequently, each head becomes an expert in recognizing its designated labels,\nrepresenting a quantitative MHA modus vivendi mechanism. These results are\nbased on a compact convolutional transformer architecture trained on the\nCIFAR-100 and Flowers-102 datasets and call for their extension to other\narchitectures and applications, such as natural language processing.\n", "link": "http://arxiv.org/abs/2501.12900v1", "date": "2025-01-22", "relevancy": 2.2293, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5642}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20CNNs%20and%20transformers%20underlying%20learning%20mechanism%20reveals%0A%20%20multi-head%20attention%20modus%20vivendi&body=Title%3A%20Unified%20CNNs%20and%20transformers%20underlying%20learning%20mechanism%20reveals%0A%20%20multi-head%20attention%20modus%20vivendi%0AAuthor%3A%20Ella%20Koresh%20and%20Ronit%20D.%20Gross%20and%20Yuval%20Meir%20and%20Yarden%20Tzach%20and%20Tal%20Halevi%20and%20Ido%20Kanter%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20evaluate%20short-range%20correlations%20in%0Ainput%20images%20which%20progress%20along%20the%20layers%2C%20whereas%20vision%20transformer%20%28ViT%29%0Aarchitectures%20evaluate%20long-range%20correlations%2C%20using%20repeated%20transformer%0Aencoders%20composed%20of%20fully%20connected%20layers.%20Both%20are%20designed%20to%20solve%20complex%0Aclassification%20tasks%20but%20from%20different%20perspectives.%20This%20study%20demonstrates%0Athat%20CNNs%20and%20ViT%20architectures%20stem%20from%20a%20unified%20underlying%20learning%0Amechanism%2C%20which%20quantitatively%20measures%20the%20single-nodal%20performance%20%28SNP%29%20of%0Aeach%20node%20in%20feedforward%20%28FF%29%20and%20multi-head%20attention%20%28MHA%29%20subblocks.%20Each%0Anode%20identifies%20small%20clusters%20of%20possible%20output%20labels%2C%20with%20additional%20noise%0Arepresented%20as%20labels%20outside%20these%20clusters.%20These%20features%20are%20progressively%0Asharpened%20along%20the%20transformer%20encoders%2C%20enhancing%20the%20signal-to-noise%20ratio.%0AThis%20unified%20underlying%20learning%20mechanism%20leads%20to%20two%20main%20findings.%20First%2C%0Ait%20enables%20an%20efficient%20applied%20nodal%20diagonal%20connection%20%28ANDC%29%20pruning%0Atechnique%20without%20affecting%20the%20accuracy.%20Second%2C%20based%20on%20the%20SNP%2C%20spontaneous%0Asymmetry%20breaking%20occurs%20among%20the%20MHA%20heads%2C%20such%20that%20each%20head%20focuses%20its%0Aattention%20on%20a%20subset%20of%20labels%20through%20cooperation%20among%20its%20SNPs.%0AConsequently%2C%20each%20head%20becomes%20an%20expert%20in%20recognizing%20its%20designated%20labels%2C%0Arepresenting%20a%20quantitative%20MHA%20modus%20vivendi%20mechanism.%20These%20results%20are%0Abased%20on%20a%20compact%20convolutional%20transformer%20architecture%20trained%20on%20the%0ACIFAR-100%20and%20Flowers-102%20datasets%20and%20call%20for%20their%20extension%20to%20other%0Aarchitectures%20and%20applications%2C%20such%20as%20natural%20language%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520CNNs%2520and%2520transformers%2520underlying%2520learning%2520mechanism%2520reveals%250A%2520%2520multi-head%2520attention%2520modus%2520vivendi%26entry.906535625%3DElla%2520Koresh%2520and%2520Ronit%2520D.%2520Gross%2520and%2520Yuval%2520Meir%2520and%2520Yarden%2520Tzach%2520and%2520Tal%2520Halevi%2520and%2520Ido%2520Kanter%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520evaluate%2520short-range%2520correlations%2520in%250Ainput%2520images%2520which%2520progress%2520along%2520the%2520layers%252C%2520whereas%2520vision%2520transformer%2520%2528ViT%2529%250Aarchitectures%2520evaluate%2520long-range%2520correlations%252C%2520using%2520repeated%2520transformer%250Aencoders%2520composed%2520of%2520fully%2520connected%2520layers.%2520Both%2520are%2520designed%2520to%2520solve%2520complex%250Aclassification%2520tasks%2520but%2520from%2520different%2520perspectives.%2520This%2520study%2520demonstrates%250Athat%2520CNNs%2520and%2520ViT%2520architectures%2520stem%2520from%2520a%2520unified%2520underlying%2520learning%250Amechanism%252C%2520which%2520quantitatively%2520measures%2520the%2520single-nodal%2520performance%2520%2528SNP%2529%2520of%250Aeach%2520node%2520in%2520feedforward%2520%2528FF%2529%2520and%2520multi-head%2520attention%2520%2528MHA%2529%2520subblocks.%2520Each%250Anode%2520identifies%2520small%2520clusters%2520of%2520possible%2520output%2520labels%252C%2520with%2520additional%2520noise%250Arepresented%2520as%2520labels%2520outside%2520these%2520clusters.%2520These%2520features%2520are%2520progressively%250Asharpened%2520along%2520the%2520transformer%2520encoders%252C%2520enhancing%2520the%2520signal-to-noise%2520ratio.%250AThis%2520unified%2520underlying%2520learning%2520mechanism%2520leads%2520to%2520two%2520main%2520findings.%2520First%252C%250Ait%2520enables%2520an%2520efficient%2520applied%2520nodal%2520diagonal%2520connection%2520%2528ANDC%2529%2520pruning%250Atechnique%2520without%2520affecting%2520the%2520accuracy.%2520Second%252C%2520based%2520on%2520the%2520SNP%252C%2520spontaneous%250Asymmetry%2520breaking%2520occurs%2520among%2520the%2520MHA%2520heads%252C%2520such%2520that%2520each%2520head%2520focuses%2520its%250Aattention%2520on%2520a%2520subset%2520of%2520labels%2520through%2520cooperation%2520among%2520its%2520SNPs.%250AConsequently%252C%2520each%2520head%2520becomes%2520an%2520expert%2520in%2520recognizing%2520its%2520designated%2520labels%252C%250Arepresenting%2520a%2520quantitative%2520MHA%2520modus%2520vivendi%2520mechanism.%2520These%2520results%2520are%250Abased%2520on%2520a%2520compact%2520convolutional%2520transformer%2520architecture%2520trained%2520on%2520the%250ACIFAR-100%2520and%2520Flowers-102%2520datasets%2520and%2520call%2520for%2520their%2520extension%2520to%2520other%250Aarchitectures%2520and%2520applications%252C%2520such%2520as%2520natural%2520language%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20CNNs%20and%20transformers%20underlying%20learning%20mechanism%20reveals%0A%20%20multi-head%20attention%20modus%20vivendi&entry.906535625=Ella%20Koresh%20and%20Ronit%20D.%20Gross%20and%20Yuval%20Meir%20and%20Yarden%20Tzach%20and%20Tal%20Halevi%20and%20Ido%20Kanter&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20evaluate%20short-range%20correlations%20in%0Ainput%20images%20which%20progress%20along%20the%20layers%2C%20whereas%20vision%20transformer%20%28ViT%29%0Aarchitectures%20evaluate%20long-range%20correlations%2C%20using%20repeated%20transformer%0Aencoders%20composed%20of%20fully%20connected%20layers.%20Both%20are%20designed%20to%20solve%20complex%0Aclassification%20tasks%20but%20from%20different%20perspectives.%20This%20study%20demonstrates%0Athat%20CNNs%20and%20ViT%20architectures%20stem%20from%20a%20unified%20underlying%20learning%0Amechanism%2C%20which%20quantitatively%20measures%20the%20single-nodal%20performance%20%28SNP%29%20of%0Aeach%20node%20in%20feedforward%20%28FF%29%20and%20multi-head%20attention%20%28MHA%29%20subblocks.%20Each%0Anode%20identifies%20small%20clusters%20of%20possible%20output%20labels%2C%20with%20additional%20noise%0Arepresented%20as%20labels%20outside%20these%20clusters.%20These%20features%20are%20progressively%0Asharpened%20along%20the%20transformer%20encoders%2C%20enhancing%20the%20signal-to-noise%20ratio.%0AThis%20unified%20underlying%20learning%20mechanism%20leads%20to%20two%20main%20findings.%20First%2C%0Ait%20enables%20an%20efficient%20applied%20nodal%20diagonal%20connection%20%28ANDC%29%20pruning%0Atechnique%20without%20affecting%20the%20accuracy.%20Second%2C%20based%20on%20the%20SNP%2C%20spontaneous%0Asymmetry%20breaking%20occurs%20among%20the%20MHA%20heads%2C%20such%20that%20each%20head%20focuses%20its%0Aattention%20on%20a%20subset%20of%20labels%20through%20cooperation%20among%20its%20SNPs.%0AConsequently%2C%20each%20head%20becomes%20an%20expert%20in%20recognizing%20its%20designated%20labels%2C%0Arepresenting%20a%20quantitative%20MHA%20modus%20vivendi%20mechanism.%20These%20results%20are%0Abased%20on%20a%20compact%20convolutional%20transformer%20architecture%20trained%20on%20the%0ACIFAR-100%20and%20Flowers-102%20datasets%20and%20call%20for%20their%20extension%20to%20other%0Aarchitectures%20and%20applications%2C%20such%20as%20natural%20language%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12900v1&entry.124074799=Read"},
{"title": "Certified Guidance for Planning with Deep Generative Models", "author": "Francesco Giacomarra and Mehran Hosseini and Nicola Paoletti and Francesca Cairoli", "abstract": "  Deep generative models, such as generative adversarial networks and diffusion\nmodels, have recently emerged as powerful tools for planning tasks and behavior\nsynthesis in autonomous systems. Various guidance strategies have been\nintroduced to steer the generative process toward outputs that are more likely\nto satisfy the planning objectives. These strategies avoid the need for model\nretraining but do not provide any guarantee that the generated outputs will\nsatisfy the desired planning objectives. To address this limitation, we\nintroduce certified guidance, an approach that modifies a generative model,\nwithout retraining it, into a new model guaranteed to satisfy a given\nspecification with probability one. We focus on Signal Temporal Logic\nspecifications, which are rich enough to describe nontrivial planning tasks.\nOur approach leverages neural network verification techniques to systematically\nexplore the latent spaces of the generative models, identifying latent regions\nthat are certifiably correct with respect to the STL property of interest. We\nevaluate the effectiveness of our method on four planning benchmarks using GANs\nand diffusion models. Our results confirm that certified guidance produces\ngenerative models that are always correct, unlike existing guidance methods\nthat are not certified.\n", "link": "http://arxiv.org/abs/2501.12815v1", "date": "2025-01-22", "relevancy": 2.2241, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5592}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5547}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Certified%20Guidance%20for%20Planning%20with%20Deep%20Generative%20Models&body=Title%3A%20Certified%20Guidance%20for%20Planning%20with%20Deep%20Generative%20Models%0AAuthor%3A%20Francesco%20Giacomarra%20and%20Mehran%20Hosseini%20and%20Nicola%20Paoletti%20and%20Francesca%20Cairoli%0AAbstract%3A%20%20%20Deep%20generative%20models%2C%20such%20as%20generative%20adversarial%20networks%20and%20diffusion%0Amodels%2C%20have%20recently%20emerged%20as%20powerful%20tools%20for%20planning%20tasks%20and%20behavior%0Asynthesis%20in%20autonomous%20systems.%20Various%20guidance%20strategies%20have%20been%0Aintroduced%20to%20steer%20the%20generative%20process%20toward%20outputs%20that%20are%20more%20likely%0Ato%20satisfy%20the%20planning%20objectives.%20These%20strategies%20avoid%20the%20need%20for%20model%0Aretraining%20but%20do%20not%20provide%20any%20guarantee%20that%20the%20generated%20outputs%20will%0Asatisfy%20the%20desired%20planning%20objectives.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20certified%20guidance%2C%20an%20approach%20that%20modifies%20a%20generative%20model%2C%0Awithout%20retraining%20it%2C%20into%20a%20new%20model%20guaranteed%20to%20satisfy%20a%20given%0Aspecification%20with%20probability%20one.%20We%20focus%20on%20Signal%20Temporal%20Logic%0Aspecifications%2C%20which%20are%20rich%20enough%20to%20describe%20nontrivial%20planning%20tasks.%0AOur%20approach%20leverages%20neural%20network%20verification%20techniques%20to%20systematically%0Aexplore%20the%20latent%20spaces%20of%20the%20generative%20models%2C%20identifying%20latent%20regions%0Athat%20are%20certifiably%20correct%20with%20respect%20to%20the%20STL%20property%20of%20interest.%20We%0Aevaluate%20the%20effectiveness%20of%20our%20method%20on%20four%20planning%20benchmarks%20using%20GANs%0Aand%20diffusion%20models.%20Our%20results%20confirm%20that%20certified%20guidance%20produces%0Agenerative%20models%20that%20are%20always%20correct%2C%20unlike%20existing%20guidance%20methods%0Athat%20are%20not%20certified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCertified%2520Guidance%2520for%2520Planning%2520with%2520Deep%2520Generative%2520Models%26entry.906535625%3DFrancesco%2520Giacomarra%2520and%2520Mehran%2520Hosseini%2520and%2520Nicola%2520Paoletti%2520and%2520Francesca%2520Cairoli%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%252C%2520such%2520as%2520generative%2520adversarial%2520networks%2520and%2520diffusion%250Amodels%252C%2520have%2520recently%2520emerged%2520as%2520powerful%2520tools%2520for%2520planning%2520tasks%2520and%2520behavior%250Asynthesis%2520in%2520autonomous%2520systems.%2520Various%2520guidance%2520strategies%2520have%2520been%250Aintroduced%2520to%2520steer%2520the%2520generative%2520process%2520toward%2520outputs%2520that%2520are%2520more%2520likely%250Ato%2520satisfy%2520the%2520planning%2520objectives.%2520These%2520strategies%2520avoid%2520the%2520need%2520for%2520model%250Aretraining%2520but%2520do%2520not%2520provide%2520any%2520guarantee%2520that%2520the%2520generated%2520outputs%2520will%250Asatisfy%2520the%2520desired%2520planning%2520objectives.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520certified%2520guidance%252C%2520an%2520approach%2520that%2520modifies%2520a%2520generative%2520model%252C%250Awithout%2520retraining%2520it%252C%2520into%2520a%2520new%2520model%2520guaranteed%2520to%2520satisfy%2520a%2520given%250Aspecification%2520with%2520probability%2520one.%2520We%2520focus%2520on%2520Signal%2520Temporal%2520Logic%250Aspecifications%252C%2520which%2520are%2520rich%2520enough%2520to%2520describe%2520nontrivial%2520planning%2520tasks.%250AOur%2520approach%2520leverages%2520neural%2520network%2520verification%2520techniques%2520to%2520systematically%250Aexplore%2520the%2520latent%2520spaces%2520of%2520the%2520generative%2520models%252C%2520identifying%2520latent%2520regions%250Athat%2520are%2520certifiably%2520correct%2520with%2520respect%2520to%2520the%2520STL%2520property%2520of%2520interest.%2520We%250Aevaluate%2520the%2520effectiveness%2520of%2520our%2520method%2520on%2520four%2520planning%2520benchmarks%2520using%2520GANs%250Aand%2520diffusion%2520models.%2520Our%2520results%2520confirm%2520that%2520certified%2520guidance%2520produces%250Agenerative%2520models%2520that%2520are%2520always%2520correct%252C%2520unlike%2520existing%2520guidance%2520methods%250Athat%2520are%2520not%2520certified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certified%20Guidance%20for%20Planning%20with%20Deep%20Generative%20Models&entry.906535625=Francesco%20Giacomarra%20and%20Mehran%20Hosseini%20and%20Nicola%20Paoletti%20and%20Francesca%20Cairoli&entry.1292438233=%20%20Deep%20generative%20models%2C%20such%20as%20generative%20adversarial%20networks%20and%20diffusion%0Amodels%2C%20have%20recently%20emerged%20as%20powerful%20tools%20for%20planning%20tasks%20and%20behavior%0Asynthesis%20in%20autonomous%20systems.%20Various%20guidance%20strategies%20have%20been%0Aintroduced%20to%20steer%20the%20generative%20process%20toward%20outputs%20that%20are%20more%20likely%0Ato%20satisfy%20the%20planning%20objectives.%20These%20strategies%20avoid%20the%20need%20for%20model%0Aretraining%20but%20do%20not%20provide%20any%20guarantee%20that%20the%20generated%20outputs%20will%0Asatisfy%20the%20desired%20planning%20objectives.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20certified%20guidance%2C%20an%20approach%20that%20modifies%20a%20generative%20model%2C%0Awithout%20retraining%20it%2C%20into%20a%20new%20model%20guaranteed%20to%20satisfy%20a%20given%0Aspecification%20with%20probability%20one.%20We%20focus%20on%20Signal%20Temporal%20Logic%0Aspecifications%2C%20which%20are%20rich%20enough%20to%20describe%20nontrivial%20planning%20tasks.%0AOur%20approach%20leverages%20neural%20network%20verification%20techniques%20to%20systematically%0Aexplore%20the%20latent%20spaces%20of%20the%20generative%20models%2C%20identifying%20latent%20regions%0Athat%20are%20certifiably%20correct%20with%20respect%20to%20the%20STL%20property%20of%20interest.%20We%0Aevaluate%20the%20effectiveness%20of%20our%20method%20on%20four%20planning%20benchmarks%20using%20GANs%0Aand%20diffusion%20models.%20Our%20results%20confirm%20that%20certified%20guidance%20produces%0Agenerative%20models%20that%20are%20always%20correct%2C%20unlike%20existing%20guidance%20methods%0Athat%20are%20not%20certified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12815v1&entry.124074799=Read"},
{"title": "Attention-Driven Hierarchical Reinforcement Learning with Particle\n  Filtering for Source Localization in Dynamic Fields", "author": "Yiwei Shi and Mengyue Yang and Qi Zhang and Weinan Zhang and Cunjia Liu and Weiru Liu", "abstract": "  In many real-world scenarios, such as gas leak detection or environmental\npollutant tracking, solving the Inverse Source Localization and\nCharacterization problem involves navigating complex, dynamic fields with\nsparse and noisy observations. Traditional methods face significant challenges,\nincluding partial observability, temporal and spatial dynamics,\nout-of-distribution generalization, and reward sparsity. To address these\nissues, we propose a hierarchical framework that integrates Bayesian inference\nand reinforcement learning. The framework leverages an attention-enhanced\nparticle filtering mechanism for efficient and accurate belief updates, and\nincorporates two complementary execution strategies: Attention Particle\nFiltering Planning and Attention Particle Filtering Reinforcement Learning.\nThese approaches optimize exploration and adaptation under uncertainty.\nTheoretical analysis proves the convergence of the attention-enhanced particle\nfilter, while extensive experiments across diverse scenarios validate the\nframework's superior accuracy, adaptability, and computational efficiency. Our\nresults highlight the framework's potential for broad applications in dynamic\nfield estimation tasks.\n", "link": "http://arxiv.org/abs/2501.13084v1", "date": "2025-01-22", "relevancy": 2.2213, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.567}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-Driven%20Hierarchical%20Reinforcement%20Learning%20with%20Particle%0A%20%20Filtering%20for%20Source%20Localization%20in%20Dynamic%20Fields&body=Title%3A%20Attention-Driven%20Hierarchical%20Reinforcement%20Learning%20with%20Particle%0A%20%20Filtering%20for%20Source%20Localization%20in%20Dynamic%20Fields%0AAuthor%3A%20Yiwei%20Shi%20and%20Mengyue%20Yang%20and%20Qi%20Zhang%20and%20Weinan%20Zhang%20and%20Cunjia%20Liu%20and%20Weiru%20Liu%0AAbstract%3A%20%20%20In%20many%20real-world%20scenarios%2C%20such%20as%20gas%20leak%20detection%20or%20environmental%0Apollutant%20tracking%2C%20solving%20the%20Inverse%20Source%20Localization%20and%0ACharacterization%20problem%20involves%20navigating%20complex%2C%20dynamic%20fields%20with%0Asparse%20and%20noisy%20observations.%20Traditional%20methods%20face%20significant%20challenges%2C%0Aincluding%20partial%20observability%2C%20temporal%20and%20spatial%20dynamics%2C%0Aout-of-distribution%20generalization%2C%20and%20reward%20sparsity.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20hierarchical%20framework%20that%20integrates%20Bayesian%20inference%0Aand%20reinforcement%20learning.%20The%20framework%20leverages%20an%20attention-enhanced%0Aparticle%20filtering%20mechanism%20for%20efficient%20and%20accurate%20belief%20updates%2C%20and%0Aincorporates%20two%20complementary%20execution%20strategies%3A%20Attention%20Particle%0AFiltering%20Planning%20and%20Attention%20Particle%20Filtering%20Reinforcement%20Learning.%0AThese%20approaches%20optimize%20exploration%20and%20adaptation%20under%20uncertainty.%0ATheoretical%20analysis%20proves%20the%20convergence%20of%20the%20attention-enhanced%20particle%0Afilter%2C%20while%20extensive%20experiments%20across%20diverse%20scenarios%20validate%20the%0Aframework%27s%20superior%20accuracy%2C%20adaptability%2C%20and%20computational%20efficiency.%20Our%0Aresults%20highlight%20the%20framework%27s%20potential%20for%20broad%20applications%20in%20dynamic%0Afield%20estimation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-Driven%2520Hierarchical%2520Reinforcement%2520Learning%2520with%2520Particle%250A%2520%2520Filtering%2520for%2520Source%2520Localization%2520in%2520Dynamic%2520Fields%26entry.906535625%3DYiwei%2520Shi%2520and%2520Mengyue%2520Yang%2520and%2520Qi%2520Zhang%2520and%2520Weinan%2520Zhang%2520and%2520Cunjia%2520Liu%2520and%2520Weiru%2520Liu%26entry.1292438233%3D%2520%2520In%2520many%2520real-world%2520scenarios%252C%2520such%2520as%2520gas%2520leak%2520detection%2520or%2520environmental%250Apollutant%2520tracking%252C%2520solving%2520the%2520Inverse%2520Source%2520Localization%2520and%250ACharacterization%2520problem%2520involves%2520navigating%2520complex%252C%2520dynamic%2520fields%2520with%250Asparse%2520and%2520noisy%2520observations.%2520Traditional%2520methods%2520face%2520significant%2520challenges%252C%250Aincluding%2520partial%2520observability%252C%2520temporal%2520and%2520spatial%2520dynamics%252C%250Aout-of-distribution%2520generalization%252C%2520and%2520reward%2520sparsity.%2520To%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520hierarchical%2520framework%2520that%2520integrates%2520Bayesian%2520inference%250Aand%2520reinforcement%2520learning.%2520The%2520framework%2520leverages%2520an%2520attention-enhanced%250Aparticle%2520filtering%2520mechanism%2520for%2520efficient%2520and%2520accurate%2520belief%2520updates%252C%2520and%250Aincorporates%2520two%2520complementary%2520execution%2520strategies%253A%2520Attention%2520Particle%250AFiltering%2520Planning%2520and%2520Attention%2520Particle%2520Filtering%2520Reinforcement%2520Learning.%250AThese%2520approaches%2520optimize%2520exploration%2520and%2520adaptation%2520under%2520uncertainty.%250ATheoretical%2520analysis%2520proves%2520the%2520convergence%2520of%2520the%2520attention-enhanced%2520particle%250Afilter%252C%2520while%2520extensive%2520experiments%2520across%2520diverse%2520scenarios%2520validate%2520the%250Aframework%2527s%2520superior%2520accuracy%252C%2520adaptability%252C%2520and%2520computational%2520efficiency.%2520Our%250Aresults%2520highlight%2520the%2520framework%2527s%2520potential%2520for%2520broad%2520applications%2520in%2520dynamic%250Afield%2520estimation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Driven%20Hierarchical%20Reinforcement%20Learning%20with%20Particle%0A%20%20Filtering%20for%20Source%20Localization%20in%20Dynamic%20Fields&entry.906535625=Yiwei%20Shi%20and%20Mengyue%20Yang%20and%20Qi%20Zhang%20and%20Weinan%20Zhang%20and%20Cunjia%20Liu%20and%20Weiru%20Liu&entry.1292438233=%20%20In%20many%20real-world%20scenarios%2C%20such%20as%20gas%20leak%20detection%20or%20environmental%0Apollutant%20tracking%2C%20solving%20the%20Inverse%20Source%20Localization%20and%0ACharacterization%20problem%20involves%20navigating%20complex%2C%20dynamic%20fields%20with%0Asparse%20and%20noisy%20observations.%20Traditional%20methods%20face%20significant%20challenges%2C%0Aincluding%20partial%20observability%2C%20temporal%20and%20spatial%20dynamics%2C%0Aout-of-distribution%20generalization%2C%20and%20reward%20sparsity.%20To%20address%20these%0Aissues%2C%20we%20propose%20a%20hierarchical%20framework%20that%20integrates%20Bayesian%20inference%0Aand%20reinforcement%20learning.%20The%20framework%20leverages%20an%20attention-enhanced%0Aparticle%20filtering%20mechanism%20for%20efficient%20and%20accurate%20belief%20updates%2C%20and%0Aincorporates%20two%20complementary%20execution%20strategies%3A%20Attention%20Particle%0AFiltering%20Planning%20and%20Attention%20Particle%20Filtering%20Reinforcement%20Learning.%0AThese%20approaches%20optimize%20exploration%20and%20adaptation%20under%20uncertainty.%0ATheoretical%20analysis%20proves%20the%20convergence%20of%20the%20attention-enhanced%20particle%0Afilter%2C%20while%20extensive%20experiments%20across%20diverse%20scenarios%20validate%20the%0Aframework%27s%20superior%20accuracy%2C%20adaptability%2C%20and%20computational%20efficiency.%20Our%0Aresults%20highlight%20the%20framework%27s%20potential%20for%20broad%20applications%20in%20dynamic%0Afield%20estimation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13084v1&entry.124074799=Read"},
{"title": "Yi-Lightning Technical Report", "author": "Alan Wake and Bei Chen and C. X. Lv and Chao Li and Chengen Huang and Chenglin Cai and Chujie Zheng and Daniel Cooper and Fan Zhou and Feng Hu and Ge Zhang and Guoyin Wang and Heng Ji and Howard Qiu and Jiangcheng Zhu and Jun Tian and Katherine Su and Lihuan Zhang and Liying Li and Ming Song and Mou Li and Peng Liu and Qicheng Hu and Shawn Wang and Shijun Zhou and Shiming Yang and Shiyong Li and Tianhang Zhu and Wen Xie and Wenhao Huang and Xiang He and Xiaobo Chen and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Yanpeng Li and Yongke Zhao and Yongzhen Luo and Yuchi Xu and Yuxuan Sha and Zhaodong Yan and Zhiyuan Liu and Zirui Zhang and Zonghong Dai", "abstract": "  This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.\n", "link": "http://arxiv.org/abs/2412.01253v5", "date": "2025-01-22", "relevancy": 2.2149, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4458}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4374}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Yi-Lightning%20Technical%20Report&body=Title%3A%20Yi-Lightning%20Technical%20Report%0AAuthor%3A%20Alan%20Wake%20and%20Bei%20Chen%20and%20C.%20X.%20Lv%20and%20Chao%20Li%20and%20Chengen%20Huang%20and%20Chenglin%20Cai%20and%20Chujie%20Zheng%20and%20Daniel%20Cooper%20and%20Fan%20Zhou%20and%20Feng%20Hu%20and%20Ge%20Zhang%20and%20Guoyin%20Wang%20and%20Heng%20Ji%20and%20Howard%20Qiu%20and%20Jiangcheng%20Zhu%20and%20Jun%20Tian%20and%20Katherine%20Su%20and%20Lihuan%20Zhang%20and%20Liying%20Li%20and%20Ming%20Song%20and%20Mou%20Li%20and%20Peng%20Liu%20and%20Qicheng%20Hu%20and%20Shawn%20Wang%20and%20Shijun%20Zhou%20and%20Shiming%20Yang%20and%20Shiyong%20Li%20and%20Tianhang%20Zhu%20and%20Wen%20Xie%20and%20Wenhao%20Huang%20and%20Xiang%20He%20and%20Xiaobo%20Chen%20and%20Xiaohui%20Hu%20and%20Xiaoyi%20Ren%20and%20Xinyao%20Niu%20and%20Yanpeng%20Li%20and%20Yongke%20Zhao%20and%20Yongzhen%20Luo%20and%20Yuchi%20Xu%20and%20Yuxuan%20Sha%20and%20Zhaodong%20Yan%20and%20Zhiyuan%20Liu%20and%20Zirui%20Zhang%20and%20Zonghong%20Dai%0AAbstract%3A%20%20%20This%20technical%20report%20presents%20Yi-Lightning%2C%20our%20latest%20flagship%20large%0Alanguage%20model%20%28LLM%29.%20It%20achieves%20exceptional%20performance%2C%20ranking%206th%20overall%0Aon%20Chatbot%20Arena%2C%20with%20particularly%20strong%20results%20%282nd%20to%204th%20place%29%20in%0Aspecialized%20categories%20including%20Chinese%2C%20Math%2C%20Coding%2C%20and%20Hard%20Prompts.%0AYi-Lightning%20leverages%20an%20enhanced%20Mixture-of-Experts%20%28MoE%29%20architecture%2C%0Afeaturing%20advanced%20expert%20segmentation%20and%20routing%20mechanisms%20coupled%20with%0Aoptimized%20KV-caching%20techniques.%20Our%20development%20process%20encompasses%0Acomprehensive%20pre-training%2C%20supervised%20fine-tuning%20%28SFT%29%2C%20and%20reinforcement%0Alearning%20from%20human%20feedback%20%28RLHF%29%2C%20where%20we%20devise%20deliberate%20strategies%20for%0Amulti-stage%20training%2C%20synthetic%20data%20construction%2C%20and%20reward%20modeling.%0AFurthermore%2C%20we%20implement%20RAISE%20%28Responsible%20AI%20Safety%20Engine%29%2C%20a%0Afour-component%20framework%20to%20address%20safety%20issues%20across%20pre-training%2C%0Apost-training%2C%20and%20serving%20phases.%20Empowered%20by%20our%20scalable%20super-computing%0Ainfrastructure%2C%20all%20these%20innovations%20substantially%20reduce%20training%2C%20deployment%0Aand%20inference%20costs%20while%20maintaining%20high-performance%20standards.%20With%20further%0Aevaluations%20on%20public%20academic%20benchmarks%2C%20Yi-Lightning%20demonstrates%0Acompetitive%20performance%20against%20top-tier%20LLMs%2C%20while%20we%20observe%20a%20notable%0Adisparity%20between%20traditional%2C%20static%20benchmark%20results%20and%20real-world%2C%20dynamic%0Ahuman%20preferences.%20This%20observation%20prompts%20a%20critical%20reassessment%20of%0Aconventional%20benchmarks%27%20utility%20in%20guiding%20the%20development%20of%20more%20intelligent%0Aand%20powerful%20AI%20systems%20for%20practical%20applications.%20Yi-Lightning%20is%20now%0Aavailable%20through%20our%20developer%20platform%20at%20https%3A//platform.lingyiwanwu.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.01253v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYi-Lightning%2520Technical%2520Report%26entry.906535625%3DAlan%2520Wake%2520and%2520Bei%2520Chen%2520and%2520C.%2520X.%2520Lv%2520and%2520Chao%2520Li%2520and%2520Chengen%2520Huang%2520and%2520Chenglin%2520Cai%2520and%2520Chujie%2520Zheng%2520and%2520Daniel%2520Cooper%2520and%2520Fan%2520Zhou%2520and%2520Feng%2520Hu%2520and%2520Ge%2520Zhang%2520and%2520Guoyin%2520Wang%2520and%2520Heng%2520Ji%2520and%2520Howard%2520Qiu%2520and%2520Jiangcheng%2520Zhu%2520and%2520Jun%2520Tian%2520and%2520Katherine%2520Su%2520and%2520Lihuan%2520Zhang%2520and%2520Liying%2520Li%2520and%2520Ming%2520Song%2520and%2520Mou%2520Li%2520and%2520Peng%2520Liu%2520and%2520Qicheng%2520Hu%2520and%2520Shawn%2520Wang%2520and%2520Shijun%2520Zhou%2520and%2520Shiming%2520Yang%2520and%2520Shiyong%2520Li%2520and%2520Tianhang%2520Zhu%2520and%2520Wen%2520Xie%2520and%2520Wenhao%2520Huang%2520and%2520Xiang%2520He%2520and%2520Xiaobo%2520Chen%2520and%2520Xiaohui%2520Hu%2520and%2520Xiaoyi%2520Ren%2520and%2520Xinyao%2520Niu%2520and%2520Yanpeng%2520Li%2520and%2520Yongke%2520Zhao%2520and%2520Yongzhen%2520Luo%2520and%2520Yuchi%2520Xu%2520and%2520Yuxuan%2520Sha%2520and%2520Zhaodong%2520Yan%2520and%2520Zhiyuan%2520Liu%2520and%2520Zirui%2520Zhang%2520and%2520Zonghong%2520Dai%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520presents%2520Yi-Lightning%252C%2520our%2520latest%2520flagship%2520large%250Alanguage%2520model%2520%2528LLM%2529.%2520It%2520achieves%2520exceptional%2520performance%252C%2520ranking%25206th%2520overall%250Aon%2520Chatbot%2520Arena%252C%2520with%2520particularly%2520strong%2520results%2520%25282nd%2520to%25204th%2520place%2529%2520in%250Aspecialized%2520categories%2520including%2520Chinese%252C%2520Math%252C%2520Coding%252C%2520and%2520Hard%2520Prompts.%250AYi-Lightning%2520leverages%2520an%2520enhanced%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%252C%250Afeaturing%2520advanced%2520expert%2520segmentation%2520and%2520routing%2520mechanisms%2520coupled%2520with%250Aoptimized%2520KV-caching%2520techniques.%2520Our%2520development%2520process%2520encompasses%250Acomprehensive%2520pre-training%252C%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520and%2520reinforcement%250Alearning%2520from%2520human%2520feedback%2520%2528RLHF%2529%252C%2520where%2520we%2520devise%2520deliberate%2520strategies%2520for%250Amulti-stage%2520training%252C%2520synthetic%2520data%2520construction%252C%2520and%2520reward%2520modeling.%250AFurthermore%252C%2520we%2520implement%2520RAISE%2520%2528Responsible%2520AI%2520Safety%2520Engine%2529%252C%2520a%250Afour-component%2520framework%2520to%2520address%2520safety%2520issues%2520across%2520pre-training%252C%250Apost-training%252C%2520and%2520serving%2520phases.%2520Empowered%2520by%2520our%2520scalable%2520super-computing%250Ainfrastructure%252C%2520all%2520these%2520innovations%2520substantially%2520reduce%2520training%252C%2520deployment%250Aand%2520inference%2520costs%2520while%2520maintaining%2520high-performance%2520standards.%2520With%2520further%250Aevaluations%2520on%2520public%2520academic%2520benchmarks%252C%2520Yi-Lightning%2520demonstrates%250Acompetitive%2520performance%2520against%2520top-tier%2520LLMs%252C%2520while%2520we%2520observe%2520a%2520notable%250Adisparity%2520between%2520traditional%252C%2520static%2520benchmark%2520results%2520and%2520real-world%252C%2520dynamic%250Ahuman%2520preferences.%2520This%2520observation%2520prompts%2520a%2520critical%2520reassessment%2520of%250Aconventional%2520benchmarks%2527%2520utility%2520in%2520guiding%2520the%2520development%2520of%2520more%2520intelligent%250Aand%2520powerful%2520AI%2520systems%2520for%2520practical%2520applications.%2520Yi-Lightning%2520is%2520now%250Aavailable%2520through%2520our%2520developer%2520platform%2520at%2520https%253A//platform.lingyiwanwu.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.01253v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Yi-Lightning%20Technical%20Report&entry.906535625=Alan%20Wake%20and%20Bei%20Chen%20and%20C.%20X.%20Lv%20and%20Chao%20Li%20and%20Chengen%20Huang%20and%20Chenglin%20Cai%20and%20Chujie%20Zheng%20and%20Daniel%20Cooper%20and%20Fan%20Zhou%20and%20Feng%20Hu%20and%20Ge%20Zhang%20and%20Guoyin%20Wang%20and%20Heng%20Ji%20and%20Howard%20Qiu%20and%20Jiangcheng%20Zhu%20and%20Jun%20Tian%20and%20Katherine%20Su%20and%20Lihuan%20Zhang%20and%20Liying%20Li%20and%20Ming%20Song%20and%20Mou%20Li%20and%20Peng%20Liu%20and%20Qicheng%20Hu%20and%20Shawn%20Wang%20and%20Shijun%20Zhou%20and%20Shiming%20Yang%20and%20Shiyong%20Li%20and%20Tianhang%20Zhu%20and%20Wen%20Xie%20and%20Wenhao%20Huang%20and%20Xiang%20He%20and%20Xiaobo%20Chen%20and%20Xiaohui%20Hu%20and%20Xiaoyi%20Ren%20and%20Xinyao%20Niu%20and%20Yanpeng%20Li%20and%20Yongke%20Zhao%20and%20Yongzhen%20Luo%20and%20Yuchi%20Xu%20and%20Yuxuan%20Sha%20and%20Zhaodong%20Yan%20and%20Zhiyuan%20Liu%20and%20Zirui%20Zhang%20and%20Zonghong%20Dai&entry.1292438233=%20%20This%20technical%20report%20presents%20Yi-Lightning%2C%20our%20latest%20flagship%20large%0Alanguage%20model%20%28LLM%29.%20It%20achieves%20exceptional%20performance%2C%20ranking%206th%20overall%0Aon%20Chatbot%20Arena%2C%20with%20particularly%20strong%20results%20%282nd%20to%204th%20place%29%20in%0Aspecialized%20categories%20including%20Chinese%2C%20Math%2C%20Coding%2C%20and%20Hard%20Prompts.%0AYi-Lightning%20leverages%20an%20enhanced%20Mixture-of-Experts%20%28MoE%29%20architecture%2C%0Afeaturing%20advanced%20expert%20segmentation%20and%20routing%20mechanisms%20coupled%20with%0Aoptimized%20KV-caching%20techniques.%20Our%20development%20process%20encompasses%0Acomprehensive%20pre-training%2C%20supervised%20fine-tuning%20%28SFT%29%2C%20and%20reinforcement%0Alearning%20from%20human%20feedback%20%28RLHF%29%2C%20where%20we%20devise%20deliberate%20strategies%20for%0Amulti-stage%20training%2C%20synthetic%20data%20construction%2C%20and%20reward%20modeling.%0AFurthermore%2C%20we%20implement%20RAISE%20%28Responsible%20AI%20Safety%20Engine%29%2C%20a%0Afour-component%20framework%20to%20address%20safety%20issues%20across%20pre-training%2C%0Apost-training%2C%20and%20serving%20phases.%20Empowered%20by%20our%20scalable%20super-computing%0Ainfrastructure%2C%20all%20these%20innovations%20substantially%20reduce%20training%2C%20deployment%0Aand%20inference%20costs%20while%20maintaining%20high-performance%20standards.%20With%20further%0Aevaluations%20on%20public%20academic%20benchmarks%2C%20Yi-Lightning%20demonstrates%0Acompetitive%20performance%20against%20top-tier%20LLMs%2C%20while%20we%20observe%20a%20notable%0Adisparity%20between%20traditional%2C%20static%20benchmark%20results%20and%20real-world%2C%20dynamic%0Ahuman%20preferences.%20This%20observation%20prompts%20a%20critical%20reassessment%20of%0Aconventional%20benchmarks%27%20utility%20in%20guiding%20the%20development%20of%20more%20intelligent%0Aand%20powerful%20AI%20systems%20for%20practical%20applications.%20Yi-Lightning%20is%20now%0Aavailable%20through%20our%20developer%20platform%20at%20https%3A//platform.lingyiwanwu.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.01253v5&entry.124074799=Read"},
{"title": "Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition", "author": "Juefei Chen and Longxiu Huang and Yimin Wei", "abstract": "  Nonnegative Matrix Factorization (NMF) is an important unsupervised learning\nmethod to extract meaningful features from data. To address the NMF problem\nwithin a polynomial time framework, researchers have introduced a separability\nassumption, which has recently evolved into the concept of coseparability. This\nadvancement offers a more efficient core representation for the original data.\nHowever, in the real world, the data is more natural to be represented as a\nmulti-dimensional array, such as images or videos. The NMF's application to\nhigh-dimensional data involves vectorization, which risks losing essential\nmulti-dimensional correlations. To retain these inherent correlations in the\ndata, we turn to tensors (multidimensional arrays) and leverage the tensor\nt-product. This approach extends the coseparable NMF to the tensor setting,\ncreating what we term coseparable Nonnegative Tensor Factorization (NTF). In\nthis work, we provide an alternating index selection method to select the\ncoseparable core. Furthermore, we validate the t-CUR sampling theory and\nintegrate it with the tensor Discrete Empirical Interpolation Method (t-DEIM)\nto introduce an alternative, randomized index selection process. These methods\nhave been tested on both synthetic and facial analysis datasets. The results\ndemonstrate the efficiency of coseparable NTF when compared to coseparable NMF.\n", "link": "http://arxiv.org/abs/2401.16836v3", "date": "2025-01-22", "relevancy": 2.2091, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4556}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4356}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coseparable%20Nonnegative%20Tensor%20Factorization%20With%20T-CUR%20Decomposition&body=Title%3A%20Coseparable%20Nonnegative%20Tensor%20Factorization%20With%20T-CUR%20Decomposition%0AAuthor%3A%20Juefei%20Chen%20and%20Longxiu%20Huang%20and%20Yimin%20Wei%0AAbstract%3A%20%20%20Nonnegative%20Matrix%20Factorization%20%28NMF%29%20is%20an%20important%20unsupervised%20learning%0Amethod%20to%20extract%20meaningful%20features%20from%20data.%20To%20address%20the%20NMF%20problem%0Awithin%20a%20polynomial%20time%20framework%2C%20researchers%20have%20introduced%20a%20separability%0Aassumption%2C%20which%20has%20recently%20evolved%20into%20the%20concept%20of%20coseparability.%20This%0Aadvancement%20offers%20a%20more%20efficient%20core%20representation%20for%20the%20original%20data.%0AHowever%2C%20in%20the%20real%20world%2C%20the%20data%20is%20more%20natural%20to%20be%20represented%20as%20a%0Amulti-dimensional%20array%2C%20such%20as%20images%20or%20videos.%20The%20NMF%27s%20application%20to%0Ahigh-dimensional%20data%20involves%20vectorization%2C%20which%20risks%20losing%20essential%0Amulti-dimensional%20correlations.%20To%20retain%20these%20inherent%20correlations%20in%20the%0Adata%2C%20we%20turn%20to%20tensors%20%28multidimensional%20arrays%29%20and%20leverage%20the%20tensor%0At-product.%20This%20approach%20extends%20the%20coseparable%20NMF%20to%20the%20tensor%20setting%2C%0Acreating%20what%20we%20term%20coseparable%20Nonnegative%20Tensor%20Factorization%20%28NTF%29.%20In%0Athis%20work%2C%20we%20provide%20an%20alternating%20index%20selection%20method%20to%20select%20the%0Acoseparable%20core.%20Furthermore%2C%20we%20validate%20the%20t-CUR%20sampling%20theory%20and%0Aintegrate%20it%20with%20the%20tensor%20Discrete%20Empirical%20Interpolation%20Method%20%28t-DEIM%29%0Ato%20introduce%20an%20alternative%2C%20randomized%20index%20selection%20process.%20These%20methods%0Ahave%20been%20tested%20on%20both%20synthetic%20and%20facial%20analysis%20datasets.%20The%20results%0Ademonstrate%20the%20efficiency%20of%20coseparable%20NTF%20when%20compared%20to%20coseparable%20NMF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16836v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoseparable%2520Nonnegative%2520Tensor%2520Factorization%2520With%2520T-CUR%2520Decomposition%26entry.906535625%3DJuefei%2520Chen%2520and%2520Longxiu%2520Huang%2520and%2520Yimin%2520Wei%26entry.1292438233%3D%2520%2520Nonnegative%2520Matrix%2520Factorization%2520%2528NMF%2529%2520is%2520an%2520important%2520unsupervised%2520learning%250Amethod%2520to%2520extract%2520meaningful%2520features%2520from%2520data.%2520To%2520address%2520the%2520NMF%2520problem%250Awithin%2520a%2520polynomial%2520time%2520framework%252C%2520researchers%2520have%2520introduced%2520a%2520separability%250Aassumption%252C%2520which%2520has%2520recently%2520evolved%2520into%2520the%2520concept%2520of%2520coseparability.%2520This%250Aadvancement%2520offers%2520a%2520more%2520efficient%2520core%2520representation%2520for%2520the%2520original%2520data.%250AHowever%252C%2520in%2520the%2520real%2520world%252C%2520the%2520data%2520is%2520more%2520natural%2520to%2520be%2520represented%2520as%2520a%250Amulti-dimensional%2520array%252C%2520such%2520as%2520images%2520or%2520videos.%2520The%2520NMF%2527s%2520application%2520to%250Ahigh-dimensional%2520data%2520involves%2520vectorization%252C%2520which%2520risks%2520losing%2520essential%250Amulti-dimensional%2520correlations.%2520To%2520retain%2520these%2520inherent%2520correlations%2520in%2520the%250Adata%252C%2520we%2520turn%2520to%2520tensors%2520%2528multidimensional%2520arrays%2529%2520and%2520leverage%2520the%2520tensor%250At-product.%2520This%2520approach%2520extends%2520the%2520coseparable%2520NMF%2520to%2520the%2520tensor%2520setting%252C%250Acreating%2520what%2520we%2520term%2520coseparable%2520Nonnegative%2520Tensor%2520Factorization%2520%2528NTF%2529.%2520In%250Athis%2520work%252C%2520we%2520provide%2520an%2520alternating%2520index%2520selection%2520method%2520to%2520select%2520the%250Acoseparable%2520core.%2520Furthermore%252C%2520we%2520validate%2520the%2520t-CUR%2520sampling%2520theory%2520and%250Aintegrate%2520it%2520with%2520the%2520tensor%2520Discrete%2520Empirical%2520Interpolation%2520Method%2520%2528t-DEIM%2529%250Ato%2520introduce%2520an%2520alternative%252C%2520randomized%2520index%2520selection%2520process.%2520These%2520methods%250Ahave%2520been%2520tested%2520on%2520both%2520synthetic%2520and%2520facial%2520analysis%2520datasets.%2520The%2520results%250Ademonstrate%2520the%2520efficiency%2520of%2520coseparable%2520NTF%2520when%2520compared%2520to%2520coseparable%2520NMF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16836v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coseparable%20Nonnegative%20Tensor%20Factorization%20With%20T-CUR%20Decomposition&entry.906535625=Juefei%20Chen%20and%20Longxiu%20Huang%20and%20Yimin%20Wei&entry.1292438233=%20%20Nonnegative%20Matrix%20Factorization%20%28NMF%29%20is%20an%20important%20unsupervised%20learning%0Amethod%20to%20extract%20meaningful%20features%20from%20data.%20To%20address%20the%20NMF%20problem%0Awithin%20a%20polynomial%20time%20framework%2C%20researchers%20have%20introduced%20a%20separability%0Aassumption%2C%20which%20has%20recently%20evolved%20into%20the%20concept%20of%20coseparability.%20This%0Aadvancement%20offers%20a%20more%20efficient%20core%20representation%20for%20the%20original%20data.%0AHowever%2C%20in%20the%20real%20world%2C%20the%20data%20is%20more%20natural%20to%20be%20represented%20as%20a%0Amulti-dimensional%20array%2C%20such%20as%20images%20or%20videos.%20The%20NMF%27s%20application%20to%0Ahigh-dimensional%20data%20involves%20vectorization%2C%20which%20risks%20losing%20essential%0Amulti-dimensional%20correlations.%20To%20retain%20these%20inherent%20correlations%20in%20the%0Adata%2C%20we%20turn%20to%20tensors%20%28multidimensional%20arrays%29%20and%20leverage%20the%20tensor%0At-product.%20This%20approach%20extends%20the%20coseparable%20NMF%20to%20the%20tensor%20setting%2C%0Acreating%20what%20we%20term%20coseparable%20Nonnegative%20Tensor%20Factorization%20%28NTF%29.%20In%0Athis%20work%2C%20we%20provide%20an%20alternating%20index%20selection%20method%20to%20select%20the%0Acoseparable%20core.%20Furthermore%2C%20we%20validate%20the%20t-CUR%20sampling%20theory%20and%0Aintegrate%20it%20with%20the%20tensor%20Discrete%20Empirical%20Interpolation%20Method%20%28t-DEIM%29%0Ato%20introduce%20an%20alternative%2C%20randomized%20index%20selection%20process.%20These%20methods%0Ahave%20been%20tested%20on%20both%20synthetic%20and%20facial%20analysis%20datasets.%20The%20results%0Ademonstrate%20the%20efficiency%20of%20coseparable%20NTF%20when%20compared%20to%20coseparable%20NMF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16836v3&entry.124074799=Read"},
{"title": "Enhancing Monocular Depth Estimation with Multi-Source Auxiliary Tasks", "author": "Alessio Quercia and Erenus Yildiz and Zhuo Cao and Kai Krajsek and Abigail Morrison and Ira Assent and Hanno Scharr", "abstract": "  Monocular depth estimation (MDE) is a challenging task in computer vision,\noften hindered by the cost and scarcity of high-quality labeled datasets. We\ntackle this challenge using auxiliary datasets from related vision tasks for an\nalternating training scheme with a shared decoder built on top of a pre-trained\nvision foundation model, while giving a higher weight to MDE. Through extensive\nexperiments we demonstrate the benefits of incorporating various in-domain\nauxiliary datasets and tasks to improve MDE quality on average by ~11%. Our\nexperimental analysis shows that auxiliary tasks have different impacts,\nconfirming the importance of task selection, highlighting that quality gains\nare not achieved by merely adding data. Remarkably, our study reveals that\nusing semantic segmentation datasets as Multi-Label Dense Classification (MLDC)\noften results in additional quality gains. Lastly, our method significantly\nimproves the data efficiency for the considered MDE datasets, enhancing their\nquality while reducing their size by at least 80%. This paves the way for using\nauxiliary data from related tasks to improve MDE quality despite limited\navailability of high-quality labeled data. Code is available at\nhttps://jugit.fz-juelich.de/ias-8/mdeaux.\n", "link": "http://arxiv.org/abs/2501.12824v1", "date": "2025-01-22", "relevancy": 2.1992, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Monocular%20Depth%20Estimation%20with%20Multi-Source%20Auxiliary%20Tasks&body=Title%3A%20Enhancing%20Monocular%20Depth%20Estimation%20with%20Multi-Source%20Auxiliary%20Tasks%0AAuthor%3A%20Alessio%20Quercia%20and%20Erenus%20Yildiz%20and%20Zhuo%20Cao%20and%20Kai%20Krajsek%20and%20Abigail%20Morrison%20and%20Ira%20Assent%20and%20Hanno%20Scharr%0AAbstract%3A%20%20%20Monocular%20depth%20estimation%20%28MDE%29%20is%20a%20challenging%20task%20in%20computer%20vision%2C%0Aoften%20hindered%20by%20the%20cost%20and%20scarcity%20of%20high-quality%20labeled%20datasets.%20We%0Atackle%20this%20challenge%20using%20auxiliary%20datasets%20from%20related%20vision%20tasks%20for%20an%0Aalternating%20training%20scheme%20with%20a%20shared%20decoder%20built%20on%20top%20of%20a%20pre-trained%0Avision%20foundation%20model%2C%20while%20giving%20a%20higher%20weight%20to%20MDE.%20Through%20extensive%0Aexperiments%20we%20demonstrate%20the%20benefits%20of%20incorporating%20various%20in-domain%0Aauxiliary%20datasets%20and%20tasks%20to%20improve%20MDE%20quality%20on%20average%20by%20~11%25.%20Our%0Aexperimental%20analysis%20shows%20that%20auxiliary%20tasks%20have%20different%20impacts%2C%0Aconfirming%20the%20importance%20of%20task%20selection%2C%20highlighting%20that%20quality%20gains%0Aare%20not%20achieved%20by%20merely%20adding%20data.%20Remarkably%2C%20our%20study%20reveals%20that%0Ausing%20semantic%20segmentation%20datasets%20as%20Multi-Label%20Dense%20Classification%20%28MLDC%29%0Aoften%20results%20in%20additional%20quality%20gains.%20Lastly%2C%20our%20method%20significantly%0Aimproves%20the%20data%20efficiency%20for%20the%20considered%20MDE%20datasets%2C%20enhancing%20their%0Aquality%20while%20reducing%20their%20size%20by%20at%20least%2080%25.%20This%20paves%20the%20way%20for%20using%0Aauxiliary%20data%20from%20related%20tasks%20to%20improve%20MDE%20quality%20despite%20limited%0Aavailability%20of%20high-quality%20labeled%20data.%20Code%20is%20available%20at%0Ahttps%3A//jugit.fz-juelich.de/ias-8/mdeaux.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Monocular%2520Depth%2520Estimation%2520with%2520Multi-Source%2520Auxiliary%2520Tasks%26entry.906535625%3DAlessio%2520Quercia%2520and%2520Erenus%2520Yildiz%2520and%2520Zhuo%2520Cao%2520and%2520Kai%2520Krajsek%2520and%2520Abigail%2520Morrison%2520and%2520Ira%2520Assent%2520and%2520Hanno%2520Scharr%26entry.1292438233%3D%2520%2520Monocular%2520depth%2520estimation%2520%2528MDE%2529%2520is%2520a%2520challenging%2520task%2520in%2520computer%2520vision%252C%250Aoften%2520hindered%2520by%2520the%2520cost%2520and%2520scarcity%2520of%2520high-quality%2520labeled%2520datasets.%2520We%250Atackle%2520this%2520challenge%2520using%2520auxiliary%2520datasets%2520from%2520related%2520vision%2520tasks%2520for%2520an%250Aalternating%2520training%2520scheme%2520with%2520a%2520shared%2520decoder%2520built%2520on%2520top%2520of%2520a%2520pre-trained%250Avision%2520foundation%2520model%252C%2520while%2520giving%2520a%2520higher%2520weight%2520to%2520MDE.%2520Through%2520extensive%250Aexperiments%2520we%2520demonstrate%2520the%2520benefits%2520of%2520incorporating%2520various%2520in-domain%250Aauxiliary%2520datasets%2520and%2520tasks%2520to%2520improve%2520MDE%2520quality%2520on%2520average%2520by%2520~11%2525.%2520Our%250Aexperimental%2520analysis%2520shows%2520that%2520auxiliary%2520tasks%2520have%2520different%2520impacts%252C%250Aconfirming%2520the%2520importance%2520of%2520task%2520selection%252C%2520highlighting%2520that%2520quality%2520gains%250Aare%2520not%2520achieved%2520by%2520merely%2520adding%2520data.%2520Remarkably%252C%2520our%2520study%2520reveals%2520that%250Ausing%2520semantic%2520segmentation%2520datasets%2520as%2520Multi-Label%2520Dense%2520Classification%2520%2528MLDC%2529%250Aoften%2520results%2520in%2520additional%2520quality%2520gains.%2520Lastly%252C%2520our%2520method%2520significantly%250Aimproves%2520the%2520data%2520efficiency%2520for%2520the%2520considered%2520MDE%2520datasets%252C%2520enhancing%2520their%250Aquality%2520while%2520reducing%2520their%2520size%2520by%2520at%2520least%252080%2525.%2520This%2520paves%2520the%2520way%2520for%2520using%250Aauxiliary%2520data%2520from%2520related%2520tasks%2520to%2520improve%2520MDE%2520quality%2520despite%2520limited%250Aavailability%2520of%2520high-quality%2520labeled%2520data.%2520Code%2520is%2520available%2520at%250Ahttps%253A//jugit.fz-juelich.de/ias-8/mdeaux.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Monocular%20Depth%20Estimation%20with%20Multi-Source%20Auxiliary%20Tasks&entry.906535625=Alessio%20Quercia%20and%20Erenus%20Yildiz%20and%20Zhuo%20Cao%20and%20Kai%20Krajsek%20and%20Abigail%20Morrison%20and%20Ira%20Assent%20and%20Hanno%20Scharr&entry.1292438233=%20%20Monocular%20depth%20estimation%20%28MDE%29%20is%20a%20challenging%20task%20in%20computer%20vision%2C%0Aoften%20hindered%20by%20the%20cost%20and%20scarcity%20of%20high-quality%20labeled%20datasets.%20We%0Atackle%20this%20challenge%20using%20auxiliary%20datasets%20from%20related%20vision%20tasks%20for%20an%0Aalternating%20training%20scheme%20with%20a%20shared%20decoder%20built%20on%20top%20of%20a%20pre-trained%0Avision%20foundation%20model%2C%20while%20giving%20a%20higher%20weight%20to%20MDE.%20Through%20extensive%0Aexperiments%20we%20demonstrate%20the%20benefits%20of%20incorporating%20various%20in-domain%0Aauxiliary%20datasets%20and%20tasks%20to%20improve%20MDE%20quality%20on%20average%20by%20~11%25.%20Our%0Aexperimental%20analysis%20shows%20that%20auxiliary%20tasks%20have%20different%20impacts%2C%0Aconfirming%20the%20importance%20of%20task%20selection%2C%20highlighting%20that%20quality%20gains%0Aare%20not%20achieved%20by%20merely%20adding%20data.%20Remarkably%2C%20our%20study%20reveals%20that%0Ausing%20semantic%20segmentation%20datasets%20as%20Multi-Label%20Dense%20Classification%20%28MLDC%29%0Aoften%20results%20in%20additional%20quality%20gains.%20Lastly%2C%20our%20method%20significantly%0Aimproves%20the%20data%20efficiency%20for%20the%20considered%20MDE%20datasets%2C%20enhancing%20their%0Aquality%20while%20reducing%20their%20size%20by%20at%20least%2080%25.%20This%20paves%20the%20way%20for%20using%0Aauxiliary%20data%20from%20related%20tasks%20to%20improve%20MDE%20quality%20despite%20limited%0Aavailability%20of%20high-quality%20labeled%20data.%20Code%20is%20available%20at%0Ahttps%3A//jugit.fz-juelich.de/ias-8/mdeaux.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12824v1&entry.124074799=Read"},
{"title": "DocTTT: Test-Time Training for Handwritten Document Recognition Using\n  Meta-Auxiliary Learning", "author": "Wenhao Gu and Li Gu and Ziqiang Wang and Ching Yee Suen and Yang Wang", "abstract": "  Despite recent significant advancements in Handwritten Document Recognition\n(HDR), the efficient and accurate recognition of text against complex\nbackgrounds, diverse handwriting styles, and varying document layouts remains a\npractical challenge. Moreover, this issue is seldom addressed in academic\nresearch, particularly in scenarios with minimal annotated data available. In\nthis paper, we introduce the DocTTT framework to address these challenges. The\nkey innovation of our approach is that it uses test-time training to adapt the\nmodel to each specific input during testing. We propose a novel Meta-Auxiliary\nlearning approach that combines Meta-learning and self-supervised Masked\nAutoencoder~(MAE). During testing, we adapt the visual representation\nparameters using a self-supervised MAE loss. During training, we learn the\nmodel parameters using a meta-learning framework, so that the model parameters\nare learned to adapt to a new input effectively. Experimental results show that\nour proposed method significantly outperforms existing state-of-the-art\napproaches on benchmark datasets.\n", "link": "http://arxiv.org/abs/2501.12898v1", "date": "2025-01-22", "relevancy": 2.1983, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5385}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocTTT%3A%20Test-Time%20Training%20for%20Handwritten%20Document%20Recognition%20Using%0A%20%20Meta-Auxiliary%20Learning&body=Title%3A%20DocTTT%3A%20Test-Time%20Training%20for%20Handwritten%20Document%20Recognition%20Using%0A%20%20Meta-Auxiliary%20Learning%0AAuthor%3A%20Wenhao%20Gu%20and%20Li%20Gu%20and%20Ziqiang%20Wang%20and%20Ching%20Yee%20Suen%20and%20Yang%20Wang%0AAbstract%3A%20%20%20Despite%20recent%20significant%20advancements%20in%20Handwritten%20Document%20Recognition%0A%28HDR%29%2C%20the%20efficient%20and%20accurate%20recognition%20of%20text%20against%20complex%0Abackgrounds%2C%20diverse%20handwriting%20styles%2C%20and%20varying%20document%20layouts%20remains%20a%0Apractical%20challenge.%20Moreover%2C%20this%20issue%20is%20seldom%20addressed%20in%20academic%0Aresearch%2C%20particularly%20in%20scenarios%20with%20minimal%20annotated%20data%20available.%20In%0Athis%20paper%2C%20we%20introduce%20the%20DocTTT%20framework%20to%20address%20these%20challenges.%20The%0Akey%20innovation%20of%20our%20approach%20is%20that%20it%20uses%20test-time%20training%20to%20adapt%20the%0Amodel%20to%20each%20specific%20input%20during%20testing.%20We%20propose%20a%20novel%20Meta-Auxiliary%0Alearning%20approach%20that%20combines%20Meta-learning%20and%20self-supervised%20Masked%0AAutoencoder~%28MAE%29.%20During%20testing%2C%20we%20adapt%20the%20visual%20representation%0Aparameters%20using%20a%20self-supervised%20MAE%20loss.%20During%20training%2C%20we%20learn%20the%0Amodel%20parameters%20using%20a%20meta-learning%20framework%2C%20so%20that%20the%20model%20parameters%0Aare%20learned%20to%20adapt%20to%20a%20new%20input%20effectively.%20Experimental%20results%20show%20that%0Aour%20proposed%20method%20significantly%20outperforms%20existing%20state-of-the-art%0Aapproaches%20on%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocTTT%253A%2520Test-Time%2520Training%2520for%2520Handwritten%2520Document%2520Recognition%2520Using%250A%2520%2520Meta-Auxiliary%2520Learning%26entry.906535625%3DWenhao%2520Gu%2520and%2520Li%2520Gu%2520and%2520Ziqiang%2520Wang%2520and%2520Ching%2520Yee%2520Suen%2520and%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520recent%2520significant%2520advancements%2520in%2520Handwritten%2520Document%2520Recognition%250A%2528HDR%2529%252C%2520the%2520efficient%2520and%2520accurate%2520recognition%2520of%2520text%2520against%2520complex%250Abackgrounds%252C%2520diverse%2520handwriting%2520styles%252C%2520and%2520varying%2520document%2520layouts%2520remains%2520a%250Apractical%2520challenge.%2520Moreover%252C%2520this%2520issue%2520is%2520seldom%2520addressed%2520in%2520academic%250Aresearch%252C%2520particularly%2520in%2520scenarios%2520with%2520minimal%2520annotated%2520data%2520available.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520the%2520DocTTT%2520framework%2520to%2520address%2520these%2520challenges.%2520The%250Akey%2520innovation%2520of%2520our%2520approach%2520is%2520that%2520it%2520uses%2520test-time%2520training%2520to%2520adapt%2520the%250Amodel%2520to%2520each%2520specific%2520input%2520during%2520testing.%2520We%2520propose%2520a%2520novel%2520Meta-Auxiliary%250Alearning%2520approach%2520that%2520combines%2520Meta-learning%2520and%2520self-supervised%2520Masked%250AAutoencoder~%2528MAE%2529.%2520During%2520testing%252C%2520we%2520adapt%2520the%2520visual%2520representation%250Aparameters%2520using%2520a%2520self-supervised%2520MAE%2520loss.%2520During%2520training%252C%2520we%2520learn%2520the%250Amodel%2520parameters%2520using%2520a%2520meta-learning%2520framework%252C%2520so%2520that%2520the%2520model%2520parameters%250Aare%2520learned%2520to%2520adapt%2520to%2520a%2520new%2520input%2520effectively.%2520Experimental%2520results%2520show%2520that%250Aour%2520proposed%2520method%2520significantly%2520outperforms%2520existing%2520state-of-the-art%250Aapproaches%2520on%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocTTT%3A%20Test-Time%20Training%20for%20Handwritten%20Document%20Recognition%20Using%0A%20%20Meta-Auxiliary%20Learning&entry.906535625=Wenhao%20Gu%20and%20Li%20Gu%20and%20Ziqiang%20Wang%20and%20Ching%20Yee%20Suen%20and%20Yang%20Wang&entry.1292438233=%20%20Despite%20recent%20significant%20advancements%20in%20Handwritten%20Document%20Recognition%0A%28HDR%29%2C%20the%20efficient%20and%20accurate%20recognition%20of%20text%20against%20complex%0Abackgrounds%2C%20diverse%20handwriting%20styles%2C%20and%20varying%20document%20layouts%20remains%20a%0Apractical%20challenge.%20Moreover%2C%20this%20issue%20is%20seldom%20addressed%20in%20academic%0Aresearch%2C%20particularly%20in%20scenarios%20with%20minimal%20annotated%20data%20available.%20In%0Athis%20paper%2C%20we%20introduce%20the%20DocTTT%20framework%20to%20address%20these%20challenges.%20The%0Akey%20innovation%20of%20our%20approach%20is%20that%20it%20uses%20test-time%20training%20to%20adapt%20the%0Amodel%20to%20each%20specific%20input%20during%20testing.%20We%20propose%20a%20novel%20Meta-Auxiliary%0Alearning%20approach%20that%20combines%20Meta-learning%20and%20self-supervised%20Masked%0AAutoencoder~%28MAE%29.%20During%20testing%2C%20we%20adapt%20the%20visual%20representation%0Aparameters%20using%20a%20self-supervised%20MAE%20loss.%20During%20training%2C%20we%20learn%20the%0Amodel%20parameters%20using%20a%20meta-learning%20framework%2C%20so%20that%20the%20model%20parameters%0Aare%20learned%20to%20adapt%20to%20a%20new%20input%20effectively.%20Experimental%20results%20show%20that%0Aour%20proposed%20method%20significantly%20outperforms%20existing%20state-of-the-art%0Aapproaches%20on%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12898v1&entry.124074799=Read"},
{"title": "UniUIR: Considering Underwater Image Restoration as An All-in-One\n  Learner", "author": "Xu Zhang and Huan Zhang and Guoli Wang and Qian Zhang and Lefei Zhang and Bo Du", "abstract": "  Existing underwater image restoration (UIR) methods generally only handle\ncolor distortion or jointly address color and haze issues, but they often\noverlook the more complex degradations that can occur in underwater scenes. To\naddress this limitation, we propose a Universal Underwater Image Restoration\nmethod, termed as UniUIR, considering the complex scenario of real-world\nunderwater mixed distortions as an all-in-one manner. To decouple\ndegradation-specific issues and explore the inter-correlations among various\ndegradations in UIR task, we designed the Mamba Mixture-of-Experts module. This\nmodule enables each expert to identify distinct types of degradation and\ncollaboratively extract task-specific priors while maintaining global feature\nrepresentation based on linear complexity. Building upon this foundation, to\nenhance degradation representation and address the task conflicts that arise\nwhen handling multiple types of degradation, we introduce the spatial-frequency\nprior generator. This module extracts degradation prior information in both\nspatial and frequency domains, and adaptively selects the most appropriate\ntask-specific prompts based on image content, thereby improving the accuracy of\nimage restoration. Finally, to more effectively address complex,\nregion-dependent distortions in UIR task, we incorporate depth information\nderived from a large-scale pre-trained depth prediction model, thereby enabling\nthe network to perceive and leverage depth variations across different image\nregions to handle localized degradation. Extensive experiments demonstrate that\nUniUIR can produce more attractive results across qualitative and quantitative\ncomparisons, and shows strong generalization than state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2501.12981v1", "date": "2025-01-22", "relevancy": 2.1907, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5774}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5485}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniUIR%3A%20Considering%20Underwater%20Image%20Restoration%20as%20An%20All-in-One%0A%20%20Learner&body=Title%3A%20UniUIR%3A%20Considering%20Underwater%20Image%20Restoration%20as%20An%20All-in-One%0A%20%20Learner%0AAuthor%3A%20Xu%20Zhang%20and%20Huan%20Zhang%20and%20Guoli%20Wang%20and%20Qian%20Zhang%20and%20Lefei%20Zhang%20and%20Bo%20Du%0AAbstract%3A%20%20%20Existing%20underwater%20image%20restoration%20%28UIR%29%20methods%20generally%20only%20handle%0Acolor%20distortion%20or%20jointly%20address%20color%20and%20haze%20issues%2C%20but%20they%20often%0Aoverlook%20the%20more%20complex%20degradations%20that%20can%20occur%20in%20underwater%20scenes.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20Universal%20Underwater%20Image%20Restoration%0Amethod%2C%20termed%20as%20UniUIR%2C%20considering%20the%20complex%20scenario%20of%20real-world%0Aunderwater%20mixed%20distortions%20as%20an%20all-in-one%20manner.%20To%20decouple%0Adegradation-specific%20issues%20and%20explore%20the%20inter-correlations%20among%20various%0Adegradations%20in%20UIR%20task%2C%20we%20designed%20the%20Mamba%20Mixture-of-Experts%20module.%20This%0Amodule%20enables%20each%20expert%20to%20identify%20distinct%20types%20of%20degradation%20and%0Acollaboratively%20extract%20task-specific%20priors%20while%20maintaining%20global%20feature%0Arepresentation%20based%20on%20linear%20complexity.%20Building%20upon%20this%20foundation%2C%20to%0Aenhance%20degradation%20representation%20and%20address%20the%20task%20conflicts%20that%20arise%0Awhen%20handling%20multiple%20types%20of%20degradation%2C%20we%20introduce%20the%20spatial-frequency%0Aprior%20generator.%20This%20module%20extracts%20degradation%20prior%20information%20in%20both%0Aspatial%20and%20frequency%20domains%2C%20and%20adaptively%20selects%20the%20most%20appropriate%0Atask-specific%20prompts%20based%20on%20image%20content%2C%20thereby%20improving%20the%20accuracy%20of%0Aimage%20restoration.%20Finally%2C%20to%20more%20effectively%20address%20complex%2C%0Aregion-dependent%20distortions%20in%20UIR%20task%2C%20we%20incorporate%20depth%20information%0Aderived%20from%20a%20large-scale%20pre-trained%20depth%20prediction%20model%2C%20thereby%20enabling%0Athe%20network%20to%20perceive%20and%20leverage%20depth%20variations%20across%20different%20image%0Aregions%20to%20handle%20localized%20degradation.%20Extensive%20experiments%20demonstrate%20that%0AUniUIR%20can%20produce%20more%20attractive%20results%20across%20qualitative%20and%20quantitative%0Acomparisons%2C%20and%20shows%20strong%20generalization%20than%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniUIR%253A%2520Considering%2520Underwater%2520Image%2520Restoration%2520as%2520An%2520All-in-One%250A%2520%2520Learner%26entry.906535625%3DXu%2520Zhang%2520and%2520Huan%2520Zhang%2520and%2520Guoli%2520Wang%2520and%2520Qian%2520Zhang%2520and%2520Lefei%2520Zhang%2520and%2520Bo%2520Du%26entry.1292438233%3D%2520%2520Existing%2520underwater%2520image%2520restoration%2520%2528UIR%2529%2520methods%2520generally%2520only%2520handle%250Acolor%2520distortion%2520or%2520jointly%2520address%2520color%2520and%2520haze%2520issues%252C%2520but%2520they%2520often%250Aoverlook%2520the%2520more%2520complex%2520degradations%2520that%2520can%2520occur%2520in%2520underwater%2520scenes.%2520To%250Aaddress%2520this%2520limitation%252C%2520we%2520propose%2520a%2520Universal%2520Underwater%2520Image%2520Restoration%250Amethod%252C%2520termed%2520as%2520UniUIR%252C%2520considering%2520the%2520complex%2520scenario%2520of%2520real-world%250Aunderwater%2520mixed%2520distortions%2520as%2520an%2520all-in-one%2520manner.%2520To%2520decouple%250Adegradation-specific%2520issues%2520and%2520explore%2520the%2520inter-correlations%2520among%2520various%250Adegradations%2520in%2520UIR%2520task%252C%2520we%2520designed%2520the%2520Mamba%2520Mixture-of-Experts%2520module.%2520This%250Amodule%2520enables%2520each%2520expert%2520to%2520identify%2520distinct%2520types%2520of%2520degradation%2520and%250Acollaboratively%2520extract%2520task-specific%2520priors%2520while%2520maintaining%2520global%2520feature%250Arepresentation%2520based%2520on%2520linear%2520complexity.%2520Building%2520upon%2520this%2520foundation%252C%2520to%250Aenhance%2520degradation%2520representation%2520and%2520address%2520the%2520task%2520conflicts%2520that%2520arise%250Awhen%2520handling%2520multiple%2520types%2520of%2520degradation%252C%2520we%2520introduce%2520the%2520spatial-frequency%250Aprior%2520generator.%2520This%2520module%2520extracts%2520degradation%2520prior%2520information%2520in%2520both%250Aspatial%2520and%2520frequency%2520domains%252C%2520and%2520adaptively%2520selects%2520the%2520most%2520appropriate%250Atask-specific%2520prompts%2520based%2520on%2520image%2520content%252C%2520thereby%2520improving%2520the%2520accuracy%2520of%250Aimage%2520restoration.%2520Finally%252C%2520to%2520more%2520effectively%2520address%2520complex%252C%250Aregion-dependent%2520distortions%2520in%2520UIR%2520task%252C%2520we%2520incorporate%2520depth%2520information%250Aderived%2520from%2520a%2520large-scale%2520pre-trained%2520depth%2520prediction%2520model%252C%2520thereby%2520enabling%250Athe%2520network%2520to%2520perceive%2520and%2520leverage%2520depth%2520variations%2520across%2520different%2520image%250Aregions%2520to%2520handle%2520localized%2520degradation.%2520Extensive%2520experiments%2520demonstrate%2520that%250AUniUIR%2520can%2520produce%2520more%2520attractive%2520results%2520across%2520qualitative%2520and%2520quantitative%250Acomparisons%252C%2520and%2520shows%2520strong%2520generalization%2520than%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniUIR%3A%20Considering%20Underwater%20Image%20Restoration%20as%20An%20All-in-One%0A%20%20Learner&entry.906535625=Xu%20Zhang%20and%20Huan%20Zhang%20and%20Guoli%20Wang%20and%20Qian%20Zhang%20and%20Lefei%20Zhang%20and%20Bo%20Du&entry.1292438233=%20%20Existing%20underwater%20image%20restoration%20%28UIR%29%20methods%20generally%20only%20handle%0Acolor%20distortion%20or%20jointly%20address%20color%20and%20haze%20issues%2C%20but%20they%20often%0Aoverlook%20the%20more%20complex%20degradations%20that%20can%20occur%20in%20underwater%20scenes.%20To%0Aaddress%20this%20limitation%2C%20we%20propose%20a%20Universal%20Underwater%20Image%20Restoration%0Amethod%2C%20termed%20as%20UniUIR%2C%20considering%20the%20complex%20scenario%20of%20real-world%0Aunderwater%20mixed%20distortions%20as%20an%20all-in-one%20manner.%20To%20decouple%0Adegradation-specific%20issues%20and%20explore%20the%20inter-correlations%20among%20various%0Adegradations%20in%20UIR%20task%2C%20we%20designed%20the%20Mamba%20Mixture-of-Experts%20module.%20This%0Amodule%20enables%20each%20expert%20to%20identify%20distinct%20types%20of%20degradation%20and%0Acollaboratively%20extract%20task-specific%20priors%20while%20maintaining%20global%20feature%0Arepresentation%20based%20on%20linear%20complexity.%20Building%20upon%20this%20foundation%2C%20to%0Aenhance%20degradation%20representation%20and%20address%20the%20task%20conflicts%20that%20arise%0Awhen%20handling%20multiple%20types%20of%20degradation%2C%20we%20introduce%20the%20spatial-frequency%0Aprior%20generator.%20This%20module%20extracts%20degradation%20prior%20information%20in%20both%0Aspatial%20and%20frequency%20domains%2C%20and%20adaptively%20selects%20the%20most%20appropriate%0Atask-specific%20prompts%20based%20on%20image%20content%2C%20thereby%20improving%20the%20accuracy%20of%0Aimage%20restoration.%20Finally%2C%20to%20more%20effectively%20address%20complex%2C%0Aregion-dependent%20distortions%20in%20UIR%20task%2C%20we%20incorporate%20depth%20information%0Aderived%20from%20a%20large-scale%20pre-trained%20depth%20prediction%20model%2C%20thereby%20enabling%0Athe%20network%20to%20perceive%20and%20leverage%20depth%20variations%20across%20different%20image%0Aregions%20to%20handle%20localized%20degradation.%20Extensive%20experiments%20demonstrate%20that%0AUniUIR%20can%20produce%20more%20attractive%20results%20across%20qualitative%20and%20quantitative%0Acomparisons%2C%20and%20shows%20strong%20generalization%20than%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12981v1&entry.124074799=Read"},
{"title": "Deep Learning-Based Image Recovery and Pose Estimation for Resident\n  Space Objects", "author": "Louis Aberdeen and Mark Hansen and Melvyn L. Smith and Lyndon Smith", "abstract": "  As the density of spacecraft in Earth's orbit increases, their recognition,\npose and trajectory identification becomes crucial for averting potential\ncollisions and executing debris removal operations. However, training models\nable to identify a spacecraft and its pose presents a significant challenge due\nto a lack of available image data for model training. This paper puts forth an\ninnovative framework for generating realistic synthetic datasets of Resident\nSpace Object (RSO) imagery. Using the International Space Station (ISS) as a\ntest case, it goes on to combine image regression with image restoration\nmethodologies to estimate pose from blurred images. An analysis of the proposed\nimage recovery and regression techniques was undertaken, providing insights\ninto the performance, potential enhancements and limitations when applied to\nreal imagery of RSOs. The image recovery approach investigated involves first\napplying image deconvolution using an effective point spread function, followed\nby detail object extraction with a U-Net. Interestingly, using only U-Net for\nimage reconstruction the best pose performance was attained, reducing the\naverage Mean Squared Error in image recovery by 97.28% and the average angular\nerror by 71.9%. The successful application of U-Net image restoration combined\nwith the Resnet50 regression network for pose estimation of the International\nSpace Station demonstrates the value of a diverse set of evaluation tools for\neffective solutions to real-world problems such as the analysis of distant\nobjects in Earth's orbit.\n", "link": "http://arxiv.org/abs/2501.13009v1", "date": "2025-01-22", "relevancy": 2.1792, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5479}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5426}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning-Based%20Image%20Recovery%20and%20Pose%20Estimation%20for%20Resident%0A%20%20Space%20Objects&body=Title%3A%20Deep%20Learning-Based%20Image%20Recovery%20and%20Pose%20Estimation%20for%20Resident%0A%20%20Space%20Objects%0AAuthor%3A%20Louis%20Aberdeen%20and%20Mark%20Hansen%20and%20Melvyn%20L.%20Smith%20and%20Lyndon%20Smith%0AAbstract%3A%20%20%20As%20the%20density%20of%20spacecraft%20in%20Earth%27s%20orbit%20increases%2C%20their%20recognition%2C%0Apose%20and%20trajectory%20identification%20becomes%20crucial%20for%20averting%20potential%0Acollisions%20and%20executing%20debris%20removal%20operations.%20However%2C%20training%20models%0Aable%20to%20identify%20a%20spacecraft%20and%20its%20pose%20presents%20a%20significant%20challenge%20due%0Ato%20a%20lack%20of%20available%20image%20data%20for%20model%20training.%20This%20paper%20puts%20forth%20an%0Ainnovative%20framework%20for%20generating%20realistic%20synthetic%20datasets%20of%20Resident%0ASpace%20Object%20%28RSO%29%20imagery.%20Using%20the%20International%20Space%20Station%20%28ISS%29%20as%20a%0Atest%20case%2C%20it%20goes%20on%20to%20combine%20image%20regression%20with%20image%20restoration%0Amethodologies%20to%20estimate%20pose%20from%20blurred%20images.%20An%20analysis%20of%20the%20proposed%0Aimage%20recovery%20and%20regression%20techniques%20was%20undertaken%2C%20providing%20insights%0Ainto%20the%20performance%2C%20potential%20enhancements%20and%20limitations%20when%20applied%20to%0Areal%20imagery%20of%20RSOs.%20The%20image%20recovery%20approach%20investigated%20involves%20first%0Aapplying%20image%20deconvolution%20using%20an%20effective%20point%20spread%20function%2C%20followed%0Aby%20detail%20object%20extraction%20with%20a%20U-Net.%20Interestingly%2C%20using%20only%20U-Net%20for%0Aimage%20reconstruction%20the%20best%20pose%20performance%20was%20attained%2C%20reducing%20the%0Aaverage%20Mean%20Squared%20Error%20in%20image%20recovery%20by%2097.28%25%20and%20the%20average%20angular%0Aerror%20by%2071.9%25.%20The%20successful%20application%20of%20U-Net%20image%20restoration%20combined%0Awith%20the%20Resnet50%20regression%20network%20for%20pose%20estimation%20of%20the%20International%0ASpace%20Station%20demonstrates%20the%20value%20of%20a%20diverse%20set%20of%20evaluation%20tools%20for%0Aeffective%20solutions%20to%20real-world%20problems%20such%20as%20the%20analysis%20of%20distant%0Aobjects%20in%20Earth%27s%20orbit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning-Based%2520Image%2520Recovery%2520and%2520Pose%2520Estimation%2520for%2520Resident%250A%2520%2520Space%2520Objects%26entry.906535625%3DLouis%2520Aberdeen%2520and%2520Mark%2520Hansen%2520and%2520Melvyn%2520L.%2520Smith%2520and%2520Lyndon%2520Smith%26entry.1292438233%3D%2520%2520As%2520the%2520density%2520of%2520spacecraft%2520in%2520Earth%2527s%2520orbit%2520increases%252C%2520their%2520recognition%252C%250Apose%2520and%2520trajectory%2520identification%2520becomes%2520crucial%2520for%2520averting%2520potential%250Acollisions%2520and%2520executing%2520debris%2520removal%2520operations.%2520However%252C%2520training%2520models%250Aable%2520to%2520identify%2520a%2520spacecraft%2520and%2520its%2520pose%2520presents%2520a%2520significant%2520challenge%2520due%250Ato%2520a%2520lack%2520of%2520available%2520image%2520data%2520for%2520model%2520training.%2520This%2520paper%2520puts%2520forth%2520an%250Ainnovative%2520framework%2520for%2520generating%2520realistic%2520synthetic%2520datasets%2520of%2520Resident%250ASpace%2520Object%2520%2528RSO%2529%2520imagery.%2520Using%2520the%2520International%2520Space%2520Station%2520%2528ISS%2529%2520as%2520a%250Atest%2520case%252C%2520it%2520goes%2520on%2520to%2520combine%2520image%2520regression%2520with%2520image%2520restoration%250Amethodologies%2520to%2520estimate%2520pose%2520from%2520blurred%2520images.%2520An%2520analysis%2520of%2520the%2520proposed%250Aimage%2520recovery%2520and%2520regression%2520techniques%2520was%2520undertaken%252C%2520providing%2520insights%250Ainto%2520the%2520performance%252C%2520potential%2520enhancements%2520and%2520limitations%2520when%2520applied%2520to%250Areal%2520imagery%2520of%2520RSOs.%2520The%2520image%2520recovery%2520approach%2520investigated%2520involves%2520first%250Aapplying%2520image%2520deconvolution%2520using%2520an%2520effective%2520point%2520spread%2520function%252C%2520followed%250Aby%2520detail%2520object%2520extraction%2520with%2520a%2520U-Net.%2520Interestingly%252C%2520using%2520only%2520U-Net%2520for%250Aimage%2520reconstruction%2520the%2520best%2520pose%2520performance%2520was%2520attained%252C%2520reducing%2520the%250Aaverage%2520Mean%2520Squared%2520Error%2520in%2520image%2520recovery%2520by%252097.28%2525%2520and%2520the%2520average%2520angular%250Aerror%2520by%252071.9%2525.%2520The%2520successful%2520application%2520of%2520U-Net%2520image%2520restoration%2520combined%250Awith%2520the%2520Resnet50%2520regression%2520network%2520for%2520pose%2520estimation%2520of%2520the%2520International%250ASpace%2520Station%2520demonstrates%2520the%2520value%2520of%2520a%2520diverse%2520set%2520of%2520evaluation%2520tools%2520for%250Aeffective%2520solutions%2520to%2520real-world%2520problems%2520such%2520as%2520the%2520analysis%2520of%2520distant%250Aobjects%2520in%2520Earth%2527s%2520orbit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning-Based%20Image%20Recovery%20and%20Pose%20Estimation%20for%20Resident%0A%20%20Space%20Objects&entry.906535625=Louis%20Aberdeen%20and%20Mark%20Hansen%20and%20Melvyn%20L.%20Smith%20and%20Lyndon%20Smith&entry.1292438233=%20%20As%20the%20density%20of%20spacecraft%20in%20Earth%27s%20orbit%20increases%2C%20their%20recognition%2C%0Apose%20and%20trajectory%20identification%20becomes%20crucial%20for%20averting%20potential%0Acollisions%20and%20executing%20debris%20removal%20operations.%20However%2C%20training%20models%0Aable%20to%20identify%20a%20spacecraft%20and%20its%20pose%20presents%20a%20significant%20challenge%20due%0Ato%20a%20lack%20of%20available%20image%20data%20for%20model%20training.%20This%20paper%20puts%20forth%20an%0Ainnovative%20framework%20for%20generating%20realistic%20synthetic%20datasets%20of%20Resident%0ASpace%20Object%20%28RSO%29%20imagery.%20Using%20the%20International%20Space%20Station%20%28ISS%29%20as%20a%0Atest%20case%2C%20it%20goes%20on%20to%20combine%20image%20regression%20with%20image%20restoration%0Amethodologies%20to%20estimate%20pose%20from%20blurred%20images.%20An%20analysis%20of%20the%20proposed%0Aimage%20recovery%20and%20regression%20techniques%20was%20undertaken%2C%20providing%20insights%0Ainto%20the%20performance%2C%20potential%20enhancements%20and%20limitations%20when%20applied%20to%0Areal%20imagery%20of%20RSOs.%20The%20image%20recovery%20approach%20investigated%20involves%20first%0Aapplying%20image%20deconvolution%20using%20an%20effective%20point%20spread%20function%2C%20followed%0Aby%20detail%20object%20extraction%20with%20a%20U-Net.%20Interestingly%2C%20using%20only%20U-Net%20for%0Aimage%20reconstruction%20the%20best%20pose%20performance%20was%20attained%2C%20reducing%20the%0Aaverage%20Mean%20Squared%20Error%20in%20image%20recovery%20by%2097.28%25%20and%20the%20average%20angular%0Aerror%20by%2071.9%25.%20The%20successful%20application%20of%20U-Net%20image%20restoration%20combined%0Awith%20the%20Resnet50%20regression%20network%20for%20pose%20estimation%20of%20the%20International%0ASpace%20Station%20demonstrates%20the%20value%20of%20a%20diverse%20set%20of%20evaluation%20tools%20for%0Aeffective%20solutions%20to%20real-world%20problems%20such%20as%20the%20analysis%20of%20distant%0Aobjects%20in%20Earth%27s%20orbit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13009v1&entry.124074799=Read"},
{"title": "Int2Planner: An Intention-based Multi-modal Motion Planner for\n  Integrated Prediction and Planning", "author": "Xiaolei Chen and Junchi Yan and Wenlong Liao and Tao He and Pai Peng", "abstract": "  Motion planning is a critical module in autonomous driving, with the primary\nchallenge of uncertainty caused by interactions with other participants. As\nmost previous methods treat prediction and planning as separate tasks, it is\ndifficult to model these interactions. Furthermore, since the route path\nnavigates ego vehicles to a predefined destination, it provides relatively\nstable intentions for ego vehicles and helps constrain uncertainty. On this\nbasis, we construct Int2Planner, an \\textbf{Int}ention-based\n\\textbf{Int}egrated motion \\textbf{Planner} achieves multi-modal planning and\nprediction. Instead of static intention points, Int2Planner utilizes route\nintention points for ego vehicles and generates corresponding planning\ntrajectories for each intention point to facilitate multi-modal planning. The\nexperiments on the private dataset and the public nuPlan benchmark show the\neffectiveness of route intention points, and Int2Planner achieves\nstate-of-the-art performance. We also deploy it in real-world vehicles and have\nconducted autonomous driving for hundreds of kilometers in urban areas. It\nfurther verifies that Int2Planner can continuously interact with the traffic\nenvironment. Code will be avaliable at https://github.com/cxlz/Int2Planner.\n", "link": "http://arxiv.org/abs/2501.12799v1", "date": "2025-01-22", "relevancy": 2.1785, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5591}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5538}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Int2Planner%3A%20An%20Intention-based%20Multi-modal%20Motion%20Planner%20for%0A%20%20Integrated%20Prediction%20and%20Planning&body=Title%3A%20Int2Planner%3A%20An%20Intention-based%20Multi-modal%20Motion%20Planner%20for%0A%20%20Integrated%20Prediction%20and%20Planning%0AAuthor%3A%20Xiaolei%20Chen%20and%20Junchi%20Yan%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng%0AAbstract%3A%20%20%20Motion%20planning%20is%20a%20critical%20module%20in%20autonomous%20driving%2C%20with%20the%20primary%0Achallenge%20of%20uncertainty%20caused%20by%20interactions%20with%20other%20participants.%20As%0Amost%20previous%20methods%20treat%20prediction%20and%20planning%20as%20separate%20tasks%2C%20it%20is%0Adifficult%20to%20model%20these%20interactions.%20Furthermore%2C%20since%20the%20route%20path%0Anavigates%20ego%20vehicles%20to%20a%20predefined%20destination%2C%20it%20provides%20relatively%0Astable%20intentions%20for%20ego%20vehicles%20and%20helps%20constrain%20uncertainty.%20On%20this%0Abasis%2C%20we%20construct%20Int2Planner%2C%20an%20%5Ctextbf%7BInt%7Dention-based%0A%5Ctextbf%7BInt%7Degrated%20motion%20%5Ctextbf%7BPlanner%7D%20achieves%20multi-modal%20planning%20and%0Aprediction.%20Instead%20of%20static%20intention%20points%2C%20Int2Planner%20utilizes%20route%0Aintention%20points%20for%20ego%20vehicles%20and%20generates%20corresponding%20planning%0Atrajectories%20for%20each%20intention%20point%20to%20facilitate%20multi-modal%20planning.%20The%0Aexperiments%20on%20the%20private%20dataset%20and%20the%20public%20nuPlan%20benchmark%20show%20the%0Aeffectiveness%20of%20route%20intention%20points%2C%20and%20Int2Planner%20achieves%0Astate-of-the-art%20performance.%20We%20also%20deploy%20it%20in%20real-world%20vehicles%20and%20have%0Aconducted%20autonomous%20driving%20for%20hundreds%20of%20kilometers%20in%20urban%20areas.%20It%0Afurther%20verifies%20that%20Int2Planner%20can%20continuously%20interact%20with%20the%20traffic%0Aenvironment.%20Code%20will%20be%20avaliable%20at%20https%3A//github.com/cxlz/Int2Planner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInt2Planner%253A%2520An%2520Intention-based%2520Multi-modal%2520Motion%2520Planner%2520for%250A%2520%2520Integrated%2520Prediction%2520and%2520Planning%26entry.906535625%3DXiaolei%2520Chen%2520and%2520Junchi%2520Yan%2520and%2520Wenlong%2520Liao%2520and%2520Tao%2520He%2520and%2520Pai%2520Peng%26entry.1292438233%3D%2520%2520Motion%2520planning%2520is%2520a%2520critical%2520module%2520in%2520autonomous%2520driving%252C%2520with%2520the%2520primary%250Achallenge%2520of%2520uncertainty%2520caused%2520by%2520interactions%2520with%2520other%2520participants.%2520As%250Amost%2520previous%2520methods%2520treat%2520prediction%2520and%2520planning%2520as%2520separate%2520tasks%252C%2520it%2520is%250Adifficult%2520to%2520model%2520these%2520interactions.%2520Furthermore%252C%2520since%2520the%2520route%2520path%250Anavigates%2520ego%2520vehicles%2520to%2520a%2520predefined%2520destination%252C%2520it%2520provides%2520relatively%250Astable%2520intentions%2520for%2520ego%2520vehicles%2520and%2520helps%2520constrain%2520uncertainty.%2520On%2520this%250Abasis%252C%2520we%2520construct%2520Int2Planner%252C%2520an%2520%255Ctextbf%257BInt%257Dention-based%250A%255Ctextbf%257BInt%257Degrated%2520motion%2520%255Ctextbf%257BPlanner%257D%2520achieves%2520multi-modal%2520planning%2520and%250Aprediction.%2520Instead%2520of%2520static%2520intention%2520points%252C%2520Int2Planner%2520utilizes%2520route%250Aintention%2520points%2520for%2520ego%2520vehicles%2520and%2520generates%2520corresponding%2520planning%250Atrajectories%2520for%2520each%2520intention%2520point%2520to%2520facilitate%2520multi-modal%2520planning.%2520The%250Aexperiments%2520on%2520the%2520private%2520dataset%2520and%2520the%2520public%2520nuPlan%2520benchmark%2520show%2520the%250Aeffectiveness%2520of%2520route%2520intention%2520points%252C%2520and%2520Int2Planner%2520achieves%250Astate-of-the-art%2520performance.%2520We%2520also%2520deploy%2520it%2520in%2520real-world%2520vehicles%2520and%2520have%250Aconducted%2520autonomous%2520driving%2520for%2520hundreds%2520of%2520kilometers%2520in%2520urban%2520areas.%2520It%250Afurther%2520verifies%2520that%2520Int2Planner%2520can%2520continuously%2520interact%2520with%2520the%2520traffic%250Aenvironment.%2520Code%2520will%2520be%2520avaliable%2520at%2520https%253A//github.com/cxlz/Int2Planner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Int2Planner%3A%20An%20Intention-based%20Multi-modal%20Motion%20Planner%20for%0A%20%20Integrated%20Prediction%20and%20Planning&entry.906535625=Xiaolei%20Chen%20and%20Junchi%20Yan%20and%20Wenlong%20Liao%20and%20Tao%20He%20and%20Pai%20Peng&entry.1292438233=%20%20Motion%20planning%20is%20a%20critical%20module%20in%20autonomous%20driving%2C%20with%20the%20primary%0Achallenge%20of%20uncertainty%20caused%20by%20interactions%20with%20other%20participants.%20As%0Amost%20previous%20methods%20treat%20prediction%20and%20planning%20as%20separate%20tasks%2C%20it%20is%0Adifficult%20to%20model%20these%20interactions.%20Furthermore%2C%20since%20the%20route%20path%0Anavigates%20ego%20vehicles%20to%20a%20predefined%20destination%2C%20it%20provides%20relatively%0Astable%20intentions%20for%20ego%20vehicles%20and%20helps%20constrain%20uncertainty.%20On%20this%0Abasis%2C%20we%20construct%20Int2Planner%2C%20an%20%5Ctextbf%7BInt%7Dention-based%0A%5Ctextbf%7BInt%7Degrated%20motion%20%5Ctextbf%7BPlanner%7D%20achieves%20multi-modal%20planning%20and%0Aprediction.%20Instead%20of%20static%20intention%20points%2C%20Int2Planner%20utilizes%20route%0Aintention%20points%20for%20ego%20vehicles%20and%20generates%20corresponding%20planning%0Atrajectories%20for%20each%20intention%20point%20to%20facilitate%20multi-modal%20planning.%20The%0Aexperiments%20on%20the%20private%20dataset%20and%20the%20public%20nuPlan%20benchmark%20show%20the%0Aeffectiveness%20of%20route%20intention%20points%2C%20and%20Int2Planner%20achieves%0Astate-of-the-art%20performance.%20We%20also%20deploy%20it%20in%20real-world%20vehicles%20and%20have%0Aconducted%20autonomous%20driving%20for%20hundreds%20of%20kilometers%20in%20urban%20areas.%20It%0Afurther%20verifies%20that%20Int2Planner%20can%20continuously%20interact%20with%20the%20traffic%0Aenvironment.%20Code%20will%20be%20avaliable%20at%20https%3A//github.com/cxlz/Int2Planner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12799v1&entry.124074799=Read"},
{"title": "Theoretical Error Analysis of Entropy Approximation for Gaussian\n  Mixtures", "author": "Takashi Furuya and Hiroyuki Kusumoto and Koichi Taniguchi and Naoya Kanno and Kazuma Suetake", "abstract": "  Gaussian mixture distributions are commonly employed to represent general\nprobability distributions. Despite the importance of using Gaussian mixtures\nfor uncertainty estimation, the entropy of a Gaussian mixture cannot be\ncalculated analytically. In this paper, we study the approximate entropy\nrepresented as the sum of the entropies of unimodal Gaussian distributions with\nmixing coefficients. This approximation is easy to calculate analytically\nregardless of dimension, but there is a lack of theoretical guarantees. We\ntheoretically analyze the approximation error between the true and the\napproximate entropy to reveal when this approximation works effectively. This\nerror is essentially controlled by how far apart each Gaussian component of the\nGaussian mixture is. To measure such separation, we introduce the ratios of the\ndistances between the means to the sum of the variances of each Gaussian\ncomponent of the Gaussian mixture, and we reveal that the error converges to\nzero as the ratios tend to infinity. In addition, the probabilistic estimate\nindicates that this convergence situation is more likely to occur in\nhigher-dimensional spaces. Therefore, our results provide a guarantee that this\napproximation works well for high-dimensional problems, such as neural networks\nthat involve a large number of parameters.\n", "link": "http://arxiv.org/abs/2202.13059v6", "date": "2025-01-22", "relevancy": 2.1677, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4491}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4258}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Error%20Analysis%20of%20Entropy%20Approximation%20for%20Gaussian%0A%20%20Mixtures&body=Title%3A%20Theoretical%20Error%20Analysis%20of%20Entropy%20Approximation%20for%20Gaussian%0A%20%20Mixtures%0AAuthor%3A%20Takashi%20Furuya%20and%20Hiroyuki%20Kusumoto%20and%20Koichi%20Taniguchi%20and%20Naoya%20Kanno%20and%20Kazuma%20Suetake%0AAbstract%3A%20%20%20Gaussian%20mixture%20distributions%20are%20commonly%20employed%20to%20represent%20general%0Aprobability%20distributions.%20Despite%20the%20importance%20of%20using%20Gaussian%20mixtures%0Afor%20uncertainty%20estimation%2C%20the%20entropy%20of%20a%20Gaussian%20mixture%20cannot%20be%0Acalculated%20analytically.%20In%20this%20paper%2C%20we%20study%20the%20approximate%20entropy%0Arepresented%20as%20the%20sum%20of%20the%20entropies%20of%20unimodal%20Gaussian%20distributions%20with%0Amixing%20coefficients.%20This%20approximation%20is%20easy%20to%20calculate%20analytically%0Aregardless%20of%20dimension%2C%20but%20there%20is%20a%20lack%20of%20theoretical%20guarantees.%20We%0Atheoretically%20analyze%20the%20approximation%20error%20between%20the%20true%20and%20the%0Aapproximate%20entropy%20to%20reveal%20when%20this%20approximation%20works%20effectively.%20This%0Aerror%20is%20essentially%20controlled%20by%20how%20far%20apart%20each%20Gaussian%20component%20of%20the%0AGaussian%20mixture%20is.%20To%20measure%20such%20separation%2C%20we%20introduce%20the%20ratios%20of%20the%0Adistances%20between%20the%20means%20to%20the%20sum%20of%20the%20variances%20of%20each%20Gaussian%0Acomponent%20of%20the%20Gaussian%20mixture%2C%20and%20we%20reveal%20that%20the%20error%20converges%20to%0Azero%20as%20the%20ratios%20tend%20to%20infinity.%20In%20addition%2C%20the%20probabilistic%20estimate%0Aindicates%20that%20this%20convergence%20situation%20is%20more%20likely%20to%20occur%20in%0Ahigher-dimensional%20spaces.%20Therefore%2C%20our%20results%20provide%20a%20guarantee%20that%20this%0Aapproximation%20works%20well%20for%20high-dimensional%20problems%2C%20such%20as%20neural%20networks%0Athat%20involve%20a%20large%20number%20of%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.13059v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Error%2520Analysis%2520of%2520Entropy%2520Approximation%2520for%2520Gaussian%250A%2520%2520Mixtures%26entry.906535625%3DTakashi%2520Furuya%2520and%2520Hiroyuki%2520Kusumoto%2520and%2520Koichi%2520Taniguchi%2520and%2520Naoya%2520Kanno%2520and%2520Kazuma%2520Suetake%26entry.1292438233%3D%2520%2520Gaussian%2520mixture%2520distributions%2520are%2520commonly%2520employed%2520to%2520represent%2520general%250Aprobability%2520distributions.%2520Despite%2520the%2520importance%2520of%2520using%2520Gaussian%2520mixtures%250Afor%2520uncertainty%2520estimation%252C%2520the%2520entropy%2520of%2520a%2520Gaussian%2520mixture%2520cannot%2520be%250Acalculated%2520analytically.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520approximate%2520entropy%250Arepresented%2520as%2520the%2520sum%2520of%2520the%2520entropies%2520of%2520unimodal%2520Gaussian%2520distributions%2520with%250Amixing%2520coefficients.%2520This%2520approximation%2520is%2520easy%2520to%2520calculate%2520analytically%250Aregardless%2520of%2520dimension%252C%2520but%2520there%2520is%2520a%2520lack%2520of%2520theoretical%2520guarantees.%2520We%250Atheoretically%2520analyze%2520the%2520approximation%2520error%2520between%2520the%2520true%2520and%2520the%250Aapproximate%2520entropy%2520to%2520reveal%2520when%2520this%2520approximation%2520works%2520effectively.%2520This%250Aerror%2520is%2520essentially%2520controlled%2520by%2520how%2520far%2520apart%2520each%2520Gaussian%2520component%2520of%2520the%250AGaussian%2520mixture%2520is.%2520To%2520measure%2520such%2520separation%252C%2520we%2520introduce%2520the%2520ratios%2520of%2520the%250Adistances%2520between%2520the%2520means%2520to%2520the%2520sum%2520of%2520the%2520variances%2520of%2520each%2520Gaussian%250Acomponent%2520of%2520the%2520Gaussian%2520mixture%252C%2520and%2520we%2520reveal%2520that%2520the%2520error%2520converges%2520to%250Azero%2520as%2520the%2520ratios%2520tend%2520to%2520infinity.%2520In%2520addition%252C%2520the%2520probabilistic%2520estimate%250Aindicates%2520that%2520this%2520convergence%2520situation%2520is%2520more%2520likely%2520to%2520occur%2520in%250Ahigher-dimensional%2520spaces.%2520Therefore%252C%2520our%2520results%2520provide%2520a%2520guarantee%2520that%2520this%250Aapproximation%2520works%2520well%2520for%2520high-dimensional%2520problems%252C%2520such%2520as%2520neural%2520networks%250Athat%2520involve%2520a%2520large%2520number%2520of%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.13059v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Error%20Analysis%20of%20Entropy%20Approximation%20for%20Gaussian%0A%20%20Mixtures&entry.906535625=Takashi%20Furuya%20and%20Hiroyuki%20Kusumoto%20and%20Koichi%20Taniguchi%20and%20Naoya%20Kanno%20and%20Kazuma%20Suetake&entry.1292438233=%20%20Gaussian%20mixture%20distributions%20are%20commonly%20employed%20to%20represent%20general%0Aprobability%20distributions.%20Despite%20the%20importance%20of%20using%20Gaussian%20mixtures%0Afor%20uncertainty%20estimation%2C%20the%20entropy%20of%20a%20Gaussian%20mixture%20cannot%20be%0Acalculated%20analytically.%20In%20this%20paper%2C%20we%20study%20the%20approximate%20entropy%0Arepresented%20as%20the%20sum%20of%20the%20entropies%20of%20unimodal%20Gaussian%20distributions%20with%0Amixing%20coefficients.%20This%20approximation%20is%20easy%20to%20calculate%20analytically%0Aregardless%20of%20dimension%2C%20but%20there%20is%20a%20lack%20of%20theoretical%20guarantees.%20We%0Atheoretically%20analyze%20the%20approximation%20error%20between%20the%20true%20and%20the%0Aapproximate%20entropy%20to%20reveal%20when%20this%20approximation%20works%20effectively.%20This%0Aerror%20is%20essentially%20controlled%20by%20how%20far%20apart%20each%20Gaussian%20component%20of%20the%0AGaussian%20mixture%20is.%20To%20measure%20such%20separation%2C%20we%20introduce%20the%20ratios%20of%20the%0Adistances%20between%20the%20means%20to%20the%20sum%20of%20the%20variances%20of%20each%20Gaussian%0Acomponent%20of%20the%20Gaussian%20mixture%2C%20and%20we%20reveal%20that%20the%20error%20converges%20to%0Azero%20as%20the%20ratios%20tend%20to%20infinity.%20In%20addition%2C%20the%20probabilistic%20estimate%0Aindicates%20that%20this%20convergence%20situation%20is%20more%20likely%20to%20occur%20in%0Ahigher-dimensional%20spaces.%20Therefore%2C%20our%20results%20provide%20a%20guarantee%20that%20this%0Aapproximation%20works%20well%20for%20high-dimensional%20problems%2C%20such%20as%20neural%20networks%0Athat%20involve%20a%20large%20number%20of%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.13059v6&entry.124074799=Read"},
{"title": "Architectural Fusion Through Contextual Partitioning in Large Language\n  Models: A Novel Approach to Parameterized Knowledge Integration", "author": "Offa Kingsleigh and Alfred Abercrombie and David Woolstencroft and Beorhtric Meadowcroft and Marcus Irvin", "abstract": "  Contextual Partitioning introduces an innovative approach to enhancing the\narchitectural design of large-scale computational models through the dynamic\nsegmentation of parameters into context-aware regions. This methodology\nemphasizes the importance of task-specific specialization, achieved through\nadaptive parameter allocation mechanisms that align with the linguistic\nfeatures of input data. Experimental evaluations demonstrated substantial\nimprovements in accuracy, perplexity, and contextual coherence across a variety\nof linguistic tasks, highlighting the adaptability and scalability of the\nproposed framework. By reducing redundancy and enhancing computational\nefficiency, Contextual Partitioning not only streamlines model operations but\nalso expands the scope of applications for advanced language processing\nsystems. The approach operates autonomously, requiring no external fine-tuning,\nthereby addressing a significant limitation in conventional parameter\noptimization techniques. Empirical results demonstrate the effectiveness of\ngradient-driven segmentation, enabling models to dynamically recalibrate and\nspecialize in response to task-specific demands. Furthermore, resource\nutilization metrics reveal notable reductions in memory usage and training\ntimes, confirming the efficiency of the approach. Observations from qualitative\nanalyses illustrate improved contextual coherence and logical flow in generated\noutputs, reinforcing the practical value of this technique. The findings\ncollectively demonstrate the potential for Contextual Partitioning to redefine\nthe scalability and adaptability of computational language architectures in\ndiverse and complex domains.\n", "link": "http://arxiv.org/abs/2501.12901v1", "date": "2025-01-22", "relevancy": 2.1573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5486}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Architectural%20Fusion%20Through%20Contextual%20Partitioning%20in%20Large%20Language%0A%20%20Models%3A%20A%20Novel%20Approach%20to%20Parameterized%20Knowledge%20Integration&body=Title%3A%20Architectural%20Fusion%20Through%20Contextual%20Partitioning%20in%20Large%20Language%0A%20%20Models%3A%20A%20Novel%20Approach%20to%20Parameterized%20Knowledge%20Integration%0AAuthor%3A%20Offa%20Kingsleigh%20and%20Alfred%20Abercrombie%20and%20David%20Woolstencroft%20and%20Beorhtric%20Meadowcroft%20and%20Marcus%20Irvin%0AAbstract%3A%20%20%20Contextual%20Partitioning%20introduces%20an%20innovative%20approach%20to%20enhancing%20the%0Aarchitectural%20design%20of%20large-scale%20computational%20models%20through%20the%20dynamic%0Asegmentation%20of%20parameters%20into%20context-aware%20regions.%20This%20methodology%0Aemphasizes%20the%20importance%20of%20task-specific%20specialization%2C%20achieved%20through%0Aadaptive%20parameter%20allocation%20mechanisms%20that%20align%20with%20the%20linguistic%0Afeatures%20of%20input%20data.%20Experimental%20evaluations%20demonstrated%20substantial%0Aimprovements%20in%20accuracy%2C%20perplexity%2C%20and%20contextual%20coherence%20across%20a%20variety%0Aof%20linguistic%20tasks%2C%20highlighting%20the%20adaptability%20and%20scalability%20of%20the%0Aproposed%20framework.%20By%20reducing%20redundancy%20and%20enhancing%20computational%0Aefficiency%2C%20Contextual%20Partitioning%20not%20only%20streamlines%20model%20operations%20but%0Aalso%20expands%20the%20scope%20of%20applications%20for%20advanced%20language%20processing%0Asystems.%20The%20approach%20operates%20autonomously%2C%20requiring%20no%20external%20fine-tuning%2C%0Athereby%20addressing%20a%20significant%20limitation%20in%20conventional%20parameter%0Aoptimization%20techniques.%20Empirical%20results%20demonstrate%20the%20effectiveness%20of%0Agradient-driven%20segmentation%2C%20enabling%20models%20to%20dynamically%20recalibrate%20and%0Aspecialize%20in%20response%20to%20task-specific%20demands.%20Furthermore%2C%20resource%0Autilization%20metrics%20reveal%20notable%20reductions%20in%20memory%20usage%20and%20training%0Atimes%2C%20confirming%20the%20efficiency%20of%20the%20approach.%20Observations%20from%20qualitative%0Aanalyses%20illustrate%20improved%20contextual%20coherence%20and%20logical%20flow%20in%20generated%0Aoutputs%2C%20reinforcing%20the%20practical%20value%20of%20this%20technique.%20The%20findings%0Acollectively%20demonstrate%20the%20potential%20for%20Contextual%20Partitioning%20to%20redefine%0Athe%20scalability%20and%20adaptability%20of%20computational%20language%20architectures%20in%0Adiverse%20and%20complex%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArchitectural%2520Fusion%2520Through%2520Contextual%2520Partitioning%2520in%2520Large%2520Language%250A%2520%2520Models%253A%2520A%2520Novel%2520Approach%2520to%2520Parameterized%2520Knowledge%2520Integration%26entry.906535625%3DOffa%2520Kingsleigh%2520and%2520Alfred%2520Abercrombie%2520and%2520David%2520Woolstencroft%2520and%2520Beorhtric%2520Meadowcroft%2520and%2520Marcus%2520Irvin%26entry.1292438233%3D%2520%2520Contextual%2520Partitioning%2520introduces%2520an%2520innovative%2520approach%2520to%2520enhancing%2520the%250Aarchitectural%2520design%2520of%2520large-scale%2520computational%2520models%2520through%2520the%2520dynamic%250Asegmentation%2520of%2520parameters%2520into%2520context-aware%2520regions.%2520This%2520methodology%250Aemphasizes%2520the%2520importance%2520of%2520task-specific%2520specialization%252C%2520achieved%2520through%250Aadaptive%2520parameter%2520allocation%2520mechanisms%2520that%2520align%2520with%2520the%2520linguistic%250Afeatures%2520of%2520input%2520data.%2520Experimental%2520evaluations%2520demonstrated%2520substantial%250Aimprovements%2520in%2520accuracy%252C%2520perplexity%252C%2520and%2520contextual%2520coherence%2520across%2520a%2520variety%250Aof%2520linguistic%2520tasks%252C%2520highlighting%2520the%2520adaptability%2520and%2520scalability%2520of%2520the%250Aproposed%2520framework.%2520By%2520reducing%2520redundancy%2520and%2520enhancing%2520computational%250Aefficiency%252C%2520Contextual%2520Partitioning%2520not%2520only%2520streamlines%2520model%2520operations%2520but%250Aalso%2520expands%2520the%2520scope%2520of%2520applications%2520for%2520advanced%2520language%2520processing%250Asystems.%2520The%2520approach%2520operates%2520autonomously%252C%2520requiring%2520no%2520external%2520fine-tuning%252C%250Athereby%2520addressing%2520a%2520significant%2520limitation%2520in%2520conventional%2520parameter%250Aoptimization%2520techniques.%2520Empirical%2520results%2520demonstrate%2520the%2520effectiveness%2520of%250Agradient-driven%2520segmentation%252C%2520enabling%2520models%2520to%2520dynamically%2520recalibrate%2520and%250Aspecialize%2520in%2520response%2520to%2520task-specific%2520demands.%2520Furthermore%252C%2520resource%250Autilization%2520metrics%2520reveal%2520notable%2520reductions%2520in%2520memory%2520usage%2520and%2520training%250Atimes%252C%2520confirming%2520the%2520efficiency%2520of%2520the%2520approach.%2520Observations%2520from%2520qualitative%250Aanalyses%2520illustrate%2520improved%2520contextual%2520coherence%2520and%2520logical%2520flow%2520in%2520generated%250Aoutputs%252C%2520reinforcing%2520the%2520practical%2520value%2520of%2520this%2520technique.%2520The%2520findings%250Acollectively%2520demonstrate%2520the%2520potential%2520for%2520Contextual%2520Partitioning%2520to%2520redefine%250Athe%2520scalability%2520and%2520adaptability%2520of%2520computational%2520language%2520architectures%2520in%250Adiverse%2520and%2520complex%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Architectural%20Fusion%20Through%20Contextual%20Partitioning%20in%20Large%20Language%0A%20%20Models%3A%20A%20Novel%20Approach%20to%20Parameterized%20Knowledge%20Integration&entry.906535625=Offa%20Kingsleigh%20and%20Alfred%20Abercrombie%20and%20David%20Woolstencroft%20and%20Beorhtric%20Meadowcroft%20and%20Marcus%20Irvin&entry.1292438233=%20%20Contextual%20Partitioning%20introduces%20an%20innovative%20approach%20to%20enhancing%20the%0Aarchitectural%20design%20of%20large-scale%20computational%20models%20through%20the%20dynamic%0Asegmentation%20of%20parameters%20into%20context-aware%20regions.%20This%20methodology%0Aemphasizes%20the%20importance%20of%20task-specific%20specialization%2C%20achieved%20through%0Aadaptive%20parameter%20allocation%20mechanisms%20that%20align%20with%20the%20linguistic%0Afeatures%20of%20input%20data.%20Experimental%20evaluations%20demonstrated%20substantial%0Aimprovements%20in%20accuracy%2C%20perplexity%2C%20and%20contextual%20coherence%20across%20a%20variety%0Aof%20linguistic%20tasks%2C%20highlighting%20the%20adaptability%20and%20scalability%20of%20the%0Aproposed%20framework.%20By%20reducing%20redundancy%20and%20enhancing%20computational%0Aefficiency%2C%20Contextual%20Partitioning%20not%20only%20streamlines%20model%20operations%20but%0Aalso%20expands%20the%20scope%20of%20applications%20for%20advanced%20language%20processing%0Asystems.%20The%20approach%20operates%20autonomously%2C%20requiring%20no%20external%20fine-tuning%2C%0Athereby%20addressing%20a%20significant%20limitation%20in%20conventional%20parameter%0Aoptimization%20techniques.%20Empirical%20results%20demonstrate%20the%20effectiveness%20of%0Agradient-driven%20segmentation%2C%20enabling%20models%20to%20dynamically%20recalibrate%20and%0Aspecialize%20in%20response%20to%20task-specific%20demands.%20Furthermore%2C%20resource%0Autilization%20metrics%20reveal%20notable%20reductions%20in%20memory%20usage%20and%20training%0Atimes%2C%20confirming%20the%20efficiency%20of%20the%20approach.%20Observations%20from%20qualitative%0Aanalyses%20illustrate%20improved%20contextual%20coherence%20and%20logical%20flow%20in%20generated%0Aoutputs%2C%20reinforcing%20the%20practical%20value%20of%20this%20technique.%20The%20findings%0Acollectively%20demonstrate%20the%20potential%20for%20Contextual%20Partitioning%20to%20redefine%0Athe%20scalability%20and%20adaptability%20of%20computational%20language%20architectures%20in%0Adiverse%20and%20complex%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12901v1&entry.124074799=Read"},
{"title": "CHaRNet: Conditioned Heatmap Regression for Robust Dental Landmark\n  Localization", "author": "Jos\u00e9 Rodr\u00edguez-Ortega and Siham Tabik", "abstract": "  Identifying anatomical landmarks in 3D dental models is crucial for\northodontic treatment. Manually placing these key points is complex,\ntime-consuming, and requires expert knowledge. While some machine learning\nmethods have been proposed for automatic tooth landmark detection in 3D\nIntraoral Scans (IOS), research remains limited, with no fully end-to-end\napproaches that avoid teeth segmentation.\n  We propose CHaRNet (Conditioned Heatmap Regression Network), the first\nend-to-end deep learning method for tooth landmark detection in 3D IOS. Unlike\ntraditional two-stage methods that segment teeth before detecting landmarks,\nCHaRNet directly detects landmarks on the input point cloud. It consists of\nfour key modules: (1) a point cloud encoder, (2) a point cloud decoder with a\nheatmap regression head, (3) a teeth presence classification head, and (4) the\ninnovative Conditioned Heatmap Regression (CHaR) module. The CHaR module\nrefines landmark regression by leveraging teeth presence classification,\nenabling dynamic adaptation to cases with missing teeth and improving accuracy\nin complex dental models.\n  We evaluate CHaRNet using five point cloud learning algorithms to validate\nthe effectiveness of the CHaR module and test it on a clinical dataset of\n$1,214$ annotated 3D dental models. Both the dataset and code will be publicly\nreleased to address the lack of open datasets in orthodontics, promote\nbenchmarking, and inspire new research.\n  CHaRNet achieves a Mean Euclidean Distance Error (MEDE) of 1.28 mm and a Mean\nSuccess Ratio (MSR) of 82.40\\%, demonstrating robust performance. Notably, it\nexcels in handling irregular dental geometries, such as models with missing\nteeth. This end-to-end approach streamlines orthodontic workflows, improves 3D\nIOS analysis precision, and facilitates efficient computer-assisted treatment\nplanning.\n", "link": "http://arxiv.org/abs/2501.13073v1", "date": "2025-01-22", "relevancy": 2.1456, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5665}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHaRNet%3A%20Conditioned%20Heatmap%20Regression%20for%20Robust%20Dental%20Landmark%0A%20%20Localization&body=Title%3A%20CHaRNet%3A%20Conditioned%20Heatmap%20Regression%20for%20Robust%20Dental%20Landmark%0A%20%20Localization%0AAuthor%3A%20Jos%C3%A9%20Rodr%C3%ADguez-Ortega%20and%20Siham%20Tabik%0AAbstract%3A%20%20%20Identifying%20anatomical%20landmarks%20in%203D%20dental%20models%20is%20crucial%20for%0Aorthodontic%20treatment.%20Manually%20placing%20these%20key%20points%20is%20complex%2C%0Atime-consuming%2C%20and%20requires%20expert%20knowledge.%20While%20some%20machine%20learning%0Amethods%20have%20been%20proposed%20for%20automatic%20tooth%20landmark%20detection%20in%203D%0AIntraoral%20Scans%20%28IOS%29%2C%20research%20remains%20limited%2C%20with%20no%20fully%20end-to-end%0Aapproaches%20that%20avoid%20teeth%20segmentation.%0A%20%20We%20propose%20CHaRNet%20%28Conditioned%20Heatmap%20Regression%20Network%29%2C%20the%20first%0Aend-to-end%20deep%20learning%20method%20for%20tooth%20landmark%20detection%20in%203D%20IOS.%20Unlike%0Atraditional%20two-stage%20methods%20that%20segment%20teeth%20before%20detecting%20landmarks%2C%0ACHaRNet%20directly%20detects%20landmarks%20on%20the%20input%20point%20cloud.%20It%20consists%20of%0Afour%20key%20modules%3A%20%281%29%20a%20point%20cloud%20encoder%2C%20%282%29%20a%20point%20cloud%20decoder%20with%20a%0Aheatmap%20regression%20head%2C%20%283%29%20a%20teeth%20presence%20classification%20head%2C%20and%20%284%29%20the%0Ainnovative%20Conditioned%20Heatmap%20Regression%20%28CHaR%29%20module.%20The%20CHaR%20module%0Arefines%20landmark%20regression%20by%20leveraging%20teeth%20presence%20classification%2C%0Aenabling%20dynamic%20adaptation%20to%20cases%20with%20missing%20teeth%20and%20improving%20accuracy%0Ain%20complex%20dental%20models.%0A%20%20We%20evaluate%20CHaRNet%20using%20five%20point%20cloud%20learning%20algorithms%20to%20validate%0Athe%20effectiveness%20of%20the%20CHaR%20module%20and%20test%20it%20on%20a%20clinical%20dataset%20of%0A%241%2C214%24%20annotated%203D%20dental%20models.%20Both%20the%20dataset%20and%20code%20will%20be%20publicly%0Areleased%20to%20address%20the%20lack%20of%20open%20datasets%20in%20orthodontics%2C%20promote%0Abenchmarking%2C%20and%20inspire%20new%20research.%0A%20%20CHaRNet%20achieves%20a%20Mean%20Euclidean%20Distance%20Error%20%28MEDE%29%20of%201.28%20mm%20and%20a%20Mean%0ASuccess%20Ratio%20%28MSR%29%20of%2082.40%5C%25%2C%20demonstrating%20robust%20performance.%20Notably%2C%20it%0Aexcels%20in%20handling%20irregular%20dental%20geometries%2C%20such%20as%20models%20with%20missing%0Ateeth.%20This%20end-to-end%20approach%20streamlines%20orthodontic%20workflows%2C%20improves%203D%0AIOS%20analysis%20precision%2C%20and%20facilitates%20efficient%20computer-assisted%20treatment%0Aplanning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHaRNet%253A%2520Conditioned%2520Heatmap%2520Regression%2520for%2520Robust%2520Dental%2520Landmark%250A%2520%2520Localization%26entry.906535625%3DJos%25C3%25A9%2520Rodr%25C3%25ADguez-Ortega%2520and%2520Siham%2520Tabik%26entry.1292438233%3D%2520%2520Identifying%2520anatomical%2520landmarks%2520in%25203D%2520dental%2520models%2520is%2520crucial%2520for%250Aorthodontic%2520treatment.%2520Manually%2520placing%2520these%2520key%2520points%2520is%2520complex%252C%250Atime-consuming%252C%2520and%2520requires%2520expert%2520knowledge.%2520While%2520some%2520machine%2520learning%250Amethods%2520have%2520been%2520proposed%2520for%2520automatic%2520tooth%2520landmark%2520detection%2520in%25203D%250AIntraoral%2520Scans%2520%2528IOS%2529%252C%2520research%2520remains%2520limited%252C%2520with%2520no%2520fully%2520end-to-end%250Aapproaches%2520that%2520avoid%2520teeth%2520segmentation.%250A%2520%2520We%2520propose%2520CHaRNet%2520%2528Conditioned%2520Heatmap%2520Regression%2520Network%2529%252C%2520the%2520first%250Aend-to-end%2520deep%2520learning%2520method%2520for%2520tooth%2520landmark%2520detection%2520in%25203D%2520IOS.%2520Unlike%250Atraditional%2520two-stage%2520methods%2520that%2520segment%2520teeth%2520before%2520detecting%2520landmarks%252C%250ACHaRNet%2520directly%2520detects%2520landmarks%2520on%2520the%2520input%2520point%2520cloud.%2520It%2520consists%2520of%250Afour%2520key%2520modules%253A%2520%25281%2529%2520a%2520point%2520cloud%2520encoder%252C%2520%25282%2529%2520a%2520point%2520cloud%2520decoder%2520with%2520a%250Aheatmap%2520regression%2520head%252C%2520%25283%2529%2520a%2520teeth%2520presence%2520classification%2520head%252C%2520and%2520%25284%2529%2520the%250Ainnovative%2520Conditioned%2520Heatmap%2520Regression%2520%2528CHaR%2529%2520module.%2520The%2520CHaR%2520module%250Arefines%2520landmark%2520regression%2520by%2520leveraging%2520teeth%2520presence%2520classification%252C%250Aenabling%2520dynamic%2520adaptation%2520to%2520cases%2520with%2520missing%2520teeth%2520and%2520improving%2520accuracy%250Ain%2520complex%2520dental%2520models.%250A%2520%2520We%2520evaluate%2520CHaRNet%2520using%2520five%2520point%2520cloud%2520learning%2520algorithms%2520to%2520validate%250Athe%2520effectiveness%2520of%2520the%2520CHaR%2520module%2520and%2520test%2520it%2520on%2520a%2520clinical%2520dataset%2520of%250A%25241%252C214%2524%2520annotated%25203D%2520dental%2520models.%2520Both%2520the%2520dataset%2520and%2520code%2520will%2520be%2520publicly%250Areleased%2520to%2520address%2520the%2520lack%2520of%2520open%2520datasets%2520in%2520orthodontics%252C%2520promote%250Abenchmarking%252C%2520and%2520inspire%2520new%2520research.%250A%2520%2520CHaRNet%2520achieves%2520a%2520Mean%2520Euclidean%2520Distance%2520Error%2520%2528MEDE%2529%2520of%25201.28%2520mm%2520and%2520a%2520Mean%250ASuccess%2520Ratio%2520%2528MSR%2529%2520of%252082.40%255C%2525%252C%2520demonstrating%2520robust%2520performance.%2520Notably%252C%2520it%250Aexcels%2520in%2520handling%2520irregular%2520dental%2520geometries%252C%2520such%2520as%2520models%2520with%2520missing%250Ateeth.%2520This%2520end-to-end%2520approach%2520streamlines%2520orthodontic%2520workflows%252C%2520improves%25203D%250AIOS%2520analysis%2520precision%252C%2520and%2520facilitates%2520efficient%2520computer-assisted%2520treatment%250Aplanning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHaRNet%3A%20Conditioned%20Heatmap%20Regression%20for%20Robust%20Dental%20Landmark%0A%20%20Localization&entry.906535625=Jos%C3%A9%20Rodr%C3%ADguez-Ortega%20and%20Siham%20Tabik&entry.1292438233=%20%20Identifying%20anatomical%20landmarks%20in%203D%20dental%20models%20is%20crucial%20for%0Aorthodontic%20treatment.%20Manually%20placing%20these%20key%20points%20is%20complex%2C%0Atime-consuming%2C%20and%20requires%20expert%20knowledge.%20While%20some%20machine%20learning%0Amethods%20have%20been%20proposed%20for%20automatic%20tooth%20landmark%20detection%20in%203D%0AIntraoral%20Scans%20%28IOS%29%2C%20research%20remains%20limited%2C%20with%20no%20fully%20end-to-end%0Aapproaches%20that%20avoid%20teeth%20segmentation.%0A%20%20We%20propose%20CHaRNet%20%28Conditioned%20Heatmap%20Regression%20Network%29%2C%20the%20first%0Aend-to-end%20deep%20learning%20method%20for%20tooth%20landmark%20detection%20in%203D%20IOS.%20Unlike%0Atraditional%20two-stage%20methods%20that%20segment%20teeth%20before%20detecting%20landmarks%2C%0ACHaRNet%20directly%20detects%20landmarks%20on%20the%20input%20point%20cloud.%20It%20consists%20of%0Afour%20key%20modules%3A%20%281%29%20a%20point%20cloud%20encoder%2C%20%282%29%20a%20point%20cloud%20decoder%20with%20a%0Aheatmap%20regression%20head%2C%20%283%29%20a%20teeth%20presence%20classification%20head%2C%20and%20%284%29%20the%0Ainnovative%20Conditioned%20Heatmap%20Regression%20%28CHaR%29%20module.%20The%20CHaR%20module%0Arefines%20landmark%20regression%20by%20leveraging%20teeth%20presence%20classification%2C%0Aenabling%20dynamic%20adaptation%20to%20cases%20with%20missing%20teeth%20and%20improving%20accuracy%0Ain%20complex%20dental%20models.%0A%20%20We%20evaluate%20CHaRNet%20using%20five%20point%20cloud%20learning%20algorithms%20to%20validate%0Athe%20effectiveness%20of%20the%20CHaR%20module%20and%20test%20it%20on%20a%20clinical%20dataset%20of%0A%241%2C214%24%20annotated%203D%20dental%20models.%20Both%20the%20dataset%20and%20code%20will%20be%20publicly%0Areleased%20to%20address%20the%20lack%20of%20open%20datasets%20in%20orthodontics%2C%20promote%0Abenchmarking%2C%20and%20inspire%20new%20research.%0A%20%20CHaRNet%20achieves%20a%20Mean%20Euclidean%20Distance%20Error%20%28MEDE%29%20of%201.28%20mm%20and%20a%20Mean%0ASuccess%20Ratio%20%28MSR%29%20of%2082.40%5C%25%2C%20demonstrating%20robust%20performance.%20Notably%2C%20it%0Aexcels%20in%20handling%20irregular%20dental%20geometries%2C%20such%20as%20models%20with%20missing%0Ateeth.%20This%20end-to-end%20approach%20streamlines%20orthodontic%20workflows%2C%20improves%203D%0AIOS%20analysis%20precision%2C%20and%20facilitates%20efficient%20computer-assisted%20treatment%0Aplanning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13073v1&entry.124074799=Read"},
{"title": "Revisit Self-Debugging with Self-Generated Tests for Code Generation", "author": "Xiancai Chen and Zhengwei Tao and Kechi Zhang and Changzhi Zhou and Wanli Gu and Yuanpeng He and Mengdi Zhang and Xunliang Cai and Haiyan Zhao and Zhi Jin", "abstract": "  Large language models (LLMs) have shown significant advancements in code\ngeneration, but still face challenges on tasks beyond their basic capabilities.\nRecently, the notion of self-debugging has been proposed to boost the\nperformance of code generation by leveraging execution feedback from tests.\nDespite its promise, the availability of high-quality tests in real-world\nscenarios is limited. In this context, self-debugging with self-generated tests\nis a promising solution but lacks a full exploration of its limitations and\npractical potential. Therefore, we investigate its efficacy on diverse\nprogramming problems. To deepen our understanding, we propose two distinct\nparadigms for the process: post-execution and in-execution self-debugging.\nWithin the scope of self-contained Python programming tasks, we find that\npost-execution self-debugging struggles on basic problems but shows potential\nfor improvement on competitive ones, due to the bias introduced by\nself-generated tests. On the other hand, in-execution self-debugging enables\nLLMs to mitigate the bias by solely leveraging intermediate states during\nexecution, thereby enhancing code generation.\n", "link": "http://arxiv.org/abs/2501.12793v1", "date": "2025-01-22", "relevancy": 2.1427, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4394}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4289}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisit%20Self-Debugging%20with%20Self-Generated%20Tests%20for%20Code%20Generation&body=Title%3A%20Revisit%20Self-Debugging%20with%20Self-Generated%20Tests%20for%20Code%20Generation%0AAuthor%3A%20Xiancai%20Chen%20and%20Zhengwei%20Tao%20and%20Kechi%20Zhang%20and%20Changzhi%20Zhou%20and%20Wanli%20Gu%20and%20Yuanpeng%20He%20and%20Mengdi%20Zhang%20and%20Xunliang%20Cai%20and%20Haiyan%20Zhao%20and%20Zhi%20Jin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20advancements%20in%20code%0Ageneration%2C%20but%20still%20face%20challenges%20on%20tasks%20beyond%20their%20basic%20capabilities.%0ARecently%2C%20the%20notion%20of%20self-debugging%20has%20been%20proposed%20to%20boost%20the%0Aperformance%20of%20code%20generation%20by%20leveraging%20execution%20feedback%20from%20tests.%0ADespite%20its%20promise%2C%20the%20availability%20of%20high-quality%20tests%20in%20real-world%0Ascenarios%20is%20limited.%20In%20this%20context%2C%20self-debugging%20with%20self-generated%20tests%0Ais%20a%20promising%20solution%20but%20lacks%20a%20full%20exploration%20of%20its%20limitations%20and%0Apractical%20potential.%20Therefore%2C%20we%20investigate%20its%20efficacy%20on%20diverse%0Aprogramming%20problems.%20To%20deepen%20our%20understanding%2C%20we%20propose%20two%20distinct%0Aparadigms%20for%20the%20process%3A%20post-execution%20and%20in-execution%20self-debugging.%0AWithin%20the%20scope%20of%20self-contained%20Python%20programming%20tasks%2C%20we%20find%20that%0Apost-execution%20self-debugging%20struggles%20on%20basic%20problems%20but%20shows%20potential%0Afor%20improvement%20on%20competitive%20ones%2C%20due%20to%20the%20bias%20introduced%20by%0Aself-generated%20tests.%20On%20the%20other%20hand%2C%20in-execution%20self-debugging%20enables%0ALLMs%20to%20mitigate%20the%20bias%20by%20solely%20leveraging%20intermediate%20states%20during%0Aexecution%2C%20thereby%20enhancing%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisit%2520Self-Debugging%2520with%2520Self-Generated%2520Tests%2520for%2520Code%2520Generation%26entry.906535625%3DXiancai%2520Chen%2520and%2520Zhengwei%2520Tao%2520and%2520Kechi%2520Zhang%2520and%2520Changzhi%2520Zhou%2520and%2520Wanli%2520Gu%2520and%2520Yuanpeng%2520He%2520and%2520Mengdi%2520Zhang%2520and%2520Xunliang%2520Cai%2520and%2520Haiyan%2520Zhao%2520and%2520Zhi%2520Jin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520significant%2520advancements%2520in%2520code%250Ageneration%252C%2520but%2520still%2520face%2520challenges%2520on%2520tasks%2520beyond%2520their%2520basic%2520capabilities.%250ARecently%252C%2520the%2520notion%2520of%2520self-debugging%2520has%2520been%2520proposed%2520to%2520boost%2520the%250Aperformance%2520of%2520code%2520generation%2520by%2520leveraging%2520execution%2520feedback%2520from%2520tests.%250ADespite%2520its%2520promise%252C%2520the%2520availability%2520of%2520high-quality%2520tests%2520in%2520real-world%250Ascenarios%2520is%2520limited.%2520In%2520this%2520context%252C%2520self-debugging%2520with%2520self-generated%2520tests%250Ais%2520a%2520promising%2520solution%2520but%2520lacks%2520a%2520full%2520exploration%2520of%2520its%2520limitations%2520and%250Apractical%2520potential.%2520Therefore%252C%2520we%2520investigate%2520its%2520efficacy%2520on%2520diverse%250Aprogramming%2520problems.%2520To%2520deepen%2520our%2520understanding%252C%2520we%2520propose%2520two%2520distinct%250Aparadigms%2520for%2520the%2520process%253A%2520post-execution%2520and%2520in-execution%2520self-debugging.%250AWithin%2520the%2520scope%2520of%2520self-contained%2520Python%2520programming%2520tasks%252C%2520we%2520find%2520that%250Apost-execution%2520self-debugging%2520struggles%2520on%2520basic%2520problems%2520but%2520shows%2520potential%250Afor%2520improvement%2520on%2520competitive%2520ones%252C%2520due%2520to%2520the%2520bias%2520introduced%2520by%250Aself-generated%2520tests.%2520On%2520the%2520other%2520hand%252C%2520in-execution%2520self-debugging%2520enables%250ALLMs%2520to%2520mitigate%2520the%2520bias%2520by%2520solely%2520leveraging%2520intermediate%2520states%2520during%250Aexecution%252C%2520thereby%2520enhancing%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisit%20Self-Debugging%20with%20Self-Generated%20Tests%20for%20Code%20Generation&entry.906535625=Xiancai%20Chen%20and%20Zhengwei%20Tao%20and%20Kechi%20Zhang%20and%20Changzhi%20Zhou%20and%20Wanli%20Gu%20and%20Yuanpeng%20He%20and%20Mengdi%20Zhang%20and%20Xunliang%20Cai%20and%20Haiyan%20Zhao%20and%20Zhi%20Jin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20significant%20advancements%20in%20code%0Ageneration%2C%20but%20still%20face%20challenges%20on%20tasks%20beyond%20their%20basic%20capabilities.%0ARecently%2C%20the%20notion%20of%20self-debugging%20has%20been%20proposed%20to%20boost%20the%0Aperformance%20of%20code%20generation%20by%20leveraging%20execution%20feedback%20from%20tests.%0ADespite%20its%20promise%2C%20the%20availability%20of%20high-quality%20tests%20in%20real-world%0Ascenarios%20is%20limited.%20In%20this%20context%2C%20self-debugging%20with%20self-generated%20tests%0Ais%20a%20promising%20solution%20but%20lacks%20a%20full%20exploration%20of%20its%20limitations%20and%0Apractical%20potential.%20Therefore%2C%20we%20investigate%20its%20efficacy%20on%20diverse%0Aprogramming%20problems.%20To%20deepen%20our%20understanding%2C%20we%20propose%20two%20distinct%0Aparadigms%20for%20the%20process%3A%20post-execution%20and%20in-execution%20self-debugging.%0AWithin%20the%20scope%20of%20self-contained%20Python%20programming%20tasks%2C%20we%20find%20that%0Apost-execution%20self-debugging%20struggles%20on%20basic%20problems%20but%20shows%20potential%0Afor%20improvement%20on%20competitive%20ones%2C%20due%20to%20the%20bias%20introduced%20by%0Aself-generated%20tests.%20On%20the%20other%20hand%2C%20in-execution%20self-debugging%20enables%0ALLMs%20to%20mitigate%20the%20bias%20by%20solely%20leveraging%20intermediate%20states%20during%0Aexecution%2C%20thereby%20enhancing%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12793v1&entry.124074799=Read"},
{"title": "Reasoning Language Models: A Blueprint", "author": "Maciej Besta and Julia Barth and Eric Schreiber and Ales Kubicek and Afonso Catarino and Robert Gerstenberger and Piotr Nyczyk and Patrick Iff and Yueling Li and Sam Houliston and Tomasz Sternal and Marcin Copik and Grzegorz Kwa\u015bniewski and J\u00fcrgen M\u00fcller and \u0141ukasz Flis and Hannes Eberhard and Hubert Niewiadomski and Torsten Hoefler", "abstract": "  Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation.\n", "link": "http://arxiv.org/abs/2501.11223v2", "date": "2025-01-22", "relevancy": 2.1386, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5257}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20Language%20Models%3A%20A%20Blueprint&body=Title%3A%20Reasoning%20Language%20Models%3A%20A%20Blueprint%0AAuthor%3A%20Maciej%20Besta%20and%20Julia%20Barth%20and%20Eric%20Schreiber%20and%20Ales%20Kubicek%20and%20Afonso%20Catarino%20and%20Robert%20Gerstenberger%20and%20Piotr%20Nyczyk%20and%20Patrick%20Iff%20and%20Yueling%20Li%20and%20Sam%20Houliston%20and%20Tomasz%20Sternal%20and%20Marcin%20Copik%20and%20Grzegorz%20Kwa%C5%9Bniewski%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20%C5%81ukasz%20Flis%20and%20Hannes%20Eberhard%20and%20Hubert%20Niewiadomski%20and%20Torsten%20Hoefler%0AAbstract%3A%20%20%20Reasoning%20language%20models%20%28RLMs%29%2C%20also%20known%20as%20Large%20Reasoning%20Models%0A%28LRMs%29%2C%20such%20as%20OpenAI%27s%20o1%20and%20o3%2C%20DeepSeek-V3%2C%20and%20Alibaba%27s%20QwQ%2C%20have%0Aredefined%20AI%27s%20problem-solving%20capabilities%20by%20extending%20LLMs%20with%20advanced%0Areasoning%20mechanisms.%20Yet%2C%20their%20high%20costs%2C%20proprietary%20nature%2C%20and%20complex%0Aarchitectures%20-%20uniquely%20combining%20Reinforcement%20Learning%20%28RL%29%2C%20search%0Aheuristics%2C%20and%20LLMs%20-%20present%20accessibility%20and%20scalability%20challenges.%20To%0Aaddress%20these%2C%20we%20propose%20a%20comprehensive%20blueprint%20that%20organizes%20RLM%0Acomponents%20into%20a%20modular%20framework%2C%20based%20on%20a%20survey%20and%20analysis%20of%20all%20RLM%0Aworks.%20This%20blueprint%20incorporates%20diverse%20reasoning%20structures%20%28chains%2C%20trees%2C%0Agraphs%2C%20and%20nested%20forms%29%2C%20reasoning%20strategies%20%28e.g.%2C%20Monte%20Carlo%20Tree%20Search%2C%0ABeam%20Search%29%2C%20RL%20concepts%20%28policy%2C%20value%20models%20and%20others%29%2C%20supervision%0Aschemes%20%28Outcome-Based%20and%20Process-Based%20Supervision%29%2C%20and%20other%20related%0Aconcepts%20%28e.g.%2C%20Test-Time%20Compute%2C%20Retrieval-Augmented%20Generation%2C%20agent%0Atools%29.%20We%20provide%20detailed%20mathematical%20formulations%20and%20algorithmic%0Aspecifications%20to%20simplify%20RLM%20implementation.%20By%20showing%20how%20schemes%20like%0ALLaMA-Berry%2C%20QwQ%2C%20Journey%20Learning%2C%20and%20Graph%20of%20Thoughts%20fit%20as%20special%20cases%2C%0Awe%20demonstrate%20the%20blueprint%27s%20versatility%20and%20unifying%20potential.%20To%0Aillustrate%20its%20utility%2C%20we%20introduce%20x1%2C%20a%20modular%20implementation%20for%20rapid%20RLM%0Aprototyping%20and%20experimentation.%20Using%20x1%20and%20a%20literature%20review%2C%20we%20provide%0Akey%20insights%2C%20such%20as%20multi-phase%20training%20for%20policy%20and%20value%20models%2C%20and%20the%0Aimportance%20of%20familiar%20training%20distributions.%20Finally%2C%20we%20discuss%20scalable%20RLM%0Acloud%20deployments%20and%20we%20outline%20how%20RLMs%20can%20integrate%20with%20a%20broader%20LLM%0Aecosystem.%20Our%20work%20demystifies%20RLM%20construction%2C%20democratizes%20advanced%0Areasoning%20capabilities%2C%20and%20fosters%20innovation%2C%20aiming%20to%20mitigate%20the%20gap%0Abetween%20%22rich%20AI%22%20and%20%22poor%20AI%22%20by%20lowering%20barriers%20to%20RLM%20development%20and%0Aexperimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520Language%2520Models%253A%2520A%2520Blueprint%26entry.906535625%3DMaciej%2520Besta%2520and%2520Julia%2520Barth%2520and%2520Eric%2520Schreiber%2520and%2520Ales%2520Kubicek%2520and%2520Afonso%2520Catarino%2520and%2520Robert%2520Gerstenberger%2520and%2520Piotr%2520Nyczyk%2520and%2520Patrick%2520Iff%2520and%2520Yueling%2520Li%2520and%2520Sam%2520Houliston%2520and%2520Tomasz%2520Sternal%2520and%2520Marcin%2520Copik%2520and%2520Grzegorz%2520Kwa%25C5%259Bniewski%2520and%2520J%25C3%25BCrgen%2520M%25C3%25BCller%2520and%2520%25C5%2581ukasz%2520Flis%2520and%2520Hannes%2520Eberhard%2520and%2520Hubert%2520Niewiadomski%2520and%2520Torsten%2520Hoefler%26entry.1292438233%3D%2520%2520Reasoning%2520language%2520models%2520%2528RLMs%2529%252C%2520also%2520known%2520as%2520Large%2520Reasoning%2520Models%250A%2528LRMs%2529%252C%2520such%2520as%2520OpenAI%2527s%2520o1%2520and%2520o3%252C%2520DeepSeek-V3%252C%2520and%2520Alibaba%2527s%2520QwQ%252C%2520have%250Aredefined%2520AI%2527s%2520problem-solving%2520capabilities%2520by%2520extending%2520LLMs%2520with%2520advanced%250Areasoning%2520mechanisms.%2520Yet%252C%2520their%2520high%2520costs%252C%2520proprietary%2520nature%252C%2520and%2520complex%250Aarchitectures%2520-%2520uniquely%2520combining%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520search%250Aheuristics%252C%2520and%2520LLMs%2520-%2520present%2520accessibility%2520and%2520scalability%2520challenges.%2520To%250Aaddress%2520these%252C%2520we%2520propose%2520a%2520comprehensive%2520blueprint%2520that%2520organizes%2520RLM%250Acomponents%2520into%2520a%2520modular%2520framework%252C%2520based%2520on%2520a%2520survey%2520and%2520analysis%2520of%2520all%2520RLM%250Aworks.%2520This%2520blueprint%2520incorporates%2520diverse%2520reasoning%2520structures%2520%2528chains%252C%2520trees%252C%250Agraphs%252C%2520and%2520nested%2520forms%2529%252C%2520reasoning%2520strategies%2520%2528e.g.%252C%2520Monte%2520Carlo%2520Tree%2520Search%252C%250ABeam%2520Search%2529%252C%2520RL%2520concepts%2520%2528policy%252C%2520value%2520models%2520and%2520others%2529%252C%2520supervision%250Aschemes%2520%2528Outcome-Based%2520and%2520Process-Based%2520Supervision%2529%252C%2520and%2520other%2520related%250Aconcepts%2520%2528e.g.%252C%2520Test-Time%2520Compute%252C%2520Retrieval-Augmented%2520Generation%252C%2520agent%250Atools%2529.%2520We%2520provide%2520detailed%2520mathematical%2520formulations%2520and%2520algorithmic%250Aspecifications%2520to%2520simplify%2520RLM%2520implementation.%2520By%2520showing%2520how%2520schemes%2520like%250ALLaMA-Berry%252C%2520QwQ%252C%2520Journey%2520Learning%252C%2520and%2520Graph%2520of%2520Thoughts%2520fit%2520as%2520special%2520cases%252C%250Awe%2520demonstrate%2520the%2520blueprint%2527s%2520versatility%2520and%2520unifying%2520potential.%2520To%250Aillustrate%2520its%2520utility%252C%2520we%2520introduce%2520x1%252C%2520a%2520modular%2520implementation%2520for%2520rapid%2520RLM%250Aprototyping%2520and%2520experimentation.%2520Using%2520x1%2520and%2520a%2520literature%2520review%252C%2520we%2520provide%250Akey%2520insights%252C%2520such%2520as%2520multi-phase%2520training%2520for%2520policy%2520and%2520value%2520models%252C%2520and%2520the%250Aimportance%2520of%2520familiar%2520training%2520distributions.%2520Finally%252C%2520we%2520discuss%2520scalable%2520RLM%250Acloud%2520deployments%2520and%2520we%2520outline%2520how%2520RLMs%2520can%2520integrate%2520with%2520a%2520broader%2520LLM%250Aecosystem.%2520Our%2520work%2520demystifies%2520RLM%2520construction%252C%2520democratizes%2520advanced%250Areasoning%2520capabilities%252C%2520and%2520fosters%2520innovation%252C%2520aiming%2520to%2520mitigate%2520the%2520gap%250Abetween%2520%2522rich%2520AI%2522%2520and%2520%2522poor%2520AI%2522%2520by%2520lowering%2520barriers%2520to%2520RLM%2520development%2520and%250Aexperimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20Language%20Models%3A%20A%20Blueprint&entry.906535625=Maciej%20Besta%20and%20Julia%20Barth%20and%20Eric%20Schreiber%20and%20Ales%20Kubicek%20and%20Afonso%20Catarino%20and%20Robert%20Gerstenberger%20and%20Piotr%20Nyczyk%20and%20Patrick%20Iff%20and%20Yueling%20Li%20and%20Sam%20Houliston%20and%20Tomasz%20Sternal%20and%20Marcin%20Copik%20and%20Grzegorz%20Kwa%C5%9Bniewski%20and%20J%C3%BCrgen%20M%C3%BCller%20and%20%C5%81ukasz%20Flis%20and%20Hannes%20Eberhard%20and%20Hubert%20Niewiadomski%20and%20Torsten%20Hoefler&entry.1292438233=%20%20Reasoning%20language%20models%20%28RLMs%29%2C%20also%20known%20as%20Large%20Reasoning%20Models%0A%28LRMs%29%2C%20such%20as%20OpenAI%27s%20o1%20and%20o3%2C%20DeepSeek-V3%2C%20and%20Alibaba%27s%20QwQ%2C%20have%0Aredefined%20AI%27s%20problem-solving%20capabilities%20by%20extending%20LLMs%20with%20advanced%0Areasoning%20mechanisms.%20Yet%2C%20their%20high%20costs%2C%20proprietary%20nature%2C%20and%20complex%0Aarchitectures%20-%20uniquely%20combining%20Reinforcement%20Learning%20%28RL%29%2C%20search%0Aheuristics%2C%20and%20LLMs%20-%20present%20accessibility%20and%20scalability%20challenges.%20To%0Aaddress%20these%2C%20we%20propose%20a%20comprehensive%20blueprint%20that%20organizes%20RLM%0Acomponents%20into%20a%20modular%20framework%2C%20based%20on%20a%20survey%20and%20analysis%20of%20all%20RLM%0Aworks.%20This%20blueprint%20incorporates%20diverse%20reasoning%20structures%20%28chains%2C%20trees%2C%0Agraphs%2C%20and%20nested%20forms%29%2C%20reasoning%20strategies%20%28e.g.%2C%20Monte%20Carlo%20Tree%20Search%2C%0ABeam%20Search%29%2C%20RL%20concepts%20%28policy%2C%20value%20models%20and%20others%29%2C%20supervision%0Aschemes%20%28Outcome-Based%20and%20Process-Based%20Supervision%29%2C%20and%20other%20related%0Aconcepts%20%28e.g.%2C%20Test-Time%20Compute%2C%20Retrieval-Augmented%20Generation%2C%20agent%0Atools%29.%20We%20provide%20detailed%20mathematical%20formulations%20and%20algorithmic%0Aspecifications%20to%20simplify%20RLM%20implementation.%20By%20showing%20how%20schemes%20like%0ALLaMA-Berry%2C%20QwQ%2C%20Journey%20Learning%2C%20and%20Graph%20of%20Thoughts%20fit%20as%20special%20cases%2C%0Awe%20demonstrate%20the%20blueprint%27s%20versatility%20and%20unifying%20potential.%20To%0Aillustrate%20its%20utility%2C%20we%20introduce%20x1%2C%20a%20modular%20implementation%20for%20rapid%20RLM%0Aprototyping%20and%20experimentation.%20Using%20x1%20and%20a%20literature%20review%2C%20we%20provide%0Akey%20insights%2C%20such%20as%20multi-phase%20training%20for%20policy%20and%20value%20models%2C%20and%20the%0Aimportance%20of%20familiar%20training%20distributions.%20Finally%2C%20we%20discuss%20scalable%20RLM%0Acloud%20deployments%20and%20we%20outline%20how%20RLMs%20can%20integrate%20with%20a%20broader%20LLM%0Aecosystem.%20Our%20work%20demystifies%20RLM%20construction%2C%20democratizes%20advanced%0Areasoning%20capabilities%2C%20and%20fosters%20innovation%2C%20aiming%20to%20mitigate%20the%20gap%0Abetween%20%22rich%20AI%22%20and%20%22poor%20AI%22%20by%20lowering%20barriers%20to%20RLM%20development%20and%0Aexperimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11223v2&entry.124074799=Read"},
{"title": "DynamicEarth: How Far are We from Open-Vocabulary Change Detection?", "author": "Kaiyu Li and Xiangyong Cao and Yupeng Deng and Chao Pang and Zepeng Xin and Deyu Meng and Zhi Wang", "abstract": "  Monitoring Earth's evolving land covers requires methods capable of detecting\nchanges across a wide range of categories and contexts. Existing change\ndetection methods are hindered by their dependency on predefined classes,\nreducing their effectiveness in open-world applications. To address this issue,\nwe introduce open-vocabulary change detection (OVCD), a novel task that bridges\nvision and language to detect changes across any category. Considering the lack\nof high-quality data and annotation, we propose two training-free frameworks,\nM-C-I and I-M-C, which leverage and integrate off-the-shelf foundation models\nfor the OVCD task. The insight behind the M-C-I framework is to discover all\npotential changes and then classify these changes, while the insight of I-M-C\nframework is to identify all targets of interest and then determine whether\ntheir states have changed. Based on these two frameworks, we instantiate to\nobtain several methods, e.g., SAM-DINOv2-SegEarth-OV, Grounding-DINO-SAM2-DINO,\netc. Extensive evaluations on 5 benchmark datasets demonstrate the superior\ngeneralization and robustness of our OVCD methods over existing supervised and\nunsupervised methods. To support continued exploration, we release\nDynamicEarth, a dedicated codebase designed to advance research and application\nof OVCD. https://likyoo.github.io/DynamicEarth\n", "link": "http://arxiv.org/abs/2501.12931v1", "date": "2025-01-22", "relevancy": 2.1284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicEarth%3A%20How%20Far%20are%20We%20from%20Open-Vocabulary%20Change%20Detection%3F&body=Title%3A%20DynamicEarth%3A%20How%20Far%20are%20We%20from%20Open-Vocabulary%20Change%20Detection%3F%0AAuthor%3A%20Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Yupeng%20Deng%20and%20Chao%20Pang%20and%20Zepeng%20Xin%20and%20Deyu%20Meng%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Monitoring%20Earth%27s%20evolving%20land%20covers%20requires%20methods%20capable%20of%20detecting%0Achanges%20across%20a%20wide%20range%20of%20categories%20and%20contexts.%20Existing%20change%0Adetection%20methods%20are%20hindered%20by%20their%20dependency%20on%20predefined%20classes%2C%0Areducing%20their%20effectiveness%20in%20open-world%20applications.%20To%20address%20this%20issue%2C%0Awe%20introduce%20open-vocabulary%20change%20detection%20%28OVCD%29%2C%20a%20novel%20task%20that%20bridges%0Avision%20and%20language%20to%20detect%20changes%20across%20any%20category.%20Considering%20the%20lack%0Aof%20high-quality%20data%20and%20annotation%2C%20we%20propose%20two%20training-free%20frameworks%2C%0AM-C-I%20and%20I-M-C%2C%20which%20leverage%20and%20integrate%20off-the-shelf%20foundation%20models%0Afor%20the%20OVCD%20task.%20The%20insight%20behind%20the%20M-C-I%20framework%20is%20to%20discover%20all%0Apotential%20changes%20and%20then%20classify%20these%20changes%2C%20while%20the%20insight%20of%20I-M-C%0Aframework%20is%20to%20identify%20all%20targets%20of%20interest%20and%20then%20determine%20whether%0Atheir%20states%20have%20changed.%20Based%20on%20these%20two%20frameworks%2C%20we%20instantiate%20to%0Aobtain%20several%20methods%2C%20e.g.%2C%20SAM-DINOv2-SegEarth-OV%2C%20Grounding-DINO-SAM2-DINO%2C%0Aetc.%20Extensive%20evaluations%20on%205%20benchmark%20datasets%20demonstrate%20the%20superior%0Ageneralization%20and%20robustness%20of%20our%20OVCD%20methods%20over%20existing%20supervised%20and%0Aunsupervised%20methods.%20To%20support%20continued%20exploration%2C%20we%20release%0ADynamicEarth%2C%20a%20dedicated%20codebase%20designed%20to%20advance%20research%20and%20application%0Aof%20OVCD.%20https%3A//likyoo.github.io/DynamicEarth%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicEarth%253A%2520How%2520Far%2520are%2520We%2520from%2520Open-Vocabulary%2520Change%2520Detection%253F%26entry.906535625%3DKaiyu%2520Li%2520and%2520Xiangyong%2520Cao%2520and%2520Yupeng%2520Deng%2520and%2520Chao%2520Pang%2520and%2520Zepeng%2520Xin%2520and%2520Deyu%2520Meng%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520Monitoring%2520Earth%2527s%2520evolving%2520land%2520covers%2520requires%2520methods%2520capable%2520of%2520detecting%250Achanges%2520across%2520a%2520wide%2520range%2520of%2520categories%2520and%2520contexts.%2520Existing%2520change%250Adetection%2520methods%2520are%2520hindered%2520by%2520their%2520dependency%2520on%2520predefined%2520classes%252C%250Areducing%2520their%2520effectiveness%2520in%2520open-world%2520applications.%2520To%2520address%2520this%2520issue%252C%250Awe%2520introduce%2520open-vocabulary%2520change%2520detection%2520%2528OVCD%2529%252C%2520a%2520novel%2520task%2520that%2520bridges%250Avision%2520and%2520language%2520to%2520detect%2520changes%2520across%2520any%2520category.%2520Considering%2520the%2520lack%250Aof%2520high-quality%2520data%2520and%2520annotation%252C%2520we%2520propose%2520two%2520training-free%2520frameworks%252C%250AM-C-I%2520and%2520I-M-C%252C%2520which%2520leverage%2520and%2520integrate%2520off-the-shelf%2520foundation%2520models%250Afor%2520the%2520OVCD%2520task.%2520The%2520insight%2520behind%2520the%2520M-C-I%2520framework%2520is%2520to%2520discover%2520all%250Apotential%2520changes%2520and%2520then%2520classify%2520these%2520changes%252C%2520while%2520the%2520insight%2520of%2520I-M-C%250Aframework%2520is%2520to%2520identify%2520all%2520targets%2520of%2520interest%2520and%2520then%2520determine%2520whether%250Atheir%2520states%2520have%2520changed.%2520Based%2520on%2520these%2520two%2520frameworks%252C%2520we%2520instantiate%2520to%250Aobtain%2520several%2520methods%252C%2520e.g.%252C%2520SAM-DINOv2-SegEarth-OV%252C%2520Grounding-DINO-SAM2-DINO%252C%250Aetc.%2520Extensive%2520evaluations%2520on%25205%2520benchmark%2520datasets%2520demonstrate%2520the%2520superior%250Ageneralization%2520and%2520robustness%2520of%2520our%2520OVCD%2520methods%2520over%2520existing%2520supervised%2520and%250Aunsupervised%2520methods.%2520To%2520support%2520continued%2520exploration%252C%2520we%2520release%250ADynamicEarth%252C%2520a%2520dedicated%2520codebase%2520designed%2520to%2520advance%2520research%2520and%2520application%250Aof%2520OVCD.%2520https%253A//likyoo.github.io/DynamicEarth%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicEarth%3A%20How%20Far%20are%20We%20from%20Open-Vocabulary%20Change%20Detection%3F&entry.906535625=Kaiyu%20Li%20and%20Xiangyong%20Cao%20and%20Yupeng%20Deng%20and%20Chao%20Pang%20and%20Zepeng%20Xin%20and%20Deyu%20Meng%20and%20Zhi%20Wang&entry.1292438233=%20%20Monitoring%20Earth%27s%20evolving%20land%20covers%20requires%20methods%20capable%20of%20detecting%0Achanges%20across%20a%20wide%20range%20of%20categories%20and%20contexts.%20Existing%20change%0Adetection%20methods%20are%20hindered%20by%20their%20dependency%20on%20predefined%20classes%2C%0Areducing%20their%20effectiveness%20in%20open-world%20applications.%20To%20address%20this%20issue%2C%0Awe%20introduce%20open-vocabulary%20change%20detection%20%28OVCD%29%2C%20a%20novel%20task%20that%20bridges%0Avision%20and%20language%20to%20detect%20changes%20across%20any%20category.%20Considering%20the%20lack%0Aof%20high-quality%20data%20and%20annotation%2C%20we%20propose%20two%20training-free%20frameworks%2C%0AM-C-I%20and%20I-M-C%2C%20which%20leverage%20and%20integrate%20off-the-shelf%20foundation%20models%0Afor%20the%20OVCD%20task.%20The%20insight%20behind%20the%20M-C-I%20framework%20is%20to%20discover%20all%0Apotential%20changes%20and%20then%20classify%20these%20changes%2C%20while%20the%20insight%20of%20I-M-C%0Aframework%20is%20to%20identify%20all%20targets%20of%20interest%20and%20then%20determine%20whether%0Atheir%20states%20have%20changed.%20Based%20on%20these%20two%20frameworks%2C%20we%20instantiate%20to%0Aobtain%20several%20methods%2C%20e.g.%2C%20SAM-DINOv2-SegEarth-OV%2C%20Grounding-DINO-SAM2-DINO%2C%0Aetc.%20Extensive%20evaluations%20on%205%20benchmark%20datasets%20demonstrate%20the%20superior%0Ageneralization%20and%20robustness%20of%20our%20OVCD%20methods%20over%20existing%20supervised%20and%0Aunsupervised%20methods.%20To%20support%20continued%20exploration%2C%20we%20release%0ADynamicEarth%2C%20a%20dedicated%20codebase%20designed%20to%20advance%20research%20and%20application%0Aof%20OVCD.%20https%3A//likyoo.github.io/DynamicEarth%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12931v1&entry.124074799=Read"},
{"title": "One-Class Domain Adaptation via Meta-Learning", "author": "Stephanie Holly and Thomas Bierweiler and Stefan von Dosky and Ahmed Frikha and Clemens Heitzinger and Jana Eder", "abstract": "  The deployment of IoT (Internet of Things) sensor-based machine learning\nmodels in industrial systems for anomaly classification tasks poses significant\nchallenges due to distribution shifts, as the training data acquired in\ncontrolled laboratory settings may significantly differ from real-time data in\nproduction environments. Furthermore, many real-world applications cannot\nprovide a substantial number of labeled examples for each anomalous class in\nevery new environment. It is therefore crucial to develop adaptable machine\nlearning models that can be effectively transferred from one environment to\nanother, enabling rapid adaptation using normal operational data. We extended\nthis problem setting to an arbitrary classification task and formulated the\none-class domain adaptation (OC-DA) problem setting. We took a meta-learning\napproach to tackle the challenge of OC-DA, and proposed a task sampling\nstrategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified\nthe well-established model-agnostic meta-learning (MAML) algorithm and\nintroduced the OC-DA MAML algorithm. We provided a theoretical analysis showing\nthat OC-DA MAML optimizes for meta-parameters that enable rapid one-class\nadaptation across domains. The OC-DA MAML algorithm is evaluated on the\nRainbow-MNIST meta-learning benchmark and on a real-world dataset of\nvibration-based sensor readings. The results show that OC-DA MAML significantly\nimproves the performance on the target domains and outperforms MAML using the\nstandard task sampling strategy.\n", "link": "http://arxiv.org/abs/2501.13052v1", "date": "2025-01-22", "relevancy": 2.1275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5405}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Class%20Domain%20Adaptation%20via%20Meta-Learning&body=Title%3A%20One-Class%20Domain%20Adaptation%20via%20Meta-Learning%0AAuthor%3A%20Stephanie%20Holly%20and%20Thomas%20Bierweiler%20and%20Stefan%20von%20Dosky%20and%20Ahmed%20Frikha%20and%20Clemens%20Heitzinger%20and%20Jana%20Eder%0AAbstract%3A%20%20%20The%20deployment%20of%20IoT%20%28Internet%20of%20Things%29%20sensor-based%20machine%20learning%0Amodels%20in%20industrial%20systems%20for%20anomaly%20classification%20tasks%20poses%20significant%0Achallenges%20due%20to%20distribution%20shifts%2C%20as%20the%20training%20data%20acquired%20in%0Acontrolled%20laboratory%20settings%20may%20significantly%20differ%20from%20real-time%20data%20in%0Aproduction%20environments.%20Furthermore%2C%20many%20real-world%20applications%20cannot%0Aprovide%20a%20substantial%20number%20of%20labeled%20examples%20for%20each%20anomalous%20class%20in%0Aevery%20new%20environment.%20It%20is%20therefore%20crucial%20to%20develop%20adaptable%20machine%0Alearning%20models%20that%20can%20be%20effectively%20transferred%20from%20one%20environment%20to%0Aanother%2C%20enabling%20rapid%20adaptation%20using%20normal%20operational%20data.%20We%20extended%0Athis%20problem%20setting%20to%20an%20arbitrary%20classification%20task%20and%20formulated%20the%0Aone-class%20domain%20adaptation%20%28OC-DA%29%20problem%20setting.%20We%20took%20a%20meta-learning%0Aapproach%20to%20tackle%20the%20challenge%20of%20OC-DA%2C%20and%20proposed%20a%20task%20sampling%0Astrategy%20to%20adapt%20any%20bi-level%20meta-learning%20algorithm%20to%20OC-DA.%20We%20modified%0Athe%20well-established%20model-agnostic%20meta-learning%20%28MAML%29%20algorithm%20and%0Aintroduced%20the%20OC-DA%20MAML%20algorithm.%20We%20provided%20a%20theoretical%20analysis%20showing%0Athat%20OC-DA%20MAML%20optimizes%20for%20meta-parameters%20that%20enable%20rapid%20one-class%0Aadaptation%20across%20domains.%20The%20OC-DA%20MAML%20algorithm%20is%20evaluated%20on%20the%0ARainbow-MNIST%20meta-learning%20benchmark%20and%20on%20a%20real-world%20dataset%20of%0Avibration-based%20sensor%20readings.%20The%20results%20show%20that%20OC-DA%20MAML%20significantly%0Aimproves%20the%20performance%20on%20the%20target%20domains%20and%20outperforms%20MAML%20using%20the%0Astandard%20task%20sampling%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Class%2520Domain%2520Adaptation%2520via%2520Meta-Learning%26entry.906535625%3DStephanie%2520Holly%2520and%2520Thomas%2520Bierweiler%2520and%2520Stefan%2520von%2520Dosky%2520and%2520Ahmed%2520Frikha%2520and%2520Clemens%2520Heitzinger%2520and%2520Jana%2520Eder%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520IoT%2520%2528Internet%2520of%2520Things%2529%2520sensor-based%2520machine%2520learning%250Amodels%2520in%2520industrial%2520systems%2520for%2520anomaly%2520classification%2520tasks%2520poses%2520significant%250Achallenges%2520due%2520to%2520distribution%2520shifts%252C%2520as%2520the%2520training%2520data%2520acquired%2520in%250Acontrolled%2520laboratory%2520settings%2520may%2520significantly%2520differ%2520from%2520real-time%2520data%2520in%250Aproduction%2520environments.%2520Furthermore%252C%2520many%2520real-world%2520applications%2520cannot%250Aprovide%2520a%2520substantial%2520number%2520of%2520labeled%2520examples%2520for%2520each%2520anomalous%2520class%2520in%250Aevery%2520new%2520environment.%2520It%2520is%2520therefore%2520crucial%2520to%2520develop%2520adaptable%2520machine%250Alearning%2520models%2520that%2520can%2520be%2520effectively%2520transferred%2520from%2520one%2520environment%2520to%250Aanother%252C%2520enabling%2520rapid%2520adaptation%2520using%2520normal%2520operational%2520data.%2520We%2520extended%250Athis%2520problem%2520setting%2520to%2520an%2520arbitrary%2520classification%2520task%2520and%2520formulated%2520the%250Aone-class%2520domain%2520adaptation%2520%2528OC-DA%2529%2520problem%2520setting.%2520We%2520took%2520a%2520meta-learning%250Aapproach%2520to%2520tackle%2520the%2520challenge%2520of%2520OC-DA%252C%2520and%2520proposed%2520a%2520task%2520sampling%250Astrategy%2520to%2520adapt%2520any%2520bi-level%2520meta-learning%2520algorithm%2520to%2520OC-DA.%2520We%2520modified%250Athe%2520well-established%2520model-agnostic%2520meta-learning%2520%2528MAML%2529%2520algorithm%2520and%250Aintroduced%2520the%2520OC-DA%2520MAML%2520algorithm.%2520We%2520provided%2520a%2520theoretical%2520analysis%2520showing%250Athat%2520OC-DA%2520MAML%2520optimizes%2520for%2520meta-parameters%2520that%2520enable%2520rapid%2520one-class%250Aadaptation%2520across%2520domains.%2520The%2520OC-DA%2520MAML%2520algorithm%2520is%2520evaluated%2520on%2520the%250ARainbow-MNIST%2520meta-learning%2520benchmark%2520and%2520on%2520a%2520real-world%2520dataset%2520of%250Avibration-based%2520sensor%2520readings.%2520The%2520results%2520show%2520that%2520OC-DA%2520MAML%2520significantly%250Aimproves%2520the%2520performance%2520on%2520the%2520target%2520domains%2520and%2520outperforms%2520MAML%2520using%2520the%250Astandard%2520task%2520sampling%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Class%20Domain%20Adaptation%20via%20Meta-Learning&entry.906535625=Stephanie%20Holly%20and%20Thomas%20Bierweiler%20and%20Stefan%20von%20Dosky%20and%20Ahmed%20Frikha%20and%20Clemens%20Heitzinger%20and%20Jana%20Eder&entry.1292438233=%20%20The%20deployment%20of%20IoT%20%28Internet%20of%20Things%29%20sensor-based%20machine%20learning%0Amodels%20in%20industrial%20systems%20for%20anomaly%20classification%20tasks%20poses%20significant%0Achallenges%20due%20to%20distribution%20shifts%2C%20as%20the%20training%20data%20acquired%20in%0Acontrolled%20laboratory%20settings%20may%20significantly%20differ%20from%20real-time%20data%20in%0Aproduction%20environments.%20Furthermore%2C%20many%20real-world%20applications%20cannot%0Aprovide%20a%20substantial%20number%20of%20labeled%20examples%20for%20each%20anomalous%20class%20in%0Aevery%20new%20environment.%20It%20is%20therefore%20crucial%20to%20develop%20adaptable%20machine%0Alearning%20models%20that%20can%20be%20effectively%20transferred%20from%20one%20environment%20to%0Aanother%2C%20enabling%20rapid%20adaptation%20using%20normal%20operational%20data.%20We%20extended%0Athis%20problem%20setting%20to%20an%20arbitrary%20classification%20task%20and%20formulated%20the%0Aone-class%20domain%20adaptation%20%28OC-DA%29%20problem%20setting.%20We%20took%20a%20meta-learning%0Aapproach%20to%20tackle%20the%20challenge%20of%20OC-DA%2C%20and%20proposed%20a%20task%20sampling%0Astrategy%20to%20adapt%20any%20bi-level%20meta-learning%20algorithm%20to%20OC-DA.%20We%20modified%0Athe%20well-established%20model-agnostic%20meta-learning%20%28MAML%29%20algorithm%20and%0Aintroduced%20the%20OC-DA%20MAML%20algorithm.%20We%20provided%20a%20theoretical%20analysis%20showing%0Athat%20OC-DA%20MAML%20optimizes%20for%20meta-parameters%20that%20enable%20rapid%20one-class%0Aadaptation%20across%20domains.%20The%20OC-DA%20MAML%20algorithm%20is%20evaluated%20on%20the%0ARainbow-MNIST%20meta-learning%20benchmark%20and%20on%20a%20real-world%20dataset%20of%0Avibration-based%20sensor%20readings.%20The%20results%20show%20that%20OC-DA%20MAML%20significantly%0Aimproves%20the%20performance%20on%20the%20target%20domains%20and%20outperforms%20MAML%20using%20the%0Astandard%20task%20sampling%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13052v1&entry.124074799=Read"},
{"title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home", "author": "Viktor Moskvoretskii and Maria Lysyuk and Mikhail Salnikov and Nikolay Ivanov and Sergey Pletenev and Daria Galimzianova and Nikita Krayko and Vasily Konovalov and Irina Nikishina and Alexander Panchenko", "abstract": "  Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.\n", "link": "http://arxiv.org/abs/2501.12835v1", "date": "2025-01-22", "relevancy": 2.1202, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5556}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5254}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Retrieval%20Without%20Self-Knowledge%3F%20Bringing%20Uncertainty%20Back%0A%20%20Home&body=Title%3A%20Adaptive%20Retrieval%20Without%20Self-Knowledge%3F%20Bringing%20Uncertainty%20Back%0A%20%20Home%0AAuthor%3A%20Viktor%20Moskvoretskii%20and%20Maria%20Lysyuk%20and%20Mikhail%20Salnikov%20and%20Nikolay%20Ivanov%20and%20Sergey%20Pletenev%20and%20Daria%20Galimzianova%20and%20Nikita%20Krayko%20and%20Vasily%20Konovalov%20and%20Irina%20Nikishina%20and%20Alexander%20Panchenko%0AAbstract%3A%20%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20improves%20correctness%20of%20Question%0AAnswering%20%28QA%29%20and%20addresses%20hallucinations%20in%20Large%20Language%20Models%20%28LLMs%29%2C%0Ayet%20greatly%20increase%20computational%20costs.%20Besides%2C%20RAG%20is%20not%20always%20needed%20as%0Amay%20introduce%20irrelevant%20information.%20Recent%20adaptive%20retrieval%20methods%0Aintegrate%20LLMs%27%20intrinsic%20knowledge%20with%20external%20information%20appealing%20to%20LLM%0Aself-knowledge%2C%20but%20they%20often%20neglect%20efficiency%20evaluations%20and%20comparisons%0Awith%20uncertainty%20estimation%20techniques.%20We%20bridge%20this%20gap%20by%20conducting%20a%0Acomprehensive%20analysis%20of%2035%20adaptive%20retrieval%20methods%2C%20including%208%20recent%0Aapproaches%20and%2027%20uncertainty%20estimation%20techniques%2C%20across%206%20datasets%20using%2010%0Ametrics%20for%20QA%20performance%2C%20self-knowledge%2C%20and%20efficiency.%20Our%20findings%20show%0Athat%20uncertainty%20estimation%20techniques%20often%20outperform%20complex%20pipelines%20in%0Aterms%20of%20efficiency%20and%20self-knowledge%2C%20while%20maintaining%20comparable%20QA%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Retrieval%2520Without%2520Self-Knowledge%253F%2520Bringing%2520Uncertainty%2520Back%250A%2520%2520Home%26entry.906535625%3DViktor%2520Moskvoretskii%2520and%2520Maria%2520Lysyuk%2520and%2520Mikhail%2520Salnikov%2520and%2520Nikolay%2520Ivanov%2520and%2520Sergey%2520Pletenev%2520and%2520Daria%2520Galimzianova%2520and%2520Nikita%2520Krayko%2520and%2520Vasily%2520Konovalov%2520and%2520Irina%2520Nikishina%2520and%2520Alexander%2520Panchenko%26entry.1292438233%3D%2520%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520improves%2520correctness%2520of%2520Question%250AAnswering%2520%2528QA%2529%2520and%2520addresses%2520hallucinations%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%250Ayet%2520greatly%2520increase%2520computational%2520costs.%2520Besides%252C%2520RAG%2520is%2520not%2520always%2520needed%2520as%250Amay%2520introduce%2520irrelevant%2520information.%2520Recent%2520adaptive%2520retrieval%2520methods%250Aintegrate%2520LLMs%2527%2520intrinsic%2520knowledge%2520with%2520external%2520information%2520appealing%2520to%2520LLM%250Aself-knowledge%252C%2520but%2520they%2520often%2520neglect%2520efficiency%2520evaluations%2520and%2520comparisons%250Awith%2520uncertainty%2520estimation%2520techniques.%2520We%2520bridge%2520this%2520gap%2520by%2520conducting%2520a%250Acomprehensive%2520analysis%2520of%252035%2520adaptive%2520retrieval%2520methods%252C%2520including%25208%2520recent%250Aapproaches%2520and%252027%2520uncertainty%2520estimation%2520techniques%252C%2520across%25206%2520datasets%2520using%252010%250Ametrics%2520for%2520QA%2520performance%252C%2520self-knowledge%252C%2520and%2520efficiency.%2520Our%2520findings%2520show%250Athat%2520uncertainty%2520estimation%2520techniques%2520often%2520outperform%2520complex%2520pipelines%2520in%250Aterms%2520of%2520efficiency%2520and%2520self-knowledge%252C%2520while%2520maintaining%2520comparable%2520QA%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Retrieval%20Without%20Self-Knowledge%3F%20Bringing%20Uncertainty%20Back%0A%20%20Home&entry.906535625=Viktor%20Moskvoretskii%20and%20Maria%20Lysyuk%20and%20Mikhail%20Salnikov%20and%20Nikolay%20Ivanov%20and%20Sergey%20Pletenev%20and%20Daria%20Galimzianova%20and%20Nikita%20Krayko%20and%20Vasily%20Konovalov%20and%20Irina%20Nikishina%20and%20Alexander%20Panchenko&entry.1292438233=%20%20Retrieval%20Augmented%20Generation%20%28RAG%29%20improves%20correctness%20of%20Question%0AAnswering%20%28QA%29%20and%20addresses%20hallucinations%20in%20Large%20Language%20Models%20%28LLMs%29%2C%0Ayet%20greatly%20increase%20computational%20costs.%20Besides%2C%20RAG%20is%20not%20always%20needed%20as%0Amay%20introduce%20irrelevant%20information.%20Recent%20adaptive%20retrieval%20methods%0Aintegrate%20LLMs%27%20intrinsic%20knowledge%20with%20external%20information%20appealing%20to%20LLM%0Aself-knowledge%2C%20but%20they%20often%20neglect%20efficiency%20evaluations%20and%20comparisons%0Awith%20uncertainty%20estimation%20techniques.%20We%20bridge%20this%20gap%20by%20conducting%20a%0Acomprehensive%20analysis%20of%2035%20adaptive%20retrieval%20methods%2C%20including%208%20recent%0Aapproaches%20and%2027%20uncertainty%20estimation%20techniques%2C%20across%206%20datasets%20using%2010%0Ametrics%20for%20QA%20performance%2C%20self-knowledge%2C%20and%20efficiency.%20Our%20findings%20show%0Athat%20uncertainty%20estimation%20techniques%20often%20outperform%20complex%20pipelines%20in%0Aterms%20of%20efficiency%20and%20self-knowledge%2C%20while%20maintaining%20comparable%20QA%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12835v1&entry.124074799=Read"},
{"title": "SMART-Vision: Survey of Modern Action Recognition Techniques in Vision", "author": "Ali K. AlShami and Ryan Rabinowitz and Khang Lam and Yousra Shleibik and Melkamu Mersha and Terrance Boult and Jugal Kalita", "abstract": "  Human Action Recognition (HAR) is a challenging domain in computer vision,\ninvolving recognizing complex patterns by analyzing the spatiotemporal dynamics\nof individuals' movements in videos. These patterns arise in sequential data,\nsuch as video frames, which are often essential to accurately distinguish\nactions that would be ambiguous in a single image. HAR has garnered\nconsiderable interest due to its broad applicability, ranging from robotics and\nsurveillance systems to sports motion analysis, healthcare, and the burgeoning\nfield of autonomous vehicles. While several taxonomies have been proposed to\ncategorize HAR approaches in surveys, they often overlook hybrid methodologies\nand fail to demonstrate how different models incorporate various architectures\nand modalities. In this comprehensive survey, we present the novel SMART-Vision\ntaxonomy, which illustrates how innovations in deep learning for HAR complement\none another, leading to hybrid approaches beyond traditional categories. Our\nsurvey provides a clear roadmap from foundational HAR works to current\nstate-of-the-art systems, highlighting emerging research directions and\naddressing unresolved challenges in discussion sections for architectures\nwithin the HAR domain. We provide details of the research datasets that various\napproaches used to measure and compare goodness HAR approaches. We also explore\nthe rapidly emerging field of Open-HAR systems, which challenges HAR systems by\npresenting samples from unknown, novel classes during test time.\n", "link": "http://arxiv.org/abs/2501.13066v1", "date": "2025-01-22", "relevancy": 2.1128, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5288}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART-Vision%3A%20Survey%20of%20Modern%20Action%20Recognition%20Techniques%20in%20Vision&body=Title%3A%20SMART-Vision%3A%20Survey%20of%20Modern%20Action%20Recognition%20Techniques%20in%20Vision%0AAuthor%3A%20Ali%20K.%20AlShami%20and%20Ryan%20Rabinowitz%20and%20Khang%20Lam%20and%20Yousra%20Shleibik%20and%20Melkamu%20Mersha%20and%20Terrance%20Boult%20and%20Jugal%20Kalita%0AAbstract%3A%20%20%20Human%20Action%20Recognition%20%28HAR%29%20is%20a%20challenging%20domain%20in%20computer%20vision%2C%0Ainvolving%20recognizing%20complex%20patterns%20by%20analyzing%20the%20spatiotemporal%20dynamics%0Aof%20individuals%27%20movements%20in%20videos.%20These%20patterns%20arise%20in%20sequential%20data%2C%0Asuch%20as%20video%20frames%2C%20which%20are%20often%20essential%20to%20accurately%20distinguish%0Aactions%20that%20would%20be%20ambiguous%20in%20a%20single%20image.%20HAR%20has%20garnered%0Aconsiderable%20interest%20due%20to%20its%20broad%20applicability%2C%20ranging%20from%20robotics%20and%0Asurveillance%20systems%20to%20sports%20motion%20analysis%2C%20healthcare%2C%20and%20the%20burgeoning%0Afield%20of%20autonomous%20vehicles.%20While%20several%20taxonomies%20have%20been%20proposed%20to%0Acategorize%20HAR%20approaches%20in%20surveys%2C%20they%20often%20overlook%20hybrid%20methodologies%0Aand%20fail%20to%20demonstrate%20how%20different%20models%20incorporate%20various%20architectures%0Aand%20modalities.%20In%20this%20comprehensive%20survey%2C%20we%20present%20the%20novel%20SMART-Vision%0Ataxonomy%2C%20which%20illustrates%20how%20innovations%20in%20deep%20learning%20for%20HAR%20complement%0Aone%20another%2C%20leading%20to%20hybrid%20approaches%20beyond%20traditional%20categories.%20Our%0Asurvey%20provides%20a%20clear%20roadmap%20from%20foundational%20HAR%20works%20to%20current%0Astate-of-the-art%20systems%2C%20highlighting%20emerging%20research%20directions%20and%0Aaddressing%20unresolved%20challenges%20in%20discussion%20sections%20for%20architectures%0Awithin%20the%20HAR%20domain.%20We%20provide%20details%20of%20the%20research%20datasets%20that%20various%0Aapproaches%20used%20to%20measure%20and%20compare%20goodness%20HAR%20approaches.%20We%20also%20explore%0Athe%20rapidly%20emerging%20field%20of%20Open-HAR%20systems%2C%20which%20challenges%20HAR%20systems%20by%0Apresenting%20samples%20from%20unknown%2C%20novel%20classes%20during%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART-Vision%253A%2520Survey%2520of%2520Modern%2520Action%2520Recognition%2520Techniques%2520in%2520Vision%26entry.906535625%3DAli%2520K.%2520AlShami%2520and%2520Ryan%2520Rabinowitz%2520and%2520Khang%2520Lam%2520and%2520Yousra%2520Shleibik%2520and%2520Melkamu%2520Mersha%2520and%2520Terrance%2520Boult%2520and%2520Jugal%2520Kalita%26entry.1292438233%3D%2520%2520Human%2520Action%2520Recognition%2520%2528HAR%2529%2520is%2520a%2520challenging%2520domain%2520in%2520computer%2520vision%252C%250Ainvolving%2520recognizing%2520complex%2520patterns%2520by%2520analyzing%2520the%2520spatiotemporal%2520dynamics%250Aof%2520individuals%2527%2520movements%2520in%2520videos.%2520These%2520patterns%2520arise%2520in%2520sequential%2520data%252C%250Asuch%2520as%2520video%2520frames%252C%2520which%2520are%2520often%2520essential%2520to%2520accurately%2520distinguish%250Aactions%2520that%2520would%2520be%2520ambiguous%2520in%2520a%2520single%2520image.%2520HAR%2520has%2520garnered%250Aconsiderable%2520interest%2520due%2520to%2520its%2520broad%2520applicability%252C%2520ranging%2520from%2520robotics%2520and%250Asurveillance%2520systems%2520to%2520sports%2520motion%2520analysis%252C%2520healthcare%252C%2520and%2520the%2520burgeoning%250Afield%2520of%2520autonomous%2520vehicles.%2520While%2520several%2520taxonomies%2520have%2520been%2520proposed%2520to%250Acategorize%2520HAR%2520approaches%2520in%2520surveys%252C%2520they%2520often%2520overlook%2520hybrid%2520methodologies%250Aand%2520fail%2520to%2520demonstrate%2520how%2520different%2520models%2520incorporate%2520various%2520architectures%250Aand%2520modalities.%2520In%2520this%2520comprehensive%2520survey%252C%2520we%2520present%2520the%2520novel%2520SMART-Vision%250Ataxonomy%252C%2520which%2520illustrates%2520how%2520innovations%2520in%2520deep%2520learning%2520for%2520HAR%2520complement%250Aone%2520another%252C%2520leading%2520to%2520hybrid%2520approaches%2520beyond%2520traditional%2520categories.%2520Our%250Asurvey%2520provides%2520a%2520clear%2520roadmap%2520from%2520foundational%2520HAR%2520works%2520to%2520current%250Astate-of-the-art%2520systems%252C%2520highlighting%2520emerging%2520research%2520directions%2520and%250Aaddressing%2520unresolved%2520challenges%2520in%2520discussion%2520sections%2520for%2520architectures%250Awithin%2520the%2520HAR%2520domain.%2520We%2520provide%2520details%2520of%2520the%2520research%2520datasets%2520that%2520various%250Aapproaches%2520used%2520to%2520measure%2520and%2520compare%2520goodness%2520HAR%2520approaches.%2520We%2520also%2520explore%250Athe%2520rapidly%2520emerging%2520field%2520of%2520Open-HAR%2520systems%252C%2520which%2520challenges%2520HAR%2520systems%2520by%250Apresenting%2520samples%2520from%2520unknown%252C%2520novel%2520classes%2520during%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART-Vision%3A%20Survey%20of%20Modern%20Action%20Recognition%20Techniques%20in%20Vision&entry.906535625=Ali%20K.%20AlShami%20and%20Ryan%20Rabinowitz%20and%20Khang%20Lam%20and%20Yousra%20Shleibik%20and%20Melkamu%20Mersha%20and%20Terrance%20Boult%20and%20Jugal%20Kalita&entry.1292438233=%20%20Human%20Action%20Recognition%20%28HAR%29%20is%20a%20challenging%20domain%20in%20computer%20vision%2C%0Ainvolving%20recognizing%20complex%20patterns%20by%20analyzing%20the%20spatiotemporal%20dynamics%0Aof%20individuals%27%20movements%20in%20videos.%20These%20patterns%20arise%20in%20sequential%20data%2C%0Asuch%20as%20video%20frames%2C%20which%20are%20often%20essential%20to%20accurately%20distinguish%0Aactions%20that%20would%20be%20ambiguous%20in%20a%20single%20image.%20HAR%20has%20garnered%0Aconsiderable%20interest%20due%20to%20its%20broad%20applicability%2C%20ranging%20from%20robotics%20and%0Asurveillance%20systems%20to%20sports%20motion%20analysis%2C%20healthcare%2C%20and%20the%20burgeoning%0Afield%20of%20autonomous%20vehicles.%20While%20several%20taxonomies%20have%20been%20proposed%20to%0Acategorize%20HAR%20approaches%20in%20surveys%2C%20they%20often%20overlook%20hybrid%20methodologies%0Aand%20fail%20to%20demonstrate%20how%20different%20models%20incorporate%20various%20architectures%0Aand%20modalities.%20In%20this%20comprehensive%20survey%2C%20we%20present%20the%20novel%20SMART-Vision%0Ataxonomy%2C%20which%20illustrates%20how%20innovations%20in%20deep%20learning%20for%20HAR%20complement%0Aone%20another%2C%20leading%20to%20hybrid%20approaches%20beyond%20traditional%20categories.%20Our%0Asurvey%20provides%20a%20clear%20roadmap%20from%20foundational%20HAR%20works%20to%20current%0Astate-of-the-art%20systems%2C%20highlighting%20emerging%20research%20directions%20and%0Aaddressing%20unresolved%20challenges%20in%20discussion%20sections%20for%20architectures%0Awithin%20the%20HAR%20domain.%20We%20provide%20details%20of%20the%20research%20datasets%20that%20various%0Aapproaches%20used%20to%20measure%20and%20compare%20goodness%20HAR%20approaches.%20We%20also%20explore%0Athe%20rapidly%20emerging%20field%20of%20Open-HAR%20systems%2C%20which%20challenges%20HAR%20systems%20by%0Apresenting%20samples%20from%20unknown%2C%20novel%20classes%20during%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13066v1&entry.124074799=Read"},
{"title": "Non-adaptive Learning of Random Hypergraphs with Queries", "author": "Bethany Austhof and Lev Reyzin and Erasmo Tani", "abstract": "  We study the problem of learning a hidden hypergraph $G=(V,E)$ by making a\nsingle batch of queries (non-adaptively). We consider the hyperedge detection\nmodel, in which every query must be of the form:\n  ``Does this set $S\\subseteq V$ contain at least one full hyperedge?''\n  In this model, it is known that there is no algorithm that allows to\nnon-adaptively learn arbitrary hypergraphs by making fewer than\n$\\Omega(\\min\\{m^2\\log n, n^2\\})$ even when the hypergraph is constrained to be\n$2$-uniform (i.e. the hypergraph is simply a graph). Recently, Li et al.\novercame this lower bound in the setting in which $G$ is a graph by assuming\nthat the graph learned is sampled from an Erd\\H{o}s-R\\'enyi model. We\ngeneralize the result of Li et al. to the setting of random $k$-uniform\nhypergraphs. To achieve this result, we leverage a novel equivalence between\nthe problem of learning a single hyperedge and the standard group testing\nproblem. This latter result may also be of independent interest.\n", "link": "http://arxiv.org/abs/2501.12771v1", "date": "2025-01-22", "relevancy": 2.1099, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.429}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4197}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-adaptive%20Learning%20of%20Random%20Hypergraphs%20with%20Queries&body=Title%3A%20Non-adaptive%20Learning%20of%20Random%20Hypergraphs%20with%20Queries%0AAuthor%3A%20Bethany%20Austhof%20and%20Lev%20Reyzin%20and%20Erasmo%20Tani%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20a%20hidden%20hypergraph%20%24G%3D%28V%2CE%29%24%20by%20making%20a%0Asingle%20batch%20of%20queries%20%28non-adaptively%29.%20We%20consider%20the%20hyperedge%20detection%0Amodel%2C%20in%20which%20every%20query%20must%20be%20of%20the%20form%3A%0A%20%20%60%60Does%20this%20set%20%24S%5Csubseteq%20V%24%20contain%20at%20least%20one%20full%20hyperedge%3F%27%27%0A%20%20In%20this%20model%2C%20it%20is%20known%20that%20there%20is%20no%20algorithm%20that%20allows%20to%0Anon-adaptively%20learn%20arbitrary%20hypergraphs%20by%20making%20fewer%20than%0A%24%5COmega%28%5Cmin%5C%7Bm%5E2%5Clog%20n%2C%20n%5E2%5C%7D%29%24%20even%20when%20the%20hypergraph%20is%20constrained%20to%20be%0A%242%24-uniform%20%28i.e.%20the%20hypergraph%20is%20simply%20a%20graph%29.%20Recently%2C%20Li%20et%20al.%0Aovercame%20this%20lower%20bound%20in%20the%20setting%20in%20which%20%24G%24%20is%20a%20graph%20by%20assuming%0Athat%20the%20graph%20learned%20is%20sampled%20from%20an%20Erd%5CH%7Bo%7Ds-R%5C%27enyi%20model.%20We%0Ageneralize%20the%20result%20of%20Li%20et%20al.%20to%20the%20setting%20of%20random%20%24k%24-uniform%0Ahypergraphs.%20To%20achieve%20this%20result%2C%20we%20leverage%20a%20novel%20equivalence%20between%0Athe%20problem%20of%20learning%20a%20single%20hyperedge%20and%20the%20standard%20group%20testing%0Aproblem.%20This%20latter%20result%20may%20also%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-adaptive%2520Learning%2520of%2520Random%2520Hypergraphs%2520with%2520Queries%26entry.906535625%3DBethany%2520Austhof%2520and%2520Lev%2520Reyzin%2520and%2520Erasmo%2520Tani%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520learning%2520a%2520hidden%2520hypergraph%2520%2524G%253D%2528V%252CE%2529%2524%2520by%2520making%2520a%250Asingle%2520batch%2520of%2520queries%2520%2528non-adaptively%2529.%2520We%2520consider%2520the%2520hyperedge%2520detection%250Amodel%252C%2520in%2520which%2520every%2520query%2520must%2520be%2520of%2520the%2520form%253A%250A%2520%2520%2560%2560Does%2520this%2520set%2520%2524S%255Csubseteq%2520V%2524%2520contain%2520at%2520least%2520one%2520full%2520hyperedge%253F%2527%2527%250A%2520%2520In%2520this%2520model%252C%2520it%2520is%2520known%2520that%2520there%2520is%2520no%2520algorithm%2520that%2520allows%2520to%250Anon-adaptively%2520learn%2520arbitrary%2520hypergraphs%2520by%2520making%2520fewer%2520than%250A%2524%255COmega%2528%255Cmin%255C%257Bm%255E2%255Clog%2520n%252C%2520n%255E2%255C%257D%2529%2524%2520even%2520when%2520the%2520hypergraph%2520is%2520constrained%2520to%2520be%250A%25242%2524-uniform%2520%2528i.e.%2520the%2520hypergraph%2520is%2520simply%2520a%2520graph%2529.%2520Recently%252C%2520Li%2520et%2520al.%250Aovercame%2520this%2520lower%2520bound%2520in%2520the%2520setting%2520in%2520which%2520%2524G%2524%2520is%2520a%2520graph%2520by%2520assuming%250Athat%2520the%2520graph%2520learned%2520is%2520sampled%2520from%2520an%2520Erd%255CH%257Bo%257Ds-R%255C%2527enyi%2520model.%2520We%250Ageneralize%2520the%2520result%2520of%2520Li%2520et%2520al.%2520to%2520the%2520setting%2520of%2520random%2520%2524k%2524-uniform%250Ahypergraphs.%2520To%2520achieve%2520this%2520result%252C%2520we%2520leverage%2520a%2520novel%2520equivalence%2520between%250Athe%2520problem%2520of%2520learning%2520a%2520single%2520hyperedge%2520and%2520the%2520standard%2520group%2520testing%250Aproblem.%2520This%2520latter%2520result%2520may%2520also%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-adaptive%20Learning%20of%20Random%20Hypergraphs%20with%20Queries&entry.906535625=Bethany%20Austhof%20and%20Lev%20Reyzin%20and%20Erasmo%20Tani&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20a%20hidden%20hypergraph%20%24G%3D%28V%2CE%29%24%20by%20making%20a%0Asingle%20batch%20of%20queries%20%28non-adaptively%29.%20We%20consider%20the%20hyperedge%20detection%0Amodel%2C%20in%20which%20every%20query%20must%20be%20of%20the%20form%3A%0A%20%20%60%60Does%20this%20set%20%24S%5Csubseteq%20V%24%20contain%20at%20least%20one%20full%20hyperedge%3F%27%27%0A%20%20In%20this%20model%2C%20it%20is%20known%20that%20there%20is%20no%20algorithm%20that%20allows%20to%0Anon-adaptively%20learn%20arbitrary%20hypergraphs%20by%20making%20fewer%20than%0A%24%5COmega%28%5Cmin%5C%7Bm%5E2%5Clog%20n%2C%20n%5E2%5C%7D%29%24%20even%20when%20the%20hypergraph%20is%20constrained%20to%20be%0A%242%24-uniform%20%28i.e.%20the%20hypergraph%20is%20simply%20a%20graph%29.%20Recently%2C%20Li%20et%20al.%0Aovercame%20this%20lower%20bound%20in%20the%20setting%20in%20which%20%24G%24%20is%20a%20graph%20by%20assuming%0Athat%20the%20graph%20learned%20is%20sampled%20from%20an%20Erd%5CH%7Bo%7Ds-R%5C%27enyi%20model.%20We%0Ageneralize%20the%20result%20of%20Li%20et%20al.%20to%20the%20setting%20of%20random%20%24k%24-uniform%0Ahypergraphs.%20To%20achieve%20this%20result%2C%20we%20leverage%20a%20novel%20equivalence%20between%0Athe%20problem%20of%20learning%20a%20single%20hyperedge%20and%20the%20standard%20group%20testing%0Aproblem.%20This%20latter%20result%20may%20also%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12771v1&entry.124074799=Read"},
{"title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning", "author": " DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang", "abstract": "  We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.\n", "link": "http://arxiv.org/abs/2501.12948v1", "date": "2025-01-22", "relevancy": 2.0982, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSeek-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20DeepSeek-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20%20DeepSeek-AI%20and%20Daya%20Guo%20and%20Dejian%20Yang%20and%20Haowei%20Zhang%20and%20Junxiao%20Song%20and%20Ruoyu%20Zhang%20and%20Runxin%20Xu%20and%20Qihao%20Zhu%20and%20Shirong%20Ma%20and%20Peiyi%20Wang%20and%20Xiao%20Bi%20and%20Xiaokang%20Zhang%20and%20Xingkai%20Yu%20and%20Yu%20Wu%20and%20Z.%20F.%20Wu%20and%20Zhibin%20Gou%20and%20Zhihong%20Shao%20and%20Zhuoshu%20Li%20and%20Ziyi%20Gao%20and%20Aixin%20Liu%20and%20Bing%20Xue%20and%20Bingxuan%20Wang%20and%20Bochao%20Wu%20and%20Bei%20Feng%20and%20Chengda%20Lu%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chenyu%20Zhang%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Deli%20Chen%20and%20Dongjie%20Ji%20and%20Erhang%20Li%20and%20Fangyun%20Lin%20and%20Fucong%20Dai%20and%20Fuli%20Luo%20and%20Guangbo%20Hao%20and%20Guanting%20Chen%20and%20Guowei%20Li%20and%20H.%20Zhang%20and%20Han%20Bao%20and%20Hanwei%20Xu%20and%20Haocheng%20Wang%20and%20Honghui%20Ding%20and%20Huajian%20Xin%20and%20Huazuo%20Gao%20and%20Hui%20Qu%20and%20Hui%20Li%20and%20Jianzhong%20Guo%20and%20Jiashi%20Li%20and%20Jiawei%20Wang%20and%20Jingchang%20Chen%20and%20Jingyang%20Yuan%20and%20Junjie%20Qiu%20and%20Junlong%20Li%20and%20J.%20L.%20Cai%20and%20Jiaqi%20Ni%20and%20Jian%20Liang%20and%20Jin%20Chen%20and%20Kai%20Dong%20and%20Kai%20Hu%20and%20Kaige%20Gao%20and%20Kang%20Guan%20and%20Kexin%20Huang%20and%20Kuai%20Yu%20and%20Lean%20Wang%20and%20Lecong%20Zhang%20and%20Liang%20Zhao%20and%20Litong%20Wang%20and%20Liyue%20Zhang%20and%20Lei%20Xu%20and%20Leyi%20Xia%20and%20Mingchuan%20Zhang%20and%20Minghua%20Zhang%20and%20Minghui%20Tang%20and%20Meng%20Li%20and%20Miaojun%20Wang%20and%20Mingming%20Li%20and%20Ning%20Tian%20and%20Panpan%20Huang%20and%20Peng%20Zhang%20and%20Qiancheng%20Wang%20and%20Qinyu%20Chen%20and%20Qiushi%20Du%20and%20Ruiqi%20Ge%20and%20Ruisong%20Zhang%20and%20Ruizhe%20Pan%20and%20Runji%20Wang%20and%20R.%20J.%20Chen%20and%20R.%20L.%20Jin%20and%20Ruyi%20Chen%20and%20Shanghao%20Lu%20and%20Shangyan%20Zhou%20and%20Shanhuang%20Chen%20and%20Shengfeng%20Ye%20and%20Shiyu%20Wang%20and%20Shuiping%20Yu%20and%20Shunfeng%20Zhou%20and%20Shuting%20Pan%20and%20S.%20S.%20Li%20and%20Shuang%20Zhou%20and%20Shaoqing%20Wu%20and%20Shengfeng%20Ye%20and%20Tao%20Yun%20and%20Tian%20Pei%20and%20Tianyu%20Sun%20and%20T.%20Wang%20and%20Wangding%20Zeng%20and%20Wanjia%20Zhao%20and%20Wen%20Liu%20and%20Wenfeng%20Liang%20and%20Wenjun%20Gao%20and%20Wenqin%20Yu%20and%20Wentao%20Zhang%20and%20W.%20L.%20Xiao%20and%20Wei%20An%20and%20Xiaodong%20Liu%20and%20Xiaohan%20Wang%20and%20Xiaokang%20Chen%20and%20Xiaotao%20Nie%20and%20Xin%20Cheng%20and%20Xin%20Liu%20and%20Xin%20Xie%20and%20Xingchao%20Liu%20and%20Xinyu%20Yang%20and%20Xinyuan%20Li%20and%20Xuecheng%20Su%20and%20Xuheng%20Lin%20and%20X.%20Q.%20Li%20and%20Xiangyue%20Jin%20and%20Xiaojin%20Shen%20and%20Xiaosha%20Chen%20and%20Xiaowen%20Sun%20and%20Xiaoxiang%20Wang%20and%20Xinnan%20Song%20and%20Xinyi%20Zhou%20and%20Xianzu%20Wang%20and%20Xinxia%20Shan%20and%20Y.%20K.%20Li%20and%20Y.%20Q.%20Wang%20and%20Y.%20X.%20Wei%20and%20Yang%20Zhang%20and%20Yanhong%20Xu%20and%20Yao%20Li%20and%20Yao%20Zhao%20and%20Yaofeng%20Sun%20and%20Yaohui%20Wang%20and%20Yi%20Yu%20and%20Yichao%20Zhang%20and%20Yifan%20Shi%20and%20Yiliang%20Xiong%20and%20Ying%20He%20and%20Yishi%20Piao%20and%20Yisong%20Wang%20and%20Yixuan%20Tan%20and%20Yiyang%20Ma%20and%20Yiyuan%20Liu%20and%20Yongqiang%20Guo%20and%20Yuan%20Ou%20and%20Yuduan%20Wang%20and%20Yue%20Gong%20and%20Yuheng%20Zou%20and%20Yujia%20He%20and%20Yunfan%20Xiong%20and%20Yuxiang%20Luo%20and%20Yuxiang%20You%20and%20Yuxuan%20Liu%20and%20Yuyang%20Zhou%20and%20Y.%20X.%20Zhu%20and%20Yanhong%20Xu%20and%20Yanping%20Huang%20and%20Yaohui%20Li%20and%20Yi%20Zheng%20and%20Yuchen%20Zhu%20and%20Yunxian%20Ma%20and%20Ying%20Tang%20and%20Yukun%20Zha%20and%20Yuting%20Yan%20and%20Z.%20Z.%20Ren%20and%20Zehui%20Ren%20and%20Zhangli%20Sha%20and%20Zhe%20Fu%20and%20Zhean%20Xu%20and%20Zhenda%20Xie%20and%20Zhengyan%20Zhang%20and%20Zhewen%20Hao%20and%20Zhicheng%20Ma%20and%20Zhigang%20Yan%20and%20Zhiyu%20Wu%20and%20Zihui%20Gu%20and%20Zijia%20Zhu%20and%20Zijun%20Liu%20and%20Zilin%20Li%20and%20Ziwei%20Xie%20and%20Ziyang%20Song%20and%20Zizheng%20Pan%20and%20Zhen%20Huang%20and%20Zhipeng%20Xu%20and%20Zhongyu%20Zhang%20and%20Zhen%20Zhang%0AAbstract%3A%20%20%20We%20introduce%20our%20first-generation%20reasoning%20models%2C%20DeepSeek-R1-Zero%20and%0ADeepSeek-R1.%20DeepSeek-R1-Zero%2C%20a%20model%20trained%20via%20large-scale%20reinforcement%0Alearning%20%28RL%29%20without%20supervised%20fine-tuning%20%28SFT%29%20as%20a%20preliminary%20step%2C%0Ademonstrates%20remarkable%20reasoning%20capabilities.%20Through%20RL%2C%20DeepSeek-R1-Zero%0Anaturally%20emerges%20with%20numerous%20powerful%20and%20intriguing%20reasoning%20behaviors.%0AHowever%2C%20it%20encounters%20challenges%20such%20as%20poor%20readability%2C%20and%20language%0Amixing.%20To%20address%20these%20issues%20and%20further%20enhance%20reasoning%20performance%2C%20we%0Aintroduce%20DeepSeek-R1%2C%20which%20incorporates%20multi-stage%20training%20and%20cold-start%0Adata%20before%20RL.%20DeepSeek-R1%20achieves%20performance%20comparable%20to%20OpenAI-o1-1217%0Aon%20reasoning%20tasks.%20To%20support%20the%20research%20community%2C%20we%20open-source%0ADeepSeek-R1-Zero%2C%20DeepSeek-R1%2C%20and%20six%20dense%20models%20%281.5B%2C%207B%2C%208B%2C%2014B%2C%2032B%2C%0A70B%29%20distilled%20from%20DeepSeek-R1%20based%20on%20Qwen%20and%20Llama.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSeek-R1%253A%2520Incentivizing%2520Reasoning%2520Capability%2520in%2520LLMs%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3D%2520DeepSeek-AI%2520and%2520Daya%2520Guo%2520and%2520Dejian%2520Yang%2520and%2520Haowei%2520Zhang%2520and%2520Junxiao%2520Song%2520and%2520Ruoyu%2520Zhang%2520and%2520Runxin%2520Xu%2520and%2520Qihao%2520Zhu%2520and%2520Shirong%2520Ma%2520and%2520Peiyi%2520Wang%2520and%2520Xiao%2520Bi%2520and%2520Xiaokang%2520Zhang%2520and%2520Xingkai%2520Yu%2520and%2520Yu%2520Wu%2520and%2520Z.%2520F.%2520Wu%2520and%2520Zhibin%2520Gou%2520and%2520Zhihong%2520Shao%2520and%2520Zhuoshu%2520Li%2520and%2520Ziyi%2520Gao%2520and%2520Aixin%2520Liu%2520and%2520Bing%2520Xue%2520and%2520Bingxuan%2520Wang%2520and%2520Bochao%2520Wu%2520and%2520Bei%2520Feng%2520and%2520Chengda%2520Lu%2520and%2520Chenggang%2520Zhao%2520and%2520Chengqi%2520Deng%2520and%2520Chenyu%2520Zhang%2520and%2520Chong%2520Ruan%2520and%2520Damai%2520Dai%2520and%2520Deli%2520Chen%2520and%2520Dongjie%2520Ji%2520and%2520Erhang%2520Li%2520and%2520Fangyun%2520Lin%2520and%2520Fucong%2520Dai%2520and%2520Fuli%2520Luo%2520and%2520Guangbo%2520Hao%2520and%2520Guanting%2520Chen%2520and%2520Guowei%2520Li%2520and%2520H.%2520Zhang%2520and%2520Han%2520Bao%2520and%2520Hanwei%2520Xu%2520and%2520Haocheng%2520Wang%2520and%2520Honghui%2520Ding%2520and%2520Huajian%2520Xin%2520and%2520Huazuo%2520Gao%2520and%2520Hui%2520Qu%2520and%2520Hui%2520Li%2520and%2520Jianzhong%2520Guo%2520and%2520Jiashi%2520Li%2520and%2520Jiawei%2520Wang%2520and%2520Jingchang%2520Chen%2520and%2520Jingyang%2520Yuan%2520and%2520Junjie%2520Qiu%2520and%2520Junlong%2520Li%2520and%2520J.%2520L.%2520Cai%2520and%2520Jiaqi%2520Ni%2520and%2520Jian%2520Liang%2520and%2520Jin%2520Chen%2520and%2520Kai%2520Dong%2520and%2520Kai%2520Hu%2520and%2520Kaige%2520Gao%2520and%2520Kang%2520Guan%2520and%2520Kexin%2520Huang%2520and%2520Kuai%2520Yu%2520and%2520Lean%2520Wang%2520and%2520Lecong%2520Zhang%2520and%2520Liang%2520Zhao%2520and%2520Litong%2520Wang%2520and%2520Liyue%2520Zhang%2520and%2520Lei%2520Xu%2520and%2520Leyi%2520Xia%2520and%2520Mingchuan%2520Zhang%2520and%2520Minghua%2520Zhang%2520and%2520Minghui%2520Tang%2520and%2520Meng%2520Li%2520and%2520Miaojun%2520Wang%2520and%2520Mingming%2520Li%2520and%2520Ning%2520Tian%2520and%2520Panpan%2520Huang%2520and%2520Peng%2520Zhang%2520and%2520Qiancheng%2520Wang%2520and%2520Qinyu%2520Chen%2520and%2520Qiushi%2520Du%2520and%2520Ruiqi%2520Ge%2520and%2520Ruisong%2520Zhang%2520and%2520Ruizhe%2520Pan%2520and%2520Runji%2520Wang%2520and%2520R.%2520J.%2520Chen%2520and%2520R.%2520L.%2520Jin%2520and%2520Ruyi%2520Chen%2520and%2520Shanghao%2520Lu%2520and%2520Shangyan%2520Zhou%2520and%2520Shanhuang%2520Chen%2520and%2520Shengfeng%2520Ye%2520and%2520Shiyu%2520Wang%2520and%2520Shuiping%2520Yu%2520and%2520Shunfeng%2520Zhou%2520and%2520Shuting%2520Pan%2520and%2520S.%2520S.%2520Li%2520and%2520Shuang%2520Zhou%2520and%2520Shaoqing%2520Wu%2520and%2520Shengfeng%2520Ye%2520and%2520Tao%2520Yun%2520and%2520Tian%2520Pei%2520and%2520Tianyu%2520Sun%2520and%2520T.%2520Wang%2520and%2520Wangding%2520Zeng%2520and%2520Wanjia%2520Zhao%2520and%2520Wen%2520Liu%2520and%2520Wenfeng%2520Liang%2520and%2520Wenjun%2520Gao%2520and%2520Wenqin%2520Yu%2520and%2520Wentao%2520Zhang%2520and%2520W.%2520L.%2520Xiao%2520and%2520Wei%2520An%2520and%2520Xiaodong%2520Liu%2520and%2520Xiaohan%2520Wang%2520and%2520Xiaokang%2520Chen%2520and%2520Xiaotao%2520Nie%2520and%2520Xin%2520Cheng%2520and%2520Xin%2520Liu%2520and%2520Xin%2520Xie%2520and%2520Xingchao%2520Liu%2520and%2520Xinyu%2520Yang%2520and%2520Xinyuan%2520Li%2520and%2520Xuecheng%2520Su%2520and%2520Xuheng%2520Lin%2520and%2520X.%2520Q.%2520Li%2520and%2520Xiangyue%2520Jin%2520and%2520Xiaojin%2520Shen%2520and%2520Xiaosha%2520Chen%2520and%2520Xiaowen%2520Sun%2520and%2520Xiaoxiang%2520Wang%2520and%2520Xinnan%2520Song%2520and%2520Xinyi%2520Zhou%2520and%2520Xianzu%2520Wang%2520and%2520Xinxia%2520Shan%2520and%2520Y.%2520K.%2520Li%2520and%2520Y.%2520Q.%2520Wang%2520and%2520Y.%2520X.%2520Wei%2520and%2520Yang%2520Zhang%2520and%2520Yanhong%2520Xu%2520and%2520Yao%2520Li%2520and%2520Yao%2520Zhao%2520and%2520Yaofeng%2520Sun%2520and%2520Yaohui%2520Wang%2520and%2520Yi%2520Yu%2520and%2520Yichao%2520Zhang%2520and%2520Yifan%2520Shi%2520and%2520Yiliang%2520Xiong%2520and%2520Ying%2520He%2520and%2520Yishi%2520Piao%2520and%2520Yisong%2520Wang%2520and%2520Yixuan%2520Tan%2520and%2520Yiyang%2520Ma%2520and%2520Yiyuan%2520Liu%2520and%2520Yongqiang%2520Guo%2520and%2520Yuan%2520Ou%2520and%2520Yuduan%2520Wang%2520and%2520Yue%2520Gong%2520and%2520Yuheng%2520Zou%2520and%2520Yujia%2520He%2520and%2520Yunfan%2520Xiong%2520and%2520Yuxiang%2520Luo%2520and%2520Yuxiang%2520You%2520and%2520Yuxuan%2520Liu%2520and%2520Yuyang%2520Zhou%2520and%2520Y.%2520X.%2520Zhu%2520and%2520Yanhong%2520Xu%2520and%2520Yanping%2520Huang%2520and%2520Yaohui%2520Li%2520and%2520Yi%2520Zheng%2520and%2520Yuchen%2520Zhu%2520and%2520Yunxian%2520Ma%2520and%2520Ying%2520Tang%2520and%2520Yukun%2520Zha%2520and%2520Yuting%2520Yan%2520and%2520Z.%2520Z.%2520Ren%2520and%2520Zehui%2520Ren%2520and%2520Zhangli%2520Sha%2520and%2520Zhe%2520Fu%2520and%2520Zhean%2520Xu%2520and%2520Zhenda%2520Xie%2520and%2520Zhengyan%2520Zhang%2520and%2520Zhewen%2520Hao%2520and%2520Zhicheng%2520Ma%2520and%2520Zhigang%2520Yan%2520and%2520Zhiyu%2520Wu%2520and%2520Zihui%2520Gu%2520and%2520Zijia%2520Zhu%2520and%2520Zijun%2520Liu%2520and%2520Zilin%2520Li%2520and%2520Ziwei%2520Xie%2520and%2520Ziyang%2520Song%2520and%2520Zizheng%2520Pan%2520and%2520Zhen%2520Huang%2520and%2520Zhipeng%2520Xu%2520and%2520Zhongyu%2520Zhang%2520and%2520Zhen%2520Zhang%26entry.1292438233%3D%2520%2520We%2520introduce%2520our%2520first-generation%2520reasoning%2520models%252C%2520DeepSeek-R1-Zero%2520and%250ADeepSeek-R1.%2520DeepSeek-R1-Zero%252C%2520a%2520model%2520trained%2520via%2520large-scale%2520reinforcement%250Alearning%2520%2528RL%2529%2520without%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520as%2520a%2520preliminary%2520step%252C%250Ademonstrates%2520remarkable%2520reasoning%2520capabilities.%2520Through%2520RL%252C%2520DeepSeek-R1-Zero%250Anaturally%2520emerges%2520with%2520numerous%2520powerful%2520and%2520intriguing%2520reasoning%2520behaviors.%250AHowever%252C%2520it%2520encounters%2520challenges%2520such%2520as%2520poor%2520readability%252C%2520and%2520language%250Amixing.%2520To%2520address%2520these%2520issues%2520and%2520further%2520enhance%2520reasoning%2520performance%252C%2520we%250Aintroduce%2520DeepSeek-R1%252C%2520which%2520incorporates%2520multi-stage%2520training%2520and%2520cold-start%250Adata%2520before%2520RL.%2520DeepSeek-R1%2520achieves%2520performance%2520comparable%2520to%2520OpenAI-o1-1217%250Aon%2520reasoning%2520tasks.%2520To%2520support%2520the%2520research%2520community%252C%2520we%2520open-source%250ADeepSeek-R1-Zero%252C%2520DeepSeek-R1%252C%2520and%2520six%2520dense%2520models%2520%25281.5B%252C%25207B%252C%25208B%252C%252014B%252C%252032B%252C%250A70B%2529%2520distilled%2520from%2520DeepSeek-R1%2520based%2520on%2520Qwen%2520and%2520Llama.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%0A%20%20Reinforcement%20Learning&entry.906535625=%20DeepSeek-AI%20and%20Daya%20Guo%20and%20Dejian%20Yang%20and%20Haowei%20Zhang%20and%20Junxiao%20Song%20and%20Ruoyu%20Zhang%20and%20Runxin%20Xu%20and%20Qihao%20Zhu%20and%20Shirong%20Ma%20and%20Peiyi%20Wang%20and%20Xiao%20Bi%20and%20Xiaokang%20Zhang%20and%20Xingkai%20Yu%20and%20Yu%20Wu%20and%20Z.%20F.%20Wu%20and%20Zhibin%20Gou%20and%20Zhihong%20Shao%20and%20Zhuoshu%20Li%20and%20Ziyi%20Gao%20and%20Aixin%20Liu%20and%20Bing%20Xue%20and%20Bingxuan%20Wang%20and%20Bochao%20Wu%20and%20Bei%20Feng%20and%20Chengda%20Lu%20and%20Chenggang%20Zhao%20and%20Chengqi%20Deng%20and%20Chenyu%20Zhang%20and%20Chong%20Ruan%20and%20Damai%20Dai%20and%20Deli%20Chen%20and%20Dongjie%20Ji%20and%20Erhang%20Li%20and%20Fangyun%20Lin%20and%20Fucong%20Dai%20and%20Fuli%20Luo%20and%20Guangbo%20Hao%20and%20Guanting%20Chen%20and%20Guowei%20Li%20and%20H.%20Zhang%20and%20Han%20Bao%20and%20Hanwei%20Xu%20and%20Haocheng%20Wang%20and%20Honghui%20Ding%20and%20Huajian%20Xin%20and%20Huazuo%20Gao%20and%20Hui%20Qu%20and%20Hui%20Li%20and%20Jianzhong%20Guo%20and%20Jiashi%20Li%20and%20Jiawei%20Wang%20and%20Jingchang%20Chen%20and%20Jingyang%20Yuan%20and%20Junjie%20Qiu%20and%20Junlong%20Li%20and%20J.%20L.%20Cai%20and%20Jiaqi%20Ni%20and%20Jian%20Liang%20and%20Jin%20Chen%20and%20Kai%20Dong%20and%20Kai%20Hu%20and%20Kaige%20Gao%20and%20Kang%20Guan%20and%20Kexin%20Huang%20and%20Kuai%20Yu%20and%20Lean%20Wang%20and%20Lecong%20Zhang%20and%20Liang%20Zhao%20and%20Litong%20Wang%20and%20Liyue%20Zhang%20and%20Lei%20Xu%20and%20Leyi%20Xia%20and%20Mingchuan%20Zhang%20and%20Minghua%20Zhang%20and%20Minghui%20Tang%20and%20Meng%20Li%20and%20Miaojun%20Wang%20and%20Mingming%20Li%20and%20Ning%20Tian%20and%20Panpan%20Huang%20and%20Peng%20Zhang%20and%20Qiancheng%20Wang%20and%20Qinyu%20Chen%20and%20Qiushi%20Du%20and%20Ruiqi%20Ge%20and%20Ruisong%20Zhang%20and%20Ruizhe%20Pan%20and%20Runji%20Wang%20and%20R.%20J.%20Chen%20and%20R.%20L.%20Jin%20and%20Ruyi%20Chen%20and%20Shanghao%20Lu%20and%20Shangyan%20Zhou%20and%20Shanhuang%20Chen%20and%20Shengfeng%20Ye%20and%20Shiyu%20Wang%20and%20Shuiping%20Yu%20and%20Shunfeng%20Zhou%20and%20Shuting%20Pan%20and%20S.%20S.%20Li%20and%20Shuang%20Zhou%20and%20Shaoqing%20Wu%20and%20Shengfeng%20Ye%20and%20Tao%20Yun%20and%20Tian%20Pei%20and%20Tianyu%20Sun%20and%20T.%20Wang%20and%20Wangding%20Zeng%20and%20Wanjia%20Zhao%20and%20Wen%20Liu%20and%20Wenfeng%20Liang%20and%20Wenjun%20Gao%20and%20Wenqin%20Yu%20and%20Wentao%20Zhang%20and%20W.%20L.%20Xiao%20and%20Wei%20An%20and%20Xiaodong%20Liu%20and%20Xiaohan%20Wang%20and%20Xiaokang%20Chen%20and%20Xiaotao%20Nie%20and%20Xin%20Cheng%20and%20Xin%20Liu%20and%20Xin%20Xie%20and%20Xingchao%20Liu%20and%20Xinyu%20Yang%20and%20Xinyuan%20Li%20and%20Xuecheng%20Su%20and%20Xuheng%20Lin%20and%20X.%20Q.%20Li%20and%20Xiangyue%20Jin%20and%20Xiaojin%20Shen%20and%20Xiaosha%20Chen%20and%20Xiaowen%20Sun%20and%20Xiaoxiang%20Wang%20and%20Xinnan%20Song%20and%20Xinyi%20Zhou%20and%20Xianzu%20Wang%20and%20Xinxia%20Shan%20and%20Y.%20K.%20Li%20and%20Y.%20Q.%20Wang%20and%20Y.%20X.%20Wei%20and%20Yang%20Zhang%20and%20Yanhong%20Xu%20and%20Yao%20Li%20and%20Yao%20Zhao%20and%20Yaofeng%20Sun%20and%20Yaohui%20Wang%20and%20Yi%20Yu%20and%20Yichao%20Zhang%20and%20Yifan%20Shi%20and%20Yiliang%20Xiong%20and%20Ying%20He%20and%20Yishi%20Piao%20and%20Yisong%20Wang%20and%20Yixuan%20Tan%20and%20Yiyang%20Ma%20and%20Yiyuan%20Liu%20and%20Yongqiang%20Guo%20and%20Yuan%20Ou%20and%20Yuduan%20Wang%20and%20Yue%20Gong%20and%20Yuheng%20Zou%20and%20Yujia%20He%20and%20Yunfan%20Xiong%20and%20Yuxiang%20Luo%20and%20Yuxiang%20You%20and%20Yuxuan%20Liu%20and%20Yuyang%20Zhou%20and%20Y.%20X.%20Zhu%20and%20Yanhong%20Xu%20and%20Yanping%20Huang%20and%20Yaohui%20Li%20and%20Yi%20Zheng%20and%20Yuchen%20Zhu%20and%20Yunxian%20Ma%20and%20Ying%20Tang%20and%20Yukun%20Zha%20and%20Yuting%20Yan%20and%20Z.%20Z.%20Ren%20and%20Zehui%20Ren%20and%20Zhangli%20Sha%20and%20Zhe%20Fu%20and%20Zhean%20Xu%20and%20Zhenda%20Xie%20and%20Zhengyan%20Zhang%20and%20Zhewen%20Hao%20and%20Zhicheng%20Ma%20and%20Zhigang%20Yan%20and%20Zhiyu%20Wu%20and%20Zihui%20Gu%20and%20Zijia%20Zhu%20and%20Zijun%20Liu%20and%20Zilin%20Li%20and%20Ziwei%20Xie%20and%20Ziyang%20Song%20and%20Zizheng%20Pan%20and%20Zhen%20Huang%20and%20Zhipeng%20Xu%20and%20Zhongyu%20Zhang%20and%20Zhen%20Zhang&entry.1292438233=%20%20We%20introduce%20our%20first-generation%20reasoning%20models%2C%20DeepSeek-R1-Zero%20and%0ADeepSeek-R1.%20DeepSeek-R1-Zero%2C%20a%20model%20trained%20via%20large-scale%20reinforcement%0Alearning%20%28RL%29%20without%20supervised%20fine-tuning%20%28SFT%29%20as%20a%20preliminary%20step%2C%0Ademonstrates%20remarkable%20reasoning%20capabilities.%20Through%20RL%2C%20DeepSeek-R1-Zero%0Anaturally%20emerges%20with%20numerous%20powerful%20and%20intriguing%20reasoning%20behaviors.%0AHowever%2C%20it%20encounters%20challenges%20such%20as%20poor%20readability%2C%20and%20language%0Amixing.%20To%20address%20these%20issues%20and%20further%20enhance%20reasoning%20performance%2C%20we%0Aintroduce%20DeepSeek-R1%2C%20which%20incorporates%20multi-stage%20training%20and%20cold-start%0Adata%20before%20RL.%20DeepSeek-R1%20achieves%20performance%20comparable%20to%20OpenAI-o1-1217%0Aon%20reasoning%20tasks.%20To%20support%20the%20research%20community%2C%20we%20open-source%0ADeepSeek-R1-Zero%2C%20DeepSeek-R1%2C%20and%20six%20dense%20models%20%281.5B%2C%207B%2C%208B%2C%2014B%2C%2032B%2C%0A70B%29%20distilled%20from%20DeepSeek-R1%20based%20on%20Qwen%20and%20Llama.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12948v1&entry.124074799=Read"},
{"title": "AI-driven View Guidance System in Intra-cardiac Echocardiography Imaging", "author": "Jaeyoung Huh and Paul Klein and Gareth Funka-Lea and Puneet Sharma and Ankur Kapoor and Young-Ho Kim", "abstract": "  Intra-cardiac echocardiography (ICE) is a crucial imaging modality used in\nelectrophysiology (EP) and structural heart disease (SHD) interventions,\nproviding realtime, high-resolution views from within the heart. Despite its\nadvantages, effective manipulation of the ICE catheter requires significant\nexpertise, which can lead to inconsistent outcomes, especially among less\nexperienced operators. To address this challenge, we propose an AIdriven view\nguidance system that operates in a continuous closed-loop with\nhuman-in-the-loop feedback, designed to assist users in navigating ICE imaging\nwithout requiring specialized knowledge. Specifically, our method models the\nrelative position and orientation vectors between arbitrary views and\nclinically defined ICE views in a spatial coordinate system. It guides users on\nhow to manipulate the ICE catheter to transition from the current view to the\ndesired view over time. By operating in a closedloop configuration, the system\ncontinuously predicts and updates the necessary catheter manipulations,\nensuring seamless integration into existing clinical workflows. The\neffectiveness of the proposed system is demonstrated through a simulation-based\nperformance evaluation using real clinical data, achieving an 89% success rate\nwith 6,532 test cases. Additionally, a semi-simulation experiment with\nhuman-in-the-loop testing validated the feasibility of continuous yet discrete\nguidance. These results underscore the potential of the proposed method to\nenhance the accuracy and efficiency of ICE imaging procedures.\n", "link": "http://arxiv.org/abs/2409.16898v3", "date": "2025-01-22", "relevancy": 2.0801, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5495}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5003}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging&body=Title%3A%20AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging%0AAuthor%3A%20Jaeyoung%20Huh%20and%20Paul%20Klein%20and%20Gareth%20Funka-Lea%20and%20Puneet%20Sharma%20and%20Ankur%20Kapoor%20and%20Young-Ho%20Kim%0AAbstract%3A%20%20%20Intra-cardiac%20echocardiography%20%28ICE%29%20is%20a%20crucial%20imaging%20modality%20used%20in%0Aelectrophysiology%20%28EP%29%20and%20structural%20heart%20disease%20%28SHD%29%20interventions%2C%0Aproviding%20realtime%2C%20high-resolution%20views%20from%20within%20the%20heart.%20Despite%20its%0Aadvantages%2C%20effective%20manipulation%20of%20the%20ICE%20catheter%20requires%20significant%0Aexpertise%2C%20which%20can%20lead%20to%20inconsistent%20outcomes%2C%20especially%20among%20less%0Aexperienced%20operators.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20AIdriven%20view%0Aguidance%20system%20that%20operates%20in%20a%20continuous%20closed-loop%20with%0Ahuman-in-the-loop%20feedback%2C%20designed%20to%20assist%20users%20in%20navigating%20ICE%20imaging%0Awithout%20requiring%20specialized%20knowledge.%20Specifically%2C%20our%20method%20models%20the%0Arelative%20position%20and%20orientation%20vectors%20between%20arbitrary%20views%20and%0Aclinically%20defined%20ICE%20views%20in%20a%20spatial%20coordinate%20system.%20It%20guides%20users%20on%0Ahow%20to%20manipulate%20the%20ICE%20catheter%20to%20transition%20from%20the%20current%20view%20to%20the%0Adesired%20view%20over%20time.%20By%20operating%20in%20a%20closedloop%20configuration%2C%20the%20system%0Acontinuously%20predicts%20and%20updates%20the%20necessary%20catheter%20manipulations%2C%0Aensuring%20seamless%20integration%20into%20existing%20clinical%20workflows.%20The%0Aeffectiveness%20of%20the%20proposed%20system%20is%20demonstrated%20through%20a%20simulation-based%0Aperformance%20evaluation%20using%20real%20clinical%20data%2C%20achieving%20an%2089%25%20success%20rate%0Awith%206%2C532%20test%20cases.%20Additionally%2C%20a%20semi-simulation%20experiment%20with%0Ahuman-in-the-loop%20testing%20validated%20the%20feasibility%20of%20continuous%20yet%20discrete%0Aguidance.%20These%20results%20underscore%20the%20potential%20of%20the%20proposed%20method%20to%0Aenhance%20the%20accuracy%20and%20efficiency%20of%20ICE%20imaging%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16898v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-driven%2520View%2520Guidance%2520System%2520in%2520Intra-cardiac%2520Echocardiography%2520Imaging%26entry.906535625%3DJaeyoung%2520Huh%2520and%2520Paul%2520Klein%2520and%2520Gareth%2520Funka-Lea%2520and%2520Puneet%2520Sharma%2520and%2520Ankur%2520Kapoor%2520and%2520Young-Ho%2520Kim%26entry.1292438233%3D%2520%2520Intra-cardiac%2520echocardiography%2520%2528ICE%2529%2520is%2520a%2520crucial%2520imaging%2520modality%2520used%2520in%250Aelectrophysiology%2520%2528EP%2529%2520and%2520structural%2520heart%2520disease%2520%2528SHD%2529%2520interventions%252C%250Aproviding%2520realtime%252C%2520high-resolution%2520views%2520from%2520within%2520the%2520heart.%2520Despite%2520its%250Aadvantages%252C%2520effective%2520manipulation%2520of%2520the%2520ICE%2520catheter%2520requires%2520significant%250Aexpertise%252C%2520which%2520can%2520lead%2520to%2520inconsistent%2520outcomes%252C%2520especially%2520among%2520less%250Aexperienced%2520operators.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520AIdriven%2520view%250Aguidance%2520system%2520that%2520operates%2520in%2520a%2520continuous%2520closed-loop%2520with%250Ahuman-in-the-loop%2520feedback%252C%2520designed%2520to%2520assist%2520users%2520in%2520navigating%2520ICE%2520imaging%250Awithout%2520requiring%2520specialized%2520knowledge.%2520Specifically%252C%2520our%2520method%2520models%2520the%250Arelative%2520position%2520and%2520orientation%2520vectors%2520between%2520arbitrary%2520views%2520and%250Aclinically%2520defined%2520ICE%2520views%2520in%2520a%2520spatial%2520coordinate%2520system.%2520It%2520guides%2520users%2520on%250Ahow%2520to%2520manipulate%2520the%2520ICE%2520catheter%2520to%2520transition%2520from%2520the%2520current%2520view%2520to%2520the%250Adesired%2520view%2520over%2520time.%2520By%2520operating%2520in%2520a%2520closedloop%2520configuration%252C%2520the%2520system%250Acontinuously%2520predicts%2520and%2520updates%2520the%2520necessary%2520catheter%2520manipulations%252C%250Aensuring%2520seamless%2520integration%2520into%2520existing%2520clinical%2520workflows.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520system%2520is%2520demonstrated%2520through%2520a%2520simulation-based%250Aperformance%2520evaluation%2520using%2520real%2520clinical%2520data%252C%2520achieving%2520an%252089%2525%2520success%2520rate%250Awith%25206%252C532%2520test%2520cases.%2520Additionally%252C%2520a%2520semi-simulation%2520experiment%2520with%250Ahuman-in-the-loop%2520testing%2520validated%2520the%2520feasibility%2520of%2520continuous%2520yet%2520discrete%250Aguidance.%2520These%2520results%2520underscore%2520the%2520potential%2520of%2520the%2520proposed%2520method%2520to%250Aenhance%2520the%2520accuracy%2520and%2520efficiency%2520of%2520ICE%2520imaging%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16898v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging&entry.906535625=Jaeyoung%20Huh%20and%20Paul%20Klein%20and%20Gareth%20Funka-Lea%20and%20Puneet%20Sharma%20and%20Ankur%20Kapoor%20and%20Young-Ho%20Kim&entry.1292438233=%20%20Intra-cardiac%20echocardiography%20%28ICE%29%20is%20a%20crucial%20imaging%20modality%20used%20in%0Aelectrophysiology%20%28EP%29%20and%20structural%20heart%20disease%20%28SHD%29%20interventions%2C%0Aproviding%20realtime%2C%20high-resolution%20views%20from%20within%20the%20heart.%20Despite%20its%0Aadvantages%2C%20effective%20manipulation%20of%20the%20ICE%20catheter%20requires%20significant%0Aexpertise%2C%20which%20can%20lead%20to%20inconsistent%20outcomes%2C%20especially%20among%20less%0Aexperienced%20operators.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20AIdriven%20view%0Aguidance%20system%20that%20operates%20in%20a%20continuous%20closed-loop%20with%0Ahuman-in-the-loop%20feedback%2C%20designed%20to%20assist%20users%20in%20navigating%20ICE%20imaging%0Awithout%20requiring%20specialized%20knowledge.%20Specifically%2C%20our%20method%20models%20the%0Arelative%20position%20and%20orientation%20vectors%20between%20arbitrary%20views%20and%0Aclinically%20defined%20ICE%20views%20in%20a%20spatial%20coordinate%20system.%20It%20guides%20users%20on%0Ahow%20to%20manipulate%20the%20ICE%20catheter%20to%20transition%20from%20the%20current%20view%20to%20the%0Adesired%20view%20over%20time.%20By%20operating%20in%20a%20closedloop%20configuration%2C%20the%20system%0Acontinuously%20predicts%20and%20updates%20the%20necessary%20catheter%20manipulations%2C%0Aensuring%20seamless%20integration%20into%20existing%20clinical%20workflows.%20The%0Aeffectiveness%20of%20the%20proposed%20system%20is%20demonstrated%20through%20a%20simulation-based%0Aperformance%20evaluation%20using%20real%20clinical%20data%2C%20achieving%20an%2089%25%20success%20rate%0Awith%206%2C532%20test%20cases.%20Additionally%2C%20a%20semi-simulation%20experiment%20with%0Ahuman-in-the-loop%20testing%20validated%20the%20feasibility%20of%20continuous%20yet%20discrete%0Aguidance.%20These%20results%20underscore%20the%20potential%20of%20the%20proposed%20method%20to%0Aenhance%20the%20accuracy%20and%20efficiency%20of%20ICE%20imaging%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16898v3&entry.124074799=Read"},
{"title": "Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI\n  Generation and Diffuse Glioma Growth Prediction", "author": "Qinghui Liu and Elies Fuster-Garcia and Ivar Thokle Hovden and Bradley J MacIntosh and Edvard Gr\u00f8dem and Petter Brandal and Carles Lopez-Mateu and Donatas Sederevicius and Karoline Skogen and Till Schellhorn and Atle Bj\u00f8rnerud and Kyrre Eeg Emblem", "abstract": "  Diffuse gliomas are malignant brain tumors that grow widespread through the\nbrain. The complex interactions between neoplastic cells and normal tissue, as\nwell as the treatment-induced changes often encountered, make glioma tumor\ngrowth modeling challenging. In this paper, we present a novel end-to-end\nnetwork capable of future predictions of tumor masks and multi-parametric\nmagnetic resonance images (MRI) of how the tumor will look at any future time\npoints for different treatment plans. Our approach is based on cutting-edge\ndiffusion probabilistic models and deep-segmentation neural networks. We\nincluded sequential multi-parametric MRI and treatment information as\nconditioning inputs to guide the generative diffusion process as well as a\njoint segmentation process. This allows for tumor growth estimates and\nrealistic MRI generation at any given treatment and time point. We trained the\nmodel using real-world postoperative longitudinal MRI data with glioma tumor\ngrowth trajectories represented as tumor segmentation maps over time. The model\ndemonstrates promising performance across various tasks, including generating\nhigh-quality multi-parametric MRI with tumor masks, performing time-series\ntumor segmentations, and providing uncertainty estimates. Combined with the\ntreatment-aware generated MRI, the tumor growth predictions with uncertainty\nestimates can provide useful information for clinical decision-making.\n", "link": "http://arxiv.org/abs/2309.05406v5", "date": "2025-01-22", "relevancy": 2.058, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.559}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5056}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Treatment-aware%20Diffusion%20Probabilistic%20Model%20for%20Longitudinal%20MRI%0A%20%20Generation%20and%20Diffuse%20Glioma%20Growth%20Prediction&body=Title%3A%20Treatment-aware%20Diffusion%20Probabilistic%20Model%20for%20Longitudinal%20MRI%0A%20%20Generation%20and%20Diffuse%20Glioma%20Growth%20Prediction%0AAuthor%3A%20Qinghui%20Liu%20and%20Elies%20Fuster-Garcia%20and%20Ivar%20Thokle%20Hovden%20and%20Bradley%20J%20MacIntosh%20and%20Edvard%20Gr%C3%B8dem%20and%20Petter%20Brandal%20and%20Carles%20Lopez-Mateu%20and%20Donatas%20Sederevicius%20and%20Karoline%20Skogen%20and%20Till%20Schellhorn%20and%20Atle%20Bj%C3%B8rnerud%20and%20Kyrre%20Eeg%20Emblem%0AAbstract%3A%20%20%20Diffuse%20gliomas%20are%20malignant%20brain%20tumors%20that%20grow%20widespread%20through%20the%0Abrain.%20The%20complex%20interactions%20between%20neoplastic%20cells%20and%20normal%20tissue%2C%20as%0Awell%20as%20the%20treatment-induced%20changes%20often%20encountered%2C%20make%20glioma%20tumor%0Agrowth%20modeling%20challenging.%20In%20this%20paper%2C%20we%20present%20a%20novel%20end-to-end%0Anetwork%20capable%20of%20future%20predictions%20of%20tumor%20masks%20and%20multi-parametric%0Amagnetic%20resonance%20images%20%28MRI%29%20of%20how%20the%20tumor%20will%20look%20at%20any%20future%20time%0Apoints%20for%20different%20treatment%20plans.%20Our%20approach%20is%20based%20on%20cutting-edge%0Adiffusion%20probabilistic%20models%20and%20deep-segmentation%20neural%20networks.%20We%0Aincluded%20sequential%20multi-parametric%20MRI%20and%20treatment%20information%20as%0Aconditioning%20inputs%20to%20guide%20the%20generative%20diffusion%20process%20as%20well%20as%20a%0Ajoint%20segmentation%20process.%20This%20allows%20for%20tumor%20growth%20estimates%20and%0Arealistic%20MRI%20generation%20at%20any%20given%20treatment%20and%20time%20point.%20We%20trained%20the%0Amodel%20using%20real-world%20postoperative%20longitudinal%20MRI%20data%20with%20glioma%20tumor%0Agrowth%20trajectories%20represented%20as%20tumor%20segmentation%20maps%20over%20time.%20The%20model%0Ademonstrates%20promising%20performance%20across%20various%20tasks%2C%20including%20generating%0Ahigh-quality%20multi-parametric%20MRI%20with%20tumor%20masks%2C%20performing%20time-series%0Atumor%20segmentations%2C%20and%20providing%20uncertainty%20estimates.%20Combined%20with%20the%0Atreatment-aware%20generated%20MRI%2C%20the%20tumor%20growth%20predictions%20with%20uncertainty%0Aestimates%20can%20provide%20useful%20information%20for%20clinical%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05406v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTreatment-aware%2520Diffusion%2520Probabilistic%2520Model%2520for%2520Longitudinal%2520MRI%250A%2520%2520Generation%2520and%2520Diffuse%2520Glioma%2520Growth%2520Prediction%26entry.906535625%3DQinghui%2520Liu%2520and%2520Elies%2520Fuster-Garcia%2520and%2520Ivar%2520Thokle%2520Hovden%2520and%2520Bradley%2520J%2520MacIntosh%2520and%2520Edvard%2520Gr%25C3%25B8dem%2520and%2520Petter%2520Brandal%2520and%2520Carles%2520Lopez-Mateu%2520and%2520Donatas%2520Sederevicius%2520and%2520Karoline%2520Skogen%2520and%2520Till%2520Schellhorn%2520and%2520Atle%2520Bj%25C3%25B8rnerud%2520and%2520Kyrre%2520Eeg%2520Emblem%26entry.1292438233%3D%2520%2520Diffuse%2520gliomas%2520are%2520malignant%2520brain%2520tumors%2520that%2520grow%2520widespread%2520through%2520the%250Abrain.%2520The%2520complex%2520interactions%2520between%2520neoplastic%2520cells%2520and%2520normal%2520tissue%252C%2520as%250Awell%2520as%2520the%2520treatment-induced%2520changes%2520often%2520encountered%252C%2520make%2520glioma%2520tumor%250Agrowth%2520modeling%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520end-to-end%250Anetwork%2520capable%2520of%2520future%2520predictions%2520of%2520tumor%2520masks%2520and%2520multi-parametric%250Amagnetic%2520resonance%2520images%2520%2528MRI%2529%2520of%2520how%2520the%2520tumor%2520will%2520look%2520at%2520any%2520future%2520time%250Apoints%2520for%2520different%2520treatment%2520plans.%2520Our%2520approach%2520is%2520based%2520on%2520cutting-edge%250Adiffusion%2520probabilistic%2520models%2520and%2520deep-segmentation%2520neural%2520networks.%2520We%250Aincluded%2520sequential%2520multi-parametric%2520MRI%2520and%2520treatment%2520information%2520as%250Aconditioning%2520inputs%2520to%2520guide%2520the%2520generative%2520diffusion%2520process%2520as%2520well%2520as%2520a%250Ajoint%2520segmentation%2520process.%2520This%2520allows%2520for%2520tumor%2520growth%2520estimates%2520and%250Arealistic%2520MRI%2520generation%2520at%2520any%2520given%2520treatment%2520and%2520time%2520point.%2520We%2520trained%2520the%250Amodel%2520using%2520real-world%2520postoperative%2520longitudinal%2520MRI%2520data%2520with%2520glioma%2520tumor%250Agrowth%2520trajectories%2520represented%2520as%2520tumor%2520segmentation%2520maps%2520over%2520time.%2520The%2520model%250Ademonstrates%2520promising%2520performance%2520across%2520various%2520tasks%252C%2520including%2520generating%250Ahigh-quality%2520multi-parametric%2520MRI%2520with%2520tumor%2520masks%252C%2520performing%2520time-series%250Atumor%2520segmentations%252C%2520and%2520providing%2520uncertainty%2520estimates.%2520Combined%2520with%2520the%250Atreatment-aware%2520generated%2520MRI%252C%2520the%2520tumor%2520growth%2520predictions%2520with%2520uncertainty%250Aestimates%2520can%2520provide%2520useful%2520information%2520for%2520clinical%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05406v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Treatment-aware%20Diffusion%20Probabilistic%20Model%20for%20Longitudinal%20MRI%0A%20%20Generation%20and%20Diffuse%20Glioma%20Growth%20Prediction&entry.906535625=Qinghui%20Liu%20and%20Elies%20Fuster-Garcia%20and%20Ivar%20Thokle%20Hovden%20and%20Bradley%20J%20MacIntosh%20and%20Edvard%20Gr%C3%B8dem%20and%20Petter%20Brandal%20and%20Carles%20Lopez-Mateu%20and%20Donatas%20Sederevicius%20and%20Karoline%20Skogen%20and%20Till%20Schellhorn%20and%20Atle%20Bj%C3%B8rnerud%20and%20Kyrre%20Eeg%20Emblem&entry.1292438233=%20%20Diffuse%20gliomas%20are%20malignant%20brain%20tumors%20that%20grow%20widespread%20through%20the%0Abrain.%20The%20complex%20interactions%20between%20neoplastic%20cells%20and%20normal%20tissue%2C%20as%0Awell%20as%20the%20treatment-induced%20changes%20often%20encountered%2C%20make%20glioma%20tumor%0Agrowth%20modeling%20challenging.%20In%20this%20paper%2C%20we%20present%20a%20novel%20end-to-end%0Anetwork%20capable%20of%20future%20predictions%20of%20tumor%20masks%20and%20multi-parametric%0Amagnetic%20resonance%20images%20%28MRI%29%20of%20how%20the%20tumor%20will%20look%20at%20any%20future%20time%0Apoints%20for%20different%20treatment%20plans.%20Our%20approach%20is%20based%20on%20cutting-edge%0Adiffusion%20probabilistic%20models%20and%20deep-segmentation%20neural%20networks.%20We%0Aincluded%20sequential%20multi-parametric%20MRI%20and%20treatment%20information%20as%0Aconditioning%20inputs%20to%20guide%20the%20generative%20diffusion%20process%20as%20well%20as%20a%0Ajoint%20segmentation%20process.%20This%20allows%20for%20tumor%20growth%20estimates%20and%0Arealistic%20MRI%20generation%20at%20any%20given%20treatment%20and%20time%20point.%20We%20trained%20the%0Amodel%20using%20real-world%20postoperative%20longitudinal%20MRI%20data%20with%20glioma%20tumor%0Agrowth%20trajectories%20represented%20as%20tumor%20segmentation%20maps%20over%20time.%20The%20model%0Ademonstrates%20promising%20performance%20across%20various%20tasks%2C%20including%20generating%0Ahigh-quality%20multi-parametric%20MRI%20with%20tumor%20masks%2C%20performing%20time-series%0Atumor%20segmentations%2C%20and%20providing%20uncertainty%20estimates.%20Combined%20with%20the%0Atreatment-aware%20generated%20MRI%2C%20the%20tumor%20growth%20predictions%20with%20uncertainty%0Aestimates%20can%20provide%20useful%20information%20for%20clinical%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05406v5&entry.124074799=Read"},
{"title": "A CNN-Transformer for Classification of Longitudinal 3D MRI Images -- A\n  Case Study on Hepatocellular Carcinoma Prediction", "author": "Jakob Nolte and Maureen M. J. Guichelaar and Donald E. Bouman and Stephanie M. van den Berg and Maryam Amir Haeri", "abstract": "  Longitudinal MRI analysis is crucial for predicting disease outcomes,\nparticularly in chronic conditions like hepatocellular carcinoma (HCC), where\nearly detection can significantly influence treatment strategies and patient\nprognosis. Yet, due to challenges like limited data availability, subtle\nparenchymal changes, and the irregular timing of medical screenings, current\napproaches have so far focused on cross-sectional imaging data. To address\nthis, we propose HCCNet, a novel model architecture that integrates a 3D\nadaptation of the ConvNeXt CNN architecture with a Transformer encoder,\ncapturing both the intricate spatial features of 3D MRIs and the complex\ntemporal dependencies across different time points. HCCNet utilizes a two-stage\npre-training process tailored for longitudinal MRI data. The CNN backbone is\npre-trained using a self-supervised learning framework adapted for 3D MRIs,\nwhile the Transformer encoder is pre-trained with a sequence-order-prediction\ntask to enhance its understanding of disease progression over time. We\ndemonstrate the effectiveness of HCCNet by applying it to a cohort of liver\ncirrhosis patients undergoing regular MRI screenings for HCC surveillance. Our\nresults show that HCCNet significantly improves predictive accuracy and\nreliability over baseline models, providing a robust tool for personalized HCC\nsurveillance. The methodological approach presented in this paper is versatile\nand can be adapted to various longitudinal MRI screening applications. Its\nability to handle varying patient record lengths and irregular screening\nintervals establishes it as an invaluable framework for monitoring chronic\ndiseases, where timely and accurate disease prognosis is critical for effective\ntreatment planning.\n", "link": "http://arxiv.org/abs/2501.10733v2", "date": "2025-01-22", "relevancy": 2.0512, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5375}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.514}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20CNN-Transformer%20for%20Classification%20of%20Longitudinal%203D%20MRI%20Images%20--%20A%0A%20%20Case%20Study%20on%20Hepatocellular%20Carcinoma%20Prediction&body=Title%3A%20A%20CNN-Transformer%20for%20Classification%20of%20Longitudinal%203D%20MRI%20Images%20--%20A%0A%20%20Case%20Study%20on%20Hepatocellular%20Carcinoma%20Prediction%0AAuthor%3A%20Jakob%20Nolte%20and%20Maureen%20M.%20J.%20Guichelaar%20and%20Donald%20E.%20Bouman%20and%20Stephanie%20M.%20van%20den%20Berg%20and%20Maryam%20Amir%20Haeri%0AAbstract%3A%20%20%20Longitudinal%20MRI%20analysis%20is%20crucial%20for%20predicting%20disease%20outcomes%2C%0Aparticularly%20in%20chronic%20conditions%20like%20hepatocellular%20carcinoma%20%28HCC%29%2C%20where%0Aearly%20detection%20can%20significantly%20influence%20treatment%20strategies%20and%20patient%0Aprognosis.%20Yet%2C%20due%20to%20challenges%20like%20limited%20data%20availability%2C%20subtle%0Aparenchymal%20changes%2C%20and%20the%20irregular%20timing%20of%20medical%20screenings%2C%20current%0Aapproaches%20have%20so%20far%20focused%20on%20cross-sectional%20imaging%20data.%20To%20address%0Athis%2C%20we%20propose%20HCCNet%2C%20a%20novel%20model%20architecture%20that%20integrates%20a%203D%0Aadaptation%20of%20the%20ConvNeXt%20CNN%20architecture%20with%20a%20Transformer%20encoder%2C%0Acapturing%20both%20the%20intricate%20spatial%20features%20of%203D%20MRIs%20and%20the%20complex%0Atemporal%20dependencies%20across%20different%20time%20points.%20HCCNet%20utilizes%20a%20two-stage%0Apre-training%20process%20tailored%20for%20longitudinal%20MRI%20data.%20The%20CNN%20backbone%20is%0Apre-trained%20using%20a%20self-supervised%20learning%20framework%20adapted%20for%203D%20MRIs%2C%0Awhile%20the%20Transformer%20encoder%20is%20pre-trained%20with%20a%20sequence-order-prediction%0Atask%20to%20enhance%20its%20understanding%20of%20disease%20progression%20over%20time.%20We%0Ademonstrate%20the%20effectiveness%20of%20HCCNet%20by%20applying%20it%20to%20a%20cohort%20of%20liver%0Acirrhosis%20patients%20undergoing%20regular%20MRI%20screenings%20for%20HCC%20surveillance.%20Our%0Aresults%20show%20that%20HCCNet%20significantly%20improves%20predictive%20accuracy%20and%0Areliability%20over%20baseline%20models%2C%20providing%20a%20robust%20tool%20for%20personalized%20HCC%0Asurveillance.%20The%20methodological%20approach%20presented%20in%20this%20paper%20is%20versatile%0Aand%20can%20be%20adapted%20to%20various%20longitudinal%20MRI%20screening%20applications.%20Its%0Aability%20to%20handle%20varying%20patient%20record%20lengths%20and%20irregular%20screening%0Aintervals%20establishes%20it%20as%20an%20invaluable%20framework%20for%20monitoring%20chronic%0Adiseases%2C%20where%20timely%20and%20accurate%20disease%20prognosis%20is%20critical%20for%20effective%0Atreatment%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10733v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520CNN-Transformer%2520for%2520Classification%2520of%2520Longitudinal%25203D%2520MRI%2520Images%2520--%2520A%250A%2520%2520Case%2520Study%2520on%2520Hepatocellular%2520Carcinoma%2520Prediction%26entry.906535625%3DJakob%2520Nolte%2520and%2520Maureen%2520M.%2520J.%2520Guichelaar%2520and%2520Donald%2520E.%2520Bouman%2520and%2520Stephanie%2520M.%2520van%2520den%2520Berg%2520and%2520Maryam%2520Amir%2520Haeri%26entry.1292438233%3D%2520%2520Longitudinal%2520MRI%2520analysis%2520is%2520crucial%2520for%2520predicting%2520disease%2520outcomes%252C%250Aparticularly%2520in%2520chronic%2520conditions%2520like%2520hepatocellular%2520carcinoma%2520%2528HCC%2529%252C%2520where%250Aearly%2520detection%2520can%2520significantly%2520influence%2520treatment%2520strategies%2520and%2520patient%250Aprognosis.%2520Yet%252C%2520due%2520to%2520challenges%2520like%2520limited%2520data%2520availability%252C%2520subtle%250Aparenchymal%2520changes%252C%2520and%2520the%2520irregular%2520timing%2520of%2520medical%2520screenings%252C%2520current%250Aapproaches%2520have%2520so%2520far%2520focused%2520on%2520cross-sectional%2520imaging%2520data.%2520To%2520address%250Athis%252C%2520we%2520propose%2520HCCNet%252C%2520a%2520novel%2520model%2520architecture%2520that%2520integrates%2520a%25203D%250Aadaptation%2520of%2520the%2520ConvNeXt%2520CNN%2520architecture%2520with%2520a%2520Transformer%2520encoder%252C%250Acapturing%2520both%2520the%2520intricate%2520spatial%2520features%2520of%25203D%2520MRIs%2520and%2520the%2520complex%250Atemporal%2520dependencies%2520across%2520different%2520time%2520points.%2520HCCNet%2520utilizes%2520a%2520two-stage%250Apre-training%2520process%2520tailored%2520for%2520longitudinal%2520MRI%2520data.%2520The%2520CNN%2520backbone%2520is%250Apre-trained%2520using%2520a%2520self-supervised%2520learning%2520framework%2520adapted%2520for%25203D%2520MRIs%252C%250Awhile%2520the%2520Transformer%2520encoder%2520is%2520pre-trained%2520with%2520a%2520sequence-order-prediction%250Atask%2520to%2520enhance%2520its%2520understanding%2520of%2520disease%2520progression%2520over%2520time.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520HCCNet%2520by%2520applying%2520it%2520to%2520a%2520cohort%2520of%2520liver%250Acirrhosis%2520patients%2520undergoing%2520regular%2520MRI%2520screenings%2520for%2520HCC%2520surveillance.%2520Our%250Aresults%2520show%2520that%2520HCCNet%2520significantly%2520improves%2520predictive%2520accuracy%2520and%250Areliability%2520over%2520baseline%2520models%252C%2520providing%2520a%2520robust%2520tool%2520for%2520personalized%2520HCC%250Asurveillance.%2520The%2520methodological%2520approach%2520presented%2520in%2520this%2520paper%2520is%2520versatile%250Aand%2520can%2520be%2520adapted%2520to%2520various%2520longitudinal%2520MRI%2520screening%2520applications.%2520Its%250Aability%2520to%2520handle%2520varying%2520patient%2520record%2520lengths%2520and%2520irregular%2520screening%250Aintervals%2520establishes%2520it%2520as%2520an%2520invaluable%2520framework%2520for%2520monitoring%2520chronic%250Adiseases%252C%2520where%2520timely%2520and%2520accurate%2520disease%2520prognosis%2520is%2520critical%2520for%2520effective%250Atreatment%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10733v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20CNN-Transformer%20for%20Classification%20of%20Longitudinal%203D%20MRI%20Images%20--%20A%0A%20%20Case%20Study%20on%20Hepatocellular%20Carcinoma%20Prediction&entry.906535625=Jakob%20Nolte%20and%20Maureen%20M.%20J.%20Guichelaar%20and%20Donald%20E.%20Bouman%20and%20Stephanie%20M.%20van%20den%20Berg%20and%20Maryam%20Amir%20Haeri&entry.1292438233=%20%20Longitudinal%20MRI%20analysis%20is%20crucial%20for%20predicting%20disease%20outcomes%2C%0Aparticularly%20in%20chronic%20conditions%20like%20hepatocellular%20carcinoma%20%28HCC%29%2C%20where%0Aearly%20detection%20can%20significantly%20influence%20treatment%20strategies%20and%20patient%0Aprognosis.%20Yet%2C%20due%20to%20challenges%20like%20limited%20data%20availability%2C%20subtle%0Aparenchymal%20changes%2C%20and%20the%20irregular%20timing%20of%20medical%20screenings%2C%20current%0Aapproaches%20have%20so%20far%20focused%20on%20cross-sectional%20imaging%20data.%20To%20address%0Athis%2C%20we%20propose%20HCCNet%2C%20a%20novel%20model%20architecture%20that%20integrates%20a%203D%0Aadaptation%20of%20the%20ConvNeXt%20CNN%20architecture%20with%20a%20Transformer%20encoder%2C%0Acapturing%20both%20the%20intricate%20spatial%20features%20of%203D%20MRIs%20and%20the%20complex%0Atemporal%20dependencies%20across%20different%20time%20points.%20HCCNet%20utilizes%20a%20two-stage%0Apre-training%20process%20tailored%20for%20longitudinal%20MRI%20data.%20The%20CNN%20backbone%20is%0Apre-trained%20using%20a%20self-supervised%20learning%20framework%20adapted%20for%203D%20MRIs%2C%0Awhile%20the%20Transformer%20encoder%20is%20pre-trained%20with%20a%20sequence-order-prediction%0Atask%20to%20enhance%20its%20understanding%20of%20disease%20progression%20over%20time.%20We%0Ademonstrate%20the%20effectiveness%20of%20HCCNet%20by%20applying%20it%20to%20a%20cohort%20of%20liver%0Acirrhosis%20patients%20undergoing%20regular%20MRI%20screenings%20for%20HCC%20surveillance.%20Our%0Aresults%20show%20that%20HCCNet%20significantly%20improves%20predictive%20accuracy%20and%0Areliability%20over%20baseline%20models%2C%20providing%20a%20robust%20tool%20for%20personalized%20HCC%0Asurveillance.%20The%20methodological%20approach%20presented%20in%20this%20paper%20is%20versatile%0Aand%20can%20be%20adapted%20to%20various%20longitudinal%20MRI%20screening%20applications.%20Its%0Aability%20to%20handle%20varying%20patient%20record%20lengths%20and%20irregular%20screening%0Aintervals%20establishes%20it%20as%20an%20invaluable%20framework%20for%20monitoring%20chronic%0Adiseases%2C%20where%20timely%20and%20accurate%20disease%20prognosis%20is%20critical%20for%20effective%0Atreatment%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10733v2&entry.124074799=Read"},
{"title": "Modality Unified Attack for Omni-Modality Person Re-Identification", "author": "Yuan Bian and Min Liu and Yunqi Yi and Xueping Wang and Yunfeng Ma and Yaonan Wang", "abstract": "  Deep learning based person re-identification (re-id) models have been widely\nemployed in surveillance systems. Recent studies have demonstrated that\nblack-box single-modality and cross-modality re-id models are vulnerable to\nadversarial examples (AEs), leaving the robustness of multi-modality re-id\nmodels unexplored. Due to the lack of knowledge about the specific type of\nmodel deployed in the target black-box surveillance system, we aim to generate\nmodality unified AEs for omni-modality (single-, cross- and multi-modality)\nre-id models. Specifically, we propose a novel Modality Unified Attack method\nto train modality-specific adversarial generators to generate AEs that\neffectively attack different omni-modality models. A multi-modality model is\nadopted as the surrogate model, wherein the features of each modality are\nperturbed by metric disruption loss before fusion. To collapse the common\nfeatures of omni-modality models, Cross Modality Simulated Disruption approach\nis introduced to mimic the cross-modality feature embeddings by intentionally\nfeeding images to non-corresponding modality-specific subnetworks of the\nsurrogate model. Moreover, Multi Modality Collaborative Disruption strategy is\ndevised to facilitate the attacker to comprehensively corrupt the informative\ncontent of person images by leveraging a multi modality feature collaborative\nmetric disruption loss. Extensive experiments show that our MUA method can\neffectively attack the omni-modality re-id models, achieving 55.9%, 24.4%,\n49.0% and 62.7% mean mAP Drop Rate, respectively.\n", "link": "http://arxiv.org/abs/2501.12761v1", "date": "2025-01-22", "relevancy": 2.0471, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5366}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.51}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modality%20Unified%20Attack%20for%20Omni-Modality%20Person%20Re-Identification&body=Title%3A%20Modality%20Unified%20Attack%20for%20Omni-Modality%20Person%20Re-Identification%0AAuthor%3A%20Yuan%20Bian%20and%20Min%20Liu%20and%20Yunqi%20Yi%20and%20Xueping%20Wang%20and%20Yunfeng%20Ma%20and%20Yaonan%20Wang%0AAbstract%3A%20%20%20Deep%20learning%20based%20person%20re-identification%20%28re-id%29%20models%20have%20been%20widely%0Aemployed%20in%20surveillance%20systems.%20Recent%20studies%20have%20demonstrated%20that%0Ablack-box%20single-modality%20and%20cross-modality%20re-id%20models%20are%20vulnerable%20to%0Aadversarial%20examples%20%28AEs%29%2C%20leaving%20the%20robustness%20of%20multi-modality%20re-id%0Amodels%20unexplored.%20Due%20to%20the%20lack%20of%20knowledge%20about%20the%20specific%20type%20of%0Amodel%20deployed%20in%20the%20target%20black-box%20surveillance%20system%2C%20we%20aim%20to%20generate%0Amodality%20unified%20AEs%20for%20omni-modality%20%28single-%2C%20cross-%20and%20multi-modality%29%0Are-id%20models.%20Specifically%2C%20we%20propose%20a%20novel%20Modality%20Unified%20Attack%20method%0Ato%20train%20modality-specific%20adversarial%20generators%20to%20generate%20AEs%20that%0Aeffectively%20attack%20different%20omni-modality%20models.%20A%20multi-modality%20model%20is%0Aadopted%20as%20the%20surrogate%20model%2C%20wherein%20the%20features%20of%20each%20modality%20are%0Aperturbed%20by%20metric%20disruption%20loss%20before%20fusion.%20To%20collapse%20the%20common%0Afeatures%20of%20omni-modality%20models%2C%20Cross%20Modality%20Simulated%20Disruption%20approach%0Ais%20introduced%20to%20mimic%20the%20cross-modality%20feature%20embeddings%20by%20intentionally%0Afeeding%20images%20to%20non-corresponding%20modality-specific%20subnetworks%20of%20the%0Asurrogate%20model.%20Moreover%2C%20Multi%20Modality%20Collaborative%20Disruption%20strategy%20is%0Adevised%20to%20facilitate%20the%20attacker%20to%20comprehensively%20corrupt%20the%20informative%0Acontent%20of%20person%20images%20by%20leveraging%20a%20multi%20modality%20feature%20collaborative%0Ametric%20disruption%20loss.%20Extensive%20experiments%20show%20that%20our%20MUA%20method%20can%0Aeffectively%20attack%20the%20omni-modality%20re-id%20models%2C%20achieving%2055.9%25%2C%2024.4%25%2C%0A49.0%25%20and%2062.7%25%20mean%20mAP%20Drop%20Rate%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModality%2520Unified%2520Attack%2520for%2520Omni-Modality%2520Person%2520Re-Identification%26entry.906535625%3DYuan%2520Bian%2520and%2520Min%2520Liu%2520and%2520Yunqi%2520Yi%2520and%2520Xueping%2520Wang%2520and%2520Yunfeng%2520Ma%2520and%2520Yaonan%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520learning%2520based%2520person%2520re-identification%2520%2528re-id%2529%2520models%2520have%2520been%2520widely%250Aemployed%2520in%2520surveillance%2520systems.%2520Recent%2520studies%2520have%2520demonstrated%2520that%250Ablack-box%2520single-modality%2520and%2520cross-modality%2520re-id%2520models%2520are%2520vulnerable%2520to%250Aadversarial%2520examples%2520%2528AEs%2529%252C%2520leaving%2520the%2520robustness%2520of%2520multi-modality%2520re-id%250Amodels%2520unexplored.%2520Due%2520to%2520the%2520lack%2520of%2520knowledge%2520about%2520the%2520specific%2520type%2520of%250Amodel%2520deployed%2520in%2520the%2520target%2520black-box%2520surveillance%2520system%252C%2520we%2520aim%2520to%2520generate%250Amodality%2520unified%2520AEs%2520for%2520omni-modality%2520%2528single-%252C%2520cross-%2520and%2520multi-modality%2529%250Are-id%2520models.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520Modality%2520Unified%2520Attack%2520method%250Ato%2520train%2520modality-specific%2520adversarial%2520generators%2520to%2520generate%2520AEs%2520that%250Aeffectively%2520attack%2520different%2520omni-modality%2520models.%2520A%2520multi-modality%2520model%2520is%250Aadopted%2520as%2520the%2520surrogate%2520model%252C%2520wherein%2520the%2520features%2520of%2520each%2520modality%2520are%250Aperturbed%2520by%2520metric%2520disruption%2520loss%2520before%2520fusion.%2520To%2520collapse%2520the%2520common%250Afeatures%2520of%2520omni-modality%2520models%252C%2520Cross%2520Modality%2520Simulated%2520Disruption%2520approach%250Ais%2520introduced%2520to%2520mimic%2520the%2520cross-modality%2520feature%2520embeddings%2520by%2520intentionally%250Afeeding%2520images%2520to%2520non-corresponding%2520modality-specific%2520subnetworks%2520of%2520the%250Asurrogate%2520model.%2520Moreover%252C%2520Multi%2520Modality%2520Collaborative%2520Disruption%2520strategy%2520is%250Adevised%2520to%2520facilitate%2520the%2520attacker%2520to%2520comprehensively%2520corrupt%2520the%2520informative%250Acontent%2520of%2520person%2520images%2520by%2520leveraging%2520a%2520multi%2520modality%2520feature%2520collaborative%250Ametric%2520disruption%2520loss.%2520Extensive%2520experiments%2520show%2520that%2520our%2520MUA%2520method%2520can%250Aeffectively%2520attack%2520the%2520omni-modality%2520re-id%2520models%252C%2520achieving%252055.9%2525%252C%252024.4%2525%252C%250A49.0%2525%2520and%252062.7%2525%2520mean%2520mAP%2520Drop%2520Rate%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modality%20Unified%20Attack%20for%20Omni-Modality%20Person%20Re-Identification&entry.906535625=Yuan%20Bian%20and%20Min%20Liu%20and%20Yunqi%20Yi%20and%20Xueping%20Wang%20and%20Yunfeng%20Ma%20and%20Yaonan%20Wang&entry.1292438233=%20%20Deep%20learning%20based%20person%20re-identification%20%28re-id%29%20models%20have%20been%20widely%0Aemployed%20in%20surveillance%20systems.%20Recent%20studies%20have%20demonstrated%20that%0Ablack-box%20single-modality%20and%20cross-modality%20re-id%20models%20are%20vulnerable%20to%0Aadversarial%20examples%20%28AEs%29%2C%20leaving%20the%20robustness%20of%20multi-modality%20re-id%0Amodels%20unexplored.%20Due%20to%20the%20lack%20of%20knowledge%20about%20the%20specific%20type%20of%0Amodel%20deployed%20in%20the%20target%20black-box%20surveillance%20system%2C%20we%20aim%20to%20generate%0Amodality%20unified%20AEs%20for%20omni-modality%20%28single-%2C%20cross-%20and%20multi-modality%29%0Are-id%20models.%20Specifically%2C%20we%20propose%20a%20novel%20Modality%20Unified%20Attack%20method%0Ato%20train%20modality-specific%20adversarial%20generators%20to%20generate%20AEs%20that%0Aeffectively%20attack%20different%20omni-modality%20models.%20A%20multi-modality%20model%20is%0Aadopted%20as%20the%20surrogate%20model%2C%20wherein%20the%20features%20of%20each%20modality%20are%0Aperturbed%20by%20metric%20disruption%20loss%20before%20fusion.%20To%20collapse%20the%20common%0Afeatures%20of%20omni-modality%20models%2C%20Cross%20Modality%20Simulated%20Disruption%20approach%0Ais%20introduced%20to%20mimic%20the%20cross-modality%20feature%20embeddings%20by%20intentionally%0Afeeding%20images%20to%20non-corresponding%20modality-specific%20subnetworks%20of%20the%0Asurrogate%20model.%20Moreover%2C%20Multi%20Modality%20Collaborative%20Disruption%20strategy%20is%0Adevised%20to%20facilitate%20the%20attacker%20to%20comprehensively%20corrupt%20the%20informative%0Acontent%20of%20person%20images%20by%20leveraging%20a%20multi%20modality%20feature%20collaborative%0Ametric%20disruption%20loss.%20Extensive%20experiments%20show%20that%20our%20MUA%20method%20can%0Aeffectively%20attack%20the%20omni-modality%20re-id%20models%2C%20achieving%2055.9%25%2C%2024.4%25%2C%0A49.0%25%20and%2062.7%25%20mean%20mAP%20Drop%20Rate%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12761v1&entry.124074799=Read"},
{"title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "author": "Pengxiang Zhao and Xiaoming Yuan", "abstract": "  Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.\n", "link": "http://arxiv.org/abs/2501.12956v1", "date": "2025-01-22", "relevancy": 2.0455, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.515}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5089}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GANQ%3A%20GPU-Adaptive%20Non-Uniform%20Quantization%20for%20Large%20Language%20Models&body=Title%3A%20GANQ%3A%20GPU-Adaptive%20Non-Uniform%20Quantization%20for%20Large%20Language%20Models%0AAuthor%3A%20Pengxiang%20Zhao%20and%20Xiaoming%20Yuan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20deployment%20challenges%20due%20to%0Atheir%20substantial%20resource%20requirements.%20While%20low-bit%20quantized%20weights%20can%0Areduce%20memory%20usage%20and%20improve%20inference%20efficiency%2C%20current%20hardware%20lacks%0Anative%20support%20for%20mixed-precision%20General%20Matrix%20Multiplication%20%28mpGEMM%29%2C%0Aresulting%20in%20inefficient%20dequantization-based%20implementations.%20Moreover%2C%0Auniform%20quantization%20methods%20often%20fail%20to%20capture%20weight%20distributions%0Aadequately%2C%20leading%20to%20performance%20degradation.%20We%20propose%20GANQ%20%28GPU-Adaptive%0ANon-Uniform%20Quantization%29%2C%20a%20layer-wise%20post-training%20non-uniform%20quantization%0Aframework%20optimized%20for%20hardware-efficient%20lookup%20table-based%20mpGEMM.%20GANQ%0Aachieves%20superior%20quantization%20performance%20by%20utilizing%20a%20training-free%2C%0AGPU-adaptive%20optimization%20algorithm%20to%20efficiently%20reduce%20layer-wise%0Aquantization%20errors.%20Extensive%20experiments%20demonstrate%20GANQ%27s%20ability%20to%20reduce%0Athe%20perplexity%20gap%20from%20the%20FP16%20baseline%20compared%20to%20state-of-the-art%20methods%0Afor%20both%203-bit%20and%204-bit%20quantization.%20Furthermore%2C%20when%20deployed%20on%20a%20single%0ANVIDIA%20RTX%204090%20GPU%2C%20GANQ%27s%20quantized%20models%20achieve%20up%20to%202.57%24%5Ctimes%24%20speedup%0Aover%20the%20baseline%2C%20advancing%20memory%20and%20inference%20efficiency%20in%20LLM%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGANQ%253A%2520GPU-Adaptive%2520Non-Uniform%2520Quantization%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DPengxiang%2520Zhao%2520and%2520Xiaoming%2520Yuan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520face%2520significant%2520deployment%2520challenges%2520due%2520to%250Atheir%2520substantial%2520resource%2520requirements.%2520While%2520low-bit%2520quantized%2520weights%2520can%250Areduce%2520memory%2520usage%2520and%2520improve%2520inference%2520efficiency%252C%2520current%2520hardware%2520lacks%250Anative%2520support%2520for%2520mixed-precision%2520General%2520Matrix%2520Multiplication%2520%2528mpGEMM%2529%252C%250Aresulting%2520in%2520inefficient%2520dequantization-based%2520implementations.%2520Moreover%252C%250Auniform%2520quantization%2520methods%2520often%2520fail%2520to%2520capture%2520weight%2520distributions%250Aadequately%252C%2520leading%2520to%2520performance%2520degradation.%2520We%2520propose%2520GANQ%2520%2528GPU-Adaptive%250ANon-Uniform%2520Quantization%2529%252C%2520a%2520layer-wise%2520post-training%2520non-uniform%2520quantization%250Aframework%2520optimized%2520for%2520hardware-efficient%2520lookup%2520table-based%2520mpGEMM.%2520GANQ%250Aachieves%2520superior%2520quantization%2520performance%2520by%2520utilizing%2520a%2520training-free%252C%250AGPU-adaptive%2520optimization%2520algorithm%2520to%2520efficiently%2520reduce%2520layer-wise%250Aquantization%2520errors.%2520Extensive%2520experiments%2520demonstrate%2520GANQ%2527s%2520ability%2520to%2520reduce%250Athe%2520perplexity%2520gap%2520from%2520the%2520FP16%2520baseline%2520compared%2520to%2520state-of-the-art%2520methods%250Afor%2520both%25203-bit%2520and%25204-bit%2520quantization.%2520Furthermore%252C%2520when%2520deployed%2520on%2520a%2520single%250ANVIDIA%2520RTX%25204090%2520GPU%252C%2520GANQ%2527s%2520quantized%2520models%2520achieve%2520up%2520to%25202.57%2524%255Ctimes%2524%2520speedup%250Aover%2520the%2520baseline%252C%2520advancing%2520memory%2520and%2520inference%2520efficiency%2520in%2520LLM%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GANQ%3A%20GPU-Adaptive%20Non-Uniform%20Quantization%20for%20Large%20Language%20Models&entry.906535625=Pengxiang%20Zhao%20and%20Xiaoming%20Yuan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20face%20significant%20deployment%20challenges%20due%20to%0Atheir%20substantial%20resource%20requirements.%20While%20low-bit%20quantized%20weights%20can%0Areduce%20memory%20usage%20and%20improve%20inference%20efficiency%2C%20current%20hardware%20lacks%0Anative%20support%20for%20mixed-precision%20General%20Matrix%20Multiplication%20%28mpGEMM%29%2C%0Aresulting%20in%20inefficient%20dequantization-based%20implementations.%20Moreover%2C%0Auniform%20quantization%20methods%20often%20fail%20to%20capture%20weight%20distributions%0Aadequately%2C%20leading%20to%20performance%20degradation.%20We%20propose%20GANQ%20%28GPU-Adaptive%0ANon-Uniform%20Quantization%29%2C%20a%20layer-wise%20post-training%20non-uniform%20quantization%0Aframework%20optimized%20for%20hardware-efficient%20lookup%20table-based%20mpGEMM.%20GANQ%0Aachieves%20superior%20quantization%20performance%20by%20utilizing%20a%20training-free%2C%0AGPU-adaptive%20optimization%20algorithm%20to%20efficiently%20reduce%20layer-wise%0Aquantization%20errors.%20Extensive%20experiments%20demonstrate%20GANQ%27s%20ability%20to%20reduce%0Athe%20perplexity%20gap%20from%20the%20FP16%20baseline%20compared%20to%20state-of-the-art%20methods%0Afor%20both%203-bit%20and%204-bit%20quantization.%20Furthermore%2C%20when%20deployed%20on%20a%20single%0ANVIDIA%20RTX%204090%20GPU%2C%20GANQ%27s%20quantized%20models%20achieve%20up%20to%202.57%24%5Ctimes%24%20speedup%0Aover%20the%20baseline%2C%20advancing%20memory%20and%20inference%20efficiency%20in%20LLM%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12956v1&entry.124074799=Read"},
{"title": "NExtLong: Toward Effective Long-Context Training without Long Documents", "author": "Chaochen Gao and Xing Wu and Zijia Lin and Debing Zhang and Songlin Hu", "abstract": "  Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs.\n", "link": "http://arxiv.org/abs/2501.12766v1", "date": "2025-01-22", "relevancy": 2.044, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NExtLong%3A%20Toward%20Effective%20Long-Context%20Training%20without%20Long%20Documents&body=Title%3A%20NExtLong%3A%20Toward%20Effective%20Long-Context%20Training%20without%20Long%20Documents%0AAuthor%3A%20Chaochen%20Gao%20and%20Xing%20Wu%20and%20Zijia%20Lin%20and%20Debing%20Zhang%20and%20Songlin%20Hu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20with%20extended%20context%20windows%20have%20made%0Asignificant%20strides%20yet%20remain%20a%20challenge%20due%20to%20the%20scarcity%20of%20long%0Adocuments.%20Existing%20methods%20tend%20to%20synthesize%20long-context%20data%20but%20lack%20a%0Aclear%20mechanism%20to%20reinforce%20the%20long-range%20dependency%20modeling.%20To%20address%0Athis%20limitation%2C%20we%20propose%20NExtLong%2C%20a%20novel%20framework%20for%20synthesizing%0Along-context%20data%20through%20Negative%20document%20Extension.%20NExtLong%20decomposes%20a%0Adocument%20into%20multiple%20meta-chunks%20and%20extends%20the%20context%20by%20interleaving%20hard%0Anegative%20distractors%20retrieved%20from%20pretraining%20corpora.%20This%20approach%20compels%0Athe%20model%20to%20discriminate%20long-range%20dependent%20context%20from%20distracting%0Acontent%2C%20enhancing%20its%20ability%20to%20model%20long-range%20dependencies.%20Extensive%0Aexperiments%20demonstrate%20that%20NExtLong%20achieves%20significant%20performance%0Aimprovements%20on%20the%20HELMET%20and%20RULER%20benchmarks%20compared%20to%20existing%0Along-context%20synthesis%20approaches%20and%20leading%20models%2C%20which%20are%20trained%20on%0Anon-synthetic%20long%20documents.%20These%20findings%20highlight%20NExtLong%27s%20ability%20to%0Areduce%20reliance%20on%20non-synthetic%20long%20documents%2C%20making%20it%20an%20effective%0Aframework%20for%20developing%20advanced%20long-context%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNExtLong%253A%2520Toward%2520Effective%2520Long-Context%2520Training%2520without%2520Long%2520Documents%26entry.906535625%3DChaochen%2520Gao%2520and%2520Xing%2520Wu%2520and%2520Zijia%2520Lin%2520and%2520Debing%2520Zhang%2520and%2520Songlin%2520Hu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520with%2520extended%2520context%2520windows%2520have%2520made%250Asignificant%2520strides%2520yet%2520remain%2520a%2520challenge%2520due%2520to%2520the%2520scarcity%2520of%2520long%250Adocuments.%2520Existing%2520methods%2520tend%2520to%2520synthesize%2520long-context%2520data%2520but%2520lack%2520a%250Aclear%2520mechanism%2520to%2520reinforce%2520the%2520long-range%2520dependency%2520modeling.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520propose%2520NExtLong%252C%2520a%2520novel%2520framework%2520for%2520synthesizing%250Along-context%2520data%2520through%2520Negative%2520document%2520Extension.%2520NExtLong%2520decomposes%2520a%250Adocument%2520into%2520multiple%2520meta-chunks%2520and%2520extends%2520the%2520context%2520by%2520interleaving%2520hard%250Anegative%2520distractors%2520retrieved%2520from%2520pretraining%2520corpora.%2520This%2520approach%2520compels%250Athe%2520model%2520to%2520discriminate%2520long-range%2520dependent%2520context%2520from%2520distracting%250Acontent%252C%2520enhancing%2520its%2520ability%2520to%2520model%2520long-range%2520dependencies.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520NExtLong%2520achieves%2520significant%2520performance%250Aimprovements%2520on%2520the%2520HELMET%2520and%2520RULER%2520benchmarks%2520compared%2520to%2520existing%250Along-context%2520synthesis%2520approaches%2520and%2520leading%2520models%252C%2520which%2520are%2520trained%2520on%250Anon-synthetic%2520long%2520documents.%2520These%2520findings%2520highlight%2520NExtLong%2527s%2520ability%2520to%250Areduce%2520reliance%2520on%2520non-synthetic%2520long%2520documents%252C%2520making%2520it%2520an%2520effective%250Aframework%2520for%2520developing%2520advanced%2520long-context%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NExtLong%3A%20Toward%20Effective%20Long-Context%20Training%20without%20Long%20Documents&entry.906535625=Chaochen%20Gao%20and%20Xing%20Wu%20and%20Zijia%20Lin%20and%20Debing%20Zhang%20and%20Songlin%20Hu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20with%20extended%20context%20windows%20have%20made%0Asignificant%20strides%20yet%20remain%20a%20challenge%20due%20to%20the%20scarcity%20of%20long%0Adocuments.%20Existing%20methods%20tend%20to%20synthesize%20long-context%20data%20but%20lack%20a%0Aclear%20mechanism%20to%20reinforce%20the%20long-range%20dependency%20modeling.%20To%20address%0Athis%20limitation%2C%20we%20propose%20NExtLong%2C%20a%20novel%20framework%20for%20synthesizing%0Along-context%20data%20through%20Negative%20document%20Extension.%20NExtLong%20decomposes%20a%0Adocument%20into%20multiple%20meta-chunks%20and%20extends%20the%20context%20by%20interleaving%20hard%0Anegative%20distractors%20retrieved%20from%20pretraining%20corpora.%20This%20approach%20compels%0Athe%20model%20to%20discriminate%20long-range%20dependent%20context%20from%20distracting%0Acontent%2C%20enhancing%20its%20ability%20to%20model%20long-range%20dependencies.%20Extensive%0Aexperiments%20demonstrate%20that%20NExtLong%20achieves%20significant%20performance%0Aimprovements%20on%20the%20HELMET%20and%20RULER%20benchmarks%20compared%20to%20existing%0Along-context%20synthesis%20approaches%20and%20leading%20models%2C%20which%20are%20trained%20on%0Anon-synthetic%20long%20documents.%20These%20findings%20highlight%20NExtLong%27s%20ability%20to%0Areduce%20reliance%20on%20non-synthetic%20long%20documents%2C%20making%20it%20an%20effective%0Aframework%20for%20developing%20advanced%20long-context%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12766v1&entry.124074799=Read"},
{"title": "An Embedding is Worth a Thousand Noisy Labels", "author": "Francesco Di Salvo and Sebastian Doerrich and Ines Rieger and Christian Ledig", "abstract": "  The performance of deep neural networks scales with dataset size and label\nquality, rendering the efficient mitigation of low-quality data annotations\ncrucial for building robust and cost-effective systems. Existing strategies to\naddress label noise exhibit severe limitations due to computational complexity\nand application dependency. In this work, we propose WANN, a Weighted Adaptive\nNearest Neighbor approach that builds on self-supervised feature\nrepresentations obtained from foundation models. To guide the weighted voting\nscheme, we introduce a reliability score, which measures the likelihood of a\ndata label being correct. WANN outperforms reference methods, including a\nlinear layer trained with robust loss functions, on diverse datasets of varying\nsize and under various noise types and severities. WANN also exhibits superior\ngeneralization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed\nk-NNs. Furthermore, the proposed weighting scheme enhances supervised\ndimensionality reduction under noisy labels. This yields a significant boost in\nclassification performance with 10x and 100x smaller image embeddings,\nminimizing latency and storage requirements. Our approach, emphasizing\nefficiency and explainability, emerges as a simple, robust solution to overcome\ninherent limitations of deep neural network training. The code is available at\nhttps://github.com/francescodisalvo05/wann-noisy-labels .\n", "link": "http://arxiv.org/abs/2408.14358v2", "date": "2025-01-22", "relevancy": 2.0226, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5167}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5155}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Embedding%20is%20Worth%20a%20Thousand%20Noisy%20Labels&body=Title%3A%20An%20Embedding%20is%20Worth%20a%20Thousand%20Noisy%20Labels%0AAuthor%3A%20Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Ines%20Rieger%20and%20Christian%20Ledig%0AAbstract%3A%20%20%20The%20performance%20of%20deep%20neural%20networks%20scales%20with%20dataset%20size%20and%20label%0Aquality%2C%20rendering%20the%20efficient%20mitigation%20of%20low-quality%20data%20annotations%0Acrucial%20for%20building%20robust%20and%20cost-effective%20systems.%20Existing%20strategies%20to%0Aaddress%20label%20noise%20exhibit%20severe%20limitations%20due%20to%20computational%20complexity%0Aand%20application%20dependency.%20In%20this%20work%2C%20we%20propose%20WANN%2C%20a%20Weighted%20Adaptive%0ANearest%20Neighbor%20approach%20that%20builds%20on%20self-supervised%20feature%0Arepresentations%20obtained%20from%20foundation%20models.%20To%20guide%20the%20weighted%20voting%0Ascheme%2C%20we%20introduce%20a%20reliability%20score%2C%20which%20measures%20the%20likelihood%20of%20a%0Adata%20label%20being%20correct.%20WANN%20outperforms%20reference%20methods%2C%20including%20a%0Alinear%20layer%20trained%20with%20robust%20loss%20functions%2C%20on%20diverse%20datasets%20of%20varying%0Asize%20and%20under%20various%20noise%20types%20and%20severities.%20WANN%20also%20exhibits%20superior%0Ageneralization%20on%20imbalanced%20data%20compared%20to%20both%20Adaptive-NNs%20%28ANN%29%20and%20fixed%0Ak-NNs.%20Furthermore%2C%20the%20proposed%20weighting%20scheme%20enhances%20supervised%0Adimensionality%20reduction%20under%20noisy%20labels.%20This%20yields%20a%20significant%20boost%20in%0Aclassification%20performance%20with%2010x%20and%20100x%20smaller%20image%20embeddings%2C%0Aminimizing%20latency%20and%20storage%20requirements.%20Our%20approach%2C%20emphasizing%0Aefficiency%20and%20explainability%2C%20emerges%20as%20a%20simple%2C%20robust%20solution%20to%20overcome%0Ainherent%20limitations%20of%20deep%20neural%20network%20training.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/wann-noisy-labels%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14358v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Embedding%2520is%2520Worth%2520a%2520Thousand%2520Noisy%2520Labels%26entry.906535625%3DFrancesco%2520Di%2520Salvo%2520and%2520Sebastian%2520Doerrich%2520and%2520Ines%2520Rieger%2520and%2520Christian%2520Ledig%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520deep%2520neural%2520networks%2520scales%2520with%2520dataset%2520size%2520and%2520label%250Aquality%252C%2520rendering%2520the%2520efficient%2520mitigation%2520of%2520low-quality%2520data%2520annotations%250Acrucial%2520for%2520building%2520robust%2520and%2520cost-effective%2520systems.%2520Existing%2520strategies%2520to%250Aaddress%2520label%2520noise%2520exhibit%2520severe%2520limitations%2520due%2520to%2520computational%2520complexity%250Aand%2520application%2520dependency.%2520In%2520this%2520work%252C%2520we%2520propose%2520WANN%252C%2520a%2520Weighted%2520Adaptive%250ANearest%2520Neighbor%2520approach%2520that%2520builds%2520on%2520self-supervised%2520feature%250Arepresentations%2520obtained%2520from%2520foundation%2520models.%2520To%2520guide%2520the%2520weighted%2520voting%250Ascheme%252C%2520we%2520introduce%2520a%2520reliability%2520score%252C%2520which%2520measures%2520the%2520likelihood%2520of%2520a%250Adata%2520label%2520being%2520correct.%2520WANN%2520outperforms%2520reference%2520methods%252C%2520including%2520a%250Alinear%2520layer%2520trained%2520with%2520robust%2520loss%2520functions%252C%2520on%2520diverse%2520datasets%2520of%2520varying%250Asize%2520and%2520under%2520various%2520noise%2520types%2520and%2520severities.%2520WANN%2520also%2520exhibits%2520superior%250Ageneralization%2520on%2520imbalanced%2520data%2520compared%2520to%2520both%2520Adaptive-NNs%2520%2528ANN%2529%2520and%2520fixed%250Ak-NNs.%2520Furthermore%252C%2520the%2520proposed%2520weighting%2520scheme%2520enhances%2520supervised%250Adimensionality%2520reduction%2520under%2520noisy%2520labels.%2520This%2520yields%2520a%2520significant%2520boost%2520in%250Aclassification%2520performance%2520with%252010x%2520and%2520100x%2520smaller%2520image%2520embeddings%252C%250Aminimizing%2520latency%2520and%2520storage%2520requirements.%2520Our%2520approach%252C%2520emphasizing%250Aefficiency%2520and%2520explainability%252C%2520emerges%2520as%2520a%2520simple%252C%2520robust%2520solution%2520to%2520overcome%250Ainherent%2520limitations%2520of%2520deep%2520neural%2520network%2520training.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/francescodisalvo05/wann-noisy-labels%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14358v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Embedding%20is%20Worth%20a%20Thousand%20Noisy%20Labels&entry.906535625=Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Ines%20Rieger%20and%20Christian%20Ledig&entry.1292438233=%20%20The%20performance%20of%20deep%20neural%20networks%20scales%20with%20dataset%20size%20and%20label%0Aquality%2C%20rendering%20the%20efficient%20mitigation%20of%20low-quality%20data%20annotations%0Acrucial%20for%20building%20robust%20and%20cost-effective%20systems.%20Existing%20strategies%20to%0Aaddress%20label%20noise%20exhibit%20severe%20limitations%20due%20to%20computational%20complexity%0Aand%20application%20dependency.%20In%20this%20work%2C%20we%20propose%20WANN%2C%20a%20Weighted%20Adaptive%0ANearest%20Neighbor%20approach%20that%20builds%20on%20self-supervised%20feature%0Arepresentations%20obtained%20from%20foundation%20models.%20To%20guide%20the%20weighted%20voting%0Ascheme%2C%20we%20introduce%20a%20reliability%20score%2C%20which%20measures%20the%20likelihood%20of%20a%0Adata%20label%20being%20correct.%20WANN%20outperforms%20reference%20methods%2C%20including%20a%0Alinear%20layer%20trained%20with%20robust%20loss%20functions%2C%20on%20diverse%20datasets%20of%20varying%0Asize%20and%20under%20various%20noise%20types%20and%20severities.%20WANN%20also%20exhibits%20superior%0Ageneralization%20on%20imbalanced%20data%20compared%20to%20both%20Adaptive-NNs%20%28ANN%29%20and%20fixed%0Ak-NNs.%20Furthermore%2C%20the%20proposed%20weighting%20scheme%20enhances%20supervised%0Adimensionality%20reduction%20under%20noisy%20labels.%20This%20yields%20a%20significant%20boost%20in%0Aclassification%20performance%20with%2010x%20and%20100x%20smaller%20image%20embeddings%2C%0Aminimizing%20latency%20and%20storage%20requirements.%20Our%20approach%2C%20emphasizing%0Aefficiency%20and%20explainability%2C%20emerges%20as%20a%20simple%2C%20robust%20solution%20to%20overcome%0Ainherent%20limitations%20of%20deep%20neural%20network%20training.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/francescodisalvo05/wann-noisy-labels%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14358v2&entry.124074799=Read"},
{"title": "Provably-Safe Neural Network Training Using Hybrid Zonotope Reachability\n  Analysis", "author": "Long Kiu Chung and Shreyas Kousik", "abstract": "  Even though neural networks are being increasingly deployed in\nsafety-critical applications, it remains difficult to enforce constraints on\ntheir output, meaning that it is hard to guarantee safety in such settings.\nTowards addressing this, many existing methods seek to verify a neural\nnetwork's satisfaction of safety constraints, but do not address how to correct\nan \"unsafe\" network. On the other hand, the few works that extract a training\nsignal from verification cannot handle non-convex sets, and are either\nconservative or slow. To address these challenges, this work proposes a neural\nnetwork training method that can encourage the exact reachable set of a\nnon-convex input set through a neural network with rectified linear unit (ReLU)\nnonlinearities to avoid a non-convex unsafe region, using recent results in\nnon-convex set representation with hybrid zonotopes and extracting gradient\ninformation from mixed-integer linear programs (MILPs). The proposed method is\nfast, with the computational complexity of each training iteration comparable\nto that of solving a linear program (LP) with number of dimensions and\nconstraints linear to the number of neurons and complexity of input and unsafe\nsets. For a neural network with three hidden layers of width 30, the method was\nable to drive the reachable set of a non-convex input set with 55 generators\nand 26 constraints out of a non-convex unsafe region with 21 generators and 11\nconstraints in 490 seconds.\n", "link": "http://arxiv.org/abs/2501.13023v1", "date": "2025-01-22", "relevancy": 2.0186, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5141}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5059}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably-Safe%20Neural%20Network%20Training%20Using%20Hybrid%20Zonotope%20Reachability%0A%20%20Analysis&body=Title%3A%20Provably-Safe%20Neural%20Network%20Training%20Using%20Hybrid%20Zonotope%20Reachability%0A%20%20Analysis%0AAuthor%3A%20Long%20Kiu%20Chung%20and%20Shreyas%20Kousik%0AAbstract%3A%20%20%20Even%20though%20neural%20networks%20are%20being%20increasingly%20deployed%20in%0Asafety-critical%20applications%2C%20it%20remains%20difficult%20to%20enforce%20constraints%20on%0Atheir%20output%2C%20meaning%20that%20it%20is%20hard%20to%20guarantee%20safety%20in%20such%20settings.%0ATowards%20addressing%20this%2C%20many%20existing%20methods%20seek%20to%20verify%20a%20neural%0Anetwork%27s%20satisfaction%20of%20safety%20constraints%2C%20but%20do%20not%20address%20how%20to%20correct%0Aan%20%22unsafe%22%20network.%20On%20the%20other%20hand%2C%20the%20few%20works%20that%20extract%20a%20training%0Asignal%20from%20verification%20cannot%20handle%20non-convex%20sets%2C%20and%20are%20either%0Aconservative%20or%20slow.%20To%20address%20these%20challenges%2C%20this%20work%20proposes%20a%20neural%0Anetwork%20training%20method%20that%20can%20encourage%20the%20exact%20reachable%20set%20of%20a%0Anon-convex%20input%20set%20through%20a%20neural%20network%20with%20rectified%20linear%20unit%20%28ReLU%29%0Anonlinearities%20to%20avoid%20a%20non-convex%20unsafe%20region%2C%20using%20recent%20results%20in%0Anon-convex%20set%20representation%20with%20hybrid%20zonotopes%20and%20extracting%20gradient%0Ainformation%20from%20mixed-integer%20linear%20programs%20%28MILPs%29.%20The%20proposed%20method%20is%0Afast%2C%20with%20the%20computational%20complexity%20of%20each%20training%20iteration%20comparable%0Ato%20that%20of%20solving%20a%20linear%20program%20%28LP%29%20with%20number%20of%20dimensions%20and%0Aconstraints%20linear%20to%20the%20number%20of%20neurons%20and%20complexity%20of%20input%20and%20unsafe%0Asets.%20For%20a%20neural%20network%20with%20three%20hidden%20layers%20of%20width%2030%2C%20the%20method%20was%0Aable%20to%20drive%20the%20reachable%20set%20of%20a%20non-convex%20input%20set%20with%2055%20generators%0Aand%2026%20constraints%20out%20of%20a%20non-convex%20unsafe%20region%20with%2021%20generators%20and%2011%0Aconstraints%20in%20490%20seconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably-Safe%2520Neural%2520Network%2520Training%2520Using%2520Hybrid%2520Zonotope%2520Reachability%250A%2520%2520Analysis%26entry.906535625%3DLong%2520Kiu%2520Chung%2520and%2520Shreyas%2520Kousik%26entry.1292438233%3D%2520%2520Even%2520though%2520neural%2520networks%2520are%2520being%2520increasingly%2520deployed%2520in%250Asafety-critical%2520applications%252C%2520it%2520remains%2520difficult%2520to%2520enforce%2520constraints%2520on%250Atheir%2520output%252C%2520meaning%2520that%2520it%2520is%2520hard%2520to%2520guarantee%2520safety%2520in%2520such%2520settings.%250ATowards%2520addressing%2520this%252C%2520many%2520existing%2520methods%2520seek%2520to%2520verify%2520a%2520neural%250Anetwork%2527s%2520satisfaction%2520of%2520safety%2520constraints%252C%2520but%2520do%2520not%2520address%2520how%2520to%2520correct%250Aan%2520%2522unsafe%2522%2520network.%2520On%2520the%2520other%2520hand%252C%2520the%2520few%2520works%2520that%2520extract%2520a%2520training%250Asignal%2520from%2520verification%2520cannot%2520handle%2520non-convex%2520sets%252C%2520and%2520are%2520either%250Aconservative%2520or%2520slow.%2520To%2520address%2520these%2520challenges%252C%2520this%2520work%2520proposes%2520a%2520neural%250Anetwork%2520training%2520method%2520that%2520can%2520encourage%2520the%2520exact%2520reachable%2520set%2520of%2520a%250Anon-convex%2520input%2520set%2520through%2520a%2520neural%2520network%2520with%2520rectified%2520linear%2520unit%2520%2528ReLU%2529%250Anonlinearities%2520to%2520avoid%2520a%2520non-convex%2520unsafe%2520region%252C%2520using%2520recent%2520results%2520in%250Anon-convex%2520set%2520representation%2520with%2520hybrid%2520zonotopes%2520and%2520extracting%2520gradient%250Ainformation%2520from%2520mixed-integer%2520linear%2520programs%2520%2528MILPs%2529.%2520The%2520proposed%2520method%2520is%250Afast%252C%2520with%2520the%2520computational%2520complexity%2520of%2520each%2520training%2520iteration%2520comparable%250Ato%2520that%2520of%2520solving%2520a%2520linear%2520program%2520%2528LP%2529%2520with%2520number%2520of%2520dimensions%2520and%250Aconstraints%2520linear%2520to%2520the%2520number%2520of%2520neurons%2520and%2520complexity%2520of%2520input%2520and%2520unsafe%250Asets.%2520For%2520a%2520neural%2520network%2520with%2520three%2520hidden%2520layers%2520of%2520width%252030%252C%2520the%2520method%2520was%250Aable%2520to%2520drive%2520the%2520reachable%2520set%2520of%2520a%2520non-convex%2520input%2520set%2520with%252055%2520generators%250Aand%252026%2520constraints%2520out%2520of%2520a%2520non-convex%2520unsafe%2520region%2520with%252021%2520generators%2520and%252011%250Aconstraints%2520in%2520490%2520seconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably-Safe%20Neural%20Network%20Training%20Using%20Hybrid%20Zonotope%20Reachability%0A%20%20Analysis&entry.906535625=Long%20Kiu%20Chung%20and%20Shreyas%20Kousik&entry.1292438233=%20%20Even%20though%20neural%20networks%20are%20being%20increasingly%20deployed%20in%0Asafety-critical%20applications%2C%20it%20remains%20difficult%20to%20enforce%20constraints%20on%0Atheir%20output%2C%20meaning%20that%20it%20is%20hard%20to%20guarantee%20safety%20in%20such%20settings.%0ATowards%20addressing%20this%2C%20many%20existing%20methods%20seek%20to%20verify%20a%20neural%0Anetwork%27s%20satisfaction%20of%20safety%20constraints%2C%20but%20do%20not%20address%20how%20to%20correct%0Aan%20%22unsafe%22%20network.%20On%20the%20other%20hand%2C%20the%20few%20works%20that%20extract%20a%20training%0Asignal%20from%20verification%20cannot%20handle%20non-convex%20sets%2C%20and%20are%20either%0Aconservative%20or%20slow.%20To%20address%20these%20challenges%2C%20this%20work%20proposes%20a%20neural%0Anetwork%20training%20method%20that%20can%20encourage%20the%20exact%20reachable%20set%20of%20a%0Anon-convex%20input%20set%20through%20a%20neural%20network%20with%20rectified%20linear%20unit%20%28ReLU%29%0Anonlinearities%20to%20avoid%20a%20non-convex%20unsafe%20region%2C%20using%20recent%20results%20in%0Anon-convex%20set%20representation%20with%20hybrid%20zonotopes%20and%20extracting%20gradient%0Ainformation%20from%20mixed-integer%20linear%20programs%20%28MILPs%29.%20The%20proposed%20method%20is%0Afast%2C%20with%20the%20computational%20complexity%20of%20each%20training%20iteration%20comparable%0Ato%20that%20of%20solving%20a%20linear%20program%20%28LP%29%20with%20number%20of%20dimensions%20and%0Aconstraints%20linear%20to%20the%20number%20of%20neurons%20and%20complexity%20of%20input%20and%20unsafe%0Asets.%20For%20a%20neural%20network%20with%20three%20hidden%20layers%20of%20width%2030%2C%20the%20method%20was%0Aable%20to%20drive%20the%20reachable%20set%20of%20a%20non-convex%20input%20set%20with%2055%20generators%0Aand%2026%20constraints%20out%20of%20a%20non-convex%20unsafe%20region%20with%2021%20generators%20and%2011%0Aconstraints%20in%20490%20seconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13023v1&entry.124074799=Read"},
{"title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection", "author": "Pablo Miralles-Gonz\u00e1lez and Javier Huertas-Tato and Alejandro Mart\u00edn and David Camacho", "abstract": "  The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.\n", "link": "http://arxiv.org/abs/2501.03940v2", "date": "2025-01-22", "relevancy": 2.0167, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5292}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.496}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20all%20tokens%20are%20created%20equal%3A%20Perplexity%20Attention%20Weighted%20Networks%0A%20%20for%20AI%20generated%20text%20detection&body=Title%3A%20Not%20all%20tokens%20are%20created%20equal%3A%20Perplexity%20Attention%20Weighted%20Networks%0A%20%20for%20AI%20generated%20text%20detection%0AAuthor%3A%20Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho%0AAbstract%3A%20%20%20The%20rapid%20advancement%20in%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aenhanced%20their%20ability%20to%20generate%20coherent%20and%20contextually%20relevant%20text%2C%0Araising%20concerns%20about%20the%20misuse%20of%20AI-generated%20content%20and%20making%20it%0Acritical%20to%20detect%20it.%20However%2C%20the%20task%20remains%20challenging%2C%20particularly%20in%0Aunseen%20domains%20or%20with%20unfamiliar%20LLMs.%20Leveraging%20LLM%20next-token%20distribution%0Aoutputs%20offers%20a%20theoretically%20appealing%20approach%20for%20detection%2C%20as%20they%0Aencapsulate%20insights%20from%20the%20models%27%20extensive%20pre-training%20on%20diverse%0Acorpora.%20Despite%20its%20promise%2C%20zero-shot%20methods%20that%20attempt%20to%20operationalize%0Athese%20outputs%20have%20met%20with%20limited%20success.%20We%20hypothesize%20that%20one%20of%20the%0Aproblems%20is%20that%20they%20use%20the%20mean%20to%20aggregate%20next-token%20distribution%20metrics%0Aacross%20tokens%2C%20when%20some%20tokens%20are%20naturally%20easier%20or%20harder%20to%20predict%20and%0Ashould%20be%20weighted%20differently.%20Based%20on%20this%20idea%2C%20we%20propose%20the%20Perplexity%0AAttention%20Weighted%20Network%20%28PAWN%29%2C%20which%20uses%20the%20last%20hidden%20states%20of%20the%20LLM%0Aand%20positions%20to%20weight%20the%20sum%20of%20a%20series%20of%20features%20based%20on%20metrics%20from%0Athe%20next-token%20distribution%20across%20the%20sequence%20length.%20Although%20not%20zero-shot%2C%0Aour%20method%20allows%20us%20to%20cache%20the%20last%20hidden%20states%20and%20next-token%0Adistribution%20metrics%20on%20disk%2C%20greatly%20reducing%20the%20training%20resource%0Arequirements.%20PAWN%20shows%20competitive%20and%20even%20better%20performance%0Ain-distribution%20than%20the%20strongest%20baselines%20%28fine-tuned%20LMs%29%20with%20a%20fraction%0Aof%20their%20trainable%20parameters.%20Our%20model%20also%20generalizes%20better%20to%20unseen%0Adomains%20and%20source%20models%2C%20with%20smaller%20variability%20in%20the%20decision%20boundary%0Aacross%20distribution%20shifts.%20It%20is%20also%20more%20robust%20to%20adversarial%20attacks%2C%20and%0Aif%20the%20backbone%20has%20multilingual%20capabilities%2C%20it%20presents%20decent%0Ageneralization%20to%20languages%20not%20seen%20during%20supervised%20training%2C%20with%20LLaMA3-1B%0Areaching%20a%20mean%20macro-averaged%20F1%20score%20of%2081.46%25%20in%20cross-validation%20with%20nine%0Alanguages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.03940v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520all%2520tokens%2520are%2520created%2520equal%253A%2520Perplexity%2520Attention%2520Weighted%2520Networks%250A%2520%2520for%2520AI%2520generated%2520text%2520detection%26entry.906535625%3DPablo%2520Miralles-Gonz%25C3%25A1lez%2520and%2520Javier%2520Huertas-Tato%2520and%2520Alejandro%2520Mart%25C3%25ADn%2520and%2520David%2520Camacho%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520significantly%250Aenhanced%2520their%2520ability%2520to%2520generate%2520coherent%2520and%2520contextually%2520relevant%2520text%252C%250Araising%2520concerns%2520about%2520the%2520misuse%2520of%2520AI-generated%2520content%2520and%2520making%2520it%250Acritical%2520to%2520detect%2520it.%2520However%252C%2520the%2520task%2520remains%2520challenging%252C%2520particularly%2520in%250Aunseen%2520domains%2520or%2520with%2520unfamiliar%2520LLMs.%2520Leveraging%2520LLM%2520next-token%2520distribution%250Aoutputs%2520offers%2520a%2520theoretically%2520appealing%2520approach%2520for%2520detection%252C%2520as%2520they%250Aencapsulate%2520insights%2520from%2520the%2520models%2527%2520extensive%2520pre-training%2520on%2520diverse%250Acorpora.%2520Despite%2520its%2520promise%252C%2520zero-shot%2520methods%2520that%2520attempt%2520to%2520operationalize%250Athese%2520outputs%2520have%2520met%2520with%2520limited%2520success.%2520We%2520hypothesize%2520that%2520one%2520of%2520the%250Aproblems%2520is%2520that%2520they%2520use%2520the%2520mean%2520to%2520aggregate%2520next-token%2520distribution%2520metrics%250Aacross%2520tokens%252C%2520when%2520some%2520tokens%2520are%2520naturally%2520easier%2520or%2520harder%2520to%2520predict%2520and%250Ashould%2520be%2520weighted%2520differently.%2520Based%2520on%2520this%2520idea%252C%2520we%2520propose%2520the%2520Perplexity%250AAttention%2520Weighted%2520Network%2520%2528PAWN%2529%252C%2520which%2520uses%2520the%2520last%2520hidden%2520states%2520of%2520the%2520LLM%250Aand%2520positions%2520to%2520weight%2520the%2520sum%2520of%2520a%2520series%2520of%2520features%2520based%2520on%2520metrics%2520from%250Athe%2520next-token%2520distribution%2520across%2520the%2520sequence%2520length.%2520Although%2520not%2520zero-shot%252C%250Aour%2520method%2520allows%2520us%2520to%2520cache%2520the%2520last%2520hidden%2520states%2520and%2520next-token%250Adistribution%2520metrics%2520on%2520disk%252C%2520greatly%2520reducing%2520the%2520training%2520resource%250Arequirements.%2520PAWN%2520shows%2520competitive%2520and%2520even%2520better%2520performance%250Ain-distribution%2520than%2520the%2520strongest%2520baselines%2520%2528fine-tuned%2520LMs%2529%2520with%2520a%2520fraction%250Aof%2520their%2520trainable%2520parameters.%2520Our%2520model%2520also%2520generalizes%2520better%2520to%2520unseen%250Adomains%2520and%2520source%2520models%252C%2520with%2520smaller%2520variability%2520in%2520the%2520decision%2520boundary%250Aacross%2520distribution%2520shifts.%2520It%2520is%2520also%2520more%2520robust%2520to%2520adversarial%2520attacks%252C%2520and%250Aif%2520the%2520backbone%2520has%2520multilingual%2520capabilities%252C%2520it%2520presents%2520decent%250Ageneralization%2520to%2520languages%2520not%2520seen%2520during%2520supervised%2520training%252C%2520with%2520LLaMA3-1B%250Areaching%2520a%2520mean%2520macro-averaged%2520F1%2520score%2520of%252081.46%2525%2520in%2520cross-validation%2520with%2520nine%250Alanguages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03940v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20all%20tokens%20are%20created%20equal%3A%20Perplexity%20Attention%20Weighted%20Networks%0A%20%20for%20AI%20generated%20text%20detection&entry.906535625=Pablo%20Miralles-Gonz%C3%A1lez%20and%20Javier%20Huertas-Tato%20and%20Alejandro%20Mart%C3%ADn%20and%20David%20Camacho&entry.1292438233=%20%20The%20rapid%20advancement%20in%20large%20language%20models%20%28LLMs%29%20has%20significantly%0Aenhanced%20their%20ability%20to%20generate%20coherent%20and%20contextually%20relevant%20text%2C%0Araising%20concerns%20about%20the%20misuse%20of%20AI-generated%20content%20and%20making%20it%0Acritical%20to%20detect%20it.%20However%2C%20the%20task%20remains%20challenging%2C%20particularly%20in%0Aunseen%20domains%20or%20with%20unfamiliar%20LLMs.%20Leveraging%20LLM%20next-token%20distribution%0Aoutputs%20offers%20a%20theoretically%20appealing%20approach%20for%20detection%2C%20as%20they%0Aencapsulate%20insights%20from%20the%20models%27%20extensive%20pre-training%20on%20diverse%0Acorpora.%20Despite%20its%20promise%2C%20zero-shot%20methods%20that%20attempt%20to%20operationalize%0Athese%20outputs%20have%20met%20with%20limited%20success.%20We%20hypothesize%20that%20one%20of%20the%0Aproblems%20is%20that%20they%20use%20the%20mean%20to%20aggregate%20next-token%20distribution%20metrics%0Aacross%20tokens%2C%20when%20some%20tokens%20are%20naturally%20easier%20or%20harder%20to%20predict%20and%0Ashould%20be%20weighted%20differently.%20Based%20on%20this%20idea%2C%20we%20propose%20the%20Perplexity%0AAttention%20Weighted%20Network%20%28PAWN%29%2C%20which%20uses%20the%20last%20hidden%20states%20of%20the%20LLM%0Aand%20positions%20to%20weight%20the%20sum%20of%20a%20series%20of%20features%20based%20on%20metrics%20from%0Athe%20next-token%20distribution%20across%20the%20sequence%20length.%20Although%20not%20zero-shot%2C%0Aour%20method%20allows%20us%20to%20cache%20the%20last%20hidden%20states%20and%20next-token%0Adistribution%20metrics%20on%20disk%2C%20greatly%20reducing%20the%20training%20resource%0Arequirements.%20PAWN%20shows%20competitive%20and%20even%20better%20performance%0Ain-distribution%20than%20the%20strongest%20baselines%20%28fine-tuned%20LMs%29%20with%20a%20fraction%0Aof%20their%20trainable%20parameters.%20Our%20model%20also%20generalizes%20better%20to%20unseen%0Adomains%20and%20source%20models%2C%20with%20smaller%20variability%20in%20the%20decision%20boundary%0Aacross%20distribution%20shifts.%20It%20is%20also%20more%20robust%20to%20adversarial%20attacks%2C%20and%0Aif%20the%20backbone%20has%20multilingual%20capabilities%2C%20it%20presents%20decent%0Ageneralization%20to%20languages%20not%20seen%20during%20supervised%20training%2C%20with%20LLaMA3-1B%0Areaching%20a%20mean%20macro-averaged%20F1%20score%20of%2081.46%25%20in%20cross-validation%20with%20nine%0Alanguages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.03940v2&entry.124074799=Read"},
{"title": "LiT: Delving into a Simplified Linear Diffusion Transformer for Image\n  Generation", "author": "Jiahao Wang and Ning Kang and Lewei Yao and Mengzhao Chen and Chengyue Wu and Songyang Zhang and Shuchen Xue and Yong Liu and Taiqiang Wu and Xihui Liu and Kaipeng Zhang and Shifeng Zhang and Wenqi Shao and Zhenguo Li and Ping Luo", "abstract": "  In commonly used sub-quadratic complexity modules, linear attention benefits\nfrom simplicity and high parallelism, making it promising for image synthesis\ntasks. However, the architectural design and learning strategy for linear\nattention remain underexplored in this field. In this paper, we offer a suite\nof ready-to-use solutions for efficient linear diffusion Transformers. Our core\ncontributions include: (1) Simplified Linear Attention using few heads,\nobserving the free-lunch effect of performance without latency increase. (2)\nWeight inheritance from a fully pre-trained diffusion Transformer: initializing\nlinear Transformer using pre-trained diffusion Transformer and loading all\nparameters except for those related to linear attention. (3) Hybrid knowledge\ndistillation objective: using a pre-trained diffusion Transformer to help the\ntraining of the student linear Transformer, supervising not only the predicted\nnoise but also the variance of the reverse diffusion process. These guidelines\nlead to our proposed Linear Diffusion Transformer (LiT), an efficient\ntext-to-image Transformer that can be deployed offline on a laptop. Experiments\nshow that in class-conditional 256*256 and 512*512 ImageNet benchmark LiT\nachieves highly competitive FID while reducing training steps by 80% and 77%\ncompared to DiT. LiT also rivals methods based on Mamba or Gated Linear\nAttention. Besides, for text-to-image generation, LiT allows for the rapid\nsynthesis of up to 1K resolution photorealistic images. Project page:\nhttps://techmonsterwang.github.io/LiT/.\n", "link": "http://arxiv.org/abs/2501.12976v1", "date": "2025-01-22", "relevancy": 1.9944, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6981}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6575}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiT%3A%20Delving%20into%20a%20Simplified%20Linear%20Diffusion%20Transformer%20for%20Image%0A%20%20Generation&body=Title%3A%20LiT%3A%20Delving%20into%20a%20Simplified%20Linear%20Diffusion%20Transformer%20for%20Image%0A%20%20Generation%0AAuthor%3A%20Jiahao%20Wang%20and%20Ning%20Kang%20and%20Lewei%20Yao%20and%20Mengzhao%20Chen%20and%20Chengyue%20Wu%20and%20Songyang%20Zhang%20and%20Shuchen%20Xue%20and%20Yong%20Liu%20and%20Taiqiang%20Wu%20and%20Xihui%20Liu%20and%20Kaipeng%20Zhang%20and%20Shifeng%20Zhang%20and%20Wenqi%20Shao%20and%20Zhenguo%20Li%20and%20Ping%20Luo%0AAbstract%3A%20%20%20In%20commonly%20used%20sub-quadratic%20complexity%20modules%2C%20linear%20attention%20benefits%0Afrom%20simplicity%20and%20high%20parallelism%2C%20making%20it%20promising%20for%20image%20synthesis%0Atasks.%20However%2C%20the%20architectural%20design%20and%20learning%20strategy%20for%20linear%0Aattention%20remain%20underexplored%20in%20this%20field.%20In%20this%20paper%2C%20we%20offer%20a%20suite%0Aof%20ready-to-use%20solutions%20for%20efficient%20linear%20diffusion%20Transformers.%20Our%20core%0Acontributions%20include%3A%20%281%29%20Simplified%20Linear%20Attention%20using%20few%20heads%2C%0Aobserving%20the%20free-lunch%20effect%20of%20performance%20without%20latency%20increase.%20%282%29%0AWeight%20inheritance%20from%20a%20fully%20pre-trained%20diffusion%20Transformer%3A%20initializing%0Alinear%20Transformer%20using%20pre-trained%20diffusion%20Transformer%20and%20loading%20all%0Aparameters%20except%20for%20those%20related%20to%20linear%20attention.%20%283%29%20Hybrid%20knowledge%0Adistillation%20objective%3A%20using%20a%20pre-trained%20diffusion%20Transformer%20to%20help%20the%0Atraining%20of%20the%20student%20linear%20Transformer%2C%20supervising%20not%20only%20the%20predicted%0Anoise%20but%20also%20the%20variance%20of%20the%20reverse%20diffusion%20process.%20These%20guidelines%0Alead%20to%20our%20proposed%20Linear%20Diffusion%20Transformer%20%28LiT%29%2C%20an%20efficient%0Atext-to-image%20Transformer%20that%20can%20be%20deployed%20offline%20on%20a%20laptop.%20Experiments%0Ashow%20that%20in%20class-conditional%20256%2A256%20and%20512%2A512%20ImageNet%20benchmark%20LiT%0Aachieves%20highly%20competitive%20FID%20while%20reducing%20training%20steps%20by%2080%25%20and%2077%25%0Acompared%20to%20DiT.%20LiT%20also%20rivals%20methods%20based%20on%20Mamba%20or%20Gated%20Linear%0AAttention.%20Besides%2C%20for%20text-to-image%20generation%2C%20LiT%20allows%20for%20the%20rapid%0Asynthesis%20of%20up%20to%201K%20resolution%20photorealistic%20images.%20Project%20page%3A%0Ahttps%3A//techmonsterwang.github.io/LiT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiT%253A%2520Delving%2520into%2520a%2520Simplified%2520Linear%2520Diffusion%2520Transformer%2520for%2520Image%250A%2520%2520Generation%26entry.906535625%3DJiahao%2520Wang%2520and%2520Ning%2520Kang%2520and%2520Lewei%2520Yao%2520and%2520Mengzhao%2520Chen%2520and%2520Chengyue%2520Wu%2520and%2520Songyang%2520Zhang%2520and%2520Shuchen%2520Xue%2520and%2520Yong%2520Liu%2520and%2520Taiqiang%2520Wu%2520and%2520Xihui%2520Liu%2520and%2520Kaipeng%2520Zhang%2520and%2520Shifeng%2520Zhang%2520and%2520Wenqi%2520Shao%2520and%2520Zhenguo%2520Li%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520In%2520commonly%2520used%2520sub-quadratic%2520complexity%2520modules%252C%2520linear%2520attention%2520benefits%250Afrom%2520simplicity%2520and%2520high%2520parallelism%252C%2520making%2520it%2520promising%2520for%2520image%2520synthesis%250Atasks.%2520However%252C%2520the%2520architectural%2520design%2520and%2520learning%2520strategy%2520for%2520linear%250Aattention%2520remain%2520underexplored%2520in%2520this%2520field.%2520In%2520this%2520paper%252C%2520we%2520offer%2520a%2520suite%250Aof%2520ready-to-use%2520solutions%2520for%2520efficient%2520linear%2520diffusion%2520Transformers.%2520Our%2520core%250Acontributions%2520include%253A%2520%25281%2529%2520Simplified%2520Linear%2520Attention%2520using%2520few%2520heads%252C%250Aobserving%2520the%2520free-lunch%2520effect%2520of%2520performance%2520without%2520latency%2520increase.%2520%25282%2529%250AWeight%2520inheritance%2520from%2520a%2520fully%2520pre-trained%2520diffusion%2520Transformer%253A%2520initializing%250Alinear%2520Transformer%2520using%2520pre-trained%2520diffusion%2520Transformer%2520and%2520loading%2520all%250Aparameters%2520except%2520for%2520those%2520related%2520to%2520linear%2520attention.%2520%25283%2529%2520Hybrid%2520knowledge%250Adistillation%2520objective%253A%2520using%2520a%2520pre-trained%2520diffusion%2520Transformer%2520to%2520help%2520the%250Atraining%2520of%2520the%2520student%2520linear%2520Transformer%252C%2520supervising%2520not%2520only%2520the%2520predicted%250Anoise%2520but%2520also%2520the%2520variance%2520of%2520the%2520reverse%2520diffusion%2520process.%2520These%2520guidelines%250Alead%2520to%2520our%2520proposed%2520Linear%2520Diffusion%2520Transformer%2520%2528LiT%2529%252C%2520an%2520efficient%250Atext-to-image%2520Transformer%2520that%2520can%2520be%2520deployed%2520offline%2520on%2520a%2520laptop.%2520Experiments%250Ashow%2520that%2520in%2520class-conditional%2520256%252A256%2520and%2520512%252A512%2520ImageNet%2520benchmark%2520LiT%250Aachieves%2520highly%2520competitive%2520FID%2520while%2520reducing%2520training%2520steps%2520by%252080%2525%2520and%252077%2525%250Acompared%2520to%2520DiT.%2520LiT%2520also%2520rivals%2520methods%2520based%2520on%2520Mamba%2520or%2520Gated%2520Linear%250AAttention.%2520Besides%252C%2520for%2520text-to-image%2520generation%252C%2520LiT%2520allows%2520for%2520the%2520rapid%250Asynthesis%2520of%2520up%2520to%25201K%2520resolution%2520photorealistic%2520images.%2520Project%2520page%253A%250Ahttps%253A//techmonsterwang.github.io/LiT/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiT%3A%20Delving%20into%20a%20Simplified%20Linear%20Diffusion%20Transformer%20for%20Image%0A%20%20Generation&entry.906535625=Jiahao%20Wang%20and%20Ning%20Kang%20and%20Lewei%20Yao%20and%20Mengzhao%20Chen%20and%20Chengyue%20Wu%20and%20Songyang%20Zhang%20and%20Shuchen%20Xue%20and%20Yong%20Liu%20and%20Taiqiang%20Wu%20and%20Xihui%20Liu%20and%20Kaipeng%20Zhang%20and%20Shifeng%20Zhang%20and%20Wenqi%20Shao%20and%20Zhenguo%20Li%20and%20Ping%20Luo&entry.1292438233=%20%20In%20commonly%20used%20sub-quadratic%20complexity%20modules%2C%20linear%20attention%20benefits%0Afrom%20simplicity%20and%20high%20parallelism%2C%20making%20it%20promising%20for%20image%20synthesis%0Atasks.%20However%2C%20the%20architectural%20design%20and%20learning%20strategy%20for%20linear%0Aattention%20remain%20underexplored%20in%20this%20field.%20In%20this%20paper%2C%20we%20offer%20a%20suite%0Aof%20ready-to-use%20solutions%20for%20efficient%20linear%20diffusion%20Transformers.%20Our%20core%0Acontributions%20include%3A%20%281%29%20Simplified%20Linear%20Attention%20using%20few%20heads%2C%0Aobserving%20the%20free-lunch%20effect%20of%20performance%20without%20latency%20increase.%20%282%29%0AWeight%20inheritance%20from%20a%20fully%20pre-trained%20diffusion%20Transformer%3A%20initializing%0Alinear%20Transformer%20using%20pre-trained%20diffusion%20Transformer%20and%20loading%20all%0Aparameters%20except%20for%20those%20related%20to%20linear%20attention.%20%283%29%20Hybrid%20knowledge%0Adistillation%20objective%3A%20using%20a%20pre-trained%20diffusion%20Transformer%20to%20help%20the%0Atraining%20of%20the%20student%20linear%20Transformer%2C%20supervising%20not%20only%20the%20predicted%0Anoise%20but%20also%20the%20variance%20of%20the%20reverse%20diffusion%20process.%20These%20guidelines%0Alead%20to%20our%20proposed%20Linear%20Diffusion%20Transformer%20%28LiT%29%2C%20an%20efficient%0Atext-to-image%20Transformer%20that%20can%20be%20deployed%20offline%20on%20a%20laptop.%20Experiments%0Ashow%20that%20in%20class-conditional%20256%2A256%20and%20512%2A512%20ImageNet%20benchmark%20LiT%0Aachieves%20highly%20competitive%20FID%20while%20reducing%20training%20steps%20by%2080%25%20and%2077%25%0Acompared%20to%20DiT.%20LiT%20also%20rivals%20methods%20based%20on%20Mamba%20or%20Gated%20Linear%0AAttention.%20Besides%2C%20for%20text-to-image%20generation%2C%20LiT%20allows%20for%20the%20rapid%0Asynthesis%20of%20up%20to%201K%20resolution%20photorealistic%20images.%20Project%20page%3A%0Ahttps%3A//techmonsterwang.github.io/LiT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12976v1&entry.124074799=Read"},
{"title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching", "author": "Krishnendu S. Tharakan and Hayssam Dahrouj and Nour Kouzayha and Hesham ElSawy and Tareq Y. Al-Naffouri", "abstract": "  Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.\n", "link": "http://arxiv.org/abs/2501.11745v2", "date": "2025-01-22", "relevancy": 1.9937, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5027}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4974}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20Federated%20Learning%20for%20Cellular%20VR%3A%20Online%20Learning%20and%0A%20%20Dynamic%20Caching&body=Title%3A%20Personalized%20Federated%20Learning%20for%20Cellular%20VR%3A%20Online%20Learning%20and%0A%20%20Dynamic%20Caching%0AAuthor%3A%20Krishnendu%20S.%20Tharakan%20and%20Hayssam%20Dahrouj%20and%20Nour%20Kouzayha%20and%20Hesham%20ElSawy%20and%20Tareq%20Y.%20Al-Naffouri%0AAbstract%3A%20%20%20Delivering%20an%20immersive%20experience%20to%20virtual%20reality%20%28VR%29%20users%20through%0Awireless%20connectivity%20offers%20the%20freedom%20to%20engage%20from%20anywhere%20at%20any%20time.%0ANevertheless%2C%20it%20is%20challenging%20to%20ensure%20seamless%20wireless%20connectivity%20that%0Adelivers%20real-time%20and%20high-quality%20videos%20to%20the%20VR%20users.%20This%20paper%20proposes%0Aa%20field%20of%20view%20%28FoV%29%20aware%20caching%20for%20mobile%20edge%20computing%20%28MEC%29-enabled%0Awireless%20VR%20network.%20In%20particular%2C%20the%20FoV%20of%20each%20VR%20user%20is%0Acached/prefetched%20at%20the%20base%20stations%20%28BSs%29%20based%20on%20the%20caching%20strategies%0Atailored%20to%20each%20BS.%20Specifically%2C%20decentralized%20and%20personalized%20federated%0Alearning%20%28DP-FL%29%20based%20caching%20strategies%20with%20guarantees%20are%20presented.%0AConsidering%20VR%20systems%20composed%20of%20multiple%20VR%20devices%20and%20BSs%2C%20a%20DP-FL%20caching%0Aalgorithm%20is%20implemented%20at%20each%20BS%20to%20personalize%20content%20delivery%20for%20VR%0Ausers.%20The%20utilized%20DP-FL%20algorithm%20guarantees%20a%20probably%20approximately%20correct%0A%28PAC%29%20bound%20on%20the%20conditional%20average%20cache%20hit.%20Further%2C%20to%20reduce%20the%20cost%0Aof%20communicating%20gradients%2C%20one-bit%20quantization%20of%20the%20stochastic%20gradient%0Adescent%20%28OBSGD%29%20is%20proposed%2C%20and%20a%20convergence%20guarantee%20of%0A%24%5Cmathcal%7BO%7D%281/%5Csqrt%7BT%7D%29%24%20is%20obtained%20for%20the%20proposed%20algorithm%2C%20where%20%24T%24%20is%0Athe%20number%20of%20iterations.%20Additionally%2C%20to%20better%20account%20for%20the%20wireless%0Achannel%20dynamics%2C%20the%20FoVs%20are%20grouped%20into%20multicast%20or%20unicast%20groups%20based%0Aon%20the%20number%20of%20requesting%20VR%20users.%20The%20performance%20of%20the%20proposed%20DP-FL%0Aalgorithm%20is%20validated%20through%20realistic%20VR%20head-tracking%20dataset%2C%20and%20the%0Aproposed%20algorithm%20is%20shown%20to%20have%20better%20performance%20in%20terms%20of%20average%0Adelay%20and%20cache%20hit%20as%20compared%20to%20baseline%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520Federated%2520Learning%2520for%2520Cellular%2520VR%253A%2520Online%2520Learning%2520and%250A%2520%2520Dynamic%2520Caching%26entry.906535625%3DKrishnendu%2520S.%2520Tharakan%2520and%2520Hayssam%2520Dahrouj%2520and%2520Nour%2520Kouzayha%2520and%2520Hesham%2520ElSawy%2520and%2520Tareq%2520Y.%2520Al-Naffouri%26entry.1292438233%3D%2520%2520Delivering%2520an%2520immersive%2520experience%2520to%2520virtual%2520reality%2520%2528VR%2529%2520users%2520through%250Awireless%2520connectivity%2520offers%2520the%2520freedom%2520to%2520engage%2520from%2520anywhere%2520at%2520any%2520time.%250ANevertheless%252C%2520it%2520is%2520challenging%2520to%2520ensure%2520seamless%2520wireless%2520connectivity%2520that%250Adelivers%2520real-time%2520and%2520high-quality%2520videos%2520to%2520the%2520VR%2520users.%2520This%2520paper%2520proposes%250Aa%2520field%2520of%2520view%2520%2528FoV%2529%2520aware%2520caching%2520for%2520mobile%2520edge%2520computing%2520%2528MEC%2529-enabled%250Awireless%2520VR%2520network.%2520In%2520particular%252C%2520the%2520FoV%2520of%2520each%2520VR%2520user%2520is%250Acached/prefetched%2520at%2520the%2520base%2520stations%2520%2528BSs%2529%2520based%2520on%2520the%2520caching%2520strategies%250Atailored%2520to%2520each%2520BS.%2520Specifically%252C%2520decentralized%2520and%2520personalized%2520federated%250Alearning%2520%2528DP-FL%2529%2520based%2520caching%2520strategies%2520with%2520guarantees%2520are%2520presented.%250AConsidering%2520VR%2520systems%2520composed%2520of%2520multiple%2520VR%2520devices%2520and%2520BSs%252C%2520a%2520DP-FL%2520caching%250Aalgorithm%2520is%2520implemented%2520at%2520each%2520BS%2520to%2520personalize%2520content%2520delivery%2520for%2520VR%250Ausers.%2520The%2520utilized%2520DP-FL%2520algorithm%2520guarantees%2520a%2520probably%2520approximately%2520correct%250A%2528PAC%2529%2520bound%2520on%2520the%2520conditional%2520average%2520cache%2520hit.%2520Further%252C%2520to%2520reduce%2520the%2520cost%250Aof%2520communicating%2520gradients%252C%2520one-bit%2520quantization%2520of%2520the%2520stochastic%2520gradient%250Adescent%2520%2528OBSGD%2529%2520is%2520proposed%252C%2520and%2520a%2520convergence%2520guarantee%2520of%250A%2524%255Cmathcal%257BO%257D%25281/%255Csqrt%257BT%257D%2529%2524%2520is%2520obtained%2520for%2520the%2520proposed%2520algorithm%252C%2520where%2520%2524T%2524%2520is%250Athe%2520number%2520of%2520iterations.%2520Additionally%252C%2520to%2520better%2520account%2520for%2520the%2520wireless%250Achannel%2520dynamics%252C%2520the%2520FoVs%2520are%2520grouped%2520into%2520multicast%2520or%2520unicast%2520groups%2520based%250Aon%2520the%2520number%2520of%2520requesting%2520VR%2520users.%2520The%2520performance%2520of%2520the%2520proposed%2520DP-FL%250Aalgorithm%2520is%2520validated%2520through%2520realistic%2520VR%2520head-tracking%2520dataset%252C%2520and%2520the%250Aproposed%2520algorithm%2520is%2520shown%2520to%2520have%2520better%2520performance%2520in%2520terms%2520of%2520average%250Adelay%2520and%2520cache%2520hit%2520as%2520compared%2520to%2520baseline%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20Federated%20Learning%20for%20Cellular%20VR%3A%20Online%20Learning%20and%0A%20%20Dynamic%20Caching&entry.906535625=Krishnendu%20S.%20Tharakan%20and%20Hayssam%20Dahrouj%20and%20Nour%20Kouzayha%20and%20Hesham%20ElSawy%20and%20Tareq%20Y.%20Al-Naffouri&entry.1292438233=%20%20Delivering%20an%20immersive%20experience%20to%20virtual%20reality%20%28VR%29%20users%20through%0Awireless%20connectivity%20offers%20the%20freedom%20to%20engage%20from%20anywhere%20at%20any%20time.%0ANevertheless%2C%20it%20is%20challenging%20to%20ensure%20seamless%20wireless%20connectivity%20that%0Adelivers%20real-time%20and%20high-quality%20videos%20to%20the%20VR%20users.%20This%20paper%20proposes%0Aa%20field%20of%20view%20%28FoV%29%20aware%20caching%20for%20mobile%20edge%20computing%20%28MEC%29-enabled%0Awireless%20VR%20network.%20In%20particular%2C%20the%20FoV%20of%20each%20VR%20user%20is%0Acached/prefetched%20at%20the%20base%20stations%20%28BSs%29%20based%20on%20the%20caching%20strategies%0Atailored%20to%20each%20BS.%20Specifically%2C%20decentralized%20and%20personalized%20federated%0Alearning%20%28DP-FL%29%20based%20caching%20strategies%20with%20guarantees%20are%20presented.%0AConsidering%20VR%20systems%20composed%20of%20multiple%20VR%20devices%20and%20BSs%2C%20a%20DP-FL%20caching%0Aalgorithm%20is%20implemented%20at%20each%20BS%20to%20personalize%20content%20delivery%20for%20VR%0Ausers.%20The%20utilized%20DP-FL%20algorithm%20guarantees%20a%20probably%20approximately%20correct%0A%28PAC%29%20bound%20on%20the%20conditional%20average%20cache%20hit.%20Further%2C%20to%20reduce%20the%20cost%0Aof%20communicating%20gradients%2C%20one-bit%20quantization%20of%20the%20stochastic%20gradient%0Adescent%20%28OBSGD%29%20is%20proposed%2C%20and%20a%20convergence%20guarantee%20of%0A%24%5Cmathcal%7BO%7D%281/%5Csqrt%7BT%7D%29%24%20is%20obtained%20for%20the%20proposed%20algorithm%2C%20where%20%24T%24%20is%0Athe%20number%20of%20iterations.%20Additionally%2C%20to%20better%20account%20for%20the%20wireless%0Achannel%20dynamics%2C%20the%20FoVs%20are%20grouped%20into%20multicast%20or%20unicast%20groups%20based%0Aon%20the%20number%20of%20requesting%20VR%20users.%20The%20performance%20of%20the%20proposed%20DP-FL%0Aalgorithm%20is%20validated%20through%20realistic%20VR%20head-tracking%20dataset%2C%20and%20the%0Aproposed%20algorithm%20is%20shown%20to%20have%20better%20performance%20in%20terms%20of%20average%0Adelay%20and%20cache%20hit%20as%20compared%20to%20baseline%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11745v2&entry.124074799=Read"},
{"title": "A Probabilistic Model for Self-Supervised Learning", "author": "Maximilian Fleissner and Pascal Esser and Debarghya Ghoshdastidar", "abstract": "  Self-supervised learning (SSL) aims to find meaningful representations from\nunlabeled data by encoding semantic similarities through data augmentations.\nDespite its current popularity, theoretical insights about SSL are still\nscarce. For example, it is not yet known whether commonly used SSL loss\nfunctions can be related to a statistical model, much in the same as OLS,\ngeneralized linear models or PCA naturally emerge as maximum likelihood\nestimates of an underlying generative process. In this short paper, we consider\na latent variable statistical model for SSL that exhibits an interesting\nproperty: Depending on the informativeness of the data augmentations, the MLE\nof the model either reduces to PCA, or approaches a simple non-contrastive\nloss. We analyze the model and also empirically illustrate our findings.\n", "link": "http://arxiv.org/abs/2501.13031v1", "date": "2025-01-22", "relevancy": 1.9882, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4865}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Probabilistic%20Model%20for%20Self-Supervised%20Learning&body=Title%3A%20A%20Probabilistic%20Model%20for%20Self-Supervised%20Learning%0AAuthor%3A%20Maximilian%20Fleissner%20and%20Pascal%20Esser%20and%20Debarghya%20Ghoshdastidar%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20aims%20to%20find%20meaningful%20representations%20from%0Aunlabeled%20data%20by%20encoding%20semantic%20similarities%20through%20data%20augmentations.%0ADespite%20its%20current%20popularity%2C%20theoretical%20insights%20about%20SSL%20are%20still%0Ascarce.%20For%20example%2C%20it%20is%20not%20yet%20known%20whether%20commonly%20used%20SSL%20loss%0Afunctions%20can%20be%20related%20to%20a%20statistical%20model%2C%20much%20in%20the%20same%20as%20OLS%2C%0Ageneralized%20linear%20models%20or%20PCA%20naturally%20emerge%20as%20maximum%20likelihood%0Aestimates%20of%20an%20underlying%20generative%20process.%20In%20this%20short%20paper%2C%20we%20consider%0Aa%20latent%20variable%20statistical%20model%20for%20SSL%20that%20exhibits%20an%20interesting%0Aproperty%3A%20Depending%20on%20the%20informativeness%20of%20the%20data%20augmentations%2C%20the%20MLE%0Aof%20the%20model%20either%20reduces%20to%20PCA%2C%20or%20approaches%20a%20simple%20non-contrastive%0Aloss.%20We%20analyze%20the%20model%20and%20also%20empirically%20illustrate%20our%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Probabilistic%2520Model%2520for%2520Self-Supervised%2520Learning%26entry.906535625%3DMaximilian%2520Fleissner%2520and%2520Pascal%2520Esser%2520and%2520Debarghya%2520Ghoshdastidar%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520aims%2520to%2520find%2520meaningful%2520representations%2520from%250Aunlabeled%2520data%2520by%2520encoding%2520semantic%2520similarities%2520through%2520data%2520augmentations.%250ADespite%2520its%2520current%2520popularity%252C%2520theoretical%2520insights%2520about%2520SSL%2520are%2520still%250Ascarce.%2520For%2520example%252C%2520it%2520is%2520not%2520yet%2520known%2520whether%2520commonly%2520used%2520SSL%2520loss%250Afunctions%2520can%2520be%2520related%2520to%2520a%2520statistical%2520model%252C%2520much%2520in%2520the%2520same%2520as%2520OLS%252C%250Ageneralized%2520linear%2520models%2520or%2520PCA%2520naturally%2520emerge%2520as%2520maximum%2520likelihood%250Aestimates%2520of%2520an%2520underlying%2520generative%2520process.%2520In%2520this%2520short%2520paper%252C%2520we%2520consider%250Aa%2520latent%2520variable%2520statistical%2520model%2520for%2520SSL%2520that%2520exhibits%2520an%2520interesting%250Aproperty%253A%2520Depending%2520on%2520the%2520informativeness%2520of%2520the%2520data%2520augmentations%252C%2520the%2520MLE%250Aof%2520the%2520model%2520either%2520reduces%2520to%2520PCA%252C%2520or%2520approaches%2520a%2520simple%2520non-contrastive%250Aloss.%2520We%2520analyze%2520the%2520model%2520and%2520also%2520empirically%2520illustrate%2520our%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Probabilistic%20Model%20for%20Self-Supervised%20Learning&entry.906535625=Maximilian%20Fleissner%20and%20Pascal%20Esser%20and%20Debarghya%20Ghoshdastidar&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20aims%20to%20find%20meaningful%20representations%20from%0Aunlabeled%20data%20by%20encoding%20semantic%20similarities%20through%20data%20augmentations.%0ADespite%20its%20current%20popularity%2C%20theoretical%20insights%20about%20SSL%20are%20still%0Ascarce.%20For%20example%2C%20it%20is%20not%20yet%20known%20whether%20commonly%20used%20SSL%20loss%0Afunctions%20can%20be%20related%20to%20a%20statistical%20model%2C%20much%20in%20the%20same%20as%20OLS%2C%0Ageneralized%20linear%20models%20or%20PCA%20naturally%20emerge%20as%20maximum%20likelihood%0Aestimates%20of%20an%20underlying%20generative%20process.%20In%20this%20short%20paper%2C%20we%20consider%0Aa%20latent%20variable%20statistical%20model%20for%20SSL%20that%20exhibits%20an%20interesting%0Aproperty%3A%20Depending%20on%20the%20informativeness%20of%20the%20data%20augmentations%2C%20the%20MLE%0Aof%20the%20model%20either%20reduces%20to%20PCA%2C%20or%20approaches%20a%20simple%20non-contrastive%0Aloss.%20We%20analyze%20the%20model%20and%20also%20empirically%20illustrate%20our%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13031v1&entry.124074799=Read"},
{"title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant", "author": "Armand Nicolicioiu and Eugenia Iofinova and Eldar Kurtic and Mahdi Nikdan and Andrei Panferov and Ilia Markov and Nir Shavit and Dan Alistarh", "abstract": "  The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We are releasing the full Panza code as well\nas a new \"David\" personalized email dataset licensed for research use, both\navailable on https://github.com/IST-DASLab/PanzaMail.\n", "link": "http://arxiv.org/abs/2407.10994v2", "date": "2025-01-22", "relevancy": 1.9717, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5037}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4892}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panza%3A%20Design%20and%20Analysis%20of%20a%20Fully-Local%20Personalized%20Text%20Writing%0A%20%20Assistant&body=Title%3A%20Panza%3A%20Design%20and%20Analysis%20of%20a%20Fully-Local%20Personalized%20Text%20Writing%0A%20%20Assistant%0AAuthor%3A%20Armand%20Nicolicioiu%20and%20Eugenia%20Iofinova%20and%20Eldar%20Kurtic%20and%20Mahdi%20Nikdan%20and%20Andrei%20Panferov%20and%20Ilia%20Markov%20and%20Nir%20Shavit%20and%20Dan%20Alistarh%0AAbstract%3A%20%20%20The%20availability%20of%20powerful%20open-source%20large%20language%20models%20%28LLMs%29%20opens%0Aexciting%20use%20cases%2C%20such%20as%20automated%20personal%20assistants%20that%20adapt%20to%20the%0Auser%27s%20unique%20data%20and%20demands.%20Two%20key%20requirements%20for%20such%20assistants%20are%0Apersonalization%20-%20in%20the%20sense%20that%20the%20assistant%20should%20reflect%20the%20user%27s%20own%0Awriting%20style%20-%20and%20privacy%20-%20users%20may%20prefer%20to%20always%20store%20their%20personal%0Adata%20locally%2C%20on%20their%20own%20computing%20device.%20In%20this%20application%20paper%2C%20we%0Apresent%20a%20new%20design%20and%20evaluation%20for%20such%20an%20automated%20assistant%2C%20for%20the%0Aspecific%20use%20case%20of%20email%20generation%2C%20which%20we%20call%20Panza.%20Specifically%2C%20Panza%0Acan%20be%20trained%20and%20deployed%20locally%20on%20commodity%20hardware%2C%20and%20is%20personalized%0Ato%20the%20user%27s%20writing%20style.%20Panza%27s%20personalization%20features%20are%20based%20on%20a%0Acombination%20of%20fine-tuning%20using%20a%20variant%20of%20the%20Reverse%20Instructions%0Atechnique%20together%20with%20Retrieval-Augmented%20Generation%20%28RAG%29.%20We%20demonstrate%0Athat%20this%20combination%20allows%20us%20to%20fine-tune%20an%20LLM%20to%20better%20reflect%20a%20user%27s%0Awriting%20style%20using%20limited%20data%2C%20while%20executing%20on%20extremely%20limited%0Aresources%2C%20e.g.%20on%20a%20free%20Google%20Colab%20instance.%20Our%20key%20methodological%0Acontribution%20is%20what%20we%20believe%20to%20be%20the%20first%20detailed%20study%20of%20evaluation%0Ametrics%20for%20this%20personalized%20writing%20task%2C%20and%20of%20how%20different%20choices%20of%0Asystem%20components%20-%20e.g.%20the%20use%20of%20RAG%20and%20of%20different%20fine-tuning%20approaches%0A-%20impact%20the%20system%27s%20performance.%20We%20are%20releasing%20the%20full%20Panza%20code%20as%20well%0Aas%20a%20new%20%22David%22%20personalized%20email%20dataset%20licensed%20for%20research%20use%2C%20both%0Aavailable%20on%20https%3A//github.com/IST-DASLab/PanzaMail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanza%253A%2520Design%2520and%2520Analysis%2520of%2520a%2520Fully-Local%2520Personalized%2520Text%2520Writing%250A%2520%2520Assistant%26entry.906535625%3DArmand%2520Nicolicioiu%2520and%2520Eugenia%2520Iofinova%2520and%2520Eldar%2520Kurtic%2520and%2520Mahdi%2520Nikdan%2520and%2520Andrei%2520Panferov%2520and%2520Ilia%2520Markov%2520and%2520Nir%2520Shavit%2520and%2520Dan%2520Alistarh%26entry.1292438233%3D%2520%2520The%2520availability%2520of%2520powerful%2520open-source%2520large%2520language%2520models%2520%2528LLMs%2529%2520opens%250Aexciting%2520use%2520cases%252C%2520such%2520as%2520automated%2520personal%2520assistants%2520that%2520adapt%2520to%2520the%250Auser%2527s%2520unique%2520data%2520and%2520demands.%2520Two%2520key%2520requirements%2520for%2520such%2520assistants%2520are%250Apersonalization%2520-%2520in%2520the%2520sense%2520that%2520the%2520assistant%2520should%2520reflect%2520the%2520user%2527s%2520own%250Awriting%2520style%2520-%2520and%2520privacy%2520-%2520users%2520may%2520prefer%2520to%2520always%2520store%2520their%2520personal%250Adata%2520locally%252C%2520on%2520their%2520own%2520computing%2520device.%2520In%2520this%2520application%2520paper%252C%2520we%250Apresent%2520a%2520new%2520design%2520and%2520evaluation%2520for%2520such%2520an%2520automated%2520assistant%252C%2520for%2520the%250Aspecific%2520use%2520case%2520of%2520email%2520generation%252C%2520which%2520we%2520call%2520Panza.%2520Specifically%252C%2520Panza%250Acan%2520be%2520trained%2520and%2520deployed%2520locally%2520on%2520commodity%2520hardware%252C%2520and%2520is%2520personalized%250Ato%2520the%2520user%2527s%2520writing%2520style.%2520Panza%2527s%2520personalization%2520features%2520are%2520based%2520on%2520a%250Acombination%2520of%2520fine-tuning%2520using%2520a%2520variant%2520of%2520the%2520Reverse%2520Instructions%250Atechnique%2520together%2520with%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529.%2520We%2520demonstrate%250Athat%2520this%2520combination%2520allows%2520us%2520to%2520fine-tune%2520an%2520LLM%2520to%2520better%2520reflect%2520a%2520user%2527s%250Awriting%2520style%2520using%2520limited%2520data%252C%2520while%2520executing%2520on%2520extremely%2520limited%250Aresources%252C%2520e.g.%2520on%2520a%2520free%2520Google%2520Colab%2520instance.%2520Our%2520key%2520methodological%250Acontribution%2520is%2520what%2520we%2520believe%2520to%2520be%2520the%2520first%2520detailed%2520study%2520of%2520evaluation%250Ametrics%2520for%2520this%2520personalized%2520writing%2520task%252C%2520and%2520of%2520how%2520different%2520choices%2520of%250Asystem%2520components%2520-%2520e.g.%2520the%2520use%2520of%2520RAG%2520and%2520of%2520different%2520fine-tuning%2520approaches%250A-%2520impact%2520the%2520system%2527s%2520performance.%2520We%2520are%2520releasing%2520the%2520full%2520Panza%2520code%2520as%2520well%250Aas%2520a%2520new%2520%2522David%2522%2520personalized%2520email%2520dataset%2520licensed%2520for%2520research%2520use%252C%2520both%250Aavailable%2520on%2520https%253A//github.com/IST-DASLab/PanzaMail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panza%3A%20Design%20and%20Analysis%20of%20a%20Fully-Local%20Personalized%20Text%20Writing%0A%20%20Assistant&entry.906535625=Armand%20Nicolicioiu%20and%20Eugenia%20Iofinova%20and%20Eldar%20Kurtic%20and%20Mahdi%20Nikdan%20and%20Andrei%20Panferov%20and%20Ilia%20Markov%20and%20Nir%20Shavit%20and%20Dan%20Alistarh&entry.1292438233=%20%20The%20availability%20of%20powerful%20open-source%20large%20language%20models%20%28LLMs%29%20opens%0Aexciting%20use%20cases%2C%20such%20as%20automated%20personal%20assistants%20that%20adapt%20to%20the%0Auser%27s%20unique%20data%20and%20demands.%20Two%20key%20requirements%20for%20such%20assistants%20are%0Apersonalization%20-%20in%20the%20sense%20that%20the%20assistant%20should%20reflect%20the%20user%27s%20own%0Awriting%20style%20-%20and%20privacy%20-%20users%20may%20prefer%20to%20always%20store%20their%20personal%0Adata%20locally%2C%20on%20their%20own%20computing%20device.%20In%20this%20application%20paper%2C%20we%0Apresent%20a%20new%20design%20and%20evaluation%20for%20such%20an%20automated%20assistant%2C%20for%20the%0Aspecific%20use%20case%20of%20email%20generation%2C%20which%20we%20call%20Panza.%20Specifically%2C%20Panza%0Acan%20be%20trained%20and%20deployed%20locally%20on%20commodity%20hardware%2C%20and%20is%20personalized%0Ato%20the%20user%27s%20writing%20style.%20Panza%27s%20personalization%20features%20are%20based%20on%20a%0Acombination%20of%20fine-tuning%20using%20a%20variant%20of%20the%20Reverse%20Instructions%0Atechnique%20together%20with%20Retrieval-Augmented%20Generation%20%28RAG%29.%20We%20demonstrate%0Athat%20this%20combination%20allows%20us%20to%20fine-tune%20an%20LLM%20to%20better%20reflect%20a%20user%27s%0Awriting%20style%20using%20limited%20data%2C%20while%20executing%20on%20extremely%20limited%0Aresources%2C%20e.g.%20on%20a%20free%20Google%20Colab%20instance.%20Our%20key%20methodological%0Acontribution%20is%20what%20we%20believe%20to%20be%20the%20first%20detailed%20study%20of%20evaluation%0Ametrics%20for%20this%20personalized%20writing%20task%2C%20and%20of%20how%20different%20choices%20of%0Asystem%20components%20-%20e.g.%20the%20use%20of%20RAG%20and%20of%20different%20fine-tuning%20approaches%0A-%20impact%20the%20system%27s%20performance.%20We%20are%20releasing%20the%20full%20Panza%20code%20as%20well%0Aas%20a%20new%20%22David%22%20personalized%20email%20dataset%20licensed%20for%20research%20use%2C%20both%0Aavailable%20on%20https%3A//github.com/IST-DASLab/PanzaMail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10994v2&entry.124074799=Read"},
{"title": "AdaWM: Adaptive World Model based Planning for Autonomous Driving", "author": "Hang Wang and Xin Ye and Feng Tao and Abhirup Mallik and Burhaneddin Yaman and Liu Ren and Junshan Zhang", "abstract": "  World model based reinforcement learning (RL) has emerged as a promising\napproach for autonomous driving, which learns a latent dynamics model and uses\nit to train a planning policy. To speed up the learning process, the\npretrain-finetune paradigm is often used, where online RL is initialized by a\npretrained model and a policy learned offline. However, naively performing such\ninitialization in RL may result in dramatic performance degradation during the\nonline interactions in the new task. To tackle this challenge, we first analyze\nthe performance degradation and identify two primary root causes therein: the\nmismatch of the planning policy and the mismatch of the dynamics model, due to\ndistribution shift. We further analyze the effects of these factors on\nperformance degradation during finetuning, and our findings reveal that the\nchoice of finetuning strategies plays a pivotal role in mitigating these\neffects. We then introduce AdaWM, an Adaptive World Model based planning\nmethod, featuring two key steps: (a) mismatch identification, which quantifies\nthe mismatches and informs the finetuning strategy, and (b) alignment-driven\nfinetuning, which selectively updates either the policy or the model as needed\nusing efficient low-rank updates. Extensive experiments on the challenging\nCARLA driving tasks demonstrate that AdaWM significantly improves the\nfinetuning process, resulting in more robust and efficient performance in\nautonomous driving systems.\n", "link": "http://arxiv.org/abs/2501.13072v1", "date": "2025-01-22", "relevancy": 1.9708, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5095}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5007}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaWM%3A%20Adaptive%20World%20Model%20based%20Planning%20for%20Autonomous%20Driving&body=Title%3A%20AdaWM%3A%20Adaptive%20World%20Model%20based%20Planning%20for%20Autonomous%20Driving%0AAuthor%3A%20Hang%20Wang%20and%20Xin%20Ye%20and%20Feng%20Tao%20and%20Abhirup%20Mallik%20and%20Burhaneddin%20Yaman%20and%20Liu%20Ren%20and%20Junshan%20Zhang%0AAbstract%3A%20%20%20World%20model%20based%20reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%0Aapproach%20for%20autonomous%20driving%2C%20which%20learns%20a%20latent%20dynamics%20model%20and%20uses%0Ait%20to%20train%20a%20planning%20policy.%20To%20speed%20up%20the%20learning%20process%2C%20the%0Apretrain-finetune%20paradigm%20is%20often%20used%2C%20where%20online%20RL%20is%20initialized%20by%20a%0Apretrained%20model%20and%20a%20policy%20learned%20offline.%20However%2C%20naively%20performing%20such%0Ainitialization%20in%20RL%20may%20result%20in%20dramatic%20performance%20degradation%20during%20the%0Aonline%20interactions%20in%20the%20new%20task.%20To%20tackle%20this%20challenge%2C%20we%20first%20analyze%0Athe%20performance%20degradation%20and%20identify%20two%20primary%20root%20causes%20therein%3A%20the%0Amismatch%20of%20the%20planning%20policy%20and%20the%20mismatch%20of%20the%20dynamics%20model%2C%20due%20to%0Adistribution%20shift.%20We%20further%20analyze%20the%20effects%20of%20these%20factors%20on%0Aperformance%20degradation%20during%20finetuning%2C%20and%20our%20findings%20reveal%20that%20the%0Achoice%20of%20finetuning%20strategies%20plays%20a%20pivotal%20role%20in%20mitigating%20these%0Aeffects.%20We%20then%20introduce%20AdaWM%2C%20an%20Adaptive%20World%20Model%20based%20planning%0Amethod%2C%20featuring%20two%20key%20steps%3A%20%28a%29%20mismatch%20identification%2C%20which%20quantifies%0Athe%20mismatches%20and%20informs%20the%20finetuning%20strategy%2C%20and%20%28b%29%20alignment-driven%0Afinetuning%2C%20which%20selectively%20updates%20either%20the%20policy%20or%20the%20model%20as%20needed%0Ausing%20efficient%20low-rank%20updates.%20Extensive%20experiments%20on%20the%20challenging%0ACARLA%20driving%20tasks%20demonstrate%20that%20AdaWM%20significantly%20improves%20the%0Afinetuning%20process%2C%20resulting%20in%20more%20robust%20and%20efficient%20performance%20in%0Aautonomous%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaWM%253A%2520Adaptive%2520World%2520Model%2520based%2520Planning%2520for%2520Autonomous%2520Driving%26entry.906535625%3DHang%2520Wang%2520and%2520Xin%2520Ye%2520and%2520Feng%2520Tao%2520and%2520Abhirup%2520Mallik%2520and%2520Burhaneddin%2520Yaman%2520and%2520Liu%2520Ren%2520and%2520Junshan%2520Zhang%26entry.1292438233%3D%2520%2520World%2520model%2520based%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520promising%250Aapproach%2520for%2520autonomous%2520driving%252C%2520which%2520learns%2520a%2520latent%2520dynamics%2520model%2520and%2520uses%250Ait%2520to%2520train%2520a%2520planning%2520policy.%2520To%2520speed%2520up%2520the%2520learning%2520process%252C%2520the%250Apretrain-finetune%2520paradigm%2520is%2520often%2520used%252C%2520where%2520online%2520RL%2520is%2520initialized%2520by%2520a%250Apretrained%2520model%2520and%2520a%2520policy%2520learned%2520offline.%2520However%252C%2520naively%2520performing%2520such%250Ainitialization%2520in%2520RL%2520may%2520result%2520in%2520dramatic%2520performance%2520degradation%2520during%2520the%250Aonline%2520interactions%2520in%2520the%2520new%2520task.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520first%2520analyze%250Athe%2520performance%2520degradation%2520and%2520identify%2520two%2520primary%2520root%2520causes%2520therein%253A%2520the%250Amismatch%2520of%2520the%2520planning%2520policy%2520and%2520the%2520mismatch%2520of%2520the%2520dynamics%2520model%252C%2520due%2520to%250Adistribution%2520shift.%2520We%2520further%2520analyze%2520the%2520effects%2520of%2520these%2520factors%2520on%250Aperformance%2520degradation%2520during%2520finetuning%252C%2520and%2520our%2520findings%2520reveal%2520that%2520the%250Achoice%2520of%2520finetuning%2520strategies%2520plays%2520a%2520pivotal%2520role%2520in%2520mitigating%2520these%250Aeffects.%2520We%2520then%2520introduce%2520AdaWM%252C%2520an%2520Adaptive%2520World%2520Model%2520based%2520planning%250Amethod%252C%2520featuring%2520two%2520key%2520steps%253A%2520%2528a%2529%2520mismatch%2520identification%252C%2520which%2520quantifies%250Athe%2520mismatches%2520and%2520informs%2520the%2520finetuning%2520strategy%252C%2520and%2520%2528b%2529%2520alignment-driven%250Afinetuning%252C%2520which%2520selectively%2520updates%2520either%2520the%2520policy%2520or%2520the%2520model%2520as%2520needed%250Ausing%2520efficient%2520low-rank%2520updates.%2520Extensive%2520experiments%2520on%2520the%2520challenging%250ACARLA%2520driving%2520tasks%2520demonstrate%2520that%2520AdaWM%2520significantly%2520improves%2520the%250Afinetuning%2520process%252C%2520resulting%2520in%2520more%2520robust%2520and%2520efficient%2520performance%2520in%250Aautonomous%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaWM%3A%20Adaptive%20World%20Model%20based%20Planning%20for%20Autonomous%20Driving&entry.906535625=Hang%20Wang%20and%20Xin%20Ye%20and%20Feng%20Tao%20and%20Abhirup%20Mallik%20and%20Burhaneddin%20Yaman%20and%20Liu%20Ren%20and%20Junshan%20Zhang&entry.1292438233=%20%20World%20model%20based%20reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%0Aapproach%20for%20autonomous%20driving%2C%20which%20learns%20a%20latent%20dynamics%20model%20and%20uses%0Ait%20to%20train%20a%20planning%20policy.%20To%20speed%20up%20the%20learning%20process%2C%20the%0Apretrain-finetune%20paradigm%20is%20often%20used%2C%20where%20online%20RL%20is%20initialized%20by%20a%0Apretrained%20model%20and%20a%20policy%20learned%20offline.%20However%2C%20naively%20performing%20such%0Ainitialization%20in%20RL%20may%20result%20in%20dramatic%20performance%20degradation%20during%20the%0Aonline%20interactions%20in%20the%20new%20task.%20To%20tackle%20this%20challenge%2C%20we%20first%20analyze%0Athe%20performance%20degradation%20and%20identify%20two%20primary%20root%20causes%20therein%3A%20the%0Amismatch%20of%20the%20planning%20policy%20and%20the%20mismatch%20of%20the%20dynamics%20model%2C%20due%20to%0Adistribution%20shift.%20We%20further%20analyze%20the%20effects%20of%20these%20factors%20on%0Aperformance%20degradation%20during%20finetuning%2C%20and%20our%20findings%20reveal%20that%20the%0Achoice%20of%20finetuning%20strategies%20plays%20a%20pivotal%20role%20in%20mitigating%20these%0Aeffects.%20We%20then%20introduce%20AdaWM%2C%20an%20Adaptive%20World%20Model%20based%20planning%0Amethod%2C%20featuring%20two%20key%20steps%3A%20%28a%29%20mismatch%20identification%2C%20which%20quantifies%0Athe%20mismatches%20and%20informs%20the%20finetuning%20strategy%2C%20and%20%28b%29%20alignment-driven%0Afinetuning%2C%20which%20selectively%20updates%20either%20the%20policy%20or%20the%20model%20as%20needed%0Ausing%20efficient%20low-rank%20updates.%20Extensive%20experiments%20on%20the%20challenging%0ACARLA%20driving%20tasks%20demonstrate%20that%20AdaWM%20significantly%20improves%20the%0Afinetuning%20process%2C%20resulting%20in%20more%20robust%20and%20efficient%20performance%20in%0Aautonomous%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13072v1&entry.124074799=Read"},
{"title": "Advanced deep architecture pruning using single filter performance", "author": "Yarden Tzach and Yuval Meir and Ronit D. Gross and Ofek Tevet and Ella Koresh and Ido Kanter", "abstract": "  Pruning the parameters and structure of neural networks reduces the\ncomputational complexity, energy consumption, and latency during inference.\nRecently, a novel underlying mechanism for successful deep learning (DL) was\npresented based on a method that quantitatively measures the single filter\nperformance in each layer of a DL architecture, and a new comprehensive\nmechanism of how deep learning works was presented. Herein, we demonstrate how\nthis understanding paves the path to highly dilute the convolutional layers of\ndeep architectures without affecting their overall accuracy using applied\nfilter cluster connections (AFCC). AFCC is exemplified on VGG-11 and\nEfficientNet-B0 architectures trained on CIFAR-100, and its high pruning\noutperforms other techniques using the same pruning magnitude. Additionally,\nthis technique is broadened to single nodal performance and highly pruning of\nfully connected layers, suggesting a possible implementation to considerably\nreduce the complexity of over-parameterized AI tasks.\n", "link": "http://arxiv.org/abs/2501.12880v1", "date": "2025-01-22", "relevancy": 1.9275, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.488}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4828}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20deep%20architecture%20pruning%20using%20single%20filter%20performance&body=Title%3A%20Advanced%20deep%20architecture%20pruning%20using%20single%20filter%20performance%0AAuthor%3A%20Yarden%20Tzach%20and%20Yuval%20Meir%20and%20Ronit%20D.%20Gross%20and%20Ofek%20Tevet%20and%20Ella%20Koresh%20and%20Ido%20Kanter%0AAbstract%3A%20%20%20Pruning%20the%20parameters%20and%20structure%20of%20neural%20networks%20reduces%20the%0Acomputational%20complexity%2C%20energy%20consumption%2C%20and%20latency%20during%20inference.%0ARecently%2C%20a%20novel%20underlying%20mechanism%20for%20successful%20deep%20learning%20%28DL%29%20was%0Apresented%20based%20on%20a%20method%20that%20quantitatively%20measures%20the%20single%20filter%0Aperformance%20in%20each%20layer%20of%20a%20DL%20architecture%2C%20and%20a%20new%20comprehensive%0Amechanism%20of%20how%20deep%20learning%20works%20was%20presented.%20Herein%2C%20we%20demonstrate%20how%0Athis%20understanding%20paves%20the%20path%20to%20highly%20dilute%20the%20convolutional%20layers%20of%0Adeep%20architectures%20without%20affecting%20their%20overall%20accuracy%20using%20applied%0Afilter%20cluster%20connections%20%28AFCC%29.%20AFCC%20is%20exemplified%20on%20VGG-11%20and%0AEfficientNet-B0%20architectures%20trained%20on%20CIFAR-100%2C%20and%20its%20high%20pruning%0Aoutperforms%20other%20techniques%20using%20the%20same%20pruning%20magnitude.%20Additionally%2C%0Athis%20technique%20is%20broadened%20to%20single%20nodal%20performance%20and%20highly%20pruning%20of%0Afully%20connected%20layers%2C%20suggesting%20a%20possible%20implementation%20to%20considerably%0Areduce%20the%20complexity%20of%20over-parameterized%20AI%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12880v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520deep%2520architecture%2520pruning%2520using%2520single%2520filter%2520performance%26entry.906535625%3DYarden%2520Tzach%2520and%2520Yuval%2520Meir%2520and%2520Ronit%2520D.%2520Gross%2520and%2520Ofek%2520Tevet%2520and%2520Ella%2520Koresh%2520and%2520Ido%2520Kanter%26entry.1292438233%3D%2520%2520Pruning%2520the%2520parameters%2520and%2520structure%2520of%2520neural%2520networks%2520reduces%2520the%250Acomputational%2520complexity%252C%2520energy%2520consumption%252C%2520and%2520latency%2520during%2520inference.%250ARecently%252C%2520a%2520novel%2520underlying%2520mechanism%2520for%2520successful%2520deep%2520learning%2520%2528DL%2529%2520was%250Apresented%2520based%2520on%2520a%2520method%2520that%2520quantitatively%2520measures%2520the%2520single%2520filter%250Aperformance%2520in%2520each%2520layer%2520of%2520a%2520DL%2520architecture%252C%2520and%2520a%2520new%2520comprehensive%250Amechanism%2520of%2520how%2520deep%2520learning%2520works%2520was%2520presented.%2520Herein%252C%2520we%2520demonstrate%2520how%250Athis%2520understanding%2520paves%2520the%2520path%2520to%2520highly%2520dilute%2520the%2520convolutional%2520layers%2520of%250Adeep%2520architectures%2520without%2520affecting%2520their%2520overall%2520accuracy%2520using%2520applied%250Afilter%2520cluster%2520connections%2520%2528AFCC%2529.%2520AFCC%2520is%2520exemplified%2520on%2520VGG-11%2520and%250AEfficientNet-B0%2520architectures%2520trained%2520on%2520CIFAR-100%252C%2520and%2520its%2520high%2520pruning%250Aoutperforms%2520other%2520techniques%2520using%2520the%2520same%2520pruning%2520magnitude.%2520Additionally%252C%250Athis%2520technique%2520is%2520broadened%2520to%2520single%2520nodal%2520performance%2520and%2520highly%2520pruning%2520of%250Afully%2520connected%2520layers%252C%2520suggesting%2520a%2520possible%2520implementation%2520to%2520considerably%250Areduce%2520the%2520complexity%2520of%2520over-parameterized%2520AI%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12880v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20deep%20architecture%20pruning%20using%20single%20filter%20performance&entry.906535625=Yarden%20Tzach%20and%20Yuval%20Meir%20and%20Ronit%20D.%20Gross%20and%20Ofek%20Tevet%20and%20Ella%20Koresh%20and%20Ido%20Kanter&entry.1292438233=%20%20Pruning%20the%20parameters%20and%20structure%20of%20neural%20networks%20reduces%20the%0Acomputational%20complexity%2C%20energy%20consumption%2C%20and%20latency%20during%20inference.%0ARecently%2C%20a%20novel%20underlying%20mechanism%20for%20successful%20deep%20learning%20%28DL%29%20was%0Apresented%20based%20on%20a%20method%20that%20quantitatively%20measures%20the%20single%20filter%0Aperformance%20in%20each%20layer%20of%20a%20DL%20architecture%2C%20and%20a%20new%20comprehensive%0Amechanism%20of%20how%20deep%20learning%20works%20was%20presented.%20Herein%2C%20we%20demonstrate%20how%0Athis%20understanding%20paves%20the%20path%20to%20highly%20dilute%20the%20convolutional%20layers%20of%0Adeep%20architectures%20without%20affecting%20their%20overall%20accuracy%20using%20applied%0Afilter%20cluster%20connections%20%28AFCC%29.%20AFCC%20is%20exemplified%20on%20VGG-11%20and%0AEfficientNet-B0%20architectures%20trained%20on%20CIFAR-100%2C%20and%20its%20high%20pruning%0Aoutperforms%20other%20techniques%20using%20the%20same%20pruning%20magnitude.%20Additionally%2C%0Athis%20technique%20is%20broadened%20to%20single%20nodal%20performance%20and%20highly%20pruning%20of%0Afully%20connected%20layers%2C%20suggesting%20a%20possible%20implementation%20to%20considerably%0Areduce%20the%20complexity%20of%20over-parameterized%20AI%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12880v1&entry.124074799=Read"},
{"title": "Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning", "author": "Mohammadreza Nakhaei and Aidan Scannell and Joni Pajarinen", "abstract": "  Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks.\n", "link": "http://arxiv.org/abs/2412.14834v2", "date": "2025-01-22", "relevancy": 1.9153, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4916}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4833}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entropy%20Regularized%20Task%20Representation%20Learning%20for%20Offline%0A%20%20Meta-Reinforcement%20Learning&body=Title%3A%20Entropy%20Regularized%20Task%20Representation%20Learning%20for%20Offline%0A%20%20Meta-Reinforcement%20Learning%0AAuthor%3A%20Mohammadreza%20Nakhaei%20and%20Aidan%20Scannell%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Offline%20meta-reinforcement%20learning%20aims%20to%20equip%20agents%20with%20the%20ability%20to%0Arapidly%20adapt%20to%20new%20tasks%20by%20training%20on%20data%20from%20a%20set%20of%20different%20tasks.%0AContext-based%20approaches%20utilize%20a%20history%20of%20state-action-reward%20transitions%0A--%20referred%20to%20as%20the%20context%20--%20to%20infer%20representations%20of%20the%20current%20task%2C%0Aand%20then%20condition%20the%20agent%2C%20i.e.%2C%20the%20policy%20and%20value%20function%2C%20on%20the%20task%0Arepresentations.%20Intuitively%2C%20the%20better%20the%20task%20representations%20capture%20the%0Aunderlying%20tasks%2C%20the%20better%20the%20agent%20can%20generalize%20to%20new%20tasks.%0AUnfortunately%2C%20context-based%20approaches%20suffer%20from%20distribution%20mismatch%2C%20as%0Athe%20context%20in%20the%20offline%20data%20does%20not%20match%20the%20context%20at%20test%20time%2C%0Alimiting%20their%20ability%20to%20generalize%20to%20the%20test%20tasks.%20This%20leads%20to%20the%20task%0Arepresentations%20overfitting%20to%20the%20offline%20training%20data.%20Intuitively%2C%20the%20task%0Arepresentations%20should%20be%20independent%20of%20the%20behavior%20policy%20used%20to%20collect%0Athe%20offline%20data.%20To%20address%20this%20issue%2C%20we%20approximately%20minimize%20the%20mutual%0Ainformation%20between%20the%20distribution%20over%20the%20task%20representations%20and%20behavior%0Apolicy%20by%20maximizing%20the%20entropy%20of%20behavior%20policy%20conditioned%20on%20the%20task%0Arepresentations.%20We%20validate%20our%20approach%20in%20MuJoCo%20environments%2C%20showing%20that%0Acompared%20to%20baselines%2C%20our%20task%20representations%20more%20faithfully%20represent%20the%0Aunderlying%20tasks%2C%20leading%20to%20outperforming%20prior%20methods%20in%20both%0Ain-distribution%20and%20out-of-distribution%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.14834v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntropy%2520Regularized%2520Task%2520Representation%2520Learning%2520for%2520Offline%250A%2520%2520Meta-Reinforcement%2520Learning%26entry.906535625%3DMohammadreza%2520Nakhaei%2520and%2520Aidan%2520Scannell%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Offline%2520meta-reinforcement%2520learning%2520aims%2520to%2520equip%2520agents%2520with%2520the%2520ability%2520to%250Arapidly%2520adapt%2520to%2520new%2520tasks%2520by%2520training%2520on%2520data%2520from%2520a%2520set%2520of%2520different%2520tasks.%250AContext-based%2520approaches%2520utilize%2520a%2520history%2520of%2520state-action-reward%2520transitions%250A--%2520referred%2520to%2520as%2520the%2520context%2520--%2520to%2520infer%2520representations%2520of%2520the%2520current%2520task%252C%250Aand%2520then%2520condition%2520the%2520agent%252C%2520i.e.%252C%2520the%2520policy%2520and%2520value%2520function%252C%2520on%2520the%2520task%250Arepresentations.%2520Intuitively%252C%2520the%2520better%2520the%2520task%2520representations%2520capture%2520the%250Aunderlying%2520tasks%252C%2520the%2520better%2520the%2520agent%2520can%2520generalize%2520to%2520new%2520tasks.%250AUnfortunately%252C%2520context-based%2520approaches%2520suffer%2520from%2520distribution%2520mismatch%252C%2520as%250Athe%2520context%2520in%2520the%2520offline%2520data%2520does%2520not%2520match%2520the%2520context%2520at%2520test%2520time%252C%250Alimiting%2520their%2520ability%2520to%2520generalize%2520to%2520the%2520test%2520tasks.%2520This%2520leads%2520to%2520the%2520task%250Arepresentations%2520overfitting%2520to%2520the%2520offline%2520training%2520data.%2520Intuitively%252C%2520the%2520task%250Arepresentations%2520should%2520be%2520independent%2520of%2520the%2520behavior%2520policy%2520used%2520to%2520collect%250Athe%2520offline%2520data.%2520To%2520address%2520this%2520issue%252C%2520we%2520approximately%2520minimize%2520the%2520mutual%250Ainformation%2520between%2520the%2520distribution%2520over%2520the%2520task%2520representations%2520and%2520behavior%250Apolicy%2520by%2520maximizing%2520the%2520entropy%2520of%2520behavior%2520policy%2520conditioned%2520on%2520the%2520task%250Arepresentations.%2520We%2520validate%2520our%2520approach%2520in%2520MuJoCo%2520environments%252C%2520showing%2520that%250Acompared%2520to%2520baselines%252C%2520our%2520task%2520representations%2520more%2520faithfully%2520represent%2520the%250Aunderlying%2520tasks%252C%2520leading%2520to%2520outperforming%2520prior%2520methods%2520in%2520both%250Ain-distribution%2520and%2520out-of-distribution%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14834v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entropy%20Regularized%20Task%20Representation%20Learning%20for%20Offline%0A%20%20Meta-Reinforcement%20Learning&entry.906535625=Mohammadreza%20Nakhaei%20and%20Aidan%20Scannell%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Offline%20meta-reinforcement%20learning%20aims%20to%20equip%20agents%20with%20the%20ability%20to%0Arapidly%20adapt%20to%20new%20tasks%20by%20training%20on%20data%20from%20a%20set%20of%20different%20tasks.%0AContext-based%20approaches%20utilize%20a%20history%20of%20state-action-reward%20transitions%0A--%20referred%20to%20as%20the%20context%20--%20to%20infer%20representations%20of%20the%20current%20task%2C%0Aand%20then%20condition%20the%20agent%2C%20i.e.%2C%20the%20policy%20and%20value%20function%2C%20on%20the%20task%0Arepresentations.%20Intuitively%2C%20the%20better%20the%20task%20representations%20capture%20the%0Aunderlying%20tasks%2C%20the%20better%20the%20agent%20can%20generalize%20to%20new%20tasks.%0AUnfortunately%2C%20context-based%20approaches%20suffer%20from%20distribution%20mismatch%2C%20as%0Athe%20context%20in%20the%20offline%20data%20does%20not%20match%20the%20context%20at%20test%20time%2C%0Alimiting%20their%20ability%20to%20generalize%20to%20the%20test%20tasks.%20This%20leads%20to%20the%20task%0Arepresentations%20overfitting%20to%20the%20offline%20training%20data.%20Intuitively%2C%20the%20task%0Arepresentations%20should%20be%20independent%20of%20the%20behavior%20policy%20used%20to%20collect%0Athe%20offline%20data.%20To%20address%20this%20issue%2C%20we%20approximately%20minimize%20the%20mutual%0Ainformation%20between%20the%20distribution%20over%20the%20task%20representations%20and%20behavior%0Apolicy%20by%20maximizing%20the%20entropy%20of%20behavior%20policy%20conditioned%20on%20the%20task%0Arepresentations.%20We%20validate%20our%20approach%20in%20MuJoCo%20environments%2C%20showing%20that%0Acompared%20to%20baselines%2C%20our%20task%20representations%20more%20faithfully%20represent%20the%0Aunderlying%20tasks%2C%20leading%20to%20outperforming%20prior%20methods%20in%20both%0Ain-distribution%20and%20out-of-distribution%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.14834v2&entry.124074799=Read"},
{"title": "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling", "author": "Zhuoran Li and Ruishuo Chen and Hai Zhong and Longbo Huang", "abstract": "  Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.\n", "link": "http://arxiv.org/abs/2501.12942v1", "date": "2025-01-22", "relevancy": 1.9145, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5053}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4817}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Offline%20Critic-Guided%20Diffusion%20Policy%20for%20Multi-User%20Delay-Constrained%0A%20%20Scheduling&body=Title%3A%20Offline%20Critic-Guided%20Diffusion%20Policy%20for%20Multi-User%20Delay-Constrained%0A%20%20Scheduling%0AAuthor%3A%20Zhuoran%20Li%20and%20Ruishuo%20Chen%20and%20Hai%20Zhong%20and%20Longbo%20Huang%0AAbstract%3A%20%20%20Effective%20multi-user%20delay-constrained%20scheduling%20is%20crucial%20in%20various%0Areal-world%20applications%2C%20such%20as%20instant%20messaging%2C%20live%20streaming%2C%20and%20data%0Acenter%20management.%20In%20these%20scenarios%2C%20schedulers%20must%20make%20real-time%20decisions%0Ato%20satisfy%20both%20delay%20and%20resource%20constraints%20without%20prior%20knowledge%20of%0Asystem%20dynamics%2C%20which%20are%20often%20time-varying%20and%20challenging%20to%20estimate.%0ACurrent%20learning-based%20methods%20typically%20require%20interactions%20with%20actual%0Asystems%20during%20the%20training%20stage%2C%20which%20can%20be%20difficult%20or%20impractical%2C%20as%20it%0Ais%20capable%20of%20significantly%20degrading%20system%20performance%20and%20incurring%0Asubstantial%20service%20costs.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aoffline%20reinforcement%20learning-based%20algorithm%2C%20named%20%5Cunderline%7BS%7Dcheduling%20By%0A%5Cunderline%7BO%7Dffline%20Learning%20with%20%5Cunderline%7BC%7Dritic%20Guidance%20and%0A%5Cunderline%7BD%7Diffusion%20Generation%20%28SOCD%29%2C%20to%20learn%20efficient%20scheduling%20policies%0Apurely%20from%20pre-collected%20%5Cemph%7Boffline%20data%7D.%20SOCD%20innovatively%20employs%20a%0Adiffusion-based%20policy%20network%2C%20complemented%20by%20a%20sampling-free%20critic%20network%0Afor%20policy%20guidance.%20By%20integrating%20the%20Lagrangian%20multiplier%20optimization%20into%0Athe%20offline%20reinforcement%20learning%2C%20SOCD%20effectively%20trains%20high-quality%0Aconstraint-aware%20policies%20exclusively%20from%20available%20datasets%2C%20eliminating%20the%0Aneed%20for%20online%20interactions%20with%20the%20system.%20Experimental%20results%20demonstrate%0Athat%20SOCD%20is%20resilient%20to%20various%20system%20dynamics%2C%20including%20partially%0Aobservable%20and%20large-scale%20environments%2C%20and%20delivers%20superior%20performance%0Acompared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOffline%2520Critic-Guided%2520Diffusion%2520Policy%2520for%2520Multi-User%2520Delay-Constrained%250A%2520%2520Scheduling%26entry.906535625%3DZhuoran%2520Li%2520and%2520Ruishuo%2520Chen%2520and%2520Hai%2520Zhong%2520and%2520Longbo%2520Huang%26entry.1292438233%3D%2520%2520Effective%2520multi-user%2520delay-constrained%2520scheduling%2520is%2520crucial%2520in%2520various%250Areal-world%2520applications%252C%2520such%2520as%2520instant%2520messaging%252C%2520live%2520streaming%252C%2520and%2520data%250Acenter%2520management.%2520In%2520these%2520scenarios%252C%2520schedulers%2520must%2520make%2520real-time%2520decisions%250Ato%2520satisfy%2520both%2520delay%2520and%2520resource%2520constraints%2520without%2520prior%2520knowledge%2520of%250Asystem%2520dynamics%252C%2520which%2520are%2520often%2520time-varying%2520and%2520challenging%2520to%2520estimate.%250ACurrent%2520learning-based%2520methods%2520typically%2520require%2520interactions%2520with%2520actual%250Asystems%2520during%2520the%2520training%2520stage%252C%2520which%2520can%2520be%2520difficult%2520or%2520impractical%252C%2520as%2520it%250Ais%2520capable%2520of%2520significantly%2520degrading%2520system%2520performance%2520and%2520incurring%250Asubstantial%2520service%2520costs.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%250Aoffline%2520reinforcement%2520learning-based%2520algorithm%252C%2520named%2520%255Cunderline%257BS%257Dcheduling%2520By%250A%255Cunderline%257BO%257Dffline%2520Learning%2520with%2520%255Cunderline%257BC%257Dritic%2520Guidance%2520and%250A%255Cunderline%257BD%257Diffusion%2520Generation%2520%2528SOCD%2529%252C%2520to%2520learn%2520efficient%2520scheduling%2520policies%250Apurely%2520from%2520pre-collected%2520%255Cemph%257Boffline%2520data%257D.%2520SOCD%2520innovatively%2520employs%2520a%250Adiffusion-based%2520policy%2520network%252C%2520complemented%2520by%2520a%2520sampling-free%2520critic%2520network%250Afor%2520policy%2520guidance.%2520By%2520integrating%2520the%2520Lagrangian%2520multiplier%2520optimization%2520into%250Athe%2520offline%2520reinforcement%2520learning%252C%2520SOCD%2520effectively%2520trains%2520high-quality%250Aconstraint-aware%2520policies%2520exclusively%2520from%2520available%2520datasets%252C%2520eliminating%2520the%250Aneed%2520for%2520online%2520interactions%2520with%2520the%2520system.%2520Experimental%2520results%2520demonstrate%250Athat%2520SOCD%2520is%2520resilient%2520to%2520various%2520system%2520dynamics%252C%2520including%2520partially%250Aobservable%2520and%2520large-scale%2520environments%252C%2520and%2520delivers%2520superior%2520performance%250Acompared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Offline%20Critic-Guided%20Diffusion%20Policy%20for%20Multi-User%20Delay-Constrained%0A%20%20Scheduling&entry.906535625=Zhuoran%20Li%20and%20Ruishuo%20Chen%20and%20Hai%20Zhong%20and%20Longbo%20Huang&entry.1292438233=%20%20Effective%20multi-user%20delay-constrained%20scheduling%20is%20crucial%20in%20various%0Areal-world%20applications%2C%20such%20as%20instant%20messaging%2C%20live%20streaming%2C%20and%20data%0Acenter%20management.%20In%20these%20scenarios%2C%20schedulers%20must%20make%20real-time%20decisions%0Ato%20satisfy%20both%20delay%20and%20resource%20constraints%20without%20prior%20knowledge%20of%0Asystem%20dynamics%2C%20which%20are%20often%20time-varying%20and%20challenging%20to%20estimate.%0ACurrent%20learning-based%20methods%20typically%20require%20interactions%20with%20actual%0Asystems%20during%20the%20training%20stage%2C%20which%20can%20be%20difficult%20or%20impractical%2C%20as%20it%0Ais%20capable%20of%20significantly%20degrading%20system%20performance%20and%20incurring%0Asubstantial%20service%20costs.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%0Aoffline%20reinforcement%20learning-based%20algorithm%2C%20named%20%5Cunderline%7BS%7Dcheduling%20By%0A%5Cunderline%7BO%7Dffline%20Learning%20with%20%5Cunderline%7BC%7Dritic%20Guidance%20and%0A%5Cunderline%7BD%7Diffusion%20Generation%20%28SOCD%29%2C%20to%20learn%20efficient%20scheduling%20policies%0Apurely%20from%20pre-collected%20%5Cemph%7Boffline%20data%7D.%20SOCD%20innovatively%20employs%20a%0Adiffusion-based%20policy%20network%2C%20complemented%20by%20a%20sampling-free%20critic%20network%0Afor%20policy%20guidance.%20By%20integrating%20the%20Lagrangian%20multiplier%20optimization%20into%0Athe%20offline%20reinforcement%20learning%2C%20SOCD%20effectively%20trains%20high-quality%0Aconstraint-aware%20policies%20exclusively%20from%20available%20datasets%2C%20eliminating%20the%0Aneed%20for%20online%20interactions%20with%20the%20system.%20Experimental%20results%20demonstrate%0Athat%20SOCD%20is%20resilient%20to%20various%20system%20dynamics%2C%20including%20partially%0Aobservable%20and%20large-scale%20environments%2C%20and%20delivers%20superior%20performance%0Acompared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12942v1&entry.124074799=Read"},
{"title": "On Tradeoffs in Learning-Augmented Algorithms", "author": "Ziyad Benomar and Vianney Perchet", "abstract": "  The field of learning-augmented algorithms has gained significant attention\nin recent years. These algorithms, using potentially inaccurate predictions,\nmust exhibit three key properties: consistency, robustness, and smoothness. In\nscenarios where distributional information about predictions is available, a\nstrong expected performance is required. Typically, the design of these\nalgorithms involves a natural tradeoff between consistency and robustness, and\nprevious works aimed to achieve Pareto-optimal tradeoffs for specific problems.\nHowever, in some settings, this comes at the expense of smoothness. This paper\ndemonstrates that certain problems involve multiple tradeoffs between\nconsistency, robustness, smoothness, and average performance.\n", "link": "http://arxiv.org/abs/2501.12770v1", "date": "2025-01-22", "relevancy": 1.9128, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Tradeoffs%20in%20Learning-Augmented%20Algorithms&body=Title%3A%20On%20Tradeoffs%20in%20Learning-Augmented%20Algorithms%0AAuthor%3A%20Ziyad%20Benomar%20and%20Vianney%20Perchet%0AAbstract%3A%20%20%20The%20field%20of%20learning-augmented%20algorithms%20has%20gained%20significant%20attention%0Ain%20recent%20years.%20These%20algorithms%2C%20using%20potentially%20inaccurate%20predictions%2C%0Amust%20exhibit%20three%20key%20properties%3A%20consistency%2C%20robustness%2C%20and%20smoothness.%20In%0Ascenarios%20where%20distributional%20information%20about%20predictions%20is%20available%2C%20a%0Astrong%20expected%20performance%20is%20required.%20Typically%2C%20the%20design%20of%20these%0Aalgorithms%20involves%20a%20natural%20tradeoff%20between%20consistency%20and%20robustness%2C%20and%0Aprevious%20works%20aimed%20to%20achieve%20Pareto-optimal%20tradeoffs%20for%20specific%20problems.%0AHowever%2C%20in%20some%20settings%2C%20this%20comes%20at%20the%20expense%20of%20smoothness.%20This%20paper%0Ademonstrates%20that%20certain%20problems%20involve%20multiple%20tradeoffs%20between%0Aconsistency%2C%20robustness%2C%20smoothness%2C%20and%20average%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Tradeoffs%2520in%2520Learning-Augmented%2520Algorithms%26entry.906535625%3DZiyad%2520Benomar%2520and%2520Vianney%2520Perchet%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520learning-augmented%2520algorithms%2520has%2520gained%2520significant%2520attention%250Ain%2520recent%2520years.%2520These%2520algorithms%252C%2520using%2520potentially%2520inaccurate%2520predictions%252C%250Amust%2520exhibit%2520three%2520key%2520properties%253A%2520consistency%252C%2520robustness%252C%2520and%2520smoothness.%2520In%250Ascenarios%2520where%2520distributional%2520information%2520about%2520predictions%2520is%2520available%252C%2520a%250Astrong%2520expected%2520performance%2520is%2520required.%2520Typically%252C%2520the%2520design%2520of%2520these%250Aalgorithms%2520involves%2520a%2520natural%2520tradeoff%2520between%2520consistency%2520and%2520robustness%252C%2520and%250Aprevious%2520works%2520aimed%2520to%2520achieve%2520Pareto-optimal%2520tradeoffs%2520for%2520specific%2520problems.%250AHowever%252C%2520in%2520some%2520settings%252C%2520this%2520comes%2520at%2520the%2520expense%2520of%2520smoothness.%2520This%2520paper%250Ademonstrates%2520that%2520certain%2520problems%2520involve%2520multiple%2520tradeoffs%2520between%250Aconsistency%252C%2520robustness%252C%2520smoothness%252C%2520and%2520average%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Tradeoffs%20in%20Learning-Augmented%20Algorithms&entry.906535625=Ziyad%20Benomar%20and%20Vianney%20Perchet&entry.1292438233=%20%20The%20field%20of%20learning-augmented%20algorithms%20has%20gained%20significant%20attention%0Ain%20recent%20years.%20These%20algorithms%2C%20using%20potentially%20inaccurate%20predictions%2C%0Amust%20exhibit%20three%20key%20properties%3A%20consistency%2C%20robustness%2C%20and%20smoothness.%20In%0Ascenarios%20where%20distributional%20information%20about%20predictions%20is%20available%2C%20a%0Astrong%20expected%20performance%20is%20required.%20Typically%2C%20the%20design%20of%20these%0Aalgorithms%20involves%20a%20natural%20tradeoff%20between%20consistency%20and%20robustness%2C%20and%0Aprevious%20works%20aimed%20to%20achieve%20Pareto-optimal%20tradeoffs%20for%20specific%20problems.%0AHowever%2C%20in%20some%20settings%2C%20this%20comes%20at%20the%20expense%20of%20smoothness.%20This%20paper%0Ademonstrates%20that%20certain%20problems%20involve%20multiple%20tradeoffs%20between%0Aconsistency%2C%20robustness%2C%20smoothness%2C%20and%20average%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12770v1&entry.124074799=Read"},
{"title": "Drone Carrier: An Integrated Unmanned Surface Vehicle for Autonomous\n  Inspection and Intervention in GNSS-Denied Maritime Environment", "author": "Yihao Dong and Muhayyu Ud Din and Francesco Lagala and Hailiang Kuang and Jianjun Sun and Siyuan Yang and Irfan Hussain and Shaoming He", "abstract": "  This paper introduces an innovative drone carrier concept that is applied in\nmaritime port security or offshore rescue. This system works with a\nheterogeneous system consisting of multiple Unmanned Aerial Vehicles (UAVs) and\nUnmanned Surface Vehicles (USVs) to perform inspection and intervention tasks\nin GNSS-denied or interrupted environments. The carrier, an electric catamaran\nmeasuring 4m by 7m, features a 4m by 6m deck supporting automated takeoff and\nlanding for four DJI M300 drones, along with a 10kg-payload manipulator\noperable in up to level 3 sea conditions. Utilizing an offshore gimbal camera\nfor navigation, the carrier can autonomously navigate, approach and dock with\nnon-cooperative vessels, guided by an onboard camera, LiDAR, and Doppler\nVelocity Log (DVL) over a 3 km$^2$ area. UAVs equipped with onboard\nUltra-Wideband (UWB) technology execute mapping, detection, and manipulation\ntasks using a versatile gripper designed for wet, saline conditions.\nAdditionally, two UAVs can coordinate to transport large objects to the\nmanipulator or interact directly with them. These procedures are fully\nautomated and were successfully demonstrated at the Mohammed Bin Zayed\nInternational Robotic Competition (MBZIRC2024), where the drone carrier\nequipped with four UAVS and one manipulator, automatically accomplished the\nintervention tasks in sea-level-3 (wave height 1.25m) based on the rough target\ninformation.\n", "link": "http://arxiv.org/abs/2501.12869v1", "date": "2025-01-22", "relevancy": 1.9109, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4838}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drone%20Carrier%3A%20An%20Integrated%20Unmanned%20Surface%20Vehicle%20for%20Autonomous%0A%20%20Inspection%20and%20Intervention%20in%20GNSS-Denied%20Maritime%20Environment&body=Title%3A%20Drone%20Carrier%3A%20An%20Integrated%20Unmanned%20Surface%20Vehicle%20for%20Autonomous%0A%20%20Inspection%20and%20Intervention%20in%20GNSS-Denied%20Maritime%20Environment%0AAuthor%3A%20Yihao%20Dong%20and%20Muhayyu%20Ud%20Din%20and%20Francesco%20Lagala%20and%20Hailiang%20Kuang%20and%20Jianjun%20Sun%20and%20Siyuan%20Yang%20and%20Irfan%20Hussain%20and%20Shaoming%20He%0AAbstract%3A%20%20%20This%20paper%20introduces%20an%20innovative%20drone%20carrier%20concept%20that%20is%20applied%20in%0Amaritime%20port%20security%20or%20offshore%20rescue.%20This%20system%20works%20with%20a%0Aheterogeneous%20system%20consisting%20of%20multiple%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%0AUnmanned%20Surface%20Vehicles%20%28USVs%29%20to%20perform%20inspection%20and%20intervention%20tasks%0Ain%20GNSS-denied%20or%20interrupted%20environments.%20The%20carrier%2C%20an%20electric%20catamaran%0Ameasuring%204m%20by%207m%2C%20features%20a%204m%20by%206m%20deck%20supporting%20automated%20takeoff%20and%0Alanding%20for%20four%20DJI%20M300%20drones%2C%20along%20with%20a%2010kg-payload%20manipulator%0Aoperable%20in%20up%20to%20level%203%20sea%20conditions.%20Utilizing%20an%20offshore%20gimbal%20camera%0Afor%20navigation%2C%20the%20carrier%20can%20autonomously%20navigate%2C%20approach%20and%20dock%20with%0Anon-cooperative%20vessels%2C%20guided%20by%20an%20onboard%20camera%2C%20LiDAR%2C%20and%20Doppler%0AVelocity%20Log%20%28DVL%29%20over%20a%203%20km%24%5E2%24%20area.%20UAVs%20equipped%20with%20onboard%0AUltra-Wideband%20%28UWB%29%20technology%20execute%20mapping%2C%20detection%2C%20and%20manipulation%0Atasks%20using%20a%20versatile%20gripper%20designed%20for%20wet%2C%20saline%20conditions.%0AAdditionally%2C%20two%20UAVs%20can%20coordinate%20to%20transport%20large%20objects%20to%20the%0Amanipulator%20or%20interact%20directly%20with%20them.%20These%20procedures%20are%20fully%0Aautomated%20and%20were%20successfully%20demonstrated%20at%20the%20Mohammed%20Bin%20Zayed%0AInternational%20Robotic%20Competition%20%28MBZIRC2024%29%2C%20where%20the%20drone%20carrier%0Aequipped%20with%20four%20UAVS%20and%20one%20manipulator%2C%20automatically%20accomplished%20the%0Aintervention%20tasks%20in%20sea-level-3%20%28wave%20height%201.25m%29%20based%20on%20the%20rough%20target%0Ainformation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrone%2520Carrier%253A%2520An%2520Integrated%2520Unmanned%2520Surface%2520Vehicle%2520for%2520Autonomous%250A%2520%2520Inspection%2520and%2520Intervention%2520in%2520GNSS-Denied%2520Maritime%2520Environment%26entry.906535625%3DYihao%2520Dong%2520and%2520Muhayyu%2520Ud%2520Din%2520and%2520Francesco%2520Lagala%2520and%2520Hailiang%2520Kuang%2520and%2520Jianjun%2520Sun%2520and%2520Siyuan%2520Yang%2520and%2520Irfan%2520Hussain%2520and%2520Shaoming%2520He%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520an%2520innovative%2520drone%2520carrier%2520concept%2520that%2520is%2520applied%2520in%250Amaritime%2520port%2520security%2520or%2520offshore%2520rescue.%2520This%2520system%2520works%2520with%2520a%250Aheterogeneous%2520system%2520consisting%2520of%2520multiple%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520and%250AUnmanned%2520Surface%2520Vehicles%2520%2528USVs%2529%2520to%2520perform%2520inspection%2520and%2520intervention%2520tasks%250Ain%2520GNSS-denied%2520or%2520interrupted%2520environments.%2520The%2520carrier%252C%2520an%2520electric%2520catamaran%250Ameasuring%25204m%2520by%25207m%252C%2520features%2520a%25204m%2520by%25206m%2520deck%2520supporting%2520automated%2520takeoff%2520and%250Alanding%2520for%2520four%2520DJI%2520M300%2520drones%252C%2520along%2520with%2520a%252010kg-payload%2520manipulator%250Aoperable%2520in%2520up%2520to%2520level%25203%2520sea%2520conditions.%2520Utilizing%2520an%2520offshore%2520gimbal%2520camera%250Afor%2520navigation%252C%2520the%2520carrier%2520can%2520autonomously%2520navigate%252C%2520approach%2520and%2520dock%2520with%250Anon-cooperative%2520vessels%252C%2520guided%2520by%2520an%2520onboard%2520camera%252C%2520LiDAR%252C%2520and%2520Doppler%250AVelocity%2520Log%2520%2528DVL%2529%2520over%2520a%25203%2520km%2524%255E2%2524%2520area.%2520UAVs%2520equipped%2520with%2520onboard%250AUltra-Wideband%2520%2528UWB%2529%2520technology%2520execute%2520mapping%252C%2520detection%252C%2520and%2520manipulation%250Atasks%2520using%2520a%2520versatile%2520gripper%2520designed%2520for%2520wet%252C%2520saline%2520conditions.%250AAdditionally%252C%2520two%2520UAVs%2520can%2520coordinate%2520to%2520transport%2520large%2520objects%2520to%2520the%250Amanipulator%2520or%2520interact%2520directly%2520with%2520them.%2520These%2520procedures%2520are%2520fully%250Aautomated%2520and%2520were%2520successfully%2520demonstrated%2520at%2520the%2520Mohammed%2520Bin%2520Zayed%250AInternational%2520Robotic%2520Competition%2520%2528MBZIRC2024%2529%252C%2520where%2520the%2520drone%2520carrier%250Aequipped%2520with%2520four%2520UAVS%2520and%2520one%2520manipulator%252C%2520automatically%2520accomplished%2520the%250Aintervention%2520tasks%2520in%2520sea-level-3%2520%2528wave%2520height%25201.25m%2529%2520based%2520on%2520the%2520rough%2520target%250Ainformation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drone%20Carrier%3A%20An%20Integrated%20Unmanned%20Surface%20Vehicle%20for%20Autonomous%0A%20%20Inspection%20and%20Intervention%20in%20GNSS-Denied%20Maritime%20Environment&entry.906535625=Yihao%20Dong%20and%20Muhayyu%20Ud%20Din%20and%20Francesco%20Lagala%20and%20Hailiang%20Kuang%20and%20Jianjun%20Sun%20and%20Siyuan%20Yang%20and%20Irfan%20Hussain%20and%20Shaoming%20He&entry.1292438233=%20%20This%20paper%20introduces%20an%20innovative%20drone%20carrier%20concept%20that%20is%20applied%20in%0Amaritime%20port%20security%20or%20offshore%20rescue.%20This%20system%20works%20with%20a%0Aheterogeneous%20system%20consisting%20of%20multiple%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20and%0AUnmanned%20Surface%20Vehicles%20%28USVs%29%20to%20perform%20inspection%20and%20intervention%20tasks%0Ain%20GNSS-denied%20or%20interrupted%20environments.%20The%20carrier%2C%20an%20electric%20catamaran%0Ameasuring%204m%20by%207m%2C%20features%20a%204m%20by%206m%20deck%20supporting%20automated%20takeoff%20and%0Alanding%20for%20four%20DJI%20M300%20drones%2C%20along%20with%20a%2010kg-payload%20manipulator%0Aoperable%20in%20up%20to%20level%203%20sea%20conditions.%20Utilizing%20an%20offshore%20gimbal%20camera%0Afor%20navigation%2C%20the%20carrier%20can%20autonomously%20navigate%2C%20approach%20and%20dock%20with%0Anon-cooperative%20vessels%2C%20guided%20by%20an%20onboard%20camera%2C%20LiDAR%2C%20and%20Doppler%0AVelocity%20Log%20%28DVL%29%20over%20a%203%20km%24%5E2%24%20area.%20UAVs%20equipped%20with%20onboard%0AUltra-Wideband%20%28UWB%29%20technology%20execute%20mapping%2C%20detection%2C%20and%20manipulation%0Atasks%20using%20a%20versatile%20gripper%20designed%20for%20wet%2C%20saline%20conditions.%0AAdditionally%2C%20two%20UAVs%20can%20coordinate%20to%20transport%20large%20objects%20to%20the%0Amanipulator%20or%20interact%20directly%20with%20them.%20These%20procedures%20are%20fully%0Aautomated%20and%20were%20successfully%20demonstrated%20at%20the%20Mohammed%20Bin%20Zayed%0AInternational%20Robotic%20Competition%20%28MBZIRC2024%29%2C%20where%20the%20drone%20carrier%0Aequipped%20with%20four%20UAVS%20and%20one%20manipulator%2C%20automatically%20accomplished%20the%0Aintervention%20tasks%20in%20sea-level-3%20%28wave%20height%201.25m%29%20based%20on%20the%20rough%20target%0Ainformation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12869v1&entry.124074799=Read"},
{"title": "Capsule Vision 2024 Challenge: Multi-Class Abnormality Classification\n  for Video Capsule Endoscopy", "author": "Palak Handa and Amirreza Mahbod and Florian Schwarzhans and Ramona Woitek and Nidhi Goel and Manas Dhir and Deepti Chhabra and Shreshtha Jha and Pallavi Sharma and Vijay Thakur and Simarpreet Singh Chawla and Deepak Gunjan and Jagadeesh Kakarla and Balasubramanian Raman", "abstract": "  We present the Capsule Vision 2024 Challenge: Multi-Class Abnormality\nClassification for Video Capsule Endoscopy. It was virtually organized by the\nResearch Center for Medical Image Analysis and Artificial Intelligence (MIAAI),\nDepartment of Medicine, Danube Private University, Krems, Austria in\ncollaboration with the 9th International Conference on Computer Vision & Image\nProcessing (CVIP 2024) being organized by the Indian Institute of Information\nTechnology, Design and Manufacturing (IIITDM) Kancheepuram, Chennai, India.\nThis document provides an overview of the challenge, including the registration\nprocess, rules, submission format, description of the datasets used, qualified\nteam rankings, all team descriptions, and the benchmarking results reported by\nthe organizers.\n", "link": "http://arxiv.org/abs/2408.04940v3", "date": "2025-01-22", "relevancy": 1.9048, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Capsule%20Vision%202024%20Challenge%3A%20Multi-Class%20Abnormality%20Classification%0A%20%20for%20Video%20Capsule%20Endoscopy&body=Title%3A%20Capsule%20Vision%202024%20Challenge%3A%20Multi-Class%20Abnormality%20Classification%0A%20%20for%20Video%20Capsule%20Endoscopy%0AAuthor%3A%20Palak%20Handa%20and%20Amirreza%20Mahbod%20and%20Florian%20Schwarzhans%20and%20Ramona%20Woitek%20and%20Nidhi%20Goel%20and%20Manas%20Dhir%20and%20Deepti%20Chhabra%20and%20Shreshtha%20Jha%20and%20Pallavi%20Sharma%20and%20Vijay%20Thakur%20and%20Simarpreet%20Singh%20Chawla%20and%20Deepak%20Gunjan%20and%20Jagadeesh%20Kakarla%20and%20Balasubramanian%20Raman%0AAbstract%3A%20%20%20We%20present%20the%20Capsule%20Vision%202024%20Challenge%3A%20Multi-Class%20Abnormality%0AClassification%20for%20Video%20Capsule%20Endoscopy.%20It%20was%20virtually%20organized%20by%20the%0AResearch%20Center%20for%20Medical%20Image%20Analysis%20and%20Artificial%20Intelligence%20%28MIAAI%29%2C%0ADepartment%20of%20Medicine%2C%20Danube%20Private%20University%2C%20Krems%2C%20Austria%20in%0Acollaboration%20with%20the%209th%20International%20Conference%20on%20Computer%20Vision%20%26%20Image%0AProcessing%20%28CVIP%202024%29%20being%20organized%20by%20the%20Indian%20Institute%20of%20Information%0ATechnology%2C%20Design%20and%20Manufacturing%20%28IIITDM%29%20Kancheepuram%2C%20Chennai%2C%20India.%0AThis%20document%20provides%20an%20overview%20of%20the%20challenge%2C%20including%20the%20registration%0Aprocess%2C%20rules%2C%20submission%20format%2C%20description%20of%20the%20datasets%20used%2C%20qualified%0Ateam%20rankings%2C%20all%20team%20descriptions%2C%20and%20the%20benchmarking%20results%20reported%20by%0Athe%20organizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCapsule%2520Vision%25202024%2520Challenge%253A%2520Multi-Class%2520Abnormality%2520Classification%250A%2520%2520for%2520Video%2520Capsule%2520Endoscopy%26entry.906535625%3DPalak%2520Handa%2520and%2520Amirreza%2520Mahbod%2520and%2520Florian%2520Schwarzhans%2520and%2520Ramona%2520Woitek%2520and%2520Nidhi%2520Goel%2520and%2520Manas%2520Dhir%2520and%2520Deepti%2520Chhabra%2520and%2520Shreshtha%2520Jha%2520and%2520Pallavi%2520Sharma%2520and%2520Vijay%2520Thakur%2520and%2520Simarpreet%2520Singh%2520Chawla%2520and%2520Deepak%2520Gunjan%2520and%2520Jagadeesh%2520Kakarla%2520and%2520Balasubramanian%2520Raman%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Capsule%2520Vision%25202024%2520Challenge%253A%2520Multi-Class%2520Abnormality%250AClassification%2520for%2520Video%2520Capsule%2520Endoscopy.%2520It%2520was%2520virtually%2520organized%2520by%2520the%250AResearch%2520Center%2520for%2520Medical%2520Image%2520Analysis%2520and%2520Artificial%2520Intelligence%2520%2528MIAAI%2529%252C%250ADepartment%2520of%2520Medicine%252C%2520Danube%2520Private%2520University%252C%2520Krems%252C%2520Austria%2520in%250Acollaboration%2520with%2520the%25209th%2520International%2520Conference%2520on%2520Computer%2520Vision%2520%2526%2520Image%250AProcessing%2520%2528CVIP%25202024%2529%2520being%2520organized%2520by%2520the%2520Indian%2520Institute%2520of%2520Information%250ATechnology%252C%2520Design%2520and%2520Manufacturing%2520%2528IIITDM%2529%2520Kancheepuram%252C%2520Chennai%252C%2520India.%250AThis%2520document%2520provides%2520an%2520overview%2520of%2520the%2520challenge%252C%2520including%2520the%2520registration%250Aprocess%252C%2520rules%252C%2520submission%2520format%252C%2520description%2520of%2520the%2520datasets%2520used%252C%2520qualified%250Ateam%2520rankings%252C%2520all%2520team%2520descriptions%252C%2520and%2520the%2520benchmarking%2520results%2520reported%2520by%250Athe%2520organizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Capsule%20Vision%202024%20Challenge%3A%20Multi-Class%20Abnormality%20Classification%0A%20%20for%20Video%20Capsule%20Endoscopy&entry.906535625=Palak%20Handa%20and%20Amirreza%20Mahbod%20and%20Florian%20Schwarzhans%20and%20Ramona%20Woitek%20and%20Nidhi%20Goel%20and%20Manas%20Dhir%20and%20Deepti%20Chhabra%20and%20Shreshtha%20Jha%20and%20Pallavi%20Sharma%20and%20Vijay%20Thakur%20and%20Simarpreet%20Singh%20Chawla%20and%20Deepak%20Gunjan%20and%20Jagadeesh%20Kakarla%20and%20Balasubramanian%20Raman&entry.1292438233=%20%20We%20present%20the%20Capsule%20Vision%202024%20Challenge%3A%20Multi-Class%20Abnormality%0AClassification%20for%20Video%20Capsule%20Endoscopy.%20It%20was%20virtually%20organized%20by%20the%0AResearch%20Center%20for%20Medical%20Image%20Analysis%20and%20Artificial%20Intelligence%20%28MIAAI%29%2C%0ADepartment%20of%20Medicine%2C%20Danube%20Private%20University%2C%20Krems%2C%20Austria%20in%0Acollaboration%20with%20the%209th%20International%20Conference%20on%20Computer%20Vision%20%26%20Image%0AProcessing%20%28CVIP%202024%29%20being%20organized%20by%20the%20Indian%20Institute%20of%20Information%0ATechnology%2C%20Design%20and%20Manufacturing%20%28IIITDM%29%20Kancheepuram%2C%20Chennai%2C%20India.%0AThis%20document%20provides%20an%20overview%20of%20the%20challenge%2C%20including%20the%20registration%0Aprocess%2C%20rules%2C%20submission%20format%2C%20description%20of%20the%20datasets%20used%2C%20qualified%0Ateam%20rankings%2C%20all%20team%20descriptions%2C%20and%20the%20benchmarking%20results%20reported%20by%0Athe%20organizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04940v3&entry.124074799=Read"},
{"title": "Pay Attention and Move Better: Harnessing Attention for Interactive\n  Motion Generation and Training-free Editing", "author": "Ling-Hao Chen and Shunlin Lu and Wenxun Dai and Zhiyang Dou and Xuan Ju and Jingbo Wang and Taku Komura and Lei Zhang", "abstract": "  This research delves into the problem of interactive editing of human motion\ngeneration. Previous motion diffusion models lack explicit modeling of the\nword-level text-motion correspondence and good explainability, hence\nrestricting their fine-grained editing ability. To address this issue, we\npropose an attention-based motion diffusion model, namely MotionCLR, with CLeaR\nmodeling of attention mechanisms. Technically, MotionCLR models the in-modality\nand cross-modality interactions with self-attention and cross-attention,\nrespectively. More specifically, the self-attention mechanism aims to measure\nthe sequential similarity between frames and impacts the order of motion\nfeatures. By contrast, the cross-attention mechanism works to find the\nfine-grained word-sequence correspondence and activate the corresponding\ntimesteps in the motion sequence. Based on these key properties, we develop a\nversatile set of simple yet effective motion editing methods via manipulating\nattention maps, such as motion (de-)emphasizing, in-place motion replacement,\nand example-based motion generation, etc. For further verification of the\nexplainability of the attention mechanism, we additionally explore the\npotential of action-counting and grounded motion generation ability via\nattention maps. Our experimental results show that our method enjoys good\ngeneration and editing ability with good explainability.\n", "link": "http://arxiv.org/abs/2410.18977v2", "date": "2025-01-22", "relevancy": 1.8941, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6524}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.65}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pay%20Attention%20and%20Move%20Better%3A%20Harnessing%20Attention%20for%20Interactive%0A%20%20Motion%20Generation%20and%20Training-free%20Editing&body=Title%3A%20Pay%20Attention%20and%20Move%20Better%3A%20Harnessing%20Attention%20for%20Interactive%0A%20%20Motion%20Generation%20and%20Training-free%20Editing%0AAuthor%3A%20Ling-Hao%20Chen%20and%20Shunlin%20Lu%20and%20Wenxun%20Dai%20and%20Zhiyang%20Dou%20and%20Xuan%20Ju%20and%20Jingbo%20Wang%20and%20Taku%20Komura%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20This%20research%20delves%20into%20the%20problem%20of%20interactive%20editing%20of%20human%20motion%0Ageneration.%20Previous%20motion%20diffusion%20models%20lack%20explicit%20modeling%20of%20the%0Aword-level%20text-motion%20correspondence%20and%20good%20explainability%2C%20hence%0Arestricting%20their%20fine-grained%20editing%20ability.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20attention-based%20motion%20diffusion%20model%2C%20namely%20MotionCLR%2C%20with%20CLeaR%0Amodeling%20of%20attention%20mechanisms.%20Technically%2C%20MotionCLR%20models%20the%20in-modality%0Aand%20cross-modality%20interactions%20with%20self-attention%20and%20cross-attention%2C%0Arespectively.%20More%20specifically%2C%20the%20self-attention%20mechanism%20aims%20to%20measure%0Athe%20sequential%20similarity%20between%20frames%20and%20impacts%20the%20order%20of%20motion%0Afeatures.%20By%20contrast%2C%20the%20cross-attention%20mechanism%20works%20to%20find%20the%0Afine-grained%20word-sequence%20correspondence%20and%20activate%20the%20corresponding%0Atimesteps%20in%20the%20motion%20sequence.%20Based%20on%20these%20key%20properties%2C%20we%20develop%20a%0Aversatile%20set%20of%20simple%20yet%20effective%20motion%20editing%20methods%20via%20manipulating%0Aattention%20maps%2C%20such%20as%20motion%20%28de-%29emphasizing%2C%20in-place%20motion%20replacement%2C%0Aand%20example-based%20motion%20generation%2C%20etc.%20For%20further%20verification%20of%20the%0Aexplainability%20of%20the%20attention%20mechanism%2C%20we%20additionally%20explore%20the%0Apotential%20of%20action-counting%20and%20grounded%20motion%20generation%20ability%20via%0Aattention%20maps.%20Our%20experimental%20results%20show%20that%20our%20method%20enjoys%20good%0Ageneration%20and%20editing%20ability%20with%20good%20explainability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPay%2520Attention%2520and%2520Move%2520Better%253A%2520Harnessing%2520Attention%2520for%2520Interactive%250A%2520%2520Motion%2520Generation%2520and%2520Training-free%2520Editing%26entry.906535625%3DLing-Hao%2520Chen%2520and%2520Shunlin%2520Lu%2520and%2520Wenxun%2520Dai%2520and%2520Zhiyang%2520Dou%2520and%2520Xuan%2520Ju%2520and%2520Jingbo%2520Wang%2520and%2520Taku%2520Komura%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520This%2520research%2520delves%2520into%2520the%2520problem%2520of%2520interactive%2520editing%2520of%2520human%2520motion%250Ageneration.%2520Previous%2520motion%2520diffusion%2520models%2520lack%2520explicit%2520modeling%2520of%2520the%250Aword-level%2520text-motion%2520correspondence%2520and%2520good%2520explainability%252C%2520hence%250Arestricting%2520their%2520fine-grained%2520editing%2520ability.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520an%2520attention-based%2520motion%2520diffusion%2520model%252C%2520namely%2520MotionCLR%252C%2520with%2520CLeaR%250Amodeling%2520of%2520attention%2520mechanisms.%2520Technically%252C%2520MotionCLR%2520models%2520the%2520in-modality%250Aand%2520cross-modality%2520interactions%2520with%2520self-attention%2520and%2520cross-attention%252C%250Arespectively.%2520More%2520specifically%252C%2520the%2520self-attention%2520mechanism%2520aims%2520to%2520measure%250Athe%2520sequential%2520similarity%2520between%2520frames%2520and%2520impacts%2520the%2520order%2520of%2520motion%250Afeatures.%2520By%2520contrast%252C%2520the%2520cross-attention%2520mechanism%2520works%2520to%2520find%2520the%250Afine-grained%2520word-sequence%2520correspondence%2520and%2520activate%2520the%2520corresponding%250Atimesteps%2520in%2520the%2520motion%2520sequence.%2520Based%2520on%2520these%2520key%2520properties%252C%2520we%2520develop%2520a%250Aversatile%2520set%2520of%2520simple%2520yet%2520effective%2520motion%2520editing%2520methods%2520via%2520manipulating%250Aattention%2520maps%252C%2520such%2520as%2520motion%2520%2528de-%2529emphasizing%252C%2520in-place%2520motion%2520replacement%252C%250Aand%2520example-based%2520motion%2520generation%252C%2520etc.%2520For%2520further%2520verification%2520of%2520the%250Aexplainability%2520of%2520the%2520attention%2520mechanism%252C%2520we%2520additionally%2520explore%2520the%250Apotential%2520of%2520action-counting%2520and%2520grounded%2520motion%2520generation%2520ability%2520via%250Aattention%2520maps.%2520Our%2520experimental%2520results%2520show%2520that%2520our%2520method%2520enjoys%2520good%250Ageneration%2520and%2520editing%2520ability%2520with%2520good%2520explainability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pay%20Attention%20and%20Move%20Better%3A%20Harnessing%20Attention%20for%20Interactive%0A%20%20Motion%20Generation%20and%20Training-free%20Editing&entry.906535625=Ling-Hao%20Chen%20and%20Shunlin%20Lu%20and%20Wenxun%20Dai%20and%20Zhiyang%20Dou%20and%20Xuan%20Ju%20and%20Jingbo%20Wang%20and%20Taku%20Komura%20and%20Lei%20Zhang&entry.1292438233=%20%20This%20research%20delves%20into%20the%20problem%20of%20interactive%20editing%20of%20human%20motion%0Ageneration.%20Previous%20motion%20diffusion%20models%20lack%20explicit%20modeling%20of%20the%0Aword-level%20text-motion%20correspondence%20and%20good%20explainability%2C%20hence%0Arestricting%20their%20fine-grained%20editing%20ability.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20attention-based%20motion%20diffusion%20model%2C%20namely%20MotionCLR%2C%20with%20CLeaR%0Amodeling%20of%20attention%20mechanisms.%20Technically%2C%20MotionCLR%20models%20the%20in-modality%0Aand%20cross-modality%20interactions%20with%20self-attention%20and%20cross-attention%2C%0Arespectively.%20More%20specifically%2C%20the%20self-attention%20mechanism%20aims%20to%20measure%0Athe%20sequential%20similarity%20between%20frames%20and%20impacts%20the%20order%20of%20motion%0Afeatures.%20By%20contrast%2C%20the%20cross-attention%20mechanism%20works%20to%20find%20the%0Afine-grained%20word-sequence%20correspondence%20and%20activate%20the%20corresponding%0Atimesteps%20in%20the%20motion%20sequence.%20Based%20on%20these%20key%20properties%2C%20we%20develop%20a%0Aversatile%20set%20of%20simple%20yet%20effective%20motion%20editing%20methods%20via%20manipulating%0Aattention%20maps%2C%20such%20as%20motion%20%28de-%29emphasizing%2C%20in-place%20motion%20replacement%2C%0Aand%20example-based%20motion%20generation%2C%20etc.%20For%20further%20verification%20of%20the%0Aexplainability%20of%20the%20attention%20mechanism%2C%20we%20additionally%20explore%20the%0Apotential%20of%20action-counting%20and%20grounded%20motion%20generation%20ability%20via%0Aattention%20maps.%20Our%20experimental%20results%20show%20that%20our%20method%20enjoys%20good%0Ageneration%20and%20editing%20ability%20with%20good%20explainability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18977v2&entry.124074799=Read"},
{"title": "Reinforcement learning Based Automated Design of Differential Evolution\n  Algorithm for Black-box Optimization", "author": "Xu Yang and Rui Wang and Kaiwen Li and Ling Wang", "abstract": "  Differential evolution (DE) algorithm is recognized as one of the most\neffective evolutionary algorithms, demonstrating remarkable efficacy in\nblack-box optimization due to its derivative-free nature. Numerous enhancements\nto the fundamental DE have been proposed, incorporating innovative mutation\nstrategies and sophisticated parameter tuning techniques to improve\nperformance. However, no single variant has proven universally superior across\nall problems. To address this challenge, we introduce a novel framework that\nemploys reinforcement learning (RL) to automatically design DE for black-box\noptimization through meta-learning. RL acts as an advanced meta-optimizer,\ngenerating a customized DE configuration that includes an optimal\ninitialization strategy, update rule, and hyperparameters tailored to a\nspecific black-box optimization problem. This process is informed by a detailed\nanalysis of the problem characteristics. In this proof-of-concept study, we\nutilize a double deep Q-network for implementation, considering a subset of 40\npossible strategy combinations and parameter optimizations simultaneously. The\nframework's performance is evaluated against black-box optimization benchmarks\nand compared with state-of-the-art algorithms. The experimental results\nhighlight the promising potential of our proposed framework.\n", "link": "http://arxiv.org/abs/2501.12881v1", "date": "2025-01-22", "relevancy": 1.8923, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4967}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4575}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20learning%20Based%20Automated%20Design%20of%20Differential%20Evolution%0A%20%20Algorithm%20for%20Black-box%20Optimization&body=Title%3A%20Reinforcement%20learning%20Based%20Automated%20Design%20of%20Differential%20Evolution%0A%20%20Algorithm%20for%20Black-box%20Optimization%0AAuthor%3A%20Xu%20Yang%20and%20Rui%20Wang%20and%20Kaiwen%20Li%20and%20Ling%20Wang%0AAbstract%3A%20%20%20Differential%20evolution%20%28DE%29%20algorithm%20is%20recognized%20as%20one%20of%20the%20most%0Aeffective%20evolutionary%20algorithms%2C%20demonstrating%20remarkable%20efficacy%20in%0Ablack-box%20optimization%20due%20to%20its%20derivative-free%20nature.%20Numerous%20enhancements%0Ato%20the%20fundamental%20DE%20have%20been%20proposed%2C%20incorporating%20innovative%20mutation%0Astrategies%20and%20sophisticated%20parameter%20tuning%20techniques%20to%20improve%0Aperformance.%20However%2C%20no%20single%20variant%20has%20proven%20universally%20superior%20across%0Aall%20problems.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20novel%20framework%20that%0Aemploys%20reinforcement%20learning%20%28RL%29%20to%20automatically%20design%20DE%20for%20black-box%0Aoptimization%20through%20meta-learning.%20RL%20acts%20as%20an%20advanced%20meta-optimizer%2C%0Agenerating%20a%20customized%20DE%20configuration%20that%20includes%20an%20optimal%0Ainitialization%20strategy%2C%20update%20rule%2C%20and%20hyperparameters%20tailored%20to%20a%0Aspecific%20black-box%20optimization%20problem.%20This%20process%20is%20informed%20by%20a%20detailed%0Aanalysis%20of%20the%20problem%20characteristics.%20In%20this%20proof-of-concept%20study%2C%20we%0Autilize%20a%20double%20deep%20Q-network%20for%20implementation%2C%20considering%20a%20subset%20of%2040%0Apossible%20strategy%20combinations%20and%20parameter%20optimizations%20simultaneously.%20The%0Aframework%27s%20performance%20is%20evaluated%20against%20black-box%20optimization%20benchmarks%0Aand%20compared%20with%20state-of-the-art%20algorithms.%20The%20experimental%20results%0Ahighlight%20the%20promising%20potential%20of%20our%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520learning%2520Based%2520Automated%2520Design%2520of%2520Differential%2520Evolution%250A%2520%2520Algorithm%2520for%2520Black-box%2520Optimization%26entry.906535625%3DXu%2520Yang%2520and%2520Rui%2520Wang%2520and%2520Kaiwen%2520Li%2520and%2520Ling%2520Wang%26entry.1292438233%3D%2520%2520Differential%2520evolution%2520%2528DE%2529%2520algorithm%2520is%2520recognized%2520as%2520one%2520of%2520the%2520most%250Aeffective%2520evolutionary%2520algorithms%252C%2520demonstrating%2520remarkable%2520efficacy%2520in%250Ablack-box%2520optimization%2520due%2520to%2520its%2520derivative-free%2520nature.%2520Numerous%2520enhancements%250Ato%2520the%2520fundamental%2520DE%2520have%2520been%2520proposed%252C%2520incorporating%2520innovative%2520mutation%250Astrategies%2520and%2520sophisticated%2520parameter%2520tuning%2520techniques%2520to%2520improve%250Aperformance.%2520However%252C%2520no%2520single%2520variant%2520has%2520proven%2520universally%2520superior%2520across%250Aall%2520problems.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%250Aemploys%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520automatically%2520design%2520DE%2520for%2520black-box%250Aoptimization%2520through%2520meta-learning.%2520RL%2520acts%2520as%2520an%2520advanced%2520meta-optimizer%252C%250Agenerating%2520a%2520customized%2520DE%2520configuration%2520that%2520includes%2520an%2520optimal%250Ainitialization%2520strategy%252C%2520update%2520rule%252C%2520and%2520hyperparameters%2520tailored%2520to%2520a%250Aspecific%2520black-box%2520optimization%2520problem.%2520This%2520process%2520is%2520informed%2520by%2520a%2520detailed%250Aanalysis%2520of%2520the%2520problem%2520characteristics.%2520In%2520this%2520proof-of-concept%2520study%252C%2520we%250Autilize%2520a%2520double%2520deep%2520Q-network%2520for%2520implementation%252C%2520considering%2520a%2520subset%2520of%252040%250Apossible%2520strategy%2520combinations%2520and%2520parameter%2520optimizations%2520simultaneously.%2520The%250Aframework%2527s%2520performance%2520is%2520evaluated%2520against%2520black-box%2520optimization%2520benchmarks%250Aand%2520compared%2520with%2520state-of-the-art%2520algorithms.%2520The%2520experimental%2520results%250Ahighlight%2520the%2520promising%2520potential%2520of%2520our%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20learning%20Based%20Automated%20Design%20of%20Differential%20Evolution%0A%20%20Algorithm%20for%20Black-box%20Optimization&entry.906535625=Xu%20Yang%20and%20Rui%20Wang%20and%20Kaiwen%20Li%20and%20Ling%20Wang&entry.1292438233=%20%20Differential%20evolution%20%28DE%29%20algorithm%20is%20recognized%20as%20one%20of%20the%20most%0Aeffective%20evolutionary%20algorithms%2C%20demonstrating%20remarkable%20efficacy%20in%0Ablack-box%20optimization%20due%20to%20its%20derivative-free%20nature.%20Numerous%20enhancements%0Ato%20the%20fundamental%20DE%20have%20been%20proposed%2C%20incorporating%20innovative%20mutation%0Astrategies%20and%20sophisticated%20parameter%20tuning%20techniques%20to%20improve%0Aperformance.%20However%2C%20no%20single%20variant%20has%20proven%20universally%20superior%20across%0Aall%20problems.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20novel%20framework%20that%0Aemploys%20reinforcement%20learning%20%28RL%29%20to%20automatically%20design%20DE%20for%20black-box%0Aoptimization%20through%20meta-learning.%20RL%20acts%20as%20an%20advanced%20meta-optimizer%2C%0Agenerating%20a%20customized%20DE%20configuration%20that%20includes%20an%20optimal%0Ainitialization%20strategy%2C%20update%20rule%2C%20and%20hyperparameters%20tailored%20to%20a%0Aspecific%20black-box%20optimization%20problem.%20This%20process%20is%20informed%20by%20a%20detailed%0Aanalysis%20of%20the%20problem%20characteristics.%20In%20this%20proof-of-concept%20study%2C%20we%0Autilize%20a%20double%20deep%20Q-network%20for%20implementation%2C%20considering%20a%20subset%20of%2040%0Apossible%20strategy%20combinations%20and%20parameter%20optimizations%20simultaneously.%20The%0Aframework%27s%20performance%20is%20evaluated%20against%20black-box%20optimization%20benchmarks%0Aand%20compared%20with%20state-of-the-art%20algorithms.%20The%20experimental%20results%0Ahighlight%20the%20promising%20potential%20of%20our%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12881v1&entry.124074799=Read"},
{"title": "CHG Shapley: Efficient Data Valuation and Selection towards Trustworthy\n  Machine Learning", "author": "Huaiguang Cai", "abstract": "  Understanding the decision-making process of machine learning models is\ncrucial for ensuring trustworthy machine learning. Data Shapley, a landmark\nstudy on data valuation, advances this understanding by assessing the\ncontribution of each datum to model performance. However, the\nresource-intensive and time-consuming nature of multiple model retraining poses\nchallenges for applying Data Shapley to large datasets. To address this, we\npropose the CHG (compound of Hardness and Gradient) utility function, which\napproximates the utility of each data subset on model performance in every\ntraining epoch. By deriving the closed-form Shapley value for each data point\nusing the CHG utility function, we reduce the computational complexity to that\nof a single model retraining, achieving a quadratic improvement over existing\nmarginal contribution-based methods. We further leverage CHG Shapley for\nreal-time data selection, conducting experiments across three settings:\nstandard datasets, label noise datasets, and class imbalance datasets. These\nexperiments demonstrate its effectiveness in identifying high-value and noisy\ndata. By enabling efficient data valuation, CHG Shapley promotes trustworthy\nmodel training through a novel data-centric perspective. Our codes are\navailable at https://github.com/caihuaiguang/CHG-Shapley-for-Data-Valuation and\nhttps://github.com/caihuaiguang/CHG-Shapley-for-Data-Selection.\n", "link": "http://arxiv.org/abs/2406.11730v3", "date": "2025-01-22", "relevancy": 1.8778, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4734}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4708}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CHG%20Shapley%3A%20Efficient%20Data%20Valuation%20and%20Selection%20towards%20Trustworthy%0A%20%20Machine%20Learning&body=Title%3A%20CHG%20Shapley%3A%20Efficient%20Data%20Valuation%20and%20Selection%20towards%20Trustworthy%0A%20%20Machine%20Learning%0AAuthor%3A%20Huaiguang%20Cai%0AAbstract%3A%20%20%20Understanding%20the%20decision-making%20process%20of%20machine%20learning%20models%20is%0Acrucial%20for%20ensuring%20trustworthy%20machine%20learning.%20Data%20Shapley%2C%20a%20landmark%0Astudy%20on%20data%20valuation%2C%20advances%20this%20understanding%20by%20assessing%20the%0Acontribution%20of%20each%20datum%20to%20model%20performance.%20However%2C%20the%0Aresource-intensive%20and%20time-consuming%20nature%20of%20multiple%20model%20retraining%20poses%0Achallenges%20for%20applying%20Data%20Shapley%20to%20large%20datasets.%20To%20address%20this%2C%20we%0Apropose%20the%20CHG%20%28compound%20of%20Hardness%20and%20Gradient%29%20utility%20function%2C%20which%0Aapproximates%20the%20utility%20of%20each%20data%20subset%20on%20model%20performance%20in%20every%0Atraining%20epoch.%20By%20deriving%20the%20closed-form%20Shapley%20value%20for%20each%20data%20point%0Ausing%20the%20CHG%20utility%20function%2C%20we%20reduce%20the%20computational%20complexity%20to%20that%0Aof%20a%20single%20model%20retraining%2C%20achieving%20a%20quadratic%20improvement%20over%20existing%0Amarginal%20contribution-based%20methods.%20We%20further%20leverage%20CHG%20Shapley%20for%0Areal-time%20data%20selection%2C%20conducting%20experiments%20across%20three%20settings%3A%0Astandard%20datasets%2C%20label%20noise%20datasets%2C%20and%20class%20imbalance%20datasets.%20These%0Aexperiments%20demonstrate%20its%20effectiveness%20in%20identifying%20high-value%20and%20noisy%0Adata.%20By%20enabling%20efficient%20data%20valuation%2C%20CHG%20Shapley%20promotes%20trustworthy%0Amodel%20training%20through%20a%20novel%20data-centric%20perspective.%20Our%20codes%20are%0Aavailable%20at%20https%3A//github.com/caihuaiguang/CHG-Shapley-for-Data-Valuation%20and%0Ahttps%3A//github.com/caihuaiguang/CHG-Shapley-for-Data-Selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11730v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCHG%2520Shapley%253A%2520Efficient%2520Data%2520Valuation%2520and%2520Selection%2520towards%2520Trustworthy%250A%2520%2520Machine%2520Learning%26entry.906535625%3DHuaiguang%2520Cai%26entry.1292438233%3D%2520%2520Understanding%2520the%2520decision-making%2520process%2520of%2520machine%2520learning%2520models%2520is%250Acrucial%2520for%2520ensuring%2520trustworthy%2520machine%2520learning.%2520Data%2520Shapley%252C%2520a%2520landmark%250Astudy%2520on%2520data%2520valuation%252C%2520advances%2520this%2520understanding%2520by%2520assessing%2520the%250Acontribution%2520of%2520each%2520datum%2520to%2520model%2520performance.%2520However%252C%2520the%250Aresource-intensive%2520and%2520time-consuming%2520nature%2520of%2520multiple%2520model%2520retraining%2520poses%250Achallenges%2520for%2520applying%2520Data%2520Shapley%2520to%2520large%2520datasets.%2520To%2520address%2520this%252C%2520we%250Apropose%2520the%2520CHG%2520%2528compound%2520of%2520Hardness%2520and%2520Gradient%2529%2520utility%2520function%252C%2520which%250Aapproximates%2520the%2520utility%2520of%2520each%2520data%2520subset%2520on%2520model%2520performance%2520in%2520every%250Atraining%2520epoch.%2520By%2520deriving%2520the%2520closed-form%2520Shapley%2520value%2520for%2520each%2520data%2520point%250Ausing%2520the%2520CHG%2520utility%2520function%252C%2520we%2520reduce%2520the%2520computational%2520complexity%2520to%2520that%250Aof%2520a%2520single%2520model%2520retraining%252C%2520achieving%2520a%2520quadratic%2520improvement%2520over%2520existing%250Amarginal%2520contribution-based%2520methods.%2520We%2520further%2520leverage%2520CHG%2520Shapley%2520for%250Areal-time%2520data%2520selection%252C%2520conducting%2520experiments%2520across%2520three%2520settings%253A%250Astandard%2520datasets%252C%2520label%2520noise%2520datasets%252C%2520and%2520class%2520imbalance%2520datasets.%2520These%250Aexperiments%2520demonstrate%2520its%2520effectiveness%2520in%2520identifying%2520high-value%2520and%2520noisy%250Adata.%2520By%2520enabling%2520efficient%2520data%2520valuation%252C%2520CHG%2520Shapley%2520promotes%2520trustworthy%250Amodel%2520training%2520through%2520a%2520novel%2520data-centric%2520perspective.%2520Our%2520codes%2520are%250Aavailable%2520at%2520https%253A//github.com/caihuaiguang/CHG-Shapley-for-Data-Valuation%2520and%250Ahttps%253A//github.com/caihuaiguang/CHG-Shapley-for-Data-Selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11730v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CHG%20Shapley%3A%20Efficient%20Data%20Valuation%20and%20Selection%20towards%20Trustworthy%0A%20%20Machine%20Learning&entry.906535625=Huaiguang%20Cai&entry.1292438233=%20%20Understanding%20the%20decision-making%20process%20of%20machine%20learning%20models%20is%0Acrucial%20for%20ensuring%20trustworthy%20machine%20learning.%20Data%20Shapley%2C%20a%20landmark%0Astudy%20on%20data%20valuation%2C%20advances%20this%20understanding%20by%20assessing%20the%0Acontribution%20of%20each%20datum%20to%20model%20performance.%20However%2C%20the%0Aresource-intensive%20and%20time-consuming%20nature%20of%20multiple%20model%20retraining%20poses%0Achallenges%20for%20applying%20Data%20Shapley%20to%20large%20datasets.%20To%20address%20this%2C%20we%0Apropose%20the%20CHG%20%28compound%20of%20Hardness%20and%20Gradient%29%20utility%20function%2C%20which%0Aapproximates%20the%20utility%20of%20each%20data%20subset%20on%20model%20performance%20in%20every%0Atraining%20epoch.%20By%20deriving%20the%20closed-form%20Shapley%20value%20for%20each%20data%20point%0Ausing%20the%20CHG%20utility%20function%2C%20we%20reduce%20the%20computational%20complexity%20to%20that%0Aof%20a%20single%20model%20retraining%2C%20achieving%20a%20quadratic%20improvement%20over%20existing%0Amarginal%20contribution-based%20methods.%20We%20further%20leverage%20CHG%20Shapley%20for%0Areal-time%20data%20selection%2C%20conducting%20experiments%20across%20three%20settings%3A%0Astandard%20datasets%2C%20label%20noise%20datasets%2C%20and%20class%20imbalance%20datasets.%20These%0Aexperiments%20demonstrate%20its%20effectiveness%20in%20identifying%20high-value%20and%20noisy%0Adata.%20By%20enabling%20efficient%20data%20valuation%2C%20CHG%20Shapley%20promotes%20trustworthy%0Amodel%20training%20through%20a%20novel%20data-centric%20perspective.%20Our%20codes%20are%0Aavailable%20at%20https%3A//github.com/caihuaiguang/CHG-Shapley-for-Data-Valuation%20and%0Ahttps%3A//github.com/caihuaiguang/CHG-Shapley-for-Data-Selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11730v3&entry.124074799=Read"},
{"title": "Web vs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students", "author": "Aayush Kumar and Daniel Prol and Amin Alipour and Sruti Srinivasa Ragavan", "abstract": "  LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information.\n", "link": "http://arxiv.org/abs/2501.11935v2", "date": "2025-01-22", "relevancy": 1.8717, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Web%20vs.%20LLMs%3A%20An%20Empirical%20Study%20of%20Learning%20Behaviors%20of%20CS2%20Students&body=Title%3A%20Web%20vs.%20LLMs%3A%20An%20Empirical%20Study%20of%20Learning%20Behaviors%20of%20CS2%20Students%0AAuthor%3A%20Aayush%20Kumar%20and%20Daniel%20Prol%20and%20Amin%20Alipour%20and%20Sruti%20Srinivasa%20Ragavan%0AAbstract%3A%20%20%20LLMs%20such%20as%20ChatGPT%20have%20been%20widely%20adopted%20by%20students%20in%20higher%20education%0Aas%20tools%20for%20learning%20programming%20and%20related%20concepts.%20However%2C%20it%20remains%0Aunclear%20how%20effective%20students%20are%20and%20what%20strategies%20students%20use%20while%0Alearning%20with%20LLMs.%20Since%20the%20majority%20of%20students%27%20experiences%20in%20online%0Aself-learning%20have%20come%20through%20using%20search%20engines%20such%20as%20Google%2C%20evaluating%0AAI%20tools%20in%20this%20context%20can%20help%20us%20address%20these%20gaps.%20In%20this%20mixed%20methods%0Aresearch%2C%20we%20conducted%20an%20exploratory%20within-subjects%20study%20to%20understand%20how%0ACS2%20students%20learn%20programming%20concepts%20using%20both%20LLMs%20as%20well%20as%20traditional%0Aonline%20methods%20such%20as%20educational%20websites%20and%20videos%20to%20examine%20how%20students%0Aapproach%20learning%20within%20and%20across%20both%20scenarios.%20We%20discovered%20that%20students%0Afound%20it%20easier%20to%20learn%20a%20more%20difficult%20concept%20using%20traditional%20methods%0Athan%20using%20ChatGPT.%20We%20also%20found%20that%20students%20ask%20fewer%20follow-ups%20and%20use%0Amore%20keyword-based%20queries%20for%20search%20engines%20while%20their%20prompts%20to%20LLMs%20tend%0Ato%20explicitly%20ask%20for%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11935v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeb%2520vs.%2520LLMs%253A%2520An%2520Empirical%2520Study%2520of%2520Learning%2520Behaviors%2520of%2520CS2%2520Students%26entry.906535625%3DAayush%2520Kumar%2520and%2520Daniel%2520Prol%2520and%2520Amin%2520Alipour%2520and%2520Sruti%2520Srinivasa%2520Ragavan%26entry.1292438233%3D%2520%2520LLMs%2520such%2520as%2520ChatGPT%2520have%2520been%2520widely%2520adopted%2520by%2520students%2520in%2520higher%2520education%250Aas%2520tools%2520for%2520learning%2520programming%2520and%2520related%2520concepts.%2520However%252C%2520it%2520remains%250Aunclear%2520how%2520effective%2520students%2520are%2520and%2520what%2520strategies%2520students%2520use%2520while%250Alearning%2520with%2520LLMs.%2520Since%2520the%2520majority%2520of%2520students%2527%2520experiences%2520in%2520online%250Aself-learning%2520have%2520come%2520through%2520using%2520search%2520engines%2520such%2520as%2520Google%252C%2520evaluating%250AAI%2520tools%2520in%2520this%2520context%2520can%2520help%2520us%2520address%2520these%2520gaps.%2520In%2520this%2520mixed%2520methods%250Aresearch%252C%2520we%2520conducted%2520an%2520exploratory%2520within-subjects%2520study%2520to%2520understand%2520how%250ACS2%2520students%2520learn%2520programming%2520concepts%2520using%2520both%2520LLMs%2520as%2520well%2520as%2520traditional%250Aonline%2520methods%2520such%2520as%2520educational%2520websites%2520and%2520videos%2520to%2520examine%2520how%2520students%250Aapproach%2520learning%2520within%2520and%2520across%2520both%2520scenarios.%2520We%2520discovered%2520that%2520students%250Afound%2520it%2520easier%2520to%2520learn%2520a%2520more%2520difficult%2520concept%2520using%2520traditional%2520methods%250Athan%2520using%2520ChatGPT.%2520We%2520also%2520found%2520that%2520students%2520ask%2520fewer%2520follow-ups%2520and%2520use%250Amore%2520keyword-based%2520queries%2520for%2520search%2520engines%2520while%2520their%2520prompts%2520to%2520LLMs%2520tend%250Ato%2520explicitly%2520ask%2520for%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11935v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Web%20vs.%20LLMs%3A%20An%20Empirical%20Study%20of%20Learning%20Behaviors%20of%20CS2%20Students&entry.906535625=Aayush%20Kumar%20and%20Daniel%20Prol%20and%20Amin%20Alipour%20and%20Sruti%20Srinivasa%20Ragavan&entry.1292438233=%20%20LLMs%20such%20as%20ChatGPT%20have%20been%20widely%20adopted%20by%20students%20in%20higher%20education%0Aas%20tools%20for%20learning%20programming%20and%20related%20concepts.%20However%2C%20it%20remains%0Aunclear%20how%20effective%20students%20are%20and%20what%20strategies%20students%20use%20while%0Alearning%20with%20LLMs.%20Since%20the%20majority%20of%20students%27%20experiences%20in%20online%0Aself-learning%20have%20come%20through%20using%20search%20engines%20such%20as%20Google%2C%20evaluating%0AAI%20tools%20in%20this%20context%20can%20help%20us%20address%20these%20gaps.%20In%20this%20mixed%20methods%0Aresearch%2C%20we%20conducted%20an%20exploratory%20within-subjects%20study%20to%20understand%20how%0ACS2%20students%20learn%20programming%20concepts%20using%20both%20LLMs%20as%20well%20as%20traditional%0Aonline%20methods%20such%20as%20educational%20websites%20and%20videos%20to%20examine%20how%20students%0Aapproach%20learning%20within%20and%20across%20both%20scenarios.%20We%20discovered%20that%20students%0Afound%20it%20easier%20to%20learn%20a%20more%20difficult%20concept%20using%20traditional%20methods%0Athan%20using%20ChatGPT.%20We%20also%20found%20that%20students%20ask%20fewer%20follow-ups%20and%20use%0Amore%20keyword-based%20queries%20for%20search%20engines%20while%20their%20prompts%20to%20LLMs%20tend%0Ato%20explicitly%20ask%20for%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11935v2&entry.124074799=Read"},
{"title": "Correctness Assessment of Code Generated by Large Language Models Using\n  Internal Representations", "author": "Tuan-Dung Bui and Thanh Trong Vu and Thu-Trang Nguyen and Son Nguyen and Hieu Dinh Vo", "abstract": "  Ensuring the correctness of code generated by Large Language Models (LLMs)\npresents a significant challenge in AI-driven software development. Existing\napproaches predominantly rely on black-box (closed-box) approaches that\nevaluate correctness post-generation, failing to utilize the rich insights\nembedded in the LLMs' internal states during code generation. In this paper, we\nintroduce OPENIA, a novel white-box (open-box) framework that leverages these\ninternal representations to assess the correctness of LLM-generated code.\nOPENIA systematically analyzes the intermediate states of representative\nopen-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and\nMagicCoder, across diverse code generation benchmarks. Our empirical analysis\nreveals that these internal representations encode latent information, which\nstrongly correlates with the correctness of the generated code. Building on\nthese insights, OPENIA uses a white-box/open-box approach to make informed\npredictions about code correctness, offering significant advantages in\nadaptability and robustness over traditional classification-based methods and\nzero-shot approaches. Experimental results demonstrate that OPENIA consistently\noutperforms baseline models, achieving higher accuracy, precision, recall, and\nF1-Scores with up to a 2X improvement in standalone code generation and a 46%\nenhancement in repository-specific scenarios. By unlocking the potential of\nin-process signals, OPENIA paves the way for more proactive and efficient\nquality assurance mechanisms in LLM-assisted code generation.\n", "link": "http://arxiv.org/abs/2501.12934v1", "date": "2025-01-22", "relevancy": 1.8638, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4688}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correctness%20Assessment%20of%20Code%20Generated%20by%20Large%20Language%20Models%20Using%0A%20%20Internal%20Representations&body=Title%3A%20Correctness%20Assessment%20of%20Code%20Generated%20by%20Large%20Language%20Models%20Using%0A%20%20Internal%20Representations%0AAuthor%3A%20Tuan-Dung%20Bui%20and%20Thanh%20Trong%20Vu%20and%20Thu-Trang%20Nguyen%20and%20Son%20Nguyen%20and%20Hieu%20Dinh%20Vo%0AAbstract%3A%20%20%20Ensuring%20the%20correctness%20of%20code%20generated%20by%20Large%20Language%20Models%20%28LLMs%29%0Apresents%20a%20significant%20challenge%20in%20AI-driven%20software%20development.%20Existing%0Aapproaches%20predominantly%20rely%20on%20black-box%20%28closed-box%29%20approaches%20that%0Aevaluate%20correctness%20post-generation%2C%20failing%20to%20utilize%20the%20rich%20insights%0Aembedded%20in%20the%20LLMs%27%20internal%20states%20during%20code%20generation.%20In%20this%20paper%2C%20we%0Aintroduce%20OPENIA%2C%20a%20novel%20white-box%20%28open-box%29%20framework%20that%20leverages%20these%0Ainternal%20representations%20to%20assess%20the%20correctness%20of%20LLM-generated%20code.%0AOPENIA%20systematically%20analyzes%20the%20intermediate%20states%20of%20representative%0Aopen-source%20LLMs%20specialized%20for%20code%2C%20including%20DeepSeek-Coder%2C%20CodeLlama%2C%20and%0AMagicCoder%2C%20across%20diverse%20code%20generation%20benchmarks.%20Our%20empirical%20analysis%0Areveals%20that%20these%20internal%20representations%20encode%20latent%20information%2C%20which%0Astrongly%20correlates%20with%20the%20correctness%20of%20the%20generated%20code.%20Building%20on%0Athese%20insights%2C%20OPENIA%20uses%20a%20white-box/open-box%20approach%20to%20make%20informed%0Apredictions%20about%20code%20correctness%2C%20offering%20significant%20advantages%20in%0Aadaptability%20and%20robustness%20over%20traditional%20classification-based%20methods%20and%0Azero-shot%20approaches.%20Experimental%20results%20demonstrate%20that%20OPENIA%20consistently%0Aoutperforms%20baseline%20models%2C%20achieving%20higher%20accuracy%2C%20precision%2C%20recall%2C%20and%0AF1-Scores%20with%20up%20to%20a%202X%20improvement%20in%20standalone%20code%20generation%20and%20a%2046%25%0Aenhancement%20in%20repository-specific%20scenarios.%20By%20unlocking%20the%20potential%20of%0Ain-process%20signals%2C%20OPENIA%20paves%20the%20way%20for%20more%20proactive%20and%20efficient%0Aquality%20assurance%20mechanisms%20in%20LLM-assisted%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrectness%2520Assessment%2520of%2520Code%2520Generated%2520by%2520Large%2520Language%2520Models%2520Using%250A%2520%2520Internal%2520Representations%26entry.906535625%3DTuan-Dung%2520Bui%2520and%2520Thanh%2520Trong%2520Vu%2520and%2520Thu-Trang%2520Nguyen%2520and%2520Son%2520Nguyen%2520and%2520Hieu%2520Dinh%2520Vo%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520correctness%2520of%2520code%2520generated%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Apresents%2520a%2520significant%2520challenge%2520in%2520AI-driven%2520software%2520development.%2520Existing%250Aapproaches%2520predominantly%2520rely%2520on%2520black-box%2520%2528closed-box%2529%2520approaches%2520that%250Aevaluate%2520correctness%2520post-generation%252C%2520failing%2520to%2520utilize%2520the%2520rich%2520insights%250Aembedded%2520in%2520the%2520LLMs%2527%2520internal%2520states%2520during%2520code%2520generation.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520OPENIA%252C%2520a%2520novel%2520white-box%2520%2528open-box%2529%2520framework%2520that%2520leverages%2520these%250Ainternal%2520representations%2520to%2520assess%2520the%2520correctness%2520of%2520LLM-generated%2520code.%250AOPENIA%2520systematically%2520analyzes%2520the%2520intermediate%2520states%2520of%2520representative%250Aopen-source%2520LLMs%2520specialized%2520for%2520code%252C%2520including%2520DeepSeek-Coder%252C%2520CodeLlama%252C%2520and%250AMagicCoder%252C%2520across%2520diverse%2520code%2520generation%2520benchmarks.%2520Our%2520empirical%2520analysis%250Areveals%2520that%2520these%2520internal%2520representations%2520encode%2520latent%2520information%252C%2520which%250Astrongly%2520correlates%2520with%2520the%2520correctness%2520of%2520the%2520generated%2520code.%2520Building%2520on%250Athese%2520insights%252C%2520OPENIA%2520uses%2520a%2520white-box/open-box%2520approach%2520to%2520make%2520informed%250Apredictions%2520about%2520code%2520correctness%252C%2520offering%2520significant%2520advantages%2520in%250Aadaptability%2520and%2520robustness%2520over%2520traditional%2520classification-based%2520methods%2520and%250Azero-shot%2520approaches.%2520Experimental%2520results%2520demonstrate%2520that%2520OPENIA%2520consistently%250Aoutperforms%2520baseline%2520models%252C%2520achieving%2520higher%2520accuracy%252C%2520precision%252C%2520recall%252C%2520and%250AF1-Scores%2520with%2520up%2520to%2520a%25202X%2520improvement%2520in%2520standalone%2520code%2520generation%2520and%2520a%252046%2525%250Aenhancement%2520in%2520repository-specific%2520scenarios.%2520By%2520unlocking%2520the%2520potential%2520of%250Ain-process%2520signals%252C%2520OPENIA%2520paves%2520the%2520way%2520for%2520more%2520proactive%2520and%2520efficient%250Aquality%2520assurance%2520mechanisms%2520in%2520LLM-assisted%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correctness%20Assessment%20of%20Code%20Generated%20by%20Large%20Language%20Models%20Using%0A%20%20Internal%20Representations&entry.906535625=Tuan-Dung%20Bui%20and%20Thanh%20Trong%20Vu%20and%20Thu-Trang%20Nguyen%20and%20Son%20Nguyen%20and%20Hieu%20Dinh%20Vo&entry.1292438233=%20%20Ensuring%20the%20correctness%20of%20code%20generated%20by%20Large%20Language%20Models%20%28LLMs%29%0Apresents%20a%20significant%20challenge%20in%20AI-driven%20software%20development.%20Existing%0Aapproaches%20predominantly%20rely%20on%20black-box%20%28closed-box%29%20approaches%20that%0Aevaluate%20correctness%20post-generation%2C%20failing%20to%20utilize%20the%20rich%20insights%0Aembedded%20in%20the%20LLMs%27%20internal%20states%20during%20code%20generation.%20In%20this%20paper%2C%20we%0Aintroduce%20OPENIA%2C%20a%20novel%20white-box%20%28open-box%29%20framework%20that%20leverages%20these%0Ainternal%20representations%20to%20assess%20the%20correctness%20of%20LLM-generated%20code.%0AOPENIA%20systematically%20analyzes%20the%20intermediate%20states%20of%20representative%0Aopen-source%20LLMs%20specialized%20for%20code%2C%20including%20DeepSeek-Coder%2C%20CodeLlama%2C%20and%0AMagicCoder%2C%20across%20diverse%20code%20generation%20benchmarks.%20Our%20empirical%20analysis%0Areveals%20that%20these%20internal%20representations%20encode%20latent%20information%2C%20which%0Astrongly%20correlates%20with%20the%20correctness%20of%20the%20generated%20code.%20Building%20on%0Athese%20insights%2C%20OPENIA%20uses%20a%20white-box/open-box%20approach%20to%20make%20informed%0Apredictions%20about%20code%20correctness%2C%20offering%20significant%20advantages%20in%0Aadaptability%20and%20robustness%20over%20traditional%20classification-based%20methods%20and%0Azero-shot%20approaches.%20Experimental%20results%20demonstrate%20that%20OPENIA%20consistently%0Aoutperforms%20baseline%20models%2C%20achieving%20higher%20accuracy%2C%20precision%2C%20recall%2C%20and%0AF1-Scores%20with%20up%20to%20a%202X%20improvement%20in%20standalone%20code%20generation%20and%20a%2046%25%0Aenhancement%20in%20repository-specific%20scenarios.%20By%20unlocking%20the%20potential%20of%0Ain-process%20signals%2C%20OPENIA%20paves%20the%20way%20for%20more%20proactive%20and%20efficient%0Aquality%20assurance%20mechanisms%20in%20LLM-assisted%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12934v1&entry.124074799=Read"},
{"title": "FlanEC: Exploring Flan-T5 for Post-ASR Error Correction", "author": "Moreno La Quatra and Valerio Mario Salerno and Yu Tsao and Sabato Marco Siniscalchi", "abstract": "  In this paper, we present an encoder-decoder model leveraging Flan-T5 for\npost-Automatic Speech Recognition (ASR) Generative Speech Error Correction\n(GenSEC), and we refer to it as FlanEC. We explore its application within the\nGenSEC framework to enhance ASR outputs by mapping n-best hypotheses into a\nsingle output sentence. By utilizing n-best lists from ASR models, we aim to\nimprove the linguistic correctness, accuracy, and grammaticality of final ASR\ntranscriptions. Specifically, we investigate whether scaling the training data\nand incorporating diverse datasets can lead to significant improvements in\npost-ASR error correction. We evaluate FlanEC using the HyPoradise dataset,\nproviding a comprehensive analysis of the model's effectiveness in this domain.\nFurthermore, we assess the proposed approach under different settings to\nevaluate model scalability and efficiency, offering valuable insights into the\npotential of instruction-tuned encoder-decoder models for this task.\n", "link": "http://arxiv.org/abs/2501.12979v1", "date": "2025-01-22", "relevancy": 1.8611, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlanEC%3A%20Exploring%20Flan-T5%20for%20Post-ASR%20Error%20Correction&body=Title%3A%20FlanEC%3A%20Exploring%20Flan-T5%20for%20Post-ASR%20Error%20Correction%0AAuthor%3A%20Moreno%20La%20Quatra%20and%20Valerio%20Mario%20Salerno%20and%20Yu%20Tsao%20and%20Sabato%20Marco%20Siniscalchi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20an%20encoder-decoder%20model%20leveraging%20Flan-T5%20for%0Apost-Automatic%20Speech%20Recognition%20%28ASR%29%20Generative%20Speech%20Error%20Correction%0A%28GenSEC%29%2C%20and%20we%20refer%20to%20it%20as%20FlanEC.%20We%20explore%20its%20application%20within%20the%0AGenSEC%20framework%20to%20enhance%20ASR%20outputs%20by%20mapping%20n-best%20hypotheses%20into%20a%0Asingle%20output%20sentence.%20By%20utilizing%20n-best%20lists%20from%20ASR%20models%2C%20we%20aim%20to%0Aimprove%20the%20linguistic%20correctness%2C%20accuracy%2C%20and%20grammaticality%20of%20final%20ASR%0Atranscriptions.%20Specifically%2C%20we%20investigate%20whether%20scaling%20the%20training%20data%0Aand%20incorporating%20diverse%20datasets%20can%20lead%20to%20significant%20improvements%20in%0Apost-ASR%20error%20correction.%20We%20evaluate%20FlanEC%20using%20the%20HyPoradise%20dataset%2C%0Aproviding%20a%20comprehensive%20analysis%20of%20the%20model%27s%20effectiveness%20in%20this%20domain.%0AFurthermore%2C%20we%20assess%20the%20proposed%20approach%20under%20different%20settings%20to%0Aevaluate%20model%20scalability%20and%20efficiency%2C%20offering%20valuable%20insights%20into%20the%0Apotential%20of%20instruction-tuned%20encoder-decoder%20models%20for%20this%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlanEC%253A%2520Exploring%2520Flan-T5%2520for%2520Post-ASR%2520Error%2520Correction%26entry.906535625%3DMoreno%2520La%2520Quatra%2520and%2520Valerio%2520Mario%2520Salerno%2520and%2520Yu%2520Tsao%2520and%2520Sabato%2520Marco%2520Siniscalchi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520encoder-decoder%2520model%2520leveraging%2520Flan-T5%2520for%250Apost-Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%2520Generative%2520Speech%2520Error%2520Correction%250A%2528GenSEC%2529%252C%2520and%2520we%2520refer%2520to%2520it%2520as%2520FlanEC.%2520We%2520explore%2520its%2520application%2520within%2520the%250AGenSEC%2520framework%2520to%2520enhance%2520ASR%2520outputs%2520by%2520mapping%2520n-best%2520hypotheses%2520into%2520a%250Asingle%2520output%2520sentence.%2520By%2520utilizing%2520n-best%2520lists%2520from%2520ASR%2520models%252C%2520we%2520aim%2520to%250Aimprove%2520the%2520linguistic%2520correctness%252C%2520accuracy%252C%2520and%2520grammaticality%2520of%2520final%2520ASR%250Atranscriptions.%2520Specifically%252C%2520we%2520investigate%2520whether%2520scaling%2520the%2520training%2520data%250Aand%2520incorporating%2520diverse%2520datasets%2520can%2520lead%2520to%2520significant%2520improvements%2520in%250Apost-ASR%2520error%2520correction.%2520We%2520evaluate%2520FlanEC%2520using%2520the%2520HyPoradise%2520dataset%252C%250Aproviding%2520a%2520comprehensive%2520analysis%2520of%2520the%2520model%2527s%2520effectiveness%2520in%2520this%2520domain.%250AFurthermore%252C%2520we%2520assess%2520the%2520proposed%2520approach%2520under%2520different%2520settings%2520to%250Aevaluate%2520model%2520scalability%2520and%2520efficiency%252C%2520offering%2520valuable%2520insights%2520into%2520the%250Apotential%2520of%2520instruction-tuned%2520encoder-decoder%2520models%2520for%2520this%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlanEC%3A%20Exploring%20Flan-T5%20for%20Post-ASR%20Error%20Correction&entry.906535625=Moreno%20La%20Quatra%20and%20Valerio%20Mario%20Salerno%20and%20Yu%20Tsao%20and%20Sabato%20Marco%20Siniscalchi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20an%20encoder-decoder%20model%20leveraging%20Flan-T5%20for%0Apost-Automatic%20Speech%20Recognition%20%28ASR%29%20Generative%20Speech%20Error%20Correction%0A%28GenSEC%29%2C%20and%20we%20refer%20to%20it%20as%20FlanEC.%20We%20explore%20its%20application%20within%20the%0AGenSEC%20framework%20to%20enhance%20ASR%20outputs%20by%20mapping%20n-best%20hypotheses%20into%20a%0Asingle%20output%20sentence.%20By%20utilizing%20n-best%20lists%20from%20ASR%20models%2C%20we%20aim%20to%0Aimprove%20the%20linguistic%20correctness%2C%20accuracy%2C%20and%20grammaticality%20of%20final%20ASR%0Atranscriptions.%20Specifically%2C%20we%20investigate%20whether%20scaling%20the%20training%20data%0Aand%20incorporating%20diverse%20datasets%20can%20lead%20to%20significant%20improvements%20in%0Apost-ASR%20error%20correction.%20We%20evaluate%20FlanEC%20using%20the%20HyPoradise%20dataset%2C%0Aproviding%20a%20comprehensive%20analysis%20of%20the%20model%27s%20effectiveness%20in%20this%20domain.%0AFurthermore%2C%20we%20assess%20the%20proposed%20approach%20under%20different%20settings%20to%0Aevaluate%20model%20scalability%20and%20efficiency%2C%20offering%20valuable%20insights%20into%20the%0Apotential%20of%20instruction-tuned%20encoder-decoder%20models%20for%20this%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12979v1&entry.124074799=Read"},
{"title": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback", "author": "Matthew Gwilliam and Han Cai and Di Wu and Abhinav Shrivastava and Zhiyu Cheng", "abstract": "  We propose Inner Loop Feedback (ILF), a novel approach to accelerate\ndiffusion models' inference. ILF trains a lightweight module to predict future\nfeatures in the denoising process by leveraging the outputs from a chosen\ndiffusion backbone block at a given time step. This approach exploits two key\nintuitions; (1) the outputs of a given block at adjacent time steps are\nsimilar, and (2) performing partial computations for a step imposes a lower\nburden on the model than skipping the step entirely. Our method is highly\nflexible, since we find that the feedback module itself can simply be a block\nfrom the diffusion backbone, with all settings copied. Its influence on the\ndiffusion forward can be tempered with a learnable scaling factor from zero\ninitialization. We train this module using distillation losses; however, unlike\nsome prior work where a full diffusion backbone serves as the student, our\nmodel freezes the backbone, training only the feedback module. While many\nefforts to optimize diffusion models focus on achieving acceptable image\nquality in extremely few steps (1-4 steps), our emphasis is on matching best\ncase results (typically achieved in 20 steps) while significantly reducing\nruntime. ILF achieves this balance effectively, demonstrating strong\nperformance for both class-to-image generation with diffusion transformer (DiT)\nand text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The\nquality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP\nImage Quality Assessment, ImageReward, and qualitative comparisons.\n", "link": "http://arxiv.org/abs/2501.13107v1", "date": "2025-01-22", "relevancy": 1.8575, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6665}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6336}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerate%20High-Quality%20Diffusion%20Models%20with%20Inner%20Loop%20Feedback&body=Title%3A%20Accelerate%20High-Quality%20Diffusion%20Models%20with%20Inner%20Loop%20Feedback%0AAuthor%3A%20Matthew%20Gwilliam%20and%20Han%20Cai%20and%20Di%20Wu%20and%20Abhinav%20Shrivastava%20and%20Zhiyu%20Cheng%0AAbstract%3A%20%20%20We%20propose%20Inner%20Loop%20Feedback%20%28ILF%29%2C%20a%20novel%20approach%20to%20accelerate%0Adiffusion%20models%27%20inference.%20ILF%20trains%20a%20lightweight%20module%20to%20predict%20future%0Afeatures%20in%20the%20denoising%20process%20by%20leveraging%20the%20outputs%20from%20a%20chosen%0Adiffusion%20backbone%20block%20at%20a%20given%20time%20step.%20This%20approach%20exploits%20two%20key%0Aintuitions%3B%20%281%29%20the%20outputs%20of%20a%20given%20block%20at%20adjacent%20time%20steps%20are%0Asimilar%2C%20and%20%282%29%20performing%20partial%20computations%20for%20a%20step%20imposes%20a%20lower%0Aburden%20on%20the%20model%20than%20skipping%20the%20step%20entirely.%20Our%20method%20is%20highly%0Aflexible%2C%20since%20we%20find%20that%20the%20feedback%20module%20itself%20can%20simply%20be%20a%20block%0Afrom%20the%20diffusion%20backbone%2C%20with%20all%20settings%20copied.%20Its%20influence%20on%20the%0Adiffusion%20forward%20can%20be%20tempered%20with%20a%20learnable%20scaling%20factor%20from%20zero%0Ainitialization.%20We%20train%20this%20module%20using%20distillation%20losses%3B%20however%2C%20unlike%0Asome%20prior%20work%20where%20a%20full%20diffusion%20backbone%20serves%20as%20the%20student%2C%20our%0Amodel%20freezes%20the%20backbone%2C%20training%20only%20the%20feedback%20module.%20While%20many%0Aefforts%20to%20optimize%20diffusion%20models%20focus%20on%20achieving%20acceptable%20image%0Aquality%20in%20extremely%20few%20steps%20%281-4%20steps%29%2C%20our%20emphasis%20is%20on%20matching%20best%0Acase%20results%20%28typically%20achieved%20in%2020%20steps%29%20while%20significantly%20reducing%0Aruntime.%20ILF%20achieves%20this%20balance%20effectively%2C%20demonstrating%20strong%0Aperformance%20for%20both%20class-to-image%20generation%20with%20diffusion%20transformer%20%28DiT%29%0Aand%20text-to-image%20generation%20with%20DiT-based%20PixArt-alpha%20and%20PixArt-sigma.%20The%0Aquality%20of%20ILF%27s%201.7x-1.8x%20speedups%20are%20confirmed%20by%20FID%2C%20CLIP%20score%2C%20CLIP%0AImage%20Quality%20Assessment%2C%20ImageReward%2C%20and%20qualitative%20comparisons.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerate%2520High-Quality%2520Diffusion%2520Models%2520with%2520Inner%2520Loop%2520Feedback%26entry.906535625%3DMatthew%2520Gwilliam%2520and%2520Han%2520Cai%2520and%2520Di%2520Wu%2520and%2520Abhinav%2520Shrivastava%2520and%2520Zhiyu%2520Cheng%26entry.1292438233%3D%2520%2520We%2520propose%2520Inner%2520Loop%2520Feedback%2520%2528ILF%2529%252C%2520a%2520novel%2520approach%2520to%2520accelerate%250Adiffusion%2520models%2527%2520inference.%2520ILF%2520trains%2520a%2520lightweight%2520module%2520to%2520predict%2520future%250Afeatures%2520in%2520the%2520denoising%2520process%2520by%2520leveraging%2520the%2520outputs%2520from%2520a%2520chosen%250Adiffusion%2520backbone%2520block%2520at%2520a%2520given%2520time%2520step.%2520This%2520approach%2520exploits%2520two%2520key%250Aintuitions%253B%2520%25281%2529%2520the%2520outputs%2520of%2520a%2520given%2520block%2520at%2520adjacent%2520time%2520steps%2520are%250Asimilar%252C%2520and%2520%25282%2529%2520performing%2520partial%2520computations%2520for%2520a%2520step%2520imposes%2520a%2520lower%250Aburden%2520on%2520the%2520model%2520than%2520skipping%2520the%2520step%2520entirely.%2520Our%2520method%2520is%2520highly%250Aflexible%252C%2520since%2520we%2520find%2520that%2520the%2520feedback%2520module%2520itself%2520can%2520simply%2520be%2520a%2520block%250Afrom%2520the%2520diffusion%2520backbone%252C%2520with%2520all%2520settings%2520copied.%2520Its%2520influence%2520on%2520the%250Adiffusion%2520forward%2520can%2520be%2520tempered%2520with%2520a%2520learnable%2520scaling%2520factor%2520from%2520zero%250Ainitialization.%2520We%2520train%2520this%2520module%2520using%2520distillation%2520losses%253B%2520however%252C%2520unlike%250Asome%2520prior%2520work%2520where%2520a%2520full%2520diffusion%2520backbone%2520serves%2520as%2520the%2520student%252C%2520our%250Amodel%2520freezes%2520the%2520backbone%252C%2520training%2520only%2520the%2520feedback%2520module.%2520While%2520many%250Aefforts%2520to%2520optimize%2520diffusion%2520models%2520focus%2520on%2520achieving%2520acceptable%2520image%250Aquality%2520in%2520extremely%2520few%2520steps%2520%25281-4%2520steps%2529%252C%2520our%2520emphasis%2520is%2520on%2520matching%2520best%250Acase%2520results%2520%2528typically%2520achieved%2520in%252020%2520steps%2529%2520while%2520significantly%2520reducing%250Aruntime.%2520ILF%2520achieves%2520this%2520balance%2520effectively%252C%2520demonstrating%2520strong%250Aperformance%2520for%2520both%2520class-to-image%2520generation%2520with%2520diffusion%2520transformer%2520%2528DiT%2529%250Aand%2520text-to-image%2520generation%2520with%2520DiT-based%2520PixArt-alpha%2520and%2520PixArt-sigma.%2520The%250Aquality%2520of%2520ILF%2527s%25201.7x-1.8x%2520speedups%2520are%2520confirmed%2520by%2520FID%252C%2520CLIP%2520score%252C%2520CLIP%250AImage%2520Quality%2520Assessment%252C%2520ImageReward%252C%2520and%2520qualitative%2520comparisons.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerate%20High-Quality%20Diffusion%20Models%20with%20Inner%20Loop%20Feedback&entry.906535625=Matthew%20Gwilliam%20and%20Han%20Cai%20and%20Di%20Wu%20and%20Abhinav%20Shrivastava%20and%20Zhiyu%20Cheng&entry.1292438233=%20%20We%20propose%20Inner%20Loop%20Feedback%20%28ILF%29%2C%20a%20novel%20approach%20to%20accelerate%0Adiffusion%20models%27%20inference.%20ILF%20trains%20a%20lightweight%20module%20to%20predict%20future%0Afeatures%20in%20the%20denoising%20process%20by%20leveraging%20the%20outputs%20from%20a%20chosen%0Adiffusion%20backbone%20block%20at%20a%20given%20time%20step.%20This%20approach%20exploits%20two%20key%0Aintuitions%3B%20%281%29%20the%20outputs%20of%20a%20given%20block%20at%20adjacent%20time%20steps%20are%0Asimilar%2C%20and%20%282%29%20performing%20partial%20computations%20for%20a%20step%20imposes%20a%20lower%0Aburden%20on%20the%20model%20than%20skipping%20the%20step%20entirely.%20Our%20method%20is%20highly%0Aflexible%2C%20since%20we%20find%20that%20the%20feedback%20module%20itself%20can%20simply%20be%20a%20block%0Afrom%20the%20diffusion%20backbone%2C%20with%20all%20settings%20copied.%20Its%20influence%20on%20the%0Adiffusion%20forward%20can%20be%20tempered%20with%20a%20learnable%20scaling%20factor%20from%20zero%0Ainitialization.%20We%20train%20this%20module%20using%20distillation%20losses%3B%20however%2C%20unlike%0Asome%20prior%20work%20where%20a%20full%20diffusion%20backbone%20serves%20as%20the%20student%2C%20our%0Amodel%20freezes%20the%20backbone%2C%20training%20only%20the%20feedback%20module.%20While%20many%0Aefforts%20to%20optimize%20diffusion%20models%20focus%20on%20achieving%20acceptable%20image%0Aquality%20in%20extremely%20few%20steps%20%281-4%20steps%29%2C%20our%20emphasis%20is%20on%20matching%20best%0Acase%20results%20%28typically%20achieved%20in%2020%20steps%29%20while%20significantly%20reducing%0Aruntime.%20ILF%20achieves%20this%20balance%20effectively%2C%20demonstrating%20strong%0Aperformance%20for%20both%20class-to-image%20generation%20with%20diffusion%20transformer%20%28DiT%29%0Aand%20text-to-image%20generation%20with%20DiT-based%20PixArt-alpha%20and%20PixArt-sigma.%20The%0Aquality%20of%20ILF%27s%201.7x-1.8x%20speedups%20are%20confirmed%20by%20FID%2C%20CLIP%20score%2C%20CLIP%0AImage%20Quality%20Assessment%2C%20ImageReward%2C%20and%20qualitative%20comparisons.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13107v1&entry.124074799=Read"},
{"title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks", "author": "Felipe Marra and Lucas N. Ferreira", "abstract": "  This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.\n", "link": "http://arxiv.org/abs/2411.03948v2", "date": "2025-01-22", "relevancy": 1.8573, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4938}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4521}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-Form%20Text-to-Music%20Generation%20with%20Adaptive%20Prompts%3A%20A%20Case%20of%0A%20%20Study%20in%20Tabletop%20Role-Playing%20Games%20Soundtracks&body=Title%3A%20Long-Form%20Text-to-Music%20Generation%20with%20Adaptive%20Prompts%3A%20A%20Case%20of%0A%20%20Study%20in%20Tabletop%20Role-Playing%20Games%20Soundtracks%0AAuthor%3A%20Felipe%20Marra%20and%20Lucas%20N.%20Ferreira%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20capabilities%20of%20text-to-audio%20music%20generation%0Amodels%20in%20producing%20long-form%20music%20with%20prompts%20that%20change%20over%20time%2C%0Afocusing%20on%20soundtrack%20generation%20for%20Tabletop%20Role-Playing%20Games%20%28TRPGs%29.%20We%0Aintroduce%20Babel%20Bardo%2C%20a%20system%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20to%0Atransform%20speech%20transcriptions%20into%20music%20descriptions%20for%20controlling%20a%0Atext-to-music%20model.%20Four%20versions%20of%20Babel%20Bardo%20were%20compared%20in%20two%20TRPG%0Acampaigns%3A%20a%20baseline%20using%20direct%20speech%20transcriptions%2C%20and%20three%20LLM-based%0Aversions%20with%20varying%20approaches%20to%20music%20description%20generation.%20Evaluations%0Aconsidered%20audio%20quality%2C%20story%20alignment%2C%20and%20transition%20smoothness.%20Results%0Aindicate%20that%20detailed%20music%20descriptions%20improve%20audio%20quality%20while%0Amaintaining%20consistency%20across%20consecutive%20descriptions%20enhances%20story%0Aalignment%20and%20transition%20smoothness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03948v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-Form%2520Text-to-Music%2520Generation%2520with%2520Adaptive%2520Prompts%253A%2520A%2520Case%2520of%250A%2520%2520Study%2520in%2520Tabletop%2520Role-Playing%2520Games%2520Soundtracks%26entry.906535625%3DFelipe%2520Marra%2520and%2520Lucas%2520N.%2520Ferreira%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520capabilities%2520of%2520text-to-audio%2520music%2520generation%250Amodels%2520in%2520producing%2520long-form%2520music%2520with%2520prompts%2520that%2520change%2520over%2520time%252C%250Afocusing%2520on%2520soundtrack%2520generation%2520for%2520Tabletop%2520Role-Playing%2520Games%2520%2528TRPGs%2529.%2520We%250Aintroduce%2520Babel%2520Bardo%252C%2520a%2520system%2520that%2520uses%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Atransform%2520speech%2520transcriptions%2520into%2520music%2520descriptions%2520for%2520controlling%2520a%250Atext-to-music%2520model.%2520Four%2520versions%2520of%2520Babel%2520Bardo%2520were%2520compared%2520in%2520two%2520TRPG%250Acampaigns%253A%2520a%2520baseline%2520using%2520direct%2520speech%2520transcriptions%252C%2520and%2520three%2520LLM-based%250Aversions%2520with%2520varying%2520approaches%2520to%2520music%2520description%2520generation.%2520Evaluations%250Aconsidered%2520audio%2520quality%252C%2520story%2520alignment%252C%2520and%2520transition%2520smoothness.%2520Results%250Aindicate%2520that%2520detailed%2520music%2520descriptions%2520improve%2520audio%2520quality%2520while%250Amaintaining%2520consistency%2520across%2520consecutive%2520descriptions%2520enhances%2520story%250Aalignment%2520and%2520transition%2520smoothness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03948v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-Form%20Text-to-Music%20Generation%20with%20Adaptive%20Prompts%3A%20A%20Case%20of%0A%20%20Study%20in%20Tabletop%20Role-Playing%20Games%20Soundtracks&entry.906535625=Felipe%20Marra%20and%20Lucas%20N.%20Ferreira&entry.1292438233=%20%20This%20paper%20investigates%20the%20capabilities%20of%20text-to-audio%20music%20generation%0Amodels%20in%20producing%20long-form%20music%20with%20prompts%20that%20change%20over%20time%2C%0Afocusing%20on%20soundtrack%20generation%20for%20Tabletop%20Role-Playing%20Games%20%28TRPGs%29.%20We%0Aintroduce%20Babel%20Bardo%2C%20a%20system%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20to%0Atransform%20speech%20transcriptions%20into%20music%20descriptions%20for%20controlling%20a%0Atext-to-music%20model.%20Four%20versions%20of%20Babel%20Bardo%20were%20compared%20in%20two%20TRPG%0Acampaigns%3A%20a%20baseline%20using%20direct%20speech%20transcriptions%2C%20and%20three%20LLM-based%0Aversions%20with%20varying%20approaches%20to%20music%20description%20generation.%20Evaluations%0Aconsidered%20audio%20quality%2C%20story%20alignment%2C%20and%20transition%20smoothness.%20Results%0Aindicate%20that%20detailed%20music%20descriptions%20improve%20audio%20quality%20while%0Amaintaining%20consistency%20across%20consecutive%20descriptions%20enhances%20story%0Aalignment%20and%20transition%20smoothness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03948v2&entry.124074799=Read"},
{"title": "Autonomy-of-Experts Models", "author": "Ang Lv and Ruobing Xie and Yining Qian and Songhao Wu and Xingwu Sun and Zhanhui Kang and Di Wang and Rui Yan", "abstract": "  Mixture-of-Experts (MoE) models mostly use a router to assign tokens to\nspecific expert modules, activating only partial parameters and often\noutperforming dense models. We argue that the separation between the router's\ndecision-making and the experts' execution is a critical yet overlooked issue,\nleading to suboptimal expert selection and ineffective learning. To address\nthis, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which\nexperts autonomously select themselves to process inputs. AoE is based on the\ninsight that an expert is aware of its own capacity to effectively process a\ntoken, an awareness reflected in the scale of its internal activations. In AoE,\nrouters are removed; instead, experts pre-compute internal activations for\ninputs and are ranked based on their activation norms. Only the top-ranking\nexperts proceed with the forward pass, while the others abort. The overhead of\npre-computing activations is reduced through a low-rank weight factorization.\nThis self-evaluating-then-partner-comparing approach ensures improved expert\nselection and effective learning. We pre-train language models having 700M up\nto 4B parameters, demonstrating that AoE outperforms traditional MoE models\nwith comparable efficiency.\n", "link": "http://arxiv.org/abs/2501.13074v1", "date": "2025-01-22", "relevancy": 1.8366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.46}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomy-of-Experts%20Models&body=Title%3A%20Autonomy-of-Experts%20Models%0AAuthor%3A%20Ang%20Lv%20and%20Ruobing%20Xie%20and%20Yining%20Qian%20and%20Songhao%20Wu%20and%20Xingwu%20Sun%20and%20Zhanhui%20Kang%20and%20Di%20Wang%20and%20Rui%20Yan%0AAbstract%3A%20%20%20Mixture-of-Experts%20%28MoE%29%20models%20mostly%20use%20a%20router%20to%20assign%20tokens%20to%0Aspecific%20expert%20modules%2C%20activating%20only%20partial%20parameters%20and%20often%0Aoutperforming%20dense%20models.%20We%20argue%20that%20the%20separation%20between%20the%20router%27s%0Adecision-making%20and%20the%20experts%27%20execution%20is%20a%20critical%20yet%20overlooked%20issue%2C%0Aleading%20to%20suboptimal%20expert%20selection%20and%20ineffective%20learning.%20To%20address%0Athis%2C%20we%20propose%20Autonomy-of-Experts%20%28AoE%29%2C%20a%20novel%20MoE%20paradigm%20in%20which%0Aexperts%20autonomously%20select%20themselves%20to%20process%20inputs.%20AoE%20is%20based%20on%20the%0Ainsight%20that%20an%20expert%20is%20aware%20of%20its%20own%20capacity%20to%20effectively%20process%20a%0Atoken%2C%20an%20awareness%20reflected%20in%20the%20scale%20of%20its%20internal%20activations.%20In%20AoE%2C%0Arouters%20are%20removed%3B%20instead%2C%20experts%20pre-compute%20internal%20activations%20for%0Ainputs%20and%20are%20ranked%20based%20on%20their%20activation%20norms.%20Only%20the%20top-ranking%0Aexperts%20proceed%20with%20the%20forward%20pass%2C%20while%20the%20others%20abort.%20The%20overhead%20of%0Apre-computing%20activations%20is%20reduced%20through%20a%20low-rank%20weight%20factorization.%0AThis%20self-evaluating-then-partner-comparing%20approach%20ensures%20improved%20expert%0Aselection%20and%20effective%20learning.%20We%20pre-train%20language%20models%20having%20700M%20up%0Ato%204B%20parameters%2C%20demonstrating%20that%20AoE%20outperforms%20traditional%20MoE%20models%0Awith%20comparable%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomy-of-Experts%2520Models%26entry.906535625%3DAng%2520Lv%2520and%2520Ruobing%2520Xie%2520and%2520Yining%2520Qian%2520and%2520Songhao%2520Wu%2520and%2520Xingwu%2520Sun%2520and%2520Zhanhui%2520Kang%2520and%2520Di%2520Wang%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520Mixture-of-Experts%2520%2528MoE%2529%2520models%2520mostly%2520use%2520a%2520router%2520to%2520assign%2520tokens%2520to%250Aspecific%2520expert%2520modules%252C%2520activating%2520only%2520partial%2520parameters%2520and%2520often%250Aoutperforming%2520dense%2520models.%2520We%2520argue%2520that%2520the%2520separation%2520between%2520the%2520router%2527s%250Adecision-making%2520and%2520the%2520experts%2527%2520execution%2520is%2520a%2520critical%2520yet%2520overlooked%2520issue%252C%250Aleading%2520to%2520suboptimal%2520expert%2520selection%2520and%2520ineffective%2520learning.%2520To%2520address%250Athis%252C%2520we%2520propose%2520Autonomy-of-Experts%2520%2528AoE%2529%252C%2520a%2520novel%2520MoE%2520paradigm%2520in%2520which%250Aexperts%2520autonomously%2520select%2520themselves%2520to%2520process%2520inputs.%2520AoE%2520is%2520based%2520on%2520the%250Ainsight%2520that%2520an%2520expert%2520is%2520aware%2520of%2520its%2520own%2520capacity%2520to%2520effectively%2520process%2520a%250Atoken%252C%2520an%2520awareness%2520reflected%2520in%2520the%2520scale%2520of%2520its%2520internal%2520activations.%2520In%2520AoE%252C%250Arouters%2520are%2520removed%253B%2520instead%252C%2520experts%2520pre-compute%2520internal%2520activations%2520for%250Ainputs%2520and%2520are%2520ranked%2520based%2520on%2520their%2520activation%2520norms.%2520Only%2520the%2520top-ranking%250Aexperts%2520proceed%2520with%2520the%2520forward%2520pass%252C%2520while%2520the%2520others%2520abort.%2520The%2520overhead%2520of%250Apre-computing%2520activations%2520is%2520reduced%2520through%2520a%2520low-rank%2520weight%2520factorization.%250AThis%2520self-evaluating-then-partner-comparing%2520approach%2520ensures%2520improved%2520expert%250Aselection%2520and%2520effective%2520learning.%2520We%2520pre-train%2520language%2520models%2520having%2520700M%2520up%250Ato%25204B%2520parameters%252C%2520demonstrating%2520that%2520AoE%2520outperforms%2520traditional%2520MoE%2520models%250Awith%2520comparable%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomy-of-Experts%20Models&entry.906535625=Ang%20Lv%20and%20Ruobing%20Xie%20and%20Yining%20Qian%20and%20Songhao%20Wu%20and%20Xingwu%20Sun%20and%20Zhanhui%20Kang%20and%20Di%20Wang%20and%20Rui%20Yan&entry.1292438233=%20%20Mixture-of-Experts%20%28MoE%29%20models%20mostly%20use%20a%20router%20to%20assign%20tokens%20to%0Aspecific%20expert%20modules%2C%20activating%20only%20partial%20parameters%20and%20often%0Aoutperforming%20dense%20models.%20We%20argue%20that%20the%20separation%20between%20the%20router%27s%0Adecision-making%20and%20the%20experts%27%20execution%20is%20a%20critical%20yet%20overlooked%20issue%2C%0Aleading%20to%20suboptimal%20expert%20selection%20and%20ineffective%20learning.%20To%20address%0Athis%2C%20we%20propose%20Autonomy-of-Experts%20%28AoE%29%2C%20a%20novel%20MoE%20paradigm%20in%20which%0Aexperts%20autonomously%20select%20themselves%20to%20process%20inputs.%20AoE%20is%20based%20on%20the%0Ainsight%20that%20an%20expert%20is%20aware%20of%20its%20own%20capacity%20to%20effectively%20process%20a%0Atoken%2C%20an%20awareness%20reflected%20in%20the%20scale%20of%20its%20internal%20activations.%20In%20AoE%2C%0Arouters%20are%20removed%3B%20instead%2C%20experts%20pre-compute%20internal%20activations%20for%0Ainputs%20and%20are%20ranked%20based%20on%20their%20activation%20norms.%20Only%20the%20top-ranking%0Aexperts%20proceed%20with%20the%20forward%20pass%2C%20while%20the%20others%20abort.%20The%20overhead%20of%0Apre-computing%20activations%20is%20reduced%20through%20a%20low-rank%20weight%20factorization.%0AThis%20self-evaluating-then-partner-comparing%20approach%20ensures%20improved%20expert%0Aselection%20and%20effective%20learning.%20We%20pre-train%20language%20models%20having%20700M%20up%0Ato%204B%20parameters%2C%20demonstrating%20that%20AoE%20outperforms%20traditional%20MoE%20models%0Awith%20comparable%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13074v1&entry.124074799=Read"},
{"title": "Machine Learning Modeling for Multi-order Human Visual Motion Processing", "author": "Zitang Sun and Yen-Ju Chen and Yung-Hao Yang and Yuan Li and Shin'ya Nishida", "abstract": "  Our research aims to develop machines that learn to perceive visual motion as\ndo humans. While recent advances in computer vision (CV) have enabled DNN-based\nmodels to accurately estimate optical flow in naturalistic images, a\nsignificant disparity remains between CV models and the biological visual\nsystem in both architecture and behavior. This disparity includes humans'\nability to perceive the motion of higher-order image features (second-order\nmotion), which many CV models fail to capture because of their reliance on the\nintensity conservation law. Our model architecture mimics the cortical V1-MT\nmotion processing pathway, utilizing a trainable motion energy sensor bank and\na recurrent graph network. Supervised learning employing diverse naturalistic\nvideos allows the model to replicate psychophysical and physiological findings\nabout first-order (luminance-based) motion perception. For second-order motion,\ninspired by neuroscientific findings, the model includes an additional sensing\npathway with nonlinear preprocessing before motion energy sensing, implemented\nusing a simple multilayer 3D CNN block. When exploring how the brain acquired\nthe ability to perceive second-order motion in natural environments, in which\npure second-order signals are rare, we hypothesized that second-order\nmechanisms were critical when estimating robust object motion amidst optical\nfluctuations, such as highlights on glossy surfaces. We trained our\ndual-pathway model on novel motion datasets with varying material properties of\nmoving objects. We found that training to estimate object motion from\nnon-Lambertian materials naturally endowed the model with the capacity to\nperceive second-order motion, as can humans. The resulting model effectively\naligns with biological systems while generalizing to both first- and\nsecond-order motion phenomena in natural scenes.\n", "link": "http://arxiv.org/abs/2501.12810v1", "date": "2025-01-22", "relevancy": 1.8359, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6294}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6031}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Modeling%20for%20Multi-order%20Human%20Visual%20Motion%20Processing&body=Title%3A%20Machine%20Learning%20Modeling%20for%20Multi-order%20Human%20Visual%20Motion%20Processing%0AAuthor%3A%20Zitang%20Sun%20and%20Yen-Ju%20Chen%20and%20Yung-Hao%20Yang%20and%20Yuan%20Li%20and%20Shin%27ya%20Nishida%0AAbstract%3A%20%20%20Our%20research%20aims%20to%20develop%20machines%20that%20learn%20to%20perceive%20visual%20motion%20as%0Ado%20humans.%20While%20recent%20advances%20in%20computer%20vision%20%28CV%29%20have%20enabled%20DNN-based%0Amodels%20to%20accurately%20estimate%20optical%20flow%20in%20naturalistic%20images%2C%20a%0Asignificant%20disparity%20remains%20between%20CV%20models%20and%20the%20biological%20visual%0Asystem%20in%20both%20architecture%20and%20behavior.%20This%20disparity%20includes%20humans%27%0Aability%20to%20perceive%20the%20motion%20of%20higher-order%20image%20features%20%28second-order%0Amotion%29%2C%20which%20many%20CV%20models%20fail%20to%20capture%20because%20of%20their%20reliance%20on%20the%0Aintensity%20conservation%20law.%20Our%20model%20architecture%20mimics%20the%20cortical%20V1-MT%0Amotion%20processing%20pathway%2C%20utilizing%20a%20trainable%20motion%20energy%20sensor%20bank%20and%0Aa%20recurrent%20graph%20network.%20Supervised%20learning%20employing%20diverse%20naturalistic%0Avideos%20allows%20the%20model%20to%20replicate%20psychophysical%20and%20physiological%20findings%0Aabout%20first-order%20%28luminance-based%29%20motion%20perception.%20For%20second-order%20motion%2C%0Ainspired%20by%20neuroscientific%20findings%2C%20the%20model%20includes%20an%20additional%20sensing%0Apathway%20with%20nonlinear%20preprocessing%20before%20motion%20energy%20sensing%2C%20implemented%0Ausing%20a%20simple%20multilayer%203D%20CNN%20block.%20When%20exploring%20how%20the%20brain%20acquired%0Athe%20ability%20to%20perceive%20second-order%20motion%20in%20natural%20environments%2C%20in%20which%0Apure%20second-order%20signals%20are%20rare%2C%20we%20hypothesized%20that%20second-order%0Amechanisms%20were%20critical%20when%20estimating%20robust%20object%20motion%20amidst%20optical%0Afluctuations%2C%20such%20as%20highlights%20on%20glossy%20surfaces.%20We%20trained%20our%0Adual-pathway%20model%20on%20novel%20motion%20datasets%20with%20varying%20material%20properties%20of%0Amoving%20objects.%20We%20found%20that%20training%20to%20estimate%20object%20motion%20from%0Anon-Lambertian%20materials%20naturally%20endowed%20the%20model%20with%20the%20capacity%20to%0Aperceive%20second-order%20motion%2C%20as%20can%20humans.%20The%20resulting%20model%20effectively%0Aaligns%20with%20biological%20systems%20while%20generalizing%20to%20both%20first-%20and%0Asecond-order%20motion%20phenomena%20in%20natural%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Modeling%2520for%2520Multi-order%2520Human%2520Visual%2520Motion%2520Processing%26entry.906535625%3DZitang%2520Sun%2520and%2520Yen-Ju%2520Chen%2520and%2520Yung-Hao%2520Yang%2520and%2520Yuan%2520Li%2520and%2520Shin%2527ya%2520Nishida%26entry.1292438233%3D%2520%2520Our%2520research%2520aims%2520to%2520develop%2520machines%2520that%2520learn%2520to%2520perceive%2520visual%2520motion%2520as%250Ado%2520humans.%2520While%2520recent%2520advances%2520in%2520computer%2520vision%2520%2528CV%2529%2520have%2520enabled%2520DNN-based%250Amodels%2520to%2520accurately%2520estimate%2520optical%2520flow%2520in%2520naturalistic%2520images%252C%2520a%250Asignificant%2520disparity%2520remains%2520between%2520CV%2520models%2520and%2520the%2520biological%2520visual%250Asystem%2520in%2520both%2520architecture%2520and%2520behavior.%2520This%2520disparity%2520includes%2520humans%2527%250Aability%2520to%2520perceive%2520the%2520motion%2520of%2520higher-order%2520image%2520features%2520%2528second-order%250Amotion%2529%252C%2520which%2520many%2520CV%2520models%2520fail%2520to%2520capture%2520because%2520of%2520their%2520reliance%2520on%2520the%250Aintensity%2520conservation%2520law.%2520Our%2520model%2520architecture%2520mimics%2520the%2520cortical%2520V1-MT%250Amotion%2520processing%2520pathway%252C%2520utilizing%2520a%2520trainable%2520motion%2520energy%2520sensor%2520bank%2520and%250Aa%2520recurrent%2520graph%2520network.%2520Supervised%2520learning%2520employing%2520diverse%2520naturalistic%250Avideos%2520allows%2520the%2520model%2520to%2520replicate%2520psychophysical%2520and%2520physiological%2520findings%250Aabout%2520first-order%2520%2528luminance-based%2529%2520motion%2520perception.%2520For%2520second-order%2520motion%252C%250Ainspired%2520by%2520neuroscientific%2520findings%252C%2520the%2520model%2520includes%2520an%2520additional%2520sensing%250Apathway%2520with%2520nonlinear%2520preprocessing%2520before%2520motion%2520energy%2520sensing%252C%2520implemented%250Ausing%2520a%2520simple%2520multilayer%25203D%2520CNN%2520block.%2520When%2520exploring%2520how%2520the%2520brain%2520acquired%250Athe%2520ability%2520to%2520perceive%2520second-order%2520motion%2520in%2520natural%2520environments%252C%2520in%2520which%250Apure%2520second-order%2520signals%2520are%2520rare%252C%2520we%2520hypothesized%2520that%2520second-order%250Amechanisms%2520were%2520critical%2520when%2520estimating%2520robust%2520object%2520motion%2520amidst%2520optical%250Afluctuations%252C%2520such%2520as%2520highlights%2520on%2520glossy%2520surfaces.%2520We%2520trained%2520our%250Adual-pathway%2520model%2520on%2520novel%2520motion%2520datasets%2520with%2520varying%2520material%2520properties%2520of%250Amoving%2520objects.%2520We%2520found%2520that%2520training%2520to%2520estimate%2520object%2520motion%2520from%250Anon-Lambertian%2520materials%2520naturally%2520endowed%2520the%2520model%2520with%2520the%2520capacity%2520to%250Aperceive%2520second-order%2520motion%252C%2520as%2520can%2520humans.%2520The%2520resulting%2520model%2520effectively%250Aaligns%2520with%2520biological%2520systems%2520while%2520generalizing%2520to%2520both%2520first-%2520and%250Asecond-order%2520motion%2520phenomena%2520in%2520natural%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Modeling%20for%20Multi-order%20Human%20Visual%20Motion%20Processing&entry.906535625=Zitang%20Sun%20and%20Yen-Ju%20Chen%20and%20Yung-Hao%20Yang%20and%20Yuan%20Li%20and%20Shin%27ya%20Nishida&entry.1292438233=%20%20Our%20research%20aims%20to%20develop%20machines%20that%20learn%20to%20perceive%20visual%20motion%20as%0Ado%20humans.%20While%20recent%20advances%20in%20computer%20vision%20%28CV%29%20have%20enabled%20DNN-based%0Amodels%20to%20accurately%20estimate%20optical%20flow%20in%20naturalistic%20images%2C%20a%0Asignificant%20disparity%20remains%20between%20CV%20models%20and%20the%20biological%20visual%0Asystem%20in%20both%20architecture%20and%20behavior.%20This%20disparity%20includes%20humans%27%0Aability%20to%20perceive%20the%20motion%20of%20higher-order%20image%20features%20%28second-order%0Amotion%29%2C%20which%20many%20CV%20models%20fail%20to%20capture%20because%20of%20their%20reliance%20on%20the%0Aintensity%20conservation%20law.%20Our%20model%20architecture%20mimics%20the%20cortical%20V1-MT%0Amotion%20processing%20pathway%2C%20utilizing%20a%20trainable%20motion%20energy%20sensor%20bank%20and%0Aa%20recurrent%20graph%20network.%20Supervised%20learning%20employing%20diverse%20naturalistic%0Avideos%20allows%20the%20model%20to%20replicate%20psychophysical%20and%20physiological%20findings%0Aabout%20first-order%20%28luminance-based%29%20motion%20perception.%20For%20second-order%20motion%2C%0Ainspired%20by%20neuroscientific%20findings%2C%20the%20model%20includes%20an%20additional%20sensing%0Apathway%20with%20nonlinear%20preprocessing%20before%20motion%20energy%20sensing%2C%20implemented%0Ausing%20a%20simple%20multilayer%203D%20CNN%20block.%20When%20exploring%20how%20the%20brain%20acquired%0Athe%20ability%20to%20perceive%20second-order%20motion%20in%20natural%20environments%2C%20in%20which%0Apure%20second-order%20signals%20are%20rare%2C%20we%20hypothesized%20that%20second-order%0Amechanisms%20were%20critical%20when%20estimating%20robust%20object%20motion%20amidst%20optical%0Afluctuations%2C%20such%20as%20highlights%20on%20glossy%20surfaces.%20We%20trained%20our%0Adual-pathway%20model%20on%20novel%20motion%20datasets%20with%20varying%20material%20properties%20of%0Amoving%20objects.%20We%20found%20that%20training%20to%20estimate%20object%20motion%20from%0Anon-Lambertian%20materials%20naturally%20endowed%20the%20model%20with%20the%20capacity%20to%0Aperceive%20second-order%20motion%2C%20as%20can%20humans.%20The%20resulting%20model%20effectively%0Aaligns%20with%20biological%20systems%20while%20generalizing%20to%20both%20first-%20and%0Asecond-order%20motion%20phenomena%20in%20natural%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12810v1&entry.124074799=Read"},
{"title": "Guaranteed Recovery of Unambiguous Clusters", "author": "Kayvon Mazooji and Ilan Shomorony", "abstract": "  Clustering is often a challenging problem because of the inherent ambiguity\nin what the \"correct\" clustering should be. Even when the number of clusters\n$K$ is known, this ambiguity often still exists, particularly when there is\nvariation in density among different clusters, and clusters have multiple\nrelatively separated regions of high density. In this paper we propose an\ninformation-theoretic characterization of when a $K$-clustering is ambiguous,\nand design an algorithm that recovers the clustering whenever it is\nunambiguous. This characterization formalizes the situation when two high\ndensity regions within a cluster are separable enough that they look more like\ntwo distinct clusters than two truly distinct clusters in the clustering. The\nalgorithm first identifies $K$ partial clusters (or \"seeds\") using a\ndensity-based approach, and then adds unclustered points to the initial $K$\npartial clusters in a greedy manner to form a complete clustering. We implement\nand test a version of the algorithm that is modified to effectively handle\noverlapping clusters, and observe that it requires little parameter selection\nand displays improved performance on many datasets compared to widely used\nalgorithms for non-convex cluster recovery.\n", "link": "http://arxiv.org/abs/2501.13093v1", "date": "2025-01-22", "relevancy": 1.7948, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4541}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guaranteed%20Recovery%20of%20Unambiguous%20Clusters&body=Title%3A%20Guaranteed%20Recovery%20of%20Unambiguous%20Clusters%0AAuthor%3A%20Kayvon%20Mazooji%20and%20Ilan%20Shomorony%0AAbstract%3A%20%20%20Clustering%20is%20often%20a%20challenging%20problem%20because%20of%20the%20inherent%20ambiguity%0Ain%20what%20the%20%22correct%22%20clustering%20should%20be.%20Even%20when%20the%20number%20of%20clusters%0A%24K%24%20is%20known%2C%20this%20ambiguity%20often%20still%20exists%2C%20particularly%20when%20there%20is%0Avariation%20in%20density%20among%20different%20clusters%2C%20and%20clusters%20have%20multiple%0Arelatively%20separated%20regions%20of%20high%20density.%20In%20this%20paper%20we%20propose%20an%0Ainformation-theoretic%20characterization%20of%20when%20a%20%24K%24-clustering%20is%20ambiguous%2C%0Aand%20design%20an%20algorithm%20that%20recovers%20the%20clustering%20whenever%20it%20is%0Aunambiguous.%20This%20characterization%20formalizes%20the%20situation%20when%20two%20high%0Adensity%20regions%20within%20a%20cluster%20are%20separable%20enough%20that%20they%20look%20more%20like%0Atwo%20distinct%20clusters%20than%20two%20truly%20distinct%20clusters%20in%20the%20clustering.%20The%0Aalgorithm%20first%20identifies%20%24K%24%20partial%20clusters%20%28or%20%22seeds%22%29%20using%20a%0Adensity-based%20approach%2C%20and%20then%20adds%20unclustered%20points%20to%20the%20initial%20%24K%24%0Apartial%20clusters%20in%20a%20greedy%20manner%20to%20form%20a%20complete%20clustering.%20We%20implement%0Aand%20test%20a%20version%20of%20the%20algorithm%20that%20is%20modified%20to%20effectively%20handle%0Aoverlapping%20clusters%2C%20and%20observe%20that%20it%20requires%20little%20parameter%20selection%0Aand%20displays%20improved%20performance%20on%20many%20datasets%20compared%20to%20widely%20used%0Aalgorithms%20for%20non-convex%20cluster%20recovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuaranteed%2520Recovery%2520of%2520Unambiguous%2520Clusters%26entry.906535625%3DKayvon%2520Mazooji%2520and%2520Ilan%2520Shomorony%26entry.1292438233%3D%2520%2520Clustering%2520is%2520often%2520a%2520challenging%2520problem%2520because%2520of%2520the%2520inherent%2520ambiguity%250Ain%2520what%2520the%2520%2522correct%2522%2520clustering%2520should%2520be.%2520Even%2520when%2520the%2520number%2520of%2520clusters%250A%2524K%2524%2520is%2520known%252C%2520this%2520ambiguity%2520often%2520still%2520exists%252C%2520particularly%2520when%2520there%2520is%250Avariation%2520in%2520density%2520among%2520different%2520clusters%252C%2520and%2520clusters%2520have%2520multiple%250Arelatively%2520separated%2520regions%2520of%2520high%2520density.%2520In%2520this%2520paper%2520we%2520propose%2520an%250Ainformation-theoretic%2520characterization%2520of%2520when%2520a%2520%2524K%2524-clustering%2520is%2520ambiguous%252C%250Aand%2520design%2520an%2520algorithm%2520that%2520recovers%2520the%2520clustering%2520whenever%2520it%2520is%250Aunambiguous.%2520This%2520characterization%2520formalizes%2520the%2520situation%2520when%2520two%2520high%250Adensity%2520regions%2520within%2520a%2520cluster%2520are%2520separable%2520enough%2520that%2520they%2520look%2520more%2520like%250Atwo%2520distinct%2520clusters%2520than%2520two%2520truly%2520distinct%2520clusters%2520in%2520the%2520clustering.%2520The%250Aalgorithm%2520first%2520identifies%2520%2524K%2524%2520partial%2520clusters%2520%2528or%2520%2522seeds%2522%2529%2520using%2520a%250Adensity-based%2520approach%252C%2520and%2520then%2520adds%2520unclustered%2520points%2520to%2520the%2520initial%2520%2524K%2524%250Apartial%2520clusters%2520in%2520a%2520greedy%2520manner%2520to%2520form%2520a%2520complete%2520clustering.%2520We%2520implement%250Aand%2520test%2520a%2520version%2520of%2520the%2520algorithm%2520that%2520is%2520modified%2520to%2520effectively%2520handle%250Aoverlapping%2520clusters%252C%2520and%2520observe%2520that%2520it%2520requires%2520little%2520parameter%2520selection%250Aand%2520displays%2520improved%2520performance%2520on%2520many%2520datasets%2520compared%2520to%2520widely%2520used%250Aalgorithms%2520for%2520non-convex%2520cluster%2520recovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guaranteed%20Recovery%20of%20Unambiguous%20Clusters&entry.906535625=Kayvon%20Mazooji%20and%20Ilan%20Shomorony&entry.1292438233=%20%20Clustering%20is%20often%20a%20challenging%20problem%20because%20of%20the%20inherent%20ambiguity%0Ain%20what%20the%20%22correct%22%20clustering%20should%20be.%20Even%20when%20the%20number%20of%20clusters%0A%24K%24%20is%20known%2C%20this%20ambiguity%20often%20still%20exists%2C%20particularly%20when%20there%20is%0Avariation%20in%20density%20among%20different%20clusters%2C%20and%20clusters%20have%20multiple%0Arelatively%20separated%20regions%20of%20high%20density.%20In%20this%20paper%20we%20propose%20an%0Ainformation-theoretic%20characterization%20of%20when%20a%20%24K%24-clustering%20is%20ambiguous%2C%0Aand%20design%20an%20algorithm%20that%20recovers%20the%20clustering%20whenever%20it%20is%0Aunambiguous.%20This%20characterization%20formalizes%20the%20situation%20when%20two%20high%0Adensity%20regions%20within%20a%20cluster%20are%20separable%20enough%20that%20they%20look%20more%20like%0Atwo%20distinct%20clusters%20than%20two%20truly%20distinct%20clusters%20in%20the%20clustering.%20The%0Aalgorithm%20first%20identifies%20%24K%24%20partial%20clusters%20%28or%20%22seeds%22%29%20using%20a%0Adensity-based%20approach%2C%20and%20then%20adds%20unclustered%20points%20to%20the%20initial%20%24K%24%0Apartial%20clusters%20in%20a%20greedy%20manner%20to%20form%20a%20complete%20clustering.%20We%20implement%0Aand%20test%20a%20version%20of%20the%20algorithm%20that%20is%20modified%20to%20effectively%20handle%0Aoverlapping%20clusters%2C%20and%20observe%20that%20it%20requires%20little%20parameter%20selection%0Aand%20displays%20improved%20performance%20on%20many%20datasets%20compared%20to%20widely%20used%0Aalgorithms%20for%20non-convex%20cluster%20recovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13093v1&entry.124074799=Read"},
{"title": "An Offline Multi-Agent Reinforcement Learning Framework for Radio\n  Resource Management", "author": "Eslam Eldeeb and Hirley Alves", "abstract": "  Offline multi-agent reinforcement learning (MARL) addresses key limitations\nof online MARL, such as safety concerns, expensive data collection, extended\ntraining intervals, and high signaling overhead caused by online interactions\nwith the environment. In this work, we propose an offline MARL algorithm for\nradio resource management (RRM), focusing on optimizing scheduling policies for\nmultiple access points (APs) to jointly maximize the sum and tail rates of user\nequipment (UEs). We evaluate three training paradigms: centralized,\nindependent, and centralized training with decentralized execution (CTDE). Our\nsimulation results demonstrate that the proposed offline MARL framework\noutperforms conventional baseline approaches, achieving over a 15\\% improvement\nin a weighted combination of sum and tail rates. Additionally, the CTDE\nframework strikes an effective balance, reducing the computational complexity\nof centralized methods while addressing the inefficiencies of independent\ntraining. These results underscore the potential of offline MARL to deliver\nscalable, robust, and efficient solutions for resource management in dynamic\nwireless networks.\n", "link": "http://arxiv.org/abs/2501.12991v1", "date": "2025-01-22", "relevancy": 1.7836, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4762}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4509}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Offline%20Multi-Agent%20Reinforcement%20Learning%20Framework%20for%20Radio%0A%20%20Resource%20Management&body=Title%3A%20An%20Offline%20Multi-Agent%20Reinforcement%20Learning%20Framework%20for%20Radio%0A%20%20Resource%20Management%0AAuthor%3A%20Eslam%20Eldeeb%20and%20Hirley%20Alves%0AAbstract%3A%20%20%20Offline%20multi-agent%20reinforcement%20learning%20%28MARL%29%20addresses%20key%20limitations%0Aof%20online%20MARL%2C%20such%20as%20safety%20concerns%2C%20expensive%20data%20collection%2C%20extended%0Atraining%20intervals%2C%20and%20high%20signaling%20overhead%20caused%20by%20online%20interactions%0Awith%20the%20environment.%20In%20this%20work%2C%20we%20propose%20an%20offline%20MARL%20algorithm%20for%0Aradio%20resource%20management%20%28RRM%29%2C%20focusing%20on%20optimizing%20scheduling%20policies%20for%0Amultiple%20access%20points%20%28APs%29%20to%20jointly%20maximize%20the%20sum%20and%20tail%20rates%20of%20user%0Aequipment%20%28UEs%29.%20We%20evaluate%20three%20training%20paradigms%3A%20centralized%2C%0Aindependent%2C%20and%20centralized%20training%20with%20decentralized%20execution%20%28CTDE%29.%20Our%0Asimulation%20results%20demonstrate%20that%20the%20proposed%20offline%20MARL%20framework%0Aoutperforms%20conventional%20baseline%20approaches%2C%20achieving%20over%20a%2015%5C%25%20improvement%0Ain%20a%20weighted%20combination%20of%20sum%20and%20tail%20rates.%20Additionally%2C%20the%20CTDE%0Aframework%20strikes%20an%20effective%20balance%2C%20reducing%20the%20computational%20complexity%0Aof%20centralized%20methods%20while%20addressing%20the%20inefficiencies%20of%20independent%0Atraining.%20These%20results%20underscore%20the%20potential%20of%20offline%20MARL%20to%20deliver%0Ascalable%2C%20robust%2C%20and%20efficient%20solutions%20for%20resource%20management%20in%20dynamic%0Awireless%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Offline%2520Multi-Agent%2520Reinforcement%2520Learning%2520Framework%2520for%2520Radio%250A%2520%2520Resource%2520Management%26entry.906535625%3DEslam%2520Eldeeb%2520and%2520Hirley%2520Alves%26entry.1292438233%3D%2520%2520Offline%2520multi-agent%2520reinforcement%2520learning%2520%2528MARL%2529%2520addresses%2520key%2520limitations%250Aof%2520online%2520MARL%252C%2520such%2520as%2520safety%2520concerns%252C%2520expensive%2520data%2520collection%252C%2520extended%250Atraining%2520intervals%252C%2520and%2520high%2520signaling%2520overhead%2520caused%2520by%2520online%2520interactions%250Awith%2520the%2520environment.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520offline%2520MARL%2520algorithm%2520for%250Aradio%2520resource%2520management%2520%2528RRM%2529%252C%2520focusing%2520on%2520optimizing%2520scheduling%2520policies%2520for%250Amultiple%2520access%2520points%2520%2528APs%2529%2520to%2520jointly%2520maximize%2520the%2520sum%2520and%2520tail%2520rates%2520of%2520user%250Aequipment%2520%2528UEs%2529.%2520We%2520evaluate%2520three%2520training%2520paradigms%253A%2520centralized%252C%250Aindependent%252C%2520and%2520centralized%2520training%2520with%2520decentralized%2520execution%2520%2528CTDE%2529.%2520Our%250Asimulation%2520results%2520demonstrate%2520that%2520the%2520proposed%2520offline%2520MARL%2520framework%250Aoutperforms%2520conventional%2520baseline%2520approaches%252C%2520achieving%2520over%2520a%252015%255C%2525%2520improvement%250Ain%2520a%2520weighted%2520combination%2520of%2520sum%2520and%2520tail%2520rates.%2520Additionally%252C%2520the%2520CTDE%250Aframework%2520strikes%2520an%2520effective%2520balance%252C%2520reducing%2520the%2520computational%2520complexity%250Aof%2520centralized%2520methods%2520while%2520addressing%2520the%2520inefficiencies%2520of%2520independent%250Atraining.%2520These%2520results%2520underscore%2520the%2520potential%2520of%2520offline%2520MARL%2520to%2520deliver%250Ascalable%252C%2520robust%252C%2520and%2520efficient%2520solutions%2520for%2520resource%2520management%2520in%2520dynamic%250Awireless%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Offline%20Multi-Agent%20Reinforcement%20Learning%20Framework%20for%20Radio%0A%20%20Resource%20Management&entry.906535625=Eslam%20Eldeeb%20and%20Hirley%20Alves&entry.1292438233=%20%20Offline%20multi-agent%20reinforcement%20learning%20%28MARL%29%20addresses%20key%20limitations%0Aof%20online%20MARL%2C%20such%20as%20safety%20concerns%2C%20expensive%20data%20collection%2C%20extended%0Atraining%20intervals%2C%20and%20high%20signaling%20overhead%20caused%20by%20online%20interactions%0Awith%20the%20environment.%20In%20this%20work%2C%20we%20propose%20an%20offline%20MARL%20algorithm%20for%0Aradio%20resource%20management%20%28RRM%29%2C%20focusing%20on%20optimizing%20scheduling%20policies%20for%0Amultiple%20access%20points%20%28APs%29%20to%20jointly%20maximize%20the%20sum%20and%20tail%20rates%20of%20user%0Aequipment%20%28UEs%29.%20We%20evaluate%20three%20training%20paradigms%3A%20centralized%2C%0Aindependent%2C%20and%20centralized%20training%20with%20decentralized%20execution%20%28CTDE%29.%20Our%0Asimulation%20results%20demonstrate%20that%20the%20proposed%20offline%20MARL%20framework%0Aoutperforms%20conventional%20baseline%20approaches%2C%20achieving%20over%20a%2015%5C%25%20improvement%0Ain%20a%20weighted%20combination%20of%20sum%20and%20tail%20rates.%20Additionally%2C%20the%20CTDE%0Aframework%20strikes%20an%20effective%20balance%2C%20reducing%20the%20computational%20complexity%0Aof%20centralized%20methods%20while%20addressing%20the%20inefficiencies%20of%20independent%0Atraining.%20These%20results%20underscore%20the%20potential%20of%20offline%20MARL%20to%20deliver%0Ascalable%2C%20robust%2C%20and%20efficient%20solutions%20for%20resource%20management%20in%20dynamic%0Awireless%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12991v1&entry.124074799=Read"},
{"title": "Episodic memory in AI agents poses risks that should be studied and\n  mitigated", "author": "Chad DeChant", "abstract": "  Most current AI models have little ability to store and later retrieve a\nrecord or representation of what they do. In human cognition, episodic memories\nplay an important role in both recall of the past as well as planning for the\nfuture. The ability to form and use episodic memories would similarly enable a\nbroad range of improved capabilities in an AI agent that interacts with and\ntakes actions in the world. Researchers have begun directing more attention to\ndeveloping memory abilities in AI models. It is therefore likely that models\nwith such capability will be become widespread in the near future. This could\nin some ways contribute to making such AI agents safer by enabling users to\nbetter monitor, understand, and control their actions. However, as a new\ncapability with wide applications, we argue that it will also introduce\nsignificant new risks that researchers should begin to study and address. We\noutline these risks and benefits and propose four principles to guide the\ndevelopment of episodic memory capabilities so that these will enhance, rather\nthan undermine, the effort to keep AI safe and trustworthy.\n", "link": "http://arxiv.org/abs/2501.11739v2", "date": "2025-01-22", "relevancy": 1.7835, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4574}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.45}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4327}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Episodic%20memory%20in%20AI%20agents%20poses%20risks%20that%20should%20be%20studied%20and%0A%20%20mitigated&body=Title%3A%20Episodic%20memory%20in%20AI%20agents%20poses%20risks%20that%20should%20be%20studied%20and%0A%20%20mitigated%0AAuthor%3A%20Chad%20DeChant%0AAbstract%3A%20%20%20Most%20current%20AI%20models%20have%20little%20ability%20to%20store%20and%20later%20retrieve%20a%0Arecord%20or%20representation%20of%20what%20they%20do.%20In%20human%20cognition%2C%20episodic%20memories%0Aplay%20an%20important%20role%20in%20both%20recall%20of%20the%20past%20as%20well%20as%20planning%20for%20the%0Afuture.%20The%20ability%20to%20form%20and%20use%20episodic%20memories%20would%20similarly%20enable%20a%0Abroad%20range%20of%20improved%20capabilities%20in%20an%20AI%20agent%20that%20interacts%20with%20and%0Atakes%20actions%20in%20the%20world.%20Researchers%20have%20begun%20directing%20more%20attention%20to%0Adeveloping%20memory%20abilities%20in%20AI%20models.%20It%20is%20therefore%20likely%20that%20models%0Awith%20such%20capability%20will%20be%20become%20widespread%20in%20the%20near%20future.%20This%20could%0Ain%20some%20ways%20contribute%20to%20making%20such%20AI%20agents%20safer%20by%20enabling%20users%20to%0Abetter%20monitor%2C%20understand%2C%20and%20control%20their%20actions.%20However%2C%20as%20a%20new%0Acapability%20with%20wide%20applications%2C%20we%20argue%20that%20it%20will%20also%20introduce%0Asignificant%20new%20risks%20that%20researchers%20should%20begin%20to%20study%20and%20address.%20We%0Aoutline%20these%20risks%20and%20benefits%20and%20propose%20four%20principles%20to%20guide%20the%0Adevelopment%20of%20episodic%20memory%20capabilities%20so%20that%20these%20will%20enhance%2C%20rather%0Athan%20undermine%2C%20the%20effort%20to%20keep%20AI%20safe%20and%20trustworthy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.11739v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEpisodic%2520memory%2520in%2520AI%2520agents%2520poses%2520risks%2520that%2520should%2520be%2520studied%2520and%250A%2520%2520mitigated%26entry.906535625%3DChad%2520DeChant%26entry.1292438233%3D%2520%2520Most%2520current%2520AI%2520models%2520have%2520little%2520ability%2520to%2520store%2520and%2520later%2520retrieve%2520a%250Arecord%2520or%2520representation%2520of%2520what%2520they%2520do.%2520In%2520human%2520cognition%252C%2520episodic%2520memories%250Aplay%2520an%2520important%2520role%2520in%2520both%2520recall%2520of%2520the%2520past%2520as%2520well%2520as%2520planning%2520for%2520the%250Afuture.%2520The%2520ability%2520to%2520form%2520and%2520use%2520episodic%2520memories%2520would%2520similarly%2520enable%2520a%250Abroad%2520range%2520of%2520improved%2520capabilities%2520in%2520an%2520AI%2520agent%2520that%2520interacts%2520with%2520and%250Atakes%2520actions%2520in%2520the%2520world.%2520Researchers%2520have%2520begun%2520directing%2520more%2520attention%2520to%250Adeveloping%2520memory%2520abilities%2520in%2520AI%2520models.%2520It%2520is%2520therefore%2520likely%2520that%2520models%250Awith%2520such%2520capability%2520will%2520be%2520become%2520widespread%2520in%2520the%2520near%2520future.%2520This%2520could%250Ain%2520some%2520ways%2520contribute%2520to%2520making%2520such%2520AI%2520agents%2520safer%2520by%2520enabling%2520users%2520to%250Abetter%2520monitor%252C%2520understand%252C%2520and%2520control%2520their%2520actions.%2520However%252C%2520as%2520a%2520new%250Acapability%2520with%2520wide%2520applications%252C%2520we%2520argue%2520that%2520it%2520will%2520also%2520introduce%250Asignificant%2520new%2520risks%2520that%2520researchers%2520should%2520begin%2520to%2520study%2520and%2520address.%2520We%250Aoutline%2520these%2520risks%2520and%2520benefits%2520and%2520propose%2520four%2520principles%2520to%2520guide%2520the%250Adevelopment%2520of%2520episodic%2520memory%2520capabilities%2520so%2520that%2520these%2520will%2520enhance%252C%2520rather%250Athan%2520undermine%252C%2520the%2520effort%2520to%2520keep%2520AI%2520safe%2520and%2520trustworthy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.11739v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Episodic%20memory%20in%20AI%20agents%20poses%20risks%20that%20should%20be%20studied%20and%0A%20%20mitigated&entry.906535625=Chad%20DeChant&entry.1292438233=%20%20Most%20current%20AI%20models%20have%20little%20ability%20to%20store%20and%20later%20retrieve%20a%0Arecord%20or%20representation%20of%20what%20they%20do.%20In%20human%20cognition%2C%20episodic%20memories%0Aplay%20an%20important%20role%20in%20both%20recall%20of%20the%20past%20as%20well%20as%20planning%20for%20the%0Afuture.%20The%20ability%20to%20form%20and%20use%20episodic%20memories%20would%20similarly%20enable%20a%0Abroad%20range%20of%20improved%20capabilities%20in%20an%20AI%20agent%20that%20interacts%20with%20and%0Atakes%20actions%20in%20the%20world.%20Researchers%20have%20begun%20directing%20more%20attention%20to%0Adeveloping%20memory%20abilities%20in%20AI%20models.%20It%20is%20therefore%20likely%20that%20models%0Awith%20such%20capability%20will%20be%20become%20widespread%20in%20the%20near%20future.%20This%20could%0Ain%20some%20ways%20contribute%20to%20making%20such%20AI%20agents%20safer%20by%20enabling%20users%20to%0Abetter%20monitor%2C%20understand%2C%20and%20control%20their%20actions.%20However%2C%20as%20a%20new%0Acapability%20with%20wide%20applications%2C%20we%20argue%20that%20it%20will%20also%20introduce%0Asignificant%20new%20risks%20that%20researchers%20should%20begin%20to%20study%20and%20address.%20We%0Aoutline%20these%20risks%20and%20benefits%20and%20propose%20four%20principles%20to%20guide%20the%0Adevelopment%20of%20episodic%20memory%20capabilities%20so%20that%20these%20will%20enhance%2C%20rather%0Athan%20undermine%2C%20the%20effort%20to%20keep%20AI%20safe%20and%20trustworthy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.11739v2&entry.124074799=Read"},
{"title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos", "author": "Sili Chen and Hengkai Guo and Shengnan Zhu and Feihu Zhang and Zilong Huang and Jiashi Feng and Bingyi Kang", "abstract": "  Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS.\n", "link": "http://arxiv.org/abs/2501.12375v2", "date": "2025-01-22", "relevancy": 1.7696, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5986}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5877}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Depth%20Anything%3A%20Consistent%20Depth%20Estimation%20for%20Super-Long%20Videos&body=Title%3A%20Video%20Depth%20Anything%3A%20Consistent%20Depth%20Estimation%20for%20Super-Long%20Videos%0AAuthor%3A%20Sili%20Chen%20and%20Hengkai%20Guo%20and%20Shengnan%20Zhu%20and%20Feihu%20Zhang%20and%20Zilong%20Huang%20and%20Jiashi%20Feng%20and%20Bingyi%20Kang%0AAbstract%3A%20%20%20Depth%20Anything%20has%20achieved%20remarkable%20success%20in%20monocular%20depth%20estimation%0Awith%20strong%20generalization%20ability.%20However%2C%20it%20suffers%20from%20temporal%0Ainconsistency%20in%20videos%2C%20hindering%20its%20practical%20applications.%20Various%20methods%0Ahave%20been%20proposed%20to%20alleviate%20this%20issue%20by%20leveraging%20video%20generation%0Amodels%20or%20introducing%20priors%20from%20optical%20flow%20and%20camera%20poses.%20Nonetheless%2C%0Athese%20methods%20are%20only%20applicable%20to%20short%20videos%20%28%3C%2010%20seconds%29%20and%20require%20a%0Atrade-off%20between%20quality%20and%20computational%20efficiency.%20We%20propose%20Video%20Depth%0AAnything%20for%20high-quality%2C%20consistent%20depth%20estimation%20in%20super-long%20videos%0A%28over%20several%20minutes%29%20without%20sacrificing%20efficiency.%20We%20base%20our%20model%20on%0ADepth%20Anything%20V2%20and%20replace%20its%20head%20with%20an%20efficient%20spatial-temporal%20head.%0AWe%20design%20a%20straightforward%20yet%20effective%20temporal%20consistency%20loss%20by%0Aconstraining%20the%20temporal%20depth%20gradient%2C%20eliminating%20the%20need%20for%20additional%0Ageometric%20priors.%20The%20model%20is%20trained%20on%20a%20joint%20dataset%20of%20video%20depth%20and%0Aunlabeled%20images%2C%20similar%20to%20Depth%20Anything%20V2.%20Moreover%2C%20a%20novel%0Akey-frame-based%20strategy%20is%20developed%20for%20long%20video%20inference.%20Experiments%0Ashow%20that%20our%20model%20can%20be%20applied%20to%20arbitrarily%20long%20videos%20without%0Acompromising%20quality%2C%20consistency%2C%20or%20generalization%20ability.%20Comprehensive%0Aevaluations%20on%20multiple%20video%20benchmarks%20demonstrate%20that%20our%20approach%20sets%20a%0Anew%20state-of-the-art%20in%20zero-shot%20video%20depth%20estimation.%20We%20offer%20models%20of%0Adifferent%20scales%20to%20support%20a%20range%20of%20scenarios%2C%20with%20our%20smallest%20model%0Acapable%20of%20real-time%20performance%20at%2030%20FPS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12375v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Depth%2520Anything%253A%2520Consistent%2520Depth%2520Estimation%2520for%2520Super-Long%2520Videos%26entry.906535625%3DSili%2520Chen%2520and%2520Hengkai%2520Guo%2520and%2520Shengnan%2520Zhu%2520and%2520Feihu%2520Zhang%2520and%2520Zilong%2520Huang%2520and%2520Jiashi%2520Feng%2520and%2520Bingyi%2520Kang%26entry.1292438233%3D%2520%2520Depth%2520Anything%2520has%2520achieved%2520remarkable%2520success%2520in%2520monocular%2520depth%2520estimation%250Awith%2520strong%2520generalization%2520ability.%2520However%252C%2520it%2520suffers%2520from%2520temporal%250Ainconsistency%2520in%2520videos%252C%2520hindering%2520its%2520practical%2520applications.%2520Various%2520methods%250Ahave%2520been%2520proposed%2520to%2520alleviate%2520this%2520issue%2520by%2520leveraging%2520video%2520generation%250Amodels%2520or%2520introducing%2520priors%2520from%2520optical%2520flow%2520and%2520camera%2520poses.%2520Nonetheless%252C%250Athese%2520methods%2520are%2520only%2520applicable%2520to%2520short%2520videos%2520%2528%253C%252010%2520seconds%2529%2520and%2520require%2520a%250Atrade-off%2520between%2520quality%2520and%2520computational%2520efficiency.%2520We%2520propose%2520Video%2520Depth%250AAnything%2520for%2520high-quality%252C%2520consistent%2520depth%2520estimation%2520in%2520super-long%2520videos%250A%2528over%2520several%2520minutes%2529%2520without%2520sacrificing%2520efficiency.%2520We%2520base%2520our%2520model%2520on%250ADepth%2520Anything%2520V2%2520and%2520replace%2520its%2520head%2520with%2520an%2520efficient%2520spatial-temporal%2520head.%250AWe%2520design%2520a%2520straightforward%2520yet%2520effective%2520temporal%2520consistency%2520loss%2520by%250Aconstraining%2520the%2520temporal%2520depth%2520gradient%252C%2520eliminating%2520the%2520need%2520for%2520additional%250Ageometric%2520priors.%2520The%2520model%2520is%2520trained%2520on%2520a%2520joint%2520dataset%2520of%2520video%2520depth%2520and%250Aunlabeled%2520images%252C%2520similar%2520to%2520Depth%2520Anything%2520V2.%2520Moreover%252C%2520a%2520novel%250Akey-frame-based%2520strategy%2520is%2520developed%2520for%2520long%2520video%2520inference.%2520Experiments%250Ashow%2520that%2520our%2520model%2520can%2520be%2520applied%2520to%2520arbitrarily%2520long%2520videos%2520without%250Acompromising%2520quality%252C%2520consistency%252C%2520or%2520generalization%2520ability.%2520Comprehensive%250Aevaluations%2520on%2520multiple%2520video%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520sets%2520a%250Anew%2520state-of-the-art%2520in%2520zero-shot%2520video%2520depth%2520estimation.%2520We%2520offer%2520models%2520of%250Adifferent%2520scales%2520to%2520support%2520a%2520range%2520of%2520scenarios%252C%2520with%2520our%2520smallest%2520model%250Acapable%2520of%2520real-time%2520performance%2520at%252030%2520FPS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12375v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Depth%20Anything%3A%20Consistent%20Depth%20Estimation%20for%20Super-Long%20Videos&entry.906535625=Sili%20Chen%20and%20Hengkai%20Guo%20and%20Shengnan%20Zhu%20and%20Feihu%20Zhang%20and%20Zilong%20Huang%20and%20Jiashi%20Feng%20and%20Bingyi%20Kang&entry.1292438233=%20%20Depth%20Anything%20has%20achieved%20remarkable%20success%20in%20monocular%20depth%20estimation%0Awith%20strong%20generalization%20ability.%20However%2C%20it%20suffers%20from%20temporal%0Ainconsistency%20in%20videos%2C%20hindering%20its%20practical%20applications.%20Various%20methods%0Ahave%20been%20proposed%20to%20alleviate%20this%20issue%20by%20leveraging%20video%20generation%0Amodels%20or%20introducing%20priors%20from%20optical%20flow%20and%20camera%20poses.%20Nonetheless%2C%0Athese%20methods%20are%20only%20applicable%20to%20short%20videos%20%28%3C%2010%20seconds%29%20and%20require%20a%0Atrade-off%20between%20quality%20and%20computational%20efficiency.%20We%20propose%20Video%20Depth%0AAnything%20for%20high-quality%2C%20consistent%20depth%20estimation%20in%20super-long%20videos%0A%28over%20several%20minutes%29%20without%20sacrificing%20efficiency.%20We%20base%20our%20model%20on%0ADepth%20Anything%20V2%20and%20replace%20its%20head%20with%20an%20efficient%20spatial-temporal%20head.%0AWe%20design%20a%20straightforward%20yet%20effective%20temporal%20consistency%20loss%20by%0Aconstraining%20the%20temporal%20depth%20gradient%2C%20eliminating%20the%20need%20for%20additional%0Ageometric%20priors.%20The%20model%20is%20trained%20on%20a%20joint%20dataset%20of%20video%20depth%20and%0Aunlabeled%20images%2C%20similar%20to%20Depth%20Anything%20V2.%20Moreover%2C%20a%20novel%0Akey-frame-based%20strategy%20is%20developed%20for%20long%20video%20inference.%20Experiments%0Ashow%20that%20our%20model%20can%20be%20applied%20to%20arbitrarily%20long%20videos%20without%0Acompromising%20quality%2C%20consistency%2C%20or%20generalization%20ability.%20Comprehensive%0Aevaluations%20on%20multiple%20video%20benchmarks%20demonstrate%20that%20our%20approach%20sets%20a%0Anew%20state-of-the-art%20in%20zero-shot%20video%20depth%20estimation.%20We%20offer%20models%20of%0Adifferent%20scales%20to%20support%20a%20range%20of%20scenarios%2C%20with%20our%20smallest%20model%0Acapable%20of%20real-time%20performance%20at%2030%20FPS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12375v2&entry.124074799=Read"},
{"title": "AMM-Diff: Adaptive Multi-Modality Diffusion Network for Missing Modality\n  Imputation", "author": "Aghiles Kebaili and J\u00e9r\u00f4me Lapuyade-Lahorgue and Pierre Vera and Su Ruan", "abstract": "  In clinical practice, full imaging is not always feasible, often due to\ncomplex acquisition protocols, stringent privacy regulations, or specific\nclinical needs. However, missing MR modalities pose significant challenges for\ntasks like brain tumor segmentation, especially in deep learning-based\nsegmentation, as each modality provides complementary information crucial for\nimproving accuracy. A promising solution is missing data imputation, where\nabsent modalities are generated from available ones. While generative models\nhave been widely used for this purpose, most state-of-the-art approaches are\nlimited to single or dual target translations, lacking the adaptability to\ngenerate missing modalities based on varying input configurations. To address\nthis, we propose an Adaptive Multi-Modality Diffusion Network (AMM-Diff), a\nnovel diffusion-based generative model capable of handling any number of input\nmodalities and generating the missing ones. We designed an Image-Frequency\nFusion Network (IFFN) that learns a unified feature representation through a\nself-supervised pretext task across the full input modalities and their\nselected high-frequency Fourier components. The proposed diffusion model\nleverages this representation, encapsulating prior knowledge of the complete\nmodalities, and combines it with an adaptive reconstruction strategy to achieve\nmissing modality completion. Experimental results on the BraTS 2021 dataset\ndemonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2501.12840v1", "date": "2025-01-22", "relevancy": 1.7693, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6171}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6115}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMM-Diff%3A%20Adaptive%20Multi-Modality%20Diffusion%20Network%20for%20Missing%20Modality%0A%20%20Imputation&body=Title%3A%20AMM-Diff%3A%20Adaptive%20Multi-Modality%20Diffusion%20Network%20for%20Missing%20Modality%0A%20%20Imputation%0AAuthor%3A%20Aghiles%20Kebaili%20and%20J%C3%A9r%C3%B4me%20Lapuyade-Lahorgue%20and%20Pierre%20Vera%20and%20Su%20Ruan%0AAbstract%3A%20%20%20In%20clinical%20practice%2C%20full%20imaging%20is%20not%20always%20feasible%2C%20often%20due%20to%0Acomplex%20acquisition%20protocols%2C%20stringent%20privacy%20regulations%2C%20or%20specific%0Aclinical%20needs.%20However%2C%20missing%20MR%20modalities%20pose%20significant%20challenges%20for%0Atasks%20like%20brain%20tumor%20segmentation%2C%20especially%20in%20deep%20learning-based%0Asegmentation%2C%20as%20each%20modality%20provides%20complementary%20information%20crucial%20for%0Aimproving%20accuracy.%20A%20promising%20solution%20is%20missing%20data%20imputation%2C%20where%0Aabsent%20modalities%20are%20generated%20from%20available%20ones.%20While%20generative%20models%0Ahave%20been%20widely%20used%20for%20this%20purpose%2C%20most%20state-of-the-art%20approaches%20are%0Alimited%20to%20single%20or%20dual%20target%20translations%2C%20lacking%20the%20adaptability%20to%0Agenerate%20missing%20modalities%20based%20on%20varying%20input%20configurations.%20To%20address%0Athis%2C%20we%20propose%20an%20Adaptive%20Multi-Modality%20Diffusion%20Network%20%28AMM-Diff%29%2C%20a%0Anovel%20diffusion-based%20generative%20model%20capable%20of%20handling%20any%20number%20of%20input%0Amodalities%20and%20generating%20the%20missing%20ones.%20We%20designed%20an%20Image-Frequency%0AFusion%20Network%20%28IFFN%29%20that%20learns%20a%20unified%20feature%20representation%20through%20a%0Aself-supervised%20pretext%20task%20across%20the%20full%20input%20modalities%20and%20their%0Aselected%20high-frequency%20Fourier%20components.%20The%20proposed%20diffusion%20model%0Aleverages%20this%20representation%2C%20encapsulating%20prior%20knowledge%20of%20the%20complete%0Amodalities%2C%20and%20combines%20it%20with%20an%20adaptive%20reconstruction%20strategy%20to%20achieve%0Amissing%20modality%20completion.%20Experimental%20results%20on%20the%20BraTS%202021%20dataset%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMM-Diff%253A%2520Adaptive%2520Multi-Modality%2520Diffusion%2520Network%2520for%2520Missing%2520Modality%250A%2520%2520Imputation%26entry.906535625%3DAghiles%2520Kebaili%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Lapuyade-Lahorgue%2520and%2520Pierre%2520Vera%2520and%2520Su%2520Ruan%26entry.1292438233%3D%2520%2520In%2520clinical%2520practice%252C%2520full%2520imaging%2520is%2520not%2520always%2520feasible%252C%2520often%2520due%2520to%250Acomplex%2520acquisition%2520protocols%252C%2520stringent%2520privacy%2520regulations%252C%2520or%2520specific%250Aclinical%2520needs.%2520However%252C%2520missing%2520MR%2520modalities%2520pose%2520significant%2520challenges%2520for%250Atasks%2520like%2520brain%2520tumor%2520segmentation%252C%2520especially%2520in%2520deep%2520learning-based%250Asegmentation%252C%2520as%2520each%2520modality%2520provides%2520complementary%2520information%2520crucial%2520for%250Aimproving%2520accuracy.%2520A%2520promising%2520solution%2520is%2520missing%2520data%2520imputation%252C%2520where%250Aabsent%2520modalities%2520are%2520generated%2520from%2520available%2520ones.%2520While%2520generative%2520models%250Ahave%2520been%2520widely%2520used%2520for%2520this%2520purpose%252C%2520most%2520state-of-the-art%2520approaches%2520are%250Alimited%2520to%2520single%2520or%2520dual%2520target%2520translations%252C%2520lacking%2520the%2520adaptability%2520to%250Agenerate%2520missing%2520modalities%2520based%2520on%2520varying%2520input%2520configurations.%2520To%2520address%250Athis%252C%2520we%2520propose%2520an%2520Adaptive%2520Multi-Modality%2520Diffusion%2520Network%2520%2528AMM-Diff%2529%252C%2520a%250Anovel%2520diffusion-based%2520generative%2520model%2520capable%2520of%2520handling%2520any%2520number%2520of%2520input%250Amodalities%2520and%2520generating%2520the%2520missing%2520ones.%2520We%2520designed%2520an%2520Image-Frequency%250AFusion%2520Network%2520%2528IFFN%2529%2520that%2520learns%2520a%2520unified%2520feature%2520representation%2520through%2520a%250Aself-supervised%2520pretext%2520task%2520across%2520the%2520full%2520input%2520modalities%2520and%2520their%250Aselected%2520high-frequency%2520Fourier%2520components.%2520The%2520proposed%2520diffusion%2520model%250Aleverages%2520this%2520representation%252C%2520encapsulating%2520prior%2520knowledge%2520of%2520the%2520complete%250Amodalities%252C%2520and%2520combines%2520it%2520with%2520an%2520adaptive%2520reconstruction%2520strategy%2520to%2520achieve%250Amissing%2520modality%2520completion.%2520Experimental%2520results%2520on%2520the%2520BraTS%25202021%2520dataset%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMM-Diff%3A%20Adaptive%20Multi-Modality%20Diffusion%20Network%20for%20Missing%20Modality%0A%20%20Imputation&entry.906535625=Aghiles%20Kebaili%20and%20J%C3%A9r%C3%B4me%20Lapuyade-Lahorgue%20and%20Pierre%20Vera%20and%20Su%20Ruan&entry.1292438233=%20%20In%20clinical%20practice%2C%20full%20imaging%20is%20not%20always%20feasible%2C%20often%20due%20to%0Acomplex%20acquisition%20protocols%2C%20stringent%20privacy%20regulations%2C%20or%20specific%0Aclinical%20needs.%20However%2C%20missing%20MR%20modalities%20pose%20significant%20challenges%20for%0Atasks%20like%20brain%20tumor%20segmentation%2C%20especially%20in%20deep%20learning-based%0Asegmentation%2C%20as%20each%20modality%20provides%20complementary%20information%20crucial%20for%0Aimproving%20accuracy.%20A%20promising%20solution%20is%20missing%20data%20imputation%2C%20where%0Aabsent%20modalities%20are%20generated%20from%20available%20ones.%20While%20generative%20models%0Ahave%20been%20widely%20used%20for%20this%20purpose%2C%20most%20state-of-the-art%20approaches%20are%0Alimited%20to%20single%20or%20dual%20target%20translations%2C%20lacking%20the%20adaptability%20to%0Agenerate%20missing%20modalities%20based%20on%20varying%20input%20configurations.%20To%20address%0Athis%2C%20we%20propose%20an%20Adaptive%20Multi-Modality%20Diffusion%20Network%20%28AMM-Diff%29%2C%20a%0Anovel%20diffusion-based%20generative%20model%20capable%20of%20handling%20any%20number%20of%20input%0Amodalities%20and%20generating%20the%20missing%20ones.%20We%20designed%20an%20Image-Frequency%0AFusion%20Network%20%28IFFN%29%20that%20learns%20a%20unified%20feature%20representation%20through%20a%0Aself-supervised%20pretext%20task%20across%20the%20full%20input%20modalities%20and%20their%0Aselected%20high-frequency%20Fourier%20components.%20The%20proposed%20diffusion%20model%0Aleverages%20this%20representation%2C%20encapsulating%20prior%20knowledge%20of%20the%20complete%0Amodalities%2C%20and%20combines%20it%20with%20an%20adaptive%20reconstruction%20strategy%20to%20achieve%0Amissing%20modality%20completion.%20Experimental%20results%20on%20the%20BraTS%202021%20dataset%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12840v1&entry.124074799=Read"},
{"title": "Robust Representation Consistency Model via Contrastive Denoising", "author": "Jiachen Lei and Julius Berner and Jiongxiao Wang and Zhongzhu Chen and Zhongjia Ba and Kui Ren and Jun Zhu and Anima Anandkumar", "abstract": "  Robustness is essential for deep neural networks, especially in\nsecurity-sensitive applications. To this end, randomized smoothing provides\ntheoretical guarantees for certifying robustness against adversarial\nperturbations. Recently, diffusion models have been successfully employed for\nrandomized smoothing to purify noise-perturbed samples before making\npredictions with a standard classifier. While these methods excel at small\nperturbation radii, they struggle with larger perturbations and incur a\nsignificant computational overhead during inference compared to classical\nmethods. To address this, we reformulate the generative modeling task along the\ndiffusion trajectories in pixel space as a discriminative task in the latent\nspace. Specifically, we use instance discrimination to achieve consistent\nrepresentations along the trajectories by aligning temporally adjacent points.\nAfter fine-tuning based on the learned representations, our model enables\nimplicit denoising-then-classification via a single prediction, substantially\nreducing inference costs. We conduct extensive experiments on various datasets\nand achieve state-of-the-art performance with minimal computation budget during\ninference. For example, our method outperforms the certified accuracy of\ndiffusion-based methods on ImageNet across all perturbation radii by 5.3% on\naverage, with up to 11.6% at larger radii, while reducing inference costs by\n85$\\times$ on average. Codes are available at:\nhttps://github.com/jiachenlei/rRCM.\n", "link": "http://arxiv.org/abs/2501.13094v1", "date": "2025-01-22", "relevancy": 1.7664, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6109}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5858}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Representation%20Consistency%20Model%20via%20Contrastive%20Denoising&body=Title%3A%20Robust%20Representation%20Consistency%20Model%20via%20Contrastive%20Denoising%0AAuthor%3A%20Jiachen%20Lei%20and%20Julius%20Berner%20and%20Jiongxiao%20Wang%20and%20Zhongzhu%20Chen%20and%20Zhongjia%20Ba%20and%20Kui%20Ren%20and%20Jun%20Zhu%20and%20Anima%20Anandkumar%0AAbstract%3A%20%20%20Robustness%20is%20essential%20for%20deep%20neural%20networks%2C%20especially%20in%0Asecurity-sensitive%20applications.%20To%20this%20end%2C%20randomized%20smoothing%20provides%0Atheoretical%20guarantees%20for%20certifying%20robustness%20against%20adversarial%0Aperturbations.%20Recently%2C%20diffusion%20models%20have%20been%20successfully%20employed%20for%0Arandomized%20smoothing%20to%20purify%20noise-perturbed%20samples%20before%20making%0Apredictions%20with%20a%20standard%20classifier.%20While%20these%20methods%20excel%20at%20small%0Aperturbation%20radii%2C%20they%20struggle%20with%20larger%20perturbations%20and%20incur%20a%0Asignificant%20computational%20overhead%20during%20inference%20compared%20to%20classical%0Amethods.%20To%20address%20this%2C%20we%20reformulate%20the%20generative%20modeling%20task%20along%20the%0Adiffusion%20trajectories%20in%20pixel%20space%20as%20a%20discriminative%20task%20in%20the%20latent%0Aspace.%20Specifically%2C%20we%20use%20instance%20discrimination%20to%20achieve%20consistent%0Arepresentations%20along%20the%20trajectories%20by%20aligning%20temporally%20adjacent%20points.%0AAfter%20fine-tuning%20based%20on%20the%20learned%20representations%2C%20our%20model%20enables%0Aimplicit%20denoising-then-classification%20via%20a%20single%20prediction%2C%20substantially%0Areducing%20inference%20costs.%20We%20conduct%20extensive%20experiments%20on%20various%20datasets%0Aand%20achieve%20state-of-the-art%20performance%20with%20minimal%20computation%20budget%20during%0Ainference.%20For%20example%2C%20our%20method%20outperforms%20the%20certified%20accuracy%20of%0Adiffusion-based%20methods%20on%20ImageNet%20across%20all%20perturbation%20radii%20by%205.3%25%20on%0Aaverage%2C%20with%20up%20to%2011.6%25%20at%20larger%20radii%2C%20while%20reducing%20inference%20costs%20by%0A85%24%5Ctimes%24%20on%20average.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/jiachenlei/rRCM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13094v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Representation%2520Consistency%2520Model%2520via%2520Contrastive%2520Denoising%26entry.906535625%3DJiachen%2520Lei%2520and%2520Julius%2520Berner%2520and%2520Jiongxiao%2520Wang%2520and%2520Zhongzhu%2520Chen%2520and%2520Zhongjia%2520Ba%2520and%2520Kui%2520Ren%2520and%2520Jun%2520Zhu%2520and%2520Anima%2520Anandkumar%26entry.1292438233%3D%2520%2520Robustness%2520is%2520essential%2520for%2520deep%2520neural%2520networks%252C%2520especially%2520in%250Asecurity-sensitive%2520applications.%2520To%2520this%2520end%252C%2520randomized%2520smoothing%2520provides%250Atheoretical%2520guarantees%2520for%2520certifying%2520robustness%2520against%2520adversarial%250Aperturbations.%2520Recently%252C%2520diffusion%2520models%2520have%2520been%2520successfully%2520employed%2520for%250Arandomized%2520smoothing%2520to%2520purify%2520noise-perturbed%2520samples%2520before%2520making%250Apredictions%2520with%2520a%2520standard%2520classifier.%2520While%2520these%2520methods%2520excel%2520at%2520small%250Aperturbation%2520radii%252C%2520they%2520struggle%2520with%2520larger%2520perturbations%2520and%2520incur%2520a%250Asignificant%2520computational%2520overhead%2520during%2520inference%2520compared%2520to%2520classical%250Amethods.%2520To%2520address%2520this%252C%2520we%2520reformulate%2520the%2520generative%2520modeling%2520task%2520along%2520the%250Adiffusion%2520trajectories%2520in%2520pixel%2520space%2520as%2520a%2520discriminative%2520task%2520in%2520the%2520latent%250Aspace.%2520Specifically%252C%2520we%2520use%2520instance%2520discrimination%2520to%2520achieve%2520consistent%250Arepresentations%2520along%2520the%2520trajectories%2520by%2520aligning%2520temporally%2520adjacent%2520points.%250AAfter%2520fine-tuning%2520based%2520on%2520the%2520learned%2520representations%252C%2520our%2520model%2520enables%250Aimplicit%2520denoising-then-classification%2520via%2520a%2520single%2520prediction%252C%2520substantially%250Areducing%2520inference%2520costs.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520various%2520datasets%250Aand%2520achieve%2520state-of-the-art%2520performance%2520with%2520minimal%2520computation%2520budget%2520during%250Ainference.%2520For%2520example%252C%2520our%2520method%2520outperforms%2520the%2520certified%2520accuracy%2520of%250Adiffusion-based%2520methods%2520on%2520ImageNet%2520across%2520all%2520perturbation%2520radii%2520by%25205.3%2525%2520on%250Aaverage%252C%2520with%2520up%2520to%252011.6%2525%2520at%2520larger%2520radii%252C%2520while%2520reducing%2520inference%2520costs%2520by%250A85%2524%255Ctimes%2524%2520on%2520average.%2520Codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/jiachenlei/rRCM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13094v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Representation%20Consistency%20Model%20via%20Contrastive%20Denoising&entry.906535625=Jiachen%20Lei%20and%20Julius%20Berner%20and%20Jiongxiao%20Wang%20and%20Zhongzhu%20Chen%20and%20Zhongjia%20Ba%20and%20Kui%20Ren%20and%20Jun%20Zhu%20and%20Anima%20Anandkumar&entry.1292438233=%20%20Robustness%20is%20essential%20for%20deep%20neural%20networks%2C%20especially%20in%0Asecurity-sensitive%20applications.%20To%20this%20end%2C%20randomized%20smoothing%20provides%0Atheoretical%20guarantees%20for%20certifying%20robustness%20against%20adversarial%0Aperturbations.%20Recently%2C%20diffusion%20models%20have%20been%20successfully%20employed%20for%0Arandomized%20smoothing%20to%20purify%20noise-perturbed%20samples%20before%20making%0Apredictions%20with%20a%20standard%20classifier.%20While%20these%20methods%20excel%20at%20small%0Aperturbation%20radii%2C%20they%20struggle%20with%20larger%20perturbations%20and%20incur%20a%0Asignificant%20computational%20overhead%20during%20inference%20compared%20to%20classical%0Amethods.%20To%20address%20this%2C%20we%20reformulate%20the%20generative%20modeling%20task%20along%20the%0Adiffusion%20trajectories%20in%20pixel%20space%20as%20a%20discriminative%20task%20in%20the%20latent%0Aspace.%20Specifically%2C%20we%20use%20instance%20discrimination%20to%20achieve%20consistent%0Arepresentations%20along%20the%20trajectories%20by%20aligning%20temporally%20adjacent%20points.%0AAfter%20fine-tuning%20based%20on%20the%20learned%20representations%2C%20our%20model%20enables%0Aimplicit%20denoising-then-classification%20via%20a%20single%20prediction%2C%20substantially%0Areducing%20inference%20costs.%20We%20conduct%20extensive%20experiments%20on%20various%20datasets%0Aand%20achieve%20state-of-the-art%20performance%20with%20minimal%20computation%20budget%20during%0Ainference.%20For%20example%2C%20our%20method%20outperforms%20the%20certified%20accuracy%20of%0Adiffusion-based%20methods%20on%20ImageNet%20across%20all%20perturbation%20radii%20by%205.3%25%20on%0Aaverage%2C%20with%20up%20to%2011.6%25%20at%20larger%20radii%2C%20while%20reducing%20inference%20costs%20by%0A85%24%5Ctimes%24%20on%20average.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/jiachenlei/rRCM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13094v1&entry.124074799=Read"},
{"title": "Distributional Counterfactual Explanations With Optimal Transport", "author": "Lei You and Lele Cao and Mattias Nilsson and Bo Zhao and Lei Lei", "abstract": "  Counterfactual explanations (CE) are the de facto method for providing\ninsights into black-box decision-making models by identifying alternative\ninputs that lead to different outcomes. However, existing CE approaches,\nincluding group and global methods, focus predominantly on specific input\nmodifications, lacking the ability to capture nuanced distributional\ncharacteristics that influence model outcomes across the entire input-output\nspectrum. This paper proposes distributional counterfactual explanation (DCE),\nshifting focus to the distributional properties of observed and counterfactual\ndata, thus providing broader insights. DCE is particularly beneficial for\nstakeholders making strategic decisions based on statistical data analysis, as\nit makes the statistical distribution of the counterfactual resembles the one\nof the factual when aligning model outputs with a target\ndistribution\\textemdash something that the existing CE methods cannot fully\nachieve. We leverage optimal transport (OT) to formulate a chance-constrained\noptimization problem, deriving a counterfactual distribution aligned with its\nfactual counterpart, supported by statistical confidence. The efficacy of this\napproach is demonstrated through experiments, highlighting its potential to\nprovide deeper insights into decision-making models.\n", "link": "http://arxiv.org/abs/2401.13112v5", "date": "2025-01-22", "relevancy": 0.8673, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.445}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributional%20Counterfactual%20Explanations%20With%20Optimal%20Transport&body=Title%3A%20Distributional%20Counterfactual%20Explanations%20With%20Optimal%20Transport%0AAuthor%3A%20Lei%20You%20and%20Lele%20Cao%20and%20Mattias%20Nilsson%20and%20Bo%20Zhao%20and%20Lei%20Lei%0AAbstract%3A%20%20%20Counterfactual%20explanations%20%28CE%29%20are%20the%20de%20facto%20method%20for%20providing%0Ainsights%20into%20black-box%20decision-making%20models%20by%20identifying%20alternative%0Ainputs%20that%20lead%20to%20different%20outcomes.%20However%2C%20existing%20CE%20approaches%2C%0Aincluding%20group%20and%20global%20methods%2C%20focus%20predominantly%20on%20specific%20input%0Amodifications%2C%20lacking%20the%20ability%20to%20capture%20nuanced%20distributional%0Acharacteristics%20that%20influence%20model%20outcomes%20across%20the%20entire%20input-output%0Aspectrum.%20This%20paper%20proposes%20distributional%20counterfactual%20explanation%20%28DCE%29%2C%0Ashifting%20focus%20to%20the%20distributional%20properties%20of%20observed%20and%20counterfactual%0Adata%2C%20thus%20providing%20broader%20insights.%20DCE%20is%20particularly%20beneficial%20for%0Astakeholders%20making%20strategic%20decisions%20based%20on%20statistical%20data%20analysis%2C%20as%0Ait%20makes%20the%20statistical%20distribution%20of%20the%20counterfactual%20resembles%20the%20one%0Aof%20the%20factual%20when%20aligning%20model%20outputs%20with%20a%20target%0Adistribution%5Ctextemdash%20something%20that%20the%20existing%20CE%20methods%20cannot%20fully%0Aachieve.%20We%20leverage%20optimal%20transport%20%28OT%29%20to%20formulate%20a%20chance-constrained%0Aoptimization%20problem%2C%20deriving%20a%20counterfactual%20distribution%20aligned%20with%20its%0Afactual%20counterpart%2C%20supported%20by%20statistical%20confidence.%20The%20efficacy%20of%20this%0Aapproach%20is%20demonstrated%20through%20experiments%2C%20highlighting%20its%20potential%20to%0Aprovide%20deeper%20insights%20into%20decision-making%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13112v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributional%2520Counterfactual%2520Explanations%2520With%2520Optimal%2520Transport%26entry.906535625%3DLei%2520You%2520and%2520Lele%2520Cao%2520and%2520Mattias%2520Nilsson%2520and%2520Bo%2520Zhao%2520and%2520Lei%2520Lei%26entry.1292438233%3D%2520%2520Counterfactual%2520explanations%2520%2528CE%2529%2520are%2520the%2520de%2520facto%2520method%2520for%2520providing%250Ainsights%2520into%2520black-box%2520decision-making%2520models%2520by%2520identifying%2520alternative%250Ainputs%2520that%2520lead%2520to%2520different%2520outcomes.%2520However%252C%2520existing%2520CE%2520approaches%252C%250Aincluding%2520group%2520and%2520global%2520methods%252C%2520focus%2520predominantly%2520on%2520specific%2520input%250Amodifications%252C%2520lacking%2520the%2520ability%2520to%2520capture%2520nuanced%2520distributional%250Acharacteristics%2520that%2520influence%2520model%2520outcomes%2520across%2520the%2520entire%2520input-output%250Aspectrum.%2520This%2520paper%2520proposes%2520distributional%2520counterfactual%2520explanation%2520%2528DCE%2529%252C%250Ashifting%2520focus%2520to%2520the%2520distributional%2520properties%2520of%2520observed%2520and%2520counterfactual%250Adata%252C%2520thus%2520providing%2520broader%2520insights.%2520DCE%2520is%2520particularly%2520beneficial%2520for%250Astakeholders%2520making%2520strategic%2520decisions%2520based%2520on%2520statistical%2520data%2520analysis%252C%2520as%250Ait%2520makes%2520the%2520statistical%2520distribution%2520of%2520the%2520counterfactual%2520resembles%2520the%2520one%250Aof%2520the%2520factual%2520when%2520aligning%2520model%2520outputs%2520with%2520a%2520target%250Adistribution%255Ctextemdash%2520something%2520that%2520the%2520existing%2520CE%2520methods%2520cannot%2520fully%250Aachieve.%2520We%2520leverage%2520optimal%2520transport%2520%2528OT%2529%2520to%2520formulate%2520a%2520chance-constrained%250Aoptimization%2520problem%252C%2520deriving%2520a%2520counterfactual%2520distribution%2520aligned%2520with%2520its%250Afactual%2520counterpart%252C%2520supported%2520by%2520statistical%2520confidence.%2520The%2520efficacy%2520of%2520this%250Aapproach%2520is%2520demonstrated%2520through%2520experiments%252C%2520highlighting%2520its%2520potential%2520to%250Aprovide%2520deeper%2520insights%2520into%2520decision-making%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13112v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributional%20Counterfactual%20Explanations%20With%20Optimal%20Transport&entry.906535625=Lei%20You%20and%20Lele%20Cao%20and%20Mattias%20Nilsson%20and%20Bo%20Zhao%20and%20Lei%20Lei&entry.1292438233=%20%20Counterfactual%20explanations%20%28CE%29%20are%20the%20de%20facto%20method%20for%20providing%0Ainsights%20into%20black-box%20decision-making%20models%20by%20identifying%20alternative%0Ainputs%20that%20lead%20to%20different%20outcomes.%20However%2C%20existing%20CE%20approaches%2C%0Aincluding%20group%20and%20global%20methods%2C%20focus%20predominantly%20on%20specific%20input%0Amodifications%2C%20lacking%20the%20ability%20to%20capture%20nuanced%20distributional%0Acharacteristics%20that%20influence%20model%20outcomes%20across%20the%20entire%20input-output%0Aspectrum.%20This%20paper%20proposes%20distributional%20counterfactual%20explanation%20%28DCE%29%2C%0Ashifting%20focus%20to%20the%20distributional%20properties%20of%20observed%20and%20counterfactual%0Adata%2C%20thus%20providing%20broader%20insights.%20DCE%20is%20particularly%20beneficial%20for%0Astakeholders%20making%20strategic%20decisions%20based%20on%20statistical%20data%20analysis%2C%20as%0Ait%20makes%20the%20statistical%20distribution%20of%20the%20counterfactual%20resembles%20the%20one%0Aof%20the%20factual%20when%20aligning%20model%20outputs%20with%20a%20target%0Adistribution%5Ctextemdash%20something%20that%20the%20existing%20CE%20methods%20cannot%20fully%0Aachieve.%20We%20leverage%20optimal%20transport%20%28OT%29%20to%20formulate%20a%20chance-constrained%0Aoptimization%20problem%2C%20deriving%20a%20counterfactual%20distribution%20aligned%20with%20its%0Afactual%20counterpart%2C%20supported%20by%20statistical%20confidence.%20The%20efficacy%20of%20this%0Aapproach%20is%20demonstrated%20through%20experiments%2C%20highlighting%20its%20potential%20to%0Aprovide%20deeper%20insights%20into%20decision-making%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13112v5&entry.124074799=Read"},
{"title": "Unveiling Zero-Space Detection: A Novel Framework for Autonomous\n  Ransomware Identification in High-Velocity Environments", "author": "Lafedi Svet and Arthur Brightwell and Augustus Wildflower and Cecily Marshwood", "abstract": "  Modern cybersecurity landscapes increasingly demand sophisticated detection\nframeworks capable of identifying evolving threats with precision and\nadaptability. The proposed Zero-Space Detection framework introduces a novel\napproach that dynamically identifies latent behavioral patterns through\nunsupervised clustering and advanced deep learning techniques. Designed to\naddress the limitations of signature-based and heuristic methods, it operates\neffectively in high-velocity environments by integrating multi-phase filtering\nand ensemble learning for refined decision-making. Experimental evaluation\nreveals high detection rates across diverse ransomware families, including\nLockBit, Conti, REvil, and BlackMatter, while maintaining low false positive\nrates and scalable performance. Computational overhead remains minimal, with\naverage processing times ensuring compatibility with real-time systems even\nunder peak operational loads. The framework demonstrates resilience against\nadversarial strategies such as obfuscation and encryption speed variability,\nwhich frequently challenge conventional detection systems. Analysis across\nmultiple data sources highlights its versatility in handling diverse file types\nand operational contexts. Comprehensive metrics, including detection\nprobability, latency, and resource efficiency, validate its efficacy under\nreal-world conditions. Through its modular architecture, the framework achieves\nseamless integration with existing cybersecurity infrastructures without\nsignificant reconfiguration. The results demonstrate its robustness and\nscalability, offering a transformative paradigm for ransomware identification\nin dynamic and resource-constrained environments.\n", "link": "http://arxiv.org/abs/2501.12811v1", "date": "2025-01-22", "relevancy": 1.3287, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4442}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4436}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.44}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Zero-Space%20Detection%3A%20A%20Novel%20Framework%20for%20Autonomous%0A%20%20Ransomware%20Identification%20in%20High-Velocity%20Environments&body=Title%3A%20Unveiling%20Zero-Space%20Detection%3A%20A%20Novel%20Framework%20for%20Autonomous%0A%20%20Ransomware%20Identification%20in%20High-Velocity%20Environments%0AAuthor%3A%20Lafedi%20Svet%20and%20Arthur%20Brightwell%20and%20Augustus%20Wildflower%20and%20Cecily%20Marshwood%0AAbstract%3A%20%20%20Modern%20cybersecurity%20landscapes%20increasingly%20demand%20sophisticated%20detection%0Aframeworks%20capable%20of%20identifying%20evolving%20threats%20with%20precision%20and%0Aadaptability.%20The%20proposed%20Zero-Space%20Detection%20framework%20introduces%20a%20novel%0Aapproach%20that%20dynamically%20identifies%20latent%20behavioral%20patterns%20through%0Aunsupervised%20clustering%20and%20advanced%20deep%20learning%20techniques.%20Designed%20to%0Aaddress%20the%20limitations%20of%20signature-based%20and%20heuristic%20methods%2C%20it%20operates%0Aeffectively%20in%20high-velocity%20environments%20by%20integrating%20multi-phase%20filtering%0Aand%20ensemble%20learning%20for%20refined%20decision-making.%20Experimental%20evaluation%0Areveals%20high%20detection%20rates%20across%20diverse%20ransomware%20families%2C%20including%0ALockBit%2C%20Conti%2C%20REvil%2C%20and%20BlackMatter%2C%20while%20maintaining%20low%20false%20positive%0Arates%20and%20scalable%20performance.%20Computational%20overhead%20remains%20minimal%2C%20with%0Aaverage%20processing%20times%20ensuring%20compatibility%20with%20real-time%20systems%20even%0Aunder%20peak%20operational%20loads.%20The%20framework%20demonstrates%20resilience%20against%0Aadversarial%20strategies%20such%20as%20obfuscation%20and%20encryption%20speed%20variability%2C%0Awhich%20frequently%20challenge%20conventional%20detection%20systems.%20Analysis%20across%0Amultiple%20data%20sources%20highlights%20its%20versatility%20in%20handling%20diverse%20file%20types%0Aand%20operational%20contexts.%20Comprehensive%20metrics%2C%20including%20detection%0Aprobability%2C%20latency%2C%20and%20resource%20efficiency%2C%20validate%20its%20efficacy%20under%0Areal-world%20conditions.%20Through%20its%20modular%20architecture%2C%20the%20framework%20achieves%0Aseamless%20integration%20with%20existing%20cybersecurity%20infrastructures%20without%0Asignificant%20reconfiguration.%20The%20results%20demonstrate%20its%20robustness%20and%0Ascalability%2C%20offering%20a%20transformative%20paradigm%20for%20ransomware%20identification%0Ain%20dynamic%20and%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12811v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Zero-Space%2520Detection%253A%2520A%2520Novel%2520Framework%2520for%2520Autonomous%250A%2520%2520Ransomware%2520Identification%2520in%2520High-Velocity%2520Environments%26entry.906535625%3DLafedi%2520Svet%2520and%2520Arthur%2520Brightwell%2520and%2520Augustus%2520Wildflower%2520and%2520Cecily%2520Marshwood%26entry.1292438233%3D%2520%2520Modern%2520cybersecurity%2520landscapes%2520increasingly%2520demand%2520sophisticated%2520detection%250Aframeworks%2520capable%2520of%2520identifying%2520evolving%2520threats%2520with%2520precision%2520and%250Aadaptability.%2520The%2520proposed%2520Zero-Space%2520Detection%2520framework%2520introduces%2520a%2520novel%250Aapproach%2520that%2520dynamically%2520identifies%2520latent%2520behavioral%2520patterns%2520through%250Aunsupervised%2520clustering%2520and%2520advanced%2520deep%2520learning%2520techniques.%2520Designed%2520to%250Aaddress%2520the%2520limitations%2520of%2520signature-based%2520and%2520heuristic%2520methods%252C%2520it%2520operates%250Aeffectively%2520in%2520high-velocity%2520environments%2520by%2520integrating%2520multi-phase%2520filtering%250Aand%2520ensemble%2520learning%2520for%2520refined%2520decision-making.%2520Experimental%2520evaluation%250Areveals%2520high%2520detection%2520rates%2520across%2520diverse%2520ransomware%2520families%252C%2520including%250ALockBit%252C%2520Conti%252C%2520REvil%252C%2520and%2520BlackMatter%252C%2520while%2520maintaining%2520low%2520false%2520positive%250Arates%2520and%2520scalable%2520performance.%2520Computational%2520overhead%2520remains%2520minimal%252C%2520with%250Aaverage%2520processing%2520times%2520ensuring%2520compatibility%2520with%2520real-time%2520systems%2520even%250Aunder%2520peak%2520operational%2520loads.%2520The%2520framework%2520demonstrates%2520resilience%2520against%250Aadversarial%2520strategies%2520such%2520as%2520obfuscation%2520and%2520encryption%2520speed%2520variability%252C%250Awhich%2520frequently%2520challenge%2520conventional%2520detection%2520systems.%2520Analysis%2520across%250Amultiple%2520data%2520sources%2520highlights%2520its%2520versatility%2520in%2520handling%2520diverse%2520file%2520types%250Aand%2520operational%2520contexts.%2520Comprehensive%2520metrics%252C%2520including%2520detection%250Aprobability%252C%2520latency%252C%2520and%2520resource%2520efficiency%252C%2520validate%2520its%2520efficacy%2520under%250Areal-world%2520conditions.%2520Through%2520its%2520modular%2520architecture%252C%2520the%2520framework%2520achieves%250Aseamless%2520integration%2520with%2520existing%2520cybersecurity%2520infrastructures%2520without%250Asignificant%2520reconfiguration.%2520The%2520results%2520demonstrate%2520its%2520robustness%2520and%250Ascalability%252C%2520offering%2520a%2520transformative%2520paradigm%2520for%2520ransomware%2520identification%250Ain%2520dynamic%2520and%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12811v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Zero-Space%20Detection%3A%20A%20Novel%20Framework%20for%20Autonomous%0A%20%20Ransomware%20Identification%20in%20High-Velocity%20Environments&entry.906535625=Lafedi%20Svet%20and%20Arthur%20Brightwell%20and%20Augustus%20Wildflower%20and%20Cecily%20Marshwood&entry.1292438233=%20%20Modern%20cybersecurity%20landscapes%20increasingly%20demand%20sophisticated%20detection%0Aframeworks%20capable%20of%20identifying%20evolving%20threats%20with%20precision%20and%0Aadaptability.%20The%20proposed%20Zero-Space%20Detection%20framework%20introduces%20a%20novel%0Aapproach%20that%20dynamically%20identifies%20latent%20behavioral%20patterns%20through%0Aunsupervised%20clustering%20and%20advanced%20deep%20learning%20techniques.%20Designed%20to%0Aaddress%20the%20limitations%20of%20signature-based%20and%20heuristic%20methods%2C%20it%20operates%0Aeffectively%20in%20high-velocity%20environments%20by%20integrating%20multi-phase%20filtering%0Aand%20ensemble%20learning%20for%20refined%20decision-making.%20Experimental%20evaluation%0Areveals%20high%20detection%20rates%20across%20diverse%20ransomware%20families%2C%20including%0ALockBit%2C%20Conti%2C%20REvil%2C%20and%20BlackMatter%2C%20while%20maintaining%20low%20false%20positive%0Arates%20and%20scalable%20performance.%20Computational%20overhead%20remains%20minimal%2C%20with%0Aaverage%20processing%20times%20ensuring%20compatibility%20with%20real-time%20systems%20even%0Aunder%20peak%20operational%20loads.%20The%20framework%20demonstrates%20resilience%20against%0Aadversarial%20strategies%20such%20as%20obfuscation%20and%20encryption%20speed%20variability%2C%0Awhich%20frequently%20challenge%20conventional%20detection%20systems.%20Analysis%20across%0Amultiple%20data%20sources%20highlights%20its%20versatility%20in%20handling%20diverse%20file%20types%0Aand%20operational%20contexts.%20Comprehensive%20metrics%2C%20including%20detection%0Aprobability%2C%20latency%2C%20and%20resource%20efficiency%2C%20validate%20its%20efficacy%20under%0Areal-world%20conditions.%20Through%20its%20modular%20architecture%2C%20the%20framework%20achieves%0Aseamless%20integration%20with%20existing%20cybersecurity%20infrastructures%20without%0Asignificant%20reconfiguration.%20The%20results%20demonstrate%20its%20robustness%20and%0Ascalability%2C%20offering%20a%20transformative%20paradigm%20for%20ransomware%20identification%0Ain%20dynamic%20and%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12811v1&entry.124074799=Read"},
{"title": "It's complicated. The relationship of algorithmic fairness and\n  non-discrimination regulations in the EU AI Act", "author": "Kristof Meding", "abstract": "  What constitutes a fair decision? This question is not only difficult for\nhumans but becomes more challenging when Artificial Intelligence (AI) models\nare used. In light of discriminatory algorithmic behaviors, the EU has recently\npassed the AI Act, which mandates specific rules for AI models, incorporating\nboth traditional legal non-discrimination regulations and machine learning\nbased algorithmic fairness concepts. This paper aims to bridge these two\ndifferent concepts in the AI Act through: First a high-level introduction of\nboth concepts targeting legal and computer science-oriented scholars, and\nsecond an in-depth analysis of the AI Act's relationship between legal\nnon-discrimination regulations and algorithmic fairness. Our analysis reveals\nthree key findings: (1.), most non-discrimination regulations target only\nhigh-risk AI systems. (2.), the regulation of high-risk systems encompasses\nboth data input requirements and output monitoring, though these regulations\nare often inconsistent and raise questions of computational feasibility. (3.)\nRegulations for General Purpose AI Models, such as Large Language Models that\nare not simultaneously classified as high-risk systems, currently lack\nspecificity compared to other regulations. Based on these findings, we\nrecommend developing more specific auditing and testing methodologies for AI\nsystems. This paper aims to serve as a foundation for future interdisciplinary\ncollaboration between legal scholars and computer science-oriented machine\nlearning researchers studying discrimination in AI systems.\n", "link": "http://arxiv.org/abs/2501.12962v1", "date": "2025-01-22", "relevancy": 1.7143, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4637}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20It%27s%20complicated.%20The%20relationship%20of%20algorithmic%20fairness%20and%0A%20%20non-discrimination%20regulations%20in%20the%20EU%20AI%20Act&body=Title%3A%20It%27s%20complicated.%20The%20relationship%20of%20algorithmic%20fairness%20and%0A%20%20non-discrimination%20regulations%20in%20the%20EU%20AI%20Act%0AAuthor%3A%20Kristof%20Meding%0AAbstract%3A%20%20%20What%20constitutes%20a%20fair%20decision%3F%20This%20question%20is%20not%20only%20difficult%20for%0Ahumans%20but%20becomes%20more%20challenging%20when%20Artificial%20Intelligence%20%28AI%29%20models%0Aare%20used.%20In%20light%20of%20discriminatory%20algorithmic%20behaviors%2C%20the%20EU%20has%20recently%0Apassed%20the%20AI%20Act%2C%20which%20mandates%20specific%20rules%20for%20AI%20models%2C%20incorporating%0Aboth%20traditional%20legal%20non-discrimination%20regulations%20and%20machine%20learning%0Abased%20algorithmic%20fairness%20concepts.%20This%20paper%20aims%20to%20bridge%20these%20two%0Adifferent%20concepts%20in%20the%20AI%20Act%20through%3A%20First%20a%20high-level%20introduction%20of%0Aboth%20concepts%20targeting%20legal%20and%20computer%20science-oriented%20scholars%2C%20and%0Asecond%20an%20in-depth%20analysis%20of%20the%20AI%20Act%27s%20relationship%20between%20legal%0Anon-discrimination%20regulations%20and%20algorithmic%20fairness.%20Our%20analysis%20reveals%0Athree%20key%20findings%3A%20%281.%29%2C%20most%20non-discrimination%20regulations%20target%20only%0Ahigh-risk%20AI%20systems.%20%282.%29%2C%20the%20regulation%20of%20high-risk%20systems%20encompasses%0Aboth%20data%20input%20requirements%20and%20output%20monitoring%2C%20though%20these%20regulations%0Aare%20often%20inconsistent%20and%20raise%20questions%20of%20computational%20feasibility.%20%283.%29%0ARegulations%20for%20General%20Purpose%20AI%20Models%2C%20such%20as%20Large%20Language%20Models%20that%0Aare%20not%20simultaneously%20classified%20as%20high-risk%20systems%2C%20currently%20lack%0Aspecificity%20compared%20to%20other%20regulations.%20Based%20on%20these%20findings%2C%20we%0Arecommend%20developing%20more%20specific%20auditing%20and%20testing%20methodologies%20for%20AI%0Asystems.%20This%20paper%20aims%20to%20serve%20as%20a%20foundation%20for%20future%20interdisciplinary%0Acollaboration%20between%20legal%20scholars%20and%20computer%20science-oriented%20machine%0Alearning%20researchers%20studying%20discrimination%20in%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIt%2527s%2520complicated.%2520The%2520relationship%2520of%2520algorithmic%2520fairness%2520and%250A%2520%2520non-discrimination%2520regulations%2520in%2520the%2520EU%2520AI%2520Act%26entry.906535625%3DKristof%2520Meding%26entry.1292438233%3D%2520%2520What%2520constitutes%2520a%2520fair%2520decision%253F%2520This%2520question%2520is%2520not%2520only%2520difficult%2520for%250Ahumans%2520but%2520becomes%2520more%2520challenging%2520when%2520Artificial%2520Intelligence%2520%2528AI%2529%2520models%250Aare%2520used.%2520In%2520light%2520of%2520discriminatory%2520algorithmic%2520behaviors%252C%2520the%2520EU%2520has%2520recently%250Apassed%2520the%2520AI%2520Act%252C%2520which%2520mandates%2520specific%2520rules%2520for%2520AI%2520models%252C%2520incorporating%250Aboth%2520traditional%2520legal%2520non-discrimination%2520regulations%2520and%2520machine%2520learning%250Abased%2520algorithmic%2520fairness%2520concepts.%2520This%2520paper%2520aims%2520to%2520bridge%2520these%2520two%250Adifferent%2520concepts%2520in%2520the%2520AI%2520Act%2520through%253A%2520First%2520a%2520high-level%2520introduction%2520of%250Aboth%2520concepts%2520targeting%2520legal%2520and%2520computer%2520science-oriented%2520scholars%252C%2520and%250Asecond%2520an%2520in-depth%2520analysis%2520of%2520the%2520AI%2520Act%2527s%2520relationship%2520between%2520legal%250Anon-discrimination%2520regulations%2520and%2520algorithmic%2520fairness.%2520Our%2520analysis%2520reveals%250Athree%2520key%2520findings%253A%2520%25281.%2529%252C%2520most%2520non-discrimination%2520regulations%2520target%2520only%250Ahigh-risk%2520AI%2520systems.%2520%25282.%2529%252C%2520the%2520regulation%2520of%2520high-risk%2520systems%2520encompasses%250Aboth%2520data%2520input%2520requirements%2520and%2520output%2520monitoring%252C%2520though%2520these%2520regulations%250Aare%2520often%2520inconsistent%2520and%2520raise%2520questions%2520of%2520computational%2520feasibility.%2520%25283.%2529%250ARegulations%2520for%2520General%2520Purpose%2520AI%2520Models%252C%2520such%2520as%2520Large%2520Language%2520Models%2520that%250Aare%2520not%2520simultaneously%2520classified%2520as%2520high-risk%2520systems%252C%2520currently%2520lack%250Aspecificity%2520compared%2520to%2520other%2520regulations.%2520Based%2520on%2520these%2520findings%252C%2520we%250Arecommend%2520developing%2520more%2520specific%2520auditing%2520and%2520testing%2520methodologies%2520for%2520AI%250Asystems.%2520This%2520paper%2520aims%2520to%2520serve%2520as%2520a%2520foundation%2520for%2520future%2520interdisciplinary%250Acollaboration%2520between%2520legal%2520scholars%2520and%2520computer%2520science-oriented%2520machine%250Alearning%2520researchers%2520studying%2520discrimination%2520in%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=It%27s%20complicated.%20The%20relationship%20of%20algorithmic%20fairness%20and%0A%20%20non-discrimination%20regulations%20in%20the%20EU%20AI%20Act&entry.906535625=Kristof%20Meding&entry.1292438233=%20%20What%20constitutes%20a%20fair%20decision%3F%20This%20question%20is%20not%20only%20difficult%20for%0Ahumans%20but%20becomes%20more%20challenging%20when%20Artificial%20Intelligence%20%28AI%29%20models%0Aare%20used.%20In%20light%20of%20discriminatory%20algorithmic%20behaviors%2C%20the%20EU%20has%20recently%0Apassed%20the%20AI%20Act%2C%20which%20mandates%20specific%20rules%20for%20AI%20models%2C%20incorporating%0Aboth%20traditional%20legal%20non-discrimination%20regulations%20and%20machine%20learning%0Abased%20algorithmic%20fairness%20concepts.%20This%20paper%20aims%20to%20bridge%20these%20two%0Adifferent%20concepts%20in%20the%20AI%20Act%20through%3A%20First%20a%20high-level%20introduction%20of%0Aboth%20concepts%20targeting%20legal%20and%20computer%20science-oriented%20scholars%2C%20and%0Asecond%20an%20in-depth%20analysis%20of%20the%20AI%20Act%27s%20relationship%20between%20legal%0Anon-discrimination%20regulations%20and%20algorithmic%20fairness.%20Our%20analysis%20reveals%0Athree%20key%20findings%3A%20%281.%29%2C%20most%20non-discrimination%20regulations%20target%20only%0Ahigh-risk%20AI%20systems.%20%282.%29%2C%20the%20regulation%20of%20high-risk%20systems%20encompasses%0Aboth%20data%20input%20requirements%20and%20output%20monitoring%2C%20though%20these%20regulations%0Aare%20often%20inconsistent%20and%20raise%20questions%20of%20computational%20feasibility.%20%283.%29%0ARegulations%20for%20General%20Purpose%20AI%20Models%2C%20such%20as%20Large%20Language%20Models%20that%0Aare%20not%20simultaneously%20classified%20as%20high-risk%20systems%2C%20currently%20lack%0Aspecificity%20compared%20to%20other%20regulations.%20Based%20on%20these%20findings%2C%20we%0Arecommend%20developing%20more%20specific%20auditing%20and%20testing%20methodologies%20for%20AI%0Asystems.%20This%20paper%20aims%20to%20serve%20as%20a%20foundation%20for%20future%20interdisciplinary%0Acollaboration%20between%20legal%20scholars%20and%20computer%20science-oriented%20machine%0Alearning%20researchers%20studying%20discrimination%20in%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12962v1&entry.124074799=Read"},
{"title": "Decision Mamba: A Multi-Grained State Space Model with Self-Evolution\n  Regularization for Offline RL", "author": "Qi Lv and Xiang Deng and Gongwei Chen and Michael Yu Wang and Liqiang Nie", "abstract": "  While the conditional sequence modeling with the transformer architecture has\ndemonstrated its effectiveness in dealing with offline reinforcement learning\n(RL) tasks, it is struggle to handle out-of-distribution states and actions.\nExisting work attempts to address this issue by data augmentation with the\nlearned policy or adding extra constraints with the value-based RL algorithm.\nHowever, these studies still fail to overcome the following challenges: (1)\ninsufficiently utilizing the historical temporal information among inter-steps,\n(2) overlooking the local intrastep relationships among return-to-gos (RTGs),\nstates, and actions, (3) overfitting suboptimal trajectories with noisy labels.\nTo address these challenges, we propose Decision Mamba (DM), a novel\nmulti-grained state space model (SSM) with a self-evolving policy learning\nstrategy. DM explicitly models the historical hidden state to extract the\ntemporal information by using the mamba architecture. To capture the\nrelationship among RTG-state-action triplets, a fine-grained SSM module is\ndesigned and integrated into the original coarse-grained SSM in mamba,\nresulting in a novel mamba architecture tailored for offline RL. Finally, to\nmitigate the overfitting issue on noisy trajectories, a self-evolving policy is\nproposed by using progressive regularization. The policy evolves by using its\nown past knowledge to refine the suboptimal actions, thus enhancing its\nrobustness on noisy demonstrations. Extensive experiments on various tasks show\nthat DM outperforms other baselines substantially.\n", "link": "http://arxiv.org/abs/2406.05427v3", "date": "2025-01-22", "relevancy": 1.5608, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5358}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decision%20Mamba%3A%20A%20Multi-Grained%20State%20Space%20Model%20with%20Self-Evolution%0A%20%20Regularization%20for%20Offline%20RL&body=Title%3A%20Decision%20Mamba%3A%20A%20Multi-Grained%20State%20Space%20Model%20with%20Self-Evolution%0A%20%20Regularization%20for%20Offline%20RL%0AAuthor%3A%20Qi%20Lv%20and%20Xiang%20Deng%20and%20Gongwei%20Chen%20and%20Michael%20Yu%20Wang%20and%20Liqiang%20Nie%0AAbstract%3A%20%20%20While%20the%20conditional%20sequence%20modeling%20with%20the%20transformer%20architecture%20has%0Ademonstrated%20its%20effectiveness%20in%20dealing%20with%20offline%20reinforcement%20learning%0A%28RL%29%20tasks%2C%20it%20is%20struggle%20to%20handle%20out-of-distribution%20states%20and%20actions.%0AExisting%20work%20attempts%20to%20address%20this%20issue%20by%20data%20augmentation%20with%20the%0Alearned%20policy%20or%20adding%20extra%20constraints%20with%20the%20value-based%20RL%20algorithm.%0AHowever%2C%20these%20studies%20still%20fail%20to%20overcome%20the%20following%20challenges%3A%20%281%29%0Ainsufficiently%20utilizing%20the%20historical%20temporal%20information%20among%20inter-steps%2C%0A%282%29%20overlooking%20the%20local%20intrastep%20relationships%20among%20return-to-gos%20%28RTGs%29%2C%0Astates%2C%20and%20actions%2C%20%283%29%20overfitting%20suboptimal%20trajectories%20with%20noisy%20labels.%0ATo%20address%20these%20challenges%2C%20we%20propose%20Decision%20Mamba%20%28DM%29%2C%20a%20novel%0Amulti-grained%20state%20space%20model%20%28SSM%29%20with%20a%20self-evolving%20policy%20learning%0Astrategy.%20DM%20explicitly%20models%20the%20historical%20hidden%20state%20to%20extract%20the%0Atemporal%20information%20by%20using%20the%20mamba%20architecture.%20To%20capture%20the%0Arelationship%20among%20RTG-state-action%20triplets%2C%20a%20fine-grained%20SSM%20module%20is%0Adesigned%20and%20integrated%20into%20the%20original%20coarse-grained%20SSM%20in%20mamba%2C%0Aresulting%20in%20a%20novel%20mamba%20architecture%20tailored%20for%20offline%20RL.%20Finally%2C%20to%0Amitigate%20the%20overfitting%20issue%20on%20noisy%20trajectories%2C%20a%20self-evolving%20policy%20is%0Aproposed%20by%20using%20progressive%20regularization.%20The%20policy%20evolves%20by%20using%20its%0Aown%20past%20knowledge%20to%20refine%20the%20suboptimal%20actions%2C%20thus%20enhancing%20its%0Arobustness%20on%20noisy%20demonstrations.%20Extensive%20experiments%20on%20various%20tasks%20show%0Athat%20DM%20outperforms%20other%20baselines%20substantially.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05427v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecision%2520Mamba%253A%2520A%2520Multi-Grained%2520State%2520Space%2520Model%2520with%2520Self-Evolution%250A%2520%2520Regularization%2520for%2520Offline%2520RL%26entry.906535625%3DQi%2520Lv%2520and%2520Xiang%2520Deng%2520and%2520Gongwei%2520Chen%2520and%2520Michael%2520Yu%2520Wang%2520and%2520Liqiang%2520Nie%26entry.1292438233%3D%2520%2520While%2520the%2520conditional%2520sequence%2520modeling%2520with%2520the%2520transformer%2520architecture%2520has%250Ademonstrated%2520its%2520effectiveness%2520in%2520dealing%2520with%2520offline%2520reinforcement%2520learning%250A%2528RL%2529%2520tasks%252C%2520it%2520is%2520struggle%2520to%2520handle%2520out-of-distribution%2520states%2520and%2520actions.%250AExisting%2520work%2520attempts%2520to%2520address%2520this%2520issue%2520by%2520data%2520augmentation%2520with%2520the%250Alearned%2520policy%2520or%2520adding%2520extra%2520constraints%2520with%2520the%2520value-based%2520RL%2520algorithm.%250AHowever%252C%2520these%2520studies%2520still%2520fail%2520to%2520overcome%2520the%2520following%2520challenges%253A%2520%25281%2529%250Ainsufficiently%2520utilizing%2520the%2520historical%2520temporal%2520information%2520among%2520inter-steps%252C%250A%25282%2529%2520overlooking%2520the%2520local%2520intrastep%2520relationships%2520among%2520return-to-gos%2520%2528RTGs%2529%252C%250Astates%252C%2520and%2520actions%252C%2520%25283%2529%2520overfitting%2520suboptimal%2520trajectories%2520with%2520noisy%2520labels.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520Decision%2520Mamba%2520%2528DM%2529%252C%2520a%2520novel%250Amulti-grained%2520state%2520space%2520model%2520%2528SSM%2529%2520with%2520a%2520self-evolving%2520policy%2520learning%250Astrategy.%2520DM%2520explicitly%2520models%2520the%2520historical%2520hidden%2520state%2520to%2520extract%2520the%250Atemporal%2520information%2520by%2520using%2520the%2520mamba%2520architecture.%2520To%2520capture%2520the%250Arelationship%2520among%2520RTG-state-action%2520triplets%252C%2520a%2520fine-grained%2520SSM%2520module%2520is%250Adesigned%2520and%2520integrated%2520into%2520the%2520original%2520coarse-grained%2520SSM%2520in%2520mamba%252C%250Aresulting%2520in%2520a%2520novel%2520mamba%2520architecture%2520tailored%2520for%2520offline%2520RL.%2520Finally%252C%2520to%250Amitigate%2520the%2520overfitting%2520issue%2520on%2520noisy%2520trajectories%252C%2520a%2520self-evolving%2520policy%2520is%250Aproposed%2520by%2520using%2520progressive%2520regularization.%2520The%2520policy%2520evolves%2520by%2520using%2520its%250Aown%2520past%2520knowledge%2520to%2520refine%2520the%2520suboptimal%2520actions%252C%2520thus%2520enhancing%2520its%250Arobustness%2520on%2520noisy%2520demonstrations.%2520Extensive%2520experiments%2520on%2520various%2520tasks%2520show%250Athat%2520DM%2520outperforms%2520other%2520baselines%2520substantially.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05427v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decision%20Mamba%3A%20A%20Multi-Grained%20State%20Space%20Model%20with%20Self-Evolution%0A%20%20Regularization%20for%20Offline%20RL&entry.906535625=Qi%20Lv%20and%20Xiang%20Deng%20and%20Gongwei%20Chen%20and%20Michael%20Yu%20Wang%20and%20Liqiang%20Nie&entry.1292438233=%20%20While%20the%20conditional%20sequence%20modeling%20with%20the%20transformer%20architecture%20has%0Ademonstrated%20its%20effectiveness%20in%20dealing%20with%20offline%20reinforcement%20learning%0A%28RL%29%20tasks%2C%20it%20is%20struggle%20to%20handle%20out-of-distribution%20states%20and%20actions.%0AExisting%20work%20attempts%20to%20address%20this%20issue%20by%20data%20augmentation%20with%20the%0Alearned%20policy%20or%20adding%20extra%20constraints%20with%20the%20value-based%20RL%20algorithm.%0AHowever%2C%20these%20studies%20still%20fail%20to%20overcome%20the%20following%20challenges%3A%20%281%29%0Ainsufficiently%20utilizing%20the%20historical%20temporal%20information%20among%20inter-steps%2C%0A%282%29%20overlooking%20the%20local%20intrastep%20relationships%20among%20return-to-gos%20%28RTGs%29%2C%0Astates%2C%20and%20actions%2C%20%283%29%20overfitting%20suboptimal%20trajectories%20with%20noisy%20labels.%0ATo%20address%20these%20challenges%2C%20we%20propose%20Decision%20Mamba%20%28DM%29%2C%20a%20novel%0Amulti-grained%20state%20space%20model%20%28SSM%29%20with%20a%20self-evolving%20policy%20learning%0Astrategy.%20DM%20explicitly%20models%20the%20historical%20hidden%20state%20to%20extract%20the%0Atemporal%20information%20by%20using%20the%20mamba%20architecture.%20To%20capture%20the%0Arelationship%20among%20RTG-state-action%20triplets%2C%20a%20fine-grained%20SSM%20module%20is%0Adesigned%20and%20integrated%20into%20the%20original%20coarse-grained%20SSM%20in%20mamba%2C%0Aresulting%20in%20a%20novel%20mamba%20architecture%20tailored%20for%20offline%20RL.%20Finally%2C%20to%0Amitigate%20the%20overfitting%20issue%20on%20noisy%20trajectories%2C%20a%20self-evolving%20policy%20is%0Aproposed%20by%20using%20progressive%20regularization.%20The%20policy%20evolves%20by%20using%20its%0Aown%20past%20knowledge%20to%20refine%20the%20suboptimal%20actions%2C%20thus%20enhancing%20its%0Arobustness%20on%20noisy%20demonstrations.%20Extensive%20experiments%20on%20various%20tasks%20show%0Athat%20DM%20outperforms%20other%20baselines%20substantially.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05427v3&entry.124074799=Read"},
{"title": "Fast Ergodic Search with Kernel Functions", "author": "Max Muchen Sun and Ayush Gaggar and Peter Trautman and Todd Murphey", "abstract": "  Ergodic search enables optimal exploration of an information distribution\nwhile guaranteeing the asymptotic coverage of the search space. However,\ncurrent methods typically have exponential computation complexity in the search\nspace dimension and are restricted to Euclidean space. We introduce a\ncomputationally efficient ergodic search method. Our contributions are\ntwo-fold. First, we develop a kernel-based ergodic metric and generalize it\nfrom Euclidean space to Lie groups. We formally prove the proposed metric is\nconsistent with the standard ergodic metric while guaranteeing linear\ncomplexity in the search space dimension. Secondly, we derive the first-order\noptimality condition of the kernel ergodic metric for nonlinear systems, which\nenables efficient trajectory optimization. Comprehensive numerical benchmarks\nshow that the proposed method is at least two orders of magnitude faster than\nthe state-of-the-art algorithm. Finally, we demonstrate the proposed algorithm\nwith a peg-in-hole insertion task. We formulate the problem as a coverage task\nin the space of SE(3) and use a 30-second-long human demonstration as the prior\ndistribution for ergodic coverage. Ergodicity guarantees the asymptotic\nsolution of the peg-in-hole problem so long as the solution resides within the\nprior information distribution, which is seen in the 100% success rate.\n", "link": "http://arxiv.org/abs/2403.01536v2", "date": "2025-01-22", "relevancy": 1.2628, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4254}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4233}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Ergodic%20Search%20with%20Kernel%20Functions&body=Title%3A%20Fast%20Ergodic%20Search%20with%20Kernel%20Functions%0AAuthor%3A%20Max%20Muchen%20Sun%20and%20Ayush%20Gaggar%20and%20Peter%20Trautman%20and%20Todd%20Murphey%0AAbstract%3A%20%20%20Ergodic%20search%20enables%20optimal%20exploration%20of%20an%20information%20distribution%0Awhile%20guaranteeing%20the%20asymptotic%20coverage%20of%20the%20search%20space.%20However%2C%0Acurrent%20methods%20typically%20have%20exponential%20computation%20complexity%20in%20the%20search%0Aspace%20dimension%20and%20are%20restricted%20to%20Euclidean%20space.%20We%20introduce%20a%0Acomputationally%20efficient%20ergodic%20search%20method.%20Our%20contributions%20are%0Atwo-fold.%20First%2C%20we%20develop%20a%20kernel-based%20ergodic%20metric%20and%20generalize%20it%0Afrom%20Euclidean%20space%20to%20Lie%20groups.%20We%20formally%20prove%20the%20proposed%20metric%20is%0Aconsistent%20with%20the%20standard%20ergodic%20metric%20while%20guaranteeing%20linear%0Acomplexity%20in%20the%20search%20space%20dimension.%20Secondly%2C%20we%20derive%20the%20first-order%0Aoptimality%20condition%20of%20the%20kernel%20ergodic%20metric%20for%20nonlinear%20systems%2C%20which%0Aenables%20efficient%20trajectory%20optimization.%20Comprehensive%20numerical%20benchmarks%0Ashow%20that%20the%20proposed%20method%20is%20at%20least%20two%20orders%20of%20magnitude%20faster%20than%0Athe%20state-of-the-art%20algorithm.%20Finally%2C%20we%20demonstrate%20the%20proposed%20algorithm%0Awith%20a%20peg-in-hole%20insertion%20task.%20We%20formulate%20the%20problem%20as%20a%20coverage%20task%0Ain%20the%20space%20of%20SE%283%29%20and%20use%20a%2030-second-long%20human%20demonstration%20as%20the%20prior%0Adistribution%20for%20ergodic%20coverage.%20Ergodicity%20guarantees%20the%20asymptotic%0Asolution%20of%20the%20peg-in-hole%20problem%20so%20long%20as%20the%20solution%20resides%20within%20the%0Aprior%20information%20distribution%2C%20which%20is%20seen%20in%20the%20100%25%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01536v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Ergodic%2520Search%2520with%2520Kernel%2520Functions%26entry.906535625%3DMax%2520Muchen%2520Sun%2520and%2520Ayush%2520Gaggar%2520and%2520Peter%2520Trautman%2520and%2520Todd%2520Murphey%26entry.1292438233%3D%2520%2520Ergodic%2520search%2520enables%2520optimal%2520exploration%2520of%2520an%2520information%2520distribution%250Awhile%2520guaranteeing%2520the%2520asymptotic%2520coverage%2520of%2520the%2520search%2520space.%2520However%252C%250Acurrent%2520methods%2520typically%2520have%2520exponential%2520computation%2520complexity%2520in%2520the%2520search%250Aspace%2520dimension%2520and%2520are%2520restricted%2520to%2520Euclidean%2520space.%2520We%2520introduce%2520a%250Acomputationally%2520efficient%2520ergodic%2520search%2520method.%2520Our%2520contributions%2520are%250Atwo-fold.%2520First%252C%2520we%2520develop%2520a%2520kernel-based%2520ergodic%2520metric%2520and%2520generalize%2520it%250Afrom%2520Euclidean%2520space%2520to%2520Lie%2520groups.%2520We%2520formally%2520prove%2520the%2520proposed%2520metric%2520is%250Aconsistent%2520with%2520the%2520standard%2520ergodic%2520metric%2520while%2520guaranteeing%2520linear%250Acomplexity%2520in%2520the%2520search%2520space%2520dimension.%2520Secondly%252C%2520we%2520derive%2520the%2520first-order%250Aoptimality%2520condition%2520of%2520the%2520kernel%2520ergodic%2520metric%2520for%2520nonlinear%2520systems%252C%2520which%250Aenables%2520efficient%2520trajectory%2520optimization.%2520Comprehensive%2520numerical%2520benchmarks%250Ashow%2520that%2520the%2520proposed%2520method%2520is%2520at%2520least%2520two%2520orders%2520of%2520magnitude%2520faster%2520than%250Athe%2520state-of-the-art%2520algorithm.%2520Finally%252C%2520we%2520demonstrate%2520the%2520proposed%2520algorithm%250Awith%2520a%2520peg-in-hole%2520insertion%2520task.%2520We%2520formulate%2520the%2520problem%2520as%2520a%2520coverage%2520task%250Ain%2520the%2520space%2520of%2520SE%25283%2529%2520and%2520use%2520a%252030-second-long%2520human%2520demonstration%2520as%2520the%2520prior%250Adistribution%2520for%2520ergodic%2520coverage.%2520Ergodicity%2520guarantees%2520the%2520asymptotic%250Asolution%2520of%2520the%2520peg-in-hole%2520problem%2520so%2520long%2520as%2520the%2520solution%2520resides%2520within%2520the%250Aprior%2520information%2520distribution%252C%2520which%2520is%2520seen%2520in%2520the%2520100%2525%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01536v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Ergodic%20Search%20with%20Kernel%20Functions&entry.906535625=Max%20Muchen%20Sun%20and%20Ayush%20Gaggar%20and%20Peter%20Trautman%20and%20Todd%20Murphey&entry.1292438233=%20%20Ergodic%20search%20enables%20optimal%20exploration%20of%20an%20information%20distribution%0Awhile%20guaranteeing%20the%20asymptotic%20coverage%20of%20the%20search%20space.%20However%2C%0Acurrent%20methods%20typically%20have%20exponential%20computation%20complexity%20in%20the%20search%0Aspace%20dimension%20and%20are%20restricted%20to%20Euclidean%20space.%20We%20introduce%20a%0Acomputationally%20efficient%20ergodic%20search%20method.%20Our%20contributions%20are%0Atwo-fold.%20First%2C%20we%20develop%20a%20kernel-based%20ergodic%20metric%20and%20generalize%20it%0Afrom%20Euclidean%20space%20to%20Lie%20groups.%20We%20formally%20prove%20the%20proposed%20metric%20is%0Aconsistent%20with%20the%20standard%20ergodic%20metric%20while%20guaranteeing%20linear%0Acomplexity%20in%20the%20search%20space%20dimension.%20Secondly%2C%20we%20derive%20the%20first-order%0Aoptimality%20condition%20of%20the%20kernel%20ergodic%20metric%20for%20nonlinear%20systems%2C%20which%0Aenables%20efficient%20trajectory%20optimization.%20Comprehensive%20numerical%20benchmarks%0Ashow%20that%20the%20proposed%20method%20is%20at%20least%20two%20orders%20of%20magnitude%20faster%20than%0Athe%20state-of-the-art%20algorithm.%20Finally%2C%20we%20demonstrate%20the%20proposed%20algorithm%0Awith%20a%20peg-in-hole%20insertion%20task.%20We%20formulate%20the%20problem%20as%20a%20coverage%20task%0Ain%20the%20space%20of%20SE%283%29%20and%20use%20a%2030-second-long%20human%20demonstration%20as%20the%20prior%0Adistribution%20for%20ergodic%20coverage.%20Ergodicity%20guarantees%20the%20asymptotic%0Asolution%20of%20the%20peg-in-hole%20problem%20so%20long%20as%20the%20solution%20resides%20within%20the%0Aprior%20information%20distribution%2C%20which%20is%20seen%20in%20the%20100%25%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01536v2&entry.124074799=Read"},
{"title": "Locate, Assign, Refine: Taming Customized Promptable Image Inpainting", "author": "Yulin Pan and Chaojie Mao and Zeyinzi Jiang and Zhen Han and Jingfeng Zhang and Xiangteng He", "abstract": "  Prior studies have made significant progress in image inpainting guided by\neither text description or subject image. However, the research on inpainting\nwith flexible guidance or control, i.e., text-only, image-only, and their\ncombination, is still in the early stage. Therefore, in this paper, we\nintroduce the multimodal promptable image inpainting project: a new task model,\nand data for taming customized image inpainting. We propose LAR-Gen, a novel\napproach for image inpainting that enables seamless inpainting of specific\nregion in images corresponding to the mask prompt, incorporating both the text\nprompt and image prompt. Our LAR-Gen adopts a coarse-to-fine manner to ensure\nthe context consistency of source image, subject identity consistency, local\nsemantic consistency to the text description, and smoothness consistency. It\nconsists of three mechanisms: (i) Locate mechanism: concatenating the noise\nwith masked scene image to achieve precise regional editing, (ii) Assign\nmechanism: employing decoupled cross-attention mechanism to accommodate\nmulti-modal guidance, and (iii) Refine mechanism: using a novel RefineNet to\nsupplement subject details. Additionally, to address the issue of scarce\ntraining data, we introduce a novel data engine to automatically extract\nsubstantial pairs of data consisting of local text prompts and corresponding\nvisual instances from a vast image data, leveraging publicly available\npre-trained large models. Extensive experiments and various application\nscenarios demonstrate the superiority of LAR-Gen in terms of both identity\npreservation and text semantic consistency.\n", "link": "http://arxiv.org/abs/2403.19534v2", "date": "2025-01-22", "relevancy": 1.6523, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5721}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5564}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.54}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locate%2C%20Assign%2C%20Refine%3A%20Taming%20Customized%20Promptable%20Image%20Inpainting&body=Title%3A%20Locate%2C%20Assign%2C%20Refine%3A%20Taming%20Customized%20Promptable%20Image%20Inpainting%0AAuthor%3A%20Yulin%20Pan%20and%20Chaojie%20Mao%20and%20Zeyinzi%20Jiang%20and%20Zhen%20Han%20and%20Jingfeng%20Zhang%20and%20Xiangteng%20He%0AAbstract%3A%20%20%20Prior%20studies%20have%20made%20significant%20progress%20in%20image%20inpainting%20guided%20by%0Aeither%20text%20description%20or%20subject%20image.%20However%2C%20the%20research%20on%20inpainting%0Awith%20flexible%20guidance%20or%20control%2C%20i.e.%2C%20text-only%2C%20image-only%2C%20and%20their%0Acombination%2C%20is%20still%20in%20the%20early%20stage.%20Therefore%2C%20in%20this%20paper%2C%20we%0Aintroduce%20the%20multimodal%20promptable%20image%20inpainting%20project%3A%20a%20new%20task%20model%2C%0Aand%20data%20for%20taming%20customized%20image%20inpainting.%20We%20propose%20LAR-Gen%2C%20a%20novel%0Aapproach%20for%20image%20inpainting%20that%20enables%20seamless%20inpainting%20of%20specific%0Aregion%20in%20images%20corresponding%20to%20the%20mask%20prompt%2C%20incorporating%20both%20the%20text%0Aprompt%20and%20image%20prompt.%20Our%20LAR-Gen%20adopts%20a%20coarse-to-fine%20manner%20to%20ensure%0Athe%20context%20consistency%20of%20source%20image%2C%20subject%20identity%20consistency%2C%20local%0Asemantic%20consistency%20to%20the%20text%20description%2C%20and%20smoothness%20consistency.%20It%0Aconsists%20of%20three%20mechanisms%3A%20%28i%29%20Locate%20mechanism%3A%20concatenating%20the%20noise%0Awith%20masked%20scene%20image%20to%20achieve%20precise%20regional%20editing%2C%20%28ii%29%20Assign%0Amechanism%3A%20employing%20decoupled%20cross-attention%20mechanism%20to%20accommodate%0Amulti-modal%20guidance%2C%20and%20%28iii%29%20Refine%20mechanism%3A%20using%20a%20novel%20RefineNet%20to%0Asupplement%20subject%20details.%20Additionally%2C%20to%20address%20the%20issue%20of%20scarce%0Atraining%20data%2C%20we%20introduce%20a%20novel%20data%20engine%20to%20automatically%20extract%0Asubstantial%20pairs%20of%20data%20consisting%20of%20local%20text%20prompts%20and%20corresponding%0Avisual%20instances%20from%20a%20vast%20image%20data%2C%20leveraging%20publicly%20available%0Apre-trained%20large%20models.%20Extensive%20experiments%20and%20various%20application%0Ascenarios%20demonstrate%20the%20superiority%20of%20LAR-Gen%20in%20terms%20of%20both%20identity%0Apreservation%20and%20text%20semantic%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocate%252C%2520Assign%252C%2520Refine%253A%2520Taming%2520Customized%2520Promptable%2520Image%2520Inpainting%26entry.906535625%3DYulin%2520Pan%2520and%2520Chaojie%2520Mao%2520and%2520Zeyinzi%2520Jiang%2520and%2520Zhen%2520Han%2520and%2520Jingfeng%2520Zhang%2520and%2520Xiangteng%2520He%26entry.1292438233%3D%2520%2520Prior%2520studies%2520have%2520made%2520significant%2520progress%2520in%2520image%2520inpainting%2520guided%2520by%250Aeither%2520text%2520description%2520or%2520subject%2520image.%2520However%252C%2520the%2520research%2520on%2520inpainting%250Awith%2520flexible%2520guidance%2520or%2520control%252C%2520i.e.%252C%2520text-only%252C%2520image-only%252C%2520and%2520their%250Acombination%252C%2520is%2520still%2520in%2520the%2520early%2520stage.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520multimodal%2520promptable%2520image%2520inpainting%2520project%253A%2520a%2520new%2520task%2520model%252C%250Aand%2520data%2520for%2520taming%2520customized%2520image%2520inpainting.%2520We%2520propose%2520LAR-Gen%252C%2520a%2520novel%250Aapproach%2520for%2520image%2520inpainting%2520that%2520enables%2520seamless%2520inpainting%2520of%2520specific%250Aregion%2520in%2520images%2520corresponding%2520to%2520the%2520mask%2520prompt%252C%2520incorporating%2520both%2520the%2520text%250Aprompt%2520and%2520image%2520prompt.%2520Our%2520LAR-Gen%2520adopts%2520a%2520coarse-to-fine%2520manner%2520to%2520ensure%250Athe%2520context%2520consistency%2520of%2520source%2520image%252C%2520subject%2520identity%2520consistency%252C%2520local%250Asemantic%2520consistency%2520to%2520the%2520text%2520description%252C%2520and%2520smoothness%2520consistency.%2520It%250Aconsists%2520of%2520three%2520mechanisms%253A%2520%2528i%2529%2520Locate%2520mechanism%253A%2520concatenating%2520the%2520noise%250Awith%2520masked%2520scene%2520image%2520to%2520achieve%2520precise%2520regional%2520editing%252C%2520%2528ii%2529%2520Assign%250Amechanism%253A%2520employing%2520decoupled%2520cross-attention%2520mechanism%2520to%2520accommodate%250Amulti-modal%2520guidance%252C%2520and%2520%2528iii%2529%2520Refine%2520mechanism%253A%2520using%2520a%2520novel%2520RefineNet%2520to%250Asupplement%2520subject%2520details.%2520Additionally%252C%2520to%2520address%2520the%2520issue%2520of%2520scarce%250Atraining%2520data%252C%2520we%2520introduce%2520a%2520novel%2520data%2520engine%2520to%2520automatically%2520extract%250Asubstantial%2520pairs%2520of%2520data%2520consisting%2520of%2520local%2520text%2520prompts%2520and%2520corresponding%250Avisual%2520instances%2520from%2520a%2520vast%2520image%2520data%252C%2520leveraging%2520publicly%2520available%250Apre-trained%2520large%2520models.%2520Extensive%2520experiments%2520and%2520various%2520application%250Ascenarios%2520demonstrate%2520the%2520superiority%2520of%2520LAR-Gen%2520in%2520terms%2520of%2520both%2520identity%250Apreservation%2520and%2520text%2520semantic%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locate%2C%20Assign%2C%20Refine%3A%20Taming%20Customized%20Promptable%20Image%20Inpainting&entry.906535625=Yulin%20Pan%20and%20Chaojie%20Mao%20and%20Zeyinzi%20Jiang%20and%20Zhen%20Han%20and%20Jingfeng%20Zhang%20and%20Xiangteng%20He&entry.1292438233=%20%20Prior%20studies%20have%20made%20significant%20progress%20in%20image%20inpainting%20guided%20by%0Aeither%20text%20description%20or%20subject%20image.%20However%2C%20the%20research%20on%20inpainting%0Awith%20flexible%20guidance%20or%20control%2C%20i.e.%2C%20text-only%2C%20image-only%2C%20and%20their%0Acombination%2C%20is%20still%20in%20the%20early%20stage.%20Therefore%2C%20in%20this%20paper%2C%20we%0Aintroduce%20the%20multimodal%20promptable%20image%20inpainting%20project%3A%20a%20new%20task%20model%2C%0Aand%20data%20for%20taming%20customized%20image%20inpainting.%20We%20propose%20LAR-Gen%2C%20a%20novel%0Aapproach%20for%20image%20inpainting%20that%20enables%20seamless%20inpainting%20of%20specific%0Aregion%20in%20images%20corresponding%20to%20the%20mask%20prompt%2C%20incorporating%20both%20the%20text%0Aprompt%20and%20image%20prompt.%20Our%20LAR-Gen%20adopts%20a%20coarse-to-fine%20manner%20to%20ensure%0Athe%20context%20consistency%20of%20source%20image%2C%20subject%20identity%20consistency%2C%20local%0Asemantic%20consistency%20to%20the%20text%20description%2C%20and%20smoothness%20consistency.%20It%0Aconsists%20of%20three%20mechanisms%3A%20%28i%29%20Locate%20mechanism%3A%20concatenating%20the%20noise%0Awith%20masked%20scene%20image%20to%20achieve%20precise%20regional%20editing%2C%20%28ii%29%20Assign%0Amechanism%3A%20employing%20decoupled%20cross-attention%20mechanism%20to%20accommodate%0Amulti-modal%20guidance%2C%20and%20%28iii%29%20Refine%20mechanism%3A%20using%20a%20novel%20RefineNet%20to%0Asupplement%20subject%20details.%20Additionally%2C%20to%20address%20the%20issue%20of%20scarce%0Atraining%20data%2C%20we%20introduce%20a%20novel%20data%20engine%20to%20automatically%20extract%0Asubstantial%20pairs%20of%20data%20consisting%20of%20local%20text%20prompts%20and%20corresponding%0Avisual%20instances%20from%20a%20vast%20image%20data%2C%20leveraging%20publicly%20available%0Apre-trained%20large%20models.%20Extensive%20experiments%20and%20various%20application%0Ascenarios%20demonstrate%20the%20superiority%20of%20LAR-Gen%20in%20terms%20of%20both%20identity%0Apreservation%20and%20text%20semantic%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19534v2&entry.124074799=Read"},
{"title": "A Structural Complexity Analysis of Hierarchical Task Network Planning", "author": "Cornelius Brand and Robert Ganian and Fionn Mc Inerney and Simon Wietheger", "abstract": "  We perform a refined complexity-theoretic analysis of three classical\nproblems in the context of Hierarchical Task Network Planning: the verification\nof a provided plan, whether an executable plan exists, and whether a given\nstate can be reached. Our focus lies on identifying structural properties which\nyield tractability. We obtain new polynomial algorithms for all three problems\non a natural class of primitive networks, along with corresponding lower\nbounds. We also obtain an algorithmic meta-theorem for lifting polynomial-time\nsolvability from primitive to general task networks, and prove that its\npreconditions are tight. Finally, we analyze the parameterized complexity of\nthe three problems.\n", "link": "http://arxiv.org/abs/2401.14174v2", "date": "2025-01-22", "relevancy": 1.6262, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4066}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Structural%20Complexity%20Analysis%20of%20Hierarchical%20Task%20Network%20Planning&body=Title%3A%20A%20Structural%20Complexity%20Analysis%20of%20Hierarchical%20Task%20Network%20Planning%0AAuthor%3A%20Cornelius%20Brand%20and%20Robert%20Ganian%20and%20Fionn%20Mc%20Inerney%20and%20Simon%20Wietheger%0AAbstract%3A%20%20%20We%20perform%20a%20refined%20complexity-theoretic%20analysis%20of%20three%20classical%0Aproblems%20in%20the%20context%20of%20Hierarchical%20Task%20Network%20Planning%3A%20the%20verification%0Aof%20a%20provided%20plan%2C%20whether%20an%20executable%20plan%20exists%2C%20and%20whether%20a%20given%0Astate%20can%20be%20reached.%20Our%20focus%20lies%20on%20identifying%20structural%20properties%20which%0Ayield%20tractability.%20We%20obtain%20new%20polynomial%20algorithms%20for%20all%20three%20problems%0Aon%20a%20natural%20class%20of%20primitive%20networks%2C%20along%20with%20corresponding%20lower%0Abounds.%20We%20also%20obtain%20an%20algorithmic%20meta-theorem%20for%20lifting%20polynomial-time%0Asolvability%20from%20primitive%20to%20general%20task%20networks%2C%20and%20prove%20that%20its%0Apreconditions%20are%20tight.%20Finally%2C%20we%20analyze%20the%20parameterized%20complexity%20of%0Athe%20three%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.14174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Structural%2520Complexity%2520Analysis%2520of%2520Hierarchical%2520Task%2520Network%2520Planning%26entry.906535625%3DCornelius%2520Brand%2520and%2520Robert%2520Ganian%2520and%2520Fionn%2520Mc%2520Inerney%2520and%2520Simon%2520Wietheger%26entry.1292438233%3D%2520%2520We%2520perform%2520a%2520refined%2520complexity-theoretic%2520analysis%2520of%2520three%2520classical%250Aproblems%2520in%2520the%2520context%2520of%2520Hierarchical%2520Task%2520Network%2520Planning%253A%2520the%2520verification%250Aof%2520a%2520provided%2520plan%252C%2520whether%2520an%2520executable%2520plan%2520exists%252C%2520and%2520whether%2520a%2520given%250Astate%2520can%2520be%2520reached.%2520Our%2520focus%2520lies%2520on%2520identifying%2520structural%2520properties%2520which%250Ayield%2520tractability.%2520We%2520obtain%2520new%2520polynomial%2520algorithms%2520for%2520all%2520three%2520problems%250Aon%2520a%2520natural%2520class%2520of%2520primitive%2520networks%252C%2520along%2520with%2520corresponding%2520lower%250Abounds.%2520We%2520also%2520obtain%2520an%2520algorithmic%2520meta-theorem%2520for%2520lifting%2520polynomial-time%250Asolvability%2520from%2520primitive%2520to%2520general%2520task%2520networks%252C%2520and%2520prove%2520that%2520its%250Apreconditions%2520are%2520tight.%2520Finally%252C%2520we%2520analyze%2520the%2520parameterized%2520complexity%2520of%250Athe%2520three%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.14174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Structural%20Complexity%20Analysis%20of%20Hierarchical%20Task%20Network%20Planning&entry.906535625=Cornelius%20Brand%20and%20Robert%20Ganian%20and%20Fionn%20Mc%20Inerney%20and%20Simon%20Wietheger&entry.1292438233=%20%20We%20perform%20a%20refined%20complexity-theoretic%20analysis%20of%20three%20classical%0Aproblems%20in%20the%20context%20of%20Hierarchical%20Task%20Network%20Planning%3A%20the%20verification%0Aof%20a%20provided%20plan%2C%20whether%20an%20executable%20plan%20exists%2C%20and%20whether%20a%20given%0Astate%20can%20be%20reached.%20Our%20focus%20lies%20on%20identifying%20structural%20properties%20which%0Ayield%20tractability.%20We%20obtain%20new%20polynomial%20algorithms%20for%20all%20three%20problems%0Aon%20a%20natural%20class%20of%20primitive%20networks%2C%20along%20with%20corresponding%20lower%0Abounds.%20We%20also%20obtain%20an%20algorithmic%20meta-theorem%20for%20lifting%20polynomial-time%0Asolvability%20from%20primitive%20to%20general%20task%20networks%2C%20and%20prove%20that%20its%0Apreconditions%20are%20tight.%20Finally%2C%20we%20analyze%20the%20parameterized%20complexity%20of%0Athe%20three%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.14174v2&entry.124074799=Read"},
{"title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs", "author": "Amirmohammad Farzaneh and Osvaldo Simeone", "abstract": "  In sensitive application domains, multi-objective hyperparameter selection\ncan ensure the reliability of AI models prior to deployment, while optimizing\nauxiliary performance metrics. The state-of-the-art Pareto Testing (PT) method\nguarantees statistical reliability constraints by adopting a multiple\nhypothesis testing framework. In PT, hyperparameters are validated one at a\ntime, following a data-driven order determined by expected reliability levels.\nThis paper introduces a novel framework for multi-objective hyperparameter\nselection that captures the interdependencies among the reliability levels of\ndifferent hyperparameter configurations using a directed acyclic graph (DAG),\nwhich is termed the reliability graph (RG). The RG is constructed based on\nprior information and data by using the Bradley-Terry model. The proposed\napproach, RG-based PT (RG-PT), leverages the RG to enable the efficient,\nparallel testing of multiple hyperparameters at the same reliability level. By\nintegrating False Discovery Rate (FDR) control, RG-PT ensures robust\nstatistical reliability guarantees and is shown via experiments across diverse\ndomains to consistently yield superior solutions for multi-objective\ncalibration problems.\n", "link": "http://arxiv.org/abs/2501.13018v1", "date": "2025-01-22", "relevancy": 0.8929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4941}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4237}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Objective%20Hyperparameter%20Selection%20via%20Hypothesis%20Testing%20on%0A%20%20Reliability%20Graphs&body=Title%3A%20Multi-Objective%20Hyperparameter%20Selection%20via%20Hypothesis%20Testing%20on%0A%20%20Reliability%20Graphs%0AAuthor%3A%20Amirmohammad%20Farzaneh%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20In%20sensitive%20application%20domains%2C%20multi-objective%20hyperparameter%20selection%0Acan%20ensure%20the%20reliability%20of%20AI%20models%20prior%20to%20deployment%2C%20while%20optimizing%0Aauxiliary%20performance%20metrics.%20The%20state-of-the-art%20Pareto%20Testing%20%28PT%29%20method%0Aguarantees%20statistical%20reliability%20constraints%20by%20adopting%20a%20multiple%0Ahypothesis%20testing%20framework.%20In%20PT%2C%20hyperparameters%20are%20validated%20one%20at%20a%0Atime%2C%20following%20a%20data-driven%20order%20determined%20by%20expected%20reliability%20levels.%0AThis%20paper%20introduces%20a%20novel%20framework%20for%20multi-objective%20hyperparameter%0Aselection%20that%20captures%20the%20interdependencies%20among%20the%20reliability%20levels%20of%0Adifferent%20hyperparameter%20configurations%20using%20a%20directed%20acyclic%20graph%20%28DAG%29%2C%0Awhich%20is%20termed%20the%20reliability%20graph%20%28RG%29.%20The%20RG%20is%20constructed%20based%20on%0Aprior%20information%20and%20data%20by%20using%20the%20Bradley-Terry%20model.%20The%20proposed%0Aapproach%2C%20RG-based%20PT%20%28RG-PT%29%2C%20leverages%20the%20RG%20to%20enable%20the%20efficient%2C%0Aparallel%20testing%20of%20multiple%20hyperparameters%20at%20the%20same%20reliability%20level.%20By%0Aintegrating%20False%20Discovery%20Rate%20%28FDR%29%20control%2C%20RG-PT%20ensures%20robust%0Astatistical%20reliability%20guarantees%20and%20is%20shown%20via%20experiments%20across%20diverse%0Adomains%20to%20consistently%20yield%20superior%20solutions%20for%20multi-objective%0Acalibration%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Objective%2520Hyperparameter%2520Selection%2520via%2520Hypothesis%2520Testing%2520on%250A%2520%2520Reliability%2520Graphs%26entry.906535625%3DAmirmohammad%2520Farzaneh%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520In%2520sensitive%2520application%2520domains%252C%2520multi-objective%2520hyperparameter%2520selection%250Acan%2520ensure%2520the%2520reliability%2520of%2520AI%2520models%2520prior%2520to%2520deployment%252C%2520while%2520optimizing%250Aauxiliary%2520performance%2520metrics.%2520The%2520state-of-the-art%2520Pareto%2520Testing%2520%2528PT%2529%2520method%250Aguarantees%2520statistical%2520reliability%2520constraints%2520by%2520adopting%2520a%2520multiple%250Ahypothesis%2520testing%2520framework.%2520In%2520PT%252C%2520hyperparameters%2520are%2520validated%2520one%2520at%2520a%250Atime%252C%2520following%2520a%2520data-driven%2520order%2520determined%2520by%2520expected%2520reliability%2520levels.%250AThis%2520paper%2520introduces%2520a%2520novel%2520framework%2520for%2520multi-objective%2520hyperparameter%250Aselection%2520that%2520captures%2520the%2520interdependencies%2520among%2520the%2520reliability%2520levels%2520of%250Adifferent%2520hyperparameter%2520configurations%2520using%2520a%2520directed%2520acyclic%2520graph%2520%2528DAG%2529%252C%250Awhich%2520is%2520termed%2520the%2520reliability%2520graph%2520%2528RG%2529.%2520The%2520RG%2520is%2520constructed%2520based%2520on%250Aprior%2520information%2520and%2520data%2520by%2520using%2520the%2520Bradley-Terry%2520model.%2520The%2520proposed%250Aapproach%252C%2520RG-based%2520PT%2520%2528RG-PT%2529%252C%2520leverages%2520the%2520RG%2520to%2520enable%2520the%2520efficient%252C%250Aparallel%2520testing%2520of%2520multiple%2520hyperparameters%2520at%2520the%2520same%2520reliability%2520level.%2520By%250Aintegrating%2520False%2520Discovery%2520Rate%2520%2528FDR%2529%2520control%252C%2520RG-PT%2520ensures%2520robust%250Astatistical%2520reliability%2520guarantees%2520and%2520is%2520shown%2520via%2520experiments%2520across%2520diverse%250Adomains%2520to%2520consistently%2520yield%2520superior%2520solutions%2520for%2520multi-objective%250Acalibration%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Objective%20Hyperparameter%20Selection%20via%20Hypothesis%20Testing%20on%0A%20%20Reliability%20Graphs&entry.906535625=Amirmohammad%20Farzaneh%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20In%20sensitive%20application%20domains%2C%20multi-objective%20hyperparameter%20selection%0Acan%20ensure%20the%20reliability%20of%20AI%20models%20prior%20to%20deployment%2C%20while%20optimizing%0Aauxiliary%20performance%20metrics.%20The%20state-of-the-art%20Pareto%20Testing%20%28PT%29%20method%0Aguarantees%20statistical%20reliability%20constraints%20by%20adopting%20a%20multiple%0Ahypothesis%20testing%20framework.%20In%20PT%2C%20hyperparameters%20are%20validated%20one%20at%20a%0Atime%2C%20following%20a%20data-driven%20order%20determined%20by%20expected%20reliability%20levels.%0AThis%20paper%20introduces%20a%20novel%20framework%20for%20multi-objective%20hyperparameter%0Aselection%20that%20captures%20the%20interdependencies%20among%20the%20reliability%20levels%20of%0Adifferent%20hyperparameter%20configurations%20using%20a%20directed%20acyclic%20graph%20%28DAG%29%2C%0Awhich%20is%20termed%20the%20reliability%20graph%20%28RG%29.%20The%20RG%20is%20constructed%20based%20on%0Aprior%20information%20and%20data%20by%20using%20the%20Bradley-Terry%20model.%20The%20proposed%0Aapproach%2C%20RG-based%20PT%20%28RG-PT%29%2C%20leverages%20the%20RG%20to%20enable%20the%20efficient%2C%0Aparallel%20testing%20of%20multiple%20hyperparameters%20at%20the%20same%20reliability%20level.%20By%0Aintegrating%20False%20Discovery%20Rate%20%28FDR%29%20control%2C%20RG-PT%20ensures%20robust%0Astatistical%20reliability%20guarantees%20and%20is%20shown%20via%20experiments%20across%20diverse%0Adomains%20to%20consistently%20yield%20superior%20solutions%20for%20multi-objective%0Acalibration%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13018v1&entry.124074799=Read"},
{"title": "Evolution and The Knightian Blindspot of Machine Learning", "author": "Joel Lehman and Elliot Meyerson and Tarek El-Gaaly and Kenneth O. Stanley and Tarin Ziyaee", "abstract": "  This paper claims that machine learning (ML) largely overlooks an important\nfacet of general intelligence: robustness to a qualitatively unknown future in\nan open world. Such robustness relates to Knightian uncertainty (KU) in\neconomics, i.e. uncertainty that cannot be quantified, which is excluded from\nconsideration in ML's key formalisms. This paper aims to identify this blind\nspot, argue its importance, and catalyze research into addressing it, which we\nbelieve is necessary to create truly robust open-world AI. To help illuminate\nthe blind spot, we contrast one area of ML, reinforcement learning (RL), with\nthe process of biological evolution. Despite staggering ongoing progress, RL\nstill struggles in open-world situations, often failing under unforeseen\nsituations. For example, the idea of zero-shot transferring a self-driving car\npolicy trained only in the US to the UK currently seems exceedingly ambitious.\nIn dramatic contrast, biological evolution routinely produces agents that\nthrive within an open world, sometimes even to situations that are remarkably\nout-of-distribution (e.g. invasive species; or humans, who do undertake such\nzero-shot international driving). Interestingly, evolution achieves such\nrobustness without explicit theory, formalisms, or mathematical gradients. We\nexplore the assumptions underlying RL's typical formalisms, showing how they\nlimit RL's engagement with the unknown unknowns characteristic of an\never-changing complex world. Further, we identify mechanisms through which\nevolutionary processes foster robustness to novel and unpredictable challenges,\nand discuss potential pathways to algorithmically embody them. The conclusion\nis that the intriguing remaining fragility of ML may result from blind spots in\nits formalisms, and that significant gains may result from direct confrontation\nwith the challenge of KU.\n", "link": "http://arxiv.org/abs/2501.13075v1", "date": "2025-01-22", "relevancy": 1.5598, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5348}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5186}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%20and%20The%20Knightian%20Blindspot%20of%20Machine%20Learning&body=Title%3A%20Evolution%20and%20The%20Knightian%20Blindspot%20of%20Machine%20Learning%0AAuthor%3A%20Joel%20Lehman%20and%20Elliot%20Meyerson%20and%20Tarek%20El-Gaaly%20and%20Kenneth%20O.%20Stanley%20and%20Tarin%20Ziyaee%0AAbstract%3A%20%20%20This%20paper%20claims%20that%20machine%20learning%20%28ML%29%20largely%20overlooks%20an%20important%0Afacet%20of%20general%20intelligence%3A%20robustness%20to%20a%20qualitatively%20unknown%20future%20in%0Aan%20open%20world.%20Such%20robustness%20relates%20to%20Knightian%20uncertainty%20%28KU%29%20in%0Aeconomics%2C%20i.e.%20uncertainty%20that%20cannot%20be%20quantified%2C%20which%20is%20excluded%20from%0Aconsideration%20in%20ML%27s%20key%20formalisms.%20This%20paper%20aims%20to%20identify%20this%20blind%0Aspot%2C%20argue%20its%20importance%2C%20and%20catalyze%20research%20into%20addressing%20it%2C%20which%20we%0Abelieve%20is%20necessary%20to%20create%20truly%20robust%20open-world%20AI.%20To%20help%20illuminate%0Athe%20blind%20spot%2C%20we%20contrast%20one%20area%20of%20ML%2C%20reinforcement%20learning%20%28RL%29%2C%20with%0Athe%20process%20of%20biological%20evolution.%20Despite%20staggering%20ongoing%20progress%2C%20RL%0Astill%20struggles%20in%20open-world%20situations%2C%20often%20failing%20under%20unforeseen%0Asituations.%20For%20example%2C%20the%20idea%20of%20zero-shot%20transferring%20a%20self-driving%20car%0Apolicy%20trained%20only%20in%20the%20US%20to%20the%20UK%20currently%20seems%20exceedingly%20ambitious.%0AIn%20dramatic%20contrast%2C%20biological%20evolution%20routinely%20produces%20agents%20that%0Athrive%20within%20an%20open%20world%2C%20sometimes%20even%20to%20situations%20that%20are%20remarkably%0Aout-of-distribution%20%28e.g.%20invasive%20species%3B%20or%20humans%2C%20who%20do%20undertake%20such%0Azero-shot%20international%20driving%29.%20Interestingly%2C%20evolution%20achieves%20such%0Arobustness%20without%20explicit%20theory%2C%20formalisms%2C%20or%20mathematical%20gradients.%20We%0Aexplore%20the%20assumptions%20underlying%20RL%27s%20typical%20formalisms%2C%20showing%20how%20they%0Alimit%20RL%27s%20engagement%20with%20the%20unknown%20unknowns%20characteristic%20of%20an%0Aever-changing%20complex%20world.%20Further%2C%20we%20identify%20mechanisms%20through%20which%0Aevolutionary%20processes%20foster%20robustness%20to%20novel%20and%20unpredictable%20challenges%2C%0Aand%20discuss%20potential%20pathways%20to%20algorithmically%20embody%20them.%20The%20conclusion%0Ais%20that%20the%20intriguing%20remaining%20fragility%20of%20ML%20may%20result%20from%20blind%20spots%20in%0Aits%20formalisms%2C%20and%20that%20significant%20gains%20may%20result%20from%20direct%20confrontation%0Awith%20the%20challenge%20of%20KU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%2520and%2520The%2520Knightian%2520Blindspot%2520of%2520Machine%2520Learning%26entry.906535625%3DJoel%2520Lehman%2520and%2520Elliot%2520Meyerson%2520and%2520Tarek%2520El-Gaaly%2520and%2520Kenneth%2520O.%2520Stanley%2520and%2520Tarin%2520Ziyaee%26entry.1292438233%3D%2520%2520This%2520paper%2520claims%2520that%2520machine%2520learning%2520%2528ML%2529%2520largely%2520overlooks%2520an%2520important%250Afacet%2520of%2520general%2520intelligence%253A%2520robustness%2520to%2520a%2520qualitatively%2520unknown%2520future%2520in%250Aan%2520open%2520world.%2520Such%2520robustness%2520relates%2520to%2520Knightian%2520uncertainty%2520%2528KU%2529%2520in%250Aeconomics%252C%2520i.e.%2520uncertainty%2520that%2520cannot%2520be%2520quantified%252C%2520which%2520is%2520excluded%2520from%250Aconsideration%2520in%2520ML%2527s%2520key%2520formalisms.%2520This%2520paper%2520aims%2520to%2520identify%2520this%2520blind%250Aspot%252C%2520argue%2520its%2520importance%252C%2520and%2520catalyze%2520research%2520into%2520addressing%2520it%252C%2520which%2520we%250Abelieve%2520is%2520necessary%2520to%2520create%2520truly%2520robust%2520open-world%2520AI.%2520To%2520help%2520illuminate%250Athe%2520blind%2520spot%252C%2520we%2520contrast%2520one%2520area%2520of%2520ML%252C%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520with%250Athe%2520process%2520of%2520biological%2520evolution.%2520Despite%2520staggering%2520ongoing%2520progress%252C%2520RL%250Astill%2520struggles%2520in%2520open-world%2520situations%252C%2520often%2520failing%2520under%2520unforeseen%250Asituations.%2520For%2520example%252C%2520the%2520idea%2520of%2520zero-shot%2520transferring%2520a%2520self-driving%2520car%250Apolicy%2520trained%2520only%2520in%2520the%2520US%2520to%2520the%2520UK%2520currently%2520seems%2520exceedingly%2520ambitious.%250AIn%2520dramatic%2520contrast%252C%2520biological%2520evolution%2520routinely%2520produces%2520agents%2520that%250Athrive%2520within%2520an%2520open%2520world%252C%2520sometimes%2520even%2520to%2520situations%2520that%2520are%2520remarkably%250Aout-of-distribution%2520%2528e.g.%2520invasive%2520species%253B%2520or%2520humans%252C%2520who%2520do%2520undertake%2520such%250Azero-shot%2520international%2520driving%2529.%2520Interestingly%252C%2520evolution%2520achieves%2520such%250Arobustness%2520without%2520explicit%2520theory%252C%2520formalisms%252C%2520or%2520mathematical%2520gradients.%2520We%250Aexplore%2520the%2520assumptions%2520underlying%2520RL%2527s%2520typical%2520formalisms%252C%2520showing%2520how%2520they%250Alimit%2520RL%2527s%2520engagement%2520with%2520the%2520unknown%2520unknowns%2520characteristic%2520of%2520an%250Aever-changing%2520complex%2520world.%2520Further%252C%2520we%2520identify%2520mechanisms%2520through%2520which%250Aevolutionary%2520processes%2520foster%2520robustness%2520to%2520novel%2520and%2520unpredictable%2520challenges%252C%250Aand%2520discuss%2520potential%2520pathways%2520to%2520algorithmically%2520embody%2520them.%2520The%2520conclusion%250Ais%2520that%2520the%2520intriguing%2520remaining%2520fragility%2520of%2520ML%2520may%2520result%2520from%2520blind%2520spots%2520in%250Aits%2520formalisms%252C%2520and%2520that%2520significant%2520gains%2520may%2520result%2520from%2520direct%2520confrontation%250Awith%2520the%2520challenge%2520of%2520KU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%20and%20The%20Knightian%20Blindspot%20of%20Machine%20Learning&entry.906535625=Joel%20Lehman%20and%20Elliot%20Meyerson%20and%20Tarek%20El-Gaaly%20and%20Kenneth%20O.%20Stanley%20and%20Tarin%20Ziyaee&entry.1292438233=%20%20This%20paper%20claims%20that%20machine%20learning%20%28ML%29%20largely%20overlooks%20an%20important%0Afacet%20of%20general%20intelligence%3A%20robustness%20to%20a%20qualitatively%20unknown%20future%20in%0Aan%20open%20world.%20Such%20robustness%20relates%20to%20Knightian%20uncertainty%20%28KU%29%20in%0Aeconomics%2C%20i.e.%20uncertainty%20that%20cannot%20be%20quantified%2C%20which%20is%20excluded%20from%0Aconsideration%20in%20ML%27s%20key%20formalisms.%20This%20paper%20aims%20to%20identify%20this%20blind%0Aspot%2C%20argue%20its%20importance%2C%20and%20catalyze%20research%20into%20addressing%20it%2C%20which%20we%0Abelieve%20is%20necessary%20to%20create%20truly%20robust%20open-world%20AI.%20To%20help%20illuminate%0Athe%20blind%20spot%2C%20we%20contrast%20one%20area%20of%20ML%2C%20reinforcement%20learning%20%28RL%29%2C%20with%0Athe%20process%20of%20biological%20evolution.%20Despite%20staggering%20ongoing%20progress%2C%20RL%0Astill%20struggles%20in%20open-world%20situations%2C%20often%20failing%20under%20unforeseen%0Asituations.%20For%20example%2C%20the%20idea%20of%20zero-shot%20transferring%20a%20self-driving%20car%0Apolicy%20trained%20only%20in%20the%20US%20to%20the%20UK%20currently%20seems%20exceedingly%20ambitious.%0AIn%20dramatic%20contrast%2C%20biological%20evolution%20routinely%20produces%20agents%20that%0Athrive%20within%20an%20open%20world%2C%20sometimes%20even%20to%20situations%20that%20are%20remarkably%0Aout-of-distribution%20%28e.g.%20invasive%20species%3B%20or%20humans%2C%20who%20do%20undertake%20such%0Azero-shot%20international%20driving%29.%20Interestingly%2C%20evolution%20achieves%20such%0Arobustness%20without%20explicit%20theory%2C%20formalisms%2C%20or%20mathematical%20gradients.%20We%0Aexplore%20the%20assumptions%20underlying%20RL%27s%20typical%20formalisms%2C%20showing%20how%20they%0Alimit%20RL%27s%20engagement%20with%20the%20unknown%20unknowns%20characteristic%20of%20an%0Aever-changing%20complex%20world.%20Further%2C%20we%20identify%20mechanisms%20through%20which%0Aevolutionary%20processes%20foster%20robustness%20to%20novel%20and%20unpredictable%20challenges%2C%0Aand%20discuss%20potential%20pathways%20to%20algorithmically%20embody%20them.%20The%20conclusion%0Ais%20that%20the%20intriguing%20remaining%20fragility%20of%20ML%20may%20result%20from%20blind%20spots%20in%0Aits%20formalisms%2C%20and%20that%20significant%20gains%20may%20result%20from%20direct%20confrontation%0Awith%20the%20challenge%20of%20KU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13075v1&entry.124074799=Read"},
{"title": "Hybrid Losses for Hierarchical Embedding Learning", "author": "Haokun Tian and Stefan Lattner and Brian McFee and Charalampos Saitis", "abstract": "  In traditional supervised learning, the cross-entropy loss treats all\nincorrect predictions equally, ignoring the relevance or proximity of wrong\nlabels to the correct answer. By leveraging a tree hierarchy for fine-grained\nlabels, we investigate hybrid losses, such as generalised triplet and\ncross-entropy losses, to enforce similarity between labels within a multi-task\nlearning framework. We propose metrics to evaluate the embedding space\nstructure and assess the model's ability to generalise to unseen classes, that\nis, to infer similar classes for data belonging to unseen categories. Our\nexperiments on OrchideaSOL, a four-level hierarchical instrument sound dataset\nwith nearly 200 detailed categories, demonstrate that the proposed hybrid\nlosses outperform previous works in classification, retrieval, embedding space\nstructure, and generalisation.\n", "link": "http://arxiv.org/abs/2501.12796v1", "date": "2025-01-22", "relevancy": 1.5031, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5103}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.49}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Losses%20for%20Hierarchical%20Embedding%20Learning&body=Title%3A%20Hybrid%20Losses%20for%20Hierarchical%20Embedding%20Learning%0AAuthor%3A%20Haokun%20Tian%20and%20Stefan%20Lattner%20and%20Brian%20McFee%20and%20Charalampos%20Saitis%0AAbstract%3A%20%20%20In%20traditional%20supervised%20learning%2C%20the%20cross-entropy%20loss%20treats%20all%0Aincorrect%20predictions%20equally%2C%20ignoring%20the%20relevance%20or%20proximity%20of%20wrong%0Alabels%20to%20the%20correct%20answer.%20By%20leveraging%20a%20tree%20hierarchy%20for%20fine-grained%0Alabels%2C%20we%20investigate%20hybrid%20losses%2C%20such%20as%20generalised%20triplet%20and%0Across-entropy%20losses%2C%20to%20enforce%20similarity%20between%20labels%20within%20a%20multi-task%0Alearning%20framework.%20We%20propose%20metrics%20to%20evaluate%20the%20embedding%20space%0Astructure%20and%20assess%20the%20model%27s%20ability%20to%20generalise%20to%20unseen%20classes%2C%20that%0Ais%2C%20to%20infer%20similar%20classes%20for%20data%20belonging%20to%20unseen%20categories.%20Our%0Aexperiments%20on%20OrchideaSOL%2C%20a%20four-level%20hierarchical%20instrument%20sound%20dataset%0Awith%20nearly%20200%20detailed%20categories%2C%20demonstrate%20that%20the%20proposed%20hybrid%0Alosses%20outperform%20previous%20works%20in%20classification%2C%20retrieval%2C%20embedding%20space%0Astructure%2C%20and%20generalisation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Losses%2520for%2520Hierarchical%2520Embedding%2520Learning%26entry.906535625%3DHaokun%2520Tian%2520and%2520Stefan%2520Lattner%2520and%2520Brian%2520McFee%2520and%2520Charalampos%2520Saitis%26entry.1292438233%3D%2520%2520In%2520traditional%2520supervised%2520learning%252C%2520the%2520cross-entropy%2520loss%2520treats%2520all%250Aincorrect%2520predictions%2520equally%252C%2520ignoring%2520the%2520relevance%2520or%2520proximity%2520of%2520wrong%250Alabels%2520to%2520the%2520correct%2520answer.%2520By%2520leveraging%2520a%2520tree%2520hierarchy%2520for%2520fine-grained%250Alabels%252C%2520we%2520investigate%2520hybrid%2520losses%252C%2520such%2520as%2520generalised%2520triplet%2520and%250Across-entropy%2520losses%252C%2520to%2520enforce%2520similarity%2520between%2520labels%2520within%2520a%2520multi-task%250Alearning%2520framework.%2520We%2520propose%2520metrics%2520to%2520evaluate%2520the%2520embedding%2520space%250Astructure%2520and%2520assess%2520the%2520model%2527s%2520ability%2520to%2520generalise%2520to%2520unseen%2520classes%252C%2520that%250Ais%252C%2520to%2520infer%2520similar%2520classes%2520for%2520data%2520belonging%2520to%2520unseen%2520categories.%2520Our%250Aexperiments%2520on%2520OrchideaSOL%252C%2520a%2520four-level%2520hierarchical%2520instrument%2520sound%2520dataset%250Awith%2520nearly%2520200%2520detailed%2520categories%252C%2520demonstrate%2520that%2520the%2520proposed%2520hybrid%250Alosses%2520outperform%2520previous%2520works%2520in%2520classification%252C%2520retrieval%252C%2520embedding%2520space%250Astructure%252C%2520and%2520generalisation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Losses%20for%20Hierarchical%20Embedding%20Learning&entry.906535625=Haokun%20Tian%20and%20Stefan%20Lattner%20and%20Brian%20McFee%20and%20Charalampos%20Saitis&entry.1292438233=%20%20In%20traditional%20supervised%20learning%2C%20the%20cross-entropy%20loss%20treats%20all%0Aincorrect%20predictions%20equally%2C%20ignoring%20the%20relevance%20or%20proximity%20of%20wrong%0Alabels%20to%20the%20correct%20answer.%20By%20leveraging%20a%20tree%20hierarchy%20for%20fine-grained%0Alabels%2C%20we%20investigate%20hybrid%20losses%2C%20such%20as%20generalised%20triplet%20and%0Across-entropy%20losses%2C%20to%20enforce%20similarity%20between%20labels%20within%20a%20multi-task%0Alearning%20framework.%20We%20propose%20metrics%20to%20evaluate%20the%20embedding%20space%0Astructure%20and%20assess%20the%20model%27s%20ability%20to%20generalise%20to%20unseen%20classes%2C%20that%0Ais%2C%20to%20infer%20similar%20classes%20for%20data%20belonging%20to%20unseen%20categories.%20Our%0Aexperiments%20on%20OrchideaSOL%2C%20a%20four-level%20hierarchical%20instrument%20sound%20dataset%0Awith%20nearly%20200%20detailed%20categories%2C%20demonstrate%20that%20the%20proposed%20hybrid%0Alosses%20outperform%20previous%20works%20in%20classification%2C%20retrieval%2C%20embedding%20space%0Astructure%2C%20and%20generalisation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12796v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


