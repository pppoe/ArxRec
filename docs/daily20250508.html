<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250507.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Geometry-Aware Texture Generation for 3D Head Modeling with\n  Artist-driven Control", "author": "Amin Fadaeinejad and Abdallah Dib and Luiz Gustavo Hafemann and Emeline Got and Trevor Anderson and Amaury Depierre and Nikolaus F. Troje and Marcus A. Brubaker and Marc-Andr\u00e9 Carbonneau", "abstract": "  Creating realistic 3D head assets for virtual characters that match a precise\nartistic vision remains labor-intensive. We present a novel framework that\nstreamlines this process by providing artists with intuitive control over\ngenerated 3D heads. Our approach uses a geometry-aware texture synthesis\npipeline that learns correlations between head geometry and skin texture maps\nacross different demographics. The framework offers three levels of artistic\ncontrol: manipulation of overall head geometry, adjustment of skin tone while\npreserving facial characteristics, and fine-grained editing of details such as\nwrinkles or facial hair. Our pipeline allows artists to make edits to a single\ntexture map using familiar tools, with our system automatically propagating\nthese changes coherently across the remaining texture maps needed for realistic\nrendering. Experiments demonstrate that our method produces diverse results\nwith clean geometries. We showcase practical applications focusing on intuitive\ncontrol for artists, including skin tone adjustments and simplified editing\nworkflows for adding age-related details or removing unwanted features from\nscanned models. This integrated approach aims to streamline the artistic\nworkflow in virtual character creation.\n", "link": "http://arxiv.org/abs/2505.04387v1", "date": "2025-05-07", "relevancy": 3.3098, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6666}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6666}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Texture%20Generation%20for%203D%20Head%20Modeling%20with%0A%20%20Artist-driven%20Control&body=Title%3A%20Geometry-Aware%20Texture%20Generation%20for%203D%20Head%20Modeling%20with%0A%20%20Artist-driven%20Control%0AAuthor%3A%20Amin%20Fadaeinejad%20and%20Abdallah%20Dib%20and%20Luiz%20Gustavo%20Hafemann%20and%20Emeline%20Got%20and%20Trevor%20Anderson%20and%20Amaury%20Depierre%20and%20Nikolaus%20F.%20Troje%20and%20Marcus%20A.%20Brubaker%20and%20Marc-Andr%C3%A9%20Carbonneau%0AAbstract%3A%20%20%20Creating%20realistic%203D%20head%20assets%20for%20virtual%20characters%20that%20match%20a%20precise%0Aartistic%20vision%20remains%20labor-intensive.%20We%20present%20a%20novel%20framework%20that%0Astreamlines%20this%20process%20by%20providing%20artists%20with%20intuitive%20control%20over%0Agenerated%203D%20heads.%20Our%20approach%20uses%20a%20geometry-aware%20texture%20synthesis%0Apipeline%20that%20learns%20correlations%20between%20head%20geometry%20and%20skin%20texture%20maps%0Aacross%20different%20demographics.%20The%20framework%20offers%20three%20levels%20of%20artistic%0Acontrol%3A%20manipulation%20of%20overall%20head%20geometry%2C%20adjustment%20of%20skin%20tone%20while%0Apreserving%20facial%20characteristics%2C%20and%20fine-grained%20editing%20of%20details%20such%20as%0Awrinkles%20or%20facial%20hair.%20Our%20pipeline%20allows%20artists%20to%20make%20edits%20to%20a%20single%0Atexture%20map%20using%20familiar%20tools%2C%20with%20our%20system%20automatically%20propagating%0Athese%20changes%20coherently%20across%20the%20remaining%20texture%20maps%20needed%20for%20realistic%0Arendering.%20Experiments%20demonstrate%20that%20our%20method%20produces%20diverse%20results%0Awith%20clean%20geometries.%20We%20showcase%20practical%20applications%20focusing%20on%20intuitive%0Acontrol%20for%20artists%2C%20including%20skin%20tone%20adjustments%20and%20simplified%20editing%0Aworkflows%20for%20adding%20age-related%20details%20or%20removing%20unwanted%20features%20from%0Ascanned%20models.%20This%20integrated%20approach%20aims%20to%20streamline%20the%20artistic%0Aworkflow%20in%20virtual%20character%20creation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Texture%2520Generation%2520for%25203D%2520Head%2520Modeling%2520with%250A%2520%2520Artist-driven%2520Control%26entry.906535625%3DAmin%2520Fadaeinejad%2520and%2520Abdallah%2520Dib%2520and%2520Luiz%2520Gustavo%2520Hafemann%2520and%2520Emeline%2520Got%2520and%2520Trevor%2520Anderson%2520and%2520Amaury%2520Depierre%2520and%2520Nikolaus%2520F.%2520Troje%2520and%2520Marcus%2520A.%2520Brubaker%2520and%2520Marc-Andr%25C3%25A9%2520Carbonneau%26entry.1292438233%3D%2520%2520Creating%2520realistic%25203D%2520head%2520assets%2520for%2520virtual%2520characters%2520that%2520match%2520a%2520precise%250Aartistic%2520vision%2520remains%2520labor-intensive.%2520We%2520present%2520a%2520novel%2520framework%2520that%250Astreamlines%2520this%2520process%2520by%2520providing%2520artists%2520with%2520intuitive%2520control%2520over%250Agenerated%25203D%2520heads.%2520Our%2520approach%2520uses%2520a%2520geometry-aware%2520texture%2520synthesis%250Apipeline%2520that%2520learns%2520correlations%2520between%2520head%2520geometry%2520and%2520skin%2520texture%2520maps%250Aacross%2520different%2520demographics.%2520The%2520framework%2520offers%2520three%2520levels%2520of%2520artistic%250Acontrol%253A%2520manipulation%2520of%2520overall%2520head%2520geometry%252C%2520adjustment%2520of%2520skin%2520tone%2520while%250Apreserving%2520facial%2520characteristics%252C%2520and%2520fine-grained%2520editing%2520of%2520details%2520such%2520as%250Awrinkles%2520or%2520facial%2520hair.%2520Our%2520pipeline%2520allows%2520artists%2520to%2520make%2520edits%2520to%2520a%2520single%250Atexture%2520map%2520using%2520familiar%2520tools%252C%2520with%2520our%2520system%2520automatically%2520propagating%250Athese%2520changes%2520coherently%2520across%2520the%2520remaining%2520texture%2520maps%2520needed%2520for%2520realistic%250Arendering.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520produces%2520diverse%2520results%250Awith%2520clean%2520geometries.%2520We%2520showcase%2520practical%2520applications%2520focusing%2520on%2520intuitive%250Acontrol%2520for%2520artists%252C%2520including%2520skin%2520tone%2520adjustments%2520and%2520simplified%2520editing%250Aworkflows%2520for%2520adding%2520age-related%2520details%2520or%2520removing%2520unwanted%2520features%2520from%250Ascanned%2520models.%2520This%2520integrated%2520approach%2520aims%2520to%2520streamline%2520the%2520artistic%250Aworkflow%2520in%2520virtual%2520character%2520creation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Texture%20Generation%20for%203D%20Head%20Modeling%20with%0A%20%20Artist-driven%20Control&entry.906535625=Amin%20Fadaeinejad%20and%20Abdallah%20Dib%20and%20Luiz%20Gustavo%20Hafemann%20and%20Emeline%20Got%20and%20Trevor%20Anderson%20and%20Amaury%20Depierre%20and%20Nikolaus%20F.%20Troje%20and%20Marcus%20A.%20Brubaker%20and%20Marc-Andr%C3%A9%20Carbonneau&entry.1292438233=%20%20Creating%20realistic%203D%20head%20assets%20for%20virtual%20characters%20that%20match%20a%20precise%0Aartistic%20vision%20remains%20labor-intensive.%20We%20present%20a%20novel%20framework%20that%0Astreamlines%20this%20process%20by%20providing%20artists%20with%20intuitive%20control%20over%0Agenerated%203D%20heads.%20Our%20approach%20uses%20a%20geometry-aware%20texture%20synthesis%0Apipeline%20that%20learns%20correlations%20between%20head%20geometry%20and%20skin%20texture%20maps%0Aacross%20different%20demographics.%20The%20framework%20offers%20three%20levels%20of%20artistic%0Acontrol%3A%20manipulation%20of%20overall%20head%20geometry%2C%20adjustment%20of%20skin%20tone%20while%0Apreserving%20facial%20characteristics%2C%20and%20fine-grained%20editing%20of%20details%20such%20as%0Awrinkles%20or%20facial%20hair.%20Our%20pipeline%20allows%20artists%20to%20make%20edits%20to%20a%20single%0Atexture%20map%20using%20familiar%20tools%2C%20with%20our%20system%20automatically%20propagating%0Athese%20changes%20coherently%20across%20the%20remaining%20texture%20maps%20needed%20for%20realistic%0Arendering.%20Experiments%20demonstrate%20that%20our%20method%20produces%20diverse%20results%0Awith%20clean%20geometries.%20We%20showcase%20practical%20applications%20focusing%20on%20intuitive%0Acontrol%20for%20artists%2C%20including%20skin%20tone%20adjustments%20and%20simplified%20editing%0Aworkflows%20for%20adding%20age-related%20details%20or%20removing%20unwanted%20features%20from%0Ascanned%20models.%20This%20integrated%20approach%20aims%20to%20streamline%20the%20artistic%0Aworkflow%20in%20virtual%20character%20creation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04387v1&entry.124074799=Read"},
{"title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning", "author": "Xianhang Li and Yanqing Liu and Haoqin Tu and Hongru Zhu and Cihang Xie", "abstract": "  OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments.\n", "link": "http://arxiv.org/abs/2505.04601v1", "date": "2025-05-07", "relevancy": 3.095, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6396}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6396}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenVision%3A%20A%20Fully-Open%2C%20Cost-Effective%20Family%20of%20Advanced%20Vision%0A%20%20Encoders%20for%20Multimodal%20Learning&body=Title%3A%20OpenVision%3A%20A%20Fully-Open%2C%20Cost-Effective%20Family%20of%20Advanced%20Vision%0A%20%20Encoders%20for%20Multimodal%20Learning%0AAuthor%3A%20Xianhang%20Li%20and%20Yanqing%20Liu%20and%20Haoqin%20Tu%20and%20Hongru%20Zhu%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20OpenAI%27s%20CLIP%2C%20released%20in%20early%202021%2C%20have%20long%20been%20the%20go-to%20choice%20of%0Avision%20encoder%20for%20building%20multimodal%20foundation%20models.%20Although%20recent%0Aalternatives%20such%20as%20SigLIP%20have%20begun%20to%20challenge%20this%20status%20quo%2C%20to%20our%0Aknowledge%20none%20are%20fully%20open%3A%20their%20training%20data%20remains%20proprietary%20and/or%0Atheir%20training%20recipes%20are%20not%20released.%20This%20paper%20fills%20this%20gap%20with%0AOpenVision%2C%20a%20fully-open%2C%20cost-effective%20family%20of%20vision%20encoders%20that%20match%0Aor%20surpass%20the%20performance%20of%20OpenAI%27s%20CLIP%20when%20integrated%20into%20multimodal%0Aframeworks%20like%20LLaVA.%20OpenVision%20builds%20on%20existing%20works%20--%20e.g.%2C%20CLIPS%20for%0Atraining%20framework%20and%20Recap-DataComp-1B%20for%20training%20data%20--%20while%20revealing%0Amultiple%20key%20insights%20in%20enhancing%20encoder%20quality%20and%20showcasing%20practical%0Abenefits%20in%20advancing%20multimodal%20models.%20By%20releasing%20vision%20encoders%20spanning%0Afrom%205.9M%20to%20632.1M%20parameters%2C%20OpenVision%20offers%20practitioners%20a%20flexible%0Atrade-off%20between%20capacity%20and%20efficiency%20in%20building%20multimodal%20models%3A%20larger%0Amodels%20deliver%20enhanced%20multimodal%20performance%2C%20while%20smaller%20versions%20enable%0Alightweight%2C%20edge-ready%20multimodal%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenVision%253A%2520A%2520Fully-Open%252C%2520Cost-Effective%2520Family%2520of%2520Advanced%2520Vision%250A%2520%2520Encoders%2520for%2520Multimodal%2520Learning%26entry.906535625%3DXianhang%2520Li%2520and%2520Yanqing%2520Liu%2520and%2520Haoqin%2520Tu%2520and%2520Hongru%2520Zhu%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520OpenAI%2527s%2520CLIP%252C%2520released%2520in%2520early%25202021%252C%2520have%2520long%2520been%2520the%2520go-to%2520choice%2520of%250Avision%2520encoder%2520for%2520building%2520multimodal%2520foundation%2520models.%2520Although%2520recent%250Aalternatives%2520such%2520as%2520SigLIP%2520have%2520begun%2520to%2520challenge%2520this%2520status%2520quo%252C%2520to%2520our%250Aknowledge%2520none%2520are%2520fully%2520open%253A%2520their%2520training%2520data%2520remains%2520proprietary%2520and/or%250Atheir%2520training%2520recipes%2520are%2520not%2520released.%2520This%2520paper%2520fills%2520this%2520gap%2520with%250AOpenVision%252C%2520a%2520fully-open%252C%2520cost-effective%2520family%2520of%2520vision%2520encoders%2520that%2520match%250Aor%2520surpass%2520the%2520performance%2520of%2520OpenAI%2527s%2520CLIP%2520when%2520integrated%2520into%2520multimodal%250Aframeworks%2520like%2520LLaVA.%2520OpenVision%2520builds%2520on%2520existing%2520works%2520--%2520e.g.%252C%2520CLIPS%2520for%250Atraining%2520framework%2520and%2520Recap-DataComp-1B%2520for%2520training%2520data%2520--%2520while%2520revealing%250Amultiple%2520key%2520insights%2520in%2520enhancing%2520encoder%2520quality%2520and%2520showcasing%2520practical%250Abenefits%2520in%2520advancing%2520multimodal%2520models.%2520By%2520releasing%2520vision%2520encoders%2520spanning%250Afrom%25205.9M%2520to%2520632.1M%2520parameters%252C%2520OpenVision%2520offers%2520practitioners%2520a%2520flexible%250Atrade-off%2520between%2520capacity%2520and%2520efficiency%2520in%2520building%2520multimodal%2520models%253A%2520larger%250Amodels%2520deliver%2520enhanced%2520multimodal%2520performance%252C%2520while%2520smaller%2520versions%2520enable%250Alightweight%252C%2520edge-ready%2520multimodal%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenVision%3A%20A%20Fully-Open%2C%20Cost-Effective%20Family%20of%20Advanced%20Vision%0A%20%20Encoders%20for%20Multimodal%20Learning&entry.906535625=Xianhang%20Li%20and%20Yanqing%20Liu%20and%20Haoqin%20Tu%20and%20Hongru%20Zhu%20and%20Cihang%20Xie&entry.1292438233=%20%20OpenAI%27s%20CLIP%2C%20released%20in%20early%202021%2C%20have%20long%20been%20the%20go-to%20choice%20of%0Avision%20encoder%20for%20building%20multimodal%20foundation%20models.%20Although%20recent%0Aalternatives%20such%20as%20SigLIP%20have%20begun%20to%20challenge%20this%20status%20quo%2C%20to%20our%0Aknowledge%20none%20are%20fully%20open%3A%20their%20training%20data%20remains%20proprietary%20and/or%0Atheir%20training%20recipes%20are%20not%20released.%20This%20paper%20fills%20this%20gap%20with%0AOpenVision%2C%20a%20fully-open%2C%20cost-effective%20family%20of%20vision%20encoders%20that%20match%0Aor%20surpass%20the%20performance%20of%20OpenAI%27s%20CLIP%20when%20integrated%20into%20multimodal%0Aframeworks%20like%20LLaVA.%20OpenVision%20builds%20on%20existing%20works%20--%20e.g.%2C%20CLIPS%20for%0Atraining%20framework%20and%20Recap-DataComp-1B%20for%20training%20data%20--%20while%20revealing%0Amultiple%20key%20insights%20in%20enhancing%20encoder%20quality%20and%20showcasing%20practical%0Abenefits%20in%20advancing%20multimodal%20models.%20By%20releasing%20vision%20encoders%20spanning%0Afrom%205.9M%20to%20632.1M%20parameters%2C%20OpenVision%20offers%20practitioners%20a%20flexible%0Atrade-off%20between%20capacity%20and%20efficiency%20in%20building%20multimodal%20models%3A%20larger%0Amodels%20deliver%20enhanced%20multimodal%20performance%2C%20while%20smaller%20versions%20enable%0Alightweight%2C%20edge-ready%20multimodal%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04601v1&entry.124074799=Read"},
{"title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation", "author": "Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Chunyu Wang and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu", "abstract": "  CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining.\n", "link": "http://arxiv.org/abs/2411.04997v4", "date": "2025-05-07", "relevancy": 3.0791, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlocks%20Richer%20Visual%20Representation&body=Title%3A%20LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlocks%20Richer%20Visual%20Representation%0AAuthor%3A%20Weiquan%20Huang%20and%20Aoqi%20Wu%20and%20Yifan%20Yang%20and%20Xufang%20Luo%20and%20Yuqing%20Yang%20and%20Liang%20Hu%20and%20Qi%20Dai%20and%20Chunyu%20Wang%20and%20Xiyang%20Dai%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu%0AAbstract%3A%20%20%20CLIP%20is%20a%20foundational%20multimodal%20model%20that%20aligns%20image%20and%20text%20features%0Ainto%20a%20shared%20representation%20space%20via%20contrastive%20learning%20on%20large-scale%0Aimage-text%20pairs.%20Its%20effectiveness%20primarily%20stems%20from%20the%20use%20of%20natural%0Alanguage%20as%20rich%20supervision.%20Motivated%20by%20the%20remarkable%20advancements%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20this%20work%20explores%20how%20LLMs%27%20superior%20text%0Aunderstanding%20and%20extensive%20open-world%20knowledge%20can%20enhance%20CLIP%27s%20capability%2C%0Aespecially%20for%20processing%20longer%20and%20more%20complex%20image%20captions.%20We%20propose%20an%0Aefficient%20post-training%20strategy%20that%20integrates%20LLMs%20into%20pretrained%20CLIP.%20To%0Aaddress%20the%20challenge%20posed%20by%20the%20autoregressive%20nature%20of%20LLMs%2C%20we%20introduce%0Aa%20caption-to-caption%20contrastive%20fine-tuning%20framework%2C%20significantly%20enhancing%0Athe%20discriminative%20quality%20of%20LLM%20outputs.%20Extensive%20experiments%20demonstrate%0Athat%20our%20approach%20outperforms%20LoRA-based%20methods%2C%20achieving%20nearly%20fourfold%0Afaster%20training%20with%20superior%20performance.%20Furthermore%2C%20we%20validate%20substantial%0Aimprovements%20over%20state-of-the-art%20models%20such%20as%20CLIP%2C%20EVA02%2C%20and%20SigLip2%0Aacross%20various%20zero-shot%20multimodal%20retrieval%20tasks%2C%20cross-lingual%20retrieval%0Atasks%2C%20and%20multimodal%20language%20model%20pretraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04997v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM2CLIP%253A%2520Powerful%2520Language%2520Model%2520Unlocks%2520Richer%2520Visual%2520Representation%26entry.906535625%3DWeiquan%2520Huang%2520and%2520Aoqi%2520Wu%2520and%2520Yifan%2520Yang%2520and%2520Xufang%2520Luo%2520and%2520Yuqing%2520Yang%2520and%2520Liang%2520Hu%2520and%2520Qi%2520Dai%2520and%2520Chunyu%2520Wang%2520and%2520Xiyang%2520Dai%2520and%2520Dongdong%2520Chen%2520and%2520Chong%2520Luo%2520and%2520Lili%2520Qiu%26entry.1292438233%3D%2520%2520CLIP%2520is%2520a%2520foundational%2520multimodal%2520model%2520that%2520aligns%2520image%2520and%2520text%2520features%250Ainto%2520a%2520shared%2520representation%2520space%2520via%2520contrastive%2520learning%2520on%2520large-scale%250Aimage-text%2520pairs.%2520Its%2520effectiveness%2520primarily%2520stems%2520from%2520the%2520use%2520of%2520natural%250Alanguage%2520as%2520rich%2520supervision.%2520Motivated%2520by%2520the%2520remarkable%2520advancements%2520in%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520this%2520work%2520explores%2520how%2520LLMs%2527%2520superior%2520text%250Aunderstanding%2520and%2520extensive%2520open-world%2520knowledge%2520can%2520enhance%2520CLIP%2527s%2520capability%252C%250Aespecially%2520for%2520processing%2520longer%2520and%2520more%2520complex%2520image%2520captions.%2520We%2520propose%2520an%250Aefficient%2520post-training%2520strategy%2520that%2520integrates%2520LLMs%2520into%2520pretrained%2520CLIP.%2520To%250Aaddress%2520the%2520challenge%2520posed%2520by%2520the%2520autoregressive%2520nature%2520of%2520LLMs%252C%2520we%2520introduce%250Aa%2520caption-to-caption%2520contrastive%2520fine-tuning%2520framework%252C%2520significantly%2520enhancing%250Athe%2520discriminative%2520quality%2520of%2520LLM%2520outputs.%2520Extensive%2520experiments%2520demonstrate%250Athat%2520our%2520approach%2520outperforms%2520LoRA-based%2520methods%252C%2520achieving%2520nearly%2520fourfold%250Afaster%2520training%2520with%2520superior%2520performance.%2520Furthermore%252C%2520we%2520validate%2520substantial%250Aimprovements%2520over%2520state-of-the-art%2520models%2520such%2520as%2520CLIP%252C%2520EVA02%252C%2520and%2520SigLip2%250Aacross%2520various%2520zero-shot%2520multimodal%2520retrieval%2520tasks%252C%2520cross-lingual%2520retrieval%250Atasks%252C%2520and%2520multimodal%2520language%2520model%2520pretraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04997v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM2CLIP%3A%20Powerful%20Language%20Model%20Unlocks%20Richer%20Visual%20Representation&entry.906535625=Weiquan%20Huang%20and%20Aoqi%20Wu%20and%20Yifan%20Yang%20and%20Xufang%20Luo%20and%20Yuqing%20Yang%20and%20Liang%20Hu%20and%20Qi%20Dai%20and%20Chunyu%20Wang%20and%20Xiyang%20Dai%20and%20Dongdong%20Chen%20and%20Chong%20Luo%20and%20Lili%20Qiu&entry.1292438233=%20%20CLIP%20is%20a%20foundational%20multimodal%20model%20that%20aligns%20image%20and%20text%20features%0Ainto%20a%20shared%20representation%20space%20via%20contrastive%20learning%20on%20large-scale%0Aimage-text%20pairs.%20Its%20effectiveness%20primarily%20stems%20from%20the%20use%20of%20natural%0Alanguage%20as%20rich%20supervision.%20Motivated%20by%20the%20remarkable%20advancements%20in%20large%0Alanguage%20models%20%28LLMs%29%2C%20this%20work%20explores%20how%20LLMs%27%20superior%20text%0Aunderstanding%20and%20extensive%20open-world%20knowledge%20can%20enhance%20CLIP%27s%20capability%2C%0Aespecially%20for%20processing%20longer%20and%20more%20complex%20image%20captions.%20We%20propose%20an%0Aefficient%20post-training%20strategy%20that%20integrates%20LLMs%20into%20pretrained%20CLIP.%20To%0Aaddress%20the%20challenge%20posed%20by%20the%20autoregressive%20nature%20of%20LLMs%2C%20we%20introduce%0Aa%20caption-to-caption%20contrastive%20fine-tuning%20framework%2C%20significantly%20enhancing%0Athe%20discriminative%20quality%20of%20LLM%20outputs.%20Extensive%20experiments%20demonstrate%0Athat%20our%20approach%20outperforms%20LoRA-based%20methods%2C%20achieving%20nearly%20fourfold%0Afaster%20training%20with%20superior%20performance.%20Furthermore%2C%20we%20validate%20substantial%0Aimprovements%20over%20state-of-the-art%20models%20such%20as%20CLIP%2C%20EVA02%2C%20and%20SigLip2%0Aacross%20various%20zero-shot%20multimodal%20retrieval%20tasks%2C%20cross-lingual%20retrieval%0Atasks%2C%20and%20multimodal%20language%20model%20pretraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04997v4&entry.124074799=Read"},
{"title": "Vision-Language Models Create Cross-Modal Task Representations", "author": "Grace Luo and Trevor Darrell and Amir Bar", "abstract": "  Autoregressive vision-language models (VLMs) can handle many tasks within a\nsingle model, yet the representations that enable this capability remain\nopaque. We find that VLMs align conceptually equivalent inputs into a shared\ntask vector, which is invariant to modality (text, image) and format (examples,\ninstruction), and may simplify VLM processing. We measure this alignment via\ncross-modal transfer -- the ability of a task vector derived in one modality to\ntrigger the correct generation in another -- on a range of tasks and model\narchitectures. Although the task vector is highly compressed, we find that this\nsingle vector outperforms prompting the model with the full task information,\nunique to this cross-modal case. Furthermore, we show that task vectors can be\ntransferred from a base language model to its fine-tuned vision-language\ncounterpart, and that they can be derived solely from instructions without the\nneed for examples. Taken together, our findings shed light on how VLMs\ninternally process task information, and how they map different modalities into\ncommon semantic representations. Project page:\nhttps://vlm-cross-modal-reps.github.io.\n", "link": "http://arxiv.org/abs/2410.22330v2", "date": "2025-05-07", "relevancy": 3.0688, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Language%20Models%20Create%20Cross-Modal%20Task%20Representations&body=Title%3A%20Vision-Language%20Models%20Create%20Cross-Modal%20Task%20Representations%0AAuthor%3A%20Grace%20Luo%20and%20Trevor%20Darrell%20and%20Amir%20Bar%0AAbstract%3A%20%20%20Autoregressive%20vision-language%20models%20%28VLMs%29%20can%20handle%20many%20tasks%20within%20a%0Asingle%20model%2C%20yet%20the%20representations%20that%20enable%20this%20capability%20remain%0Aopaque.%20We%20find%20that%20VLMs%20align%20conceptually%20equivalent%20inputs%20into%20a%20shared%0Atask%20vector%2C%20which%20is%20invariant%20to%20modality%20%28text%2C%20image%29%20and%20format%20%28examples%2C%0Ainstruction%29%2C%20and%20may%20simplify%20VLM%20processing.%20We%20measure%20this%20alignment%20via%0Across-modal%20transfer%20--%20the%20ability%20of%20a%20task%20vector%20derived%20in%20one%20modality%20to%0Atrigger%20the%20correct%20generation%20in%20another%20--%20on%20a%20range%20of%20tasks%20and%20model%0Aarchitectures.%20Although%20the%20task%20vector%20is%20highly%20compressed%2C%20we%20find%20that%20this%0Asingle%20vector%20outperforms%20prompting%20the%20model%20with%20the%20full%20task%20information%2C%0Aunique%20to%20this%20cross-modal%20case.%20Furthermore%2C%20we%20show%20that%20task%20vectors%20can%20be%0Atransferred%20from%20a%20base%20language%20model%20to%20its%20fine-tuned%20vision-language%0Acounterpart%2C%20and%20that%20they%20can%20be%20derived%20solely%20from%20instructions%20without%20the%0Aneed%20for%20examples.%20Taken%20together%2C%20our%20findings%20shed%20light%20on%20how%20VLMs%0Ainternally%20process%20task%20information%2C%20and%20how%20they%20map%20different%20modalities%20into%0Acommon%20semantic%20representations.%20Project%20page%3A%0Ahttps%3A//vlm-cross-modal-reps.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Language%2520Models%2520Create%2520Cross-Modal%2520Task%2520Representations%26entry.906535625%3DGrace%2520Luo%2520and%2520Trevor%2520Darrell%2520and%2520Amir%2520Bar%26entry.1292438233%3D%2520%2520Autoregressive%2520vision-language%2520models%2520%2528VLMs%2529%2520can%2520handle%2520many%2520tasks%2520within%2520a%250Asingle%2520model%252C%2520yet%2520the%2520representations%2520that%2520enable%2520this%2520capability%2520remain%250Aopaque.%2520We%2520find%2520that%2520VLMs%2520align%2520conceptually%2520equivalent%2520inputs%2520into%2520a%2520shared%250Atask%2520vector%252C%2520which%2520is%2520invariant%2520to%2520modality%2520%2528text%252C%2520image%2529%2520and%2520format%2520%2528examples%252C%250Ainstruction%2529%252C%2520and%2520may%2520simplify%2520VLM%2520processing.%2520We%2520measure%2520this%2520alignment%2520via%250Across-modal%2520transfer%2520--%2520the%2520ability%2520of%2520a%2520task%2520vector%2520derived%2520in%2520one%2520modality%2520to%250Atrigger%2520the%2520correct%2520generation%2520in%2520another%2520--%2520on%2520a%2520range%2520of%2520tasks%2520and%2520model%250Aarchitectures.%2520Although%2520the%2520task%2520vector%2520is%2520highly%2520compressed%252C%2520we%2520find%2520that%2520this%250Asingle%2520vector%2520outperforms%2520prompting%2520the%2520model%2520with%2520the%2520full%2520task%2520information%252C%250Aunique%2520to%2520this%2520cross-modal%2520case.%2520Furthermore%252C%2520we%2520show%2520that%2520task%2520vectors%2520can%2520be%250Atransferred%2520from%2520a%2520base%2520language%2520model%2520to%2520its%2520fine-tuned%2520vision-language%250Acounterpart%252C%2520and%2520that%2520they%2520can%2520be%2520derived%2520solely%2520from%2520instructions%2520without%2520the%250Aneed%2520for%2520examples.%2520Taken%2520together%252C%2520our%2520findings%2520shed%2520light%2520on%2520how%2520VLMs%250Ainternally%2520process%2520task%2520information%252C%2520and%2520how%2520they%2520map%2520different%2520modalities%2520into%250Acommon%2520semantic%2520representations.%2520Project%2520page%253A%250Ahttps%253A//vlm-cross-modal-reps.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Language%20Models%20Create%20Cross-Modal%20Task%20Representations&entry.906535625=Grace%20Luo%20and%20Trevor%20Darrell%20and%20Amir%20Bar&entry.1292438233=%20%20Autoregressive%20vision-language%20models%20%28VLMs%29%20can%20handle%20many%20tasks%20within%20a%0Asingle%20model%2C%20yet%20the%20representations%20that%20enable%20this%20capability%20remain%0Aopaque.%20We%20find%20that%20VLMs%20align%20conceptually%20equivalent%20inputs%20into%20a%20shared%0Atask%20vector%2C%20which%20is%20invariant%20to%20modality%20%28text%2C%20image%29%20and%20format%20%28examples%2C%0Ainstruction%29%2C%20and%20may%20simplify%20VLM%20processing.%20We%20measure%20this%20alignment%20via%0Across-modal%20transfer%20--%20the%20ability%20of%20a%20task%20vector%20derived%20in%20one%20modality%20to%0Atrigger%20the%20correct%20generation%20in%20another%20--%20on%20a%20range%20of%20tasks%20and%20model%0Aarchitectures.%20Although%20the%20task%20vector%20is%20highly%20compressed%2C%20we%20find%20that%20this%0Asingle%20vector%20outperforms%20prompting%20the%20model%20with%20the%20full%20task%20information%2C%0Aunique%20to%20this%20cross-modal%20case.%20Furthermore%2C%20we%20show%20that%20task%20vectors%20can%20be%0Atransferred%20from%20a%20base%20language%20model%20to%20its%20fine-tuned%20vision-language%0Acounterpart%2C%20and%20that%20they%20can%20be%20derived%20solely%20from%20instructions%20without%20the%0Aneed%20for%20examples.%20Taken%20together%2C%20our%20findings%20shed%20light%20on%20how%20VLMs%0Ainternally%20process%20task%20information%2C%20and%20how%20they%20map%20different%20modalities%20into%0Acommon%20semantic%20representations.%20Project%20page%3A%0Ahttps%3A//vlm-cross-modal-reps.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22330v2&entry.124074799=Read"},
{"title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception", "author": "Junjie Wang and Bin Chen and Yulin Li and Bin Kang and Yichi Chen and Zhuotao Tian", "abstract": "  Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.\n", "link": "http://arxiv.org/abs/2505.04410v1", "date": "2025-05-07", "relevancy": 3.0643, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCLIP%3A%20Decoupled%20Learning%20for%20Open-Vocabulary%20Dense%20Perception&body=Title%3A%20DeCLIP%3A%20Decoupled%20Learning%20for%20Open-Vocabulary%20Dense%20Perception%0AAuthor%3A%20Junjie%20Wang%20and%20Bin%20Chen%20and%20Yulin%20Li%20and%20Bin%20Kang%20and%20Yichi%20Chen%20and%20Zhuotao%20Tian%0AAbstract%3A%20%20%20Dense%20visual%20prediction%20tasks%20have%20been%20constrained%20by%20their%20reliance%20on%0Apredefined%20categories%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios%0Awhere%20visual%20concepts%20are%20unbounded.%20While%20Vision-Language%20Models%20%28VLMs%29%20like%0ACLIP%20have%20shown%20promise%20in%20open-vocabulary%20tasks%2C%20their%20direct%20application%20to%0Adense%20prediction%20often%20leads%20to%20suboptimal%20performance%20due%20to%20limitations%20in%0Alocal%20feature%20representation.%20In%20this%20work%2C%20we%20present%20our%20observation%20that%0ACLIP%27s%20image%20tokens%20struggle%20to%20effectively%20aggregate%20information%20from%0Aspatially%20or%20semantically%20related%20regions%2C%20resulting%20in%20features%20that%20lack%0Alocal%20discriminability%20and%20spatial%20consistency.%20To%20address%20this%20issue%2C%20we%0Apropose%20DeCLIP%2C%20a%20novel%20framework%20that%20enhances%20CLIP%20by%20decoupling%20the%0Aself-attention%20module%20to%20obtain%20%60%60content%27%27%20and%20%60%60context%27%27%20features%0Arespectively.%20The%20%60%60content%27%27%20features%20are%20aligned%20with%20image%20crop%0Arepresentations%20to%20improve%20local%20discriminability%2C%20while%20%60%60context%27%27%20features%0Alearn%20to%20retain%20the%20spatial%20correlations%20under%20the%20guidance%20of%20vision%0Afoundation%20models%2C%20such%20as%20DINO.%20Extensive%20experiments%20demonstrate%20that%20DeCLIP%0Asignificantly%20outperforms%20existing%20methods%20across%20multiple%20open-vocabulary%0Adense%20prediction%20tasks%2C%20including%20object%20detection%20and%20semantic%20segmentation.%0ACode%20is%20available%20at%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/xiaomoguhz/DeCLIP%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCLIP%253A%2520Decoupled%2520Learning%2520for%2520Open-Vocabulary%2520Dense%2520Perception%26entry.906535625%3DJunjie%2520Wang%2520and%2520Bin%2520Chen%2520and%2520Yulin%2520Li%2520and%2520Bin%2520Kang%2520and%2520Yichi%2520Chen%2520and%2520Zhuotao%2520Tian%26entry.1292438233%3D%2520%2520Dense%2520visual%2520prediction%2520tasks%2520have%2520been%2520constrained%2520by%2520their%2520reliance%2520on%250Apredefined%2520categories%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%2520scenarios%250Awhere%2520visual%2520concepts%2520are%2520unbounded.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520like%250ACLIP%2520have%2520shown%2520promise%2520in%2520open-vocabulary%2520tasks%252C%2520their%2520direct%2520application%2520to%250Adense%2520prediction%2520often%2520leads%2520to%2520suboptimal%2520performance%2520due%2520to%2520limitations%2520in%250Alocal%2520feature%2520representation.%2520In%2520this%2520work%252C%2520we%2520present%2520our%2520observation%2520that%250ACLIP%2527s%2520image%2520tokens%2520struggle%2520to%2520effectively%2520aggregate%2520information%2520from%250Aspatially%2520or%2520semantically%2520related%2520regions%252C%2520resulting%2520in%2520features%2520that%2520lack%250Alocal%2520discriminability%2520and%2520spatial%2520consistency.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520DeCLIP%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520CLIP%2520by%2520decoupling%2520the%250Aself-attention%2520module%2520to%2520obtain%2520%2560%2560content%2527%2527%2520and%2520%2560%2560context%2527%2527%2520features%250Arespectively.%2520The%2520%2560%2560content%2527%2527%2520features%2520are%2520aligned%2520with%2520image%2520crop%250Arepresentations%2520to%2520improve%2520local%2520discriminability%252C%2520while%2520%2560%2560context%2527%2527%2520features%250Alearn%2520to%2520retain%2520the%2520spatial%2520correlations%2520under%2520the%2520guidance%2520of%2520vision%250Afoundation%2520models%252C%2520such%2520as%2520DINO.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DeCLIP%250Asignificantly%2520outperforms%2520existing%2520methods%2520across%2520multiple%2520open-vocabulary%250Adense%2520prediction%2520tasks%252C%2520including%2520object%2520detection%2520and%2520semantic%2520segmentation.%250ACode%2520is%2520available%2520at%2520%255Ctextcolor%257Bmagenta%257D%257Bhttps%253A//github.com/xiaomoguhz/DeCLIP%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCLIP%3A%20Decoupled%20Learning%20for%20Open-Vocabulary%20Dense%20Perception&entry.906535625=Junjie%20Wang%20and%20Bin%20Chen%20and%20Yulin%20Li%20and%20Bin%20Kang%20and%20Yichi%20Chen%20and%20Zhuotao%20Tian&entry.1292438233=%20%20Dense%20visual%20prediction%20tasks%20have%20been%20constrained%20by%20their%20reliance%20on%0Apredefined%20categories%2C%20limiting%20their%20applicability%20in%20real-world%20scenarios%0Awhere%20visual%20concepts%20are%20unbounded.%20While%20Vision-Language%20Models%20%28VLMs%29%20like%0ACLIP%20have%20shown%20promise%20in%20open-vocabulary%20tasks%2C%20their%20direct%20application%20to%0Adense%20prediction%20often%20leads%20to%20suboptimal%20performance%20due%20to%20limitations%20in%0Alocal%20feature%20representation.%20In%20this%20work%2C%20we%20present%20our%20observation%20that%0ACLIP%27s%20image%20tokens%20struggle%20to%20effectively%20aggregate%20information%20from%0Aspatially%20or%20semantically%20related%20regions%2C%20resulting%20in%20features%20that%20lack%0Alocal%20discriminability%20and%20spatial%20consistency.%20To%20address%20this%20issue%2C%20we%0Apropose%20DeCLIP%2C%20a%20novel%20framework%20that%20enhances%20CLIP%20by%20decoupling%20the%0Aself-attention%20module%20to%20obtain%20%60%60content%27%27%20and%20%60%60context%27%27%20features%0Arespectively.%20The%20%60%60content%27%27%20features%20are%20aligned%20with%20image%20crop%0Arepresentations%20to%20improve%20local%20discriminability%2C%20while%20%60%60context%27%27%20features%0Alearn%20to%20retain%20the%20spatial%20correlations%20under%20the%20guidance%20of%20vision%0Afoundation%20models%2C%20such%20as%20DINO.%20Extensive%20experiments%20demonstrate%20that%20DeCLIP%0Asignificantly%20outperforms%20existing%20methods%20across%20multiple%20open-vocabulary%0Adense%20prediction%20tasks%2C%20including%20object%20detection%20and%20semantic%20segmentation.%0ACode%20is%20available%20at%20%5Ctextcolor%7Bmagenta%7D%7Bhttps%3A//github.com/xiaomoguhz/DeCLIP%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04410v1&entry.124074799=Read"},
{"title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer", "author": "Jingwen Ye and Yuze He and Yanning Zhou and Yiqin Zhu and Kaiwen Xiao and Yong-Jin Liu and Wei Yang and Xiao Han", "abstract": "  Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io\n", "link": "http://arxiv.org/abs/2505.04622v1", "date": "2025-05-07", "relevancy": 2.8755, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5798}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5798}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrimitiveAnything%3A%20Human-Crafted%203D%20Primitive%20Assembly%20Generation%20with%0A%20%20Auto-Regressive%20Transformer&body=Title%3A%20PrimitiveAnything%3A%20Human-Crafted%203D%20Primitive%20Assembly%20Generation%20with%0A%20%20Auto-Regressive%20Transformer%0AAuthor%3A%20Jingwen%20Ye%20and%20Yuze%20He%20and%20Yanning%20Zhou%20and%20Yiqin%20Zhu%20and%20Kaiwen%20Xiao%20and%20Yong-Jin%20Liu%20and%20Wei%20Yang%20and%20Xiao%20Han%0AAbstract%3A%20%20%20Shape%20primitive%20abstraction%2C%20which%20decomposes%20complex%203D%20shapes%20into%20simple%0Ageometric%20elements%2C%20plays%20a%20crucial%20role%20in%20human%20visual%20cognition%20and%20has%0Abroad%20applications%20in%20computer%20vision%20and%20graphics.%20While%20recent%20advances%20in%203D%0Acontent%20generation%20have%20shown%20remarkable%20progress%2C%20existing%20primitive%0Aabstraction%20methods%20either%20rely%20on%20geometric%20optimization%20with%20limited%20semantic%0Aunderstanding%20or%20learn%20from%20small-scale%2C%20category-specific%20datasets%2C%20struggling%0Ato%20generalize%20across%20diverse%20shape%20categories.%20We%20present%20PrimitiveAnything%2C%20a%0Anovel%20framework%20that%20reformulates%20shape%20primitive%20abstraction%20as%20a%20primitive%0Aassembly%20generation%20task.%20PrimitiveAnything%20includes%20a%20shape-conditioned%0Aprimitive%20transformer%20for%20auto-regressive%20generation%20and%20an%20ambiguity-free%0Aparameterization%20scheme%20to%20represent%20multiple%20types%20of%20primitives%20in%20a%20unified%0Amanner.%20The%20proposed%20framework%20directly%20learns%20the%20process%20of%20primitive%0Aassembly%20from%20large-scale%20human-crafted%20abstractions%2C%20enabling%20it%20to%20capture%0Ahow%20humans%20decompose%20complex%20shapes%20into%20primitive%20elements.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20PrimitiveAnything%20can%20generate%20high-quality%0Aprimitive%20assemblies%20that%20better%20align%20with%20human%20perception%20while%20maintaining%0Ageometric%20fidelity%20across%20diverse%20shape%20categories.%20It%20benefits%20various%203D%0Aapplications%20and%20shows%20potential%20for%20enabling%20primitive-based%20user-generated%0Acontent%20%28UGC%29%20in%20games.%20Project%20page%3A%20https%3A//primitiveanything.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrimitiveAnything%253A%2520Human-Crafted%25203D%2520Primitive%2520Assembly%2520Generation%2520with%250A%2520%2520Auto-Regressive%2520Transformer%26entry.906535625%3DJingwen%2520Ye%2520and%2520Yuze%2520He%2520and%2520Yanning%2520Zhou%2520and%2520Yiqin%2520Zhu%2520and%2520Kaiwen%2520Xiao%2520and%2520Yong-Jin%2520Liu%2520and%2520Wei%2520Yang%2520and%2520Xiao%2520Han%26entry.1292438233%3D%2520%2520Shape%2520primitive%2520abstraction%252C%2520which%2520decomposes%2520complex%25203D%2520shapes%2520into%2520simple%250Ageometric%2520elements%252C%2520plays%2520a%2520crucial%2520role%2520in%2520human%2520visual%2520cognition%2520and%2520has%250Abroad%2520applications%2520in%2520computer%2520vision%2520and%2520graphics.%2520While%2520recent%2520advances%2520in%25203D%250Acontent%2520generation%2520have%2520shown%2520remarkable%2520progress%252C%2520existing%2520primitive%250Aabstraction%2520methods%2520either%2520rely%2520on%2520geometric%2520optimization%2520with%2520limited%2520semantic%250Aunderstanding%2520or%2520learn%2520from%2520small-scale%252C%2520category-specific%2520datasets%252C%2520struggling%250Ato%2520generalize%2520across%2520diverse%2520shape%2520categories.%2520We%2520present%2520PrimitiveAnything%252C%2520a%250Anovel%2520framework%2520that%2520reformulates%2520shape%2520primitive%2520abstraction%2520as%2520a%2520primitive%250Aassembly%2520generation%2520task.%2520PrimitiveAnything%2520includes%2520a%2520shape-conditioned%250Aprimitive%2520transformer%2520for%2520auto-regressive%2520generation%2520and%2520an%2520ambiguity-free%250Aparameterization%2520scheme%2520to%2520represent%2520multiple%2520types%2520of%2520primitives%2520in%2520a%2520unified%250Amanner.%2520The%2520proposed%2520framework%2520directly%2520learns%2520the%2520process%2520of%2520primitive%250Aassembly%2520from%2520large-scale%2520human-crafted%2520abstractions%252C%2520enabling%2520it%2520to%2520capture%250Ahow%2520humans%2520decompose%2520complex%2520shapes%2520into%2520primitive%2520elements.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520PrimitiveAnything%2520can%2520generate%2520high-quality%250Aprimitive%2520assemblies%2520that%2520better%2520align%2520with%2520human%2520perception%2520while%2520maintaining%250Ageometric%2520fidelity%2520across%2520diverse%2520shape%2520categories.%2520It%2520benefits%2520various%25203D%250Aapplications%2520and%2520shows%2520potential%2520for%2520enabling%2520primitive-based%2520user-generated%250Acontent%2520%2528UGC%2529%2520in%2520games.%2520Project%2520page%253A%2520https%253A//primitiveanything.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrimitiveAnything%3A%20Human-Crafted%203D%20Primitive%20Assembly%20Generation%20with%0A%20%20Auto-Regressive%20Transformer&entry.906535625=Jingwen%20Ye%20and%20Yuze%20He%20and%20Yanning%20Zhou%20and%20Yiqin%20Zhu%20and%20Kaiwen%20Xiao%20and%20Yong-Jin%20Liu%20and%20Wei%20Yang%20and%20Xiao%20Han&entry.1292438233=%20%20Shape%20primitive%20abstraction%2C%20which%20decomposes%20complex%203D%20shapes%20into%20simple%0Ageometric%20elements%2C%20plays%20a%20crucial%20role%20in%20human%20visual%20cognition%20and%20has%0Abroad%20applications%20in%20computer%20vision%20and%20graphics.%20While%20recent%20advances%20in%203D%0Acontent%20generation%20have%20shown%20remarkable%20progress%2C%20existing%20primitive%0Aabstraction%20methods%20either%20rely%20on%20geometric%20optimization%20with%20limited%20semantic%0Aunderstanding%20or%20learn%20from%20small-scale%2C%20category-specific%20datasets%2C%20struggling%0Ato%20generalize%20across%20diverse%20shape%20categories.%20We%20present%20PrimitiveAnything%2C%20a%0Anovel%20framework%20that%20reformulates%20shape%20primitive%20abstraction%20as%20a%20primitive%0Aassembly%20generation%20task.%20PrimitiveAnything%20includes%20a%20shape-conditioned%0Aprimitive%20transformer%20for%20auto-regressive%20generation%20and%20an%20ambiguity-free%0Aparameterization%20scheme%20to%20represent%20multiple%20types%20of%20primitives%20in%20a%20unified%0Amanner.%20The%20proposed%20framework%20directly%20learns%20the%20process%20of%20primitive%0Aassembly%20from%20large-scale%20human-crafted%20abstractions%2C%20enabling%20it%20to%20capture%0Ahow%20humans%20decompose%20complex%20shapes%20into%20primitive%20elements.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20PrimitiveAnything%20can%20generate%20high-quality%0Aprimitive%20assemblies%20that%20better%20align%20with%20human%20perception%20while%20maintaining%0Ageometric%20fidelity%20across%20diverse%20shape%20categories.%20It%20benefits%20various%203D%0Aapplications%20and%20shows%20potential%20for%20enabling%20primitive-based%20user-generated%0Acontent%20%28UGC%29%20in%20games.%20Project%20page%3A%20https%3A//primitiveanything.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04622v1&entry.124074799=Read"},
{"title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models", "author": "Omkar Thawakar and Abdelrahman Shaker and Sahal Shaji Mullappilly and Hisham Cholakkal and Rao Muhammad Anwer and Salman Khan and Jorma Laaksonen and Fahad Shahbaz Khan", "abstract": "  The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.\n", "link": "http://arxiv.org/abs/2306.07971v2", "date": "2025-05-07", "relevancy": 2.8046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XrayGPT%3A%20Chest%20Radiographs%20Summarization%20using%20Medical%20Vision-Language%0A%20%20Models&body=Title%3A%20XrayGPT%3A%20Chest%20Radiographs%20Summarization%20using%20Medical%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Omkar%20Thawakar%20and%20Abdelrahman%20Shaker%20and%20Sahal%20Shaji%20Mullappilly%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%20and%20Jorma%20Laaksonen%20and%20Fahad%20Shahbaz%20Khan%0AAbstract%3A%20%20%20The%20latest%20breakthroughs%20in%20large%20vision-language%20models%2C%20such%20as%20Bard%20and%0AGPT-4%2C%20have%20showcased%20extraordinary%20abilities%20in%20performing%20a%20wide%20range%20of%0Atasks.%20Such%20models%20are%20trained%20on%20massive%20datasets%20comprising%20billions%20of%0Apublic%20image-text%20pairs%20with%20diverse%20tasks.%20However%2C%20their%20performance%20on%0Atask-specific%20domains%2C%20such%20as%20radiology%2C%20is%20still%20under-investigated%20and%0Apotentially%20limited%20due%20to%20a%20lack%20of%20sophistication%20in%20understanding%20biomedical%0Aimages.%20On%20the%20other%20hand%2C%20conversational%20medical%20models%20have%20exhibited%0Aremarkable%20success%20but%20have%20mainly%20focused%20on%20text-based%20analysis.%20In%20this%0Apaper%2C%20we%20introduce%20XrayGPT%2C%20a%20novel%20conversational%20medical%20vision-language%0Amodel%20that%20can%20analyze%20and%20answer%20open-ended%20questions%20about%20chest%20radiographs.%0ASpecifically%2C%20we%20align%20both%20medical%20visual%20encoder%20%28MedClip%29%20with%20a%20fine-tuned%0Alarge%20language%20model%20%28Vicuna%29%2C%20using%20a%20simple%20linear%20transformation.%20This%0Aalignment%20enables%20our%20model%20to%20possess%20exceptional%20visual%20conversation%0Aabilities%2C%20grounded%20in%20a%20deep%20understanding%20of%20radiographs%20and%20medical%20domain%0Aknowledge.%20To%20enhance%20the%20performance%20of%20LLMs%20in%20the%20medical%20context%2C%20we%0Agenerate%20~217k%20interactive%20and%20high-quality%20summaries%20from%20free-text%20radiology%0Areports.%20These%20summaries%20serve%20to%20enhance%20the%20performance%20of%20LLMs%20through%20the%0Afine-tuning%20process.%20Our%20approach%20opens%20up%20new%20avenues%20the%20research%20for%0Aadvancing%20the%20automated%20analysis%20of%20chest%20radiographs.%20Our%20open-source%20demos%2C%0Amodels%2C%20and%20instruction%20sets%20are%20available%20at%3A%0Ahttps%3A//github.com/mbzuai-oryx/XrayGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07971v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXrayGPT%253A%2520Chest%2520Radiographs%2520Summarization%2520using%2520Medical%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DOmkar%2520Thawakar%2520and%2520Abdelrahman%2520Shaker%2520and%2520Sahal%2520Shaji%2520Mullappilly%2520and%2520Hisham%2520Cholakkal%2520and%2520Rao%2520Muhammad%2520Anwer%2520and%2520Salman%2520Khan%2520and%2520Jorma%2520Laaksonen%2520and%2520Fahad%2520Shahbaz%2520Khan%26entry.1292438233%3D%2520%2520The%2520latest%2520breakthroughs%2520in%2520large%2520vision-language%2520models%252C%2520such%2520as%2520Bard%2520and%250AGPT-4%252C%2520have%2520showcased%2520extraordinary%2520abilities%2520in%2520performing%2520a%2520wide%2520range%2520of%250Atasks.%2520Such%2520models%2520are%2520trained%2520on%2520massive%2520datasets%2520comprising%2520billions%2520of%250Apublic%2520image-text%2520pairs%2520with%2520diverse%2520tasks.%2520However%252C%2520their%2520performance%2520on%250Atask-specific%2520domains%252C%2520such%2520as%2520radiology%252C%2520is%2520still%2520under-investigated%2520and%250Apotentially%2520limited%2520due%2520to%2520a%2520lack%2520of%2520sophistication%2520in%2520understanding%2520biomedical%250Aimages.%2520On%2520the%2520other%2520hand%252C%2520conversational%2520medical%2520models%2520have%2520exhibited%250Aremarkable%2520success%2520but%2520have%2520mainly%2520focused%2520on%2520text-based%2520analysis.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520XrayGPT%252C%2520a%2520novel%2520conversational%2520medical%2520vision-language%250Amodel%2520that%2520can%2520analyze%2520and%2520answer%2520open-ended%2520questions%2520about%2520chest%2520radiographs.%250ASpecifically%252C%2520we%2520align%2520both%2520medical%2520visual%2520encoder%2520%2528MedClip%2529%2520with%2520a%2520fine-tuned%250Alarge%2520language%2520model%2520%2528Vicuna%2529%252C%2520using%2520a%2520simple%2520linear%2520transformation.%2520This%250Aalignment%2520enables%2520our%2520model%2520to%2520possess%2520exceptional%2520visual%2520conversation%250Aabilities%252C%2520grounded%2520in%2520a%2520deep%2520understanding%2520of%2520radiographs%2520and%2520medical%2520domain%250Aknowledge.%2520To%2520enhance%2520the%2520performance%2520of%2520LLMs%2520in%2520the%2520medical%2520context%252C%2520we%250Agenerate%2520~217k%2520interactive%2520and%2520high-quality%2520summaries%2520from%2520free-text%2520radiology%250Areports.%2520These%2520summaries%2520serve%2520to%2520enhance%2520the%2520performance%2520of%2520LLMs%2520through%2520the%250Afine-tuning%2520process.%2520Our%2520approach%2520opens%2520up%2520new%2520avenues%2520the%2520research%2520for%250Aadvancing%2520the%2520automated%2520analysis%2520of%2520chest%2520radiographs.%2520Our%2520open-source%2520demos%252C%250Amodels%252C%2520and%2520instruction%2520sets%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/mbzuai-oryx/XrayGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07971v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XrayGPT%3A%20Chest%20Radiographs%20Summarization%20using%20Medical%20Vision-Language%0A%20%20Models&entry.906535625=Omkar%20Thawakar%20and%20Abdelrahman%20Shaker%20and%20Sahal%20Shaji%20Mullappilly%20and%20Hisham%20Cholakkal%20and%20Rao%20Muhammad%20Anwer%20and%20Salman%20Khan%20and%20Jorma%20Laaksonen%20and%20Fahad%20Shahbaz%20Khan&entry.1292438233=%20%20The%20latest%20breakthroughs%20in%20large%20vision-language%20models%2C%20such%20as%20Bard%20and%0AGPT-4%2C%20have%20showcased%20extraordinary%20abilities%20in%20performing%20a%20wide%20range%20of%0Atasks.%20Such%20models%20are%20trained%20on%20massive%20datasets%20comprising%20billions%20of%0Apublic%20image-text%20pairs%20with%20diverse%20tasks.%20However%2C%20their%20performance%20on%0Atask-specific%20domains%2C%20such%20as%20radiology%2C%20is%20still%20under-investigated%20and%0Apotentially%20limited%20due%20to%20a%20lack%20of%20sophistication%20in%20understanding%20biomedical%0Aimages.%20On%20the%20other%20hand%2C%20conversational%20medical%20models%20have%20exhibited%0Aremarkable%20success%20but%20have%20mainly%20focused%20on%20text-based%20analysis.%20In%20this%0Apaper%2C%20we%20introduce%20XrayGPT%2C%20a%20novel%20conversational%20medical%20vision-language%0Amodel%20that%20can%20analyze%20and%20answer%20open-ended%20questions%20about%20chest%20radiographs.%0ASpecifically%2C%20we%20align%20both%20medical%20visual%20encoder%20%28MedClip%29%20with%20a%20fine-tuned%0Alarge%20language%20model%20%28Vicuna%29%2C%20using%20a%20simple%20linear%20transformation.%20This%0Aalignment%20enables%20our%20model%20to%20possess%20exceptional%20visual%20conversation%0Aabilities%2C%20grounded%20in%20a%20deep%20understanding%20of%20radiographs%20and%20medical%20domain%0Aknowledge.%20To%20enhance%20the%20performance%20of%20LLMs%20in%20the%20medical%20context%2C%20we%0Agenerate%20~217k%20interactive%20and%20high-quality%20summaries%20from%20free-text%20radiology%0Areports.%20These%20summaries%20serve%20to%20enhance%20the%20performance%20of%20LLMs%20through%20the%0Afine-tuning%20process.%20Our%20approach%20opens%20up%20new%20avenues%20the%20research%20for%0Aadvancing%20the%20automated%20analysis%20of%20chest%20radiographs.%20Our%20open-source%20demos%2C%0Amodels%2C%20and%20instruction%20sets%20are%20available%20at%3A%0Ahttps%3A//github.com/mbzuai-oryx/XrayGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07971v2&entry.124074799=Read"},
{"title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning", "author": "Zhenghao Xing and Xiaowei Hu and Chi-Wing Fu and Wenhai Wang and Jifeng Dai and Pheng-Ann Heng", "abstract": "  Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.\n", "link": "http://arxiv.org/abs/2505.04623v1", "date": "2025-05-07", "relevancy": 2.7877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoInk-R1%3A%20Exploring%20Audio-Visual%20Reasoning%20in%20Multimodal%20LLMs%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20EchoInk-R1%3A%20Exploring%20Audio-Visual%20Reasoning%20in%20Multimodal%20LLMs%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Zhenghao%20Xing%20and%20Xiaowei%20Hu%20and%20Chi-Wing%20Fu%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Pheng-Ann%20Heng%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20perception%20across%0Atext%2C%20vision%2C%20and%20audio%2C%20yet%20they%20often%20struggle%20with%20structured%20cross-modal%0Areasoning%2C%20particularly%20when%20integrating%20audio%20and%20visual%20signals.%20We%20introduce%0AEchoInk-R1%2C%20a%20reinforcement%20learning%20framework%20that%20enhances%20such%20reasoning%20in%0AMLLMs.%20Built%20upon%20the%20Qwen2.5-Omni-7B%20foundation%20and%20optimized%20with%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%2C%20EchoInk-R1%20tackles%20multiple-choice%0Aquestion%20answering%20over%20synchronized%20audio-image%20pairs.%20To%20enable%20this%2C%20we%0Acurate%20AVQA-R1-6K%2C%20a%20dataset%20pairing%20such%20audio-image%20inputs%20with%0Amultiple-choice%20questions%20derived%20from%20OmniInstruct-v1.%20EchoInk-R1-7B%20achieves%0A85.77%25%20accuracy%20on%20the%20validation%20set%2C%20outperforming%20the%20base%20model%2C%20which%0Ascores%2080.53%25%2C%20using%20only%20562%20reinforcement%20learning%20steps.%20Beyond%20accuracy%2C%0AEchoInk-R1%20demonstrates%20reflective%20reasoning%20by%20revisiting%20initial%0Ainterpretations%20and%20refining%20responses%20when%20facing%20ambiguous%20multimodal%20inputs.%0AThese%20results%20suggest%20that%20lightweight%20reinforcement%20learning%20fine-tuning%0Aenhances%20cross-modal%20reasoning%20in%20MLLMs.%20EchoInk-R1%20is%20the%20first%20framework%20to%0Aunify%20audio%2C%20visual%2C%20and%20textual%20modalities%20for%20general%20open-world%20reasoning%0Avia%20reinforcement%20learning.%20Code%20and%20data%20are%20publicly%20released%20to%20facilitate%0Afurther%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoInk-R1%253A%2520Exploring%2520Audio-Visual%2520Reasoning%2520in%2520Multimodal%2520LLMs%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DZhenghao%2520Xing%2520and%2520Xiaowei%2520Hu%2520and%2520Chi-Wing%2520Fu%2520and%2520Wenhai%2520Wang%2520and%2520Jifeng%2520Dai%2520and%2520Pheng-Ann%2520Heng%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520advanced%2520perception%2520across%250Atext%252C%2520vision%252C%2520and%2520audio%252C%2520yet%2520they%2520often%2520struggle%2520with%2520structured%2520cross-modal%250Areasoning%252C%2520particularly%2520when%2520integrating%2520audio%2520and%2520visual%2520signals.%2520We%2520introduce%250AEchoInk-R1%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%2520enhances%2520such%2520reasoning%2520in%250AMLLMs.%2520Built%2520upon%2520the%2520Qwen2.5-Omni-7B%2520foundation%2520and%2520optimized%2520with%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%252C%2520EchoInk-R1%2520tackles%2520multiple-choice%250Aquestion%2520answering%2520over%2520synchronized%2520audio-image%2520pairs.%2520To%2520enable%2520this%252C%2520we%250Acurate%2520AVQA-R1-6K%252C%2520a%2520dataset%2520pairing%2520such%2520audio-image%2520inputs%2520with%250Amultiple-choice%2520questions%2520derived%2520from%2520OmniInstruct-v1.%2520EchoInk-R1-7B%2520achieves%250A85.77%2525%2520accuracy%2520on%2520the%2520validation%2520set%252C%2520outperforming%2520the%2520base%2520model%252C%2520which%250Ascores%252080.53%2525%252C%2520using%2520only%2520562%2520reinforcement%2520learning%2520steps.%2520Beyond%2520accuracy%252C%250AEchoInk-R1%2520demonstrates%2520reflective%2520reasoning%2520by%2520revisiting%2520initial%250Ainterpretations%2520and%2520refining%2520responses%2520when%2520facing%2520ambiguous%2520multimodal%2520inputs.%250AThese%2520results%2520suggest%2520that%2520lightweight%2520reinforcement%2520learning%2520fine-tuning%250Aenhances%2520cross-modal%2520reasoning%2520in%2520MLLMs.%2520EchoInk-R1%2520is%2520the%2520first%2520framework%2520to%250Aunify%2520audio%252C%2520visual%252C%2520and%2520textual%2520modalities%2520for%2520general%2520open-world%2520reasoning%250Avia%2520reinforcement%2520learning.%2520Code%2520and%2520data%2520are%2520publicly%2520released%2520to%2520facilitate%250Afurther%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoInk-R1%3A%20Exploring%20Audio-Visual%20Reasoning%20in%20Multimodal%20LLMs%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Zhenghao%20Xing%20and%20Xiaowei%20Hu%20and%20Chi-Wing%20Fu%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Pheng-Ann%20Heng&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20perception%20across%0Atext%2C%20vision%2C%20and%20audio%2C%20yet%20they%20often%20struggle%20with%20structured%20cross-modal%0Areasoning%2C%20particularly%20when%20integrating%20audio%20and%20visual%20signals.%20We%20introduce%0AEchoInk-R1%2C%20a%20reinforcement%20learning%20framework%20that%20enhances%20such%20reasoning%20in%0AMLLMs.%20Built%20upon%20the%20Qwen2.5-Omni-7B%20foundation%20and%20optimized%20with%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%2C%20EchoInk-R1%20tackles%20multiple-choice%0Aquestion%20answering%20over%20synchronized%20audio-image%20pairs.%20To%20enable%20this%2C%20we%0Acurate%20AVQA-R1-6K%2C%20a%20dataset%20pairing%20such%20audio-image%20inputs%20with%0Amultiple-choice%20questions%20derived%20from%20OmniInstruct-v1.%20EchoInk-R1-7B%20achieves%0A85.77%25%20accuracy%20on%20the%20validation%20set%2C%20outperforming%20the%20base%20model%2C%20which%0Ascores%2080.53%25%2C%20using%20only%20562%20reinforcement%20learning%20steps.%20Beyond%20accuracy%2C%0AEchoInk-R1%20demonstrates%20reflective%20reasoning%20by%20revisiting%20initial%0Ainterpretations%20and%20refining%20responses%20when%20facing%20ambiguous%20multimodal%20inputs.%0AThese%20results%20suggest%20that%20lightweight%20reinforcement%20learning%20fine-tuning%0Aenhances%20cross-modal%20reasoning%20in%20MLLMs.%20EchoInk-R1%20is%20the%20first%20framework%20to%0Aunify%20audio%2C%20visual%2C%20and%20textual%20modalities%20for%20general%20open-world%20reasoning%0Avia%20reinforcement%20learning.%20Code%20and%20data%20are%20publicly%20released%20to%20facilitate%0Afurther%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04623v1&entry.124074799=Read"},
{"title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation", "author": "Jiahao Li and Weijian Ma and Xueyang Li and Yunzhong Lou and Guichun Zhou and Xiangdong Zhou", "abstract": "  Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.\n", "link": "http://arxiv.org/abs/2505.04481v1", "date": "2025-05-07", "relevancy": 2.785, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5749}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAD-Llama%3A%20Leveraging%20Large%20Language%20Models%20for%20Computer-Aided%20Design%0A%20%20Parametric%203D%20Model%20Generation&body=Title%3A%20CAD-Llama%3A%20Leveraging%20Large%20Language%20Models%20for%20Computer-Aided%20Design%0A%20%20Parametric%203D%20Model%20Generation%0AAuthor%3A%20Jiahao%20Li%20and%20Weijian%20Ma%20and%20Xueyang%20Li%20and%20Yunzhong%20Lou%20and%20Guichun%20Zhou%20and%20Xiangdong%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20success%2C%0Aprompting%20increased%20interest%20in%20expanding%20their%20generative%20capabilities%20beyond%0Ageneral%20text%20into%20domain-specific%20areas.%20This%20study%20investigates%20the%20generation%0Aof%20parametric%20sequences%20for%20computer-aided%20design%20%28CAD%29%20models%20using%20LLMs.%20This%0Aendeavor%20represents%20an%20initial%20step%20towards%20creating%20parametric%203D%20shapes%20with%0ALLMs%2C%20as%20CAD%20model%20parameters%20directly%20correlate%20with%20shapes%20in%0Athree-dimensional%20space.%20Despite%20the%20formidable%20generative%20capacities%20of%20LLMs%2C%0Athis%20task%20remains%20challenging%2C%20as%20these%20models%20neither%20encounter%20parametric%0Asequences%20during%20their%20pretraining%20phase%20nor%20possess%20direct%20awareness%20of%203D%0Astructures.%20To%20address%20this%2C%20we%20present%20CAD-Llama%2C%20a%20framework%20designed%20to%0Aenhance%20pretrained%20LLMs%20for%20generating%20parametric%203D%20CAD%20models.%20Specifically%2C%0Awe%20develop%20a%20hierarchical%20annotation%20pipeline%20and%20a%20code-like%20format%20to%0Atranslate%20parametric%203D%20CAD%20command%20sequences%20into%20Structured%20Parametric%20CAD%0ACode%20%28SPCC%29%2C%20incorporating%20hierarchical%20semantic%20descriptions.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%20pretraining%20approach%20utilizing%20SPCC%2C%20followed%20by%20an%0Ainstruction%20tuning%20process%20aligned%20with%20CAD-specific%20guidelines.%20This%0Amethodology%20aims%20to%20equip%20LLMs%20with%20the%20spatial%20knowledge%20inherent%20in%0Aparametric%20sequences.%20Experimental%20results%20demonstrate%20that%20our%20framework%0Asignificantly%20outperforms%20prior%20autoregressive%20methods%20and%20existing%20LLM%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAD-Llama%253A%2520Leveraging%2520Large%2520Language%2520Models%2520for%2520Computer-Aided%2520Design%250A%2520%2520Parametric%25203D%2520Model%2520Generation%26entry.906535625%3DJiahao%2520Li%2520and%2520Weijian%2520Ma%2520and%2520Xueyang%2520Li%2520and%2520Yunzhong%2520Lou%2520and%2520Guichun%2520Zhou%2520and%2520Xiangdong%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520success%252C%250Aprompting%2520increased%2520interest%2520in%2520expanding%2520their%2520generative%2520capabilities%2520beyond%250Ageneral%2520text%2520into%2520domain-specific%2520areas.%2520This%2520study%2520investigates%2520the%2520generation%250Aof%2520parametric%2520sequences%2520for%2520computer-aided%2520design%2520%2528CAD%2529%2520models%2520using%2520LLMs.%2520This%250Aendeavor%2520represents%2520an%2520initial%2520step%2520towards%2520creating%2520parametric%25203D%2520shapes%2520with%250ALLMs%252C%2520as%2520CAD%2520model%2520parameters%2520directly%2520correlate%2520with%2520shapes%2520in%250Athree-dimensional%2520space.%2520Despite%2520the%2520formidable%2520generative%2520capacities%2520of%2520LLMs%252C%250Athis%2520task%2520remains%2520challenging%252C%2520as%2520these%2520models%2520neither%2520encounter%2520parametric%250Asequences%2520during%2520their%2520pretraining%2520phase%2520nor%2520possess%2520direct%2520awareness%2520of%25203D%250Astructures.%2520To%2520address%2520this%252C%2520we%2520present%2520CAD-Llama%252C%2520a%2520framework%2520designed%2520to%250Aenhance%2520pretrained%2520LLMs%2520for%2520generating%2520parametric%25203D%2520CAD%2520models.%2520Specifically%252C%250Awe%2520develop%2520a%2520hierarchical%2520annotation%2520pipeline%2520and%2520a%2520code-like%2520format%2520to%250Atranslate%2520parametric%25203D%2520CAD%2520command%2520sequences%2520into%2520Structured%2520Parametric%2520CAD%250ACode%2520%2528SPCC%2529%252C%2520incorporating%2520hierarchical%2520semantic%2520descriptions.%2520Furthermore%252C%2520we%250Apropose%2520an%2520adaptive%2520pretraining%2520approach%2520utilizing%2520SPCC%252C%2520followed%2520by%2520an%250Ainstruction%2520tuning%2520process%2520aligned%2520with%2520CAD-specific%2520guidelines.%2520This%250Amethodology%2520aims%2520to%2520equip%2520LLMs%2520with%2520the%2520spatial%2520knowledge%2520inherent%2520in%250Aparametric%2520sequences.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520framework%250Asignificantly%2520outperforms%2520prior%2520autoregressive%2520methods%2520and%2520existing%2520LLM%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAD-Llama%3A%20Leveraging%20Large%20Language%20Models%20for%20Computer-Aided%20Design%0A%20%20Parametric%203D%20Model%20Generation&entry.906535625=Jiahao%20Li%20and%20Weijian%20Ma%20and%20Xueyang%20Li%20and%20Yunzhong%20Lou%20and%20Guichun%20Zhou%20and%20Xiangdong%20Zhou&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20significant%20success%2C%0Aprompting%20increased%20interest%20in%20expanding%20their%20generative%20capabilities%20beyond%0Ageneral%20text%20into%20domain-specific%20areas.%20This%20study%20investigates%20the%20generation%0Aof%20parametric%20sequences%20for%20computer-aided%20design%20%28CAD%29%20models%20using%20LLMs.%20This%0Aendeavor%20represents%20an%20initial%20step%20towards%20creating%20parametric%203D%20shapes%20with%0ALLMs%2C%20as%20CAD%20model%20parameters%20directly%20correlate%20with%20shapes%20in%0Athree-dimensional%20space.%20Despite%20the%20formidable%20generative%20capacities%20of%20LLMs%2C%0Athis%20task%20remains%20challenging%2C%20as%20these%20models%20neither%20encounter%20parametric%0Asequences%20during%20their%20pretraining%20phase%20nor%20possess%20direct%20awareness%20of%203D%0Astructures.%20To%20address%20this%2C%20we%20present%20CAD-Llama%2C%20a%20framework%20designed%20to%0Aenhance%20pretrained%20LLMs%20for%20generating%20parametric%203D%20CAD%20models.%20Specifically%2C%0Awe%20develop%20a%20hierarchical%20annotation%20pipeline%20and%20a%20code-like%20format%20to%0Atranslate%20parametric%203D%20CAD%20command%20sequences%20into%20Structured%20Parametric%20CAD%0ACode%20%28SPCC%29%2C%20incorporating%20hierarchical%20semantic%20descriptions.%20Furthermore%2C%20we%0Apropose%20an%20adaptive%20pretraining%20approach%20utilizing%20SPCC%2C%20followed%20by%20an%0Ainstruction%20tuning%20process%20aligned%20with%20CAD-specific%20guidelines.%20This%0Amethodology%20aims%20to%20equip%20LLMs%20with%20the%20spatial%20knowledge%20inherent%20in%0Aparametric%20sequences.%20Experimental%20results%20demonstrate%20that%20our%20framework%0Asignificantly%20outperforms%20prior%20autoregressive%20methods%20and%20existing%20LLM%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04481v1&entry.124074799=Read"},
{"title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral\n  Grids for Gradient-Based Mesh Optimization", "author": "Alexandre Binninger and Ruben Wiersma and Philipp Herholz and Olga Sorkine-Hornung", "abstract": "  We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration.\n", "link": "http://arxiv.org/abs/2505.04590v1", "date": "2025-05-07", "relevancy": 2.7668, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5901}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5493}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TetWeave%3A%20Isosurface%20Extraction%20using%20On-The-Fly%20Delaunay%20Tetrahedral%0A%20%20Grids%20for%20Gradient-Based%20Mesh%20Optimization&body=Title%3A%20TetWeave%3A%20Isosurface%20Extraction%20using%20On-The-Fly%20Delaunay%20Tetrahedral%0A%20%20Grids%20for%20Gradient-Based%20Mesh%20Optimization%0AAuthor%3A%20Alexandre%20Binninger%20and%20Ruben%20Wiersma%20and%20Philipp%20Herholz%20and%20Olga%20Sorkine-Hornung%0AAbstract%3A%20%20%20We%20introduce%20TetWeave%2C%20a%20novel%20isosurface%20representation%20for%20gradient-based%0Amesh%20optimization%20that%20jointly%20optimizes%20the%20placement%20of%20a%20tetrahedral%20grid%0Aused%20for%20Marching%20Tetrahedra%20and%20a%20novel%20directional%20signed%20distance%20at%20each%0Apoint.%20TetWeave%20constructs%20tetrahedral%20grids%20on-the-fly%20via%20Delaunay%0Atriangulation%2C%20enabling%20increased%20flexibility%20compared%20to%20predefined%20grids.%20The%0Aextracted%20meshes%20are%20guaranteed%20to%20be%20watertight%2C%20two-manifold%20and%0Aintersection-free.%20The%20flexibility%20of%20TetWeave%20enables%20a%20resampling%20strategy%0Athat%20places%20new%20points%20where%20reconstruction%20error%20is%20high%20and%20allows%20to%0Aencourage%20mesh%20fairness%20without%20compromising%20on%20reconstruction%20error.%20This%0Aleads%20to%20high-quality%2C%20adaptive%20meshes%20that%20require%20minimal%20memory%20usage%20and%0Afew%20parameters%20to%20optimize.%20Consequently%2C%20TetWeave%20exhibits%20near-linear%20memory%0Ascaling%20relative%20to%20the%20vertex%20count%20of%20the%20output%20mesh%20-%20a%20substantial%0Aimprovement%20over%20predefined%20grids.%20We%20demonstrate%20the%20applicability%20of%20TetWeave%0Ato%20a%20broad%20range%20of%20challenging%20tasks%20in%20computer%20graphics%20and%20vision%2C%20such%20as%0Amulti-view%203D%20reconstruction%2C%20mesh%20compression%20and%20geometric%20texture%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTetWeave%253A%2520Isosurface%2520Extraction%2520using%2520On-The-Fly%2520Delaunay%2520Tetrahedral%250A%2520%2520Grids%2520for%2520Gradient-Based%2520Mesh%2520Optimization%26entry.906535625%3DAlexandre%2520Binninger%2520and%2520Ruben%2520Wiersma%2520and%2520Philipp%2520Herholz%2520and%2520Olga%2520Sorkine-Hornung%26entry.1292438233%3D%2520%2520We%2520introduce%2520TetWeave%252C%2520a%2520novel%2520isosurface%2520representation%2520for%2520gradient-based%250Amesh%2520optimization%2520that%2520jointly%2520optimizes%2520the%2520placement%2520of%2520a%2520tetrahedral%2520grid%250Aused%2520for%2520Marching%2520Tetrahedra%2520and%2520a%2520novel%2520directional%2520signed%2520distance%2520at%2520each%250Apoint.%2520TetWeave%2520constructs%2520tetrahedral%2520grids%2520on-the-fly%2520via%2520Delaunay%250Atriangulation%252C%2520enabling%2520increased%2520flexibility%2520compared%2520to%2520predefined%2520grids.%2520The%250Aextracted%2520meshes%2520are%2520guaranteed%2520to%2520be%2520watertight%252C%2520two-manifold%2520and%250Aintersection-free.%2520The%2520flexibility%2520of%2520TetWeave%2520enables%2520a%2520resampling%2520strategy%250Athat%2520places%2520new%2520points%2520where%2520reconstruction%2520error%2520is%2520high%2520and%2520allows%2520to%250Aencourage%2520mesh%2520fairness%2520without%2520compromising%2520on%2520reconstruction%2520error.%2520This%250Aleads%2520to%2520high-quality%252C%2520adaptive%2520meshes%2520that%2520require%2520minimal%2520memory%2520usage%2520and%250Afew%2520parameters%2520to%2520optimize.%2520Consequently%252C%2520TetWeave%2520exhibits%2520near-linear%2520memory%250Ascaling%2520relative%2520to%2520the%2520vertex%2520count%2520of%2520the%2520output%2520mesh%2520-%2520a%2520substantial%250Aimprovement%2520over%2520predefined%2520grids.%2520We%2520demonstrate%2520the%2520applicability%2520of%2520TetWeave%250Ato%2520a%2520broad%2520range%2520of%2520challenging%2520tasks%2520in%2520computer%2520graphics%2520and%2520vision%252C%2520such%2520as%250Amulti-view%25203D%2520reconstruction%252C%2520mesh%2520compression%2520and%2520geometric%2520texture%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TetWeave%3A%20Isosurface%20Extraction%20using%20On-The-Fly%20Delaunay%20Tetrahedral%0A%20%20Grids%20for%20Gradient-Based%20Mesh%20Optimization&entry.906535625=Alexandre%20Binninger%20and%20Ruben%20Wiersma%20and%20Philipp%20Herholz%20and%20Olga%20Sorkine-Hornung&entry.1292438233=%20%20We%20introduce%20TetWeave%2C%20a%20novel%20isosurface%20representation%20for%20gradient-based%0Amesh%20optimization%20that%20jointly%20optimizes%20the%20placement%20of%20a%20tetrahedral%20grid%0Aused%20for%20Marching%20Tetrahedra%20and%20a%20novel%20directional%20signed%20distance%20at%20each%0Apoint.%20TetWeave%20constructs%20tetrahedral%20grids%20on-the-fly%20via%20Delaunay%0Atriangulation%2C%20enabling%20increased%20flexibility%20compared%20to%20predefined%20grids.%20The%0Aextracted%20meshes%20are%20guaranteed%20to%20be%20watertight%2C%20two-manifold%20and%0Aintersection-free.%20The%20flexibility%20of%20TetWeave%20enables%20a%20resampling%20strategy%0Athat%20places%20new%20points%20where%20reconstruction%20error%20is%20high%20and%20allows%20to%0Aencourage%20mesh%20fairness%20without%20compromising%20on%20reconstruction%20error.%20This%0Aleads%20to%20high-quality%2C%20adaptive%20meshes%20that%20require%20minimal%20memory%20usage%20and%0Afew%20parameters%20to%20optimize.%20Consequently%2C%20TetWeave%20exhibits%20near-linear%20memory%0Ascaling%20relative%20to%20the%20vertex%20count%20of%20the%20output%20mesh%20-%20a%20substantial%0Aimprovement%20over%20predefined%20grids.%20We%20demonstrate%20the%20applicability%20of%20TetWeave%0Ato%20a%20broad%20range%20of%20challenging%20tasks%20in%20computer%20graphics%20and%20vision%2C%20such%20as%0Amulti-view%203D%20reconstruction%2C%20mesh%20compression%20and%20geometric%20texture%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04590v1&entry.124074799=Read"},
{"title": "Latent Manifold Reconstruction and Representation with Topological and\n  Geometrical Regularization", "author": "Ren Wang and Pengcheng Zhou", "abstract": "  Manifold learning aims to discover and represent low-dimensional structures\nunderlying high-dimensional data while preserving critical topological and\ngeometric properties. Existing methods often fail to capture local details with\nglobal topological integrity from noisy data or construct a balanced\ndimensionality reduction, resulting in distorted or fractured embeddings. We\npresent an AutoEncoder-based method that integrates a manifold reconstruction\nlayer, which uncovers latent manifold structures from noisy point clouds, and\nfurther provides regularizations on topological and geometric properties during\ndimensionality reduction, whereas the two components promote each other during\ntraining. Experiments on point cloud datasets demonstrate that our method\noutperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in\ndiscovering manifold structures from noisy data and preserving them through\ndimensionality reduction, as validated by visualization and quantitative\nmetrics. This work demonstrates the significance of combining manifold\nreconstruction with manifold learning to achieve reliable representation of the\nlatent manifold, particularly when dealing with noisy real-world data. Code\nrepository: https://github.com/Thanatorika/mrtg.\n", "link": "http://arxiv.org/abs/2505.04412v1", "date": "2025-05-07", "relevancy": 2.7329, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5489}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5469}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Manifold%20Reconstruction%20and%20Representation%20with%20Topological%20and%0A%20%20Geometrical%20Regularization&body=Title%3A%20Latent%20Manifold%20Reconstruction%20and%20Representation%20with%20Topological%20and%0A%20%20Geometrical%20Regularization%0AAuthor%3A%20Ren%20Wang%20and%20Pengcheng%20Zhou%0AAbstract%3A%20%20%20Manifold%20learning%20aims%20to%20discover%20and%20represent%20low-dimensional%20structures%0Aunderlying%20high-dimensional%20data%20while%20preserving%20critical%20topological%20and%0Ageometric%20properties.%20Existing%20methods%20often%20fail%20to%20capture%20local%20details%20with%0Aglobal%20topological%20integrity%20from%20noisy%20data%20or%20construct%20a%20balanced%0Adimensionality%20reduction%2C%20resulting%20in%20distorted%20or%20fractured%20embeddings.%20We%0Apresent%20an%20AutoEncoder-based%20method%20that%20integrates%20a%20manifold%20reconstruction%0Alayer%2C%20which%20uncovers%20latent%20manifold%20structures%20from%20noisy%20point%20clouds%2C%20and%0Afurther%20provides%20regularizations%20on%20topological%20and%20geometric%20properties%20during%0Adimensionality%20reduction%2C%20whereas%20the%20two%20components%20promote%20each%20other%20during%0Atraining.%20Experiments%20on%20point%20cloud%20datasets%20demonstrate%20that%20our%20method%0Aoutperforms%20baselines%20like%20t-SNE%2C%20UMAP%2C%20and%20Topological%20AutoEncoders%20in%0Adiscovering%20manifold%20structures%20from%20noisy%20data%20and%20preserving%20them%20through%0Adimensionality%20reduction%2C%20as%20validated%20by%20visualization%20and%20quantitative%0Ametrics.%20This%20work%20demonstrates%20the%20significance%20of%20combining%20manifold%0Areconstruction%20with%20manifold%20learning%20to%20achieve%20reliable%20representation%20of%20the%0Alatent%20manifold%2C%20particularly%20when%20dealing%20with%20noisy%20real-world%20data.%20Code%0Arepository%3A%20https%3A//github.com/Thanatorika/mrtg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04412v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Manifold%2520Reconstruction%2520and%2520Representation%2520with%2520Topological%2520and%250A%2520%2520Geometrical%2520Regularization%26entry.906535625%3DRen%2520Wang%2520and%2520Pengcheng%2520Zhou%26entry.1292438233%3D%2520%2520Manifold%2520learning%2520aims%2520to%2520discover%2520and%2520represent%2520low-dimensional%2520structures%250Aunderlying%2520high-dimensional%2520data%2520while%2520preserving%2520critical%2520topological%2520and%250Ageometric%2520properties.%2520Existing%2520methods%2520often%2520fail%2520to%2520capture%2520local%2520details%2520with%250Aglobal%2520topological%2520integrity%2520from%2520noisy%2520data%2520or%2520construct%2520a%2520balanced%250Adimensionality%2520reduction%252C%2520resulting%2520in%2520distorted%2520or%2520fractured%2520embeddings.%2520We%250Apresent%2520an%2520AutoEncoder-based%2520method%2520that%2520integrates%2520a%2520manifold%2520reconstruction%250Alayer%252C%2520which%2520uncovers%2520latent%2520manifold%2520structures%2520from%2520noisy%2520point%2520clouds%252C%2520and%250Afurther%2520provides%2520regularizations%2520on%2520topological%2520and%2520geometric%2520properties%2520during%250Adimensionality%2520reduction%252C%2520whereas%2520the%2520two%2520components%2520promote%2520each%2520other%2520during%250Atraining.%2520Experiments%2520on%2520point%2520cloud%2520datasets%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520baselines%2520like%2520t-SNE%252C%2520UMAP%252C%2520and%2520Topological%2520AutoEncoders%2520in%250Adiscovering%2520manifold%2520structures%2520from%2520noisy%2520data%2520and%2520preserving%2520them%2520through%250Adimensionality%2520reduction%252C%2520as%2520validated%2520by%2520visualization%2520and%2520quantitative%250Ametrics.%2520This%2520work%2520demonstrates%2520the%2520significance%2520of%2520combining%2520manifold%250Areconstruction%2520with%2520manifold%2520learning%2520to%2520achieve%2520reliable%2520representation%2520of%2520the%250Alatent%2520manifold%252C%2520particularly%2520when%2520dealing%2520with%2520noisy%2520real-world%2520data.%2520Code%250Arepository%253A%2520https%253A//github.com/Thanatorika/mrtg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04412v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Manifold%20Reconstruction%20and%20Representation%20with%20Topological%20and%0A%20%20Geometrical%20Regularization&entry.906535625=Ren%20Wang%20and%20Pengcheng%20Zhou&entry.1292438233=%20%20Manifold%20learning%20aims%20to%20discover%20and%20represent%20low-dimensional%20structures%0Aunderlying%20high-dimensional%20data%20while%20preserving%20critical%20topological%20and%0Ageometric%20properties.%20Existing%20methods%20often%20fail%20to%20capture%20local%20details%20with%0Aglobal%20topological%20integrity%20from%20noisy%20data%20or%20construct%20a%20balanced%0Adimensionality%20reduction%2C%20resulting%20in%20distorted%20or%20fractured%20embeddings.%20We%0Apresent%20an%20AutoEncoder-based%20method%20that%20integrates%20a%20manifold%20reconstruction%0Alayer%2C%20which%20uncovers%20latent%20manifold%20structures%20from%20noisy%20point%20clouds%2C%20and%0Afurther%20provides%20regularizations%20on%20topological%20and%20geometric%20properties%20during%0Adimensionality%20reduction%2C%20whereas%20the%20two%20components%20promote%20each%20other%20during%0Atraining.%20Experiments%20on%20point%20cloud%20datasets%20demonstrate%20that%20our%20method%0Aoutperforms%20baselines%20like%20t-SNE%2C%20UMAP%2C%20and%20Topological%20AutoEncoders%20in%0Adiscovering%20manifold%20structures%20from%20noisy%20data%20and%20preserving%20them%20through%0Adimensionality%20reduction%2C%20as%20validated%20by%20visualization%20and%20quantitative%0Ametrics.%20This%20work%20demonstrates%20the%20significance%20of%20combining%20manifold%0Areconstruction%20with%20manifold%20learning%20to%20achieve%20reliable%20representation%20of%20the%0Alatent%20manifold%2C%20particularly%20when%20dealing%20with%20noisy%20real-world%20data.%20Code%0Arepository%3A%20https%3A//github.com/Thanatorika/mrtg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04412v1&entry.124074799=Read"},
{"title": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion", "author": "Shuaiting Li and Juncan Deng and Zeyu Wang and Kedong Xu and Rongtao Deng and Hong Gu and Haibin Shen and Kejie Huang", "abstract": "  Text-to-image generation via Stable Diffusion models (SDM) have demonstrated\nremarkable capabilities. However, their computational intensity, particularly\nin the iterative denoising process, hinders real-time deployment in\nlatency-sensitive applications. While Recent studies have explored\npost-training quantization (PTQ) and quantization-aware training (QAT) methods\nto compress Diffusion models, existing methods often overlook the consistency\nbetween results generated by quantized models and those from floating-point\nmodels. This consistency is paramount for professional applications where both\nefficiency and output reliability are essential. To ensure that quantized SDM\ngenerates high-quality and consistent images, we propose an efficient\nquantization framework for SDM. Our framework introduces a Serial-to-Parallel\npipeline that simultaneously maintains training-inference consistency and\nensures optimization stability. Building upon this foundation, we further\ndevelop several techniques including multi-timestep activation quantization,\ntime information precalculation, inter-layer distillation, and selective\nfreezing, to achieve high-fidelity generation in comparison to floating-point\nmodels while maintaining quantization efficiency.\n  Through comprehensive evaluation across multiple Stable Diffusion variants\n(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over\nstate-of-the-art approaches with shorter training times. Under W4A8\nquantization settings, we achieve significant improvements in both distribution\nsimilarity and visual fidelity, while preserving a high image quality.\n", "link": "http://arxiv.org/abs/2412.06661v2", "date": "2025-05-07", "relevancy": 2.7167, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7266}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6841}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiency%20Meets%20Fidelity%3A%20A%20Novel%20Quantization%20Framework%20for%20Stable%0A%20%20Diffusion&body=Title%3A%20Efficiency%20Meets%20Fidelity%3A%20A%20Novel%20Quantization%20Framework%20for%20Stable%0A%20%20Diffusion%0AAuthor%3A%20Shuaiting%20Li%20and%20Juncan%20Deng%20and%20Zeyu%20Wang%20and%20Kedong%20Xu%20and%20Rongtao%20Deng%20and%20Hong%20Gu%20and%20Haibin%20Shen%20and%20Kejie%20Huang%0AAbstract%3A%20%20%20Text-to-image%20generation%20via%20Stable%20Diffusion%20models%20%28SDM%29%20have%20demonstrated%0Aremarkable%20capabilities.%20However%2C%20their%20computational%20intensity%2C%20particularly%0Ain%20the%20iterative%20denoising%20process%2C%20hinders%20real-time%20deployment%20in%0Alatency-sensitive%20applications.%20While%20Recent%20studies%20have%20explored%0Apost-training%20quantization%20%28PTQ%29%20and%20quantization-aware%20training%20%28QAT%29%20methods%0Ato%20compress%20Diffusion%20models%2C%20existing%20methods%20often%20overlook%20the%20consistency%0Abetween%20results%20generated%20by%20quantized%20models%20and%20those%20from%20floating-point%0Amodels.%20This%20consistency%20is%20paramount%20for%20professional%20applications%20where%20both%0Aefficiency%20and%20output%20reliability%20are%20essential.%20To%20ensure%20that%20quantized%20SDM%0Agenerates%20high-quality%20and%20consistent%20images%2C%20we%20propose%20an%20efficient%0Aquantization%20framework%20for%20SDM.%20Our%20framework%20introduces%20a%20Serial-to-Parallel%0Apipeline%20that%20simultaneously%20maintains%20training-inference%20consistency%20and%0Aensures%20optimization%20stability.%20Building%20upon%20this%20foundation%2C%20we%20further%0Adevelop%20several%20techniques%20including%20multi-timestep%20activation%20quantization%2C%0Atime%20information%20precalculation%2C%20inter-layer%20distillation%2C%20and%20selective%0Afreezing%2C%20to%20achieve%20high-fidelity%20generation%20in%20comparison%20to%20floating-point%0Amodels%20while%20maintaining%20quantization%20efficiency.%0A%20%20Through%20comprehensive%20evaluation%20across%20multiple%20Stable%20Diffusion%20variants%0A%28v1-4%2C%20v2-1%2C%20XL%201.0%2C%20and%20v3%29%2C%20our%20method%20demonstrates%20superior%20performance%20over%0Astate-of-the-art%20approaches%20with%20shorter%20training%20times.%20Under%20W4A8%0Aquantization%20settings%2C%20we%20achieve%20significant%20improvements%20in%20both%20distribution%0Asimilarity%20and%20visual%20fidelity%2C%20while%20preserving%20a%20high%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.06661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiency%2520Meets%2520Fidelity%253A%2520A%2520Novel%2520Quantization%2520Framework%2520for%2520Stable%250A%2520%2520Diffusion%26entry.906535625%3DShuaiting%2520Li%2520and%2520Juncan%2520Deng%2520and%2520Zeyu%2520Wang%2520and%2520Kedong%2520Xu%2520and%2520Rongtao%2520Deng%2520and%2520Hong%2520Gu%2520and%2520Haibin%2520Shen%2520and%2520Kejie%2520Huang%26entry.1292438233%3D%2520%2520Text-to-image%2520generation%2520via%2520Stable%2520Diffusion%2520models%2520%2528SDM%2529%2520have%2520demonstrated%250Aremarkable%2520capabilities.%2520However%252C%2520their%2520computational%2520intensity%252C%2520particularly%250Ain%2520the%2520iterative%2520denoising%2520process%252C%2520hinders%2520real-time%2520deployment%2520in%250Alatency-sensitive%2520applications.%2520While%2520Recent%2520studies%2520have%2520explored%250Apost-training%2520quantization%2520%2528PTQ%2529%2520and%2520quantization-aware%2520training%2520%2528QAT%2529%2520methods%250Ato%2520compress%2520Diffusion%2520models%252C%2520existing%2520methods%2520often%2520overlook%2520the%2520consistency%250Abetween%2520results%2520generated%2520by%2520quantized%2520models%2520and%2520those%2520from%2520floating-point%250Amodels.%2520This%2520consistency%2520is%2520paramount%2520for%2520professional%2520applications%2520where%2520both%250Aefficiency%2520and%2520output%2520reliability%2520are%2520essential.%2520To%2520ensure%2520that%2520quantized%2520SDM%250Agenerates%2520high-quality%2520and%2520consistent%2520images%252C%2520we%2520propose%2520an%2520efficient%250Aquantization%2520framework%2520for%2520SDM.%2520Our%2520framework%2520introduces%2520a%2520Serial-to-Parallel%250Apipeline%2520that%2520simultaneously%2520maintains%2520training-inference%2520consistency%2520and%250Aensures%2520optimization%2520stability.%2520Building%2520upon%2520this%2520foundation%252C%2520we%2520further%250Adevelop%2520several%2520techniques%2520including%2520multi-timestep%2520activation%2520quantization%252C%250Atime%2520information%2520precalculation%252C%2520inter-layer%2520distillation%252C%2520and%2520selective%250Afreezing%252C%2520to%2520achieve%2520high-fidelity%2520generation%2520in%2520comparison%2520to%2520floating-point%250Amodels%2520while%2520maintaining%2520quantization%2520efficiency.%250A%2520%2520Through%2520comprehensive%2520evaluation%2520across%2520multiple%2520Stable%2520Diffusion%2520variants%250A%2528v1-4%252C%2520v2-1%252C%2520XL%25201.0%252C%2520and%2520v3%2529%252C%2520our%2520method%2520demonstrates%2520superior%2520performance%2520over%250Astate-of-the-art%2520approaches%2520with%2520shorter%2520training%2520times.%2520Under%2520W4A8%250Aquantization%2520settings%252C%2520we%2520achieve%2520significant%2520improvements%2520in%2520both%2520distribution%250Asimilarity%2520and%2520visual%2520fidelity%252C%2520while%2520preserving%2520a%2520high%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiency%20Meets%20Fidelity%3A%20A%20Novel%20Quantization%20Framework%20for%20Stable%0A%20%20Diffusion&entry.906535625=Shuaiting%20Li%20and%20Juncan%20Deng%20and%20Zeyu%20Wang%20and%20Kedong%20Xu%20and%20Rongtao%20Deng%20and%20Hong%20Gu%20and%20Haibin%20Shen%20and%20Kejie%20Huang&entry.1292438233=%20%20Text-to-image%20generation%20via%20Stable%20Diffusion%20models%20%28SDM%29%20have%20demonstrated%0Aremarkable%20capabilities.%20However%2C%20their%20computational%20intensity%2C%20particularly%0Ain%20the%20iterative%20denoising%20process%2C%20hinders%20real-time%20deployment%20in%0Alatency-sensitive%20applications.%20While%20Recent%20studies%20have%20explored%0Apost-training%20quantization%20%28PTQ%29%20and%20quantization-aware%20training%20%28QAT%29%20methods%0Ato%20compress%20Diffusion%20models%2C%20existing%20methods%20often%20overlook%20the%20consistency%0Abetween%20results%20generated%20by%20quantized%20models%20and%20those%20from%20floating-point%0Amodels.%20This%20consistency%20is%20paramount%20for%20professional%20applications%20where%20both%0Aefficiency%20and%20output%20reliability%20are%20essential.%20To%20ensure%20that%20quantized%20SDM%0Agenerates%20high-quality%20and%20consistent%20images%2C%20we%20propose%20an%20efficient%0Aquantization%20framework%20for%20SDM.%20Our%20framework%20introduces%20a%20Serial-to-Parallel%0Apipeline%20that%20simultaneously%20maintains%20training-inference%20consistency%20and%0Aensures%20optimization%20stability.%20Building%20upon%20this%20foundation%2C%20we%20further%0Adevelop%20several%20techniques%20including%20multi-timestep%20activation%20quantization%2C%0Atime%20information%20precalculation%2C%20inter-layer%20distillation%2C%20and%20selective%0Afreezing%2C%20to%20achieve%20high-fidelity%20generation%20in%20comparison%20to%20floating-point%0Amodels%20while%20maintaining%20quantization%20efficiency.%0A%20%20Through%20comprehensive%20evaluation%20across%20multiple%20Stable%20Diffusion%20variants%0A%28v1-4%2C%20v2-1%2C%20XL%201.0%2C%20and%20v3%29%2C%20our%20method%20demonstrates%20superior%20performance%20over%0Astate-of-the-art%20approaches%20with%20shorter%20training%20times.%20Under%20W4A8%0Aquantization%20settings%2C%20we%20achieve%20significant%20improvements%20in%20both%20distribution%0Asimilarity%20and%20visual%20fidelity%2C%20while%20preserving%20a%20high%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.06661v2&entry.124074799=Read"},
{"title": "Learning Real Facial Concepts for Independent Deepfake Detection", "author": "Ming-Hui Liu and Harry Cheng and Tianyi Wang and Xin Luo and Xin-Shun Xu", "abstract": "  Deepfake detection models often struggle with generalization to unseen\ndatasets, manifesting as misclassifying real instances as fake in target\ndomains. This is primarily due to an overreliance on forgery artifacts and a\nlimited understanding of real faces. To address this challenge, we propose a\nnovel approach RealID to enhance generalization by learning a comprehensive\nconcept of real faces while assessing the probabilities of belonging to the\nreal and fake classes independently. RealID comprises two key modules: the Real\nConcept Capture Module (RealC2) and the Independent Dual-Decision Classifier\n(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various\nprototypes for real faces, allowing the model to capture a comprehensive\nconcept of real class. Meanwhile, IDC redefines the classification strategy by\nmaking independent decisions based on the concept of the real class and the\npresence of forgery artifacts. Through the combined effect of the above\nmodules, the influence of forgery-irrelevant patterns is alleviated, and\nextensive experiments on five widely used datasets demonstrate that RealID\nsignificantly outperforms existing state-of-the-art methods, achieving a 1.74%\nimprovement in average accuracy.\n", "link": "http://arxiv.org/abs/2505.04460v1", "date": "2025-05-07", "relevancy": 2.7002, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5599}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5519}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Real%20Facial%20Concepts%20for%20Independent%20Deepfake%20Detection&body=Title%3A%20Learning%20Real%20Facial%20Concepts%20for%20Independent%20Deepfake%20Detection%0AAuthor%3A%20Ming-Hui%20Liu%20and%20Harry%20Cheng%20and%20Tianyi%20Wang%20and%20Xin%20Luo%20and%20Xin-Shun%20Xu%0AAbstract%3A%20%20%20Deepfake%20detection%20models%20often%20struggle%20with%20generalization%20to%20unseen%0Adatasets%2C%20manifesting%20as%20misclassifying%20real%20instances%20as%20fake%20in%20target%0Adomains.%20This%20is%20primarily%20due%20to%20an%20overreliance%20on%20forgery%20artifacts%20and%20a%0Alimited%20understanding%20of%20real%20faces.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Anovel%20approach%20RealID%20to%20enhance%20generalization%20by%20learning%20a%20comprehensive%0Aconcept%20of%20real%20faces%20while%20assessing%20the%20probabilities%20of%20belonging%20to%20the%0Areal%20and%20fake%20classes%20independently.%20RealID%20comprises%20two%20key%20modules%3A%20the%20Real%0AConcept%20Capture%20Module%20%28RealC2%29%20and%20the%20Independent%20Dual-Decision%20Classifier%0A%28IDC%29.%20With%20the%20assistance%20of%20a%20MultiReal%20Memory%2C%20RealC2%20maintains%20various%0Aprototypes%20for%20real%20faces%2C%20allowing%20the%20model%20to%20capture%20a%20comprehensive%0Aconcept%20of%20real%20class.%20Meanwhile%2C%20IDC%20redefines%20the%20classification%20strategy%20by%0Amaking%20independent%20decisions%20based%20on%20the%20concept%20of%20the%20real%20class%20and%20the%0Apresence%20of%20forgery%20artifacts.%20Through%20the%20combined%20effect%20of%20the%20above%0Amodules%2C%20the%20influence%20of%20forgery-irrelevant%20patterns%20is%20alleviated%2C%20and%0Aextensive%20experiments%20on%20five%20widely%20used%20datasets%20demonstrate%20that%20RealID%0Asignificantly%20outperforms%20existing%20state-of-the-art%20methods%2C%20achieving%20a%201.74%25%0Aimprovement%20in%20average%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Real%2520Facial%2520Concepts%2520for%2520Independent%2520Deepfake%2520Detection%26entry.906535625%3DMing-Hui%2520Liu%2520and%2520Harry%2520Cheng%2520and%2520Tianyi%2520Wang%2520and%2520Xin%2520Luo%2520and%2520Xin-Shun%2520Xu%26entry.1292438233%3D%2520%2520Deepfake%2520detection%2520models%2520often%2520struggle%2520with%2520generalization%2520to%2520unseen%250Adatasets%252C%2520manifesting%2520as%2520misclassifying%2520real%2520instances%2520as%2520fake%2520in%2520target%250Adomains.%2520This%2520is%2520primarily%2520due%2520to%2520an%2520overreliance%2520on%2520forgery%2520artifacts%2520and%2520a%250Alimited%2520understanding%2520of%2520real%2520faces.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520RealID%2520to%2520enhance%2520generalization%2520by%2520learning%2520a%2520comprehensive%250Aconcept%2520of%2520real%2520faces%2520while%2520assessing%2520the%2520probabilities%2520of%2520belonging%2520to%2520the%250Areal%2520and%2520fake%2520classes%2520independently.%2520RealID%2520comprises%2520two%2520key%2520modules%253A%2520the%2520Real%250AConcept%2520Capture%2520Module%2520%2528RealC2%2529%2520and%2520the%2520Independent%2520Dual-Decision%2520Classifier%250A%2528IDC%2529.%2520With%2520the%2520assistance%2520of%2520a%2520MultiReal%2520Memory%252C%2520RealC2%2520maintains%2520various%250Aprototypes%2520for%2520real%2520faces%252C%2520allowing%2520the%2520model%2520to%2520capture%2520a%2520comprehensive%250Aconcept%2520of%2520real%2520class.%2520Meanwhile%252C%2520IDC%2520redefines%2520the%2520classification%2520strategy%2520by%250Amaking%2520independent%2520decisions%2520based%2520on%2520the%2520concept%2520of%2520the%2520real%2520class%2520and%2520the%250Apresence%2520of%2520forgery%2520artifacts.%2520Through%2520the%2520combined%2520effect%2520of%2520the%2520above%250Amodules%252C%2520the%2520influence%2520of%2520forgery-irrelevant%2520patterns%2520is%2520alleviated%252C%2520and%250Aextensive%2520experiments%2520on%2520five%2520widely%2520used%2520datasets%2520demonstrate%2520that%2520RealID%250Asignificantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%252C%2520achieving%2520a%25201.74%2525%250Aimprovement%2520in%2520average%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Real%20Facial%20Concepts%20for%20Independent%20Deepfake%20Detection&entry.906535625=Ming-Hui%20Liu%20and%20Harry%20Cheng%20and%20Tianyi%20Wang%20and%20Xin%20Luo%20and%20Xin-Shun%20Xu&entry.1292438233=%20%20Deepfake%20detection%20models%20often%20struggle%20with%20generalization%20to%20unseen%0Adatasets%2C%20manifesting%20as%20misclassifying%20real%20instances%20as%20fake%20in%20target%0Adomains.%20This%20is%20primarily%20due%20to%20an%20overreliance%20on%20forgery%20artifacts%20and%20a%0Alimited%20understanding%20of%20real%20faces.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Anovel%20approach%20RealID%20to%20enhance%20generalization%20by%20learning%20a%20comprehensive%0Aconcept%20of%20real%20faces%20while%20assessing%20the%20probabilities%20of%20belonging%20to%20the%0Areal%20and%20fake%20classes%20independently.%20RealID%20comprises%20two%20key%20modules%3A%20the%20Real%0AConcept%20Capture%20Module%20%28RealC2%29%20and%20the%20Independent%20Dual-Decision%20Classifier%0A%28IDC%29.%20With%20the%20assistance%20of%20a%20MultiReal%20Memory%2C%20RealC2%20maintains%20various%0Aprototypes%20for%20real%20faces%2C%20allowing%20the%20model%20to%20capture%20a%20comprehensive%0Aconcept%20of%20real%20class.%20Meanwhile%2C%20IDC%20redefines%20the%20classification%20strategy%20by%0Amaking%20independent%20decisions%20based%20on%20the%20concept%20of%20the%20real%20class%20and%20the%0Apresence%20of%20forgery%20artifacts.%20Through%20the%20combined%20effect%20of%20the%20above%0Amodules%2C%20the%20influence%20of%20forgery-irrelevant%20patterns%20is%20alleviated%2C%20and%0Aextensive%20experiments%20on%20five%20widely%20used%20datasets%20demonstrate%20that%20RealID%0Asignificantly%20outperforms%20existing%20state-of-the-art%20methods%2C%20achieving%20a%201.74%25%0Aimprovement%20in%20average%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04460v1&entry.124074799=Read"},
{"title": "Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either\n  Stereo or Mono Fail", "author": "Luca Bartolomei and Fabio Tosi and Matteo Poggi and Stefano Mattoccia", "abstract": "  We introduce Stereo Anywhere, a novel stereo-matching framework that combines\ngeometric constraints with robust priors from monocular depth Vision Foundation\nModels (VFMs). By elegantly coupling these complementary worlds through a\ndual-branch architecture, we seamlessly integrate stereo matching with learned\ncontextual cues. Following this design, our framework introduces novel cost\nvolume fusion mechanisms that effectively handle critical challenges such as\ntextureless regions, occlusions, and non-Lambertian surfaces. Through our novel\noptical illusion dataset, MonoTrap, and extensive evaluation across multiple\nbenchmarks, we demonstrate that our synthetic-only trained model achieves\nstate-of-the-art results in zero-shot generalization, significantly\noutperforming existing solutions while showing remarkable robustness to\nchallenging cases such as mirrors and transparencies.\n", "link": "http://arxiv.org/abs/2412.04472v2", "date": "2025-05-07", "relevancy": 2.6617, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo%20Anywhere%3A%20Robust%20Zero-Shot%20Deep%20Stereo%20Matching%20Even%20Where%20Either%0A%20%20Stereo%20or%20Mono%20Fail&body=Title%3A%20Stereo%20Anywhere%3A%20Robust%20Zero-Shot%20Deep%20Stereo%20Matching%20Even%20Where%20Either%0A%20%20Stereo%20or%20Mono%20Fail%0AAuthor%3A%20Luca%20Bartolomei%20and%20Fabio%20Tosi%20and%20Matteo%20Poggi%20and%20Stefano%20Mattoccia%0AAbstract%3A%20%20%20We%20introduce%20Stereo%20Anywhere%2C%20a%20novel%20stereo-matching%20framework%20that%20combines%0Ageometric%20constraints%20with%20robust%20priors%20from%20monocular%20depth%20Vision%20Foundation%0AModels%20%28VFMs%29.%20By%20elegantly%20coupling%20these%20complementary%20worlds%20through%20a%0Adual-branch%20architecture%2C%20we%20seamlessly%20integrate%20stereo%20matching%20with%20learned%0Acontextual%20cues.%20Following%20this%20design%2C%20our%20framework%20introduces%20novel%20cost%0Avolume%20fusion%20mechanisms%20that%20effectively%20handle%20critical%20challenges%20such%20as%0Atextureless%20regions%2C%20occlusions%2C%20and%20non-Lambertian%20surfaces.%20Through%20our%20novel%0Aoptical%20illusion%20dataset%2C%20MonoTrap%2C%20and%20extensive%20evaluation%20across%20multiple%0Abenchmarks%2C%20we%20demonstrate%20that%20our%20synthetic-only%20trained%20model%20achieves%0Astate-of-the-art%20results%20in%20zero-shot%20generalization%2C%20significantly%0Aoutperforming%20existing%20solutions%20while%20showing%20remarkable%20robustness%20to%0Achallenging%20cases%20such%20as%20mirrors%20and%20transparencies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo%2520Anywhere%253A%2520Robust%2520Zero-Shot%2520Deep%2520Stereo%2520Matching%2520Even%2520Where%2520Either%250A%2520%2520Stereo%2520or%2520Mono%2520Fail%26entry.906535625%3DLuca%2520Bartolomei%2520and%2520Fabio%2520Tosi%2520and%2520Matteo%2520Poggi%2520and%2520Stefano%2520Mattoccia%26entry.1292438233%3D%2520%2520We%2520introduce%2520Stereo%2520Anywhere%252C%2520a%2520novel%2520stereo-matching%2520framework%2520that%2520combines%250Ageometric%2520constraints%2520with%2520robust%2520priors%2520from%2520monocular%2520depth%2520Vision%2520Foundation%250AModels%2520%2528VFMs%2529.%2520By%2520elegantly%2520coupling%2520these%2520complementary%2520worlds%2520through%2520a%250Adual-branch%2520architecture%252C%2520we%2520seamlessly%2520integrate%2520stereo%2520matching%2520with%2520learned%250Acontextual%2520cues.%2520Following%2520this%2520design%252C%2520our%2520framework%2520introduces%2520novel%2520cost%250Avolume%2520fusion%2520mechanisms%2520that%2520effectively%2520handle%2520critical%2520challenges%2520such%2520as%250Atextureless%2520regions%252C%2520occlusions%252C%2520and%2520non-Lambertian%2520surfaces.%2520Through%2520our%2520novel%250Aoptical%2520illusion%2520dataset%252C%2520MonoTrap%252C%2520and%2520extensive%2520evaluation%2520across%2520multiple%250Abenchmarks%252C%2520we%2520demonstrate%2520that%2520our%2520synthetic-only%2520trained%2520model%2520achieves%250Astate-of-the-art%2520results%2520in%2520zero-shot%2520generalization%252C%2520significantly%250Aoutperforming%2520existing%2520solutions%2520while%2520showing%2520remarkable%2520robustness%2520to%250Achallenging%2520cases%2520such%2520as%2520mirrors%2520and%2520transparencies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo%20Anywhere%3A%20Robust%20Zero-Shot%20Deep%20Stereo%20Matching%20Even%20Where%20Either%0A%20%20Stereo%20or%20Mono%20Fail&entry.906535625=Luca%20Bartolomei%20and%20Fabio%20Tosi%20and%20Matteo%20Poggi%20and%20Stefano%20Mattoccia&entry.1292438233=%20%20We%20introduce%20Stereo%20Anywhere%2C%20a%20novel%20stereo-matching%20framework%20that%20combines%0Ageometric%20constraints%20with%20robust%20priors%20from%20monocular%20depth%20Vision%20Foundation%0AModels%20%28VFMs%29.%20By%20elegantly%20coupling%20these%20complementary%20worlds%20through%20a%0Adual-branch%20architecture%2C%20we%20seamlessly%20integrate%20stereo%20matching%20with%20learned%0Acontextual%20cues.%20Following%20this%20design%2C%20our%20framework%20introduces%20novel%20cost%0Avolume%20fusion%20mechanisms%20that%20effectively%20handle%20critical%20challenges%20such%20as%0Atextureless%20regions%2C%20occlusions%2C%20and%20non-Lambertian%20surfaces.%20Through%20our%20novel%0Aoptical%20illusion%20dataset%2C%20MonoTrap%2C%20and%20extensive%20evaluation%20across%20multiple%0Abenchmarks%2C%20we%20demonstrate%20that%20our%20synthetic-only%20trained%20model%20achieves%0Astate-of-the-art%20results%20in%20zero-shot%20generalization%2C%20significantly%0Aoutperforming%20existing%20solutions%20while%20showing%20remarkable%20robustness%20to%0Achallenging%20cases%20such%20as%20mirrors%20and%20transparencies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04472v2&entry.124074799=Read"},
{"title": "Registration of 3D Point Sets Using Exponential-based Similarity Matrix", "author": "Ashutosh Singandhupe and Sanket Lokhande and Hung Manh La", "abstract": "  Point cloud registration is a fundamental problem in computer vision and\nrobotics, involving the alignment of 3D point sets captured from varying\nviewpoints using depth sensors such as LiDAR or structured light. In modern\nrobotic systems, especially those focused on mapping, it is essential to merge\nmultiple views of the same environment accurately. However, state-of-the-art\nregistration techniques often struggle when large rotational differences exist\nbetween point sets or when the data is significantly corrupted by sensor noise.\nThese challenges can lead to misalignments and, consequently, to inaccurate or\ndistorted 3D reconstructions. In this work, we address both these limitations\nby proposing a robust modification to the classic Iterative Closest Point (ICP)\nalgorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),\nintegrates a Gaussian-inspired exponential weighting scheme to construct a\nsimilarity matrix that dynamically adapts across iterations. This matrix\nfacilitates improved estimation of both rotational and translational components\nduring alignment. We demonstrate the robustness of ESM-ICP in two challenging\nscenarios: (i) large rotational discrepancies between the source and target\npoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results show\nthat ESM-ICP outperforms traditional geometric registration techniques as well\nas several recent learning-based methods. To encourage reproducibility and\ncommunity engagement, our full implementation is made publicly available on\nGitHub. https://github.com/aralab-unr/ESM_ICP\n", "link": "http://arxiv.org/abs/2505.04540v1", "date": "2025-05-07", "relevancy": 2.6567, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5393}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.533}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Registration%20of%203D%20Point%20Sets%20Using%20Exponential-based%20Similarity%20Matrix&body=Title%3A%20Registration%20of%203D%20Point%20Sets%20Using%20Exponential-based%20Similarity%20Matrix%0AAuthor%3A%20Ashutosh%20Singandhupe%20and%20Sanket%20Lokhande%20and%20Hung%20Manh%20La%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%20and%0Arobotics%2C%20involving%20the%20alignment%20of%203D%20point%20sets%20captured%20from%20varying%0Aviewpoints%20using%20depth%20sensors%20such%20as%20LiDAR%20or%20structured%20light.%20In%20modern%0Arobotic%20systems%2C%20especially%20those%20focused%20on%20mapping%2C%20it%20is%20essential%20to%20merge%0Amultiple%20views%20of%20the%20same%20environment%20accurately.%20However%2C%20state-of-the-art%0Aregistration%20techniques%20often%20struggle%20when%20large%20rotational%20differences%20exist%0Abetween%20point%20sets%20or%20when%20the%20data%20is%20significantly%20corrupted%20by%20sensor%20noise.%0AThese%20challenges%20can%20lead%20to%20misalignments%20and%2C%20consequently%2C%20to%20inaccurate%20or%0Adistorted%203D%20reconstructions.%20In%20this%20work%2C%20we%20address%20both%20these%20limitations%0Aby%20proposing%20a%20robust%20modification%20to%20the%20classic%20Iterative%20Closest%20Point%20%28ICP%29%0Aalgorithm.%20Our%20method%2C%20termed%20Exponential%20Similarity%20Matrix%20ICP%20%28ESM-ICP%29%2C%0Aintegrates%20a%20Gaussian-inspired%20exponential%20weighting%20scheme%20to%20construct%20a%0Asimilarity%20matrix%20that%20dynamically%20adapts%20across%20iterations.%20This%20matrix%0Afacilitates%20improved%20estimation%20of%20both%20rotational%20and%20translational%20components%0Aduring%20alignment.%20We%20demonstrate%20the%20robustness%20of%20ESM-ICP%20in%20two%20challenging%0Ascenarios%3A%20%28i%29%20large%20rotational%20discrepancies%20between%20the%20source%20and%20target%0Apoint%20clouds%2C%20and%20%28ii%29%20data%20corrupted%20by%20non-Gaussian%20noise.%20Our%20results%20show%0Athat%20ESM-ICP%20outperforms%20traditional%20geometric%20registration%20techniques%20as%20well%0Aas%20several%20recent%20learning-based%20methods.%20To%20encourage%20reproducibility%20and%0Acommunity%20engagement%2C%20our%20full%20implementation%20is%20made%20publicly%20available%20on%0AGitHub.%20https%3A//github.com/aralab-unr/ESM_ICP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegistration%2520of%25203D%2520Point%2520Sets%2520Using%2520Exponential-based%2520Similarity%2520Matrix%26entry.906535625%3DAshutosh%2520Singandhupe%2520and%2520Sanket%2520Lokhande%2520and%2520Hung%2520Manh%2520La%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520a%2520fundamental%2520problem%2520in%2520computer%2520vision%2520and%250Arobotics%252C%2520involving%2520the%2520alignment%2520of%25203D%2520point%2520sets%2520captured%2520from%2520varying%250Aviewpoints%2520using%2520depth%2520sensors%2520such%2520as%2520LiDAR%2520or%2520structured%2520light.%2520In%2520modern%250Arobotic%2520systems%252C%2520especially%2520those%2520focused%2520on%2520mapping%252C%2520it%2520is%2520essential%2520to%2520merge%250Amultiple%2520views%2520of%2520the%2520same%2520environment%2520accurately.%2520However%252C%2520state-of-the-art%250Aregistration%2520techniques%2520often%2520struggle%2520when%2520large%2520rotational%2520differences%2520exist%250Abetween%2520point%2520sets%2520or%2520when%2520the%2520data%2520is%2520significantly%2520corrupted%2520by%2520sensor%2520noise.%250AThese%2520challenges%2520can%2520lead%2520to%2520misalignments%2520and%252C%2520consequently%252C%2520to%2520inaccurate%2520or%250Adistorted%25203D%2520reconstructions.%2520In%2520this%2520work%252C%2520we%2520address%2520both%2520these%2520limitations%250Aby%2520proposing%2520a%2520robust%2520modification%2520to%2520the%2520classic%2520Iterative%2520Closest%2520Point%2520%2528ICP%2529%250Aalgorithm.%2520Our%2520method%252C%2520termed%2520Exponential%2520Similarity%2520Matrix%2520ICP%2520%2528ESM-ICP%2529%252C%250Aintegrates%2520a%2520Gaussian-inspired%2520exponential%2520weighting%2520scheme%2520to%2520construct%2520a%250Asimilarity%2520matrix%2520that%2520dynamically%2520adapts%2520across%2520iterations.%2520This%2520matrix%250Afacilitates%2520improved%2520estimation%2520of%2520both%2520rotational%2520and%2520translational%2520components%250Aduring%2520alignment.%2520We%2520demonstrate%2520the%2520robustness%2520of%2520ESM-ICP%2520in%2520two%2520challenging%250Ascenarios%253A%2520%2528i%2529%2520large%2520rotational%2520discrepancies%2520between%2520the%2520source%2520and%2520target%250Apoint%2520clouds%252C%2520and%2520%2528ii%2529%2520data%2520corrupted%2520by%2520non-Gaussian%2520noise.%2520Our%2520results%2520show%250Athat%2520ESM-ICP%2520outperforms%2520traditional%2520geometric%2520registration%2520techniques%2520as%2520well%250Aas%2520several%2520recent%2520learning-based%2520methods.%2520To%2520encourage%2520reproducibility%2520and%250Acommunity%2520engagement%252C%2520our%2520full%2520implementation%2520is%2520made%2520publicly%2520available%2520on%250AGitHub.%2520https%253A//github.com/aralab-unr/ESM_ICP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Registration%20of%203D%20Point%20Sets%20Using%20Exponential-based%20Similarity%20Matrix&entry.906535625=Ashutosh%20Singandhupe%20and%20Sanket%20Lokhande%20and%20Hung%20Manh%20La&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20fundamental%20problem%20in%20computer%20vision%20and%0Arobotics%2C%20involving%20the%20alignment%20of%203D%20point%20sets%20captured%20from%20varying%0Aviewpoints%20using%20depth%20sensors%20such%20as%20LiDAR%20or%20structured%20light.%20In%20modern%0Arobotic%20systems%2C%20especially%20those%20focused%20on%20mapping%2C%20it%20is%20essential%20to%20merge%0Amultiple%20views%20of%20the%20same%20environment%20accurately.%20However%2C%20state-of-the-art%0Aregistration%20techniques%20often%20struggle%20when%20large%20rotational%20differences%20exist%0Abetween%20point%20sets%20or%20when%20the%20data%20is%20significantly%20corrupted%20by%20sensor%20noise.%0AThese%20challenges%20can%20lead%20to%20misalignments%20and%2C%20consequently%2C%20to%20inaccurate%20or%0Adistorted%203D%20reconstructions.%20In%20this%20work%2C%20we%20address%20both%20these%20limitations%0Aby%20proposing%20a%20robust%20modification%20to%20the%20classic%20Iterative%20Closest%20Point%20%28ICP%29%0Aalgorithm.%20Our%20method%2C%20termed%20Exponential%20Similarity%20Matrix%20ICP%20%28ESM-ICP%29%2C%0Aintegrates%20a%20Gaussian-inspired%20exponential%20weighting%20scheme%20to%20construct%20a%0Asimilarity%20matrix%20that%20dynamically%20adapts%20across%20iterations.%20This%20matrix%0Afacilitates%20improved%20estimation%20of%20both%20rotational%20and%20translational%20components%0Aduring%20alignment.%20We%20demonstrate%20the%20robustness%20of%20ESM-ICP%20in%20two%20challenging%0Ascenarios%3A%20%28i%29%20large%20rotational%20discrepancies%20between%20the%20source%20and%20target%0Apoint%20clouds%2C%20and%20%28ii%29%20data%20corrupted%20by%20non-Gaussian%20noise.%20Our%20results%20show%0Athat%20ESM-ICP%20outperforms%20traditional%20geometric%20registration%20techniques%20as%20well%0Aas%20several%20recent%20learning-based%20methods.%20To%20encourage%20reproducibility%20and%0Acommunity%20engagement%2C%20our%20full%20implementation%20is%20made%20publicly%20available%20on%0AGitHub.%20https%3A//github.com/aralab-unr/ESM_ICP%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04540v1&entry.124074799=Read"},
{"title": "Tetrahedron-Net for Medical Image Registration", "author": "Jinhai Xiang and Shuai Guo and Qianru Han and Dantong Shi and Xinwei He and Xiang Bai", "abstract": "  Medical image registration plays a vital role in medical image processing.\nExtracting expressive representations for medical images is crucial for\nimproving the registration quality. One common practice for this end is\nconstructing a convolutional backbone to enable interactions with skip\nconnections among feature extraction layers. The de facto structure, U-Net-like\nnetworks, has attempted to design skip connections such as nested or full-scale\nones to connect one single encoder and one single decoder to improve its\nrepresentation capacity. Despite being effective, it still does not fully\nexplore interactions with a single encoder and decoder architectures. In this\npaper, we embrace this observation and introduce a simple yet effective\nalternative strategy to enhance the representations for registrations by\nappending one additional decoder. The new decoder is designed to interact with\nboth the original encoder and decoder. In this way, it not only reuses feature\npresentation from corresponding layers in the encoder but also interacts with\nthe original decoder to corporately give more accurate registration results.\nThe new architecture is concise yet generalized, with only one encoder and two\ndecoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.\nThree instantiations of Tetrahedron-Net are further constructed regarding the\ndifferent structures of the appended decoder. Our extensive experiments prove\nthat superior performance can be obtained on several representative benchmarks\nof medical image registration. Finally, such a ``Tetrahedron'' design can also\nbe easily integrated into popular U-Net-like architectures including\nVoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.\n", "link": "http://arxiv.org/abs/2505.04380v1", "date": "2025-05-07", "relevancy": 2.652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5317}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tetrahedron-Net%20for%20Medical%20Image%20Registration&body=Title%3A%20Tetrahedron-Net%20for%20Medical%20Image%20Registration%0AAuthor%3A%20Jinhai%20Xiang%20and%20Shuai%20Guo%20and%20Qianru%20Han%20and%20Dantong%20Shi%20and%20Xinwei%20He%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Medical%20image%20registration%20plays%20a%20vital%20role%20in%20medical%20image%20processing.%0AExtracting%20expressive%20representations%20for%20medical%20images%20is%20crucial%20for%0Aimproving%20the%20registration%20quality.%20One%20common%20practice%20for%20this%20end%20is%0Aconstructing%20a%20convolutional%20backbone%20to%20enable%20interactions%20with%20skip%0Aconnections%20among%20feature%20extraction%20layers.%20The%20de%20facto%20structure%2C%20U-Net-like%0Anetworks%2C%20has%20attempted%20to%20design%20skip%20connections%20such%20as%20nested%20or%20full-scale%0Aones%20to%20connect%20one%20single%20encoder%20and%20one%20single%20decoder%20to%20improve%20its%0Arepresentation%20capacity.%20Despite%20being%20effective%2C%20it%20still%20does%20not%20fully%0Aexplore%20interactions%20with%20a%20single%20encoder%20and%20decoder%20architectures.%20In%20this%0Apaper%2C%20we%20embrace%20this%20observation%20and%20introduce%20a%20simple%20yet%20effective%0Aalternative%20strategy%20to%20enhance%20the%20representations%20for%20registrations%20by%0Aappending%20one%20additional%20decoder.%20The%20new%20decoder%20is%20designed%20to%20interact%20with%0Aboth%20the%20original%20encoder%20and%20decoder.%20In%20this%20way%2C%20it%20not%20only%20reuses%20feature%0Apresentation%20from%20corresponding%20layers%20in%20the%20encoder%20but%20also%20interacts%20with%0Athe%20original%20decoder%20to%20corporately%20give%20more%20accurate%20registration%20results.%0AThe%20new%20architecture%20is%20concise%20yet%20generalized%2C%20with%20only%20one%20encoder%20and%20two%0Adecoders%20forming%20a%20%60%60Tetrahedron%27%27%20structure%2C%20thereby%20dubbed%20Tetrahedron-Net.%0AThree%20instantiations%20of%20Tetrahedron-Net%20are%20further%20constructed%20regarding%20the%0Adifferent%20structures%20of%20the%20appended%20decoder.%20Our%20extensive%20experiments%20prove%0Athat%20superior%20performance%20can%20be%20obtained%20on%20several%20representative%20benchmarks%0Aof%20medical%20image%20registration.%20Finally%2C%20such%20a%20%60%60Tetrahedron%27%27%20design%20can%20also%0Abe%20easily%20integrated%20into%20popular%20U-Net-like%20architectures%20including%0AVoxelMorph%2C%20ViT-V-Net%2C%20and%20TransMorph%2C%20leading%20to%20consistent%20performance%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTetrahedron-Net%2520for%2520Medical%2520Image%2520Registration%26entry.906535625%3DJinhai%2520Xiang%2520and%2520Shuai%2520Guo%2520and%2520Qianru%2520Han%2520and%2520Dantong%2520Shi%2520and%2520Xinwei%2520He%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Medical%2520image%2520registration%2520plays%2520a%2520vital%2520role%2520in%2520medical%2520image%2520processing.%250AExtracting%2520expressive%2520representations%2520for%2520medical%2520images%2520is%2520crucial%2520for%250Aimproving%2520the%2520registration%2520quality.%2520One%2520common%2520practice%2520for%2520this%2520end%2520is%250Aconstructing%2520a%2520convolutional%2520backbone%2520to%2520enable%2520interactions%2520with%2520skip%250Aconnections%2520among%2520feature%2520extraction%2520layers.%2520The%2520de%2520facto%2520structure%252C%2520U-Net-like%250Anetworks%252C%2520has%2520attempted%2520to%2520design%2520skip%2520connections%2520such%2520as%2520nested%2520or%2520full-scale%250Aones%2520to%2520connect%2520one%2520single%2520encoder%2520and%2520one%2520single%2520decoder%2520to%2520improve%2520its%250Arepresentation%2520capacity.%2520Despite%2520being%2520effective%252C%2520it%2520still%2520does%2520not%2520fully%250Aexplore%2520interactions%2520with%2520a%2520single%2520encoder%2520and%2520decoder%2520architectures.%2520In%2520this%250Apaper%252C%2520we%2520embrace%2520this%2520observation%2520and%2520introduce%2520a%2520simple%2520yet%2520effective%250Aalternative%2520strategy%2520to%2520enhance%2520the%2520representations%2520for%2520registrations%2520by%250Aappending%2520one%2520additional%2520decoder.%2520The%2520new%2520decoder%2520is%2520designed%2520to%2520interact%2520with%250Aboth%2520the%2520original%2520encoder%2520and%2520decoder.%2520In%2520this%2520way%252C%2520it%2520not%2520only%2520reuses%2520feature%250Apresentation%2520from%2520corresponding%2520layers%2520in%2520the%2520encoder%2520but%2520also%2520interacts%2520with%250Athe%2520original%2520decoder%2520to%2520corporately%2520give%2520more%2520accurate%2520registration%2520results.%250AThe%2520new%2520architecture%2520is%2520concise%2520yet%2520generalized%252C%2520with%2520only%2520one%2520encoder%2520and%2520two%250Adecoders%2520forming%2520a%2520%2560%2560Tetrahedron%2527%2527%2520structure%252C%2520thereby%2520dubbed%2520Tetrahedron-Net.%250AThree%2520instantiations%2520of%2520Tetrahedron-Net%2520are%2520further%2520constructed%2520regarding%2520the%250Adifferent%2520structures%2520of%2520the%2520appended%2520decoder.%2520Our%2520extensive%2520experiments%2520prove%250Athat%2520superior%2520performance%2520can%2520be%2520obtained%2520on%2520several%2520representative%2520benchmarks%250Aof%2520medical%2520image%2520registration.%2520Finally%252C%2520such%2520a%2520%2560%2560Tetrahedron%2527%2527%2520design%2520can%2520also%250Abe%2520easily%2520integrated%2520into%2520popular%2520U-Net-like%2520architectures%2520including%250AVoxelMorph%252C%2520ViT-V-Net%252C%2520and%2520TransMorph%252C%2520leading%2520to%2520consistent%2520performance%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tetrahedron-Net%20for%20Medical%20Image%20Registration&entry.906535625=Jinhai%20Xiang%20and%20Shuai%20Guo%20and%20Qianru%20Han%20and%20Dantong%20Shi%20and%20Xinwei%20He%20and%20Xiang%20Bai&entry.1292438233=%20%20Medical%20image%20registration%20plays%20a%20vital%20role%20in%20medical%20image%20processing.%0AExtracting%20expressive%20representations%20for%20medical%20images%20is%20crucial%20for%0Aimproving%20the%20registration%20quality.%20One%20common%20practice%20for%20this%20end%20is%0Aconstructing%20a%20convolutional%20backbone%20to%20enable%20interactions%20with%20skip%0Aconnections%20among%20feature%20extraction%20layers.%20The%20de%20facto%20structure%2C%20U-Net-like%0Anetworks%2C%20has%20attempted%20to%20design%20skip%20connections%20such%20as%20nested%20or%20full-scale%0Aones%20to%20connect%20one%20single%20encoder%20and%20one%20single%20decoder%20to%20improve%20its%0Arepresentation%20capacity.%20Despite%20being%20effective%2C%20it%20still%20does%20not%20fully%0Aexplore%20interactions%20with%20a%20single%20encoder%20and%20decoder%20architectures.%20In%20this%0Apaper%2C%20we%20embrace%20this%20observation%20and%20introduce%20a%20simple%20yet%20effective%0Aalternative%20strategy%20to%20enhance%20the%20representations%20for%20registrations%20by%0Aappending%20one%20additional%20decoder.%20The%20new%20decoder%20is%20designed%20to%20interact%20with%0Aboth%20the%20original%20encoder%20and%20decoder.%20In%20this%20way%2C%20it%20not%20only%20reuses%20feature%0Apresentation%20from%20corresponding%20layers%20in%20the%20encoder%20but%20also%20interacts%20with%0Athe%20original%20decoder%20to%20corporately%20give%20more%20accurate%20registration%20results.%0AThe%20new%20architecture%20is%20concise%20yet%20generalized%2C%20with%20only%20one%20encoder%20and%20two%0Adecoders%20forming%20a%20%60%60Tetrahedron%27%27%20structure%2C%20thereby%20dubbed%20Tetrahedron-Net.%0AThree%20instantiations%20of%20Tetrahedron-Net%20are%20further%20constructed%20regarding%20the%0Adifferent%20structures%20of%20the%20appended%20decoder.%20Our%20extensive%20experiments%20prove%0Athat%20superior%20performance%20can%20be%20obtained%20on%20several%20representative%20benchmarks%0Aof%20medical%20image%20registration.%20Finally%2C%20such%20a%20%60%60Tetrahedron%27%27%20design%20can%20also%0Abe%20easily%20integrated%20into%20popular%20U-Net-like%20architectures%20including%0AVoxelMorph%2C%20ViT-V-Net%2C%20and%20TransMorph%2C%20leading%20to%20consistent%20performance%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04380v1&entry.124074799=Read"},
{"title": "Hard-Negative Sampling for Contrastive Learning: Optimal Representation\n  Geometry and Neural- vs Dimensional-Collapse", "author": "Ruijie Jiang and Thuan Nguyen and Shuchin Aeron and Prakash Ishwar", "abstract": "  For a widely-studied data model and general loss and sample-hardening\nfunctions we prove that the losses of Supervised Contrastive Learning (SCL),\nHard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by\nrepresentations that exhibit Neural-Collapse (NC), i.e., the class means form\nan Equiangular Tight Frame (ETF) and data from the same class are mapped to the\nsame representation. We also prove that for any representation mapping, the\nHSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and\nUCL losses. In contrast to existing literature, our theoretical results for SCL\ndo not require class-conditional independence of augmented views and work for a\ngeneral loss function class that includes the widely used InfoNCE loss\nfunction. Moreover, our proofs are simpler, compact, and transparent. Similar\nto existing literature, our theoretical claims also hold for the practical\nscenario where batching is used for optimization. We empirically demonstrate,\nfor the first time, that Adam optimization (with batching) of HSCL and HUCL\nlosses with random initialization and suitable hardness levels can indeed\nconverge to the NC-geometry if we incorporate unit-ball or unit-sphere feature\nnormalization. Without incorporating hard-negatives or feature normalization,\nhowever, the representations learned via Adam suffer from Dimensional-Collapse\n(DC) and fail to attain the NC-geometry. These results exemplify the role of\nhard-negative sampling in contrastive representation learning and we conclude\nwith several open theoretical problems for future work. The code can be found\nat https://github.com/rjiang03/HCL/tree/main\n", "link": "http://arxiv.org/abs/2311.05139v3", "date": "2025-05-07", "relevancy": 2.6331, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5259}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hard-Negative%20Sampling%20for%20Contrastive%20Learning%3A%20Optimal%20Representation%0A%20%20Geometry%20and%20Neural-%20vs%20Dimensional-Collapse&body=Title%3A%20Hard-Negative%20Sampling%20for%20Contrastive%20Learning%3A%20Optimal%20Representation%0A%20%20Geometry%20and%20Neural-%20vs%20Dimensional-Collapse%0AAuthor%3A%20Ruijie%20Jiang%20and%20Thuan%20Nguyen%20and%20Shuchin%20Aeron%20and%20Prakash%20Ishwar%0AAbstract%3A%20%20%20For%20a%20widely-studied%20data%20model%20and%20general%20loss%20and%20sample-hardening%0Afunctions%20we%20prove%20that%20the%20losses%20of%20Supervised%20Contrastive%20Learning%20%28SCL%29%2C%0AHard-SCL%20%28HSCL%29%2C%20and%20Unsupervised%20Contrastive%20Learning%20%28UCL%29%20are%20minimized%20by%0Arepresentations%20that%20exhibit%20Neural-Collapse%20%28NC%29%2C%20i.e.%2C%20the%20class%20means%20form%0Aan%20Equiangular%20Tight%20Frame%20%28ETF%29%20and%20data%20from%20the%20same%20class%20are%20mapped%20to%20the%0Asame%20representation.%20We%20also%20prove%20that%20for%20any%20representation%20mapping%2C%20the%0AHSCL%20and%20Hard-UCL%20%28HUCL%29%20losses%20are%20lower%20bounded%20by%20the%20corresponding%20SCL%20and%0AUCL%20losses.%20In%20contrast%20to%20existing%20literature%2C%20our%20theoretical%20results%20for%20SCL%0Ado%20not%20require%20class-conditional%20independence%20of%20augmented%20views%20and%20work%20for%20a%0Ageneral%20loss%20function%20class%20that%20includes%20the%20widely%20used%20InfoNCE%20loss%0Afunction.%20Moreover%2C%20our%20proofs%20are%20simpler%2C%20compact%2C%20and%20transparent.%20Similar%0Ato%20existing%20literature%2C%20our%20theoretical%20claims%20also%20hold%20for%20the%20practical%0Ascenario%20where%20batching%20is%20used%20for%20optimization.%20We%20empirically%20demonstrate%2C%0Afor%20the%20first%20time%2C%20that%20Adam%20optimization%20%28with%20batching%29%20of%20HSCL%20and%20HUCL%0Alosses%20with%20random%20initialization%20and%20suitable%20hardness%20levels%20can%20indeed%0Aconverge%20to%20the%20NC-geometry%20if%20we%20incorporate%20unit-ball%20or%20unit-sphere%20feature%0Anormalization.%20Without%20incorporating%20hard-negatives%20or%20feature%20normalization%2C%0Ahowever%2C%20the%20representations%20learned%20via%20Adam%20suffer%20from%20Dimensional-Collapse%0A%28DC%29%20and%20fail%20to%20attain%20the%20NC-geometry.%20These%20results%20exemplify%20the%20role%20of%0Ahard-negative%20sampling%20in%20contrastive%20representation%20learning%20and%20we%20conclude%0Awith%20several%20open%20theoretical%20problems%20for%20future%20work.%20The%20code%20can%20be%20found%0Aat%20https%3A//github.com/rjiang03/HCL/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05139v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHard-Negative%2520Sampling%2520for%2520Contrastive%2520Learning%253A%2520Optimal%2520Representation%250A%2520%2520Geometry%2520and%2520Neural-%2520vs%2520Dimensional-Collapse%26entry.906535625%3DRuijie%2520Jiang%2520and%2520Thuan%2520Nguyen%2520and%2520Shuchin%2520Aeron%2520and%2520Prakash%2520Ishwar%26entry.1292438233%3D%2520%2520For%2520a%2520widely-studied%2520data%2520model%2520and%2520general%2520loss%2520and%2520sample-hardening%250Afunctions%2520we%2520prove%2520that%2520the%2520losses%2520of%2520Supervised%2520Contrastive%2520Learning%2520%2528SCL%2529%252C%250AHard-SCL%2520%2528HSCL%2529%252C%2520and%2520Unsupervised%2520Contrastive%2520Learning%2520%2528UCL%2529%2520are%2520minimized%2520by%250Arepresentations%2520that%2520exhibit%2520Neural-Collapse%2520%2528NC%2529%252C%2520i.e.%252C%2520the%2520class%2520means%2520form%250Aan%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520and%2520data%2520from%2520the%2520same%2520class%2520are%2520mapped%2520to%2520the%250Asame%2520representation.%2520We%2520also%2520prove%2520that%2520for%2520any%2520representation%2520mapping%252C%2520the%250AHSCL%2520and%2520Hard-UCL%2520%2528HUCL%2529%2520losses%2520are%2520lower%2520bounded%2520by%2520the%2520corresponding%2520SCL%2520and%250AUCL%2520losses.%2520In%2520contrast%2520to%2520existing%2520literature%252C%2520our%2520theoretical%2520results%2520for%2520SCL%250Ado%2520not%2520require%2520class-conditional%2520independence%2520of%2520augmented%2520views%2520and%2520work%2520for%2520a%250Ageneral%2520loss%2520function%2520class%2520that%2520includes%2520the%2520widely%2520used%2520InfoNCE%2520loss%250Afunction.%2520Moreover%252C%2520our%2520proofs%2520are%2520simpler%252C%2520compact%252C%2520and%2520transparent.%2520Similar%250Ato%2520existing%2520literature%252C%2520our%2520theoretical%2520claims%2520also%2520hold%2520for%2520the%2520practical%250Ascenario%2520where%2520batching%2520is%2520used%2520for%2520optimization.%2520We%2520empirically%2520demonstrate%252C%250Afor%2520the%2520first%2520time%252C%2520that%2520Adam%2520optimization%2520%2528with%2520batching%2529%2520of%2520HSCL%2520and%2520HUCL%250Alosses%2520with%2520random%2520initialization%2520and%2520suitable%2520hardness%2520levels%2520can%2520indeed%250Aconverge%2520to%2520the%2520NC-geometry%2520if%2520we%2520incorporate%2520unit-ball%2520or%2520unit-sphere%2520feature%250Anormalization.%2520Without%2520incorporating%2520hard-negatives%2520or%2520feature%2520normalization%252C%250Ahowever%252C%2520the%2520representations%2520learned%2520via%2520Adam%2520suffer%2520from%2520Dimensional-Collapse%250A%2528DC%2529%2520and%2520fail%2520to%2520attain%2520the%2520NC-geometry.%2520These%2520results%2520exemplify%2520the%2520role%2520of%250Ahard-negative%2520sampling%2520in%2520contrastive%2520representation%2520learning%2520and%2520we%2520conclude%250Awith%2520several%2520open%2520theoretical%2520problems%2520for%2520future%2520work.%2520The%2520code%2520can%2520be%2520found%250Aat%2520https%253A//github.com/rjiang03/HCL/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05139v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hard-Negative%20Sampling%20for%20Contrastive%20Learning%3A%20Optimal%20Representation%0A%20%20Geometry%20and%20Neural-%20vs%20Dimensional-Collapse&entry.906535625=Ruijie%20Jiang%20and%20Thuan%20Nguyen%20and%20Shuchin%20Aeron%20and%20Prakash%20Ishwar&entry.1292438233=%20%20For%20a%20widely-studied%20data%20model%20and%20general%20loss%20and%20sample-hardening%0Afunctions%20we%20prove%20that%20the%20losses%20of%20Supervised%20Contrastive%20Learning%20%28SCL%29%2C%0AHard-SCL%20%28HSCL%29%2C%20and%20Unsupervised%20Contrastive%20Learning%20%28UCL%29%20are%20minimized%20by%0Arepresentations%20that%20exhibit%20Neural-Collapse%20%28NC%29%2C%20i.e.%2C%20the%20class%20means%20form%0Aan%20Equiangular%20Tight%20Frame%20%28ETF%29%20and%20data%20from%20the%20same%20class%20are%20mapped%20to%20the%0Asame%20representation.%20We%20also%20prove%20that%20for%20any%20representation%20mapping%2C%20the%0AHSCL%20and%20Hard-UCL%20%28HUCL%29%20losses%20are%20lower%20bounded%20by%20the%20corresponding%20SCL%20and%0AUCL%20losses.%20In%20contrast%20to%20existing%20literature%2C%20our%20theoretical%20results%20for%20SCL%0Ado%20not%20require%20class-conditional%20independence%20of%20augmented%20views%20and%20work%20for%20a%0Ageneral%20loss%20function%20class%20that%20includes%20the%20widely%20used%20InfoNCE%20loss%0Afunction.%20Moreover%2C%20our%20proofs%20are%20simpler%2C%20compact%2C%20and%20transparent.%20Similar%0Ato%20existing%20literature%2C%20our%20theoretical%20claims%20also%20hold%20for%20the%20practical%0Ascenario%20where%20batching%20is%20used%20for%20optimization.%20We%20empirically%20demonstrate%2C%0Afor%20the%20first%20time%2C%20that%20Adam%20optimization%20%28with%20batching%29%20of%20HSCL%20and%20HUCL%0Alosses%20with%20random%20initialization%20and%20suitable%20hardness%20levels%20can%20indeed%0Aconverge%20to%20the%20NC-geometry%20if%20we%20incorporate%20unit-ball%20or%20unit-sphere%20feature%0Anormalization.%20Without%20incorporating%20hard-negatives%20or%20feature%20normalization%2C%0Ahowever%2C%20the%20representations%20learned%20via%20Adam%20suffer%20from%20Dimensional-Collapse%0A%28DC%29%20and%20fail%20to%20attain%20the%20NC-geometry.%20These%20results%20exemplify%20the%20role%20of%0Ahard-negative%20sampling%20in%20contrastive%20representation%20learning%20and%20we%20conclude%0Awith%20several%20open%20theoretical%20problems%20for%20future%20work.%20The%20code%20can%20be%20found%0Aat%20https%3A//github.com/rjiang03/HCL/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05139v3&entry.124074799=Read"},
{"title": "On Path to Multimodal Generalist: General-Level and General-Bench", "author": "Hao Fei and Yuan Zhou and Juncheng Li and Xiangtai Li and Qingshan Xu and Bobo Li and Shengqiong Wu and Yaoting Wang and Junbao Zhou and Jiahao Meng and Qingyu Shi and Zhiyuan Zhou and Liangtao Shi and Minghe Gao and Daoan Zhang and Zhiqi Ge and Weiming Wu and Siliang Tang and Kaihang Pan and Yaobo Ye and Haobo Yuan and Tao Zhang and Tianjie Ju and Zixiang Meng and Shilin Xu and Liyu Jia and Wentao Hu and Meng Luo and Jiebo Luo and Tat-Seng Chua and Shuicheng Yan and Hanwang Zhang", "abstract": "  The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/\n", "link": "http://arxiv.org/abs/2505.04620v1", "date": "2025-05-07", "relevancy": 2.6144, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Path%20to%20Multimodal%20Generalist%3A%20General-Level%20and%20General-Bench&body=Title%3A%20On%20Path%20to%20Multimodal%20Generalist%3A%20General-Level%20and%20General-Bench%0AAuthor%3A%20Hao%20Fei%20and%20Yuan%20Zhou%20and%20Juncheng%20Li%20and%20Xiangtai%20Li%20and%20Qingshan%20Xu%20and%20Bobo%20Li%20and%20Shengqiong%20Wu%20and%20Yaoting%20Wang%20and%20Junbao%20Zhou%20and%20Jiahao%20Meng%20and%20Qingyu%20Shi%20and%20Zhiyuan%20Zhou%20and%20Liangtao%20Shi%20and%20Minghe%20Gao%20and%20Daoan%20Zhang%20and%20Zhiqi%20Ge%20and%20Weiming%20Wu%20and%20Siliang%20Tang%20and%20Kaihang%20Pan%20and%20Yaobo%20Ye%20and%20Haobo%20Yuan%20and%20Tao%20Zhang%20and%20Tianjie%20Ju%20and%20Zixiang%20Meng%20and%20Shilin%20Xu%20and%20Liyu%20Jia%20and%20Wentao%20Hu%20and%20Meng%20Luo%20and%20Jiebo%20Luo%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20The%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20is%20currently%20experiencing%20rapid%0Agrowth%2C%20driven%20by%20the%20advanced%20capabilities%20of%20LLMs.%20Unlike%20earlier%0Aspecialists%2C%20existing%20MLLMs%20are%20evolving%20towards%20a%20Multimodal%20Generalist%0Aparadigm.%20Initially%20limited%20to%20understanding%20multiple%20modalities%2C%20these%20models%0Ahave%20advanced%20to%20not%20only%20comprehend%20but%20also%20generate%20across%20modalities.%20Their%0Acapabilities%20have%20expanded%20from%20coarse-grained%20to%20fine-grained%20multimodal%0Aunderstanding%20and%20from%20supporting%20limited%20modalities%20to%20arbitrary%20ones.%20While%0Amany%20benchmarks%20exist%20to%20assess%20MLLMs%2C%20a%20critical%20question%20arises%3A%20Can%20we%0Asimply%20assume%20that%20higher%20performance%20across%20tasks%20indicates%20a%20stronger%20MLLM%0Acapability%2C%20bringing%20us%20closer%20to%20human-level%20AI%3F%20We%20argue%20that%20the%20answer%20is%0Anot%20as%20straightforward%20as%20it%20seems.%20This%20project%20introduces%20General-Level%2C%20an%0Aevaluation%20framework%20that%20defines%205-scale%20levels%20of%20MLLM%20performance%20and%0Agenerality%2C%20offering%20a%20methodology%20to%20compare%20MLLMs%20and%20gauge%20the%20progress%20of%0Aexisting%20systems%20towards%20more%20robust%20multimodal%20generalists%20and%2C%20ultimately%2C%0Atowards%20AGI.%20At%20the%20core%20of%20the%20framework%20is%20the%20concept%20of%20Synergy%2C%20which%0Ameasures%20whether%20models%20maintain%20consistent%20capabilities%20across%20comprehension%0Aand%20generation%2C%20and%20across%20multiple%20modalities.%20To%20support%20this%20evaluation%2C%20we%0Apresent%20General-Bench%2C%20which%20encompasses%20a%20broader%20spectrum%20of%20skills%2C%0Amodalities%2C%20formats%2C%20and%20capabilities%2C%20including%20over%20700%20tasks%20and%20325%2C800%0Ainstances.%20The%20evaluation%20results%20that%20involve%20over%20100%20existing%0Astate-of-the-art%20MLLMs%20uncover%20the%20capability%20rankings%20of%20generalists%2C%0Ahighlighting%20the%20challenges%20in%20reaching%20genuine%20AI.%20We%20expect%20this%20project%20to%0Apave%20the%20way%20for%20future%20research%20on%20next-generation%20multimodal%20foundation%0Amodels%2C%20providing%20a%20robust%20infrastructure%20to%20accelerate%20the%20realization%20of%20AGI.%0AProject%20page%3A%20https%3A//generalist.top/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Path%2520to%2520Multimodal%2520Generalist%253A%2520General-Level%2520and%2520General-Bench%26entry.906535625%3DHao%2520Fei%2520and%2520Yuan%2520Zhou%2520and%2520Juncheng%2520Li%2520and%2520Xiangtai%2520Li%2520and%2520Qingshan%2520Xu%2520and%2520Bobo%2520Li%2520and%2520Shengqiong%2520Wu%2520and%2520Yaoting%2520Wang%2520and%2520Junbao%2520Zhou%2520and%2520Jiahao%2520Meng%2520and%2520Qingyu%2520Shi%2520and%2520Zhiyuan%2520Zhou%2520and%2520Liangtao%2520Shi%2520and%2520Minghe%2520Gao%2520and%2520Daoan%2520Zhang%2520and%2520Zhiqi%2520Ge%2520and%2520Weiming%2520Wu%2520and%2520Siliang%2520Tang%2520and%2520Kaihang%2520Pan%2520and%2520Yaobo%2520Ye%2520and%2520Haobo%2520Yuan%2520and%2520Tao%2520Zhang%2520and%2520Tianjie%2520Ju%2520and%2520Zixiang%2520Meng%2520and%2520Shilin%2520Xu%2520and%2520Liyu%2520Jia%2520and%2520Wentao%2520Hu%2520and%2520Meng%2520Luo%2520and%2520Jiebo%2520Luo%2520and%2520Tat-Seng%2520Chua%2520and%2520Shuicheng%2520Yan%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520is%2520currently%2520experiencing%2520rapid%250Agrowth%252C%2520driven%2520by%2520the%2520advanced%2520capabilities%2520of%2520LLMs.%2520Unlike%2520earlier%250Aspecialists%252C%2520existing%2520MLLMs%2520are%2520evolving%2520towards%2520a%2520Multimodal%2520Generalist%250Aparadigm.%2520Initially%2520limited%2520to%2520understanding%2520multiple%2520modalities%252C%2520these%2520models%250Ahave%2520advanced%2520to%2520not%2520only%2520comprehend%2520but%2520also%2520generate%2520across%2520modalities.%2520Their%250Acapabilities%2520have%2520expanded%2520from%2520coarse-grained%2520to%2520fine-grained%2520multimodal%250Aunderstanding%2520and%2520from%2520supporting%2520limited%2520modalities%2520to%2520arbitrary%2520ones.%2520While%250Amany%2520benchmarks%2520exist%2520to%2520assess%2520MLLMs%252C%2520a%2520critical%2520question%2520arises%253A%2520Can%2520we%250Asimply%2520assume%2520that%2520higher%2520performance%2520across%2520tasks%2520indicates%2520a%2520stronger%2520MLLM%250Acapability%252C%2520bringing%2520us%2520closer%2520to%2520human-level%2520AI%253F%2520We%2520argue%2520that%2520the%2520answer%2520is%250Anot%2520as%2520straightforward%2520as%2520it%2520seems.%2520This%2520project%2520introduces%2520General-Level%252C%2520an%250Aevaluation%2520framework%2520that%2520defines%25205-scale%2520levels%2520of%2520MLLM%2520performance%2520and%250Agenerality%252C%2520offering%2520a%2520methodology%2520to%2520compare%2520MLLMs%2520and%2520gauge%2520the%2520progress%2520of%250Aexisting%2520systems%2520towards%2520more%2520robust%2520multimodal%2520generalists%2520and%252C%2520ultimately%252C%250Atowards%2520AGI.%2520At%2520the%2520core%2520of%2520the%2520framework%2520is%2520the%2520concept%2520of%2520Synergy%252C%2520which%250Ameasures%2520whether%2520models%2520maintain%2520consistent%2520capabilities%2520across%2520comprehension%250Aand%2520generation%252C%2520and%2520across%2520multiple%2520modalities.%2520To%2520support%2520this%2520evaluation%252C%2520we%250Apresent%2520General-Bench%252C%2520which%2520encompasses%2520a%2520broader%2520spectrum%2520of%2520skills%252C%250Amodalities%252C%2520formats%252C%2520and%2520capabilities%252C%2520including%2520over%2520700%2520tasks%2520and%2520325%252C800%250Ainstances.%2520The%2520evaluation%2520results%2520that%2520involve%2520over%2520100%2520existing%250Astate-of-the-art%2520MLLMs%2520uncover%2520the%2520capability%2520rankings%2520of%2520generalists%252C%250Ahighlighting%2520the%2520challenges%2520in%2520reaching%2520genuine%2520AI.%2520We%2520expect%2520this%2520project%2520to%250Apave%2520the%2520way%2520for%2520future%2520research%2520on%2520next-generation%2520multimodal%2520foundation%250Amodels%252C%2520providing%2520a%2520robust%2520infrastructure%2520to%2520accelerate%2520the%2520realization%2520of%2520AGI.%250AProject%2520page%253A%2520https%253A//generalist.top/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Path%20to%20Multimodal%20Generalist%3A%20General-Level%20and%20General-Bench&entry.906535625=Hao%20Fei%20and%20Yuan%20Zhou%20and%20Juncheng%20Li%20and%20Xiangtai%20Li%20and%20Qingshan%20Xu%20and%20Bobo%20Li%20and%20Shengqiong%20Wu%20and%20Yaoting%20Wang%20and%20Junbao%20Zhou%20and%20Jiahao%20Meng%20and%20Qingyu%20Shi%20and%20Zhiyuan%20Zhou%20and%20Liangtao%20Shi%20and%20Minghe%20Gao%20and%20Daoan%20Zhang%20and%20Zhiqi%20Ge%20and%20Weiming%20Wu%20and%20Siliang%20Tang%20and%20Kaihang%20Pan%20and%20Yaobo%20Ye%20and%20Haobo%20Yuan%20and%20Tao%20Zhang%20and%20Tianjie%20Ju%20and%20Zixiang%20Meng%20and%20Shilin%20Xu%20and%20Liyu%20Jia%20and%20Wentao%20Hu%20and%20Meng%20Luo%20and%20Jiebo%20Luo%20and%20Tat-Seng%20Chua%20and%20Shuicheng%20Yan%20and%20Hanwang%20Zhang&entry.1292438233=%20%20The%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20is%20currently%20experiencing%20rapid%0Agrowth%2C%20driven%20by%20the%20advanced%20capabilities%20of%20LLMs.%20Unlike%20earlier%0Aspecialists%2C%20existing%20MLLMs%20are%20evolving%20towards%20a%20Multimodal%20Generalist%0Aparadigm.%20Initially%20limited%20to%20understanding%20multiple%20modalities%2C%20these%20models%0Ahave%20advanced%20to%20not%20only%20comprehend%20but%20also%20generate%20across%20modalities.%20Their%0Acapabilities%20have%20expanded%20from%20coarse-grained%20to%20fine-grained%20multimodal%0Aunderstanding%20and%20from%20supporting%20limited%20modalities%20to%20arbitrary%20ones.%20While%0Amany%20benchmarks%20exist%20to%20assess%20MLLMs%2C%20a%20critical%20question%20arises%3A%20Can%20we%0Asimply%20assume%20that%20higher%20performance%20across%20tasks%20indicates%20a%20stronger%20MLLM%0Acapability%2C%20bringing%20us%20closer%20to%20human-level%20AI%3F%20We%20argue%20that%20the%20answer%20is%0Anot%20as%20straightforward%20as%20it%20seems.%20This%20project%20introduces%20General-Level%2C%20an%0Aevaluation%20framework%20that%20defines%205-scale%20levels%20of%20MLLM%20performance%20and%0Agenerality%2C%20offering%20a%20methodology%20to%20compare%20MLLMs%20and%20gauge%20the%20progress%20of%0Aexisting%20systems%20towards%20more%20robust%20multimodal%20generalists%20and%2C%20ultimately%2C%0Atowards%20AGI.%20At%20the%20core%20of%20the%20framework%20is%20the%20concept%20of%20Synergy%2C%20which%0Ameasures%20whether%20models%20maintain%20consistent%20capabilities%20across%20comprehension%0Aand%20generation%2C%20and%20across%20multiple%20modalities.%20To%20support%20this%20evaluation%2C%20we%0Apresent%20General-Bench%2C%20which%20encompasses%20a%20broader%20spectrum%20of%20skills%2C%0Amodalities%2C%20formats%2C%20and%20capabilities%2C%20including%20over%20700%20tasks%20and%20325%2C800%0Ainstances.%20The%20evaluation%20results%20that%20involve%20over%20100%20existing%0Astate-of-the-art%20MLLMs%20uncover%20the%20capability%20rankings%20of%20generalists%2C%0Ahighlighting%20the%20challenges%20in%20reaching%20genuine%20AI.%20We%20expect%20this%20project%20to%0Apave%20the%20way%20for%20future%20research%20on%20next-generation%20multimodal%20foundation%0Amodels%2C%20providing%20a%20robust%20infrastructure%20to%20accelerate%20the%20realization%20of%20AGI.%0AProject%20page%3A%20https%3A//generalist.top/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04620v1&entry.124074799=Read"},
{"title": "Enhancing Target-unspecific Tasks through a Features Matrix", "author": "Fangming Cui and Yonggang Zhang and Xuan Wang and Xinmei Tian and Jun Yu", "abstract": "  Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2505.03414v2", "date": "2025-05-07", "relevancy": 2.6085, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Target-unspecific%20Tasks%20through%20a%20Features%20Matrix&body=Title%3A%20Enhancing%20Target-unspecific%20Tasks%20through%20a%20Features%20Matrix%0AAuthor%3A%20Fangming%20Cui%20and%20Yonggang%20Zhang%20and%20Xuan%20Wang%20and%20Xinmei%20Tian%20and%20Jun%20Yu%0AAbstract%3A%20%20%20Recent%20developments%20in%20prompt%20learning%20of%20large%20vision-language%20models%20have%0Asignificantly%20improved%20performance%20in%20target-specific%20tasks.%20However%2C%20these%0Aprompt%20optimizing%20methods%20often%20struggle%20to%20tackle%20the%20target-unspecific%20or%0Ageneralizable%20tasks%20effectively.%20It%20may%20be%20attributed%20to%20the%20fact%20that%0Aoverfitting%20training%20causes%20the%20model%20to%20forget%20its%20general%20knowledge%20having%0Astrong%20promotion%20on%20target-unspecific%20tasks.%20To%20alleviate%20this%20issue%2C%20we%0Apropose%20a%20novel%20Features%20Matrix%20%28FM%29%20regularization%20approach%20designed%20to%0Aenhance%20these%20models%20on%20target-unspecific%20tasks.%20Our%20method%20extracts%20and%0Aleverages%20general%20knowledge%2C%20shaping%20a%20Features%20Matrix%20%28FM%29.%20Specifically%2C%20the%0AFM%20captures%20the%20semantics%20of%20diverse%20inputs%20from%20a%20deep%20and%20fine%20perspective%2C%0Apreserving%20essential%20general%20knowledge%2C%20which%20mitigates%20the%20risk%20of%0Aoverfitting.%20Representative%20evaluations%20demonstrate%20that%3A%201%29%20the%20FM%20is%0Acompatible%20with%20existing%20frameworks%20as%20a%20generic%20and%20flexible%20module%2C%20and%202%29%0Athe%20FM%20significantly%20showcases%20its%20effectiveness%20in%20enhancing%20target-unspecific%0Atasks%2C%20achieving%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03414v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Target-unspecific%2520Tasks%2520through%2520a%2520Features%2520Matrix%26entry.906535625%3DFangming%2520Cui%2520and%2520Yonggang%2520Zhang%2520and%2520Xuan%2520Wang%2520and%2520Xinmei%2520Tian%2520and%2520Jun%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520developments%2520in%2520prompt%2520learning%2520of%2520large%2520vision-language%2520models%2520have%250Asignificantly%2520improved%2520performance%2520in%2520target-specific%2520tasks.%2520However%252C%2520these%250Aprompt%2520optimizing%2520methods%2520often%2520struggle%2520to%2520tackle%2520the%2520target-unspecific%2520or%250Ageneralizable%2520tasks%2520effectively.%2520It%2520may%2520be%2520attributed%2520to%2520the%2520fact%2520that%250Aoverfitting%2520training%2520causes%2520the%2520model%2520to%2520forget%2520its%2520general%2520knowledge%2520having%250Astrong%2520promotion%2520on%2520target-unspecific%2520tasks.%2520To%2520alleviate%2520this%2520issue%252C%2520we%250Apropose%2520a%2520novel%2520Features%2520Matrix%2520%2528FM%2529%2520regularization%2520approach%2520designed%2520to%250Aenhance%2520these%2520models%2520on%2520target-unspecific%2520tasks.%2520Our%2520method%2520extracts%2520and%250Aleverages%2520general%2520knowledge%252C%2520shaping%2520a%2520Features%2520Matrix%2520%2528FM%2529.%2520Specifically%252C%2520the%250AFM%2520captures%2520the%2520semantics%2520of%2520diverse%2520inputs%2520from%2520a%2520deep%2520and%2520fine%2520perspective%252C%250Apreserving%2520essential%2520general%2520knowledge%252C%2520which%2520mitigates%2520the%2520risk%2520of%250Aoverfitting.%2520Representative%2520evaluations%2520demonstrate%2520that%253A%25201%2529%2520the%2520FM%2520is%250Acompatible%2520with%2520existing%2520frameworks%2520as%2520a%2520generic%2520and%2520flexible%2520module%252C%2520and%25202%2529%250Athe%2520FM%2520significantly%2520showcases%2520its%2520effectiveness%2520in%2520enhancing%2520target-unspecific%250Atasks%252C%2520achieving%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03414v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Target-unspecific%20Tasks%20through%20a%20Features%20Matrix&entry.906535625=Fangming%20Cui%20and%20Yonggang%20Zhang%20and%20Xuan%20Wang%20and%20Xinmei%20Tian%20and%20Jun%20Yu&entry.1292438233=%20%20Recent%20developments%20in%20prompt%20learning%20of%20large%20vision-language%20models%20have%0Asignificantly%20improved%20performance%20in%20target-specific%20tasks.%20However%2C%20these%0Aprompt%20optimizing%20methods%20often%20struggle%20to%20tackle%20the%20target-unspecific%20or%0Ageneralizable%20tasks%20effectively.%20It%20may%20be%20attributed%20to%20the%20fact%20that%0Aoverfitting%20training%20causes%20the%20model%20to%20forget%20its%20general%20knowledge%20having%0Astrong%20promotion%20on%20target-unspecific%20tasks.%20To%20alleviate%20this%20issue%2C%20we%0Apropose%20a%20novel%20Features%20Matrix%20%28FM%29%20regularization%20approach%20designed%20to%0Aenhance%20these%20models%20on%20target-unspecific%20tasks.%20Our%20method%20extracts%20and%0Aleverages%20general%20knowledge%2C%20shaping%20a%20Features%20Matrix%20%28FM%29.%20Specifically%2C%20the%0AFM%20captures%20the%20semantics%20of%20diverse%20inputs%20from%20a%20deep%20and%20fine%20perspective%2C%0Apreserving%20essential%20general%20knowledge%2C%20which%20mitigates%20the%20risk%20of%0Aoverfitting.%20Representative%20evaluations%20demonstrate%20that%3A%201%29%20the%20FM%20is%0Acompatible%20with%20existing%20frameworks%20as%20a%20generic%20and%20flexible%20module%2C%20and%202%29%0Athe%20FM%20significantly%20showcases%20its%20effectiveness%20in%20enhancing%20target-unspecific%0Atasks%2C%20achieving%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03414v2&entry.124074799=Read"},
{"title": "VecCity: A Taxonomy-guided Library for Map Entity Representation\n  Learning", "author": "Wentao Zhang and Jingyuan Wang and Yifan Yang and Leong Hou U", "abstract": "  Electronic maps consist of diverse entities, such as points of interest\n(POIs), road networks, and land parcels, playing a vital role in applications\nlike ITS and LBS. Map entity representation learning (MapRL) generates\nversatile and reusable data representations, providing essential tools for\nefficiently managing and utilizing map entity data. Despite the progress in\nMapRL, two key challenges constrain further development. First, existing\nresearch is fragmented, with models classified by the type of map entity,\nlimiting the reusability of techniques across different tasks. Second, the lack\nof unified benchmarks makes systematic evaluation and comparison of models\ndifficult. To address these challenges, we propose a novel taxonomy for MapRL\nthat organizes models based on functional module-such as encoders, pre-training\ntasks, and downstream tasks-rather than by entity type. Building on this\ntaxonomy, we present a taxonomy-driven library, VecCity, which offers\neasy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation.\nThe library integrates datasets from nine cities and reproduces 21 mainstream\nMapRL models, establishing the first standardized benchmarks for the field.\nVecCity also allows users to modify and extend models through modular\ncomponents, facilitating seamless experimentation. Our comprehensive\nexperiments cover multiple types of map entities and evaluate 21 VecCity\npre-built models across various downstream tasks. Experimental results\ndemonstrate the effectiveness of VecCity in streamlining model development and\nprovide insights into the impact of various components on performance. By\npromoting modular design and reusability, VecCity offers a unified framework to\nadvance research and innovation in MapRL. The code is available at\nhttps://github.com/Bigscity-VecCity/VecCity.\n", "link": "http://arxiv.org/abs/2411.00874v2", "date": "2025-05-07", "relevancy": 2.5992, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VecCity%3A%20A%20Taxonomy-guided%20Library%20for%20Map%20Entity%20Representation%0A%20%20Learning&body=Title%3A%20VecCity%3A%20A%20Taxonomy-guided%20Library%20for%20Map%20Entity%20Representation%0A%20%20Learning%0AAuthor%3A%20Wentao%20Zhang%20and%20Jingyuan%20Wang%20and%20Yifan%20Yang%20and%20Leong%20Hou%20U%0AAbstract%3A%20%20%20Electronic%20maps%20consist%20of%20diverse%20entities%2C%20such%20as%20points%20of%20interest%0A%28POIs%29%2C%20road%20networks%2C%20and%20land%20parcels%2C%20playing%20a%20vital%20role%20in%20applications%0Alike%20ITS%20and%20LBS.%20Map%20entity%20representation%20learning%20%28MapRL%29%20generates%0Aversatile%20and%20reusable%20data%20representations%2C%20providing%20essential%20tools%20for%0Aefficiently%20managing%20and%20utilizing%20map%20entity%20data.%20Despite%20the%20progress%20in%0AMapRL%2C%20two%20key%20challenges%20constrain%20further%20development.%20First%2C%20existing%0Aresearch%20is%20fragmented%2C%20with%20models%20classified%20by%20the%20type%20of%20map%20entity%2C%0Alimiting%20the%20reusability%20of%20techniques%20across%20different%20tasks.%20Second%2C%20the%20lack%0Aof%20unified%20benchmarks%20makes%20systematic%20evaluation%20and%20comparison%20of%20models%0Adifficult.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20taxonomy%20for%20MapRL%0Athat%20organizes%20models%20based%20on%20functional%20module-such%20as%20encoders%2C%20pre-training%0Atasks%2C%20and%20downstream%20tasks-rather%20than%20by%20entity%20type.%20Building%20on%20this%0Ataxonomy%2C%20we%20present%20a%20taxonomy-driven%20library%2C%20VecCity%2C%20which%20offers%0Aeasy-to-use%20interfaces%20for%20encoding%2C%20pre-training%2C%20fine-tuning%2C%20and%20evaluation.%0AThe%20library%20integrates%20datasets%20from%20nine%20cities%20and%20reproduces%2021%20mainstream%0AMapRL%20models%2C%20establishing%20the%20first%20standardized%20benchmarks%20for%20the%20field.%0AVecCity%20also%20allows%20users%20to%20modify%20and%20extend%20models%20through%20modular%0Acomponents%2C%20facilitating%20seamless%20experimentation.%20Our%20comprehensive%0Aexperiments%20cover%20multiple%20types%20of%20map%20entities%20and%20evaluate%2021%20VecCity%0Apre-built%20models%20across%20various%20downstream%20tasks.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20VecCity%20in%20streamlining%20model%20development%20and%0Aprovide%20insights%20into%20the%20impact%20of%20various%20components%20on%20performance.%20By%0Apromoting%20modular%20design%20and%20reusability%2C%20VecCity%20offers%20a%20unified%20framework%20to%0Aadvance%20research%20and%20innovation%20in%20MapRL.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Bigscity-VecCity/VecCity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00874v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVecCity%253A%2520A%2520Taxonomy-guided%2520Library%2520for%2520Map%2520Entity%2520Representation%250A%2520%2520Learning%26entry.906535625%3DWentao%2520Zhang%2520and%2520Jingyuan%2520Wang%2520and%2520Yifan%2520Yang%2520and%2520Leong%2520Hou%2520U%26entry.1292438233%3D%2520%2520Electronic%2520maps%2520consist%2520of%2520diverse%2520entities%252C%2520such%2520as%2520points%2520of%2520interest%250A%2528POIs%2529%252C%2520road%2520networks%252C%2520and%2520land%2520parcels%252C%2520playing%2520a%2520vital%2520role%2520in%2520applications%250Alike%2520ITS%2520and%2520LBS.%2520Map%2520entity%2520representation%2520learning%2520%2528MapRL%2529%2520generates%250Aversatile%2520and%2520reusable%2520data%2520representations%252C%2520providing%2520essential%2520tools%2520for%250Aefficiently%2520managing%2520and%2520utilizing%2520map%2520entity%2520data.%2520Despite%2520the%2520progress%2520in%250AMapRL%252C%2520two%2520key%2520challenges%2520constrain%2520further%2520development.%2520First%252C%2520existing%250Aresearch%2520is%2520fragmented%252C%2520with%2520models%2520classified%2520by%2520the%2520type%2520of%2520map%2520entity%252C%250Alimiting%2520the%2520reusability%2520of%2520techniques%2520across%2520different%2520tasks.%2520Second%252C%2520the%2520lack%250Aof%2520unified%2520benchmarks%2520makes%2520systematic%2520evaluation%2520and%2520comparison%2520of%2520models%250Adifficult.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520taxonomy%2520for%2520MapRL%250Athat%2520organizes%2520models%2520based%2520on%2520functional%2520module-such%2520as%2520encoders%252C%2520pre-training%250Atasks%252C%2520and%2520downstream%2520tasks-rather%2520than%2520by%2520entity%2520type.%2520Building%2520on%2520this%250Ataxonomy%252C%2520we%2520present%2520a%2520taxonomy-driven%2520library%252C%2520VecCity%252C%2520which%2520offers%250Aeasy-to-use%2520interfaces%2520for%2520encoding%252C%2520pre-training%252C%2520fine-tuning%252C%2520and%2520evaluation.%250AThe%2520library%2520integrates%2520datasets%2520from%2520nine%2520cities%2520and%2520reproduces%252021%2520mainstream%250AMapRL%2520models%252C%2520establishing%2520the%2520first%2520standardized%2520benchmarks%2520for%2520the%2520field.%250AVecCity%2520also%2520allows%2520users%2520to%2520modify%2520and%2520extend%2520models%2520through%2520modular%250Acomponents%252C%2520facilitating%2520seamless%2520experimentation.%2520Our%2520comprehensive%250Aexperiments%2520cover%2520multiple%2520types%2520of%2520map%2520entities%2520and%2520evaluate%252021%2520VecCity%250Apre-built%2520models%2520across%2520various%2520downstream%2520tasks.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520VecCity%2520in%2520streamlining%2520model%2520development%2520and%250Aprovide%2520insights%2520into%2520the%2520impact%2520of%2520various%2520components%2520on%2520performance.%2520By%250Apromoting%2520modular%2520design%2520and%2520reusability%252C%2520VecCity%2520offers%2520a%2520unified%2520framework%2520to%250Aadvance%2520research%2520and%2520innovation%2520in%2520MapRL.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Bigscity-VecCity/VecCity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00874v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VecCity%3A%20A%20Taxonomy-guided%20Library%20for%20Map%20Entity%20Representation%0A%20%20Learning&entry.906535625=Wentao%20Zhang%20and%20Jingyuan%20Wang%20and%20Yifan%20Yang%20and%20Leong%20Hou%20U&entry.1292438233=%20%20Electronic%20maps%20consist%20of%20diverse%20entities%2C%20such%20as%20points%20of%20interest%0A%28POIs%29%2C%20road%20networks%2C%20and%20land%20parcels%2C%20playing%20a%20vital%20role%20in%20applications%0Alike%20ITS%20and%20LBS.%20Map%20entity%20representation%20learning%20%28MapRL%29%20generates%0Aversatile%20and%20reusable%20data%20representations%2C%20providing%20essential%20tools%20for%0Aefficiently%20managing%20and%20utilizing%20map%20entity%20data.%20Despite%20the%20progress%20in%0AMapRL%2C%20two%20key%20challenges%20constrain%20further%20development.%20First%2C%20existing%0Aresearch%20is%20fragmented%2C%20with%20models%20classified%20by%20the%20type%20of%20map%20entity%2C%0Alimiting%20the%20reusability%20of%20techniques%20across%20different%20tasks.%20Second%2C%20the%20lack%0Aof%20unified%20benchmarks%20makes%20systematic%20evaluation%20and%20comparison%20of%20models%0Adifficult.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20taxonomy%20for%20MapRL%0Athat%20organizes%20models%20based%20on%20functional%20module-such%20as%20encoders%2C%20pre-training%0Atasks%2C%20and%20downstream%20tasks-rather%20than%20by%20entity%20type.%20Building%20on%20this%0Ataxonomy%2C%20we%20present%20a%20taxonomy-driven%20library%2C%20VecCity%2C%20which%20offers%0Aeasy-to-use%20interfaces%20for%20encoding%2C%20pre-training%2C%20fine-tuning%2C%20and%20evaluation.%0AThe%20library%20integrates%20datasets%20from%20nine%20cities%20and%20reproduces%2021%20mainstream%0AMapRL%20models%2C%20establishing%20the%20first%20standardized%20benchmarks%20for%20the%20field.%0AVecCity%20also%20allows%20users%20to%20modify%20and%20extend%20models%20through%20modular%0Acomponents%2C%20facilitating%20seamless%20experimentation.%20Our%20comprehensive%0Aexperiments%20cover%20multiple%20types%20of%20map%20entities%20and%20evaluate%2021%20VecCity%0Apre-built%20models%20across%20various%20downstream%20tasks.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20VecCity%20in%20streamlining%20model%20development%20and%0Aprovide%20insights%20into%20the%20impact%20of%20various%20components%20on%20performance.%20By%0Apromoting%20modular%20design%20and%20reusability%2C%20VecCity%20offers%20a%20unified%20framework%20to%0Aadvance%20research%20and%20innovation%20in%20MapRL.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Bigscity-VecCity/VecCity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00874v2&entry.124074799=Read"},
{"title": "Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions\n  Using Diffusion Model", "author": "Pengfei Guo and Can Zhao and Dong Yang and Yufan He and Vishwesh Nath and Ziyue Xu and Pedro R. A. S. Bassi and Zongwei Zhou and Benjamin D. Simon and Stephanie Anne Harmon and Baris Turkbey and Daguang Xu", "abstract": "  Generating 3D CT volumes from descriptive free-text inputs presents a\ntransformative opportunity in diagnostics and research. In this paper, we\nintroduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual\ndescriptions using the diffusion model. Unlike previous methods that rely on\nfixed-format text input, Text2CT employs a novel prompt formulation that\nenables generation from diverse, free-text descriptions. The proposed framework\nencodes medical text into latent representations and decodes them into\nhigh-resolution 3D CT scans, effectively bridging the gap between semantic text\ninputs and detailed volumetric representations in a unified 3D framework. Our\nmethod demonstrates superior performance in preserving anatomical fidelity and\ncapturing intricate structures as described in the input text. Extensive\nevaluations show that our approach achieves state-of-the-art results, offering\npromising potential applications in diagnostics, and data augmentation.\n", "link": "http://arxiv.org/abs/2505.04522v1", "date": "2025-05-07", "relevancy": 2.5677, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6538}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6538}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2CT%3A%20Towards%203D%20CT%20Volume%20Generation%20from%20Free-text%20Descriptions%0A%20%20Using%20Diffusion%20Model&body=Title%3A%20Text2CT%3A%20Towards%203D%20CT%20Volume%20Generation%20from%20Free-text%20Descriptions%0A%20%20Using%20Diffusion%20Model%0AAuthor%3A%20Pengfei%20Guo%20and%20Can%20Zhao%20and%20Dong%20Yang%20and%20Yufan%20He%20and%20Vishwesh%20Nath%20and%20Ziyue%20Xu%20and%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Zongwei%20Zhou%20and%20Benjamin%20D.%20Simon%20and%20Stephanie%20Anne%20Harmon%20and%20Baris%20Turkbey%20and%20Daguang%20Xu%0AAbstract%3A%20%20%20Generating%203D%20CT%20volumes%20from%20descriptive%20free-text%20inputs%20presents%20a%0Atransformative%20opportunity%20in%20diagnostics%20and%20research.%20In%20this%20paper%2C%20we%0Aintroduce%20Text2CT%2C%20a%20novel%20approach%20for%20synthesizing%203D%20CT%20volumes%20from%20textual%0Adescriptions%20using%20the%20diffusion%20model.%20Unlike%20previous%20methods%20that%20rely%20on%0Afixed-format%20text%20input%2C%20Text2CT%20employs%20a%20novel%20prompt%20formulation%20that%0Aenables%20generation%20from%20diverse%2C%20free-text%20descriptions.%20The%20proposed%20framework%0Aencodes%20medical%20text%20into%20latent%20representations%20and%20decodes%20them%20into%0Ahigh-resolution%203D%20CT%20scans%2C%20effectively%20bridging%20the%20gap%20between%20semantic%20text%0Ainputs%20and%20detailed%20volumetric%20representations%20in%20a%20unified%203D%20framework.%20Our%0Amethod%20demonstrates%20superior%20performance%20in%20preserving%20anatomical%20fidelity%20and%0Acapturing%20intricate%20structures%20as%20described%20in%20the%20input%20text.%20Extensive%0Aevaluations%20show%20that%20our%20approach%20achieves%20state-of-the-art%20results%2C%20offering%0Apromising%20potential%20applications%20in%20diagnostics%2C%20and%20data%20augmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2CT%253A%2520Towards%25203D%2520CT%2520Volume%2520Generation%2520from%2520Free-text%2520Descriptions%250A%2520%2520Using%2520Diffusion%2520Model%26entry.906535625%3DPengfei%2520Guo%2520and%2520Can%2520Zhao%2520and%2520Dong%2520Yang%2520and%2520Yufan%2520He%2520and%2520Vishwesh%2520Nath%2520and%2520Ziyue%2520Xu%2520and%2520Pedro%2520R.%2520A.%2520S.%2520Bassi%2520and%2520Zongwei%2520Zhou%2520and%2520Benjamin%2520D.%2520Simon%2520and%2520Stephanie%2520Anne%2520Harmon%2520and%2520Baris%2520Turkbey%2520and%2520Daguang%2520Xu%26entry.1292438233%3D%2520%2520Generating%25203D%2520CT%2520volumes%2520from%2520descriptive%2520free-text%2520inputs%2520presents%2520a%250Atransformative%2520opportunity%2520in%2520diagnostics%2520and%2520research.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520Text2CT%252C%2520a%2520novel%2520approach%2520for%2520synthesizing%25203D%2520CT%2520volumes%2520from%2520textual%250Adescriptions%2520using%2520the%2520diffusion%2520model.%2520Unlike%2520previous%2520methods%2520that%2520rely%2520on%250Afixed-format%2520text%2520input%252C%2520Text2CT%2520employs%2520a%2520novel%2520prompt%2520formulation%2520that%250Aenables%2520generation%2520from%2520diverse%252C%2520free-text%2520descriptions.%2520The%2520proposed%2520framework%250Aencodes%2520medical%2520text%2520into%2520latent%2520representations%2520and%2520decodes%2520them%2520into%250Ahigh-resolution%25203D%2520CT%2520scans%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520semantic%2520text%250Ainputs%2520and%2520detailed%2520volumetric%2520representations%2520in%2520a%2520unified%25203D%2520framework.%2520Our%250Amethod%2520demonstrates%2520superior%2520performance%2520in%2520preserving%2520anatomical%2520fidelity%2520and%250Acapturing%2520intricate%2520structures%2520as%2520described%2520in%2520the%2520input%2520text.%2520Extensive%250Aevaluations%2520show%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%252C%2520offering%250Apromising%2520potential%2520applications%2520in%2520diagnostics%252C%2520and%2520data%2520augmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2CT%3A%20Towards%203D%20CT%20Volume%20Generation%20from%20Free-text%20Descriptions%0A%20%20Using%20Diffusion%20Model&entry.906535625=Pengfei%20Guo%20and%20Can%20Zhao%20and%20Dong%20Yang%20and%20Yufan%20He%20and%20Vishwesh%20Nath%20and%20Ziyue%20Xu%20and%20Pedro%20R.%20A.%20S.%20Bassi%20and%20Zongwei%20Zhou%20and%20Benjamin%20D.%20Simon%20and%20Stephanie%20Anne%20Harmon%20and%20Baris%20Turkbey%20and%20Daguang%20Xu&entry.1292438233=%20%20Generating%203D%20CT%20volumes%20from%20descriptive%20free-text%20inputs%20presents%20a%0Atransformative%20opportunity%20in%20diagnostics%20and%20research.%20In%20this%20paper%2C%20we%0Aintroduce%20Text2CT%2C%20a%20novel%20approach%20for%20synthesizing%203D%20CT%20volumes%20from%20textual%0Adescriptions%20using%20the%20diffusion%20model.%20Unlike%20previous%20methods%20that%20rely%20on%0Afixed-format%20text%20input%2C%20Text2CT%20employs%20a%20novel%20prompt%20formulation%20that%0Aenables%20generation%20from%20diverse%2C%20free-text%20descriptions.%20The%20proposed%20framework%0Aencodes%20medical%20text%20into%20latent%20representations%20and%20decodes%20them%20into%0Ahigh-resolution%203D%20CT%20scans%2C%20effectively%20bridging%20the%20gap%20between%20semantic%20text%0Ainputs%20and%20detailed%20volumetric%20representations%20in%20a%20unified%203D%20framework.%20Our%0Amethod%20demonstrates%20superior%20performance%20in%20preserving%20anatomical%20fidelity%20and%0Acapturing%20intricate%20structures%20as%20described%20in%20the%20input%20text.%20Extensive%0Aevaluations%20show%20that%20our%20approach%20achieves%20state-of-the-art%20results%2C%20offering%0Apromising%20potential%20applications%20in%20diagnostics%2C%20and%20data%20augmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04522v1&entry.124074799=Read"},
{"title": "Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise\n  Scheduling", "author": "Nannan Li and Kevin J. Shih and Bryan A. Plummer", "abstract": "  Given an isolated garment image in a canonical product view and a separate\nimage of a person, the virtual try-on task aims to generate a new image of the\nperson wearing the target garment. Prior virtual try-on works face two major\nchallenges in achieving this goal: a) the paired (human, garment) training data\nhas limited availability; b) generating textures on the human that perfectly\nmatch that of the prompted garment is difficult, often resulting in distorted\ntext and faded textures. Our work explores ways to tackle these issues through\nboth synthetic data as well as model refinement. We introduce a garment\nextraction model that generates (human, synthetic garment) pairs from a single\nimage of a clothed individual. The synthetic pairs can then be used to augment\nthe training of virtual try-on. We also propose an Error-Aware Refinement-based\nSchr\\\"odinger Bridge (EARSB) that surgically targets localized generation\nerrors for correcting the output of a base virtual try-on model. To identify\nlikely errors, we propose a weakly-supervised error classifier that localizes\nregions for refinement, subsequently augmenting the Schr\\\"odinger Bridge's\nnoise schedule with its confidence heatmap. Experiments on VITON-HD and\nDressCode-Upper demonstrate that our synthetic data augmentation enhances the\nperformance of prior work, while EARSB improves the overall image quality. In\nuser studies, our model is preferred by the users in an average of 59% of\ncases.\n", "link": "http://arxiv.org/abs/2501.04666v3", "date": "2025-05-07", "relevancy": 2.5542, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6466}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6435}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Virtual%20Try-On%20with%20Synthetic%20Pairs%20and%20Error-Aware%20Noise%0A%20%20Scheduling&body=Title%3A%20Enhancing%20Virtual%20Try-On%20with%20Synthetic%20Pairs%20and%20Error-Aware%20Noise%0A%20%20Scheduling%0AAuthor%3A%20Nannan%20Li%20and%20Kevin%20J.%20Shih%20and%20Bryan%20A.%20Plummer%0AAbstract%3A%20%20%20Given%20an%20isolated%20garment%20image%20in%20a%20canonical%20product%20view%20and%20a%20separate%0Aimage%20of%20a%20person%2C%20the%20virtual%20try-on%20task%20aims%20to%20generate%20a%20new%20image%20of%20the%0Aperson%20wearing%20the%20target%20garment.%20Prior%20virtual%20try-on%20works%20face%20two%20major%0Achallenges%20in%20achieving%20this%20goal%3A%20a%29%20the%20paired%20%28human%2C%20garment%29%20training%20data%0Ahas%20limited%20availability%3B%20b%29%20generating%20textures%20on%20the%20human%20that%20perfectly%0Amatch%20that%20of%20the%20prompted%20garment%20is%20difficult%2C%20often%20resulting%20in%20distorted%0Atext%20and%20faded%20textures.%20Our%20work%20explores%20ways%20to%20tackle%20these%20issues%20through%0Aboth%20synthetic%20data%20as%20well%20as%20model%20refinement.%20We%20introduce%20a%20garment%0Aextraction%20model%20that%20generates%20%28human%2C%20synthetic%20garment%29%20pairs%20from%20a%20single%0Aimage%20of%20a%20clothed%20individual.%20The%20synthetic%20pairs%20can%20then%20be%20used%20to%20augment%0Athe%20training%20of%20virtual%20try-on.%20We%20also%20propose%20an%20Error-Aware%20Refinement-based%0ASchr%5C%22odinger%20Bridge%20%28EARSB%29%20that%20surgically%20targets%20localized%20generation%0Aerrors%20for%20correcting%20the%20output%20of%20a%20base%20virtual%20try-on%20model.%20To%20identify%0Alikely%20errors%2C%20we%20propose%20a%20weakly-supervised%20error%20classifier%20that%20localizes%0Aregions%20for%20refinement%2C%20subsequently%20augmenting%20the%20Schr%5C%22odinger%20Bridge%27s%0Anoise%20schedule%20with%20its%20confidence%20heatmap.%20Experiments%20on%20VITON-HD%20and%0ADressCode-Upper%20demonstrate%20that%20our%20synthetic%20data%20augmentation%20enhances%20the%0Aperformance%20of%20prior%20work%2C%20while%20EARSB%20improves%20the%20overall%20image%20quality.%20In%0Auser%20studies%2C%20our%20model%20is%20preferred%20by%20the%20users%20in%20an%20average%20of%2059%25%20of%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.04666v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Virtual%2520Try-On%2520with%2520Synthetic%2520Pairs%2520and%2520Error-Aware%2520Noise%250A%2520%2520Scheduling%26entry.906535625%3DNannan%2520Li%2520and%2520Kevin%2520J.%2520Shih%2520and%2520Bryan%2520A.%2520Plummer%26entry.1292438233%3D%2520%2520Given%2520an%2520isolated%2520garment%2520image%2520in%2520a%2520canonical%2520product%2520view%2520and%2520a%2520separate%250Aimage%2520of%2520a%2520person%252C%2520the%2520virtual%2520try-on%2520task%2520aims%2520to%2520generate%2520a%2520new%2520image%2520of%2520the%250Aperson%2520wearing%2520the%2520target%2520garment.%2520Prior%2520virtual%2520try-on%2520works%2520face%2520two%2520major%250Achallenges%2520in%2520achieving%2520this%2520goal%253A%2520a%2529%2520the%2520paired%2520%2528human%252C%2520garment%2529%2520training%2520data%250Ahas%2520limited%2520availability%253B%2520b%2529%2520generating%2520textures%2520on%2520the%2520human%2520that%2520perfectly%250Amatch%2520that%2520of%2520the%2520prompted%2520garment%2520is%2520difficult%252C%2520often%2520resulting%2520in%2520distorted%250Atext%2520and%2520faded%2520textures.%2520Our%2520work%2520explores%2520ways%2520to%2520tackle%2520these%2520issues%2520through%250Aboth%2520synthetic%2520data%2520as%2520well%2520as%2520model%2520refinement.%2520We%2520introduce%2520a%2520garment%250Aextraction%2520model%2520that%2520generates%2520%2528human%252C%2520synthetic%2520garment%2529%2520pairs%2520from%2520a%2520single%250Aimage%2520of%2520a%2520clothed%2520individual.%2520The%2520synthetic%2520pairs%2520can%2520then%2520be%2520used%2520to%2520augment%250Athe%2520training%2520of%2520virtual%2520try-on.%2520We%2520also%2520propose%2520an%2520Error-Aware%2520Refinement-based%250ASchr%255C%2522odinger%2520Bridge%2520%2528EARSB%2529%2520that%2520surgically%2520targets%2520localized%2520generation%250Aerrors%2520for%2520correcting%2520the%2520output%2520of%2520a%2520base%2520virtual%2520try-on%2520model.%2520To%2520identify%250Alikely%2520errors%252C%2520we%2520propose%2520a%2520weakly-supervised%2520error%2520classifier%2520that%2520localizes%250Aregions%2520for%2520refinement%252C%2520subsequently%2520augmenting%2520the%2520Schr%255C%2522odinger%2520Bridge%2527s%250Anoise%2520schedule%2520with%2520its%2520confidence%2520heatmap.%2520Experiments%2520on%2520VITON-HD%2520and%250ADressCode-Upper%2520demonstrate%2520that%2520our%2520synthetic%2520data%2520augmentation%2520enhances%2520the%250Aperformance%2520of%2520prior%2520work%252C%2520while%2520EARSB%2520improves%2520the%2520overall%2520image%2520quality.%2520In%250Auser%2520studies%252C%2520our%2520model%2520is%2520preferred%2520by%2520the%2520users%2520in%2520an%2520average%2520of%252059%2525%2520of%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.04666v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Virtual%20Try-On%20with%20Synthetic%20Pairs%20and%20Error-Aware%20Noise%0A%20%20Scheduling&entry.906535625=Nannan%20Li%20and%20Kevin%20J.%20Shih%20and%20Bryan%20A.%20Plummer&entry.1292438233=%20%20Given%20an%20isolated%20garment%20image%20in%20a%20canonical%20product%20view%20and%20a%20separate%0Aimage%20of%20a%20person%2C%20the%20virtual%20try-on%20task%20aims%20to%20generate%20a%20new%20image%20of%20the%0Aperson%20wearing%20the%20target%20garment.%20Prior%20virtual%20try-on%20works%20face%20two%20major%0Achallenges%20in%20achieving%20this%20goal%3A%20a%29%20the%20paired%20%28human%2C%20garment%29%20training%20data%0Ahas%20limited%20availability%3B%20b%29%20generating%20textures%20on%20the%20human%20that%20perfectly%0Amatch%20that%20of%20the%20prompted%20garment%20is%20difficult%2C%20often%20resulting%20in%20distorted%0Atext%20and%20faded%20textures.%20Our%20work%20explores%20ways%20to%20tackle%20these%20issues%20through%0Aboth%20synthetic%20data%20as%20well%20as%20model%20refinement.%20We%20introduce%20a%20garment%0Aextraction%20model%20that%20generates%20%28human%2C%20synthetic%20garment%29%20pairs%20from%20a%20single%0Aimage%20of%20a%20clothed%20individual.%20The%20synthetic%20pairs%20can%20then%20be%20used%20to%20augment%0Athe%20training%20of%20virtual%20try-on.%20We%20also%20propose%20an%20Error-Aware%20Refinement-based%0ASchr%5C%22odinger%20Bridge%20%28EARSB%29%20that%20surgically%20targets%20localized%20generation%0Aerrors%20for%20correcting%20the%20output%20of%20a%20base%20virtual%20try-on%20model.%20To%20identify%0Alikely%20errors%2C%20we%20propose%20a%20weakly-supervised%20error%20classifier%20that%20localizes%0Aregions%20for%20refinement%2C%20subsequently%20augmenting%20the%20Schr%5C%22odinger%20Bridge%27s%0Anoise%20schedule%20with%20its%20confidence%20heatmap.%20Experiments%20on%20VITON-HD%20and%0ADressCode-Upper%20demonstrate%20that%20our%20synthetic%20data%20augmentation%20enhances%20the%0Aperformance%20of%20prior%20work%2C%20while%20EARSB%20improves%20the%20overall%20image%20quality.%20In%0Auser%20studies%2C%20our%20model%20is%20preferred%20by%20the%20users%20in%20an%20average%20of%2059%25%20of%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.04666v3&entry.124074799=Read"},
{"title": "Multi-Granular Attention based Heterogeneous Hypergraph Neural Network", "author": "Hong Jin and Kaicheng Zhou and Jie Yin and Lan You and Zhifeng Zhou", "abstract": "  Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong\nabilities to learn node representations by effectively extracting complex\nstructural and semantic information in heterogeneous graphs. Most of the\nprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging\nmeta-path based message passing to learn latent node representations. However,\ndue to the pairwise nature of meta-paths, these models fail to capture\nhigh-order relations among nodes, resulting in suboptimal performance.\nAdditionally, the challenge of ``over-squashing'', where long-range message\npassing in HeteGNNs leads to severe information distortion, further limits the\nefficacy of these models. To address these limitations, this paper proposes\nMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural\nNetwork for heterogeneous graph representation learning. MGA-HHN introduces two\nkey innovations: (1) a novel approach for constructing meta-path based\nheterogeneous hypergraphs that explicitly models higher-order semantic\ninformation in heterogeneous graphs through multiple views, and (2) a\nmulti-granular attention mechanism that operates at both the node and hyperedge\nlevels. This mechanism enables the model to capture fine-grained interactions\namong nodes sharing the same semantic context within a hyperedge type, while\npreserving the diversity of semantics across different hyperedge types. As\nsuch, MGA-HHN effectively mitigates long-range message distortion and generates\nmore expressive node representations. Extensive experiments on real-world\nbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art\nmodels, showcasing its effectiveness in node classification, node clustering\nand visualization tasks.\n", "link": "http://arxiv.org/abs/2505.04340v1", "date": "2025-05-07", "relevancy": 2.5472, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5171}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5057}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Granular%20Attention%20based%20Heterogeneous%20Hypergraph%20Neural%20Network&body=Title%3A%20Multi-Granular%20Attention%20based%20Heterogeneous%20Hypergraph%20Neural%20Network%0AAuthor%3A%20Hong%20Jin%20and%20Kaicheng%20Zhou%20and%20Jie%20Yin%20and%20Lan%20You%20and%20Zhifeng%20Zhou%0AAbstract%3A%20%20%20Heterogeneous%20graph%20neural%20networks%20%28HeteGNNs%29%20have%20demonstrated%20strong%0Aabilities%20to%20learn%20node%20representations%20by%20effectively%20extracting%20complex%0Astructural%20and%20semantic%20information%20in%20heterogeneous%20graphs.%20Most%20of%20the%0Aprevailing%20HeteGNNs%20follow%20the%20neighborhood%20aggregation%20paradigm%2C%20leveraging%0Ameta-path%20based%20message%20passing%20to%20learn%20latent%20node%20representations.%20However%2C%0Adue%20to%20the%20pairwise%20nature%20of%20meta-paths%2C%20these%20models%20fail%20to%20capture%0Ahigh-order%20relations%20among%20nodes%2C%20resulting%20in%20suboptimal%20performance.%0AAdditionally%2C%20the%20challenge%20of%20%60%60over-squashing%27%27%2C%20where%20long-range%20message%0Apassing%20in%20HeteGNNs%20leads%20to%20severe%20information%20distortion%2C%20further%20limits%20the%0Aefficacy%20of%20these%20models.%20To%20address%20these%20limitations%2C%20this%20paper%20proposes%0AMGA-HHN%2C%20a%20Multi-Granular%20Attention%20based%20Heterogeneous%20Hypergraph%20Neural%0ANetwork%20for%20heterogeneous%20graph%20representation%20learning.%20MGA-HHN%20introduces%20two%0Akey%20innovations%3A%20%281%29%20a%20novel%20approach%20for%20constructing%20meta-path%20based%0Aheterogeneous%20hypergraphs%20that%20explicitly%20models%20higher-order%20semantic%0Ainformation%20in%20heterogeneous%20graphs%20through%20multiple%20views%2C%20and%20%282%29%20a%0Amulti-granular%20attention%20mechanism%20that%20operates%20at%20both%20the%20node%20and%20hyperedge%0Alevels.%20This%20mechanism%20enables%20the%20model%20to%20capture%20fine-grained%20interactions%0Aamong%20nodes%20sharing%20the%20same%20semantic%20context%20within%20a%20hyperedge%20type%2C%20while%0Apreserving%20the%20diversity%20of%20semantics%20across%20different%20hyperedge%20types.%20As%0Asuch%2C%20MGA-HHN%20effectively%20mitigates%20long-range%20message%20distortion%20and%20generates%0Amore%20expressive%20node%20representations.%20Extensive%20experiments%20on%20real-world%0Abenchmark%20datasets%20demonstrate%20that%20MGA-HHN%20outperforms%20state-of-the-art%0Amodels%2C%20showcasing%20its%20effectiveness%20in%20node%20classification%2C%20node%20clustering%0Aand%20visualization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Granular%2520Attention%2520based%2520Heterogeneous%2520Hypergraph%2520Neural%2520Network%26entry.906535625%3DHong%2520Jin%2520and%2520Kaicheng%2520Zhou%2520and%2520Jie%2520Yin%2520and%2520Lan%2520You%2520and%2520Zhifeng%2520Zhou%26entry.1292438233%3D%2520%2520Heterogeneous%2520graph%2520neural%2520networks%2520%2528HeteGNNs%2529%2520have%2520demonstrated%2520strong%250Aabilities%2520to%2520learn%2520node%2520representations%2520by%2520effectively%2520extracting%2520complex%250Astructural%2520and%2520semantic%2520information%2520in%2520heterogeneous%2520graphs.%2520Most%2520of%2520the%250Aprevailing%2520HeteGNNs%2520follow%2520the%2520neighborhood%2520aggregation%2520paradigm%252C%2520leveraging%250Ameta-path%2520based%2520message%2520passing%2520to%2520learn%2520latent%2520node%2520representations.%2520However%252C%250Adue%2520to%2520the%2520pairwise%2520nature%2520of%2520meta-paths%252C%2520these%2520models%2520fail%2520to%2520capture%250Ahigh-order%2520relations%2520among%2520nodes%252C%2520resulting%2520in%2520suboptimal%2520performance.%250AAdditionally%252C%2520the%2520challenge%2520of%2520%2560%2560over-squashing%2527%2527%252C%2520where%2520long-range%2520message%250Apassing%2520in%2520HeteGNNs%2520leads%2520to%2520severe%2520information%2520distortion%252C%2520further%2520limits%2520the%250Aefficacy%2520of%2520these%2520models.%2520To%2520address%2520these%2520limitations%252C%2520this%2520paper%2520proposes%250AMGA-HHN%252C%2520a%2520Multi-Granular%2520Attention%2520based%2520Heterogeneous%2520Hypergraph%2520Neural%250ANetwork%2520for%2520heterogeneous%2520graph%2520representation%2520learning.%2520MGA-HHN%2520introduces%2520two%250Akey%2520innovations%253A%2520%25281%2529%2520a%2520novel%2520approach%2520for%2520constructing%2520meta-path%2520based%250Aheterogeneous%2520hypergraphs%2520that%2520explicitly%2520models%2520higher-order%2520semantic%250Ainformation%2520in%2520heterogeneous%2520graphs%2520through%2520multiple%2520views%252C%2520and%2520%25282%2529%2520a%250Amulti-granular%2520attention%2520mechanism%2520that%2520operates%2520at%2520both%2520the%2520node%2520and%2520hyperedge%250Alevels.%2520This%2520mechanism%2520enables%2520the%2520model%2520to%2520capture%2520fine-grained%2520interactions%250Aamong%2520nodes%2520sharing%2520the%2520same%2520semantic%2520context%2520within%2520a%2520hyperedge%2520type%252C%2520while%250Apreserving%2520the%2520diversity%2520of%2520semantics%2520across%2520different%2520hyperedge%2520types.%2520As%250Asuch%252C%2520MGA-HHN%2520effectively%2520mitigates%2520long-range%2520message%2520distortion%2520and%2520generates%250Amore%2520expressive%2520node%2520representations.%2520Extensive%2520experiments%2520on%2520real-world%250Abenchmark%2520datasets%2520demonstrate%2520that%2520MGA-HHN%2520outperforms%2520state-of-the-art%250Amodels%252C%2520showcasing%2520its%2520effectiveness%2520in%2520node%2520classification%252C%2520node%2520clustering%250Aand%2520visualization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Granular%20Attention%20based%20Heterogeneous%20Hypergraph%20Neural%20Network&entry.906535625=Hong%20Jin%20and%20Kaicheng%20Zhou%20and%20Jie%20Yin%20and%20Lan%20You%20and%20Zhifeng%20Zhou&entry.1292438233=%20%20Heterogeneous%20graph%20neural%20networks%20%28HeteGNNs%29%20have%20demonstrated%20strong%0Aabilities%20to%20learn%20node%20representations%20by%20effectively%20extracting%20complex%0Astructural%20and%20semantic%20information%20in%20heterogeneous%20graphs.%20Most%20of%20the%0Aprevailing%20HeteGNNs%20follow%20the%20neighborhood%20aggregation%20paradigm%2C%20leveraging%0Ameta-path%20based%20message%20passing%20to%20learn%20latent%20node%20representations.%20However%2C%0Adue%20to%20the%20pairwise%20nature%20of%20meta-paths%2C%20these%20models%20fail%20to%20capture%0Ahigh-order%20relations%20among%20nodes%2C%20resulting%20in%20suboptimal%20performance.%0AAdditionally%2C%20the%20challenge%20of%20%60%60over-squashing%27%27%2C%20where%20long-range%20message%0Apassing%20in%20HeteGNNs%20leads%20to%20severe%20information%20distortion%2C%20further%20limits%20the%0Aefficacy%20of%20these%20models.%20To%20address%20these%20limitations%2C%20this%20paper%20proposes%0AMGA-HHN%2C%20a%20Multi-Granular%20Attention%20based%20Heterogeneous%20Hypergraph%20Neural%0ANetwork%20for%20heterogeneous%20graph%20representation%20learning.%20MGA-HHN%20introduces%20two%0Akey%20innovations%3A%20%281%29%20a%20novel%20approach%20for%20constructing%20meta-path%20based%0Aheterogeneous%20hypergraphs%20that%20explicitly%20models%20higher-order%20semantic%0Ainformation%20in%20heterogeneous%20graphs%20through%20multiple%20views%2C%20and%20%282%29%20a%0Amulti-granular%20attention%20mechanism%20that%20operates%20at%20both%20the%20node%20and%20hyperedge%0Alevels.%20This%20mechanism%20enables%20the%20model%20to%20capture%20fine-grained%20interactions%0Aamong%20nodes%20sharing%20the%20same%20semantic%20context%20within%20a%20hyperedge%20type%2C%20while%0Apreserving%20the%20diversity%20of%20semantics%20across%20different%20hyperedge%20types.%20As%0Asuch%2C%20MGA-HHN%20effectively%20mitigates%20long-range%20message%20distortion%20and%20generates%0Amore%20expressive%20node%20representations.%20Extensive%20experiments%20on%20real-world%0Abenchmark%20datasets%20demonstrate%20that%20MGA-HHN%20outperforms%20state-of-the-art%0Amodels%2C%20showcasing%20its%20effectiveness%20in%20node%20classification%2C%20node%20clustering%0Aand%20visualization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04340v1&entry.124074799=Read"},
{"title": "Weakly-supervised Audio Temporal Forgery Localization via Progressive\n  Audio-language Co-learning Network", "author": "Junyan Wu and Wenbo Xu and Wei Lu and Xiangyang Luo and Rui Yang and Shize Guo", "abstract": "  Audio temporal forgery localization (ATFL) aims to find the precise forgery\nregions of the partial spoof audio that is purposefully modified. Existing ATFL\nmethods rely on training efficient networks using fine-grained annotations,\nwhich are obtained costly and challenging in real-world scenarios. To meet this\nchallenge, in this paper, we propose a progressive audio-language co-learning\nnetwork (LOCO) that adopts co-learning and self-supervision manners to prompt\nlocalization performance under weak supervision scenarios. Specifically, an\naudio-language co-learning module is first designed to capture forgery\nconsensus features by aligning semantics from temporal and global perspectives.\nIn this module, forgery-aware prompts are constructed by using utterance-level\nannotations together with learnable prompts, which can incorporate semantic\npriors into temporal content features dynamically. In addition, a forgery\nlocalization module is applied to produce forgery proposals based on fused\nforgery-class activation sequences. Finally, a progressive refinement strategy\nis introduced to generate pseudo frame-level labels and leverage supervised\nsemantic contrastive learning to amplify the semantic distinction between real\nand fake content, thereby continuously optimizing forgery-aware features.\nExtensive experiments show that the proposed LOCO achieves SOTA performance on\nthree public benchmarks.\n", "link": "http://arxiv.org/abs/2505.01880v2", "date": "2025-05-07", "relevancy": 2.4889, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5229}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4924}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-supervised%20Audio%20Temporal%20Forgery%20Localization%20via%20Progressive%0A%20%20Audio-language%20Co-learning%20Network&body=Title%3A%20Weakly-supervised%20Audio%20Temporal%20Forgery%20Localization%20via%20Progressive%0A%20%20Audio-language%20Co-learning%20Network%0AAuthor%3A%20Junyan%20Wu%20and%20Wenbo%20Xu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%20and%20Rui%20Yang%20and%20Shize%20Guo%0AAbstract%3A%20%20%20Audio%20temporal%20forgery%20localization%20%28ATFL%29%20aims%20to%20find%20the%20precise%20forgery%0Aregions%20of%20the%20partial%20spoof%20audio%20that%20is%20purposefully%20modified.%20Existing%20ATFL%0Amethods%20rely%20on%20training%20efficient%20networks%20using%20fine-grained%20annotations%2C%0Awhich%20are%20obtained%20costly%20and%20challenging%20in%20real-world%20scenarios.%20To%20meet%20this%0Achallenge%2C%20in%20this%20paper%2C%20we%20propose%20a%20progressive%20audio-language%20co-learning%0Anetwork%20%28LOCO%29%20that%20adopts%20co-learning%20and%20self-supervision%20manners%20to%20prompt%0Alocalization%20performance%20under%20weak%20supervision%20scenarios.%20Specifically%2C%20an%0Aaudio-language%20co-learning%20module%20is%20first%20designed%20to%20capture%20forgery%0Aconsensus%20features%20by%20aligning%20semantics%20from%20temporal%20and%20global%20perspectives.%0AIn%20this%20module%2C%20forgery-aware%20prompts%20are%20constructed%20by%20using%20utterance-level%0Aannotations%20together%20with%20learnable%20prompts%2C%20which%20can%20incorporate%20semantic%0Apriors%20into%20temporal%20content%20features%20dynamically.%20In%20addition%2C%20a%20forgery%0Alocalization%20module%20is%20applied%20to%20produce%20forgery%20proposals%20based%20on%20fused%0Aforgery-class%20activation%20sequences.%20Finally%2C%20a%20progressive%20refinement%20strategy%0Ais%20introduced%20to%20generate%20pseudo%20frame-level%20labels%20and%20leverage%20supervised%0Asemantic%20contrastive%20learning%20to%20amplify%20the%20semantic%20distinction%20between%20real%0Aand%20fake%20content%2C%20thereby%20continuously%20optimizing%20forgery-aware%20features.%0AExtensive%20experiments%20show%20that%20the%20proposed%20LOCO%20achieves%20SOTA%20performance%20on%0Athree%20public%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01880v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-supervised%2520Audio%2520Temporal%2520Forgery%2520Localization%2520via%2520Progressive%250A%2520%2520Audio-language%2520Co-learning%2520Network%26entry.906535625%3DJunyan%2520Wu%2520and%2520Wenbo%2520Xu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%2520and%2520Rui%2520Yang%2520and%2520Shize%2520Guo%26entry.1292438233%3D%2520%2520Audio%2520temporal%2520forgery%2520localization%2520%2528ATFL%2529%2520aims%2520to%2520find%2520the%2520precise%2520forgery%250Aregions%2520of%2520the%2520partial%2520spoof%2520audio%2520that%2520is%2520purposefully%2520modified.%2520Existing%2520ATFL%250Amethods%2520rely%2520on%2520training%2520efficient%2520networks%2520using%2520fine-grained%2520annotations%252C%250Awhich%2520are%2520obtained%2520costly%2520and%2520challenging%2520in%2520real-world%2520scenarios.%2520To%2520meet%2520this%250Achallenge%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520progressive%2520audio-language%2520co-learning%250Anetwork%2520%2528LOCO%2529%2520that%2520adopts%2520co-learning%2520and%2520self-supervision%2520manners%2520to%2520prompt%250Alocalization%2520performance%2520under%2520weak%2520supervision%2520scenarios.%2520Specifically%252C%2520an%250Aaudio-language%2520co-learning%2520module%2520is%2520first%2520designed%2520to%2520capture%2520forgery%250Aconsensus%2520features%2520by%2520aligning%2520semantics%2520from%2520temporal%2520and%2520global%2520perspectives.%250AIn%2520this%2520module%252C%2520forgery-aware%2520prompts%2520are%2520constructed%2520by%2520using%2520utterance-level%250Aannotations%2520together%2520with%2520learnable%2520prompts%252C%2520which%2520can%2520incorporate%2520semantic%250Apriors%2520into%2520temporal%2520content%2520features%2520dynamically.%2520In%2520addition%252C%2520a%2520forgery%250Alocalization%2520module%2520is%2520applied%2520to%2520produce%2520forgery%2520proposals%2520based%2520on%2520fused%250Aforgery-class%2520activation%2520sequences.%2520Finally%252C%2520a%2520progressive%2520refinement%2520strategy%250Ais%2520introduced%2520to%2520generate%2520pseudo%2520frame-level%2520labels%2520and%2520leverage%2520supervised%250Asemantic%2520contrastive%2520learning%2520to%2520amplify%2520the%2520semantic%2520distinction%2520between%2520real%250Aand%2520fake%2520content%252C%2520thereby%2520continuously%2520optimizing%2520forgery-aware%2520features.%250AExtensive%2520experiments%2520show%2520that%2520the%2520proposed%2520LOCO%2520achieves%2520SOTA%2520performance%2520on%250Athree%2520public%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01880v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-supervised%20Audio%20Temporal%20Forgery%20Localization%20via%20Progressive%0A%20%20Audio-language%20Co-learning%20Network&entry.906535625=Junyan%20Wu%20and%20Wenbo%20Xu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%20and%20Rui%20Yang%20and%20Shize%20Guo&entry.1292438233=%20%20Audio%20temporal%20forgery%20localization%20%28ATFL%29%20aims%20to%20find%20the%20precise%20forgery%0Aregions%20of%20the%20partial%20spoof%20audio%20that%20is%20purposefully%20modified.%20Existing%20ATFL%0Amethods%20rely%20on%20training%20efficient%20networks%20using%20fine-grained%20annotations%2C%0Awhich%20are%20obtained%20costly%20and%20challenging%20in%20real-world%20scenarios.%20To%20meet%20this%0Achallenge%2C%20in%20this%20paper%2C%20we%20propose%20a%20progressive%20audio-language%20co-learning%0Anetwork%20%28LOCO%29%20that%20adopts%20co-learning%20and%20self-supervision%20manners%20to%20prompt%0Alocalization%20performance%20under%20weak%20supervision%20scenarios.%20Specifically%2C%20an%0Aaudio-language%20co-learning%20module%20is%20first%20designed%20to%20capture%20forgery%0Aconsensus%20features%20by%20aligning%20semantics%20from%20temporal%20and%20global%20perspectives.%0AIn%20this%20module%2C%20forgery-aware%20prompts%20are%20constructed%20by%20using%20utterance-level%0Aannotations%20together%20with%20learnable%20prompts%2C%20which%20can%20incorporate%20semantic%0Apriors%20into%20temporal%20content%20features%20dynamically.%20In%20addition%2C%20a%20forgery%0Alocalization%20module%20is%20applied%20to%20produce%20forgery%20proposals%20based%20on%20fused%0Aforgery-class%20activation%20sequences.%20Finally%2C%20a%20progressive%20refinement%20strategy%0Ais%20introduced%20to%20generate%20pseudo%20frame-level%20labels%20and%20leverage%20supervised%0Asemantic%20contrastive%20learning%20to%20amplify%20the%20semantic%20distinction%20between%20real%0Aand%20fake%20content%2C%20thereby%20continuously%20optimizing%20forgery-aware%20features.%0AExtensive%20experiments%20show%20that%20the%20proposed%20LOCO%20achieves%20SOTA%20performance%20on%0Athree%20public%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01880v2&entry.124074799=Read"},
{"title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "author": "Andrew Zhao and Yiran Wu and Yang Yue and Tong Wu and Quentin Xu and Yang Yue and Matthieu Lin and Shenzhi Wang and Qingyun Wu and Zilong Zheng and Gao Huang", "abstract": "  Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.\n", "link": "http://arxiv.org/abs/2505.03335v2", "date": "2025-05-07", "relevancy": 2.3988, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Absolute%20Zero%3A%20Reinforced%20Self-play%20Reasoning%20with%20Zero%20Data&body=Title%3A%20Absolute%20Zero%3A%20Reinforced%20Self-play%20Reasoning%20with%20Zero%20Data%0AAuthor%3A%20Andrew%20Zhao%20and%20Yiran%20Wu%20and%20Yang%20Yue%20and%20Tong%20Wu%20and%20Quentin%20Xu%20and%20Yang%20Yue%20and%20Matthieu%20Lin%20and%20Shenzhi%20Wang%20and%20Qingyun%20Wu%20and%20Zilong%20Zheng%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20shown%20promise%20in%0Aenhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%20by%20learning%0Adirectly%20from%20outcome-based%20rewards.%20Recent%20RLVR%20works%20that%20operate%20under%20the%0Azero%20setting%20avoid%20supervision%20in%20labeling%20the%20reasoning%20process%2C%20but%20still%0Adepend%20on%20manually%20curated%20collections%20of%20questions%20and%20answers%20for%20training.%0AThe%20scarcity%20of%20high-quality%2C%20human-produced%20examples%20raises%20concerns%20about%20the%0Along-term%20scalability%20of%20relying%20on%20human%20supervision%2C%20a%20challenge%20already%0Aevident%20in%20the%20domain%20of%20language%20model%20pretraining.%20Furthermore%2C%20in%20a%0Ahypothetical%20future%20where%20AI%20surpasses%20human%20intelligence%2C%20tasks%20provided%20by%0Ahumans%20may%20offer%20limited%20learning%20potential%20for%20a%20superintelligent%20system.%20To%0Aaddress%20these%20concerns%2C%20we%20propose%20a%20new%20RLVR%20paradigm%20called%20Absolute%20Zero%2C%20in%0Awhich%20a%20single%20model%20learns%20to%20propose%20tasks%20that%20maximize%20its%20own%20learning%0Aprogress%20and%20improves%20reasoning%20by%20solving%20them%2C%20without%20relying%20on%20any%0Aexternal%20data.%20Under%20this%20paradigm%2C%20we%20introduce%20the%20Absolute%20Zero%20Reasoner%0A%28AZR%29%2C%20a%20system%20that%20self-evolves%20its%20training%20curriculum%20and%20reasoning%20ability%0Aby%20using%20a%20code%20executor%20to%20both%20validate%20proposed%20code%20reasoning%20tasks%20and%0Averify%20answers%2C%20serving%20as%20an%20unified%20source%20of%20verifiable%20reward%20to%20guide%0Aopen-ended%20yet%20grounded%20learning.%20Despite%20being%20trained%20entirely%20without%0Aexternal%20data%2C%20AZR%20achieves%20overall%20SOTA%20performance%20on%20coding%20and%20mathematical%0Areasoning%20tasks%2C%20outperforming%20existing%20zero-setting%20models%20that%20rely%20on%20tens%0Aof%20thousands%20of%20in-domain%20human-curated%20examples.%20Furthermore%2C%20we%20demonstrate%0Athat%20AZR%20can%20be%20effectively%20applied%20across%20different%20model%20scales%20and%20is%0Acompatible%20with%20various%20model%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbsolute%2520Zero%253A%2520Reinforced%2520Self-play%2520Reasoning%2520with%2520Zero%2520Data%26entry.906535625%3DAndrew%2520Zhao%2520and%2520Yiran%2520Wu%2520and%2520Yang%2520Yue%2520and%2520Tong%2520Wu%2520and%2520Quentin%2520Xu%2520and%2520Yang%2520Yue%2520and%2520Matthieu%2520Lin%2520and%2520Shenzhi%2520Wang%2520and%2520Qingyun%2520Wu%2520and%2520Zilong%2520Zheng%2520and%2520Gao%2520Huang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%2520has%2520shown%2520promise%2520in%250Aenhancing%2520the%2520reasoning%2520capabilities%2520of%2520large%2520language%2520models%2520by%2520learning%250Adirectly%2520from%2520outcome-based%2520rewards.%2520Recent%2520RLVR%2520works%2520that%2520operate%2520under%2520the%250Azero%2520setting%2520avoid%2520supervision%2520in%2520labeling%2520the%2520reasoning%2520process%252C%2520but%2520still%250Adepend%2520on%2520manually%2520curated%2520collections%2520of%2520questions%2520and%2520answers%2520for%2520training.%250AThe%2520scarcity%2520of%2520high-quality%252C%2520human-produced%2520examples%2520raises%2520concerns%2520about%2520the%250Along-term%2520scalability%2520of%2520relying%2520on%2520human%2520supervision%252C%2520a%2520challenge%2520already%250Aevident%2520in%2520the%2520domain%2520of%2520language%2520model%2520pretraining.%2520Furthermore%252C%2520in%2520a%250Ahypothetical%2520future%2520where%2520AI%2520surpasses%2520human%2520intelligence%252C%2520tasks%2520provided%2520by%250Ahumans%2520may%2520offer%2520limited%2520learning%2520potential%2520for%2520a%2520superintelligent%2520system.%2520To%250Aaddress%2520these%2520concerns%252C%2520we%2520propose%2520a%2520new%2520RLVR%2520paradigm%2520called%2520Absolute%2520Zero%252C%2520in%250Awhich%2520a%2520single%2520model%2520learns%2520to%2520propose%2520tasks%2520that%2520maximize%2520its%2520own%2520learning%250Aprogress%2520and%2520improves%2520reasoning%2520by%2520solving%2520them%252C%2520without%2520relying%2520on%2520any%250Aexternal%2520data.%2520Under%2520this%2520paradigm%252C%2520we%2520introduce%2520the%2520Absolute%2520Zero%2520Reasoner%250A%2528AZR%2529%252C%2520a%2520system%2520that%2520self-evolves%2520its%2520training%2520curriculum%2520and%2520reasoning%2520ability%250Aby%2520using%2520a%2520code%2520executor%2520to%2520both%2520validate%2520proposed%2520code%2520reasoning%2520tasks%2520and%250Averify%2520answers%252C%2520serving%2520as%2520an%2520unified%2520source%2520of%2520verifiable%2520reward%2520to%2520guide%250Aopen-ended%2520yet%2520grounded%2520learning.%2520Despite%2520being%2520trained%2520entirely%2520without%250Aexternal%2520data%252C%2520AZR%2520achieves%2520overall%2520SOTA%2520performance%2520on%2520coding%2520and%2520mathematical%250Areasoning%2520tasks%252C%2520outperforming%2520existing%2520zero-setting%2520models%2520that%2520rely%2520on%2520tens%250Aof%2520thousands%2520of%2520in-domain%2520human-curated%2520examples.%2520Furthermore%252C%2520we%2520demonstrate%250Athat%2520AZR%2520can%2520be%2520effectively%2520applied%2520across%2520different%2520model%2520scales%2520and%2520is%250Acompatible%2520with%2520various%2520model%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Absolute%20Zero%3A%20Reinforced%20Self-play%20Reasoning%20with%20Zero%20Data&entry.906535625=Andrew%20Zhao%20and%20Yiran%20Wu%20and%20Yang%20Yue%20and%20Tong%20Wu%20and%20Quentin%20Xu%20and%20Yang%20Yue%20and%20Matthieu%20Lin%20and%20Shenzhi%20Wang%20and%20Qingyun%20Wu%20and%20Zilong%20Zheng%20and%20Gao%20Huang&entry.1292438233=%20%20Reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%20has%20shown%20promise%20in%0Aenhancing%20the%20reasoning%20capabilities%20of%20large%20language%20models%20by%20learning%0Adirectly%20from%20outcome-based%20rewards.%20Recent%20RLVR%20works%20that%20operate%20under%20the%0Azero%20setting%20avoid%20supervision%20in%20labeling%20the%20reasoning%20process%2C%20but%20still%0Adepend%20on%20manually%20curated%20collections%20of%20questions%20and%20answers%20for%20training.%0AThe%20scarcity%20of%20high-quality%2C%20human-produced%20examples%20raises%20concerns%20about%20the%0Along-term%20scalability%20of%20relying%20on%20human%20supervision%2C%20a%20challenge%20already%0Aevident%20in%20the%20domain%20of%20language%20model%20pretraining.%20Furthermore%2C%20in%20a%0Ahypothetical%20future%20where%20AI%20surpasses%20human%20intelligence%2C%20tasks%20provided%20by%0Ahumans%20may%20offer%20limited%20learning%20potential%20for%20a%20superintelligent%20system.%20To%0Aaddress%20these%20concerns%2C%20we%20propose%20a%20new%20RLVR%20paradigm%20called%20Absolute%20Zero%2C%20in%0Awhich%20a%20single%20model%20learns%20to%20propose%20tasks%20that%20maximize%20its%20own%20learning%0Aprogress%20and%20improves%20reasoning%20by%20solving%20them%2C%20without%20relying%20on%20any%0Aexternal%20data.%20Under%20this%20paradigm%2C%20we%20introduce%20the%20Absolute%20Zero%20Reasoner%0A%28AZR%29%2C%20a%20system%20that%20self-evolves%20its%20training%20curriculum%20and%20reasoning%20ability%0Aby%20using%20a%20code%20executor%20to%20both%20validate%20proposed%20code%20reasoning%20tasks%20and%0Averify%20answers%2C%20serving%20as%20an%20unified%20source%20of%20verifiable%20reward%20to%20guide%0Aopen-ended%20yet%20grounded%20learning.%20Despite%20being%20trained%20entirely%20without%0Aexternal%20data%2C%20AZR%20achieves%20overall%20SOTA%20performance%20on%20coding%20and%20mathematical%0Areasoning%20tasks%2C%20outperforming%20existing%20zero-setting%20models%20that%20rely%20on%20tens%0Aof%20thousands%20of%20in-domain%20human-curated%20examples.%20Furthermore%2C%20we%20demonstrate%0Athat%20AZR%20can%20be%20effectively%20applied%20across%20different%20model%20scales%20and%20is%0Acompatible%20with%20various%20model%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03335v2&entry.124074799=Read"},
{"title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning", "author": "Kunlun Xu and Xu Zou and Gang Hua and Jiahuan Zhou", "abstract": "  Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt\n", "link": "http://arxiv.org/abs/2505.04575v1", "date": "2025-05-07", "relevancy": 2.3948, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4867}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4858}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Componential%20Prompt-Knowledge%20Alignment%20for%20Domain%20Incremental%20Learning&body=Title%3A%20Componential%20Prompt-Knowledge%20Alignment%20for%20Domain%20Incremental%20Learning%0AAuthor%3A%20Kunlun%20Xu%20and%20Xu%20Zou%20and%20Gang%20Hua%20and%20Jiahuan%20Zhou%0AAbstract%3A%20%20%20Domain%20Incremental%20Learning%20%28DIL%29%20aims%20to%20learn%20from%20non-stationary%20data%0Astreams%20across%20domains%20while%20retaining%20and%20utilizing%20past%20knowledge.%20Although%0Aprompt-based%20methods%20effectively%20store%20multi-domain%20knowledge%20in%20prompt%0Aparameters%20and%20obtain%20advanced%20performance%20through%20cross-domain%20prompt%20fusion%2C%0Awe%20reveal%20an%20intrinsic%20limitation%3A%20component-wise%20misalignment%20between%0Adomain-specific%20prompts%20leads%20to%20conflicting%20knowledge%20integration%20and%20degraded%0Apredictions.%20This%20arises%20from%20the%20random%20positioning%20of%20knowledge%20components%0Awithin%20prompts%2C%20where%20irrelevant%20component%20fusion%20introduces%20interference.To%0Aaddress%20this%2C%20we%20propose%20Componential%20Prompt-Knowledge%20Alignment%20%28KA-Prompt%29%2C%20a%0Anovel%20prompt-based%20DIL%20method%20that%20introduces%20component-aware%20prompt-knowledge%0Aalignment%20during%20training%2C%20significantly%20improving%20both%20the%20learning%20and%0Ainference%20capacity%20of%20the%20model.%20KA-Prompt%20operates%20in%20two%20phases%3A%20%281%29%20Initial%0AComponential%20Structure%20Configuring%2C%20where%20a%20set%20of%20old%20prompts%20containing%0Aknowledge%20relevant%20to%20the%20new%20domain%20are%20mined%20via%20greedy%20search%2C%20which%20is%20then%0Aexploited%20to%20initialize%20new%20prompts%20to%20achieve%20reusable%20knowledge%20transfer%20and%0Aestablish%20intrinsic%20alignment%20between%20new%20and%20old%20prompts.%20%282%29%20Online%20Alignment%0APreservation%2C%20which%20dynamically%20identifies%20the%20target%20old%20prompts%20and%20applies%0Aadaptive%20componential%20consistency%20constraints%20as%20new%20prompts%20evolve.%20Extensive%0Aexperiments%20on%20DIL%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20KA-Prompt.%0AOur%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zhoujiahuan1991/ICML2025-KA-Prompt%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComponential%2520Prompt-Knowledge%2520Alignment%2520for%2520Domain%2520Incremental%2520Learning%26entry.906535625%3DKunlun%2520Xu%2520and%2520Xu%2520Zou%2520and%2520Gang%2520Hua%2520and%2520Jiahuan%2520Zhou%26entry.1292438233%3D%2520%2520Domain%2520Incremental%2520Learning%2520%2528DIL%2529%2520aims%2520to%2520learn%2520from%2520non-stationary%2520data%250Astreams%2520across%2520domains%2520while%2520retaining%2520and%2520utilizing%2520past%2520knowledge.%2520Although%250Aprompt-based%2520methods%2520effectively%2520store%2520multi-domain%2520knowledge%2520in%2520prompt%250Aparameters%2520and%2520obtain%2520advanced%2520performance%2520through%2520cross-domain%2520prompt%2520fusion%252C%250Awe%2520reveal%2520an%2520intrinsic%2520limitation%253A%2520component-wise%2520misalignment%2520between%250Adomain-specific%2520prompts%2520leads%2520to%2520conflicting%2520knowledge%2520integration%2520and%2520degraded%250Apredictions.%2520This%2520arises%2520from%2520the%2520random%2520positioning%2520of%2520knowledge%2520components%250Awithin%2520prompts%252C%2520where%2520irrelevant%2520component%2520fusion%2520introduces%2520interference.To%250Aaddress%2520this%252C%2520we%2520propose%2520Componential%2520Prompt-Knowledge%2520Alignment%2520%2528KA-Prompt%2529%252C%2520a%250Anovel%2520prompt-based%2520DIL%2520method%2520that%2520introduces%2520component-aware%2520prompt-knowledge%250Aalignment%2520during%2520training%252C%2520significantly%2520improving%2520both%2520the%2520learning%2520and%250Ainference%2520capacity%2520of%2520the%2520model.%2520KA-Prompt%2520operates%2520in%2520two%2520phases%253A%2520%25281%2529%2520Initial%250AComponential%2520Structure%2520Configuring%252C%2520where%2520a%2520set%2520of%2520old%2520prompts%2520containing%250Aknowledge%2520relevant%2520to%2520the%2520new%2520domain%2520are%2520mined%2520via%2520greedy%2520search%252C%2520which%2520is%2520then%250Aexploited%2520to%2520initialize%2520new%2520prompts%2520to%2520achieve%2520reusable%2520knowledge%2520transfer%2520and%250Aestablish%2520intrinsic%2520alignment%2520between%2520new%2520and%2520old%2520prompts.%2520%25282%2529%2520Online%2520Alignment%250APreservation%252C%2520which%2520dynamically%2520identifies%2520the%2520target%2520old%2520prompts%2520and%2520applies%250Aadaptive%2520componential%2520consistency%2520constraints%2520as%2520new%2520prompts%2520evolve.%2520Extensive%250Aexperiments%2520on%2520DIL%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520KA-Prompt.%250AOur%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zhoujiahuan1991/ICML2025-KA-Prompt%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Componential%20Prompt-Knowledge%20Alignment%20for%20Domain%20Incremental%20Learning&entry.906535625=Kunlun%20Xu%20and%20Xu%20Zou%20and%20Gang%20Hua%20and%20Jiahuan%20Zhou&entry.1292438233=%20%20Domain%20Incremental%20Learning%20%28DIL%29%20aims%20to%20learn%20from%20non-stationary%20data%0Astreams%20across%20domains%20while%20retaining%20and%20utilizing%20past%20knowledge.%20Although%0Aprompt-based%20methods%20effectively%20store%20multi-domain%20knowledge%20in%20prompt%0Aparameters%20and%20obtain%20advanced%20performance%20through%20cross-domain%20prompt%20fusion%2C%0Awe%20reveal%20an%20intrinsic%20limitation%3A%20component-wise%20misalignment%20between%0Adomain-specific%20prompts%20leads%20to%20conflicting%20knowledge%20integration%20and%20degraded%0Apredictions.%20This%20arises%20from%20the%20random%20positioning%20of%20knowledge%20components%0Awithin%20prompts%2C%20where%20irrelevant%20component%20fusion%20introduces%20interference.To%0Aaddress%20this%2C%20we%20propose%20Componential%20Prompt-Knowledge%20Alignment%20%28KA-Prompt%29%2C%20a%0Anovel%20prompt-based%20DIL%20method%20that%20introduces%20component-aware%20prompt-knowledge%0Aalignment%20during%20training%2C%20significantly%20improving%20both%20the%20learning%20and%0Ainference%20capacity%20of%20the%20model.%20KA-Prompt%20operates%20in%20two%20phases%3A%20%281%29%20Initial%0AComponential%20Structure%20Configuring%2C%20where%20a%20set%20of%20old%20prompts%20containing%0Aknowledge%20relevant%20to%20the%20new%20domain%20are%20mined%20via%20greedy%20search%2C%20which%20is%20then%0Aexploited%20to%20initialize%20new%20prompts%20to%20achieve%20reusable%20knowledge%20transfer%20and%0Aestablish%20intrinsic%20alignment%20between%20new%20and%20old%20prompts.%20%282%29%20Online%20Alignment%0APreservation%2C%20which%20dynamically%20identifies%20the%20target%20old%20prompts%20and%20applies%0Aadaptive%20componential%20consistency%20constraints%20as%20new%20prompts%20evolve.%20Extensive%0Aexperiments%20on%20DIL%20benchmarks%20demonstrate%20the%20effectiveness%20of%20our%20KA-Prompt.%0AOur%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/zhoujiahuan1991/ICML2025-KA-Prompt%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04575v1&entry.124074799=Read"},
{"title": "\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting\n  Individuals with Visual Impairments", "author": "Ziyi Zhang and Zhen Sun and Zongmin Zhang and Zifan Peng and Yuemeng Zhao and Zichun Wang and Zeren Luo and Ruiting Zuo and Xinlei He", "abstract": "  The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield.\n", "link": "http://arxiv.org/abs/2505.04488v1", "date": "2025-05-07", "relevancy": 2.3819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6007}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22I%20Can%20See%20Forever%21%22%3A%20Evaluating%20Real-time%20VideoLLMs%20for%20Assisting%0A%20%20Individuals%20with%20Visual%20Impairments&body=Title%3A%20%22I%20Can%20See%20Forever%21%22%3A%20Evaluating%20Real-time%20VideoLLMs%20for%20Assisting%0A%20%20Individuals%20with%20Visual%20Impairments%0AAuthor%3A%20Ziyi%20Zhang%20and%20Zhen%20Sun%20and%20Zongmin%20Zhang%20and%20Zifan%20Peng%20and%20Yuemeng%20Zhao%20and%20Zichun%20Wang%20and%20Zeren%20Luo%20and%20Ruiting%20Zuo%20and%20Xinlei%20He%0AAbstract%3A%20%20%20The%20visually%20impaired%20population%2C%20especially%20the%20severely%20visually%20impaired%2C%0Ais%20currently%20large%20in%20scale%2C%20and%20daily%20activities%20pose%20significant%20challenges%0Afor%20them.%20Although%20many%20studies%20use%20large%20language%20and%20vision-language%20models%0Ato%20assist%20the%20blind%2C%20most%20focus%20on%20static%20content%20and%20fail%20to%20meet%20real-time%0Aperception%20needs%20in%20dynamic%20and%20complex%20environments%2C%20such%20as%20daily%20activities.%0ATo%20provide%20them%20with%20more%20effective%20intelligent%20assistance%2C%20it%20is%20imperative%20to%0Aincorporate%20advanced%20visual%20understanding%20technologies.%20Although%20real-time%0Avision%20and%20speech%20interaction%20VideoLLMs%20demonstrate%20strong%20real-time%20visual%0Aunderstanding%2C%20no%20prior%20work%20has%20systematically%20evaluated%20their%20effectiveness%0Ain%20assisting%20visually%20impaired%20individuals.%20In%20this%20work%2C%20we%20conduct%20the%20first%0Asuch%20evaluation.%20First%2C%20we%20construct%20a%20benchmark%20dataset%20%28VisAssistDaily%29%2C%0Acovering%20three%20categories%20of%20assistive%20tasks%20for%20visually%20impaired%20individuals%3A%0ABasic%20Skills%2C%20Home%20Life%20Tasks%2C%20and%20Social%20Life%20Tasks.%20The%20results%20show%20that%0AGPT-4o%20achieves%20the%20highest%20task%20success%20rate.%20Next%2C%20we%20conduct%20a%20user%20study%20to%0Aevaluate%20the%20models%20in%20both%20closed-world%20and%20open-world%20scenarios%2C%20further%0Aexploring%20the%20practical%20challenges%20of%20applying%20VideoLLMs%20in%20assistive%20contexts.%0AOne%20key%20issue%20we%20identify%20is%20the%20difficulty%20current%20models%20face%20in%20perceiving%0Apotential%20hazards%20in%20dynamic%20environments.%20To%20address%20this%2C%20we%20build%20an%0Aenvironment-awareness%20dataset%20named%20SafeVid%20and%20introduce%20a%20polling%20mechanism%0Athat%20enables%20the%20model%20to%20proactively%20detect%20environmental%20risks.%20We%20hope%20this%0Awork%20provides%20valuable%20insights%20and%20inspiration%20for%20future%20research%20in%20this%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522I%2520Can%2520See%2520Forever%2521%2522%253A%2520Evaluating%2520Real-time%2520VideoLLMs%2520for%2520Assisting%250A%2520%2520Individuals%2520with%2520Visual%2520Impairments%26entry.906535625%3DZiyi%2520Zhang%2520and%2520Zhen%2520Sun%2520and%2520Zongmin%2520Zhang%2520and%2520Zifan%2520Peng%2520and%2520Yuemeng%2520Zhao%2520and%2520Zichun%2520Wang%2520and%2520Zeren%2520Luo%2520and%2520Ruiting%2520Zuo%2520and%2520Xinlei%2520He%26entry.1292438233%3D%2520%2520The%2520visually%2520impaired%2520population%252C%2520especially%2520the%2520severely%2520visually%2520impaired%252C%250Ais%2520currently%2520large%2520in%2520scale%252C%2520and%2520daily%2520activities%2520pose%2520significant%2520challenges%250Afor%2520them.%2520Although%2520many%2520studies%2520use%2520large%2520language%2520and%2520vision-language%2520models%250Ato%2520assist%2520the%2520blind%252C%2520most%2520focus%2520on%2520static%2520content%2520and%2520fail%2520to%2520meet%2520real-time%250Aperception%2520needs%2520in%2520dynamic%2520and%2520complex%2520environments%252C%2520such%2520as%2520daily%2520activities.%250ATo%2520provide%2520them%2520with%2520more%2520effective%2520intelligent%2520assistance%252C%2520it%2520is%2520imperative%2520to%250Aincorporate%2520advanced%2520visual%2520understanding%2520technologies.%2520Although%2520real-time%250Avision%2520and%2520speech%2520interaction%2520VideoLLMs%2520demonstrate%2520strong%2520real-time%2520visual%250Aunderstanding%252C%2520no%2520prior%2520work%2520has%2520systematically%2520evaluated%2520their%2520effectiveness%250Ain%2520assisting%2520visually%2520impaired%2520individuals.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%2520first%250Asuch%2520evaluation.%2520First%252C%2520we%2520construct%2520a%2520benchmark%2520dataset%2520%2528VisAssistDaily%2529%252C%250Acovering%2520three%2520categories%2520of%2520assistive%2520tasks%2520for%2520visually%2520impaired%2520individuals%253A%250ABasic%2520Skills%252C%2520Home%2520Life%2520Tasks%252C%2520and%2520Social%2520Life%2520Tasks.%2520The%2520results%2520show%2520that%250AGPT-4o%2520achieves%2520the%2520highest%2520task%2520success%2520rate.%2520Next%252C%2520we%2520conduct%2520a%2520user%2520study%2520to%250Aevaluate%2520the%2520models%2520in%2520both%2520closed-world%2520and%2520open-world%2520scenarios%252C%2520further%250Aexploring%2520the%2520practical%2520challenges%2520of%2520applying%2520VideoLLMs%2520in%2520assistive%2520contexts.%250AOne%2520key%2520issue%2520we%2520identify%2520is%2520the%2520difficulty%2520current%2520models%2520face%2520in%2520perceiving%250Apotential%2520hazards%2520in%2520dynamic%2520environments.%2520To%2520address%2520this%252C%2520we%2520build%2520an%250Aenvironment-awareness%2520dataset%2520named%2520SafeVid%2520and%2520introduce%2520a%2520polling%2520mechanism%250Athat%2520enables%2520the%2520model%2520to%2520proactively%2520detect%2520environmental%2520risks.%2520We%2520hope%2520this%250Awork%2520provides%2520valuable%2520insights%2520and%2520inspiration%2520for%2520future%2520research%2520in%2520this%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22I%20Can%20See%20Forever%21%22%3A%20Evaluating%20Real-time%20VideoLLMs%20for%20Assisting%0A%20%20Individuals%20with%20Visual%20Impairments&entry.906535625=Ziyi%20Zhang%20and%20Zhen%20Sun%20and%20Zongmin%20Zhang%20and%20Zifan%20Peng%20and%20Yuemeng%20Zhao%20and%20Zichun%20Wang%20and%20Zeren%20Luo%20and%20Ruiting%20Zuo%20and%20Xinlei%20He&entry.1292438233=%20%20The%20visually%20impaired%20population%2C%20especially%20the%20severely%20visually%20impaired%2C%0Ais%20currently%20large%20in%20scale%2C%20and%20daily%20activities%20pose%20significant%20challenges%0Afor%20them.%20Although%20many%20studies%20use%20large%20language%20and%20vision-language%20models%0Ato%20assist%20the%20blind%2C%20most%20focus%20on%20static%20content%20and%20fail%20to%20meet%20real-time%0Aperception%20needs%20in%20dynamic%20and%20complex%20environments%2C%20such%20as%20daily%20activities.%0ATo%20provide%20them%20with%20more%20effective%20intelligent%20assistance%2C%20it%20is%20imperative%20to%0Aincorporate%20advanced%20visual%20understanding%20technologies.%20Although%20real-time%0Avision%20and%20speech%20interaction%20VideoLLMs%20demonstrate%20strong%20real-time%20visual%0Aunderstanding%2C%20no%20prior%20work%20has%20systematically%20evaluated%20their%20effectiveness%0Ain%20assisting%20visually%20impaired%20individuals.%20In%20this%20work%2C%20we%20conduct%20the%20first%0Asuch%20evaluation.%20First%2C%20we%20construct%20a%20benchmark%20dataset%20%28VisAssistDaily%29%2C%0Acovering%20three%20categories%20of%20assistive%20tasks%20for%20visually%20impaired%20individuals%3A%0ABasic%20Skills%2C%20Home%20Life%20Tasks%2C%20and%20Social%20Life%20Tasks.%20The%20results%20show%20that%0AGPT-4o%20achieves%20the%20highest%20task%20success%20rate.%20Next%2C%20we%20conduct%20a%20user%20study%20to%0Aevaluate%20the%20models%20in%20both%20closed-world%20and%20open-world%20scenarios%2C%20further%0Aexploring%20the%20practical%20challenges%20of%20applying%20VideoLLMs%20in%20assistive%20contexts.%0AOne%20key%20issue%20we%20identify%20is%20the%20difficulty%20current%20models%20face%20in%20perceiving%0Apotential%20hazards%20in%20dynamic%20environments.%20To%20address%20this%2C%20we%20build%20an%0Aenvironment-awareness%20dataset%20named%20SafeVid%20and%20introduce%20a%20polling%20mechanism%0Athat%20enables%20the%20model%20to%20proactively%20detect%20environmental%20risks.%20We%20hope%20this%0Awork%20provides%20valuable%20insights%20and%20inspiration%20for%20future%20research%20in%20this%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04488v1&entry.124074799=Read"},
{"title": "VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling", "author": "Zeyue Tian and Zhaoyang Liu and Ruibin Yuan and Jiahao Pan and Qifeng Liu and Xu Tan and Qifeng Chen and Wei Xue and Yike Guo", "abstract": "  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 360K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets are available at\nhttps://vidmuse.github.io/.\n", "link": "http://arxiv.org/abs/2406.04321v3", "date": "2025-05-07", "relevancy": 2.3079, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6003}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5665}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidMuse%3A%20A%20Simple%20Video-to-Music%20Generation%20Framework%20with%0A%20%20Long-Short-Term%20Modeling&body=Title%3A%20VidMuse%3A%20A%20Simple%20Video-to-Music%20Generation%20Framework%20with%0A%20%20Long-Short-Term%20Modeling%0AAuthor%3A%20Zeyue%20Tian%20and%20Zhaoyang%20Liu%20and%20Ruibin%20Yuan%20and%20Jiahao%20Pan%20and%20Qifeng%20Liu%20and%20Xu%20Tan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20systematically%20study%20music%20generation%20conditioned%20solely%20on%0Athe%20video.%20First%2C%20we%20present%20a%20large-scale%20dataset%20comprising%20360K%20video-music%0Apairs%2C%20including%20various%20genres%20such%20as%20movie%20trailers%2C%20advertisements%2C%20and%0Adocumentaries.%20Furthermore%2C%20we%20propose%20VidMuse%2C%20a%20simple%20framework%20for%0Agenerating%20music%20aligned%20with%20video%20inputs.%20VidMuse%20stands%20out%20by%20producing%0Ahigh-fidelity%20music%20that%20is%20both%20acoustically%20and%20semantically%20aligned%20with%20the%0Avideo.%20By%20incorporating%20local%20and%20global%20visual%20cues%2C%20VidMuse%20enables%20the%0Acreation%20of%20musically%20coherent%20audio%20tracks%20that%20consistently%20match%20the%20video%0Acontent%20through%20Long-Short-Term%20modeling.%20Through%20extensive%20experiments%2C%0AVidMuse%20outperforms%20existing%20models%20in%20terms%20of%20audio%20quality%2C%20diversity%2C%20and%0Aaudio-visual%20alignment.%20The%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//vidmuse.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04321v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidMuse%253A%2520A%2520Simple%2520Video-to-Music%2520Generation%2520Framework%2520with%250A%2520%2520Long-Short-Term%2520Modeling%26entry.906535625%3DZeyue%2520Tian%2520and%2520Zhaoyang%2520Liu%2520and%2520Ruibin%2520Yuan%2520and%2520Jiahao%2520Pan%2520and%2520Qifeng%2520Liu%2520and%2520Xu%2520Tan%2520and%2520Qifeng%2520Chen%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520systematically%2520study%2520music%2520generation%2520conditioned%2520solely%2520on%250Athe%2520video.%2520First%252C%2520we%2520present%2520a%2520large-scale%2520dataset%2520comprising%2520360K%2520video-music%250Apairs%252C%2520including%2520various%2520genres%2520such%2520as%2520movie%2520trailers%252C%2520advertisements%252C%2520and%250Adocumentaries.%2520Furthermore%252C%2520we%2520propose%2520VidMuse%252C%2520a%2520simple%2520framework%2520for%250Agenerating%2520music%2520aligned%2520with%2520video%2520inputs.%2520VidMuse%2520stands%2520out%2520by%2520producing%250Ahigh-fidelity%2520music%2520that%2520is%2520both%2520acoustically%2520and%2520semantically%2520aligned%2520with%2520the%250Avideo.%2520By%2520incorporating%2520local%2520and%2520global%2520visual%2520cues%252C%2520VidMuse%2520enables%2520the%250Acreation%2520of%2520musically%2520coherent%2520audio%2520tracks%2520that%2520consistently%2520match%2520the%2520video%250Acontent%2520through%2520Long-Short-Term%2520modeling.%2520Through%2520extensive%2520experiments%252C%250AVidMuse%2520outperforms%2520existing%2520models%2520in%2520terms%2520of%2520audio%2520quality%252C%2520diversity%252C%2520and%250Aaudio-visual%2520alignment.%2520The%2520code%2520and%2520datasets%2520are%2520available%2520at%250Ahttps%253A//vidmuse.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04321v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidMuse%3A%20A%20Simple%20Video-to-Music%20Generation%20Framework%20with%0A%20%20Long-Short-Term%20Modeling&entry.906535625=Zeyue%20Tian%20and%20Zhaoyang%20Liu%20and%20Ruibin%20Yuan%20and%20Jiahao%20Pan%20and%20Qifeng%20Liu%20and%20Xu%20Tan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20In%20this%20work%2C%20we%20systematically%20study%20music%20generation%20conditioned%20solely%20on%0Athe%20video.%20First%2C%20we%20present%20a%20large-scale%20dataset%20comprising%20360K%20video-music%0Apairs%2C%20including%20various%20genres%20such%20as%20movie%20trailers%2C%20advertisements%2C%20and%0Adocumentaries.%20Furthermore%2C%20we%20propose%20VidMuse%2C%20a%20simple%20framework%20for%0Agenerating%20music%20aligned%20with%20video%20inputs.%20VidMuse%20stands%20out%20by%20producing%0Ahigh-fidelity%20music%20that%20is%20both%20acoustically%20and%20semantically%20aligned%20with%20the%0Avideo.%20By%20incorporating%20local%20and%20global%20visual%20cues%2C%20VidMuse%20enables%20the%0Acreation%20of%20musically%20coherent%20audio%20tracks%20that%20consistently%20match%20the%20video%0Acontent%20through%20Long-Short-Term%20modeling.%20Through%20extensive%20experiments%2C%0AVidMuse%20outperforms%20existing%20models%20in%20terms%20of%20audio%20quality%2C%20diversity%2C%20and%0Aaudio-visual%20alignment.%20The%20code%20and%20datasets%20are%20available%20at%0Ahttps%3A//vidmuse.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04321v3&entry.124074799=Read"},
{"title": "Label-efficient Single Photon Images Classification via Active Learning", "author": "Zili Zhang and Ziting Wen and Yiheng Qiang and Hongzhou Dong and Wenle Dong and Xinyang Li and Xiaofan Wang and Xiaoqiang Ren", "abstract": "  Single-photon LiDAR achieves high-precision 3D imaging in extreme\nenvironments through quantum-level photon detection technology. Current\nresearch primarily focuses on reconstructing 3D scenes from sparse photon\nevents, whereas the semantic interpretation of single-photon images remains\nunderexplored, due to high annotation costs and inefficient labeling\nstrategies. This paper presents the first active learning framework for\nsingle-photon image classification. The core contribution is an imaging\ncondition-aware sampling strategy that integrates synthetic augmentation to\nmodel variability across imaging conditions. By identifying samples where the\nmodel is both uncertain and sensitive to these conditions, the proposed method\nselectively annotates only the most informative examples. Experiments on both\nsynthetic and real-world datasets show that our approach outperforms all\nbaselines and achieves high classification accuracy with significantly fewer\nlabeled samples. Specifically, our approach achieves 97% accuracy on synthetic\nsingle-photon data using only 1.5% labeled samples. On real-world data, we\nmaintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher\nthan the best-performing baseline. This illustrates that active learning\nenables the same level of classification performance on single-photon images as\non classical images, opening doors to large-scale integration of single-photon\ndata in real-world applications.\n", "link": "http://arxiv.org/abs/2505.04376v1", "date": "2025-05-07", "relevancy": 2.3019, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.602}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Label-efficient%20Single%20Photon%20Images%20Classification%20via%20Active%20Learning&body=Title%3A%20Label-efficient%20Single%20Photon%20Images%20Classification%20via%20Active%20Learning%0AAuthor%3A%20Zili%20Zhang%20and%20Ziting%20Wen%20and%20Yiheng%20Qiang%20and%20Hongzhou%20Dong%20and%20Wenle%20Dong%20and%20Xinyang%20Li%20and%20Xiaofan%20Wang%20and%20Xiaoqiang%20Ren%0AAbstract%3A%20%20%20Single-photon%20LiDAR%20achieves%20high-precision%203D%20imaging%20in%20extreme%0Aenvironments%20through%20quantum-level%20photon%20detection%20technology.%20Current%0Aresearch%20primarily%20focuses%20on%20reconstructing%203D%20scenes%20from%20sparse%20photon%0Aevents%2C%20whereas%20the%20semantic%20interpretation%20of%20single-photon%20images%20remains%0Aunderexplored%2C%20due%20to%20high%20annotation%20costs%20and%20inefficient%20labeling%0Astrategies.%20This%20paper%20presents%20the%20first%20active%20learning%20framework%20for%0Asingle-photon%20image%20classification.%20The%20core%20contribution%20is%20an%20imaging%0Acondition-aware%20sampling%20strategy%20that%20integrates%20synthetic%20augmentation%20to%0Amodel%20variability%20across%20imaging%20conditions.%20By%20identifying%20samples%20where%20the%0Amodel%20is%20both%20uncertain%20and%20sensitive%20to%20these%20conditions%2C%20the%20proposed%20method%0Aselectively%20annotates%20only%20the%20most%20informative%20examples.%20Experiments%20on%20both%0Asynthetic%20and%20real-world%20datasets%20show%20that%20our%20approach%20outperforms%20all%0Abaselines%20and%20achieves%20high%20classification%20accuracy%20with%20significantly%20fewer%0Alabeled%20samples.%20Specifically%2C%20our%20approach%20achieves%2097%25%20accuracy%20on%20synthetic%0Asingle-photon%20data%20using%20only%201.5%25%20labeled%20samples.%20On%20real-world%20data%2C%20we%0Amaintain%2090.63%25%20accuracy%20with%20just%208%25%20labeled%20samples%2C%20which%20is%204.51%25%20higher%0Athan%20the%20best-performing%20baseline.%20This%20illustrates%20that%20active%20learning%0Aenables%20the%20same%20level%20of%20classification%20performance%20on%20single-photon%20images%20as%0Aon%20classical%20images%2C%20opening%20doors%20to%20large-scale%20integration%20of%20single-photon%0Adata%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabel-efficient%2520Single%2520Photon%2520Images%2520Classification%2520via%2520Active%2520Learning%26entry.906535625%3DZili%2520Zhang%2520and%2520Ziting%2520Wen%2520and%2520Yiheng%2520Qiang%2520and%2520Hongzhou%2520Dong%2520and%2520Wenle%2520Dong%2520and%2520Xinyang%2520Li%2520and%2520Xiaofan%2520Wang%2520and%2520Xiaoqiang%2520Ren%26entry.1292438233%3D%2520%2520Single-photon%2520LiDAR%2520achieves%2520high-precision%25203D%2520imaging%2520in%2520extreme%250Aenvironments%2520through%2520quantum-level%2520photon%2520detection%2520technology.%2520Current%250Aresearch%2520primarily%2520focuses%2520on%2520reconstructing%25203D%2520scenes%2520from%2520sparse%2520photon%250Aevents%252C%2520whereas%2520the%2520semantic%2520interpretation%2520of%2520single-photon%2520images%2520remains%250Aunderexplored%252C%2520due%2520to%2520high%2520annotation%2520costs%2520and%2520inefficient%2520labeling%250Astrategies.%2520This%2520paper%2520presents%2520the%2520first%2520active%2520learning%2520framework%2520for%250Asingle-photon%2520image%2520classification.%2520The%2520core%2520contribution%2520is%2520an%2520imaging%250Acondition-aware%2520sampling%2520strategy%2520that%2520integrates%2520synthetic%2520augmentation%2520to%250Amodel%2520variability%2520across%2520imaging%2520conditions.%2520By%2520identifying%2520samples%2520where%2520the%250Amodel%2520is%2520both%2520uncertain%2520and%2520sensitive%2520to%2520these%2520conditions%252C%2520the%2520proposed%2520method%250Aselectively%2520annotates%2520only%2520the%2520most%2520informative%2520examples.%2520Experiments%2520on%2520both%250Asynthetic%2520and%2520real-world%2520datasets%2520show%2520that%2520our%2520approach%2520outperforms%2520all%250Abaselines%2520and%2520achieves%2520high%2520classification%2520accuracy%2520with%2520significantly%2520fewer%250Alabeled%2520samples.%2520Specifically%252C%2520our%2520approach%2520achieves%252097%2525%2520accuracy%2520on%2520synthetic%250Asingle-photon%2520data%2520using%2520only%25201.5%2525%2520labeled%2520samples.%2520On%2520real-world%2520data%252C%2520we%250Amaintain%252090.63%2525%2520accuracy%2520with%2520just%25208%2525%2520labeled%2520samples%252C%2520which%2520is%25204.51%2525%2520higher%250Athan%2520the%2520best-performing%2520baseline.%2520This%2520illustrates%2520that%2520active%2520learning%250Aenables%2520the%2520same%2520level%2520of%2520classification%2520performance%2520on%2520single-photon%2520images%2520as%250Aon%2520classical%2520images%252C%2520opening%2520doors%2520to%2520large-scale%2520integration%2520of%2520single-photon%250Adata%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Label-efficient%20Single%20Photon%20Images%20Classification%20via%20Active%20Learning&entry.906535625=Zili%20Zhang%20and%20Ziting%20Wen%20and%20Yiheng%20Qiang%20and%20Hongzhou%20Dong%20and%20Wenle%20Dong%20and%20Xinyang%20Li%20and%20Xiaofan%20Wang%20and%20Xiaoqiang%20Ren&entry.1292438233=%20%20Single-photon%20LiDAR%20achieves%20high-precision%203D%20imaging%20in%20extreme%0Aenvironments%20through%20quantum-level%20photon%20detection%20technology.%20Current%0Aresearch%20primarily%20focuses%20on%20reconstructing%203D%20scenes%20from%20sparse%20photon%0Aevents%2C%20whereas%20the%20semantic%20interpretation%20of%20single-photon%20images%20remains%0Aunderexplored%2C%20due%20to%20high%20annotation%20costs%20and%20inefficient%20labeling%0Astrategies.%20This%20paper%20presents%20the%20first%20active%20learning%20framework%20for%0Asingle-photon%20image%20classification.%20The%20core%20contribution%20is%20an%20imaging%0Acondition-aware%20sampling%20strategy%20that%20integrates%20synthetic%20augmentation%20to%0Amodel%20variability%20across%20imaging%20conditions.%20By%20identifying%20samples%20where%20the%0Amodel%20is%20both%20uncertain%20and%20sensitive%20to%20these%20conditions%2C%20the%20proposed%20method%0Aselectively%20annotates%20only%20the%20most%20informative%20examples.%20Experiments%20on%20both%0Asynthetic%20and%20real-world%20datasets%20show%20that%20our%20approach%20outperforms%20all%0Abaselines%20and%20achieves%20high%20classification%20accuracy%20with%20significantly%20fewer%0Alabeled%20samples.%20Specifically%2C%20our%20approach%20achieves%2097%25%20accuracy%20on%20synthetic%0Asingle-photon%20data%20using%20only%201.5%25%20labeled%20samples.%20On%20real-world%20data%2C%20we%0Amaintain%2090.63%25%20accuracy%20with%20just%208%25%20labeled%20samples%2C%20which%20is%204.51%25%20higher%0Athan%20the%20best-performing%20baseline.%20This%20illustrates%20that%20active%20learning%0Aenables%20the%20same%20level%20of%20classification%20performance%20on%20single-photon%20images%20as%0Aon%20classical%20images%2C%20opening%20doors%20to%20large-scale%20integration%20of%20single-photon%0Adata%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04376v1&entry.124074799=Read"},
{"title": "MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition", "author": "Qiannan Fan and Zhuoyang Li and Jitong Li and Chenyang Cao", "abstract": "  With the continuous impact of epidemics, people have become accustomed to\nwearing masks. However, most current occluded face recognition (OFR) algorithms\nlack prior knowledge of occlusions, resulting in poor performance when dealing\nwith occluded faces of varying types and severity in reality. Recognizing\noccluded faces is still a significant challenge, which greatly affects the\nconvenience of people's daily lives. In this paper, we propose an\nidentity-gated mixture of diffusion experts (MoDE) for OFR. Each\ndiffusion-based generative expert estimates one possible complete image for\noccluded faces. Considering the random sampling process of the diffusion model,\nwhich introduces inevitable differences and variations between the inpainted\nfaces and the real ones. To ensemble effective information from\nmulti-reconstructed faces, we introduce an identity-gating network to evaluate\nthe contribution of each reconstructed face to the identity and adaptively\nintegrate the predictions in the decision space. Moreover, our MoDE is a\nplug-and-play module for most existing face recognition models. Extensive\nexperiments on three public face datasets and two datasets in the wild validate\nour advanced performance for various occlusions in comparison with the\ncompeting methods.\n", "link": "http://arxiv.org/abs/2505.04306v1", "date": "2025-05-07", "relevancy": 2.2968, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.585}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5741}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoDE%3A%20Mixture%20of%20Diffusion%20Experts%20for%20Any%20Occluded%20Face%20Recognition&body=Title%3A%20MoDE%3A%20Mixture%20of%20Diffusion%20Experts%20for%20Any%20Occluded%20Face%20Recognition%0AAuthor%3A%20Qiannan%20Fan%20and%20Zhuoyang%20Li%20and%20Jitong%20Li%20and%20Chenyang%20Cao%0AAbstract%3A%20%20%20With%20the%20continuous%20impact%20of%20epidemics%2C%20people%20have%20become%20accustomed%20to%0Awearing%20masks.%20However%2C%20most%20current%20occluded%20face%20recognition%20%28OFR%29%20algorithms%0Alack%20prior%20knowledge%20of%20occlusions%2C%20resulting%20in%20poor%20performance%20when%20dealing%0Awith%20occluded%20faces%20of%20varying%20types%20and%20severity%20in%20reality.%20Recognizing%0Aoccluded%20faces%20is%20still%20a%20significant%20challenge%2C%20which%20greatly%20affects%20the%0Aconvenience%20of%20people%27s%20daily%20lives.%20In%20this%20paper%2C%20we%20propose%20an%0Aidentity-gated%20mixture%20of%20diffusion%20experts%20%28MoDE%29%20for%20OFR.%20Each%0Adiffusion-based%20generative%20expert%20estimates%20one%20possible%20complete%20image%20for%0Aoccluded%20faces.%20Considering%20the%20random%20sampling%20process%20of%20the%20diffusion%20model%2C%0Awhich%20introduces%20inevitable%20differences%20and%20variations%20between%20the%20inpainted%0Afaces%20and%20the%20real%20ones.%20To%20ensemble%20effective%20information%20from%0Amulti-reconstructed%20faces%2C%20we%20introduce%20an%20identity-gating%20network%20to%20evaluate%0Athe%20contribution%20of%20each%20reconstructed%20face%20to%20the%20identity%20and%20adaptively%0Aintegrate%20the%20predictions%20in%20the%20decision%20space.%20Moreover%2C%20our%20MoDE%20is%20a%0Aplug-and-play%20module%20for%20most%20existing%20face%20recognition%20models.%20Extensive%0Aexperiments%20on%20three%20public%20face%20datasets%20and%20two%20datasets%20in%20the%20wild%20validate%0Aour%20advanced%20performance%20for%20various%20occlusions%20in%20comparison%20with%20the%0Acompeting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04306v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoDE%253A%2520Mixture%2520of%2520Diffusion%2520Experts%2520for%2520Any%2520Occluded%2520Face%2520Recognition%26entry.906535625%3DQiannan%2520Fan%2520and%2520Zhuoyang%2520Li%2520and%2520Jitong%2520Li%2520and%2520Chenyang%2520Cao%26entry.1292438233%3D%2520%2520With%2520the%2520continuous%2520impact%2520of%2520epidemics%252C%2520people%2520have%2520become%2520accustomed%2520to%250Awearing%2520masks.%2520However%252C%2520most%2520current%2520occluded%2520face%2520recognition%2520%2528OFR%2529%2520algorithms%250Alack%2520prior%2520knowledge%2520of%2520occlusions%252C%2520resulting%2520in%2520poor%2520performance%2520when%2520dealing%250Awith%2520occluded%2520faces%2520of%2520varying%2520types%2520and%2520severity%2520in%2520reality.%2520Recognizing%250Aoccluded%2520faces%2520is%2520still%2520a%2520significant%2520challenge%252C%2520which%2520greatly%2520affects%2520the%250Aconvenience%2520of%2520people%2527s%2520daily%2520lives.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aidentity-gated%2520mixture%2520of%2520diffusion%2520experts%2520%2528MoDE%2529%2520for%2520OFR.%2520Each%250Adiffusion-based%2520generative%2520expert%2520estimates%2520one%2520possible%2520complete%2520image%2520for%250Aoccluded%2520faces.%2520Considering%2520the%2520random%2520sampling%2520process%2520of%2520the%2520diffusion%2520model%252C%250Awhich%2520introduces%2520inevitable%2520differences%2520and%2520variations%2520between%2520the%2520inpainted%250Afaces%2520and%2520the%2520real%2520ones.%2520To%2520ensemble%2520effective%2520information%2520from%250Amulti-reconstructed%2520faces%252C%2520we%2520introduce%2520an%2520identity-gating%2520network%2520to%2520evaluate%250Athe%2520contribution%2520of%2520each%2520reconstructed%2520face%2520to%2520the%2520identity%2520and%2520adaptively%250Aintegrate%2520the%2520predictions%2520in%2520the%2520decision%2520space.%2520Moreover%252C%2520our%2520MoDE%2520is%2520a%250Aplug-and-play%2520module%2520for%2520most%2520existing%2520face%2520recognition%2520models.%2520Extensive%250Aexperiments%2520on%2520three%2520public%2520face%2520datasets%2520and%2520two%2520datasets%2520in%2520the%2520wild%2520validate%250Aour%2520advanced%2520performance%2520for%2520various%2520occlusions%2520in%2520comparison%2520with%2520the%250Acompeting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04306v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoDE%3A%20Mixture%20of%20Diffusion%20Experts%20for%20Any%20Occluded%20Face%20Recognition&entry.906535625=Qiannan%20Fan%20and%20Zhuoyang%20Li%20and%20Jitong%20Li%20and%20Chenyang%20Cao&entry.1292438233=%20%20With%20the%20continuous%20impact%20of%20epidemics%2C%20people%20have%20become%20accustomed%20to%0Awearing%20masks.%20However%2C%20most%20current%20occluded%20face%20recognition%20%28OFR%29%20algorithms%0Alack%20prior%20knowledge%20of%20occlusions%2C%20resulting%20in%20poor%20performance%20when%20dealing%0Awith%20occluded%20faces%20of%20varying%20types%20and%20severity%20in%20reality.%20Recognizing%0Aoccluded%20faces%20is%20still%20a%20significant%20challenge%2C%20which%20greatly%20affects%20the%0Aconvenience%20of%20people%27s%20daily%20lives.%20In%20this%20paper%2C%20we%20propose%20an%0Aidentity-gated%20mixture%20of%20diffusion%20experts%20%28MoDE%29%20for%20OFR.%20Each%0Adiffusion-based%20generative%20expert%20estimates%20one%20possible%20complete%20image%20for%0Aoccluded%20faces.%20Considering%20the%20random%20sampling%20process%20of%20the%20diffusion%20model%2C%0Awhich%20introduces%20inevitable%20differences%20and%20variations%20between%20the%20inpainted%0Afaces%20and%20the%20real%20ones.%20To%20ensemble%20effective%20information%20from%0Amulti-reconstructed%20faces%2C%20we%20introduce%20an%20identity-gating%20network%20to%20evaluate%0Athe%20contribution%20of%20each%20reconstructed%20face%20to%20the%20identity%20and%20adaptively%0Aintegrate%20the%20predictions%20in%20the%20decision%20space.%20Moreover%2C%20our%20MoDE%20is%20a%0Aplug-and-play%20module%20for%20most%20existing%20face%20recognition%20models.%20Extensive%0Aexperiments%20on%20three%20public%20face%20datasets%20and%20two%20datasets%20in%20the%20wild%20validate%0Aour%20advanced%20performance%20for%20various%20occlusions%20in%20comparison%20with%20the%0Acompeting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04306v1&entry.124074799=Read"},
{"title": "Multi-Robot Motion Planning with Diffusion Models", "author": "Yorai Shaoul and Itamar Mishani and Shivam Vats and Jiaoyang Li and Maxim Likhachev", "abstract": "  Diffusion models have recently been successfully applied to a wide range of\nrobotics applications for learning complex multi-modal behaviors from data.\nHowever, prior works have mostly been confined to single-robot and small-scale\nenvironments due to the high sample complexity of learning multi-robot\ndiffusion models. In this paper, we propose a method for generating\ncollision-free multi-robot trajectories that conform to underlying data\ndistributions while using only single-robot data. Our algorithm, Multi-robot\nMulti-model planning Diffusion (MMD), does so by combining learned diffusion\nmodels with classical search-based techniques -- generating data-driven motions\nunder collision constraints. Scaling further, we show how to compose multiple\ndiffusion models to plan in large environments where a single diffusion model\nfails to generalize well. We demonstrate the effectiveness of our approach in\nplanning for dozens of robots in a variety of simulated scenarios motivated by\nlogistics environments. View video demonstrations and code at:\nhttps://multi-robot-diffusion.github.io/.\n", "link": "http://arxiv.org/abs/2410.03072v2", "date": "2025-05-07", "relevancy": 2.2832, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5951}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5874}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Robot%20Motion%20Planning%20with%20Diffusion%20Models&body=Title%3A%20Multi-Robot%20Motion%20Planning%20with%20Diffusion%20Models%0AAuthor%3A%20Yorai%20Shaoul%20and%20Itamar%20Mishani%20and%20Shivam%20Vats%20and%20Jiaoyang%20Li%20and%20Maxim%20Likhachev%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20been%20successfully%20applied%20to%20a%20wide%20range%20of%0Arobotics%20applications%20for%20learning%20complex%20multi-modal%20behaviors%20from%20data.%0AHowever%2C%20prior%20works%20have%20mostly%20been%20confined%20to%20single-robot%20and%20small-scale%0Aenvironments%20due%20to%20the%20high%20sample%20complexity%20of%20learning%20multi-robot%0Adiffusion%20models.%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20generating%0Acollision-free%20multi-robot%20trajectories%20that%20conform%20to%20underlying%20data%0Adistributions%20while%20using%20only%20single-robot%20data.%20Our%20algorithm%2C%20Multi-robot%0AMulti-model%20planning%20Diffusion%20%28MMD%29%2C%20does%20so%20by%20combining%20learned%20diffusion%0Amodels%20with%20classical%20search-based%20techniques%20--%20generating%20data-driven%20motions%0Aunder%20collision%20constraints.%20Scaling%20further%2C%20we%20show%20how%20to%20compose%20multiple%0Adiffusion%20models%20to%20plan%20in%20large%20environments%20where%20a%20single%20diffusion%20model%0Afails%20to%20generalize%20well.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%0Aplanning%20for%20dozens%20of%20robots%20in%20a%20variety%20of%20simulated%20scenarios%20motivated%20by%0Alogistics%20environments.%20View%20video%20demonstrations%20and%20code%20at%3A%0Ahttps%3A//multi-robot-diffusion.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03072v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Robot%2520Motion%2520Planning%2520with%2520Diffusion%2520Models%26entry.906535625%3DYorai%2520Shaoul%2520and%2520Itamar%2520Mishani%2520and%2520Shivam%2520Vats%2520and%2520Jiaoyang%2520Li%2520and%2520Maxim%2520Likhachev%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520recently%2520been%2520successfully%2520applied%2520to%2520a%2520wide%2520range%2520of%250Arobotics%2520applications%2520for%2520learning%2520complex%2520multi-modal%2520behaviors%2520from%2520data.%250AHowever%252C%2520prior%2520works%2520have%2520mostly%2520been%2520confined%2520to%2520single-robot%2520and%2520small-scale%250Aenvironments%2520due%2520to%2520the%2520high%2520sample%2520complexity%2520of%2520learning%2520multi-robot%250Adiffusion%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520for%2520generating%250Acollision-free%2520multi-robot%2520trajectories%2520that%2520conform%2520to%2520underlying%2520data%250Adistributions%2520while%2520using%2520only%2520single-robot%2520data.%2520Our%2520algorithm%252C%2520Multi-robot%250AMulti-model%2520planning%2520Diffusion%2520%2528MMD%2529%252C%2520does%2520so%2520by%2520combining%2520learned%2520diffusion%250Amodels%2520with%2520classical%2520search-based%2520techniques%2520--%2520generating%2520data-driven%2520motions%250Aunder%2520collision%2520constraints.%2520Scaling%2520further%252C%2520we%2520show%2520how%2520to%2520compose%2520multiple%250Adiffusion%2520models%2520to%2520plan%2520in%2520large%2520environments%2520where%2520a%2520single%2520diffusion%2520model%250Afails%2520to%2520generalize%2520well.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%250Aplanning%2520for%2520dozens%2520of%2520robots%2520in%2520a%2520variety%2520of%2520simulated%2520scenarios%2520motivated%2520by%250Alogistics%2520environments.%2520View%2520video%2520demonstrations%2520and%2520code%2520at%253A%250Ahttps%253A//multi-robot-diffusion.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03072v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Robot%20Motion%20Planning%20with%20Diffusion%20Models&entry.906535625=Yorai%20Shaoul%20and%20Itamar%20Mishani%20and%20Shivam%20Vats%20and%20Jiaoyang%20Li%20and%20Maxim%20Likhachev&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20been%20successfully%20applied%20to%20a%20wide%20range%20of%0Arobotics%20applications%20for%20learning%20complex%20multi-modal%20behaviors%20from%20data.%0AHowever%2C%20prior%20works%20have%20mostly%20been%20confined%20to%20single-robot%20and%20small-scale%0Aenvironments%20due%20to%20the%20high%20sample%20complexity%20of%20learning%20multi-robot%0Adiffusion%20models.%20In%20this%20paper%2C%20we%20propose%20a%20method%20for%20generating%0Acollision-free%20multi-robot%20trajectories%20that%20conform%20to%20underlying%20data%0Adistributions%20while%20using%20only%20single-robot%20data.%20Our%20algorithm%2C%20Multi-robot%0AMulti-model%20planning%20Diffusion%20%28MMD%29%2C%20does%20so%20by%20combining%20learned%20diffusion%0Amodels%20with%20classical%20search-based%20techniques%20--%20generating%20data-driven%20motions%0Aunder%20collision%20constraints.%20Scaling%20further%2C%20we%20show%20how%20to%20compose%20multiple%0Adiffusion%20models%20to%20plan%20in%20large%20environments%20where%20a%20single%20diffusion%20model%0Afails%20to%20generalize%20well.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%0Aplanning%20for%20dozens%20of%20robots%20in%20a%20variety%20of%20simulated%20scenarios%20motivated%20by%0Alogistics%20environments.%20View%20video%20demonstrations%20and%20code%20at%3A%0Ahttps%3A//multi-robot-diffusion.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03072v2&entry.124074799=Read"},
{"title": "Purity Law for Generalizable Neural TSP Solvers", "author": "Wenzhao Liu and Haoran Li and Congying Han and Zicheng Zhang and Anqi Li and Tiande Guo", "abstract": "  Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference.\n", "link": "http://arxiv.org/abs/2505.04558v1", "date": "2025-05-07", "relevancy": 2.2696, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4357}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4281}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Purity%20Law%20for%20Generalizable%20Neural%20TSP%20Solvers&body=Title%3A%20Purity%20Law%20for%20Generalizable%20Neural%20TSP%20Solvers%0AAuthor%3A%20Wenzhao%20Liu%20and%20Haoran%20Li%20and%20Congying%20Han%20and%20Zicheng%20Zhang%20and%20Anqi%20Li%20and%20Tiande%20Guo%0AAbstract%3A%20%20%20Achieving%20generalization%20in%20neural%20approaches%20across%20different%20scales%20and%0Adistributions%20remains%20a%20significant%20challenge%20for%20the%20Traveling%20Salesman%0AProblem~%28TSP%29.%20A%20key%20obstacle%20is%20that%20neural%20networks%20often%20fail%20to%20learn%0Arobust%20principles%20for%20identifying%20universal%20patterns%20and%20deriving%20optimal%0Asolutions%20from%20diverse%20instances.%20In%20this%20paper%2C%20we%20first%20uncover%20Purity%20Law%0A%28PuLa%29%2C%20a%20fundamental%20structural%20principle%20for%20optimal%20TSP%20solutions%2C%20defining%0Athat%20edge%20prevalence%20grows%20exponentially%20with%20the%20sparsity%20of%20surrounding%0Avertices.%20Statistically%20validated%20across%20diverse%20instances%2C%20PuLa%20reveals%20a%0Aconsistent%20bias%20toward%20local%20sparsity%20in%20global%20optima.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20Purity%20Policy%20Optimization~%28PUPO%29%2C%20a%20novel%20training%0Aparadigm%20that%20explicitly%20aligns%20characteristics%20of%20neural%20solutions%20with%20PuLa%0Aduring%20the%20solution%20construction%20process%20to%20enhance%20generalization.%20Extensive%0Aexperiments%20demonstrate%20that%20PUPO%20can%20be%20seamlessly%20integrated%20with%20popular%0Aneural%20solvers%2C%20significantly%20enhancing%20their%20generalization%20performance%0Awithout%20incurring%20additional%20computational%20overhead%20during%20inference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPurity%2520Law%2520for%2520Generalizable%2520Neural%2520TSP%2520Solvers%26entry.906535625%3DWenzhao%2520Liu%2520and%2520Haoran%2520Li%2520and%2520Congying%2520Han%2520and%2520Zicheng%2520Zhang%2520and%2520Anqi%2520Li%2520and%2520Tiande%2520Guo%26entry.1292438233%3D%2520%2520Achieving%2520generalization%2520in%2520neural%2520approaches%2520across%2520different%2520scales%2520and%250Adistributions%2520remains%2520a%2520significant%2520challenge%2520for%2520the%2520Traveling%2520Salesman%250AProblem~%2528TSP%2529.%2520A%2520key%2520obstacle%2520is%2520that%2520neural%2520networks%2520often%2520fail%2520to%2520learn%250Arobust%2520principles%2520for%2520identifying%2520universal%2520patterns%2520and%2520deriving%2520optimal%250Asolutions%2520from%2520diverse%2520instances.%2520In%2520this%2520paper%252C%2520we%2520first%2520uncover%2520Purity%2520Law%250A%2528PuLa%2529%252C%2520a%2520fundamental%2520structural%2520principle%2520for%2520optimal%2520TSP%2520solutions%252C%2520defining%250Athat%2520edge%2520prevalence%2520grows%2520exponentially%2520with%2520the%2520sparsity%2520of%2520surrounding%250Avertices.%2520Statistically%2520validated%2520across%2520diverse%2520instances%252C%2520PuLa%2520reveals%2520a%250Aconsistent%2520bias%2520toward%2520local%2520sparsity%2520in%2520global%2520optima.%2520Building%2520on%2520this%250Ainsight%252C%2520we%2520propose%2520Purity%2520Policy%2520Optimization~%2528PUPO%2529%252C%2520a%2520novel%2520training%250Aparadigm%2520that%2520explicitly%2520aligns%2520characteristics%2520of%2520neural%2520solutions%2520with%2520PuLa%250Aduring%2520the%2520solution%2520construction%2520process%2520to%2520enhance%2520generalization.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520PUPO%2520can%2520be%2520seamlessly%2520integrated%2520with%2520popular%250Aneural%2520solvers%252C%2520significantly%2520enhancing%2520their%2520generalization%2520performance%250Awithout%2520incurring%2520additional%2520computational%2520overhead%2520during%2520inference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Purity%20Law%20for%20Generalizable%20Neural%20TSP%20Solvers&entry.906535625=Wenzhao%20Liu%20and%20Haoran%20Li%20and%20Congying%20Han%20and%20Zicheng%20Zhang%20and%20Anqi%20Li%20and%20Tiande%20Guo&entry.1292438233=%20%20Achieving%20generalization%20in%20neural%20approaches%20across%20different%20scales%20and%0Adistributions%20remains%20a%20significant%20challenge%20for%20the%20Traveling%20Salesman%0AProblem~%28TSP%29.%20A%20key%20obstacle%20is%20that%20neural%20networks%20often%20fail%20to%20learn%0Arobust%20principles%20for%20identifying%20universal%20patterns%20and%20deriving%20optimal%0Asolutions%20from%20diverse%20instances.%20In%20this%20paper%2C%20we%20first%20uncover%20Purity%20Law%0A%28PuLa%29%2C%20a%20fundamental%20structural%20principle%20for%20optimal%20TSP%20solutions%2C%20defining%0Athat%20edge%20prevalence%20grows%20exponentially%20with%20the%20sparsity%20of%20surrounding%0Avertices.%20Statistically%20validated%20across%20diverse%20instances%2C%20PuLa%20reveals%20a%0Aconsistent%20bias%20toward%20local%20sparsity%20in%20global%20optima.%20Building%20on%20this%0Ainsight%2C%20we%20propose%20Purity%20Policy%20Optimization~%28PUPO%29%2C%20a%20novel%20training%0Aparadigm%20that%20explicitly%20aligns%20characteristics%20of%20neural%20solutions%20with%20PuLa%0Aduring%20the%20solution%20construction%20process%20to%20enhance%20generalization.%20Extensive%0Aexperiments%20demonstrate%20that%20PUPO%20can%20be%20seamlessly%20integrated%20with%20popular%0Aneural%20solvers%2C%20significantly%20enhancing%20their%20generalization%20performance%0Awithout%20incurring%20additional%20computational%20overhead%20during%20inference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04558v1&entry.124074799=Read"},
{"title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation", "author": "Edward Humes and Xiaomin Lin and Uttej Kallakuri and Tinoosh Mohsenin", "abstract": "  Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU.\n", "link": "http://arxiv.org/abs/2505.04529v1", "date": "2025-05-07", "relevancy": 2.2621, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5931}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAFT%3A%20Robust%20Augmentation%20of%20FeaTures%20for%20Image%20Segmentation&body=Title%3A%20RAFT%3A%20Robust%20Augmentation%20of%20FeaTures%20for%20Image%20Segmentation%0AAuthor%3A%20Edward%20Humes%20and%20Xiaomin%20Lin%20and%20Uttej%20Kallakuri%20and%20Tinoosh%20Mohsenin%0AAbstract%3A%20%20%20Image%20segmentation%20is%20a%20powerful%20computer%20vision%20technique%20for%20scene%0Aunderstanding.%20However%2C%20real-world%20deployment%20is%20stymied%20by%20the%20need%20for%0Ahigh-quality%2C%20meticulously%20labeled%20datasets.%20Synthetic%20data%20provides%0Ahigh-quality%20labels%20while%20reducing%20the%20need%20for%20manual%20data%20collection%20and%0Aannotation.%20However%2C%20deep%20neural%20networks%20trained%20on%20synthetic%20data%20often%20face%0Athe%20Syn2Real%20problem%2C%20leading%20to%20poor%20performance%20in%20real-world%20deployments.%0A%20%20To%20mitigate%20the%20aforementioned%20gap%20in%20image%20segmentation%2C%20we%20propose%20RAFT%2C%20a%0Anovel%20framework%20for%20adapting%20image%20segmentation%20models%20using%20minimal%20labeled%0Areal-world%20data%20through%20data%20and%20feature%20augmentations%2C%20as%20well%20as%20active%0Alearning.%20To%20validate%20RAFT%2C%20we%20perform%20experiments%20on%20the%20synthetic-to-real%0A%22SYNTHIA-%3ECityscapes%22%20and%20%22GTAV-%3ECityscapes%22%20benchmarks.%20We%20managed%20to%20surpass%0Athe%20previous%20state%20of%20the%20art%2C%20HALO.%20SYNTHIA-%3ECityscapes%20experiences%20an%0Aimprovement%20in%20mIoU%2A%20upon%20domain%20adaptation%20of%202.1%25/79.9%25%2C%20and%20GTAV-%3ECityscapes%0Aexperiences%20a%200.4%25/78.2%25%20improvement%20in%20mIoU.%20Furthermore%2C%20we%20test%20our%20approach%0Aon%20the%20real-to-real%20benchmark%20of%20%22Cityscapes-%3EACDC%22%2C%20and%20again%20surpass%20HALO%2C%0Awith%20a%20gain%20in%20mIoU%20upon%20adaptation%20of%201.3%25/73.2%25.%20Finally%2C%20we%20examine%20the%0Aeffect%20of%20the%20allocated%20annotation%20budget%20and%20various%20components%20of%20RAFT%20upon%0Athe%20final%20transfer%20mIoU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAFT%253A%2520Robust%2520Augmentation%2520of%2520FeaTures%2520for%2520Image%2520Segmentation%26entry.906535625%3DEdward%2520Humes%2520and%2520Xiaomin%2520Lin%2520and%2520Uttej%2520Kallakuri%2520and%2520Tinoosh%2520Mohsenin%26entry.1292438233%3D%2520%2520Image%2520segmentation%2520is%2520a%2520powerful%2520computer%2520vision%2520technique%2520for%2520scene%250Aunderstanding.%2520However%252C%2520real-world%2520deployment%2520is%2520stymied%2520by%2520the%2520need%2520for%250Ahigh-quality%252C%2520meticulously%2520labeled%2520datasets.%2520Synthetic%2520data%2520provides%250Ahigh-quality%2520labels%2520while%2520reducing%2520the%2520need%2520for%2520manual%2520data%2520collection%2520and%250Aannotation.%2520However%252C%2520deep%2520neural%2520networks%2520trained%2520on%2520synthetic%2520data%2520often%2520face%250Athe%2520Syn2Real%2520problem%252C%2520leading%2520to%2520poor%2520performance%2520in%2520real-world%2520deployments.%250A%2520%2520To%2520mitigate%2520the%2520aforementioned%2520gap%2520in%2520image%2520segmentation%252C%2520we%2520propose%2520RAFT%252C%2520a%250Anovel%2520framework%2520for%2520adapting%2520image%2520segmentation%2520models%2520using%2520minimal%2520labeled%250Areal-world%2520data%2520through%2520data%2520and%2520feature%2520augmentations%252C%2520as%2520well%2520as%2520active%250Alearning.%2520To%2520validate%2520RAFT%252C%2520we%2520perform%2520experiments%2520on%2520the%2520synthetic-to-real%250A%2522SYNTHIA-%253ECityscapes%2522%2520and%2520%2522GTAV-%253ECityscapes%2522%2520benchmarks.%2520We%2520managed%2520to%2520surpass%250Athe%2520previous%2520state%2520of%2520the%2520art%252C%2520HALO.%2520SYNTHIA-%253ECityscapes%2520experiences%2520an%250Aimprovement%2520in%2520mIoU%252A%2520upon%2520domain%2520adaptation%2520of%25202.1%2525/79.9%2525%252C%2520and%2520GTAV-%253ECityscapes%250Aexperiences%2520a%25200.4%2525/78.2%2525%2520improvement%2520in%2520mIoU.%2520Furthermore%252C%2520we%2520test%2520our%2520approach%250Aon%2520the%2520real-to-real%2520benchmark%2520of%2520%2522Cityscapes-%253EACDC%2522%252C%2520and%2520again%2520surpass%2520HALO%252C%250Awith%2520a%2520gain%2520in%2520mIoU%2520upon%2520adaptation%2520of%25201.3%2525/73.2%2525.%2520Finally%252C%2520we%2520examine%2520the%250Aeffect%2520of%2520the%2520allocated%2520annotation%2520budget%2520and%2520various%2520components%2520of%2520RAFT%2520upon%250Athe%2520final%2520transfer%2520mIoU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAFT%3A%20Robust%20Augmentation%20of%20FeaTures%20for%20Image%20Segmentation&entry.906535625=Edward%20Humes%20and%20Xiaomin%20Lin%20and%20Uttej%20Kallakuri%20and%20Tinoosh%20Mohsenin&entry.1292438233=%20%20Image%20segmentation%20is%20a%20powerful%20computer%20vision%20technique%20for%20scene%0Aunderstanding.%20However%2C%20real-world%20deployment%20is%20stymied%20by%20the%20need%20for%0Ahigh-quality%2C%20meticulously%20labeled%20datasets.%20Synthetic%20data%20provides%0Ahigh-quality%20labels%20while%20reducing%20the%20need%20for%20manual%20data%20collection%20and%0Aannotation.%20However%2C%20deep%20neural%20networks%20trained%20on%20synthetic%20data%20often%20face%0Athe%20Syn2Real%20problem%2C%20leading%20to%20poor%20performance%20in%20real-world%20deployments.%0A%20%20To%20mitigate%20the%20aforementioned%20gap%20in%20image%20segmentation%2C%20we%20propose%20RAFT%2C%20a%0Anovel%20framework%20for%20adapting%20image%20segmentation%20models%20using%20minimal%20labeled%0Areal-world%20data%20through%20data%20and%20feature%20augmentations%2C%20as%20well%20as%20active%0Alearning.%20To%20validate%20RAFT%2C%20we%20perform%20experiments%20on%20the%20synthetic-to-real%0A%22SYNTHIA-%3ECityscapes%22%20and%20%22GTAV-%3ECityscapes%22%20benchmarks.%20We%20managed%20to%20surpass%0Athe%20previous%20state%20of%20the%20art%2C%20HALO.%20SYNTHIA-%3ECityscapes%20experiences%20an%0Aimprovement%20in%20mIoU%2A%20upon%20domain%20adaptation%20of%202.1%25/79.9%25%2C%20and%20GTAV-%3ECityscapes%0Aexperiences%20a%200.4%25/78.2%25%20improvement%20in%20mIoU.%20Furthermore%2C%20we%20test%20our%20approach%0Aon%20the%20real-to-real%20benchmark%20of%20%22Cityscapes-%3EACDC%22%2C%20and%20again%20surpass%20HALO%2C%0Awith%20a%20gain%20in%20mIoU%20upon%20adaptation%20of%201.3%25/73.2%25.%20Finally%2C%20we%20examine%20the%0Aeffect%20of%20the%20allocated%20annotation%20budget%20and%20various%20components%20of%20RAFT%20upon%0Athe%20final%20transfer%20mIoU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04529v1&entry.124074799=Read"},
{"title": "Localized Diffusion Models for High Dimensional Distributions Generation", "author": "Georg A. Gottwald and Shuigen Liu and Youssef Marzouk and Sebastian Reich and Xin T. Tong", "abstract": "  Diffusion models are the state-of-the-art tools for various generative tasks.\nHowever, estimating high-dimensional score functions makes them potentially\nsuffer from the curse of dimensionality (CoD). This underscores the importance\nof better understanding and exploiting low-dimensional structure in the target\ndistribution. In this work, we consider locality structure, which describes\nsparse dependencies between model components. Under locality structure, the\nscore function is effectively low-dimensional, so that it can be estimated by a\nlocalized neural network with significantly reduced sample complexity. This\nmotivates the localized diffusion model, where a localized score matching loss\nis used to train the score function within a localized hypothesis space. We\nprove that such localization enables diffusion models to circumvent CoD, at the\nprice of additional localization error. Under realistic sample size scaling, we\nshow both theoretically and numerically that a moderate localization radius can\nbalance the statistical and localization error, leading to a better overall\nperformance. The localized structure also facilitates parallel training of\ndiffusion models, making it potentially more efficient for large-scale\napplications.\n", "link": "http://arxiv.org/abs/2505.04417v1", "date": "2025-05-07", "relevancy": 2.2607, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6099}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5649}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localized%20Diffusion%20Models%20for%20High%20Dimensional%20Distributions%20Generation&body=Title%3A%20Localized%20Diffusion%20Models%20for%20High%20Dimensional%20Distributions%20Generation%0AAuthor%3A%20Georg%20A.%20Gottwald%20and%20Shuigen%20Liu%20and%20Youssef%20Marzouk%20and%20Sebastian%20Reich%20and%20Xin%20T.%20Tong%0AAbstract%3A%20%20%20Diffusion%20models%20are%20the%20state-of-the-art%20tools%20for%20various%20generative%20tasks.%0AHowever%2C%20estimating%20high-dimensional%20score%20functions%20makes%20them%20potentially%0Asuffer%20from%20the%20curse%20of%20dimensionality%20%28CoD%29.%20This%20underscores%20the%20importance%0Aof%20better%20understanding%20and%20exploiting%20low-dimensional%20structure%20in%20the%20target%0Adistribution.%20In%20this%20work%2C%20we%20consider%20locality%20structure%2C%20which%20describes%0Asparse%20dependencies%20between%20model%20components.%20Under%20locality%20structure%2C%20the%0Ascore%20function%20is%20effectively%20low-dimensional%2C%20so%20that%20it%20can%20be%20estimated%20by%20a%0Alocalized%20neural%20network%20with%20significantly%20reduced%20sample%20complexity.%20This%0Amotivates%20the%20localized%20diffusion%20model%2C%20where%20a%20localized%20score%20matching%20loss%0Ais%20used%20to%20train%20the%20score%20function%20within%20a%20localized%20hypothesis%20space.%20We%0Aprove%20that%20such%20localization%20enables%20diffusion%20models%20to%20circumvent%20CoD%2C%20at%20the%0Aprice%20of%20additional%20localization%20error.%20Under%20realistic%20sample%20size%20scaling%2C%20we%0Ashow%20both%20theoretically%20and%20numerically%20that%20a%20moderate%20localization%20radius%20can%0Abalance%20the%20statistical%20and%20localization%20error%2C%20leading%20to%20a%20better%20overall%0Aperformance.%20The%20localized%20structure%20also%20facilitates%20parallel%20training%20of%0Adiffusion%20models%2C%20making%20it%20potentially%20more%20efficient%20for%20large-scale%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalized%2520Diffusion%2520Models%2520for%2520High%2520Dimensional%2520Distributions%2520Generation%26entry.906535625%3DGeorg%2520A.%2520Gottwald%2520and%2520Shuigen%2520Liu%2520and%2520Youssef%2520Marzouk%2520and%2520Sebastian%2520Reich%2520and%2520Xin%2520T.%2520Tong%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520are%2520the%2520state-of-the-art%2520tools%2520for%2520various%2520generative%2520tasks.%250AHowever%252C%2520estimating%2520high-dimensional%2520score%2520functions%2520makes%2520them%2520potentially%250Asuffer%2520from%2520the%2520curse%2520of%2520dimensionality%2520%2528CoD%2529.%2520This%2520underscores%2520the%2520importance%250Aof%2520better%2520understanding%2520and%2520exploiting%2520low-dimensional%2520structure%2520in%2520the%2520target%250Adistribution.%2520In%2520this%2520work%252C%2520we%2520consider%2520locality%2520structure%252C%2520which%2520describes%250Asparse%2520dependencies%2520between%2520model%2520components.%2520Under%2520locality%2520structure%252C%2520the%250Ascore%2520function%2520is%2520effectively%2520low-dimensional%252C%2520so%2520that%2520it%2520can%2520be%2520estimated%2520by%2520a%250Alocalized%2520neural%2520network%2520with%2520significantly%2520reduced%2520sample%2520complexity.%2520This%250Amotivates%2520the%2520localized%2520diffusion%2520model%252C%2520where%2520a%2520localized%2520score%2520matching%2520loss%250Ais%2520used%2520to%2520train%2520the%2520score%2520function%2520within%2520a%2520localized%2520hypothesis%2520space.%2520We%250Aprove%2520that%2520such%2520localization%2520enables%2520diffusion%2520models%2520to%2520circumvent%2520CoD%252C%2520at%2520the%250Aprice%2520of%2520additional%2520localization%2520error.%2520Under%2520realistic%2520sample%2520size%2520scaling%252C%2520we%250Ashow%2520both%2520theoretically%2520and%2520numerically%2520that%2520a%2520moderate%2520localization%2520radius%2520can%250Abalance%2520the%2520statistical%2520and%2520localization%2520error%252C%2520leading%2520to%2520a%2520better%2520overall%250Aperformance.%2520The%2520localized%2520structure%2520also%2520facilitates%2520parallel%2520training%2520of%250Adiffusion%2520models%252C%2520making%2520it%2520potentially%2520more%2520efficient%2520for%2520large-scale%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localized%20Diffusion%20Models%20for%20High%20Dimensional%20Distributions%20Generation&entry.906535625=Georg%20A.%20Gottwald%20and%20Shuigen%20Liu%20and%20Youssef%20Marzouk%20and%20Sebastian%20Reich%20and%20Xin%20T.%20Tong&entry.1292438233=%20%20Diffusion%20models%20are%20the%20state-of-the-art%20tools%20for%20various%20generative%20tasks.%0AHowever%2C%20estimating%20high-dimensional%20score%20functions%20makes%20them%20potentially%0Asuffer%20from%20the%20curse%20of%20dimensionality%20%28CoD%29.%20This%20underscores%20the%20importance%0Aof%20better%20understanding%20and%20exploiting%20low-dimensional%20structure%20in%20the%20target%0Adistribution.%20In%20this%20work%2C%20we%20consider%20locality%20structure%2C%20which%20describes%0Asparse%20dependencies%20between%20model%20components.%20Under%20locality%20structure%2C%20the%0Ascore%20function%20is%20effectively%20low-dimensional%2C%20so%20that%20it%20can%20be%20estimated%20by%20a%0Alocalized%20neural%20network%20with%20significantly%20reduced%20sample%20complexity.%20This%0Amotivates%20the%20localized%20diffusion%20model%2C%20where%20a%20localized%20score%20matching%20loss%0Ais%20used%20to%20train%20the%20score%20function%20within%20a%20localized%20hypothesis%20space.%20We%0Aprove%20that%20such%20localization%20enables%20diffusion%20models%20to%20circumvent%20CoD%2C%20at%20the%0Aprice%20of%20additional%20localization%20error.%20Under%20realistic%20sample%20size%20scaling%2C%20we%0Ashow%20both%20theoretically%20and%20numerically%20that%20a%20moderate%20localization%20radius%20can%0Abalance%20the%20statistical%20and%20localization%20error%2C%20leading%20to%20a%20better%20overall%0Aperformance.%20The%20localized%20structure%20also%20facilitates%20parallel%20training%20of%0Adiffusion%20models%2C%20making%20it%20potentially%20more%20efficient%20for%20large-scale%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04417v1&entry.124074799=Read"},
{"title": "MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action\n  Recognition and Transformer-based Sensor Fusion", "author": "Trung Thanh Nguyen and Yasutomo Kawanishi and Vijay John and Takahiro Komamizu and Ichiro Ide", "abstract": "  Multi-modal multi-view action recognition is a rapidly growing field in\ncomputer vision, offering significant potential for applications in\nsurveillance. However, current datasets often fail to address real-world\nchallenges such as wide-area distributed settings, asynchronous data streams,\nand the lack of frame-level annotations. Furthermore, existing methods face\ndifficulties in effectively modeling inter-view relationships and enhancing\nspatial feature learning. In this paper, we introduce the MultiSensor-Home\ndataset, a novel benchmark designed for comprehensive action recognition in\nhome environments, and also propose the Multi-modal Multi-view\nTransformer-based Sensor Fusion (MultiTSF) method. The proposed\nMultiSensor-Home dataset features untrimmed videos captured by distributed\nsensors, providing high-resolution RGB and audio data along with detailed\nmulti-view frame-level action labels. The proposed MultiTSF method leverages a\nTransformer-based fusion mechanism to dynamically model inter-view\nrelationships. Furthermore, the proposed method integrates a human detection\nmodule to enhance spatial feature learning, guiding the model to prioritize\nframes with human activity to enhance action the recognition accuracy.\nExperiments on the proposed MultiSensor-Home and the existing MM-Office\ndatasets demonstrate the superiority of MultiTSF over the state-of-the-art\nmethods. Quantitative and qualitative results highlight the effectiveness of\nthe proposed method in advancing real-world multi-modal multi-view action\nrecognition. The source code is available at\nhttps://github.com/thanhhff/MultiTSF.\n", "link": "http://arxiv.org/abs/2504.02287v3", "date": "2025-05-07", "relevancy": 2.2576, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5705}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiSensor-Home%3A%20A%20Wide-area%20Multi-modal%20Multi-view%20Dataset%20for%20Action%0A%20%20Recognition%20and%20Transformer-based%20Sensor%20Fusion&body=Title%3A%20MultiSensor-Home%3A%20A%20Wide-area%20Multi-modal%20Multi-view%20Dataset%20for%20Action%0A%20%20Recognition%20and%20Transformer-based%20Sensor%20Fusion%0AAuthor%3A%20Trung%20Thanh%20Nguyen%20and%20Yasutomo%20Kawanishi%20and%20Vijay%20John%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide%0AAbstract%3A%20%20%20Multi-modal%20multi-view%20action%20recognition%20is%20a%20rapidly%20growing%20field%20in%0Acomputer%20vision%2C%20offering%20significant%20potential%20for%20applications%20in%0Asurveillance.%20However%2C%20current%20datasets%20often%20fail%20to%20address%20real-world%0Achallenges%20such%20as%20wide-area%20distributed%20settings%2C%20asynchronous%20data%20streams%2C%0Aand%20the%20lack%20of%20frame-level%20annotations.%20Furthermore%2C%20existing%20methods%20face%0Adifficulties%20in%20effectively%20modeling%20inter-view%20relationships%20and%20enhancing%0Aspatial%20feature%20learning.%20In%20this%20paper%2C%20we%20introduce%20the%20MultiSensor-Home%0Adataset%2C%20a%20novel%20benchmark%20designed%20for%20comprehensive%20action%20recognition%20in%0Ahome%20environments%2C%20and%20also%20propose%20the%20Multi-modal%20Multi-view%0ATransformer-based%20Sensor%20Fusion%20%28MultiTSF%29%20method.%20The%20proposed%0AMultiSensor-Home%20dataset%20features%20untrimmed%20videos%20captured%20by%20distributed%0Asensors%2C%20providing%20high-resolution%20RGB%20and%20audio%20data%20along%20with%20detailed%0Amulti-view%20frame-level%20action%20labels.%20The%20proposed%20MultiTSF%20method%20leverages%20a%0ATransformer-based%20fusion%20mechanism%20to%20dynamically%20model%20inter-view%0Arelationships.%20Furthermore%2C%20the%20proposed%20method%20integrates%20a%20human%20detection%0Amodule%20to%20enhance%20spatial%20feature%20learning%2C%20guiding%20the%20model%20to%20prioritize%0Aframes%20with%20human%20activity%20to%20enhance%20action%20the%20recognition%20accuracy.%0AExperiments%20on%20the%20proposed%20MultiSensor-Home%20and%20the%20existing%20MM-Office%0Adatasets%20demonstrate%20the%20superiority%20of%20MultiTSF%20over%20the%20state-of-the-art%0Amethods.%20Quantitative%20and%20qualitative%20results%20highlight%20the%20effectiveness%20of%0Athe%20proposed%20method%20in%20advancing%20real-world%20multi-modal%20multi-view%20action%0Arecognition.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/thanhhff/MultiTSF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.02287v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiSensor-Home%253A%2520A%2520Wide-area%2520Multi-modal%2520Multi-view%2520Dataset%2520for%2520Action%250A%2520%2520Recognition%2520and%2520Transformer-based%2520Sensor%2520Fusion%26entry.906535625%3DTrung%2520Thanh%2520Nguyen%2520and%2520Yasutomo%2520Kawanishi%2520and%2520Vijay%2520John%2520and%2520Takahiro%2520Komamizu%2520and%2520Ichiro%2520Ide%26entry.1292438233%3D%2520%2520Multi-modal%2520multi-view%2520action%2520recognition%2520is%2520a%2520rapidly%2520growing%2520field%2520in%250Acomputer%2520vision%252C%2520offering%2520significant%2520potential%2520for%2520applications%2520in%250Asurveillance.%2520However%252C%2520current%2520datasets%2520often%2520fail%2520to%2520address%2520real-world%250Achallenges%2520such%2520as%2520wide-area%2520distributed%2520settings%252C%2520asynchronous%2520data%2520streams%252C%250Aand%2520the%2520lack%2520of%2520frame-level%2520annotations.%2520Furthermore%252C%2520existing%2520methods%2520face%250Adifficulties%2520in%2520effectively%2520modeling%2520inter-view%2520relationships%2520and%2520enhancing%250Aspatial%2520feature%2520learning.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520MultiSensor-Home%250Adataset%252C%2520a%2520novel%2520benchmark%2520designed%2520for%2520comprehensive%2520action%2520recognition%2520in%250Ahome%2520environments%252C%2520and%2520also%2520propose%2520the%2520Multi-modal%2520Multi-view%250ATransformer-based%2520Sensor%2520Fusion%2520%2528MultiTSF%2529%2520method.%2520The%2520proposed%250AMultiSensor-Home%2520dataset%2520features%2520untrimmed%2520videos%2520captured%2520by%2520distributed%250Asensors%252C%2520providing%2520high-resolution%2520RGB%2520and%2520audio%2520data%2520along%2520with%2520detailed%250Amulti-view%2520frame-level%2520action%2520labels.%2520The%2520proposed%2520MultiTSF%2520method%2520leverages%2520a%250ATransformer-based%2520fusion%2520mechanism%2520to%2520dynamically%2520model%2520inter-view%250Arelationships.%2520Furthermore%252C%2520the%2520proposed%2520method%2520integrates%2520a%2520human%2520detection%250Amodule%2520to%2520enhance%2520spatial%2520feature%2520learning%252C%2520guiding%2520the%2520model%2520to%2520prioritize%250Aframes%2520with%2520human%2520activity%2520to%2520enhance%2520action%2520the%2520recognition%2520accuracy.%250AExperiments%2520on%2520the%2520proposed%2520MultiSensor-Home%2520and%2520the%2520existing%2520MM-Office%250Adatasets%2520demonstrate%2520the%2520superiority%2520of%2520MultiTSF%2520over%2520the%2520state-of-the-art%250Amethods.%2520Quantitative%2520and%2520qualitative%2520results%2520highlight%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520method%2520in%2520advancing%2520real-world%2520multi-modal%2520multi-view%2520action%250Arecognition.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/thanhhff/MultiTSF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.02287v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiSensor-Home%3A%20A%20Wide-area%20Multi-modal%20Multi-view%20Dataset%20for%20Action%0A%20%20Recognition%20and%20Transformer-based%20Sensor%20Fusion&entry.906535625=Trung%20Thanh%20Nguyen%20and%20Yasutomo%20Kawanishi%20and%20Vijay%20John%20and%20Takahiro%20Komamizu%20and%20Ichiro%20Ide&entry.1292438233=%20%20Multi-modal%20multi-view%20action%20recognition%20is%20a%20rapidly%20growing%20field%20in%0Acomputer%20vision%2C%20offering%20significant%20potential%20for%20applications%20in%0Asurveillance.%20However%2C%20current%20datasets%20often%20fail%20to%20address%20real-world%0Achallenges%20such%20as%20wide-area%20distributed%20settings%2C%20asynchronous%20data%20streams%2C%0Aand%20the%20lack%20of%20frame-level%20annotations.%20Furthermore%2C%20existing%20methods%20face%0Adifficulties%20in%20effectively%20modeling%20inter-view%20relationships%20and%20enhancing%0Aspatial%20feature%20learning.%20In%20this%20paper%2C%20we%20introduce%20the%20MultiSensor-Home%0Adataset%2C%20a%20novel%20benchmark%20designed%20for%20comprehensive%20action%20recognition%20in%0Ahome%20environments%2C%20and%20also%20propose%20the%20Multi-modal%20Multi-view%0ATransformer-based%20Sensor%20Fusion%20%28MultiTSF%29%20method.%20The%20proposed%0AMultiSensor-Home%20dataset%20features%20untrimmed%20videos%20captured%20by%20distributed%0Asensors%2C%20providing%20high-resolution%20RGB%20and%20audio%20data%20along%20with%20detailed%0Amulti-view%20frame-level%20action%20labels.%20The%20proposed%20MultiTSF%20method%20leverages%20a%0ATransformer-based%20fusion%20mechanism%20to%20dynamically%20model%20inter-view%0Arelationships.%20Furthermore%2C%20the%20proposed%20method%20integrates%20a%20human%20detection%0Amodule%20to%20enhance%20spatial%20feature%20learning%2C%20guiding%20the%20model%20to%20prioritize%0Aframes%20with%20human%20activity%20to%20enhance%20action%20the%20recognition%20accuracy.%0AExperiments%20on%20the%20proposed%20MultiSensor-Home%20and%20the%20existing%20MM-Office%0Adatasets%20demonstrate%20the%20superiority%20of%20MultiTSF%20over%20the%20state-of-the-art%0Amethods.%20Quantitative%20and%20qualitative%20results%20highlight%20the%20effectiveness%20of%0Athe%20proposed%20method%20in%20advancing%20real-world%20multi-modal%20multi-view%20action%0Arecognition.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/thanhhff/MultiTSF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.02287v3&entry.124074799=Read"},
{"title": "From Two Sample Testing to Singular Gaussian Discrimination", "author": "Leonardo V. Santoro and Kartik G. Waghmare and Victor M. Panaretos", "abstract": "  We establish that testing for the equality of two probability measures on a\ngeneral separable and compact metric space is equivalent to testing for the\nsingularity between two corresponding Gaussian measures on a suitable\nReproducing Kernel Hilbert Space. The corresponding Gaussians are defined via\nthe notion of kernel mean and covariance embedding of a probability measure.\nDiscerning two singular Gaussians is fundamentally simpler from an\ninformation-theoretic perspective than non-parametric two-sample testing,\nparticularly in high-dimensional settings. Our proof leverages the\nFeldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert\nspaces, and shows that discrepancies between distributions are heavily\nmagnified through their corresponding Gaussian embeddings: at a population\nlevel, distinct probability measures lead to essentially separated Gaussian\nembeddings. This appears to be a new instance of the blessing of dimensionality\nthat can be harnessed for the design of efficient inference tools in great\ngenerality.\n", "link": "http://arxiv.org/abs/2505.04613v1", "date": "2025-05-07", "relevancy": 2.2561, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4563}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4535}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Two%20Sample%20Testing%20to%20Singular%20Gaussian%20Discrimination&body=Title%3A%20From%20Two%20Sample%20Testing%20to%20Singular%20Gaussian%20Discrimination%0AAuthor%3A%20Leonardo%20V.%20Santoro%20and%20Kartik%20G.%20Waghmare%20and%20Victor%20M.%20Panaretos%0AAbstract%3A%20%20%20We%20establish%20that%20testing%20for%20the%20equality%20of%20two%20probability%20measures%20on%20a%0Ageneral%20separable%20and%20compact%20metric%20space%20is%20equivalent%20to%20testing%20for%20the%0Asingularity%20between%20two%20corresponding%20Gaussian%20measures%20on%20a%20suitable%0AReproducing%20Kernel%20Hilbert%20Space.%20The%20corresponding%20Gaussians%20are%20defined%20via%0Athe%20notion%20of%20kernel%20mean%20and%20covariance%20embedding%20of%20a%20probability%20measure.%0ADiscerning%20two%20singular%20Gaussians%20is%20fundamentally%20simpler%20from%20an%0Ainformation-theoretic%20perspective%20than%20non-parametric%20two-sample%20testing%2C%0Aparticularly%20in%20high-dimensional%20settings.%20Our%20proof%20leverages%20the%0AFeldman-Hajek%20criterion%20for%20singularity/equivalence%20of%20Gaussians%20on%20Hilbert%0Aspaces%2C%20and%20shows%20that%20discrepancies%20between%20distributions%20are%20heavily%0Amagnified%20through%20their%20corresponding%20Gaussian%20embeddings%3A%20at%20a%20population%0Alevel%2C%20distinct%20probability%20measures%20lead%20to%20essentially%20separated%20Gaussian%0Aembeddings.%20This%20appears%20to%20be%20a%20new%20instance%20of%20the%20blessing%20of%20dimensionality%0Athat%20can%20be%20harnessed%20for%20the%20design%20of%20efficient%20inference%20tools%20in%20great%0Agenerality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Two%2520Sample%2520Testing%2520to%2520Singular%2520Gaussian%2520Discrimination%26entry.906535625%3DLeonardo%2520V.%2520Santoro%2520and%2520Kartik%2520G.%2520Waghmare%2520and%2520Victor%2520M.%2520Panaretos%26entry.1292438233%3D%2520%2520We%2520establish%2520that%2520testing%2520for%2520the%2520equality%2520of%2520two%2520probability%2520measures%2520on%2520a%250Ageneral%2520separable%2520and%2520compact%2520metric%2520space%2520is%2520equivalent%2520to%2520testing%2520for%2520the%250Asingularity%2520between%2520two%2520corresponding%2520Gaussian%2520measures%2520on%2520a%2520suitable%250AReproducing%2520Kernel%2520Hilbert%2520Space.%2520The%2520corresponding%2520Gaussians%2520are%2520defined%2520via%250Athe%2520notion%2520of%2520kernel%2520mean%2520and%2520covariance%2520embedding%2520of%2520a%2520probability%2520measure.%250ADiscerning%2520two%2520singular%2520Gaussians%2520is%2520fundamentally%2520simpler%2520from%2520an%250Ainformation-theoretic%2520perspective%2520than%2520non-parametric%2520two-sample%2520testing%252C%250Aparticularly%2520in%2520high-dimensional%2520settings.%2520Our%2520proof%2520leverages%2520the%250AFeldman-Hajek%2520criterion%2520for%2520singularity/equivalence%2520of%2520Gaussians%2520on%2520Hilbert%250Aspaces%252C%2520and%2520shows%2520that%2520discrepancies%2520between%2520distributions%2520are%2520heavily%250Amagnified%2520through%2520their%2520corresponding%2520Gaussian%2520embeddings%253A%2520at%2520a%2520population%250Alevel%252C%2520distinct%2520probability%2520measures%2520lead%2520to%2520essentially%2520separated%2520Gaussian%250Aembeddings.%2520This%2520appears%2520to%2520be%2520a%2520new%2520instance%2520of%2520the%2520blessing%2520of%2520dimensionality%250Athat%2520can%2520be%2520harnessed%2520for%2520the%2520design%2520of%2520efficient%2520inference%2520tools%2520in%2520great%250Agenerality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Two%20Sample%20Testing%20to%20Singular%20Gaussian%20Discrimination&entry.906535625=Leonardo%20V.%20Santoro%20and%20Kartik%20G.%20Waghmare%20and%20Victor%20M.%20Panaretos&entry.1292438233=%20%20We%20establish%20that%20testing%20for%20the%20equality%20of%20two%20probability%20measures%20on%20a%0Ageneral%20separable%20and%20compact%20metric%20space%20is%20equivalent%20to%20testing%20for%20the%0Asingularity%20between%20two%20corresponding%20Gaussian%20measures%20on%20a%20suitable%0AReproducing%20Kernel%20Hilbert%20Space.%20The%20corresponding%20Gaussians%20are%20defined%20via%0Athe%20notion%20of%20kernel%20mean%20and%20covariance%20embedding%20of%20a%20probability%20measure.%0ADiscerning%20two%20singular%20Gaussians%20is%20fundamentally%20simpler%20from%20an%0Ainformation-theoretic%20perspective%20than%20non-parametric%20two-sample%20testing%2C%0Aparticularly%20in%20high-dimensional%20settings.%20Our%20proof%20leverages%20the%0AFeldman-Hajek%20criterion%20for%20singularity/equivalence%20of%20Gaussians%20on%20Hilbert%0Aspaces%2C%20and%20shows%20that%20discrepancies%20between%20distributions%20are%20heavily%0Amagnified%20through%20their%20corresponding%20Gaussian%20embeddings%3A%20at%20a%20population%0Alevel%2C%20distinct%20probability%20measures%20lead%20to%20essentially%20separated%20Gaussian%0Aembeddings.%20This%20appears%20to%20be%20a%20new%20instance%20of%20the%20blessing%20of%20dimensionality%0Athat%20can%20be%20harnessed%20for%20the%20design%20of%20efficient%20inference%20tools%20in%20great%0Agenerality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04613v1&entry.124074799=Read"},
{"title": "HM-DF SNN: Transcending Conventional Online Learning with Advanced\n  Training and Deployment", "author": "Zecheng Hao and Yifan Huang and Zijie Xu and Wenxuan Liu and Yuanhong Tang and Zhaofei Yu and Tiejun Huang", "abstract": "  Spiking Neural Networks (SNNs) are considered to have enormous potential in\nthe future development of Artificial Intelligence due to their brain-inspired\nand energy-efficient properties. Compared to vanilla Spatial-Temporal\nBack-propagation (STBP) training methods, online training can effectively\novercome the risk of GPU memory explosion. However, current online learning\nframework cannot tackle the inseparability problem of temporal dependent\ngradients and merely aim to optimize the training memory, resulting in no\nperformance advantages compared to the STBP training models in the inference\nphase. To address the aforementioned challenges, we propose Hybrid\nMechanism-Driven Firing (HM-DF) model, which is a family of advanced models\nthat respectively adopt different spiking calculation schemes in the\nupper-region and lower-region of the firing threshold. We point out that HM-DF\nmodel can effectively separate temporal gradients and tackle the mismatch\nproblem of surrogate gradients, as well as achieving full-stage optimization\ntowards computation speed and memory footprint. Experimental results have\ndemonstrated that HM-DF model can be flexibly combined with various techniques\nto achieve state-of-the-art performance in the field of online learning,\nwithout triggering further power consumption.\n", "link": "http://arxiv.org/abs/2410.07547v2", "date": "2025-05-07", "relevancy": 2.2555, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6115}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.577}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HM-DF%20SNN%3A%20Transcending%20Conventional%20Online%20Learning%20with%20Advanced%0A%20%20Training%20and%20Deployment&body=Title%3A%20HM-DF%20SNN%3A%20Transcending%20Conventional%20Online%20Learning%20with%20Advanced%0A%20%20Training%20and%20Deployment%0AAuthor%3A%20Zecheng%20Hao%20and%20Yifan%20Huang%20and%20Zijie%20Xu%20and%20Wenxuan%20Liu%20and%20Yuanhong%20Tang%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20considered%20to%20have%20enormous%20potential%20in%0Athe%20future%20development%20of%20Artificial%20Intelligence%20due%20to%20their%20brain-inspired%0Aand%20energy-efficient%20properties.%20Compared%20to%20vanilla%20Spatial-Temporal%0ABack-propagation%20%28STBP%29%20training%20methods%2C%20online%20training%20can%20effectively%0Aovercome%20the%20risk%20of%20GPU%20memory%20explosion.%20However%2C%20current%20online%20learning%0Aframework%20cannot%20tackle%20the%20inseparability%20problem%20of%20temporal%20dependent%0Agradients%20and%20merely%20aim%20to%20optimize%20the%20training%20memory%2C%20resulting%20in%20no%0Aperformance%20advantages%20compared%20to%20the%20STBP%20training%20models%20in%20the%20inference%0Aphase.%20To%20address%20the%20aforementioned%20challenges%2C%20we%20propose%20Hybrid%0AMechanism-Driven%20Firing%20%28HM-DF%29%20model%2C%20which%20is%20a%20family%20of%20advanced%20models%0Athat%20respectively%20adopt%20different%20spiking%20calculation%20schemes%20in%20the%0Aupper-region%20and%20lower-region%20of%20the%20firing%20threshold.%20We%20point%20out%20that%20HM-DF%0Amodel%20can%20effectively%20separate%20temporal%20gradients%20and%20tackle%20the%20mismatch%0Aproblem%20of%20surrogate%20gradients%2C%20as%20well%20as%20achieving%20full-stage%20optimization%0Atowards%20computation%20speed%20and%20memory%20footprint.%20Experimental%20results%20have%0Ademonstrated%20that%20HM-DF%20model%20can%20be%20flexibly%20combined%20with%20various%20techniques%0Ato%20achieve%20state-of-the-art%20performance%20in%20the%20field%20of%20online%20learning%2C%0Awithout%20triggering%20further%20power%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07547v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHM-DF%2520SNN%253A%2520Transcending%2520Conventional%2520Online%2520Learning%2520with%2520Advanced%250A%2520%2520Training%2520and%2520Deployment%26entry.906535625%3DZecheng%2520Hao%2520and%2520Yifan%2520Huang%2520and%2520Zijie%2520Xu%2520and%2520Wenxuan%2520Liu%2520and%2520Yuanhong%2520Tang%2520and%2520Zhaofei%2520Yu%2520and%2520Tiejun%2520Huang%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520considered%2520to%2520have%2520enormous%2520potential%2520in%250Athe%2520future%2520development%2520of%2520Artificial%2520Intelligence%2520due%2520to%2520their%2520brain-inspired%250Aand%2520energy-efficient%2520properties.%2520Compared%2520to%2520vanilla%2520Spatial-Temporal%250ABack-propagation%2520%2528STBP%2529%2520training%2520methods%252C%2520online%2520training%2520can%2520effectively%250Aovercome%2520the%2520risk%2520of%2520GPU%2520memory%2520explosion.%2520However%252C%2520current%2520online%2520learning%250Aframework%2520cannot%2520tackle%2520the%2520inseparability%2520problem%2520of%2520temporal%2520dependent%250Agradients%2520and%2520merely%2520aim%2520to%2520optimize%2520the%2520training%2520memory%252C%2520resulting%2520in%2520no%250Aperformance%2520advantages%2520compared%2520to%2520the%2520STBP%2520training%2520models%2520in%2520the%2520inference%250Aphase.%2520To%2520address%2520the%2520aforementioned%2520challenges%252C%2520we%2520propose%2520Hybrid%250AMechanism-Driven%2520Firing%2520%2528HM-DF%2529%2520model%252C%2520which%2520is%2520a%2520family%2520of%2520advanced%2520models%250Athat%2520respectively%2520adopt%2520different%2520spiking%2520calculation%2520schemes%2520in%2520the%250Aupper-region%2520and%2520lower-region%2520of%2520the%2520firing%2520threshold.%2520We%2520point%2520out%2520that%2520HM-DF%250Amodel%2520can%2520effectively%2520separate%2520temporal%2520gradients%2520and%2520tackle%2520the%2520mismatch%250Aproblem%2520of%2520surrogate%2520gradients%252C%2520as%2520well%2520as%2520achieving%2520full-stage%2520optimization%250Atowards%2520computation%2520speed%2520and%2520memory%2520footprint.%2520Experimental%2520results%2520have%250Ademonstrated%2520that%2520HM-DF%2520model%2520can%2520be%2520flexibly%2520combined%2520with%2520various%2520techniques%250Ato%2520achieve%2520state-of-the-art%2520performance%2520in%2520the%2520field%2520of%2520online%2520learning%252C%250Awithout%2520triggering%2520further%2520power%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07547v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HM-DF%20SNN%3A%20Transcending%20Conventional%20Online%20Learning%20with%20Advanced%0A%20%20Training%20and%20Deployment&entry.906535625=Zecheng%20Hao%20and%20Yifan%20Huang%20and%20Zijie%20Xu%20and%20Wenxuan%20Liu%20and%20Yuanhong%20Tang%20and%20Zhaofei%20Yu%20and%20Tiejun%20Huang&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20considered%20to%20have%20enormous%20potential%20in%0Athe%20future%20development%20of%20Artificial%20Intelligence%20due%20to%20their%20brain-inspired%0Aand%20energy-efficient%20properties.%20Compared%20to%20vanilla%20Spatial-Temporal%0ABack-propagation%20%28STBP%29%20training%20methods%2C%20online%20training%20can%20effectively%0Aovercome%20the%20risk%20of%20GPU%20memory%20explosion.%20However%2C%20current%20online%20learning%0Aframework%20cannot%20tackle%20the%20inseparability%20problem%20of%20temporal%20dependent%0Agradients%20and%20merely%20aim%20to%20optimize%20the%20training%20memory%2C%20resulting%20in%20no%0Aperformance%20advantages%20compared%20to%20the%20STBP%20training%20models%20in%20the%20inference%0Aphase.%20To%20address%20the%20aforementioned%20challenges%2C%20we%20propose%20Hybrid%0AMechanism-Driven%20Firing%20%28HM-DF%29%20model%2C%20which%20is%20a%20family%20of%20advanced%20models%0Athat%20respectively%20adopt%20different%20spiking%20calculation%20schemes%20in%20the%0Aupper-region%20and%20lower-region%20of%20the%20firing%20threshold.%20We%20point%20out%20that%20HM-DF%0Amodel%20can%20effectively%20separate%20temporal%20gradients%20and%20tackle%20the%20mismatch%0Aproblem%20of%20surrogate%20gradients%2C%20as%20well%20as%20achieving%20full-stage%20optimization%0Atowards%20computation%20speed%20and%20memory%20footprint.%20Experimental%20results%20have%0Ademonstrated%20that%20HM-DF%20model%20can%20be%20flexibly%20combined%20with%20various%20techniques%0Ato%20achieve%20state-of-the-art%20performance%20in%20the%20field%20of%20online%20learning%2C%0Awithout%20triggering%20further%20power%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07547v2&entry.124074799=Read"},
{"title": "Towards Initialization-Agnostic Clustering with Iterative Adaptive\n  Resonance Theory", "author": "Xiaozheng Qu and Zhaochuan Li and Zhuang Qi and Xiang Li and Haibei Huang and Lei Meng and Xiangxu Meng", "abstract": "  The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is\nhighly dependent on the preset vigilance parameter, where deviations in its\nvalue can lead to significant fluctuations in clustering results, severely\nlimiting its practicality for non-expert users. Existing approaches generally\nenhance vigilance parameter robustness through adaptive mechanisms such as\nparticle swarm optimization and fuzzy logic rules. However, they often\nintroduce additional hyperparameters or complex frameworks that contradict the\noriginal simplicity of the algorithm. To address this, we propose Iterative\nRefinement Adaptive Resonance Theory (IR-ART), which integrates three key\nphases into a unified iterative framework: (1) Cluster Stability Detection: A\ndynamic stability detection module that identifies unstable clusters by\nanalyzing the change of sample size (number of samples in the cluster) in\niteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that\neliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance\nregion expansion mechanism that adaptively adjusts similarity thresholds.\nIndependent of the specific execution of clustering, these three phases\nsequentially focus on analyzing the implicit knowledge within the iterative\nprocess, adjusting weights and vigilance parameters, thereby laying a\nfoundation for the next iteration. Experimental evaluation on 15 datasets\ndemonstrates that IR-ART improves tolerance to suboptimal vigilance parameter\nvalues while preserving the parameter simplicity of Fuzzy ART. Case studies\nvisually confirm the algorithm's self-optimization capability through iterative\nrefinement, making it particularly suitable for non-expert users in\nresource-constrained scenarios.\n", "link": "http://arxiv.org/abs/2505.04440v1", "date": "2025-05-07", "relevancy": 2.2527, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4625}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4493}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Initialization-Agnostic%20Clustering%20with%20Iterative%20Adaptive%0A%20%20Resonance%20Theory&body=Title%3A%20Towards%20Initialization-Agnostic%20Clustering%20with%20Iterative%20Adaptive%0A%20%20Resonance%20Theory%0AAuthor%3A%20Xiaozheng%20Qu%20and%20Zhaochuan%20Li%20and%20Zhuang%20Qi%20and%20Xiang%20Li%20and%20Haibei%20Huang%20and%20Lei%20Meng%20and%20Xiangxu%20Meng%0AAbstract%3A%20%20%20The%20clustering%20performance%20of%20Fuzzy%20Adaptive%20Resonance%20Theory%20%28Fuzzy%20ART%29%20is%0Ahighly%20dependent%20on%20the%20preset%20vigilance%20parameter%2C%20where%20deviations%20in%20its%0Avalue%20can%20lead%20to%20significant%20fluctuations%20in%20clustering%20results%2C%20severely%0Alimiting%20its%20practicality%20for%20non-expert%20users.%20Existing%20approaches%20generally%0Aenhance%20vigilance%20parameter%20robustness%20through%20adaptive%20mechanisms%20such%20as%0Aparticle%20swarm%20optimization%20and%20fuzzy%20logic%20rules.%20However%2C%20they%20often%0Aintroduce%20additional%20hyperparameters%20or%20complex%20frameworks%20that%20contradict%20the%0Aoriginal%20simplicity%20of%20the%20algorithm.%20To%20address%20this%2C%20we%20propose%20Iterative%0ARefinement%20Adaptive%20Resonance%20Theory%20%28IR-ART%29%2C%20which%20integrates%20three%20key%0Aphases%20into%20a%20unified%20iterative%20framework%3A%20%281%29%20Cluster%20Stability%20Detection%3A%20A%0Adynamic%20stability%20detection%20module%20that%20identifies%20unstable%20clusters%20by%0Aanalyzing%20the%20change%20of%20sample%20size%20%28number%20of%20samples%20in%20the%20cluster%29%20in%0Aiteration.%20%282%29%20Unstable%20Cluster%20Deletion%3A%20An%20evolutionary%20pruning%20module%20that%0Aeliminates%20low-quality%20clusters.%20%283%29%20Vigilance%20Region%20Expansion%3A%20A%20vigilance%0Aregion%20expansion%20mechanism%20that%20adaptively%20adjusts%20similarity%20thresholds.%0AIndependent%20of%20the%20specific%20execution%20of%20clustering%2C%20these%20three%20phases%0Asequentially%20focus%20on%20analyzing%20the%20implicit%20knowledge%20within%20the%20iterative%0Aprocess%2C%20adjusting%20weights%20and%20vigilance%20parameters%2C%20thereby%20laying%20a%0Afoundation%20for%20the%20next%20iteration.%20Experimental%20evaluation%20on%2015%20datasets%0Ademonstrates%20that%20IR-ART%20improves%20tolerance%20to%20suboptimal%20vigilance%20parameter%0Avalues%20while%20preserving%20the%20parameter%20simplicity%20of%20Fuzzy%20ART.%20Case%20studies%0Avisually%20confirm%20the%20algorithm%27s%20self-optimization%20capability%20through%20iterative%0Arefinement%2C%20making%20it%20particularly%20suitable%20for%20non-expert%20users%20in%0Aresource-constrained%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Initialization-Agnostic%2520Clustering%2520with%2520Iterative%2520Adaptive%250A%2520%2520Resonance%2520Theory%26entry.906535625%3DXiaozheng%2520Qu%2520and%2520Zhaochuan%2520Li%2520and%2520Zhuang%2520Qi%2520and%2520Xiang%2520Li%2520and%2520Haibei%2520Huang%2520and%2520Lei%2520Meng%2520and%2520Xiangxu%2520Meng%26entry.1292438233%3D%2520%2520The%2520clustering%2520performance%2520of%2520Fuzzy%2520Adaptive%2520Resonance%2520Theory%2520%2528Fuzzy%2520ART%2529%2520is%250Ahighly%2520dependent%2520on%2520the%2520preset%2520vigilance%2520parameter%252C%2520where%2520deviations%2520in%2520its%250Avalue%2520can%2520lead%2520to%2520significant%2520fluctuations%2520in%2520clustering%2520results%252C%2520severely%250Alimiting%2520its%2520practicality%2520for%2520non-expert%2520users.%2520Existing%2520approaches%2520generally%250Aenhance%2520vigilance%2520parameter%2520robustness%2520through%2520adaptive%2520mechanisms%2520such%2520as%250Aparticle%2520swarm%2520optimization%2520and%2520fuzzy%2520logic%2520rules.%2520However%252C%2520they%2520often%250Aintroduce%2520additional%2520hyperparameters%2520or%2520complex%2520frameworks%2520that%2520contradict%2520the%250Aoriginal%2520simplicity%2520of%2520the%2520algorithm.%2520To%2520address%2520this%252C%2520we%2520propose%2520Iterative%250ARefinement%2520Adaptive%2520Resonance%2520Theory%2520%2528IR-ART%2529%252C%2520which%2520integrates%2520three%2520key%250Aphases%2520into%2520a%2520unified%2520iterative%2520framework%253A%2520%25281%2529%2520Cluster%2520Stability%2520Detection%253A%2520A%250Adynamic%2520stability%2520detection%2520module%2520that%2520identifies%2520unstable%2520clusters%2520by%250Aanalyzing%2520the%2520change%2520of%2520sample%2520size%2520%2528number%2520of%2520samples%2520in%2520the%2520cluster%2529%2520in%250Aiteration.%2520%25282%2529%2520Unstable%2520Cluster%2520Deletion%253A%2520An%2520evolutionary%2520pruning%2520module%2520that%250Aeliminates%2520low-quality%2520clusters.%2520%25283%2529%2520Vigilance%2520Region%2520Expansion%253A%2520A%2520vigilance%250Aregion%2520expansion%2520mechanism%2520that%2520adaptively%2520adjusts%2520similarity%2520thresholds.%250AIndependent%2520of%2520the%2520specific%2520execution%2520of%2520clustering%252C%2520these%2520three%2520phases%250Asequentially%2520focus%2520on%2520analyzing%2520the%2520implicit%2520knowledge%2520within%2520the%2520iterative%250Aprocess%252C%2520adjusting%2520weights%2520and%2520vigilance%2520parameters%252C%2520thereby%2520laying%2520a%250Afoundation%2520for%2520the%2520next%2520iteration.%2520Experimental%2520evaluation%2520on%252015%2520datasets%250Ademonstrates%2520that%2520IR-ART%2520improves%2520tolerance%2520to%2520suboptimal%2520vigilance%2520parameter%250Avalues%2520while%2520preserving%2520the%2520parameter%2520simplicity%2520of%2520Fuzzy%2520ART.%2520Case%2520studies%250Avisually%2520confirm%2520the%2520algorithm%2527s%2520self-optimization%2520capability%2520through%2520iterative%250Arefinement%252C%2520making%2520it%2520particularly%2520suitable%2520for%2520non-expert%2520users%2520in%250Aresource-constrained%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Initialization-Agnostic%20Clustering%20with%20Iterative%20Adaptive%0A%20%20Resonance%20Theory&entry.906535625=Xiaozheng%20Qu%20and%20Zhaochuan%20Li%20and%20Zhuang%20Qi%20and%20Xiang%20Li%20and%20Haibei%20Huang%20and%20Lei%20Meng%20and%20Xiangxu%20Meng&entry.1292438233=%20%20The%20clustering%20performance%20of%20Fuzzy%20Adaptive%20Resonance%20Theory%20%28Fuzzy%20ART%29%20is%0Ahighly%20dependent%20on%20the%20preset%20vigilance%20parameter%2C%20where%20deviations%20in%20its%0Avalue%20can%20lead%20to%20significant%20fluctuations%20in%20clustering%20results%2C%20severely%0Alimiting%20its%20practicality%20for%20non-expert%20users.%20Existing%20approaches%20generally%0Aenhance%20vigilance%20parameter%20robustness%20through%20adaptive%20mechanisms%20such%20as%0Aparticle%20swarm%20optimization%20and%20fuzzy%20logic%20rules.%20However%2C%20they%20often%0Aintroduce%20additional%20hyperparameters%20or%20complex%20frameworks%20that%20contradict%20the%0Aoriginal%20simplicity%20of%20the%20algorithm.%20To%20address%20this%2C%20we%20propose%20Iterative%0ARefinement%20Adaptive%20Resonance%20Theory%20%28IR-ART%29%2C%20which%20integrates%20three%20key%0Aphases%20into%20a%20unified%20iterative%20framework%3A%20%281%29%20Cluster%20Stability%20Detection%3A%20A%0Adynamic%20stability%20detection%20module%20that%20identifies%20unstable%20clusters%20by%0Aanalyzing%20the%20change%20of%20sample%20size%20%28number%20of%20samples%20in%20the%20cluster%29%20in%0Aiteration.%20%282%29%20Unstable%20Cluster%20Deletion%3A%20An%20evolutionary%20pruning%20module%20that%0Aeliminates%20low-quality%20clusters.%20%283%29%20Vigilance%20Region%20Expansion%3A%20A%20vigilance%0Aregion%20expansion%20mechanism%20that%20adaptively%20adjusts%20similarity%20thresholds.%0AIndependent%20of%20the%20specific%20execution%20of%20clustering%2C%20these%20three%20phases%0Asequentially%20focus%20on%20analyzing%20the%20implicit%20knowledge%20within%20the%20iterative%0Aprocess%2C%20adjusting%20weights%20and%20vigilance%20parameters%2C%20thereby%20laying%20a%0Afoundation%20for%20the%20next%20iteration.%20Experimental%20evaluation%20on%2015%20datasets%0Ademonstrates%20that%20IR-ART%20improves%20tolerance%20to%20suboptimal%20vigilance%20parameter%0Avalues%20while%20preserving%20the%20parameter%20simplicity%20of%20Fuzzy%20ART.%20Case%20studies%0Avisually%20confirm%20the%20algorithm%27s%20self-optimization%20capability%20through%20iterative%0Arefinement%2C%20making%20it%20particularly%20suitable%20for%20non-expert%20users%20in%0Aresource-constrained%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04440v1&entry.124074799=Read"},
{"title": "FastMap: Revisiting Dense and Scalable Structure from Motion", "author": "Jiahao Li and Haochen Wang and Muhammad Zubair Irshad and Igor Vasiljevic and Matthew R. Walter and Vitor Campagnolo Guizilini and Greg Shakhnarovich", "abstract": "  We propose FastMap, a new global structure from motion method focused on\nspeed and simplicity. Previous methods like COLMAP and GLOMAP are able to\nestimate high-precision camera poses, but suffer from poor scalability when the\nnumber of matched keypoint pairs becomes large. We identify two key factors\nleading to this problem: poor parallelization and computationally expensive\noptimization steps. To overcome these issues, we design an SfM framework that\nrelies entirely on GPU-friendly operations, making it easily parallelizable.\nMoreover, each optimization step runs in time linear to the number of image\npairs, independent of keypoint pairs or 3D points. Through extensive\nexperiments, we show that FastMap is one to two orders of magnitude faster than\nCOLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.\n", "link": "http://arxiv.org/abs/2505.04612v1", "date": "2025-05-07", "relevancy": 2.2464, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.592}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5402}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastMap%3A%20Revisiting%20Dense%20and%20Scalable%20Structure%20from%20Motion&body=Title%3A%20FastMap%3A%20Revisiting%20Dense%20and%20Scalable%20Structure%20from%20Motion%0AAuthor%3A%20Jiahao%20Li%20and%20Haochen%20Wang%20and%20Muhammad%20Zubair%20Irshad%20and%20Igor%20Vasiljevic%20and%20Matthew%20R.%20Walter%20and%20Vitor%20Campagnolo%20Guizilini%20and%20Greg%20Shakhnarovich%0AAbstract%3A%20%20%20We%20propose%20FastMap%2C%20a%20new%20global%20structure%20from%20motion%20method%20focused%20on%0Aspeed%20and%20simplicity.%20Previous%20methods%20like%20COLMAP%20and%20GLOMAP%20are%20able%20to%0Aestimate%20high-precision%20camera%20poses%2C%20but%20suffer%20from%20poor%20scalability%20when%20the%0Anumber%20of%20matched%20keypoint%20pairs%20becomes%20large.%20We%20identify%20two%20key%20factors%0Aleading%20to%20this%20problem%3A%20poor%20parallelization%20and%20computationally%20expensive%0Aoptimization%20steps.%20To%20overcome%20these%20issues%2C%20we%20design%20an%20SfM%20framework%20that%0Arelies%20entirely%20on%20GPU-friendly%20operations%2C%20making%20it%20easily%20parallelizable.%0AMoreover%2C%20each%20optimization%20step%20runs%20in%20time%20linear%20to%20the%20number%20of%20image%0Apairs%2C%20independent%20of%20keypoint%20pairs%20or%203D%20points.%20Through%20extensive%0Aexperiments%2C%20we%20show%20that%20FastMap%20is%20one%20to%20two%20orders%20of%20magnitude%20faster%20than%0ACOLMAP%20and%20GLOMAP%20on%20large-scale%20scenes%20with%20comparable%20pose%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastMap%253A%2520Revisiting%2520Dense%2520and%2520Scalable%2520Structure%2520from%2520Motion%26entry.906535625%3DJiahao%2520Li%2520and%2520Haochen%2520Wang%2520and%2520Muhammad%2520Zubair%2520Irshad%2520and%2520Igor%2520Vasiljevic%2520and%2520Matthew%2520R.%2520Walter%2520and%2520Vitor%2520Campagnolo%2520Guizilini%2520and%2520Greg%2520Shakhnarovich%26entry.1292438233%3D%2520%2520We%2520propose%2520FastMap%252C%2520a%2520new%2520global%2520structure%2520from%2520motion%2520method%2520focused%2520on%250Aspeed%2520and%2520simplicity.%2520Previous%2520methods%2520like%2520COLMAP%2520and%2520GLOMAP%2520are%2520able%2520to%250Aestimate%2520high-precision%2520camera%2520poses%252C%2520but%2520suffer%2520from%2520poor%2520scalability%2520when%2520the%250Anumber%2520of%2520matched%2520keypoint%2520pairs%2520becomes%2520large.%2520We%2520identify%2520two%2520key%2520factors%250Aleading%2520to%2520this%2520problem%253A%2520poor%2520parallelization%2520and%2520computationally%2520expensive%250Aoptimization%2520steps.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520design%2520an%2520SfM%2520framework%2520that%250Arelies%2520entirely%2520on%2520GPU-friendly%2520operations%252C%2520making%2520it%2520easily%2520parallelizable.%250AMoreover%252C%2520each%2520optimization%2520step%2520runs%2520in%2520time%2520linear%2520to%2520the%2520number%2520of%2520image%250Apairs%252C%2520independent%2520of%2520keypoint%2520pairs%2520or%25203D%2520points.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520show%2520that%2520FastMap%2520is%2520one%2520to%2520two%2520orders%2520of%2520magnitude%2520faster%2520than%250ACOLMAP%2520and%2520GLOMAP%2520on%2520large-scale%2520scenes%2520with%2520comparable%2520pose%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastMap%3A%20Revisiting%20Dense%20and%20Scalable%20Structure%20from%20Motion&entry.906535625=Jiahao%20Li%20and%20Haochen%20Wang%20and%20Muhammad%20Zubair%20Irshad%20and%20Igor%20Vasiljevic%20and%20Matthew%20R.%20Walter%20and%20Vitor%20Campagnolo%20Guizilini%20and%20Greg%20Shakhnarovich&entry.1292438233=%20%20We%20propose%20FastMap%2C%20a%20new%20global%20structure%20from%20motion%20method%20focused%20on%0Aspeed%20and%20simplicity.%20Previous%20methods%20like%20COLMAP%20and%20GLOMAP%20are%20able%20to%0Aestimate%20high-precision%20camera%20poses%2C%20but%20suffer%20from%20poor%20scalability%20when%20the%0Anumber%20of%20matched%20keypoint%20pairs%20becomes%20large.%20We%20identify%20two%20key%20factors%0Aleading%20to%20this%20problem%3A%20poor%20parallelization%20and%20computationally%20expensive%0Aoptimization%20steps.%20To%20overcome%20these%20issues%2C%20we%20design%20an%20SfM%20framework%20that%0Arelies%20entirely%20on%20GPU-friendly%20operations%2C%20making%20it%20easily%20parallelizable.%0AMoreover%2C%20each%20optimization%20step%20runs%20in%20time%20linear%20to%20the%20number%20of%20image%0Apairs%2C%20independent%20of%20keypoint%20pairs%20or%203D%20points.%20Through%20extensive%0Aexperiments%2C%20we%20show%20that%20FastMap%20is%20one%20to%20two%20orders%20of%20magnitude%20faster%20than%0ACOLMAP%20and%20GLOMAP%20on%20large-scale%20scenes%20with%20comparable%20pose%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04612v1&entry.124074799=Read"},
{"title": "Is What You Ask For What You Get? Investigating Concept Associations in\n  Text-to-Image Models", "author": "Salma S. Abdel Magid and Weiwei Pan and Simon Warchol and Grace Guo and Junsik Kim and Mahia Rahman and Hanspeter Pfister", "abstract": "  Text-to-image (T2I) models are increasingly used in impactful real-life\napplications. As such, there is a growing need to audit these models to ensure\nthat they generate desirable, task-appropriate images. However, systematically\ninspecting the associations between prompts and generated content in a\nhuman-understandable way remains challenging. To address this, we propose\nConcept2Concept, a framework where we characterize conditional distributions of\nvision language models using interpretable concepts and metrics that can be\ndefined in terms of these concepts. This characterization allows us to use our\nframework to audit models and prompt-datasets. To demonstrate, we investigate\nseveral case studies of conditional distributions of prompts, such as\nuser-defined distributions or empirical, real-world distributions. Lastly, we\nimplement Concept2Concept as an open-source interactive visualization tool to\nfacilitate use by non-technical end-users. A demo is available at\nhttps://tinyurl.com/Concept2ConceptDemo.\n", "link": "http://arxiv.org/abs/2410.04634v3", "date": "2025-05-07", "relevancy": 2.2282, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20What%20You%20Ask%20For%20What%20You%20Get%3F%20Investigating%20Concept%20Associations%20in%0A%20%20Text-to-Image%20Models&body=Title%3A%20Is%20What%20You%20Ask%20For%20What%20You%20Get%3F%20Investigating%20Concept%20Associations%20in%0A%20%20Text-to-Image%20Models%0AAuthor%3A%20Salma%20S.%20Abdel%20Magid%20and%20Weiwei%20Pan%20and%20Simon%20Warchol%20and%20Grace%20Guo%20and%20Junsik%20Kim%20and%20Mahia%20Rahman%20and%20Hanspeter%20Pfister%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20models%20are%20increasingly%20used%20in%20impactful%20real-life%0Aapplications.%20As%20such%2C%20there%20is%20a%20growing%20need%20to%20audit%20these%20models%20to%20ensure%0Athat%20they%20generate%20desirable%2C%20task-appropriate%20images.%20However%2C%20systematically%0Ainspecting%20the%20associations%20between%20prompts%20and%20generated%20content%20in%20a%0Ahuman-understandable%20way%20remains%20challenging.%20To%20address%20this%2C%20we%20propose%0AConcept2Concept%2C%20a%20framework%20where%20we%20characterize%20conditional%20distributions%20of%0Avision%20language%20models%20using%20interpretable%20concepts%20and%20metrics%20that%20can%20be%0Adefined%20in%20terms%20of%20these%20concepts.%20This%20characterization%20allows%20us%20to%20use%20our%0Aframework%20to%20audit%20models%20and%20prompt-datasets.%20To%20demonstrate%2C%20we%20investigate%0Aseveral%20case%20studies%20of%20conditional%20distributions%20of%20prompts%2C%20such%20as%0Auser-defined%20distributions%20or%20empirical%2C%20real-world%20distributions.%20Lastly%2C%20we%0Aimplement%20Concept2Concept%20as%20an%20open-source%20interactive%20visualization%20tool%20to%0Afacilitate%20use%20by%20non-technical%20end-users.%20A%20demo%20is%20available%20at%0Ahttps%3A//tinyurl.com/Concept2ConceptDemo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.04634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520What%2520You%2520Ask%2520For%2520What%2520You%2520Get%253F%2520Investigating%2520Concept%2520Associations%2520in%250A%2520%2520Text-to-Image%2520Models%26entry.906535625%3DSalma%2520S.%2520Abdel%2520Magid%2520and%2520Weiwei%2520Pan%2520and%2520Simon%2520Warchol%2520and%2520Grace%2520Guo%2520and%2520Junsik%2520Kim%2520and%2520Mahia%2520Rahman%2520and%2520Hanspeter%2520Pfister%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520models%2520are%2520increasingly%2520used%2520in%2520impactful%2520real-life%250Aapplications.%2520As%2520such%252C%2520there%2520is%2520a%2520growing%2520need%2520to%2520audit%2520these%2520models%2520to%2520ensure%250Athat%2520they%2520generate%2520desirable%252C%2520task-appropriate%2520images.%2520However%252C%2520systematically%250Ainspecting%2520the%2520associations%2520between%2520prompts%2520and%2520generated%2520content%2520in%2520a%250Ahuman-understandable%2520way%2520remains%2520challenging.%2520To%2520address%2520this%252C%2520we%2520propose%250AConcept2Concept%252C%2520a%2520framework%2520where%2520we%2520characterize%2520conditional%2520distributions%2520of%250Avision%2520language%2520models%2520using%2520interpretable%2520concepts%2520and%2520metrics%2520that%2520can%2520be%250Adefined%2520in%2520terms%2520of%2520these%2520concepts.%2520This%2520characterization%2520allows%2520us%2520to%2520use%2520our%250Aframework%2520to%2520audit%2520models%2520and%2520prompt-datasets.%2520To%2520demonstrate%252C%2520we%2520investigate%250Aseveral%2520case%2520studies%2520of%2520conditional%2520distributions%2520of%2520prompts%252C%2520such%2520as%250Auser-defined%2520distributions%2520or%2520empirical%252C%2520real-world%2520distributions.%2520Lastly%252C%2520we%250Aimplement%2520Concept2Concept%2520as%2520an%2520open-source%2520interactive%2520visualization%2520tool%2520to%250Afacilitate%2520use%2520by%2520non-technical%2520end-users.%2520A%2520demo%2520is%2520available%2520at%250Ahttps%253A//tinyurl.com/Concept2ConceptDemo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.04634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20What%20You%20Ask%20For%20What%20You%20Get%3F%20Investigating%20Concept%20Associations%20in%0A%20%20Text-to-Image%20Models&entry.906535625=Salma%20S.%20Abdel%20Magid%20and%20Weiwei%20Pan%20and%20Simon%20Warchol%20and%20Grace%20Guo%20and%20Junsik%20Kim%20and%20Mahia%20Rahman%20and%20Hanspeter%20Pfister&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20models%20are%20increasingly%20used%20in%20impactful%20real-life%0Aapplications.%20As%20such%2C%20there%20is%20a%20growing%20need%20to%20audit%20these%20models%20to%20ensure%0Athat%20they%20generate%20desirable%2C%20task-appropriate%20images.%20However%2C%20systematically%0Ainspecting%20the%20associations%20between%20prompts%20and%20generated%20content%20in%20a%0Ahuman-understandable%20way%20remains%20challenging.%20To%20address%20this%2C%20we%20propose%0AConcept2Concept%2C%20a%20framework%20where%20we%20characterize%20conditional%20distributions%20of%0Avision%20language%20models%20using%20interpretable%20concepts%20and%20metrics%20that%20can%20be%0Adefined%20in%20terms%20of%20these%20concepts.%20This%20characterization%20allows%20us%20to%20use%20our%0Aframework%20to%20audit%20models%20and%20prompt-datasets.%20To%20demonstrate%2C%20we%20investigate%0Aseveral%20case%20studies%20of%20conditional%20distributions%20of%20prompts%2C%20such%20as%0Auser-defined%20distributions%20or%20empirical%2C%20real-world%20distributions.%20Lastly%2C%20we%0Aimplement%20Concept2Concept%20as%20an%20open-source%20interactive%20visualization%20tool%20to%0Afacilitate%20use%20by%20non-technical%20end-users.%20A%20demo%20is%20available%20at%0Ahttps%3A//tinyurl.com/Concept2ConceptDemo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.04634v3&entry.124074799=Read"},
{"title": "Predicting Road Surface Anomalies by Visual Tracking of a Preceding\n  Vehicle", "author": "Petr Jahoda and Jan Cech", "abstract": "  A novel approach to detect road surface anomalies by visual tracking of a\npreceding vehicle is proposed. The method is versatile, predicting any kind of\nroad anomalies, such as potholes, bumps, debris, etc., unlike direct\nobservation methods that rely on training visual detectors of those cases. The\nmethod operates in low visibility conditions or in dense traffic where the\nanomaly is occluded by a preceding vehicle. Anomalies are detected\npredictively, i.e., before a vehicle encounters them, which allows to\npre-configure low-level vehicle systems (such as chassis) or to plan an\navoidance maneuver in case of autonomous driving. A challenge is that the\nsignal coming from camera-based tracking of a preceding vehicle may be weak and\ndisturbed by camera ego motion due to vibrations affecting the ego vehicle.\nTherefore, we propose an efficient method to compensate camera pitch rotation\nby an iterative robust estimator. Our experiments on both controlled setup and\nnormal traffic conditions show that road anomalies can be detected reliably at\na distance even in challenging cases where the ego vehicle traverses imperfect\nroad surfaces. The method is effective and performs in real time on standard\nconsumer hardware.\n", "link": "http://arxiv.org/abs/2505.04392v1", "date": "2025-05-07", "relevancy": 2.2279, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5585}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Road%20Surface%20Anomalies%20by%20Visual%20Tracking%20of%20a%20Preceding%0A%20%20Vehicle&body=Title%3A%20Predicting%20Road%20Surface%20Anomalies%20by%20Visual%20Tracking%20of%20a%20Preceding%0A%20%20Vehicle%0AAuthor%3A%20Petr%20Jahoda%20and%20Jan%20Cech%0AAbstract%3A%20%20%20A%20novel%20approach%20to%20detect%20road%20surface%20anomalies%20by%20visual%20tracking%20of%20a%0Apreceding%20vehicle%20is%20proposed.%20The%20method%20is%20versatile%2C%20predicting%20any%20kind%20of%0Aroad%20anomalies%2C%20such%20as%20potholes%2C%20bumps%2C%20debris%2C%20etc.%2C%20unlike%20direct%0Aobservation%20methods%20that%20rely%20on%20training%20visual%20detectors%20of%20those%20cases.%20The%0Amethod%20operates%20in%20low%20visibility%20conditions%20or%20in%20dense%20traffic%20where%20the%0Aanomaly%20is%20occluded%20by%20a%20preceding%20vehicle.%20Anomalies%20are%20detected%0Apredictively%2C%20i.e.%2C%20before%20a%20vehicle%20encounters%20them%2C%20which%20allows%20to%0Apre-configure%20low-level%20vehicle%20systems%20%28such%20as%20chassis%29%20or%20to%20plan%20an%0Aavoidance%20maneuver%20in%20case%20of%20autonomous%20driving.%20A%20challenge%20is%20that%20the%0Asignal%20coming%20from%20camera-based%20tracking%20of%20a%20preceding%20vehicle%20may%20be%20weak%20and%0Adisturbed%20by%20camera%20ego%20motion%20due%20to%20vibrations%20affecting%20the%20ego%20vehicle.%0ATherefore%2C%20we%20propose%20an%20efficient%20method%20to%20compensate%20camera%20pitch%20rotation%0Aby%20an%20iterative%20robust%20estimator.%20Our%20experiments%20on%20both%20controlled%20setup%20and%0Anormal%20traffic%20conditions%20show%20that%20road%20anomalies%20can%20be%20detected%20reliably%20at%0Aa%20distance%20even%20in%20challenging%20cases%20where%20the%20ego%20vehicle%20traverses%20imperfect%0Aroad%20surfaces.%20The%20method%20is%20effective%20and%20performs%20in%20real%20time%20on%20standard%0Aconsumer%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Road%2520Surface%2520Anomalies%2520by%2520Visual%2520Tracking%2520of%2520a%2520Preceding%250A%2520%2520Vehicle%26entry.906535625%3DPetr%2520Jahoda%2520and%2520Jan%2520Cech%26entry.1292438233%3D%2520%2520A%2520novel%2520approach%2520to%2520detect%2520road%2520surface%2520anomalies%2520by%2520visual%2520tracking%2520of%2520a%250Apreceding%2520vehicle%2520is%2520proposed.%2520The%2520method%2520is%2520versatile%252C%2520predicting%2520any%2520kind%2520of%250Aroad%2520anomalies%252C%2520such%2520as%2520potholes%252C%2520bumps%252C%2520debris%252C%2520etc.%252C%2520unlike%2520direct%250Aobservation%2520methods%2520that%2520rely%2520on%2520training%2520visual%2520detectors%2520of%2520those%2520cases.%2520The%250Amethod%2520operates%2520in%2520low%2520visibility%2520conditions%2520or%2520in%2520dense%2520traffic%2520where%2520the%250Aanomaly%2520is%2520occluded%2520by%2520a%2520preceding%2520vehicle.%2520Anomalies%2520are%2520detected%250Apredictively%252C%2520i.e.%252C%2520before%2520a%2520vehicle%2520encounters%2520them%252C%2520which%2520allows%2520to%250Apre-configure%2520low-level%2520vehicle%2520systems%2520%2528such%2520as%2520chassis%2529%2520or%2520to%2520plan%2520an%250Aavoidance%2520maneuver%2520in%2520case%2520of%2520autonomous%2520driving.%2520A%2520challenge%2520is%2520that%2520the%250Asignal%2520coming%2520from%2520camera-based%2520tracking%2520of%2520a%2520preceding%2520vehicle%2520may%2520be%2520weak%2520and%250Adisturbed%2520by%2520camera%2520ego%2520motion%2520due%2520to%2520vibrations%2520affecting%2520the%2520ego%2520vehicle.%250ATherefore%252C%2520we%2520propose%2520an%2520efficient%2520method%2520to%2520compensate%2520camera%2520pitch%2520rotation%250Aby%2520an%2520iterative%2520robust%2520estimator.%2520Our%2520experiments%2520on%2520both%2520controlled%2520setup%2520and%250Anormal%2520traffic%2520conditions%2520show%2520that%2520road%2520anomalies%2520can%2520be%2520detected%2520reliably%2520at%250Aa%2520distance%2520even%2520in%2520challenging%2520cases%2520where%2520the%2520ego%2520vehicle%2520traverses%2520imperfect%250Aroad%2520surfaces.%2520The%2520method%2520is%2520effective%2520and%2520performs%2520in%2520real%2520time%2520on%2520standard%250Aconsumer%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Road%20Surface%20Anomalies%20by%20Visual%20Tracking%20of%20a%20Preceding%0A%20%20Vehicle&entry.906535625=Petr%20Jahoda%20and%20Jan%20Cech&entry.1292438233=%20%20A%20novel%20approach%20to%20detect%20road%20surface%20anomalies%20by%20visual%20tracking%20of%20a%0Apreceding%20vehicle%20is%20proposed.%20The%20method%20is%20versatile%2C%20predicting%20any%20kind%20of%0Aroad%20anomalies%2C%20such%20as%20potholes%2C%20bumps%2C%20debris%2C%20etc.%2C%20unlike%20direct%0Aobservation%20methods%20that%20rely%20on%20training%20visual%20detectors%20of%20those%20cases.%20The%0Amethod%20operates%20in%20low%20visibility%20conditions%20or%20in%20dense%20traffic%20where%20the%0Aanomaly%20is%20occluded%20by%20a%20preceding%20vehicle.%20Anomalies%20are%20detected%0Apredictively%2C%20i.e.%2C%20before%20a%20vehicle%20encounters%20them%2C%20which%20allows%20to%0Apre-configure%20low-level%20vehicle%20systems%20%28such%20as%20chassis%29%20or%20to%20plan%20an%0Aavoidance%20maneuver%20in%20case%20of%20autonomous%20driving.%20A%20challenge%20is%20that%20the%0Asignal%20coming%20from%20camera-based%20tracking%20of%20a%20preceding%20vehicle%20may%20be%20weak%20and%0Adisturbed%20by%20camera%20ego%20motion%20due%20to%20vibrations%20affecting%20the%20ego%20vehicle.%0ATherefore%2C%20we%20propose%20an%20efficient%20method%20to%20compensate%20camera%20pitch%20rotation%0Aby%20an%20iterative%20robust%20estimator.%20Our%20experiments%20on%20both%20controlled%20setup%20and%0Anormal%20traffic%20conditions%20show%20that%20road%20anomalies%20can%20be%20detected%20reliably%20at%0Aa%20distance%20even%20in%20challenging%20cases%20where%20the%20ego%20vehicle%20traverses%20imperfect%0Aroad%20surfaces.%20The%20method%20is%20effective%20and%20performs%20in%20real%20time%20on%20standard%0Aconsumer%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04392v1&entry.124074799=Read"},
{"title": "DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage\n  Object Detection", "author": "A. Enes Doruk and Hasan F. Ates", "abstract": "  Recent 2D CNN-based domain adaptation approaches struggle with long-range\ndependencies due to limited receptive fields, making it difficult to adapt to\ntarget domains with significant spatial distribution changes. While\ntransformer-based domain adaptation methods better capture distant\nrelationships through self-attention mechanisms that facilitate more effective\ncross-domain feature alignment, their quadratic computational complexity makes\npractical deployment challenging for object detection tasks across diverse\ndomains. Inspired by the global modeling and linear computation complexity of\nthe Mamba architecture, we present the first domain-adaptive Mamba-based\none-stage object detection model, termed DA-Mamba. Specifically, we combine\nMamba's efficient state-space modeling with attention mechanisms to address\ndomain-specific spatial and channel-wise variations. Our design leverages\ndomain-adaptive spatial and channel-wise scanning within the Mamba block to\nextract highly transferable representations for efficient sequential\nprocessing, while cross-attention modules generate long-range, mixed-domain\nspatial features to enable robust soft alignment across domains. Besides,\nmotivated by the observation that hybrid architectures introduce feature noise\nin domain adaptation tasks, we propose an entropy-based knowledge distillation\nframework with margin ReLU, which adaptively refines multi-level\nrepresentations by suppressing irrelevant activations and aligning uncertainty\nacross source and target domains. Finally, to prevent overfitting caused by the\nmixed-up features generated through cross-attention mechanisms, we propose\nentropy-driven gating attention with random perturbations that simultaneously\nrefine target features and enhance model generalization.\n", "link": "http://arxiv.org/abs/2502.11178v2", "date": "2025-05-07", "relevancy": 2.2165, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5757}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5426}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA-Mamba%3A%20Domain%20Adaptive%20Hybrid%20Mamba-Transformer%20Based%20One-Stage%0A%20%20Object%20Detection&body=Title%3A%20DA-Mamba%3A%20Domain%20Adaptive%20Hybrid%20Mamba-Transformer%20Based%20One-Stage%0A%20%20Object%20Detection%0AAuthor%3A%20A.%20Enes%20Doruk%20and%20Hasan%20F.%20Ates%0AAbstract%3A%20%20%20Recent%202D%20CNN-based%20domain%20adaptation%20approaches%20struggle%20with%20long-range%0Adependencies%20due%20to%20limited%20receptive%20fields%2C%20making%20it%20difficult%20to%20adapt%20to%0Atarget%20domains%20with%20significant%20spatial%20distribution%20changes.%20While%0Atransformer-based%20domain%20adaptation%20methods%20better%20capture%20distant%0Arelationships%20through%20self-attention%20mechanisms%20that%20facilitate%20more%20effective%0Across-domain%20feature%20alignment%2C%20their%20quadratic%20computational%20complexity%20makes%0Apractical%20deployment%20challenging%20for%20object%20detection%20tasks%20across%20diverse%0Adomains.%20Inspired%20by%20the%20global%20modeling%20and%20linear%20computation%20complexity%20of%0Athe%20Mamba%20architecture%2C%20we%20present%20the%20first%20domain-adaptive%20Mamba-based%0Aone-stage%20object%20detection%20model%2C%20termed%20DA-Mamba.%20Specifically%2C%20we%20combine%0AMamba%27s%20efficient%20state-space%20modeling%20with%20attention%20mechanisms%20to%20address%0Adomain-specific%20spatial%20and%20channel-wise%20variations.%20Our%20design%20leverages%0Adomain-adaptive%20spatial%20and%20channel-wise%20scanning%20within%20the%20Mamba%20block%20to%0Aextract%20highly%20transferable%20representations%20for%20efficient%20sequential%0Aprocessing%2C%20while%20cross-attention%20modules%20generate%20long-range%2C%20mixed-domain%0Aspatial%20features%20to%20enable%20robust%20soft%20alignment%20across%20domains.%20Besides%2C%0Amotivated%20by%20the%20observation%20that%20hybrid%20architectures%20introduce%20feature%20noise%0Ain%20domain%20adaptation%20tasks%2C%20we%20propose%20an%20entropy-based%20knowledge%20distillation%0Aframework%20with%20margin%20ReLU%2C%20which%20adaptively%20refines%20multi-level%0Arepresentations%20by%20suppressing%20irrelevant%20activations%20and%20aligning%20uncertainty%0Aacross%20source%20and%20target%20domains.%20Finally%2C%20to%20prevent%20overfitting%20caused%20by%20the%0Amixed-up%20features%20generated%20through%20cross-attention%20mechanisms%2C%20we%20propose%0Aentropy-driven%20gating%20attention%20with%20random%20perturbations%20that%20simultaneously%0Arefine%20target%20features%20and%20enhance%20model%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.11178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA-Mamba%253A%2520Domain%2520Adaptive%2520Hybrid%2520Mamba-Transformer%2520Based%2520One-Stage%250A%2520%2520Object%2520Detection%26entry.906535625%3DA.%2520Enes%2520Doruk%2520and%2520Hasan%2520F.%2520Ates%26entry.1292438233%3D%2520%2520Recent%25202D%2520CNN-based%2520domain%2520adaptation%2520approaches%2520struggle%2520with%2520long-range%250Adependencies%2520due%2520to%2520limited%2520receptive%2520fields%252C%2520making%2520it%2520difficult%2520to%2520adapt%2520to%250Atarget%2520domains%2520with%2520significant%2520spatial%2520distribution%2520changes.%2520While%250Atransformer-based%2520domain%2520adaptation%2520methods%2520better%2520capture%2520distant%250Arelationships%2520through%2520self-attention%2520mechanisms%2520that%2520facilitate%2520more%2520effective%250Across-domain%2520feature%2520alignment%252C%2520their%2520quadratic%2520computational%2520complexity%2520makes%250Apractical%2520deployment%2520challenging%2520for%2520object%2520detection%2520tasks%2520across%2520diverse%250Adomains.%2520Inspired%2520by%2520the%2520global%2520modeling%2520and%2520linear%2520computation%2520complexity%2520of%250Athe%2520Mamba%2520architecture%252C%2520we%2520present%2520the%2520first%2520domain-adaptive%2520Mamba-based%250Aone-stage%2520object%2520detection%2520model%252C%2520termed%2520DA-Mamba.%2520Specifically%252C%2520we%2520combine%250AMamba%2527s%2520efficient%2520state-space%2520modeling%2520with%2520attention%2520mechanisms%2520to%2520address%250Adomain-specific%2520spatial%2520and%2520channel-wise%2520variations.%2520Our%2520design%2520leverages%250Adomain-adaptive%2520spatial%2520and%2520channel-wise%2520scanning%2520within%2520the%2520Mamba%2520block%2520to%250Aextract%2520highly%2520transferable%2520representations%2520for%2520efficient%2520sequential%250Aprocessing%252C%2520while%2520cross-attention%2520modules%2520generate%2520long-range%252C%2520mixed-domain%250Aspatial%2520features%2520to%2520enable%2520robust%2520soft%2520alignment%2520across%2520domains.%2520Besides%252C%250Amotivated%2520by%2520the%2520observation%2520that%2520hybrid%2520architectures%2520introduce%2520feature%2520noise%250Ain%2520domain%2520adaptation%2520tasks%252C%2520we%2520propose%2520an%2520entropy-based%2520knowledge%2520distillation%250Aframework%2520with%2520margin%2520ReLU%252C%2520which%2520adaptively%2520refines%2520multi-level%250Arepresentations%2520by%2520suppressing%2520irrelevant%2520activations%2520and%2520aligning%2520uncertainty%250Aacross%2520source%2520and%2520target%2520domains.%2520Finally%252C%2520to%2520prevent%2520overfitting%2520caused%2520by%2520the%250Amixed-up%2520features%2520generated%2520through%2520cross-attention%2520mechanisms%252C%2520we%2520propose%250Aentropy-driven%2520gating%2520attention%2520with%2520random%2520perturbations%2520that%2520simultaneously%250Arefine%2520target%2520features%2520and%2520enhance%2520model%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.11178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-Mamba%3A%20Domain%20Adaptive%20Hybrid%20Mamba-Transformer%20Based%20One-Stage%0A%20%20Object%20Detection&entry.906535625=A.%20Enes%20Doruk%20and%20Hasan%20F.%20Ates&entry.1292438233=%20%20Recent%202D%20CNN-based%20domain%20adaptation%20approaches%20struggle%20with%20long-range%0Adependencies%20due%20to%20limited%20receptive%20fields%2C%20making%20it%20difficult%20to%20adapt%20to%0Atarget%20domains%20with%20significant%20spatial%20distribution%20changes.%20While%0Atransformer-based%20domain%20adaptation%20methods%20better%20capture%20distant%0Arelationships%20through%20self-attention%20mechanisms%20that%20facilitate%20more%20effective%0Across-domain%20feature%20alignment%2C%20their%20quadratic%20computational%20complexity%20makes%0Apractical%20deployment%20challenging%20for%20object%20detection%20tasks%20across%20diverse%0Adomains.%20Inspired%20by%20the%20global%20modeling%20and%20linear%20computation%20complexity%20of%0Athe%20Mamba%20architecture%2C%20we%20present%20the%20first%20domain-adaptive%20Mamba-based%0Aone-stage%20object%20detection%20model%2C%20termed%20DA-Mamba.%20Specifically%2C%20we%20combine%0AMamba%27s%20efficient%20state-space%20modeling%20with%20attention%20mechanisms%20to%20address%0Adomain-specific%20spatial%20and%20channel-wise%20variations.%20Our%20design%20leverages%0Adomain-adaptive%20spatial%20and%20channel-wise%20scanning%20within%20the%20Mamba%20block%20to%0Aextract%20highly%20transferable%20representations%20for%20efficient%20sequential%0Aprocessing%2C%20while%20cross-attention%20modules%20generate%20long-range%2C%20mixed-domain%0Aspatial%20features%20to%20enable%20robust%20soft%20alignment%20across%20domains.%20Besides%2C%0Amotivated%20by%20the%20observation%20that%20hybrid%20architectures%20introduce%20feature%20noise%0Ain%20domain%20adaptation%20tasks%2C%20we%20propose%20an%20entropy-based%20knowledge%20distillation%0Aframework%20with%20margin%20ReLU%2C%20which%20adaptively%20refines%20multi-level%0Arepresentations%20by%20suppressing%20irrelevant%20activations%20and%20aligning%20uncertainty%0Aacross%20source%20and%20target%20domains.%20Finally%2C%20to%20prevent%20overfitting%20caused%20by%20the%0Amixed-up%20features%20generated%20through%20cross-attention%20mechanisms%2C%20we%20propose%0Aentropy-driven%20gating%20attention%20with%20random%20perturbations%20that%20simultaneously%0Arefine%20target%20features%20and%20enhance%20model%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.11178v2&entry.124074799=Read"},
{"title": "Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners", "author": "Hung Manh Pham and Aaqib Saeed and Dong Ma", "abstract": "  The accurate interpretation of Electrocardiogram (ECG) signals is pivotal for\ndiagnosing cardiovascular diseases. Integrating ECG signals with accompanying\ntextual reports further holds immense potential to enhance clinical diagnostics\nby combining physiological data and qualitative insights. However, this\nintegration faces significant challenges due to inherent modality disparities\nand the scarcity of labeled data for robust cross-modal learning. To address\nthese obstacles, we propose D-BETA, a novel framework that pre-trains ECG and\ntext data using a contrastive masked auto-encoder architecture. D-BETA uniquely\ncombines the strengths of generative with boosted discriminative capabilities\nto achieve robust cross-modal representations. This is accomplished through\nmasked modality modeling, specialized loss functions, and an improved negative\nsampling strategy tailored for cross-modal alignment. Extensive experiments on\nfive public datasets across diverse downstream tasks demonstrate that D-BETA\nsignificantly outperforms existing methods, achieving an average AUC\nimprovement of 15% in linear probing with only one percent of training data and\n2% in zero-shot performance without requiring training data over\nstate-of-the-art models. These results highlight the effectiveness of D-BETA,\nunderscoring its potential to advance automated clinical diagnostics through\nmulti-modal representations. Our sample code and checkpoint are made available\nat https://github.com/manhph2211/D-BETA.\n", "link": "http://arxiv.org/abs/2410.02131v3", "date": "2025-05-07", "relevancy": 2.1953, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5647}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5586}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Masked%20ECG-Text%20Auto-Encoders%20as%20Discriminative%20Learners&body=Title%3A%20Boosting%20Masked%20ECG-Text%20Auto-Encoders%20as%20Discriminative%20Learners%0AAuthor%3A%20Hung%20Manh%20Pham%20and%20Aaqib%20Saeed%20and%20Dong%20Ma%0AAbstract%3A%20%20%20The%20accurate%20interpretation%20of%20Electrocardiogram%20%28ECG%29%20signals%20is%20pivotal%20for%0Adiagnosing%20cardiovascular%20diseases.%20Integrating%20ECG%20signals%20with%20accompanying%0Atextual%20reports%20further%20holds%20immense%20potential%20to%20enhance%20clinical%20diagnostics%0Aby%20combining%20physiological%20data%20and%20qualitative%20insights.%20However%2C%20this%0Aintegration%20faces%20significant%20challenges%20due%20to%20inherent%20modality%20disparities%0Aand%20the%20scarcity%20of%20labeled%20data%20for%20robust%20cross-modal%20learning.%20To%20address%0Athese%20obstacles%2C%20we%20propose%20D-BETA%2C%20a%20novel%20framework%20that%20pre-trains%20ECG%20and%0Atext%20data%20using%20a%20contrastive%20masked%20auto-encoder%20architecture.%20D-BETA%20uniquely%0Acombines%20the%20strengths%20of%20generative%20with%20boosted%20discriminative%20capabilities%0Ato%20achieve%20robust%20cross-modal%20representations.%20This%20is%20accomplished%20through%0Amasked%20modality%20modeling%2C%20specialized%20loss%20functions%2C%20and%20an%20improved%20negative%0Asampling%20strategy%20tailored%20for%20cross-modal%20alignment.%20Extensive%20experiments%20on%0Afive%20public%20datasets%20across%20diverse%20downstream%20tasks%20demonstrate%20that%20D-BETA%0Asignificantly%20outperforms%20existing%20methods%2C%20achieving%20an%20average%20AUC%0Aimprovement%20of%2015%25%20in%20linear%20probing%20with%20only%20one%20percent%20of%20training%20data%20and%0A2%25%20in%20zero-shot%20performance%20without%20requiring%20training%20data%20over%0Astate-of-the-art%20models.%20These%20results%20highlight%20the%20effectiveness%20of%20D-BETA%2C%0Aunderscoring%20its%20potential%20to%20advance%20automated%20clinical%20diagnostics%20through%0Amulti-modal%20representations.%20Our%20sample%20code%20and%20checkpoint%20are%20made%20available%0Aat%20https%3A//github.com/manhph2211/D-BETA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02131v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Masked%2520ECG-Text%2520Auto-Encoders%2520as%2520Discriminative%2520Learners%26entry.906535625%3DHung%2520Manh%2520Pham%2520and%2520Aaqib%2520Saeed%2520and%2520Dong%2520Ma%26entry.1292438233%3D%2520%2520The%2520accurate%2520interpretation%2520of%2520Electrocardiogram%2520%2528ECG%2529%2520signals%2520is%2520pivotal%2520for%250Adiagnosing%2520cardiovascular%2520diseases.%2520Integrating%2520ECG%2520signals%2520with%2520accompanying%250Atextual%2520reports%2520further%2520holds%2520immense%2520potential%2520to%2520enhance%2520clinical%2520diagnostics%250Aby%2520combining%2520physiological%2520data%2520and%2520qualitative%2520insights.%2520However%252C%2520this%250Aintegration%2520faces%2520significant%2520challenges%2520due%2520to%2520inherent%2520modality%2520disparities%250Aand%2520the%2520scarcity%2520of%2520labeled%2520data%2520for%2520robust%2520cross-modal%2520learning.%2520To%2520address%250Athese%2520obstacles%252C%2520we%2520propose%2520D-BETA%252C%2520a%2520novel%2520framework%2520that%2520pre-trains%2520ECG%2520and%250Atext%2520data%2520using%2520a%2520contrastive%2520masked%2520auto-encoder%2520architecture.%2520D-BETA%2520uniquely%250Acombines%2520the%2520strengths%2520of%2520generative%2520with%2520boosted%2520discriminative%2520capabilities%250Ato%2520achieve%2520robust%2520cross-modal%2520representations.%2520This%2520is%2520accomplished%2520through%250Amasked%2520modality%2520modeling%252C%2520specialized%2520loss%2520functions%252C%2520and%2520an%2520improved%2520negative%250Asampling%2520strategy%2520tailored%2520for%2520cross-modal%2520alignment.%2520Extensive%2520experiments%2520on%250Afive%2520public%2520datasets%2520across%2520diverse%2520downstream%2520tasks%2520demonstrate%2520that%2520D-BETA%250Asignificantly%2520outperforms%2520existing%2520methods%252C%2520achieving%2520an%2520average%2520AUC%250Aimprovement%2520of%252015%2525%2520in%2520linear%2520probing%2520with%2520only%2520one%2520percent%2520of%2520training%2520data%2520and%250A2%2525%2520in%2520zero-shot%2520performance%2520without%2520requiring%2520training%2520data%2520over%250Astate-of-the-art%2520models.%2520These%2520results%2520highlight%2520the%2520effectiveness%2520of%2520D-BETA%252C%250Aunderscoring%2520its%2520potential%2520to%2520advance%2520automated%2520clinical%2520diagnostics%2520through%250Amulti-modal%2520representations.%2520Our%2520sample%2520code%2520and%2520checkpoint%2520are%2520made%2520available%250Aat%2520https%253A//github.com/manhph2211/D-BETA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02131v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Masked%20ECG-Text%20Auto-Encoders%20as%20Discriminative%20Learners&entry.906535625=Hung%20Manh%20Pham%20and%20Aaqib%20Saeed%20and%20Dong%20Ma&entry.1292438233=%20%20The%20accurate%20interpretation%20of%20Electrocardiogram%20%28ECG%29%20signals%20is%20pivotal%20for%0Adiagnosing%20cardiovascular%20diseases.%20Integrating%20ECG%20signals%20with%20accompanying%0Atextual%20reports%20further%20holds%20immense%20potential%20to%20enhance%20clinical%20diagnostics%0Aby%20combining%20physiological%20data%20and%20qualitative%20insights.%20However%2C%20this%0Aintegration%20faces%20significant%20challenges%20due%20to%20inherent%20modality%20disparities%0Aand%20the%20scarcity%20of%20labeled%20data%20for%20robust%20cross-modal%20learning.%20To%20address%0Athese%20obstacles%2C%20we%20propose%20D-BETA%2C%20a%20novel%20framework%20that%20pre-trains%20ECG%20and%0Atext%20data%20using%20a%20contrastive%20masked%20auto-encoder%20architecture.%20D-BETA%20uniquely%0Acombines%20the%20strengths%20of%20generative%20with%20boosted%20discriminative%20capabilities%0Ato%20achieve%20robust%20cross-modal%20representations.%20This%20is%20accomplished%20through%0Amasked%20modality%20modeling%2C%20specialized%20loss%20functions%2C%20and%20an%20improved%20negative%0Asampling%20strategy%20tailored%20for%20cross-modal%20alignment.%20Extensive%20experiments%20on%0Afive%20public%20datasets%20across%20diverse%20downstream%20tasks%20demonstrate%20that%20D-BETA%0Asignificantly%20outperforms%20existing%20methods%2C%20achieving%20an%20average%20AUC%0Aimprovement%20of%2015%25%20in%20linear%20probing%20with%20only%20one%20percent%20of%20training%20data%20and%0A2%25%20in%20zero-shot%20performance%20without%20requiring%20training%20data%20over%0Astate-of-the-art%20models.%20These%20results%20highlight%20the%20effectiveness%20of%20D-BETA%2C%0Aunderscoring%20its%20potential%20to%20advance%20automated%20clinical%20diagnostics%20through%0Amulti-modal%20representations.%20Our%20sample%20code%20and%20checkpoint%20are%20made%20available%0Aat%20https%3A//github.com/manhph2211/D-BETA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02131v3&entry.124074799=Read"},
{"title": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs", "author": "Mirazul Haque and Petr Babkin and Farima Farmahinifarahani and Manuela Veloso", "abstract": "  Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs.\n", "link": "http://arxiv.org/abs/2505.04441v1", "date": "2025-05-07", "relevancy": 2.1936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4395}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4395}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Effectively%20Leveraging%20Execution%20Traces%20for%20Program%20Repair%20with%0A%20%20Code%20LLMs&body=Title%3A%20Towards%20Effectively%20Leveraging%20Execution%20Traces%20for%20Program%20Repair%20with%0A%20%20Code%20LLMs%0AAuthor%3A%20Mirazul%20Haque%20and%20Petr%20Babkin%20and%20Farima%20Farmahinifarahani%20and%20Manuela%20Veloso%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20show%20promising%20performance%20on%20various%0Aprogramming%20tasks%2C%20including%20Automatic%20Program%20Repair%20%28APR%29.%20However%2C%20most%0Aapproaches%20to%20LLM-based%20APR%20are%20limited%20to%20the%20static%20analysis%20of%20the%20programs%2C%0Awhile%20disregarding%20their%20runtime%20behavior.%20Inspired%20by%20knowledge-augmented%20NLP%2C%0Ain%20this%20work%2C%20we%20aim%20to%20remedy%20this%20potential%20blind%20spot%20by%20augmenting%20standard%0AAPR%20prompts%20with%20program%20execution%20traces.%20We%20evaluate%20our%20approach%20using%20the%0AGPT%20family%20of%20models%20on%20three%20popular%20APR%20datasets.%20Our%20findings%20suggest%20that%0Asimply%20incorporating%20execution%20traces%20into%20the%20prompt%20provides%20a%20limited%0Aperformance%20improvement%20over%20trace-free%20baselines%2C%20in%20only%202%20out%20of%206%20tested%0Adataset%20/%20model%20configurations.%20We%20further%20find%20that%20the%20effectiveness%20of%0Aexecution%20traces%20for%20APR%20diminishes%20as%20their%20complexity%20increases.%20We%20explore%0Aseveral%20strategies%20for%20leveraging%20traces%20in%20prompts%20and%20demonstrate%20that%0ALLM-optimized%20prompts%20help%20outperform%20trace-free%20prompts%20more%20consistently.%0AAdditionally%2C%20we%20show%20trace-based%20prompting%20to%20be%20superior%20to%20finetuning%20a%0Asmaller%20LLM%20on%20a%20small-scale%20dataset%3B%20and%20conduct%20probing%20studies%20reinforcing%0Athe%20notion%20that%20execution%20traces%20can%20complement%20the%20reasoning%20abilities%20of%20the%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Effectively%2520Leveraging%2520Execution%2520Traces%2520for%2520Program%2520Repair%2520with%250A%2520%2520Code%2520LLMs%26entry.906535625%3DMirazul%2520Haque%2520and%2520Petr%2520Babkin%2520and%2520Farima%2520Farmahinifarahani%2520and%2520Manuela%2520Veloso%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520show%2520promising%2520performance%2520on%2520various%250Aprogramming%2520tasks%252C%2520including%2520Automatic%2520Program%2520Repair%2520%2528APR%2529.%2520However%252C%2520most%250Aapproaches%2520to%2520LLM-based%2520APR%2520are%2520limited%2520to%2520the%2520static%2520analysis%2520of%2520the%2520programs%252C%250Awhile%2520disregarding%2520their%2520runtime%2520behavior.%2520Inspired%2520by%2520knowledge-augmented%2520NLP%252C%250Ain%2520this%2520work%252C%2520we%2520aim%2520to%2520remedy%2520this%2520potential%2520blind%2520spot%2520by%2520augmenting%2520standard%250AAPR%2520prompts%2520with%2520program%2520execution%2520traces.%2520We%2520evaluate%2520our%2520approach%2520using%2520the%250AGPT%2520family%2520of%2520models%2520on%2520three%2520popular%2520APR%2520datasets.%2520Our%2520findings%2520suggest%2520that%250Asimply%2520incorporating%2520execution%2520traces%2520into%2520the%2520prompt%2520provides%2520a%2520limited%250Aperformance%2520improvement%2520over%2520trace-free%2520baselines%252C%2520in%2520only%25202%2520out%2520of%25206%2520tested%250Adataset%2520/%2520model%2520configurations.%2520We%2520further%2520find%2520that%2520the%2520effectiveness%2520of%250Aexecution%2520traces%2520for%2520APR%2520diminishes%2520as%2520their%2520complexity%2520increases.%2520We%2520explore%250Aseveral%2520strategies%2520for%2520leveraging%2520traces%2520in%2520prompts%2520and%2520demonstrate%2520that%250ALLM-optimized%2520prompts%2520help%2520outperform%2520trace-free%2520prompts%2520more%2520consistently.%250AAdditionally%252C%2520we%2520show%2520trace-based%2520prompting%2520to%2520be%2520superior%2520to%2520finetuning%2520a%250Asmaller%2520LLM%2520on%2520a%2520small-scale%2520dataset%253B%2520and%2520conduct%2520probing%2520studies%2520reinforcing%250Athe%2520notion%2520that%2520execution%2520traces%2520can%2520complement%2520the%2520reasoning%2520abilities%2520of%2520the%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Effectively%20Leveraging%20Execution%20Traces%20for%20Program%20Repair%20with%0A%20%20Code%20LLMs&entry.906535625=Mirazul%20Haque%20and%20Petr%20Babkin%20and%20Farima%20Farmahinifarahani%20and%20Manuela%20Veloso&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20show%20promising%20performance%20on%20various%0Aprogramming%20tasks%2C%20including%20Automatic%20Program%20Repair%20%28APR%29.%20However%2C%20most%0Aapproaches%20to%20LLM-based%20APR%20are%20limited%20to%20the%20static%20analysis%20of%20the%20programs%2C%0Awhile%20disregarding%20their%20runtime%20behavior.%20Inspired%20by%20knowledge-augmented%20NLP%2C%0Ain%20this%20work%2C%20we%20aim%20to%20remedy%20this%20potential%20blind%20spot%20by%20augmenting%20standard%0AAPR%20prompts%20with%20program%20execution%20traces.%20We%20evaluate%20our%20approach%20using%20the%0AGPT%20family%20of%20models%20on%20three%20popular%20APR%20datasets.%20Our%20findings%20suggest%20that%0Asimply%20incorporating%20execution%20traces%20into%20the%20prompt%20provides%20a%20limited%0Aperformance%20improvement%20over%20trace-free%20baselines%2C%20in%20only%202%20out%20of%206%20tested%0Adataset%20/%20model%20configurations.%20We%20further%20find%20that%20the%20effectiveness%20of%0Aexecution%20traces%20for%20APR%20diminishes%20as%20their%20complexity%20increases.%20We%20explore%0Aseveral%20strategies%20for%20leveraging%20traces%20in%20prompts%20and%20demonstrate%20that%0ALLM-optimized%20prompts%20help%20outperform%20trace-free%20prompts%20more%20consistently.%0AAdditionally%2C%20we%20show%20trace-based%20prompting%20to%20be%20superior%20to%20finetuning%20a%0Asmaller%20LLM%20on%20a%20small-scale%20dataset%3B%20and%20conduct%20probing%20studies%20reinforcing%0Athe%20notion%20that%20execution%20traces%20can%20complement%20the%20reasoning%20abilities%20of%20the%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04441v1&entry.124074799=Read"},
{"title": "Estimating LLM Uncertainty with Logits", "author": "Huan Ma and Jingdong Chen and Joey Tianyi Zhou and Guangyu Wang and Changqing Zhang", "abstract": "  Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.\n", "link": "http://arxiv.org/abs/2502.00290v4", "date": "2025-05-07", "relevancy": 2.1906, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.614}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5384}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20LLM%20Uncertainty%20with%20Logits&body=Title%3A%20Estimating%20LLM%20Uncertainty%20with%20Logits%0AAuthor%3A%20Huan%20Ma%20and%20Jingdong%20Chen%20and%20Joey%20Tianyi%20Zhou%20and%20Guangyu%20Wang%20and%20Changqing%20Zhang%0AAbstract%3A%20%20%20Over%20the%20past%20few%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20developed%20rapidly%0Aand%20are%20widely%20applied%20in%20various%20domains.%20However%2C%20LLMs%20face%20the%20issue%20of%0Ahallucinations%2C%20generating%20responses%20that%20may%20be%20unreliable%20when%20the%20models%0Alack%20relevant%20knowledge.%20To%20be%20aware%20of%20potential%20hallucinations%2C%20uncertainty%0Aestimation%20methods%20have%20been%20introduced%2C%20and%20most%20of%20them%20have%20confirmed%20that%0Areliability%20lies%20in%20critical%20tokens.%20However%2C%20probability-based%20methods%20perform%0Apoorly%20in%20identifying%20token%20reliability%2C%20limiting%20their%20practical%20utility.%20In%0Athis%20paper%2C%20we%20reveal%20that%20the%20probability-based%20method%20fails%20to%20estimate%20token%0Areliability%20due%20to%20the%20loss%20of%20evidence%20strength%20information%20which%20is%0Aaccumulated%20in%20the%20training%20stage.%20Therefore%2C%20we%20present%20Logits-induced%20token%0Auncertainty%20%28LogTokU%29%2C%20a%20framework%20for%20estimating%20decoupled%20token%20uncertainty%0Ain%20LLMs%2C%20enabling%20real-time%20uncertainty%20estimation%20without%20requiring%20multiple%0Asampling%20processes.%20We%20employ%20evidence%20modeling%20to%20implement%20LogTokU%20and%20use%0Athe%20estimated%20uncertainty%20to%20guide%20downstream%20tasks.%20The%20experimental%20results%0Ademonstrate%20that%20LogTokU%20has%20significant%20effectiveness%20and%20promise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00290v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520LLM%2520Uncertainty%2520with%2520Logits%26entry.906535625%3DHuan%2520Ma%2520and%2520Jingdong%2520Chen%2520and%2520Joey%2520Tianyi%2520Zhou%2520and%2520Guangyu%2520Wang%2520and%2520Changqing%2520Zhang%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520few%2520years%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520developed%2520rapidly%250Aand%2520are%2520widely%2520applied%2520in%2520various%2520domains.%2520However%252C%2520LLMs%2520face%2520the%2520issue%2520of%250Ahallucinations%252C%2520generating%2520responses%2520that%2520may%2520be%2520unreliable%2520when%2520the%2520models%250Alack%2520relevant%2520knowledge.%2520To%2520be%2520aware%2520of%2520potential%2520hallucinations%252C%2520uncertainty%250Aestimation%2520methods%2520have%2520been%2520introduced%252C%2520and%2520most%2520of%2520them%2520have%2520confirmed%2520that%250Areliability%2520lies%2520in%2520critical%2520tokens.%2520However%252C%2520probability-based%2520methods%2520perform%250Apoorly%2520in%2520identifying%2520token%2520reliability%252C%2520limiting%2520their%2520practical%2520utility.%2520In%250Athis%2520paper%252C%2520we%2520reveal%2520that%2520the%2520probability-based%2520method%2520fails%2520to%2520estimate%2520token%250Areliability%2520due%2520to%2520the%2520loss%2520of%2520evidence%2520strength%2520information%2520which%2520is%250Aaccumulated%2520in%2520the%2520training%2520stage.%2520Therefore%252C%2520we%2520present%2520Logits-induced%2520token%250Auncertainty%2520%2528LogTokU%2529%252C%2520a%2520framework%2520for%2520estimating%2520decoupled%2520token%2520uncertainty%250Ain%2520LLMs%252C%2520enabling%2520real-time%2520uncertainty%2520estimation%2520without%2520requiring%2520multiple%250Asampling%2520processes.%2520We%2520employ%2520evidence%2520modeling%2520to%2520implement%2520LogTokU%2520and%2520use%250Athe%2520estimated%2520uncertainty%2520to%2520guide%2520downstream%2520tasks.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520LogTokU%2520has%2520significant%2520effectiveness%2520and%2520promise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00290v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20LLM%20Uncertainty%20with%20Logits&entry.906535625=Huan%20Ma%20and%20Jingdong%20Chen%20and%20Joey%20Tianyi%20Zhou%20and%20Guangyu%20Wang%20and%20Changqing%20Zhang&entry.1292438233=%20%20Over%20the%20past%20few%20years%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20developed%20rapidly%0Aand%20are%20widely%20applied%20in%20various%20domains.%20However%2C%20LLMs%20face%20the%20issue%20of%0Ahallucinations%2C%20generating%20responses%20that%20may%20be%20unreliable%20when%20the%20models%0Alack%20relevant%20knowledge.%20To%20be%20aware%20of%20potential%20hallucinations%2C%20uncertainty%0Aestimation%20methods%20have%20been%20introduced%2C%20and%20most%20of%20them%20have%20confirmed%20that%0Areliability%20lies%20in%20critical%20tokens.%20However%2C%20probability-based%20methods%20perform%0Apoorly%20in%20identifying%20token%20reliability%2C%20limiting%20their%20practical%20utility.%20In%0Athis%20paper%2C%20we%20reveal%20that%20the%20probability-based%20method%20fails%20to%20estimate%20token%0Areliability%20due%20to%20the%20loss%20of%20evidence%20strength%20information%20which%20is%0Aaccumulated%20in%20the%20training%20stage.%20Therefore%2C%20we%20present%20Logits-induced%20token%0Auncertainty%20%28LogTokU%29%2C%20a%20framework%20for%20estimating%20decoupled%20token%20uncertainty%0Ain%20LLMs%2C%20enabling%20real-time%20uncertainty%20estimation%20without%20requiring%20multiple%0Asampling%20processes.%20We%20employ%20evidence%20modeling%20to%20implement%20LogTokU%20and%20use%0Athe%20estimated%20uncertainty%20to%20guide%20downstream%20tasks.%20The%20experimental%20results%0Ademonstrate%20that%20LogTokU%20has%20significant%20effectiveness%20and%20promise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00290v4&entry.124074799=Read"},
{"title": "Evaluation Framework for Sensor Configuration Impact on Deep\n  Learning-Based Perception", "author": "A Gamage and V Donzella", "abstract": "  Current research on automotive perception systems predominantly focusses on\neither improving the performance of sensor technology or enhancing the\nperception functions in isolation. High-level perception functions are\nincreasingly based on deep learning (DL) models due to their improved\nperformance and generalisability compared to traditional algorithms. Despite\nthe vital need to evaluate the performance of DL-based perception functions\nunder real-world conditions using onboard sensor inputs, there is a lack of\nframeworks to implement such systematic evaluations. This paper presents a\nversatile framework to evaluate the impact of perception sensor modalities and\nparameter settings on DL-based perception functions. Using a simulation\nenvironment, the framework facilitates sensor modality selection and parameter\ntuning under different operational design domain conditions. Its effectiveness\nis demonstrated through a case study involving a state-of-the-art surround\ntrajectory prediction model, highlighting performance differences across the\nsensor modalities radar and camera. Different settings for the parameter,\nhorizontal field of view (HFOV) were evaluated to identify the optimal\nconfiguration. The results indicate that a radar sensor with a narrow HFOV is\nthe most suitable configuration for the evaluated perception algorithm. The\nproposed framework offers a holistic approach to the design of the perception\nsensor suite, significantly contributing to the development of robust\nperception systems for automated driving systems.\n", "link": "http://arxiv.org/abs/2503.05939v2", "date": "2025-05-07", "relevancy": 2.1904, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5493}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20Framework%20for%20Sensor%20Configuration%20Impact%20on%20Deep%0A%20%20Learning-Based%20Perception&body=Title%3A%20Evaluation%20Framework%20for%20Sensor%20Configuration%20Impact%20on%20Deep%0A%20%20Learning-Based%20Perception%0AAuthor%3A%20A%20Gamage%20and%20V%20Donzella%0AAbstract%3A%20%20%20Current%20research%20on%20automotive%20perception%20systems%20predominantly%20focusses%20on%0Aeither%20improving%20the%20performance%20of%20sensor%20technology%20or%20enhancing%20the%0Aperception%20functions%20in%20isolation.%20High-level%20perception%20functions%20are%0Aincreasingly%20based%20on%20deep%20learning%20%28DL%29%20models%20due%20to%20their%20improved%0Aperformance%20and%20generalisability%20compared%20to%20traditional%20algorithms.%20Despite%0Athe%20vital%20need%20to%20evaluate%20the%20performance%20of%20DL-based%20perception%20functions%0Aunder%20real-world%20conditions%20using%20onboard%20sensor%20inputs%2C%20there%20is%20a%20lack%20of%0Aframeworks%20to%20implement%20such%20systematic%20evaluations.%20This%20paper%20presents%20a%0Aversatile%20framework%20to%20evaluate%20the%20impact%20of%20perception%20sensor%20modalities%20and%0Aparameter%20settings%20on%20DL-based%20perception%20functions.%20Using%20a%20simulation%0Aenvironment%2C%20the%20framework%20facilitates%20sensor%20modality%20selection%20and%20parameter%0Atuning%20under%20different%20operational%20design%20domain%20conditions.%20Its%20effectiveness%0Ais%20demonstrated%20through%20a%20case%20study%20involving%20a%20state-of-the-art%20surround%0Atrajectory%20prediction%20model%2C%20highlighting%20performance%20differences%20across%20the%0Asensor%20modalities%20radar%20and%20camera.%20Different%20settings%20for%20the%20parameter%2C%0Ahorizontal%20field%20of%20view%20%28HFOV%29%20were%20evaluated%20to%20identify%20the%20optimal%0Aconfiguration.%20The%20results%20indicate%20that%20a%20radar%20sensor%20with%20a%20narrow%20HFOV%20is%0Athe%20most%20suitable%20configuration%20for%20the%20evaluated%20perception%20algorithm.%20The%0Aproposed%20framework%20offers%20a%20holistic%20approach%20to%20the%20design%20of%20the%20perception%0Asensor%20suite%2C%20significantly%20contributing%20to%20the%20development%20of%20robust%0Aperception%20systems%20for%20automated%20driving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520Framework%2520for%2520Sensor%2520Configuration%2520Impact%2520on%2520Deep%250A%2520%2520Learning-Based%2520Perception%26entry.906535625%3DA%2520Gamage%2520and%2520V%2520Donzella%26entry.1292438233%3D%2520%2520Current%2520research%2520on%2520automotive%2520perception%2520systems%2520predominantly%2520focusses%2520on%250Aeither%2520improving%2520the%2520performance%2520of%2520sensor%2520technology%2520or%2520enhancing%2520the%250Aperception%2520functions%2520in%2520isolation.%2520High-level%2520perception%2520functions%2520are%250Aincreasingly%2520based%2520on%2520deep%2520learning%2520%2528DL%2529%2520models%2520due%2520to%2520their%2520improved%250Aperformance%2520and%2520generalisability%2520compared%2520to%2520traditional%2520algorithms.%2520Despite%250Athe%2520vital%2520need%2520to%2520evaluate%2520the%2520performance%2520of%2520DL-based%2520perception%2520functions%250Aunder%2520real-world%2520conditions%2520using%2520onboard%2520sensor%2520inputs%252C%2520there%2520is%2520a%2520lack%2520of%250Aframeworks%2520to%2520implement%2520such%2520systematic%2520evaluations.%2520This%2520paper%2520presents%2520a%250Aversatile%2520framework%2520to%2520evaluate%2520the%2520impact%2520of%2520perception%2520sensor%2520modalities%2520and%250Aparameter%2520settings%2520on%2520DL-based%2520perception%2520functions.%2520Using%2520a%2520simulation%250Aenvironment%252C%2520the%2520framework%2520facilitates%2520sensor%2520modality%2520selection%2520and%2520parameter%250Atuning%2520under%2520different%2520operational%2520design%2520domain%2520conditions.%2520Its%2520effectiveness%250Ais%2520demonstrated%2520through%2520a%2520case%2520study%2520involving%2520a%2520state-of-the-art%2520surround%250Atrajectory%2520prediction%2520model%252C%2520highlighting%2520performance%2520differences%2520across%2520the%250Asensor%2520modalities%2520radar%2520and%2520camera.%2520Different%2520settings%2520for%2520the%2520parameter%252C%250Ahorizontal%2520field%2520of%2520view%2520%2528HFOV%2529%2520were%2520evaluated%2520to%2520identify%2520the%2520optimal%250Aconfiguration.%2520The%2520results%2520indicate%2520that%2520a%2520radar%2520sensor%2520with%2520a%2520narrow%2520HFOV%2520is%250Athe%2520most%2520suitable%2520configuration%2520for%2520the%2520evaluated%2520perception%2520algorithm.%2520The%250Aproposed%2520framework%2520offers%2520a%2520holistic%2520approach%2520to%2520the%2520design%2520of%2520the%2520perception%250Asensor%2520suite%252C%2520significantly%2520contributing%2520to%2520the%2520development%2520of%2520robust%250Aperception%2520systems%2520for%2520automated%2520driving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20Framework%20for%20Sensor%20Configuration%20Impact%20on%20Deep%0A%20%20Learning-Based%20Perception&entry.906535625=A%20Gamage%20and%20V%20Donzella&entry.1292438233=%20%20Current%20research%20on%20automotive%20perception%20systems%20predominantly%20focusses%20on%0Aeither%20improving%20the%20performance%20of%20sensor%20technology%20or%20enhancing%20the%0Aperception%20functions%20in%20isolation.%20High-level%20perception%20functions%20are%0Aincreasingly%20based%20on%20deep%20learning%20%28DL%29%20models%20due%20to%20their%20improved%0Aperformance%20and%20generalisability%20compared%20to%20traditional%20algorithms.%20Despite%0Athe%20vital%20need%20to%20evaluate%20the%20performance%20of%20DL-based%20perception%20functions%0Aunder%20real-world%20conditions%20using%20onboard%20sensor%20inputs%2C%20there%20is%20a%20lack%20of%0Aframeworks%20to%20implement%20such%20systematic%20evaluations.%20This%20paper%20presents%20a%0Aversatile%20framework%20to%20evaluate%20the%20impact%20of%20perception%20sensor%20modalities%20and%0Aparameter%20settings%20on%20DL-based%20perception%20functions.%20Using%20a%20simulation%0Aenvironment%2C%20the%20framework%20facilitates%20sensor%20modality%20selection%20and%20parameter%0Atuning%20under%20different%20operational%20design%20domain%20conditions.%20Its%20effectiveness%0Ais%20demonstrated%20through%20a%20case%20study%20involving%20a%20state-of-the-art%20surround%0Atrajectory%20prediction%20model%2C%20highlighting%20performance%20differences%20across%20the%0Asensor%20modalities%20radar%20and%20camera.%20Different%20settings%20for%20the%20parameter%2C%0Ahorizontal%20field%20of%20view%20%28HFOV%29%20were%20evaluated%20to%20identify%20the%20optimal%0Aconfiguration.%20The%20results%20indicate%20that%20a%20radar%20sensor%20with%20a%20narrow%20HFOV%20is%0Athe%20most%20suitable%20configuration%20for%20the%20evaluated%20perception%20algorithm.%20The%0Aproposed%20framework%20offers%20a%20holistic%20approach%20to%20the%20design%20of%20the%20perception%0Asensor%20suite%2C%20significantly%20contributing%20to%20the%20development%20of%20robust%0Aperception%20systems%20for%20automated%20driving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05939v2&entry.124074799=Read"},
{"title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild", "author": "Weihao Zeng and Yuzhen Huang and Qian Liu and Wei Liu and Keqing He and Zejun Ma and Junxian He", "abstract": "  DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.\n", "link": "http://arxiv.org/abs/2503.18892v2", "date": "2025-05-07", "relevancy": 2.1833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5512}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimpleRL-Zoo%3A%20Investigating%20and%20Taming%20Zero%20Reinforcement%20Learning%20for%0A%20%20Open%20Base%20Models%20in%20the%20Wild&body=Title%3A%20SimpleRL-Zoo%3A%20Investigating%20and%20Taming%20Zero%20Reinforcement%20Learning%20for%0A%20%20Open%20Base%20Models%20in%20the%20Wild%0AAuthor%3A%20Weihao%20Zeng%20and%20Yuzhen%20Huang%20and%20Qian%20Liu%20and%20Wei%20Liu%20and%20Keqing%20He%20and%20Zejun%20Ma%20and%20Junxian%20He%0AAbstract%3A%20%20%20DeepSeek-R1%20has%20shown%20that%20long%20chain-of-thought%20%28CoT%29%20reasoning%20can%0Anaturally%20emerge%20through%20a%20simple%20reinforcement%20learning%20%28RL%29%20framework%20with%0Arule-based%20rewards%2C%20where%20the%20training%20may%20directly%20start%20from%20the%20base%0Amodels-a%20paradigm%20referred%20to%20as%20zero%20RL%20training.%20Most%20recent%20efforts%20to%0Areproduce%20zero%20RL%20training%20have%20primarily%20focused%20on%20the%20Qwen2.5%20model%20series%2C%0Awhich%20may%20not%20be%20representative%20as%20we%20find%20the%20base%20models%20already%20exhibit%0Astrong%20instruction-following%20and%20self-reflection%20abilities.%20In%20this%20work%2C%20we%0Ainvestigate%20zero%20RL%20training%20across%2010%20diverse%20base%20models%2C%20spanning%20different%0Afamilies%20and%20sizes%20including%20LLama3-8B%2C%20Mistral-7B/24B%2C%20DeepSeek-Math-7B%2C%0AQwen2.5-math-7B%2C%20and%20all%20Qwen2.5%20models%20from%200.5B%20to%2032B.%20Leveraging%20several%0Akey%20design%20strategies-such%20as%20adjusting%20format%20reward%20and%20controlling%20query%0Adifficulty-we%20achieve%20substantial%20improvements%20in%20both%20reasoning%20accuracy%20and%0Aresponse%20length%20across%20most%20settings.%20However%2C%20by%20carefully%20monitoring%20the%0Atraining%20dynamics%2C%20we%20observe%20that%20different%20base%20models%20exhibit%20distinct%0Apatterns%20during%20training.%20For%20instance%2C%20the%20increased%20response%20length%20does%20not%0Aalways%20correlate%20with%20the%20emergence%20of%20certain%20cognitive%20behaviors%20such%20as%0Averification%20%28i.e.%2C%20the%20%22aha%20moment%22%29.%20Notably%2C%20we%20observe%20the%20%22aha%20moment%22%20for%0Athe%20first%20time%20in%20small%20models%20not%20from%20the%20Qwen%20family.%20We%20share%20the%20key%0Adesigns%20that%20enable%20successful%20zero%20RL%20training%2C%20along%20with%20our%20findings%20and%0Apractices.%20To%20facilitate%20further%20research%2C%20we%20open-source%20the%20code%2C%20models%2C%20and%0Aanalysis%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18892v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimpleRL-Zoo%253A%2520Investigating%2520and%2520Taming%2520Zero%2520Reinforcement%2520Learning%2520for%250A%2520%2520Open%2520Base%2520Models%2520in%2520the%2520Wild%26entry.906535625%3DWeihao%2520Zeng%2520and%2520Yuzhen%2520Huang%2520and%2520Qian%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Keqing%2520He%2520and%2520Zejun%2520Ma%2520and%2520Junxian%2520He%26entry.1292438233%3D%2520%2520DeepSeek-R1%2520has%2520shown%2520that%2520long%2520chain-of-thought%2520%2528CoT%2529%2520reasoning%2520can%250Anaturally%2520emerge%2520through%2520a%2520simple%2520reinforcement%2520learning%2520%2528RL%2529%2520framework%2520with%250Arule-based%2520rewards%252C%2520where%2520the%2520training%2520may%2520directly%2520start%2520from%2520the%2520base%250Amodels-a%2520paradigm%2520referred%2520to%2520as%2520zero%2520RL%2520training.%2520Most%2520recent%2520efforts%2520to%250Areproduce%2520zero%2520RL%2520training%2520have%2520primarily%2520focused%2520on%2520the%2520Qwen2.5%2520model%2520series%252C%250Awhich%2520may%2520not%2520be%2520representative%2520as%2520we%2520find%2520the%2520base%2520models%2520already%2520exhibit%250Astrong%2520instruction-following%2520and%2520self-reflection%2520abilities.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520zero%2520RL%2520training%2520across%252010%2520diverse%2520base%2520models%252C%2520spanning%2520different%250Afamilies%2520and%2520sizes%2520including%2520LLama3-8B%252C%2520Mistral-7B/24B%252C%2520DeepSeek-Math-7B%252C%250AQwen2.5-math-7B%252C%2520and%2520all%2520Qwen2.5%2520models%2520from%25200.5B%2520to%252032B.%2520Leveraging%2520several%250Akey%2520design%2520strategies-such%2520as%2520adjusting%2520format%2520reward%2520and%2520controlling%2520query%250Adifficulty-we%2520achieve%2520substantial%2520improvements%2520in%2520both%2520reasoning%2520accuracy%2520and%250Aresponse%2520length%2520across%2520most%2520settings.%2520However%252C%2520by%2520carefully%2520monitoring%2520the%250Atraining%2520dynamics%252C%2520we%2520observe%2520that%2520different%2520base%2520models%2520exhibit%2520distinct%250Apatterns%2520during%2520training.%2520For%2520instance%252C%2520the%2520increased%2520response%2520length%2520does%2520not%250Aalways%2520correlate%2520with%2520the%2520emergence%2520of%2520certain%2520cognitive%2520behaviors%2520such%2520as%250Averification%2520%2528i.e.%252C%2520the%2520%2522aha%2520moment%2522%2529.%2520Notably%252C%2520we%2520observe%2520the%2520%2522aha%2520moment%2522%2520for%250Athe%2520first%2520time%2520in%2520small%2520models%2520not%2520from%2520the%2520Qwen%2520family.%2520We%2520share%2520the%2520key%250Adesigns%2520that%2520enable%2520successful%2520zero%2520RL%2520training%252C%2520along%2520with%2520our%2520findings%2520and%250Apractices.%2520To%2520facilitate%2520further%2520research%252C%2520we%2520open-source%2520the%2520code%252C%2520models%252C%2520and%250Aanalysis%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18892v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimpleRL-Zoo%3A%20Investigating%20and%20Taming%20Zero%20Reinforcement%20Learning%20for%0A%20%20Open%20Base%20Models%20in%20the%20Wild&entry.906535625=Weihao%20Zeng%20and%20Yuzhen%20Huang%20and%20Qian%20Liu%20and%20Wei%20Liu%20and%20Keqing%20He%20and%20Zejun%20Ma%20and%20Junxian%20He&entry.1292438233=%20%20DeepSeek-R1%20has%20shown%20that%20long%20chain-of-thought%20%28CoT%29%20reasoning%20can%0Anaturally%20emerge%20through%20a%20simple%20reinforcement%20learning%20%28RL%29%20framework%20with%0Arule-based%20rewards%2C%20where%20the%20training%20may%20directly%20start%20from%20the%20base%0Amodels-a%20paradigm%20referred%20to%20as%20zero%20RL%20training.%20Most%20recent%20efforts%20to%0Areproduce%20zero%20RL%20training%20have%20primarily%20focused%20on%20the%20Qwen2.5%20model%20series%2C%0Awhich%20may%20not%20be%20representative%20as%20we%20find%20the%20base%20models%20already%20exhibit%0Astrong%20instruction-following%20and%20self-reflection%20abilities.%20In%20this%20work%2C%20we%0Ainvestigate%20zero%20RL%20training%20across%2010%20diverse%20base%20models%2C%20spanning%20different%0Afamilies%20and%20sizes%20including%20LLama3-8B%2C%20Mistral-7B/24B%2C%20DeepSeek-Math-7B%2C%0AQwen2.5-math-7B%2C%20and%20all%20Qwen2.5%20models%20from%200.5B%20to%2032B.%20Leveraging%20several%0Akey%20design%20strategies-such%20as%20adjusting%20format%20reward%20and%20controlling%20query%0Adifficulty-we%20achieve%20substantial%20improvements%20in%20both%20reasoning%20accuracy%20and%0Aresponse%20length%20across%20most%20settings.%20However%2C%20by%20carefully%20monitoring%20the%0Atraining%20dynamics%2C%20we%20observe%20that%20different%20base%20models%20exhibit%20distinct%0Apatterns%20during%20training.%20For%20instance%2C%20the%20increased%20response%20length%20does%20not%0Aalways%20correlate%20with%20the%20emergence%20of%20certain%20cognitive%20behaviors%20such%20as%0Averification%20%28i.e.%2C%20the%20%22aha%20moment%22%29.%20Notably%2C%20we%20observe%20the%20%22aha%20moment%22%20for%0Athe%20first%20time%20in%20small%20models%20not%20from%20the%20Qwen%20family.%20We%20share%20the%20key%0Adesigns%20that%20enable%20successful%20zero%20RL%20training%2C%20along%20with%20our%20findings%20and%0Apractices.%20To%20facilitate%20further%20research%2C%20we%20open-source%20the%20code%2C%20models%2C%20and%0Aanalysis%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18892v2&entry.124074799=Read"},
{"title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction", "author": "Inclusion AI and Biao Gong and Cheng Zou and Dandan Zheng and Hu Yu and Jingdong Chen and Jianxin Sun and Junbo Zhao and Jun Zhou and Kaixiang Ji and Lixiang Ru and Libin Wang and Qingpei Guo and Rui Liu and Weilong Chai and Xinyu Xiao and Ziyuan Huang", "abstract": "  We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.\n", "link": "http://arxiv.org/abs/2505.02471v2", "date": "2025-05-07", "relevancy": 2.177, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5353}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ming-Lite-Uni%3A%20Advancements%20in%20Unified%20Architecture%20for%20Natural%0A%20%20Multimodal%20Interaction&body=Title%3A%20Ming-Lite-Uni%3A%20Advancements%20in%20Unified%20Architecture%20for%20Natural%0A%20%20Multimodal%20Interaction%0AAuthor%3A%20Inclusion%20AI%20and%20Biao%20Gong%20and%20Cheng%20Zou%20and%20Dandan%20Zheng%20and%20Hu%20Yu%20and%20Jingdong%20Chen%20and%20Jianxin%20Sun%20and%20Junbo%20Zhao%20and%20Jun%20Zhou%20and%20Kaixiang%20Ji%20and%20Lixiang%20Ru%20and%20Libin%20Wang%20and%20Qingpei%20Guo%20and%20Rui%20Liu%20and%20Weilong%20Chai%20and%20Xinyu%20Xiao%20and%20Ziyuan%20Huang%0AAbstract%3A%20%20%20We%20introduce%20Ming-Lite-Uni%2C%20an%20open-source%20multimodal%20framework%20featuring%20a%0Anewly%20designed%20unified%20visual%20generator%20and%20a%20native%20multimodal%20autoregressive%0Amodel%20tailored%20for%20unifying%20vision%20and%20language.%20Specifically%2C%20this%20project%0Aprovides%20an%20open-source%20implementation%20of%20the%20integrated%20MetaQueries%20and%0AM2-omni%20framework%2C%20while%20introducing%20the%20novel%20multi-scale%20learnable%20tokens%20and%0Amulti-scale%20representation%20alignment%20strategy.%20By%20leveraging%20a%20fixed%20MLLM%20and%20a%0Alearnable%20diffusion%20model%2C%20Ming-Lite-Uni%20enables%20native%20multimodal%20AR%20models%20to%0Aperform%20both%20text-to-image%20generation%20and%20instruction%20based%20image%20editing%0Atasks%2C%20expanding%20their%20capabilities%20beyond%20pure%20visual%20understanding.%20Our%0Aexperimental%20results%20demonstrate%20the%20strong%20performance%20of%20Ming-Lite-Uni%20and%0Aillustrate%20the%20impressive%20fluid%20nature%20of%20its%20interactive%20process.%20All%20code%20and%0Amodel%20weights%20are%20open-sourced%20to%20foster%20further%20exploration%20within%20the%0Acommunity.%20Notably%2C%20this%20work%20aligns%20with%20concurrent%20multimodal%20AI%20milestones%20-%0Asuch%20as%20ChatGPT-4o%20with%20native%20image%20generation%20updated%20in%20March%2025%2C%202025%20-%0Aunderscoring%20the%20broader%20significance%20of%20unified%20models%20like%20Ming-Lite-Uni%20on%0Athe%20path%20toward%20AGI.%20Ming-Lite-Uni%20is%20in%20alpha%20stage%20and%20will%20soon%20be%20further%0Arefined.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02471v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMing-Lite-Uni%253A%2520Advancements%2520in%2520Unified%2520Architecture%2520for%2520Natural%250A%2520%2520Multimodal%2520Interaction%26entry.906535625%3DInclusion%2520AI%2520and%2520Biao%2520Gong%2520and%2520Cheng%2520Zou%2520and%2520Dandan%2520Zheng%2520and%2520Hu%2520Yu%2520and%2520Jingdong%2520Chen%2520and%2520Jianxin%2520Sun%2520and%2520Junbo%2520Zhao%2520and%2520Jun%2520Zhou%2520and%2520Kaixiang%2520Ji%2520and%2520Lixiang%2520Ru%2520and%2520Libin%2520Wang%2520and%2520Qingpei%2520Guo%2520and%2520Rui%2520Liu%2520and%2520Weilong%2520Chai%2520and%2520Xinyu%2520Xiao%2520and%2520Ziyuan%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Ming-Lite-Uni%252C%2520an%2520open-source%2520multimodal%2520framework%2520featuring%2520a%250Anewly%2520designed%2520unified%2520visual%2520generator%2520and%2520a%2520native%2520multimodal%2520autoregressive%250Amodel%2520tailored%2520for%2520unifying%2520vision%2520and%2520language.%2520Specifically%252C%2520this%2520project%250Aprovides%2520an%2520open-source%2520implementation%2520of%2520the%2520integrated%2520MetaQueries%2520and%250AM2-omni%2520framework%252C%2520while%2520introducing%2520the%2520novel%2520multi-scale%2520learnable%2520tokens%2520and%250Amulti-scale%2520representation%2520alignment%2520strategy.%2520By%2520leveraging%2520a%2520fixed%2520MLLM%2520and%2520a%250Alearnable%2520diffusion%2520model%252C%2520Ming-Lite-Uni%2520enables%2520native%2520multimodal%2520AR%2520models%2520to%250Aperform%2520both%2520text-to-image%2520generation%2520and%2520instruction%2520based%2520image%2520editing%250Atasks%252C%2520expanding%2520their%2520capabilities%2520beyond%2520pure%2520visual%2520understanding.%2520Our%250Aexperimental%2520results%2520demonstrate%2520the%2520strong%2520performance%2520of%2520Ming-Lite-Uni%2520and%250Aillustrate%2520the%2520impressive%2520fluid%2520nature%2520of%2520its%2520interactive%2520process.%2520All%2520code%2520and%250Amodel%2520weights%2520are%2520open-sourced%2520to%2520foster%2520further%2520exploration%2520within%2520the%250Acommunity.%2520Notably%252C%2520this%2520work%2520aligns%2520with%2520concurrent%2520multimodal%2520AI%2520milestones%2520-%250Asuch%2520as%2520ChatGPT-4o%2520with%2520native%2520image%2520generation%2520updated%2520in%2520March%252025%252C%25202025%2520-%250Aunderscoring%2520the%2520broader%2520significance%2520of%2520unified%2520models%2520like%2520Ming-Lite-Uni%2520on%250Athe%2520path%2520toward%2520AGI.%2520Ming-Lite-Uni%2520is%2520in%2520alpha%2520stage%2520and%2520will%2520soon%2520be%2520further%250Arefined.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02471v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ming-Lite-Uni%3A%20Advancements%20in%20Unified%20Architecture%20for%20Natural%0A%20%20Multimodal%20Interaction&entry.906535625=Inclusion%20AI%20and%20Biao%20Gong%20and%20Cheng%20Zou%20and%20Dandan%20Zheng%20and%20Hu%20Yu%20and%20Jingdong%20Chen%20and%20Jianxin%20Sun%20and%20Junbo%20Zhao%20and%20Jun%20Zhou%20and%20Kaixiang%20Ji%20and%20Lixiang%20Ru%20and%20Libin%20Wang%20and%20Qingpei%20Guo%20and%20Rui%20Liu%20and%20Weilong%20Chai%20and%20Xinyu%20Xiao%20and%20Ziyuan%20Huang&entry.1292438233=%20%20We%20introduce%20Ming-Lite-Uni%2C%20an%20open-source%20multimodal%20framework%20featuring%20a%0Anewly%20designed%20unified%20visual%20generator%20and%20a%20native%20multimodal%20autoregressive%0Amodel%20tailored%20for%20unifying%20vision%20and%20language.%20Specifically%2C%20this%20project%0Aprovides%20an%20open-source%20implementation%20of%20the%20integrated%20MetaQueries%20and%0AM2-omni%20framework%2C%20while%20introducing%20the%20novel%20multi-scale%20learnable%20tokens%20and%0Amulti-scale%20representation%20alignment%20strategy.%20By%20leveraging%20a%20fixed%20MLLM%20and%20a%0Alearnable%20diffusion%20model%2C%20Ming-Lite-Uni%20enables%20native%20multimodal%20AR%20models%20to%0Aperform%20both%20text-to-image%20generation%20and%20instruction%20based%20image%20editing%0Atasks%2C%20expanding%20their%20capabilities%20beyond%20pure%20visual%20understanding.%20Our%0Aexperimental%20results%20demonstrate%20the%20strong%20performance%20of%20Ming-Lite-Uni%20and%0Aillustrate%20the%20impressive%20fluid%20nature%20of%20its%20interactive%20process.%20All%20code%20and%0Amodel%20weights%20are%20open-sourced%20to%20foster%20further%20exploration%20within%20the%0Acommunity.%20Notably%2C%20this%20work%20aligns%20with%20concurrent%20multimodal%20AI%20milestones%20-%0Asuch%20as%20ChatGPT-4o%20with%20native%20image%20generation%20updated%20in%20March%2025%2C%202025%20-%0Aunderscoring%20the%20broader%20significance%20of%20unified%20models%20like%20Ming-Lite-Uni%20on%0Athe%20path%20toward%20AGI.%20Ming-Lite-Uni%20is%20in%20alpha%20stage%20and%20will%20soon%20be%20further%0Arefined.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02471v2&entry.124074799=Read"},
{"title": "Question-Answering Dense Video Events", "author": "Hangyu Qin and Junbin Xiao and Angela Yao", "abstract": "  This paper presents question-answering on dense video events, a novel task\nthat answers and grounds dense-event questions in long videos, thus challenging\nMLLMs to faithfully comprehend and reason about multiple events over extended\nperiods of time. To facilitate the study, we construct DeVE-QA -- a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. Our benchmarking\nshows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we\npropose DeVi, a novel training-free MLLM approach that highlights a\nhierarchical captioning module, a temporal event memory module, and a\nself-consistency checking module to respectively detect, contextualize and\nmemorize, and ground dense-events in long videos for question answering.\nExtensive experiments show that DeVi is superior at answering dense-event\nquestions and grounding relevant video moments. Compared with existing MLLMs,\nit achieves a remarkable increase of 4.8% and 2.1% for G(round)QA accuracy on\nDeVE-QA~and NExT-GQA, respectively. Our data and code will be released upon\nacceptance.\n", "link": "http://arxiv.org/abs/2409.04388v4", "date": "2025-05-07", "relevancy": 2.1673, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5467}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Question-Answering%20Dense%20Video%20Events&body=Title%3A%20Question-Answering%20Dense%20Video%20Events%0AAuthor%3A%20Hangyu%20Qin%20and%20Junbin%20Xiao%20and%20Angela%20Yao%0AAbstract%3A%20%20%20This%20paper%20presents%20question-answering%20on%20dense%20video%20events%2C%20a%20novel%20task%0Athat%20answers%20and%20grounds%20dense-event%20questions%20in%20long%20videos%2C%20thus%20challenging%0AMLLMs%20to%20faithfully%20comprehend%20and%20reason%20about%20multiple%20events%20over%20extended%0Aperiods%20of%20time.%20To%20facilitate%20the%20study%2C%20we%20construct%20DeVE-QA%20--%20a%20dataset%0Afeaturing%2078K%20questions%20about%2026K%20events%20on%2010.6K%20long%20videos.%20Our%20benchmarking%0Ashows%20that%20state-of-the-art%20MLLMs%20struggle%20on%20DeVE-QA.%20For%20improvement%2C%20we%0Apropose%20DeVi%2C%20a%20novel%20training-free%20MLLM%20approach%20that%20highlights%20a%0Ahierarchical%20captioning%20module%2C%20a%20temporal%20event%20memory%20module%2C%20and%20a%0Aself-consistency%20checking%20module%20to%20respectively%20detect%2C%20contextualize%20and%0Amemorize%2C%20and%20ground%20dense-events%20in%20long%20videos%20for%20question%20answering.%0AExtensive%20experiments%20show%20that%20DeVi%20is%20superior%20at%20answering%20dense-event%0Aquestions%20and%20grounding%20relevant%20video%20moments.%20Compared%20with%20existing%20MLLMs%2C%0Ait%20achieves%20a%20remarkable%20increase%20of%204.8%25%20and%202.1%25%20for%20G%28round%29QA%20accuracy%20on%0ADeVE-QA~and%20NExT-GQA%2C%20respectively.%20Our%20data%20and%20code%20will%20be%20released%20upon%0Aacceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04388v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuestion-Answering%2520Dense%2520Video%2520Events%26entry.906535625%3DHangyu%2520Qin%2520and%2520Junbin%2520Xiao%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520question-answering%2520on%2520dense%2520video%2520events%252C%2520a%2520novel%2520task%250Athat%2520answers%2520and%2520grounds%2520dense-event%2520questions%2520in%2520long%2520videos%252C%2520thus%2520challenging%250AMLLMs%2520to%2520faithfully%2520comprehend%2520and%2520reason%2520about%2520multiple%2520events%2520over%2520extended%250Aperiods%2520of%2520time.%2520To%2520facilitate%2520the%2520study%252C%2520we%2520construct%2520DeVE-QA%2520--%2520a%2520dataset%250Afeaturing%252078K%2520questions%2520about%252026K%2520events%2520on%252010.6K%2520long%2520videos.%2520Our%2520benchmarking%250Ashows%2520that%2520state-of-the-art%2520MLLMs%2520struggle%2520on%2520DeVE-QA.%2520For%2520improvement%252C%2520we%250Apropose%2520DeVi%252C%2520a%2520novel%2520training-free%2520MLLM%2520approach%2520that%2520highlights%2520a%250Ahierarchical%2520captioning%2520module%252C%2520a%2520temporal%2520event%2520memory%2520module%252C%2520and%2520a%250Aself-consistency%2520checking%2520module%2520to%2520respectively%2520detect%252C%2520contextualize%2520and%250Amemorize%252C%2520and%2520ground%2520dense-events%2520in%2520long%2520videos%2520for%2520question%2520answering.%250AExtensive%2520experiments%2520show%2520that%2520DeVi%2520is%2520superior%2520at%2520answering%2520dense-event%250Aquestions%2520and%2520grounding%2520relevant%2520video%2520moments.%2520Compared%2520with%2520existing%2520MLLMs%252C%250Ait%2520achieves%2520a%2520remarkable%2520increase%2520of%25204.8%2525%2520and%25202.1%2525%2520for%2520G%2528round%2529QA%2520accuracy%2520on%250ADeVE-QA~and%2520NExT-GQA%252C%2520respectively.%2520Our%2520data%2520and%2520code%2520will%2520be%2520released%2520upon%250Aacceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04388v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Question-Answering%20Dense%20Video%20Events&entry.906535625=Hangyu%20Qin%20and%20Junbin%20Xiao%20and%20Angela%20Yao&entry.1292438233=%20%20This%20paper%20presents%20question-answering%20on%20dense%20video%20events%2C%20a%20novel%20task%0Athat%20answers%20and%20grounds%20dense-event%20questions%20in%20long%20videos%2C%20thus%20challenging%0AMLLMs%20to%20faithfully%20comprehend%20and%20reason%20about%20multiple%20events%20over%20extended%0Aperiods%20of%20time.%20To%20facilitate%20the%20study%2C%20we%20construct%20DeVE-QA%20--%20a%20dataset%0Afeaturing%2078K%20questions%20about%2026K%20events%20on%2010.6K%20long%20videos.%20Our%20benchmarking%0Ashows%20that%20state-of-the-art%20MLLMs%20struggle%20on%20DeVE-QA.%20For%20improvement%2C%20we%0Apropose%20DeVi%2C%20a%20novel%20training-free%20MLLM%20approach%20that%20highlights%20a%0Ahierarchical%20captioning%20module%2C%20a%20temporal%20event%20memory%20module%2C%20and%20a%0Aself-consistency%20checking%20module%20to%20respectively%20detect%2C%20contextualize%20and%0Amemorize%2C%20and%20ground%20dense-events%20in%20long%20videos%20for%20question%20answering.%0AExtensive%20experiments%20show%20that%20DeVi%20is%20superior%20at%20answering%20dense-event%0Aquestions%20and%20grounding%20relevant%20video%20moments.%20Compared%20with%20existing%20MLLMs%2C%0Ait%20achieves%20a%20remarkable%20increase%20of%204.8%25%20and%202.1%25%20for%20G%28round%29QA%20accuracy%20on%0ADeVE-QA~and%20NExT-GQA%2C%20respectively.%20Our%20data%20and%20code%20will%20be%20released%20upon%0Aacceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04388v4&entry.124074799=Read"},
{"title": "A Systematic Literature Review of Spatio-Temporal Graph Neural Network\n  Models for Time Series Forecasting and Classification", "author": "Flavio Corradini and Flavio Gerosa and Marco Gori and Carlo Lucheroni and Marco Piangerelli and Martina Zannotti", "abstract": "  In recent years, spatio-temporal graph neural networks (GNNs) have attracted\nconsiderable interest in the field of time series analysis, due to their\nability to capture dependencies among variables and across time points. The\nobjective of the presented systematic literature review is hence to provide a\ncomprehensive overview of the various modeling approaches and application\ndomains of GNNs for time series classification and forecasting. A database\nsearch was conducted, and over 150 journal papers were selected for a detailed\nexamination of the current state-of-the-art in the field. This examination is\nintended to offer to the reader a comprehensive collection of proposed models,\nlinks to related source code, available datasets, benchmark models, and fitting\nresults. All this information is hoped to assist researchers in future studies.\nTo the best of our knowledge, this is the first systematic literature review\npresenting a detailed comparison of the results of current spatio-temporal GNN\nmodels in different domains. In addition, in its final part this review\ndiscusses current limitations and challenges in the application of\nspatio-temporal GNNs, such as comparability, reproducibility, explainability,\npoor information capacity, and scalability.\n", "link": "http://arxiv.org/abs/2410.22377v2", "date": "2025-05-07", "relevancy": 2.1637, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4368}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4308}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Systematic%20Literature%20Review%20of%20Spatio-Temporal%20Graph%20Neural%20Network%0A%20%20Models%20for%20Time%20Series%20Forecasting%20and%20Classification&body=Title%3A%20A%20Systematic%20Literature%20Review%20of%20Spatio-Temporal%20Graph%20Neural%20Network%0A%20%20Models%20for%20Time%20Series%20Forecasting%20and%20Classification%0AAuthor%3A%20Flavio%20Corradini%20and%20Flavio%20Gerosa%20and%20Marco%20Gori%20and%20Carlo%20Lucheroni%20and%20Marco%20Piangerelli%20and%20Martina%20Zannotti%0AAbstract%3A%20%20%20In%20recent%20years%2C%20spatio-temporal%20graph%20neural%20networks%20%28GNNs%29%20have%20attracted%0Aconsiderable%20interest%20in%20the%20field%20of%20time%20series%20analysis%2C%20due%20to%20their%0Aability%20to%20capture%20dependencies%20among%20variables%20and%20across%20time%20points.%20The%0Aobjective%20of%20the%20presented%20systematic%20literature%20review%20is%20hence%20to%20provide%20a%0Acomprehensive%20overview%20of%20the%20various%20modeling%20approaches%20and%20application%0Adomains%20of%20GNNs%20for%20time%20series%20classification%20and%20forecasting.%20A%20database%0Asearch%20was%20conducted%2C%20and%20over%20150%20journal%20papers%20were%20selected%20for%20a%20detailed%0Aexamination%20of%20the%20current%20state-of-the-art%20in%20the%20field.%20This%20examination%20is%0Aintended%20to%20offer%20to%20the%20reader%20a%20comprehensive%20collection%20of%20proposed%20models%2C%0Alinks%20to%20related%20source%20code%2C%20available%20datasets%2C%20benchmark%20models%2C%20and%20fitting%0Aresults.%20All%20this%20information%20is%20hoped%20to%20assist%20researchers%20in%20future%20studies.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20systematic%20literature%20review%0Apresenting%20a%20detailed%20comparison%20of%20the%20results%20of%20current%20spatio-temporal%20GNN%0Amodels%20in%20different%20domains.%20In%20addition%2C%20in%20its%20final%20part%20this%20review%0Adiscusses%20current%20limitations%20and%20challenges%20in%20the%20application%20of%0Aspatio-temporal%20GNNs%2C%20such%20as%20comparability%2C%20reproducibility%2C%20explainability%2C%0Apoor%20information%20capacity%2C%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Systematic%2520Literature%2520Review%2520of%2520Spatio-Temporal%2520Graph%2520Neural%2520Network%250A%2520%2520Models%2520for%2520Time%2520Series%2520Forecasting%2520and%2520Classification%26entry.906535625%3DFlavio%2520Corradini%2520and%2520Flavio%2520Gerosa%2520and%2520Marco%2520Gori%2520and%2520Carlo%2520Lucheroni%2520and%2520Marco%2520Piangerelli%2520and%2520Martina%2520Zannotti%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520spatio-temporal%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520attracted%250Aconsiderable%2520interest%2520in%2520the%2520field%2520of%2520time%2520series%2520analysis%252C%2520due%2520to%2520their%250Aability%2520to%2520capture%2520dependencies%2520among%2520variables%2520and%2520across%2520time%2520points.%2520The%250Aobjective%2520of%2520the%2520presented%2520systematic%2520literature%2520review%2520is%2520hence%2520to%2520provide%2520a%250Acomprehensive%2520overview%2520of%2520the%2520various%2520modeling%2520approaches%2520and%2520application%250Adomains%2520of%2520GNNs%2520for%2520time%2520series%2520classification%2520and%2520forecasting.%2520A%2520database%250Asearch%2520was%2520conducted%252C%2520and%2520over%2520150%2520journal%2520papers%2520were%2520selected%2520for%2520a%2520detailed%250Aexamination%2520of%2520the%2520current%2520state-of-the-art%2520in%2520the%2520field.%2520This%2520examination%2520is%250Aintended%2520to%2520offer%2520to%2520the%2520reader%2520a%2520comprehensive%2520collection%2520of%2520proposed%2520models%252C%250Alinks%2520to%2520related%2520source%2520code%252C%2520available%2520datasets%252C%2520benchmark%2520models%252C%2520and%2520fitting%250Aresults.%2520All%2520this%2520information%2520is%2520hoped%2520to%2520assist%2520researchers%2520in%2520future%2520studies.%250ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520systematic%2520literature%2520review%250Apresenting%2520a%2520detailed%2520comparison%2520of%2520the%2520results%2520of%2520current%2520spatio-temporal%2520GNN%250Amodels%2520in%2520different%2520domains.%2520In%2520addition%252C%2520in%2520its%2520final%2520part%2520this%2520review%250Adiscusses%2520current%2520limitations%2520and%2520challenges%2520in%2520the%2520application%2520of%250Aspatio-temporal%2520GNNs%252C%2520such%2520as%2520comparability%252C%2520reproducibility%252C%2520explainability%252C%250Apoor%2520information%2520capacity%252C%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Systematic%20Literature%20Review%20of%20Spatio-Temporal%20Graph%20Neural%20Network%0A%20%20Models%20for%20Time%20Series%20Forecasting%20and%20Classification&entry.906535625=Flavio%20Corradini%20and%20Flavio%20Gerosa%20and%20Marco%20Gori%20and%20Carlo%20Lucheroni%20and%20Marco%20Piangerelli%20and%20Martina%20Zannotti&entry.1292438233=%20%20In%20recent%20years%2C%20spatio-temporal%20graph%20neural%20networks%20%28GNNs%29%20have%20attracted%0Aconsiderable%20interest%20in%20the%20field%20of%20time%20series%20analysis%2C%20due%20to%20their%0Aability%20to%20capture%20dependencies%20among%20variables%20and%20across%20time%20points.%20The%0Aobjective%20of%20the%20presented%20systematic%20literature%20review%20is%20hence%20to%20provide%20a%0Acomprehensive%20overview%20of%20the%20various%20modeling%20approaches%20and%20application%0Adomains%20of%20GNNs%20for%20time%20series%20classification%20and%20forecasting.%20A%20database%0Asearch%20was%20conducted%2C%20and%20over%20150%20journal%20papers%20were%20selected%20for%20a%20detailed%0Aexamination%20of%20the%20current%20state-of-the-art%20in%20the%20field.%20This%20examination%20is%0Aintended%20to%20offer%20to%20the%20reader%20a%20comprehensive%20collection%20of%20proposed%20models%2C%0Alinks%20to%20related%20source%20code%2C%20available%20datasets%2C%20benchmark%20models%2C%20and%20fitting%0Aresults.%20All%20this%20information%20is%20hoped%20to%20assist%20researchers%20in%20future%20studies.%0ATo%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20systematic%20literature%20review%0Apresenting%20a%20detailed%20comparison%20of%20the%20results%20of%20current%20spatio-temporal%20GNN%0Amodels%20in%20different%20domains.%20In%20addition%2C%20in%20its%20final%20part%20this%20review%0Adiscusses%20current%20limitations%20and%20challenges%20in%20the%20application%20of%0Aspatio-temporal%20GNNs%2C%20such%20as%20comparability%2C%20reproducibility%2C%20explainability%2C%0Apoor%20information%20capacity%2C%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22377v2&entry.124074799=Read"},
{"title": "Defining and Quantifying Creative Behavior in Popular Image Generators", "author": "Aditi Ramaswamy", "abstract": "  Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.\n", "link": "http://arxiv.org/abs/2505.04497v1", "date": "2025-05-07", "relevancy": 2.1625, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5653}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defining%20and%20Quantifying%20Creative%20Behavior%20in%20Popular%20Image%20Generators&body=Title%3A%20Defining%20and%20Quantifying%20Creative%20Behavior%20in%20Popular%20Image%20Generators%0AAuthor%3A%20Aditi%20Ramaswamy%0AAbstract%3A%20%20%20Creativity%20of%20generative%20AI%20models%20has%20been%20a%20subject%20of%20scientific%20debate%20in%0Athe%20last%20years%2C%20without%20a%20conclusive%20answer.%20In%20this%20paper%2C%20we%20study%20creativity%0Afrom%20a%20practical%20perspective%20and%20introduce%20quantitative%20measures%20that%20help%20the%0Auser%20to%20choose%20a%20suitable%20AI%20model%20for%20a%20given%20task.%20We%20evaluated%20our%20measures%0Aon%20a%20number%20of%20popular%20image-to-image%20generation%20models%2C%20and%20the%20results%20of%0Athis%20suggest%20that%20our%20measures%20conform%20to%20human%20intuition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefining%2520and%2520Quantifying%2520Creative%2520Behavior%2520in%2520Popular%2520Image%2520Generators%26entry.906535625%3DAditi%2520Ramaswamy%26entry.1292438233%3D%2520%2520Creativity%2520of%2520generative%2520AI%2520models%2520has%2520been%2520a%2520subject%2520of%2520scientific%2520debate%2520in%250Athe%2520last%2520years%252C%2520without%2520a%2520conclusive%2520answer.%2520In%2520this%2520paper%252C%2520we%2520study%2520creativity%250Afrom%2520a%2520practical%2520perspective%2520and%2520introduce%2520quantitative%2520measures%2520that%2520help%2520the%250Auser%2520to%2520choose%2520a%2520suitable%2520AI%2520model%2520for%2520a%2520given%2520task.%2520We%2520evaluated%2520our%2520measures%250Aon%2520a%2520number%2520of%2520popular%2520image-to-image%2520generation%2520models%252C%2520and%2520the%2520results%2520of%250Athis%2520suggest%2520that%2520our%2520measures%2520conform%2520to%2520human%2520intuition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defining%20and%20Quantifying%20Creative%20Behavior%20in%20Popular%20Image%20Generators&entry.906535625=Aditi%20Ramaswamy&entry.1292438233=%20%20Creativity%20of%20generative%20AI%20models%20has%20been%20a%20subject%20of%20scientific%20debate%20in%0Athe%20last%20years%2C%20without%20a%20conclusive%20answer.%20In%20this%20paper%2C%20we%20study%20creativity%0Afrom%20a%20practical%20perspective%20and%20introduce%20quantitative%20measures%20that%20help%20the%0Auser%20to%20choose%20a%20suitable%20AI%20model%20for%20a%20given%20task.%20We%20evaluated%20our%20measures%0Aon%20a%20number%20of%20popular%20image-to-image%20generation%20models%2C%20and%20the%20results%20of%0Athis%20suggest%20that%20our%20measures%20conform%20to%20human%20intuition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04497v1&entry.124074799=Read"},
{"title": "XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis", "author": "Hao Li and Chenming Wu and Ming Yuan and Yan Zhang and Chen Zhao and Chunyu Song and Haocheng Feng and Errui Ding and Dingwen Zhang and Jingdong Wang", "abstract": "  Comprehensive testing of autonomous systems through simulation is essential\nto ensure the safety of autonomous driving vehicles. This requires the\ngeneration of safety-critical scenarios that extend beyond the limitations of\nreal-world data collection, as many of these scenarios are rare or rarely\nencountered on public roads. However, evaluating most existing novel view\nsynthesis (NVS) methods relies on sporadic sampling of image frames from the\ntraining data, comparing the rendered images with ground-truth images.\nUnfortunately, this evaluation protocol falls short of meeting the actual\nrequirements in closed-loop simulations. Specifically, the true application\ndemands the capability to render novel views that extend beyond the original\ntrajectory (such as cross-lane views), which are challenging to capture in the\nreal world. To address this, this paper presents a synthetic dataset for novel\ndriving view synthesis evaluation, which is specifically designed for\nautonomous driving simulations. This unique dataset includes testing images\ncaptured by deviating from the training trajectory by $1-4$ meters. It\ncomprises six sequences that cover various times and weather conditions. Each\nsequence contains $450$ training images, $120$ testing images, and their\ncorresponding camera poses and intrinsic parameters. Leveraging this novel\ndataset, we establish the first realistic benchmark for evaluating existing NVS\napproaches under front-only and multicamera settings. The experimental findings\nunderscore the significant gap in current approaches, revealing their\ninadequate ability to fulfill the demanding prerequisites of cross-lane or\nclosed-loop simulation.\n", "link": "http://arxiv.org/abs/2406.18360v3", "date": "2025-05-07", "relevancy": 2.1539, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5639}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5334}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis&body=Title%3A%20XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis%0AAuthor%3A%20Hao%20Li%20and%20Chenming%20Wu%20and%20Ming%20Yuan%20and%20Yan%20Zhang%20and%20Chen%20Zhao%20and%20Chunyu%20Song%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Dingwen%20Zhang%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Comprehensive%20testing%20of%20autonomous%20systems%20through%20simulation%20is%20essential%0Ato%20ensure%20the%20safety%20of%20autonomous%20driving%20vehicles.%20This%20requires%20the%0Ageneration%20of%20safety-critical%20scenarios%20that%20extend%20beyond%20the%20limitations%20of%0Areal-world%20data%20collection%2C%20as%20many%20of%20these%20scenarios%20are%20rare%20or%20rarely%0Aencountered%20on%20public%20roads.%20However%2C%20evaluating%20most%20existing%20novel%20view%0Asynthesis%20%28NVS%29%20methods%20relies%20on%20sporadic%20sampling%20of%20image%20frames%20from%20the%0Atraining%20data%2C%20comparing%20the%20rendered%20images%20with%20ground-truth%20images.%0AUnfortunately%2C%20this%20evaluation%20protocol%20falls%20short%20of%20meeting%20the%20actual%0Arequirements%20in%20closed-loop%20simulations.%20Specifically%2C%20the%20true%20application%0Ademands%20the%20capability%20to%20render%20novel%20views%20that%20extend%20beyond%20the%20original%0Atrajectory%20%28such%20as%20cross-lane%20views%29%2C%20which%20are%20challenging%20to%20capture%20in%20the%0Areal%20world.%20To%20address%20this%2C%20this%20paper%20presents%20a%20synthetic%20dataset%20for%20novel%0Adriving%20view%20synthesis%20evaluation%2C%20which%20is%20specifically%20designed%20for%0Aautonomous%20driving%20simulations.%20This%20unique%20dataset%20includes%20testing%20images%0Acaptured%20by%20deviating%20from%20the%20training%20trajectory%20by%20%241-4%24%20meters.%20It%0Acomprises%20six%20sequences%20that%20cover%20various%20times%20and%20weather%20conditions.%20Each%0Asequence%20contains%20%24450%24%20training%20images%2C%20%24120%24%20testing%20images%2C%20and%20their%0Acorresponding%20camera%20poses%20and%20intrinsic%20parameters.%20Leveraging%20this%20novel%0Adataset%2C%20we%20establish%20the%20first%20realistic%20benchmark%20for%20evaluating%20existing%20NVS%0Aapproaches%20under%20front-only%20and%20multicamera%20settings.%20The%20experimental%20findings%0Aunderscore%20the%20significant%20gap%20in%20current%20approaches%2C%20revealing%20their%0Ainadequate%20ability%20to%20fulfill%20the%20demanding%20prerequisites%20of%20cross-lane%20or%0Aclosed-loop%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18360v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXLD%253A%2520A%2520Cross-Lane%2520Dataset%2520for%2520Benchmarking%2520Novel%2520Driving%2520View%2520Synthesis%26entry.906535625%3DHao%2520Li%2520and%2520Chenming%2520Wu%2520and%2520Ming%2520Yuan%2520and%2520Yan%2520Zhang%2520and%2520Chen%2520Zhao%2520and%2520Chunyu%2520Song%2520and%2520Haocheng%2520Feng%2520and%2520Errui%2520Ding%2520and%2520Dingwen%2520Zhang%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Comprehensive%2520testing%2520of%2520autonomous%2520systems%2520through%2520simulation%2520is%2520essential%250Ato%2520ensure%2520the%2520safety%2520of%2520autonomous%2520driving%2520vehicles.%2520This%2520requires%2520the%250Ageneration%2520of%2520safety-critical%2520scenarios%2520that%2520extend%2520beyond%2520the%2520limitations%2520of%250Areal-world%2520data%2520collection%252C%2520as%2520many%2520of%2520these%2520scenarios%2520are%2520rare%2520or%2520rarely%250Aencountered%2520on%2520public%2520roads.%2520However%252C%2520evaluating%2520most%2520existing%2520novel%2520view%250Asynthesis%2520%2528NVS%2529%2520methods%2520relies%2520on%2520sporadic%2520sampling%2520of%2520image%2520frames%2520from%2520the%250Atraining%2520data%252C%2520comparing%2520the%2520rendered%2520images%2520with%2520ground-truth%2520images.%250AUnfortunately%252C%2520this%2520evaluation%2520protocol%2520falls%2520short%2520of%2520meeting%2520the%2520actual%250Arequirements%2520in%2520closed-loop%2520simulations.%2520Specifically%252C%2520the%2520true%2520application%250Ademands%2520the%2520capability%2520to%2520render%2520novel%2520views%2520that%2520extend%2520beyond%2520the%2520original%250Atrajectory%2520%2528such%2520as%2520cross-lane%2520views%2529%252C%2520which%2520are%2520challenging%2520to%2520capture%2520in%2520the%250Areal%2520world.%2520To%2520address%2520this%252C%2520this%2520paper%2520presents%2520a%2520synthetic%2520dataset%2520for%2520novel%250Adriving%2520view%2520synthesis%2520evaluation%252C%2520which%2520is%2520specifically%2520designed%2520for%250Aautonomous%2520driving%2520simulations.%2520This%2520unique%2520dataset%2520includes%2520testing%2520images%250Acaptured%2520by%2520deviating%2520from%2520the%2520training%2520trajectory%2520by%2520%25241-4%2524%2520meters.%2520It%250Acomprises%2520six%2520sequences%2520that%2520cover%2520various%2520times%2520and%2520weather%2520conditions.%2520Each%250Asequence%2520contains%2520%2524450%2524%2520training%2520images%252C%2520%2524120%2524%2520testing%2520images%252C%2520and%2520their%250Acorresponding%2520camera%2520poses%2520and%2520intrinsic%2520parameters.%2520Leveraging%2520this%2520novel%250Adataset%252C%2520we%2520establish%2520the%2520first%2520realistic%2520benchmark%2520for%2520evaluating%2520existing%2520NVS%250Aapproaches%2520under%2520front-only%2520and%2520multicamera%2520settings.%2520The%2520experimental%2520findings%250Aunderscore%2520the%2520significant%2520gap%2520in%2520current%2520approaches%252C%2520revealing%2520their%250Ainadequate%2520ability%2520to%2520fulfill%2520the%2520demanding%2520prerequisites%2520of%2520cross-lane%2520or%250Aclosed-loop%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18360v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XLD%3A%20A%20Cross-Lane%20Dataset%20for%20Benchmarking%20Novel%20Driving%20View%20Synthesis&entry.906535625=Hao%20Li%20and%20Chenming%20Wu%20and%20Ming%20Yuan%20and%20Yan%20Zhang%20and%20Chen%20Zhao%20and%20Chunyu%20Song%20and%20Haocheng%20Feng%20and%20Errui%20Ding%20and%20Dingwen%20Zhang%20and%20Jingdong%20Wang&entry.1292438233=%20%20Comprehensive%20testing%20of%20autonomous%20systems%20through%20simulation%20is%20essential%0Ato%20ensure%20the%20safety%20of%20autonomous%20driving%20vehicles.%20This%20requires%20the%0Ageneration%20of%20safety-critical%20scenarios%20that%20extend%20beyond%20the%20limitations%20of%0Areal-world%20data%20collection%2C%20as%20many%20of%20these%20scenarios%20are%20rare%20or%20rarely%0Aencountered%20on%20public%20roads.%20However%2C%20evaluating%20most%20existing%20novel%20view%0Asynthesis%20%28NVS%29%20methods%20relies%20on%20sporadic%20sampling%20of%20image%20frames%20from%20the%0Atraining%20data%2C%20comparing%20the%20rendered%20images%20with%20ground-truth%20images.%0AUnfortunately%2C%20this%20evaluation%20protocol%20falls%20short%20of%20meeting%20the%20actual%0Arequirements%20in%20closed-loop%20simulations.%20Specifically%2C%20the%20true%20application%0Ademands%20the%20capability%20to%20render%20novel%20views%20that%20extend%20beyond%20the%20original%0Atrajectory%20%28such%20as%20cross-lane%20views%29%2C%20which%20are%20challenging%20to%20capture%20in%20the%0Areal%20world.%20To%20address%20this%2C%20this%20paper%20presents%20a%20synthetic%20dataset%20for%20novel%0Adriving%20view%20synthesis%20evaluation%2C%20which%20is%20specifically%20designed%20for%0Aautonomous%20driving%20simulations.%20This%20unique%20dataset%20includes%20testing%20images%0Acaptured%20by%20deviating%20from%20the%20training%20trajectory%20by%20%241-4%24%20meters.%20It%0Acomprises%20six%20sequences%20that%20cover%20various%20times%20and%20weather%20conditions.%20Each%0Asequence%20contains%20%24450%24%20training%20images%2C%20%24120%24%20testing%20images%2C%20and%20their%0Acorresponding%20camera%20poses%20and%20intrinsic%20parameters.%20Leveraging%20this%20novel%0Adataset%2C%20we%20establish%20the%20first%20realistic%20benchmark%20for%20evaluating%20existing%20NVS%0Aapproaches%20under%20front-only%20and%20multicamera%20settings.%20The%20experimental%20findings%0Aunderscore%20the%20significant%20gap%20in%20current%20approaches%2C%20revealing%20their%0Ainadequate%20ability%20to%20fulfill%20the%20demanding%20prerequisites%20of%20cross-lane%20or%0Aclosed-loop%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18360v3&entry.124074799=Read"},
{"title": "Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera\n  Surveillance Systems", "author": "Mohammad Merati and David Casta\u00f1\u00f3n", "abstract": "  This paper presents a novel approach for optimizing the scheduling and\ncontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.\nThe proposed method integrates Kalman filters for motion prediction with a\ndynamic network flow model to enhance real-time video capture efficiency. By\nassigning Kalman filters to tracked objects, the system predicts future\nlocations, enabling precise scheduling of camera tasks. This prediction-driven\napproach is formulated as a network flow optimization, ensuring scalability and\nadaptability to various surveillance scenarios. To further reduce redundant\nmonitoring, we also incorporate group-tracking nodes, allowing multiple objects\nto be captured within a single camera focus when appropriate. In addition, a\nvalue-based system is introduced to prioritize camera actions, focusing on the\ntimely capture of critical events. By adjusting the decay rates of these values\nover time, the system ensures prompt responses to tasks with imminent\ndeadlines. Extensive simulations demonstrate that this approach improves\ncoverage, reduces average wait times, and minimizes missed events compared to\ntraditional master-slave camera systems. Overall, our method significantly\nenhances the efficiency, scalability, and effectiveness of surveillance\nsystems, particularly in dynamic and crowded environments.\n", "link": "http://arxiv.org/abs/2505.04596v1", "date": "2025-05-07", "relevancy": 2.1478, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5372}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Network%20Flow%20Optimization%20for%20Task%20Scheduling%20in%20PTZ%20Camera%0A%20%20Surveillance%20Systems&body=Title%3A%20Dynamic%20Network%20Flow%20Optimization%20for%20Task%20Scheduling%20in%20PTZ%20Camera%0A%20%20Surveillance%20Systems%0AAuthor%3A%20Mohammad%20Merati%20and%20David%20Casta%C3%B1%C3%B3n%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20for%20optimizing%20the%20scheduling%20and%0Acontrol%20of%20Pan-Tilt-Zoom%20%28PTZ%29%20cameras%20in%20dynamic%20surveillance%20environments.%0AThe%20proposed%20method%20integrates%20Kalman%20filters%20for%20motion%20prediction%20with%20a%0Adynamic%20network%20flow%20model%20to%20enhance%20real-time%20video%20capture%20efficiency.%20By%0Aassigning%20Kalman%20filters%20to%20tracked%20objects%2C%20the%20system%20predicts%20future%0Alocations%2C%20enabling%20precise%20scheduling%20of%20camera%20tasks.%20This%20prediction-driven%0Aapproach%20is%20formulated%20as%20a%20network%20flow%20optimization%2C%20ensuring%20scalability%20and%0Aadaptability%20to%20various%20surveillance%20scenarios.%20To%20further%20reduce%20redundant%0Amonitoring%2C%20we%20also%20incorporate%20group-tracking%20nodes%2C%20allowing%20multiple%20objects%0Ato%20be%20captured%20within%20a%20single%20camera%20focus%20when%20appropriate.%20In%20addition%2C%20a%0Avalue-based%20system%20is%20introduced%20to%20prioritize%20camera%20actions%2C%20focusing%20on%20the%0Atimely%20capture%20of%20critical%20events.%20By%20adjusting%20the%20decay%20rates%20of%20these%20values%0Aover%20time%2C%20the%20system%20ensures%20prompt%20responses%20to%20tasks%20with%20imminent%0Adeadlines.%20Extensive%20simulations%20demonstrate%20that%20this%20approach%20improves%0Acoverage%2C%20reduces%20average%20wait%20times%2C%20and%20minimizes%20missed%20events%20compared%20to%0Atraditional%20master-slave%20camera%20systems.%20Overall%2C%20our%20method%20significantly%0Aenhances%20the%20efficiency%2C%20scalability%2C%20and%20effectiveness%20of%20surveillance%0Asystems%2C%20particularly%20in%20dynamic%20and%20crowded%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Network%2520Flow%2520Optimization%2520for%2520Task%2520Scheduling%2520in%2520PTZ%2520Camera%250A%2520%2520Surveillance%2520Systems%26entry.906535625%3DMohammad%2520Merati%2520and%2520David%2520Casta%25C3%25B1%25C3%25B3n%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%2520optimizing%2520the%2520scheduling%2520and%250Acontrol%2520of%2520Pan-Tilt-Zoom%2520%2528PTZ%2529%2520cameras%2520in%2520dynamic%2520surveillance%2520environments.%250AThe%2520proposed%2520method%2520integrates%2520Kalman%2520filters%2520for%2520motion%2520prediction%2520with%2520a%250Adynamic%2520network%2520flow%2520model%2520to%2520enhance%2520real-time%2520video%2520capture%2520efficiency.%2520By%250Aassigning%2520Kalman%2520filters%2520to%2520tracked%2520objects%252C%2520the%2520system%2520predicts%2520future%250Alocations%252C%2520enabling%2520precise%2520scheduling%2520of%2520camera%2520tasks.%2520This%2520prediction-driven%250Aapproach%2520is%2520formulated%2520as%2520a%2520network%2520flow%2520optimization%252C%2520ensuring%2520scalability%2520and%250Aadaptability%2520to%2520various%2520surveillance%2520scenarios.%2520To%2520further%2520reduce%2520redundant%250Amonitoring%252C%2520we%2520also%2520incorporate%2520group-tracking%2520nodes%252C%2520allowing%2520multiple%2520objects%250Ato%2520be%2520captured%2520within%2520a%2520single%2520camera%2520focus%2520when%2520appropriate.%2520In%2520addition%252C%2520a%250Avalue-based%2520system%2520is%2520introduced%2520to%2520prioritize%2520camera%2520actions%252C%2520focusing%2520on%2520the%250Atimely%2520capture%2520of%2520critical%2520events.%2520By%2520adjusting%2520the%2520decay%2520rates%2520of%2520these%2520values%250Aover%2520time%252C%2520the%2520system%2520ensures%2520prompt%2520responses%2520to%2520tasks%2520with%2520imminent%250Adeadlines.%2520Extensive%2520simulations%2520demonstrate%2520that%2520this%2520approach%2520improves%250Acoverage%252C%2520reduces%2520average%2520wait%2520times%252C%2520and%2520minimizes%2520missed%2520events%2520compared%2520to%250Atraditional%2520master-slave%2520camera%2520systems.%2520Overall%252C%2520our%2520method%2520significantly%250Aenhances%2520the%2520efficiency%252C%2520scalability%252C%2520and%2520effectiveness%2520of%2520surveillance%250Asystems%252C%2520particularly%2520in%2520dynamic%2520and%2520crowded%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Network%20Flow%20Optimization%20for%20Task%20Scheduling%20in%20PTZ%20Camera%0A%20%20Surveillance%20Systems&entry.906535625=Mohammad%20Merati%20and%20David%20Casta%C3%B1%C3%B3n&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20for%20optimizing%20the%20scheduling%20and%0Acontrol%20of%20Pan-Tilt-Zoom%20%28PTZ%29%20cameras%20in%20dynamic%20surveillance%20environments.%0AThe%20proposed%20method%20integrates%20Kalman%20filters%20for%20motion%20prediction%20with%20a%0Adynamic%20network%20flow%20model%20to%20enhance%20real-time%20video%20capture%20efficiency.%20By%0Aassigning%20Kalman%20filters%20to%20tracked%20objects%2C%20the%20system%20predicts%20future%0Alocations%2C%20enabling%20precise%20scheduling%20of%20camera%20tasks.%20This%20prediction-driven%0Aapproach%20is%20formulated%20as%20a%20network%20flow%20optimization%2C%20ensuring%20scalability%20and%0Aadaptability%20to%20various%20surveillance%20scenarios.%20To%20further%20reduce%20redundant%0Amonitoring%2C%20we%20also%20incorporate%20group-tracking%20nodes%2C%20allowing%20multiple%20objects%0Ato%20be%20captured%20within%20a%20single%20camera%20focus%20when%20appropriate.%20In%20addition%2C%20a%0Avalue-based%20system%20is%20introduced%20to%20prioritize%20camera%20actions%2C%20focusing%20on%20the%0Atimely%20capture%20of%20critical%20events.%20By%20adjusting%20the%20decay%20rates%20of%20these%20values%0Aover%20time%2C%20the%20system%20ensures%20prompt%20responses%20to%20tasks%20with%20imminent%0Adeadlines.%20Extensive%20simulations%20demonstrate%20that%20this%20approach%20improves%0Acoverage%2C%20reduces%20average%20wait%20times%2C%20and%20minimizes%20missed%20events%20compared%20to%0Atraditional%20master-slave%20camera%20systems.%20Overall%2C%20our%20method%20significantly%0Aenhances%20the%20efficiency%2C%20scalability%2C%20and%20effectiveness%20of%20surveillance%0Asystems%2C%20particularly%20in%20dynamic%20and%20crowded%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04596v1&entry.124074799=Read"},
{"title": "DATA: Multi-Disentanglement based Contrastive Learning for Open-World\n  Semi-Supervised Deepfake Attribution", "author": "Ming-Hui Liu and Xiao-Qian Liu and Xin Luo and Xin-Shun Xu", "abstract": "  Deepfake attribution (DFA) aims to perform multiclassification on different\nfacial manipulation techniques, thereby mitigating the detrimental effects of\nforgery content on the social order and personal reputations. However, previous\nmethods focus only on method-specific clues, which easily lead to overfitting,\nwhile overlooking the crucial role of common forgery features. Additionally,\nthey struggle to distinguish between uncertain novel classes in more practical\nopen-world scenarios. To address these issues, in this paper we propose an\ninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, to\nenhance the generalization ability on novel classes for the open-world\nsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all\ngeneration techniques can be abstracted into a similar architecture, DATA\ndefines the concept of 'Orthonormal Deepfake Basis' for the first time and\nutilizes it to disentangle method-specific features, thereby reducing the\noverfitting on forgery-irrelevant information. Furthermore, an augmented-memory\nmechanism is designed to assist in novel class discovery and contrastive\nlearning, which aims to obtain clear class boundaries for the novel classes\nthrough instance-level disentanglements. Additionally, to enhance the\nstandardization and discrimination of features, DATA uses bases contrastive\nloss and center contrastive loss as auxiliaries for the aforementioned modules.\nExtensive experimental evaluations show that DATA achieves state-of-the-art\nperformance on the OSS-DFA benchmark, e.g., there are notable accuracy\nimprovements in 2.55% / 5.7% under different settings, compared with the\nexisting methods.\n", "link": "http://arxiv.org/abs/2505.04384v1", "date": "2025-05-07", "relevancy": 2.143, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5523}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5336}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DATA%3A%20Multi-Disentanglement%20based%20Contrastive%20Learning%20for%20Open-World%0A%20%20Semi-Supervised%20Deepfake%20Attribution&body=Title%3A%20DATA%3A%20Multi-Disentanglement%20based%20Contrastive%20Learning%20for%20Open-World%0A%20%20Semi-Supervised%20Deepfake%20Attribution%0AAuthor%3A%20Ming-Hui%20Liu%20and%20Xiao-Qian%20Liu%20and%20Xin%20Luo%20and%20Xin-Shun%20Xu%0AAbstract%3A%20%20%20Deepfake%20attribution%20%28DFA%29%20aims%20to%20perform%20multiclassification%20on%20different%0Afacial%20manipulation%20techniques%2C%20thereby%20mitigating%20the%20detrimental%20effects%20of%0Aforgery%20content%20on%20the%20social%20order%20and%20personal%20reputations.%20However%2C%20previous%0Amethods%20focus%20only%20on%20method-specific%20clues%2C%20which%20easily%20lead%20to%20overfitting%2C%0Awhile%20overlooking%20the%20crucial%20role%20of%20common%20forgery%20features.%20Additionally%2C%0Athey%20struggle%20to%20distinguish%20between%20uncertain%20novel%20classes%20in%20more%20practical%0Aopen-world%20scenarios.%20To%20address%20these%20issues%2C%20in%20this%20paper%20we%20propose%20an%0Ainnovative%20multi-DisentAnglement%20based%20conTrastive%20leArning%20framework%2C%20DATA%2C%20to%0Aenhance%20the%20generalization%20ability%20on%20novel%20classes%20for%20the%20open-world%0Asemi-supervised%20deepfake%20attribution%20%28OSS-DFA%29%20task.%20Specifically%2C%20since%20all%0Ageneration%20techniques%20can%20be%20abstracted%20into%20a%20similar%20architecture%2C%20DATA%0Adefines%20the%20concept%20of%20%27Orthonormal%20Deepfake%20Basis%27%20for%20the%20first%20time%20and%0Autilizes%20it%20to%20disentangle%20method-specific%20features%2C%20thereby%20reducing%20the%0Aoverfitting%20on%20forgery-irrelevant%20information.%20Furthermore%2C%20an%20augmented-memory%0Amechanism%20is%20designed%20to%20assist%20in%20novel%20class%20discovery%20and%20contrastive%0Alearning%2C%20which%20aims%20to%20obtain%20clear%20class%20boundaries%20for%20the%20novel%20classes%0Athrough%20instance-level%20disentanglements.%20Additionally%2C%20to%20enhance%20the%0Astandardization%20and%20discrimination%20of%20features%2C%20DATA%20uses%20bases%20contrastive%0Aloss%20and%20center%20contrastive%20loss%20as%20auxiliaries%20for%20the%20aforementioned%20modules.%0AExtensive%20experimental%20evaluations%20show%20that%20DATA%20achieves%20state-of-the-art%0Aperformance%20on%20the%20OSS-DFA%20benchmark%2C%20e.g.%2C%20there%20are%20notable%20accuracy%0Aimprovements%20in%202.55%25%20/%205.7%25%20under%20different%20settings%2C%20compared%20with%20the%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDATA%253A%2520Multi-Disentanglement%2520based%2520Contrastive%2520Learning%2520for%2520Open-World%250A%2520%2520Semi-Supervised%2520Deepfake%2520Attribution%26entry.906535625%3DMing-Hui%2520Liu%2520and%2520Xiao-Qian%2520Liu%2520and%2520Xin%2520Luo%2520and%2520Xin-Shun%2520Xu%26entry.1292438233%3D%2520%2520Deepfake%2520attribution%2520%2528DFA%2529%2520aims%2520to%2520perform%2520multiclassification%2520on%2520different%250Afacial%2520manipulation%2520techniques%252C%2520thereby%2520mitigating%2520the%2520detrimental%2520effects%2520of%250Aforgery%2520content%2520on%2520the%2520social%2520order%2520and%2520personal%2520reputations.%2520However%252C%2520previous%250Amethods%2520focus%2520only%2520on%2520method-specific%2520clues%252C%2520which%2520easily%2520lead%2520to%2520overfitting%252C%250Awhile%2520overlooking%2520the%2520crucial%2520role%2520of%2520common%2520forgery%2520features.%2520Additionally%252C%250Athey%2520struggle%2520to%2520distinguish%2520between%2520uncertain%2520novel%2520classes%2520in%2520more%2520practical%250Aopen-world%2520scenarios.%2520To%2520address%2520these%2520issues%252C%2520in%2520this%2520paper%2520we%2520propose%2520an%250Ainnovative%2520multi-DisentAnglement%2520based%2520conTrastive%2520leArning%2520framework%252C%2520DATA%252C%2520to%250Aenhance%2520the%2520generalization%2520ability%2520on%2520novel%2520classes%2520for%2520the%2520open-world%250Asemi-supervised%2520deepfake%2520attribution%2520%2528OSS-DFA%2529%2520task.%2520Specifically%252C%2520since%2520all%250Ageneration%2520techniques%2520can%2520be%2520abstracted%2520into%2520a%2520similar%2520architecture%252C%2520DATA%250Adefines%2520the%2520concept%2520of%2520%2527Orthonormal%2520Deepfake%2520Basis%2527%2520for%2520the%2520first%2520time%2520and%250Autilizes%2520it%2520to%2520disentangle%2520method-specific%2520features%252C%2520thereby%2520reducing%2520the%250Aoverfitting%2520on%2520forgery-irrelevant%2520information.%2520Furthermore%252C%2520an%2520augmented-memory%250Amechanism%2520is%2520designed%2520to%2520assist%2520in%2520novel%2520class%2520discovery%2520and%2520contrastive%250Alearning%252C%2520which%2520aims%2520to%2520obtain%2520clear%2520class%2520boundaries%2520for%2520the%2520novel%2520classes%250Athrough%2520instance-level%2520disentanglements.%2520Additionally%252C%2520to%2520enhance%2520the%250Astandardization%2520and%2520discrimination%2520of%2520features%252C%2520DATA%2520uses%2520bases%2520contrastive%250Aloss%2520and%2520center%2520contrastive%2520loss%2520as%2520auxiliaries%2520for%2520the%2520aforementioned%2520modules.%250AExtensive%2520experimental%2520evaluations%2520show%2520that%2520DATA%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520OSS-DFA%2520benchmark%252C%2520e.g.%252C%2520there%2520are%2520notable%2520accuracy%250Aimprovements%2520in%25202.55%2525%2520/%25205.7%2525%2520under%2520different%2520settings%252C%2520compared%2520with%2520the%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DATA%3A%20Multi-Disentanglement%20based%20Contrastive%20Learning%20for%20Open-World%0A%20%20Semi-Supervised%20Deepfake%20Attribution&entry.906535625=Ming-Hui%20Liu%20and%20Xiao-Qian%20Liu%20and%20Xin%20Luo%20and%20Xin-Shun%20Xu&entry.1292438233=%20%20Deepfake%20attribution%20%28DFA%29%20aims%20to%20perform%20multiclassification%20on%20different%0Afacial%20manipulation%20techniques%2C%20thereby%20mitigating%20the%20detrimental%20effects%20of%0Aforgery%20content%20on%20the%20social%20order%20and%20personal%20reputations.%20However%2C%20previous%0Amethods%20focus%20only%20on%20method-specific%20clues%2C%20which%20easily%20lead%20to%20overfitting%2C%0Awhile%20overlooking%20the%20crucial%20role%20of%20common%20forgery%20features.%20Additionally%2C%0Athey%20struggle%20to%20distinguish%20between%20uncertain%20novel%20classes%20in%20more%20practical%0Aopen-world%20scenarios.%20To%20address%20these%20issues%2C%20in%20this%20paper%20we%20propose%20an%0Ainnovative%20multi-DisentAnglement%20based%20conTrastive%20leArning%20framework%2C%20DATA%2C%20to%0Aenhance%20the%20generalization%20ability%20on%20novel%20classes%20for%20the%20open-world%0Asemi-supervised%20deepfake%20attribution%20%28OSS-DFA%29%20task.%20Specifically%2C%20since%20all%0Ageneration%20techniques%20can%20be%20abstracted%20into%20a%20similar%20architecture%2C%20DATA%0Adefines%20the%20concept%20of%20%27Orthonormal%20Deepfake%20Basis%27%20for%20the%20first%20time%20and%0Autilizes%20it%20to%20disentangle%20method-specific%20features%2C%20thereby%20reducing%20the%0Aoverfitting%20on%20forgery-irrelevant%20information.%20Furthermore%2C%20an%20augmented-memory%0Amechanism%20is%20designed%20to%20assist%20in%20novel%20class%20discovery%20and%20contrastive%0Alearning%2C%20which%20aims%20to%20obtain%20clear%20class%20boundaries%20for%20the%20novel%20classes%0Athrough%20instance-level%20disentanglements.%20Additionally%2C%20to%20enhance%20the%0Astandardization%20and%20discrimination%20of%20features%2C%20DATA%20uses%20bases%20contrastive%0Aloss%20and%20center%20contrastive%20loss%20as%20auxiliaries%20for%20the%20aforementioned%20modules.%0AExtensive%20experimental%20evaluations%20show%20that%20DATA%20achieves%20state-of-the-art%0Aperformance%20on%20the%20OSS-DFA%20benchmark%2C%20e.g.%2C%20there%20are%20notable%20accuracy%0Aimprovements%20in%202.55%25%20/%205.7%25%20under%20different%20settings%2C%20compared%20with%20the%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04384v1&entry.124074799=Read"},
{"title": "Do We Still Need to Work on Odometry for Autonomous Driving?", "author": "Cedric Le Gentil and Daniil Lisus and Timothy D. Barfoot", "abstract": "  Over the past decades, a tremendous amount of work has addressed the topic of\nego-motion estimation of moving platforms based on various proprioceptive and\nexteroceptive sensors. At the cost of ever-increasing computational load and\nsensor complexity, odometry algorithms have reached impressive levels of\naccuracy with minimal drift in various conditions. In this paper, we question\nthe need for more research on odometry for autonomous driving by assessing the\naccuracy of one of the simplest algorithms: the direct integration of wheel\nencoder data and yaw rate measurements from a gyroscope. We denote this\nalgorithm as Odometer-Gyroscope (OG) odometry. This work shows that OG odometry\ncan outperform current state-of-the-art radar-inertial SE(2) odometry for a\nfraction of the computational cost in most scenarios. For example, the OG\nodometry is on top of the Boreas leaderboard with a relative translation error\nof 0.20%, while the second-best method displays an error of 0.26%.\nLidar-inertial approaches can provide more accurate estimates, but the\ncomputational load is three orders of magnitude higher than the OG odometry. To\nfurther the analysis, we have pushed the limits of the OG odometry by purposely\nviolating its fundamental no-slip assumption using data collected during a\nheavy snowstorm with different driving behaviours. Our conclusion shows that a\nsignificant amount of slippage is required to result in non-satisfactory pose\nestimates from the OG odometry.\n", "link": "http://arxiv.org/abs/2505.04438v1", "date": "2025-05-07", "relevancy": 2.1426, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5666}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.543}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20We%20Still%20Need%20to%20Work%20on%20Odometry%20for%20Autonomous%20Driving%3F&body=Title%3A%20Do%20We%20Still%20Need%20to%20Work%20on%20Odometry%20for%20Autonomous%20Driving%3F%0AAuthor%3A%20Cedric%20Le%20Gentil%20and%20Daniil%20Lisus%20and%20Timothy%20D.%20Barfoot%0AAbstract%3A%20%20%20Over%20the%20past%20decades%2C%20a%20tremendous%20amount%20of%20work%20has%20addressed%20the%20topic%20of%0Aego-motion%20estimation%20of%20moving%20platforms%20based%20on%20various%20proprioceptive%20and%0Aexteroceptive%20sensors.%20At%20the%20cost%20of%20ever-increasing%20computational%20load%20and%0Asensor%20complexity%2C%20odometry%20algorithms%20have%20reached%20impressive%20levels%20of%0Aaccuracy%20with%20minimal%20drift%20in%20various%20conditions.%20In%20this%20paper%2C%20we%20question%0Athe%20need%20for%20more%20research%20on%20odometry%20for%20autonomous%20driving%20by%20assessing%20the%0Aaccuracy%20of%20one%20of%20the%20simplest%20algorithms%3A%20the%20direct%20integration%20of%20wheel%0Aencoder%20data%20and%20yaw%20rate%20measurements%20from%20a%20gyroscope.%20We%20denote%20this%0Aalgorithm%20as%20Odometer-Gyroscope%20%28OG%29%20odometry.%20This%20work%20shows%20that%20OG%20odometry%0Acan%20outperform%20current%20state-of-the-art%20radar-inertial%20SE%282%29%20odometry%20for%20a%0Afraction%20of%20the%20computational%20cost%20in%20most%20scenarios.%20For%20example%2C%20the%20OG%0Aodometry%20is%20on%20top%20of%20the%20Boreas%20leaderboard%20with%20a%20relative%20translation%20error%0Aof%200.20%25%2C%20while%20the%20second-best%20method%20displays%20an%20error%20of%200.26%25.%0ALidar-inertial%20approaches%20can%20provide%20more%20accurate%20estimates%2C%20but%20the%0Acomputational%20load%20is%20three%20orders%20of%20magnitude%20higher%20than%20the%20OG%20odometry.%20To%0Afurther%20the%20analysis%2C%20we%20have%20pushed%20the%20limits%20of%20the%20OG%20odometry%20by%20purposely%0Aviolating%20its%20fundamental%20no-slip%20assumption%20using%20data%20collected%20during%20a%0Aheavy%20snowstorm%20with%20different%20driving%20behaviours.%20Our%20conclusion%20shows%20that%20a%0Asignificant%20amount%20of%20slippage%20is%20required%20to%20result%20in%20non-satisfactory%20pose%0Aestimates%20from%20the%20OG%20odometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520We%2520Still%2520Need%2520to%2520Work%2520on%2520Odometry%2520for%2520Autonomous%2520Driving%253F%26entry.906535625%3DCedric%2520Le%2520Gentil%2520and%2520Daniil%2520Lisus%2520and%2520Timothy%2520D.%2520Barfoot%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decades%252C%2520a%2520tremendous%2520amount%2520of%2520work%2520has%2520addressed%2520the%2520topic%2520of%250Aego-motion%2520estimation%2520of%2520moving%2520platforms%2520based%2520on%2520various%2520proprioceptive%2520and%250Aexteroceptive%2520sensors.%2520At%2520the%2520cost%2520of%2520ever-increasing%2520computational%2520load%2520and%250Asensor%2520complexity%252C%2520odometry%2520algorithms%2520have%2520reached%2520impressive%2520levels%2520of%250Aaccuracy%2520with%2520minimal%2520drift%2520in%2520various%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520question%250Athe%2520need%2520for%2520more%2520research%2520on%2520odometry%2520for%2520autonomous%2520driving%2520by%2520assessing%2520the%250Aaccuracy%2520of%2520one%2520of%2520the%2520simplest%2520algorithms%253A%2520the%2520direct%2520integration%2520of%2520wheel%250Aencoder%2520data%2520and%2520yaw%2520rate%2520measurements%2520from%2520a%2520gyroscope.%2520We%2520denote%2520this%250Aalgorithm%2520as%2520Odometer-Gyroscope%2520%2528OG%2529%2520odometry.%2520This%2520work%2520shows%2520that%2520OG%2520odometry%250Acan%2520outperform%2520current%2520state-of-the-art%2520radar-inertial%2520SE%25282%2529%2520odometry%2520for%2520a%250Afraction%2520of%2520the%2520computational%2520cost%2520in%2520most%2520scenarios.%2520For%2520example%252C%2520the%2520OG%250Aodometry%2520is%2520on%2520top%2520of%2520the%2520Boreas%2520leaderboard%2520with%2520a%2520relative%2520translation%2520error%250Aof%25200.20%2525%252C%2520while%2520the%2520second-best%2520method%2520displays%2520an%2520error%2520of%25200.26%2525.%250ALidar-inertial%2520approaches%2520can%2520provide%2520more%2520accurate%2520estimates%252C%2520but%2520the%250Acomputational%2520load%2520is%2520three%2520orders%2520of%2520magnitude%2520higher%2520than%2520the%2520OG%2520odometry.%2520To%250Afurther%2520the%2520analysis%252C%2520we%2520have%2520pushed%2520the%2520limits%2520of%2520the%2520OG%2520odometry%2520by%2520purposely%250Aviolating%2520its%2520fundamental%2520no-slip%2520assumption%2520using%2520data%2520collected%2520during%2520a%250Aheavy%2520snowstorm%2520with%2520different%2520driving%2520behaviours.%2520Our%2520conclusion%2520shows%2520that%2520a%250Asignificant%2520amount%2520of%2520slippage%2520is%2520required%2520to%2520result%2520in%2520non-satisfactory%2520pose%250Aestimates%2520from%2520the%2520OG%2520odometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20We%20Still%20Need%20to%20Work%20on%20Odometry%20for%20Autonomous%20Driving%3F&entry.906535625=Cedric%20Le%20Gentil%20and%20Daniil%20Lisus%20and%20Timothy%20D.%20Barfoot&entry.1292438233=%20%20Over%20the%20past%20decades%2C%20a%20tremendous%20amount%20of%20work%20has%20addressed%20the%20topic%20of%0Aego-motion%20estimation%20of%20moving%20platforms%20based%20on%20various%20proprioceptive%20and%0Aexteroceptive%20sensors.%20At%20the%20cost%20of%20ever-increasing%20computational%20load%20and%0Asensor%20complexity%2C%20odometry%20algorithms%20have%20reached%20impressive%20levels%20of%0Aaccuracy%20with%20minimal%20drift%20in%20various%20conditions.%20In%20this%20paper%2C%20we%20question%0Athe%20need%20for%20more%20research%20on%20odometry%20for%20autonomous%20driving%20by%20assessing%20the%0Aaccuracy%20of%20one%20of%20the%20simplest%20algorithms%3A%20the%20direct%20integration%20of%20wheel%0Aencoder%20data%20and%20yaw%20rate%20measurements%20from%20a%20gyroscope.%20We%20denote%20this%0Aalgorithm%20as%20Odometer-Gyroscope%20%28OG%29%20odometry.%20This%20work%20shows%20that%20OG%20odometry%0Acan%20outperform%20current%20state-of-the-art%20radar-inertial%20SE%282%29%20odometry%20for%20a%0Afraction%20of%20the%20computational%20cost%20in%20most%20scenarios.%20For%20example%2C%20the%20OG%0Aodometry%20is%20on%20top%20of%20the%20Boreas%20leaderboard%20with%20a%20relative%20translation%20error%0Aof%200.20%25%2C%20while%20the%20second-best%20method%20displays%20an%20error%20of%200.26%25.%0ALidar-inertial%20approaches%20can%20provide%20more%20accurate%20estimates%2C%20but%20the%0Acomputational%20load%20is%20three%20orders%20of%20magnitude%20higher%20than%20the%20OG%20odometry.%20To%0Afurther%20the%20analysis%2C%20we%20have%20pushed%20the%20limits%20of%20the%20OG%20odometry%20by%20purposely%0Aviolating%20its%20fundamental%20no-slip%20assumption%20using%20data%20collected%20during%20a%0Aheavy%20snowstorm%20with%20different%20driving%20behaviours.%20Our%20conclusion%20shows%20that%20a%0Asignificant%20amount%20of%20slippage%20is%20required%20to%20result%20in%20non-satisfactory%20pose%0Aestimates%20from%20the%20OG%20odometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04438v1&entry.124074799=Read"},
{"title": "Uncertainty for SVBRDF Acquisition using Frequency Analysis", "author": "Ruben Wiersma and Julien Philip and Milo\u0161 Ha\u0161an and Krishna Mullia and Fujun Luan and Elmar Eisemann and Valentin Deschaintre", "abstract": "  This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view\ncaptures. Under uncontrolled illumination and unstructured viewpoints, there is\nno guarantee that the observations contain enough information to reconstruct\nthe appearance properties of a captured object. We study this ambiguity, or\nuncertainty, using entropy and accelerate the analysis by using the frequency\ndomain, rather than the domain of incoming and outgoing viewing angles. The\nresult is a method that computes a map of uncertainty over an entire object\nwithin a millisecond. We find that the frequency model allows us to recover\nSVBRDF parameters with competitive performance, that the accelerated entropy\ncomputation matches results with a physically-based path tracer, and that there\nis a positive correlation between error and uncertainty. We then show that the\nuncertainty map can be applied to improve SVBRDF acquisition using capture\nguidance, sharing information on the surface, and using a diffusion model to\ninpaint uncertain regions. Our code is available at\nhttps://github.com/rubenwiersma/svbrdf_uncertainty.\n", "link": "http://arxiv.org/abs/2406.17774v3", "date": "2025-05-07", "relevancy": 2.1357, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5672}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20for%20SVBRDF%20Acquisition%20using%20Frequency%20Analysis&body=Title%3A%20Uncertainty%20for%20SVBRDF%20Acquisition%20using%20Frequency%20Analysis%0AAuthor%3A%20Ruben%20Wiersma%20and%20Julien%20Philip%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Krishna%20Mullia%20and%20Fujun%20Luan%20and%20Elmar%20Eisemann%20and%20Valentin%20Deschaintre%0AAbstract%3A%20%20%20This%20paper%20aims%20to%20quantify%20uncertainty%20for%20SVBRDF%20acquisition%20in%20multi-view%0Acaptures.%20Under%20uncontrolled%20illumination%20and%20unstructured%20viewpoints%2C%20there%20is%0Ano%20guarantee%20that%20the%20observations%20contain%20enough%20information%20to%20reconstruct%0Athe%20appearance%20properties%20of%20a%20captured%20object.%20We%20study%20this%20ambiguity%2C%20or%0Auncertainty%2C%20using%20entropy%20and%20accelerate%20the%20analysis%20by%20using%20the%20frequency%0Adomain%2C%20rather%20than%20the%20domain%20of%20incoming%20and%20outgoing%20viewing%20angles.%20The%0Aresult%20is%20a%20method%20that%20computes%20a%20map%20of%20uncertainty%20over%20an%20entire%20object%0Awithin%20a%20millisecond.%20We%20find%20that%20the%20frequency%20model%20allows%20us%20to%20recover%0ASVBRDF%20parameters%20with%20competitive%20performance%2C%20that%20the%20accelerated%20entropy%0Acomputation%20matches%20results%20with%20a%20physically-based%20path%20tracer%2C%20and%20that%20there%0Ais%20a%20positive%20correlation%20between%20error%20and%20uncertainty.%20We%20then%20show%20that%20the%0Auncertainty%20map%20can%20be%20applied%20to%20improve%20SVBRDF%20acquisition%20using%20capture%0Aguidance%2C%20sharing%20information%20on%20the%20surface%2C%20and%20using%20a%20diffusion%20model%20to%0Ainpaint%20uncertain%20regions.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/rubenwiersma/svbrdf_uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17774v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520for%2520SVBRDF%2520Acquisition%2520using%2520Frequency%2520Analysis%26entry.906535625%3DRuben%2520Wiersma%2520and%2520Julien%2520Philip%2520and%2520Milo%25C5%25A1%2520Ha%25C5%25A1an%2520and%2520Krishna%2520Mullia%2520and%2520Fujun%2520Luan%2520and%2520Elmar%2520Eisemann%2520and%2520Valentin%2520Deschaintre%26entry.1292438233%3D%2520%2520This%2520paper%2520aims%2520to%2520quantify%2520uncertainty%2520for%2520SVBRDF%2520acquisition%2520in%2520multi-view%250Acaptures.%2520Under%2520uncontrolled%2520illumination%2520and%2520unstructured%2520viewpoints%252C%2520there%2520is%250Ano%2520guarantee%2520that%2520the%2520observations%2520contain%2520enough%2520information%2520to%2520reconstruct%250Athe%2520appearance%2520properties%2520of%2520a%2520captured%2520object.%2520We%2520study%2520this%2520ambiguity%252C%2520or%250Auncertainty%252C%2520using%2520entropy%2520and%2520accelerate%2520the%2520analysis%2520by%2520using%2520the%2520frequency%250Adomain%252C%2520rather%2520than%2520the%2520domain%2520of%2520incoming%2520and%2520outgoing%2520viewing%2520angles.%2520The%250Aresult%2520is%2520a%2520method%2520that%2520computes%2520a%2520map%2520of%2520uncertainty%2520over%2520an%2520entire%2520object%250Awithin%2520a%2520millisecond.%2520We%2520find%2520that%2520the%2520frequency%2520model%2520allows%2520us%2520to%2520recover%250ASVBRDF%2520parameters%2520with%2520competitive%2520performance%252C%2520that%2520the%2520accelerated%2520entropy%250Acomputation%2520matches%2520results%2520with%2520a%2520physically-based%2520path%2520tracer%252C%2520and%2520that%2520there%250Ais%2520a%2520positive%2520correlation%2520between%2520error%2520and%2520uncertainty.%2520We%2520then%2520show%2520that%2520the%250Auncertainty%2520map%2520can%2520be%2520applied%2520to%2520improve%2520SVBRDF%2520acquisition%2520using%2520capture%250Aguidance%252C%2520sharing%2520information%2520on%2520the%2520surface%252C%2520and%2520using%2520a%2520diffusion%2520model%2520to%250Ainpaint%2520uncertain%2520regions.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/rubenwiersma/svbrdf_uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17774v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20for%20SVBRDF%20Acquisition%20using%20Frequency%20Analysis&entry.906535625=Ruben%20Wiersma%20and%20Julien%20Philip%20and%20Milo%C5%A1%20Ha%C5%A1an%20and%20Krishna%20Mullia%20and%20Fujun%20Luan%20and%20Elmar%20Eisemann%20and%20Valentin%20Deschaintre&entry.1292438233=%20%20This%20paper%20aims%20to%20quantify%20uncertainty%20for%20SVBRDF%20acquisition%20in%20multi-view%0Acaptures.%20Under%20uncontrolled%20illumination%20and%20unstructured%20viewpoints%2C%20there%20is%0Ano%20guarantee%20that%20the%20observations%20contain%20enough%20information%20to%20reconstruct%0Athe%20appearance%20properties%20of%20a%20captured%20object.%20We%20study%20this%20ambiguity%2C%20or%0Auncertainty%2C%20using%20entropy%20and%20accelerate%20the%20analysis%20by%20using%20the%20frequency%0Adomain%2C%20rather%20than%20the%20domain%20of%20incoming%20and%20outgoing%20viewing%20angles.%20The%0Aresult%20is%20a%20method%20that%20computes%20a%20map%20of%20uncertainty%20over%20an%20entire%20object%0Awithin%20a%20millisecond.%20We%20find%20that%20the%20frequency%20model%20allows%20us%20to%20recover%0ASVBRDF%20parameters%20with%20competitive%20performance%2C%20that%20the%20accelerated%20entropy%0Acomputation%20matches%20results%20with%20a%20physically-based%20path%20tracer%2C%20and%20that%20there%0Ais%20a%20positive%20correlation%20between%20error%20and%20uncertainty.%20We%20then%20show%20that%20the%0Auncertainty%20map%20can%20be%20applied%20to%20improve%20SVBRDF%20acquisition%20using%20capture%0Aguidance%2C%20sharing%20information%20on%20the%20surface%2C%20and%20using%20a%20diffusion%20model%20to%0Ainpaint%20uncertain%20regions.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/rubenwiersma/svbrdf_uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17774v3&entry.124074799=Read"},
{"title": "Topology-Driven Clustering: Enhancing Performance with Betti Number\n  Filtration", "author": "Arghya Pratihar and Kushal Bose and Swagatam Das", "abstract": "  Clustering aims to form groups of similar data points in an unsupervised\nregime. Yet, clustering complex datasets containing critically intertwined\nshapes poses significant challenges. The prevailing clustering algorithms\nwidely depend on evaluating similarity measures based on Euclidean metrics.\nExploring topological characteristics to perform clustering of complex datasets\ninevitably presents a better scope. The topological clustering algorithms\npredominantly perceive the point set through the lens of Simplicial complexes\nand Persistent homology. Despite these approaches, the existing topological\nclustering algorithms cannot somehow fully exploit topological structures and\nshow inconsistent performances on some highly complicated datasets. This work\naims to mitigate the limitations by identifying topologically similar neighbors\nthrough the Vietoris-Rips complex and Betti number filtration. In addition, we\nintroduce the concept of the Betti sequences to capture flexibly essential\nfeatures from the topological structures. Our proposed algorithm is adept at\nclustering complex, intertwined shapes contained in the datasets. We carried\nout experiments on several synthetic and real-world datasets. Our algorithm\ndemonstrated commendable performances across the datasets compared to some of\nthe well-known topology-based clustering algorithms.\n", "link": "http://arxiv.org/abs/2505.04346v1", "date": "2025-05-07", "relevancy": 2.108, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4359}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4225}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-Driven%20Clustering%3A%20Enhancing%20Performance%20with%20Betti%20Number%0A%20%20Filtration&body=Title%3A%20Topology-Driven%20Clustering%3A%20Enhancing%20Performance%20with%20Betti%20Number%0A%20%20Filtration%0AAuthor%3A%20Arghya%20Pratihar%20and%20Kushal%20Bose%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20Clustering%20aims%20to%20form%20groups%20of%20similar%20data%20points%20in%20an%20unsupervised%0Aregime.%20Yet%2C%20clustering%20complex%20datasets%20containing%20critically%20intertwined%0Ashapes%20poses%20significant%20challenges.%20The%20prevailing%20clustering%20algorithms%0Awidely%20depend%20on%20evaluating%20similarity%20measures%20based%20on%20Euclidean%20metrics.%0AExploring%20topological%20characteristics%20to%20perform%20clustering%20of%20complex%20datasets%0Ainevitably%20presents%20a%20better%20scope.%20The%20topological%20clustering%20algorithms%0Apredominantly%20perceive%20the%20point%20set%20through%20the%20lens%20of%20Simplicial%20complexes%0Aand%20Persistent%20homology.%20Despite%20these%20approaches%2C%20the%20existing%20topological%0Aclustering%20algorithms%20cannot%20somehow%20fully%20exploit%20topological%20structures%20and%0Ashow%20inconsistent%20performances%20on%20some%20highly%20complicated%20datasets.%20This%20work%0Aaims%20to%20mitigate%20the%20limitations%20by%20identifying%20topologically%20similar%20neighbors%0Athrough%20the%20Vietoris-Rips%20complex%20and%20Betti%20number%20filtration.%20In%20addition%2C%20we%0Aintroduce%20the%20concept%20of%20the%20Betti%20sequences%20to%20capture%20flexibly%20essential%0Afeatures%20from%20the%20topological%20structures.%20Our%20proposed%20algorithm%20is%20adept%20at%0Aclustering%20complex%2C%20intertwined%20shapes%20contained%20in%20the%20datasets.%20We%20carried%0Aout%20experiments%20on%20several%20synthetic%20and%20real-world%20datasets.%20Our%20algorithm%0Ademonstrated%20commendable%20performances%20across%20the%20datasets%20compared%20to%20some%20of%0Athe%20well-known%20topology-based%20clustering%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-Driven%2520Clustering%253A%2520Enhancing%2520Performance%2520with%2520Betti%2520Number%250A%2520%2520Filtration%26entry.906535625%3DArghya%2520Pratihar%2520and%2520Kushal%2520Bose%2520and%2520Swagatam%2520Das%26entry.1292438233%3D%2520%2520Clustering%2520aims%2520to%2520form%2520groups%2520of%2520similar%2520data%2520points%2520in%2520an%2520unsupervised%250Aregime.%2520Yet%252C%2520clustering%2520complex%2520datasets%2520containing%2520critically%2520intertwined%250Ashapes%2520poses%2520significant%2520challenges.%2520The%2520prevailing%2520clustering%2520algorithms%250Awidely%2520depend%2520on%2520evaluating%2520similarity%2520measures%2520based%2520on%2520Euclidean%2520metrics.%250AExploring%2520topological%2520characteristics%2520to%2520perform%2520clustering%2520of%2520complex%2520datasets%250Ainevitably%2520presents%2520a%2520better%2520scope.%2520The%2520topological%2520clustering%2520algorithms%250Apredominantly%2520perceive%2520the%2520point%2520set%2520through%2520the%2520lens%2520of%2520Simplicial%2520complexes%250Aand%2520Persistent%2520homology.%2520Despite%2520these%2520approaches%252C%2520the%2520existing%2520topological%250Aclustering%2520algorithms%2520cannot%2520somehow%2520fully%2520exploit%2520topological%2520structures%2520and%250Ashow%2520inconsistent%2520performances%2520on%2520some%2520highly%2520complicated%2520datasets.%2520This%2520work%250Aaims%2520to%2520mitigate%2520the%2520limitations%2520by%2520identifying%2520topologically%2520similar%2520neighbors%250Athrough%2520the%2520Vietoris-Rips%2520complex%2520and%2520Betti%2520number%2520filtration.%2520In%2520addition%252C%2520we%250Aintroduce%2520the%2520concept%2520of%2520the%2520Betti%2520sequences%2520to%2520capture%2520flexibly%2520essential%250Afeatures%2520from%2520the%2520topological%2520structures.%2520Our%2520proposed%2520algorithm%2520is%2520adept%2520at%250Aclustering%2520complex%252C%2520intertwined%2520shapes%2520contained%2520in%2520the%2520datasets.%2520We%2520carried%250Aout%2520experiments%2520on%2520several%2520synthetic%2520and%2520real-world%2520datasets.%2520Our%2520algorithm%250Ademonstrated%2520commendable%2520performances%2520across%2520the%2520datasets%2520compared%2520to%2520some%2520of%250Athe%2520well-known%2520topology-based%2520clustering%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-Driven%20Clustering%3A%20Enhancing%20Performance%20with%20Betti%20Number%0A%20%20Filtration&entry.906535625=Arghya%20Pratihar%20and%20Kushal%20Bose%20and%20Swagatam%20Das&entry.1292438233=%20%20Clustering%20aims%20to%20form%20groups%20of%20similar%20data%20points%20in%20an%20unsupervised%0Aregime.%20Yet%2C%20clustering%20complex%20datasets%20containing%20critically%20intertwined%0Ashapes%20poses%20significant%20challenges.%20The%20prevailing%20clustering%20algorithms%0Awidely%20depend%20on%20evaluating%20similarity%20measures%20based%20on%20Euclidean%20metrics.%0AExploring%20topological%20characteristics%20to%20perform%20clustering%20of%20complex%20datasets%0Ainevitably%20presents%20a%20better%20scope.%20The%20topological%20clustering%20algorithms%0Apredominantly%20perceive%20the%20point%20set%20through%20the%20lens%20of%20Simplicial%20complexes%0Aand%20Persistent%20homology.%20Despite%20these%20approaches%2C%20the%20existing%20topological%0Aclustering%20algorithms%20cannot%20somehow%20fully%20exploit%20topological%20structures%20and%0Ashow%20inconsistent%20performances%20on%20some%20highly%20complicated%20datasets.%20This%20work%0Aaims%20to%20mitigate%20the%20limitations%20by%20identifying%20topologically%20similar%20neighbors%0Athrough%20the%20Vietoris-Rips%20complex%20and%20Betti%20number%20filtration.%20In%20addition%2C%20we%0Aintroduce%20the%20concept%20of%20the%20Betti%20sequences%20to%20capture%20flexibly%20essential%0Afeatures%20from%20the%20topological%20structures.%20Our%20proposed%20algorithm%20is%20adept%20at%0Aclustering%20complex%2C%20intertwined%20shapes%20contained%20in%20the%20datasets.%20We%20carried%0Aout%20experiments%20on%20several%20synthetic%20and%20real-world%20datasets.%20Our%20algorithm%0Ademonstrated%20commendable%20performances%20across%20the%20datasets%20compared%20to%20some%20of%0Athe%20well-known%20topology-based%20clustering%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04346v1&entry.124074799=Read"},
{"title": "Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm\n  for Clustering Large Datasets", "author": "Jenni Lampainen and Kaisa Joki and Napsu Karmitsa and Marko M. M\u00e4kel\u00e4", "abstract": "  Clustering is a fundamental task in data mining and machine learning,\nparticularly for analyzing large-scale data. In this paper, we introduce\nClust-Splitter, an efficient algorithm based on nonsmooth optimization,\ndesigned to solve the minimum sum-of-squares clustering problem in very large\ndatasets. The clustering task is approached through a sequence of three\nnonsmooth optimization problems: two auxiliary problems used to generate\nsuitable starting points, followed by a main clustering formulation. To solve\nthese problems effectively, the limited memory bundle method is combined with\nan incremental approach to develop the Clust-Splitter algorithm. We evaluate\nClust-Splitter on real-world datasets characterized by both a large number of\nattributes and a large number of data points and compare its performance with\nseveral state-of-the-art large-scale clustering algorithms. Experimental\nresults demonstrate the efficiency of the proposed method for clustering very\nlarge datasets, as well as the high quality of its solutions, which are on par\nwith those of the best existing methods.\n", "link": "http://arxiv.org/abs/2505.04389v1", "date": "2025-05-07", "relevancy": 2.1, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.43}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4186}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clust-Splitter%20%24-%24%20an%20Efficient%20Nonsmooth%20Optimization-Based%20Algorithm%0A%20%20for%20Clustering%20Large%20Datasets&body=Title%3A%20Clust-Splitter%20%24-%24%20an%20Efficient%20Nonsmooth%20Optimization-Based%20Algorithm%0A%20%20for%20Clustering%20Large%20Datasets%0AAuthor%3A%20Jenni%20Lampainen%20and%20Kaisa%20Joki%20and%20Napsu%20Karmitsa%20and%20Marko%20M.%20M%C3%A4kel%C3%A4%0AAbstract%3A%20%20%20Clustering%20is%20a%20fundamental%20task%20in%20data%20mining%20and%20machine%20learning%2C%0Aparticularly%20for%20analyzing%20large-scale%20data.%20In%20this%20paper%2C%20we%20introduce%0AClust-Splitter%2C%20an%20efficient%20algorithm%20based%20on%20nonsmooth%20optimization%2C%0Adesigned%20to%20solve%20the%20minimum%20sum-of-squares%20clustering%20problem%20in%20very%20large%0Adatasets.%20The%20clustering%20task%20is%20approached%20through%20a%20sequence%20of%20three%0Anonsmooth%20optimization%20problems%3A%20two%20auxiliary%20problems%20used%20to%20generate%0Asuitable%20starting%20points%2C%20followed%20by%20a%20main%20clustering%20formulation.%20To%20solve%0Athese%20problems%20effectively%2C%20the%20limited%20memory%20bundle%20method%20is%20combined%20with%0Aan%20incremental%20approach%20to%20develop%20the%20Clust-Splitter%20algorithm.%20We%20evaluate%0AClust-Splitter%20on%20real-world%20datasets%20characterized%20by%20both%20a%20large%20number%20of%0Aattributes%20and%20a%20large%20number%20of%20data%20points%20and%20compare%20its%20performance%20with%0Aseveral%20state-of-the-art%20large-scale%20clustering%20algorithms.%20Experimental%0Aresults%20demonstrate%20the%20efficiency%20of%20the%20proposed%20method%20for%20clustering%20very%0Alarge%20datasets%2C%20as%20well%20as%20the%20high%20quality%20of%20its%20solutions%2C%20which%20are%20on%20par%0Awith%20those%20of%20the%20best%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04389v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClust-Splitter%2520%2524-%2524%2520an%2520Efficient%2520Nonsmooth%2520Optimization-Based%2520Algorithm%250A%2520%2520for%2520Clustering%2520Large%2520Datasets%26entry.906535625%3DJenni%2520Lampainen%2520and%2520Kaisa%2520Joki%2520and%2520Napsu%2520Karmitsa%2520and%2520Marko%2520M.%2520M%25C3%25A4kel%25C3%25A4%26entry.1292438233%3D%2520%2520Clustering%2520is%2520a%2520fundamental%2520task%2520in%2520data%2520mining%2520and%2520machine%2520learning%252C%250Aparticularly%2520for%2520analyzing%2520large-scale%2520data.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AClust-Splitter%252C%2520an%2520efficient%2520algorithm%2520based%2520on%2520nonsmooth%2520optimization%252C%250Adesigned%2520to%2520solve%2520the%2520minimum%2520sum-of-squares%2520clustering%2520problem%2520in%2520very%2520large%250Adatasets.%2520The%2520clustering%2520task%2520is%2520approached%2520through%2520a%2520sequence%2520of%2520three%250Anonsmooth%2520optimization%2520problems%253A%2520two%2520auxiliary%2520problems%2520used%2520to%2520generate%250Asuitable%2520starting%2520points%252C%2520followed%2520by%2520a%2520main%2520clustering%2520formulation.%2520To%2520solve%250Athese%2520problems%2520effectively%252C%2520the%2520limited%2520memory%2520bundle%2520method%2520is%2520combined%2520with%250Aan%2520incremental%2520approach%2520to%2520develop%2520the%2520Clust-Splitter%2520algorithm.%2520We%2520evaluate%250AClust-Splitter%2520on%2520real-world%2520datasets%2520characterized%2520by%2520both%2520a%2520large%2520number%2520of%250Aattributes%2520and%2520a%2520large%2520number%2520of%2520data%2520points%2520and%2520compare%2520its%2520performance%2520with%250Aseveral%2520state-of-the-art%2520large-scale%2520clustering%2520algorithms.%2520Experimental%250Aresults%2520demonstrate%2520the%2520efficiency%2520of%2520the%2520proposed%2520method%2520for%2520clustering%2520very%250Alarge%2520datasets%252C%2520as%2520well%2520as%2520the%2520high%2520quality%2520of%2520its%2520solutions%252C%2520which%2520are%2520on%2520par%250Awith%2520those%2520of%2520the%2520best%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04389v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clust-Splitter%20%24-%24%20an%20Efficient%20Nonsmooth%20Optimization-Based%20Algorithm%0A%20%20for%20Clustering%20Large%20Datasets&entry.906535625=Jenni%20Lampainen%20and%20Kaisa%20Joki%20and%20Napsu%20Karmitsa%20and%20Marko%20M.%20M%C3%A4kel%C3%A4&entry.1292438233=%20%20Clustering%20is%20a%20fundamental%20task%20in%20data%20mining%20and%20machine%20learning%2C%0Aparticularly%20for%20analyzing%20large-scale%20data.%20In%20this%20paper%2C%20we%20introduce%0AClust-Splitter%2C%20an%20efficient%20algorithm%20based%20on%20nonsmooth%20optimization%2C%0Adesigned%20to%20solve%20the%20minimum%20sum-of-squares%20clustering%20problem%20in%20very%20large%0Adatasets.%20The%20clustering%20task%20is%20approached%20through%20a%20sequence%20of%20three%0Anonsmooth%20optimization%20problems%3A%20two%20auxiliary%20problems%20used%20to%20generate%0Asuitable%20starting%20points%2C%20followed%20by%20a%20main%20clustering%20formulation.%20To%20solve%0Athese%20problems%20effectively%2C%20the%20limited%20memory%20bundle%20method%20is%20combined%20with%0Aan%20incremental%20approach%20to%20develop%20the%20Clust-Splitter%20algorithm.%20We%20evaluate%0AClust-Splitter%20on%20real-world%20datasets%20characterized%20by%20both%20a%20large%20number%20of%0Aattributes%20and%20a%20large%20number%20of%20data%20points%20and%20compare%20its%20performance%20with%0Aseveral%20state-of-the-art%20large-scale%20clustering%20algorithms.%20Experimental%0Aresults%20demonstrate%20the%20efficiency%20of%20the%20proposed%20method%20for%20clustering%20very%0Alarge%20datasets%2C%20as%20well%20as%20the%20high%20quality%20of%20its%20solutions%2C%20which%20are%20on%20par%0Awith%20those%20of%20the%20best%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04389v1&entry.124074799=Read"},
{"title": "Balancing Accuracy, Calibration, and Efficiency in Active Learning with\n  Vision Transformers Under Label Noise", "author": "Moseli Mots'oehli and Hope Mogale and Kyungim Baek", "abstract": "  Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation.\n", "link": "http://arxiv.org/abs/2505.04375v1", "date": "2025-05-07", "relevancy": 2.0903, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5847}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Balancing%20Accuracy%2C%20Calibration%2C%20and%20Efficiency%20in%20Active%20Learning%20with%0A%20%20Vision%20Transformers%20Under%20Label%20Noise&body=Title%3A%20Balancing%20Accuracy%2C%20Calibration%2C%20and%20Efficiency%20in%20Active%20Learning%20with%0A%20%20Vision%20Transformers%20Under%20Label%20Noise%0AAuthor%3A%20Moseli%20Mots%27oehli%20and%20Hope%20Mogale%20and%20Kyungim%20Baek%0AAbstract%3A%20%20%20Fine-tuning%20pre-trained%20convolutional%20neural%20networks%20on%20ImageNet%20for%0Adownstream%20tasks%20is%20well-established.%20Still%2C%20the%20impact%20of%20model%20size%20on%20the%0Aperformance%20of%20vision%20transformers%20in%20similar%20scenarios%2C%20particularly%20under%0Alabel%20noise%2C%20remains%20largely%20unexplored.%20Given%20the%20utility%20and%20versatility%20of%0Atransformer%20architectures%2C%20this%20study%20investigates%20their%20practicality%20under%0Alow-budget%20constraints%20and%20noisy%20labels.%20We%20explore%20how%20classification%20accuracy%0Aand%20calibration%20are%20affected%20by%20symmetric%20label%20noise%20in%20active%20learning%0Asettings%2C%20evaluating%20four%20vision%20transformer%20configurations%20%28Base%20and%20Large%0Awith%2016x16%20and%2032x32%20patch%20sizes%29%20and%20three%20Swin%20Transformer%20configurations%0A%28Tiny%2C%20Small%2C%20and%20Base%29%20on%20CIFAR10%20and%20CIFAR100%20datasets%2C%20under%20varying%20label%0Anoise%20rates.%20Our%20findings%20show%20that%20larger%20ViT%20models%20%28ViTl32%20in%20particular%29%0Aconsistently%20outperform%20their%20smaller%20counterparts%20in%20both%20accuracy%20and%0Acalibration%2C%20even%20under%20moderate%20to%20high%20label%20noise%2C%20while%20Swin%20Transformers%0Aexhibit%20weaker%20robustness%20across%20all%20noise%20levels.%20We%20find%20that%20smaller%20patch%0Asizes%20do%20not%20always%20lead%20to%20better%20performance%2C%20as%20ViTl16%20performs%20consistently%0Aworse%20than%20ViTl32%20while%20incurring%20a%20higher%20computational%20cost.%20We%20also%20find%0Athat%20information-based%20Active%20Learning%20strategies%20only%20provide%20meaningful%0Aaccuracy%20improvements%20at%20moderate%20label%20noise%20rates%2C%20but%20they%20result%20in%20poorer%0Acalibration%20compared%20to%20models%20trained%20on%20randomly%20acquired%20labels%2C%20especially%0Aat%20high%20label%20noise%20rates.%20We%20hope%20these%20insights%20provide%20actionable%20guidance%0Afor%20practitioners%20looking%20to%20deploy%20vision%20transformers%20in%20resource-constrained%0Aenvironments%2C%20where%20balancing%20model%20complexity%2C%20label%20noise%2C%20and%20compute%0Aefficiency%20is%20critical%20in%20model%20fine-tuning%20or%20distillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBalancing%2520Accuracy%252C%2520Calibration%252C%2520and%2520Efficiency%2520in%2520Active%2520Learning%2520with%250A%2520%2520Vision%2520Transformers%2520Under%2520Label%2520Noise%26entry.906535625%3DMoseli%2520Mots%2527oehli%2520and%2520Hope%2520Mogale%2520and%2520Kyungim%2520Baek%26entry.1292438233%3D%2520%2520Fine-tuning%2520pre-trained%2520convolutional%2520neural%2520networks%2520on%2520ImageNet%2520for%250Adownstream%2520tasks%2520is%2520well-established.%2520Still%252C%2520the%2520impact%2520of%2520model%2520size%2520on%2520the%250Aperformance%2520of%2520vision%2520transformers%2520in%2520similar%2520scenarios%252C%2520particularly%2520under%250Alabel%2520noise%252C%2520remains%2520largely%2520unexplored.%2520Given%2520the%2520utility%2520and%2520versatility%2520of%250Atransformer%2520architectures%252C%2520this%2520study%2520investigates%2520their%2520practicality%2520under%250Alow-budget%2520constraints%2520and%2520noisy%2520labels.%2520We%2520explore%2520how%2520classification%2520accuracy%250Aand%2520calibration%2520are%2520affected%2520by%2520symmetric%2520label%2520noise%2520in%2520active%2520learning%250Asettings%252C%2520evaluating%2520four%2520vision%2520transformer%2520configurations%2520%2528Base%2520and%2520Large%250Awith%252016x16%2520and%252032x32%2520patch%2520sizes%2529%2520and%2520three%2520Swin%2520Transformer%2520configurations%250A%2528Tiny%252C%2520Small%252C%2520and%2520Base%2529%2520on%2520CIFAR10%2520and%2520CIFAR100%2520datasets%252C%2520under%2520varying%2520label%250Anoise%2520rates.%2520Our%2520findings%2520show%2520that%2520larger%2520ViT%2520models%2520%2528ViTl32%2520in%2520particular%2529%250Aconsistently%2520outperform%2520their%2520smaller%2520counterparts%2520in%2520both%2520accuracy%2520and%250Acalibration%252C%2520even%2520under%2520moderate%2520to%2520high%2520label%2520noise%252C%2520while%2520Swin%2520Transformers%250Aexhibit%2520weaker%2520robustness%2520across%2520all%2520noise%2520levels.%2520We%2520find%2520that%2520smaller%2520patch%250Asizes%2520do%2520not%2520always%2520lead%2520to%2520better%2520performance%252C%2520as%2520ViTl16%2520performs%2520consistently%250Aworse%2520than%2520ViTl32%2520while%2520incurring%2520a%2520higher%2520computational%2520cost.%2520We%2520also%2520find%250Athat%2520information-based%2520Active%2520Learning%2520strategies%2520only%2520provide%2520meaningful%250Aaccuracy%2520improvements%2520at%2520moderate%2520label%2520noise%2520rates%252C%2520but%2520they%2520result%2520in%2520poorer%250Acalibration%2520compared%2520to%2520models%2520trained%2520on%2520randomly%2520acquired%2520labels%252C%2520especially%250Aat%2520high%2520label%2520noise%2520rates.%2520We%2520hope%2520these%2520insights%2520provide%2520actionable%2520guidance%250Afor%2520practitioners%2520looking%2520to%2520deploy%2520vision%2520transformers%2520in%2520resource-constrained%250Aenvironments%252C%2520where%2520balancing%2520model%2520complexity%252C%2520label%2520noise%252C%2520and%2520compute%250Aefficiency%2520is%2520critical%2520in%2520model%2520fine-tuning%2520or%2520distillation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Balancing%20Accuracy%2C%20Calibration%2C%20and%20Efficiency%20in%20Active%20Learning%20with%0A%20%20Vision%20Transformers%20Under%20Label%20Noise&entry.906535625=Moseli%20Mots%27oehli%20and%20Hope%20Mogale%20and%20Kyungim%20Baek&entry.1292438233=%20%20Fine-tuning%20pre-trained%20convolutional%20neural%20networks%20on%20ImageNet%20for%0Adownstream%20tasks%20is%20well-established.%20Still%2C%20the%20impact%20of%20model%20size%20on%20the%0Aperformance%20of%20vision%20transformers%20in%20similar%20scenarios%2C%20particularly%20under%0Alabel%20noise%2C%20remains%20largely%20unexplored.%20Given%20the%20utility%20and%20versatility%20of%0Atransformer%20architectures%2C%20this%20study%20investigates%20their%20practicality%20under%0Alow-budget%20constraints%20and%20noisy%20labels.%20We%20explore%20how%20classification%20accuracy%0Aand%20calibration%20are%20affected%20by%20symmetric%20label%20noise%20in%20active%20learning%0Asettings%2C%20evaluating%20four%20vision%20transformer%20configurations%20%28Base%20and%20Large%0Awith%2016x16%20and%2032x32%20patch%20sizes%29%20and%20three%20Swin%20Transformer%20configurations%0A%28Tiny%2C%20Small%2C%20and%20Base%29%20on%20CIFAR10%20and%20CIFAR100%20datasets%2C%20under%20varying%20label%0Anoise%20rates.%20Our%20findings%20show%20that%20larger%20ViT%20models%20%28ViTl32%20in%20particular%29%0Aconsistently%20outperform%20their%20smaller%20counterparts%20in%20both%20accuracy%20and%0Acalibration%2C%20even%20under%20moderate%20to%20high%20label%20noise%2C%20while%20Swin%20Transformers%0Aexhibit%20weaker%20robustness%20across%20all%20noise%20levels.%20We%20find%20that%20smaller%20patch%0Asizes%20do%20not%20always%20lead%20to%20better%20performance%2C%20as%20ViTl16%20performs%20consistently%0Aworse%20than%20ViTl32%20while%20incurring%20a%20higher%20computational%20cost.%20We%20also%20find%0Athat%20information-based%20Active%20Learning%20strategies%20only%20provide%20meaningful%0Aaccuracy%20improvements%20at%20moderate%20label%20noise%20rates%2C%20but%20they%20result%20in%20poorer%0Acalibration%20compared%20to%20models%20trained%20on%20randomly%20acquired%20labels%2C%20especially%0Aat%20high%20label%20noise%20rates.%20We%20hope%20these%20insights%20provide%20actionable%20guidance%0Afor%20practitioners%20looking%20to%20deploy%20vision%20transformers%20in%20resource-constrained%0Aenvironments%2C%20where%20balancing%20model%20complexity%2C%20label%20noise%2C%20and%20compute%0Aefficiency%20is%20critical%20in%20model%20fine-tuning%20or%20distillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04375v1&entry.124074799=Read"},
{"title": "Discrete Optimal Transport and Voice Conversion", "author": "Anton Selitskiy and Maitreya Kocharekar", "abstract": "  In this work, we address the voice conversion (VC) task using a vector-based\ninterface. To align audio embeddings between speakers, we employ discrete\noptimal transport mapping. Our evaluation results demonstrate the high quality\nand effectiveness of this method. Additionally, we show that applying discrete\noptimal transport as a post-processing step in audio generation can lead to the\nincorrect classification of synthetic audio as real.\n", "link": "http://arxiv.org/abs/2505.04382v1", "date": "2025-05-07", "relevancy": 2.0846, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5285}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.517}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Optimal%20Transport%20and%20Voice%20Conversion&body=Title%3A%20Discrete%20Optimal%20Transport%20and%20Voice%20Conversion%0AAuthor%3A%20Anton%20Selitskiy%20and%20Maitreya%20Kocharekar%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20voice%20conversion%20%28VC%29%20task%20using%20a%20vector-based%0Ainterface.%20To%20align%20audio%20embeddings%20between%20speakers%2C%20we%20employ%20discrete%0Aoptimal%20transport%20mapping.%20Our%20evaluation%20results%20demonstrate%20the%20high%20quality%0Aand%20effectiveness%20of%20this%20method.%20Additionally%2C%20we%20show%20that%20applying%20discrete%0Aoptimal%20transport%20as%20a%20post-processing%20step%20in%20audio%20generation%20can%20lead%20to%20the%0Aincorrect%20classification%20of%20synthetic%20audio%20as%20real.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Optimal%2520Transport%2520and%2520Voice%2520Conversion%26entry.906535625%3DAnton%2520Selitskiy%2520and%2520Maitreya%2520Kocharekar%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520voice%2520conversion%2520%2528VC%2529%2520task%2520using%2520a%2520vector-based%250Ainterface.%2520To%2520align%2520audio%2520embeddings%2520between%2520speakers%252C%2520we%2520employ%2520discrete%250Aoptimal%2520transport%2520mapping.%2520Our%2520evaluation%2520results%2520demonstrate%2520the%2520high%2520quality%250Aand%2520effectiveness%2520of%2520this%2520method.%2520Additionally%252C%2520we%2520show%2520that%2520applying%2520discrete%250Aoptimal%2520transport%2520as%2520a%2520post-processing%2520step%2520in%2520audio%2520generation%2520can%2520lead%2520to%2520the%250Aincorrect%2520classification%2520of%2520synthetic%2520audio%2520as%2520real.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Optimal%20Transport%20and%20Voice%20Conversion&entry.906535625=Anton%20Selitskiy%20and%20Maitreya%20Kocharekar&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20voice%20conversion%20%28VC%29%20task%20using%20a%20vector-based%0Ainterface.%20To%20align%20audio%20embeddings%20between%20speakers%2C%20we%20employ%20discrete%0Aoptimal%20transport%20mapping.%20Our%20evaluation%20results%20demonstrate%20the%20high%20quality%0Aand%20effectiveness%20of%20this%20method.%20Additionally%2C%20we%20show%20that%20applying%20discrete%0Aoptimal%20transport%20as%20a%20post-processing%20step%20in%20audio%20generation%20can%20lead%20to%20the%0Aincorrect%20classification%20of%20synthetic%20audio%20as%20real.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04382v1&entry.124074799=Read"},
{"title": "From Latent to Lucid: Transforming Knowledge Graph Embeddings into\n  Interpretable Structures with KGEPrisma", "author": "Christoph Wehner and Chrysa Iliopoulou and Ute Schmid and Tarek R. Besold", "abstract": "  In this paper, we introduce a post-hoc and local explainable AI method\ntailored for Knowledge Graph Embedding (KGE) models. These models are essential\nto Knowledge Graph Completion yet criticized for their opaque, black-box\nnature. Despite their significant success in capturing the semantics of\nknowledge graphs through high-dimensional latent representations, their\ninherent complexity poses substantial challenges to explainability. While\nexisting methods like Kelpie use resource-intensive perturbation to explain KGE\nmodels, our approach directly decodes the latent representations encoded by KGE\nmodels, leveraging the smoothness of the embeddings, which follows the\nprinciple that similar embeddings reflect similar behaviours within the\nKnowledge Graph, meaning that nodes are similarly embedded because their graph\nneighbourhood looks similar. This principle is commonly referred to as\nsmoothness. By identifying symbolic structures, in the form of triples, within\nthe subgraph neighborhoods of similarly embedded entities, our method\nidentifies the statistical regularities on which the models rely and translates\nthese insights into human-understandable symbolic rules and facts. This bridges\nthe gap between the abstract representations of KGE models and their predictive\noutputs, offering clear, interpretable insights. Key contributions include a\nnovel post-hoc and local explainable AI method for KGE models that provides\nimmediate, faithful explanations without retraining, facilitating real-time\napplication on large-scale knowledge graphs. The method's flexibility enables\nthe generation of rule-based, instance-based, and analogy-based explanations,\nmeeting diverse user needs. Extensive evaluations show the effectiveness of our\napproach in delivering faithful and well-localized explanations, enhancing the\ntransparency and trustworthiness of KGE models.\n", "link": "http://arxiv.org/abs/2406.01759v2", "date": "2025-05-07", "relevancy": 2.0604, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Latent%20to%20Lucid%3A%20Transforming%20Knowledge%20Graph%20Embeddings%20into%0A%20%20Interpretable%20Structures%20with%20KGEPrisma&body=Title%3A%20From%20Latent%20to%20Lucid%3A%20Transforming%20Knowledge%20Graph%20Embeddings%20into%0A%20%20Interpretable%20Structures%20with%20KGEPrisma%0AAuthor%3A%20Christoph%20Wehner%20and%20Chrysa%20Iliopoulou%20and%20Ute%20Schmid%20and%20Tarek%20R.%20Besold%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20a%20post-hoc%20and%20local%20explainable%20AI%20method%0Atailored%20for%20Knowledge%20Graph%20Embedding%20%28KGE%29%20models.%20These%20models%20are%20essential%0Ato%20Knowledge%20Graph%20Completion%20yet%20criticized%20for%20their%20opaque%2C%20black-box%0Anature.%20Despite%20their%20significant%20success%20in%20capturing%20the%20semantics%20of%0Aknowledge%20graphs%20through%20high-dimensional%20latent%20representations%2C%20their%0Ainherent%20complexity%20poses%20substantial%20challenges%20to%20explainability.%20While%0Aexisting%20methods%20like%20Kelpie%20use%20resource-intensive%20perturbation%20to%20explain%20KGE%0Amodels%2C%20our%20approach%20directly%20decodes%20the%20latent%20representations%20encoded%20by%20KGE%0Amodels%2C%20leveraging%20the%20smoothness%20of%20the%20embeddings%2C%20which%20follows%20the%0Aprinciple%20that%20similar%20embeddings%20reflect%20similar%20behaviours%20within%20the%0AKnowledge%20Graph%2C%20meaning%20that%20nodes%20are%20similarly%20embedded%20because%20their%20graph%0Aneighbourhood%20looks%20similar.%20This%20principle%20is%20commonly%20referred%20to%20as%0Asmoothness.%20By%20identifying%20symbolic%20structures%2C%20in%20the%20form%20of%20triples%2C%20within%0Athe%20subgraph%20neighborhoods%20of%20similarly%20embedded%20entities%2C%20our%20method%0Aidentifies%20the%20statistical%20regularities%20on%20which%20the%20models%20rely%20and%20translates%0Athese%20insights%20into%20human-understandable%20symbolic%20rules%20and%20facts.%20This%20bridges%0Athe%20gap%20between%20the%20abstract%20representations%20of%20KGE%20models%20and%20their%20predictive%0Aoutputs%2C%20offering%20clear%2C%20interpretable%20insights.%20Key%20contributions%20include%20a%0Anovel%20post-hoc%20and%20local%20explainable%20AI%20method%20for%20KGE%20models%20that%20provides%0Aimmediate%2C%20faithful%20explanations%20without%20retraining%2C%20facilitating%20real-time%0Aapplication%20on%20large-scale%20knowledge%20graphs.%20The%20method%27s%20flexibility%20enables%0Athe%20generation%20of%20rule-based%2C%20instance-based%2C%20and%20analogy-based%20explanations%2C%0Ameeting%20diverse%20user%20needs.%20Extensive%20evaluations%20show%20the%20effectiveness%20of%20our%0Aapproach%20in%20delivering%20faithful%20and%20well-localized%20explanations%2C%20enhancing%20the%0Atransparency%20and%20trustworthiness%20of%20KGE%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Latent%2520to%2520Lucid%253A%2520Transforming%2520Knowledge%2520Graph%2520Embeddings%2520into%250A%2520%2520Interpretable%2520Structures%2520with%2520KGEPrisma%26entry.906535625%3DChristoph%2520Wehner%2520and%2520Chrysa%2520Iliopoulou%2520and%2520Ute%2520Schmid%2520and%2520Tarek%2520R.%2520Besold%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520post-hoc%2520and%2520local%2520explainable%2520AI%2520method%250Atailored%2520for%2520Knowledge%2520Graph%2520Embedding%2520%2528KGE%2529%2520models.%2520These%2520models%2520are%2520essential%250Ato%2520Knowledge%2520Graph%2520Completion%2520yet%2520criticized%2520for%2520their%2520opaque%252C%2520black-box%250Anature.%2520Despite%2520their%2520significant%2520success%2520in%2520capturing%2520the%2520semantics%2520of%250Aknowledge%2520graphs%2520through%2520high-dimensional%2520latent%2520representations%252C%2520their%250Ainherent%2520complexity%2520poses%2520substantial%2520challenges%2520to%2520explainability.%2520While%250Aexisting%2520methods%2520like%2520Kelpie%2520use%2520resource-intensive%2520perturbation%2520to%2520explain%2520KGE%250Amodels%252C%2520our%2520approach%2520directly%2520decodes%2520the%2520latent%2520representations%2520encoded%2520by%2520KGE%250Amodels%252C%2520leveraging%2520the%2520smoothness%2520of%2520the%2520embeddings%252C%2520which%2520follows%2520the%250Aprinciple%2520that%2520similar%2520embeddings%2520reflect%2520similar%2520behaviours%2520within%2520the%250AKnowledge%2520Graph%252C%2520meaning%2520that%2520nodes%2520are%2520similarly%2520embedded%2520because%2520their%2520graph%250Aneighbourhood%2520looks%2520similar.%2520This%2520principle%2520is%2520commonly%2520referred%2520to%2520as%250Asmoothness.%2520By%2520identifying%2520symbolic%2520structures%252C%2520in%2520the%2520form%2520of%2520triples%252C%2520within%250Athe%2520subgraph%2520neighborhoods%2520of%2520similarly%2520embedded%2520entities%252C%2520our%2520method%250Aidentifies%2520the%2520statistical%2520regularities%2520on%2520which%2520the%2520models%2520rely%2520and%2520translates%250Athese%2520insights%2520into%2520human-understandable%2520symbolic%2520rules%2520and%2520facts.%2520This%2520bridges%250Athe%2520gap%2520between%2520the%2520abstract%2520representations%2520of%2520KGE%2520models%2520and%2520their%2520predictive%250Aoutputs%252C%2520offering%2520clear%252C%2520interpretable%2520insights.%2520Key%2520contributions%2520include%2520a%250Anovel%2520post-hoc%2520and%2520local%2520explainable%2520AI%2520method%2520for%2520KGE%2520models%2520that%2520provides%250Aimmediate%252C%2520faithful%2520explanations%2520without%2520retraining%252C%2520facilitating%2520real-time%250Aapplication%2520on%2520large-scale%2520knowledge%2520graphs.%2520The%2520method%2527s%2520flexibility%2520enables%250Athe%2520generation%2520of%2520rule-based%252C%2520instance-based%252C%2520and%2520analogy-based%2520explanations%252C%250Ameeting%2520diverse%2520user%2520needs.%2520Extensive%2520evaluations%2520show%2520the%2520effectiveness%2520of%2520our%250Aapproach%2520in%2520delivering%2520faithful%2520and%2520well-localized%2520explanations%252C%2520enhancing%2520the%250Atransparency%2520and%2520trustworthiness%2520of%2520KGE%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Latent%20to%20Lucid%3A%20Transforming%20Knowledge%20Graph%20Embeddings%20into%0A%20%20Interpretable%20Structures%20with%20KGEPrisma&entry.906535625=Christoph%20Wehner%20and%20Chrysa%20Iliopoulou%20and%20Ute%20Schmid%20and%20Tarek%20R.%20Besold&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20a%20post-hoc%20and%20local%20explainable%20AI%20method%0Atailored%20for%20Knowledge%20Graph%20Embedding%20%28KGE%29%20models.%20These%20models%20are%20essential%0Ato%20Knowledge%20Graph%20Completion%20yet%20criticized%20for%20their%20opaque%2C%20black-box%0Anature.%20Despite%20their%20significant%20success%20in%20capturing%20the%20semantics%20of%0Aknowledge%20graphs%20through%20high-dimensional%20latent%20representations%2C%20their%0Ainherent%20complexity%20poses%20substantial%20challenges%20to%20explainability.%20While%0Aexisting%20methods%20like%20Kelpie%20use%20resource-intensive%20perturbation%20to%20explain%20KGE%0Amodels%2C%20our%20approach%20directly%20decodes%20the%20latent%20representations%20encoded%20by%20KGE%0Amodels%2C%20leveraging%20the%20smoothness%20of%20the%20embeddings%2C%20which%20follows%20the%0Aprinciple%20that%20similar%20embeddings%20reflect%20similar%20behaviours%20within%20the%0AKnowledge%20Graph%2C%20meaning%20that%20nodes%20are%20similarly%20embedded%20because%20their%20graph%0Aneighbourhood%20looks%20similar.%20This%20principle%20is%20commonly%20referred%20to%20as%0Asmoothness.%20By%20identifying%20symbolic%20structures%2C%20in%20the%20form%20of%20triples%2C%20within%0Athe%20subgraph%20neighborhoods%20of%20similarly%20embedded%20entities%2C%20our%20method%0Aidentifies%20the%20statistical%20regularities%20on%20which%20the%20models%20rely%20and%20translates%0Athese%20insights%20into%20human-understandable%20symbolic%20rules%20and%20facts.%20This%20bridges%0Athe%20gap%20between%20the%20abstract%20representations%20of%20KGE%20models%20and%20their%20predictive%0Aoutputs%2C%20offering%20clear%2C%20interpretable%20insights.%20Key%20contributions%20include%20a%0Anovel%20post-hoc%20and%20local%20explainable%20AI%20method%20for%20KGE%20models%20that%20provides%0Aimmediate%2C%20faithful%20explanations%20without%20retraining%2C%20facilitating%20real-time%0Aapplication%20on%20large-scale%20knowledge%20graphs.%20The%20method%27s%20flexibility%20enables%0Athe%20generation%20of%20rule-based%2C%20instance-based%2C%20and%20analogy-based%20explanations%2C%0Ameeting%20diverse%20user%20needs.%20Extensive%20evaluations%20show%20the%20effectiveness%20of%20our%0Aapproach%20in%20delivering%20faithful%20and%20well-localized%20explanations%2C%20enhancing%20the%0Atransparency%20and%20trustworthiness%20of%20KGE%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01759v2&entry.124074799=Read"},
{"title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models", "author": "Xiaoyu Xu and Minxin Du and Qingqing Ye and Haibo Hu", "abstract": "  Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.\n", "link": "http://arxiv.org/abs/2505.04416v1", "date": "2025-05-07", "relevancy": 2.0576, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5486}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OBLIVIATE%3A%20Robust%20and%20Practical%20Machine%20Unlearning%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20OBLIVIATE%3A%20Robust%20and%20Practical%20Machine%20Unlearning%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Xiaoyu%20Xu%20and%20Minxin%20Du%20and%20Qingqing%20Ye%20and%20Haibo%20Hu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20trained%20over%20extensive%20corpora%20risk%20memorizing%0Asensitive%2C%20copyrighted%2C%20or%20toxic%20content.%20To%20address%20this%2C%20we%20propose%0AOBLIVIATE%2C%20a%20robust%20unlearning%20framework%20that%20removes%20targeted%20data%20while%0Apreserving%20model%20utility.%20The%20framework%20follows%20a%20structured%20process%3A%0Aextracting%20target%20tokens%2C%20building%20retain%20sets%2C%20and%20fine-tuning%20with%20a%20tailored%0Aloss%20function%20comprising%20three%20components%20--%20masking%2C%20distillation%2C%20and%20world%0Afact.%20Using%20low-rank%20adapters%20%28LoRA%29%2C%20it%20ensures%20efficiency%20without%0Acompromising%20unlearning%20quality.%20We%20conduct%20experiments%20on%20multiple%20datasets%2C%0Aincluding%20the%20Harry%20Potter%20series%2C%20WMDP%2C%20and%20TOFU%2C%20using%20a%20comprehensive%20suite%0Aof%20metrics%3A%20forget%20quality%20%28new%20document-level%20memorization%20score%29%2C%20model%0Autility%2C%20and%20fluency.%20Results%20demonstrate%20its%20effectiveness%20in%20resisting%0Amembership%20inference%20attacks%2C%20minimizing%20the%20impact%20on%20retained%20data%2C%20and%0Amaintaining%20robustness%20across%20diverse%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOBLIVIATE%253A%2520Robust%2520and%2520Practical%2520Machine%2520Unlearning%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DXiaoyu%2520Xu%2520and%2520Minxin%2520Du%2520and%2520Qingqing%2520Ye%2520and%2520Haibo%2520Hu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520trained%2520over%2520extensive%2520corpora%2520risk%2520memorizing%250Asensitive%252C%2520copyrighted%252C%2520or%2520toxic%2520content.%2520To%2520address%2520this%252C%2520we%2520propose%250AOBLIVIATE%252C%2520a%2520robust%2520unlearning%2520framework%2520that%2520removes%2520targeted%2520data%2520while%250Apreserving%2520model%2520utility.%2520The%2520framework%2520follows%2520a%2520structured%2520process%253A%250Aextracting%2520target%2520tokens%252C%2520building%2520retain%2520sets%252C%2520and%2520fine-tuning%2520with%2520a%2520tailored%250Aloss%2520function%2520comprising%2520three%2520components%2520--%2520masking%252C%2520distillation%252C%2520and%2520world%250Afact.%2520Using%2520low-rank%2520adapters%2520%2528LoRA%2529%252C%2520it%2520ensures%2520efficiency%2520without%250Acompromising%2520unlearning%2520quality.%2520We%2520conduct%2520experiments%2520on%2520multiple%2520datasets%252C%250Aincluding%2520the%2520Harry%2520Potter%2520series%252C%2520WMDP%252C%2520and%2520TOFU%252C%2520using%2520a%2520comprehensive%2520suite%250Aof%2520metrics%253A%2520forget%2520quality%2520%2528new%2520document-level%2520memorization%2520score%2529%252C%2520model%250Autility%252C%2520and%2520fluency.%2520Results%2520demonstrate%2520its%2520effectiveness%2520in%2520resisting%250Amembership%2520inference%2520attacks%252C%2520minimizing%2520the%2520impact%2520on%2520retained%2520data%252C%2520and%250Amaintaining%2520robustness%2520across%2520diverse%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OBLIVIATE%3A%20Robust%20and%20Practical%20Machine%20Unlearning%20for%20Large%20Language%0A%20%20Models&entry.906535625=Xiaoyu%20Xu%20and%20Minxin%20Du%20and%20Qingqing%20Ye%20and%20Haibo%20Hu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20trained%20over%20extensive%20corpora%20risk%20memorizing%0Asensitive%2C%20copyrighted%2C%20or%20toxic%20content.%20To%20address%20this%2C%20we%20propose%0AOBLIVIATE%2C%20a%20robust%20unlearning%20framework%20that%20removes%20targeted%20data%20while%0Apreserving%20model%20utility.%20The%20framework%20follows%20a%20structured%20process%3A%0Aextracting%20target%20tokens%2C%20building%20retain%20sets%2C%20and%20fine-tuning%20with%20a%20tailored%0Aloss%20function%20comprising%20three%20components%20--%20masking%2C%20distillation%2C%20and%20world%0Afact.%20Using%20low-rank%20adapters%20%28LoRA%29%2C%20it%20ensures%20efficiency%20without%0Acompromising%20unlearning%20quality.%20We%20conduct%20experiments%20on%20multiple%20datasets%2C%0Aincluding%20the%20Harry%20Potter%20series%2C%20WMDP%2C%20and%20TOFU%2C%20using%20a%20comprehensive%20suite%0Aof%20metrics%3A%20forget%20quality%20%28new%20document-level%20memorization%20score%29%2C%20model%0Autility%2C%20and%20fluency.%20Results%20demonstrate%20its%20effectiveness%20in%20resisting%0Amembership%20inference%20attacks%2C%20minimizing%20the%20impact%20on%20retained%20data%2C%20and%0Amaintaining%20robustness%20across%20diverse%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04416v1&entry.124074799=Read"},
{"title": "FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training", "author": "Haoyuan Li and Mathias Funk and Jindong Wang and Aaqib Saeed", "abstract": "  Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget.\n", "link": "http://arxiv.org/abs/2504.03783v3", "date": "2025-05-07", "relevancy": 2.0562, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5216}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAST%3A%20Federated%20Active%20Learning%20with%20Foundation%20Models%20for%0A%20%20Communication-efficient%20Sampling%20and%20Training&body=Title%3A%20FAST%3A%20Federated%20Active%20Learning%20with%20Foundation%20Models%20for%0A%20%20Communication-efficient%20Sampling%20and%20Training%0AAuthor%3A%20Haoyuan%20Li%20and%20Mathias%20Funk%20and%20Jindong%20Wang%20and%20Aaqib%20Saeed%0AAbstract%3A%20%20%20Federated%20Active%20Learning%20%28FAL%29%20has%20emerged%20as%20a%20promising%20framework%20to%0Aleverage%20large%20quantities%20of%20unlabeled%20data%20across%20distributed%20clients%20while%0Apreserving%20data%20privacy.%20However%2C%20real-world%20deployments%20remain%20limited%20by%20high%0Aannotation%20costs%20and%20communication-intensive%20sampling%20processes%2C%20particularly%0Ain%20a%20cross-silo%20setting%2C%20when%20clients%20possess%20substantial%20local%20datasets.%20This%0Apaper%20addresses%20the%20crucial%20question%3A%20What%20is%20the%20best%20practice%20to%20reduce%0Acommunication%20costs%20in%20human-in-the-loop%20learning%20with%20minimal%20annotator%0Aeffort%3F%20Existing%20FAL%20methods%20typically%20rely%20on%20iterative%20annotation%20processes%0Athat%20separate%20active%20sampling%20from%20federated%20updates%2C%20leading%20to%20multiple%0Arounds%20of%20expensive%20communication%20and%20annotation.%20In%20response%2C%20we%20introduce%0AFAST%2C%20a%20two-pass%20FAL%20framework%20that%20harnesses%20foundation%20models%20for%20weak%0Alabeling%20in%20a%20preliminary%20pass%2C%20followed%20by%20a%20refinement%20pass%20focused%0Aexclusively%20on%20the%20most%20uncertain%20samples.%20By%20leveraging%20representation%0Aknowledge%20from%20foundation%20models%20and%20integrating%20refinement%20steps%20into%20a%0Astreamlined%20workflow%2C%20FAST%20substantially%20reduces%20the%20overhead%20incurred%20by%0Aiterative%20active%20sampling.%20Extensive%20experiments%20on%20diverse%20medical%20and%20natural%0Aimage%20benchmarks%20demonstrate%20that%20FAST%20outperforms%20existing%20FAL%20methods%20by%20an%0Aaverage%20of%204.36%25%20while%20reducing%20communication%20rounds%20eightfold%20under%20a%20limited%0A5%25%20labeling%20budget.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.03783v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAST%253A%2520Federated%2520Active%2520Learning%2520with%2520Foundation%2520Models%2520for%250A%2520%2520Communication-efficient%2520Sampling%2520and%2520Training%26entry.906535625%3DHaoyuan%2520Li%2520and%2520Mathias%2520Funk%2520and%2520Jindong%2520Wang%2520and%2520Aaqib%2520Saeed%26entry.1292438233%3D%2520%2520Federated%2520Active%2520Learning%2520%2528FAL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520framework%2520to%250Aleverage%2520large%2520quantities%2520of%2520unlabeled%2520data%2520across%2520distributed%2520clients%2520while%250Apreserving%2520data%2520privacy.%2520However%252C%2520real-world%2520deployments%2520remain%2520limited%2520by%2520high%250Aannotation%2520costs%2520and%2520communication-intensive%2520sampling%2520processes%252C%2520particularly%250Ain%2520a%2520cross-silo%2520setting%252C%2520when%2520clients%2520possess%2520substantial%2520local%2520datasets.%2520This%250Apaper%2520addresses%2520the%2520crucial%2520question%253A%2520What%2520is%2520the%2520best%2520practice%2520to%2520reduce%250Acommunication%2520costs%2520in%2520human-in-the-loop%2520learning%2520with%2520minimal%2520annotator%250Aeffort%253F%2520Existing%2520FAL%2520methods%2520typically%2520rely%2520on%2520iterative%2520annotation%2520processes%250Athat%2520separate%2520active%2520sampling%2520from%2520federated%2520updates%252C%2520leading%2520to%2520multiple%250Arounds%2520of%2520expensive%2520communication%2520and%2520annotation.%2520In%2520response%252C%2520we%2520introduce%250AFAST%252C%2520a%2520two-pass%2520FAL%2520framework%2520that%2520harnesses%2520foundation%2520models%2520for%2520weak%250Alabeling%2520in%2520a%2520preliminary%2520pass%252C%2520followed%2520by%2520a%2520refinement%2520pass%2520focused%250Aexclusively%2520on%2520the%2520most%2520uncertain%2520samples.%2520By%2520leveraging%2520representation%250Aknowledge%2520from%2520foundation%2520models%2520and%2520integrating%2520refinement%2520steps%2520into%2520a%250Astreamlined%2520workflow%252C%2520FAST%2520substantially%2520reduces%2520the%2520overhead%2520incurred%2520by%250Aiterative%2520active%2520sampling.%2520Extensive%2520experiments%2520on%2520diverse%2520medical%2520and%2520natural%250Aimage%2520benchmarks%2520demonstrate%2520that%2520FAST%2520outperforms%2520existing%2520FAL%2520methods%2520by%2520an%250Aaverage%2520of%25204.36%2525%2520while%2520reducing%2520communication%2520rounds%2520eightfold%2520under%2520a%2520limited%250A5%2525%2520labeling%2520budget.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.03783v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAST%3A%20Federated%20Active%20Learning%20with%20Foundation%20Models%20for%0A%20%20Communication-efficient%20Sampling%20and%20Training&entry.906535625=Haoyuan%20Li%20and%20Mathias%20Funk%20and%20Jindong%20Wang%20and%20Aaqib%20Saeed&entry.1292438233=%20%20Federated%20Active%20Learning%20%28FAL%29%20has%20emerged%20as%20a%20promising%20framework%20to%0Aleverage%20large%20quantities%20of%20unlabeled%20data%20across%20distributed%20clients%20while%0Apreserving%20data%20privacy.%20However%2C%20real-world%20deployments%20remain%20limited%20by%20high%0Aannotation%20costs%20and%20communication-intensive%20sampling%20processes%2C%20particularly%0Ain%20a%20cross-silo%20setting%2C%20when%20clients%20possess%20substantial%20local%20datasets.%20This%0Apaper%20addresses%20the%20crucial%20question%3A%20What%20is%20the%20best%20practice%20to%20reduce%0Acommunication%20costs%20in%20human-in-the-loop%20learning%20with%20minimal%20annotator%0Aeffort%3F%20Existing%20FAL%20methods%20typically%20rely%20on%20iterative%20annotation%20processes%0Athat%20separate%20active%20sampling%20from%20federated%20updates%2C%20leading%20to%20multiple%0Arounds%20of%20expensive%20communication%20and%20annotation.%20In%20response%2C%20we%20introduce%0AFAST%2C%20a%20two-pass%20FAL%20framework%20that%20harnesses%20foundation%20models%20for%20weak%0Alabeling%20in%20a%20preliminary%20pass%2C%20followed%20by%20a%20refinement%20pass%20focused%0Aexclusively%20on%20the%20most%20uncertain%20samples.%20By%20leveraging%20representation%0Aknowledge%20from%20foundation%20models%20and%20integrating%20refinement%20steps%20into%20a%0Astreamlined%20workflow%2C%20FAST%20substantially%20reduces%20the%20overhead%20incurred%20by%0Aiterative%20active%20sampling.%20Extensive%20experiments%20on%20diverse%20medical%20and%20natural%0Aimage%20benchmarks%20demonstrate%20that%20FAST%20outperforms%20existing%20FAL%20methods%20by%20an%0Aaverage%20of%204.36%25%20while%20reducing%20communication%20rounds%20eightfold%20under%20a%20limited%0A5%25%20labeling%20budget.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.03783v3&entry.124074799=Read"},
{"title": "Implicitly Aligning Humans and Autonomous Agents through Shared Task\n  Abstractions", "author": "St\u00e9phane Aroca-Ouellette and Miguel Aroca-Ouellette and Katharina von der Wense and Alessandro Roncone", "abstract": "  In collaborative tasks, autonomous agents fall short of humans in their\ncapability to quickly adapt to new and unfamiliar teammates. We posit that a\nlimiting factor for zero-shot coordination is the lack of shared task\nabstractions, a mechanism humans rely on to implicitly align with teammates. To\naddress this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework\nleveraging hierarchical reinforcement learning to mimic the structured approach\nhumans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,\ndemonstrating statistically significant improvement over existing baselines\nwhen paired with both unseen agents and humans, providing better resilience to\nenvironmental shifts, and outperforming all state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2505.04579v1", "date": "2025-05-07", "relevancy": 2.0478, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5261}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5104}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicitly%20Aligning%20Humans%20and%20Autonomous%20Agents%20through%20Shared%20Task%0A%20%20Abstractions&body=Title%3A%20Implicitly%20Aligning%20Humans%20and%20Autonomous%20Agents%20through%20Shared%20Task%0A%20%20Abstractions%0AAuthor%3A%20St%C3%A9phane%20Aroca-Ouellette%20and%20Miguel%20Aroca-Ouellette%20and%20Katharina%20von%20der%20Wense%20and%20Alessandro%20Roncone%0AAbstract%3A%20%20%20In%20collaborative%20tasks%2C%20autonomous%20agents%20fall%20short%20of%20humans%20in%20their%0Acapability%20to%20quickly%20adapt%20to%20new%20and%20unfamiliar%20teammates.%20We%20posit%20that%20a%0Alimiting%20factor%20for%20zero-shot%20coordination%20is%20the%20lack%20of%20shared%20task%0Aabstractions%2C%20a%20mechanism%20humans%20rely%20on%20to%20implicitly%20align%20with%20teammates.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20HA%24%5E2%24%3A%20Hierarchical%20Ad%20Hoc%20Agents%2C%20a%20framework%0Aleveraging%20hierarchical%20reinforcement%20learning%20to%20mimic%20the%20structured%20approach%0Ahumans%20use%20in%20collaboration.%20We%20evaluate%20HA%24%5E2%24%20in%20the%20Overcooked%20environment%2C%0Ademonstrating%20statistically%20significant%20improvement%20over%20existing%20baselines%0Awhen%20paired%20with%20both%20unseen%20agents%20and%20humans%2C%20providing%20better%20resilience%20to%0Aenvironmental%20shifts%2C%20and%20outperforming%20all%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicitly%2520Aligning%2520Humans%2520and%2520Autonomous%2520Agents%2520through%2520Shared%2520Task%250A%2520%2520Abstractions%26entry.906535625%3DSt%25C3%25A9phane%2520Aroca-Ouellette%2520and%2520Miguel%2520Aroca-Ouellette%2520and%2520Katharina%2520von%2520der%2520Wense%2520and%2520Alessandro%2520Roncone%26entry.1292438233%3D%2520%2520In%2520collaborative%2520tasks%252C%2520autonomous%2520agents%2520fall%2520short%2520of%2520humans%2520in%2520their%250Acapability%2520to%2520quickly%2520adapt%2520to%2520new%2520and%2520unfamiliar%2520teammates.%2520We%2520posit%2520that%2520a%250Alimiting%2520factor%2520for%2520zero-shot%2520coordination%2520is%2520the%2520lack%2520of%2520shared%2520task%250Aabstractions%252C%2520a%2520mechanism%2520humans%2520rely%2520on%2520to%2520implicitly%2520align%2520with%2520teammates.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520HA%2524%255E2%2524%253A%2520Hierarchical%2520Ad%2520Hoc%2520Agents%252C%2520a%2520framework%250Aleveraging%2520hierarchical%2520reinforcement%2520learning%2520to%2520mimic%2520the%2520structured%2520approach%250Ahumans%2520use%2520in%2520collaboration.%2520We%2520evaluate%2520HA%2524%255E2%2524%2520in%2520the%2520Overcooked%2520environment%252C%250Ademonstrating%2520statistically%2520significant%2520improvement%2520over%2520existing%2520baselines%250Awhen%2520paired%2520with%2520both%2520unseen%2520agents%2520and%2520humans%252C%2520providing%2520better%2520resilience%2520to%250Aenvironmental%2520shifts%252C%2520and%2520outperforming%2520all%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicitly%20Aligning%20Humans%20and%20Autonomous%20Agents%20through%20Shared%20Task%0A%20%20Abstractions&entry.906535625=St%C3%A9phane%20Aroca-Ouellette%20and%20Miguel%20Aroca-Ouellette%20and%20Katharina%20von%20der%20Wense%20and%20Alessandro%20Roncone&entry.1292438233=%20%20In%20collaborative%20tasks%2C%20autonomous%20agents%20fall%20short%20of%20humans%20in%20their%0Acapability%20to%20quickly%20adapt%20to%20new%20and%20unfamiliar%20teammates.%20We%20posit%20that%20a%0Alimiting%20factor%20for%20zero-shot%20coordination%20is%20the%20lack%20of%20shared%20task%0Aabstractions%2C%20a%20mechanism%20humans%20rely%20on%20to%20implicitly%20align%20with%20teammates.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20HA%24%5E2%24%3A%20Hierarchical%20Ad%20Hoc%20Agents%2C%20a%20framework%0Aleveraging%20hierarchical%20reinforcement%20learning%20to%20mimic%20the%20structured%20approach%0Ahumans%20use%20in%20collaboration.%20We%20evaluate%20HA%24%5E2%24%20in%20the%20Overcooked%20environment%2C%0Ademonstrating%20statistically%20significant%20improvement%20over%20existing%20baselines%0Awhen%20paired%20with%20both%20unseen%20agents%20and%20humans%2C%20providing%20better%20resilience%20to%0Aenvironmental%20shifts%2C%20and%20outperforming%20all%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04579v1&entry.124074799=Read"},
{"title": "Overcoming Data Scarcity in Generative Language Modelling for\n  Low-Resource Languages: A Systematic Review", "author": "Josh McGiff and Nikola S. Nikolov", "abstract": "  Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies.\n", "link": "http://arxiv.org/abs/2505.04531v1", "date": "2025-05-07", "relevancy": 2.0382, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5221}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5087}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Data%20Scarcity%20in%20Generative%20Language%20Modelling%20for%0A%20%20Low-Resource%20Languages%3A%20A%20Systematic%20Review&body=Title%3A%20Overcoming%20Data%20Scarcity%20in%20Generative%20Language%20Modelling%20for%0A%20%20Low-Resource%20Languages%3A%20A%20Systematic%20Review%0AAuthor%3A%20Josh%20McGiff%20and%20Nikola%20S.%20Nikolov%0AAbstract%3A%20%20%20Generative%20language%20modelling%20has%20surged%20in%20popularity%20with%20the%20emergence%20of%0Aservices%20such%20as%20ChatGPT%20and%20Google%20Gemini.%20While%20these%20models%20have%0Ademonstrated%20transformative%20potential%20in%20productivity%20and%20communication%2C%20they%0Aoverwhelmingly%20cater%20to%20high-resource%20languages%20like%20English.%20This%20has%0Aamplified%20concerns%20over%20linguistic%20inequality%20in%20natural%20language%20processing%0A%28NLP%29.%20This%20paper%20presents%20the%20first%20systematic%20review%20focused%20specifically%20on%0Astrategies%20to%20address%20data%20scarcity%20in%20generative%20language%20modelling%20for%0Alow-resource%20languages%20%28LRL%29.%20Drawing%20from%2054%20studies%2C%20we%20identify%2C%20categorise%0Aand%20evaluate%20technical%20approaches%2C%20including%20monolingual%20data%20augmentation%2C%0Aback-translation%2C%20multilingual%20training%2C%20and%20prompt%20engineering%2C%20across%0Agenerative%20tasks.%20We%20also%20analyse%20trends%20in%20architecture%20choices%2C%20language%0Afamily%20representation%2C%20and%20evaluation%20methods.%20Our%20findings%20highlight%20a%20strong%0Areliance%20on%20transformer-based%20models%2C%20a%20concentration%20on%20a%20small%20subset%20of%0ALRLs%2C%20and%20a%20lack%20of%20consistent%20evaluation%20across%20studies.%20We%20conclude%20with%0Arecommendations%20for%20extending%20these%20methods%20to%20a%20wider%20range%20of%20LRLs%20and%0Aoutline%20open%20challenges%20in%20building%20equitable%20generative%20language%20systems.%0AUltimately%2C%20this%20review%20aims%20to%20support%20researchers%20and%20developers%20in%20building%0Ainclusive%20AI%20tools%20for%20underrepresented%20languages%2C%20a%20necessary%20step%20toward%0Aempowering%20LRL%20speakers%20and%20the%20preservation%20of%20linguistic%20diversity%20in%20a%20world%0Aincreasingly%20shaped%20by%20large-scale%20language%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Data%2520Scarcity%2520in%2520Generative%2520Language%2520Modelling%2520for%250A%2520%2520Low-Resource%2520Languages%253A%2520A%2520Systematic%2520Review%26entry.906535625%3DJosh%2520McGiff%2520and%2520Nikola%2520S.%2520Nikolov%26entry.1292438233%3D%2520%2520Generative%2520language%2520modelling%2520has%2520surged%2520in%2520popularity%2520with%2520the%2520emergence%2520of%250Aservices%2520such%2520as%2520ChatGPT%2520and%2520Google%2520Gemini.%2520While%2520these%2520models%2520have%250Ademonstrated%2520transformative%2520potential%2520in%2520productivity%2520and%2520communication%252C%2520they%250Aoverwhelmingly%2520cater%2520to%2520high-resource%2520languages%2520like%2520English.%2520This%2520has%250Aamplified%2520concerns%2520over%2520linguistic%2520inequality%2520in%2520natural%2520language%2520processing%250A%2528NLP%2529.%2520This%2520paper%2520presents%2520the%2520first%2520systematic%2520review%2520focused%2520specifically%2520on%250Astrategies%2520to%2520address%2520data%2520scarcity%2520in%2520generative%2520language%2520modelling%2520for%250Alow-resource%2520languages%2520%2528LRL%2529.%2520Drawing%2520from%252054%2520studies%252C%2520we%2520identify%252C%2520categorise%250Aand%2520evaluate%2520technical%2520approaches%252C%2520including%2520monolingual%2520data%2520augmentation%252C%250Aback-translation%252C%2520multilingual%2520training%252C%2520and%2520prompt%2520engineering%252C%2520across%250Agenerative%2520tasks.%2520We%2520also%2520analyse%2520trends%2520in%2520architecture%2520choices%252C%2520language%250Afamily%2520representation%252C%2520and%2520evaluation%2520methods.%2520Our%2520findings%2520highlight%2520a%2520strong%250Areliance%2520on%2520transformer-based%2520models%252C%2520a%2520concentration%2520on%2520a%2520small%2520subset%2520of%250ALRLs%252C%2520and%2520a%2520lack%2520of%2520consistent%2520evaluation%2520across%2520studies.%2520We%2520conclude%2520with%250Arecommendations%2520for%2520extending%2520these%2520methods%2520to%2520a%2520wider%2520range%2520of%2520LRLs%2520and%250Aoutline%2520open%2520challenges%2520in%2520building%2520equitable%2520generative%2520language%2520systems.%250AUltimately%252C%2520this%2520review%2520aims%2520to%2520support%2520researchers%2520and%2520developers%2520in%2520building%250Ainclusive%2520AI%2520tools%2520for%2520underrepresented%2520languages%252C%2520a%2520necessary%2520step%2520toward%250Aempowering%2520LRL%2520speakers%2520and%2520the%2520preservation%2520of%2520linguistic%2520diversity%2520in%2520a%2520world%250Aincreasingly%2520shaped%2520by%2520large-scale%2520language%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Data%20Scarcity%20in%20Generative%20Language%20Modelling%20for%0A%20%20Low-Resource%20Languages%3A%20A%20Systematic%20Review&entry.906535625=Josh%20McGiff%20and%20Nikola%20S.%20Nikolov&entry.1292438233=%20%20Generative%20language%20modelling%20has%20surged%20in%20popularity%20with%20the%20emergence%20of%0Aservices%20such%20as%20ChatGPT%20and%20Google%20Gemini.%20While%20these%20models%20have%0Ademonstrated%20transformative%20potential%20in%20productivity%20and%20communication%2C%20they%0Aoverwhelmingly%20cater%20to%20high-resource%20languages%20like%20English.%20This%20has%0Aamplified%20concerns%20over%20linguistic%20inequality%20in%20natural%20language%20processing%0A%28NLP%29.%20This%20paper%20presents%20the%20first%20systematic%20review%20focused%20specifically%20on%0Astrategies%20to%20address%20data%20scarcity%20in%20generative%20language%20modelling%20for%0Alow-resource%20languages%20%28LRL%29.%20Drawing%20from%2054%20studies%2C%20we%20identify%2C%20categorise%0Aand%20evaluate%20technical%20approaches%2C%20including%20monolingual%20data%20augmentation%2C%0Aback-translation%2C%20multilingual%20training%2C%20and%20prompt%20engineering%2C%20across%0Agenerative%20tasks.%20We%20also%20analyse%20trends%20in%20architecture%20choices%2C%20language%0Afamily%20representation%2C%20and%20evaluation%20methods.%20Our%20findings%20highlight%20a%20strong%0Areliance%20on%20transformer-based%20models%2C%20a%20concentration%20on%20a%20small%20subset%20of%0ALRLs%2C%20and%20a%20lack%20of%20consistent%20evaluation%20across%20studies.%20We%20conclude%20with%0Arecommendations%20for%20extending%20these%20methods%20to%20a%20wider%20range%20of%20LRLs%20and%0Aoutline%20open%20challenges%20in%20building%20equitable%20generative%20language%20systems.%0AUltimately%2C%20this%20review%20aims%20to%20support%20researchers%20and%20developers%20in%20building%0Ainclusive%20AI%20tools%20for%20underrepresented%20languages%2C%20a%20necessary%20step%20toward%0Aempowering%20LRL%20speakers%20and%20the%20preservation%20of%20linguistic%20diversity%20in%20a%20world%0Aincreasingly%20shaped%20by%20large-scale%20language%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04531v1&entry.124074799=Read"},
{"title": "SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin\n  Transformer", "author": "Young-Hu Park and Rae-Hong Park and Hyung-Min Park", "abstract": "  This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model.\n", "link": "http://arxiv.org/abs/2505.04394v1", "date": "2025-05-07", "relevancy": 2.0107, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwinLip%3A%20An%20Efficient%20Visual%20Speech%20Encoder%20for%20Lip%20Reading%20Using%20Swin%0A%20%20Transformer&body=Title%3A%20SwinLip%3A%20An%20Efficient%20Visual%20Speech%20Encoder%20for%20Lip%20Reading%20Using%20Swin%0A%20%20Transformer%0AAuthor%3A%20Young-Hu%20Park%20and%20Rae-Hong%20Park%20and%20Hyung-Min%20Park%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20efficient%20visual%20speech%20encoder%20for%20lip%20reading.%20While%0Amost%20recent%20lip%20reading%20studies%20have%20been%20based%20on%20the%20ResNet%20architecture%20and%0Ahave%20achieved%20significant%20success%2C%20they%20are%20not%20sufficiently%20suitable%20for%0Aefficiently%20capturing%20lip%20reading%20features%20due%20to%20high%20computational%20complexity%0Ain%20modeling%20spatio-temporal%20information.%20Additionally%2C%20using%20a%20complex%20visual%0Amodel%20not%20only%20increases%20the%20complexity%20of%20lip%20reading%20models%20but%20also%20induces%0Adelays%20in%20the%20overall%20network%20for%20multi-modal%20studies%20%28e.g.%2C%20audio-visual%0Aspeech%20recognition%2C%20speech%20enhancement%2C%20and%20speech%20separation%29.%20To%20overcome%20the%0Alimitations%20of%20Convolutional%20Neural%20Network%20%28CNN%29-based%20models%2C%20we%20apply%20the%0Ahierarchical%20structure%20and%20window%20self-attention%20of%20the%20Swin%20Transformer%20to%20lip%0Areading.%20We%20configure%20a%20new%20lightweight%20scale%20of%20the%20Swin%20Transformer%20suitable%0Afor%20processing%20lip%20reading%20data%20and%20present%20the%20SwinLip%20visual%20speech%20encoder%2C%0Awhich%20efficiently%20reduces%20computational%20load%20by%20integrating%20modified%0AConvolution-augmented%20Transformer%20%28Conformer%29%20temporal%20embeddings%20with%0Aconventional%20spatial%20embeddings%20in%20the%20hierarchical%20structure.%20Through%0Aextensive%20experiments%2C%20we%20have%20validated%20that%20our%20SwinLip%20successfully%20improves%0Athe%20performance%20and%20inference%20speed%20of%20the%20lip%20reading%20network%20when%20applied%20to%0Avarious%20backbones%20for%20word%20and%20sentence%20recognition%2C%20reducing%20computational%0Aload.%20In%20particular%2C%20our%20SwinLip%20demonstrated%20robust%20performance%20in%20both%0AEnglish%20LRW%20and%20Mandarin%20LRW-1000%20datasets%20and%20achieved%20state-of-the-art%0Aperformance%20on%20the%20Mandarin%20LRW-1000%20dataset%20with%20less%20computation%20compared%20to%0Athe%20existing%20state-of-the-art%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwinLip%253A%2520An%2520Efficient%2520Visual%2520Speech%2520Encoder%2520for%2520Lip%2520Reading%2520Using%2520Swin%250A%2520%2520Transformer%26entry.906535625%3DYoung-Hu%2520Park%2520and%2520Rae-Hong%2520Park%2520and%2520Hyung-Min%2520Park%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520efficient%2520visual%2520speech%2520encoder%2520for%2520lip%2520reading.%2520While%250Amost%2520recent%2520lip%2520reading%2520studies%2520have%2520been%2520based%2520on%2520the%2520ResNet%2520architecture%2520and%250Ahave%2520achieved%2520significant%2520success%252C%2520they%2520are%2520not%2520sufficiently%2520suitable%2520for%250Aefficiently%2520capturing%2520lip%2520reading%2520features%2520due%2520to%2520high%2520computational%2520complexity%250Ain%2520modeling%2520spatio-temporal%2520information.%2520Additionally%252C%2520using%2520a%2520complex%2520visual%250Amodel%2520not%2520only%2520increases%2520the%2520complexity%2520of%2520lip%2520reading%2520models%2520but%2520also%2520induces%250Adelays%2520in%2520the%2520overall%2520network%2520for%2520multi-modal%2520studies%2520%2528e.g.%252C%2520audio-visual%250Aspeech%2520recognition%252C%2520speech%2520enhancement%252C%2520and%2520speech%2520separation%2529.%2520To%2520overcome%2520the%250Alimitations%2520of%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529-based%2520models%252C%2520we%2520apply%2520the%250Ahierarchical%2520structure%2520and%2520window%2520self-attention%2520of%2520the%2520Swin%2520Transformer%2520to%2520lip%250Areading.%2520We%2520configure%2520a%2520new%2520lightweight%2520scale%2520of%2520the%2520Swin%2520Transformer%2520suitable%250Afor%2520processing%2520lip%2520reading%2520data%2520and%2520present%2520the%2520SwinLip%2520visual%2520speech%2520encoder%252C%250Awhich%2520efficiently%2520reduces%2520computational%2520load%2520by%2520integrating%2520modified%250AConvolution-augmented%2520Transformer%2520%2528Conformer%2529%2520temporal%2520embeddings%2520with%250Aconventional%2520spatial%2520embeddings%2520in%2520the%2520hierarchical%2520structure.%2520Through%250Aextensive%2520experiments%252C%2520we%2520have%2520validated%2520that%2520our%2520SwinLip%2520successfully%2520improves%250Athe%2520performance%2520and%2520inference%2520speed%2520of%2520the%2520lip%2520reading%2520network%2520when%2520applied%2520to%250Avarious%2520backbones%2520for%2520word%2520and%2520sentence%2520recognition%252C%2520reducing%2520computational%250Aload.%2520In%2520particular%252C%2520our%2520SwinLip%2520demonstrated%2520robust%2520performance%2520in%2520both%250AEnglish%2520LRW%2520and%2520Mandarin%2520LRW-1000%2520datasets%2520and%2520achieved%2520state-of-the-art%250Aperformance%2520on%2520the%2520Mandarin%2520LRW-1000%2520dataset%2520with%2520less%2520computation%2520compared%2520to%250Athe%2520existing%2520state-of-the-art%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwinLip%3A%20An%20Efficient%20Visual%20Speech%20Encoder%20for%20Lip%20Reading%20Using%20Swin%0A%20%20Transformer&entry.906535625=Young-Hu%20Park%20and%20Rae-Hong%20Park%20and%20Hyung-Min%20Park&entry.1292438233=%20%20This%20paper%20presents%20an%20efficient%20visual%20speech%20encoder%20for%20lip%20reading.%20While%0Amost%20recent%20lip%20reading%20studies%20have%20been%20based%20on%20the%20ResNet%20architecture%20and%0Ahave%20achieved%20significant%20success%2C%20they%20are%20not%20sufficiently%20suitable%20for%0Aefficiently%20capturing%20lip%20reading%20features%20due%20to%20high%20computational%20complexity%0Ain%20modeling%20spatio-temporal%20information.%20Additionally%2C%20using%20a%20complex%20visual%0Amodel%20not%20only%20increases%20the%20complexity%20of%20lip%20reading%20models%20but%20also%20induces%0Adelays%20in%20the%20overall%20network%20for%20multi-modal%20studies%20%28e.g.%2C%20audio-visual%0Aspeech%20recognition%2C%20speech%20enhancement%2C%20and%20speech%20separation%29.%20To%20overcome%20the%0Alimitations%20of%20Convolutional%20Neural%20Network%20%28CNN%29-based%20models%2C%20we%20apply%20the%0Ahierarchical%20structure%20and%20window%20self-attention%20of%20the%20Swin%20Transformer%20to%20lip%0Areading.%20We%20configure%20a%20new%20lightweight%20scale%20of%20the%20Swin%20Transformer%20suitable%0Afor%20processing%20lip%20reading%20data%20and%20present%20the%20SwinLip%20visual%20speech%20encoder%2C%0Awhich%20efficiently%20reduces%20computational%20load%20by%20integrating%20modified%0AConvolution-augmented%20Transformer%20%28Conformer%29%20temporal%20embeddings%20with%0Aconventional%20spatial%20embeddings%20in%20the%20hierarchical%20structure.%20Through%0Aextensive%20experiments%2C%20we%20have%20validated%20that%20our%20SwinLip%20successfully%20improves%0Athe%20performance%20and%20inference%20speed%20of%20the%20lip%20reading%20network%20when%20applied%20to%0Avarious%20backbones%20for%20word%20and%20sentence%20recognition%2C%20reducing%20computational%0Aload.%20In%20particular%2C%20our%20SwinLip%20demonstrated%20robust%20performance%20in%20both%0AEnglish%20LRW%20and%20Mandarin%20LRW-1000%20datasets%20and%20achieved%20state-of-the-art%0Aperformance%20on%20the%20Mandarin%20LRW-1000%20dataset%20with%20less%20computation%20compared%20to%0Athe%20existing%20state-of-the-art%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04394v1&entry.124074799=Read"},
{"title": "Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video\n  Face Detection and Recognition", "author": "Asma Baobaid and Mahmoud Meribout", "abstract": "  Video face detection and recognition in public places at the edge is required\nin several applications, such as security reinforcement and contactless access\nto authorized venues. This paper aims to maximize the simultaneous usage of\nhardware engines available in edge GPUs nowadays by leveraging the concurrency\nand pipelining of tasks required for face detection and recognition. This also\nincludes the video decoding task, which is required in most face monitoring\napplications as the video streams are usually carried via Gbps Ethernet\nnetwork. This constitutes an improvement over previous works where the tasks\nare usually allocated to a single engine due to the lack of a unified and\nautomated framework that simultaneously explores all hardware engines. In\naddition, previously, the input faces were usually embedded in still images or\nwithin raw video streams that overlook the burst delay caused by the decoding\nstage. The results on real-life video streams suggest that simultaneously using\nall the hardware engines available in the recent NVIDIA edge Orin GPU, higher\nthroughput, and a slight saving of power consumption of around 300 mW,\naccounting for around 5%, have been achieved while satisfying the real-time\nperformance constraint. The performance gets even higher by considering several\nvideo streams simultaneously. Further performance improvement could have been\nobtained if the number of shuffle layers that were created by the tensor RT\nframework for the face recognition task was lower. Thus, the paper suggests\nsome hardware improvements to the existing edge GPU processors to enhance their\nperformance even higher.\n", "link": "http://arxiv.org/abs/2505.04502v1", "date": "2025-05-07", "relevancy": 2.0089, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5079}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5014}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Simultaneous%20Usage%20of%20Edge%20GPU%20Hardware%20Engines%20for%20Video%0A%20%20Face%20Detection%20and%20Recognition&body=Title%3A%20Leveraging%20Simultaneous%20Usage%20of%20Edge%20GPU%20Hardware%20Engines%20for%20Video%0A%20%20Face%20Detection%20and%20Recognition%0AAuthor%3A%20Asma%20Baobaid%20and%20Mahmoud%20Meribout%0AAbstract%3A%20%20%20Video%20face%20detection%20and%20recognition%20in%20public%20places%20at%20the%20edge%20is%20required%0Ain%20several%20applications%2C%20such%20as%20security%20reinforcement%20and%20contactless%20access%0Ato%20authorized%20venues.%20This%20paper%20aims%20to%20maximize%20the%20simultaneous%20usage%20of%0Ahardware%20engines%20available%20in%20edge%20GPUs%20nowadays%20by%20leveraging%20the%20concurrency%0Aand%20pipelining%20of%20tasks%20required%20for%20face%20detection%20and%20recognition.%20This%20also%0Aincludes%20the%20video%20decoding%20task%2C%20which%20is%20required%20in%20most%20face%20monitoring%0Aapplications%20as%20the%20video%20streams%20are%20usually%20carried%20via%20Gbps%20Ethernet%0Anetwork.%20This%20constitutes%20an%20improvement%20over%20previous%20works%20where%20the%20tasks%0Aare%20usually%20allocated%20to%20a%20single%20engine%20due%20to%20the%20lack%20of%20a%20unified%20and%0Aautomated%20framework%20that%20simultaneously%20explores%20all%20hardware%20engines.%20In%0Aaddition%2C%20previously%2C%20the%20input%20faces%20were%20usually%20embedded%20in%20still%20images%20or%0Awithin%20raw%20video%20streams%20that%20overlook%20the%20burst%20delay%20caused%20by%20the%20decoding%0Astage.%20The%20results%20on%20real-life%20video%20streams%20suggest%20that%20simultaneously%20using%0Aall%20the%20hardware%20engines%20available%20in%20the%20recent%20NVIDIA%20edge%20Orin%20GPU%2C%20higher%0Athroughput%2C%20and%20a%20slight%20saving%20of%20power%20consumption%20of%20around%20300%20mW%2C%0Aaccounting%20for%20around%205%25%2C%20have%20been%20achieved%20while%20satisfying%20the%20real-time%0Aperformance%20constraint.%20The%20performance%20gets%20even%20higher%20by%20considering%20several%0Avideo%20streams%20simultaneously.%20Further%20performance%20improvement%20could%20have%20been%0Aobtained%20if%20the%20number%20of%20shuffle%20layers%20that%20were%20created%20by%20the%20tensor%20RT%0Aframework%20for%20the%20face%20recognition%20task%20was%20lower.%20Thus%2C%20the%20paper%20suggests%0Asome%20hardware%20improvements%20to%20the%20existing%20edge%20GPU%20processors%20to%20enhance%20their%0Aperformance%20even%20higher.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Simultaneous%2520Usage%2520of%2520Edge%2520GPU%2520Hardware%2520Engines%2520for%2520Video%250A%2520%2520Face%2520Detection%2520and%2520Recognition%26entry.906535625%3DAsma%2520Baobaid%2520and%2520Mahmoud%2520Meribout%26entry.1292438233%3D%2520%2520Video%2520face%2520detection%2520and%2520recognition%2520in%2520public%2520places%2520at%2520the%2520edge%2520is%2520required%250Ain%2520several%2520applications%252C%2520such%2520as%2520security%2520reinforcement%2520and%2520contactless%2520access%250Ato%2520authorized%2520venues.%2520This%2520paper%2520aims%2520to%2520maximize%2520the%2520simultaneous%2520usage%2520of%250Ahardware%2520engines%2520available%2520in%2520edge%2520GPUs%2520nowadays%2520by%2520leveraging%2520the%2520concurrency%250Aand%2520pipelining%2520of%2520tasks%2520required%2520for%2520face%2520detection%2520and%2520recognition.%2520This%2520also%250Aincludes%2520the%2520video%2520decoding%2520task%252C%2520which%2520is%2520required%2520in%2520most%2520face%2520monitoring%250Aapplications%2520as%2520the%2520video%2520streams%2520are%2520usually%2520carried%2520via%2520Gbps%2520Ethernet%250Anetwork.%2520This%2520constitutes%2520an%2520improvement%2520over%2520previous%2520works%2520where%2520the%2520tasks%250Aare%2520usually%2520allocated%2520to%2520a%2520single%2520engine%2520due%2520to%2520the%2520lack%2520of%2520a%2520unified%2520and%250Aautomated%2520framework%2520that%2520simultaneously%2520explores%2520all%2520hardware%2520engines.%2520In%250Aaddition%252C%2520previously%252C%2520the%2520input%2520faces%2520were%2520usually%2520embedded%2520in%2520still%2520images%2520or%250Awithin%2520raw%2520video%2520streams%2520that%2520overlook%2520the%2520burst%2520delay%2520caused%2520by%2520the%2520decoding%250Astage.%2520The%2520results%2520on%2520real-life%2520video%2520streams%2520suggest%2520that%2520simultaneously%2520using%250Aall%2520the%2520hardware%2520engines%2520available%2520in%2520the%2520recent%2520NVIDIA%2520edge%2520Orin%2520GPU%252C%2520higher%250Athroughput%252C%2520and%2520a%2520slight%2520saving%2520of%2520power%2520consumption%2520of%2520around%2520300%2520mW%252C%250Aaccounting%2520for%2520around%25205%2525%252C%2520have%2520been%2520achieved%2520while%2520satisfying%2520the%2520real-time%250Aperformance%2520constraint.%2520The%2520performance%2520gets%2520even%2520higher%2520by%2520considering%2520several%250Avideo%2520streams%2520simultaneously.%2520Further%2520performance%2520improvement%2520could%2520have%2520been%250Aobtained%2520if%2520the%2520number%2520of%2520shuffle%2520layers%2520that%2520were%2520created%2520by%2520the%2520tensor%2520RT%250Aframework%2520for%2520the%2520face%2520recognition%2520task%2520was%2520lower.%2520Thus%252C%2520the%2520paper%2520suggests%250Asome%2520hardware%2520improvements%2520to%2520the%2520existing%2520edge%2520GPU%2520processors%2520to%2520enhance%2520their%250Aperformance%2520even%2520higher.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Simultaneous%20Usage%20of%20Edge%20GPU%20Hardware%20Engines%20for%20Video%0A%20%20Face%20Detection%20and%20Recognition&entry.906535625=Asma%20Baobaid%20and%20Mahmoud%20Meribout&entry.1292438233=%20%20Video%20face%20detection%20and%20recognition%20in%20public%20places%20at%20the%20edge%20is%20required%0Ain%20several%20applications%2C%20such%20as%20security%20reinforcement%20and%20contactless%20access%0Ato%20authorized%20venues.%20This%20paper%20aims%20to%20maximize%20the%20simultaneous%20usage%20of%0Ahardware%20engines%20available%20in%20edge%20GPUs%20nowadays%20by%20leveraging%20the%20concurrency%0Aand%20pipelining%20of%20tasks%20required%20for%20face%20detection%20and%20recognition.%20This%20also%0Aincludes%20the%20video%20decoding%20task%2C%20which%20is%20required%20in%20most%20face%20monitoring%0Aapplications%20as%20the%20video%20streams%20are%20usually%20carried%20via%20Gbps%20Ethernet%0Anetwork.%20This%20constitutes%20an%20improvement%20over%20previous%20works%20where%20the%20tasks%0Aare%20usually%20allocated%20to%20a%20single%20engine%20due%20to%20the%20lack%20of%20a%20unified%20and%0Aautomated%20framework%20that%20simultaneously%20explores%20all%20hardware%20engines.%20In%0Aaddition%2C%20previously%2C%20the%20input%20faces%20were%20usually%20embedded%20in%20still%20images%20or%0Awithin%20raw%20video%20streams%20that%20overlook%20the%20burst%20delay%20caused%20by%20the%20decoding%0Astage.%20The%20results%20on%20real-life%20video%20streams%20suggest%20that%20simultaneously%20using%0Aall%20the%20hardware%20engines%20available%20in%20the%20recent%20NVIDIA%20edge%20Orin%20GPU%2C%20higher%0Athroughput%2C%20and%20a%20slight%20saving%20of%20power%20consumption%20of%20around%20300%20mW%2C%0Aaccounting%20for%20around%205%25%2C%20have%20been%20achieved%20while%20satisfying%20the%20real-time%0Aperformance%20constraint.%20The%20performance%20gets%20even%20higher%20by%20considering%20several%0Avideo%20streams%20simultaneously.%20Further%20performance%20improvement%20could%20have%20been%0Aobtained%20if%20the%20number%20of%20shuffle%20layers%20that%20were%20created%20by%20the%20tensor%20RT%0Aframework%20for%20the%20face%20recognition%20task%20was%20lower.%20Thus%2C%20the%20paper%20suggests%0Asome%20hardware%20improvements%20to%20the%20existing%20edge%20GPU%20processors%20to%20enhance%20their%0Aperformance%20even%20higher.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04502v1&entry.124074799=Read"},
{"title": "Federated Generalised Variational Inference: A Robust Probabilistic\n  Federated Learning Framework", "author": "Terje Mildner and Oliver Hamelijnck and Paris Giampouras and Theodoros Damoulas", "abstract": "  We introduce FedGVI, a probabilistic Federated Learning (FL) framework that\nis robust to both prior and likelihood misspecification. FedGVI addresses\nlimitations in both frequentist and Bayesian FL by providing unbiased\npredictions under model misspecification, with calibrated uncertainty\nquantification. Our approach generalises previous FL approaches, specifically\nPartitioned Variational Inference (Ashman et al., 2022), by allowing robust and\nconjugate updates, decreasing computational complexity at the clients. We offer\ntheoretical analysis in terms of fixed-point convergence, optimality of the\ncavity distribution, and provable robustness to likelihood misspecification.\nFurther, we empirically demonstrate the effectiveness of FedGVI in terms of\nimproved robustness and predictive performance on multiple synthetic and real\nworld classification data sets.\n", "link": "http://arxiv.org/abs/2502.00846v2", "date": "2025-05-07", "relevancy": 2.0057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5088}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Generalised%20Variational%20Inference%3A%20A%20Robust%20Probabilistic%0A%20%20Federated%20Learning%20Framework&body=Title%3A%20Federated%20Generalised%20Variational%20Inference%3A%20A%20Robust%20Probabilistic%0A%20%20Federated%20Learning%20Framework%0AAuthor%3A%20Terje%20Mildner%20and%20Oliver%20Hamelijnck%20and%20Paris%20Giampouras%20and%20Theodoros%20Damoulas%0AAbstract%3A%20%20%20We%20introduce%20FedGVI%2C%20a%20probabilistic%20Federated%20Learning%20%28FL%29%20framework%20that%0Ais%20robust%20to%20both%20prior%20and%20likelihood%20misspecification.%20FedGVI%20addresses%0Alimitations%20in%20both%20frequentist%20and%20Bayesian%20FL%20by%20providing%20unbiased%0Apredictions%20under%20model%20misspecification%2C%20with%20calibrated%20uncertainty%0Aquantification.%20Our%20approach%20generalises%20previous%20FL%20approaches%2C%20specifically%0APartitioned%20Variational%20Inference%20%28Ashman%20et%20al.%2C%202022%29%2C%20by%20allowing%20robust%20and%0Aconjugate%20updates%2C%20decreasing%20computational%20complexity%20at%20the%20clients.%20We%20offer%0Atheoretical%20analysis%20in%20terms%20of%20fixed-point%20convergence%2C%20optimality%20of%20the%0Acavity%20distribution%2C%20and%20provable%20robustness%20to%20likelihood%20misspecification.%0AFurther%2C%20we%20empirically%20demonstrate%20the%20effectiveness%20of%20FedGVI%20in%20terms%20of%0Aimproved%20robustness%20and%20predictive%20performance%20on%20multiple%20synthetic%20and%20real%0Aworld%20classification%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.00846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Generalised%2520Variational%2520Inference%253A%2520A%2520Robust%2520Probabilistic%250A%2520%2520Federated%2520Learning%2520Framework%26entry.906535625%3DTerje%2520Mildner%2520and%2520Oliver%2520Hamelijnck%2520and%2520Paris%2520Giampouras%2520and%2520Theodoros%2520Damoulas%26entry.1292438233%3D%2520%2520We%2520introduce%2520FedGVI%252C%2520a%2520probabilistic%2520Federated%2520Learning%2520%2528FL%2529%2520framework%2520that%250Ais%2520robust%2520to%2520both%2520prior%2520and%2520likelihood%2520misspecification.%2520FedGVI%2520addresses%250Alimitations%2520in%2520both%2520frequentist%2520and%2520Bayesian%2520FL%2520by%2520providing%2520unbiased%250Apredictions%2520under%2520model%2520misspecification%252C%2520with%2520calibrated%2520uncertainty%250Aquantification.%2520Our%2520approach%2520generalises%2520previous%2520FL%2520approaches%252C%2520specifically%250APartitioned%2520Variational%2520Inference%2520%2528Ashman%2520et%2520al.%252C%25202022%2529%252C%2520by%2520allowing%2520robust%2520and%250Aconjugate%2520updates%252C%2520decreasing%2520computational%2520complexity%2520at%2520the%2520clients.%2520We%2520offer%250Atheoretical%2520analysis%2520in%2520terms%2520of%2520fixed-point%2520convergence%252C%2520optimality%2520of%2520the%250Acavity%2520distribution%252C%2520and%2520provable%2520robustness%2520to%2520likelihood%2520misspecification.%250AFurther%252C%2520we%2520empirically%2520demonstrate%2520the%2520effectiveness%2520of%2520FedGVI%2520in%2520terms%2520of%250Aimproved%2520robustness%2520and%2520predictive%2520performance%2520on%2520multiple%2520synthetic%2520and%2520real%250Aworld%2520classification%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.00846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Generalised%20Variational%20Inference%3A%20A%20Robust%20Probabilistic%0A%20%20Federated%20Learning%20Framework&entry.906535625=Terje%20Mildner%20and%20Oliver%20Hamelijnck%20and%20Paris%20Giampouras%20and%20Theodoros%20Damoulas&entry.1292438233=%20%20We%20introduce%20FedGVI%2C%20a%20probabilistic%20Federated%20Learning%20%28FL%29%20framework%20that%0Ais%20robust%20to%20both%20prior%20and%20likelihood%20misspecification.%20FedGVI%20addresses%0Alimitations%20in%20both%20frequentist%20and%20Bayesian%20FL%20by%20providing%20unbiased%0Apredictions%20under%20model%20misspecification%2C%20with%20calibrated%20uncertainty%0Aquantification.%20Our%20approach%20generalises%20previous%20FL%20approaches%2C%20specifically%0APartitioned%20Variational%20Inference%20%28Ashman%20et%20al.%2C%202022%29%2C%20by%20allowing%20robust%20and%0Aconjugate%20updates%2C%20decreasing%20computational%20complexity%20at%20the%20clients.%20We%20offer%0Atheoretical%20analysis%20in%20terms%20of%20fixed-point%20convergence%2C%20optimality%20of%20the%0Acavity%20distribution%2C%20and%20provable%20robustness%20to%20likelihood%20misspecification.%0AFurther%2C%20we%20empirically%20demonstrate%20the%20effectiveness%20of%20FedGVI%20in%20terms%20of%0Aimproved%20robustness%20and%20predictive%20performance%20on%20multiple%20synthetic%20and%20real%0Aworld%20classification%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.00846v2&entry.124074799=Read"},
{"title": "Information-Geometric Barycenters for Bayesian Federated Learning", "author": "Nour Jamoussi and Giuseppe Serra and Photios A. Stavrou and Marios Kountouris", "abstract": "  Federated learning (FL) is a widely used and impactful distributed\noptimization framework that achieves consensus through averaging locally\ntrained models. While effective, this approach may not align well with Bayesian\ninference, where the model space has the structure of a distribution space.\nTaking an information-geometric perspective, we reinterpret FL aggregation as\nthe problem of finding the barycenter of local posteriors using a prespecified\ndivergence metric, minimizing the average discrepancy across clients. This\nperspective provides a unifying framework that generalizes many existing\nmethods and offers crisp insights into their theoretical underpinnings. We then\npropose BA-BFL, an algorithm that retains the convergence properties of\nFederated Averaging in non-convex settings. In non-independent and identically\ndistributed scenarios, we conduct extensive comparisons with statistical\naggregation techniques, showing that BA-BFL achieves performance comparable to\nstate-of-the-art methods while offering a geometric interpretation of the\naggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep\nLearning, exploring the impact of Bayesian layers on uncertainty quantification\nand model calibration.\n", "link": "http://arxiv.org/abs/2412.11646v2", "date": "2025-05-07", "relevancy": 2.0033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Information-Geometric%20Barycenters%20for%20Bayesian%20Federated%20Learning&body=Title%3A%20Information-Geometric%20Barycenters%20for%20Bayesian%20Federated%20Learning%0AAuthor%3A%20Nour%20Jamoussi%20and%20Giuseppe%20Serra%20and%20Photios%20A.%20Stavrou%20and%20Marios%20Kountouris%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20widely%20used%20and%20impactful%20distributed%0Aoptimization%20framework%20that%20achieves%20consensus%20through%20averaging%20locally%0Atrained%20models.%20While%20effective%2C%20this%20approach%20may%20not%20align%20well%20with%20Bayesian%0Ainference%2C%20where%20the%20model%20space%20has%20the%20structure%20of%20a%20distribution%20space.%0ATaking%20an%20information-geometric%20perspective%2C%20we%20reinterpret%20FL%20aggregation%20as%0Athe%20problem%20of%20finding%20the%20barycenter%20of%20local%20posteriors%20using%20a%20prespecified%0Adivergence%20metric%2C%20minimizing%20the%20average%20discrepancy%20across%20clients.%20This%0Aperspective%20provides%20a%20unifying%20framework%20that%20generalizes%20many%20existing%0Amethods%20and%20offers%20crisp%20insights%20into%20their%20theoretical%20underpinnings.%20We%20then%0Apropose%20BA-BFL%2C%20an%20algorithm%20that%20retains%20the%20convergence%20properties%20of%0AFederated%20Averaging%20in%20non-convex%20settings.%20In%20non-independent%20and%20identically%0Adistributed%20scenarios%2C%20we%20conduct%20extensive%20comparisons%20with%20statistical%0Aaggregation%20techniques%2C%20showing%20that%20BA-BFL%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20methods%20while%20offering%20a%20geometric%20interpretation%20of%20the%0Aaggregation%20phase.%20Additionally%2C%20we%20extend%20our%20analysis%20to%20Hybrid%20Bayesian%20Deep%0ALearning%2C%20exploring%20the%20impact%20of%20Bayesian%20layers%20on%20uncertainty%20quantification%0Aand%20model%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformation-Geometric%2520Barycenters%2520for%2520Bayesian%2520Federated%2520Learning%26entry.906535625%3DNour%2520Jamoussi%2520and%2520Giuseppe%2520Serra%2520and%2520Photios%2520A.%2520Stavrou%2520and%2520Marios%2520Kountouris%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520is%2520a%2520widely%2520used%2520and%2520impactful%2520distributed%250Aoptimization%2520framework%2520that%2520achieves%2520consensus%2520through%2520averaging%2520locally%250Atrained%2520models.%2520While%2520effective%252C%2520this%2520approach%2520may%2520not%2520align%2520well%2520with%2520Bayesian%250Ainference%252C%2520where%2520the%2520model%2520space%2520has%2520the%2520structure%2520of%2520a%2520distribution%2520space.%250ATaking%2520an%2520information-geometric%2520perspective%252C%2520we%2520reinterpret%2520FL%2520aggregation%2520as%250Athe%2520problem%2520of%2520finding%2520the%2520barycenter%2520of%2520local%2520posteriors%2520using%2520a%2520prespecified%250Adivergence%2520metric%252C%2520minimizing%2520the%2520average%2520discrepancy%2520across%2520clients.%2520This%250Aperspective%2520provides%2520a%2520unifying%2520framework%2520that%2520generalizes%2520many%2520existing%250Amethods%2520and%2520offers%2520crisp%2520insights%2520into%2520their%2520theoretical%2520underpinnings.%2520We%2520then%250Apropose%2520BA-BFL%252C%2520an%2520algorithm%2520that%2520retains%2520the%2520convergence%2520properties%2520of%250AFederated%2520Averaging%2520in%2520non-convex%2520settings.%2520In%2520non-independent%2520and%2520identically%250Adistributed%2520scenarios%252C%2520we%2520conduct%2520extensive%2520comparisons%2520with%2520statistical%250Aaggregation%2520techniques%252C%2520showing%2520that%2520BA-BFL%2520achieves%2520performance%2520comparable%2520to%250Astate-of-the-art%2520methods%2520while%2520offering%2520a%2520geometric%2520interpretation%2520of%2520the%250Aaggregation%2520phase.%2520Additionally%252C%2520we%2520extend%2520our%2520analysis%2520to%2520Hybrid%2520Bayesian%2520Deep%250ALearning%252C%2520exploring%2520the%2520impact%2520of%2520Bayesian%2520layers%2520on%2520uncertainty%2520quantification%250Aand%2520model%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information-Geometric%20Barycenters%20for%20Bayesian%20Federated%20Learning&entry.906535625=Nour%20Jamoussi%20and%20Giuseppe%20Serra%20and%20Photios%20A.%20Stavrou%20and%20Marios%20Kountouris&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20widely%20used%20and%20impactful%20distributed%0Aoptimization%20framework%20that%20achieves%20consensus%20through%20averaging%20locally%0Atrained%20models.%20While%20effective%2C%20this%20approach%20may%20not%20align%20well%20with%20Bayesian%0Ainference%2C%20where%20the%20model%20space%20has%20the%20structure%20of%20a%20distribution%20space.%0ATaking%20an%20information-geometric%20perspective%2C%20we%20reinterpret%20FL%20aggregation%20as%0Athe%20problem%20of%20finding%20the%20barycenter%20of%20local%20posteriors%20using%20a%20prespecified%0Adivergence%20metric%2C%20minimizing%20the%20average%20discrepancy%20across%20clients.%20This%0Aperspective%20provides%20a%20unifying%20framework%20that%20generalizes%20many%20existing%0Amethods%20and%20offers%20crisp%20insights%20into%20their%20theoretical%20underpinnings.%20We%20then%0Apropose%20BA-BFL%2C%20an%20algorithm%20that%20retains%20the%20convergence%20properties%20of%0AFederated%20Averaging%20in%20non-convex%20settings.%20In%20non-independent%20and%20identically%0Adistributed%20scenarios%2C%20we%20conduct%20extensive%20comparisons%20with%20statistical%0Aaggregation%20techniques%2C%20showing%20that%20BA-BFL%20achieves%20performance%20comparable%20to%0Astate-of-the-art%20methods%20while%20offering%20a%20geometric%20interpretation%20of%20the%0Aaggregation%20phase.%20Additionally%2C%20we%20extend%20our%20analysis%20to%20Hybrid%20Bayesian%20Deep%0ALearning%2C%20exploring%20the%20impact%20of%20Bayesian%20layers%20on%20uncertainty%20quantification%0Aand%20model%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11646v2&entry.124074799=Read"},
{"title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation", "author": "Teng Hu and Zhentao Yu and Zhengguang Zhou and Sen Liang and Yuan Zhou and Qin Lin and Qinglin Lu", "abstract": "  Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.\n", "link": "http://arxiv.org/abs/2505.04512v1", "date": "2025-05-07", "relevancy": 1.9974, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6952}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6711}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HunyuanCustom%3A%20A%20Multimodal-Driven%20Architecture%20for%20Customized%20Video%0A%20%20Generation&body=Title%3A%20HunyuanCustom%3A%20A%20Multimodal-Driven%20Architecture%20for%20Customized%20Video%0A%20%20Generation%0AAuthor%3A%20Teng%20Hu%20and%20Zhentao%20Yu%20and%20Zhengguang%20Zhou%20and%20Sen%20Liang%20and%20Yuan%20Zhou%20and%20Qin%20Lin%20and%20Qinglin%20Lu%0AAbstract%3A%20%20%20Customized%20video%20generation%20aims%20to%20produce%20videos%20featuring%20specific%0Asubjects%20under%20flexible%20user-defined%20conditions%2C%20yet%20existing%20methods%20often%0Astruggle%20with%20identity%20consistency%20and%20limited%20input%20modalities.%20In%20this%20paper%2C%0Awe%20propose%20HunyuanCustom%2C%20a%20multi-modal%20customized%20video%20generation%20framework%0Athat%20emphasizes%20subject%20consistency%20while%20supporting%20image%2C%20audio%2C%20video%2C%20and%0Atext%20conditions.%20Built%20upon%20HunyuanVideo%2C%20our%20model%20first%20addresses%20the%0Aimage-text%20conditioned%20generation%20task%20by%20introducing%20a%20text-image%20fusion%0Amodule%20based%20on%20LLaVA%20for%20enhanced%20multi-modal%20understanding%2C%20along%20with%20an%0Aimage%20ID%20enhancement%20module%20that%20leverages%20temporal%20concatenation%20to%20reinforce%0Aidentity%20features%20across%20frames.%20To%20enable%20audio-%20and%20video-conditioned%0Ageneration%2C%20we%20further%20propose%20modality-specific%20condition%20injection%0Amechanisms%3A%20an%20AudioNet%20module%20that%20achieves%20hierarchical%20alignment%20via%20spatial%0Across-attention%2C%20and%20a%20video-driven%20injection%20module%20that%20integrates%0Alatent-compressed%20conditional%20video%20through%20a%20patchify-based%20feature-alignment%0Anetwork.%20Extensive%20experiments%20on%20single-%20and%20multi-subject%20scenarios%0Ademonstrate%20that%20HunyuanCustom%20significantly%20outperforms%20state-of-the-art%20open-%0Aand%20closed-source%20methods%20in%20terms%20of%20ID%20consistency%2C%20realism%2C%20and%20text-video%0Aalignment.%20Moreover%2C%20we%20validate%20its%20robustness%20across%20downstream%20tasks%2C%0Aincluding%20audio%20and%20video-driven%20customized%20video%20generation.%20Our%20results%0Ahighlight%20the%20effectiveness%20of%20multi-modal%20conditioning%20and%20identity-preserving%0Astrategies%20in%20advancing%20controllable%20video%20generation.%20All%20the%20code%20and%20models%0Aare%20available%20at%20https%3A//hunyuancustom.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuanCustom%253A%2520A%2520Multimodal-Driven%2520Architecture%2520for%2520Customized%2520Video%250A%2520%2520Generation%26entry.906535625%3DTeng%2520Hu%2520and%2520Zhentao%2520Yu%2520and%2520Zhengguang%2520Zhou%2520and%2520Sen%2520Liang%2520and%2520Yuan%2520Zhou%2520and%2520Qin%2520Lin%2520and%2520Qinglin%2520Lu%26entry.1292438233%3D%2520%2520Customized%2520video%2520generation%2520aims%2520to%2520produce%2520videos%2520featuring%2520specific%250Asubjects%2520under%2520flexible%2520user-defined%2520conditions%252C%2520yet%2520existing%2520methods%2520often%250Astruggle%2520with%2520identity%2520consistency%2520and%2520limited%2520input%2520modalities.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520HunyuanCustom%252C%2520a%2520multi-modal%2520customized%2520video%2520generation%2520framework%250Athat%2520emphasizes%2520subject%2520consistency%2520while%2520supporting%2520image%252C%2520audio%252C%2520video%252C%2520and%250Atext%2520conditions.%2520Built%2520upon%2520HunyuanVideo%252C%2520our%2520model%2520first%2520addresses%2520the%250Aimage-text%2520conditioned%2520generation%2520task%2520by%2520introducing%2520a%2520text-image%2520fusion%250Amodule%2520based%2520on%2520LLaVA%2520for%2520enhanced%2520multi-modal%2520understanding%252C%2520along%2520with%2520an%250Aimage%2520ID%2520enhancement%2520module%2520that%2520leverages%2520temporal%2520concatenation%2520to%2520reinforce%250Aidentity%2520features%2520across%2520frames.%2520To%2520enable%2520audio-%2520and%2520video-conditioned%250Ageneration%252C%2520we%2520further%2520propose%2520modality-specific%2520condition%2520injection%250Amechanisms%253A%2520an%2520AudioNet%2520module%2520that%2520achieves%2520hierarchical%2520alignment%2520via%2520spatial%250Across-attention%252C%2520and%2520a%2520video-driven%2520injection%2520module%2520that%2520integrates%250Alatent-compressed%2520conditional%2520video%2520through%2520a%2520patchify-based%2520feature-alignment%250Anetwork.%2520Extensive%2520experiments%2520on%2520single-%2520and%2520multi-subject%2520scenarios%250Ademonstrate%2520that%2520HunyuanCustom%2520significantly%2520outperforms%2520state-of-the-art%2520open-%250Aand%2520closed-source%2520methods%2520in%2520terms%2520of%2520ID%2520consistency%252C%2520realism%252C%2520and%2520text-video%250Aalignment.%2520Moreover%252C%2520we%2520validate%2520its%2520robustness%2520across%2520downstream%2520tasks%252C%250Aincluding%2520audio%2520and%2520video-driven%2520customized%2520video%2520generation.%2520Our%2520results%250Ahighlight%2520the%2520effectiveness%2520of%2520multi-modal%2520conditioning%2520and%2520identity-preserving%250Astrategies%2520in%2520advancing%2520controllable%2520video%2520generation.%2520All%2520the%2520code%2520and%2520models%250Aare%2520available%2520at%2520https%253A//hunyuancustom.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HunyuanCustom%3A%20A%20Multimodal-Driven%20Architecture%20for%20Customized%20Video%0A%20%20Generation&entry.906535625=Teng%20Hu%20and%20Zhentao%20Yu%20and%20Zhengguang%20Zhou%20and%20Sen%20Liang%20and%20Yuan%20Zhou%20and%20Qin%20Lin%20and%20Qinglin%20Lu&entry.1292438233=%20%20Customized%20video%20generation%20aims%20to%20produce%20videos%20featuring%20specific%0Asubjects%20under%20flexible%20user-defined%20conditions%2C%20yet%20existing%20methods%20often%0Astruggle%20with%20identity%20consistency%20and%20limited%20input%20modalities.%20In%20this%20paper%2C%0Awe%20propose%20HunyuanCustom%2C%20a%20multi-modal%20customized%20video%20generation%20framework%0Athat%20emphasizes%20subject%20consistency%20while%20supporting%20image%2C%20audio%2C%20video%2C%20and%0Atext%20conditions.%20Built%20upon%20HunyuanVideo%2C%20our%20model%20first%20addresses%20the%0Aimage-text%20conditioned%20generation%20task%20by%20introducing%20a%20text-image%20fusion%0Amodule%20based%20on%20LLaVA%20for%20enhanced%20multi-modal%20understanding%2C%20along%20with%20an%0Aimage%20ID%20enhancement%20module%20that%20leverages%20temporal%20concatenation%20to%20reinforce%0Aidentity%20features%20across%20frames.%20To%20enable%20audio-%20and%20video-conditioned%0Ageneration%2C%20we%20further%20propose%20modality-specific%20condition%20injection%0Amechanisms%3A%20an%20AudioNet%20module%20that%20achieves%20hierarchical%20alignment%20via%20spatial%0Across-attention%2C%20and%20a%20video-driven%20injection%20module%20that%20integrates%0Alatent-compressed%20conditional%20video%20through%20a%20patchify-based%20feature-alignment%0Anetwork.%20Extensive%20experiments%20on%20single-%20and%20multi-subject%20scenarios%0Ademonstrate%20that%20HunyuanCustom%20significantly%20outperforms%20state-of-the-art%20open-%0Aand%20closed-source%20methods%20in%20terms%20of%20ID%20consistency%2C%20realism%2C%20and%20text-video%0Aalignment.%20Moreover%2C%20we%20validate%20its%20robustness%20across%20downstream%20tasks%2C%0Aincluding%20audio%20and%20video-driven%20customized%20video%20generation.%20Our%20results%0Ahighlight%20the%20effectiveness%20of%20multi-modal%20conditioning%20and%20identity-preserving%0Astrategies%20in%20advancing%20controllable%20video%20generation.%20All%20the%20code%20and%20models%0Aare%20available%20at%20https%3A//hunyuancustom.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04512v1&entry.124074799=Read"},
{"title": "Flow Models for Unbounded and Geometry-Aware Distributional\n  Reinforcement Learning", "author": "Simo Alami C. and Rim Kaddah and Jesse Read and Marie-Paule Cani", "abstract": "  We introduce a new architecture for Distributional Reinforcement Learning\n(DistRL) that models return distributions using normalizing flows. This\napproach enables flexible, unbounded support for return distributions, in\ncontrast to categorical approaches like C51 that rely on fixed or bounded\nrepresentations. It also offers richer modeling capacity to capture\nmulti-modality, skewness, and tail behavior than quantile based approaches. Our\nmethod is significantly more parameter-efficient than categorical approaches.\nStandard metrics used to train existing models like KL divergence or\nWasserstein distance either are scale insensitive or have biased sample\ngradients, especially when return supports do not overlap. To address this, we\npropose a novel surrogate for the Cram\\`er distance, that is geometry-aware and\ncomputable directly from the return distribution's PDF, avoiding the costly CDF\ncomputation. We test our model on the ATARI-5 sub-benchmark and show that our\napproach outperforms PDF based models while remaining competitive with quantile\nbased methods.\n", "link": "http://arxiv.org/abs/2505.04310v1", "date": "2025-05-07", "relevancy": 1.9873, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6074}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20Models%20for%20Unbounded%20and%20Geometry-Aware%20Distributional%0A%20%20Reinforcement%20Learning&body=Title%3A%20Flow%20Models%20for%20Unbounded%20and%20Geometry-Aware%20Distributional%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Simo%20Alami%20C.%20and%20Rim%20Kaddah%20and%20Jesse%20Read%20and%20Marie-Paule%20Cani%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20architecture%20for%20Distributional%20Reinforcement%20Learning%0A%28DistRL%29%20that%20models%20return%20distributions%20using%20normalizing%20flows.%20This%0Aapproach%20enables%20flexible%2C%20unbounded%20support%20for%20return%20distributions%2C%20in%0Acontrast%20to%20categorical%20approaches%20like%20C51%20that%20rely%20on%20fixed%20or%20bounded%0Arepresentations.%20It%20also%20offers%20richer%20modeling%20capacity%20to%20capture%0Amulti-modality%2C%20skewness%2C%20and%20tail%20behavior%20than%20quantile%20based%20approaches.%20Our%0Amethod%20is%20significantly%20more%20parameter-efficient%20than%20categorical%20approaches.%0AStandard%20metrics%20used%20to%20train%20existing%20models%20like%20KL%20divergence%20or%0AWasserstein%20distance%20either%20are%20scale%20insensitive%20or%20have%20biased%20sample%0Agradients%2C%20especially%20when%20return%20supports%20do%20not%20overlap.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20surrogate%20for%20the%20Cram%5C%60er%20distance%2C%20that%20is%20geometry-aware%20and%0Acomputable%20directly%20from%20the%20return%20distribution%27s%20PDF%2C%20avoiding%20the%20costly%20CDF%0Acomputation.%20We%20test%20our%20model%20on%20the%20ATARI-5%20sub-benchmark%20and%20show%20that%20our%0Aapproach%20outperforms%20PDF%20based%20models%20while%20remaining%20competitive%20with%20quantile%0Abased%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520Models%2520for%2520Unbounded%2520and%2520Geometry-Aware%2520Distributional%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DSimo%2520Alami%2520C.%2520and%2520Rim%2520Kaddah%2520and%2520Jesse%2520Read%2520and%2520Marie-Paule%2520Cani%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520architecture%2520for%2520Distributional%2520Reinforcement%2520Learning%250A%2528DistRL%2529%2520that%2520models%2520return%2520distributions%2520using%2520normalizing%2520flows.%2520This%250Aapproach%2520enables%2520flexible%252C%2520unbounded%2520support%2520for%2520return%2520distributions%252C%2520in%250Acontrast%2520to%2520categorical%2520approaches%2520like%2520C51%2520that%2520rely%2520on%2520fixed%2520or%2520bounded%250Arepresentations.%2520It%2520also%2520offers%2520richer%2520modeling%2520capacity%2520to%2520capture%250Amulti-modality%252C%2520skewness%252C%2520and%2520tail%2520behavior%2520than%2520quantile%2520based%2520approaches.%2520Our%250Amethod%2520is%2520significantly%2520more%2520parameter-efficient%2520than%2520categorical%2520approaches.%250AStandard%2520metrics%2520used%2520to%2520train%2520existing%2520models%2520like%2520KL%2520divergence%2520or%250AWasserstein%2520distance%2520either%2520are%2520scale%2520insensitive%2520or%2520have%2520biased%2520sample%250Agradients%252C%2520especially%2520when%2520return%2520supports%2520do%2520not%2520overlap.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520novel%2520surrogate%2520for%2520the%2520Cram%255C%2560er%2520distance%252C%2520that%2520is%2520geometry-aware%2520and%250Acomputable%2520directly%2520from%2520the%2520return%2520distribution%2527s%2520PDF%252C%2520avoiding%2520the%2520costly%2520CDF%250Acomputation.%2520We%2520test%2520our%2520model%2520on%2520the%2520ATARI-5%2520sub-benchmark%2520and%2520show%2520that%2520our%250Aapproach%2520outperforms%2520PDF%2520based%2520models%2520while%2520remaining%2520competitive%2520with%2520quantile%250Abased%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20Models%20for%20Unbounded%20and%20Geometry-Aware%20Distributional%0A%20%20Reinforcement%20Learning&entry.906535625=Simo%20Alami%20C.%20and%20Rim%20Kaddah%20and%20Jesse%20Read%20and%20Marie-Paule%20Cani&entry.1292438233=%20%20We%20introduce%20a%20new%20architecture%20for%20Distributional%20Reinforcement%20Learning%0A%28DistRL%29%20that%20models%20return%20distributions%20using%20normalizing%20flows.%20This%0Aapproach%20enables%20flexible%2C%20unbounded%20support%20for%20return%20distributions%2C%20in%0Acontrast%20to%20categorical%20approaches%20like%20C51%20that%20rely%20on%20fixed%20or%20bounded%0Arepresentations.%20It%20also%20offers%20richer%20modeling%20capacity%20to%20capture%0Amulti-modality%2C%20skewness%2C%20and%20tail%20behavior%20than%20quantile%20based%20approaches.%20Our%0Amethod%20is%20significantly%20more%20parameter-efficient%20than%20categorical%20approaches.%0AStandard%20metrics%20used%20to%20train%20existing%20models%20like%20KL%20divergence%20or%0AWasserstein%20distance%20either%20are%20scale%20insensitive%20or%20have%20biased%20sample%0Agradients%2C%20especially%20when%20return%20supports%20do%20not%20overlap.%20To%20address%20this%2C%20we%0Apropose%20a%20novel%20surrogate%20for%20the%20Cram%5C%60er%20distance%2C%20that%20is%20geometry-aware%20and%0Acomputable%20directly%20from%20the%20return%20distribution%27s%20PDF%2C%20avoiding%20the%20costly%20CDF%0Acomputation.%20We%20test%20our%20model%20on%20the%20ATARI-5%20sub-benchmark%20and%20show%20that%20our%0Aapproach%20outperforms%20PDF%20based%20models%20while%20remaining%20competitive%20with%20quantile%0Abased%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04310v1&entry.124074799=Read"},
{"title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play\n  Reinforcement Learning", "author": "Ruize Zhang and Sirui Xiang and Zelai Xu and Feng Gao and Shilong Ji and Wenhao Tang and Wenbo Ding and Chao Yu and Yu Wang", "abstract": "  In this paper, we tackle the problem of learning to play 3v3 multi-drone\nvolleyball, a new embodied competitive task that requires both high-level\nstrategic coordination and low-level agile control. The task is turn-based,\nmulti-agent, and physically grounded, posing significant challenges due to its\nlong-horizon dependencies, tight inter-agent coupling, and the underactuated\ndynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play\n(HCSP), a hierarchical reinforcement learning framework that separates\ncentralized high-level strategic decision-making from decentralized low-level\nmotion control. We design a three-stage population-based training pipeline to\nenable both strategy and skill to emerge from scratch without expert\ndemonstrations: (I) training diverse low-level skills, (II) learning high-level\nstrategy via self-play with fixed low-level controllers, and (III) joint\nfine-tuning through co-self-play. Experiments show that HCSP achieves superior\nperformance, outperforming non-hierarchical self-play and rule-based\nhierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate\nagainst the two-stage variant. Moreover, co-self-play leads to emergent team\nbehaviors such as role switching and coordinated formations, demonstrating the\neffectiveness of our hierarchical design and training scheme.\n", "link": "http://arxiv.org/abs/2505.04317v1", "date": "2025-05-07", "relevancy": 1.9854, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5151}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4937}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mastering%20Multi-Drone%20Volleyball%20through%20Hierarchical%20Co-Self-Play%0A%20%20Reinforcement%20Learning&body=Title%3A%20Mastering%20Multi-Drone%20Volleyball%20through%20Hierarchical%20Co-Self-Play%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Ruize%20Zhang%20and%20Sirui%20Xiang%20and%20Zelai%20Xu%20and%20Feng%20Gao%20and%20Shilong%20Ji%20and%20Wenhao%20Tang%20and%20Wenbo%20Ding%20and%20Chao%20Yu%20and%20Yu%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20tackle%20the%20problem%20of%20learning%20to%20play%203v3%20multi-drone%0Avolleyball%2C%20a%20new%20embodied%20competitive%20task%20that%20requires%20both%20high-level%0Astrategic%20coordination%20and%20low-level%20agile%20control.%20The%20task%20is%20turn-based%2C%0Amulti-agent%2C%20and%20physically%20grounded%2C%20posing%20significant%20challenges%20due%20to%20its%0Along-horizon%20dependencies%2C%20tight%20inter-agent%20coupling%2C%20and%20the%20underactuated%0Adynamics%20of%20quadrotors.%20To%20address%20this%2C%20we%20propose%20Hierarchical%20Co-Self-Play%0A%28HCSP%29%2C%20a%20hierarchical%20reinforcement%20learning%20framework%20that%20separates%0Acentralized%20high-level%20strategic%20decision-making%20from%20decentralized%20low-level%0Amotion%20control.%20We%20design%20a%20three-stage%20population-based%20training%20pipeline%20to%0Aenable%20both%20strategy%20and%20skill%20to%20emerge%20from%20scratch%20without%20expert%0Ademonstrations%3A%20%28I%29%20training%20diverse%20low-level%20skills%2C%20%28II%29%20learning%20high-level%0Astrategy%20via%20self-play%20with%20fixed%20low-level%20controllers%2C%20and%20%28III%29%20joint%0Afine-tuning%20through%20co-self-play.%20Experiments%20show%20that%20HCSP%20achieves%20superior%0Aperformance%2C%20outperforming%20non-hierarchical%20self-play%20and%20rule-based%0Ahierarchical%20baselines%20with%20an%20average%2082.9%5C%25%20win%20rate%20and%20a%2071.5%5C%25%20win%20rate%0Aagainst%20the%20two-stage%20variant.%20Moreover%2C%20co-self-play%20leads%20to%20emergent%20team%0Abehaviors%20such%20as%20role%20switching%20and%20coordinated%20formations%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20hierarchical%20design%20and%20training%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMastering%2520Multi-Drone%2520Volleyball%2520through%2520Hierarchical%2520Co-Self-Play%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DRuize%2520Zhang%2520and%2520Sirui%2520Xiang%2520and%2520Zelai%2520Xu%2520and%2520Feng%2520Gao%2520and%2520Shilong%2520Ji%2520and%2520Wenhao%2520Tang%2520and%2520Wenbo%2520Ding%2520and%2520Chao%2520Yu%2520and%2520Yu%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520problem%2520of%2520learning%2520to%2520play%25203v3%2520multi-drone%250Avolleyball%252C%2520a%2520new%2520embodied%2520competitive%2520task%2520that%2520requires%2520both%2520high-level%250Astrategic%2520coordination%2520and%2520low-level%2520agile%2520control.%2520The%2520task%2520is%2520turn-based%252C%250Amulti-agent%252C%2520and%2520physically%2520grounded%252C%2520posing%2520significant%2520challenges%2520due%2520to%2520its%250Along-horizon%2520dependencies%252C%2520tight%2520inter-agent%2520coupling%252C%2520and%2520the%2520underactuated%250Adynamics%2520of%2520quadrotors.%2520To%2520address%2520this%252C%2520we%2520propose%2520Hierarchical%2520Co-Self-Play%250A%2528HCSP%2529%252C%2520a%2520hierarchical%2520reinforcement%2520learning%2520framework%2520that%2520separates%250Acentralized%2520high-level%2520strategic%2520decision-making%2520from%2520decentralized%2520low-level%250Amotion%2520control.%2520We%2520design%2520a%2520three-stage%2520population-based%2520training%2520pipeline%2520to%250Aenable%2520both%2520strategy%2520and%2520skill%2520to%2520emerge%2520from%2520scratch%2520without%2520expert%250Ademonstrations%253A%2520%2528I%2529%2520training%2520diverse%2520low-level%2520skills%252C%2520%2528II%2529%2520learning%2520high-level%250Astrategy%2520via%2520self-play%2520with%2520fixed%2520low-level%2520controllers%252C%2520and%2520%2528III%2529%2520joint%250Afine-tuning%2520through%2520co-self-play.%2520Experiments%2520show%2520that%2520HCSP%2520achieves%2520superior%250Aperformance%252C%2520outperforming%2520non-hierarchical%2520self-play%2520and%2520rule-based%250Ahierarchical%2520baselines%2520with%2520an%2520average%252082.9%255C%2525%2520win%2520rate%2520and%2520a%252071.5%255C%2525%2520win%2520rate%250Aagainst%2520the%2520two-stage%2520variant.%2520Moreover%252C%2520co-self-play%2520leads%2520to%2520emergent%2520team%250Abehaviors%2520such%2520as%2520role%2520switching%2520and%2520coordinated%2520formations%252C%2520demonstrating%2520the%250Aeffectiveness%2520of%2520our%2520hierarchical%2520design%2520and%2520training%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mastering%20Multi-Drone%20Volleyball%20through%20Hierarchical%20Co-Self-Play%0A%20%20Reinforcement%20Learning&entry.906535625=Ruize%20Zhang%20and%20Sirui%20Xiang%20and%20Zelai%20Xu%20and%20Feng%20Gao%20and%20Shilong%20Ji%20and%20Wenhao%20Tang%20and%20Wenbo%20Ding%20and%20Chao%20Yu%20and%20Yu%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20tackle%20the%20problem%20of%20learning%20to%20play%203v3%20multi-drone%0Avolleyball%2C%20a%20new%20embodied%20competitive%20task%20that%20requires%20both%20high-level%0Astrategic%20coordination%20and%20low-level%20agile%20control.%20The%20task%20is%20turn-based%2C%0Amulti-agent%2C%20and%20physically%20grounded%2C%20posing%20significant%20challenges%20due%20to%20its%0Along-horizon%20dependencies%2C%20tight%20inter-agent%20coupling%2C%20and%20the%20underactuated%0Adynamics%20of%20quadrotors.%20To%20address%20this%2C%20we%20propose%20Hierarchical%20Co-Self-Play%0A%28HCSP%29%2C%20a%20hierarchical%20reinforcement%20learning%20framework%20that%20separates%0Acentralized%20high-level%20strategic%20decision-making%20from%20decentralized%20low-level%0Amotion%20control.%20We%20design%20a%20three-stage%20population-based%20training%20pipeline%20to%0Aenable%20both%20strategy%20and%20skill%20to%20emerge%20from%20scratch%20without%20expert%0Ademonstrations%3A%20%28I%29%20training%20diverse%20low-level%20skills%2C%20%28II%29%20learning%20high-level%0Astrategy%20via%20self-play%20with%20fixed%20low-level%20controllers%2C%20and%20%28III%29%20joint%0Afine-tuning%20through%20co-self-play.%20Experiments%20show%20that%20HCSP%20achieves%20superior%0Aperformance%2C%20outperforming%20non-hierarchical%20self-play%20and%20rule-based%0Ahierarchical%20baselines%20with%20an%20average%2082.9%5C%25%20win%20rate%20and%20a%2071.5%5C%25%20win%20rate%0Aagainst%20the%20two-stage%20variant.%20Moreover%2C%20co-self-play%20leads%20to%20emergent%20team%0Abehaviors%20such%20as%20role%20switching%20and%20coordinated%20formations%2C%20demonstrating%20the%0Aeffectiveness%20of%20our%20hierarchical%20design%20and%20training%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04317v1&entry.124074799=Read"},
{"title": "A Two-Timescale Primal-Dual Framework for Reinforcement Learning via\n  Online Dual Variable Guidance", "author": "Axel Friedrich Wolter and Tobias Sutter", "abstract": "  We study reinforcement learning by combining recent advances in regularized\nlinear programming formulations with the classical theory of stochastic\napproximation. Motivated by the challenge of designing algorithms that leverage\noff-policy data while maintaining on-policy exploration, we propose PGDA-RL, a\nnovel primal-dual Projected Gradient Descent-Ascent algorithm for solving\nregularized Markov Decision Processes (MDPs). PGDA-RL integrates experience\nreplay-based gradient estimation with a two-timescale decomposition of the\nunderlying nested optimization problem. The algorithm operates asynchronously,\ninteracts with the environment through a single trajectory of correlated data,\nand updates its policy online in response to the dual variable associated with\nthe occupation measure of the underlying MDP. We prove that PGDA-RL converges\nalmost surely to the optimal value function and policy of the regularized MDP.\nOur convergence analysis relies on tools from stochastic approximation theory\nand holds under weaker assumptions than those required by existing primal-dual\nRL approaches, notably removing the need for a simulator or a fixed behavioral\npolicy.\n", "link": "http://arxiv.org/abs/2505.04494v1", "date": "2025-05-07", "relevancy": 1.9715, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4904}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Two-Timescale%20Primal-Dual%20Framework%20for%20Reinforcement%20Learning%20via%0A%20%20Online%20Dual%20Variable%20Guidance&body=Title%3A%20A%20Two-Timescale%20Primal-Dual%20Framework%20for%20Reinforcement%20Learning%20via%0A%20%20Online%20Dual%20Variable%20Guidance%0AAuthor%3A%20Axel%20Friedrich%20Wolter%20and%20Tobias%20Sutter%0AAbstract%3A%20%20%20We%20study%20reinforcement%20learning%20by%20combining%20recent%20advances%20in%20regularized%0Alinear%20programming%20formulations%20with%20the%20classical%20theory%20of%20stochastic%0Aapproximation.%20Motivated%20by%20the%20challenge%20of%20designing%20algorithms%20that%20leverage%0Aoff-policy%20data%20while%20maintaining%20on-policy%20exploration%2C%20we%20propose%20PGDA-RL%2C%20a%0Anovel%20primal-dual%20Projected%20Gradient%20Descent-Ascent%20algorithm%20for%20solving%0Aregularized%20Markov%20Decision%20Processes%20%28MDPs%29.%20PGDA-RL%20integrates%20experience%0Areplay-based%20gradient%20estimation%20with%20a%20two-timescale%20decomposition%20of%20the%0Aunderlying%20nested%20optimization%20problem.%20The%20algorithm%20operates%20asynchronously%2C%0Ainteracts%20with%20the%20environment%20through%20a%20single%20trajectory%20of%20correlated%20data%2C%0Aand%20updates%20its%20policy%20online%20in%20response%20to%20the%20dual%20variable%20associated%20with%0Athe%20occupation%20measure%20of%20the%20underlying%20MDP.%20We%20prove%20that%20PGDA-RL%20converges%0Aalmost%20surely%20to%20the%20optimal%20value%20function%20and%20policy%20of%20the%20regularized%20MDP.%0AOur%20convergence%20analysis%20relies%20on%20tools%20from%20stochastic%20approximation%20theory%0Aand%20holds%20under%20weaker%20assumptions%20than%20those%20required%20by%20existing%20primal-dual%0ARL%20approaches%2C%20notably%20removing%20the%20need%20for%20a%20simulator%20or%20a%20fixed%20behavioral%0Apolicy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Two-Timescale%2520Primal-Dual%2520Framework%2520for%2520Reinforcement%2520Learning%2520via%250A%2520%2520Online%2520Dual%2520Variable%2520Guidance%26entry.906535625%3DAxel%2520Friedrich%2520Wolter%2520and%2520Tobias%2520Sutter%26entry.1292438233%3D%2520%2520We%2520study%2520reinforcement%2520learning%2520by%2520combining%2520recent%2520advances%2520in%2520regularized%250Alinear%2520programming%2520formulations%2520with%2520the%2520classical%2520theory%2520of%2520stochastic%250Aapproximation.%2520Motivated%2520by%2520the%2520challenge%2520of%2520designing%2520algorithms%2520that%2520leverage%250Aoff-policy%2520data%2520while%2520maintaining%2520on-policy%2520exploration%252C%2520we%2520propose%2520PGDA-RL%252C%2520a%250Anovel%2520primal-dual%2520Projected%2520Gradient%2520Descent-Ascent%2520algorithm%2520for%2520solving%250Aregularized%2520Markov%2520Decision%2520Processes%2520%2528MDPs%2529.%2520PGDA-RL%2520integrates%2520experience%250Areplay-based%2520gradient%2520estimation%2520with%2520a%2520two-timescale%2520decomposition%2520of%2520the%250Aunderlying%2520nested%2520optimization%2520problem.%2520The%2520algorithm%2520operates%2520asynchronously%252C%250Ainteracts%2520with%2520the%2520environment%2520through%2520a%2520single%2520trajectory%2520of%2520correlated%2520data%252C%250Aand%2520updates%2520its%2520policy%2520online%2520in%2520response%2520to%2520the%2520dual%2520variable%2520associated%2520with%250Athe%2520occupation%2520measure%2520of%2520the%2520underlying%2520MDP.%2520We%2520prove%2520that%2520PGDA-RL%2520converges%250Aalmost%2520surely%2520to%2520the%2520optimal%2520value%2520function%2520and%2520policy%2520of%2520the%2520regularized%2520MDP.%250AOur%2520convergence%2520analysis%2520relies%2520on%2520tools%2520from%2520stochastic%2520approximation%2520theory%250Aand%2520holds%2520under%2520weaker%2520assumptions%2520than%2520those%2520required%2520by%2520existing%2520primal-dual%250ARL%2520approaches%252C%2520notably%2520removing%2520the%2520need%2520for%2520a%2520simulator%2520or%2520a%2520fixed%2520behavioral%250Apolicy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Two-Timescale%20Primal-Dual%20Framework%20for%20Reinforcement%20Learning%20via%0A%20%20Online%20Dual%20Variable%20Guidance&entry.906535625=Axel%20Friedrich%20Wolter%20and%20Tobias%20Sutter&entry.1292438233=%20%20We%20study%20reinforcement%20learning%20by%20combining%20recent%20advances%20in%20regularized%0Alinear%20programming%20formulations%20with%20the%20classical%20theory%20of%20stochastic%0Aapproximation.%20Motivated%20by%20the%20challenge%20of%20designing%20algorithms%20that%20leverage%0Aoff-policy%20data%20while%20maintaining%20on-policy%20exploration%2C%20we%20propose%20PGDA-RL%2C%20a%0Anovel%20primal-dual%20Projected%20Gradient%20Descent-Ascent%20algorithm%20for%20solving%0Aregularized%20Markov%20Decision%20Processes%20%28MDPs%29.%20PGDA-RL%20integrates%20experience%0Areplay-based%20gradient%20estimation%20with%20a%20two-timescale%20decomposition%20of%20the%0Aunderlying%20nested%20optimization%20problem.%20The%20algorithm%20operates%20asynchronously%2C%0Ainteracts%20with%20the%20environment%20through%20a%20single%20trajectory%20of%20correlated%20data%2C%0Aand%20updates%20its%20policy%20online%20in%20response%20to%20the%20dual%20variable%20associated%20with%0Athe%20occupation%20measure%20of%20the%20underlying%20MDP.%20We%20prove%20that%20PGDA-RL%20converges%0Aalmost%20surely%20to%20the%20optimal%20value%20function%20and%20policy%20of%20the%20regularized%20MDP.%0AOur%20convergence%20analysis%20relies%20on%20tools%20from%20stochastic%20approximation%20theory%0Aand%20holds%20under%20weaker%20assumptions%20than%20those%20required%20by%20existing%20primal-dual%0ARL%20approaches%2C%20notably%20removing%20the%20need%20for%20a%20simulator%20or%20a%20fixed%20behavioral%0Apolicy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04494v1&entry.124074799=Read"},
{"title": "Advances in Automated Fetal Brain MRI Segmentation and Biometry:\n  Insights from the FeTA 2024 Challenge", "author": "Vladyslav Zalevskyi and Thomas Sanchez and Misha Kaandorp and Margaux Roulet and Diego Fajardo-Rojas and Liu Li and Jana Hutter and Hongwei Bran Li and Matthew Barkovich and Hui Ji and Luca Wilhelmi and Aline D\u00e4ndliker and C\u00e9line Steger and M\u00e9riam Koob and Yvan Gomez and Anton Jakov\u010di\u0107 and Melita Klai\u0107 and Ana Ad\u017ei\u0107 and Pavel Markovi\u0107 and Gracia Grabari\u0107 and Milan Rados and Jordina Aviles Verdera and Gregor Kasprian and Gregor Dovjak and Raphael Gaubert-Rachm\u00fchl and Maurice Aschwanden and Qi Zeng and Davood Karimi and Denis Peruzzo and Tommaso Ciceri and Giorgio Longari and Rachika E. Hamadache and Amina Bouzid and Xavier Llad\u00f3 and Simone Chiarella and Gerard Mart\u00ed-Juan and Miguel \u00c1ngel Gonz\u00e1lez Ballester and Marco Castellaro and Marco Pinamonti and Valentina Visani and Robin Cremese and Ke\u00efn Sam and Fleur Gaudfernau and Param Ahir and Mehul Parikh and Maximilian Zenk and Michael Baumgartner and Klaus Maier-Hein and Li Tianhong and Yang Hong and Zhao Longfei and Domen Preloznik and \u017diga \u0160piclin and Jae Won Choi and Muyang Li and Jia Fu and Guotai Wang and Jingwen Jiang and Lyuyang Tong and Bo Du and Milton O. Candela-Leal and Andrea Gondova and Sungmin You and Abdul Qayyum and Moona Mazher and Steven A Niederer and Andras Jakab and Roxane Licandro and Kelly Payette and Meritxell Bach Cuadra", "abstract": "  Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools.\n", "link": "http://arxiv.org/abs/2505.02784v2", "date": "2025-05-07", "relevancy": 1.9676, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.496}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Automated%20Fetal%20Brain%20MRI%20Segmentation%20and%20Biometry%3A%0A%20%20Insights%20from%20the%20FeTA%202024%20Challenge&body=Title%3A%20Advances%20in%20Automated%20Fetal%20Brain%20MRI%20Segmentation%20and%20Biometry%3A%0A%20%20Insights%20from%20the%20FeTA%202024%20Challenge%0AAuthor%3A%20Vladyslav%20Zalevskyi%20and%20Thomas%20Sanchez%20and%20Misha%20Kaandorp%20and%20Margaux%20Roulet%20and%20Diego%20Fajardo-Rojas%20and%20Liu%20Li%20and%20Jana%20Hutter%20and%20Hongwei%20Bran%20Li%20and%20Matthew%20Barkovich%20and%20Hui%20Ji%20and%20Luca%20Wilhelmi%20and%20Aline%20D%C3%A4ndliker%20and%20C%C3%A9line%20Steger%20and%20M%C3%A9riam%20Koob%20and%20Yvan%20Gomez%20and%20Anton%20Jakov%C4%8Di%C4%87%20and%20Melita%20Klai%C4%87%20and%20Ana%20Ad%C5%BEi%C4%87%20and%20Pavel%20Markovi%C4%87%20and%20Gracia%20Grabari%C4%87%20and%20Milan%20Rados%20and%20Jordina%20Aviles%20Verdera%20and%20Gregor%20Kasprian%20and%20Gregor%20Dovjak%20and%20Raphael%20Gaubert-Rachm%C3%BChl%20and%20Maurice%20Aschwanden%20and%20Qi%20Zeng%20and%20Davood%20Karimi%20and%20Denis%20Peruzzo%20and%20Tommaso%20Ciceri%20and%20Giorgio%20Longari%20and%20Rachika%20E.%20Hamadache%20and%20Amina%20Bouzid%20and%20Xavier%20Llad%C3%B3%20and%20Simone%20Chiarella%20and%20Gerard%20Mart%C3%AD-Juan%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez%20Ballester%20and%20Marco%20Castellaro%20and%20Marco%20Pinamonti%20and%20Valentina%20Visani%20and%20Robin%20Cremese%20and%20Ke%C3%AFn%20Sam%20and%20Fleur%20Gaudfernau%20and%20Param%20Ahir%20and%20Mehul%20Parikh%20and%20Maximilian%20Zenk%20and%20Michael%20Baumgartner%20and%20Klaus%20Maier-Hein%20and%20Li%20Tianhong%20and%20Yang%20Hong%20and%20Zhao%20Longfei%20and%20Domen%20Preloznik%20and%20%C5%BDiga%20%C5%A0piclin%20and%20Jae%20Won%20Choi%20and%20Muyang%20Li%20and%20Jia%20Fu%20and%20Guotai%20Wang%20and%20Jingwen%20Jiang%20and%20Lyuyang%20Tong%20and%20Bo%20Du%20and%20Milton%20O.%20Candela-Leal%20and%20Andrea%20Gondova%20and%20Sungmin%20You%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Andras%20Jakab%20and%20Roxane%20Licandro%20and%20Kelly%20Payette%20and%20Meritxell%20Bach%20Cuadra%0AAbstract%3A%20%20%20Accurate%20fetal%20brain%20tissue%20segmentation%20and%20biometric%20analysis%20are%20essential%0Afor%20studying%20brain%20development%20in%20utero.%20The%20FeTA%20Challenge%202024%20advanced%0Aautomated%20fetal%20brain%20MRI%20analysis%20by%20introducing%20biometry%20prediction%20as%20a%20new%0Atask%20alongside%20tissue%20segmentation.%20For%20the%20first%20time%2C%20our%20diverse%0Amulti-centric%20test%20set%20included%20data%20from%20a%20new%20low-field%20%280.55T%29%20MRI%20dataset.%0AEvaluation%20metrics%20were%20also%20expanded%20to%20include%20the%20topology-specific%20Euler%0Acharacteristic%20difference%20%28ED%29.%20Sixteen%20teams%20submitted%20segmentation%20methods%2C%0Amost%20of%20which%20performed%20consistently%20across%20both%20high-%20and%20low-field%20scans.%0AHowever%2C%20longitudinal%20trends%20indicate%20that%20segmentation%20accuracy%20may%20be%0Areaching%20a%20plateau%2C%20with%20results%20now%20approaching%20inter-rater%20variability.%20The%0AED%20metric%20uncovered%20topological%20differences%20that%20were%20missed%20by%20conventional%0Ametrics%2C%20while%20the%20low-field%20dataset%20achieved%20the%20highest%20segmentation%20scores%2C%0Ahighlighting%20the%20potential%20of%20affordable%20imaging%20systems%20when%20paired%20with%0Ahigh-quality%20reconstruction.%20Seven%20teams%20participated%20in%20the%20biometry%20task%2C%20but%0Amost%20methods%20failed%20to%20outperform%20a%20simple%20baseline%20that%20predicted%20measurements%0Abased%20solely%20on%20gestational%20age%2C%20underscoring%20the%20challenge%20of%20extracting%0Areliable%20biometric%20estimates%20from%20image%20data%20alone.%20Domain%20shift%20analysis%0Aidentified%20image%20quality%20as%20the%20most%20significant%20factor%20affecting%20model%0Ageneralization%2C%20with%20super-resolution%20pipelines%20also%20playing%20a%20substantial%0Arole.%20Other%20factors%2C%20such%20as%20gestational%20age%2C%20pathology%2C%20and%20acquisition%20site%2C%0Ahad%20smaller%2C%20though%20still%20measurable%2C%20effects.%20Overall%2C%20FeTA%202024%20offers%20a%0Acomprehensive%20benchmark%20for%20multi-class%20segmentation%20and%20biometry%20estimation%20in%0Afetal%20brain%20MRI%2C%20underscoring%20the%20need%20for%20data-centric%20approaches%2C%20improved%0Atopological%20evaluation%2C%20and%20greater%20dataset%20diversity%20to%20enable%20clinically%0Arobust%20and%20generalizable%20AI%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.02784v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Automated%2520Fetal%2520Brain%2520MRI%2520Segmentation%2520and%2520Biometry%253A%250A%2520%2520Insights%2520from%2520the%2520FeTA%25202024%2520Challenge%26entry.906535625%3DVladyslav%2520Zalevskyi%2520and%2520Thomas%2520Sanchez%2520and%2520Misha%2520Kaandorp%2520and%2520Margaux%2520Roulet%2520and%2520Diego%2520Fajardo-Rojas%2520and%2520Liu%2520Li%2520and%2520Jana%2520Hutter%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Matthew%2520Barkovich%2520and%2520Hui%2520Ji%2520and%2520Luca%2520Wilhelmi%2520and%2520Aline%2520D%25C3%25A4ndliker%2520and%2520C%25C3%25A9line%2520Steger%2520and%2520M%25C3%25A9riam%2520Koob%2520and%2520Yvan%2520Gomez%2520and%2520Anton%2520Jakov%25C4%258Di%25C4%2587%2520and%2520Melita%2520Klai%25C4%2587%2520and%2520Ana%2520Ad%25C5%25BEi%25C4%2587%2520and%2520Pavel%2520Markovi%25C4%2587%2520and%2520Gracia%2520Grabari%25C4%2587%2520and%2520Milan%2520Rados%2520and%2520Jordina%2520Aviles%2520Verdera%2520and%2520Gregor%2520Kasprian%2520and%2520Gregor%2520Dovjak%2520and%2520Raphael%2520Gaubert-Rachm%25C3%25BChl%2520and%2520Maurice%2520Aschwanden%2520and%2520Qi%2520Zeng%2520and%2520Davood%2520Karimi%2520and%2520Denis%2520Peruzzo%2520and%2520Tommaso%2520Ciceri%2520and%2520Giorgio%2520Longari%2520and%2520Rachika%2520E.%2520Hamadache%2520and%2520Amina%2520Bouzid%2520and%2520Xavier%2520Llad%25C3%25B3%2520and%2520Simone%2520Chiarella%2520and%2520Gerard%2520Mart%25C3%25AD-Juan%2520and%2520Miguel%2520%25C3%2581ngel%2520Gonz%25C3%25A1lez%2520Ballester%2520and%2520Marco%2520Castellaro%2520and%2520Marco%2520Pinamonti%2520and%2520Valentina%2520Visani%2520and%2520Robin%2520Cremese%2520and%2520Ke%25C3%25AFn%2520Sam%2520and%2520Fleur%2520Gaudfernau%2520and%2520Param%2520Ahir%2520and%2520Mehul%2520Parikh%2520and%2520Maximilian%2520Zenk%2520and%2520Michael%2520Baumgartner%2520and%2520Klaus%2520Maier-Hein%2520and%2520Li%2520Tianhong%2520and%2520Yang%2520Hong%2520and%2520Zhao%2520Longfei%2520and%2520Domen%2520Preloznik%2520and%2520%25C5%25BDiga%2520%25C5%25A0piclin%2520and%2520Jae%2520Won%2520Choi%2520and%2520Muyang%2520Li%2520and%2520Jia%2520Fu%2520and%2520Guotai%2520Wang%2520and%2520Jingwen%2520Jiang%2520and%2520Lyuyang%2520Tong%2520and%2520Bo%2520Du%2520and%2520Milton%2520O.%2520Candela-Leal%2520and%2520Andrea%2520Gondova%2520and%2520Sungmin%2520You%2520and%2520Abdul%2520Qayyum%2520and%2520Moona%2520Mazher%2520and%2520Steven%2520A%2520Niederer%2520and%2520Andras%2520Jakab%2520and%2520Roxane%2520Licandro%2520and%2520Kelly%2520Payette%2520and%2520Meritxell%2520Bach%2520Cuadra%26entry.1292438233%3D%2520%2520Accurate%2520fetal%2520brain%2520tissue%2520segmentation%2520and%2520biometric%2520analysis%2520are%2520essential%250Afor%2520studying%2520brain%2520development%2520in%2520utero.%2520The%2520FeTA%2520Challenge%25202024%2520advanced%250Aautomated%2520fetal%2520brain%2520MRI%2520analysis%2520by%2520introducing%2520biometry%2520prediction%2520as%2520a%2520new%250Atask%2520alongside%2520tissue%2520segmentation.%2520For%2520the%2520first%2520time%252C%2520our%2520diverse%250Amulti-centric%2520test%2520set%2520included%2520data%2520from%2520a%2520new%2520low-field%2520%25280.55T%2529%2520MRI%2520dataset.%250AEvaluation%2520metrics%2520were%2520also%2520expanded%2520to%2520include%2520the%2520topology-specific%2520Euler%250Acharacteristic%2520difference%2520%2528ED%2529.%2520Sixteen%2520teams%2520submitted%2520segmentation%2520methods%252C%250Amost%2520of%2520which%2520performed%2520consistently%2520across%2520both%2520high-%2520and%2520low-field%2520scans.%250AHowever%252C%2520longitudinal%2520trends%2520indicate%2520that%2520segmentation%2520accuracy%2520may%2520be%250Areaching%2520a%2520plateau%252C%2520with%2520results%2520now%2520approaching%2520inter-rater%2520variability.%2520The%250AED%2520metric%2520uncovered%2520topological%2520differences%2520that%2520were%2520missed%2520by%2520conventional%250Ametrics%252C%2520while%2520the%2520low-field%2520dataset%2520achieved%2520the%2520highest%2520segmentation%2520scores%252C%250Ahighlighting%2520the%2520potential%2520of%2520affordable%2520imaging%2520systems%2520when%2520paired%2520with%250Ahigh-quality%2520reconstruction.%2520Seven%2520teams%2520participated%2520in%2520the%2520biometry%2520task%252C%2520but%250Amost%2520methods%2520failed%2520to%2520outperform%2520a%2520simple%2520baseline%2520that%2520predicted%2520measurements%250Abased%2520solely%2520on%2520gestational%2520age%252C%2520underscoring%2520the%2520challenge%2520of%2520extracting%250Areliable%2520biometric%2520estimates%2520from%2520image%2520data%2520alone.%2520Domain%2520shift%2520analysis%250Aidentified%2520image%2520quality%2520as%2520the%2520most%2520significant%2520factor%2520affecting%2520model%250Ageneralization%252C%2520with%2520super-resolution%2520pipelines%2520also%2520playing%2520a%2520substantial%250Arole.%2520Other%2520factors%252C%2520such%2520as%2520gestational%2520age%252C%2520pathology%252C%2520and%2520acquisition%2520site%252C%250Ahad%2520smaller%252C%2520though%2520still%2520measurable%252C%2520effects.%2520Overall%252C%2520FeTA%25202024%2520offers%2520a%250Acomprehensive%2520benchmark%2520for%2520multi-class%2520segmentation%2520and%2520biometry%2520estimation%2520in%250Afetal%2520brain%2520MRI%252C%2520underscoring%2520the%2520need%2520for%2520data-centric%2520approaches%252C%2520improved%250Atopological%2520evaluation%252C%2520and%2520greater%2520dataset%2520diversity%2520to%2520enable%2520clinically%250Arobust%2520and%2520generalizable%2520AI%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02784v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Automated%20Fetal%20Brain%20MRI%20Segmentation%20and%20Biometry%3A%0A%20%20Insights%20from%20the%20FeTA%202024%20Challenge&entry.906535625=Vladyslav%20Zalevskyi%20and%20Thomas%20Sanchez%20and%20Misha%20Kaandorp%20and%20Margaux%20Roulet%20and%20Diego%20Fajardo-Rojas%20and%20Liu%20Li%20and%20Jana%20Hutter%20and%20Hongwei%20Bran%20Li%20and%20Matthew%20Barkovich%20and%20Hui%20Ji%20and%20Luca%20Wilhelmi%20and%20Aline%20D%C3%A4ndliker%20and%20C%C3%A9line%20Steger%20and%20M%C3%A9riam%20Koob%20and%20Yvan%20Gomez%20and%20Anton%20Jakov%C4%8Di%C4%87%20and%20Melita%20Klai%C4%87%20and%20Ana%20Ad%C5%BEi%C4%87%20and%20Pavel%20Markovi%C4%87%20and%20Gracia%20Grabari%C4%87%20and%20Milan%20Rados%20and%20Jordina%20Aviles%20Verdera%20and%20Gregor%20Kasprian%20and%20Gregor%20Dovjak%20and%20Raphael%20Gaubert-Rachm%C3%BChl%20and%20Maurice%20Aschwanden%20and%20Qi%20Zeng%20and%20Davood%20Karimi%20and%20Denis%20Peruzzo%20and%20Tommaso%20Ciceri%20and%20Giorgio%20Longari%20and%20Rachika%20E.%20Hamadache%20and%20Amina%20Bouzid%20and%20Xavier%20Llad%C3%B3%20and%20Simone%20Chiarella%20and%20Gerard%20Mart%C3%AD-Juan%20and%20Miguel%20%C3%81ngel%20Gonz%C3%A1lez%20Ballester%20and%20Marco%20Castellaro%20and%20Marco%20Pinamonti%20and%20Valentina%20Visani%20and%20Robin%20Cremese%20and%20Ke%C3%AFn%20Sam%20and%20Fleur%20Gaudfernau%20and%20Param%20Ahir%20and%20Mehul%20Parikh%20and%20Maximilian%20Zenk%20and%20Michael%20Baumgartner%20and%20Klaus%20Maier-Hein%20and%20Li%20Tianhong%20and%20Yang%20Hong%20and%20Zhao%20Longfei%20and%20Domen%20Preloznik%20and%20%C5%BDiga%20%C5%A0piclin%20and%20Jae%20Won%20Choi%20and%20Muyang%20Li%20and%20Jia%20Fu%20and%20Guotai%20Wang%20and%20Jingwen%20Jiang%20and%20Lyuyang%20Tong%20and%20Bo%20Du%20and%20Milton%20O.%20Candela-Leal%20and%20Andrea%20Gondova%20and%20Sungmin%20You%20and%20Abdul%20Qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Andras%20Jakab%20and%20Roxane%20Licandro%20and%20Kelly%20Payette%20and%20Meritxell%20Bach%20Cuadra&entry.1292438233=%20%20Accurate%20fetal%20brain%20tissue%20segmentation%20and%20biometric%20analysis%20are%20essential%0Afor%20studying%20brain%20development%20in%20utero.%20The%20FeTA%20Challenge%202024%20advanced%0Aautomated%20fetal%20brain%20MRI%20analysis%20by%20introducing%20biometry%20prediction%20as%20a%20new%0Atask%20alongside%20tissue%20segmentation.%20For%20the%20first%20time%2C%20our%20diverse%0Amulti-centric%20test%20set%20included%20data%20from%20a%20new%20low-field%20%280.55T%29%20MRI%20dataset.%0AEvaluation%20metrics%20were%20also%20expanded%20to%20include%20the%20topology-specific%20Euler%0Acharacteristic%20difference%20%28ED%29.%20Sixteen%20teams%20submitted%20segmentation%20methods%2C%0Amost%20of%20which%20performed%20consistently%20across%20both%20high-%20and%20low-field%20scans.%0AHowever%2C%20longitudinal%20trends%20indicate%20that%20segmentation%20accuracy%20may%20be%0Areaching%20a%20plateau%2C%20with%20results%20now%20approaching%20inter-rater%20variability.%20The%0AED%20metric%20uncovered%20topological%20differences%20that%20were%20missed%20by%20conventional%0Ametrics%2C%20while%20the%20low-field%20dataset%20achieved%20the%20highest%20segmentation%20scores%2C%0Ahighlighting%20the%20potential%20of%20affordable%20imaging%20systems%20when%20paired%20with%0Ahigh-quality%20reconstruction.%20Seven%20teams%20participated%20in%20the%20biometry%20task%2C%20but%0Amost%20methods%20failed%20to%20outperform%20a%20simple%20baseline%20that%20predicted%20measurements%0Abased%20solely%20on%20gestational%20age%2C%20underscoring%20the%20challenge%20of%20extracting%0Areliable%20biometric%20estimates%20from%20image%20data%20alone.%20Domain%20shift%20analysis%0Aidentified%20image%20quality%20as%20the%20most%20significant%20factor%20affecting%20model%0Ageneralization%2C%20with%20super-resolution%20pipelines%20also%20playing%20a%20substantial%0Arole.%20Other%20factors%2C%20such%20as%20gestational%20age%2C%20pathology%2C%20and%20acquisition%20site%2C%0Ahad%20smaller%2C%20though%20still%20measurable%2C%20effects.%20Overall%2C%20FeTA%202024%20offers%20a%0Acomprehensive%20benchmark%20for%20multi-class%20segmentation%20and%20biometry%20estimation%20in%0Afetal%20brain%20MRI%2C%20underscoring%20the%20need%20for%20data-centric%20approaches%2C%20improved%0Atopological%20evaluation%2C%20and%20greater%20dataset%20diversity%20to%20enable%20clinically%0Arobust%20and%20generalizable%20AI%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.02784v2&entry.124074799=Read"},
{"title": "Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder", "author": "Priyam Gupta and Peter J. Schmid and Denis Sipp and Taraneh Sayadi and Georgios Rigas", "abstract": "  The Koopman operator presents an attractive approach to achieve global\nlinearization of nonlinear systems, making it a valuable method for simplifying\nthe understanding of complex dynamics. While data-driven methodologies have\nexhibited promise in approximating finite Koopman operators, they grapple with\nvarious challenges, such as the judicious selection of observables,\ndimensionality reduction, and the ability to predict complex system behaviours\naccurately. This study presents a novel approach termed Mori-Zwanzig\nautoencoder (MZ-AE) to robustly approximate the Koopman operator in\nlow-dimensional spaces. The proposed method leverages a nonlinear autoencoder\nto extract key observables for approximating a finite invariant Koopman\nsubspace and integrates a non-Markovian correction mechanism using the\nMori-Zwanzig formalism. Consequently, this approach yields an approximate\nclosure of the dynamics within the latent manifold of the nonlinear\nautoencoder, thereby enhancing the accuracy and stability of the Koopman\noperator approximation. Demonstrations showcase the technique's improved\npredictive capability for flow around a cylinder. It also provides a low\ndimensional approximation for Kuramoto-Sivashinsky (KS) with promising\nshort-term predictability and robust long-term statistical performance. By\nbridging the gap between data-driven techniques and the mathematical\nfoundations of Koopman theory, MZ-AE offers a promising avenue for improved\nunderstanding and prediction of complex nonlinear dynamics.\n", "link": "http://arxiv.org/abs/2310.10745v3", "date": "2025-05-07", "relevancy": 1.9611, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5044}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mori-Zwanzig%20latent%20space%20Koopman%20closure%20for%20nonlinear%20autoencoder&body=Title%3A%20Mori-Zwanzig%20latent%20space%20Koopman%20closure%20for%20nonlinear%20autoencoder%0AAuthor%3A%20Priyam%20Gupta%20and%20Peter%20J.%20Schmid%20and%20Denis%20Sipp%20and%20Taraneh%20Sayadi%20and%20Georgios%20Rigas%0AAbstract%3A%20%20%20The%20Koopman%20operator%20presents%20an%20attractive%20approach%20to%20achieve%20global%0Alinearization%20of%20nonlinear%20systems%2C%20making%20it%20a%20valuable%20method%20for%20simplifying%0Athe%20understanding%20of%20complex%20dynamics.%20While%20data-driven%20methodologies%20have%0Aexhibited%20promise%20in%20approximating%20finite%20Koopman%20operators%2C%20they%20grapple%20with%0Avarious%20challenges%2C%20such%20as%20the%20judicious%20selection%20of%20observables%2C%0Adimensionality%20reduction%2C%20and%20the%20ability%20to%20predict%20complex%20system%20behaviours%0Aaccurately.%20This%20study%20presents%20a%20novel%20approach%20termed%20Mori-Zwanzig%0Aautoencoder%20%28MZ-AE%29%20to%20robustly%20approximate%20the%20Koopman%20operator%20in%0Alow-dimensional%20spaces.%20The%20proposed%20method%20leverages%20a%20nonlinear%20autoencoder%0Ato%20extract%20key%20observables%20for%20approximating%20a%20finite%20invariant%20Koopman%0Asubspace%20and%20integrates%20a%20non-Markovian%20correction%20mechanism%20using%20the%0AMori-Zwanzig%20formalism.%20Consequently%2C%20this%20approach%20yields%20an%20approximate%0Aclosure%20of%20the%20dynamics%20within%20the%20latent%20manifold%20of%20the%20nonlinear%0Aautoencoder%2C%20thereby%20enhancing%20the%20accuracy%20and%20stability%20of%20the%20Koopman%0Aoperator%20approximation.%20Demonstrations%20showcase%20the%20technique%27s%20improved%0Apredictive%20capability%20for%20flow%20around%20a%20cylinder.%20It%20also%20provides%20a%20low%0Adimensional%20approximation%20for%20Kuramoto-Sivashinsky%20%28KS%29%20with%20promising%0Ashort-term%20predictability%20and%20robust%20long-term%20statistical%20performance.%20By%0Abridging%20the%20gap%20between%20data-driven%20techniques%20and%20the%20mathematical%0Afoundations%20of%20Koopman%20theory%2C%20MZ-AE%20offers%20a%20promising%20avenue%20for%20improved%0Aunderstanding%20and%20prediction%20of%20complex%20nonlinear%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMori-Zwanzig%2520latent%2520space%2520Koopman%2520closure%2520for%2520nonlinear%2520autoencoder%26entry.906535625%3DPriyam%2520Gupta%2520and%2520Peter%2520J.%2520Schmid%2520and%2520Denis%2520Sipp%2520and%2520Taraneh%2520Sayadi%2520and%2520Georgios%2520Rigas%26entry.1292438233%3D%2520%2520The%2520Koopman%2520operator%2520presents%2520an%2520attractive%2520approach%2520to%2520achieve%2520global%250Alinearization%2520of%2520nonlinear%2520systems%252C%2520making%2520it%2520a%2520valuable%2520method%2520for%2520simplifying%250Athe%2520understanding%2520of%2520complex%2520dynamics.%2520While%2520data-driven%2520methodologies%2520have%250Aexhibited%2520promise%2520in%2520approximating%2520finite%2520Koopman%2520operators%252C%2520they%2520grapple%2520with%250Avarious%2520challenges%252C%2520such%2520as%2520the%2520judicious%2520selection%2520of%2520observables%252C%250Adimensionality%2520reduction%252C%2520and%2520the%2520ability%2520to%2520predict%2520complex%2520system%2520behaviours%250Aaccurately.%2520This%2520study%2520presents%2520a%2520novel%2520approach%2520termed%2520Mori-Zwanzig%250Aautoencoder%2520%2528MZ-AE%2529%2520to%2520robustly%2520approximate%2520the%2520Koopman%2520operator%2520in%250Alow-dimensional%2520spaces.%2520The%2520proposed%2520method%2520leverages%2520a%2520nonlinear%2520autoencoder%250Ato%2520extract%2520key%2520observables%2520for%2520approximating%2520a%2520finite%2520invariant%2520Koopman%250Asubspace%2520and%2520integrates%2520a%2520non-Markovian%2520correction%2520mechanism%2520using%2520the%250AMori-Zwanzig%2520formalism.%2520Consequently%252C%2520this%2520approach%2520yields%2520an%2520approximate%250Aclosure%2520of%2520the%2520dynamics%2520within%2520the%2520latent%2520manifold%2520of%2520the%2520nonlinear%250Aautoencoder%252C%2520thereby%2520enhancing%2520the%2520accuracy%2520and%2520stability%2520of%2520the%2520Koopman%250Aoperator%2520approximation.%2520Demonstrations%2520showcase%2520the%2520technique%2527s%2520improved%250Apredictive%2520capability%2520for%2520flow%2520around%2520a%2520cylinder.%2520It%2520also%2520provides%2520a%2520low%250Adimensional%2520approximation%2520for%2520Kuramoto-Sivashinsky%2520%2528KS%2529%2520with%2520promising%250Ashort-term%2520predictability%2520and%2520robust%2520long-term%2520statistical%2520performance.%2520By%250Abridging%2520the%2520gap%2520between%2520data-driven%2520techniques%2520and%2520the%2520mathematical%250Afoundations%2520of%2520Koopman%2520theory%252C%2520MZ-AE%2520offers%2520a%2520promising%2520avenue%2520for%2520improved%250Aunderstanding%2520and%2520prediction%2520of%2520complex%2520nonlinear%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mori-Zwanzig%20latent%20space%20Koopman%20closure%20for%20nonlinear%20autoencoder&entry.906535625=Priyam%20Gupta%20and%20Peter%20J.%20Schmid%20and%20Denis%20Sipp%20and%20Taraneh%20Sayadi%20and%20Georgios%20Rigas&entry.1292438233=%20%20The%20Koopman%20operator%20presents%20an%20attractive%20approach%20to%20achieve%20global%0Alinearization%20of%20nonlinear%20systems%2C%20making%20it%20a%20valuable%20method%20for%20simplifying%0Athe%20understanding%20of%20complex%20dynamics.%20While%20data-driven%20methodologies%20have%0Aexhibited%20promise%20in%20approximating%20finite%20Koopman%20operators%2C%20they%20grapple%20with%0Avarious%20challenges%2C%20such%20as%20the%20judicious%20selection%20of%20observables%2C%0Adimensionality%20reduction%2C%20and%20the%20ability%20to%20predict%20complex%20system%20behaviours%0Aaccurately.%20This%20study%20presents%20a%20novel%20approach%20termed%20Mori-Zwanzig%0Aautoencoder%20%28MZ-AE%29%20to%20robustly%20approximate%20the%20Koopman%20operator%20in%0Alow-dimensional%20spaces.%20The%20proposed%20method%20leverages%20a%20nonlinear%20autoencoder%0Ato%20extract%20key%20observables%20for%20approximating%20a%20finite%20invariant%20Koopman%0Asubspace%20and%20integrates%20a%20non-Markovian%20correction%20mechanism%20using%20the%0AMori-Zwanzig%20formalism.%20Consequently%2C%20this%20approach%20yields%20an%20approximate%0Aclosure%20of%20the%20dynamics%20within%20the%20latent%20manifold%20of%20the%20nonlinear%0Aautoencoder%2C%20thereby%20enhancing%20the%20accuracy%20and%20stability%20of%20the%20Koopman%0Aoperator%20approximation.%20Demonstrations%20showcase%20the%20technique%27s%20improved%0Apredictive%20capability%20for%20flow%20around%20a%20cylinder.%20It%20also%20provides%20a%20low%0Adimensional%20approximation%20for%20Kuramoto-Sivashinsky%20%28KS%29%20with%20promising%0Ashort-term%20predictability%20and%20robust%20long-term%20statistical%20performance.%20By%0Abridging%20the%20gap%20between%20data-driven%20techniques%20and%20the%20mathematical%0Afoundations%20of%20Koopman%20theory%2C%20MZ-AE%20offers%20a%20promising%20avenue%20for%20improved%0Aunderstanding%20and%20prediction%20of%20complex%20nonlinear%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10745v3&entry.124074799=Read"},
{"title": "Active Sampling for MRI-based Sequential Decision Making", "author": "Yuning Du and Jingshuai Liu and Rohan Dharmakumar and Sotirios A. Tsaftaris", "abstract": "  Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling\n", "link": "http://arxiv.org/abs/2505.04586v1", "date": "2025-05-07", "relevancy": 1.958, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5239}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4832}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Sampling%20for%20MRI-based%20Sequential%20Decision%20Making&body=Title%3A%20Active%20Sampling%20for%20MRI-based%20Sequential%20Decision%20Making%0AAuthor%3A%20Yuning%20Du%20and%20Jingshuai%20Liu%20and%20Rohan%20Dharmakumar%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Despite%20the%20superior%20diagnostic%20capability%20of%20Magnetic%20Resonance%20Imaging%0A%28MRI%29%2C%20its%20use%20as%20a%20Point-of-Care%20%28PoC%29%20device%20remains%20limited%20by%20high%20cost%20and%0Acomplexity.%20To%20enable%20such%20a%20future%20by%20reducing%20the%20magnetic%20field%20strength%2C%0Aone%20key%20approach%20will%20be%20to%20improve%20sampling%20strategies.%20Previous%20work%20has%0Ashown%20that%20it%20is%20possible%20to%20make%20diagnostic%20decisions%20directly%20from%20k-space%0Awith%20fewer%20samples.%20Such%20work%20shows%20that%20single%20diagnostic%20decisions%20can%20be%0Amade%2C%20but%20if%20we%20aspire%20to%20see%20MRI%20as%20a%20true%20PoC%2C%20multiple%20and%20sequential%0Adecisions%20are%20necessary%20while%20minimizing%20the%20number%20of%20samples%20acquired.%20We%0Apresent%20a%20novel%20multi-objective%20reinforcement%20learning%20framework%20enabling%0Acomprehensive%2C%20sequential%2C%20diagnostic%20evaluation%20from%20undersampled%20k-space%0Adata.%20Our%20approach%20during%20inference%20actively%20adapts%20to%20sequential%20decisions%20to%0Aoptimally%20sample.%20To%20achieve%20this%2C%20we%20introduce%20a%20training%20methodology%20that%0Aidentifies%20the%20samples%20that%20contribute%20the%20best%20to%20each%20diagnostic%20objective%0Ausing%20a%20step-wise%20weighting%20reward%20function.%20We%20evaluate%20our%20approach%20in%20two%0Asequential%20knee%20pathology%20assessment%20tasks%3A%20ACL%20sprain%20detection%20and%20cartilage%0Athickness%20loss%20assessment.%20Our%20framework%20achieves%20diagnostic%20performance%0Acompetitive%20with%20various%20policy-based%20benchmarks%20on%20disease%20detection%2C%20severity%0Aquantification%2C%20and%20overall%20sequential%20diagnosis%2C%20while%20substantially%20saving%0Ak-space%20samples.%20Our%20approach%20paves%20the%20way%20for%20the%20future%20of%20MRI%20as%20a%0Acomprehensive%20and%20affordable%20PoC%20device.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/vios-s/MRI_Sequential_Active_Sampling%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Sampling%2520for%2520MRI-based%2520Sequential%2520Decision%2520Making%26entry.906535625%3DYuning%2520Du%2520and%2520Jingshuai%2520Liu%2520and%2520Rohan%2520Dharmakumar%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3D%2520%2520Despite%2520the%2520superior%2520diagnostic%2520capability%2520of%2520Magnetic%2520Resonance%2520Imaging%250A%2528MRI%2529%252C%2520its%2520use%2520as%2520a%2520Point-of-Care%2520%2528PoC%2529%2520device%2520remains%2520limited%2520by%2520high%2520cost%2520and%250Acomplexity.%2520To%2520enable%2520such%2520a%2520future%2520by%2520reducing%2520the%2520magnetic%2520field%2520strength%252C%250Aone%2520key%2520approach%2520will%2520be%2520to%2520improve%2520sampling%2520strategies.%2520Previous%2520work%2520has%250Ashown%2520that%2520it%2520is%2520possible%2520to%2520make%2520diagnostic%2520decisions%2520directly%2520from%2520k-space%250Awith%2520fewer%2520samples.%2520Such%2520work%2520shows%2520that%2520single%2520diagnostic%2520decisions%2520can%2520be%250Amade%252C%2520but%2520if%2520we%2520aspire%2520to%2520see%2520MRI%2520as%2520a%2520true%2520PoC%252C%2520multiple%2520and%2520sequential%250Adecisions%2520are%2520necessary%2520while%2520minimizing%2520the%2520number%2520of%2520samples%2520acquired.%2520We%250Apresent%2520a%2520novel%2520multi-objective%2520reinforcement%2520learning%2520framework%2520enabling%250Acomprehensive%252C%2520sequential%252C%2520diagnostic%2520evaluation%2520from%2520undersampled%2520k-space%250Adata.%2520Our%2520approach%2520during%2520inference%2520actively%2520adapts%2520to%2520sequential%2520decisions%2520to%250Aoptimally%2520sample.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520training%2520methodology%2520that%250Aidentifies%2520the%2520samples%2520that%2520contribute%2520the%2520best%2520to%2520each%2520diagnostic%2520objective%250Ausing%2520a%2520step-wise%2520weighting%2520reward%2520function.%2520We%2520evaluate%2520our%2520approach%2520in%2520two%250Asequential%2520knee%2520pathology%2520assessment%2520tasks%253A%2520ACL%2520sprain%2520detection%2520and%2520cartilage%250Athickness%2520loss%2520assessment.%2520Our%2520framework%2520achieves%2520diagnostic%2520performance%250Acompetitive%2520with%2520various%2520policy-based%2520benchmarks%2520on%2520disease%2520detection%252C%2520severity%250Aquantification%252C%2520and%2520overall%2520sequential%2520diagnosis%252C%2520while%2520substantially%2520saving%250Ak-space%2520samples.%2520Our%2520approach%2520paves%2520the%2520way%2520for%2520the%2520future%2520of%2520MRI%2520as%2520a%250Acomprehensive%2520and%2520affordable%2520PoC%2520device.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/vios-s/MRI_Sequential_Active_Sampling%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Sampling%20for%20MRI-based%20Sequential%20Decision%20Making&entry.906535625=Yuning%20Du%20and%20Jingshuai%20Liu%20and%20Rohan%20Dharmakumar%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Despite%20the%20superior%20diagnostic%20capability%20of%20Magnetic%20Resonance%20Imaging%0A%28MRI%29%2C%20its%20use%20as%20a%20Point-of-Care%20%28PoC%29%20device%20remains%20limited%20by%20high%20cost%20and%0Acomplexity.%20To%20enable%20such%20a%20future%20by%20reducing%20the%20magnetic%20field%20strength%2C%0Aone%20key%20approach%20will%20be%20to%20improve%20sampling%20strategies.%20Previous%20work%20has%0Ashown%20that%20it%20is%20possible%20to%20make%20diagnostic%20decisions%20directly%20from%20k-space%0Awith%20fewer%20samples.%20Such%20work%20shows%20that%20single%20diagnostic%20decisions%20can%20be%0Amade%2C%20but%20if%20we%20aspire%20to%20see%20MRI%20as%20a%20true%20PoC%2C%20multiple%20and%20sequential%0Adecisions%20are%20necessary%20while%20minimizing%20the%20number%20of%20samples%20acquired.%20We%0Apresent%20a%20novel%20multi-objective%20reinforcement%20learning%20framework%20enabling%0Acomprehensive%2C%20sequential%2C%20diagnostic%20evaluation%20from%20undersampled%20k-space%0Adata.%20Our%20approach%20during%20inference%20actively%20adapts%20to%20sequential%20decisions%20to%0Aoptimally%20sample.%20To%20achieve%20this%2C%20we%20introduce%20a%20training%20methodology%20that%0Aidentifies%20the%20samples%20that%20contribute%20the%20best%20to%20each%20diagnostic%20objective%0Ausing%20a%20step-wise%20weighting%20reward%20function.%20We%20evaluate%20our%20approach%20in%20two%0Asequential%20knee%20pathology%20assessment%20tasks%3A%20ACL%20sprain%20detection%20and%20cartilage%0Athickness%20loss%20assessment.%20Our%20framework%20achieves%20diagnostic%20performance%0Acompetitive%20with%20various%20policy-based%20benchmarks%20on%20disease%20detection%2C%20severity%0Aquantification%2C%20and%20overall%20sequential%20diagnosis%2C%20while%20substantially%20saving%0Ak-space%20samples.%20Our%20approach%20paves%20the%20way%20for%20the%20future%20of%20MRI%20as%20a%0Acomprehensive%20and%20affordable%20PoC%20device.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/vios-s/MRI_Sequential_Active_Sampling%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04586v1&entry.124074799=Read"},
{"title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and\n  Performance in Mixed Urban Traffic", "author": "Mohammad Elayan and Wissam Kontar", "abstract": "  Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis.\n", "link": "http://arxiv.org/abs/2505.04379v1", "date": "2025-05-07", "relevancy": 1.9524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4924}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consensus-Aware%20AV%20Behavior%3A%20Trade-offs%20Between%20Safety%2C%20Interaction%2C%20and%0A%20%20Performance%20in%20Mixed%20Urban%20Traffic&body=Title%3A%20Consensus-Aware%20AV%20Behavior%3A%20Trade-offs%20Between%20Safety%2C%20Interaction%2C%20and%0A%20%20Performance%20in%20Mixed%20Urban%20Traffic%0AAuthor%3A%20Mohammad%20Elayan%20and%20Wissam%20Kontar%0AAbstract%3A%20%20%20Transportation%20systems%20have%20long%20been%20shaped%20by%20complexity%20and%20heterogeneity%2C%0Adriven%20by%20the%20interdependency%20of%20agent%20actions%20and%20traffic%20outcomes.%20The%0Adeployment%20of%20automated%20vehicles%20%28AVs%29%20in%20such%20systems%20introduces%20a%20new%0Achallenge%3A%20achieving%20consensus%20across%20safety%2C%20interaction%20quality%2C%20and%20traffic%0Aperformance.%20In%20this%20work%2C%20we%20position%20consensus%20as%20a%20fundamental%20property%20of%0Athe%20traffic%20system%20and%20aim%20to%20quantify%20it.%20We%20use%20high-resolution%20trajectory%0Adata%20from%20the%20Third%20Generation%20Simulation%20%28TGSIM%29%20dataset%20to%20empirically%0Aanalyze%20AV%20and%20human-driven%20vehicle%20%28HDV%29%20behavior%20at%20a%20signalized%20urban%0Aintersection%20and%20around%20vulnerable%20road%20users%20%28VRUs%29.%20Key%20metrics%2C%20including%0ATime-to-Collision%20%28TTC%29%2C%20Post-Encroachment%20Time%20%28PET%29%2C%20deceleration%20patterns%2C%0Aheadways%2C%20and%20string%20stability%2C%20are%20evaluated%20across%20the%20three%20performance%0Adimensions.%20Results%20show%20that%20full%20consensus%20across%20safety%2C%20interaction%2C%20and%0Aperformance%20is%20rare%2C%20with%20only%201.63%25%20of%20AV-VRU%20interaction%20frames%20meeting%20all%0Athree%20conditions.%20These%20findings%20highlight%20the%20need%20for%20AV%20models%20that%0Aexplicitly%20balance%20multi-dimensional%20performance%20in%20mixed-traffic%20environments.%0AFull%20reproducibility%20is%20supported%20via%20our%20open-source%20codebase%20on%0Ahttps%3A//github.com/wissamkontar/Consensus-AV-Analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsensus-Aware%2520AV%2520Behavior%253A%2520Trade-offs%2520Between%2520Safety%252C%2520Interaction%252C%2520and%250A%2520%2520Performance%2520in%2520Mixed%2520Urban%2520Traffic%26entry.906535625%3DMohammad%2520Elayan%2520and%2520Wissam%2520Kontar%26entry.1292438233%3D%2520%2520Transportation%2520systems%2520have%2520long%2520been%2520shaped%2520by%2520complexity%2520and%2520heterogeneity%252C%250Adriven%2520by%2520the%2520interdependency%2520of%2520agent%2520actions%2520and%2520traffic%2520outcomes.%2520The%250Adeployment%2520of%2520automated%2520vehicles%2520%2528AVs%2529%2520in%2520such%2520systems%2520introduces%2520a%2520new%250Achallenge%253A%2520achieving%2520consensus%2520across%2520safety%252C%2520interaction%2520quality%252C%2520and%2520traffic%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520position%2520consensus%2520as%2520a%2520fundamental%2520property%2520of%250Athe%2520traffic%2520system%2520and%2520aim%2520to%2520quantify%2520it.%2520We%2520use%2520high-resolution%2520trajectory%250Adata%2520from%2520the%2520Third%2520Generation%2520Simulation%2520%2528TGSIM%2529%2520dataset%2520to%2520empirically%250Aanalyze%2520AV%2520and%2520human-driven%2520vehicle%2520%2528HDV%2529%2520behavior%2520at%2520a%2520signalized%2520urban%250Aintersection%2520and%2520around%2520vulnerable%2520road%2520users%2520%2528VRUs%2529.%2520Key%2520metrics%252C%2520including%250ATime-to-Collision%2520%2528TTC%2529%252C%2520Post-Encroachment%2520Time%2520%2528PET%2529%252C%2520deceleration%2520patterns%252C%250Aheadways%252C%2520and%2520string%2520stability%252C%2520are%2520evaluated%2520across%2520the%2520three%2520performance%250Adimensions.%2520Results%2520show%2520that%2520full%2520consensus%2520across%2520safety%252C%2520interaction%252C%2520and%250Aperformance%2520is%2520rare%252C%2520with%2520only%25201.63%2525%2520of%2520AV-VRU%2520interaction%2520frames%2520meeting%2520all%250Athree%2520conditions.%2520These%2520findings%2520highlight%2520the%2520need%2520for%2520AV%2520models%2520that%250Aexplicitly%2520balance%2520multi-dimensional%2520performance%2520in%2520mixed-traffic%2520environments.%250AFull%2520reproducibility%2520is%2520supported%2520via%2520our%2520open-source%2520codebase%2520on%250Ahttps%253A//github.com/wissamkontar/Consensus-AV-Analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consensus-Aware%20AV%20Behavior%3A%20Trade-offs%20Between%20Safety%2C%20Interaction%2C%20and%0A%20%20Performance%20in%20Mixed%20Urban%20Traffic&entry.906535625=Mohammad%20Elayan%20and%20Wissam%20Kontar&entry.1292438233=%20%20Transportation%20systems%20have%20long%20been%20shaped%20by%20complexity%20and%20heterogeneity%2C%0Adriven%20by%20the%20interdependency%20of%20agent%20actions%20and%20traffic%20outcomes.%20The%0Adeployment%20of%20automated%20vehicles%20%28AVs%29%20in%20such%20systems%20introduces%20a%20new%0Achallenge%3A%20achieving%20consensus%20across%20safety%2C%20interaction%20quality%2C%20and%20traffic%0Aperformance.%20In%20this%20work%2C%20we%20position%20consensus%20as%20a%20fundamental%20property%20of%0Athe%20traffic%20system%20and%20aim%20to%20quantify%20it.%20We%20use%20high-resolution%20trajectory%0Adata%20from%20the%20Third%20Generation%20Simulation%20%28TGSIM%29%20dataset%20to%20empirically%0Aanalyze%20AV%20and%20human-driven%20vehicle%20%28HDV%29%20behavior%20at%20a%20signalized%20urban%0Aintersection%20and%20around%20vulnerable%20road%20users%20%28VRUs%29.%20Key%20metrics%2C%20including%0ATime-to-Collision%20%28TTC%29%2C%20Post-Encroachment%20Time%20%28PET%29%2C%20deceleration%20patterns%2C%0Aheadways%2C%20and%20string%20stability%2C%20are%20evaluated%20across%20the%20three%20performance%0Adimensions.%20Results%20show%20that%20full%20consensus%20across%20safety%2C%20interaction%2C%20and%0Aperformance%20is%20rare%2C%20with%20only%201.63%25%20of%20AV-VRU%20interaction%20frames%20meeting%20all%0Athree%20conditions.%20These%20findings%20highlight%20the%20need%20for%20AV%20models%20that%0Aexplicitly%20balance%20multi-dimensional%20performance%20in%20mixed-traffic%20environments.%0AFull%20reproducibility%20is%20supported%20via%20our%20open-source%20codebase%20on%0Ahttps%3A//github.com/wissamkontar/Consensus-AV-Analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04379v1&entry.124074799=Read"},
{"title": "CountDiffusion: Text-to-Image Synthesis with Training-Free\n  Counting-Guidance Diffusion", "author": "Yanyu Li and Pencheng Wan and Liang Han and Yaowei Wang and Liqiang Nie and Min Zhang", "abstract": "  Stable Diffusion has advanced text-to-image synthesis, but training models to\ngenerate images with accurate object quantity is still difficult due to the\nhigh computational cost and the challenge of teaching models the abstract\nconcept of quantity. In this paper, we propose CountDiffusion, a training-free\nframework aiming at generating images with correct object quantity from textual\ndescriptions. CountDiffusion consists of two stages. In the first stage, an\nintermediate denoising result is generated by the diffusion model to predict\nthe final synthesized image with one-step denoising, and a counting model is\nused to count the number of objects in this image. In the second stage, a\ncorrection module is used to correct the object quantity by changing the\nattention map of the object with universal guidance. The proposed\nCountDiffusion can be plugged into any diffusion-based text-to-image (T2I)\ngeneration models without further training. Experiment results demonstrate the\nsuperiority of our proposed CountDiffusion, which improves the accurate object\nquantity generation ability of T2I models by a large margin.\n", "link": "http://arxiv.org/abs/2505.04347v1", "date": "2025-05-07", "relevancy": 1.9475, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6722}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6431}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CountDiffusion%3A%20Text-to-Image%20Synthesis%20with%20Training-Free%0A%20%20Counting-Guidance%20Diffusion&body=Title%3A%20CountDiffusion%3A%20Text-to-Image%20Synthesis%20with%20Training-Free%0A%20%20Counting-Guidance%20Diffusion%0AAuthor%3A%20Yanyu%20Li%20and%20Pencheng%20Wan%20and%20Liang%20Han%20and%20Yaowei%20Wang%20and%20Liqiang%20Nie%20and%20Min%20Zhang%0AAbstract%3A%20%20%20Stable%20Diffusion%20has%20advanced%20text-to-image%20synthesis%2C%20but%20training%20models%20to%0Agenerate%20images%20with%20accurate%20object%20quantity%20is%20still%20difficult%20due%20to%20the%0Ahigh%20computational%20cost%20and%20the%20challenge%20of%20teaching%20models%20the%20abstract%0Aconcept%20of%20quantity.%20In%20this%20paper%2C%20we%20propose%20CountDiffusion%2C%20a%20training-free%0Aframework%20aiming%20at%20generating%20images%20with%20correct%20object%20quantity%20from%20textual%0Adescriptions.%20CountDiffusion%20consists%20of%20two%20stages.%20In%20the%20first%20stage%2C%20an%0Aintermediate%20denoising%20result%20is%20generated%20by%20the%20diffusion%20model%20to%20predict%0Athe%20final%20synthesized%20image%20with%20one-step%20denoising%2C%20and%20a%20counting%20model%20is%0Aused%20to%20count%20the%20number%20of%20objects%20in%20this%20image.%20In%20the%20second%20stage%2C%20a%0Acorrection%20module%20is%20used%20to%20correct%20the%20object%20quantity%20by%20changing%20the%0Aattention%20map%20of%20the%20object%20with%20universal%20guidance.%20The%20proposed%0ACountDiffusion%20can%20be%20plugged%20into%20any%20diffusion-based%20text-to-image%20%28T2I%29%0Ageneration%20models%20without%20further%20training.%20Experiment%20results%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20CountDiffusion%2C%20which%20improves%20the%20accurate%20object%0Aquantity%20generation%20ability%20of%20T2I%20models%20by%20a%20large%20margin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountDiffusion%253A%2520Text-to-Image%2520Synthesis%2520with%2520Training-Free%250A%2520%2520Counting-Guidance%2520Diffusion%26entry.906535625%3DYanyu%2520Li%2520and%2520Pencheng%2520Wan%2520and%2520Liang%2520Han%2520and%2520Yaowei%2520Wang%2520and%2520Liqiang%2520Nie%2520and%2520Min%2520Zhang%26entry.1292438233%3D%2520%2520Stable%2520Diffusion%2520has%2520advanced%2520text-to-image%2520synthesis%252C%2520but%2520training%2520models%2520to%250Agenerate%2520images%2520with%2520accurate%2520object%2520quantity%2520is%2520still%2520difficult%2520due%2520to%2520the%250Ahigh%2520computational%2520cost%2520and%2520the%2520challenge%2520of%2520teaching%2520models%2520the%2520abstract%250Aconcept%2520of%2520quantity.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CountDiffusion%252C%2520a%2520training-free%250Aframework%2520aiming%2520at%2520generating%2520images%2520with%2520correct%2520object%2520quantity%2520from%2520textual%250Adescriptions.%2520CountDiffusion%2520consists%2520of%2520two%2520stages.%2520In%2520the%2520first%2520stage%252C%2520an%250Aintermediate%2520denoising%2520result%2520is%2520generated%2520by%2520the%2520diffusion%2520model%2520to%2520predict%250Athe%2520final%2520synthesized%2520image%2520with%2520one-step%2520denoising%252C%2520and%2520a%2520counting%2520model%2520is%250Aused%2520to%2520count%2520the%2520number%2520of%2520objects%2520in%2520this%2520image.%2520In%2520the%2520second%2520stage%252C%2520a%250Acorrection%2520module%2520is%2520used%2520to%2520correct%2520the%2520object%2520quantity%2520by%2520changing%2520the%250Aattention%2520map%2520of%2520the%2520object%2520with%2520universal%2520guidance.%2520The%2520proposed%250ACountDiffusion%2520can%2520be%2520plugged%2520into%2520any%2520diffusion-based%2520text-to-image%2520%2528T2I%2529%250Ageneration%2520models%2520without%2520further%2520training.%2520Experiment%2520results%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520proposed%2520CountDiffusion%252C%2520which%2520improves%2520the%2520accurate%2520object%250Aquantity%2520generation%2520ability%2520of%2520T2I%2520models%2520by%2520a%2520large%2520margin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CountDiffusion%3A%20Text-to-Image%20Synthesis%20with%20Training-Free%0A%20%20Counting-Guidance%20Diffusion&entry.906535625=Yanyu%20Li%20and%20Pencheng%20Wan%20and%20Liang%20Han%20and%20Yaowei%20Wang%20and%20Liqiang%20Nie%20and%20Min%20Zhang&entry.1292438233=%20%20Stable%20Diffusion%20has%20advanced%20text-to-image%20synthesis%2C%20but%20training%20models%20to%0Agenerate%20images%20with%20accurate%20object%20quantity%20is%20still%20difficult%20due%20to%20the%0Ahigh%20computational%20cost%20and%20the%20challenge%20of%20teaching%20models%20the%20abstract%0Aconcept%20of%20quantity.%20In%20this%20paper%2C%20we%20propose%20CountDiffusion%2C%20a%20training-free%0Aframework%20aiming%20at%20generating%20images%20with%20correct%20object%20quantity%20from%20textual%0Adescriptions.%20CountDiffusion%20consists%20of%20two%20stages.%20In%20the%20first%20stage%2C%20an%0Aintermediate%20denoising%20result%20is%20generated%20by%20the%20diffusion%20model%20to%20predict%0Athe%20final%20synthesized%20image%20with%20one-step%20denoising%2C%20and%20a%20counting%20model%20is%0Aused%20to%20count%20the%20number%20of%20objects%20in%20this%20image.%20In%20the%20second%20stage%2C%20a%0Acorrection%20module%20is%20used%20to%20correct%20the%20object%20quantity%20by%20changing%20the%0Aattention%20map%20of%20the%20object%20with%20universal%20guidance.%20The%20proposed%0ACountDiffusion%20can%20be%20plugged%20into%20any%20diffusion-based%20text-to-image%20%28T2I%29%0Ageneration%20models%20without%20further%20training.%20Experiment%20results%20demonstrate%20the%0Asuperiority%20of%20our%20proposed%20CountDiffusion%2C%20which%20improves%20the%20accurate%20object%0Aquantity%20generation%20ability%20of%20T2I%20models%20by%20a%20large%20margin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04347v1&entry.124074799=Read"},
{"title": "Spectral and Temporal Denoising for Differentially Private Optimization", "author": "Hyeju Shin and Kyudan Jung and Seongwon Yun and Juyoung Yun", "abstract": "  This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a\ndifferentially private optimization method that addresses the challenge of\npreserving performance in DP-SGD, where added noise typically degrades model\nutility. FFTKF integrates frequency-domain noise shaping with Kalman filtering\nto enhance gradient quality while preserving $(\\varepsilon, \\delta)$-DP\nguarantees. It employs a high-frequency shaping mask in the Fourier domain to\nconcentrate differential privacy noise in less informative spectral components,\npreserving low-frequency gradient signals. A scalar-gain Kalman filter with\nfinite-difference Hessian approximation further refines the denoised gradients.\nWith a per-iteration complexity of $\\mathcal{O}(d \\log d)$, FFTKF demonstrates\nimproved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,\nand Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.\nTheoretical analysis confirms that FFTKF maintains equivalent privacy\nguarantees while achieving a tighter privacy-utility trade-off through reduced\nnoise and controlled bias.\n", "link": "http://arxiv.org/abs/2505.04468v1", "date": "2025-05-07", "relevancy": 1.9412, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.494}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4821}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20and%20Temporal%20Denoising%20for%20Differentially%20Private%20Optimization&body=Title%3A%20Spectral%20and%20Temporal%20Denoising%20for%20Differentially%20Private%20Optimization%0AAuthor%3A%20Hyeju%20Shin%20and%20Kyudan%20Jung%20and%20Seongwon%20Yun%20and%20Juyoung%20Yun%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20FFT-Enhanced%20Kalman%20Filter%20%28FFTKF%29%2C%20a%0Adifferentially%20private%20optimization%20method%20that%20addresses%20the%20challenge%20of%0Apreserving%20performance%20in%20DP-SGD%2C%20where%20added%20noise%20typically%20degrades%20model%0Autility.%20FFTKF%20integrates%20frequency-domain%20noise%20shaping%20with%20Kalman%20filtering%0Ato%20enhance%20gradient%20quality%20while%20preserving%20%24%28%5Cvarepsilon%2C%20%5Cdelta%29%24-DP%0Aguarantees.%20It%20employs%20a%20high-frequency%20shaping%20mask%20in%20the%20Fourier%20domain%20to%0Aconcentrate%20differential%20privacy%20noise%20in%20less%20informative%20spectral%20components%2C%0Apreserving%20low-frequency%20gradient%20signals.%20A%20scalar-gain%20Kalman%20filter%20with%0Afinite-difference%20Hessian%20approximation%20further%20refines%20the%20denoised%20gradients.%0AWith%20a%20per-iteration%20complexity%20of%20%24%5Cmathcal%7BO%7D%28d%20%5Clog%20d%29%24%2C%20FFTKF%20demonstrates%0Aimproved%20test%20accuracy%20over%20DP-SGD%20and%20DiSK%20across%20MNIST%2C%20CIFAR-10%2C%20CIFAR-100%2C%0Aand%20Tiny-ImageNet%20datasets%20using%20CNNs%2C%20Wide%20ResNets%2C%20and%20Vision%20Transformers.%0ATheoretical%20analysis%20confirms%20that%20FFTKF%20maintains%20equivalent%20privacy%0Aguarantees%20while%20achieving%20a%20tighter%20privacy-utility%20trade-off%20through%20reduced%0Anoise%20and%20controlled%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520and%2520Temporal%2520Denoising%2520for%2520Differentially%2520Private%2520Optimization%26entry.906535625%3DHyeju%2520Shin%2520and%2520Kyudan%2520Jung%2520and%2520Seongwon%2520Yun%2520and%2520Juyoung%2520Yun%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520FFT-Enhanced%2520Kalman%2520Filter%2520%2528FFTKF%2529%252C%2520a%250Adifferentially%2520private%2520optimization%2520method%2520that%2520addresses%2520the%2520challenge%2520of%250Apreserving%2520performance%2520in%2520DP-SGD%252C%2520where%2520added%2520noise%2520typically%2520degrades%2520model%250Autility.%2520FFTKF%2520integrates%2520frequency-domain%2520noise%2520shaping%2520with%2520Kalman%2520filtering%250Ato%2520enhance%2520gradient%2520quality%2520while%2520preserving%2520%2524%2528%255Cvarepsilon%252C%2520%255Cdelta%2529%2524-DP%250Aguarantees.%2520It%2520employs%2520a%2520high-frequency%2520shaping%2520mask%2520in%2520the%2520Fourier%2520domain%2520to%250Aconcentrate%2520differential%2520privacy%2520noise%2520in%2520less%2520informative%2520spectral%2520components%252C%250Apreserving%2520low-frequency%2520gradient%2520signals.%2520A%2520scalar-gain%2520Kalman%2520filter%2520with%250Afinite-difference%2520Hessian%2520approximation%2520further%2520refines%2520the%2520denoised%2520gradients.%250AWith%2520a%2520per-iteration%2520complexity%2520of%2520%2524%255Cmathcal%257BO%257D%2528d%2520%255Clog%2520d%2529%2524%252C%2520FFTKF%2520demonstrates%250Aimproved%2520test%2520accuracy%2520over%2520DP-SGD%2520and%2520DiSK%2520across%2520MNIST%252C%2520CIFAR-10%252C%2520CIFAR-100%252C%250Aand%2520Tiny-ImageNet%2520datasets%2520using%2520CNNs%252C%2520Wide%2520ResNets%252C%2520and%2520Vision%2520Transformers.%250ATheoretical%2520analysis%2520confirms%2520that%2520FFTKF%2520maintains%2520equivalent%2520privacy%250Aguarantees%2520while%2520achieving%2520a%2520tighter%2520privacy-utility%2520trade-off%2520through%2520reduced%250Anoise%2520and%2520controlled%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20and%20Temporal%20Denoising%20for%20Differentially%20Private%20Optimization&entry.906535625=Hyeju%20Shin%20and%20Kyudan%20Jung%20and%20Seongwon%20Yun%20and%20Juyoung%20Yun&entry.1292438233=%20%20This%20paper%20introduces%20the%20FFT-Enhanced%20Kalman%20Filter%20%28FFTKF%29%2C%20a%0Adifferentially%20private%20optimization%20method%20that%20addresses%20the%20challenge%20of%0Apreserving%20performance%20in%20DP-SGD%2C%20where%20added%20noise%20typically%20degrades%20model%0Autility.%20FFTKF%20integrates%20frequency-domain%20noise%20shaping%20with%20Kalman%20filtering%0Ato%20enhance%20gradient%20quality%20while%20preserving%20%24%28%5Cvarepsilon%2C%20%5Cdelta%29%24-DP%0Aguarantees.%20It%20employs%20a%20high-frequency%20shaping%20mask%20in%20the%20Fourier%20domain%20to%0Aconcentrate%20differential%20privacy%20noise%20in%20less%20informative%20spectral%20components%2C%0Apreserving%20low-frequency%20gradient%20signals.%20A%20scalar-gain%20Kalman%20filter%20with%0Afinite-difference%20Hessian%20approximation%20further%20refines%20the%20denoised%20gradients.%0AWith%20a%20per-iteration%20complexity%20of%20%24%5Cmathcal%7BO%7D%28d%20%5Clog%20d%29%24%2C%20FFTKF%20demonstrates%0Aimproved%20test%20accuracy%20over%20DP-SGD%20and%20DiSK%20across%20MNIST%2C%20CIFAR-10%2C%20CIFAR-100%2C%0Aand%20Tiny-ImageNet%20datasets%20using%20CNNs%2C%20Wide%20ResNets%2C%20and%20Vision%20Transformers.%0ATheoretical%20analysis%20confirms%20that%20FFTKF%20maintains%20equivalent%20privacy%0Aguarantees%20while%20achieving%20a%20tighter%20privacy-utility%20trade-off%20through%20reduced%0Anoise%20and%20controlled%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04468v1&entry.124074799=Read"},
{"title": "Edge-GPU Based Face Tracking for Face Detection and Recognition\n  Acceleration", "author": "Asma Baobaid and Mahmoud Meribout", "abstract": "  Cost-effective machine vision systems dedicated to real-time and accurate\nface detection and recognition in public places are crucial for many modern\napplications. However, despite their high performance, which could be reached\nusing specialized edge or cloud AI hardware accelerators, there is still room\nfor improvement in throughput and power consumption. This paper aims to suggest\na combined hardware-software approach that optimizes face detection and\nrecognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX\nOrin. First, it leverages the simultaneous usage of all its hardware engines to\nimprove processing time. This offers an improvement over previous works where\nthese tasks were mainly allocated automatically and exclusively to the CPU or,\nto a higher extent, to the GPU core. Additionally, the paper suggests\nintegrating a face tracker module to avoid redundantly running the face\nrecognition algorithm for every frame but only when a new face appears in the\nscene. The results of extended experiments suggest that simultaneous usage of\nall the hardware engines that are available in the Orin GPU and tracker\nintegration into the pipeline yield an impressive throughput of 290 FPS (frames\nper second) on 1920 x 1080 input size frames containing in average of 6\nfaces/frame. Additionally, a substantial saving of power consumption of around\n800 mW was achieved when compared to running the task on the CPU/GPU engines\nonly and without integrating a tracker into the Orin GPU\\'92s pipeline. This\nhardware-codesign approach can pave the way to design high-performance machine\nvision systems at the edge, critically needed in video monitoring in public\nplaces where several nearby cameras are usually deployed for a same scene.\n", "link": "http://arxiv.org/abs/2505.04524v1", "date": "2025-05-07", "relevancy": 1.9369, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5079}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4792}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge-GPU%20Based%20Face%20Tracking%20for%20Face%20Detection%20and%20Recognition%0A%20%20Acceleration&body=Title%3A%20Edge-GPU%20Based%20Face%20Tracking%20for%20Face%20Detection%20and%20Recognition%0A%20%20Acceleration%0AAuthor%3A%20Asma%20Baobaid%20and%20Mahmoud%20Meribout%0AAbstract%3A%20%20%20Cost-effective%20machine%20vision%20systems%20dedicated%20to%20real-time%20and%20accurate%0Aface%20detection%20and%20recognition%20in%20public%20places%20are%20crucial%20for%20many%20modern%0Aapplications.%20However%2C%20despite%20their%20high%20performance%2C%20which%20could%20be%20reached%0Ausing%20specialized%20edge%20or%20cloud%20AI%20hardware%20accelerators%2C%20there%20is%20still%20room%0Afor%20improvement%20in%20throughput%20and%20power%20consumption.%20This%20paper%20aims%20to%20suggest%0Aa%20combined%20hardware-software%20approach%20that%20optimizes%20face%20detection%20and%0Arecognition%20systems%20on%20one%20of%20the%20latest%20edge%20GPUs%2C%20namely%20NVIDIA%20Jetson%20AGX%0AOrin.%20First%2C%20it%20leverages%20the%20simultaneous%20usage%20of%20all%20its%20hardware%20engines%20to%0Aimprove%20processing%20time.%20This%20offers%20an%20improvement%20over%20previous%20works%20where%0Athese%20tasks%20were%20mainly%20allocated%20automatically%20and%20exclusively%20to%20the%20CPU%20or%2C%0Ato%20a%20higher%20extent%2C%20to%20the%20GPU%20core.%20Additionally%2C%20the%20paper%20suggests%0Aintegrating%20a%20face%20tracker%20module%20to%20avoid%20redundantly%20running%20the%20face%0Arecognition%20algorithm%20for%20every%20frame%20but%20only%20when%20a%20new%20face%20appears%20in%20the%0Ascene.%20The%20results%20of%20extended%20experiments%20suggest%20that%20simultaneous%20usage%20of%0Aall%20the%20hardware%20engines%20that%20are%20available%20in%20the%20Orin%20GPU%20and%20tracker%0Aintegration%20into%20the%20pipeline%20yield%20an%20impressive%20throughput%20of%20290%20FPS%20%28frames%0Aper%20second%29%20on%201920%20x%201080%20input%20size%20frames%20containing%20in%20average%20of%206%0Afaces/frame.%20Additionally%2C%20a%20substantial%20saving%20of%20power%20consumption%20of%20around%0A800%20mW%20was%20achieved%20when%20compared%20to%20running%20the%20task%20on%20the%20CPU/GPU%20engines%0Aonly%20and%20without%20integrating%20a%20tracker%20into%20the%20Orin%20GPU%5C%2792s%20pipeline.%20This%0Ahardware-codesign%20approach%20can%20pave%20the%20way%20to%20design%20high-performance%20machine%0Avision%20systems%20at%20the%20edge%2C%20critically%20needed%20in%20video%20monitoring%20in%20public%0Aplaces%20where%20several%20nearby%20cameras%20are%20usually%20deployed%20for%20a%20same%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge-GPU%2520Based%2520Face%2520Tracking%2520for%2520Face%2520Detection%2520and%2520Recognition%250A%2520%2520Acceleration%26entry.906535625%3DAsma%2520Baobaid%2520and%2520Mahmoud%2520Meribout%26entry.1292438233%3D%2520%2520Cost-effective%2520machine%2520vision%2520systems%2520dedicated%2520to%2520real-time%2520and%2520accurate%250Aface%2520detection%2520and%2520recognition%2520in%2520public%2520places%2520are%2520crucial%2520for%2520many%2520modern%250Aapplications.%2520However%252C%2520despite%2520their%2520high%2520performance%252C%2520which%2520could%2520be%2520reached%250Ausing%2520specialized%2520edge%2520or%2520cloud%2520AI%2520hardware%2520accelerators%252C%2520there%2520is%2520still%2520room%250Afor%2520improvement%2520in%2520throughput%2520and%2520power%2520consumption.%2520This%2520paper%2520aims%2520to%2520suggest%250Aa%2520combined%2520hardware-software%2520approach%2520that%2520optimizes%2520face%2520detection%2520and%250Arecognition%2520systems%2520on%2520one%2520of%2520the%2520latest%2520edge%2520GPUs%252C%2520namely%2520NVIDIA%2520Jetson%2520AGX%250AOrin.%2520First%252C%2520it%2520leverages%2520the%2520simultaneous%2520usage%2520of%2520all%2520its%2520hardware%2520engines%2520to%250Aimprove%2520processing%2520time.%2520This%2520offers%2520an%2520improvement%2520over%2520previous%2520works%2520where%250Athese%2520tasks%2520were%2520mainly%2520allocated%2520automatically%2520and%2520exclusively%2520to%2520the%2520CPU%2520or%252C%250Ato%2520a%2520higher%2520extent%252C%2520to%2520the%2520GPU%2520core.%2520Additionally%252C%2520the%2520paper%2520suggests%250Aintegrating%2520a%2520face%2520tracker%2520module%2520to%2520avoid%2520redundantly%2520running%2520the%2520face%250Arecognition%2520algorithm%2520for%2520every%2520frame%2520but%2520only%2520when%2520a%2520new%2520face%2520appears%2520in%2520the%250Ascene.%2520The%2520results%2520of%2520extended%2520experiments%2520suggest%2520that%2520simultaneous%2520usage%2520of%250Aall%2520the%2520hardware%2520engines%2520that%2520are%2520available%2520in%2520the%2520Orin%2520GPU%2520and%2520tracker%250Aintegration%2520into%2520the%2520pipeline%2520yield%2520an%2520impressive%2520throughput%2520of%2520290%2520FPS%2520%2528frames%250Aper%2520second%2529%2520on%25201920%2520x%25201080%2520input%2520size%2520frames%2520containing%2520in%2520average%2520of%25206%250Afaces/frame.%2520Additionally%252C%2520a%2520substantial%2520saving%2520of%2520power%2520consumption%2520of%2520around%250A800%2520mW%2520was%2520achieved%2520when%2520compared%2520to%2520running%2520the%2520task%2520on%2520the%2520CPU/GPU%2520engines%250Aonly%2520and%2520without%2520integrating%2520a%2520tracker%2520into%2520the%2520Orin%2520GPU%255C%252792s%2520pipeline.%2520This%250Ahardware-codesign%2520approach%2520can%2520pave%2520the%2520way%2520to%2520design%2520high-performance%2520machine%250Avision%2520systems%2520at%2520the%2520edge%252C%2520critically%2520needed%2520in%2520video%2520monitoring%2520in%2520public%250Aplaces%2520where%2520several%2520nearby%2520cameras%2520are%2520usually%2520deployed%2520for%2520a%2520same%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge-GPU%20Based%20Face%20Tracking%20for%20Face%20Detection%20and%20Recognition%0A%20%20Acceleration&entry.906535625=Asma%20Baobaid%20and%20Mahmoud%20Meribout&entry.1292438233=%20%20Cost-effective%20machine%20vision%20systems%20dedicated%20to%20real-time%20and%20accurate%0Aface%20detection%20and%20recognition%20in%20public%20places%20are%20crucial%20for%20many%20modern%0Aapplications.%20However%2C%20despite%20their%20high%20performance%2C%20which%20could%20be%20reached%0Ausing%20specialized%20edge%20or%20cloud%20AI%20hardware%20accelerators%2C%20there%20is%20still%20room%0Afor%20improvement%20in%20throughput%20and%20power%20consumption.%20This%20paper%20aims%20to%20suggest%0Aa%20combined%20hardware-software%20approach%20that%20optimizes%20face%20detection%20and%0Arecognition%20systems%20on%20one%20of%20the%20latest%20edge%20GPUs%2C%20namely%20NVIDIA%20Jetson%20AGX%0AOrin.%20First%2C%20it%20leverages%20the%20simultaneous%20usage%20of%20all%20its%20hardware%20engines%20to%0Aimprove%20processing%20time.%20This%20offers%20an%20improvement%20over%20previous%20works%20where%0Athese%20tasks%20were%20mainly%20allocated%20automatically%20and%20exclusively%20to%20the%20CPU%20or%2C%0Ato%20a%20higher%20extent%2C%20to%20the%20GPU%20core.%20Additionally%2C%20the%20paper%20suggests%0Aintegrating%20a%20face%20tracker%20module%20to%20avoid%20redundantly%20running%20the%20face%0Arecognition%20algorithm%20for%20every%20frame%20but%20only%20when%20a%20new%20face%20appears%20in%20the%0Ascene.%20The%20results%20of%20extended%20experiments%20suggest%20that%20simultaneous%20usage%20of%0Aall%20the%20hardware%20engines%20that%20are%20available%20in%20the%20Orin%20GPU%20and%20tracker%0Aintegration%20into%20the%20pipeline%20yield%20an%20impressive%20throughput%20of%20290%20FPS%20%28frames%0Aper%20second%29%20on%201920%20x%201080%20input%20size%20frames%20containing%20in%20average%20of%206%0Afaces/frame.%20Additionally%2C%20a%20substantial%20saving%20of%20power%20consumption%20of%20around%0A800%20mW%20was%20achieved%20when%20compared%20to%20running%20the%20task%20on%20the%20CPU/GPU%20engines%0Aonly%20and%20without%20integrating%20a%20tracker%20into%20the%20Orin%20GPU%5C%2792s%20pipeline.%20This%0Ahardware-codesign%20approach%20can%20pave%20the%20way%20to%20design%20high-performance%20machine%0Avision%20systems%20at%20the%20edge%2C%20critically%20needed%20in%20video%20monitoring%20in%20public%0Aplaces%20where%20several%20nearby%20cameras%20are%20usually%20deployed%20for%20a%20same%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04524v1&entry.124074799=Read"},
{"title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text\n  Classification", "author": "Junichiro Niimi", "abstract": "  With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.\n", "link": "http://arxiv.org/abs/2504.18884v2", "date": "2025-05-07", "relevancy": 1.9354, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4952}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4841}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Ensemble%20Strategy%20for%20LLM%20Inference%3A%20Towards%20More%20Stable%20Text%0A%20%20Classification&body=Title%3A%20A%20Simple%20Ensemble%20Strategy%20for%20LLM%20Inference%3A%20Towards%20More%20Stable%20Text%0A%20%20Classification%0AAuthor%3A%20Junichiro%20Niimi%0AAbstract%3A%20%20%20With%20the%20advance%20of%20large%20language%20models%20%28LLMs%29%2C%20LLMs%20have%20been%20utilized%20for%0Athe%20various%20tasks.%20However%2C%20the%20issues%20of%20variability%20and%20reproducibility%20of%0Aresults%20from%20each%20trial%20of%20LLMs%20have%20been%20largely%20overlooked%20in%20existing%0Aliterature%20while%20actual%20human%20annotation%20uses%20majority%20voting%20to%20resolve%0Adisagreements%20among%20annotators.%20Therefore%2C%20this%20study%20introduces%20the%0Astraightforward%20ensemble%20strategy%20to%20a%20sentiment%20analysis%20using%20LLMs.%20As%20the%0Aresults%2C%20we%20demonstrate%20that%20the%20ensemble%20of%20multiple%20inference%20using%0Amedium-sized%20LLMs%20produces%20more%20robust%20and%20accurate%20results%20than%20using%20a%20large%0Amodel%20with%20a%20single%20attempt%20with%20reducing%20RMSE%20by%2018.6%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Ensemble%2520Strategy%2520for%2520LLM%2520Inference%253A%2520Towards%2520More%2520Stable%2520Text%250A%2520%2520Classification%26entry.906535625%3DJunichiro%2520Niimi%26entry.1292438233%3D%2520%2520With%2520the%2520advance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520LLMs%2520have%2520been%2520utilized%2520for%250Athe%2520various%2520tasks.%2520However%252C%2520the%2520issues%2520of%2520variability%2520and%2520reproducibility%2520of%250Aresults%2520from%2520each%2520trial%2520of%2520LLMs%2520have%2520been%2520largely%2520overlooked%2520in%2520existing%250Aliterature%2520while%2520actual%2520human%2520annotation%2520uses%2520majority%2520voting%2520to%2520resolve%250Adisagreements%2520among%2520annotators.%2520Therefore%252C%2520this%2520study%2520introduces%2520the%250Astraightforward%2520ensemble%2520strategy%2520to%2520a%2520sentiment%2520analysis%2520using%2520LLMs.%2520As%2520the%250Aresults%252C%2520we%2520demonstrate%2520that%2520the%2520ensemble%2520of%2520multiple%2520inference%2520using%250Amedium-sized%2520LLMs%2520produces%2520more%2520robust%2520and%2520accurate%2520results%2520than%2520using%2520a%2520large%250Amodel%2520with%2520a%2520single%2520attempt%2520with%2520reducing%2520RMSE%2520by%252018.6%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Ensemble%20Strategy%20for%20LLM%20Inference%3A%20Towards%20More%20Stable%20Text%0A%20%20Classification&entry.906535625=Junichiro%20Niimi&entry.1292438233=%20%20With%20the%20advance%20of%20large%20language%20models%20%28LLMs%29%2C%20LLMs%20have%20been%20utilized%20for%0Athe%20various%20tasks.%20However%2C%20the%20issues%20of%20variability%20and%20reproducibility%20of%0Aresults%20from%20each%20trial%20of%20LLMs%20have%20been%20largely%20overlooked%20in%20existing%0Aliterature%20while%20actual%20human%20annotation%20uses%20majority%20voting%20to%20resolve%0Adisagreements%20among%20annotators.%20Therefore%2C%20this%20study%20introduces%20the%0Astraightforward%20ensemble%20strategy%20to%20a%20sentiment%20analysis%20using%20LLMs.%20As%20the%0Aresults%2C%20we%20demonstrate%20that%20the%20ensemble%20of%20multiple%20inference%20using%0Amedium-sized%20LLMs%20produces%20more%20robust%20and%20accurate%20results%20than%20using%20a%20large%0Amodel%20with%20a%20single%20attempt%20with%20reducing%20RMSE%20by%2018.6%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18884v2&entry.124074799=Read"},
{"title": "mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech\n  Recognition", "author": "Andrew Rouditchenko and Samuel Thomas and Hilde Kuehne and Rogerio Feris and James Glass", "abstract": "  Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio\nand can improve performance in noise, but most methods are trained only on\nEnglish data. One limitation is the lack of large-scale multilingual video\ndata, which makes it hard to train models from scratch. In this work, we\npropose mWhisper-Flamingo for multilingual AVSR which combines the strengths of\na pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable\nbetter multi-modal integration and improve the noisy multilingual performance,\nwe introduce decoder modality dropout where the model is trained both on paired\naudio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo\nachieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.\nAudio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on\nall languages in noisy conditions.\n", "link": "http://arxiv.org/abs/2502.01547v3", "date": "2025-05-07", "relevancy": 1.9318, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4789}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mWhisper-Flamingo%20for%20Multilingual%20Audio-Visual%20Noise-Robust%20Speech%0A%20%20Recognition&body=Title%3A%20mWhisper-Flamingo%20for%20Multilingual%20Audio-Visual%20Noise-Robust%20Speech%0A%20%20Recognition%0AAuthor%3A%20Andrew%20Rouditchenko%20and%20Samuel%20Thomas%20and%20Hilde%20Kuehne%20and%20Rogerio%20Feris%20and%20James%20Glass%0AAbstract%3A%20%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20combines%20lip-based%20video%20with%20audio%0Aand%20can%20improve%20performance%20in%20noise%2C%20but%20most%20methods%20are%20trained%20only%20on%0AEnglish%20data.%20One%20limitation%20is%20the%20lack%20of%20large-scale%20multilingual%20video%0Adata%2C%20which%20makes%20it%20hard%20to%20train%20models%20from%20scratch.%20In%20this%20work%2C%20we%0Apropose%20mWhisper-Flamingo%20for%20multilingual%20AVSR%20which%20combines%20the%20strengths%20of%0Aa%20pre-trained%20audio%20model%20%28Whisper%29%20and%20video%20model%20%28AV-HuBERT%29.%20To%20enable%0Abetter%20multi-modal%20integration%20and%20improve%20the%20noisy%20multilingual%20performance%2C%0Awe%20introduce%20decoder%20modality%20dropout%20where%20the%20model%20is%20trained%20both%20on%20paired%0Aaudio-visual%20inputs%20and%20separate%20audio/visual%20inputs.%20mWhisper-Flamingo%0Aachieves%20state-of-the-art%20WER%20on%20MuAViC%2C%20an%20AVSR%20dataset%20of%209%20languages.%0AAudio-visual%20mWhisper-Flamingo%20consistently%20outperforms%20audio-only%20Whisper%20on%0Aall%20languages%20in%20noisy%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01547v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DmWhisper-Flamingo%2520for%2520Multilingual%2520Audio-Visual%2520Noise-Robust%2520Speech%250A%2520%2520Recognition%26entry.906535625%3DAndrew%2520Rouditchenko%2520and%2520Samuel%2520Thomas%2520and%2520Hilde%2520Kuehne%2520and%2520Rogerio%2520Feris%2520and%2520James%2520Glass%26entry.1292438233%3D%2520%2520Audio-Visual%2520Speech%2520Recognition%2520%2528AVSR%2529%2520combines%2520lip-based%2520video%2520with%2520audio%250Aand%2520can%2520improve%2520performance%2520in%2520noise%252C%2520but%2520most%2520methods%2520are%2520trained%2520only%2520on%250AEnglish%2520data.%2520One%2520limitation%2520is%2520the%2520lack%2520of%2520large-scale%2520multilingual%2520video%250Adata%252C%2520which%2520makes%2520it%2520hard%2520to%2520train%2520models%2520from%2520scratch.%2520In%2520this%2520work%252C%2520we%250Apropose%2520mWhisper-Flamingo%2520for%2520multilingual%2520AVSR%2520which%2520combines%2520the%2520strengths%2520of%250Aa%2520pre-trained%2520audio%2520model%2520%2528Whisper%2529%2520and%2520video%2520model%2520%2528AV-HuBERT%2529.%2520To%2520enable%250Abetter%2520multi-modal%2520integration%2520and%2520improve%2520the%2520noisy%2520multilingual%2520performance%252C%250Awe%2520introduce%2520decoder%2520modality%2520dropout%2520where%2520the%2520model%2520is%2520trained%2520both%2520on%2520paired%250Aaudio-visual%2520inputs%2520and%2520separate%2520audio/visual%2520inputs.%2520mWhisper-Flamingo%250Aachieves%2520state-of-the-art%2520WER%2520on%2520MuAViC%252C%2520an%2520AVSR%2520dataset%2520of%25209%2520languages.%250AAudio-visual%2520mWhisper-Flamingo%2520consistently%2520outperforms%2520audio-only%2520Whisper%2520on%250Aall%2520languages%2520in%2520noisy%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01547v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mWhisper-Flamingo%20for%20Multilingual%20Audio-Visual%20Noise-Robust%20Speech%0A%20%20Recognition&entry.906535625=Andrew%20Rouditchenko%20and%20Samuel%20Thomas%20and%20Hilde%20Kuehne%20and%20Rogerio%20Feris%20and%20James%20Glass&entry.1292438233=%20%20Audio-Visual%20Speech%20Recognition%20%28AVSR%29%20combines%20lip-based%20video%20with%20audio%0Aand%20can%20improve%20performance%20in%20noise%2C%20but%20most%20methods%20are%20trained%20only%20on%0AEnglish%20data.%20One%20limitation%20is%20the%20lack%20of%20large-scale%20multilingual%20video%0Adata%2C%20which%20makes%20it%20hard%20to%20train%20models%20from%20scratch.%20In%20this%20work%2C%20we%0Apropose%20mWhisper-Flamingo%20for%20multilingual%20AVSR%20which%20combines%20the%20strengths%20of%0Aa%20pre-trained%20audio%20model%20%28Whisper%29%20and%20video%20model%20%28AV-HuBERT%29.%20To%20enable%0Abetter%20multi-modal%20integration%20and%20improve%20the%20noisy%20multilingual%20performance%2C%0Awe%20introduce%20decoder%20modality%20dropout%20where%20the%20model%20is%20trained%20both%20on%20paired%0Aaudio-visual%20inputs%20and%20separate%20audio/visual%20inputs.%20mWhisper-Flamingo%0Aachieves%20state-of-the-art%20WER%20on%20MuAViC%2C%20an%20AVSR%20dataset%20of%209%20languages.%0AAudio-visual%20mWhisper-Flamingo%20consistently%20outperforms%20audio-only%20Whisper%20on%0Aall%20languages%20in%20noisy%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01547v3&entry.124074799=Read"},
{"title": "Sparsity is All You Need: Rethinking Biological Pathway-Informed\n  Approaches in Deep Learning", "author": "Isabella Caranzano and Corrado Pancotti and Cesare Rollo and Flavio Sartori and Pietro Li\u00f2 and Piero Fariselli and Tiziana Sanavia", "abstract": "  Biologically-informed neural networks typically leverage pathway annotations\nto enhance performance in biomedical applications. We hypothesized that the\nbenefits of pathway integration does not arise from its biological relevance,\nbut rather from the sparsity it introduces. We conducted a comprehensive\nanalysis of all relevant pathway-based neural network models for predictive\ntasks, critically evaluating each study's contributions. From this review, we\ncurated a subset of methods for which the source code was publicly available.\nThe comparison of the biologically informed state-of-the-art deep learning\nmodels and their randomized counterparts showed that models based on randomized\ninformation performed equally well as biologically informed ones across\ndifferent metrics and datasets. Notably, in 3 out of the 15 analyzed models,\nthe randomized versions even outperformed their biologically informed\ncounterparts. Moreover, pathway-informed models did not show any clear\nadvantage in interpretability, as randomized models were still able to identify\nrelevant disease biomarkers despite lacking explicit pathway information. Our\nfindings suggest that pathway annotations may be too noisy or inadequately\nexplored by current methods. Therefore, we propose a methodology that can be\napplied to different domains and can serve as a robust benchmark for\nsystematically comparing novel pathway-informed models against their randomized\ncounterparts. This approach enables researchers to rigorously determine whether\nobserved performance improvements can be attributed to biological insights.\n", "link": "http://arxiv.org/abs/2505.04300v1", "date": "2025-05-07", "relevancy": 1.9266, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsity%20is%20All%20You%20Need%3A%20Rethinking%20Biological%20Pathway-Informed%0A%20%20Approaches%20in%20Deep%20Learning&body=Title%3A%20Sparsity%20is%20All%20You%20Need%3A%20Rethinking%20Biological%20Pathway-Informed%0A%20%20Approaches%20in%20Deep%20Learning%0AAuthor%3A%20Isabella%20Caranzano%20and%20Corrado%20Pancotti%20and%20Cesare%20Rollo%20and%20Flavio%20Sartori%20and%20Pietro%20Li%C3%B2%20and%20Piero%20Fariselli%20and%20Tiziana%20Sanavia%0AAbstract%3A%20%20%20Biologically-informed%20neural%20networks%20typically%20leverage%20pathway%20annotations%0Ato%20enhance%20performance%20in%20biomedical%20applications.%20We%20hypothesized%20that%20the%0Abenefits%20of%20pathway%20integration%20does%20not%20arise%20from%20its%20biological%20relevance%2C%0Abut%20rather%20from%20the%20sparsity%20it%20introduces.%20We%20conducted%20a%20comprehensive%0Aanalysis%20of%20all%20relevant%20pathway-based%20neural%20network%20models%20for%20predictive%0Atasks%2C%20critically%20evaluating%20each%20study%27s%20contributions.%20From%20this%20review%2C%20we%0Acurated%20a%20subset%20of%20methods%20for%20which%20the%20source%20code%20was%20publicly%20available.%0AThe%20comparison%20of%20the%20biologically%20informed%20state-of-the-art%20deep%20learning%0Amodels%20and%20their%20randomized%20counterparts%20showed%20that%20models%20based%20on%20randomized%0Ainformation%20performed%20equally%20well%20as%20biologically%20informed%20ones%20across%0Adifferent%20metrics%20and%20datasets.%20Notably%2C%20in%203%20out%20of%20the%2015%20analyzed%20models%2C%0Athe%20randomized%20versions%20even%20outperformed%20their%20biologically%20informed%0Acounterparts.%20Moreover%2C%20pathway-informed%20models%20did%20not%20show%20any%20clear%0Aadvantage%20in%20interpretability%2C%20as%20randomized%20models%20were%20still%20able%20to%20identify%0Arelevant%20disease%20biomarkers%20despite%20lacking%20explicit%20pathway%20information.%20Our%0Afindings%20suggest%20that%20pathway%20annotations%20may%20be%20too%20noisy%20or%20inadequately%0Aexplored%20by%20current%20methods.%20Therefore%2C%20we%20propose%20a%20methodology%20that%20can%20be%0Aapplied%20to%20different%20domains%20and%20can%20serve%20as%20a%20robust%20benchmark%20for%0Asystematically%20comparing%20novel%20pathway-informed%20models%20against%20their%20randomized%0Acounterparts.%20This%20approach%20enables%20researchers%20to%20rigorously%20determine%20whether%0Aobserved%20performance%20improvements%20can%20be%20attributed%20to%20biological%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsity%2520is%2520All%2520You%2520Need%253A%2520Rethinking%2520Biological%2520Pathway-Informed%250A%2520%2520Approaches%2520in%2520Deep%2520Learning%26entry.906535625%3DIsabella%2520Caranzano%2520and%2520Corrado%2520Pancotti%2520and%2520Cesare%2520Rollo%2520and%2520Flavio%2520Sartori%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Piero%2520Fariselli%2520and%2520Tiziana%2520Sanavia%26entry.1292438233%3D%2520%2520Biologically-informed%2520neural%2520networks%2520typically%2520leverage%2520pathway%2520annotations%250Ato%2520enhance%2520performance%2520in%2520biomedical%2520applications.%2520We%2520hypothesized%2520that%2520the%250Abenefits%2520of%2520pathway%2520integration%2520does%2520not%2520arise%2520from%2520its%2520biological%2520relevance%252C%250Abut%2520rather%2520from%2520the%2520sparsity%2520it%2520introduces.%2520We%2520conducted%2520a%2520comprehensive%250Aanalysis%2520of%2520all%2520relevant%2520pathway-based%2520neural%2520network%2520models%2520for%2520predictive%250Atasks%252C%2520critically%2520evaluating%2520each%2520study%2527s%2520contributions.%2520From%2520this%2520review%252C%2520we%250Acurated%2520a%2520subset%2520of%2520methods%2520for%2520which%2520the%2520source%2520code%2520was%2520publicly%2520available.%250AThe%2520comparison%2520of%2520the%2520biologically%2520informed%2520state-of-the-art%2520deep%2520learning%250Amodels%2520and%2520their%2520randomized%2520counterparts%2520showed%2520that%2520models%2520based%2520on%2520randomized%250Ainformation%2520performed%2520equally%2520well%2520as%2520biologically%2520informed%2520ones%2520across%250Adifferent%2520metrics%2520and%2520datasets.%2520Notably%252C%2520in%25203%2520out%2520of%2520the%252015%2520analyzed%2520models%252C%250Athe%2520randomized%2520versions%2520even%2520outperformed%2520their%2520biologically%2520informed%250Acounterparts.%2520Moreover%252C%2520pathway-informed%2520models%2520did%2520not%2520show%2520any%2520clear%250Aadvantage%2520in%2520interpretability%252C%2520as%2520randomized%2520models%2520were%2520still%2520able%2520to%2520identify%250Arelevant%2520disease%2520biomarkers%2520despite%2520lacking%2520explicit%2520pathway%2520information.%2520Our%250Afindings%2520suggest%2520that%2520pathway%2520annotations%2520may%2520be%2520too%2520noisy%2520or%2520inadequately%250Aexplored%2520by%2520current%2520methods.%2520Therefore%252C%2520we%2520propose%2520a%2520methodology%2520that%2520can%2520be%250Aapplied%2520to%2520different%2520domains%2520and%2520can%2520serve%2520as%2520a%2520robust%2520benchmark%2520for%250Asystematically%2520comparing%2520novel%2520pathway-informed%2520models%2520against%2520their%2520randomized%250Acounterparts.%2520This%2520approach%2520enables%2520researchers%2520to%2520rigorously%2520determine%2520whether%250Aobserved%2520performance%2520improvements%2520can%2520be%2520attributed%2520to%2520biological%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsity%20is%20All%20You%20Need%3A%20Rethinking%20Biological%20Pathway-Informed%0A%20%20Approaches%20in%20Deep%20Learning&entry.906535625=Isabella%20Caranzano%20and%20Corrado%20Pancotti%20and%20Cesare%20Rollo%20and%20Flavio%20Sartori%20and%20Pietro%20Li%C3%B2%20and%20Piero%20Fariselli%20and%20Tiziana%20Sanavia&entry.1292438233=%20%20Biologically-informed%20neural%20networks%20typically%20leverage%20pathway%20annotations%0Ato%20enhance%20performance%20in%20biomedical%20applications.%20We%20hypothesized%20that%20the%0Abenefits%20of%20pathway%20integration%20does%20not%20arise%20from%20its%20biological%20relevance%2C%0Abut%20rather%20from%20the%20sparsity%20it%20introduces.%20We%20conducted%20a%20comprehensive%0Aanalysis%20of%20all%20relevant%20pathway-based%20neural%20network%20models%20for%20predictive%0Atasks%2C%20critically%20evaluating%20each%20study%27s%20contributions.%20From%20this%20review%2C%20we%0Acurated%20a%20subset%20of%20methods%20for%20which%20the%20source%20code%20was%20publicly%20available.%0AThe%20comparison%20of%20the%20biologically%20informed%20state-of-the-art%20deep%20learning%0Amodels%20and%20their%20randomized%20counterparts%20showed%20that%20models%20based%20on%20randomized%0Ainformation%20performed%20equally%20well%20as%20biologically%20informed%20ones%20across%0Adifferent%20metrics%20and%20datasets.%20Notably%2C%20in%203%20out%20of%20the%2015%20analyzed%20models%2C%0Athe%20randomized%20versions%20even%20outperformed%20their%20biologically%20informed%0Acounterparts.%20Moreover%2C%20pathway-informed%20models%20did%20not%20show%20any%20clear%0Aadvantage%20in%20interpretability%2C%20as%20randomized%20models%20were%20still%20able%20to%20identify%0Arelevant%20disease%20biomarkers%20despite%20lacking%20explicit%20pathway%20information.%20Our%0Afindings%20suggest%20that%20pathway%20annotations%20may%20be%20too%20noisy%20or%20inadequately%0Aexplored%20by%20current%20methods.%20Therefore%2C%20we%20propose%20a%20methodology%20that%20can%20be%0Aapplied%20to%20different%20domains%20and%20can%20serve%20as%20a%20robust%20benchmark%20for%0Asystematically%20comparing%20novel%20pathway-informed%20models%20against%20their%20randomized%0Acounterparts.%20This%20approach%20enables%20researchers%20to%20rigorously%20determine%20whether%0Aobserved%20performance%20improvements%20can%20be%20attributed%20to%20biological%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04300v1&entry.124074799=Read"},
{"title": "Testing Juntas Optimally with Samples", "author": "Lorenzo Beretta and Nathaniel Harms and Caleb Koch", "abstract": "  We prove tight upper and lower bounds of\n$\\Theta\\left(\\tfrac{1}{\\epsilon}\\left( \\sqrt{2^k \\log\\binom{n}{k} } +\n\\log\\binom{n}{k} \\right)\\right)$ on the number of samples required for\ndistribution-free $k$-junta testing. This is the first tight bound for testing\na natural class of Boolean functions in the distribution-free sample-based\nmodel. Our bounds also hold for the feature selection problem, showing that a\njunta tester must learn the set of relevant variables. For tolerant junta\ntesting, we prove a sample lower bound of $\\Omega(2^{(1-o(1)) k} +\n\\log\\binom{n}{k})$ showing that, unlike standard testing, there is no large gap\nbetween tolerant testing and learning.\n", "link": "http://arxiv.org/abs/2505.04604v1", "date": "2025-05-07", "relevancy": 1.9113, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4029}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.38}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Testing%20Juntas%20Optimally%20with%20Samples&body=Title%3A%20Testing%20Juntas%20Optimally%20with%20Samples%0AAuthor%3A%20Lorenzo%20Beretta%20and%20Nathaniel%20Harms%20and%20Caleb%20Koch%0AAbstract%3A%20%20%20We%20prove%20tight%20upper%20and%20lower%20bounds%20of%0A%24%5CTheta%5Cleft%28%5Ctfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%28%20%5Csqrt%7B2%5Ek%20%5Clog%5Cbinom%7Bn%7D%7Bk%7D%20%7D%20%2B%0A%5Clog%5Cbinom%7Bn%7D%7Bk%7D%20%5Cright%29%5Cright%29%24%20on%20the%20number%20of%20samples%20required%20for%0Adistribution-free%20%24k%24-junta%20testing.%20This%20is%20the%20first%20tight%20bound%20for%20testing%0Aa%20natural%20class%20of%20Boolean%20functions%20in%20the%20distribution-free%20sample-based%0Amodel.%20Our%20bounds%20also%20hold%20for%20the%20feature%20selection%20problem%2C%20showing%20that%20a%0Ajunta%20tester%20must%20learn%20the%20set%20of%20relevant%20variables.%20For%20tolerant%20junta%0Atesting%2C%20we%20prove%20a%20sample%20lower%20bound%20of%20%24%5COmega%282%5E%7B%281-o%281%29%29%20k%7D%20%2B%0A%5Clog%5Cbinom%7Bn%7D%7Bk%7D%29%24%20showing%20that%2C%20unlike%20standard%20testing%2C%20there%20is%20no%20large%20gap%0Abetween%20tolerant%20testing%20and%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesting%2520Juntas%2520Optimally%2520with%2520Samples%26entry.906535625%3DLorenzo%2520Beretta%2520and%2520Nathaniel%2520Harms%2520and%2520Caleb%2520Koch%26entry.1292438233%3D%2520%2520We%2520prove%2520tight%2520upper%2520and%2520lower%2520bounds%2520of%250A%2524%255CTheta%255Cleft%2528%255Ctfrac%257B1%257D%257B%255Cepsilon%257D%255Cleft%2528%2520%255Csqrt%257B2%255Ek%2520%255Clog%255Cbinom%257Bn%257D%257Bk%257D%2520%257D%2520%252B%250A%255Clog%255Cbinom%257Bn%257D%257Bk%257D%2520%255Cright%2529%255Cright%2529%2524%2520on%2520the%2520number%2520of%2520samples%2520required%2520for%250Adistribution-free%2520%2524k%2524-junta%2520testing.%2520This%2520is%2520the%2520first%2520tight%2520bound%2520for%2520testing%250Aa%2520natural%2520class%2520of%2520Boolean%2520functions%2520in%2520the%2520distribution-free%2520sample-based%250Amodel.%2520Our%2520bounds%2520also%2520hold%2520for%2520the%2520feature%2520selection%2520problem%252C%2520showing%2520that%2520a%250Ajunta%2520tester%2520must%2520learn%2520the%2520set%2520of%2520relevant%2520variables.%2520For%2520tolerant%2520junta%250Atesting%252C%2520we%2520prove%2520a%2520sample%2520lower%2520bound%2520of%2520%2524%255COmega%25282%255E%257B%25281-o%25281%2529%2529%2520k%257D%2520%252B%250A%255Clog%255Cbinom%257Bn%257D%257Bk%257D%2529%2524%2520showing%2520that%252C%2520unlike%2520standard%2520testing%252C%2520there%2520is%2520no%2520large%2520gap%250Abetween%2520tolerant%2520testing%2520and%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Testing%20Juntas%20Optimally%20with%20Samples&entry.906535625=Lorenzo%20Beretta%20and%20Nathaniel%20Harms%20and%20Caleb%20Koch&entry.1292438233=%20%20We%20prove%20tight%20upper%20and%20lower%20bounds%20of%0A%24%5CTheta%5Cleft%28%5Ctfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%28%20%5Csqrt%7B2%5Ek%20%5Clog%5Cbinom%7Bn%7D%7Bk%7D%20%7D%20%2B%0A%5Clog%5Cbinom%7Bn%7D%7Bk%7D%20%5Cright%29%5Cright%29%24%20on%20the%20number%20of%20samples%20required%20for%0Adistribution-free%20%24k%24-junta%20testing.%20This%20is%20the%20first%20tight%20bound%20for%20testing%0Aa%20natural%20class%20of%20Boolean%20functions%20in%20the%20distribution-free%20sample-based%0Amodel.%20Our%20bounds%20also%20hold%20for%20the%20feature%20selection%20problem%2C%20showing%20that%20a%0Ajunta%20tester%20must%20learn%20the%20set%20of%20relevant%20variables.%20For%20tolerant%20junta%0Atesting%2C%20we%20prove%20a%20sample%20lower%20bound%20of%20%24%5COmega%282%5E%7B%281-o%281%29%29%20k%7D%20%2B%0A%5Clog%5Cbinom%7Bn%7D%7Bk%7D%29%24%20showing%20that%2C%20unlike%20standard%20testing%2C%20there%20is%20no%20large%20gap%0Abetween%20tolerant%20testing%20and%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04604v1&entry.124074799=Read"},
{"title": "Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the\n  1D Vlasov-Poisson Equations", "author": "Vincent Souveton and S\u00e9bastien Terrana", "abstract": "  Many conservative physical systems can be described using the Hamiltonian\nformalism. A notable example is the Vlasov-Poisson equations, a set of partial\ndifferential equations that govern the time evolution of a phase-space density\nfunction representing collisionless particles under a self-consistent\npotential. These equations play a central role in both plasma physics and\ncosmology. Due to the complexity of the potential involved, analytical\nsolutions are rarely available, necessitating the use of numerical methods such\nas Particle-In-Cell. In this work, we introduce a novel approach based on\nHamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic\nNeural Hamiltonian Flows. Our method transforms an initial Gaussian\ndistribution in phase space into the final distribution using a sequence of\ninvertible, volume-preserving transformations derived from Hamiltonian\ndynamics. The model is trained on a dataset comprising initial and final states\nat a fixed time T, generated via numerical simulations. After training, the\nmodel enables fast sampling of the final distribution from any given initial\nstate. Moreover, by automatically learning an interpretable physical potential,\nit can generalize to intermediate states not seen during training, offering\ninsights into the system's evolution across time.\n", "link": "http://arxiv.org/abs/2505.04471v1", "date": "2025-05-07", "relevancy": 1.9095, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5071}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4774}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hamiltonian%20Normalizing%20Flows%20as%20kinetic%20PDE%20solvers%3A%20application%20to%20the%0A%20%201D%20Vlasov-Poisson%20Equations&body=Title%3A%20Hamiltonian%20Normalizing%20Flows%20as%20kinetic%20PDE%20solvers%3A%20application%20to%20the%0A%20%201D%20Vlasov-Poisson%20Equations%0AAuthor%3A%20Vincent%20Souveton%20and%20S%C3%A9bastien%20Terrana%0AAbstract%3A%20%20%20Many%20conservative%20physical%20systems%20can%20be%20described%20using%20the%20Hamiltonian%0Aformalism.%20A%20notable%20example%20is%20the%20Vlasov-Poisson%20equations%2C%20a%20set%20of%20partial%0Adifferential%20equations%20that%20govern%20the%20time%20evolution%20of%20a%20phase-space%20density%0Afunction%20representing%20collisionless%20particles%20under%20a%20self-consistent%0Apotential.%20These%20equations%20play%20a%20central%20role%20in%20both%20plasma%20physics%20and%0Acosmology.%20Due%20to%20the%20complexity%20of%20the%20potential%20involved%2C%20analytical%0Asolutions%20are%20rarely%20available%2C%20necessitating%20the%20use%20of%20numerical%20methods%20such%0Aas%20Particle-In-Cell.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20based%20on%0AHamiltonian-informed%20Normalizing%20Flows%2C%20specifically%20a%20variant%20of%20Fixed-Kinetic%0ANeural%20Hamiltonian%20Flows.%20Our%20method%20transforms%20an%20initial%20Gaussian%0Adistribution%20in%20phase%20space%20into%20the%20final%20distribution%20using%20a%20sequence%20of%0Ainvertible%2C%20volume-preserving%20transformations%20derived%20from%20Hamiltonian%0Adynamics.%20The%20model%20is%20trained%20on%20a%20dataset%20comprising%20initial%20and%20final%20states%0Aat%20a%20fixed%20time%20T%2C%20generated%20via%20numerical%20simulations.%20After%20training%2C%20the%0Amodel%20enables%20fast%20sampling%20of%20the%20final%20distribution%20from%20any%20given%20initial%0Astate.%20Moreover%2C%20by%20automatically%20learning%20an%20interpretable%20physical%20potential%2C%0Ait%20can%20generalize%20to%20intermediate%20states%20not%20seen%20during%20training%2C%20offering%0Ainsights%20into%20the%20system%27s%20evolution%20across%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHamiltonian%2520Normalizing%2520Flows%2520as%2520kinetic%2520PDE%2520solvers%253A%2520application%2520to%2520the%250A%2520%25201D%2520Vlasov-Poisson%2520Equations%26entry.906535625%3DVincent%2520Souveton%2520and%2520S%25C3%25A9bastien%2520Terrana%26entry.1292438233%3D%2520%2520Many%2520conservative%2520physical%2520systems%2520can%2520be%2520described%2520using%2520the%2520Hamiltonian%250Aformalism.%2520A%2520notable%2520example%2520is%2520the%2520Vlasov-Poisson%2520equations%252C%2520a%2520set%2520of%2520partial%250Adifferential%2520equations%2520that%2520govern%2520the%2520time%2520evolution%2520of%2520a%2520phase-space%2520density%250Afunction%2520representing%2520collisionless%2520particles%2520under%2520a%2520self-consistent%250Apotential.%2520These%2520equations%2520play%2520a%2520central%2520role%2520in%2520both%2520plasma%2520physics%2520and%250Acosmology.%2520Due%2520to%2520the%2520complexity%2520of%2520the%2520potential%2520involved%252C%2520analytical%250Asolutions%2520are%2520rarely%2520available%252C%2520necessitating%2520the%2520use%2520of%2520numerical%2520methods%2520such%250Aas%2520Particle-In-Cell.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520based%2520on%250AHamiltonian-informed%2520Normalizing%2520Flows%252C%2520specifically%2520a%2520variant%2520of%2520Fixed-Kinetic%250ANeural%2520Hamiltonian%2520Flows.%2520Our%2520method%2520transforms%2520an%2520initial%2520Gaussian%250Adistribution%2520in%2520phase%2520space%2520into%2520the%2520final%2520distribution%2520using%2520a%2520sequence%2520of%250Ainvertible%252C%2520volume-preserving%2520transformations%2520derived%2520from%2520Hamiltonian%250Adynamics.%2520The%2520model%2520is%2520trained%2520on%2520a%2520dataset%2520comprising%2520initial%2520and%2520final%2520states%250Aat%2520a%2520fixed%2520time%2520T%252C%2520generated%2520via%2520numerical%2520simulations.%2520After%2520training%252C%2520the%250Amodel%2520enables%2520fast%2520sampling%2520of%2520the%2520final%2520distribution%2520from%2520any%2520given%2520initial%250Astate.%2520Moreover%252C%2520by%2520automatically%2520learning%2520an%2520interpretable%2520physical%2520potential%252C%250Ait%2520can%2520generalize%2520to%2520intermediate%2520states%2520not%2520seen%2520during%2520training%252C%2520offering%250Ainsights%2520into%2520the%2520system%2527s%2520evolution%2520across%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hamiltonian%20Normalizing%20Flows%20as%20kinetic%20PDE%20solvers%3A%20application%20to%20the%0A%20%201D%20Vlasov-Poisson%20Equations&entry.906535625=Vincent%20Souveton%20and%20S%C3%A9bastien%20Terrana&entry.1292438233=%20%20Many%20conservative%20physical%20systems%20can%20be%20described%20using%20the%20Hamiltonian%0Aformalism.%20A%20notable%20example%20is%20the%20Vlasov-Poisson%20equations%2C%20a%20set%20of%20partial%0Adifferential%20equations%20that%20govern%20the%20time%20evolution%20of%20a%20phase-space%20density%0Afunction%20representing%20collisionless%20particles%20under%20a%20self-consistent%0Apotential.%20These%20equations%20play%20a%20central%20role%20in%20both%20plasma%20physics%20and%0Acosmology.%20Due%20to%20the%20complexity%20of%20the%20potential%20involved%2C%20analytical%0Asolutions%20are%20rarely%20available%2C%20necessitating%20the%20use%20of%20numerical%20methods%20such%0Aas%20Particle-In-Cell.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20based%20on%0AHamiltonian-informed%20Normalizing%20Flows%2C%20specifically%20a%20variant%20of%20Fixed-Kinetic%0ANeural%20Hamiltonian%20Flows.%20Our%20method%20transforms%20an%20initial%20Gaussian%0Adistribution%20in%20phase%20space%20into%20the%20final%20distribution%20using%20a%20sequence%20of%0Ainvertible%2C%20volume-preserving%20transformations%20derived%20from%20Hamiltonian%0Adynamics.%20The%20model%20is%20trained%20on%20a%20dataset%20comprising%20initial%20and%20final%20states%0Aat%20a%20fixed%20time%20T%2C%20generated%20via%20numerical%20simulations.%20After%20training%2C%20the%0Amodel%20enables%20fast%20sampling%20of%20the%20final%20distribution%20from%20any%20given%20initial%0Astate.%20Moreover%2C%20by%20automatically%20learning%20an%20interpretable%20physical%20potential%2C%0Ait%20can%20generalize%20to%20intermediate%20states%20not%20seen%20during%20training%2C%20offering%0Ainsights%20into%20the%20system%27s%20evolution%20across%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04471v1&entry.124074799=Read"},
{"title": "A Survey on Temporal Interaction Graph Representation Learning:\n  Progress, Challenges, and Opportunities", "author": "Pengfei Jiao and Hongjiang Chen and Xuan Guo and Zhidong Zhao and Dongxiao He and Di Jin", "abstract": "  Temporal interaction graphs (TIGs), defined by sequences of timestamped\ninteraction events, have become ubiquitous in real-world applications due to\ntheir capability to model complex dynamic system behaviors. As a result,\ntemporal interaction graph representation learning (TIGRL) has garnered\nsignificant attention in recent years. TIGRL aims to embed nodes in TIGs into\nlow-dimensional representations that effectively preserve both structural and\ntemporal information, thereby enhancing the performance of downstream tasks\nsuch as classification, prediction, and clustering within constantly evolving\ndata environments. In this paper, we begin by introducing the foundational\nconcepts of TIGs and emphasize the critical role of temporal dependencies. We\nthen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,\nsystematically categorizing them based on the types of information utilized\nduring the learning process to address the unique challenges inherent to TIGs.\nTo facilitate further research and practical applications, we curate the source\nof datasets and benchmarks, providing valuable resources for empirical\ninvestigations. Finally, we examine key open challenges and explore promising\nresearch directions in TIGRL, laying the groundwork for future advancements\nthat have the potential to shape the evolution of this field.\n", "link": "http://arxiv.org/abs/2505.04461v1", "date": "2025-05-07", "relevancy": 1.9093, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4966}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4692}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Temporal%20Interaction%20Graph%20Representation%20Learning%3A%0A%20%20Progress%2C%20Challenges%2C%20and%20Opportunities&body=Title%3A%20A%20Survey%20on%20Temporal%20Interaction%20Graph%20Representation%20Learning%3A%0A%20%20Progress%2C%20Challenges%2C%20and%20Opportunities%0AAuthor%3A%20Pengfei%20Jiao%20and%20Hongjiang%20Chen%20and%20Xuan%20Guo%20and%20Zhidong%20Zhao%20and%20Dongxiao%20He%20and%20Di%20Jin%0AAbstract%3A%20%20%20Temporal%20interaction%20graphs%20%28TIGs%29%2C%20defined%20by%20sequences%20of%20timestamped%0Ainteraction%20events%2C%20have%20become%20ubiquitous%20in%20real-world%20applications%20due%20to%0Atheir%20capability%20to%20model%20complex%20dynamic%20system%20behaviors.%20As%20a%20result%2C%0Atemporal%20interaction%20graph%20representation%20learning%20%28TIGRL%29%20has%20garnered%0Asignificant%20attention%20in%20recent%20years.%20TIGRL%20aims%20to%20embed%20nodes%20in%20TIGs%20into%0Alow-dimensional%20representations%20that%20effectively%20preserve%20both%20structural%20and%0Atemporal%20information%2C%20thereby%20enhancing%20the%20performance%20of%20downstream%20tasks%0Asuch%20as%20classification%2C%20prediction%2C%20and%20clustering%20within%20constantly%20evolving%0Adata%20environments.%20In%20this%20paper%2C%20we%20begin%20by%20introducing%20the%20foundational%0Aconcepts%20of%20TIGs%20and%20emphasize%20the%20critical%20role%20of%20temporal%20dependencies.%20We%0Athen%20propose%20a%20comprehensive%20taxonomy%20of%20state-of-the-art%20TIGRL%20methods%2C%0Asystematically%20categorizing%20them%20based%20on%20the%20types%20of%20information%20utilized%0Aduring%20the%20learning%20process%20to%20address%20the%20unique%20challenges%20inherent%20to%20TIGs.%0ATo%20facilitate%20further%20research%20and%20practical%20applications%2C%20we%20curate%20the%20source%0Aof%20datasets%20and%20benchmarks%2C%20providing%20valuable%20resources%20for%20empirical%0Ainvestigations.%20Finally%2C%20we%20examine%20key%20open%20challenges%20and%20explore%20promising%0Aresearch%20directions%20in%20TIGRL%2C%20laying%20the%20groundwork%20for%20future%20advancements%0Athat%20have%20the%20potential%20to%20shape%20the%20evolution%20of%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Temporal%2520Interaction%2520Graph%2520Representation%2520Learning%253A%250A%2520%2520Progress%252C%2520Challenges%252C%2520and%2520Opportunities%26entry.906535625%3DPengfei%2520Jiao%2520and%2520Hongjiang%2520Chen%2520and%2520Xuan%2520Guo%2520and%2520Zhidong%2520Zhao%2520and%2520Dongxiao%2520He%2520and%2520Di%2520Jin%26entry.1292438233%3D%2520%2520Temporal%2520interaction%2520graphs%2520%2528TIGs%2529%252C%2520defined%2520by%2520sequences%2520of%2520timestamped%250Ainteraction%2520events%252C%2520have%2520become%2520ubiquitous%2520in%2520real-world%2520applications%2520due%2520to%250Atheir%2520capability%2520to%2520model%2520complex%2520dynamic%2520system%2520behaviors.%2520As%2520a%2520result%252C%250Atemporal%2520interaction%2520graph%2520representation%2520learning%2520%2528TIGRL%2529%2520has%2520garnered%250Asignificant%2520attention%2520in%2520recent%2520years.%2520TIGRL%2520aims%2520to%2520embed%2520nodes%2520in%2520TIGs%2520into%250Alow-dimensional%2520representations%2520that%2520effectively%2520preserve%2520both%2520structural%2520and%250Atemporal%2520information%252C%2520thereby%2520enhancing%2520the%2520performance%2520of%2520downstream%2520tasks%250Asuch%2520as%2520classification%252C%2520prediction%252C%2520and%2520clustering%2520within%2520constantly%2520evolving%250Adata%2520environments.%2520In%2520this%2520paper%252C%2520we%2520begin%2520by%2520introducing%2520the%2520foundational%250Aconcepts%2520of%2520TIGs%2520and%2520emphasize%2520the%2520critical%2520role%2520of%2520temporal%2520dependencies.%2520We%250Athen%2520propose%2520a%2520comprehensive%2520taxonomy%2520of%2520state-of-the-art%2520TIGRL%2520methods%252C%250Asystematically%2520categorizing%2520them%2520based%2520on%2520the%2520types%2520of%2520information%2520utilized%250Aduring%2520the%2520learning%2520process%2520to%2520address%2520the%2520unique%2520challenges%2520inherent%2520to%2520TIGs.%250ATo%2520facilitate%2520further%2520research%2520and%2520practical%2520applications%252C%2520we%2520curate%2520the%2520source%250Aof%2520datasets%2520and%2520benchmarks%252C%2520providing%2520valuable%2520resources%2520for%2520empirical%250Ainvestigations.%2520Finally%252C%2520we%2520examine%2520key%2520open%2520challenges%2520and%2520explore%2520promising%250Aresearch%2520directions%2520in%2520TIGRL%252C%2520laying%2520the%2520groundwork%2520for%2520future%2520advancements%250Athat%2520have%2520the%2520potential%2520to%2520shape%2520the%2520evolution%2520of%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Temporal%20Interaction%20Graph%20Representation%20Learning%3A%0A%20%20Progress%2C%20Challenges%2C%20and%20Opportunities&entry.906535625=Pengfei%20Jiao%20and%20Hongjiang%20Chen%20and%20Xuan%20Guo%20and%20Zhidong%20Zhao%20and%20Dongxiao%20He%20and%20Di%20Jin&entry.1292438233=%20%20Temporal%20interaction%20graphs%20%28TIGs%29%2C%20defined%20by%20sequences%20of%20timestamped%0Ainteraction%20events%2C%20have%20become%20ubiquitous%20in%20real-world%20applications%20due%20to%0Atheir%20capability%20to%20model%20complex%20dynamic%20system%20behaviors.%20As%20a%20result%2C%0Atemporal%20interaction%20graph%20representation%20learning%20%28TIGRL%29%20has%20garnered%0Asignificant%20attention%20in%20recent%20years.%20TIGRL%20aims%20to%20embed%20nodes%20in%20TIGs%20into%0Alow-dimensional%20representations%20that%20effectively%20preserve%20both%20structural%20and%0Atemporal%20information%2C%20thereby%20enhancing%20the%20performance%20of%20downstream%20tasks%0Asuch%20as%20classification%2C%20prediction%2C%20and%20clustering%20within%20constantly%20evolving%0Adata%20environments.%20In%20this%20paper%2C%20we%20begin%20by%20introducing%20the%20foundational%0Aconcepts%20of%20TIGs%20and%20emphasize%20the%20critical%20role%20of%20temporal%20dependencies.%20We%0Athen%20propose%20a%20comprehensive%20taxonomy%20of%20state-of-the-art%20TIGRL%20methods%2C%0Asystematically%20categorizing%20them%20based%20on%20the%20types%20of%20information%20utilized%0Aduring%20the%20learning%20process%20to%20address%20the%20unique%20challenges%20inherent%20to%20TIGs.%0ATo%20facilitate%20further%20research%20and%20practical%20applications%2C%20we%20curate%20the%20source%0Aof%20datasets%20and%20benchmarks%2C%20providing%20valuable%20resources%20for%20empirical%0Ainvestigations.%20Finally%2C%20we%20examine%20key%20open%20challenges%20and%20explore%20promising%0Aresearch%20directions%20in%20TIGRL%2C%20laying%20the%20groundwork%20for%20future%20advancements%0Athat%20have%20the%20potential%20to%20shape%20the%20evolution%20of%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04461v1&entry.124074799=Read"},
{"title": "Enhancing User Interest based on Stream Clustering and Memory Networks\n  in Large-Scale Recommender Systems", "author": "Peng Liu and Nian Wang and Cong Xu and Ming Zhao and Bin Wang and Yi Ren", "abstract": "  Recommender Systems (RSs) provide personalized recommendation service based\non user interest, which are widely used in various platforms. However, there\nare lots of users with sparse interest due to lacking consumption behaviors,\nwhich leads to poor recommendation results for them. This problem is widespread\nin large-scale RSs and is particularly difficult to address. To solve this\nchallenging problem, we propose an innovative solution called User Interest\nEnhancement (UIE). UIE enhances user interest including user profile and user\nhistory behavior sequences by leveraging the enhancement vectors and\npersonalized enhancement vectors generated based on dynamic streaming\nclustering of similar users and items from multiple perspectives, which are\nstored and updated in memory networks. UIE not only remarkably improves model\nperformance for users with sparse interest, but also delivers notable gains for\nother users. As an end-to-end solution, UIE is easy to implement on top of\nexisting ranking models. Furthermore, we extend our approach to long-tail items\nusing similar methods, which also yields excellent improvements. We conduct\nextensive offline and online experiments in a large-scale industrial RS. The\nresults demonstrate that our model substantially outperforms other existing\napproaches, especially for users with sparse interest. UIE has been deployed in\nseveral large-scale RSs at Tencent since 2022, which was made public on 21 May\n2024. In addition, UIE-based methods have also been successfully applied in\ncandidate generation, pre-ranking, and context-DNN stages. Multiple teams have\ndeveloped solutions based on UIE, focusing primarily on updating clustering\nalgorithms and attention mechanisms. As far as we know, UIE has been deployed\nby many companies. The thoughts of UIE, dynamic streaming clustering and\nsimilarity enhancement, have inspired subsequent relevant works.\n", "link": "http://arxiv.org/abs/2405.13238v5", "date": "2025-05-07", "relevancy": 1.9069, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4862}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4746}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20User%20Interest%20based%20on%20Stream%20Clustering%20and%20Memory%20Networks%0A%20%20in%20Large-Scale%20Recommender%20Systems&body=Title%3A%20Enhancing%20User%20Interest%20based%20on%20Stream%20Clustering%20and%20Memory%20Networks%0A%20%20in%20Large-Scale%20Recommender%20Systems%0AAuthor%3A%20Peng%20Liu%20and%20Nian%20Wang%20and%20Cong%20Xu%20and%20Ming%20Zhao%20and%20Bin%20Wang%20and%20Yi%20Ren%0AAbstract%3A%20%20%20Recommender%20Systems%20%28RSs%29%20provide%20personalized%20recommendation%20service%20based%0Aon%20user%20interest%2C%20which%20are%20widely%20used%20in%20various%20platforms.%20However%2C%20there%0Aare%20lots%20of%20users%20with%20sparse%20interest%20due%20to%20lacking%20consumption%20behaviors%2C%0Awhich%20leads%20to%20poor%20recommendation%20results%20for%20them.%20This%20problem%20is%20widespread%0Ain%20large-scale%20RSs%20and%20is%20particularly%20difficult%20to%20address.%20To%20solve%20this%0Achallenging%20problem%2C%20we%20propose%20an%20innovative%20solution%20called%20User%20Interest%0AEnhancement%20%28UIE%29.%20UIE%20enhances%20user%20interest%20including%20user%20profile%20and%20user%0Ahistory%20behavior%20sequences%20by%20leveraging%20the%20enhancement%20vectors%20and%0Apersonalized%20enhancement%20vectors%20generated%20based%20on%20dynamic%20streaming%0Aclustering%20of%20similar%20users%20and%20items%20from%20multiple%20perspectives%2C%20which%20are%0Astored%20and%20updated%20in%20memory%20networks.%20UIE%20not%20only%20remarkably%20improves%20model%0Aperformance%20for%20users%20with%20sparse%20interest%2C%20but%20also%20delivers%20notable%20gains%20for%0Aother%20users.%20As%20an%20end-to-end%20solution%2C%20UIE%20is%20easy%20to%20implement%20on%20top%20of%0Aexisting%20ranking%20models.%20Furthermore%2C%20we%20extend%20our%20approach%20to%20long-tail%20items%0Ausing%20similar%20methods%2C%20which%20also%20yields%20excellent%20improvements.%20We%20conduct%0Aextensive%20offline%20and%20online%20experiments%20in%20a%20large-scale%20industrial%20RS.%20The%0Aresults%20demonstrate%20that%20our%20model%20substantially%20outperforms%20other%20existing%0Aapproaches%2C%20especially%20for%20users%20with%20sparse%20interest.%20UIE%20has%20been%20deployed%20in%0Aseveral%20large-scale%20RSs%20at%20Tencent%20since%202022%2C%20which%20was%20made%20public%20on%2021%20May%0A2024.%20In%20addition%2C%20UIE-based%20methods%20have%20also%20been%20successfully%20applied%20in%0Acandidate%20generation%2C%20pre-ranking%2C%20and%20context-DNN%20stages.%20Multiple%20teams%20have%0Adeveloped%20solutions%20based%20on%20UIE%2C%20focusing%20primarily%20on%20updating%20clustering%0Aalgorithms%20and%20attention%20mechanisms.%20As%20far%20as%20we%20know%2C%20UIE%20has%20been%20deployed%0Aby%20many%20companies.%20The%20thoughts%20of%20UIE%2C%20dynamic%20streaming%20clustering%20and%0Asimilarity%20enhancement%2C%20have%20inspired%20subsequent%20relevant%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13238v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520User%2520Interest%2520based%2520on%2520Stream%2520Clustering%2520and%2520Memory%2520Networks%250A%2520%2520in%2520Large-Scale%2520Recommender%2520Systems%26entry.906535625%3DPeng%2520Liu%2520and%2520Nian%2520Wang%2520and%2520Cong%2520Xu%2520and%2520Ming%2520Zhao%2520and%2520Bin%2520Wang%2520and%2520Yi%2520Ren%26entry.1292438233%3D%2520%2520Recommender%2520Systems%2520%2528RSs%2529%2520provide%2520personalized%2520recommendation%2520service%2520based%250Aon%2520user%2520interest%252C%2520which%2520are%2520widely%2520used%2520in%2520various%2520platforms.%2520However%252C%2520there%250Aare%2520lots%2520of%2520users%2520with%2520sparse%2520interest%2520due%2520to%2520lacking%2520consumption%2520behaviors%252C%250Awhich%2520leads%2520to%2520poor%2520recommendation%2520results%2520for%2520them.%2520This%2520problem%2520is%2520widespread%250Ain%2520large-scale%2520RSs%2520and%2520is%2520particularly%2520difficult%2520to%2520address.%2520To%2520solve%2520this%250Achallenging%2520problem%252C%2520we%2520propose%2520an%2520innovative%2520solution%2520called%2520User%2520Interest%250AEnhancement%2520%2528UIE%2529.%2520UIE%2520enhances%2520user%2520interest%2520including%2520user%2520profile%2520and%2520user%250Ahistory%2520behavior%2520sequences%2520by%2520leveraging%2520the%2520enhancement%2520vectors%2520and%250Apersonalized%2520enhancement%2520vectors%2520generated%2520based%2520on%2520dynamic%2520streaming%250Aclustering%2520of%2520similar%2520users%2520and%2520items%2520from%2520multiple%2520perspectives%252C%2520which%2520are%250Astored%2520and%2520updated%2520in%2520memory%2520networks.%2520UIE%2520not%2520only%2520remarkably%2520improves%2520model%250Aperformance%2520for%2520users%2520with%2520sparse%2520interest%252C%2520but%2520also%2520delivers%2520notable%2520gains%2520for%250Aother%2520users.%2520As%2520an%2520end-to-end%2520solution%252C%2520UIE%2520is%2520easy%2520to%2520implement%2520on%2520top%2520of%250Aexisting%2520ranking%2520models.%2520Furthermore%252C%2520we%2520extend%2520our%2520approach%2520to%2520long-tail%2520items%250Ausing%2520similar%2520methods%252C%2520which%2520also%2520yields%2520excellent%2520improvements.%2520We%2520conduct%250Aextensive%2520offline%2520and%2520online%2520experiments%2520in%2520a%2520large-scale%2520industrial%2520RS.%2520The%250Aresults%2520demonstrate%2520that%2520our%2520model%2520substantially%2520outperforms%2520other%2520existing%250Aapproaches%252C%2520especially%2520for%2520users%2520with%2520sparse%2520interest.%2520UIE%2520has%2520been%2520deployed%2520in%250Aseveral%2520large-scale%2520RSs%2520at%2520Tencent%2520since%25202022%252C%2520which%2520was%2520made%2520public%2520on%252021%2520May%250A2024.%2520In%2520addition%252C%2520UIE-based%2520methods%2520have%2520also%2520been%2520successfully%2520applied%2520in%250Acandidate%2520generation%252C%2520pre-ranking%252C%2520and%2520context-DNN%2520stages.%2520Multiple%2520teams%2520have%250Adeveloped%2520solutions%2520based%2520on%2520UIE%252C%2520focusing%2520primarily%2520on%2520updating%2520clustering%250Aalgorithms%2520and%2520attention%2520mechanisms.%2520As%2520far%2520as%2520we%2520know%252C%2520UIE%2520has%2520been%2520deployed%250Aby%2520many%2520companies.%2520The%2520thoughts%2520of%2520UIE%252C%2520dynamic%2520streaming%2520clustering%2520and%250Asimilarity%2520enhancement%252C%2520have%2520inspired%2520subsequent%2520relevant%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13238v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20User%20Interest%20based%20on%20Stream%20Clustering%20and%20Memory%20Networks%0A%20%20in%20Large-Scale%20Recommender%20Systems&entry.906535625=Peng%20Liu%20and%20Nian%20Wang%20and%20Cong%20Xu%20and%20Ming%20Zhao%20and%20Bin%20Wang%20and%20Yi%20Ren&entry.1292438233=%20%20Recommender%20Systems%20%28RSs%29%20provide%20personalized%20recommendation%20service%20based%0Aon%20user%20interest%2C%20which%20are%20widely%20used%20in%20various%20platforms.%20However%2C%20there%0Aare%20lots%20of%20users%20with%20sparse%20interest%20due%20to%20lacking%20consumption%20behaviors%2C%0Awhich%20leads%20to%20poor%20recommendation%20results%20for%20them.%20This%20problem%20is%20widespread%0Ain%20large-scale%20RSs%20and%20is%20particularly%20difficult%20to%20address.%20To%20solve%20this%0Achallenging%20problem%2C%20we%20propose%20an%20innovative%20solution%20called%20User%20Interest%0AEnhancement%20%28UIE%29.%20UIE%20enhances%20user%20interest%20including%20user%20profile%20and%20user%0Ahistory%20behavior%20sequences%20by%20leveraging%20the%20enhancement%20vectors%20and%0Apersonalized%20enhancement%20vectors%20generated%20based%20on%20dynamic%20streaming%0Aclustering%20of%20similar%20users%20and%20items%20from%20multiple%20perspectives%2C%20which%20are%0Astored%20and%20updated%20in%20memory%20networks.%20UIE%20not%20only%20remarkably%20improves%20model%0Aperformance%20for%20users%20with%20sparse%20interest%2C%20but%20also%20delivers%20notable%20gains%20for%0Aother%20users.%20As%20an%20end-to-end%20solution%2C%20UIE%20is%20easy%20to%20implement%20on%20top%20of%0Aexisting%20ranking%20models.%20Furthermore%2C%20we%20extend%20our%20approach%20to%20long-tail%20items%0Ausing%20similar%20methods%2C%20which%20also%20yields%20excellent%20improvements.%20We%20conduct%0Aextensive%20offline%20and%20online%20experiments%20in%20a%20large-scale%20industrial%20RS.%20The%0Aresults%20demonstrate%20that%20our%20model%20substantially%20outperforms%20other%20existing%0Aapproaches%2C%20especially%20for%20users%20with%20sparse%20interest.%20UIE%20has%20been%20deployed%20in%0Aseveral%20large-scale%20RSs%20at%20Tencent%20since%202022%2C%20which%20was%20made%20public%20on%2021%20May%0A2024.%20In%20addition%2C%20UIE-based%20methods%20have%20also%20been%20successfully%20applied%20in%0Acandidate%20generation%2C%20pre-ranking%2C%20and%20context-DNN%20stages.%20Multiple%20teams%20have%0Adeveloped%20solutions%20based%20on%20UIE%2C%20focusing%20primarily%20on%20updating%20clustering%0Aalgorithms%20and%20attention%20mechanisms.%20As%20far%20as%20we%20know%2C%20UIE%20has%20been%20deployed%0Aby%20many%20companies.%20The%20thoughts%20of%20UIE%2C%20dynamic%20streaming%20clustering%20and%0Asimilarity%20enhancement%2C%20have%20inspired%20subsequent%20relevant%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13238v5&entry.124074799=Read"},
{"title": "FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame\n  Averaging", "author": "Ali Alawieh and Alexandru P. Condurache", "abstract": "  We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural\nnetwork architecture built on top of the well-known KPConv, a widely adopted\nbackbone for 3D point cloud analysis. Even though invariance and/or\nequivariance to Euclidean transformations are required for many common tasks,\nKPConv-based networks can only approximately achieve such properties when\ntraining on large datasets or with significant data augmentations. Using Frame\nAveraging, we allow to flexibly customize point cloud neural networks built\nwith KPConv layers, by making them exactly invariant and/or equivariant to\ntranslations, rotations and/or reflections of the input point clouds. By simply\nwrapping around an existing KPConv-based network, FA-KPConv embeds geometrical\nprior knowledge into it while preserving the number of learnable parameters and\nnot compromising any input information. We showcase the benefit of such an\nintroduced bias for point cloud classification and point cloud registration,\nespecially in challenging cases such as scarce training data or randomly\nrotated test data.\n", "link": "http://arxiv.org/abs/2505.04485v1", "date": "2025-05-07", "relevancy": 1.8977, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4765}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4757}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FA-KPConv%3A%20Introducing%20Euclidean%20Symmetries%20to%20KPConv%20via%20Frame%0A%20%20Averaging&body=Title%3A%20FA-KPConv%3A%20Introducing%20Euclidean%20Symmetries%20to%20KPConv%20via%20Frame%0A%20%20Averaging%0AAuthor%3A%20Ali%20Alawieh%20and%20Alexandru%20P.%20Condurache%0AAbstract%3A%20%20%20We%20present%20Frame-Averaging%20Kernel-Point%20Convolution%20%28FA-KPConv%29%2C%20a%20neural%0Anetwork%20architecture%20built%20on%20top%20of%20the%20well-known%20KPConv%2C%20a%20widely%20adopted%0Abackbone%20for%203D%20point%20cloud%20analysis.%20Even%20though%20invariance%20and/or%0Aequivariance%20to%20Euclidean%20transformations%20are%20required%20for%20many%20common%20tasks%2C%0AKPConv-based%20networks%20can%20only%20approximately%20achieve%20such%20properties%20when%0Atraining%20on%20large%20datasets%20or%20with%20significant%20data%20augmentations.%20Using%20Frame%0AAveraging%2C%20we%20allow%20to%20flexibly%20customize%20point%20cloud%20neural%20networks%20built%0Awith%20KPConv%20layers%2C%20by%20making%20them%20exactly%20invariant%20and/or%20equivariant%20to%0Atranslations%2C%20rotations%20and/or%20reflections%20of%20the%20input%20point%20clouds.%20By%20simply%0Awrapping%20around%20an%20existing%20KPConv-based%20network%2C%20FA-KPConv%20embeds%20geometrical%0Aprior%20knowledge%20into%20it%20while%20preserving%20the%20number%20of%20learnable%20parameters%20and%0Anot%20compromising%20any%20input%20information.%20We%20showcase%20the%20benefit%20of%20such%20an%0Aintroduced%20bias%20for%20point%20cloud%20classification%20and%20point%20cloud%20registration%2C%0Aespecially%20in%20challenging%20cases%20such%20as%20scarce%20training%20data%20or%20randomly%0Arotated%20test%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFA-KPConv%253A%2520Introducing%2520Euclidean%2520Symmetries%2520to%2520KPConv%2520via%2520Frame%250A%2520%2520Averaging%26entry.906535625%3DAli%2520Alawieh%2520and%2520Alexandru%2520P.%2520Condurache%26entry.1292438233%3D%2520%2520We%2520present%2520Frame-Averaging%2520Kernel-Point%2520Convolution%2520%2528FA-KPConv%2529%252C%2520a%2520neural%250Anetwork%2520architecture%2520built%2520on%2520top%2520of%2520the%2520well-known%2520KPConv%252C%2520a%2520widely%2520adopted%250Abackbone%2520for%25203D%2520point%2520cloud%2520analysis.%2520Even%2520though%2520invariance%2520and/or%250Aequivariance%2520to%2520Euclidean%2520transformations%2520are%2520required%2520for%2520many%2520common%2520tasks%252C%250AKPConv-based%2520networks%2520can%2520only%2520approximately%2520achieve%2520such%2520properties%2520when%250Atraining%2520on%2520large%2520datasets%2520or%2520with%2520significant%2520data%2520augmentations.%2520Using%2520Frame%250AAveraging%252C%2520we%2520allow%2520to%2520flexibly%2520customize%2520point%2520cloud%2520neural%2520networks%2520built%250Awith%2520KPConv%2520layers%252C%2520by%2520making%2520them%2520exactly%2520invariant%2520and/or%2520equivariant%2520to%250Atranslations%252C%2520rotations%2520and/or%2520reflections%2520of%2520the%2520input%2520point%2520clouds.%2520By%2520simply%250Awrapping%2520around%2520an%2520existing%2520KPConv-based%2520network%252C%2520FA-KPConv%2520embeds%2520geometrical%250Aprior%2520knowledge%2520into%2520it%2520while%2520preserving%2520the%2520number%2520of%2520learnable%2520parameters%2520and%250Anot%2520compromising%2520any%2520input%2520information.%2520We%2520showcase%2520the%2520benefit%2520of%2520such%2520an%250Aintroduced%2520bias%2520for%2520point%2520cloud%2520classification%2520and%2520point%2520cloud%2520registration%252C%250Aespecially%2520in%2520challenging%2520cases%2520such%2520as%2520scarce%2520training%2520data%2520or%2520randomly%250Arotated%2520test%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FA-KPConv%3A%20Introducing%20Euclidean%20Symmetries%20to%20KPConv%20via%20Frame%0A%20%20Averaging&entry.906535625=Ali%20Alawieh%20and%20Alexandru%20P.%20Condurache&entry.1292438233=%20%20We%20present%20Frame-Averaging%20Kernel-Point%20Convolution%20%28FA-KPConv%29%2C%20a%20neural%0Anetwork%20architecture%20built%20on%20top%20of%20the%20well-known%20KPConv%2C%20a%20widely%20adopted%0Abackbone%20for%203D%20point%20cloud%20analysis.%20Even%20though%20invariance%20and/or%0Aequivariance%20to%20Euclidean%20transformations%20are%20required%20for%20many%20common%20tasks%2C%0AKPConv-based%20networks%20can%20only%20approximately%20achieve%20such%20properties%20when%0Atraining%20on%20large%20datasets%20or%20with%20significant%20data%20augmentations.%20Using%20Frame%0AAveraging%2C%20we%20allow%20to%20flexibly%20customize%20point%20cloud%20neural%20networks%20built%0Awith%20KPConv%20layers%2C%20by%20making%20them%20exactly%20invariant%20and/or%20equivariant%20to%0Atranslations%2C%20rotations%20and/or%20reflections%20of%20the%20input%20point%20clouds.%20By%20simply%0Awrapping%20around%20an%20existing%20KPConv-based%20network%2C%20FA-KPConv%20embeds%20geometrical%0Aprior%20knowledge%20into%20it%20while%20preserving%20the%20number%20of%20learnable%20parameters%20and%0Anot%20compromising%20any%20input%20information.%20We%20showcase%20the%20benefit%20of%20such%20an%0Aintroduced%20bias%20for%20point%20cloud%20classification%20and%20point%20cloud%20registration%2C%0Aespecially%20in%20challenging%20cases%20such%20as%20scarce%20training%20data%20or%20randomly%0Arotated%20test%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04485v1&entry.124074799=Read"},
{"title": "Likelihood-Free Adaptive Bayesian Inference via Nonparametric\n  Distribution Matching", "author": "Wenhui Sophia Lu and Wing Hung Wong", "abstract": "  When the likelihood is analytically unavailable and computationally\nintractable, approximate Bayesian computation (ABC) has emerged as a widely\nused methodology for approximate posterior inference; however, it suffers from\nsevere computational inefficiency in high-dimensional settings or under diffuse\npriors. To overcome these limitations, we propose Adaptive Bayesian Inference\n(ABI), a framework that bypasses traditional data-space discrepancies and\ninstead compares distributions directly in posterior space through\nnonparametric distribution matching. By leveraging a novel Marginally-augmented\nSliced Wasserstein (MSW) distance on posterior measures and exploiting its\nquantile representation, ABI transforms the challenging problem of measuring\ndivergence between posterior distributions into a tractable sequence of\none-dimensional conditional quantile regression tasks. Moreover, we introduce a\nnew adaptive rejection sampling scheme that iteratively refines the posterior\napproximation by updating the proposal distribution via generative density\nestimation. Theoretically, we establish parametric convergence rates for the\ntrimmed MSW distance and prove that the ABI posterior converges to the true\nposterior as the tolerance threshold vanishes. Through extensive empirical\nevaluation, we demonstrate that ABI significantly outperforms data-based\nWasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free\nsimulators, especially in high-dimensional or dependent observation regimes.\n", "link": "http://arxiv.org/abs/2505.04603v1", "date": "2025-05-07", "relevancy": 1.8953, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5255}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4671}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Likelihood-Free%20Adaptive%20Bayesian%20Inference%20via%20Nonparametric%0A%20%20Distribution%20Matching&body=Title%3A%20Likelihood-Free%20Adaptive%20Bayesian%20Inference%20via%20Nonparametric%0A%20%20Distribution%20Matching%0AAuthor%3A%20Wenhui%20Sophia%20Lu%20and%20Wing%20Hung%20Wong%0AAbstract%3A%20%20%20When%20the%20likelihood%20is%20analytically%20unavailable%20and%20computationally%0Aintractable%2C%20approximate%20Bayesian%20computation%20%28ABC%29%20has%20emerged%20as%20a%20widely%0Aused%20methodology%20for%20approximate%20posterior%20inference%3B%20however%2C%20it%20suffers%20from%0Asevere%20computational%20inefficiency%20in%20high-dimensional%20settings%20or%20under%20diffuse%0Apriors.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Adaptive%20Bayesian%20Inference%0A%28ABI%29%2C%20a%20framework%20that%20bypasses%20traditional%20data-space%20discrepancies%20and%0Ainstead%20compares%20distributions%20directly%20in%20posterior%20space%20through%0Anonparametric%20distribution%20matching.%20By%20leveraging%20a%20novel%20Marginally-augmented%0ASliced%20Wasserstein%20%28MSW%29%20distance%20on%20posterior%20measures%20and%20exploiting%20its%0Aquantile%20representation%2C%20ABI%20transforms%20the%20challenging%20problem%20of%20measuring%0Adivergence%20between%20posterior%20distributions%20into%20a%20tractable%20sequence%20of%0Aone-dimensional%20conditional%20quantile%20regression%20tasks.%20Moreover%2C%20we%20introduce%20a%0Anew%20adaptive%20rejection%20sampling%20scheme%20that%20iteratively%20refines%20the%20posterior%0Aapproximation%20by%20updating%20the%20proposal%20distribution%20via%20generative%20density%0Aestimation.%20Theoretically%2C%20we%20establish%20parametric%20convergence%20rates%20for%20the%0Atrimmed%20MSW%20distance%20and%20prove%20that%20the%20ABI%20posterior%20converges%20to%20the%20true%0Aposterior%20as%20the%20tolerance%20threshold%20vanishes.%20Through%20extensive%20empirical%0Aevaluation%2C%20we%20demonstrate%20that%20ABI%20significantly%20outperforms%20data-based%0AWasserstein%20ABC%2C%20summary-based%20ABC%2C%20and%20state-of-the-art%20likelihood-free%0Asimulators%2C%20especially%20in%20high-dimensional%20or%20dependent%20observation%20regimes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikelihood-Free%2520Adaptive%2520Bayesian%2520Inference%2520via%2520Nonparametric%250A%2520%2520Distribution%2520Matching%26entry.906535625%3DWenhui%2520Sophia%2520Lu%2520and%2520Wing%2520Hung%2520Wong%26entry.1292438233%3D%2520%2520When%2520the%2520likelihood%2520is%2520analytically%2520unavailable%2520and%2520computationally%250Aintractable%252C%2520approximate%2520Bayesian%2520computation%2520%2528ABC%2529%2520has%2520emerged%2520as%2520a%2520widely%250Aused%2520methodology%2520for%2520approximate%2520posterior%2520inference%253B%2520however%252C%2520it%2520suffers%2520from%250Asevere%2520computational%2520inefficiency%2520in%2520high-dimensional%2520settings%2520or%2520under%2520diffuse%250Apriors.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Adaptive%2520Bayesian%2520Inference%250A%2528ABI%2529%252C%2520a%2520framework%2520that%2520bypasses%2520traditional%2520data-space%2520discrepancies%2520and%250Ainstead%2520compares%2520distributions%2520directly%2520in%2520posterior%2520space%2520through%250Anonparametric%2520distribution%2520matching.%2520By%2520leveraging%2520a%2520novel%2520Marginally-augmented%250ASliced%2520Wasserstein%2520%2528MSW%2529%2520distance%2520on%2520posterior%2520measures%2520and%2520exploiting%2520its%250Aquantile%2520representation%252C%2520ABI%2520transforms%2520the%2520challenging%2520problem%2520of%2520measuring%250Adivergence%2520between%2520posterior%2520distributions%2520into%2520a%2520tractable%2520sequence%2520of%250Aone-dimensional%2520conditional%2520quantile%2520regression%2520tasks.%2520Moreover%252C%2520we%2520introduce%2520a%250Anew%2520adaptive%2520rejection%2520sampling%2520scheme%2520that%2520iteratively%2520refines%2520the%2520posterior%250Aapproximation%2520by%2520updating%2520the%2520proposal%2520distribution%2520via%2520generative%2520density%250Aestimation.%2520Theoretically%252C%2520we%2520establish%2520parametric%2520convergence%2520rates%2520for%2520the%250Atrimmed%2520MSW%2520distance%2520and%2520prove%2520that%2520the%2520ABI%2520posterior%2520converges%2520to%2520the%2520true%250Aposterior%2520as%2520the%2520tolerance%2520threshold%2520vanishes.%2520Through%2520extensive%2520empirical%250Aevaluation%252C%2520we%2520demonstrate%2520that%2520ABI%2520significantly%2520outperforms%2520data-based%250AWasserstein%2520ABC%252C%2520summary-based%2520ABC%252C%2520and%2520state-of-the-art%2520likelihood-free%250Asimulators%252C%2520especially%2520in%2520high-dimensional%2520or%2520dependent%2520observation%2520regimes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Likelihood-Free%20Adaptive%20Bayesian%20Inference%20via%20Nonparametric%0A%20%20Distribution%20Matching&entry.906535625=Wenhui%20Sophia%20Lu%20and%20Wing%20Hung%20Wong&entry.1292438233=%20%20When%20the%20likelihood%20is%20analytically%20unavailable%20and%20computationally%0Aintractable%2C%20approximate%20Bayesian%20computation%20%28ABC%29%20has%20emerged%20as%20a%20widely%0Aused%20methodology%20for%20approximate%20posterior%20inference%3B%20however%2C%20it%20suffers%20from%0Asevere%20computational%20inefficiency%20in%20high-dimensional%20settings%20or%20under%20diffuse%0Apriors.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Adaptive%20Bayesian%20Inference%0A%28ABI%29%2C%20a%20framework%20that%20bypasses%20traditional%20data-space%20discrepancies%20and%0Ainstead%20compares%20distributions%20directly%20in%20posterior%20space%20through%0Anonparametric%20distribution%20matching.%20By%20leveraging%20a%20novel%20Marginally-augmented%0ASliced%20Wasserstein%20%28MSW%29%20distance%20on%20posterior%20measures%20and%20exploiting%20its%0Aquantile%20representation%2C%20ABI%20transforms%20the%20challenging%20problem%20of%20measuring%0Adivergence%20between%20posterior%20distributions%20into%20a%20tractable%20sequence%20of%0Aone-dimensional%20conditional%20quantile%20regression%20tasks.%20Moreover%2C%20we%20introduce%20a%0Anew%20adaptive%20rejection%20sampling%20scheme%20that%20iteratively%20refines%20the%20posterior%0Aapproximation%20by%20updating%20the%20proposal%20distribution%20via%20generative%20density%0Aestimation.%20Theoretically%2C%20we%20establish%20parametric%20convergence%20rates%20for%20the%0Atrimmed%20MSW%20distance%20and%20prove%20that%20the%20ABI%20posterior%20converges%20to%20the%20true%0Aposterior%20as%20the%20tolerance%20threshold%20vanishes.%20Through%20extensive%20empirical%0Aevaluation%2C%20we%20demonstrate%20that%20ABI%20significantly%20outperforms%20data-based%0AWasserstein%20ABC%2C%20summary-based%20ABC%2C%20and%20state-of-the-art%20likelihood-free%0Asimulators%2C%20especially%20in%20high-dimensional%20or%20dependent%20observation%20regimes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04603v1&entry.124074799=Read"},
{"title": "Is the end of Insight in Sight ?", "author": "Jean-Michel Tucny and Mihir Durve and Sauro Succi", "abstract": "  It is shown that the weight matrices of a Physics-informed neural network\n(PINN)-based deep learning application to a rarefied gas dynamics problem\ndescribed by the Boltzmann equation bear no evident link to the mathematical\nstructure of the physical problem. Instead, the weights appear close to\nGaussian distributed random matrices. Although significantly more work is\nneeded to support a robust assessment in this direction, these results suggest\nthat deep-learning and the numerical solution of the Boltzmann equation\nrepresent two equivalent, but largely distinct paths to the same physical\nknowledge. If so, Explainable AI might be an unrealistic target and possibly\neven an ill-posed one.\n", "link": "http://arxiv.org/abs/2505.04627v1", "date": "2025-05-07", "relevancy": 1.8834, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4843}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4806}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20the%20end%20of%20Insight%20in%20Sight%20%3F&body=Title%3A%20Is%20the%20end%20of%20Insight%20in%20Sight%20%3F%0AAuthor%3A%20Jean-Michel%20Tucny%20and%20Mihir%20Durve%20and%20Sauro%20Succi%0AAbstract%3A%20%20%20It%20is%20shown%20that%20the%20weight%20matrices%20of%20a%20Physics-informed%20neural%20network%0A%28PINN%29-based%20deep%20learning%20application%20to%20a%20rarefied%20gas%20dynamics%20problem%0Adescribed%20by%20the%20Boltzmann%20equation%20bear%20no%20evident%20link%20to%20the%20mathematical%0Astructure%20of%20the%20physical%20problem.%20Instead%2C%20the%20weights%20appear%20close%20to%0AGaussian%20distributed%20random%20matrices.%20Although%20significantly%20more%20work%20is%0Aneeded%20to%20support%20a%20robust%20assessment%20in%20this%20direction%2C%20these%20results%20suggest%0Athat%20deep-learning%20and%20the%20numerical%20solution%20of%20the%20Boltzmann%20equation%0Arepresent%20two%20equivalent%2C%20but%20largely%20distinct%20paths%20to%20the%20same%20physical%0Aknowledge.%20If%20so%2C%20Explainable%20AI%20might%20be%20an%20unrealistic%20target%20and%20possibly%0Aeven%20an%20ill-posed%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520the%2520end%2520of%2520Insight%2520in%2520Sight%2520%253F%26entry.906535625%3DJean-Michel%2520Tucny%2520and%2520Mihir%2520Durve%2520and%2520Sauro%2520Succi%26entry.1292438233%3D%2520%2520It%2520is%2520shown%2520that%2520the%2520weight%2520matrices%2520of%2520a%2520Physics-informed%2520neural%2520network%250A%2528PINN%2529-based%2520deep%2520learning%2520application%2520to%2520a%2520rarefied%2520gas%2520dynamics%2520problem%250Adescribed%2520by%2520the%2520Boltzmann%2520equation%2520bear%2520no%2520evident%2520link%2520to%2520the%2520mathematical%250Astructure%2520of%2520the%2520physical%2520problem.%2520Instead%252C%2520the%2520weights%2520appear%2520close%2520to%250AGaussian%2520distributed%2520random%2520matrices.%2520Although%2520significantly%2520more%2520work%2520is%250Aneeded%2520to%2520support%2520a%2520robust%2520assessment%2520in%2520this%2520direction%252C%2520these%2520results%2520suggest%250Athat%2520deep-learning%2520and%2520the%2520numerical%2520solution%2520of%2520the%2520Boltzmann%2520equation%250Arepresent%2520two%2520equivalent%252C%2520but%2520largely%2520distinct%2520paths%2520to%2520the%2520same%2520physical%250Aknowledge.%2520If%2520so%252C%2520Explainable%2520AI%2520might%2520be%2520an%2520unrealistic%2520target%2520and%2520possibly%250Aeven%2520an%2520ill-posed%2520one.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20the%20end%20of%20Insight%20in%20Sight%20%3F&entry.906535625=Jean-Michel%20Tucny%20and%20Mihir%20Durve%20and%20Sauro%20Succi&entry.1292438233=%20%20It%20is%20shown%20that%20the%20weight%20matrices%20of%20a%20Physics-informed%20neural%20network%0A%28PINN%29-based%20deep%20learning%20application%20to%20a%20rarefied%20gas%20dynamics%20problem%0Adescribed%20by%20the%20Boltzmann%20equation%20bear%20no%20evident%20link%20to%20the%20mathematical%0Astructure%20of%20the%20physical%20problem.%20Instead%2C%20the%20weights%20appear%20close%20to%0AGaussian%20distributed%20random%20matrices.%20Although%20significantly%20more%20work%20is%0Aneeded%20to%20support%20a%20robust%20assessment%20in%20this%20direction%2C%20these%20results%20suggest%0Athat%20deep-learning%20and%20the%20numerical%20solution%20of%20the%20Boltzmann%20equation%0Arepresent%20two%20equivalent%2C%20but%20largely%20distinct%20paths%20to%20the%20same%20physical%0Aknowledge.%20If%20so%2C%20Explainable%20AI%20might%20be%20an%20unrealistic%20target%20and%20possibly%0Aeven%20an%20ill-posed%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04627v1&entry.124074799=Read"},
{"title": "Accelerating Audio Research with Robotic Dummy Heads", "author": "Austin Lu and Kanad Sarkar and Yongjie Zhuang and Leo Lin and Ryan M Corey and Andrew C Singer", "abstract": "  This work introduces a robotic dummy head that fuses the acoustic realism of\nconventional audiological mannequins with the mobility of robots. The proposed\ndevice is capable of moving, talking, and listening as people do, and can be\nused to automate spatially-stationary audio experiments, thus accelerating the\npace of audio research. Critically, the device may also be used as a moving\nsound source in dynamic experiments, due to its quiet motor. This feature\ndifferentiates our work from previous robotic acoustic research platforms.\nValidation that the robot enables high quality audio data collection is\nprovided through various experiments and acoustic measurements. These\nexperiments also demonstrate how the robot might be used to study adaptive\nbinaural beamforming. Design files are provided as open-source to stimulate\nnovel audio research.\n", "link": "http://arxiv.org/abs/2505.04548v1", "date": "2025-05-07", "relevancy": 1.5404, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5169}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5156}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Audio%20Research%20with%20Robotic%20Dummy%20Heads&body=Title%3A%20Accelerating%20Audio%20Research%20with%20Robotic%20Dummy%20Heads%0AAuthor%3A%20Austin%20Lu%20and%20Kanad%20Sarkar%20and%20Yongjie%20Zhuang%20and%20Leo%20Lin%20and%20Ryan%20M%20Corey%20and%20Andrew%20C%20Singer%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20robotic%20dummy%20head%20that%20fuses%20the%20acoustic%20realism%20of%0Aconventional%20audiological%20mannequins%20with%20the%20mobility%20of%20robots.%20The%20proposed%0Adevice%20is%20capable%20of%20moving%2C%20talking%2C%20and%20listening%20as%20people%20do%2C%20and%20can%20be%0Aused%20to%20automate%20spatially-stationary%20audio%20experiments%2C%20thus%20accelerating%20the%0Apace%20of%20audio%20research.%20Critically%2C%20the%20device%20may%20also%20be%20used%20as%20a%20moving%0Asound%20source%20in%20dynamic%20experiments%2C%20due%20to%20its%20quiet%20motor.%20This%20feature%0Adifferentiates%20our%20work%20from%20previous%20robotic%20acoustic%20research%20platforms.%0AValidation%20that%20the%20robot%20enables%20high%20quality%20audio%20data%20collection%20is%0Aprovided%20through%20various%20experiments%20and%20acoustic%20measurements.%20These%0Aexperiments%20also%20demonstrate%20how%20the%20robot%20might%20be%20used%20to%20study%20adaptive%0Abinaural%20beamforming.%20Design%20files%20are%20provided%20as%20open-source%20to%20stimulate%0Anovel%20audio%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Audio%2520Research%2520with%2520Robotic%2520Dummy%2520Heads%26entry.906535625%3DAustin%2520Lu%2520and%2520Kanad%2520Sarkar%2520and%2520Yongjie%2520Zhuang%2520and%2520Leo%2520Lin%2520and%2520Ryan%2520M%2520Corey%2520and%2520Andrew%2520C%2520Singer%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520robotic%2520dummy%2520head%2520that%2520fuses%2520the%2520acoustic%2520realism%2520of%250Aconventional%2520audiological%2520mannequins%2520with%2520the%2520mobility%2520of%2520robots.%2520The%2520proposed%250Adevice%2520is%2520capable%2520of%2520moving%252C%2520talking%252C%2520and%2520listening%2520as%2520people%2520do%252C%2520and%2520can%2520be%250Aused%2520to%2520automate%2520spatially-stationary%2520audio%2520experiments%252C%2520thus%2520accelerating%2520the%250Apace%2520of%2520audio%2520research.%2520Critically%252C%2520the%2520device%2520may%2520also%2520be%2520used%2520as%2520a%2520moving%250Asound%2520source%2520in%2520dynamic%2520experiments%252C%2520due%2520to%2520its%2520quiet%2520motor.%2520This%2520feature%250Adifferentiates%2520our%2520work%2520from%2520previous%2520robotic%2520acoustic%2520research%2520platforms.%250AValidation%2520that%2520the%2520robot%2520enables%2520high%2520quality%2520audio%2520data%2520collection%2520is%250Aprovided%2520through%2520various%2520experiments%2520and%2520acoustic%2520measurements.%2520These%250Aexperiments%2520also%2520demonstrate%2520how%2520the%2520robot%2520might%2520be%2520used%2520to%2520study%2520adaptive%250Abinaural%2520beamforming.%2520Design%2520files%2520are%2520provided%2520as%2520open-source%2520to%2520stimulate%250Anovel%2520audio%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Audio%20Research%20with%20Robotic%20Dummy%20Heads&entry.906535625=Austin%20Lu%20and%20Kanad%20Sarkar%20and%20Yongjie%20Zhuang%20and%20Leo%20Lin%20and%20Ryan%20M%20Corey%20and%20Andrew%20C%20Singer&entry.1292438233=%20%20This%20work%20introduces%20a%20robotic%20dummy%20head%20that%20fuses%20the%20acoustic%20realism%20of%0Aconventional%20audiological%20mannequins%20with%20the%20mobility%20of%20robots.%20The%20proposed%0Adevice%20is%20capable%20of%20moving%2C%20talking%2C%20and%20listening%20as%20people%20do%2C%20and%20can%20be%0Aused%20to%20automate%20spatially-stationary%20audio%20experiments%2C%20thus%20accelerating%20the%0Apace%20of%20audio%20research.%20Critically%2C%20the%20device%20may%20also%20be%20used%20as%20a%20moving%0Asound%20source%20in%20dynamic%20experiments%2C%20due%20to%20its%20quiet%20motor.%20This%20feature%0Adifferentiates%20our%20work%20from%20previous%20robotic%20acoustic%20research%20platforms.%0AValidation%20that%20the%20robot%20enables%20high%20quality%20audio%20data%20collection%20is%0Aprovided%20through%20various%20experiments%20and%20acoustic%20measurements.%20These%0Aexperiments%20also%20demonstrate%20how%20the%20robot%20might%20be%20used%20to%20study%20adaptive%0Abinaural%20beamforming.%20Design%20files%20are%20provided%20as%20open-source%20to%20stimulate%0Anovel%20audio%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04548v1&entry.124074799=Read"},
{"title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving", "author": "Qi Liu and Xinhao Zheng and Renqiu Xia and Xingzhi Qi and Qinxiang Cao and Junchi Yan", "abstract": "  As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.\n", "link": "http://arxiv.org/abs/2505.04528v1", "date": "2025-05-07", "relevancy": 1.8808, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4736}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4736}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Theorem%20Proving%3A%20Formulation%2C%20Framework%20and%20Benchmark%20for%20Formal%0A%20%20Problem-Solving&body=Title%3A%20Beyond%20Theorem%20Proving%3A%20Formulation%2C%20Framework%20and%20Benchmark%20for%20Formal%0A%20%20Problem-Solving%0AAuthor%3A%20Qi%20Liu%20and%20Xinhao%20Zheng%20and%20Renqiu%20Xia%20and%20Xingzhi%20Qi%20and%20Qinxiang%20Cao%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20As%20a%20seemingly%20self-explanatory%20task%2C%20problem-solving%20has%20been%20a%20significant%0Acomponent%20of%20science%20and%20engineering.%20However%2C%20a%20general%20yet%20concrete%0Aformulation%20of%20problem-solving%20itself%20is%20missing.%20With%20the%20recent%20development%0Aof%20AI-based%20problem-solving%20agents%2C%20the%20demand%20for%20process-level%20verifiability%0Ais%20rapidly%20increasing%20yet%20underexplored.%20To%20fill%20these%20gaps%2C%20we%20present%20a%0Aprincipled%20formulation%20of%20problem-solving%20as%20a%20deterministic%20Markov%20decision%0Aprocess%3B%20a%20novel%20framework%2C%20FPS%20%28Formal%20Problem-Solving%29%2C%20which%20utilizes%0Aexisting%20FTP%20%28formal%20theorem%20proving%29%20environments%20to%20perform%20process-verified%0Aproblem-solving%3B%20and%20D-FPS%20%28Deductive%20FPS%29%2C%20decoupling%20solving%20and%20answer%0Averification%20for%20better%20human-alignment.%20The%20expressiveness%2C%20soundness%20and%0Acompleteness%20of%20the%20frameworks%20are%20proven.%20We%20construct%20three%20benchmarks%20on%0Aproblem-solving%3A%20FormalMath500%2C%20a%20formalization%20of%20a%20subset%20of%20the%20MATH500%0Abenchmark%3B%20MiniF2F-Solving%20and%20PutnamBench-Solving%2C%20adaptations%20of%20FTP%0Abenchmarks%20MiniF2F%20and%20PutnamBench.%20For%20faithful%2C%20interpretable%2C%20and%0Ahuman-aligned%20evaluation%2C%20we%20propose%20RPE%20%28Restricted%20Propositional%0AEquivalence%29%2C%20a%20symbolic%20approach%20to%20determine%20the%20correctness%20of%20answers%20by%0Aformal%20verification.%20We%20evaluate%20four%20prevalent%20FTP%20models%20and%20two%20prompting%0Amethods%20as%20baselines%2C%20solving%20at%20most%2023.77%25%20of%20FormalMath500%2C%2027.47%25%20of%0AMiniF2F-Solving%2C%20and%200.31%25%20of%20PutnamBench-Solving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Theorem%2520Proving%253A%2520Formulation%252C%2520Framework%2520and%2520Benchmark%2520for%2520Formal%250A%2520%2520Problem-Solving%26entry.906535625%3DQi%2520Liu%2520and%2520Xinhao%2520Zheng%2520and%2520Renqiu%2520Xia%2520and%2520Xingzhi%2520Qi%2520and%2520Qinxiang%2520Cao%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520As%2520a%2520seemingly%2520self-explanatory%2520task%252C%2520problem-solving%2520has%2520been%2520a%2520significant%250Acomponent%2520of%2520science%2520and%2520engineering.%2520However%252C%2520a%2520general%2520yet%2520concrete%250Aformulation%2520of%2520problem-solving%2520itself%2520is%2520missing.%2520With%2520the%2520recent%2520development%250Aof%2520AI-based%2520problem-solving%2520agents%252C%2520the%2520demand%2520for%2520process-level%2520verifiability%250Ais%2520rapidly%2520increasing%2520yet%2520underexplored.%2520To%2520fill%2520these%2520gaps%252C%2520we%2520present%2520a%250Aprincipled%2520formulation%2520of%2520problem-solving%2520as%2520a%2520deterministic%2520Markov%2520decision%250Aprocess%253B%2520a%2520novel%2520framework%252C%2520FPS%2520%2528Formal%2520Problem-Solving%2529%252C%2520which%2520utilizes%250Aexisting%2520FTP%2520%2528formal%2520theorem%2520proving%2529%2520environments%2520to%2520perform%2520process-verified%250Aproblem-solving%253B%2520and%2520D-FPS%2520%2528Deductive%2520FPS%2529%252C%2520decoupling%2520solving%2520and%2520answer%250Averification%2520for%2520better%2520human-alignment.%2520The%2520expressiveness%252C%2520soundness%2520and%250Acompleteness%2520of%2520the%2520frameworks%2520are%2520proven.%2520We%2520construct%2520three%2520benchmarks%2520on%250Aproblem-solving%253A%2520FormalMath500%252C%2520a%2520formalization%2520of%2520a%2520subset%2520of%2520the%2520MATH500%250Abenchmark%253B%2520MiniF2F-Solving%2520and%2520PutnamBench-Solving%252C%2520adaptations%2520of%2520FTP%250Abenchmarks%2520MiniF2F%2520and%2520PutnamBench.%2520For%2520faithful%252C%2520interpretable%252C%2520and%250Ahuman-aligned%2520evaluation%252C%2520we%2520propose%2520RPE%2520%2528Restricted%2520Propositional%250AEquivalence%2529%252C%2520a%2520symbolic%2520approach%2520to%2520determine%2520the%2520correctness%2520of%2520answers%2520by%250Aformal%2520verification.%2520We%2520evaluate%2520four%2520prevalent%2520FTP%2520models%2520and%2520two%2520prompting%250Amethods%2520as%2520baselines%252C%2520solving%2520at%2520most%252023.77%2525%2520of%2520FormalMath500%252C%252027.47%2525%2520of%250AMiniF2F-Solving%252C%2520and%25200.31%2525%2520of%2520PutnamBench-Solving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Theorem%20Proving%3A%20Formulation%2C%20Framework%20and%20Benchmark%20for%20Formal%0A%20%20Problem-Solving&entry.906535625=Qi%20Liu%20and%20Xinhao%20Zheng%20and%20Renqiu%20Xia%20and%20Xingzhi%20Qi%20and%20Qinxiang%20Cao%20and%20Junchi%20Yan&entry.1292438233=%20%20As%20a%20seemingly%20self-explanatory%20task%2C%20problem-solving%20has%20been%20a%20significant%0Acomponent%20of%20science%20and%20engineering.%20However%2C%20a%20general%20yet%20concrete%0Aformulation%20of%20problem-solving%20itself%20is%20missing.%20With%20the%20recent%20development%0Aof%20AI-based%20problem-solving%20agents%2C%20the%20demand%20for%20process-level%20verifiability%0Ais%20rapidly%20increasing%20yet%20underexplored.%20To%20fill%20these%20gaps%2C%20we%20present%20a%0Aprincipled%20formulation%20of%20problem-solving%20as%20a%20deterministic%20Markov%20decision%0Aprocess%3B%20a%20novel%20framework%2C%20FPS%20%28Formal%20Problem-Solving%29%2C%20which%20utilizes%0Aexisting%20FTP%20%28formal%20theorem%20proving%29%20environments%20to%20perform%20process-verified%0Aproblem-solving%3B%20and%20D-FPS%20%28Deductive%20FPS%29%2C%20decoupling%20solving%20and%20answer%0Averification%20for%20better%20human-alignment.%20The%20expressiveness%2C%20soundness%20and%0Acompleteness%20of%20the%20frameworks%20are%20proven.%20We%20construct%20three%20benchmarks%20on%0Aproblem-solving%3A%20FormalMath500%2C%20a%20formalization%20of%20a%20subset%20of%20the%20MATH500%0Abenchmark%3B%20MiniF2F-Solving%20and%20PutnamBench-Solving%2C%20adaptations%20of%20FTP%0Abenchmarks%20MiniF2F%20and%20PutnamBench.%20For%20faithful%2C%20interpretable%2C%20and%0Ahuman-aligned%20evaluation%2C%20we%20propose%20RPE%20%28Restricted%20Propositional%0AEquivalence%29%2C%20a%20symbolic%20approach%20to%20determine%20the%20correctness%20of%20answers%20by%0Aformal%20verification.%20We%20evaluate%20four%20prevalent%20FTP%20models%20and%20two%20prompting%0Amethods%20as%20baselines%2C%20solving%20at%20most%2023.77%25%20of%20FormalMath500%2C%2027.47%25%20of%0AMiniF2F-Solving%2C%20and%200.31%25%20of%20PutnamBench-Solving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04528v1&entry.124074799=Read"},
{"title": "Multi-turn Consistent Image Editing", "author": "Zijun Zhou and Yingying Deng and Xiangyu He and Weiming Dong and Fan Tang", "abstract": "  Many real-world applications, such as interactive photo retouching, artistic\ncontent creation, and product design, require flexible and iterative image\nediting. However, existing image editing methods primarily focus on achieving\nthe desired modifications in a single step, which often struggles with\nambiguous user intent, complex transformations, or the need for progressive\nrefinements. As a result, these methods frequently produce inconsistent\noutcomes or fail to meet user expectations. To address these challenges, we\npropose a multi-turn image editing framework that enables users to iteratively\nrefine their edits, progressively achieving more satisfactory results. Our\napproach leverages flow matching for accurate image inversion and a\ndual-objective Linear Quadratic Regulators (LQR) for stable sampling,\neffectively mitigating error accumulation. Additionally, by analyzing the\nlayer-wise roles of transformers, we introduce a adaptive attention\nhighlighting method that enhances editability while preserving multi-turn\ncoherence. Extensive experiments demonstrate that our framework significantly\nimproves edit success rates and visual fidelity compared to existing methods.\n", "link": "http://arxiv.org/abs/2505.04320v1", "date": "2025-05-07", "relevancy": 1.1175, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.597}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5421}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-turn%20Consistent%20Image%20Editing&body=Title%3A%20Multi-turn%20Consistent%20Image%20Editing%0AAuthor%3A%20Zijun%20Zhou%20and%20Yingying%20Deng%20and%20Xiangyu%20He%20and%20Weiming%20Dong%20and%20Fan%20Tang%0AAbstract%3A%20%20%20Many%20real-world%20applications%2C%20such%20as%20interactive%20photo%20retouching%2C%20artistic%0Acontent%20creation%2C%20and%20product%20design%2C%20require%20flexible%20and%20iterative%20image%0Aediting.%20However%2C%20existing%20image%20editing%20methods%20primarily%20focus%20on%20achieving%0Athe%20desired%20modifications%20in%20a%20single%20step%2C%20which%20often%20struggles%20with%0Aambiguous%20user%20intent%2C%20complex%20transformations%2C%20or%20the%20need%20for%20progressive%0Arefinements.%20As%20a%20result%2C%20these%20methods%20frequently%20produce%20inconsistent%0Aoutcomes%20or%20fail%20to%20meet%20user%20expectations.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20multi-turn%20image%20editing%20framework%20that%20enables%20users%20to%20iteratively%0Arefine%20their%20edits%2C%20progressively%20achieving%20more%20satisfactory%20results.%20Our%0Aapproach%20leverages%20flow%20matching%20for%20accurate%20image%20inversion%20and%20a%0Adual-objective%20Linear%20Quadratic%20Regulators%20%28LQR%29%20for%20stable%20sampling%2C%0Aeffectively%20mitigating%20error%20accumulation.%20Additionally%2C%20by%20analyzing%20the%0Alayer-wise%20roles%20of%20transformers%2C%20we%20introduce%20a%20adaptive%20attention%0Ahighlighting%20method%20that%20enhances%20editability%20while%20preserving%20multi-turn%0Acoherence.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20significantly%0Aimproves%20edit%20success%20rates%20and%20visual%20fidelity%20compared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-turn%2520Consistent%2520Image%2520Editing%26entry.906535625%3DZijun%2520Zhou%2520and%2520Yingying%2520Deng%2520and%2520Xiangyu%2520He%2520and%2520Weiming%2520Dong%2520and%2520Fan%2520Tang%26entry.1292438233%3D%2520%2520Many%2520real-world%2520applications%252C%2520such%2520as%2520interactive%2520photo%2520retouching%252C%2520artistic%250Acontent%2520creation%252C%2520and%2520product%2520design%252C%2520require%2520flexible%2520and%2520iterative%2520image%250Aediting.%2520However%252C%2520existing%2520image%2520editing%2520methods%2520primarily%2520focus%2520on%2520achieving%250Athe%2520desired%2520modifications%2520in%2520a%2520single%2520step%252C%2520which%2520often%2520struggles%2520with%250Aambiguous%2520user%2520intent%252C%2520complex%2520transformations%252C%2520or%2520the%2520need%2520for%2520progressive%250Arefinements.%2520As%2520a%2520result%252C%2520these%2520methods%2520frequently%2520produce%2520inconsistent%250Aoutcomes%2520or%2520fail%2520to%2520meet%2520user%2520expectations.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520multi-turn%2520image%2520editing%2520framework%2520that%2520enables%2520users%2520to%2520iteratively%250Arefine%2520their%2520edits%252C%2520progressively%2520achieving%2520more%2520satisfactory%2520results.%2520Our%250Aapproach%2520leverages%2520flow%2520matching%2520for%2520accurate%2520image%2520inversion%2520and%2520a%250Adual-objective%2520Linear%2520Quadratic%2520Regulators%2520%2528LQR%2529%2520for%2520stable%2520sampling%252C%250Aeffectively%2520mitigating%2520error%2520accumulation.%2520Additionally%252C%2520by%2520analyzing%2520the%250Alayer-wise%2520roles%2520of%2520transformers%252C%2520we%2520introduce%2520a%2520adaptive%2520attention%250Ahighlighting%2520method%2520that%2520enhances%2520editability%2520while%2520preserving%2520multi-turn%250Acoherence.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520significantly%250Aimproves%2520edit%2520success%2520rates%2520and%2520visual%2520fidelity%2520compared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-turn%20Consistent%20Image%20Editing&entry.906535625=Zijun%20Zhou%20and%20Yingying%20Deng%20and%20Xiangyu%20He%20and%20Weiming%20Dong%20and%20Fan%20Tang&entry.1292438233=%20%20Many%20real-world%20applications%2C%20such%20as%20interactive%20photo%20retouching%2C%20artistic%0Acontent%20creation%2C%20and%20product%20design%2C%20require%20flexible%20and%20iterative%20image%0Aediting.%20However%2C%20existing%20image%20editing%20methods%20primarily%20focus%20on%20achieving%0Athe%20desired%20modifications%20in%20a%20single%20step%2C%20which%20often%20struggles%20with%0Aambiguous%20user%20intent%2C%20complex%20transformations%2C%20or%20the%20need%20for%20progressive%0Arefinements.%20As%20a%20result%2C%20these%20methods%20frequently%20produce%20inconsistent%0Aoutcomes%20or%20fail%20to%20meet%20user%20expectations.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20multi-turn%20image%20editing%20framework%20that%20enables%20users%20to%20iteratively%0Arefine%20their%20edits%2C%20progressively%20achieving%20more%20satisfactory%20results.%20Our%0Aapproach%20leverages%20flow%20matching%20for%20accurate%20image%20inversion%20and%20a%0Adual-objective%20Linear%20Quadratic%20Regulators%20%28LQR%29%20for%20stable%20sampling%2C%0Aeffectively%20mitigating%20error%20accumulation.%20Additionally%2C%20by%20analyzing%20the%0Alayer-wise%20roles%20of%20transformers%2C%20we%20introduce%20a%20adaptive%20attention%0Ahighlighting%20method%20that%20enhances%20editability%20while%20preserving%20multi-turn%0Acoherence.%20Extensive%20experiments%20demonstrate%20that%20our%20framework%20significantly%0Aimproves%20edit%20success%20rates%20and%20visual%20fidelity%20compared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04320v1&entry.124074799=Read"},
{"title": "Deep Learning Innovations for Energy Efficiency: Advances in\n  Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable\n  Grid", "author": "Stavros Sykiotis", "abstract": "  The global energy landscape is undergoing a profound transformation, often\nreferred to as the energy transition, driven by the urgent need to mitigate\nclimate change, reduce greenhouse gas emissions, and ensure sustainable energy\nsupplies. However, the undoubted complexity of new investments in renewables,\nas well as the phase out of high CO2-emission energy sources, hampers the pace\nof the energy transition and raises doubts as to whether new renewable energy\nsources are capable of solely meeting the climate target goals. This highlights\nthe need to investigate alternative pathways to accelerate the energy\ntransition, by identifying human activity domains with higher/excessive energy\ndemands. Two notable examples where there is room for improvement, in the sense\nof reducing energy consumption and consequently CO2 emissions, are residential\nenergy consumption and road transport. This dissertation investigates the\ndevelopment of novel Deep Learning techniques to create tools which solve\nlimitations in these two key energy domains. Reduction of residential energy\nconsumption can be achieved by empowering end-users with the user of\nNon-Intrusive Load Monitoring, whereas optimization of EV charging with Deep\nReinforcement Learning can tackle road transport decarbonization.\n", "link": "http://arxiv.org/abs/2505.04367v1", "date": "2025-05-07", "relevancy": 1.4156, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4813}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4634}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Innovations%20for%20Energy%20Efficiency%3A%20Advances%20in%0A%20%20Non-Intrusive%20Load%20Monitoring%20and%20EV%20Charging%20Optimization%20for%20a%20Sustainable%0A%20%20Grid&body=Title%3A%20Deep%20Learning%20Innovations%20for%20Energy%20Efficiency%3A%20Advances%20in%0A%20%20Non-Intrusive%20Load%20Monitoring%20and%20EV%20Charging%20Optimization%20for%20a%20Sustainable%0A%20%20Grid%0AAuthor%3A%20Stavros%20Sykiotis%0AAbstract%3A%20%20%20The%20global%20energy%20landscape%20is%20undergoing%20a%20profound%20transformation%2C%20often%0Areferred%20to%20as%20the%20energy%20transition%2C%20driven%20by%20the%20urgent%20need%20to%20mitigate%0Aclimate%20change%2C%20reduce%20greenhouse%20gas%20emissions%2C%20and%20ensure%20sustainable%20energy%0Asupplies.%20However%2C%20the%20undoubted%20complexity%20of%20new%20investments%20in%20renewables%2C%0Aas%20well%20as%20the%20phase%20out%20of%20high%20CO2-emission%20energy%20sources%2C%20hampers%20the%20pace%0Aof%20the%20energy%20transition%20and%20raises%20doubts%20as%20to%20whether%20new%20renewable%20energy%0Asources%20are%20capable%20of%20solely%20meeting%20the%20climate%20target%20goals.%20This%20highlights%0Athe%20need%20to%20investigate%20alternative%20pathways%20to%20accelerate%20the%20energy%0Atransition%2C%20by%20identifying%20human%20activity%20domains%20with%20higher/excessive%20energy%0Ademands.%20Two%20notable%20examples%20where%20there%20is%20room%20for%20improvement%2C%20in%20the%20sense%0Aof%20reducing%20energy%20consumption%20and%20consequently%20CO2%20emissions%2C%20are%20residential%0Aenergy%20consumption%20and%20road%20transport.%20This%20dissertation%20investigates%20the%0Adevelopment%20of%20novel%20Deep%20Learning%20techniques%20to%20create%20tools%20which%20solve%0Alimitations%20in%20these%20two%20key%20energy%20domains.%20Reduction%20of%20residential%20energy%0Aconsumption%20can%20be%20achieved%20by%20empowering%20end-users%20with%20the%20user%20of%0ANon-Intrusive%20Load%20Monitoring%2C%20whereas%20optimization%20of%20EV%20charging%20with%20Deep%0AReinforcement%20Learning%20can%20tackle%20road%20transport%20decarbonization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Innovations%2520for%2520Energy%2520Efficiency%253A%2520Advances%2520in%250A%2520%2520Non-Intrusive%2520Load%2520Monitoring%2520and%2520EV%2520Charging%2520Optimization%2520for%2520a%2520Sustainable%250A%2520%2520Grid%26entry.906535625%3DStavros%2520Sykiotis%26entry.1292438233%3D%2520%2520The%2520global%2520energy%2520landscape%2520is%2520undergoing%2520a%2520profound%2520transformation%252C%2520often%250Areferred%2520to%2520as%2520the%2520energy%2520transition%252C%2520driven%2520by%2520the%2520urgent%2520need%2520to%2520mitigate%250Aclimate%2520change%252C%2520reduce%2520greenhouse%2520gas%2520emissions%252C%2520and%2520ensure%2520sustainable%2520energy%250Asupplies.%2520However%252C%2520the%2520undoubted%2520complexity%2520of%2520new%2520investments%2520in%2520renewables%252C%250Aas%2520well%2520as%2520the%2520phase%2520out%2520of%2520high%2520CO2-emission%2520energy%2520sources%252C%2520hampers%2520the%2520pace%250Aof%2520the%2520energy%2520transition%2520and%2520raises%2520doubts%2520as%2520to%2520whether%2520new%2520renewable%2520energy%250Asources%2520are%2520capable%2520of%2520solely%2520meeting%2520the%2520climate%2520target%2520goals.%2520This%2520highlights%250Athe%2520need%2520to%2520investigate%2520alternative%2520pathways%2520to%2520accelerate%2520the%2520energy%250Atransition%252C%2520by%2520identifying%2520human%2520activity%2520domains%2520with%2520higher/excessive%2520energy%250Ademands.%2520Two%2520notable%2520examples%2520where%2520there%2520is%2520room%2520for%2520improvement%252C%2520in%2520the%2520sense%250Aof%2520reducing%2520energy%2520consumption%2520and%2520consequently%2520CO2%2520emissions%252C%2520are%2520residential%250Aenergy%2520consumption%2520and%2520road%2520transport.%2520This%2520dissertation%2520investigates%2520the%250Adevelopment%2520of%2520novel%2520Deep%2520Learning%2520techniques%2520to%2520create%2520tools%2520which%2520solve%250Alimitations%2520in%2520these%2520two%2520key%2520energy%2520domains.%2520Reduction%2520of%2520residential%2520energy%250Aconsumption%2520can%2520be%2520achieved%2520by%2520empowering%2520end-users%2520with%2520the%2520user%2520of%250ANon-Intrusive%2520Load%2520Monitoring%252C%2520whereas%2520optimization%2520of%2520EV%2520charging%2520with%2520Deep%250AReinforcement%2520Learning%2520can%2520tackle%2520road%2520transport%2520decarbonization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Innovations%20for%20Energy%20Efficiency%3A%20Advances%20in%0A%20%20Non-Intrusive%20Load%20Monitoring%20and%20EV%20Charging%20Optimization%20for%20a%20Sustainable%0A%20%20Grid&entry.906535625=Stavros%20Sykiotis&entry.1292438233=%20%20The%20global%20energy%20landscape%20is%20undergoing%20a%20profound%20transformation%2C%20often%0Areferred%20to%20as%20the%20energy%20transition%2C%20driven%20by%20the%20urgent%20need%20to%20mitigate%0Aclimate%20change%2C%20reduce%20greenhouse%20gas%20emissions%2C%20and%20ensure%20sustainable%20energy%0Asupplies.%20However%2C%20the%20undoubted%20complexity%20of%20new%20investments%20in%20renewables%2C%0Aas%20well%20as%20the%20phase%20out%20of%20high%20CO2-emission%20energy%20sources%2C%20hampers%20the%20pace%0Aof%20the%20energy%20transition%20and%20raises%20doubts%20as%20to%20whether%20new%20renewable%20energy%0Asources%20are%20capable%20of%20solely%20meeting%20the%20climate%20target%20goals.%20This%20highlights%0Athe%20need%20to%20investigate%20alternative%20pathways%20to%20accelerate%20the%20energy%0Atransition%2C%20by%20identifying%20human%20activity%20domains%20with%20higher/excessive%20energy%0Ademands.%20Two%20notable%20examples%20where%20there%20is%20room%20for%20improvement%2C%20in%20the%20sense%0Aof%20reducing%20energy%20consumption%20and%20consequently%20CO2%20emissions%2C%20are%20residential%0Aenergy%20consumption%20and%20road%20transport.%20This%20dissertation%20investigates%20the%0Adevelopment%20of%20novel%20Deep%20Learning%20techniques%20to%20create%20tools%20which%20solve%0Alimitations%20in%20these%20two%20key%20energy%20domains.%20Reduction%20of%20residential%20energy%0Aconsumption%20can%20be%20achieved%20by%20empowering%20end-users%20with%20the%20user%20of%0ANon-Intrusive%20Load%20Monitoring%2C%20whereas%20optimization%20of%20EV%20charging%20with%20Deep%0AReinforcement%20Learning%20can%20tackle%20road%20transport%20decarbonization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04367v1&entry.124074799=Read"},
{"title": "Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning", "author": "Hao Peng and Xiang Huang and Shuo Sun and Ruitong Zhang and Philip S. Yu", "abstract": "  DBSCAN, a well-known density-based clustering algorithm, has gained\nwidespread popularity and usage due to its effectiveness in identifying\nclusters of arbitrary shapes and handling noisy data. However, it encounters\nchallenges in producing satisfactory cluster results when confronted with\ndatasets of varying density scales, a common scenario in real-world\napplications. In this paper, we propose a novel Adaptive and Robust DBSCAN with\nMulti-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,\nwe model the initial dataset as a two-level encoding tree and categorize the\ndata vertices into distinct density partitions according to the information\nuncertainty determined in the encoding tree. Each partition is then assigned to\nan agent to find the best clustering parameters without manual assistance. The\nallocation is density-adaptive, enabling AR-DBSCAN to effectively handle\ndiverse density distributions within the dataset by utilizing distinct agents\nfor different partitions. Second, a multi-agent deep reinforcement learning\nguided automatic parameter searching process is designed. The process of\nadjusting the parameter search direction by perceiving the clustering\nenvironment is modeled as a Markov decision process. Using a weakly-supervised\nreward training policy network, each agent adaptively learns the optimal\nclustering parameters by interacting with the clusters. Third, a recursive\nsearch mechanism adaptable to the data's scale is presented, enabling efficient\nand controlled exploration of large parameter spaces. Extensive experiments are\nconducted on nine artificial datasets and a real-world dataset. The results of\noffline and online tasks show that AR-DBSCAN not only improves clustering\naccuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,\nbut also is capable of robustly finding dominant parameters.\n", "link": "http://arxiv.org/abs/2505.04339v1", "date": "2025-05-07", "relevancy": 1.5894, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5596}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5275}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20and%20Robust%20DBSCAN%20with%20Multi-agent%20Reinforcement%20Learning&body=Title%3A%20Adaptive%20and%20Robust%20DBSCAN%20with%20Multi-agent%20Reinforcement%20Learning%0AAuthor%3A%20Hao%20Peng%20and%20Xiang%20Huang%20and%20Shuo%20Sun%20and%20Ruitong%20Zhang%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20DBSCAN%2C%20a%20well-known%20density-based%20clustering%20algorithm%2C%20has%20gained%0Awidespread%20popularity%20and%20usage%20due%20to%20its%20effectiveness%20in%20identifying%0Aclusters%20of%20arbitrary%20shapes%20and%20handling%20noisy%20data.%20However%2C%20it%20encounters%0Achallenges%20in%20producing%20satisfactory%20cluster%20results%20when%20confronted%20with%0Adatasets%20of%20varying%20density%20scales%2C%20a%20common%20scenario%20in%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Adaptive%20and%20Robust%20DBSCAN%20with%0AMulti-agent%20Reinforcement%20Learning%20cluster%20framework%2C%20namely%20AR-DBSCAN.%20First%2C%0Awe%20model%20the%20initial%20dataset%20as%20a%20two-level%20encoding%20tree%20and%20categorize%20the%0Adata%20vertices%20into%20distinct%20density%20partitions%20according%20to%20the%20information%0Auncertainty%20determined%20in%20the%20encoding%20tree.%20Each%20partition%20is%20then%20assigned%20to%0Aan%20agent%20to%20find%20the%20best%20clustering%20parameters%20without%20manual%20assistance.%20The%0Aallocation%20is%20density-adaptive%2C%20enabling%20AR-DBSCAN%20to%20effectively%20handle%0Adiverse%20density%20distributions%20within%20the%20dataset%20by%20utilizing%20distinct%20agents%0Afor%20different%20partitions.%20Second%2C%20a%20multi-agent%20deep%20reinforcement%20learning%0Aguided%20automatic%20parameter%20searching%20process%20is%20designed.%20The%20process%20of%0Aadjusting%20the%20parameter%20search%20direction%20by%20perceiving%20the%20clustering%0Aenvironment%20is%20modeled%20as%20a%20Markov%20decision%20process.%20Using%20a%20weakly-supervised%0Areward%20training%20policy%20network%2C%20each%20agent%20adaptively%20learns%20the%20optimal%0Aclustering%20parameters%20by%20interacting%20with%20the%20clusters.%20Third%2C%20a%20recursive%0Asearch%20mechanism%20adaptable%20to%20the%20data%27s%20scale%20is%20presented%2C%20enabling%20efficient%0Aand%20controlled%20exploration%20of%20large%20parameter%20spaces.%20Extensive%20experiments%20are%0Aconducted%20on%20nine%20artificial%20datasets%20and%20a%20real-world%20dataset.%20The%20results%20of%0Aoffline%20and%20online%20tasks%20show%20that%20AR-DBSCAN%20not%20only%20improves%20clustering%0Aaccuracy%20by%20up%20to%20144.1%25%20and%20175.3%25%20in%20the%20NMI%20and%20ARI%20metrics%2C%20respectively%2C%0Abut%20also%20is%20capable%20of%20robustly%20finding%20dominant%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520and%2520Robust%2520DBSCAN%2520with%2520Multi-agent%2520Reinforcement%2520Learning%26entry.906535625%3DHao%2520Peng%2520and%2520Xiang%2520Huang%2520and%2520Shuo%2520Sun%2520and%2520Ruitong%2520Zhang%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520DBSCAN%252C%2520a%2520well-known%2520density-based%2520clustering%2520algorithm%252C%2520has%2520gained%250Awidespread%2520popularity%2520and%2520usage%2520due%2520to%2520its%2520effectiveness%2520in%2520identifying%250Aclusters%2520of%2520arbitrary%2520shapes%2520and%2520handling%2520noisy%2520data.%2520However%252C%2520it%2520encounters%250Achallenges%2520in%2520producing%2520satisfactory%2520cluster%2520results%2520when%2520confronted%2520with%250Adatasets%2520of%2520varying%2520density%2520scales%252C%2520a%2520common%2520scenario%2520in%2520real-world%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Adaptive%2520and%2520Robust%2520DBSCAN%2520with%250AMulti-agent%2520Reinforcement%2520Learning%2520cluster%2520framework%252C%2520namely%2520AR-DBSCAN.%2520First%252C%250Awe%2520model%2520the%2520initial%2520dataset%2520as%2520a%2520two-level%2520encoding%2520tree%2520and%2520categorize%2520the%250Adata%2520vertices%2520into%2520distinct%2520density%2520partitions%2520according%2520to%2520the%2520information%250Auncertainty%2520determined%2520in%2520the%2520encoding%2520tree.%2520Each%2520partition%2520is%2520then%2520assigned%2520to%250Aan%2520agent%2520to%2520find%2520the%2520best%2520clustering%2520parameters%2520without%2520manual%2520assistance.%2520The%250Aallocation%2520is%2520density-adaptive%252C%2520enabling%2520AR-DBSCAN%2520to%2520effectively%2520handle%250Adiverse%2520density%2520distributions%2520within%2520the%2520dataset%2520by%2520utilizing%2520distinct%2520agents%250Afor%2520different%2520partitions.%2520Second%252C%2520a%2520multi-agent%2520deep%2520reinforcement%2520learning%250Aguided%2520automatic%2520parameter%2520searching%2520process%2520is%2520designed.%2520The%2520process%2520of%250Aadjusting%2520the%2520parameter%2520search%2520direction%2520by%2520perceiving%2520the%2520clustering%250Aenvironment%2520is%2520modeled%2520as%2520a%2520Markov%2520decision%2520process.%2520Using%2520a%2520weakly-supervised%250Areward%2520training%2520policy%2520network%252C%2520each%2520agent%2520adaptively%2520learns%2520the%2520optimal%250Aclustering%2520parameters%2520by%2520interacting%2520with%2520the%2520clusters.%2520Third%252C%2520a%2520recursive%250Asearch%2520mechanism%2520adaptable%2520to%2520the%2520data%2527s%2520scale%2520is%2520presented%252C%2520enabling%2520efficient%250Aand%2520controlled%2520exploration%2520of%2520large%2520parameter%2520spaces.%2520Extensive%2520experiments%2520are%250Aconducted%2520on%2520nine%2520artificial%2520datasets%2520and%2520a%2520real-world%2520dataset.%2520The%2520results%2520of%250Aoffline%2520and%2520online%2520tasks%2520show%2520that%2520AR-DBSCAN%2520not%2520only%2520improves%2520clustering%250Aaccuracy%2520by%2520up%2520to%2520144.1%2525%2520and%2520175.3%2525%2520in%2520the%2520NMI%2520and%2520ARI%2520metrics%252C%2520respectively%252C%250Abut%2520also%2520is%2520capable%2520of%2520robustly%2520finding%2520dominant%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20and%20Robust%20DBSCAN%20with%20Multi-agent%20Reinforcement%20Learning&entry.906535625=Hao%20Peng%20and%20Xiang%20Huang%20and%20Shuo%20Sun%20and%20Ruitong%20Zhang%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20DBSCAN%2C%20a%20well-known%20density-based%20clustering%20algorithm%2C%20has%20gained%0Awidespread%20popularity%20and%20usage%20due%20to%20its%20effectiveness%20in%20identifying%0Aclusters%20of%20arbitrary%20shapes%20and%20handling%20noisy%20data.%20However%2C%20it%20encounters%0Achallenges%20in%20producing%20satisfactory%20cluster%20results%20when%20confronted%20with%0Adatasets%20of%20varying%20density%20scales%2C%20a%20common%20scenario%20in%20real-world%0Aapplications.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Adaptive%20and%20Robust%20DBSCAN%20with%0AMulti-agent%20Reinforcement%20Learning%20cluster%20framework%2C%20namely%20AR-DBSCAN.%20First%2C%0Awe%20model%20the%20initial%20dataset%20as%20a%20two-level%20encoding%20tree%20and%20categorize%20the%0Adata%20vertices%20into%20distinct%20density%20partitions%20according%20to%20the%20information%0Auncertainty%20determined%20in%20the%20encoding%20tree.%20Each%20partition%20is%20then%20assigned%20to%0Aan%20agent%20to%20find%20the%20best%20clustering%20parameters%20without%20manual%20assistance.%20The%0Aallocation%20is%20density-adaptive%2C%20enabling%20AR-DBSCAN%20to%20effectively%20handle%0Adiverse%20density%20distributions%20within%20the%20dataset%20by%20utilizing%20distinct%20agents%0Afor%20different%20partitions.%20Second%2C%20a%20multi-agent%20deep%20reinforcement%20learning%0Aguided%20automatic%20parameter%20searching%20process%20is%20designed.%20The%20process%20of%0Aadjusting%20the%20parameter%20search%20direction%20by%20perceiving%20the%20clustering%0Aenvironment%20is%20modeled%20as%20a%20Markov%20decision%20process.%20Using%20a%20weakly-supervised%0Areward%20training%20policy%20network%2C%20each%20agent%20adaptively%20learns%20the%20optimal%0Aclustering%20parameters%20by%20interacting%20with%20the%20clusters.%20Third%2C%20a%20recursive%0Asearch%20mechanism%20adaptable%20to%20the%20data%27s%20scale%20is%20presented%2C%20enabling%20efficient%0Aand%20controlled%20exploration%20of%20large%20parameter%20spaces.%20Extensive%20experiments%20are%0Aconducted%20on%20nine%20artificial%20datasets%20and%20a%20real-world%20dataset.%20The%20results%20of%0Aoffline%20and%20online%20tasks%20show%20that%20AR-DBSCAN%20not%20only%20improves%20clustering%0Aaccuracy%20by%20up%20to%20144.1%25%20and%20175.3%25%20in%20the%20NMI%20and%20ARI%20metrics%2C%20respectively%2C%0Abut%20also%20is%20capable%20of%20robustly%20finding%20dominant%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04339v1&entry.124074799=Read"},
{"title": "Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling", "author": "Matthew X. Burns and Qingyuan Hou and Michael C. Huang", "abstract": "  Analog dynamical accelerators (DXs) are a growing sub-field in computer\narchitecture research, offering order-of-magnitude gains in power efficiency\nand latency over traditional digital methods in several machine learning,\noptimization, and sampling tasks. However, limited-capacity accelerators\nrequire hybrid analog/digital algorithms to solve real-world problems, commonly\nusing large-neighborhood local search (LNLS) frameworks. Unlike fully digital\nalgorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no\nprincipled hyperparameter selection schemes, particularly limiting cross-device\ntraining and inference.\n  In this work, we provide non-asymptotic convergence guarantees for hybrid\nLNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools\nfrom classical sampling theory, we prove exponential KL-divergence convergence\nfor randomized and cyclic block selection strategies using ideal DXs. With\nfinite device variation, we provide explicit bounds on the 2-Wasserstein bias\nin terms of step duration, noise strength, and function parameters. Our BLD\nmodel provides a key link between established theory and novel computing\nplatforms, and our theoretical results provide a closed-form expression linking\ndevice variation, algorithm hyperparameters, and performance.\n", "link": "http://arxiv.org/abs/2410.06397v2", "date": "2025-05-07", "relevancy": 1.5389, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5077}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Accuracy%20Bounds%20for%20Hybrid%20Dynamical%20Optimization%20and%20Sampling&body=Title%3A%20Provable%20Accuracy%20Bounds%20for%20Hybrid%20Dynamical%20Optimization%20and%20Sampling%0AAuthor%3A%20Matthew%20X.%20Burns%20and%20Qingyuan%20Hou%20and%20Michael%20C.%20Huang%0AAbstract%3A%20%20%20Analog%20dynamical%20accelerators%20%28DXs%29%20are%20a%20growing%20sub-field%20in%20computer%0Aarchitecture%20research%2C%20offering%20order-of-magnitude%20gains%20in%20power%20efficiency%0Aand%20latency%20over%20traditional%20digital%20methods%20in%20several%20machine%20learning%2C%0Aoptimization%2C%20and%20sampling%20tasks.%20However%2C%20limited-capacity%20accelerators%0Arequire%20hybrid%20analog/digital%20algorithms%20to%20solve%20real-world%20problems%2C%20commonly%0Ausing%20large-neighborhood%20local%20search%20%28LNLS%29%20frameworks.%20Unlike%20fully%20digital%0Aalgorithms%2C%20hybrid%20LNLS%20has%20no%20non-asymptotic%20convergence%20guarantees%20and%20no%0Aprincipled%20hyperparameter%20selection%20schemes%2C%20particularly%20limiting%20cross-device%0Atraining%20and%20inference.%0A%20%20In%20this%20work%2C%20we%20provide%20non-asymptotic%20convergence%20guarantees%20for%20hybrid%0ALNLS%20by%20reducing%20to%20block%20Langevin%20Diffusion%20%28BLD%29%20algorithms.%20Adapting%20tools%0Afrom%20classical%20sampling%20theory%2C%20we%20prove%20exponential%20KL-divergence%20convergence%0Afor%20randomized%20and%20cyclic%20block%20selection%20strategies%20using%20ideal%20DXs.%20With%0Afinite%20device%20variation%2C%20we%20provide%20explicit%20bounds%20on%20the%202-Wasserstein%20bias%0Ain%20terms%20of%20step%20duration%2C%20noise%20strength%2C%20and%20function%20parameters.%20Our%20BLD%0Amodel%20provides%20a%20key%20link%20between%20established%20theory%20and%20novel%20computing%0Aplatforms%2C%20and%20our%20theoretical%20results%20provide%20a%20closed-form%20expression%20linking%0Adevice%20variation%2C%20algorithm%20hyperparameters%2C%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Accuracy%2520Bounds%2520for%2520Hybrid%2520Dynamical%2520Optimization%2520and%2520Sampling%26entry.906535625%3DMatthew%2520X.%2520Burns%2520and%2520Qingyuan%2520Hou%2520and%2520Michael%2520C.%2520Huang%26entry.1292438233%3D%2520%2520Analog%2520dynamical%2520accelerators%2520%2528DXs%2529%2520are%2520a%2520growing%2520sub-field%2520in%2520computer%250Aarchitecture%2520research%252C%2520offering%2520order-of-magnitude%2520gains%2520in%2520power%2520efficiency%250Aand%2520latency%2520over%2520traditional%2520digital%2520methods%2520in%2520several%2520machine%2520learning%252C%250Aoptimization%252C%2520and%2520sampling%2520tasks.%2520However%252C%2520limited-capacity%2520accelerators%250Arequire%2520hybrid%2520analog/digital%2520algorithms%2520to%2520solve%2520real-world%2520problems%252C%2520commonly%250Ausing%2520large-neighborhood%2520local%2520search%2520%2528LNLS%2529%2520frameworks.%2520Unlike%2520fully%2520digital%250Aalgorithms%252C%2520hybrid%2520LNLS%2520has%2520no%2520non-asymptotic%2520convergence%2520guarantees%2520and%2520no%250Aprincipled%2520hyperparameter%2520selection%2520schemes%252C%2520particularly%2520limiting%2520cross-device%250Atraining%2520and%2520inference.%250A%2520%2520In%2520this%2520work%252C%2520we%2520provide%2520non-asymptotic%2520convergence%2520guarantees%2520for%2520hybrid%250ALNLS%2520by%2520reducing%2520to%2520block%2520Langevin%2520Diffusion%2520%2528BLD%2529%2520algorithms.%2520Adapting%2520tools%250Afrom%2520classical%2520sampling%2520theory%252C%2520we%2520prove%2520exponential%2520KL-divergence%2520convergence%250Afor%2520randomized%2520and%2520cyclic%2520block%2520selection%2520strategies%2520using%2520ideal%2520DXs.%2520With%250Afinite%2520device%2520variation%252C%2520we%2520provide%2520explicit%2520bounds%2520on%2520the%25202-Wasserstein%2520bias%250Ain%2520terms%2520of%2520step%2520duration%252C%2520noise%2520strength%252C%2520and%2520function%2520parameters.%2520Our%2520BLD%250Amodel%2520provides%2520a%2520key%2520link%2520between%2520established%2520theory%2520and%2520novel%2520computing%250Aplatforms%252C%2520and%2520our%2520theoretical%2520results%2520provide%2520a%2520closed-form%2520expression%2520linking%250Adevice%2520variation%252C%2520algorithm%2520hyperparameters%252C%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Accuracy%20Bounds%20for%20Hybrid%20Dynamical%20Optimization%20and%20Sampling&entry.906535625=Matthew%20X.%20Burns%20and%20Qingyuan%20Hou%20and%20Michael%20C.%20Huang&entry.1292438233=%20%20Analog%20dynamical%20accelerators%20%28DXs%29%20are%20a%20growing%20sub-field%20in%20computer%0Aarchitecture%20research%2C%20offering%20order-of-magnitude%20gains%20in%20power%20efficiency%0Aand%20latency%20over%20traditional%20digital%20methods%20in%20several%20machine%20learning%2C%0Aoptimization%2C%20and%20sampling%20tasks.%20However%2C%20limited-capacity%20accelerators%0Arequire%20hybrid%20analog/digital%20algorithms%20to%20solve%20real-world%20problems%2C%20commonly%0Ausing%20large-neighborhood%20local%20search%20%28LNLS%29%20frameworks.%20Unlike%20fully%20digital%0Aalgorithms%2C%20hybrid%20LNLS%20has%20no%20non-asymptotic%20convergence%20guarantees%20and%20no%0Aprincipled%20hyperparameter%20selection%20schemes%2C%20particularly%20limiting%20cross-device%0Atraining%20and%20inference.%0A%20%20In%20this%20work%2C%20we%20provide%20non-asymptotic%20convergence%20guarantees%20for%20hybrid%0ALNLS%20by%20reducing%20to%20block%20Langevin%20Diffusion%20%28BLD%29%20algorithms.%20Adapting%20tools%0Afrom%20classical%20sampling%20theory%2C%20we%20prove%20exponential%20KL-divergence%20convergence%0Afor%20randomized%20and%20cyclic%20block%20selection%20strategies%20using%20ideal%20DXs.%20With%0Afinite%20device%20variation%2C%20we%20provide%20explicit%20bounds%20on%20the%202-Wasserstein%20bias%0Ain%20terms%20of%20step%20duration%2C%20noise%20strength%2C%20and%20function%20parameters.%20Our%20BLD%0Amodel%20provides%20a%20key%20link%20between%20established%20theory%20and%20novel%20computing%0Aplatforms%2C%20and%20our%20theoretical%20results%20provide%20a%20closed-form%20expression%20linking%0Adevice%20variation%2C%20algorithm%20hyperparameters%2C%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06397v2&entry.124074799=Read"},
{"title": "Deep Learning for Sea Surface Temperature Reconstruction under Cloud\n  Occlusion", "author": "Andrea Asperti and Ali Aydogdu and Angelo Greco and Fabio Merizzi and Pietro Miraglio and Beniamino Tartufoli and Alessandro Testa and Nadia Pinardi and Paolo Oddo", "abstract": "  Sea Surface Temperature (SST) reconstructions from satellite images affected\nby cloud gaps have been extensively documented in the past three decades. Here\nwe describe several Machine Learning models to fill the cloud-occluded areas\nstarting from MODIS Aqua nighttime L3 images. To tackle this challenge, we\nemployed a type of Convolutional Neural Network model (U-net) to reconstruct\ncloud-covered portions of satellite imagery while preserving the integrity of\nobserved values in cloud-free areas. We demonstrate the outstanding precision\nof U-net with respect to available products done using OI interpolation\nalgorithms. Our best-performing architecture show 50% lower root mean square\nerrors over established gap-filling methods.\n", "link": "http://arxiv.org/abs/2412.03413v2", "date": "2025-05-07", "relevancy": 1.5713, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5368}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5215}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Sea%20Surface%20Temperature%20Reconstruction%20under%20Cloud%0A%20%20Occlusion&body=Title%3A%20Deep%20Learning%20for%20Sea%20Surface%20Temperature%20Reconstruction%20under%20Cloud%0A%20%20Occlusion%0AAuthor%3A%20Andrea%20Asperti%20and%20Ali%20Aydogdu%20and%20Angelo%20Greco%20and%20Fabio%20Merizzi%20and%20Pietro%20Miraglio%20and%20Beniamino%20Tartufoli%20and%20Alessandro%20Testa%20and%20Nadia%20Pinardi%20and%20Paolo%20Oddo%0AAbstract%3A%20%20%20Sea%20Surface%20Temperature%20%28SST%29%20reconstructions%20from%20satellite%20images%20affected%0Aby%20cloud%20gaps%20have%20been%20extensively%20documented%20in%20the%20past%20three%20decades.%20Here%0Awe%20describe%20several%20Machine%20Learning%20models%20to%20fill%20the%20cloud-occluded%20areas%0Astarting%20from%20MODIS%20Aqua%20nighttime%20L3%20images.%20To%20tackle%20this%20challenge%2C%20we%0Aemployed%20a%20type%20of%20Convolutional%20Neural%20Network%20model%20%28U-net%29%20to%20reconstruct%0Acloud-covered%20portions%20of%20satellite%20imagery%20while%20preserving%20the%20integrity%20of%0Aobserved%20values%20in%20cloud-free%20areas.%20We%20demonstrate%20the%20outstanding%20precision%0Aof%20U-net%20with%20respect%20to%20available%20products%20done%20using%20OI%20interpolation%0Aalgorithms.%20Our%20best-performing%20architecture%20show%2050%25%20lower%20root%20mean%20square%0Aerrors%20over%20established%20gap-filling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03413v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Sea%2520Surface%2520Temperature%2520Reconstruction%2520under%2520Cloud%250A%2520%2520Occlusion%26entry.906535625%3DAndrea%2520Asperti%2520and%2520Ali%2520Aydogdu%2520and%2520Angelo%2520Greco%2520and%2520Fabio%2520Merizzi%2520and%2520Pietro%2520Miraglio%2520and%2520Beniamino%2520Tartufoli%2520and%2520Alessandro%2520Testa%2520and%2520Nadia%2520Pinardi%2520and%2520Paolo%2520Oddo%26entry.1292438233%3D%2520%2520Sea%2520Surface%2520Temperature%2520%2528SST%2529%2520reconstructions%2520from%2520satellite%2520images%2520affected%250Aby%2520cloud%2520gaps%2520have%2520been%2520extensively%2520documented%2520in%2520the%2520past%2520three%2520decades.%2520Here%250Awe%2520describe%2520several%2520Machine%2520Learning%2520models%2520to%2520fill%2520the%2520cloud-occluded%2520areas%250Astarting%2520from%2520MODIS%2520Aqua%2520nighttime%2520L3%2520images.%2520To%2520tackle%2520this%2520challenge%252C%2520we%250Aemployed%2520a%2520type%2520of%2520Convolutional%2520Neural%2520Network%2520model%2520%2528U-net%2529%2520to%2520reconstruct%250Acloud-covered%2520portions%2520of%2520satellite%2520imagery%2520while%2520preserving%2520the%2520integrity%2520of%250Aobserved%2520values%2520in%2520cloud-free%2520areas.%2520We%2520demonstrate%2520the%2520outstanding%2520precision%250Aof%2520U-net%2520with%2520respect%2520to%2520available%2520products%2520done%2520using%2520OI%2520interpolation%250Aalgorithms.%2520Our%2520best-performing%2520architecture%2520show%252050%2525%2520lower%2520root%2520mean%2520square%250Aerrors%2520over%2520established%2520gap-filling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03413v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Sea%20Surface%20Temperature%20Reconstruction%20under%20Cloud%0A%20%20Occlusion&entry.906535625=Andrea%20Asperti%20and%20Ali%20Aydogdu%20and%20Angelo%20Greco%20and%20Fabio%20Merizzi%20and%20Pietro%20Miraglio%20and%20Beniamino%20Tartufoli%20and%20Alessandro%20Testa%20and%20Nadia%20Pinardi%20and%20Paolo%20Oddo&entry.1292438233=%20%20Sea%20Surface%20Temperature%20%28SST%29%20reconstructions%20from%20satellite%20images%20affected%0Aby%20cloud%20gaps%20have%20been%20extensively%20documented%20in%20the%20past%20three%20decades.%20Here%0Awe%20describe%20several%20Machine%20Learning%20models%20to%20fill%20the%20cloud-occluded%20areas%0Astarting%20from%20MODIS%20Aqua%20nighttime%20L3%20images.%20To%20tackle%20this%20challenge%2C%20we%0Aemployed%20a%20type%20of%20Convolutional%20Neural%20Network%20model%20%28U-net%29%20to%20reconstruct%0Acloud-covered%20portions%20of%20satellite%20imagery%20while%20preserving%20the%20integrity%20of%0Aobserved%20values%20in%20cloud-free%20areas.%20We%20demonstrate%20the%20outstanding%20precision%0Aof%20U-net%20with%20respect%20to%20available%20products%20done%20using%20OI%20interpolation%0Aalgorithms.%20Our%20best-performing%20architecture%20show%2050%25%20lower%20root%20mean%20square%0Aerrors%20over%20established%20gap-filling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03413v2&entry.124074799=Read"},
{"title": "Qualitative Analysis of $\u03c9$-Regular Objectives on Robust MDPs", "author": "Ali Asadi and Krishnendu Chatterjee and Ehsan Kafshdar Goharshady and Mehrdad Karrabi and Ali Shafiee", "abstract": "  Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states.\n", "link": "http://arxiv.org/abs/2505.04539v1", "date": "2025-05-07", "relevancy": 1.3464, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4801}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.46}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qualitative%20Analysis%20of%20%24%CF%89%24-Regular%20Objectives%20on%20Robust%20MDPs&body=Title%3A%20Qualitative%20Analysis%20of%20%24%CF%89%24-Regular%20Objectives%20on%20Robust%20MDPs%0AAuthor%3A%20Ali%20Asadi%20and%20Krishnendu%20Chatterjee%20and%20Ehsan%20Kafshdar%20Goharshady%20and%20Mehrdad%20Karrabi%20and%20Ali%20Shafiee%0AAbstract%3A%20%20%20Robust%20Markov%20Decision%20Processes%20%28RMDPs%29%20generalize%20classical%20MDPs%20that%0Aconsider%20uncertainties%20in%20transition%20probabilities%20by%20defining%20a%20set%20of%0Apossible%20transition%20functions.%20An%20objective%20is%20a%20set%20of%20runs%20%28or%20infinite%0Atrajectories%29%20of%20the%20RMDP%2C%20and%20the%20value%20for%20an%20objective%20is%20the%20maximal%0Aprobability%20that%20the%20agent%20can%20guarantee%20against%20the%20adversarial%20environment.%0AWe%20consider%20%28a%29%20reachability%20objectives%2C%20where%20given%20a%20target%20set%20of%20states%2C%0Athe%20goal%20is%20to%20eventually%20arrive%20at%20one%20of%20them%3B%20and%20%28b%29%20parity%20objectives%2C%0Awhich%20are%20a%20canonical%20representation%20for%20%24%5Comega%24-regular%20objectives.%20The%0Aqualitative%20analysis%20problem%20asks%20whether%20the%20objective%20can%20be%20ensured%20with%0Aprobability%201.%0A%20%20In%20this%20work%2C%20we%20study%20the%20qualitative%20problem%20for%20reachability%20and%20parity%0Aobjectives%20on%20RMDPs%20without%20making%20any%20assumption%20over%20the%20structures%20of%20the%0ARMDPs%2C%20e.g.%2C%20unichain%20or%20aperiodic.%20Our%20contributions%20are%20twofold.%20We%20first%0Apresent%20efficient%20algorithms%20with%20oracle%20access%20to%20uncertainty%20sets%20that%20solve%0Aqualitative%20problems%20of%20reachability%20and%20parity%20objectives.%20We%20then%20report%0Aexperimental%20results%20demonstrating%20the%20effectiveness%20of%20our%20oracle-based%0Aapproach%20on%20classical%20RMDP%20examples%20from%20the%20literature%20scaling%20up%20to%20thousands%0Aof%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQualitative%2520Analysis%2520of%2520%2524%25CF%2589%2524-Regular%2520Objectives%2520on%2520Robust%2520MDPs%26entry.906535625%3DAli%2520Asadi%2520and%2520Krishnendu%2520Chatterjee%2520and%2520Ehsan%2520Kafshdar%2520Goharshady%2520and%2520Mehrdad%2520Karrabi%2520and%2520Ali%2520Shafiee%26entry.1292438233%3D%2520%2520Robust%2520Markov%2520Decision%2520Processes%2520%2528RMDPs%2529%2520generalize%2520classical%2520MDPs%2520that%250Aconsider%2520uncertainties%2520in%2520transition%2520probabilities%2520by%2520defining%2520a%2520set%2520of%250Apossible%2520transition%2520functions.%2520An%2520objective%2520is%2520a%2520set%2520of%2520runs%2520%2528or%2520infinite%250Atrajectories%2529%2520of%2520the%2520RMDP%252C%2520and%2520the%2520value%2520for%2520an%2520objective%2520is%2520the%2520maximal%250Aprobability%2520that%2520the%2520agent%2520can%2520guarantee%2520against%2520the%2520adversarial%2520environment.%250AWe%2520consider%2520%2528a%2529%2520reachability%2520objectives%252C%2520where%2520given%2520a%2520target%2520set%2520of%2520states%252C%250Athe%2520goal%2520is%2520to%2520eventually%2520arrive%2520at%2520one%2520of%2520them%253B%2520and%2520%2528b%2529%2520parity%2520objectives%252C%250Awhich%2520are%2520a%2520canonical%2520representation%2520for%2520%2524%255Comega%2524-regular%2520objectives.%2520The%250Aqualitative%2520analysis%2520problem%2520asks%2520whether%2520the%2520objective%2520can%2520be%2520ensured%2520with%250Aprobability%25201.%250A%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520qualitative%2520problem%2520for%2520reachability%2520and%2520parity%250Aobjectives%2520on%2520RMDPs%2520without%2520making%2520any%2520assumption%2520over%2520the%2520structures%2520of%2520the%250ARMDPs%252C%2520e.g.%252C%2520unichain%2520or%2520aperiodic.%2520Our%2520contributions%2520are%2520twofold.%2520We%2520first%250Apresent%2520efficient%2520algorithms%2520with%2520oracle%2520access%2520to%2520uncertainty%2520sets%2520that%2520solve%250Aqualitative%2520problems%2520of%2520reachability%2520and%2520parity%2520objectives.%2520We%2520then%2520report%250Aexperimental%2520results%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520oracle-based%250Aapproach%2520on%2520classical%2520RMDP%2520examples%2520from%2520the%2520literature%2520scaling%2520up%2520to%2520thousands%250Aof%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qualitative%20Analysis%20of%20%24%CF%89%24-Regular%20Objectives%20on%20Robust%20MDPs&entry.906535625=Ali%20Asadi%20and%20Krishnendu%20Chatterjee%20and%20Ehsan%20Kafshdar%20Goharshady%20and%20Mehrdad%20Karrabi%20and%20Ali%20Shafiee&entry.1292438233=%20%20Robust%20Markov%20Decision%20Processes%20%28RMDPs%29%20generalize%20classical%20MDPs%20that%0Aconsider%20uncertainties%20in%20transition%20probabilities%20by%20defining%20a%20set%20of%0Apossible%20transition%20functions.%20An%20objective%20is%20a%20set%20of%20runs%20%28or%20infinite%0Atrajectories%29%20of%20the%20RMDP%2C%20and%20the%20value%20for%20an%20objective%20is%20the%20maximal%0Aprobability%20that%20the%20agent%20can%20guarantee%20against%20the%20adversarial%20environment.%0AWe%20consider%20%28a%29%20reachability%20objectives%2C%20where%20given%20a%20target%20set%20of%20states%2C%0Athe%20goal%20is%20to%20eventually%20arrive%20at%20one%20of%20them%3B%20and%20%28b%29%20parity%20objectives%2C%0Awhich%20are%20a%20canonical%20representation%20for%20%24%5Comega%24-regular%20objectives.%20The%0Aqualitative%20analysis%20problem%20asks%20whether%20the%20objective%20can%20be%20ensured%20with%0Aprobability%201.%0A%20%20In%20this%20work%2C%20we%20study%20the%20qualitative%20problem%20for%20reachability%20and%20parity%0Aobjectives%20on%20RMDPs%20without%20making%20any%20assumption%20over%20the%20structures%20of%20the%0ARMDPs%2C%20e.g.%2C%20unichain%20or%20aperiodic.%20Our%20contributions%20are%20twofold.%20We%20first%0Apresent%20efficient%20algorithms%20with%20oracle%20access%20to%20uncertainty%20sets%20that%20solve%0Aqualitative%20problems%20of%20reachability%20and%20parity%20objectives.%20We%20then%20report%0Aexperimental%20results%20demonstrating%20the%20effectiveness%20of%20our%20oracle-based%0Aapproach%20on%20classical%20RMDP%20examples%20from%20the%20literature%20scaling%20up%20to%20thousands%0Aof%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04539v1&entry.124074799=Read"},
{"title": "Modeling Personalized Difficulty of Rehabilitation Exercises Using\n  Causal Trees", "author": "Nathaniel Dennler and Zhonghao Shi and Uksang Yoo and Stefanos Nikolaidis and Maja Matari\u0107", "abstract": "  Rehabilitation robots are often used in game-like interactions for\nrehabilitation to increase a person's motivation to complete rehabilitation\nexercises. By adjusting exercise difficulty for a specific user throughout the\nexercise interaction, robots can maximize both the user's rehabilitation\noutcomes and the their motivation throughout the exercise. Previous approaches\nhave assumed exercises have generic difficulty values that apply to all users\nequally, however, we identified that stroke survivors have varied and unique\nperceptions of exercise difficulty. For example, some stroke survivors found\nreaching vertically more difficult than reaching farther but lower while others\nfound reaching farther more challenging than reaching vertically. In this\npaper, we formulate a causal tree-based method to calculate exercise difficulty\nbased on the user's performance. We find that this approach accurately models\nexercise difficulty and provides a readily interpretable model of why that\nexercise is difficult for both users and caretakers.\n", "link": "http://arxiv.org/abs/2505.04583v1", "date": "2025-05-07", "relevancy": 1.5043, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5428}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5044}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Personalized%20Difficulty%20of%20Rehabilitation%20Exercises%20Using%0A%20%20Causal%20Trees&body=Title%3A%20Modeling%20Personalized%20Difficulty%20of%20Rehabilitation%20Exercises%20Using%0A%20%20Causal%20Trees%0AAuthor%3A%20Nathaniel%20Dennler%20and%20Zhonghao%20Shi%20and%20Uksang%20Yoo%20and%20Stefanos%20Nikolaidis%20and%20Maja%20Matari%C4%87%0AAbstract%3A%20%20%20Rehabilitation%20robots%20are%20often%20used%20in%20game-like%20interactions%20for%0Arehabilitation%20to%20increase%20a%20person%27s%20motivation%20to%20complete%20rehabilitation%0Aexercises.%20By%20adjusting%20exercise%20difficulty%20for%20a%20specific%20user%20throughout%20the%0Aexercise%20interaction%2C%20robots%20can%20maximize%20both%20the%20user%27s%20rehabilitation%0Aoutcomes%20and%20the%20their%20motivation%20throughout%20the%20exercise.%20Previous%20approaches%0Ahave%20assumed%20exercises%20have%20generic%20difficulty%20values%20that%20apply%20to%20all%20users%0Aequally%2C%20however%2C%20we%20identified%20that%20stroke%20survivors%20have%20varied%20and%20unique%0Aperceptions%20of%20exercise%20difficulty.%20For%20example%2C%20some%20stroke%20survivors%20found%0Areaching%20vertically%20more%20difficult%20than%20reaching%20farther%20but%20lower%20while%20others%0Afound%20reaching%20farther%20more%20challenging%20than%20reaching%20vertically.%20In%20this%0Apaper%2C%20we%20formulate%20a%20causal%20tree-based%20method%20to%20calculate%20exercise%20difficulty%0Abased%20on%20the%20user%27s%20performance.%20We%20find%20that%20this%20approach%20accurately%20models%0Aexercise%20difficulty%20and%20provides%20a%20readily%20interpretable%20model%20of%20why%20that%0Aexercise%20is%20difficult%20for%20both%20users%20and%20caretakers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Personalized%2520Difficulty%2520of%2520Rehabilitation%2520Exercises%2520Using%250A%2520%2520Causal%2520Trees%26entry.906535625%3DNathaniel%2520Dennler%2520and%2520Zhonghao%2520Shi%2520and%2520Uksang%2520Yoo%2520and%2520Stefanos%2520Nikolaidis%2520and%2520Maja%2520Matari%25C4%2587%26entry.1292438233%3D%2520%2520Rehabilitation%2520robots%2520are%2520often%2520used%2520in%2520game-like%2520interactions%2520for%250Arehabilitation%2520to%2520increase%2520a%2520person%2527s%2520motivation%2520to%2520complete%2520rehabilitation%250Aexercises.%2520By%2520adjusting%2520exercise%2520difficulty%2520for%2520a%2520specific%2520user%2520throughout%2520the%250Aexercise%2520interaction%252C%2520robots%2520can%2520maximize%2520both%2520the%2520user%2527s%2520rehabilitation%250Aoutcomes%2520and%2520the%2520their%2520motivation%2520throughout%2520the%2520exercise.%2520Previous%2520approaches%250Ahave%2520assumed%2520exercises%2520have%2520generic%2520difficulty%2520values%2520that%2520apply%2520to%2520all%2520users%250Aequally%252C%2520however%252C%2520we%2520identified%2520that%2520stroke%2520survivors%2520have%2520varied%2520and%2520unique%250Aperceptions%2520of%2520exercise%2520difficulty.%2520For%2520example%252C%2520some%2520stroke%2520survivors%2520found%250Areaching%2520vertically%2520more%2520difficult%2520than%2520reaching%2520farther%2520but%2520lower%2520while%2520others%250Afound%2520reaching%2520farther%2520more%2520challenging%2520than%2520reaching%2520vertically.%2520In%2520this%250Apaper%252C%2520we%2520formulate%2520a%2520causal%2520tree-based%2520method%2520to%2520calculate%2520exercise%2520difficulty%250Abased%2520on%2520the%2520user%2527s%2520performance.%2520We%2520find%2520that%2520this%2520approach%2520accurately%2520models%250Aexercise%2520difficulty%2520and%2520provides%2520a%2520readily%2520interpretable%2520model%2520of%2520why%2520that%250Aexercise%2520is%2520difficult%2520for%2520both%2520users%2520and%2520caretakers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Personalized%20Difficulty%20of%20Rehabilitation%20Exercises%20Using%0A%20%20Causal%20Trees&entry.906535625=Nathaniel%20Dennler%20and%20Zhonghao%20Shi%20and%20Uksang%20Yoo%20and%20Stefanos%20Nikolaidis%20and%20Maja%20Matari%C4%87&entry.1292438233=%20%20Rehabilitation%20robots%20are%20often%20used%20in%20game-like%20interactions%20for%0Arehabilitation%20to%20increase%20a%20person%27s%20motivation%20to%20complete%20rehabilitation%0Aexercises.%20By%20adjusting%20exercise%20difficulty%20for%20a%20specific%20user%20throughout%20the%0Aexercise%20interaction%2C%20robots%20can%20maximize%20both%20the%20user%27s%20rehabilitation%0Aoutcomes%20and%20the%20their%20motivation%20throughout%20the%20exercise.%20Previous%20approaches%0Ahave%20assumed%20exercises%20have%20generic%20difficulty%20values%20that%20apply%20to%20all%20users%0Aequally%2C%20however%2C%20we%20identified%20that%20stroke%20survivors%20have%20varied%20and%20unique%0Aperceptions%20of%20exercise%20difficulty.%20For%20example%2C%20some%20stroke%20survivors%20found%0Areaching%20vertically%20more%20difficult%20than%20reaching%20farther%20but%20lower%20while%20others%0Afound%20reaching%20farther%20more%20challenging%20than%20reaching%20vertically.%20In%20this%0Apaper%2C%20we%20formulate%20a%20causal%20tree-based%20method%20to%20calculate%20exercise%20difficulty%0Abased%20on%20the%20user%27s%20performance.%20We%20find%20that%20this%20approach%20accurately%20models%0Aexercise%20difficulty%20and%20provides%20a%20readily%20interpretable%20model%20of%20why%20that%0Aexercise%20is%20difficult%20for%20both%20users%20and%20caretakers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04583v1&entry.124074799=Read"},
{"title": "Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation", "author": "Abdulaziz Almuzairee and Rohan Patil and Dwait Bhatt and Henrik I. Christensen", "abstract": "  Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad\n", "link": "http://arxiv.org/abs/2505.04619v1", "date": "2025-05-07", "relevancy": 1.6913, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6018}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5651}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Merging%20and%20Disentangling%20Views%20in%20Visual%20Reinforcement%20Learning%20for%0A%20%20Robotic%20Manipulation&body=Title%3A%20Merging%20and%20Disentangling%20Views%20in%20Visual%20Reinforcement%20Learning%20for%0A%20%20Robotic%20Manipulation%0AAuthor%3A%20Abdulaziz%20Almuzairee%20and%20Rohan%20Patil%20and%20Dwait%20Bhatt%20and%20Henrik%20I.%20Christensen%0AAbstract%3A%20%20%20Vision%20is%20well-known%20for%20its%20use%20in%20manipulation%2C%20especially%20using%20visual%0Aservoing.%20To%20make%20it%20robust%2C%20multiple%20cameras%20are%20needed%20to%20expand%20the%20field%20of%0Aview.%20That%20is%20computationally%20challenging.%20Merging%20multiple%20views%20and%20using%0AQ-learning%20allows%20the%20design%20of%20more%20effective%20representations%20and%20optimization%0Aof%20sample%20efficiency.%20Such%20a%20solution%20might%20be%20expensive%20to%20deploy.%20To%20mitigate%0Athis%2C%20we%20introduce%20a%20Merge%20And%20Disentanglement%20%28MAD%29%20algorithm%20that%20efficiently%0Amerges%20views%20to%20increase%20sample%20efficiency%20while%20augmenting%20with%20single-view%0Afeatures%20to%20allow%20lightweight%20deployment%20and%20ensure%20robust%20policies.%20We%0Ademonstrate%20the%20efficiency%20and%20robustness%20of%20our%20approach%20using%20Meta-World%20and%0AManiSkill3.%20For%20project%20website%20and%20code%2C%20see%20https%3A//aalmuzairee.github.io/mad%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.04619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMerging%2520and%2520Disentangling%2520Views%2520in%2520Visual%2520Reinforcement%2520Learning%2520for%250A%2520%2520Robotic%2520Manipulation%26entry.906535625%3DAbdulaziz%2520Almuzairee%2520and%2520Rohan%2520Patil%2520and%2520Dwait%2520Bhatt%2520and%2520Henrik%2520I.%2520Christensen%26entry.1292438233%3D%2520%2520Vision%2520is%2520well-known%2520for%2520its%2520use%2520in%2520manipulation%252C%2520especially%2520using%2520visual%250Aservoing.%2520To%2520make%2520it%2520robust%252C%2520multiple%2520cameras%2520are%2520needed%2520to%2520expand%2520the%2520field%2520of%250Aview.%2520That%2520is%2520computationally%2520challenging.%2520Merging%2520multiple%2520views%2520and%2520using%250AQ-learning%2520allows%2520the%2520design%2520of%2520more%2520effective%2520representations%2520and%2520optimization%250Aof%2520sample%2520efficiency.%2520Such%2520a%2520solution%2520might%2520be%2520expensive%2520to%2520deploy.%2520To%2520mitigate%250Athis%252C%2520we%2520introduce%2520a%2520Merge%2520And%2520Disentanglement%2520%2528MAD%2529%2520algorithm%2520that%2520efficiently%250Amerges%2520views%2520to%2520increase%2520sample%2520efficiency%2520while%2520augmenting%2520with%2520single-view%250Afeatures%2520to%2520allow%2520lightweight%2520deployment%2520and%2520ensure%2520robust%2520policies.%2520We%250Ademonstrate%2520the%2520efficiency%2520and%2520robustness%2520of%2520our%2520approach%2520using%2520Meta-World%2520and%250AManiSkill3.%2520For%2520project%2520website%2520and%2520code%252C%2520see%2520https%253A//aalmuzairee.github.io/mad%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.04619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Merging%20and%20Disentangling%20Views%20in%20Visual%20Reinforcement%20Learning%20for%0A%20%20Robotic%20Manipulation&entry.906535625=Abdulaziz%20Almuzairee%20and%20Rohan%20Patil%20and%20Dwait%20Bhatt%20and%20Henrik%20I.%20Christensen&entry.1292438233=%20%20Vision%20is%20well-known%20for%20its%20use%20in%20manipulation%2C%20especially%20using%20visual%0Aservoing.%20To%20make%20it%20robust%2C%20multiple%20cameras%20are%20needed%20to%20expand%20the%20field%20of%0Aview.%20That%20is%20computationally%20challenging.%20Merging%20multiple%20views%20and%20using%0AQ-learning%20allows%20the%20design%20of%20more%20effective%20representations%20and%20optimization%0Aof%20sample%20efficiency.%20Such%20a%20solution%20might%20be%20expensive%20to%20deploy.%20To%20mitigate%0Athis%2C%20we%20introduce%20a%20Merge%20And%20Disentanglement%20%28MAD%29%20algorithm%20that%20efficiently%0Amerges%20views%20to%20increase%20sample%20efficiency%20while%20augmenting%20with%20single-view%0Afeatures%20to%20allow%20lightweight%20deployment%20and%20ensure%20robust%20policies.%20We%0Ademonstrate%20the%20efficiency%20and%20robustness%20of%20our%20approach%20using%20Meta-World%20and%0AManiSkill3.%20For%20project%20website%20and%20code%2C%20see%20https%3A//aalmuzairee.github.io/mad%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.04619v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


